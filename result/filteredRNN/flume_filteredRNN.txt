public static void main(String[] args)
{    System.out.println("- Downloader started");    File baseDirectory = new File(args[0]);    System.out.println("- Using base directory: " + baseDirectory.getAbsolutePath());            File mavenWrapperPropertyFile = new File(baseDirectory, MAVEN_WRAPPER_PROPERTIES_PATH);    String url = DEFAULT_DOWNLOAD_URL;    if (mavenWrapperPropertyFile.exists()) {        FileInputStream mavenWrapperPropertyFileInputStream = null;        try {            mavenWrapperPropertyFileInputStream = new FileInputStream(mavenWrapperPropertyFile);            Properties mavenWrapperProperties = new Properties();            mavenWrapperProperties.load(mavenWrapperPropertyFileInputStream);            url = mavenWrapperProperties.getProperty(PROPERTY_NAME_WRAPPER_URL, url);        } catch (IOException e) {            System.out.println("- ERROR loading '" + MAVEN_WRAPPER_PROPERTIES_PATH + "'");        } finally {            try {                if (mavenWrapperPropertyFileInputStream != null) {                    mavenWrapperPropertyFileInputStream.close();                }            } catch (IOException e) {                        }        }    }    System.out.println("- Downloading from: : " + url);    File outputFile = new File(baseDirectory.getAbsolutePath(), MAVEN_WRAPPER_JAR_PATH);    if (!outputFile.getParentFile().exists()) {        if (!outputFile.getParentFile().mkdirs()) {            System.out.println("- ERROR creating output direcrory '" + outputFile.getParentFile().getAbsolutePath() + "'");        }    }    System.out.println("- Downloading to: " + outputFile.getAbsolutePath());    try {        downloadFileFromURL(url, outputFile);        System.out.println("Done");        System.exit(0);    } catch (Throwable e) {        System.out.println("- Error downloading");        e.printStackTrace();        System.exit(1);    }}
0
private static void downloadFileFromURL(String urlString, File destination) throws Exception
{    URL website = new URL(urlString);    ReadableByteChannel rbc;    rbc = Channels.newChannel(website.openStream());    FileOutputStream fos = new FileOutputStream(destination);    fos.getChannel().transferFrom(rbc, 0, Long.MAX_VALUE);    fos.close();    rbc.close();}
0
public static RpcClient getThriftInstance(Properties props)
{    ThriftRpcClient client = new SecureThriftRpcClient();    client.configure(props);    return client;}
0
protected void configure(Properties properties) throws FlumeException
{    super.configure(properties);    serverPrincipal = properties.getProperty(SERVER_PRINCIPAL);    if (serverPrincipal == null || serverPrincipal.isEmpty()) {        throw new IllegalArgumentException("Flume in secure mode, but Flume config doesn't " + "specify a server principal to use for Kerberos auth.");    }    String clientPrincipal = properties.getProperty(CLIENT_PRINCIPAL);    String keytab = properties.getProperty(CLIENT_KEYTAB);    this.privilegedExecutor = FlumeAuthenticationUtil.getAuthenticator(clientPrincipal, keytab);    if (!privilegedExecutor.isAuthenticated()) {        throw new FlumeException("Authentication failed in Kerberos mode for " + "principal " + clientPrincipal + " keytab " + keytab);    }}
0
protected TTransport getTransport(TSocket tsocket) throws Exception
{    Map<String, String> saslProperties = new HashMap<String, String>();    saslProperties.put(Sasl.QOP, "auth");    String[] names;    try {        names = FlumeAuthenticationUtil.splitKerberosName(serverPrincipal);    } catch (IOException e) {        throw new FlumeException("Error while trying to resolve Principal name - " + serverPrincipal, e);    }    return new UgiSaslClientTransport("GSSAPI", null, names[0], names[1], saslProperties, null, tsocket, privilegedExecutor);}
0
public void open() throws FlumeException
{    try {        this.privilegedExecutor.execute(new PrivilegedExceptionAction<Void>() {            public Void run() throws FlumeException {                                                callSuperClassOpen();                return null;            }        });    } catch (InterruptedException e) {        throw new FlumeException("Interrupted while opening underlying transport", e);    } catch (Exception e) {        throw new FlumeException("Failed to open SASL transport", e);    }}
0
public Void run() throws FlumeException
{            callSuperClassOpen();    return null;}
0
private void callSuperClassOpen() throws FlumeException
{    try {        super.open();    } catch (TTransportException e) {        throw new FlumeException("Failed to open SASL transport", e);    }}
0
public static synchronized FlumeAuthenticator getAuthenticator(String principal, String keytab) throws SecurityException
{    if (principal == null && keytab == null) {        return SimpleAuthenticator.getSimpleAuthenticator();    }    Preconditions.checkArgument(principal != null, "Principal can not be null when keytab is provided");    Preconditions.checkArgument(keytab != null, "Keytab can not be null when Principal is provided");    if (kerbAuthenticator == null) {        kerbAuthenticator = new KerberosAuthenticator();    }    kerbAuthenticator.authenticate(principal, keytab);    return kerbAuthenticator;}
0
public static CallbackHandler getSaslGssCallbackHandler()
{    return new SaslRpcServer.SaslGssCallbackHandler();}
0
public static String[] splitKerberosName(String principal) throws IOException
{    String resolvedPrinc = SecurityUtil.getServerPrincipal(principal, "");    return SaslRpcServer.splitKerberosName(resolvedPrinc);}
0
 static void clearCredentials()
{    kerbAuthenticator = null;}
0
public T execute(PrivilegedAction<T> action)
{    return privilegedExecutor.execute(action);}
0
public T execute(PrivilegedExceptionAction<T> action) throws Exception
{    return privilegedExecutor.execute(action);}
0
public synchronized PrivilegedExecutor proxyAs(String proxyUserName)
{    if (proxyUserName == null || proxyUserName.isEmpty()) {        return this;    }    if (proxyCache.get(proxyUserName) == null) {        UserGroupInformation proxyUgi;        proxyUgi = UserGroupInformation.createProxyUser(proxyUserName, ugi);        printUGI(proxyUgi);        proxyCache.put(proxyUserName, new UGIExecutor(proxyUgi));    }    return proxyCache.get(proxyUserName);}
0
public boolean isAuthenticated()
{    return true;}
0
public synchronized void authenticate(String principal, String keytab)
{        Preconditions.checkArgument(principal != null && !principal.isEmpty(), "Invalid Kerberos principal: " + String.valueOf(principal));    Preconditions.checkArgument(keytab != null && !keytab.isEmpty(), "Invalid Kerberos keytab: " + String.valueOf(keytab));    File keytabFile = new File(keytab);    Preconditions.checkArgument(keytabFile.isFile() && keytabFile.canRead(), "Keytab is not a readable file: " + String.valueOf(keytab));        String resolvedPrincipal;    try {                        resolvedPrincipal = SecurityUtil.getServerPrincipal(principal, "");    } catch (IOException e) {        throw new IllegalArgumentException("Host lookup error resolving kerberos principal (" + principal + "). Exception follows.", e);    }    Preconditions.checkNotNull(resolvedPrincipal, "Resolved Principal must not be null");                            KerberosUser newUser = new KerberosUser(resolvedPrincipal, keytab);    Preconditions.checkState(prevUser == null || prevUser.equals(newUser), "Cannot use multiple kerberos principals in the same agent. " + " Must restart agent to use new principal or keytab. " + "Previous = %s, New = %s", prevUser, newUser);        if (!UserGroupInformation.isSecurityEnabled()) {        Configuration conf = new Configuration(false);        conf.set(HADOOP_SECURITY_AUTHENTICATION, "kerberos");        UserGroupInformation.setConfiguration(conf);    }        UserGroupInformation curUser = null;    try {        curUser = UserGroupInformation.getLoginUser();        if (curUser != null && !curUser.hasKerberosCredentials()) {            curUser = null;        }    } catch (IOException e) {            }    /*     *  if ugi is not null,     *     if ugi matches currently logged in kerberos user, we are good     *     else we are logged out, so relogin our ugi     *  else if ugi is null, login and populate state     */    try {        if (ugi != null) {            if (curUser != null && curUser.getUserName().equals(ugi.getUserName())) {                            } else {                                ugi.reloginFromKeytab();            }        } else {                        UserGroupInformation.loginUserFromKeytab(resolvedPrincipal, keytab);            this.ugi = UserGroupInformation.getLoginUser();            this.prevUser = new KerberosUser(resolvedPrincipal, keytab);            this.privilegedExecutor = new UGIExecutor(this.ugi);        }    } catch (IOException e) {        throw new SecurityException("Authentication error while attempting to " + "login as kerberos principal (" + resolvedPrincipal + ") using " + "keytab (" + keytab + "). Exception follows.", e);    }    printUGI(this.ugi);}
1
private void printUGI(UserGroupInformation ugi)
{    if (ugi != null) {                AuthenticationMethod authMethod = ugi.getAuthenticationMethod();            }}
1
public void startCredentialRefresher()
{        int CHECK_TGT_INTERVAL = 120;    ScheduledExecutorService scheduler = Executors.newScheduledThreadPool(1);    scheduler.scheduleWithFixedDelay(new Runnable() {        @Override        public void run() {            try {                ugi.checkTGTAndReloginFromKeytab();            } catch (IOException e) {                            }        }    }, CHECK_TGT_INTERVAL, CHECK_TGT_INTERVAL, TimeUnit.SECONDS);}
1
public void run()
{    try {        ugi.checkTGTAndReloginFromKeytab();    } catch (IOException e) {            }}
1
 String getUserName()
{    if (ugi != null) {        return ugi.getUserName();    } else {        return null;    }}
0
public String getPrincipal()
{    return principal;}
0
public String getKeyTab()
{    return keyTab;}
0
public boolean equals(Object obj)
{    if (obj == null) {        return false;    }    if (getClass() != obj.getClass()) {        return false;    }    final KerberosUser other = (KerberosUser) obj;    if ((this.principal == null) ? (other.principal != null) : !this.principal.equals(other.principal)) {        return false;    }    if ((this.keyTab == null) ? (other.keyTab != null) : !this.keyTab.equals(other.keyTab)) {        return false;    }    return true;}
0
public int hashCode()
{    int hash = 7;    hash = 41 * hash + (this.principal != null ? this.principal.hashCode() : 0);    hash = 41 * hash + (this.keyTab != null ? this.keyTab.hashCode() : 0);    return hash;}
0
public String toString()
{    return "{ principal: " + principal + ", keytab: " + keyTab + " }";}
0
public static SimpleAuthenticator getSimpleAuthenticator()
{    return SimpleAuthenticatorHolder.authenticator;}
0
public T execute(PrivilegedExceptionAction<T> action) throws Exception
{    return action.run();}
0
public T execute(PrivilegedAction<T> action)
{    return action.run();}
0
public synchronized PrivilegedExecutor proxyAs(String proxyUserName)
{    if (proxyUserName == null || proxyUserName.isEmpty()) {        return this;    }    if (proxyCache.get(proxyUserName) == null) {        UserGroupInformation proxyUgi;        try {            proxyUgi = UserGroupInformation.createProxyUser(proxyUserName, UserGroupInformation.getCurrentUser());        } catch (IOException e) {            throw new SecurityException("Unable to create proxy User", e);        }        proxyCache.put(proxyUserName, new UGIExecutor(proxyUgi));    }    return proxyCache.get(proxyUserName);}
0
public boolean isAuthenticated()
{    return false;}
0
public void startCredentialRefresher()
{}
0
public T execute(PrivilegedAction<T> action)
{    ensureValidAuth();    return ugi.doAs(action);}
0
public T execute(PrivilegedExceptionAction<T> action) throws Exception
{    ensureValidAuth();    return ugi.doAs(action);}
0
private void ensureValidAuth()
{    reloginUGI(ugi);    if (ugi.getAuthenticationMethod().equals(AuthenticationMethod.PROXY)) {        reloginUGI(ugi.getRealUser());    }}
0
private void reloginUGI(UserGroupInformation ugi)
{    try {        if (ugi.hasKerberosCredentials()) {            long now = System.currentTimeMillis();            if (now - lastReloginAttempt < MIN_TIME_BEFORE_RELOGIN) {                return;            }            lastReloginAttempt = now;            ugi.checkTGTAndReloginFromKeytab();        }    } catch (IOException e) {        throw new SecurityException("Error trying to relogin from keytab for user " + ugi.getUserName(), e);    }}
0
 String getUserName()
{    if (ugi != null) {        return ugi.getUserName();    } else {        return null;    }}
0
public static void startMiniKdc() throws Exception
{    workDir = new File(System.getProperty("test.dir", "target"), TestFlumeAuthenticator.class.getSimpleName());    flumeKeytab = new File(workDir, "flume.keytab");    aliceKeytab = new File(workDir, "alice.keytab");    conf = MiniKdc.createConf();    kdc = new MiniKdc(conf, workDir);    kdc.start();    kdc.createPrincipal(flumeKeytab, flumePrincipal);    flumePrincipal = flumePrincipal + "@" + kdc.getRealm();    kdc.createPrincipal(aliceKeytab, alicePrincipal);    alicePrincipal = alicePrincipal + "@" + kdc.getRealm();}
0
public static void stopMiniKdc()
{    if (kdc != null) {        kdc.stop();    }}
0
public void tearDown()
{        FlumeAuthenticationUtil.clearCredentials();}
0
public void testNullLogin() throws IOException
{    String principal = null;    String keytab = null;    FlumeAuthenticator authenticator = FlumeAuthenticationUtil.getAuthenticator(principal, keytab);    assertFalse(authenticator.isAuthenticated());}
0
public void testFlumeLogin() throws IOException
{    String principal = flumePrincipal;    String keytab = flumeKeytab.getAbsolutePath();    String expResult = principal;    FlumeAuthenticator authenticator = FlumeAuthenticationUtil.getAuthenticator(principal, keytab);    assertTrue(authenticator.isAuthenticated());    String result = ((KerberosAuthenticator) authenticator).getUserName();    assertEquals("Initial login failed", expResult, result);    authenticator = FlumeAuthenticationUtil.getAuthenticator(principal, keytab);    result = ((KerberosAuthenticator) authenticator).getUserName();    assertEquals("Re-login failed", expResult, result);    principal = alicePrincipal;    keytab = aliceKeytab.getAbsolutePath();    try {        authenticator = FlumeAuthenticationUtil.getAuthenticator(principal, keytab);        result = ((KerberosAuthenticator) authenticator).getUserName();        fail("Login should have failed with a new principal: " + result);    } catch (Exception ex) {        assertTrue("Login with a new principal failed, but for an unexpected " + "reason: " + ex.getMessage(), ex.getMessage().contains("Cannot use multiple kerberos principals"));    }}
0
public void testKerberosAuthenticatorExceptionInExecute() throws Exception
{    String principal = flumePrincipal;    String keytab = flumeKeytab.getAbsolutePath();    FlumeAuthenticator authenticator = FlumeAuthenticationUtil.getAuthenticator(principal, keytab);    assertTrue(authenticator instanceof KerberosAuthenticator);    authenticator.execute(new PrivilegedExceptionAction<Object>() {        @Override        public Object run() throws Exception {            throw new IOException();        }    });}
0
public Object run() throws Exception
{    throw new IOException();}
0
public void testSimpleAuthenticatorExceptionInExecute() throws Exception
{    FlumeAuthenticator authenticator = FlumeAuthenticationUtil.getAuthenticator(null, null);    assertTrue(authenticator instanceof SimpleAuthenticator);    authenticator.execute(new PrivilegedExceptionAction<Object>() {        @Override        public Object run() throws Exception {            throw new IOException();        }    });}
0
public Object run() throws Exception
{    throw new IOException();}
0
public void testProxyAs() throws IOException
{    String username = "alice";    String expResult = username;    FlumeAuthenticator authenticator = FlumeAuthenticationUtil.getAuthenticator(null, null);    String result = ((UGIExecutor) (authenticator.proxyAs(username))).getUserName();    assertEquals("Proxy as didn't generate the expected username", expResult, result);    authenticator = FlumeAuthenticationUtil.getAuthenticator(flumePrincipal, flumeKeytab.getAbsolutePath());    String login = ((KerberosAuthenticator) authenticator).getUserName();    assertEquals("Login succeeded, but the principal doesn't match", flumePrincipal, login);    result = ((UGIExecutor) (authenticator.proxyAs(username))).getUserName();    assertEquals("Proxy as didn't generate the expected username", expResult, result);}
0
public void testFlumeLoginPrincipalWithoutRealm() throws Exception
{    String principal = "flume";    File keytab = new File(workDir, "flume2.keytab");    kdc.createPrincipal(keytab, principal);    String expResult = principal + "@" + kdc.getRealm();    FlumeAuthenticator authenticator = FlumeAuthenticationUtil.getAuthenticator(principal, keytab.getAbsolutePath());    assertTrue(authenticator.isAuthenticated());    String result = ((KerberosAuthenticator) authenticator).getUserName();    assertEquals("Initial login failed", expResult, result);    authenticator = FlumeAuthenticationUtil.getAuthenticator(principal, keytab.getAbsolutePath());    result = ((KerberosAuthenticator) authenticator).getUserName();    assertEquals("Re-login failed", expResult, result);    principal = "alice";    keytab = aliceKeytab;    try {        authenticator = FlumeAuthenticationUtil.getAuthenticator(principal, keytab.getAbsolutePath());        result = ((KerberosAuthenticator) authenticator).getUserName();        fail("Login should have failed with a new principal: " + result);    } catch (Exception ex) {        assertTrue("Login with a new principal failed, but for an unexpected " + "reason: " + ex.getMessage(), ex.getMessage().contains("Cannot use multiple kerberos principals"));    }}
0
public boolean rebuild() throws IOException, Exception
{        List<LogFile.SequentialReader> logReaders = Lists.newArrayList();    for (File logFile : logFiles) {        try {            logReaders.add(LogFileFactory.getSequentialReader(logFile, null, fsyncPerTransaction));        } catch (EOFException e) {                    }    }    long transactionIDSeed = 0;    long writeOrderIDSeed = 0;    try {        for (LogFile.SequentialReader log : logReaders) {            LogRecord entry;            int fileID = log.getLogFileID();            while ((entry = log.next()) != null) {                int offset = entry.getOffset();                TransactionEventRecord record = entry.getEvent();                long trans = record.getTransactionID();                long writeOrderID = record.getLogWriteOrderID();                transactionIDSeed = Math.max(trans, transactionIDSeed);                writeOrderIDSeed = Math.max(writeOrderID, writeOrderIDSeed);                if (record.getRecordType() == TransactionEventRecord.Type.PUT.get()) {                    uncommittedPuts.put(record.getTransactionID(), new ComparableFlumeEventPointer(new FlumeEventPointer(fileID, offset), record.getLogWriteOrderID()));                } else if (record.getRecordType() == TransactionEventRecord.Type.TAKE.get()) {                    Take take = (Take) record;                    uncommittedTakes.put(record.getTransactionID(), new ComparableFlumeEventPointer(new FlumeEventPointer(take.getFileID(), take.getOffset()), record.getLogWriteOrderID()));                } else if (record.getRecordType() == TransactionEventRecord.Type.COMMIT.get()) {                    Commit commit = (Commit) record;                    if (commit.getType() == TransactionEventRecord.Type.PUT.get()) {                        Set<ComparableFlumeEventPointer> puts = uncommittedPuts.get(record.getTransactionID());                        if (puts != null) {                            for (ComparableFlumeEventPointer put : puts) {                                if (!pendingTakes.remove(put)) {                                    committedPuts.add(put);                                }                            }                        }                    } else {                        Set<ComparableFlumeEventPointer> takes = uncommittedTakes.get(record.getTransactionID());                        if (takes != null) {                            for (ComparableFlumeEventPointer take : takes) {                                if (!committedPuts.remove(take)) {                                    pendingTakes.add(take);                                }                            }                        }                    }                } else if (record.getRecordType() == TransactionEventRecord.Type.ROLLBACK.get()) {                    if (uncommittedPuts.containsKey(record.getTransactionID())) {                        uncommittedPuts.removeAll(record.getTransactionID());                    } else {                        uncommittedTakes.removeAll(record.getTransactionID());                    }                }            }        }    } catch (Exception e) {                return false;    } finally {        TransactionIDOracle.setSeed(transactionIDSeed);        WriteOrderOracle.setSeed(writeOrderIDSeed);        for (LogFile.SequentialReader reader : logReaders) {            reader.close();        }    }    Set<ComparableFlumeEventPointer> sortedPuts = Sets.newTreeSet(committedPuts);    int count = 0;    for (ComparableFlumeEventPointer put : sortedPuts) {        queue.addTail(put.pointer);        count++;    }        return true;}
1
private void writeCheckpoint() throws IOException
{    long checkpointLogOrderID = 0;    List<LogFile.MetaDataWriter> metaDataWriters = Lists.newArrayList();    for (File logFile : logFiles) {        String name = logFile.getName();        metaDataWriters.add(LogFileFactory.getMetaDataWriter(logFile, Integer.parseInt(name.substring(name.lastIndexOf('-') + 1))));    }    try {        if (queue.checkpoint(true)) {            checkpointLogOrderID = queue.getLogWriteOrderID();            for (LogFile.MetaDataWriter metaDataWriter : metaDataWriters) {                metaDataWriter.markCheckpoint(checkpointLogOrderID);            }        }    } catch (Exception e) {            } finally {        for (LogFile.MetaDataWriter metaDataWriter : metaDataWriters) {            metaDataWriter.close();        }    }}
1
public int compareTo(ComparableFlumeEventPointer o)
{    if (orderID < o.orderID) {        return -1;    } else {                return 1;    }}
0
public int hashCode()
{    return pointer.hashCode();}
0
public boolean equals(Object o)
{    if (this == o) {        return true;    }    if (o == null) {        return false;    }    if (o.getClass() != this.getClass()) {        return false;    }    return pointer.equals(((ComparableFlumeEventPointer) o).pointer);}
0
public static void main(String[] args) throws Exception
{    Options options = new Options();    Option opt = new Option("c", true, "checkpoint directory");    opt.setRequired(true);    options.addOption(opt);    opt = new Option("l", true, "comma-separated list of log directories");    opt.setRequired(true);    options.addOption(opt);    options.addOption(opt);    opt = new Option("t", true, "capacity of the channel");    opt.setRequired(true);    options.addOption(opt);    CommandLineParser parser = new GnuParser();    CommandLine cli = parser.parse(options, args);    File checkpointDir = new File(cli.getOptionValue("c"));    String[] logDirs = cli.getOptionValue("l").split(",");    List<File> logFiles = Lists.newArrayList();    for (String logDir : logDirs) {        logFiles.addAll(LogUtils.getLogs(new File(logDir)));    }    int capacity = Integer.parseInt(cli.getOptionValue("t"));    File checkpointFile = new File(checkpointDir, "checkpoint");    if (checkpointFile.exists()) {            } else {        EventQueueBackingStore backingStore = EventQueueBackingStoreFactory.get(checkpointFile, capacity, "channel", new FileChannelCounter("Main"));        FlumeEventQueue queue = new FlumeEventQueue(backingStore, new File(checkpointDir, "inflighttakes"), new File(checkpointDir, "inflightputs"), new File(checkpointDir, Log.QUEUE_SET));        CheckpointRebuilder rebuilder = new CheckpointRebuilder(logFiles, queue, true);        if (rebuilder.rebuild()) {            rebuilder.writeCheckpoint();        } else {                    }    }}
1
public void readFields(DataInput in) throws IOException
{    super.readFields(in);    type = in.readShort();}
0
 void writeProtos(OutputStream out) throws IOException
{    ProtosFactory.Commit.Builder commitBuilder = ProtosFactory.Commit.newBuilder();    commitBuilder.setType(type);    commitBuilder.build().writeDelimitedTo(out);}
0
 void readProtos(InputStream in) throws IOException
{    ProtosFactory.Commit commit = Preconditions.checkNotNull(ProtosFactory.Commit.parseDelimitedFrom(in), "Commit cannot be null");    type = (short) commit.getType();}
0
 short getType()
{    return type;}
0
public void write(DataOutput out) throws IOException
{    super.write(out);    out.writeShort(type);}
0
 short getRecordType()
{    return Type.COMMIT.get();}
0
public String toString()
{    StringBuilder builder = new StringBuilder();    builder.append("Commit [type=");    builder.append(type);    builder.append(", getLogWriteOrderID()=");    builder.append(getLogWriteOrderID());    builder.append(", getTransactionID()=");    builder.append(getTransactionID());    builder.append("]");    return builder.toString();}
0
public Encryptor.Builder<AESCTRNoPaddingEncryptor> newEncryptorBuilder()
{    return new EncryptorBuilder();}
0
public Decryptor.Builder<AESCTRNoPaddingDecryptor> newDecryptorBuilder()
{    return new DecryptorBuilder();}
0
public AESCTRNoPaddingEncryptor build()
{    ByteBuffer buffer = ByteBuffer.allocate(16);    byte[] seed = new byte[12];    SecureRandom random = new SecureRandom();    random.nextBytes(seed);    buffer.put(seed).putInt(1);    return new AESCTRNoPaddingEncryptor(key, buffer.array());}
0
public AESCTRNoPaddingDecryptor build()
{    return new AESCTRNoPaddingDecryptor(key, parameters);}
0
public byte[] getParameters()
{    return parameters;}
0
public String getCodec()
{    return TYPE;}
0
public byte[] encrypt(byte[] clearText)
{    return doFinal(cipher, clearText);}
0
public byte[] decrypt(byte[] cipherText)
{    return doFinal(cipher, cipherText);}
0
public String getCodec()
{    return TYPE;}
0
private static byte[] doFinal(Cipher cipher, byte[] input) throws DecryptionFailureException
{    try {        return cipher.doFinal(input);    } catch (Exception e) {        String msg = "Unable to encrypt or decrypt data " + TYPE + " input.length " + input.length;                throw new DecryptionFailureException(msg, e);    }}
1
private static Cipher getCipher(Key key, int mode, byte[] parameters)
{    try {        Cipher cipher = Cipher.getInstance(TYPE);        cipher.init(mode, key, new IvParameterSpec(parameters));        return cipher;    } catch (Exception e) {        String msg = "Unable to load key using transformation: " + TYPE;        if (e instanceof InvalidKeyException) {            try {                int maxAllowedLen = Cipher.getMaxAllowedKeyLength(TYPE);                if (maxAllowedLen < 256) {                    msg += "; Warning: Maximum allowed key length = " + maxAllowedLen + " with the available JCE security policy files. Have you" + " installed the JCE unlimited strength jurisdiction policy" + " files?";                }            } catch (NoSuchAlgorithmException ex) {                msg += "; Unable to find specified algorithm?";            }        }                throw Throwables.propagate(e);    }}
1
public Builder<T> setKey(Key key)
{    this.key = Preconditions.checkNotNull(key, "key cannot be null");    return this;}
0
public Builder<T> setKey(Key key)
{    this.key = Preconditions.checkNotNull(key, "key cannot be null");    return this;}
0
public Builder<T> setParameters(byte[] parameters)
{    this.parameters = parameters;    return this;}
0
public static CipherProvider.Encryptor getEncrypter(String cipherProviderType, Key key)
{    if (cipherProviderType == null) {        return null;    }    CipherProvider provider = getProvider(cipherProviderType);    return provider.newEncryptorBuilder().setKey(key).build();}
0
public static CipherProvider.Decryptor getDecrypter(String cipherProviderType, Key key, byte[] parameters)
{    if (cipherProviderType == null) {        return null;    }    CipherProvider provider = getProvider(cipherProviderType);    return provider.newDecryptorBuilder().setKey(key).setParameters(parameters).build();}
0
private static CipherProvider getProvider(String cipherProviderType)
{    Preconditions.checkNotNull(cipherProviderType, "cipher provider type must not be null");        CipherProviderType type;    try {        type = CipherProviderType.valueOf(cipherProviderType.toUpperCase(Locale.ENGLISH));    } catch (IllegalArgumentException e) {                type = CipherProviderType.OTHER;    }    Class<? extends CipherProvider> providerClass = type.getProviderClass();        if (providerClass == null) {        try {            Class c = Class.forName(cipherProviderType);            if (c != null && CipherProvider.class.isAssignableFrom(c)) {                providerClass = (Class<? extends CipherProvider>) c;            } else {                String errMessage = "Unable to instantiate provider from " + cipherProviderType;                                throw new FlumeException(errMessage);            }        } catch (ClassNotFoundException ex) {                        throw new FlumeException(ex);        }    }    try {        return providerClass.newInstance();    } catch (Exception ex) {        String errMessage = "Cannot instantiate provider: " + cipherProviderType;                throw new FlumeException(errMessage, ex);    }}
1
public Class<? extends CipherProvider> getProviderClass()
{    return providerClass;}
0
public Key getKey(String alias)
{    String passwordFile = keyStorePasswordFile.getAbsolutePath();    try {        char[] keyPassword = keyStorePassword;        if (aliasPasswordFileMap.containsKey(alias)) {            File keyPasswordFile = aliasPasswordFileMap.get(alias);            keyPassword = Files.toString(keyPasswordFile, Charsets.UTF_8).trim().toCharArray();            passwordFile = keyPasswordFile.getAbsolutePath();        }        Key key = ks.getKey(alias, keyPassword);        if (key == null) {            throw new IllegalStateException("KeyStore returned null for " + alias);        }        return key;    } catch (Exception e) {        String msg = e.getClass().getName() + ": " + e.getMessage() + ". " + "Key = " + alias + ", passwordFile = " + passwordFile;        throw new RuntimeException(msg, e);    }}
0
public KeyProvider build(Context context)
{    String keyStoreFileName = context.getString(EncryptionConfiguration.JCE_FILE_KEY_STORE_FILE);    String keyStorePasswordFileName = context.getString(EncryptionConfiguration.JCE_FILE_KEY_STORE_PASSWORD_FILE);    Preconditions.checkState(!Strings.isNullOrEmpty(keyStoreFileName), "KeyStore file not specified");    Preconditions.checkState(!Strings.isNullOrEmpty(keyStorePasswordFileName), "KeyStore password  file not specified");    Map<String, File> aliasPasswordFileMap = Maps.newHashMap();    String passwordProtectedKeys = context.getString(EncryptionConfiguration.JCE_FILE_KEYS);    Preconditions.checkState(!Strings.isNullOrEmpty(passwordProtectedKeys), "Keys available to KeyStore was not specified or empty");    for (String passwordName : passwordProtectedKeys.trim().split("\\s+")) {        String propertyName = Joiner.on(".").join(EncryptionConfiguration.JCE_FILE_KEYS, passwordName, EncryptionConfiguration.JCE_FILE_KEY_PASSWORD_FILE);        String passwordFileName = context.getString(propertyName, keyStorePasswordFileName);        File passwordFile = new File(passwordFileName.trim());        if (passwordFile.isFile()) {            aliasPasswordFileMap.put(passwordName, passwordFile);        } else {                    }    }    File keyStoreFile = new File(keyStoreFileName.trim());    File keyStorePasswordFile = new File(keyStorePasswordFileName.trim());    return new JCEFileKeyProvider(keyStoreFile, keyStorePasswordFile, aliasPasswordFileMap);}
1
public static KeyProvider getInstance(String keyProviderType, Context context)
{    Preconditions.checkNotNull(keyProviderType, "key provider type must not be null");        KeyProviderType type;    try {        type = KeyProviderType.valueOf(keyProviderType.toUpperCase(Locale.ENGLISH));    } catch (IllegalArgumentException e) {                type = KeyProviderType.OTHER;    }    Class<? extends KeyProvider.Builder> providerClass = type.getBuilderClass();        if (providerClass == null) {        try {            Class c = Class.forName(keyProviderType);            if (c != null && KeyProvider.Builder.class.isAssignableFrom(c)) {                providerClass = (Class<? extends KeyProvider.Builder>) c;            } else {                String errMessage = "Unable to instantiate Builder from " + keyProviderType;                                throw new FlumeException(errMessage);            }        } catch (ClassNotFoundException ex) {                        throw new FlumeException(ex);        }    }        KeyProvider.Builder provider;    try {        provider = providerClass.newInstance();    } catch (InstantiationException ex) {        String errMessage = "Cannot instantiate builder: " + keyProviderType;                throw new FlumeException(errMessage, ex);    } catch (IllegalAccessException ex) {        String errMessage = "Cannot instantiate builder: " + keyProviderType;                throw new FlumeException(errMessage, ex);    }    return provider.build(context);}
1
public Class<? extends KeyProvider.Builder> getBuilderClass()
{    return keyProviderClass;}
0
 int getSize()
{    return queueSize;}
0
 void setSize(int size)
{    queueSize = size;}
0
 int getHead()
{    return queueHead;}
0
 void setHead(int head)
{    queueHead = head;}
0
 int getCapacity()
{    return capacity;}
0
 String getName()
{    return name;}
0
protected void setLogWriteOrderID(long logWriteOrderID)
{    this.logWriteOrderID = logWriteOrderID;}
0
 long getLogWriteOrderID()
{    return logWriteOrderID;}
0
 static EventQueueBackingStore get(File checkpointFile, int capacity, String name, FileChannelCounter counter) throws Exception
{    return get(checkpointFile, capacity, name, counter, true);}
0
 static EventQueueBackingStore get(File checkpointFile, int capacity, String name, FileChannelCounter counter, boolean upgrade) throws Exception
{    return get(checkpointFile, null, capacity, name, counter, upgrade, false, false);}
0
 static EventQueueBackingStore get(File checkpointFile, File backupCheckpointDir, int capacity, String name, FileChannelCounter counter, boolean upgrade, boolean shouldBackup, boolean compressBackup) throws Exception
{    File metaDataFile = Serialization.getMetaDataFile(checkpointFile);    RandomAccessFile checkpointFileHandle = null;    try {        boolean checkpointExists = checkpointFile.exists();        boolean metaDataExists = metaDataFile.exists();        if (metaDataExists) {                        if (!checkpointExists || checkpointFile.length() == 0) {                                throw new BadCheckpointException("The last checkpoint was not completed correctly, " + "since Checkpoint file does not exist while metadata " + "file does.");            }        }                if (!checkpointExists) {            if (!checkpointFile.createNewFile()) {                throw new IOException("Cannot create " + checkpointFile);            }            return new EventQueueBackingStoreFileV3(checkpointFile, capacity, name, counter, backupCheckpointDir, shouldBackup, compressBackup);        }                if (metaDataExists) {            return new EventQueueBackingStoreFileV3(checkpointFile, capacity, name, counter, backupCheckpointDir, shouldBackup, compressBackup);        }        checkpointFileHandle = new RandomAccessFile(checkpointFile, "r");        int version = (int) checkpointFileHandle.readLong();        if (Serialization.VERSION_2 == version) {            if (upgrade) {                return upgrade(checkpointFile, capacity, name, backupCheckpointDir, shouldBackup, compressBackup, counter);            }            return new EventQueueBackingStoreFileV2(checkpointFile, capacity, name, counter);        }                throw new BadCheckpointException("Checkpoint file exists with " + Serialization.VERSION_3 + " but no metadata file found.");    } finally {        if (checkpointFileHandle != null) {            try {                checkpointFileHandle.close();            } catch (IOException e) {                            }        }    }}
1
private static EventQueueBackingStore upgrade(File checkpointFile, int capacity, String name, File backupCheckpointDir, boolean shouldBackup, boolean compressBackup, FileChannelCounter counter) throws Exception
{        EventQueueBackingStoreFileV2 backingStoreV2 = new EventQueueBackingStoreFileV2(checkpointFile, capacity, name, counter);    String backupName = checkpointFile.getName() + "-backup-" + System.currentTimeMillis();    Files.copy(checkpointFile, new File(checkpointFile.getParentFile(), backupName));    File metaDataFile = Serialization.getMetaDataFile(checkpointFile);    EventQueueBackingStoreFileV3.upgrade(backingStoreV2, checkpointFile, metaDataFile);    return new EventQueueBackingStoreFileV3(checkpointFile, capacity, name, counter, backupCheckpointDir, shouldBackup, compressBackup);}
1
protected long getCheckpointLogWriteOrderID()
{    return elementsBuffer.get(INDEX_WRITE_ORDER_ID);}
0
protected void backupCheckpoint(File backupDirectory) throws IOException
{    int availablePermits = backupCompletedSema.drainPermits();    Preconditions.checkState(availablePermits == 0, "Expected no permits to be available in the backup semaphore, " + "but " + availablePermits + " permits were available.");    if (slowdownBackup) {        try {            TimeUnit.SECONDS.sleep(10);        } catch (Exception ex) {            Throwables.propagate(ex);        }    }    File backupFile = new File(backupDirectory, BACKUP_COMPLETE_FILENAME);    if (backupExists(backupDirectory)) {        if (!backupFile.delete()) {            throw new IOException("Error while doing backup of checkpoint. Could " + "not remove" + backupFile.toString() + ".");        }    }    Serialization.deleteAllFiles(backupDirectory, Log.EXCLUDES);    File checkpointDir = checkpointFile.getParentFile();    File[] checkpointFiles = checkpointDir.listFiles();    Preconditions.checkNotNull(checkpointFiles, "Could not retrieve files " + "from the checkpoint directory. Cannot complete backup of the " + "checkpoint.");    for (File origFile : checkpointFiles) {        if (Log.EXCLUDES.contains(origFile.getName())) {            continue;        }        if (compressBackup && origFile.equals(checkpointFile)) {            Serialization.compressFile(origFile, new File(backupDirectory, origFile.getName() + COMPRESSED_FILE_EXTENSION));        } else {            Serialization.copyFile(origFile, new File(backupDirectory, origFile.getName()));        }    }    Preconditions.checkState(!backupFile.exists(), "The backup file exists " + "while it is not supposed to. Are multiple channels configured to use " + "this directory: " + backupDirectory.toString() + " as backup?");    if (!backupFile.createNewFile()) {            }}
1
public static boolean restoreBackup(File checkpointDir, File backupDir) throws IOException
{    if (!backupExists(backupDir)) {        return false;    }    Serialization.deleteAllFiles(checkpointDir, Log.EXCLUDES);    File[] backupFiles = backupDir.listFiles();    if (backupFiles == null) {        return false;    } else {        for (File backupFile : backupFiles) {            String fileName = backupFile.getName();            if (!fileName.equals(BACKUP_COMPLETE_FILENAME) && !fileName.equals(Log.FILE_LOCK)) {                if (fileName.endsWith(COMPRESSED_FILE_EXTENSION)) {                    Serialization.decompressFile(backupFile, new File(checkpointDir, fileName.substring(0, fileName.lastIndexOf("."))));                } else {                    Serialization.copyFile(backupFile, new File(checkpointDir, fileName));                }            }        }        return true;    }}
0
 void beginCheckpoint() throws IOException
{        if (shouldBackup) {        int permits = backupCompletedSema.drainPermits();        Preconditions.checkState(permits <= 1, "Expected only one or less " + "permits to checkpoint, but got " + String.valueOf(permits) + " permits");        if (permits < 1) {                        throw new IOException("Previous backup of checkpoint files is still " + "in progress. Will attempt to checkpoint only at the end of the " + "next checkpoint interval. Try increasing the checkpoint interval " + "if this error happens often.");        }    }        elementsBuffer.put(INDEX_CHECKPOINT_MARKER, CHECKPOINT_INCOMPLETE);    mappedBuffer.force();}
1
 void checkpoint() throws IOException
{    setLogWriteOrderID(WriteOrderOracle.next());        elementsBuffer.put(INDEX_WRITE_ORDER_ID, getLogWriteOrderID());    try {        writeCheckpointMetaData();    } catch (IOException e) {        throw new IOException("Error writing metadata", e);    }    Iterator<Integer> it = overwriteMap.keySet().iterator();    while (it.hasNext()) {        int index = it.next();        long value = overwriteMap.get(index);        elementsBuffer.put(index, value);        it.remove();    }    Preconditions.checkState(overwriteMap.isEmpty(), "concurrent update detected ");        elementsBuffer.put(INDEX_CHECKPOINT_MARKER, CHECKPOINT_COMPLETE);    mappedBuffer.force();    if (shouldBackup) {        startBackupThread();    }}
1
private void startBackupThread()
{    Preconditions.checkNotNull(checkpointBackUpExecutor, "Expected the checkpoint backup exector to be non-null, " + "but it is null. Checkpoint will not be backed up.");        checkpointBackUpExecutor.submit(new Runnable() {        @Override        public void run() {            boolean error = false;            try {                backupCheckpoint(backupDir);            } catch (Throwable throwable) {                fileChannelCounter.incrementCheckpointBackupWriteErrorCount();                error = true;                            } finally {                backupCompletedSema.release();            }            if (!error) {                            }        }    });}
1
public void run()
{    boolean error = false;    try {        backupCheckpoint(backupDir);    } catch (Throwable throwable) {        fileChannelCounter.incrementCheckpointBackupWriteErrorCount();        error = true;            } finally {        backupCompletedSema.release();    }    if (!error) {            }}
1
 void close()
{    mappedBuffer.force();    try {        checkpointFileHandle.close();    } catch (IOException e) {            }    if (checkpointBackUpExecutor != null && !checkpointBackUpExecutor.isShutdown()) {        checkpointBackUpExecutor.shutdown();        try {                        while (!checkpointBackUpExecutor.awaitTermination(1, TimeUnit.SECONDS)) {            }        } catch (InterruptedException ex) {                    }    }}
1
 long get(int index)
{    int realIndex = getPhysicalIndex(index);    long result = EMPTY;    if (overwriteMap.containsKey(realIndex)) {        result = overwriteMap.get(realIndex);    } else {        result = elementsBuffer.get(realIndex);    }    return result;}
0
 ImmutableSortedSet<Integer> getReferenceCounts()
{    return ImmutableSortedSet.copyOf(logFileIDReferenceCounts.keySet());}
0
 void put(int index, long value)
{    int realIndex = getPhysicalIndex(index);    overwriteMap.put(realIndex, value);}
0
 boolean syncRequired()
{    return overwriteMap.size() > 0;}
0
protected void incrementFileID(int fileID)
{    AtomicInteger counter = logFileIDReferenceCounts.get(fileID);    if (counter == null) {        counter = new AtomicInteger(0);        logFileIDReferenceCounts.put(fileID, counter);    }    counter.incrementAndGet();}
0
protected void decrementFileID(int fileID)
{    AtomicInteger counter = logFileIDReferenceCounts.get(fileID);    Preconditions.checkState(counter != null, "null counter ");    int count = counter.decrementAndGet();    if (count == 0) {        logFileIDReferenceCounts.remove(fileID);    }}
0
protected int getPhysicalIndex(int index)
{    return HEADER_SIZE + (getHead() + index) % getCapacity();}
0
protected static void allocate(File file, long totalBytes) throws IOException
{    RandomAccessFile checkpointFile = new RandomAccessFile(file, "rw");    boolean success = false;    try {        if (totalBytes <= MAX_ALLOC_BUFFER_SIZE) {            /*         * totalBytes <= MAX_ALLOC_BUFFER_SIZE, so this can be cast to int         * without a problem.         */            checkpointFile.write(new byte[(int) totalBytes]);        } else {            byte[] initBuffer = new byte[MAX_ALLOC_BUFFER_SIZE];            long remainingBytes = totalBytes;            while (remainingBytes >= MAX_ALLOC_BUFFER_SIZE) {                checkpointFile.write(initBuffer);                remainingBytes -= MAX_ALLOC_BUFFER_SIZE;            }            /*         * At this point, remainingBytes is < MAX_ALLOC_BUFFER_SIZE,         * so casting to int is fine.         */            if (remainingBytes > 0) {                checkpointFile.write(initBuffer, 0, (int) remainingBytes);            }        }        success = true;    } finally {        try {            checkpointFile.close();        } catch (IOException e) {            if (success) {                throw e;            }        }    }}
0
public static boolean backupExists(File backupDir)
{    return new File(backupDir, BACKUP_COMPLETE_FILENAME).exists();}
0
public static void main(String[] args) throws Exception
{    File file = new File(args[0]);    File inflightTakesFile = new File(args[1]);    File inflightPutsFile = new File(args[2]);    File queueSetDir = new File(args[3]);    if (!file.exists()) {        throw new IOException("File " + file + " does not exist");    }    if (file.length() == 0) {        throw new IOException("File " + file + " is empty");    }    int capacity = (int) ((file.length() - (HEADER_SIZE * 8L)) / 8L);    EventQueueBackingStoreFile backingStore = (EventQueueBackingStoreFile) EventQueueBackingStoreFactory.get(file, capacity, "debug", new FileChannelCounter("Main"), false);    System.out.println("File Reference Counts" + backingStore.logFileIDReferenceCounts);    System.out.println("Queue Capacity " + backingStore.getCapacity());    System.out.println("Queue Size " + backingStore.getSize());    System.out.println("Queue Head " + backingStore.getHead());    for (int index = 0; index < backingStore.getCapacity(); index++) {        long value = backingStore.get(backingStore.getPhysicalIndex(index));        int fileID = (int) (value >>> 32);        int offset = (int) value;        System.out.println(index + ":" + Long.toHexString(value) + " fileID = " + fileID + ", offset = " + offset);    }    FlumeEventQueue queue = new FlumeEventQueue(backingStore, inflightTakesFile, inflightPutsFile, queueSetDir);    SetMultimap<Long, Long> putMap = queue.deserializeInflightPuts();    System.out.println("Inflight Puts:");    for (Long txnID : putMap.keySet()) {        Set<Long> puts = putMap.get(txnID);        System.out.println("Transaction ID: " + String.valueOf(txnID));        for (long value : puts) {            int fileID = (int) (value >>> 32);            int offset = (int) value;            System.out.println(Long.toHexString(value) + " fileID = " + fileID + ", offset = " + offset);        }    }    SetMultimap<Long, Long> takeMap = queue.deserializeInflightTakes();    System.out.println("Inflight takes:");    for (Long txnID : takeMap.keySet()) {        Set<Long> takes = takeMap.get(txnID);        System.out.println("Transaction ID: " + String.valueOf(txnID));        for (long value : takes) {            int fileID = (int) (value >>> 32);            int offset = (int) value;            System.out.println(Long.toHexString(value) + " fileID = " + fileID + ", offset = " + offset);        }    }}
0
protected int getVersion()
{    return Serialization.VERSION_2;}
0
protected void incrementFileID(int fileID)
{    super.incrementFileID(fileID);    Preconditions.checkState(logFileIDReferenceCounts.size() < MAX_ACTIVE_LOGS, "Too many active logs ");}
0
private Pair<Integer, Integer> deocodeActiveLogCounter(long value)
{    int fileId = (int) (value >>> 32);    int count = (int) value;    return Pair.of(fileId, count);}
0
private long encodeActiveLogCounter(int fileId, int count)
{    long result = fileId;    result = (long) fileId << 32;    result += (long) count;    return result;}
0
protected void writeCheckpointMetaData()
{    elementsBuffer.put(INDEX_SIZE, getSize());    elementsBuffer.put(INDEX_HEAD, getHead());    List<Long> fileIdAndCountEncoded = new ArrayList<Long>();    for (Integer fileId : logFileIDReferenceCounts.keySet()) {        Integer count = logFileIDReferenceCounts.get(fileId).get();        long value = encodeActiveLogCounter(fileId, count);        fileIdAndCountEncoded.add(value);    }    int emptySlots = MAX_ACTIVE_LOGS - fileIdAndCountEncoded.size();    for (int i = 0; i < emptySlots; i++) {        fileIdAndCountEncoded.add(0L);    }    for (int i = 0; i < MAX_ACTIVE_LOGS; i++) {        elementsBuffer.put(i + INDEX_ACTIVE_LOG, fileIdAndCountEncoded.get(i));    }}
0
 File getMetaDataFile()
{    return metaDataFile;}
0
protected int getVersion()
{    return Serialization.VERSION_3;}
0
protected void writeCheckpointMetaData() throws IOException
{    ProtosFactory.Checkpoint.Builder checkpointBuilder = ProtosFactory.Checkpoint.newBuilder();    checkpointBuilder.setVersion(getVersion());    checkpointBuilder.setQueueHead(getHead());    checkpointBuilder.setQueueSize(getSize());    checkpointBuilder.setWriteOrderID(getLogWriteOrderID());    for (Integer logFileID : logFileIDReferenceCounts.keySet()) {        int count = logFileIDReferenceCounts.get(logFileID).get();        if (count != 0) {            ProtosFactory.ActiveLog.Builder activeLogBuilder = ProtosFactory.ActiveLog.newBuilder();            activeLogBuilder.setLogFileID(logFileID);            activeLogBuilder.setCount(count);            checkpointBuilder.addActiveLogs(activeLogBuilder.build());        }    }    FileOutputStream outputStream = new FileOutputStream(metaDataFile);    try {        checkpointBuilder.build().writeDelimitedTo(outputStream);        outputStream.getChannel().force(true);    } finally {        try {            outputStream.close();        } catch (IOException e) {                    }    }}
1
 static void upgrade(EventQueueBackingStoreFileV2 backingStoreV2, File checkpointFile, File metaDataFile) throws IOException
{    int head = backingStoreV2.getHead();    int size = backingStoreV2.getSize();    long writeOrderID = backingStoreV2.getLogWriteOrderID();    Map<Integer, AtomicInteger> referenceCounts = backingStoreV2.logFileIDReferenceCounts;    ProtosFactory.Checkpoint.Builder checkpointBuilder = ProtosFactory.Checkpoint.newBuilder();    checkpointBuilder.setVersion(Serialization.VERSION_3);    checkpointBuilder.setQueueHead(head);    checkpointBuilder.setQueueSize(size);    checkpointBuilder.setWriteOrderID(writeOrderID);    for (Integer logFileID : referenceCounts.keySet()) {        int count = referenceCounts.get(logFileID).get();        if (count > 0) {            ProtosFactory.ActiveLog.Builder activeLogBuilder = ProtosFactory.ActiveLog.newBuilder();            activeLogBuilder.setLogFileID(logFileID);            activeLogBuilder.setCount(count);            checkpointBuilder.addActiveLogs(activeLogBuilder.build());        }    }    FileOutputStream outputStream = new FileOutputStream(metaDataFile);    try {        checkpointBuilder.build().writeDelimitedTo(outputStream);        outputStream.getChannel().force(true);    } finally {        try {            outputStream.close();        } catch (IOException e) {                    }    }    RandomAccessFile checkpointFileHandle = new RandomAccessFile(checkpointFile, "rw");    try {        checkpointFileHandle.seek(INDEX_VERSION * Serialization.SIZE_OF_LONG);        checkpointFileHandle.writeLong(Serialization.VERSION_3);        checkpointFileHandle.getChannel().force(true);    } finally {        try {            checkpointFileHandle.close();        } catch (IOException e) {                    }    }}
1
public static Event getEventFromTransactionEvent(TransactionEventRecord transactionEventRecord)
{    if (transactionEventRecord instanceof Put) {        return ((Put) transactionEventRecord).getEvent();    }    return null;}
0
public synchronized void setName(String name)
{    channelNameDescriptor = "[channel=" + name + "]";    super.setName(name);}
0
public void configure(Context context)
{    useDualCheckpoints = context.getBoolean(FileChannelConfiguration.USE_DUAL_CHECKPOINTS, FileChannelConfiguration.DEFAULT_USE_DUAL_CHECKPOINTS);    compressBackupCheckpoint = context.getBoolean(FileChannelConfiguration.COMPRESS_BACKUP_CHECKPOINT, FileChannelConfiguration.DEFAULT_COMPRESS_BACKUP_CHECKPOINT);    String homePath = System.getProperty("user.home").replace('\\', '/');    String strCheckpointDir = context.getString(FileChannelConfiguration.CHECKPOINT_DIR, homePath + "/.flume/file-channel/checkpoint").trim();    String strBackupCheckpointDir = context.getString(FileChannelConfiguration.BACKUP_CHECKPOINT_DIR, "").trim();    String[] strDataDirs = Iterables.toArray(Splitter.on(",").trimResults().omitEmptyStrings().split(context.getString(FileChannelConfiguration.DATA_DIRS, homePath + "/.flume/file-channel/data")), String.class);    checkpointDir = new File(strCheckpointDir);    if (useDualCheckpoints) {        Preconditions.checkState(!strBackupCheckpointDir.isEmpty(), "Dual checkpointing is enabled, but the backup directory is not set. " + "Please set " + FileChannelConfiguration.BACKUP_CHECKPOINT_DIR + " " + "to enable dual checkpointing");        backupCheckpointDir = new File(strBackupCheckpointDir);        /*       * If the backup directory is the same as the checkpoint directory,       * then throw an exception and force the config system to ignore this       * channel.       */        Preconditions.checkState(!backupCheckpointDir.equals(checkpointDir), "Could not configure " + getName() + ". The checkpoint backup " + "directory and the checkpoint directory are " + "configured to be the same.");    }    dataDirs = new File[strDataDirs.length];    for (int i = 0; i < strDataDirs.length; i++) {        dataDirs[i] = new File(strDataDirs[i]);    }    capacity = context.getInteger(FileChannelConfiguration.CAPACITY, FileChannelConfiguration.DEFAULT_CAPACITY);    if (capacity <= 0) {        capacity = FileChannelConfiguration.DEFAULT_CAPACITY;            }    keepAlive = context.getInteger(FileChannelConfiguration.KEEP_ALIVE, FileChannelConfiguration.DEFAULT_KEEP_ALIVE);    transactionCapacity = context.getInteger(FileChannelConfiguration.TRANSACTION_CAPACITY, FileChannelConfiguration.DEFAULT_TRANSACTION_CAPACITY);    if (transactionCapacity <= 0) {        transactionCapacity = FileChannelConfiguration.DEFAULT_TRANSACTION_CAPACITY;            }    Preconditions.checkState(transactionCapacity <= capacity, "File Channel transaction capacity cannot be greater than the " + "capacity of the channel.");    checkpointInterval = context.getLong(FileChannelConfiguration.CHECKPOINT_INTERVAL, FileChannelConfiguration.DEFAULT_CHECKPOINT_INTERVAL);    if (checkpointInterval <= 0) {                checkpointInterval = FileChannelConfiguration.DEFAULT_CHECKPOINT_INTERVAL;    }        maxFileSize = Math.min(context.getLong(FileChannelConfiguration.MAX_FILE_SIZE, FileChannelConfiguration.DEFAULT_MAX_FILE_SIZE), FileChannelConfiguration.DEFAULT_MAX_FILE_SIZE);    minimumRequiredSpace = Math.max(context.getLong(FileChannelConfiguration.MINIMUM_REQUIRED_SPACE, FileChannelConfiguration.DEFAULT_MINIMUM_REQUIRED_SPACE), FileChannelConfiguration.FLOOR_MINIMUM_REQUIRED_SPACE);    useLogReplayV1 = context.getBoolean(FileChannelConfiguration.USE_LOG_REPLAY_V1, FileChannelConfiguration.DEFAULT_USE_LOG_REPLAY_V1);    useFastReplay = context.getBoolean(FileChannelConfiguration.USE_FAST_REPLAY, FileChannelConfiguration.DEFAULT_USE_FAST_REPLAY);    Context encryptionContext = new Context(context.getSubProperties(EncryptionConfiguration.ENCRYPTION_PREFIX + "."));    String encryptionKeyProviderName = encryptionContext.getString(EncryptionConfiguration.KEY_PROVIDER);    encryptionActiveKey = encryptionContext.getString(EncryptionConfiguration.ACTIVE_KEY);    encryptionCipherProvider = encryptionContext.getString(EncryptionConfiguration.CIPHER_PROVIDER);    if (encryptionKeyProviderName != null) {        Preconditions.checkState(!Strings.isNullOrEmpty(encryptionActiveKey), "Encryption configuration problem: " + EncryptionConfiguration.ACTIVE_KEY + " is missing");        Preconditions.checkState(!Strings.isNullOrEmpty(encryptionCipherProvider), "Encryption configuration problem: " + EncryptionConfiguration.CIPHER_PROVIDER + " is missing");        Context keyProviderContext = new Context(encryptionContext.getSubProperties(EncryptionConfiguration.KEY_PROVIDER + "."));        encryptionKeyProvider = KeyProviderFactory.getInstance(encryptionKeyProviderName, keyProviderContext);    } else {        Preconditions.checkState(encryptionActiveKey == null, "Encryption configuration problem: " + EncryptionConfiguration.ACTIVE_KEY + " is present while key " + "provider name is not.");        Preconditions.checkState(encryptionCipherProvider == null, "Encryption configuration problem: " + EncryptionConfiguration.CIPHER_PROVIDER + " is present while " + "key provider name is not.");    }    fsyncPerTransaction = context.getBoolean(FileChannelConfiguration.FSYNC_PER_TXN, FileChannelConfiguration.DEFAULT_FSYNC_PRE_TXN);    fsyncInterval = context.getInteger(FileChannelConfiguration.FSYNC_INTERVAL, FileChannelConfiguration.DEFAULT_FSYNC_INTERVAL);    checkpointOnClose = context.getBoolean(FileChannelConfiguration.CHKPT_ONCLOSE, FileChannelConfiguration.DEFAULT_CHKPT_ONCLOSE);    if (queueRemaining == null) {        queueRemaining = new Semaphore(capacity, true);    }    if (log != null) {        log.setCheckpointInterval(checkpointInterval);        log.setMaxFileSize(maxFileSize);    }    if (channelCounter == null) {        channelCounter = new FileChannelCounter(getName());    }    channelCounter.setUnhealthy(0);}
1
public synchronized void start()
{        channelCounter.start();    try {        Builder builder = createLogBuilder();        log = builder.build();        log.replay();        setOpen(true);        int depth = getDepth();        Preconditions.checkState(queueRemaining.tryAcquire(depth), "Unable to acquire " + depth + " permits " + channelNameDescriptor);            } catch (Throwable t) {        setOpen(false);        channelCounter.setUnhealthy(1);        startupError = t;                if (t instanceof Error) {            throw (Error) t;        }    }    if (open) {        channelCounter.setChannelSize(getDepth());        channelCounter.setChannelCapacity(capacity);    }    super.start();}
1
 Builder createLogBuilder()
{    Builder builder = new Log.Builder();    builder.setCheckpointInterval(checkpointInterval);    builder.setMaxFileSize(maxFileSize);    builder.setMinimumRequiredSpace(minimumRequiredSpace);    builder.setQueueSize(capacity);    builder.setCheckpointDir(checkpointDir);    builder.setLogDirs(dataDirs);    builder.setChannelName(getName());    builder.setUseLogReplayV1(useLogReplayV1);    builder.setUseFastReplay(useFastReplay);    builder.setEncryptionKeyProvider(encryptionKeyProvider);    builder.setEncryptionKeyAlias(encryptionActiveKey);    builder.setEncryptionCipherProvider(encryptionCipherProvider);    builder.setUseDualCheckpoints(useDualCheckpoints);    builder.setCompressBackupCheckpoint(compressBackupCheckpoint);    builder.setBackupCheckpointDir(backupCheckpointDir);    builder.setFsyncPerTransaction(fsyncPerTransaction);    builder.setFsyncInterval(fsyncInterval);    builder.setCheckpointOnClose(checkpointOnClose);    builder.setChannelCounter(channelCounter);    return builder;}
0
public synchronized void stop()
{        startupError = null;    int size = getDepth();    close();    if (!open) {        channelCounter.setChannelSize(size);        channelCounter.stop();    }    super.stop();}
1
public String toString()
{    return "FileChannel " + getName() + " { dataDirs: " + Arrays.toString(dataDirs) + " }";}
0
protected BasicTransactionSemantics createTransaction()
{    if (!open) {        String msg = "Channel closed " + channelNameDescriptor;        if (startupError != null) {            msg += ". Due to " + startupError.getClass().getName() + ": " + startupError.getMessage();            throw new IllegalStateException(msg, startupError);        }        throw new IllegalStateException(msg);    }    FileBackedTransaction trans = transactions.get();    if (trans != null && !trans.isClosed()) {        Preconditions.checkState(false, "Thread has transaction which is still open: " + trans.getStateAsString() + channelNameDescriptor);    }    trans = new FileBackedTransaction(log, TransactionIDOracle.next(), transactionCapacity, keepAlive, queueRemaining, getName(), fsyncPerTransaction, channelCounter);    transactions.set(trans);    return trans;}
0
protected int getDepth()
{    Preconditions.checkState(open, "Channel closed" + channelNameDescriptor);    Preconditions.checkNotNull(log, "log");    FlumeEventQueue queue = log.getFlumeEventQueue();    Preconditions.checkNotNull(queue, "queue");    return queue.getSize();}
0
 void close()
{    if (open) {        setOpen(false);        try {            log.close();        } catch (Exception e) {                        Throwables.propagate(e);        }        log = null;        queueRemaining = null;    }}
1
 boolean didFastReplay()
{    return log.didFastReplay();}
0
 boolean didFullReplayDueToBadCheckpointException()
{    return log.didFullReplayDueToBadCheckpointException();}
0
public boolean isOpen()
{    return open;}
0
private void setOpen(boolean open)
{    this.open = open;    channelCounter.setOpen(this.open);}
0
 boolean checkpointBackupRestored()
{    if (log != null) {        return log.backupRestored();    }    return false;}
0
 Log getLog()
{    return log;}
0
 FileChannelCounter getChannelCounter()
{    return channelCounter;}
0
public long getTransactionCapacity()
{    return transactionCapacity;}
0
private boolean isClosed()
{    return State.CLOSED.equals(getState());}
0
private String getStateAsString()
{    return String.valueOf(getState());}
0
protected void doPut(Event event) throws InterruptedException
{    channelCounter.incrementEventPutAttemptCount();    if (putList.remainingCapacity() == 0) {        throw new ChannelException("Put queue for FileBackedTransaction " + "of capacity " + putList.size() + " full, consider " + "committing more frequently, increasing capacity or " + "increasing thread count. " + channelNameDescriptor);    }        if (!queueRemaining.tryAcquire(keepAlive, TimeUnit.SECONDS)) {        throw new ChannelFullException("The channel has reached it's capacity. " + "This might be the result of a sink on the channel having too " + "low of batch size, a downstream system running slower than " + "normal, or that the channel capacity is just too low. " + channelNameDescriptor);    }    boolean success = false;    log.lockShared();    try {        FlumeEventPointer ptr = log.put(transactionID, event);        Preconditions.checkState(putList.offer(ptr), "putList offer failed " + channelNameDescriptor);        queue.addWithoutCommit(ptr, transactionID);        success = true;    } catch (IOException e) {        channelCounter.incrementEventPutErrorCount();        throw new ChannelException("Put failed due to IO error " + channelNameDescriptor, e);    } finally {        log.unlockShared();        if (!success) {                                    queueRemaining.release();        }    }}
0
protected Event doTake() throws InterruptedException
{    channelCounter.incrementEventTakeAttemptCount();    if (takeList.remainingCapacity() == 0) {        throw new ChannelException("Take list for FileBackedTransaction, capacity " + takeList.size() + " full, consider committing more frequently, " + "increasing capacity, or increasing thread count. " + channelNameDescriptor);    }    log.lockShared();    try {        while (true) {            FlumeEventPointer ptr = queue.removeHead(transactionID);            if (ptr == null) {                return null;            } else {                try {                                                            Preconditions.checkState(takeList.offer(ptr), "takeList offer failed " + channelNameDescriptor);                                        log.take(transactionID, ptr);                    Event event = log.get(ptr);                    return event;                } catch (IOException e) {                    channelCounter.incrementEventTakeErrorCount();                    throw new ChannelException("Take failed due to IO error " + channelNameDescriptor, e);                } catch (NoopRecordException e) {                                        takeList.remove(ptr);                } catch (CorruptEventException ex) {                    channelCounter.incrementEventTakeErrorCount();                    if (fsyncPerTransaction) {                        throw new ChannelException(ex);                    }                                        takeList.remove(ptr);                }            }        }    } finally {        log.unlockShared();    }}
1
protected void doCommit() throws InterruptedException
{    int puts = putList.size();    int takes = takeList.size();    if (puts > 0) {        Preconditions.checkState(takes == 0, "nonzero puts and takes " + channelNameDescriptor);        log.lockShared();        try {            log.commitPut(transactionID);            channelCounter.addToEventPutSuccessCount(puts);            synchronized (queue) {                while (!putList.isEmpty()) {                    if (!queue.addTail(putList.removeFirst())) {                        StringBuilder msg = new StringBuilder();                        msg.append("Queue add failed, this shouldn't be able to ");                        msg.append("happen. A portion of the transaction has been ");                        msg.append("added to the queue but the remaining portion ");                        msg.append("cannot be added. Those messages will be consumed ");                        msg.append("despite this transaction failing. Please report.");                        msg.append(channelNameDescriptor);                                                Preconditions.checkState(false, msg.toString());                    }                }                queue.completeTransaction(transactionID);            }        } catch (IOException e) {            throw new ChannelException("Commit failed due to IO error " + channelNameDescriptor, e);        } finally {            log.unlockShared();        }    } else if (takes > 0) {        log.lockShared();        try {            log.commitTake(transactionID);            queue.completeTransaction(transactionID);            channelCounter.addToEventTakeSuccessCount(takes);        } catch (IOException e) {            throw new ChannelException("Commit failed due to IO error " + channelNameDescriptor, e);        } finally {            log.unlockShared();        }        queueRemaining.release(takes);    }    putList.clear();    takeList.clear();    channelCounter.setChannelSize(queue.getSize());}
1
protected void doRollback() throws InterruptedException
{    int puts = putList.size();    int takes = takeList.size();    log.lockShared();    try {        if (takes > 0) {            Preconditions.checkState(puts == 0, "nonzero puts and takes " + channelNameDescriptor);            synchronized (queue) {                while (!takeList.isEmpty()) {                    Preconditions.checkState(queue.addHead(takeList.removeLast()), "Queue add failed, this shouldn't be able to happen " + channelNameDescriptor);                }            }        }        putList.clear();        takeList.clear();        queue.completeTransaction(transactionID);        channelCounter.setChannelSize(queue.getSize());        log.rollback(transactionID);    } catch (IOException e) {        throw new ChannelException("Commit failed due to IO error " + channelNameDescriptor, e);    } finally {        log.unlockShared();                        queueRemaining.release(puts);    }}
0
protected CharsetEncoder initialValue()
{    return Charset.forName("UTF-8").newEncoder().onMalformedInput(CodingErrorAction.REPLACE).onUnmappableCharacter(CodingErrorAction.REPLACE);}
0
protected CharsetDecoder initialValue()
{    return Charset.forName("UTF-8").newDecoder().onMalformedInput(CodingErrorAction.REPLACE).onUnmappableCharacter(CodingErrorAction.REPLACE);}
0
public Map<String, String> getHeaders()
{    return headers;}
0
public void setHeaders(Map<String, String> headers)
{    this.headers = headers;}
0
public byte[] getBody()
{    return body;}
0
public void setBody(byte[] body)
{    this.body = body;}
0
public void write(DataOutput out) throws IOException
{    out.writeByte(0);    Map<String, String> writeHeaders = getHeaders();    if (null != writeHeaders) {        out.writeInt(headers.size());        CharsetEncoder encoder = ENCODER_FACTORY.get();        for (String key : headers.keySet()) {            out.writeByte(EVENT_MAP_TEXT_WRITABLE_ID);            ByteBuffer keyBytes = encoder.encode(CharBuffer.wrap(key.toCharArray()));            int keyLength = keyBytes.limit();            WritableUtils.writeVInt(out, keyLength);            out.write(keyBytes.array(), 0, keyLength);            String value = headers.get(key);            out.write(EVENT_MAP_TEXT_WRITABLE_ID);            ByteBuffer valueBytes = encoder.encode(CharBuffer.wrap(value.toCharArray()));            int valueLength = valueBytes.limit();            WritableUtils.writeVInt(out, valueLength);            out.write(valueBytes.array(), 0, valueLength);        }    } else {        out.writeInt(0);    }    byte[] body = getBody();    if (body == null) {        out.writeInt(-1);    } else {        out.writeInt(body.length);        out.write(body);    }}
0
public void readFields(DataInput in) throws IOException
{        byte newClasses = in.readByte();        for (byte i = 0; i < newClasses; i++) {        in.readByte();        in.readUTF();    }    Map<String, String> newHeaders = new HashMap<String, String>();    int numEntries = in.readInt();    CharsetDecoder decoder = DECODER_FACTORY.get();    for (int i = 0; i < numEntries; i++) {        byte keyClassId = in.readByte();        assert (keyClassId == EVENT_MAP_TEXT_WRITABLE_ID);        int keyLength = WritableUtils.readVInt(in);        byte[] keyBytes = new byte[keyLength];        in.readFully(keyBytes, 0, keyLength);        String key = decoder.decode(ByteBuffer.wrap(keyBytes)).toString();        byte valueClassId = in.readByte();        assert (valueClassId == EVENT_MAP_TEXT_WRITABLE_ID);        int valueLength = WritableUtils.readVInt(in);        byte[] valueBytes = new byte[valueLength];        in.readFully(valueBytes, 0, valueLength);        String value = decoder.decode(ByteBuffer.wrap(valueBytes)).toString();        newHeaders.put(key, value);    }    setHeaders(newHeaders);    byte[] body = null;    int bodyLength = in.readInt();    if (bodyLength != -1) {        body = new byte[bodyLength];        in.readFully(body);    }    setBody(body);}
0
 static FlumeEvent from(DataInput in) throws IOException
{    FlumeEvent event = new FlumeEvent();    event.readFields(in);    return event;}
0
 int getFileID()
{    return fileID;}
0
 int getOffset()
{    return offset;}
0
public long toLong()
{    long result = fileID;    result = (long) fileID << 32;    result += (long) offset;    return result;}
0
public int hashCode()
{    final int prime = 31;    int result = 1;    result = prime * result + fileID;    result = prime * result + offset;    return result;}
0
public boolean equals(Object obj)
{    if (this == obj) {        return true;    }    if (obj == null) {        return false;    }    if (getClass() != obj.getClass()) {        return false;    }    FlumeEventPointer other = (FlumeEventPointer) obj;    if (fileID != other.fileID) {        return false;    }    if (offset != other.offset) {        return false;    }    return true;}
0
public String toString()
{    return "FlumeEventPointer [fileID=" + fileID + ", offset=" + offset + "]";}
0
public static FlumeEventPointer fromLong(long value)
{    int fileID = (int) (value >>> 32);    int offset = (int) value;    return new FlumeEventPointer(fileID, offset);}
0
 SetMultimap<Long, Long> deserializeInflightPuts() throws IOException, BadCheckpointException
{    return inflightPuts.deserialize();}
0
 SetMultimap<Long, Long> deserializeInflightTakes() throws IOException, BadCheckpointException
{    return inflightTakes.deserialize();}
0
 synchronized long getLogWriteOrderID()
{    return backingStore.getLogWriteOrderID();}
0
 synchronized boolean checkpoint(boolean force) throws Exception
{    if (!backingStore.syncRequired() && !inflightTakes.syncRequired() && !force) {                                return false;    }    backingStore.beginCheckpoint();    inflightPuts.serializeAndWrite();    inflightTakes.serializeAndWrite();    backingStore.checkpoint();    return true;}
1
 synchronized FlumeEventPointer removeHead(long transactionID)
{    if (backingStore.getSize() == 0) {        return null;    }    long value = remove(0, transactionID);    Preconditions.checkState(value != EMPTY, "Empty value " + channelNameDescriptor);    FlumeEventPointer ptr = FlumeEventPointer.fromLong(value);    backingStore.decrementFileID(ptr.getFileID());    return ptr;}
0
 synchronized boolean addHead(FlumeEventPointer e)
{        if (backingStore.getSize() == backingStore.getCapacity()) {                return false;    }    long value = e.toLong();    Preconditions.checkArgument(value != EMPTY);    backingStore.incrementFileID(e.getFileID());    add(0, value);    return true;}
1
 synchronized boolean addTail(FlumeEventPointer e)
{    if (getSize() == backingStore.getCapacity()) {        return false;    }    long value = e.toLong();    Preconditions.checkArgument(value != EMPTY);    backingStore.incrementFileID(e.getFileID());    add(backingStore.getSize(), value);    return true;}
0
 synchronized void addWithoutCommit(FlumeEventPointer e, long transactionID)
{    inflightPuts.addEvent(transactionID, e.toLong());}
0
 synchronized boolean remove(FlumeEventPointer e)
{    long value = e.toLong();    Preconditions.checkArgument(value != EMPTY);    if (queueSet == null) {        throw new IllegalStateException("QueueSet is null, thus replayComplete" + " has been called which is illegal");    }    if (!queueSet.contains(value)) {        return false;    }    searchCount++;    long start = System.currentTimeMillis();    for (int i = 0; i < backingStore.getSize(); i++) {        if (get(i) == value) {            remove(i, 0);            FlumeEventPointer ptr = FlumeEventPointer.fromLong(value);            backingStore.decrementFileID(ptr.getFileID());            searchTime += System.currentTimeMillis() - start;            return true;        }    }    searchTime += System.currentTimeMillis() - start;    return false;}
0
 synchronized SortedSet<Integer> getFileIDs()
{                SortedSet<Integer> fileIDs = new TreeSet<Integer>(backingStore.getReferenceCounts());    fileIDs.addAll(inflightPuts.getFileIDs());    fileIDs.addAll(inflightTakes.getFileIDs());    return fileIDs;}
0
protected long get(int index)
{    if (index < 0 || index > backingStore.getSize() - 1) {        throw new IndexOutOfBoundsException(String.valueOf(index) + channelNameDescriptor);    }    return backingStore.get(index);}
0
private void set(int index, long value)
{    if (index < 0 || index > backingStore.getSize() - 1) {        throw new IndexOutOfBoundsException(String.valueOf(index) + channelNameDescriptor);    }    backingStore.put(index, value);}
0
protected boolean add(int index, long value)
{    if (index < 0 || index > backingStore.getSize()) {        throw new IndexOutOfBoundsException(String.valueOf(index) + channelNameDescriptor);    }    if (backingStore.getSize() == backingStore.getCapacity()) {        return false;    }    backingStore.setSize(backingStore.getSize() + 1);    if (index <= backingStore.getSize() / 2) {                backingStore.setHead(backingStore.getHead() - 1);        if (backingStore.getHead() < 0) {            backingStore.setHead(backingStore.getCapacity() - 1);        }        for (int i = 0; i < index; i++) {            set(i, get(i + 1));        }    } else {                for (int i = backingStore.getSize() - 1; i > index; i--) {            set(i, get(i - 1));        }    }    set(index, value);    if (queueSet != null) {        queueSet.add(value);    }    return true;}
0
 synchronized void completeTransaction(long transactionID)
{    if (!inflightPuts.completeTransaction(transactionID)) {        inflightTakes.completeTransaction(transactionID);    }}
0
protected synchronized long remove(int index, long transactionID)
{    if (index < 0 || index > backingStore.getSize() - 1) {        throw new IndexOutOfBoundsException("index = " + index + ", queueSize " + backingStore.getSize() + " " + channelNameDescriptor);    }    copyCount++;    long start = System.currentTimeMillis();    long value = get(index);    if (queueSet != null) {        queueSet.remove(value);    }        if (transactionID != 0) {        inflightTakes.addEvent(transactionID, value);    }    if (index > backingStore.getSize() / 2) {                for (int i = index; i < backingStore.getSize() - 1; i++) {            long rightValue = get(i + 1);            set(i, rightValue);        }        set(backingStore.getSize() - 1, EMPTY);    } else {                for (int i = index - 1; i >= 0; i--) {            long leftValue = get(i);            set(i + 1, leftValue);        }        set(0, EMPTY);        backingStore.setHead(backingStore.getHead() + 1);        if (backingStore.getHead() == backingStore.getCapacity()) {            backingStore.setHead(0);        }    }    backingStore.setSize(backingStore.getSize() - 1);    copyTime += System.currentTimeMillis() - start;    return value;}
0
protected synchronized int getSize()
{    return backingStore.getSize() + inflightTakes.getSize();}
0
public int getCapacity()
{    return backingStore.getCapacity();}
0
 synchronized void close() throws IOException
{    try {        if (db != null) {            db.close();        }    } catch (Exception ex) {            }    try {        backingStore.close();        inflightPuts.close();        inflightTakes.close();    } catch (IOException e) {            }}
1
 synchronized void replayComplete()
{    String msg = "Search Count = " + searchCount + ", Search Time = " + searchTime + ", Copy Count = " + copyCount + ", Copy Time = " + copyTime;        if (db != null) {        db.close();    }    queueSet = null;    db = null;}
1
 long getSearchCount()
{    return searchCount;}
0
 long getCopyCount()
{    return copyCount;}
0
public boolean completeTransaction(Long transactionID)
{    if (!inflightEvents.containsKey(transactionID)) {        return false;    }    inflightEvents.removeAll(transactionID);    inflightFileIDs.removeAll(transactionID);    syncRequired = true;    return true;}
0
public void addEvent(Long transactionID, Long pointer)
{    inflightEvents.put(transactionID, pointer);    inflightFileIDs.put(transactionID, FlumeEventPointer.fromLong(pointer).getFileID());    syncRequired = true;}
0
public void serializeAndWrite() throws Exception
{    Collection<Long> values = inflightEvents.values();    if (!fileChannel.isOpen()) {        file = new RandomAccessFile(inflightEventsFile, "rw");        fileChannel = file.getChannel();    }    if (values.isEmpty()) {        file.setLength(0L);    }    try {        int expectedFileSize = ((        (inflightEvents.keySet().size() * 2) +         values.size()) *         8) +         16;                        file.setLength(expectedFileSize);        Preconditions.checkState(file.length() == expectedFileSize, "Expected File size of inflight events file does not match the " + "current file size. Checkpoint is incomplete.");        file.seek(0);        final ByteBuffer buffer = ByteBuffer.allocate(expectedFileSize);        LongBuffer longBuffer = buffer.asLongBuffer();        for (Long txnID : inflightEvents.keySet()) {            Set<Long> pointers = inflightEvents.get(txnID);            longBuffer.put(txnID);            longBuffer.put((long) pointers.size());                        long[] written = ArrayUtils.toPrimitive(pointers.toArray(new Long[0]));            longBuffer.put(written);        }        byte[] checksum = digest.digest(buffer.array());        file.write(checksum);        buffer.position(0);        fileChannel.write(buffer);        fileChannel.force(true);        syncRequired = false;    } catch (IOException ex) {                throw ex;    }}
1
public SetMultimap<Long, Long> deserialize() throws IOException, BadCheckpointException
{    SetMultimap<Long, Long> inflights = HashMultimap.create();    if (!fileChannel.isOpen()) {        file = new RandomAccessFile(inflightEventsFile, "rw");        fileChannel = file.getChannel();    }    if (file.length() == 0) {        return inflights;    }    file.seek(0);    byte[] checksum = new byte[16];    file.read(checksum);    ByteBuffer buffer = ByteBuffer.allocate((int) (file.length() - file.getFilePointer()));    fileChannel.read(buffer);    byte[] fileChecksum = digest.digest(buffer.array());    if (!Arrays.equals(checksum, fileChecksum)) {        throw new BadCheckpointException("Checksum of inflights file differs" + " from the checksum expected.");    }    buffer.position(0);    LongBuffer longBuffer = buffer.asLongBuffer();    try {        while (true) {            long txnID = longBuffer.get();            int numEvents = (int) (longBuffer.get());            for (int i = 0; i < numEvents; i++) {                long val = longBuffer.get();                inflights.put(txnID, val);            }        }    } catch (BufferUnderflowException ex) {            }    return inflights;}
1
public int getSize()
{    return inflightEvents.size();}
0
public boolean syncRequired()
{    return syncRequired;}
0
public Collection<Integer> getFileIDs()
{    return inflightFileIDs.values();}
0
public Collection<Long> getInFlightPointers()
{    return inflightEvents.values();}
0
public void close() throws IOException
{    file.close();}
0
public boolean isOpen()
{    return open;}
0
public void setOpen(boolean open)
{    this.open = open;}
0
public int getClosed()
{    return open ? 0 : 1;}
0
public int getUnhealthy()
{    return unhealthy;}
0
public void setUnhealthy(int unhealthy)
{    this.unhealthy = unhealthy;}
0
public long getEventPutErrorCount()
{    return get(EVENT_PUT_ERROR_COUNT);}
0
public void incrementEventPutErrorCount()
{    increment(EVENT_PUT_ERROR_COUNT);}
0
public long getEventTakeErrorCount()
{    return get(EVENT_TAKE_ERROR_COUNT);}
0
public void incrementEventTakeErrorCount()
{    increment(EVENT_TAKE_ERROR_COUNT);}
0
public long getCheckpointWriteErrorCount()
{    return get(CHECKPOINT_WRITE_ERROR_COUNT);}
0
public void incrementCheckpointWriteErrorCount()
{    increment(CHECKPOINT_WRITE_ERROR_COUNT);}
0
public long getCheckpointBackupWriteErrorCount()
{    return get(CHECKPOINT_BACKUP_WRITE_ERROR_COUNT);}
0
public void incrementCheckpointBackupWriteErrorCount()
{    increment(CHECKPOINT_BACKUP_WRITE_ERROR_COUNT);}
0
 boolean isFsyncPerTransaction()
{    return fsyncPerTransaction;}
0
 void setFsyncPerTransaction(boolean fsyncPerTransaction)
{    this.fsyncPerTransaction = fsyncPerTransaction;}
0
 int getFsyncInterval()
{    return fsyncInterval;}
0
 void setFsyncInterval(int fsyncInterval)
{    this.fsyncInterval = fsyncInterval;}
0
 Builder setUsableSpaceRefreshInterval(long usableSpaceRefreshInterval)
{    bUsableSpaceRefreshInterval = usableSpaceRefreshInterval;    return this;}
0
 Builder setCheckpointInterval(long interval)
{    bCheckpointInterval = interval;    return this;}
0
 Builder setMaxFileSize(long maxSize)
{    bMaxFileSize = maxSize;    return this;}
0
 Builder setQueueSize(int capacity)
{    bQueueCapacity = capacity;    return this;}
0
 Builder setCheckpointDir(File cpDir)
{    bCheckpointDir = cpDir;    return this;}
0
 Builder setLogDirs(File[] dirs)
{    bLogDirs = dirs;    return this;}
0
 Builder setChannelName(String name)
{    bName = name;    return this;}
0
 Builder setMinimumRequiredSpace(long minimumRequiredSpace)
{    bMinimumRequiredSpace = minimumRequiredSpace;    return this;}
0
 Builder setUseLogReplayV1(boolean useLogReplayV1)
{    this.useLogReplayV1 = useLogReplayV1;    return this;}
0
 Builder setUseFastReplay(boolean useFastReplay)
{    this.useFastReplay = useFastReplay;    return this;}
0
 Builder setEncryptionKeyProvider(KeyProvider encryptionKeyProvider)
{    bEncryptionKeyProvider = encryptionKeyProvider;    return this;}
0
 Builder setEncryptionKeyAlias(String encryptionKeyAlias)
{    bEncryptionKeyAlias = encryptionKeyAlias;    return this;}
0
 Builder setEncryptionCipherProvider(String encryptionCipherProvider)
{    bEncryptionCipherProvider = encryptionCipherProvider;    return this;}
0
 Builder setUseDualCheckpoints(boolean UseDualCheckpoints)
{    this.bUseDualCheckpoints = UseDualCheckpoints;    return this;}
0
 Builder setCompressBackupCheckpoint(boolean compressBackupCheckpoint)
{    this.bCompressBackupCheckpoint = compressBackupCheckpoint;    return this;}
0
 Builder setBackupCheckpointDir(File backupCheckpointDir)
{    this.bBackupCheckpointDir = backupCheckpointDir;    return this;}
0
 Builder setCheckpointOnClose(boolean enableCheckpointOnClose)
{    this.checkpointOnClose = enableCheckpointOnClose;    return this;}
0
 Builder setChannelCounter(FileChannelCounter channelCounter)
{    this.channelCounter = channelCounter;    return this;}
0
 Log build() throws IOException
{    return new Log(bCheckpointInterval, bMaxFileSize, bQueueCapacity, bUseDualCheckpoints, bCompressBackupCheckpoint, bCheckpointDir, bBackupCheckpointDir, bName, useLogReplayV1, useFastReplay, bMinimumRequiredSpace, bEncryptionKeyProvider, bEncryptionKeyAlias, bEncryptionCipherProvider, bUsableSpaceRefreshInterval, fsyncPerTransaction, fsyncInterval, checkpointOnClose, channelCounter, bLogDirs);}
0
 void replay() throws IOException
{    Preconditions.checkState(!open, "Cannot replay after Log has been opened");    lockExclusive();    try {        /*       * First we are going to look through the data directories       * and find all log files. We will store the highest file id       * (at the end of the filename) we find and use that when we       * create additional log files.       *       * Also store up the list of files so we can replay them later.       */                nextFileID.set(0);        List<File> dataFiles = Lists.newArrayList();        for (File logDir : logDirs) {            for (File file : LogUtils.getLogs(logDir)) {                int id = LogUtils.getIDForFile(file);                dataFiles.add(file);                nextFileID.set(Math.max(nextFileID.get(), id));                idLogFileMap.put(id, LogFileFactory.getRandomReader(new File(logDir, PREFIX + id), encryptionKeyProvider, fsyncPerTransaction));            }        }                /*       * sort the data files by file id so we can replay them by file id       * which should approximately give us sequential events       */        LogUtils.sort(dataFiles);        boolean shouldFastReplay = this.useFastReplay;        /*       * Read the checkpoint (in memory queue) from one of two alternating       * locations. We will read the last one written to disk.       */        File checkpointFile = new File(checkpointDir, "checkpoint");        if (shouldFastReplay) {            if (checkpointFile.exists()) {                                shouldFastReplay = false;            } else {                            }        }        File inflightTakesFile = new File(checkpointDir, "inflighttakes");        File inflightPutsFile = new File(checkpointDir, "inflightputs");        File queueSetDir = new File(checkpointDir, QUEUE_SET);        EventQueueBackingStore backingStore = null;        try {            backingStore = EventQueueBackingStoreFactory.get(checkpointFile, backupCheckpointDir, queueCapacity, channelNameDescriptor, channelCounter, true, this.useDualCheckpoints, this.compressBackupCheckpoint);            queue = new FlumeEventQueue(backingStore, inflightTakesFile, inflightPutsFile, queueSetDir);                        /*         * We now have everything we need to actually replay the log files         * the queue, the timestamp the queue was written to disk, and         * the list of data files.         *         * This will throw if and only if checkpoint file was fine,         * but the inflights were not. If the checkpoint was bad, the backing         * store factory would have thrown.         */            doReplay(queue, dataFiles, encryptionKeyProvider, shouldFastReplay);        } catch (BadCheckpointException ex) {            backupRestored = false;            if (useDualCheckpoints) {                                if (EventQueueBackingStoreFile.backupExists(backupCheckpointDir)) {                    backupRestored = EventQueueBackingStoreFile.restoreBackup(checkpointDir, backupCheckpointDir);                }            }            if (!backupRestored) {                                if (!Serialization.deleteAllFiles(checkpointDir, EXCLUDES)) {                    throw new IOException("Could not delete files in checkpoint " + "directory to recover from a corrupt or incomplete checkpoint");                }            }            backingStore = EventQueueBackingStoreFactory.get(checkpointFile, backupCheckpointDir, queueCapacity, channelNameDescriptor, channelCounter, true, useDualCheckpoints, compressBackupCheckpoint);            queue = new FlumeEventQueue(backingStore, inflightTakesFile, inflightPutsFile, queueSetDir);                                    shouldFastReplay = this.useFastReplay;            doReplay(queue, dataFiles, encryptionKeyProvider, shouldFastReplay);            if (!shouldFastReplay) {                didFullReplayDueToBadCheckpointException = true;            }        }        for (int index = 0; index < logDirs.length; index++) {                        roll(index);        }        /*       * Now that we have replayed, write the current queue to disk       */        writeCheckpoint(true);        open = true;    } catch (Exception ex) {                if (ex instanceof IOException) {            throw (IOException) ex;        }        Throwables.propagate(ex);    } finally {        unlockExclusive();    }}
1
private void doReplay(FlumeEventQueue queue, List<File> dataFiles, KeyProvider encryptionKeyProvider, boolean useFastReplay) throws Exception
{    CheckpointRebuilder rebuilder = new CheckpointRebuilder(dataFiles, queue, fsyncPerTransaction);    if (useFastReplay && rebuilder.rebuild()) {        didFastReplay = true;            } else {        ReplayHandler replayHandler = new ReplayHandler(queue, encryptionKeyProvider, fsyncPerTransaction);        if (useLogReplayV1) {                        replayHandler.replayLogv1(dataFiles);        } else {                        replayHandler.replayLog(dataFiles);        }        readCount = replayHandler.getReadCount();        putCount = replayHandler.getPutCount();        takeCount = replayHandler.getTakeCount();        rollbackCount = replayHandler.getRollbackCount();        committedCount = replayHandler.getCommitCount();    }}
1
 boolean didFastReplay()
{    return didFastReplay;}
0
public int getReadCount()
{    return readCount;}
0
public int getPutCount()
{    return putCount;}
0
public int getTakeCount()
{    return takeCount;}
0
public int getCommittedCount()
{    return committedCount;}
0
public int getRollbackCount()
{    return rollbackCount;}
0
 boolean backupRestored()
{    return backupRestored;}
0
 boolean didFullReplayDueToBadCheckpointException()
{    return didFullReplayDueToBadCheckpointException;}
0
 int getNextFileID()
{    Preconditions.checkState(open, "Log is closed");    return nextFileID.get();}
0
 FlumeEventQueue getFlumeEventQueue()
{    Preconditions.checkState(open, "Log is closed");    return queue;}
0
 FlumeEvent get(FlumeEventPointer pointer) throws IOException, InterruptedException, NoopRecordException, CorruptEventException
{    Preconditions.checkState(open, "Log is closed");    int id = pointer.getFileID();    LogFile.RandomReader logFile = idLogFileMap.get(id);    Preconditions.checkNotNull(logFile, "LogFile is null for id " + id);    try {        return logFile.get(pointer.getOffset());    } catch (CorruptEventException ex) {        if (fsyncPerTransaction) {            open = false;            throw new IOException("Corrupt event found. Please run File Channel " + "Integrity tool.", ex);        }        throw ex;    }}
0
 FlumeEventPointer put(long transactionID, Event event) throws IOException
{    Preconditions.checkState(open, "Log is closed");    FlumeEvent flumeEvent = new FlumeEvent(event.getHeaders(), event.getBody());    Put put = new Put(transactionID, WriteOrderOracle.next(), flumeEvent);    ByteBuffer buffer = TransactionEventRecord.toByteBuffer(put);    int logFileIndex = nextLogWriter(transactionID);    long usableSpace = logFiles.get(logFileIndex).getUsableSpace();    long requiredSpace = minimumRequiredSpace + buffer.limit();    if (usableSpace <= requiredSpace) {        throw new IOException("Usable space exhausted, only " + usableSpace + " bytes remaining, required " + requiredSpace + " bytes");    }    boolean error = true;    try {        try {            FlumeEventPointer ptr = logFiles.get(logFileIndex).put(buffer);            error = false;            return ptr;        } catch (LogFileRetryableIOException e) {            if (!open) {                throw e;            }            roll(logFileIndex, buffer);            FlumeEventPointer ptr = logFiles.get(logFileIndex).put(buffer);            error = false;            return ptr;        }    } finally {        if (error && open) {            roll(logFileIndex);        }    }}
0
 void take(long transactionID, FlumeEventPointer pointer) throws IOException
{    Preconditions.checkState(open, "Log is closed");    Take take = new Take(transactionID, WriteOrderOracle.next(), pointer.getOffset(), pointer.getFileID());    ByteBuffer buffer = TransactionEventRecord.toByteBuffer(take);    int logFileIndex = nextLogWriter(transactionID);    long usableSpace = logFiles.get(logFileIndex).getUsableSpace();    long requiredSpace = minimumRequiredSpace + buffer.limit();    if (usableSpace <= requiredSpace) {        throw new IOException("Usable space exhausted, only " + usableSpace + " bytes remaining, required " + requiredSpace + " bytes");    }    boolean error = true;    try {        try {            logFiles.get(logFileIndex).take(buffer);            error = false;        } catch (LogFileRetryableIOException e) {            if (!open) {                throw e;            }            roll(logFileIndex, buffer);            logFiles.get(logFileIndex).take(buffer);            error = false;        }    } finally {        if (error && open) {            roll(logFileIndex);        }    }}
0
 void rollback(long transactionID) throws IOException
{    Preconditions.checkState(open, "Log is closed");    if (LOGGER.isDebugEnabled()) {            }    Rollback rollback = new Rollback(transactionID, WriteOrderOracle.next());    ByteBuffer buffer = TransactionEventRecord.toByteBuffer(rollback);    int logFileIndex = nextLogWriter(transactionID);    long usableSpace = logFiles.get(logFileIndex).getUsableSpace();    long requiredSpace = minimumRequiredSpace + buffer.limit();    if (usableSpace <= requiredSpace) {        throw new IOException("Usable space exhausted, only " + usableSpace + " bytes remaining, required " + requiredSpace + " bytes");    }    boolean error = true;    try {        try {            logFiles.get(logFileIndex).rollback(buffer);            error = false;        } catch (LogFileRetryableIOException e) {            if (!open) {                throw e;            }            roll(logFileIndex, buffer);            logFiles.get(logFileIndex).rollback(buffer);            error = false;        }    } finally {        if (error && open) {            roll(logFileIndex);        }    }}
1
 void commitPut(long transactionID) throws IOException, InterruptedException
{    Preconditions.checkState(open, "Log is closed");    commit(transactionID, TransactionEventRecord.Type.PUT.get());}
0
 void commitTake(long transactionID) throws IOException, InterruptedException
{    Preconditions.checkState(open, "Log is closed");    commit(transactionID, TransactionEventRecord.Type.TAKE.get());}
0
private void unlockExclusive()
{    checkpointWriterLock.unlock();}
0
 void lockShared()
{    checkpointReadLock.lock();}
0
 void unlockShared()
{    checkpointReadLock.unlock();}
0
private void lockExclusive()
{    checkpointWriterLock.lock();}
0
 void close() throws IOException
{    lockExclusive();    try {        open = false;        try {            if (checkpointOnClose) {                                writeCheckpoint(true);            }        } catch (Exception err) {                    }        shutdownWorker();        if (logFiles != null) {            for (int index = 0; index < logFiles.length(); index++) {                LogFile.Writer writer = logFiles.get(index);                if (writer != null) {                    writer.close();                }            }        }        synchronized (idLogFileMap) {            for (Integer logId : idLogFileMap.keySet()) {                LogFile.RandomReader reader = idLogFileMap.get(logId);                if (reader != null) {                    reader.close();                }            }        }        queue.close();        try {            unlock(checkpointDir);        } catch (IOException ex) {                    }        if (useDualCheckpoints) {            try {                unlock(backupCheckpointDir);            } catch (IOException ex) {                            }        }        for (File logDir : logDirs) {            try {                unlock(logDir);            } catch (IOException ex) {                            }        }    } finally {        unlockExclusive();    }}
1
 void shutdownWorker()
{    String msg = "Attempting to shutdown background worker.";    System.out.println(msg);        workerExecutor.shutdown();    try {        workerExecutor.awaitTermination(10, TimeUnit.SECONDS);    } catch (InterruptedException e) {            }}
1
 void setCheckpointInterval(long checkpointInterval)
{    this.checkpointInterval = checkpointInterval;}
0
 void setMaxFileSize(long maxFileSize)
{    this.maxFileSize = maxFileSize;}
0
private void commit(long transactionID, short type) throws IOException
{    Preconditions.checkState(open, "Log is closed");    Commit commit = new Commit(transactionID, WriteOrderOracle.next(), type);    ByteBuffer buffer = TransactionEventRecord.toByteBuffer(commit);    int logFileIndex = nextLogWriter(transactionID);    long usableSpace = logFiles.get(logFileIndex).getUsableSpace();    long requiredSpace = minimumRequiredSpace + buffer.limit();    if (usableSpace <= requiredSpace) {        throw new IOException("Usable space exhausted, only " + usableSpace + " bytes remaining, required " + requiredSpace + " bytes");    }    boolean error = true;    try {        try {            LogFile.Writer logFileWriter = logFiles.get(logFileIndex);                                                logFileWriter.commit(buffer);            logFileWriter.sync();            error = false;        } catch (LogFileRetryableIOException e) {            if (!open) {                throw e;            }            roll(logFileIndex, buffer);            LogFile.Writer logFileWriter = logFiles.get(logFileIndex);            logFileWriter.commit(buffer);            logFileWriter.sync();            error = false;        }    } finally {        if (error && open) {            roll(logFileIndex);        }    }}
0
private int nextLogWriter(long transactionID)
{    return (int) Math.abs(transactionID % (long) logFiles.length());}
0
private void roll(int index) throws IOException
{    roll(index, null);}
0
private synchronized void roll(int index, ByteBuffer buffer) throws IOException
{    lockShared();    try {        LogFile.Writer oldLogFile = logFiles.get(index);                if (oldLogFile == null || buffer == null || oldLogFile.isRollRequired(buffer)) {            try {                                int fileID = nextFileID.incrementAndGet();                File file = new File(logDirs[index], PREFIX + fileID);                LogFile.Writer writer = LogFileFactory.getWriter(file, fileID, maxFileSize, encryptionKey, encryptionKeyAlias, encryptionCipherProvider, usableSpaceRefreshInterval, fsyncPerTransaction, fsyncInterval);                idLogFileMap.put(fileID, LogFileFactory.getRandomReader(file, encryptionKeyProvider, fsyncPerTransaction));                                logFiles.set(index, writer);                                if (oldLogFile != null) {                    oldLogFile.close();                }            } finally {                            }        }    } finally {        unlockShared();    }}
1
private boolean writeCheckpoint() throws Exception
{    return writeCheckpoint(false);}
0
private Boolean writeCheckpoint(Boolean force) throws Exception
{    boolean checkpointCompleted = false;    long usableSpace = checkpointDir.getUsableSpace();    if (usableSpace <= minimumRequiredSpace) {        throw new IOException("Usable space exhausted, only " + usableSpace + " bytes remaining, required " + minimumRequiredSpace + " bytes");    }    lockExclusive();    SortedSet<Integer> logFileRefCountsAll = null;    SortedSet<Integer> logFileRefCountsActive = null;    try {        if (queue.checkpoint(force)) {            long logWriteOrderID = queue.getLogWriteOrderID();                                                                                    logFileRefCountsAll = queue.getFileIDs();            logFileRefCountsActive = new TreeSet<Integer>(logFileRefCountsAll);            int numFiles = logFiles.length();            for (int i = 0; i < numFiles; i++) {                LogFile.Writer logWriter = logFiles.get(i);                int logFileID = logWriter.getLogFileID();                File logFile = logWriter.getFile();                LogFile.MetaDataWriter writer = LogFileFactory.getMetaDataWriter(logFile, logFileID);                try {                    writer.markCheckpoint(logWriter.position(), logWriteOrderID);                } finally {                    writer.close();                }                logFileRefCountsAll.remove(logFileID);                            }                        Iterator<Integer> idIterator = logFileRefCountsAll.iterator();            while (idIterator.hasNext()) {                int id = idIterator.next();                LogFile.RandomReader reader = idLogFileMap.remove(id);                File file = reader.getFile();                reader.close();                LogFile.MetaDataWriter writer = LogFileFactory.getMetaDataWriter(file, id);                try {                    writer.markCheckpoint(logWriteOrderID);                } finally {                    reader = LogFileFactory.getRandomReader(file, encryptionKeyProvider, fsyncPerTransaction);                    idLogFileMap.put(id, reader);                    writer.close();                }                                idIterator.remove();            }            Preconditions.checkState(logFileRefCountsAll.size() == 0, "Could not update all data file timestamps: " + logFileRefCountsAll);                        for (int index = 0; index < logDirs.length; index++) {                logFileRefCountsActive.add(logFiles.get(index).getLogFileID());            }            checkpointCompleted = true;        }    } finally {        unlockExclusive();    }        if (open && checkpointCompleted) {        removeOldLogs(logFileRefCountsActive);    }        return true;}
1
private void removeOldLogs(SortedSet<Integer> fileIDs)
{    Preconditions.checkState(open, "Log is closed");        for (File fileToDelete : pendingDeletes) {                FileUtils.deleteQuietly(fileToDelete);    }    pendingDeletes.clear();            int minFileID = fileIDs.first();        for (File logDir : logDirs) {        List<File> logs = LogUtils.getLogs(logDir);                LogUtils.sort(logs);                int size = logs.size() - MIN_NUM_LOGS;        for (int index = 0; index < size; index++) {            File logFile = logs.get(index);            int logFileID = LogUtils.getIDForFile(logFile);            if (logFileID < minFileID) {                LogFile.RandomReader reader = idLogFileMap.remove(logFileID);                if (reader != null) {                    reader.close();                }                File metaDataFile = Serialization.getMetaDataFile(logFile);                pendingDeletes.add(logFile);                pendingDeletes.add(metaDataFile);            }        }    }}
1
private void lock(File dir) throws IOException
{    FileLock lock = tryLock(dir);    if (lock == null) {        String msg = "Cannot lock " + dir + ". The directory is already locked. " + channelNameDescriptor;                throw new IOException(msg);    }    FileLock secondLock = tryLock(dir);    if (secondLock != null) {                secondLock.release();        secondLock.channel().close();    }    locks.put(dir.getAbsolutePath(), lock);}
1
private FileLock tryLock(File dir) throws IOException
{    File lockF = new File(dir, FILE_LOCK);    lockF.deleteOnExit();    RandomAccessFile file = new RandomAccessFile(lockF, "rws");    FileLock res = null;    try {        res = file.getChannel().tryLock();    } catch (OverlappingFileLockException oe) {        file.close();        return null;    } catch (IOException e) {                file.close();        throw e;    }    return res;}
1
private void unlock(File dir) throws IOException
{    FileLock lock = locks.remove(dir.getAbsolutePath());    if (lock == null) {        return;    }    lock.release();    lock.channel().close();    lock = null;}
0
public void run()
{    try {        if (log.open) {            log.writeCheckpoint();        }    } catch (IOException e) {        log.channelCounter.incrementCheckpointWriteErrorCount();            } catch (Throwable e) {        log.channelCounter.incrementCheckpointWriteErrorCount();            }}
1
protected static void skipRecord(RandomAccessFile fileHandle, int offset) throws IOException
{    fileHandle.seek(offset);    int length = fileHandle.readInt();    fileHandle.skipBytes(length);}
0
protected RandomAccessFile getFileHandle()
{    return writeFileHandle;}
0
protected void setLastCheckpointOffset(long lastCheckpointOffset)
{    this.lastCheckpointOffset = lastCheckpointOffset;}
0
protected void setLastCheckpointWriteOrderID(long lastCheckpointWriteOrderID)
{    this.lastCheckpointWriteOrderID = lastCheckpointWriteOrderID;}
0
protected long getLastCheckpointOffset()
{    return lastCheckpointOffset;}
0
protected long getLastCheckpointWriteOrderID()
{    return lastCheckpointWriteOrderID;}
0
protected File getFile()
{    return file;}
0
protected int getLogFileID()
{    return logFileID;}
0
 void markCheckpoint(long logWriteOrderID) throws IOException
{    markCheckpoint(lastCheckpointOffset, logWriteOrderID);}
0
 void close()
{    try {        writeFileHandle.close();    } catch (IOException e) {            }}
1
 void decrement(long numBytes)
{    Preconditions.checkArgument(numBytes >= 0, "numBytes less than zero");    value.addAndGet(-numBytes);}
0
 long getUsableSpace()
{    long now = System.currentTimeMillis();    if (now - interval > lastRefresh.get()) {        value.set(fs.getUsableSpace());        lastRefresh.set(now);    }    return Math.max(value.get(), 0L);}
0
public void run()
{    try {        sync();    } catch (Throwable ex) {            }}
1
protected CipherProvider.Encryptor getEncryptor()
{    return encryptor;}
0
 int getLogFileID()
{    return logFileID;}
0
 File getFile()
{    return file;}
0
 String getParent()
{    return file.getParent();}
0
 long getUsableSpace()
{    return usableSpace.getUsableSpace();}
0
 long getMaxSize()
{    return maxFileSize;}
0
 long getLastCommitPosition()
{    return lastCommitPosition;}
0
 long getLastSyncPosition()
{    return lastSyncPosition;}
0
 long getSyncCount()
{    return syncCount;}
0
 synchronized long position() throws IOException
{    return getFileChannel().position();}
0
 synchronized FlumeEventPointer put(ByteBuffer buffer) throws IOException
{    if (encryptor != null) {        buffer = ByteBuffer.wrap(encryptor.encrypt(buffer.array()));    }    Pair<Integer, Integer> pair = write(buffer);    return new FlumeEventPointer(pair.getLeft(), pair.getRight());}
0
 synchronized void take(ByteBuffer buffer) throws IOException
{    if (encryptor != null) {        buffer = ByteBuffer.wrap(encryptor.encrypt(buffer.array()));    }    write(buffer);}
0
 synchronized void rollback(ByteBuffer buffer) throws IOException
{    if (encryptor != null) {        buffer = ByteBuffer.wrap(encryptor.encrypt(buffer.array()));    }    write(buffer);}
0
 synchronized void commit(ByteBuffer buffer) throws IOException
{    if (encryptor != null) {        buffer = ByteBuffer.wrap(encryptor.encrypt(buffer.array()));    }    write(buffer);    dirty = true;    lastCommitPosition = position();}
0
private Pair<Integer, Integer> write(ByteBuffer buffer) throws IOException
{    if (!isOpen()) {        throw new LogFileRetryableIOException("File closed " + file);    }    long length = position();    long expectedLength = length + (long) buffer.limit();    if (expectedLength > maxFileSize) {        throw new LogFileRetryableIOException(expectedLength + " > " + maxFileSize);    }    int offset = (int) length;    Preconditions.checkState(offset >= 0, String.valueOf(offset));        int recordLength = 1 + (int) Serialization.SIZE_OF_INT + buffer.limit();    usableSpace.decrement(recordLength);    preallocate(recordLength);    ByteBuffer toWrite = ByteBuffer.allocate(recordLength);    toWrite.put(OP_RECORD);    writeDelimitedBuffer(toWrite, buffer);    toWrite.position(0);    int wrote = getFileChannel().write(toWrite);    Preconditions.checkState(wrote == toWrite.limit());    return Pair.of(getLogFileID(), offset);}
0
 synchronized boolean isRollRequired(ByteBuffer buffer) throws IOException
{    return isOpen() && position() + (long) buffer.limit() > getMaxSize();}
0
 synchronized void sync() throws IOException
{    if (!fsyncPerTransaction && !dirty) {        if (LOG.isDebugEnabled()) {                    }        return;    }    if (!isOpen()) {        throw new LogFileRetryableIOException("File closed " + file);    }    if (lastSyncPosition < lastCommitPosition) {        getFileChannel().force(false);        lastSyncPosition = position();        syncCount++;        dirty = false;    }}
1
protected boolean isOpen()
{    return open;}
0
protected RandomAccessFile getFileHandle()
{    return writeFileHandle;}
0
protected FileChannel getFileChannel()
{    return writeFileChannel;}
0
 synchronized void close()
{    if (open) {        open = false;        if (!fsyncPerTransaction) {                        if (syncExecutor != null) {                                syncExecutor.shutdown();            }        }        if (writeFileChannel.isOpen()) {                        try {                writeFileChannel.force(true);            } catch (IOException e) {                            }            try {                writeFileHandle.close();            } catch (IOException e) {                            }        }    }}
1
protected void preallocate(int size) throws IOException
{    long position = position();    if (position + size > getFileChannel().size()) {                synchronized (FILL) {            FILL.position(0);            getFileChannel().write(FILL, position);        }    }}
1
public void markRecordAsNoop(long offset) throws IOException
{                    fileHandle.seek(offset);    byte byteRead = fileHandle.readByte();    Preconditions.checkState(byteRead == OP_RECORD || byteRead == OP_NOOP, "Expected to read a record but the byte read indicates EOF");    fileHandle.seek(offset);        fileHandle.writeByte(OP_NOOP);}
1
public void close()
{    try {        fileHandle.getFD().sync();        fileHandle.close();    } catch (IOException e) {            }}
1
 File getFile()
{    return file;}
0
protected KeyProvider getKeyProvider()
{    return encryptionKeyProvider;}
0
 FlumeEvent get(int offset) throws IOException, InterruptedException, CorruptEventException, NoopRecordException
{    Preconditions.checkState(open, "File closed");    RandomAccessFile fileHandle = checkOut();    boolean error = true;    try {        fileHandle.seek(offset);        byte operation = fileHandle.readByte();        if (operation == OP_NOOP) {            throw new NoopRecordException("No op record found. Corrupt record " + "may have been repaired by File Channel Integrity tool");        }        if (operation != OP_RECORD) {            throw new CorruptEventException("Operation code is invalid. File " + "is corrupt. Please run File Channel Integrity tool.");        }        TransactionEventRecord record = doGet(fileHandle);        if (!(record instanceof Put)) {            Preconditions.checkState(false, "Record is " + record.getClass().getSimpleName());        }        error = false;        return ((Put) record).getEvent();    } finally {        if (error) {            close(fileHandle, file);        } else {            checkIn(fileHandle);        }    }}
0
 synchronized void close()
{    if (open) {        open = false;                List<RandomAccessFile> fileHandles = Lists.newArrayList();        while (readFileHandles.drainTo(fileHandles) > 0) {            for (RandomAccessFile fileHandle : fileHandles) {                synchronized (fileHandle) {                    try {                        fileHandle.close();                    } catch (IOException e) {                                            }                }            }            fileHandles.clear();            try {                Thread.sleep(5L);            } catch (InterruptedException e) {                        }        }    }}
1
private RandomAccessFile open() throws IOException
{    return new RandomAccessFile(file, "r");}
0
private void checkIn(RandomAccessFile fileHandle)
{    if (!readFileHandles.offer(fileHandle)) {        close(fileHandle, file);    }}
0
private RandomAccessFile checkOut() throws IOException, InterruptedException
{    RandomAccessFile fileHandle = readFileHandles.poll();    if (fileHandle != null) {        return fileHandle;    }    int remaining = readFileHandles.remainingCapacity();    if (remaining > 0) {                return open();    }    return readFileHandles.take();}
1
private static void close(RandomAccessFile fileHandle, File file)
{    if (fileHandle != null) {        try {            fileHandle.close();        } catch (IOException e) {                    }    }}
1
protected void setLastCheckpointPosition(long lastCheckpointPosition)
{    this.lastCheckpointPosition = lastCheckpointPosition;}
0
protected void setLastCheckpointWriteOrderID(long lastCheckpointWriteOrderID)
{    this.lastCheckpointWriteOrderID = lastCheckpointWriteOrderID;}
0
protected void setPreviousCheckpointPosition(long backupCheckpointPosition)
{    this.backupCheckpointPosition = backupCheckpointPosition;}
0
protected void setPreviousCheckpointWriteOrderID(long backupCheckpointWriteOrderID)
{    this.backupCheckpointWriteOrderID = backupCheckpointWriteOrderID;}
0
protected void setLogFileID(int logFileID)
{    this.logFileID = logFileID;    Preconditions.checkArgument(logFileID >= 0, "LogFileID is not positive: " + Integer.toHexString(logFileID));}
0
protected KeyProvider getKeyProvider()
{    return encryptionKeyProvider;}
0
protected RandomAccessFile getFileHandle()
{    return fileHandle;}
0
 int getLogFileID()
{    return logFileID;}
0
 void skipToLastCheckpointPosition(long checkpointWriteOrderID) throws IOException
{    if (lastCheckpointPosition > 0L) {        long position = 0;        if (lastCheckpointWriteOrderID <= checkpointWriteOrderID) {            position = lastCheckpointPosition;        } else if (backupCheckpointWriteOrderID <= checkpointWriteOrderID && backupCheckpointPosition > 0) {            position = backupCheckpointPosition;        }        fileChannel.position(position);            } else {            }}
1
public LogRecord next() throws IOException, CorruptEventException
{    int offset = -1;    try {        long position = fileChannel.position();        if (position > FileChannelConfiguration.DEFAULT_MAX_FILE_SIZE) {                    }        offset = (int) position;        Preconditions.checkState(offset >= 0);        while (offset < fileHandle.length()) {            byte operation = fileHandle.readByte();            if (operation == OP_RECORD) {                break;            } else if (operation == OP_EOF) {                                return null;            } else if (operation == OP_NOOP) {                                skipRecord(fileHandle, offset + 1);                offset = (int) fileHandle.getFilePointer();                continue;            } else {                                return null;            }        }        if (offset >= fileHandle.length()) {            return null;        }        return doNext(offset);    } catch (EOFException e) {        return null;    } catch (IOException e) {        throw new IOException("Unable to read next Transaction from log file " + file.getCanonicalPath() + " at offset " + offset, e);    }}
1
public long getPosition() throws IOException
{    return fileChannel.position();}
0
public void close()
{    if (fileHandle != null) {        try {            fileHandle.close();        } catch (IOException e) {        }    }}
0
protected static void writeDelimitedBuffer(ByteBuffer output, ByteBuffer buffer) throws IOException
{    output.putInt(buffer.limit());    output.put(buffer);}
0
protected static byte[] readDelimitedBuffer(RandomAccessFile fileHandle) throws IOException, CorruptEventException
{    int length = fileHandle.readInt();    if (length < 0) {        throw new CorruptEventException("Length of event is: " + String.valueOf(length) + ". Event must have length >= 0. Possible corruption of data or partial fsync.");    }    byte[] buffer = new byte[length];    try {        fileHandle.readFully(buffer);    } catch (EOFException ex) {        throw new CorruptEventException("Remaining data in file less than " + "expected size of event.", ex);    }    return buffer;}
0
public static void main(String[] args) throws EOFException, IOException, CorruptEventException
{    File file = new File(args[0]);    LogFile.SequentialReader reader = null;    try {        reader = LogFileFactory.getSequentialReader(file, null, false);        LogRecord entry;        FlumeEventPointer ptr;                        int fileId = reader.getLogFileID();        int count = 0;        int readCount = 0;        int putCount = 0;        int takeCount = 0;        int rollbackCount = 0;        int commitCount = 0;        while ((entry = reader.next()) != null) {            int offset = entry.getOffset();            TransactionEventRecord record = entry.getEvent();            short type = record.getRecordType();            long trans = record.getTransactionID();            long ts = record.getLogWriteOrderID();            readCount++;            ptr = null;            if (type == TransactionEventRecord.Type.PUT.get()) {                putCount++;                ptr = new FlumeEventPointer(fileId, offset);            } else if (type == TransactionEventRecord.Type.TAKE.get()) {                takeCount++;                Take take = (Take) record;                ptr = new FlumeEventPointer(take.getFileID(), take.getOffset());            } else if (type == TransactionEventRecord.Type.ROLLBACK.get()) {                rollbackCount++;            } else if (type == TransactionEventRecord.Type.COMMIT.get()) {                commitCount++;            } else {                Preconditions.checkArgument(false, "Unknown record type: " + Integer.toHexString(type));            }            System.out.println(Joiner.on(", ").skipNulls().join(trans, ts, fileId, offset, TransactionEventRecord.getName(type), ptr));        }        System.out.println("Replayed " + count + " from " + file + " read: " + readCount + ", put: " + putCount + ", take: " + takeCount + ", rollback: " + rollbackCount + ", commit: " + commitCount);    } catch (EOFException e) {        System.out.println("Hit EOF on " + file);    } finally {        if (reader != null) {            reader.close();        }    }}
0
 static LogFile.MetaDataWriter getMetaDataWriter(File file, int logFileID) throws IOException
{    RandomAccessFile logFile = null;    try {        File metaDataFile = Serialization.getMetaDataFile(file);        if (metaDataFile.exists()) {            return new LogFileV3.MetaDataWriter(file, logFileID);        }        logFile = new RandomAccessFile(file, "r");        int version = logFile.readInt();        if (Serialization.VERSION_2 == version) {            return new LogFileV2.MetaDataWriter(file, logFileID);        }        throw new IOException("File " + file + " has bad version " + Integer.toHexString(version));    } finally {        if (logFile != null) {            try {                logFile.close();            } catch (IOException e) {                            }        }    }}
1
 static LogFile.Writer getWriter(File file, int logFileID, long maxFileSize, @Nullable Key encryptionKey, @Nullable String encryptionKeyAlias, @Nullable String encryptionCipherProvider, long usableSpaceRefreshInterval, boolean fsyncPerTransaction, int fsyncInterval) throws IOException
{    Preconditions.checkState(!file.exists(), "File already exists " + file.getAbsolutePath());    Preconditions.checkState(file.createNewFile(), "File could not be created " + file.getAbsolutePath());    return new LogFileV3.Writer(file, logFileID, maxFileSize, encryptionKey, encryptionKeyAlias, encryptionCipherProvider, usableSpaceRefreshInterval, fsyncPerTransaction, fsyncInterval);}
0
 static LogFile.RandomReader getRandomReader(File file, @Nullable KeyProvider encryptionKeyProvider, boolean fsyncPerTransaction) throws IOException
{    RandomAccessFile logFile = new RandomAccessFile(file, "r");    try {        File metaDataFile = Serialization.getMetaDataFile(file);                if (logFile.length() == 0L || metaDataFile.exists()) {            return new LogFileV3.RandomReader(file, encryptionKeyProvider, fsyncPerTransaction);        }        int version = logFile.readInt();        if (Serialization.VERSION_2 == version) {            return new LogFileV2.RandomReader(file);        }        throw new IOException("File " + file + " has bad version " + Integer.toHexString(version));    } finally {        if (logFile != null) {            try {                logFile.close();            } catch (IOException e) {                            }        }    }}
1
 static LogFile.SequentialReader getSequentialReader(File file, @Nullable KeyProvider encryptionKeyProvider, boolean fsyncPerTransaction) throws IOException
{    RandomAccessFile logFile = null;    try {        File metaDataFile = Serialization.getMetaDataFile(file);        File oldMetadataFile = Serialization.getOldMetaDataFile(file);        File tempMetadataFile = Serialization.getMetaDataTempFile(file);        boolean hasMeta = false;                if (metaDataFile.exists()) {            hasMeta = true;        } else if (tempMetadataFile.exists()) {            if (tempMetadataFile.renameTo(metaDataFile)) {                hasMeta = true;            } else {                throw new IOException("Renaming of " + tempMetadataFile.getName() + " to " + metaDataFile.getName() + " failed");            }        } else if (oldMetadataFile.exists()) {            if (oldMetadataFile.renameTo(metaDataFile)) {                hasMeta = true;            } else {                throw new IOException("Renaming of " + oldMetadataFile.getName() + " to " + metaDataFile.getName() + " failed");            }        }        if (hasMeta) {                        if (oldMetadataFile.exists()) {                oldMetadataFile.delete();            }            if (tempMetadataFile.exists()) {                tempMetadataFile.delete();            }            if (metaDataFile.length() == 0L) {                if (file.length() != 0L) {                    String msg = String.format("MetaData file %s is empty, but log %s" + " is of size %d", metaDataFile, file, file.length());                    throw new IllegalStateException(msg);                }                throw new EOFException(String.format("MetaData file %s is empty", metaDataFile));            }            return new LogFileV3.SequentialReader(file, encryptionKeyProvider, fsyncPerTransaction);        }        logFile = new RandomAccessFile(file, "r");        int version = logFile.readInt();        if (Serialization.VERSION_2 == version) {            return new LogFileV2.SequentialReader(file);        }        throw new IOException("File " + file + " has bad version " + Integer.toHexString(version));    } finally {        if (logFile != null) {            try {                logFile.close();            } catch (IOException e) {                            }        }    }}
1
 int getVersion()
{    return Serialization.VERSION_2;}
0
 void markCheckpoint(long currentPosition, long logWriteOrderID) throws IOException
{    RandomAccessFile writeFileHandle = getFileHandle();    writeFileHandle.seek(OFFSET_CHECKPOINT);    writeFileHandle.writeLong(currentPosition);    writeFileHandle.writeLong(logWriteOrderID);    writeFileHandle.getChannel().force(true);    }
1
 int getVersion()
{    return Serialization.VERSION_2;}
0
 int getVersion()
{    return Serialization.VERSION_2;}
0
protected TransactionEventRecord doGet(RandomAccessFile fileHandle) throws IOException
{    return TransactionEventRecord.fromDataInputV2(fileHandle);}
0
public int getVersion()
{    return Serialization.VERSION_2;}
0
 LogRecord doNext(int offset) throws IOException
{    TransactionEventRecord event = TransactionEventRecord.fromDataInputV2(getFileHandle());    return new LogRecord(getLogFileID(), offset, event);}
0
 int getVersion()
{    return Serialization.VERSION_3;}
0
 void markCheckpoint(long currentPosition, long logWriteOrderID) throws IOException
{    ProtosFactory.LogFileMetaData.Builder metaDataBuilder = ProtosFactory.LogFileMetaData.newBuilder(logFileMetaData);    metaDataBuilder.setCheckpointPosition(currentPosition);    metaDataBuilder.setCheckpointWriteOrderID(logWriteOrderID);    /*       * Set the previous checkpoint position and write order id so that it       * would be possible to recover from a backup.       */    metaDataBuilder.setBackupCheckpointPosition(logFileMetaData.getCheckpointPosition());    metaDataBuilder.setBackupCheckpointWriteOrderID(logFileMetaData.getCheckpointWriteOrderID());    logFileMetaData = metaDataBuilder.build();    writeDelimitedTo(logFileMetaData, metaDataFile);}
0
 ProtosFactory.LogFileMetaData read() throws IOException
{    FileInputStream inputStream = new FileInputStream(metaDataFile);    try {        ProtosFactory.LogFileMetaData metaData = Preconditions.checkNotNull(ProtosFactory.LogFileMetaData.parseDelimitedFrom(inputStream), "Metadata cannot be null");        if (metaData.getLogFileID() != logFileID) {            throw new IOException("The file id of log file: " + logFile + " is different from expected " + " id: expected = " + logFileID + ", found = " + metaData.getLogFileID());        }        return metaData;    } finally {        try {            inputStream.close();        } catch (IOException e) {                    }    }}
1
public static void writeDelimitedTo(GeneratedMessage msg, File file) throws IOException
{    File tmp = Serialization.getMetaDataTempFile(file);    FileOutputStream outputStream = new FileOutputStream(tmp);    boolean closed = false;    try {        msg.writeDelimitedTo(outputStream);        outputStream.getChannel().force(true);        outputStream.close();        closed = true;        if (!tmp.renameTo(file)) {                                                                        File oldFile = Serialization.getOldMetaDataFile(file);            if (!file.renameTo(oldFile)) {                throw new IOException("Unable to rename " + file + " to " + oldFile);            }            if (!tmp.renameTo(file)) {                throw new IOException("Unable to rename " + tmp + " over " + file);            }            oldFile.delete();        }    } finally {        if (!closed) {            try {                outputStream.close();            } catch (IOException e) {                            }        }    }}
1
 int getVersion()
{    return Serialization.VERSION_3;}
0
private void initialize() throws IOException
{    File metaDataFile = Serialization.getMetaDataFile(getFile());    FileInputStream inputStream = new FileInputStream(metaDataFile);    try {        ProtosFactory.LogFileMetaData metaData = Preconditions.checkNotNull(ProtosFactory.LogFileMetaData.parseDelimitedFrom(inputStream), "MetaData cannot be null");        int version = metaData.getVersion();        if (version != getVersion()) {            throw new IOException("Version is " + Integer.toHexString(version) + " expected " + Integer.toHexString(getVersion()) + " file: " + getFile().getCanonicalPath());        }        encryptionEnabled = false;        if (metaData.hasEncryption()) {            if (getKeyProvider() == null) {                throw new IllegalStateException("Data file is encrypted but no " + " provider was specified");            }            ProtosFactory.LogFileEncryption encryption = metaData.getEncryption();            key = getKeyProvider().getKey(encryption.getKeyAlias());            cipherProvider = encryption.getCipherProvider();            parameters = encryption.getParameters().toByteArray();            encryptionEnabled = true;        }    } finally {        try {            inputStream.close();        } catch (IOException e) {                    }    }}
1
private CipherProvider.Decryptor getDecryptor()
{    CipherProvider.Decryptor decryptor = decryptors.poll();    if (decryptor == null) {        decryptor = CipherProviderFactory.getDecrypter(cipherProvider, key, parameters);    }    return decryptor;}
0
 int getVersion()
{    return Serialization.VERSION_3;}
0
protected TransactionEventRecord doGet(RandomAccessFile fileHandle) throws IOException, CorruptEventException
{        synchronized (this) {        if (!initialized) {            initialized = true;            initialize();        }    }    boolean success = false;    CipherProvider.Decryptor decryptor = null;    try {        byte[] buffer = readDelimitedBuffer(fileHandle);        if (encryptionEnabled) {            decryptor = getDecryptor();            buffer = decryptor.decrypt(buffer);        }        TransactionEventRecord event = TransactionEventRecord.fromByteArray(buffer);        success = true;        return event;    } catch (DecryptionFailureException ex) {        throw new CorruptEventException("Error decrypting event", ex);    } finally {        if (success && encryptionEnabled && decryptor != null) {            decryptors.offer(decryptor);        }    }}
0
public int getVersion()
{    return Serialization.VERSION_3;}
0
 LogRecord doNext(int offset) throws IOException, CorruptEventException, DecryptionFailureException
{    byte[] buffer = null;    TransactionEventRecord event = null;    try {        buffer = readDelimitedBuffer(getFileHandle());        if (decryptor != null) {            buffer = decryptor.decrypt(buffer);        }        event = TransactionEventRecord.fromByteArray(buffer);    } catch (CorruptEventException ex) {                        if (!fsyncPerTransaction) {            return null;        }        throw ex;    } catch (DecryptionFailureException ex) {        if (!fsyncPerTransaction) {                        return null;        }        throw ex;    }    return new LogRecord(getLogFileID(), offset, event);}
1
public int getFileID()
{    return fileID;}
0
public int getOffset()
{    return offset;}
0
public TransactionEventRecord getEvent()
{    return event;}
0
public int compareTo(LogRecord o)
{    int result = new Long(event.getLogWriteOrderID()).compareTo(o.getEvent().getLogWriteOrderID());    if (result == 0) {                        result = new Long(event.getTransactionID()).compareTo(o.getEvent().getTransactionID());        if (result == 0) {                                    Integer thisIndex = Arrays.binarySearch(replaySortOrder, event.getRecordType());            Integer thatIndex = Arrays.binarySearch(replaySortOrder, o.getEvent().getRecordType());            return thisIndex.compareTo(thatIndex);        }    }    return result;}
0
 static void sort(List<File> logs)
{    Collections.sort(logs, new Comparator<File>() {        @Override        public int compare(File file1, File file2) {            int id1 = getIDForFile(file1);            int id2 = getIDForFile(file2);            if (id1 > id2) {                return 1;            } else if (id1 == id2) {                return 0;            }            return -1;        }    });}
0
public int compare(File file1, File file2)
{    int id1 = getIDForFile(file1);    int id2 = getIDForFile(file2);    if (id1 > id2) {        return 1;    } else if (id1 == id2) {        return 0;    }    return -1;}
0
 static int getIDForFile(File file)
{    return Integer.parseInt(file.getName().substring(Log.PREFIX.length()));}
0
 static List<File> getLogs(File logDir)
{    List<File> result = Lists.newArrayList();    File[] files = logDir.listFiles();    if (files == null) {        String msg = logDir + ".listFiles() returned null: ";        msg += "File = " + logDir.isFile() + ", ";        msg += "Exists = " + logDir.exists() + ", ";        msg += "Writable = " + logDir.canWrite();        throw new IllegalStateException(msg);    }    for (File file : files) {        String name = file.getName();        if (pattern.matcher(name).matches()) {            result.add(file);        }    }    return result;}
0
 L getLeft()
{    return left;}
0
 R getRight()
{    return right;}
0
 static Pair<L, R> of(L left, R right)
{    return new Pair<L, R>(left, right);}
0
public static void registerAllExtensions(com.google.protobuf.ExtensionRegistry registry)
{}
0
public static Checkpoint getDefaultInstance()
{    return defaultInstance;}
0
public Checkpoint getDefaultInstanceForType()
{    return defaultInstance;}
0
public final com.google.protobuf.UnknownFieldSet getUnknownFields()
{    return this.unknownFields;}
0
public static final com.google.protobuf.Descriptors.Descriptor getDescriptor()
{    return org.apache.flume.channel.file.proto.ProtosFactory.internal_static_Checkpoint_descriptor;}
0
protected com.google.protobuf.GeneratedMessage.FieldAccessorTable internalGetFieldAccessorTable()
{    return org.apache.flume.channel.file.proto.ProtosFactory.internal_static_Checkpoint_fieldAccessorTable.ensureFieldAccessorsInitialized(org.apache.flume.channel.file.proto.ProtosFactory.Checkpoint.class, org.apache.flume.channel.file.proto.ProtosFactory.Checkpoint.Builder.class);}
0
public Checkpoint parsePartialFrom(com.google.protobuf.CodedInputStream input, com.google.protobuf.ExtensionRegistryLite extensionRegistry) throws com.google.protobuf.InvalidProtocolBufferException
{    return new Checkpoint(input, extensionRegistry);}
0
public com.google.protobuf.Parser<Checkpoint> getParserForType()
{    return PARSER;}
0
public boolean hasVersion()
{    return ((bitField0_ & 0x00000001) == 0x00000001);}
0
public int getVersion()
{    return version_;}
0
public boolean hasWriteOrderID()
{    return ((bitField0_ & 0x00000002) == 0x00000002);}
0
public long getWriteOrderID()
{    return writeOrderID_;}
0
public boolean hasQueueSize()
{    return ((bitField0_ & 0x00000004) == 0x00000004);}
0
public int getQueueSize()
{    return queueSize_;}
0
public boolean hasQueueHead()
{    return ((bitField0_ & 0x00000008) == 0x00000008);}
0
public int getQueueHead()
{    return queueHead_;}
0
public java.util.List<org.apache.flume.channel.file.proto.ProtosFactory.ActiveLog> getActiveLogsList()
{    return activeLogs_;}
0
public java.util.List<? extends org.apache.flume.channel.file.proto.ProtosFactory.ActiveLogOrBuilder> getActiveLogsOrBuilderList()
{    return activeLogs_;}
0
public int getActiveLogsCount()
{    return activeLogs_.size();}
0
public org.apache.flume.channel.file.proto.ProtosFactory.ActiveLog getActiveLogs(int index)
{    return activeLogs_.get(index);}
0
public org.apache.flume.channel.file.proto.ProtosFactory.ActiveLogOrBuilder getActiveLogsOrBuilder(int index)
{    return activeLogs_.get(index);}
0
private void initFields()
{    version_ = 0;    writeOrderID_ = 0L;    queueSize_ = 0;    queueHead_ = 0;    activeLogs_ = java.util.Collections.emptyList();}
0
public final boolean isInitialized()
{    byte isInitialized = memoizedIsInitialized;    if (isInitialized != -1)        return isInitialized == 1;    if (!hasVersion()) {        memoizedIsInitialized = 0;        return false;    }    if (!hasWriteOrderID()) {        memoizedIsInitialized = 0;        return false;    }    if (!hasQueueSize()) {        memoizedIsInitialized = 0;        return false;    }    if (!hasQueueHead()) {        memoizedIsInitialized = 0;        return false;    }    for (int i = 0; i < getActiveLogsCount(); i++) {        if (!getActiveLogs(i).isInitialized()) {            memoizedIsInitialized = 0;            return false;        }    }    memoizedIsInitialized = 1;    return true;}
0
public void writeTo(com.google.protobuf.CodedOutputStream output) throws java.io.IOException
{    getSerializedSize();    if (((bitField0_ & 0x00000001) == 0x00000001)) {        output.writeSFixed32(1, version_);    }    if (((bitField0_ & 0x00000002) == 0x00000002)) {        output.writeSFixed64(2, writeOrderID_);    }    if (((bitField0_ & 0x00000004) == 0x00000004)) {        output.writeSFixed32(3, queueSize_);    }    if (((bitField0_ & 0x00000008) == 0x00000008)) {        output.writeSFixed32(4, queueHead_);    }    for (int i = 0; i < activeLogs_.size(); i++) {        output.writeMessage(5, activeLogs_.get(i));    }    getUnknownFields().writeTo(output);}
0
public int getSerializedSize()
{    int size = memoizedSerializedSize;    if (size != -1)        return size;    size = 0;    if (((bitField0_ & 0x00000001) == 0x00000001)) {        size += com.google.protobuf.CodedOutputStream.computeSFixed32Size(1, version_);    }    if (((bitField0_ & 0x00000002) == 0x00000002)) {        size += com.google.protobuf.CodedOutputStream.computeSFixed64Size(2, writeOrderID_);    }    if (((bitField0_ & 0x00000004) == 0x00000004)) {        size += com.google.protobuf.CodedOutputStream.computeSFixed32Size(3, queueSize_);    }    if (((bitField0_ & 0x00000008) == 0x00000008)) {        size += com.google.protobuf.CodedOutputStream.computeSFixed32Size(4, queueHead_);    }    for (int i = 0; i < activeLogs_.size(); i++) {        size += com.google.protobuf.CodedOutputStream.computeMessageSize(5, activeLogs_.get(i));    }    size += getUnknownFields().getSerializedSize();    memoizedSerializedSize = size;    return size;}
0
protected java.lang.Object writeReplace() throws java.io.ObjectStreamException
{    return super.writeReplace();}
0
public static org.apache.flume.channel.file.proto.ProtosFactory.Checkpoint parseFrom(com.google.protobuf.ByteString data) throws com.google.protobuf.InvalidProtocolBufferException
{    return PARSER.parseFrom(data);}
0
public static org.apache.flume.channel.file.proto.ProtosFactory.Checkpoint parseFrom(com.google.protobuf.ByteString data, com.google.protobuf.ExtensionRegistryLite extensionRegistry) throws com.google.protobuf.InvalidProtocolBufferException
{    return PARSER.parseFrom(data, extensionRegistry);}
0
public static org.apache.flume.channel.file.proto.ProtosFactory.Checkpoint parseFrom(byte[] data) throws com.google.protobuf.InvalidProtocolBufferException
{    return PARSER.parseFrom(data);}
0
public static org.apache.flume.channel.file.proto.ProtosFactory.Checkpoint parseFrom(byte[] data, com.google.protobuf.ExtensionRegistryLite extensionRegistry) throws com.google.protobuf.InvalidProtocolBufferException
{    return PARSER.parseFrom(data, extensionRegistry);}
0
public static org.apache.flume.channel.file.proto.ProtosFactory.Checkpoint parseFrom(java.io.InputStream input) throws java.io.IOException
{    return PARSER.parseFrom(input);}
0
public static org.apache.flume.channel.file.proto.ProtosFactory.Checkpoint parseFrom(java.io.InputStream input, com.google.protobuf.ExtensionRegistryLite extensionRegistry) throws java.io.IOException
{    return PARSER.parseFrom(input, extensionRegistry);}
0
public static org.apache.flume.channel.file.proto.ProtosFactory.Checkpoint parseDelimitedFrom(java.io.InputStream input) throws java.io.IOException
{    return PARSER.parseDelimitedFrom(input);}
0
public static org.apache.flume.channel.file.proto.ProtosFactory.Checkpoint parseDelimitedFrom(java.io.InputStream input, com.google.protobuf.ExtensionRegistryLite extensionRegistry) throws java.io.IOException
{    return PARSER.parseDelimitedFrom(input, extensionRegistry);}
0
public static org.apache.flume.channel.file.proto.ProtosFactory.Checkpoint parseFrom(com.google.protobuf.CodedInputStream input) throws java.io.IOException
{    return PARSER.parseFrom(input);}
0
public static org.apache.flume.channel.file.proto.ProtosFactory.Checkpoint parseFrom(com.google.protobuf.CodedInputStream input, com.google.protobuf.ExtensionRegistryLite extensionRegistry) throws java.io.IOException
{    return PARSER.parseFrom(input, extensionRegistry);}
0
public static Builder newBuilder()
{    return Builder.create();}
0
public Builder newBuilderForType()
{    return newBuilder();}
0
public static Builder newBuilder(org.apache.flume.channel.file.proto.ProtosFactory.Checkpoint prototype)
{    return newBuilder().mergeFrom(prototype);}
0
public Builder toBuilder()
{    return newBuilder(this);}
0
protected Builder newBuilderForType(com.google.protobuf.GeneratedMessage.BuilderParent parent)
{    Builder builder = new Builder(parent);    return builder;}
0
public static final com.google.protobuf.Descriptors.Descriptor getDescriptor()
{    return org.apache.flume.channel.file.proto.ProtosFactory.internal_static_Checkpoint_descriptor;}
0
protected com.google.protobuf.GeneratedMessage.FieldAccessorTable internalGetFieldAccessorTable()
{    return org.apache.flume.channel.file.proto.ProtosFactory.internal_static_Checkpoint_fieldAccessorTable.ensureFieldAccessorsInitialized(org.apache.flume.channel.file.proto.ProtosFactory.Checkpoint.class, org.apache.flume.channel.file.proto.ProtosFactory.Checkpoint.Builder.class);}
0
private void maybeForceBuilderInitialization()
{    if (com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders) {        getActiveLogsFieldBuilder();    }}
0
private static Builder create()
{    return new Builder();}
0
public Builder clear()
{    super.clear();    version_ = 0;    bitField0_ = (bitField0_ & ~0x00000001);    writeOrderID_ = 0L;    bitField0_ = (bitField0_ & ~0x00000002);    queueSize_ = 0;    bitField0_ = (bitField0_ & ~0x00000004);    queueHead_ = 0;    bitField0_ = (bitField0_ & ~0x00000008);    if (activeLogsBuilder_ == null) {        activeLogs_ = java.util.Collections.emptyList();        bitField0_ = (bitField0_ & ~0x00000010);    } else {        activeLogsBuilder_.clear();    }    return this;}
0
public Builder clone()
{    return create().mergeFrom(buildPartial());}
0
public com.google.protobuf.Descriptors.Descriptor getDescriptorForType()
{    return org.apache.flume.channel.file.proto.ProtosFactory.internal_static_Checkpoint_descriptor;}
0
public org.apache.flume.channel.file.proto.ProtosFactory.Checkpoint getDefaultInstanceForType()
{    return org.apache.flume.channel.file.proto.ProtosFactory.Checkpoint.getDefaultInstance();}
0
public org.apache.flume.channel.file.proto.ProtosFactory.Checkpoint build()
{    org.apache.flume.channel.file.proto.ProtosFactory.Checkpoint result = buildPartial();    if (!result.isInitialized()) {        throw newUninitializedMessageException(result);    }    return result;}
0
public org.apache.flume.channel.file.proto.ProtosFactory.Checkpoint buildPartial()
{    org.apache.flume.channel.file.proto.ProtosFactory.Checkpoint result = new org.apache.flume.channel.file.proto.ProtosFactory.Checkpoint(this);    int from_bitField0_ = bitField0_;    int to_bitField0_ = 0;    if (((from_bitField0_ & 0x00000001) == 0x00000001)) {        to_bitField0_ |= 0x00000001;    }    result.version_ = version_;    if (((from_bitField0_ & 0x00000002) == 0x00000002)) {        to_bitField0_ |= 0x00000002;    }    result.writeOrderID_ = writeOrderID_;    if (((from_bitField0_ & 0x00000004) == 0x00000004)) {        to_bitField0_ |= 0x00000004;    }    result.queueSize_ = queueSize_;    if (((from_bitField0_ & 0x00000008) == 0x00000008)) {        to_bitField0_ |= 0x00000008;    }    result.queueHead_ = queueHead_;    if (activeLogsBuilder_ == null) {        if (((bitField0_ & 0x00000010) == 0x00000010)) {            activeLogs_ = java.util.Collections.unmodifiableList(activeLogs_);            bitField0_ = (bitField0_ & ~0x00000010);        }        result.activeLogs_ = activeLogs_;    } else {        result.activeLogs_ = activeLogsBuilder_.build();    }    result.bitField0_ = to_bitField0_;    onBuilt();    return result;}
0
public Builder mergeFrom(com.google.protobuf.Message other)
{    if (other instanceof org.apache.flume.channel.file.proto.ProtosFactory.Checkpoint) {        return mergeFrom((org.apache.flume.channel.file.proto.ProtosFactory.Checkpoint) other);    } else {        super.mergeFrom(other);        return this;    }}
0
public Builder mergeFrom(org.apache.flume.channel.file.proto.ProtosFactory.Checkpoint other)
{    if (other == org.apache.flume.channel.file.proto.ProtosFactory.Checkpoint.getDefaultInstance())        return this;    if (other.hasVersion()) {        setVersion(other.getVersion());    }    if (other.hasWriteOrderID()) {        setWriteOrderID(other.getWriteOrderID());    }    if (other.hasQueueSize()) {        setQueueSize(other.getQueueSize());    }    if (other.hasQueueHead()) {        setQueueHead(other.getQueueHead());    }    if (activeLogsBuilder_ == null) {        if (!other.activeLogs_.isEmpty()) {            if (activeLogs_.isEmpty()) {                activeLogs_ = other.activeLogs_;                bitField0_ = (bitField0_ & ~0x00000010);            } else {                ensureActiveLogsIsMutable();                activeLogs_.addAll(other.activeLogs_);            }            onChanged();        }    } else {        if (!other.activeLogs_.isEmpty()) {            if (activeLogsBuilder_.isEmpty()) {                activeLogsBuilder_.dispose();                activeLogsBuilder_ = null;                activeLogs_ = other.activeLogs_;                bitField0_ = (bitField0_ & ~0x00000010);                activeLogsBuilder_ = com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders ? getActiveLogsFieldBuilder() : null;            } else {                activeLogsBuilder_.addAllMessages(other.activeLogs_);            }        }    }    this.mergeUnknownFields(other.getUnknownFields());    return this;}
0
public final boolean isInitialized()
{    if (!hasVersion()) {        return false;    }    if (!hasWriteOrderID()) {        return false;    }    if (!hasQueueSize()) {        return false;    }    if (!hasQueueHead()) {        return false;    }    for (int i = 0; i < getActiveLogsCount(); i++) {        if (!getActiveLogs(i).isInitialized()) {            return false;        }    }    return true;}
0
public Builder mergeFrom(com.google.protobuf.CodedInputStream input, com.google.protobuf.ExtensionRegistryLite extensionRegistry) throws java.io.IOException
{    org.apache.flume.channel.file.proto.ProtosFactory.Checkpoint parsedMessage = null;    try {        parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);    } catch (com.google.protobuf.InvalidProtocolBufferException e) {        parsedMessage = (org.apache.flume.channel.file.proto.ProtosFactory.Checkpoint) e.getUnfinishedMessage();        throw e;    } finally {        if (parsedMessage != null) {            mergeFrom(parsedMessage);        }    }    return this;}
0
public boolean hasVersion()
{    return ((bitField0_ & 0x00000001) == 0x00000001);}
0
public int getVersion()
{    return version_;}
0
public Builder setVersion(int value)
{    bitField0_ |= 0x00000001;    version_ = value;    onChanged();    return this;}
0
public Builder clearVersion()
{    bitField0_ = (bitField0_ & ~0x00000001);    version_ = 0;    onChanged();    return this;}
0
public boolean hasWriteOrderID()
{    return ((bitField0_ & 0x00000002) == 0x00000002);}
0
public long getWriteOrderID()
{    return writeOrderID_;}
0
public Builder setWriteOrderID(long value)
{    bitField0_ |= 0x00000002;    writeOrderID_ = value;    onChanged();    return this;}
0
public Builder clearWriteOrderID()
{    bitField0_ = (bitField0_ & ~0x00000002);    writeOrderID_ = 0L;    onChanged();    return this;}
0
public boolean hasQueueSize()
{    return ((bitField0_ & 0x00000004) == 0x00000004);}
0
public int getQueueSize()
{    return queueSize_;}
0
public Builder setQueueSize(int value)
{    bitField0_ |= 0x00000004;    queueSize_ = value;    onChanged();    return this;}
0
public Builder clearQueueSize()
{    bitField0_ = (bitField0_ & ~0x00000004);    queueSize_ = 0;    onChanged();    return this;}
0
public boolean hasQueueHead()
{    return ((bitField0_ & 0x00000008) == 0x00000008);}
0
public int getQueueHead()
{    return queueHead_;}
0
public Builder setQueueHead(int value)
{    bitField0_ |= 0x00000008;    queueHead_ = value;    onChanged();    return this;}
0
public Builder clearQueueHead()
{    bitField0_ = (bitField0_ & ~0x00000008);    queueHead_ = 0;    onChanged();    return this;}
0
private void ensureActiveLogsIsMutable()
{    if (!((bitField0_ & 0x00000010) == 0x00000010)) {        activeLogs_ = new java.util.ArrayList<org.apache.flume.channel.file.proto.ProtosFactory.ActiveLog>(activeLogs_);        bitField0_ |= 0x00000010;    }}
0
public java.util.List<org.apache.flume.channel.file.proto.ProtosFactory.ActiveLog> getActiveLogsList()
{    if (activeLogsBuilder_ == null) {        return java.util.Collections.unmodifiableList(activeLogs_);    } else {        return activeLogsBuilder_.getMessageList();    }}
0
public int getActiveLogsCount()
{    if (activeLogsBuilder_ == null) {        return activeLogs_.size();    } else {        return activeLogsBuilder_.getCount();    }}
0
public org.apache.flume.channel.file.proto.ProtosFactory.ActiveLog getActiveLogs(int index)
{    if (activeLogsBuilder_ == null) {        return activeLogs_.get(index);    } else {        return activeLogsBuilder_.getMessage(index);    }}
0
public Builder setActiveLogs(int index, org.apache.flume.channel.file.proto.ProtosFactory.ActiveLog value)
{    if (activeLogsBuilder_ == null) {        if (value == null) {            throw new NullPointerException();        }        ensureActiveLogsIsMutable();        activeLogs_.set(index, value);        onChanged();    } else {        activeLogsBuilder_.setMessage(index, value);    }    return this;}
0
public Builder setActiveLogs(int index, org.apache.flume.channel.file.proto.ProtosFactory.ActiveLog.Builder builderForValue)
{    if (activeLogsBuilder_ == null) {        ensureActiveLogsIsMutable();        activeLogs_.set(index, builderForValue.build());        onChanged();    } else {        activeLogsBuilder_.setMessage(index, builderForValue.build());    }    return this;}
0
public Builder addActiveLogs(org.apache.flume.channel.file.proto.ProtosFactory.ActiveLog value)
{    if (activeLogsBuilder_ == null) {        if (value == null) {            throw new NullPointerException();        }        ensureActiveLogsIsMutable();        activeLogs_.add(value);        onChanged();    } else {        activeLogsBuilder_.addMessage(value);    }    return this;}
0
public Builder addActiveLogs(int index, org.apache.flume.channel.file.proto.ProtosFactory.ActiveLog value)
{    if (activeLogsBuilder_ == null) {        if (value == null) {            throw new NullPointerException();        }        ensureActiveLogsIsMutable();        activeLogs_.add(index, value);        onChanged();    } else {        activeLogsBuilder_.addMessage(index, value);    }    return this;}
0
public Builder addActiveLogs(org.apache.flume.channel.file.proto.ProtosFactory.ActiveLog.Builder builderForValue)
{    if (activeLogsBuilder_ == null) {        ensureActiveLogsIsMutable();        activeLogs_.add(builderForValue.build());        onChanged();    } else {        activeLogsBuilder_.addMessage(builderForValue.build());    }    return this;}
0
public Builder addActiveLogs(int index, org.apache.flume.channel.file.proto.ProtosFactory.ActiveLog.Builder builderForValue)
{    if (activeLogsBuilder_ == null) {        ensureActiveLogsIsMutable();        activeLogs_.add(index, builderForValue.build());        onChanged();    } else {        activeLogsBuilder_.addMessage(index, builderForValue.build());    }    return this;}
0
public Builder addAllActiveLogs(java.lang.Iterable<? extends org.apache.flume.channel.file.proto.ProtosFactory.ActiveLog> values)
{    if (activeLogsBuilder_ == null) {        ensureActiveLogsIsMutable();        super.addAll(values, activeLogs_);        onChanged();    } else {        activeLogsBuilder_.addAllMessages(values);    }    return this;}
0
public Builder clearActiveLogs()
{    if (activeLogsBuilder_ == null) {        activeLogs_ = java.util.Collections.emptyList();        bitField0_ = (bitField0_ & ~0x00000010);        onChanged();    } else {        activeLogsBuilder_.clear();    }    return this;}
0
public Builder removeActiveLogs(int index)
{    if (activeLogsBuilder_ == null) {        ensureActiveLogsIsMutable();        activeLogs_.remove(index);        onChanged();    } else {        activeLogsBuilder_.remove(index);    }    return this;}
0
public org.apache.flume.channel.file.proto.ProtosFactory.ActiveLog.Builder getActiveLogsBuilder(int index)
{    return getActiveLogsFieldBuilder().getBuilder(index);}
0
public org.apache.flume.channel.file.proto.ProtosFactory.ActiveLogOrBuilder getActiveLogsOrBuilder(int index)
{    if (activeLogsBuilder_ == null) {        return activeLogs_.get(index);    } else {        return activeLogsBuilder_.getMessageOrBuilder(index);    }}
0
public java.util.List<? extends org.apache.flume.channel.file.proto.ProtosFactory.ActiveLogOrBuilder> getActiveLogsOrBuilderList()
{    if (activeLogsBuilder_ != null) {        return activeLogsBuilder_.getMessageOrBuilderList();    } else {        return java.util.Collections.unmodifiableList(activeLogs_);    }}
0
public org.apache.flume.channel.file.proto.ProtosFactory.ActiveLog.Builder addActiveLogsBuilder()
{    return getActiveLogsFieldBuilder().addBuilder(org.apache.flume.channel.file.proto.ProtosFactory.ActiveLog.getDefaultInstance());}
0
public org.apache.flume.channel.file.proto.ProtosFactory.ActiveLog.Builder addActiveLogsBuilder(int index)
{    return getActiveLogsFieldBuilder().addBuilder(index, org.apache.flume.channel.file.proto.ProtosFactory.ActiveLog.getDefaultInstance());}
0
public java.util.List<org.apache.flume.channel.file.proto.ProtosFactory.ActiveLog.Builder> getActiveLogsBuilderList()
{    return getActiveLogsFieldBuilder().getBuilderList();}
0
private com.google.protobuf.RepeatedFieldBuilder<org.apache.flume.channel.file.proto.ProtosFactory.ActiveLog, org.apache.flume.channel.file.proto.ProtosFactory.ActiveLog.Builder, org.apache.flume.channel.file.proto.ProtosFactory.ActiveLogOrBuilder> getActiveLogsFieldBuilder()
{    if (activeLogsBuilder_ == null) {        activeLogsBuilder_ = new com.google.protobuf.RepeatedFieldBuilder<org.apache.flume.channel.file.proto.ProtosFactory.ActiveLog, org.apache.flume.channel.file.proto.ProtosFactory.ActiveLog.Builder, org.apache.flume.channel.file.proto.ProtosFactory.ActiveLogOrBuilder>(activeLogs_, ((bitField0_ & 0x00000010) == 0x00000010), getParentForChildren(), isClean());        activeLogs_ = null;    }    return activeLogsBuilder_;}
0
public static ActiveLog getDefaultInstance()
{    return defaultInstance;}
0
public ActiveLog getDefaultInstanceForType()
{    return defaultInstance;}
0
public final com.google.protobuf.UnknownFieldSet getUnknownFields()
{    return this.unknownFields;}
0
public static final com.google.protobuf.Descriptors.Descriptor getDescriptor()
{    return org.apache.flume.channel.file.proto.ProtosFactory.internal_static_ActiveLog_descriptor;}
0
protected com.google.protobuf.GeneratedMessage.FieldAccessorTable internalGetFieldAccessorTable()
{    return org.apache.flume.channel.file.proto.ProtosFactory.internal_static_ActiveLog_fieldAccessorTable.ensureFieldAccessorsInitialized(org.apache.flume.channel.file.proto.ProtosFactory.ActiveLog.class, org.apache.flume.channel.file.proto.ProtosFactory.ActiveLog.Builder.class);}
0
public ActiveLog parsePartialFrom(com.google.protobuf.CodedInputStream input, com.google.protobuf.ExtensionRegistryLite extensionRegistry) throws com.google.protobuf.InvalidProtocolBufferException
{    return new ActiveLog(input, extensionRegistry);}
0
public com.google.protobuf.Parser<ActiveLog> getParserForType()
{    return PARSER;}
0
public boolean hasLogFileID()
{    return ((bitField0_ & 0x00000001) == 0x00000001);}
0
public int getLogFileID()
{    return logFileID_;}
0
public boolean hasCount()
{    return ((bitField0_ & 0x00000002) == 0x00000002);}
0
public int getCount()
{    return count_;}
0
private void initFields()
{    logFileID_ = 0;    count_ = 0;}
0
public final boolean isInitialized()
{    byte isInitialized = memoizedIsInitialized;    if (isInitialized != -1)        return isInitialized == 1;    if (!hasLogFileID()) {        memoizedIsInitialized = 0;        return false;    }    if (!hasCount()) {        memoizedIsInitialized = 0;        return false;    }    memoizedIsInitialized = 1;    return true;}
0
public void writeTo(com.google.protobuf.CodedOutputStream output) throws java.io.IOException
{    getSerializedSize();    if (((bitField0_ & 0x00000001) == 0x00000001)) {        output.writeSFixed32(1, logFileID_);    }    if (((bitField0_ & 0x00000002) == 0x00000002)) {        output.writeSFixed32(2, count_);    }    getUnknownFields().writeTo(output);}
0
public int getSerializedSize()
{    int size = memoizedSerializedSize;    if (size != -1)        return size;    size = 0;    if (((bitField0_ & 0x00000001) == 0x00000001)) {        size += com.google.protobuf.CodedOutputStream.computeSFixed32Size(1, logFileID_);    }    if (((bitField0_ & 0x00000002) == 0x00000002)) {        size += com.google.protobuf.CodedOutputStream.computeSFixed32Size(2, count_);    }    size += getUnknownFields().getSerializedSize();    memoizedSerializedSize = size;    return size;}
0
protected java.lang.Object writeReplace() throws java.io.ObjectStreamException
{    return super.writeReplace();}
0
public static org.apache.flume.channel.file.proto.ProtosFactory.ActiveLog parseFrom(com.google.protobuf.ByteString data) throws com.google.protobuf.InvalidProtocolBufferException
{    return PARSER.parseFrom(data);}
0
public static org.apache.flume.channel.file.proto.ProtosFactory.ActiveLog parseFrom(com.google.protobuf.ByteString data, com.google.protobuf.ExtensionRegistryLite extensionRegistry) throws com.google.protobuf.InvalidProtocolBufferException
{    return PARSER.parseFrom(data, extensionRegistry);}
0
public static org.apache.flume.channel.file.proto.ProtosFactory.ActiveLog parseFrom(byte[] data) throws com.google.protobuf.InvalidProtocolBufferException
{    return PARSER.parseFrom(data);}
0
public static org.apache.flume.channel.file.proto.ProtosFactory.ActiveLog parseFrom(byte[] data, com.google.protobuf.ExtensionRegistryLite extensionRegistry) throws com.google.protobuf.InvalidProtocolBufferException
{    return PARSER.parseFrom(data, extensionRegistry);}
0
public static org.apache.flume.channel.file.proto.ProtosFactory.ActiveLog parseFrom(java.io.InputStream input) throws java.io.IOException
{    return PARSER.parseFrom(input);}
0
public static org.apache.flume.channel.file.proto.ProtosFactory.ActiveLog parseFrom(java.io.InputStream input, com.google.protobuf.ExtensionRegistryLite extensionRegistry) throws java.io.IOException
{    return PARSER.parseFrom(input, extensionRegistry);}
0
public static org.apache.flume.channel.file.proto.ProtosFactory.ActiveLog parseDelimitedFrom(java.io.InputStream input) throws java.io.IOException
{    return PARSER.parseDelimitedFrom(input);}
0
public static org.apache.flume.channel.file.proto.ProtosFactory.ActiveLog parseDelimitedFrom(java.io.InputStream input, com.google.protobuf.ExtensionRegistryLite extensionRegistry) throws java.io.IOException
{    return PARSER.parseDelimitedFrom(input, extensionRegistry);}
0
public static org.apache.flume.channel.file.proto.ProtosFactory.ActiveLog parseFrom(com.google.protobuf.CodedInputStream input) throws java.io.IOException
{    return PARSER.parseFrom(input);}
0
public static org.apache.flume.channel.file.proto.ProtosFactory.ActiveLog parseFrom(com.google.protobuf.CodedInputStream input, com.google.protobuf.ExtensionRegistryLite extensionRegistry) throws java.io.IOException
{    return PARSER.parseFrom(input, extensionRegistry);}
0
public static Builder newBuilder()
{    return Builder.create();}
0
public Builder newBuilderForType()
{    return newBuilder();}
0
public static Builder newBuilder(org.apache.flume.channel.file.proto.ProtosFactory.ActiveLog prototype)
{    return newBuilder().mergeFrom(prototype);}
0
public Builder toBuilder()
{    return newBuilder(this);}
0
protected Builder newBuilderForType(com.google.protobuf.GeneratedMessage.BuilderParent parent)
{    Builder builder = new Builder(parent);    return builder;}
0
public static final com.google.protobuf.Descriptors.Descriptor getDescriptor()
{    return org.apache.flume.channel.file.proto.ProtosFactory.internal_static_ActiveLog_descriptor;}
0
protected com.google.protobuf.GeneratedMessage.FieldAccessorTable internalGetFieldAccessorTable()
{    return org.apache.flume.channel.file.proto.ProtosFactory.internal_static_ActiveLog_fieldAccessorTable.ensureFieldAccessorsInitialized(org.apache.flume.channel.file.proto.ProtosFactory.ActiveLog.class, org.apache.flume.channel.file.proto.ProtosFactory.ActiveLog.Builder.class);}
0
private void maybeForceBuilderInitialization()
{    if (com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders) {    }}
0
private static Builder create()
{    return new Builder();}
0
public Builder clear()
{    super.clear();    logFileID_ = 0;    bitField0_ = (bitField0_ & ~0x00000001);    count_ = 0;    bitField0_ = (bitField0_ & ~0x00000002);    return this;}
0
public Builder clone()
{    return create().mergeFrom(buildPartial());}
0
public com.google.protobuf.Descriptors.Descriptor getDescriptorForType()
{    return org.apache.flume.channel.file.proto.ProtosFactory.internal_static_ActiveLog_descriptor;}
0
public org.apache.flume.channel.file.proto.ProtosFactory.ActiveLog getDefaultInstanceForType()
{    return org.apache.flume.channel.file.proto.ProtosFactory.ActiveLog.getDefaultInstance();}
0
public org.apache.flume.channel.file.proto.ProtosFactory.ActiveLog build()
{    org.apache.flume.channel.file.proto.ProtosFactory.ActiveLog result = buildPartial();    if (!result.isInitialized()) {        throw newUninitializedMessageException(result);    }    return result;}
0
public org.apache.flume.channel.file.proto.ProtosFactory.ActiveLog buildPartial()
{    org.apache.flume.channel.file.proto.ProtosFactory.ActiveLog result = new org.apache.flume.channel.file.proto.ProtosFactory.ActiveLog(this);    int from_bitField0_ = bitField0_;    int to_bitField0_ = 0;    if (((from_bitField0_ & 0x00000001) == 0x00000001)) {        to_bitField0_ |= 0x00000001;    }    result.logFileID_ = logFileID_;    if (((from_bitField0_ & 0x00000002) == 0x00000002)) {        to_bitField0_ |= 0x00000002;    }    result.count_ = count_;    result.bitField0_ = to_bitField0_;    onBuilt();    return result;}
0
public Builder mergeFrom(com.google.protobuf.Message other)
{    if (other instanceof org.apache.flume.channel.file.proto.ProtosFactory.ActiveLog) {        return mergeFrom((org.apache.flume.channel.file.proto.ProtosFactory.ActiveLog) other);    } else {        super.mergeFrom(other);        return this;    }}
0
public Builder mergeFrom(org.apache.flume.channel.file.proto.ProtosFactory.ActiveLog other)
{    if (other == org.apache.flume.channel.file.proto.ProtosFactory.ActiveLog.getDefaultInstance())        return this;    if (other.hasLogFileID()) {        setLogFileID(other.getLogFileID());    }    if (other.hasCount()) {        setCount(other.getCount());    }    this.mergeUnknownFields(other.getUnknownFields());    return this;}
0
public final boolean isInitialized()
{    if (!hasLogFileID()) {        return false;    }    if (!hasCount()) {        return false;    }    return true;}
0
public Builder mergeFrom(com.google.protobuf.CodedInputStream input, com.google.protobuf.ExtensionRegistryLite extensionRegistry) throws java.io.IOException
{    org.apache.flume.channel.file.proto.ProtosFactory.ActiveLog parsedMessage = null;    try {        parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);    } catch (com.google.protobuf.InvalidProtocolBufferException e) {        parsedMessage = (org.apache.flume.channel.file.proto.ProtosFactory.ActiveLog) e.getUnfinishedMessage();        throw e;    } finally {        if (parsedMessage != null) {            mergeFrom(parsedMessage);        }    }    return this;}
0
public boolean hasLogFileID()
{    return ((bitField0_ & 0x00000001) == 0x00000001);}
0
public int getLogFileID()
{    return logFileID_;}
0
public Builder setLogFileID(int value)
{    bitField0_ |= 0x00000001;    logFileID_ = value;    onChanged();    return this;}
0
public Builder clearLogFileID()
{    bitField0_ = (bitField0_ & ~0x00000001);    logFileID_ = 0;    onChanged();    return this;}
0
public boolean hasCount()
{    return ((bitField0_ & 0x00000002) == 0x00000002);}
0
public int getCount()
{    return count_;}
0
public Builder setCount(int value)
{    bitField0_ |= 0x00000002;    count_ = value;    onChanged();    return this;}
0
public Builder clearCount()
{    bitField0_ = (bitField0_ & ~0x00000002);    count_ = 0;    onChanged();    return this;}
0
public static LogFileMetaData getDefaultInstance()
{    return defaultInstance;}
0
public LogFileMetaData getDefaultInstanceForType()
{    return defaultInstance;}
0
public final com.google.protobuf.UnknownFieldSet getUnknownFields()
{    return this.unknownFields;}
0
public static final com.google.protobuf.Descriptors.Descriptor getDescriptor()
{    return org.apache.flume.channel.file.proto.ProtosFactory.internal_static_LogFileMetaData_descriptor;}
0
protected com.google.protobuf.GeneratedMessage.FieldAccessorTable internalGetFieldAccessorTable()
{    return org.apache.flume.channel.file.proto.ProtosFactory.internal_static_LogFileMetaData_fieldAccessorTable.ensureFieldAccessorsInitialized(org.apache.flume.channel.file.proto.ProtosFactory.LogFileMetaData.class, org.apache.flume.channel.file.proto.ProtosFactory.LogFileMetaData.Builder.class);}
0
public LogFileMetaData parsePartialFrom(com.google.protobuf.CodedInputStream input, com.google.protobuf.ExtensionRegistryLite extensionRegistry) throws com.google.protobuf.InvalidProtocolBufferException
{    return new LogFileMetaData(input, extensionRegistry);}
0
public com.google.protobuf.Parser<LogFileMetaData> getParserForType()
{    return PARSER;}
0
public boolean hasVersion()
{    return ((bitField0_ & 0x00000001) == 0x00000001);}
0
public int getVersion()
{    return version_;}
0
public boolean hasLogFileID()
{    return ((bitField0_ & 0x00000002) == 0x00000002);}
0
public int getLogFileID()
{    return logFileID_;}
0
public boolean hasCheckpointPosition()
{    return ((bitField0_ & 0x00000004) == 0x00000004);}
0
public long getCheckpointPosition()
{    return checkpointPosition_;}
0
public boolean hasCheckpointWriteOrderID()
{    return ((bitField0_ & 0x00000008) == 0x00000008);}
0
public long getCheckpointWriteOrderID()
{    return checkpointWriteOrderID_;}
0
public boolean hasEncryption()
{    return ((bitField0_ & 0x00000010) == 0x00000010);}
0
public org.apache.flume.channel.file.proto.ProtosFactory.LogFileEncryption getEncryption()
{    return encryption_;}
0
public org.apache.flume.channel.file.proto.ProtosFactory.LogFileEncryptionOrBuilder getEncryptionOrBuilder()
{    return encryption_;}
0
public boolean hasBackupCheckpointPosition()
{    return ((bitField0_ & 0x00000020) == 0x00000020);}
0
public long getBackupCheckpointPosition()
{    return backupCheckpointPosition_;}
0
public boolean hasBackupCheckpointWriteOrderID()
{    return ((bitField0_ & 0x00000040) == 0x00000040);}
0
public long getBackupCheckpointWriteOrderID()
{    return backupCheckpointWriteOrderID_;}
0
private void initFields()
{    version_ = 0;    logFileID_ = 0;    checkpointPosition_ = 0L;    checkpointWriteOrderID_ = 0L;    encryption_ = org.apache.flume.channel.file.proto.ProtosFactory.LogFileEncryption.getDefaultInstance();    backupCheckpointPosition_ = 0L;    backupCheckpointWriteOrderID_ = 0L;}
0
public final boolean isInitialized()
{    byte isInitialized = memoizedIsInitialized;    if (isInitialized != -1)        return isInitialized == 1;    if (!hasVersion()) {        memoizedIsInitialized = 0;        return false;    }    if (!hasLogFileID()) {        memoizedIsInitialized = 0;        return false;    }    if (!hasCheckpointPosition()) {        memoizedIsInitialized = 0;        return false;    }    if (!hasCheckpointWriteOrderID()) {        memoizedIsInitialized = 0;        return false;    }    if (hasEncryption()) {        if (!getEncryption().isInitialized()) {            memoizedIsInitialized = 0;            return false;        }    }    memoizedIsInitialized = 1;    return true;}
0
public void writeTo(com.google.protobuf.CodedOutputStream output) throws java.io.IOException
{    getSerializedSize();    if (((bitField0_ & 0x00000001) == 0x00000001)) {        output.writeSFixed32(1, version_);    }    if (((bitField0_ & 0x00000002) == 0x00000002)) {        output.writeSFixed32(2, logFileID_);    }    if (((bitField0_ & 0x00000004) == 0x00000004)) {        output.writeSFixed64(3, checkpointPosition_);    }    if (((bitField0_ & 0x00000008) == 0x00000008)) {        output.writeSFixed64(4, checkpointWriteOrderID_);    }    if (((bitField0_ & 0x00000010) == 0x00000010)) {        output.writeMessage(5, encryption_);    }    if (((bitField0_ & 0x00000020) == 0x00000020)) {        output.writeSFixed64(6, backupCheckpointPosition_);    }    if (((bitField0_ & 0x00000040) == 0x00000040)) {        output.writeSFixed64(7, backupCheckpointWriteOrderID_);    }    getUnknownFields().writeTo(output);}
0
public int getSerializedSize()
{    int size = memoizedSerializedSize;    if (size != -1)        return size;    size = 0;    if (((bitField0_ & 0x00000001) == 0x00000001)) {        size += com.google.protobuf.CodedOutputStream.computeSFixed32Size(1, version_);    }    if (((bitField0_ & 0x00000002) == 0x00000002)) {        size += com.google.protobuf.CodedOutputStream.computeSFixed32Size(2, logFileID_);    }    if (((bitField0_ & 0x00000004) == 0x00000004)) {        size += com.google.protobuf.CodedOutputStream.computeSFixed64Size(3, checkpointPosition_);    }    if (((bitField0_ & 0x00000008) == 0x00000008)) {        size += com.google.protobuf.CodedOutputStream.computeSFixed64Size(4, checkpointWriteOrderID_);    }    if (((bitField0_ & 0x00000010) == 0x00000010)) {        size += com.google.protobuf.CodedOutputStream.computeMessageSize(5, encryption_);    }    if (((bitField0_ & 0x00000020) == 0x00000020)) {        size += com.google.protobuf.CodedOutputStream.computeSFixed64Size(6, backupCheckpointPosition_);    }    if (((bitField0_ & 0x00000040) == 0x00000040)) {        size += com.google.protobuf.CodedOutputStream.computeSFixed64Size(7, backupCheckpointWriteOrderID_);    }    size += getUnknownFields().getSerializedSize();    memoizedSerializedSize = size;    return size;}
0
protected java.lang.Object writeReplace() throws java.io.ObjectStreamException
{    return super.writeReplace();}
0
public static org.apache.flume.channel.file.proto.ProtosFactory.LogFileMetaData parseFrom(com.google.protobuf.ByteString data) throws com.google.protobuf.InvalidProtocolBufferException
{    return PARSER.parseFrom(data);}
0
public static org.apache.flume.channel.file.proto.ProtosFactory.LogFileMetaData parseFrom(com.google.protobuf.ByteString data, com.google.protobuf.ExtensionRegistryLite extensionRegistry) throws com.google.protobuf.InvalidProtocolBufferException
{    return PARSER.parseFrom(data, extensionRegistry);}
0
public static org.apache.flume.channel.file.proto.ProtosFactory.LogFileMetaData parseFrom(byte[] data) throws com.google.protobuf.InvalidProtocolBufferException
{    return PARSER.parseFrom(data);}
0
public static org.apache.flume.channel.file.proto.ProtosFactory.LogFileMetaData parseFrom(byte[] data, com.google.protobuf.ExtensionRegistryLite extensionRegistry) throws com.google.protobuf.InvalidProtocolBufferException
{    return PARSER.parseFrom(data, extensionRegistry);}
0
public static org.apache.flume.channel.file.proto.ProtosFactory.LogFileMetaData parseFrom(java.io.InputStream input) throws java.io.IOException
{    return PARSER.parseFrom(input);}
0
public static org.apache.flume.channel.file.proto.ProtosFactory.LogFileMetaData parseFrom(java.io.InputStream input, com.google.protobuf.ExtensionRegistryLite extensionRegistry) throws java.io.IOException
{    return PARSER.parseFrom(input, extensionRegistry);}
0
public static org.apache.flume.channel.file.proto.ProtosFactory.LogFileMetaData parseDelimitedFrom(java.io.InputStream input) throws java.io.IOException
{    return PARSER.parseDelimitedFrom(input);}
0
public static org.apache.flume.channel.file.proto.ProtosFactory.LogFileMetaData parseDelimitedFrom(java.io.InputStream input, com.google.protobuf.ExtensionRegistryLite extensionRegistry) throws java.io.IOException
{    return PARSER.parseDelimitedFrom(input, extensionRegistry);}
0
public static org.apache.flume.channel.file.proto.ProtosFactory.LogFileMetaData parseFrom(com.google.protobuf.CodedInputStream input) throws java.io.IOException
{    return PARSER.parseFrom(input);}
0
public static org.apache.flume.channel.file.proto.ProtosFactory.LogFileMetaData parseFrom(com.google.protobuf.CodedInputStream input, com.google.protobuf.ExtensionRegistryLite extensionRegistry) throws java.io.IOException
{    return PARSER.parseFrom(input, extensionRegistry);}
0
public static Builder newBuilder()
{    return Builder.create();}
0
public Builder newBuilderForType()
{    return newBuilder();}
0
public static Builder newBuilder(org.apache.flume.channel.file.proto.ProtosFactory.LogFileMetaData prototype)
{    return newBuilder().mergeFrom(prototype);}
0
public Builder toBuilder()
{    return newBuilder(this);}
0
protected Builder newBuilderForType(com.google.protobuf.GeneratedMessage.BuilderParent parent)
{    Builder builder = new Builder(parent);    return builder;}
0
public static final com.google.protobuf.Descriptors.Descriptor getDescriptor()
{    return org.apache.flume.channel.file.proto.ProtosFactory.internal_static_LogFileMetaData_descriptor;}
0
protected com.google.protobuf.GeneratedMessage.FieldAccessorTable internalGetFieldAccessorTable()
{    return org.apache.flume.channel.file.proto.ProtosFactory.internal_static_LogFileMetaData_fieldAccessorTable.ensureFieldAccessorsInitialized(org.apache.flume.channel.file.proto.ProtosFactory.LogFileMetaData.class, org.apache.flume.channel.file.proto.ProtosFactory.LogFileMetaData.Builder.class);}
0
private void maybeForceBuilderInitialization()
{    if (com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders) {        getEncryptionFieldBuilder();    }}
0
private static Builder create()
{    return new Builder();}
0
public Builder clear()
{    super.clear();    version_ = 0;    bitField0_ = (bitField0_ & ~0x00000001);    logFileID_ = 0;    bitField0_ = (bitField0_ & ~0x00000002);    checkpointPosition_ = 0L;    bitField0_ = (bitField0_ & ~0x00000004);    checkpointWriteOrderID_ = 0L;    bitField0_ = (bitField0_ & ~0x00000008);    if (encryptionBuilder_ == null) {        encryption_ = org.apache.flume.channel.file.proto.ProtosFactory.LogFileEncryption.getDefaultInstance();    } else {        encryptionBuilder_.clear();    }    bitField0_ = (bitField0_ & ~0x00000010);    backupCheckpointPosition_ = 0L;    bitField0_ = (bitField0_ & ~0x00000020);    backupCheckpointWriteOrderID_ = 0L;    bitField0_ = (bitField0_ & ~0x00000040);    return this;}
0
public Builder clone()
{    return create().mergeFrom(buildPartial());}
0
public com.google.protobuf.Descriptors.Descriptor getDescriptorForType()
{    return org.apache.flume.channel.file.proto.ProtosFactory.internal_static_LogFileMetaData_descriptor;}
0
public org.apache.flume.channel.file.proto.ProtosFactory.LogFileMetaData getDefaultInstanceForType()
{    return org.apache.flume.channel.file.proto.ProtosFactory.LogFileMetaData.getDefaultInstance();}
0
public org.apache.flume.channel.file.proto.ProtosFactory.LogFileMetaData build()
{    org.apache.flume.channel.file.proto.ProtosFactory.LogFileMetaData result = buildPartial();    if (!result.isInitialized()) {        throw newUninitializedMessageException(result);    }    return result;}
0
public org.apache.flume.channel.file.proto.ProtosFactory.LogFileMetaData buildPartial()
{    org.apache.flume.channel.file.proto.ProtosFactory.LogFileMetaData result = new org.apache.flume.channel.file.proto.ProtosFactory.LogFileMetaData(this);    int from_bitField0_ = bitField0_;    int to_bitField0_ = 0;    if (((from_bitField0_ & 0x00000001) == 0x00000001)) {        to_bitField0_ |= 0x00000001;    }    result.version_ = version_;    if (((from_bitField0_ & 0x00000002) == 0x00000002)) {        to_bitField0_ |= 0x00000002;    }    result.logFileID_ = logFileID_;    if (((from_bitField0_ & 0x00000004) == 0x00000004)) {        to_bitField0_ |= 0x00000004;    }    result.checkpointPosition_ = checkpointPosition_;    if (((from_bitField0_ & 0x00000008) == 0x00000008)) {        to_bitField0_ |= 0x00000008;    }    result.checkpointWriteOrderID_ = checkpointWriteOrderID_;    if (((from_bitField0_ & 0x00000010) == 0x00000010)) {        to_bitField0_ |= 0x00000010;    }    if (encryptionBuilder_ == null) {        result.encryption_ = encryption_;    } else {        result.encryption_ = encryptionBuilder_.build();    }    if (((from_bitField0_ & 0x00000020) == 0x00000020)) {        to_bitField0_ |= 0x00000020;    }    result.backupCheckpointPosition_ = backupCheckpointPosition_;    if (((from_bitField0_ & 0x00000040) == 0x00000040)) {        to_bitField0_ |= 0x00000040;    }    result.backupCheckpointWriteOrderID_ = backupCheckpointWriteOrderID_;    result.bitField0_ = to_bitField0_;    onBuilt();    return result;}
0
public Builder mergeFrom(com.google.protobuf.Message other)
{    if (other instanceof org.apache.flume.channel.file.proto.ProtosFactory.LogFileMetaData) {        return mergeFrom((org.apache.flume.channel.file.proto.ProtosFactory.LogFileMetaData) other);    } else {        super.mergeFrom(other);        return this;    }}
0
public Builder mergeFrom(org.apache.flume.channel.file.proto.ProtosFactory.LogFileMetaData other)
{    if (other == org.apache.flume.channel.file.proto.ProtosFactory.LogFileMetaData.getDefaultInstance())        return this;    if (other.hasVersion()) {        setVersion(other.getVersion());    }    if (other.hasLogFileID()) {        setLogFileID(other.getLogFileID());    }    if (other.hasCheckpointPosition()) {        setCheckpointPosition(other.getCheckpointPosition());    }    if (other.hasCheckpointWriteOrderID()) {        setCheckpointWriteOrderID(other.getCheckpointWriteOrderID());    }    if (other.hasEncryption()) {        mergeEncryption(other.getEncryption());    }    if (other.hasBackupCheckpointPosition()) {        setBackupCheckpointPosition(other.getBackupCheckpointPosition());    }    if (other.hasBackupCheckpointWriteOrderID()) {        setBackupCheckpointWriteOrderID(other.getBackupCheckpointWriteOrderID());    }    this.mergeUnknownFields(other.getUnknownFields());    return this;}
0
public final boolean isInitialized()
{    if (!hasVersion()) {        return false;    }    if (!hasLogFileID()) {        return false;    }    if (!hasCheckpointPosition()) {        return false;    }    if (!hasCheckpointWriteOrderID()) {        return false;    }    if (hasEncryption()) {        if (!getEncryption().isInitialized()) {            return false;        }    }    return true;}
0
public Builder mergeFrom(com.google.protobuf.CodedInputStream input, com.google.protobuf.ExtensionRegistryLite extensionRegistry) throws java.io.IOException
{    org.apache.flume.channel.file.proto.ProtosFactory.LogFileMetaData parsedMessage = null;    try {        parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);    } catch (com.google.protobuf.InvalidProtocolBufferException e) {        parsedMessage = (org.apache.flume.channel.file.proto.ProtosFactory.LogFileMetaData) e.getUnfinishedMessage();        throw e;    } finally {        if (parsedMessage != null) {            mergeFrom(parsedMessage);        }    }    return this;}
0
public boolean hasVersion()
{    return ((bitField0_ & 0x00000001) == 0x00000001);}
0
public int getVersion()
{    return version_;}
0
public Builder setVersion(int value)
{    bitField0_ |= 0x00000001;    version_ = value;    onChanged();    return this;}
0
public Builder clearVersion()
{    bitField0_ = (bitField0_ & ~0x00000001);    version_ = 0;    onChanged();    return this;}
0
public boolean hasLogFileID()
{    return ((bitField0_ & 0x00000002) == 0x00000002);}
0
public int getLogFileID()
{    return logFileID_;}
0
public Builder setLogFileID(int value)
{    bitField0_ |= 0x00000002;    logFileID_ = value;    onChanged();    return this;}
0
public Builder clearLogFileID()
{    bitField0_ = (bitField0_ & ~0x00000002);    logFileID_ = 0;    onChanged();    return this;}
0
public boolean hasCheckpointPosition()
{    return ((bitField0_ & 0x00000004) == 0x00000004);}
0
public long getCheckpointPosition()
{    return checkpointPosition_;}
0
public Builder setCheckpointPosition(long value)
{    bitField0_ |= 0x00000004;    checkpointPosition_ = value;    onChanged();    return this;}
0
public Builder clearCheckpointPosition()
{    bitField0_ = (bitField0_ & ~0x00000004);    checkpointPosition_ = 0L;    onChanged();    return this;}
0
public boolean hasCheckpointWriteOrderID()
{    return ((bitField0_ & 0x00000008) == 0x00000008);}
0
public long getCheckpointWriteOrderID()
{    return checkpointWriteOrderID_;}
0
public Builder setCheckpointWriteOrderID(long value)
{    bitField0_ |= 0x00000008;    checkpointWriteOrderID_ = value;    onChanged();    return this;}
0
public Builder clearCheckpointWriteOrderID()
{    bitField0_ = (bitField0_ & ~0x00000008);    checkpointWriteOrderID_ = 0L;    onChanged();    return this;}
0
public boolean hasEncryption()
{    return ((bitField0_ & 0x00000010) == 0x00000010);}
0
public org.apache.flume.channel.file.proto.ProtosFactory.LogFileEncryption getEncryption()
{    if (encryptionBuilder_ == null) {        return encryption_;    } else {        return encryptionBuilder_.getMessage();    }}
0
public Builder setEncryption(org.apache.flume.channel.file.proto.ProtosFactory.LogFileEncryption value)
{    if (encryptionBuilder_ == null) {        if (value == null) {            throw new NullPointerException();        }        encryption_ = value;        onChanged();    } else {        encryptionBuilder_.setMessage(value);    }    bitField0_ |= 0x00000010;    return this;}
0
public Builder setEncryption(org.apache.flume.channel.file.proto.ProtosFactory.LogFileEncryption.Builder builderForValue)
{    if (encryptionBuilder_ == null) {        encryption_ = builderForValue.build();        onChanged();    } else {        encryptionBuilder_.setMessage(builderForValue.build());    }    bitField0_ |= 0x00000010;    return this;}
0
public Builder mergeEncryption(org.apache.flume.channel.file.proto.ProtosFactory.LogFileEncryption value)
{    if (encryptionBuilder_ == null) {        if (((bitField0_ & 0x00000010) == 0x00000010) && encryption_ != org.apache.flume.channel.file.proto.ProtosFactory.LogFileEncryption.getDefaultInstance()) {            encryption_ = org.apache.flume.channel.file.proto.ProtosFactory.LogFileEncryption.newBuilder(encryption_).mergeFrom(value).buildPartial();        } else {            encryption_ = value;        }        onChanged();    } else {        encryptionBuilder_.mergeFrom(value);    }    bitField0_ |= 0x00000010;    return this;}
0
public Builder clearEncryption()
{    if (encryptionBuilder_ == null) {        encryption_ = org.apache.flume.channel.file.proto.ProtosFactory.LogFileEncryption.getDefaultInstance();        onChanged();    } else {        encryptionBuilder_.clear();    }    bitField0_ = (bitField0_ & ~0x00000010);    return this;}
0
public org.apache.flume.channel.file.proto.ProtosFactory.LogFileEncryption.Builder getEncryptionBuilder()
{    bitField0_ |= 0x00000010;    onChanged();    return getEncryptionFieldBuilder().getBuilder();}
0
public org.apache.flume.channel.file.proto.ProtosFactory.LogFileEncryptionOrBuilder getEncryptionOrBuilder()
{    if (encryptionBuilder_ != null) {        return encryptionBuilder_.getMessageOrBuilder();    } else {        return encryption_;    }}
0
private com.google.protobuf.SingleFieldBuilder<org.apache.flume.channel.file.proto.ProtosFactory.LogFileEncryption, org.apache.flume.channel.file.proto.ProtosFactory.LogFileEncryption.Builder, org.apache.flume.channel.file.proto.ProtosFactory.LogFileEncryptionOrBuilder> getEncryptionFieldBuilder()
{    if (encryptionBuilder_ == null) {        encryptionBuilder_ = new com.google.protobuf.SingleFieldBuilder<org.apache.flume.channel.file.proto.ProtosFactory.LogFileEncryption, org.apache.flume.channel.file.proto.ProtosFactory.LogFileEncryption.Builder, org.apache.flume.channel.file.proto.ProtosFactory.LogFileEncryptionOrBuilder>(encryption_, getParentForChildren(), isClean());        encryption_ = null;    }    return encryptionBuilder_;}
0
public boolean hasBackupCheckpointPosition()
{    return ((bitField0_ & 0x00000020) == 0x00000020);}
0
public long getBackupCheckpointPosition()
{    return backupCheckpointPosition_;}
0
public Builder setBackupCheckpointPosition(long value)
{    bitField0_ |= 0x00000020;    backupCheckpointPosition_ = value;    onChanged();    return this;}
0
public Builder clearBackupCheckpointPosition()
{    bitField0_ = (bitField0_ & ~0x00000020);    backupCheckpointPosition_ = 0L;    onChanged();    return this;}
0
public boolean hasBackupCheckpointWriteOrderID()
{    return ((bitField0_ & 0x00000040) == 0x00000040);}
0
public long getBackupCheckpointWriteOrderID()
{    return backupCheckpointWriteOrderID_;}
0
public Builder setBackupCheckpointWriteOrderID(long value)
{    bitField0_ |= 0x00000040;    backupCheckpointWriteOrderID_ = value;    onChanged();    return this;}
0
public Builder clearBackupCheckpointWriteOrderID()
{    bitField0_ = (bitField0_ & ~0x00000040);    backupCheckpointWriteOrderID_ = 0L;    onChanged();    return this;}
0
public static LogFileEncryption getDefaultInstance()
{    return defaultInstance;}
0
public LogFileEncryption getDefaultInstanceForType()
{    return defaultInstance;}
0
public final com.google.protobuf.UnknownFieldSet getUnknownFields()
{    return this.unknownFields;}
0
public static final com.google.protobuf.Descriptors.Descriptor getDescriptor()
{    return org.apache.flume.channel.file.proto.ProtosFactory.internal_static_LogFileEncryption_descriptor;}
0
protected com.google.protobuf.GeneratedMessage.FieldAccessorTable internalGetFieldAccessorTable()
{    return org.apache.flume.channel.file.proto.ProtosFactory.internal_static_LogFileEncryption_fieldAccessorTable.ensureFieldAccessorsInitialized(org.apache.flume.channel.file.proto.ProtosFactory.LogFileEncryption.class, org.apache.flume.channel.file.proto.ProtosFactory.LogFileEncryption.Builder.class);}
0
public LogFileEncryption parsePartialFrom(com.google.protobuf.CodedInputStream input, com.google.protobuf.ExtensionRegistryLite extensionRegistry) throws com.google.protobuf.InvalidProtocolBufferException
{    return new LogFileEncryption(input, extensionRegistry);}
0
public com.google.protobuf.Parser<LogFileEncryption> getParserForType()
{    return PARSER;}
0
public boolean hasCipherProvider()
{    return ((bitField0_ & 0x00000001) == 0x00000001);}
0
public java.lang.String getCipherProvider()
{    java.lang.Object ref = cipherProvider_;    if (ref instanceof java.lang.String) {        return (java.lang.String) ref;    } else {        com.google.protobuf.ByteString bs = (com.google.protobuf.ByteString) ref;        java.lang.String s = bs.toStringUtf8();        if (bs.isValidUtf8()) {            cipherProvider_ = s;        }        return s;    }}
0
public com.google.protobuf.ByteString getCipherProviderBytes()
{    java.lang.Object ref = cipherProvider_;    if (ref instanceof java.lang.String) {        com.google.protobuf.ByteString b = com.google.protobuf.ByteString.copyFromUtf8((java.lang.String) ref);        cipherProvider_ = b;        return b;    } else {        return (com.google.protobuf.ByteString) ref;    }}
0
public boolean hasKeyAlias()
{    return ((bitField0_ & 0x00000002) == 0x00000002);}
0
public java.lang.String getKeyAlias()
{    java.lang.Object ref = keyAlias_;    if (ref instanceof java.lang.String) {        return (java.lang.String) ref;    } else {        com.google.protobuf.ByteString bs = (com.google.protobuf.ByteString) ref;        java.lang.String s = bs.toStringUtf8();        if (bs.isValidUtf8()) {            keyAlias_ = s;        }        return s;    }}
0
public com.google.protobuf.ByteString getKeyAliasBytes()
{    java.lang.Object ref = keyAlias_;    if (ref instanceof java.lang.String) {        com.google.protobuf.ByteString b = com.google.protobuf.ByteString.copyFromUtf8((java.lang.String) ref);        keyAlias_ = b;        return b;    } else {        return (com.google.protobuf.ByteString) ref;    }}
0
public boolean hasParameters()
{    return ((bitField0_ & 0x00000004) == 0x00000004);}
0
public com.google.protobuf.ByteString getParameters()
{    return parameters_;}
0
private void initFields()
{    cipherProvider_ = "";    keyAlias_ = "";    parameters_ = com.google.protobuf.ByteString.EMPTY;}
0
public final boolean isInitialized()
{    byte isInitialized = memoizedIsInitialized;    if (isInitialized != -1)        return isInitialized == 1;    if (!hasCipherProvider()) {        memoizedIsInitialized = 0;        return false;    }    if (!hasKeyAlias()) {        memoizedIsInitialized = 0;        return false;    }    memoizedIsInitialized = 1;    return true;}
0
public void writeTo(com.google.protobuf.CodedOutputStream output) throws java.io.IOException
{    getSerializedSize();    if (((bitField0_ & 0x00000001) == 0x00000001)) {        output.writeBytes(1, getCipherProviderBytes());    }    if (((bitField0_ & 0x00000002) == 0x00000002)) {        output.writeBytes(2, getKeyAliasBytes());    }    if (((bitField0_ & 0x00000004) == 0x00000004)) {        output.writeBytes(3, parameters_);    }    getUnknownFields().writeTo(output);}
0
public int getSerializedSize()
{    int size = memoizedSerializedSize;    if (size != -1)        return size;    size = 0;    if (((bitField0_ & 0x00000001) == 0x00000001)) {        size += com.google.protobuf.CodedOutputStream.computeBytesSize(1, getCipherProviderBytes());    }    if (((bitField0_ & 0x00000002) == 0x00000002)) {        size += com.google.protobuf.CodedOutputStream.computeBytesSize(2, getKeyAliasBytes());    }    if (((bitField0_ & 0x00000004) == 0x00000004)) {        size += com.google.protobuf.CodedOutputStream.computeBytesSize(3, parameters_);    }    size += getUnknownFields().getSerializedSize();    memoizedSerializedSize = size;    return size;}
0
protected java.lang.Object writeReplace() throws java.io.ObjectStreamException
{    return super.writeReplace();}
0
public static org.apache.flume.channel.file.proto.ProtosFactory.LogFileEncryption parseFrom(com.google.protobuf.ByteString data) throws com.google.protobuf.InvalidProtocolBufferException
{    return PARSER.parseFrom(data);}
0
public static org.apache.flume.channel.file.proto.ProtosFactory.LogFileEncryption parseFrom(com.google.protobuf.ByteString data, com.google.protobuf.ExtensionRegistryLite extensionRegistry) throws com.google.protobuf.InvalidProtocolBufferException
{    return PARSER.parseFrom(data, extensionRegistry);}
0
public static org.apache.flume.channel.file.proto.ProtosFactory.LogFileEncryption parseFrom(byte[] data) throws com.google.protobuf.InvalidProtocolBufferException
{    return PARSER.parseFrom(data);}
0
public static org.apache.flume.channel.file.proto.ProtosFactory.LogFileEncryption parseFrom(byte[] data, com.google.protobuf.ExtensionRegistryLite extensionRegistry) throws com.google.protobuf.InvalidProtocolBufferException
{    return PARSER.parseFrom(data, extensionRegistry);}
0
public static org.apache.flume.channel.file.proto.ProtosFactory.LogFileEncryption parseFrom(java.io.InputStream input) throws java.io.IOException
{    return PARSER.parseFrom(input);}
0
public static org.apache.flume.channel.file.proto.ProtosFactory.LogFileEncryption parseFrom(java.io.InputStream input, com.google.protobuf.ExtensionRegistryLite extensionRegistry) throws java.io.IOException
{    return PARSER.parseFrom(input, extensionRegistry);}
0
public static org.apache.flume.channel.file.proto.ProtosFactory.LogFileEncryption parseDelimitedFrom(java.io.InputStream input) throws java.io.IOException
{    return PARSER.parseDelimitedFrom(input);}
0
public static org.apache.flume.channel.file.proto.ProtosFactory.LogFileEncryption parseDelimitedFrom(java.io.InputStream input, com.google.protobuf.ExtensionRegistryLite extensionRegistry) throws java.io.IOException
{    return PARSER.parseDelimitedFrom(input, extensionRegistry);}
0
public static org.apache.flume.channel.file.proto.ProtosFactory.LogFileEncryption parseFrom(com.google.protobuf.CodedInputStream input) throws java.io.IOException
{    return PARSER.parseFrom(input);}
0
public static org.apache.flume.channel.file.proto.ProtosFactory.LogFileEncryption parseFrom(com.google.protobuf.CodedInputStream input, com.google.protobuf.ExtensionRegistryLite extensionRegistry) throws java.io.IOException
{    return PARSER.parseFrom(input, extensionRegistry);}
0
public static Builder newBuilder()
{    return Builder.create();}
0
public Builder newBuilderForType()
{    return newBuilder();}
0
public static Builder newBuilder(org.apache.flume.channel.file.proto.ProtosFactory.LogFileEncryption prototype)
{    return newBuilder().mergeFrom(prototype);}
0
public Builder toBuilder()
{    return newBuilder(this);}
0
protected Builder newBuilderForType(com.google.protobuf.GeneratedMessage.BuilderParent parent)
{    Builder builder = new Builder(parent);    return builder;}
0
public static final com.google.protobuf.Descriptors.Descriptor getDescriptor()
{    return org.apache.flume.channel.file.proto.ProtosFactory.internal_static_LogFileEncryption_descriptor;}
0
protected com.google.protobuf.GeneratedMessage.FieldAccessorTable internalGetFieldAccessorTable()
{    return org.apache.flume.channel.file.proto.ProtosFactory.internal_static_LogFileEncryption_fieldAccessorTable.ensureFieldAccessorsInitialized(org.apache.flume.channel.file.proto.ProtosFactory.LogFileEncryption.class, org.apache.flume.channel.file.proto.ProtosFactory.LogFileEncryption.Builder.class);}
0
private void maybeForceBuilderInitialization()
{    if (com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders) {    }}
0
private static Builder create()
{    return new Builder();}
0
public Builder clear()
{    super.clear();    cipherProvider_ = "";    bitField0_ = (bitField0_ & ~0x00000001);    keyAlias_ = "";    bitField0_ = (bitField0_ & ~0x00000002);    parameters_ = com.google.protobuf.ByteString.EMPTY;    bitField0_ = (bitField0_ & ~0x00000004);    return this;}
0
public Builder clone()
{    return create().mergeFrom(buildPartial());}
0
public com.google.protobuf.Descriptors.Descriptor getDescriptorForType()
{    return org.apache.flume.channel.file.proto.ProtosFactory.internal_static_LogFileEncryption_descriptor;}
0
public org.apache.flume.channel.file.proto.ProtosFactory.LogFileEncryption getDefaultInstanceForType()
{    return org.apache.flume.channel.file.proto.ProtosFactory.LogFileEncryption.getDefaultInstance();}
0
public org.apache.flume.channel.file.proto.ProtosFactory.LogFileEncryption build()
{    org.apache.flume.channel.file.proto.ProtosFactory.LogFileEncryption result = buildPartial();    if (!result.isInitialized()) {        throw newUninitializedMessageException(result);    }    return result;}
0
public org.apache.flume.channel.file.proto.ProtosFactory.LogFileEncryption buildPartial()
{    org.apache.flume.channel.file.proto.ProtosFactory.LogFileEncryption result = new org.apache.flume.channel.file.proto.ProtosFactory.LogFileEncryption(this);    int from_bitField0_ = bitField0_;    int to_bitField0_ = 0;    if (((from_bitField0_ & 0x00000001) == 0x00000001)) {        to_bitField0_ |= 0x00000001;    }    result.cipherProvider_ = cipherProvider_;    if (((from_bitField0_ & 0x00000002) == 0x00000002)) {        to_bitField0_ |= 0x00000002;    }    result.keyAlias_ = keyAlias_;    if (((from_bitField0_ & 0x00000004) == 0x00000004)) {        to_bitField0_ |= 0x00000004;    }    result.parameters_ = parameters_;    result.bitField0_ = to_bitField0_;    onBuilt();    return result;}
0
public Builder mergeFrom(com.google.protobuf.Message other)
{    if (other instanceof org.apache.flume.channel.file.proto.ProtosFactory.LogFileEncryption) {        return mergeFrom((org.apache.flume.channel.file.proto.ProtosFactory.LogFileEncryption) other);    } else {        super.mergeFrom(other);        return this;    }}
0
public Builder mergeFrom(org.apache.flume.channel.file.proto.ProtosFactory.LogFileEncryption other)
{    if (other == org.apache.flume.channel.file.proto.ProtosFactory.LogFileEncryption.getDefaultInstance())        return this;    if (other.hasCipherProvider()) {        bitField0_ |= 0x00000001;        cipherProvider_ = other.cipherProvider_;        onChanged();    }    if (other.hasKeyAlias()) {        bitField0_ |= 0x00000002;        keyAlias_ = other.keyAlias_;        onChanged();    }    if (other.hasParameters()) {        setParameters(other.getParameters());    }    this.mergeUnknownFields(other.getUnknownFields());    return this;}
0
public final boolean isInitialized()
{    if (!hasCipherProvider()) {        return false;    }    if (!hasKeyAlias()) {        return false;    }    return true;}
0
public Builder mergeFrom(com.google.protobuf.CodedInputStream input, com.google.protobuf.ExtensionRegistryLite extensionRegistry) throws java.io.IOException
{    org.apache.flume.channel.file.proto.ProtosFactory.LogFileEncryption parsedMessage = null;    try {        parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);    } catch (com.google.protobuf.InvalidProtocolBufferException e) {        parsedMessage = (org.apache.flume.channel.file.proto.ProtosFactory.LogFileEncryption) e.getUnfinishedMessage();        throw e;    } finally {        if (parsedMessage != null) {            mergeFrom(parsedMessage);        }    }    return this;}
0
public boolean hasCipherProvider()
{    return ((bitField0_ & 0x00000001) == 0x00000001);}
0
public java.lang.String getCipherProvider()
{    java.lang.Object ref = cipherProvider_;    if (!(ref instanceof java.lang.String)) {        java.lang.String s = ((com.google.protobuf.ByteString) ref).toStringUtf8();        cipherProvider_ = s;        return s;    } else {        return (java.lang.String) ref;    }}
0
public com.google.protobuf.ByteString getCipherProviderBytes()
{    java.lang.Object ref = cipherProvider_;    if (ref instanceof String) {        com.google.protobuf.ByteString b = com.google.protobuf.ByteString.copyFromUtf8((java.lang.String) ref);        cipherProvider_ = b;        return b;    } else {        return (com.google.protobuf.ByteString) ref;    }}
0
public Builder setCipherProvider(java.lang.String value)
{    if (value == null) {        throw new NullPointerException();    }    bitField0_ |= 0x00000001;    cipherProvider_ = value;    onChanged();    return this;}
0
public Builder clearCipherProvider()
{    bitField0_ = (bitField0_ & ~0x00000001);    cipherProvider_ = getDefaultInstance().getCipherProvider();    onChanged();    return this;}
0
public Builder setCipherProviderBytes(com.google.protobuf.ByteString value)
{    if (value == null) {        throw new NullPointerException();    }    bitField0_ |= 0x00000001;    cipherProvider_ = value;    onChanged();    return this;}
0
public boolean hasKeyAlias()
{    return ((bitField0_ & 0x00000002) == 0x00000002);}
0
public java.lang.String getKeyAlias()
{    java.lang.Object ref = keyAlias_;    if (!(ref instanceof java.lang.String)) {        java.lang.String s = ((com.google.protobuf.ByteString) ref).toStringUtf8();        keyAlias_ = s;        return s;    } else {        return (java.lang.String) ref;    }}
0
public com.google.protobuf.ByteString getKeyAliasBytes()
{    java.lang.Object ref = keyAlias_;    if (ref instanceof String) {        com.google.protobuf.ByteString b = com.google.protobuf.ByteString.copyFromUtf8((java.lang.String) ref);        keyAlias_ = b;        return b;    } else {        return (com.google.protobuf.ByteString) ref;    }}
0
public Builder setKeyAlias(java.lang.String value)
{    if (value == null) {        throw new NullPointerException();    }    bitField0_ |= 0x00000002;    keyAlias_ = value;    onChanged();    return this;}
0
public Builder clearKeyAlias()
{    bitField0_ = (bitField0_ & ~0x00000002);    keyAlias_ = getDefaultInstance().getKeyAlias();    onChanged();    return this;}
0
public Builder setKeyAliasBytes(com.google.protobuf.ByteString value)
{    if (value == null) {        throw new NullPointerException();    }    bitField0_ |= 0x00000002;    keyAlias_ = value;    onChanged();    return this;}
0
public boolean hasParameters()
{    return ((bitField0_ & 0x00000004) == 0x00000004);}
0
public com.google.protobuf.ByteString getParameters()
{    return parameters_;}
0
public Builder setParameters(com.google.protobuf.ByteString value)
{    if (value == null) {        throw new NullPointerException();    }    bitField0_ |= 0x00000004;    parameters_ = value;    onChanged();    return this;}
0
public Builder clearParameters()
{    bitField0_ = (bitField0_ & ~0x00000004);    parameters_ = getDefaultInstance().getParameters();    onChanged();    return this;}
0
public static TransactionEventHeader getDefaultInstance()
{    return defaultInstance;}
0
public TransactionEventHeader getDefaultInstanceForType()
{    return defaultInstance;}
0
public final com.google.protobuf.UnknownFieldSet getUnknownFields()
{    return this.unknownFields;}
0
public static final com.google.protobuf.Descriptors.Descriptor getDescriptor()
{    return org.apache.flume.channel.file.proto.ProtosFactory.internal_static_TransactionEventHeader_descriptor;}
0
protected com.google.protobuf.GeneratedMessage.FieldAccessorTable internalGetFieldAccessorTable()
{    return org.apache.flume.channel.file.proto.ProtosFactory.internal_static_TransactionEventHeader_fieldAccessorTable.ensureFieldAccessorsInitialized(org.apache.flume.channel.file.proto.ProtosFactory.TransactionEventHeader.class, org.apache.flume.channel.file.proto.ProtosFactory.TransactionEventHeader.Builder.class);}
0
public TransactionEventHeader parsePartialFrom(com.google.protobuf.CodedInputStream input, com.google.protobuf.ExtensionRegistryLite extensionRegistry) throws com.google.protobuf.InvalidProtocolBufferException
{    return new TransactionEventHeader(input, extensionRegistry);}
0
public com.google.protobuf.Parser<TransactionEventHeader> getParserForType()
{    return PARSER;}
0
public boolean hasType()
{    return ((bitField0_ & 0x00000001) == 0x00000001);}
0
public int getType()
{    return type_;}
0
public boolean hasTransactionID()
{    return ((bitField0_ & 0x00000002) == 0x00000002);}
0
public long getTransactionID()
{    return transactionID_;}
0
public boolean hasWriteOrderID()
{    return ((bitField0_ & 0x00000004) == 0x00000004);}
0
public long getWriteOrderID()
{    return writeOrderID_;}
0
private void initFields()
{    type_ = 0;    transactionID_ = 0L;    writeOrderID_ = 0L;}
0
public final boolean isInitialized()
{    byte isInitialized = memoizedIsInitialized;    if (isInitialized != -1)        return isInitialized == 1;    if (!hasType()) {        memoizedIsInitialized = 0;        return false;    }    if (!hasTransactionID()) {        memoizedIsInitialized = 0;        return false;    }    if (!hasWriteOrderID()) {        memoizedIsInitialized = 0;        return false;    }    memoizedIsInitialized = 1;    return true;}
0
public void writeTo(com.google.protobuf.CodedOutputStream output) throws java.io.IOException
{    getSerializedSize();    if (((bitField0_ & 0x00000001) == 0x00000001)) {        output.writeSFixed32(1, type_);    }    if (((bitField0_ & 0x00000002) == 0x00000002)) {        output.writeSFixed64(2, transactionID_);    }    if (((bitField0_ & 0x00000004) == 0x00000004)) {        output.writeSFixed64(3, writeOrderID_);    }    getUnknownFields().writeTo(output);}
0
public int getSerializedSize()
{    int size = memoizedSerializedSize;    if (size != -1)        return size;    size = 0;    if (((bitField0_ & 0x00000001) == 0x00000001)) {        size += com.google.protobuf.CodedOutputStream.computeSFixed32Size(1, type_);    }    if (((bitField0_ & 0x00000002) == 0x00000002)) {        size += com.google.protobuf.CodedOutputStream.computeSFixed64Size(2, transactionID_);    }    if (((bitField0_ & 0x00000004) == 0x00000004)) {        size += com.google.protobuf.CodedOutputStream.computeSFixed64Size(3, writeOrderID_);    }    size += getUnknownFields().getSerializedSize();    memoizedSerializedSize = size;    return size;}
0
protected java.lang.Object writeReplace() throws java.io.ObjectStreamException
{    return super.writeReplace();}
0
public static org.apache.flume.channel.file.proto.ProtosFactory.TransactionEventHeader parseFrom(com.google.protobuf.ByteString data) throws com.google.protobuf.InvalidProtocolBufferException
{    return PARSER.parseFrom(data);}
0
public static org.apache.flume.channel.file.proto.ProtosFactory.TransactionEventHeader parseFrom(com.google.protobuf.ByteString data, com.google.protobuf.ExtensionRegistryLite extensionRegistry) throws com.google.protobuf.InvalidProtocolBufferException
{    return PARSER.parseFrom(data, extensionRegistry);}
0
public static org.apache.flume.channel.file.proto.ProtosFactory.TransactionEventHeader parseFrom(byte[] data) throws com.google.protobuf.InvalidProtocolBufferException
{    return PARSER.parseFrom(data);}
0
public static org.apache.flume.channel.file.proto.ProtosFactory.TransactionEventHeader parseFrom(byte[] data, com.google.protobuf.ExtensionRegistryLite extensionRegistry) throws com.google.protobuf.InvalidProtocolBufferException
{    return PARSER.parseFrom(data, extensionRegistry);}
0
public static org.apache.flume.channel.file.proto.ProtosFactory.TransactionEventHeader parseFrom(java.io.InputStream input) throws java.io.IOException
{    return PARSER.parseFrom(input);}
0
public static org.apache.flume.channel.file.proto.ProtosFactory.TransactionEventHeader parseFrom(java.io.InputStream input, com.google.protobuf.ExtensionRegistryLite extensionRegistry) throws java.io.IOException
{    return PARSER.parseFrom(input, extensionRegistry);}
0
public static org.apache.flume.channel.file.proto.ProtosFactory.TransactionEventHeader parseDelimitedFrom(java.io.InputStream input) throws java.io.IOException
{    return PARSER.parseDelimitedFrom(input);}
0
public static org.apache.flume.channel.file.proto.ProtosFactory.TransactionEventHeader parseDelimitedFrom(java.io.InputStream input, com.google.protobuf.ExtensionRegistryLite extensionRegistry) throws java.io.IOException
{    return PARSER.parseDelimitedFrom(input, extensionRegistry);}
0
public static org.apache.flume.channel.file.proto.ProtosFactory.TransactionEventHeader parseFrom(com.google.protobuf.CodedInputStream input) throws java.io.IOException
{    return PARSER.parseFrom(input);}
0
public static org.apache.flume.channel.file.proto.ProtosFactory.TransactionEventHeader parseFrom(com.google.protobuf.CodedInputStream input, com.google.protobuf.ExtensionRegistryLite extensionRegistry) throws java.io.IOException
{    return PARSER.parseFrom(input, extensionRegistry);}
0
public static Builder newBuilder()
{    return Builder.create();}
0
public Builder newBuilderForType()
{    return newBuilder();}
0
public static Builder newBuilder(org.apache.flume.channel.file.proto.ProtosFactory.TransactionEventHeader prototype)
{    return newBuilder().mergeFrom(prototype);}
0
public Builder toBuilder()
{    return newBuilder(this);}
0
protected Builder newBuilderForType(com.google.protobuf.GeneratedMessage.BuilderParent parent)
{    Builder builder = new Builder(parent);    return builder;}
0
public static final com.google.protobuf.Descriptors.Descriptor getDescriptor()
{    return org.apache.flume.channel.file.proto.ProtosFactory.internal_static_TransactionEventHeader_descriptor;}
0
protected com.google.protobuf.GeneratedMessage.FieldAccessorTable internalGetFieldAccessorTable()
{    return org.apache.flume.channel.file.proto.ProtosFactory.internal_static_TransactionEventHeader_fieldAccessorTable.ensureFieldAccessorsInitialized(org.apache.flume.channel.file.proto.ProtosFactory.TransactionEventHeader.class, org.apache.flume.channel.file.proto.ProtosFactory.TransactionEventHeader.Builder.class);}
0
private void maybeForceBuilderInitialization()
{    if (com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders) {    }}
0
private static Builder create()
{    return new Builder();}
0
public Builder clear()
{    super.clear();    type_ = 0;    bitField0_ = (bitField0_ & ~0x00000001);    transactionID_ = 0L;    bitField0_ = (bitField0_ & ~0x00000002);    writeOrderID_ = 0L;    bitField0_ = (bitField0_ & ~0x00000004);    return this;}
0
public Builder clone()
{    return create().mergeFrom(buildPartial());}
0
public com.google.protobuf.Descriptors.Descriptor getDescriptorForType()
{    return org.apache.flume.channel.file.proto.ProtosFactory.internal_static_TransactionEventHeader_descriptor;}
0
public org.apache.flume.channel.file.proto.ProtosFactory.TransactionEventHeader getDefaultInstanceForType()
{    return org.apache.flume.channel.file.proto.ProtosFactory.TransactionEventHeader.getDefaultInstance();}
0
public org.apache.flume.channel.file.proto.ProtosFactory.TransactionEventHeader build()
{    org.apache.flume.channel.file.proto.ProtosFactory.TransactionEventHeader result = buildPartial();    if (!result.isInitialized()) {        throw newUninitializedMessageException(result);    }    return result;}
0
public org.apache.flume.channel.file.proto.ProtosFactory.TransactionEventHeader buildPartial()
{    org.apache.flume.channel.file.proto.ProtosFactory.TransactionEventHeader result = new org.apache.flume.channel.file.proto.ProtosFactory.TransactionEventHeader(this);    int from_bitField0_ = bitField0_;    int to_bitField0_ = 0;    if (((from_bitField0_ & 0x00000001) == 0x00000001)) {        to_bitField0_ |= 0x00000001;    }    result.type_ = type_;    if (((from_bitField0_ & 0x00000002) == 0x00000002)) {        to_bitField0_ |= 0x00000002;    }    result.transactionID_ = transactionID_;    if (((from_bitField0_ & 0x00000004) == 0x00000004)) {        to_bitField0_ |= 0x00000004;    }    result.writeOrderID_ = writeOrderID_;    result.bitField0_ = to_bitField0_;    onBuilt();    return result;}
0
public Builder mergeFrom(com.google.protobuf.Message other)
{    if (other instanceof org.apache.flume.channel.file.proto.ProtosFactory.TransactionEventHeader) {        return mergeFrom((org.apache.flume.channel.file.proto.ProtosFactory.TransactionEventHeader) other);    } else {        super.mergeFrom(other);        return this;    }}
0
public Builder mergeFrom(org.apache.flume.channel.file.proto.ProtosFactory.TransactionEventHeader other)
{    if (other == org.apache.flume.channel.file.proto.ProtosFactory.TransactionEventHeader.getDefaultInstance())        return this;    if (other.hasType()) {        setType(other.getType());    }    if (other.hasTransactionID()) {        setTransactionID(other.getTransactionID());    }    if (other.hasWriteOrderID()) {        setWriteOrderID(other.getWriteOrderID());    }    this.mergeUnknownFields(other.getUnknownFields());    return this;}
0
public final boolean isInitialized()
{    if (!hasType()) {        return false;    }    if (!hasTransactionID()) {        return false;    }    if (!hasWriteOrderID()) {        return false;    }    return true;}
0
public Builder mergeFrom(com.google.protobuf.CodedInputStream input, com.google.protobuf.ExtensionRegistryLite extensionRegistry) throws java.io.IOException
{    org.apache.flume.channel.file.proto.ProtosFactory.TransactionEventHeader parsedMessage = null;    try {        parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);    } catch (com.google.protobuf.InvalidProtocolBufferException e) {        parsedMessage = (org.apache.flume.channel.file.proto.ProtosFactory.TransactionEventHeader) e.getUnfinishedMessage();        throw e;    } finally {        if (parsedMessage != null) {            mergeFrom(parsedMessage);        }    }    return this;}
0
public boolean hasType()
{    return ((bitField0_ & 0x00000001) == 0x00000001);}
0
public int getType()
{    return type_;}
0
public Builder setType(int value)
{    bitField0_ |= 0x00000001;    type_ = value;    onChanged();    return this;}
0
public Builder clearType()
{    bitField0_ = (bitField0_ & ~0x00000001);    type_ = 0;    onChanged();    return this;}
0
public boolean hasTransactionID()
{    return ((bitField0_ & 0x00000002) == 0x00000002);}
0
public long getTransactionID()
{    return transactionID_;}
0
public Builder setTransactionID(long value)
{    bitField0_ |= 0x00000002;    transactionID_ = value;    onChanged();    return this;}
0
public Builder clearTransactionID()
{    bitField0_ = (bitField0_ & ~0x00000002);    transactionID_ = 0L;    onChanged();    return this;}
0
public boolean hasWriteOrderID()
{    return ((bitField0_ & 0x00000004) == 0x00000004);}
0
public long getWriteOrderID()
{    return writeOrderID_;}
0
public Builder setWriteOrderID(long value)
{    bitField0_ |= 0x00000004;    writeOrderID_ = value;    onChanged();    return this;}
0
public Builder clearWriteOrderID()
{    bitField0_ = (bitField0_ & ~0x00000004);    writeOrderID_ = 0L;    onChanged();    return this;}
0
public static Put getDefaultInstance()
{    return defaultInstance;}
0
public Put getDefaultInstanceForType()
{    return defaultInstance;}
0
public final com.google.protobuf.UnknownFieldSet getUnknownFields()
{    return this.unknownFields;}
0
public static final com.google.protobuf.Descriptors.Descriptor getDescriptor()
{    return org.apache.flume.channel.file.proto.ProtosFactory.internal_static_Put_descriptor;}
0
protected com.google.protobuf.GeneratedMessage.FieldAccessorTable internalGetFieldAccessorTable()
{    return org.apache.flume.channel.file.proto.ProtosFactory.internal_static_Put_fieldAccessorTable.ensureFieldAccessorsInitialized(org.apache.flume.channel.file.proto.ProtosFactory.Put.class, org.apache.flume.channel.file.proto.ProtosFactory.Put.Builder.class);}
0
public Put parsePartialFrom(com.google.protobuf.CodedInputStream input, com.google.protobuf.ExtensionRegistryLite extensionRegistry) throws com.google.protobuf.InvalidProtocolBufferException
{    return new Put(input, extensionRegistry);}
0
public com.google.protobuf.Parser<Put> getParserForType()
{    return PARSER;}
0
public boolean hasEvent()
{    return ((bitField0_ & 0x00000001) == 0x00000001);}
0
public org.apache.flume.channel.file.proto.ProtosFactory.FlumeEvent getEvent()
{    return event_;}
0
public org.apache.flume.channel.file.proto.ProtosFactory.FlumeEventOrBuilder getEventOrBuilder()
{    return event_;}
0
public boolean hasChecksum()
{    return ((bitField0_ & 0x00000002) == 0x00000002);}
0
public long getChecksum()
{    return checksum_;}
0
private void initFields()
{    event_ = org.apache.flume.channel.file.proto.ProtosFactory.FlumeEvent.getDefaultInstance();    checksum_ = 0L;}
0
public final boolean isInitialized()
{    byte isInitialized = memoizedIsInitialized;    if (isInitialized != -1)        return isInitialized == 1;    if (!hasEvent()) {        memoizedIsInitialized = 0;        return false;    }    if (!getEvent().isInitialized()) {        memoizedIsInitialized = 0;        return false;    }    memoizedIsInitialized = 1;    return true;}
0
public void writeTo(com.google.protobuf.CodedOutputStream output) throws java.io.IOException
{    getSerializedSize();    if (((bitField0_ & 0x00000001) == 0x00000001)) {        output.writeMessage(1, event_);    }    if (((bitField0_ & 0x00000002) == 0x00000002)) {        output.writeSFixed64(2, checksum_);    }    getUnknownFields().writeTo(output);}
0
public int getSerializedSize()
{    int size = memoizedSerializedSize;    if (size != -1)        return size;    size = 0;    if (((bitField0_ & 0x00000001) == 0x00000001)) {        size += com.google.protobuf.CodedOutputStream.computeMessageSize(1, event_);    }    if (((bitField0_ & 0x00000002) == 0x00000002)) {        size += com.google.protobuf.CodedOutputStream.computeSFixed64Size(2, checksum_);    }    size += getUnknownFields().getSerializedSize();    memoizedSerializedSize = size;    return size;}
0
protected java.lang.Object writeReplace() throws java.io.ObjectStreamException
{    return super.writeReplace();}
0
public static org.apache.flume.channel.file.proto.ProtosFactory.Put parseFrom(com.google.protobuf.ByteString data) throws com.google.protobuf.InvalidProtocolBufferException
{    return PARSER.parseFrom(data);}
0
public static org.apache.flume.channel.file.proto.ProtosFactory.Put parseFrom(com.google.protobuf.ByteString data, com.google.protobuf.ExtensionRegistryLite extensionRegistry) throws com.google.protobuf.InvalidProtocolBufferException
{    return PARSER.parseFrom(data, extensionRegistry);}
0
public static org.apache.flume.channel.file.proto.ProtosFactory.Put parseFrom(byte[] data) throws com.google.protobuf.InvalidProtocolBufferException
{    return PARSER.parseFrom(data);}
0
public static org.apache.flume.channel.file.proto.ProtosFactory.Put parseFrom(byte[] data, com.google.protobuf.ExtensionRegistryLite extensionRegistry) throws com.google.protobuf.InvalidProtocolBufferException
{    return PARSER.parseFrom(data, extensionRegistry);}
0
public static org.apache.flume.channel.file.proto.ProtosFactory.Put parseFrom(java.io.InputStream input) throws java.io.IOException
{    return PARSER.parseFrom(input);}
0
public static org.apache.flume.channel.file.proto.ProtosFactory.Put parseFrom(java.io.InputStream input, com.google.protobuf.ExtensionRegistryLite extensionRegistry) throws java.io.IOException
{    return PARSER.parseFrom(input, extensionRegistry);}
0
public static org.apache.flume.channel.file.proto.ProtosFactory.Put parseDelimitedFrom(java.io.InputStream input) throws java.io.IOException
{    return PARSER.parseDelimitedFrom(input);}
0
public static org.apache.flume.channel.file.proto.ProtosFactory.Put parseDelimitedFrom(java.io.InputStream input, com.google.protobuf.ExtensionRegistryLite extensionRegistry) throws java.io.IOException
{    return PARSER.parseDelimitedFrom(input, extensionRegistry);}
0
public static org.apache.flume.channel.file.proto.ProtosFactory.Put parseFrom(com.google.protobuf.CodedInputStream input) throws java.io.IOException
{    return PARSER.parseFrom(input);}
0
public static org.apache.flume.channel.file.proto.ProtosFactory.Put parseFrom(com.google.protobuf.CodedInputStream input, com.google.protobuf.ExtensionRegistryLite extensionRegistry) throws java.io.IOException
{    return PARSER.parseFrom(input, extensionRegistry);}
0
public static Builder newBuilder()
{    return Builder.create();}
0
public Builder newBuilderForType()
{    return newBuilder();}
0
public static Builder newBuilder(org.apache.flume.channel.file.proto.ProtosFactory.Put prototype)
{    return newBuilder().mergeFrom(prototype);}
0
public Builder toBuilder()
{    return newBuilder(this);}
0
protected Builder newBuilderForType(com.google.protobuf.GeneratedMessage.BuilderParent parent)
{    Builder builder = new Builder(parent);    return builder;}
0
public static final com.google.protobuf.Descriptors.Descriptor getDescriptor()
{    return org.apache.flume.channel.file.proto.ProtosFactory.internal_static_Put_descriptor;}
0
protected com.google.protobuf.GeneratedMessage.FieldAccessorTable internalGetFieldAccessorTable()
{    return org.apache.flume.channel.file.proto.ProtosFactory.internal_static_Put_fieldAccessorTable.ensureFieldAccessorsInitialized(org.apache.flume.channel.file.proto.ProtosFactory.Put.class, org.apache.flume.channel.file.proto.ProtosFactory.Put.Builder.class);}
0
private void maybeForceBuilderInitialization()
{    if (com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders) {        getEventFieldBuilder();    }}
0
private static Builder create()
{    return new Builder();}
0
public Builder clear()
{    super.clear();    if (eventBuilder_ == null) {        event_ = org.apache.flume.channel.file.proto.ProtosFactory.FlumeEvent.getDefaultInstance();    } else {        eventBuilder_.clear();    }    bitField0_ = (bitField0_ & ~0x00000001);    checksum_ = 0L;    bitField0_ = (bitField0_ & ~0x00000002);    return this;}
0
public Builder clone()
{    return create().mergeFrom(buildPartial());}
0
public com.google.protobuf.Descriptors.Descriptor getDescriptorForType()
{    return org.apache.flume.channel.file.proto.ProtosFactory.internal_static_Put_descriptor;}
0
public org.apache.flume.channel.file.proto.ProtosFactory.Put getDefaultInstanceForType()
{    return org.apache.flume.channel.file.proto.ProtosFactory.Put.getDefaultInstance();}
0
public org.apache.flume.channel.file.proto.ProtosFactory.Put build()
{    org.apache.flume.channel.file.proto.ProtosFactory.Put result = buildPartial();    if (!result.isInitialized()) {        throw newUninitializedMessageException(result);    }    return result;}
0
public org.apache.flume.channel.file.proto.ProtosFactory.Put buildPartial()
{    org.apache.flume.channel.file.proto.ProtosFactory.Put result = new org.apache.flume.channel.file.proto.ProtosFactory.Put(this);    int from_bitField0_ = bitField0_;    int to_bitField0_ = 0;    if (((from_bitField0_ & 0x00000001) == 0x00000001)) {        to_bitField0_ |= 0x00000001;    }    if (eventBuilder_ == null) {        result.event_ = event_;    } else {        result.event_ = eventBuilder_.build();    }    if (((from_bitField0_ & 0x00000002) == 0x00000002)) {        to_bitField0_ |= 0x00000002;    }    result.checksum_ = checksum_;    result.bitField0_ = to_bitField0_;    onBuilt();    return result;}
0
public Builder mergeFrom(com.google.protobuf.Message other)
{    if (other instanceof org.apache.flume.channel.file.proto.ProtosFactory.Put) {        return mergeFrom((org.apache.flume.channel.file.proto.ProtosFactory.Put) other);    } else {        super.mergeFrom(other);        return this;    }}
0
public Builder mergeFrom(org.apache.flume.channel.file.proto.ProtosFactory.Put other)
{    if (other == org.apache.flume.channel.file.proto.ProtosFactory.Put.getDefaultInstance())        return this;    if (other.hasEvent()) {        mergeEvent(other.getEvent());    }    if (other.hasChecksum()) {        setChecksum(other.getChecksum());    }    this.mergeUnknownFields(other.getUnknownFields());    return this;}
0
public final boolean isInitialized()
{    if (!hasEvent()) {        return false;    }    if (!getEvent().isInitialized()) {        return false;    }    return true;}
0
public Builder mergeFrom(com.google.protobuf.CodedInputStream input, com.google.protobuf.ExtensionRegistryLite extensionRegistry) throws java.io.IOException
{    org.apache.flume.channel.file.proto.ProtosFactory.Put parsedMessage = null;    try {        parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);    } catch (com.google.protobuf.InvalidProtocolBufferException e) {        parsedMessage = (org.apache.flume.channel.file.proto.ProtosFactory.Put) e.getUnfinishedMessage();        throw e;    } finally {        if (parsedMessage != null) {            mergeFrom(parsedMessage);        }    }    return this;}
0
public boolean hasEvent()
{    return ((bitField0_ & 0x00000001) == 0x00000001);}
0
public org.apache.flume.channel.file.proto.ProtosFactory.FlumeEvent getEvent()
{    if (eventBuilder_ == null) {        return event_;    } else {        return eventBuilder_.getMessage();    }}
0
public Builder setEvent(org.apache.flume.channel.file.proto.ProtosFactory.FlumeEvent value)
{    if (eventBuilder_ == null) {        if (value == null) {            throw new NullPointerException();        }        event_ = value;        onChanged();    } else {        eventBuilder_.setMessage(value);    }    bitField0_ |= 0x00000001;    return this;}
0
public Builder setEvent(org.apache.flume.channel.file.proto.ProtosFactory.FlumeEvent.Builder builderForValue)
{    if (eventBuilder_ == null) {        event_ = builderForValue.build();        onChanged();    } else {        eventBuilder_.setMessage(builderForValue.build());    }    bitField0_ |= 0x00000001;    return this;}
0
public Builder mergeEvent(org.apache.flume.channel.file.proto.ProtosFactory.FlumeEvent value)
{    if (eventBuilder_ == null) {        if (((bitField0_ & 0x00000001) == 0x00000001) && event_ != org.apache.flume.channel.file.proto.ProtosFactory.FlumeEvent.getDefaultInstance()) {            event_ = org.apache.flume.channel.file.proto.ProtosFactory.FlumeEvent.newBuilder(event_).mergeFrom(value).buildPartial();        } else {            event_ = value;        }        onChanged();    } else {        eventBuilder_.mergeFrom(value);    }    bitField0_ |= 0x00000001;    return this;}
0
public Builder clearEvent()
{    if (eventBuilder_ == null) {        event_ = org.apache.flume.channel.file.proto.ProtosFactory.FlumeEvent.getDefaultInstance();        onChanged();    } else {        eventBuilder_.clear();    }    bitField0_ = (bitField0_ & ~0x00000001);    return this;}
0
public org.apache.flume.channel.file.proto.ProtosFactory.FlumeEvent.Builder getEventBuilder()
{    bitField0_ |= 0x00000001;    onChanged();    return getEventFieldBuilder().getBuilder();}
0
public org.apache.flume.channel.file.proto.ProtosFactory.FlumeEventOrBuilder getEventOrBuilder()
{    if (eventBuilder_ != null) {        return eventBuilder_.getMessageOrBuilder();    } else {        return event_;    }}
0
private com.google.protobuf.SingleFieldBuilder<org.apache.flume.channel.file.proto.ProtosFactory.FlumeEvent, org.apache.flume.channel.file.proto.ProtosFactory.FlumeEvent.Builder, org.apache.flume.channel.file.proto.ProtosFactory.FlumeEventOrBuilder> getEventFieldBuilder()
{    if (eventBuilder_ == null) {        eventBuilder_ = new com.google.protobuf.SingleFieldBuilder<org.apache.flume.channel.file.proto.ProtosFactory.FlumeEvent, org.apache.flume.channel.file.proto.ProtosFactory.FlumeEvent.Builder, org.apache.flume.channel.file.proto.ProtosFactory.FlumeEventOrBuilder>(event_, getParentForChildren(), isClean());        event_ = null;    }    return eventBuilder_;}
0
public boolean hasChecksum()
{    return ((bitField0_ & 0x00000002) == 0x00000002);}
0
public long getChecksum()
{    return checksum_;}
0
public Builder setChecksum(long value)
{    bitField0_ |= 0x00000002;    checksum_ = value;    onChanged();    return this;}
0
public Builder clearChecksum()
{    bitField0_ = (bitField0_ & ~0x00000002);    checksum_ = 0L;    onChanged();    return this;}
0
public static Take getDefaultInstance()
{    return defaultInstance;}
0
public Take getDefaultInstanceForType()
{    return defaultInstance;}
0
public final com.google.protobuf.UnknownFieldSet getUnknownFields()
{    return this.unknownFields;}
0
public static final com.google.protobuf.Descriptors.Descriptor getDescriptor()
{    return org.apache.flume.channel.file.proto.ProtosFactory.internal_static_Take_descriptor;}
0
protected com.google.protobuf.GeneratedMessage.FieldAccessorTable internalGetFieldAccessorTable()
{    return org.apache.flume.channel.file.proto.ProtosFactory.internal_static_Take_fieldAccessorTable.ensureFieldAccessorsInitialized(org.apache.flume.channel.file.proto.ProtosFactory.Take.class, org.apache.flume.channel.file.proto.ProtosFactory.Take.Builder.class);}
0
public Take parsePartialFrom(com.google.protobuf.CodedInputStream input, com.google.protobuf.ExtensionRegistryLite extensionRegistry) throws com.google.protobuf.InvalidProtocolBufferException
{    return new Take(input, extensionRegistry);}
0
public com.google.protobuf.Parser<Take> getParserForType()
{    return PARSER;}
0
public boolean hasFileID()
{    return ((bitField0_ & 0x00000001) == 0x00000001);}
0
public int getFileID()
{    return fileID_;}
0
public boolean hasOffset()
{    return ((bitField0_ & 0x00000002) == 0x00000002);}
0
public int getOffset()
{    return offset_;}
0
private void initFields()
{    fileID_ = 0;    offset_ = 0;}
0
public final boolean isInitialized()
{    byte isInitialized = memoizedIsInitialized;    if (isInitialized != -1)        return isInitialized == 1;    if (!hasFileID()) {        memoizedIsInitialized = 0;        return false;    }    if (!hasOffset()) {        memoizedIsInitialized = 0;        return false;    }    memoizedIsInitialized = 1;    return true;}
0
public void writeTo(com.google.protobuf.CodedOutputStream output) throws java.io.IOException
{    getSerializedSize();    if (((bitField0_ & 0x00000001) == 0x00000001)) {        output.writeSFixed32(1, fileID_);    }    if (((bitField0_ & 0x00000002) == 0x00000002)) {        output.writeSFixed32(2, offset_);    }    getUnknownFields().writeTo(output);}
0
public int getSerializedSize()
{    int size = memoizedSerializedSize;    if (size != -1)        return size;    size = 0;    if (((bitField0_ & 0x00000001) == 0x00000001)) {        size += com.google.protobuf.CodedOutputStream.computeSFixed32Size(1, fileID_);    }    if (((bitField0_ & 0x00000002) == 0x00000002)) {        size += com.google.protobuf.CodedOutputStream.computeSFixed32Size(2, offset_);    }    size += getUnknownFields().getSerializedSize();    memoizedSerializedSize = size;    return size;}
0
protected java.lang.Object writeReplace() throws java.io.ObjectStreamException
{    return super.writeReplace();}
0
public static org.apache.flume.channel.file.proto.ProtosFactory.Take parseFrom(com.google.protobuf.ByteString data) throws com.google.protobuf.InvalidProtocolBufferException
{    return PARSER.parseFrom(data);}
0
public static org.apache.flume.channel.file.proto.ProtosFactory.Take parseFrom(com.google.protobuf.ByteString data, com.google.protobuf.ExtensionRegistryLite extensionRegistry) throws com.google.protobuf.InvalidProtocolBufferException
{    return PARSER.parseFrom(data, extensionRegistry);}
0
public static org.apache.flume.channel.file.proto.ProtosFactory.Take parseFrom(byte[] data) throws com.google.protobuf.InvalidProtocolBufferException
{    return PARSER.parseFrom(data);}
0
public static org.apache.flume.channel.file.proto.ProtosFactory.Take parseFrom(byte[] data, com.google.protobuf.ExtensionRegistryLite extensionRegistry) throws com.google.protobuf.InvalidProtocolBufferException
{    return PARSER.parseFrom(data, extensionRegistry);}
0
public static org.apache.flume.channel.file.proto.ProtosFactory.Take parseFrom(java.io.InputStream input) throws java.io.IOException
{    return PARSER.parseFrom(input);}
0
public static org.apache.flume.channel.file.proto.ProtosFactory.Take parseFrom(java.io.InputStream input, com.google.protobuf.ExtensionRegistryLite extensionRegistry) throws java.io.IOException
{    return PARSER.parseFrom(input, extensionRegistry);}
0
public static org.apache.flume.channel.file.proto.ProtosFactory.Take parseDelimitedFrom(java.io.InputStream input) throws java.io.IOException
{    return PARSER.parseDelimitedFrom(input);}
0
public static org.apache.flume.channel.file.proto.ProtosFactory.Take parseDelimitedFrom(java.io.InputStream input, com.google.protobuf.ExtensionRegistryLite extensionRegistry) throws java.io.IOException
{    return PARSER.parseDelimitedFrom(input, extensionRegistry);}
0
public static org.apache.flume.channel.file.proto.ProtosFactory.Take parseFrom(com.google.protobuf.CodedInputStream input) throws java.io.IOException
{    return PARSER.parseFrom(input);}
0
public static org.apache.flume.channel.file.proto.ProtosFactory.Take parseFrom(com.google.protobuf.CodedInputStream input, com.google.protobuf.ExtensionRegistryLite extensionRegistry) throws java.io.IOException
{    return PARSER.parseFrom(input, extensionRegistry);}
0
public static Builder newBuilder()
{    return Builder.create();}
0
public Builder newBuilderForType()
{    return newBuilder();}
0
public static Builder newBuilder(org.apache.flume.channel.file.proto.ProtosFactory.Take prototype)
{    return newBuilder().mergeFrom(prototype);}
0
public Builder toBuilder()
{    return newBuilder(this);}
0
protected Builder newBuilderForType(com.google.protobuf.GeneratedMessage.BuilderParent parent)
{    Builder builder = new Builder(parent);    return builder;}
0
public static final com.google.protobuf.Descriptors.Descriptor getDescriptor()
{    return org.apache.flume.channel.file.proto.ProtosFactory.internal_static_Take_descriptor;}
0
protected com.google.protobuf.GeneratedMessage.FieldAccessorTable internalGetFieldAccessorTable()
{    return org.apache.flume.channel.file.proto.ProtosFactory.internal_static_Take_fieldAccessorTable.ensureFieldAccessorsInitialized(org.apache.flume.channel.file.proto.ProtosFactory.Take.class, org.apache.flume.channel.file.proto.ProtosFactory.Take.Builder.class);}
0
private void maybeForceBuilderInitialization()
{    if (com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders) {    }}
0
private static Builder create()
{    return new Builder();}
0
public Builder clear()
{    super.clear();    fileID_ = 0;    bitField0_ = (bitField0_ & ~0x00000001);    offset_ = 0;    bitField0_ = (bitField0_ & ~0x00000002);    return this;}
0
public Builder clone()
{    return create().mergeFrom(buildPartial());}
0
public com.google.protobuf.Descriptors.Descriptor getDescriptorForType()
{    return org.apache.flume.channel.file.proto.ProtosFactory.internal_static_Take_descriptor;}
0
public org.apache.flume.channel.file.proto.ProtosFactory.Take getDefaultInstanceForType()
{    return org.apache.flume.channel.file.proto.ProtosFactory.Take.getDefaultInstance();}
0
public org.apache.flume.channel.file.proto.ProtosFactory.Take build()
{    org.apache.flume.channel.file.proto.ProtosFactory.Take result = buildPartial();    if (!result.isInitialized()) {        throw newUninitializedMessageException(result);    }    return result;}
0
public org.apache.flume.channel.file.proto.ProtosFactory.Take buildPartial()
{    org.apache.flume.channel.file.proto.ProtosFactory.Take result = new org.apache.flume.channel.file.proto.ProtosFactory.Take(this);    int from_bitField0_ = bitField0_;    int to_bitField0_ = 0;    if (((from_bitField0_ & 0x00000001) == 0x00000001)) {        to_bitField0_ |= 0x00000001;    }    result.fileID_ = fileID_;    if (((from_bitField0_ & 0x00000002) == 0x00000002)) {        to_bitField0_ |= 0x00000002;    }    result.offset_ = offset_;    result.bitField0_ = to_bitField0_;    onBuilt();    return result;}
0
public Builder mergeFrom(com.google.protobuf.Message other)
{    if (other instanceof org.apache.flume.channel.file.proto.ProtosFactory.Take) {        return mergeFrom((org.apache.flume.channel.file.proto.ProtosFactory.Take) other);    } else {        super.mergeFrom(other);        return this;    }}
0
public Builder mergeFrom(org.apache.flume.channel.file.proto.ProtosFactory.Take other)
{    if (other == org.apache.flume.channel.file.proto.ProtosFactory.Take.getDefaultInstance())        return this;    if (other.hasFileID()) {        setFileID(other.getFileID());    }    if (other.hasOffset()) {        setOffset(other.getOffset());    }    this.mergeUnknownFields(other.getUnknownFields());    return this;}
0
public final boolean isInitialized()
{    if (!hasFileID()) {        return false;    }    if (!hasOffset()) {        return false;    }    return true;}
0
public Builder mergeFrom(com.google.protobuf.CodedInputStream input, com.google.protobuf.ExtensionRegistryLite extensionRegistry) throws java.io.IOException
{    org.apache.flume.channel.file.proto.ProtosFactory.Take parsedMessage = null;    try {        parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);    } catch (com.google.protobuf.InvalidProtocolBufferException e) {        parsedMessage = (org.apache.flume.channel.file.proto.ProtosFactory.Take) e.getUnfinishedMessage();        throw e;    } finally {        if (parsedMessage != null) {            mergeFrom(parsedMessage);        }    }    return this;}
0
public boolean hasFileID()
{    return ((bitField0_ & 0x00000001) == 0x00000001);}
0
public int getFileID()
{    return fileID_;}
0
public Builder setFileID(int value)
{    bitField0_ |= 0x00000001;    fileID_ = value;    onChanged();    return this;}
0
public Builder clearFileID()
{    bitField0_ = (bitField0_ & ~0x00000001);    fileID_ = 0;    onChanged();    return this;}
0
public boolean hasOffset()
{    return ((bitField0_ & 0x00000002) == 0x00000002);}
0
public int getOffset()
{    return offset_;}
0
public Builder setOffset(int value)
{    bitField0_ |= 0x00000002;    offset_ = value;    onChanged();    return this;}
0
public Builder clearOffset()
{    bitField0_ = (bitField0_ & ~0x00000002);    offset_ = 0;    onChanged();    return this;}
0
public static Rollback getDefaultInstance()
{    return defaultInstance;}
0
public Rollback getDefaultInstanceForType()
{    return defaultInstance;}
0
public final com.google.protobuf.UnknownFieldSet getUnknownFields()
{    return this.unknownFields;}
0
public static final com.google.protobuf.Descriptors.Descriptor getDescriptor()
{    return org.apache.flume.channel.file.proto.ProtosFactory.internal_static_Rollback_descriptor;}
0
protected com.google.protobuf.GeneratedMessage.FieldAccessorTable internalGetFieldAccessorTable()
{    return org.apache.flume.channel.file.proto.ProtosFactory.internal_static_Rollback_fieldAccessorTable.ensureFieldAccessorsInitialized(org.apache.flume.channel.file.proto.ProtosFactory.Rollback.class, org.apache.flume.channel.file.proto.ProtosFactory.Rollback.Builder.class);}
0
public Rollback parsePartialFrom(com.google.protobuf.CodedInputStream input, com.google.protobuf.ExtensionRegistryLite extensionRegistry) throws com.google.protobuf.InvalidProtocolBufferException
{    return new Rollback(input, extensionRegistry);}
0
public com.google.protobuf.Parser<Rollback> getParserForType()
{    return PARSER;}
0
private void initFields()
{}
0
public final boolean isInitialized()
{    byte isInitialized = memoizedIsInitialized;    if (isInitialized != -1)        return isInitialized == 1;    memoizedIsInitialized = 1;    return true;}
0
public void writeTo(com.google.protobuf.CodedOutputStream output) throws java.io.IOException
{    getSerializedSize();    getUnknownFields().writeTo(output);}
0
public int getSerializedSize()
{    int size = memoizedSerializedSize;    if (size != -1)        return size;    size = 0;    size += getUnknownFields().getSerializedSize();    memoizedSerializedSize = size;    return size;}
0
protected java.lang.Object writeReplace() throws java.io.ObjectStreamException
{    return super.writeReplace();}
0
public static org.apache.flume.channel.file.proto.ProtosFactory.Rollback parseFrom(com.google.protobuf.ByteString data) throws com.google.protobuf.InvalidProtocolBufferException
{    return PARSER.parseFrom(data);}
0
public static org.apache.flume.channel.file.proto.ProtosFactory.Rollback parseFrom(com.google.protobuf.ByteString data, com.google.protobuf.ExtensionRegistryLite extensionRegistry) throws com.google.protobuf.InvalidProtocolBufferException
{    return PARSER.parseFrom(data, extensionRegistry);}
0
public static org.apache.flume.channel.file.proto.ProtosFactory.Rollback parseFrom(byte[] data) throws com.google.protobuf.InvalidProtocolBufferException
{    return PARSER.parseFrom(data);}
0
public static org.apache.flume.channel.file.proto.ProtosFactory.Rollback parseFrom(byte[] data, com.google.protobuf.ExtensionRegistryLite extensionRegistry) throws com.google.protobuf.InvalidProtocolBufferException
{    return PARSER.parseFrom(data, extensionRegistry);}
0
public static org.apache.flume.channel.file.proto.ProtosFactory.Rollback parseFrom(java.io.InputStream input) throws java.io.IOException
{    return PARSER.parseFrom(input);}
0
public static org.apache.flume.channel.file.proto.ProtosFactory.Rollback parseFrom(java.io.InputStream input, com.google.protobuf.ExtensionRegistryLite extensionRegistry) throws java.io.IOException
{    return PARSER.parseFrom(input, extensionRegistry);}
0
public static org.apache.flume.channel.file.proto.ProtosFactory.Rollback parseDelimitedFrom(java.io.InputStream input) throws java.io.IOException
{    return PARSER.parseDelimitedFrom(input);}
0
public static org.apache.flume.channel.file.proto.ProtosFactory.Rollback parseDelimitedFrom(java.io.InputStream input, com.google.protobuf.ExtensionRegistryLite extensionRegistry) throws java.io.IOException
{    return PARSER.parseDelimitedFrom(input, extensionRegistry);}
0
public static org.apache.flume.channel.file.proto.ProtosFactory.Rollback parseFrom(com.google.protobuf.CodedInputStream input) throws java.io.IOException
{    return PARSER.parseFrom(input);}
0
public static org.apache.flume.channel.file.proto.ProtosFactory.Rollback parseFrom(com.google.protobuf.CodedInputStream input, com.google.protobuf.ExtensionRegistryLite extensionRegistry) throws java.io.IOException
{    return PARSER.parseFrom(input, extensionRegistry);}
0
public static Builder newBuilder()
{    return Builder.create();}
0
public Builder newBuilderForType()
{    return newBuilder();}
0
public static Builder newBuilder(org.apache.flume.channel.file.proto.ProtosFactory.Rollback prototype)
{    return newBuilder().mergeFrom(prototype);}
0
public Builder toBuilder()
{    return newBuilder(this);}
0
protected Builder newBuilderForType(com.google.protobuf.GeneratedMessage.BuilderParent parent)
{    Builder builder = new Builder(parent);    return builder;}
0
public static final com.google.protobuf.Descriptors.Descriptor getDescriptor()
{    return org.apache.flume.channel.file.proto.ProtosFactory.internal_static_Rollback_descriptor;}
0
protected com.google.protobuf.GeneratedMessage.FieldAccessorTable internalGetFieldAccessorTable()
{    return org.apache.flume.channel.file.proto.ProtosFactory.internal_static_Rollback_fieldAccessorTable.ensureFieldAccessorsInitialized(org.apache.flume.channel.file.proto.ProtosFactory.Rollback.class, org.apache.flume.channel.file.proto.ProtosFactory.Rollback.Builder.class);}
0
private void maybeForceBuilderInitialization()
{    if (com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders) {    }}
0
private static Builder create()
{    return new Builder();}
0
public Builder clear()
{    super.clear();    return this;}
0
public Builder clone()
{    return create().mergeFrom(buildPartial());}
0
public com.google.protobuf.Descriptors.Descriptor getDescriptorForType()
{    return org.apache.flume.channel.file.proto.ProtosFactory.internal_static_Rollback_descriptor;}
0
public org.apache.flume.channel.file.proto.ProtosFactory.Rollback getDefaultInstanceForType()
{    return org.apache.flume.channel.file.proto.ProtosFactory.Rollback.getDefaultInstance();}
0
public org.apache.flume.channel.file.proto.ProtosFactory.Rollback build()
{    org.apache.flume.channel.file.proto.ProtosFactory.Rollback result = buildPartial();    if (!result.isInitialized()) {        throw newUninitializedMessageException(result);    }    return result;}
0
public org.apache.flume.channel.file.proto.ProtosFactory.Rollback buildPartial()
{    org.apache.flume.channel.file.proto.ProtosFactory.Rollback result = new org.apache.flume.channel.file.proto.ProtosFactory.Rollback(this);    onBuilt();    return result;}
0
public Builder mergeFrom(com.google.protobuf.Message other)
{    if (other instanceof org.apache.flume.channel.file.proto.ProtosFactory.Rollback) {        return mergeFrom((org.apache.flume.channel.file.proto.ProtosFactory.Rollback) other);    } else {        super.mergeFrom(other);        return this;    }}
0
public Builder mergeFrom(org.apache.flume.channel.file.proto.ProtosFactory.Rollback other)
{    if (other == org.apache.flume.channel.file.proto.ProtosFactory.Rollback.getDefaultInstance())        return this;    this.mergeUnknownFields(other.getUnknownFields());    return this;}
0
public final boolean isInitialized()
{    return true;}
0
public Builder mergeFrom(com.google.protobuf.CodedInputStream input, com.google.protobuf.ExtensionRegistryLite extensionRegistry) throws java.io.IOException
{    org.apache.flume.channel.file.proto.ProtosFactory.Rollback parsedMessage = null;    try {        parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);    } catch (com.google.protobuf.InvalidProtocolBufferException e) {        parsedMessage = (org.apache.flume.channel.file.proto.ProtosFactory.Rollback) e.getUnfinishedMessage();        throw e;    } finally {        if (parsedMessage != null) {            mergeFrom(parsedMessage);        }    }    return this;}
0
public static Commit getDefaultInstance()
{    return defaultInstance;}
0
public Commit getDefaultInstanceForType()
{    return defaultInstance;}
0
public final com.google.protobuf.UnknownFieldSet getUnknownFields()
{    return this.unknownFields;}
0
public static final com.google.protobuf.Descriptors.Descriptor getDescriptor()
{    return org.apache.flume.channel.file.proto.ProtosFactory.internal_static_Commit_descriptor;}
0
protected com.google.protobuf.GeneratedMessage.FieldAccessorTable internalGetFieldAccessorTable()
{    return org.apache.flume.channel.file.proto.ProtosFactory.internal_static_Commit_fieldAccessorTable.ensureFieldAccessorsInitialized(org.apache.flume.channel.file.proto.ProtosFactory.Commit.class, org.apache.flume.channel.file.proto.ProtosFactory.Commit.Builder.class);}
0
public Commit parsePartialFrom(com.google.protobuf.CodedInputStream input, com.google.protobuf.ExtensionRegistryLite extensionRegistry) throws com.google.protobuf.InvalidProtocolBufferException
{    return new Commit(input, extensionRegistry);}
0
public com.google.protobuf.Parser<Commit> getParserForType()
{    return PARSER;}
0
public boolean hasType()
{    return ((bitField0_ & 0x00000001) == 0x00000001);}
0
public int getType()
{    return type_;}
0
private void initFields()
{    type_ = 0;}
0
public final boolean isInitialized()
{    byte isInitialized = memoizedIsInitialized;    if (isInitialized != -1)        return isInitialized == 1;    if (!hasType()) {        memoizedIsInitialized = 0;        return false;    }    memoizedIsInitialized = 1;    return true;}
0
public void writeTo(com.google.protobuf.CodedOutputStream output) throws java.io.IOException
{    getSerializedSize();    if (((bitField0_ & 0x00000001) == 0x00000001)) {        output.writeSFixed32(1, type_);    }    getUnknownFields().writeTo(output);}
0
public int getSerializedSize()
{    int size = memoizedSerializedSize;    if (size != -1)        return size;    size = 0;    if (((bitField0_ & 0x00000001) == 0x00000001)) {        size += com.google.protobuf.CodedOutputStream.computeSFixed32Size(1, type_);    }    size += getUnknownFields().getSerializedSize();    memoizedSerializedSize = size;    return size;}
0
protected java.lang.Object writeReplace() throws java.io.ObjectStreamException
{    return super.writeReplace();}
0
public static org.apache.flume.channel.file.proto.ProtosFactory.Commit parseFrom(com.google.protobuf.ByteString data) throws com.google.protobuf.InvalidProtocolBufferException
{    return PARSER.parseFrom(data);}
0
public static org.apache.flume.channel.file.proto.ProtosFactory.Commit parseFrom(com.google.protobuf.ByteString data, com.google.protobuf.ExtensionRegistryLite extensionRegistry) throws com.google.protobuf.InvalidProtocolBufferException
{    return PARSER.parseFrom(data, extensionRegistry);}
0
public static org.apache.flume.channel.file.proto.ProtosFactory.Commit parseFrom(byte[] data) throws com.google.protobuf.InvalidProtocolBufferException
{    return PARSER.parseFrom(data);}
0
public static org.apache.flume.channel.file.proto.ProtosFactory.Commit parseFrom(byte[] data, com.google.protobuf.ExtensionRegistryLite extensionRegistry) throws com.google.protobuf.InvalidProtocolBufferException
{    return PARSER.parseFrom(data, extensionRegistry);}
0
public static org.apache.flume.channel.file.proto.ProtosFactory.Commit parseFrom(java.io.InputStream input) throws java.io.IOException
{    return PARSER.parseFrom(input);}
0
public static org.apache.flume.channel.file.proto.ProtosFactory.Commit parseFrom(java.io.InputStream input, com.google.protobuf.ExtensionRegistryLite extensionRegistry) throws java.io.IOException
{    return PARSER.parseFrom(input, extensionRegistry);}
0
public static org.apache.flume.channel.file.proto.ProtosFactory.Commit parseDelimitedFrom(java.io.InputStream input) throws java.io.IOException
{    return PARSER.parseDelimitedFrom(input);}
0
public static org.apache.flume.channel.file.proto.ProtosFactory.Commit parseDelimitedFrom(java.io.InputStream input, com.google.protobuf.ExtensionRegistryLite extensionRegistry) throws java.io.IOException
{    return PARSER.parseDelimitedFrom(input, extensionRegistry);}
0
public static org.apache.flume.channel.file.proto.ProtosFactory.Commit parseFrom(com.google.protobuf.CodedInputStream input) throws java.io.IOException
{    return PARSER.parseFrom(input);}
0
public static org.apache.flume.channel.file.proto.ProtosFactory.Commit parseFrom(com.google.protobuf.CodedInputStream input, com.google.protobuf.ExtensionRegistryLite extensionRegistry) throws java.io.IOException
{    return PARSER.parseFrom(input, extensionRegistry);}
0
public static Builder newBuilder()
{    return Builder.create();}
0
public Builder newBuilderForType()
{    return newBuilder();}
0
public static Builder newBuilder(org.apache.flume.channel.file.proto.ProtosFactory.Commit prototype)
{    return newBuilder().mergeFrom(prototype);}
0
public Builder toBuilder()
{    return newBuilder(this);}
0
protected Builder newBuilderForType(com.google.protobuf.GeneratedMessage.BuilderParent parent)
{    Builder builder = new Builder(parent);    return builder;}
0
public static final com.google.protobuf.Descriptors.Descriptor getDescriptor()
{    return org.apache.flume.channel.file.proto.ProtosFactory.internal_static_Commit_descriptor;}
0
protected com.google.protobuf.GeneratedMessage.FieldAccessorTable internalGetFieldAccessorTable()
{    return org.apache.flume.channel.file.proto.ProtosFactory.internal_static_Commit_fieldAccessorTable.ensureFieldAccessorsInitialized(org.apache.flume.channel.file.proto.ProtosFactory.Commit.class, org.apache.flume.channel.file.proto.ProtosFactory.Commit.Builder.class);}
0
private void maybeForceBuilderInitialization()
{    if (com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders) {    }}
0
private static Builder create()
{    return new Builder();}
0
public Builder clear()
{    super.clear();    type_ = 0;    bitField0_ = (bitField0_ & ~0x00000001);    return this;}
0
public Builder clone()
{    return create().mergeFrom(buildPartial());}
0
public com.google.protobuf.Descriptors.Descriptor getDescriptorForType()
{    return org.apache.flume.channel.file.proto.ProtosFactory.internal_static_Commit_descriptor;}
0
public org.apache.flume.channel.file.proto.ProtosFactory.Commit getDefaultInstanceForType()
{    return org.apache.flume.channel.file.proto.ProtosFactory.Commit.getDefaultInstance();}
0
public org.apache.flume.channel.file.proto.ProtosFactory.Commit build()
{    org.apache.flume.channel.file.proto.ProtosFactory.Commit result = buildPartial();    if (!result.isInitialized()) {        throw newUninitializedMessageException(result);    }    return result;}
0
public org.apache.flume.channel.file.proto.ProtosFactory.Commit buildPartial()
{    org.apache.flume.channel.file.proto.ProtosFactory.Commit result = new org.apache.flume.channel.file.proto.ProtosFactory.Commit(this);    int from_bitField0_ = bitField0_;    int to_bitField0_ = 0;    if (((from_bitField0_ & 0x00000001) == 0x00000001)) {        to_bitField0_ |= 0x00000001;    }    result.type_ = type_;    result.bitField0_ = to_bitField0_;    onBuilt();    return result;}
0
public Builder mergeFrom(com.google.protobuf.Message other)
{    if (other instanceof org.apache.flume.channel.file.proto.ProtosFactory.Commit) {        return mergeFrom((org.apache.flume.channel.file.proto.ProtosFactory.Commit) other);    } else {        super.mergeFrom(other);        return this;    }}
0
public Builder mergeFrom(org.apache.flume.channel.file.proto.ProtosFactory.Commit other)
{    if (other == org.apache.flume.channel.file.proto.ProtosFactory.Commit.getDefaultInstance())        return this;    if (other.hasType()) {        setType(other.getType());    }    this.mergeUnknownFields(other.getUnknownFields());    return this;}
0
public final boolean isInitialized()
{    if (!hasType()) {        return false;    }    return true;}
0
public Builder mergeFrom(com.google.protobuf.CodedInputStream input, com.google.protobuf.ExtensionRegistryLite extensionRegistry) throws java.io.IOException
{    org.apache.flume.channel.file.proto.ProtosFactory.Commit parsedMessage = null;    try {        parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);    } catch (com.google.protobuf.InvalidProtocolBufferException e) {        parsedMessage = (org.apache.flume.channel.file.proto.ProtosFactory.Commit) e.getUnfinishedMessage();        throw e;    } finally {        if (parsedMessage != null) {            mergeFrom(parsedMessage);        }    }    return this;}
0
public boolean hasType()
{    return ((bitField0_ & 0x00000001) == 0x00000001);}
0
public int getType()
{    return type_;}
0
public Builder setType(int value)
{    bitField0_ |= 0x00000001;    type_ = value;    onChanged();    return this;}
0
public Builder clearType()
{    bitField0_ = (bitField0_ & ~0x00000001);    type_ = 0;    onChanged();    return this;}
0
public static TransactionEventFooter getDefaultInstance()
{    return defaultInstance;}
0
public TransactionEventFooter getDefaultInstanceForType()
{    return defaultInstance;}
0
public final com.google.protobuf.UnknownFieldSet getUnknownFields()
{    return this.unknownFields;}
0
public static final com.google.protobuf.Descriptors.Descriptor getDescriptor()
{    return org.apache.flume.channel.file.proto.ProtosFactory.internal_static_TransactionEventFooter_descriptor;}
0
protected com.google.protobuf.GeneratedMessage.FieldAccessorTable internalGetFieldAccessorTable()
{    return org.apache.flume.channel.file.proto.ProtosFactory.internal_static_TransactionEventFooter_fieldAccessorTable.ensureFieldAccessorsInitialized(org.apache.flume.channel.file.proto.ProtosFactory.TransactionEventFooter.class, org.apache.flume.channel.file.proto.ProtosFactory.TransactionEventFooter.Builder.class);}
0
public TransactionEventFooter parsePartialFrom(com.google.protobuf.CodedInputStream input, com.google.protobuf.ExtensionRegistryLite extensionRegistry) throws com.google.protobuf.InvalidProtocolBufferException
{    return new TransactionEventFooter(input, extensionRegistry);}
0
public com.google.protobuf.Parser<TransactionEventFooter> getParserForType()
{    return PARSER;}
0
private void initFields()
{}
0
public final boolean isInitialized()
{    byte isInitialized = memoizedIsInitialized;    if (isInitialized != -1)        return isInitialized == 1;    memoizedIsInitialized = 1;    return true;}
0
public void writeTo(com.google.protobuf.CodedOutputStream output) throws java.io.IOException
{    getSerializedSize();    getUnknownFields().writeTo(output);}
0
public int getSerializedSize()
{    int size = memoizedSerializedSize;    if (size != -1)        return size;    size = 0;    size += getUnknownFields().getSerializedSize();    memoizedSerializedSize = size;    return size;}
0
protected java.lang.Object writeReplace() throws java.io.ObjectStreamException
{    return super.writeReplace();}
0
public static org.apache.flume.channel.file.proto.ProtosFactory.TransactionEventFooter parseFrom(com.google.protobuf.ByteString data) throws com.google.protobuf.InvalidProtocolBufferException
{    return PARSER.parseFrom(data);}
0
public static org.apache.flume.channel.file.proto.ProtosFactory.TransactionEventFooter parseFrom(com.google.protobuf.ByteString data, com.google.protobuf.ExtensionRegistryLite extensionRegistry) throws com.google.protobuf.InvalidProtocolBufferException
{    return PARSER.parseFrom(data, extensionRegistry);}
0
public static org.apache.flume.channel.file.proto.ProtosFactory.TransactionEventFooter parseFrom(byte[] data) throws com.google.protobuf.InvalidProtocolBufferException
{    return PARSER.parseFrom(data);}
0
public static org.apache.flume.channel.file.proto.ProtosFactory.TransactionEventFooter parseFrom(byte[] data, com.google.protobuf.ExtensionRegistryLite extensionRegistry) throws com.google.protobuf.InvalidProtocolBufferException
{    return PARSER.parseFrom(data, extensionRegistry);}
0
public static org.apache.flume.channel.file.proto.ProtosFactory.TransactionEventFooter parseFrom(java.io.InputStream input) throws java.io.IOException
{    return PARSER.parseFrom(input);}
0
public static org.apache.flume.channel.file.proto.ProtosFactory.TransactionEventFooter parseFrom(java.io.InputStream input, com.google.protobuf.ExtensionRegistryLite extensionRegistry) throws java.io.IOException
{    return PARSER.parseFrom(input, extensionRegistry);}
0
public static org.apache.flume.channel.file.proto.ProtosFactory.TransactionEventFooter parseDelimitedFrom(java.io.InputStream input) throws java.io.IOException
{    return PARSER.parseDelimitedFrom(input);}
0
public static org.apache.flume.channel.file.proto.ProtosFactory.TransactionEventFooter parseDelimitedFrom(java.io.InputStream input, com.google.protobuf.ExtensionRegistryLite extensionRegistry) throws java.io.IOException
{    return PARSER.parseDelimitedFrom(input, extensionRegistry);}
0
public static org.apache.flume.channel.file.proto.ProtosFactory.TransactionEventFooter parseFrom(com.google.protobuf.CodedInputStream input) throws java.io.IOException
{    return PARSER.parseFrom(input);}
0
public static org.apache.flume.channel.file.proto.ProtosFactory.TransactionEventFooter parseFrom(com.google.protobuf.CodedInputStream input, com.google.protobuf.ExtensionRegistryLite extensionRegistry) throws java.io.IOException
{    return PARSER.parseFrom(input, extensionRegistry);}
0
public static Builder newBuilder()
{    return Builder.create();}
0
public Builder newBuilderForType()
{    return newBuilder();}
0
public static Builder newBuilder(org.apache.flume.channel.file.proto.ProtosFactory.TransactionEventFooter prototype)
{    return newBuilder().mergeFrom(prototype);}
0
public Builder toBuilder()
{    return newBuilder(this);}
0
protected Builder newBuilderForType(com.google.protobuf.GeneratedMessage.BuilderParent parent)
{    Builder builder = new Builder(parent);    return builder;}
0
public static final com.google.protobuf.Descriptors.Descriptor getDescriptor()
{    return org.apache.flume.channel.file.proto.ProtosFactory.internal_static_TransactionEventFooter_descriptor;}
0
protected com.google.protobuf.GeneratedMessage.FieldAccessorTable internalGetFieldAccessorTable()
{    return org.apache.flume.channel.file.proto.ProtosFactory.internal_static_TransactionEventFooter_fieldAccessorTable.ensureFieldAccessorsInitialized(org.apache.flume.channel.file.proto.ProtosFactory.TransactionEventFooter.class, org.apache.flume.channel.file.proto.ProtosFactory.TransactionEventFooter.Builder.class);}
0
private void maybeForceBuilderInitialization()
{    if (com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders) {    }}
0
private static Builder create()
{    return new Builder();}
0
public Builder clear()
{    super.clear();    return this;}
0
public Builder clone()
{    return create().mergeFrom(buildPartial());}
0
public com.google.protobuf.Descriptors.Descriptor getDescriptorForType()
{    return org.apache.flume.channel.file.proto.ProtosFactory.internal_static_TransactionEventFooter_descriptor;}
0
public org.apache.flume.channel.file.proto.ProtosFactory.TransactionEventFooter getDefaultInstanceForType()
{    return org.apache.flume.channel.file.proto.ProtosFactory.TransactionEventFooter.getDefaultInstance();}
0
public org.apache.flume.channel.file.proto.ProtosFactory.TransactionEventFooter build()
{    org.apache.flume.channel.file.proto.ProtosFactory.TransactionEventFooter result = buildPartial();    if (!result.isInitialized()) {        throw newUninitializedMessageException(result);    }    return result;}
0
public org.apache.flume.channel.file.proto.ProtosFactory.TransactionEventFooter buildPartial()
{    org.apache.flume.channel.file.proto.ProtosFactory.TransactionEventFooter result = new org.apache.flume.channel.file.proto.ProtosFactory.TransactionEventFooter(this);    onBuilt();    return result;}
0
public Builder mergeFrom(com.google.protobuf.Message other)
{    if (other instanceof org.apache.flume.channel.file.proto.ProtosFactory.TransactionEventFooter) {        return mergeFrom((org.apache.flume.channel.file.proto.ProtosFactory.TransactionEventFooter) other);    } else {        super.mergeFrom(other);        return this;    }}
0
public Builder mergeFrom(org.apache.flume.channel.file.proto.ProtosFactory.TransactionEventFooter other)
{    if (other == org.apache.flume.channel.file.proto.ProtosFactory.TransactionEventFooter.getDefaultInstance())        return this;    this.mergeUnknownFields(other.getUnknownFields());    return this;}
0
public final boolean isInitialized()
{    return true;}
0
public Builder mergeFrom(com.google.protobuf.CodedInputStream input, com.google.protobuf.ExtensionRegistryLite extensionRegistry) throws java.io.IOException
{    org.apache.flume.channel.file.proto.ProtosFactory.TransactionEventFooter parsedMessage = null;    try {        parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);    } catch (com.google.protobuf.InvalidProtocolBufferException e) {        parsedMessage = (org.apache.flume.channel.file.proto.ProtosFactory.TransactionEventFooter) e.getUnfinishedMessage();        throw e;    } finally {        if (parsedMessage != null) {            mergeFrom(parsedMessage);        }    }    return this;}
0
public static FlumeEvent getDefaultInstance()
{    return defaultInstance;}
0
public FlumeEvent getDefaultInstanceForType()
{    return defaultInstance;}
0
public final com.google.protobuf.UnknownFieldSet getUnknownFields()
{    return this.unknownFields;}
0
public static final com.google.protobuf.Descriptors.Descriptor getDescriptor()
{    return org.apache.flume.channel.file.proto.ProtosFactory.internal_static_FlumeEvent_descriptor;}
0
protected com.google.protobuf.GeneratedMessage.FieldAccessorTable internalGetFieldAccessorTable()
{    return org.apache.flume.channel.file.proto.ProtosFactory.internal_static_FlumeEvent_fieldAccessorTable.ensureFieldAccessorsInitialized(org.apache.flume.channel.file.proto.ProtosFactory.FlumeEvent.class, org.apache.flume.channel.file.proto.ProtosFactory.FlumeEvent.Builder.class);}
0
public FlumeEvent parsePartialFrom(com.google.protobuf.CodedInputStream input, com.google.protobuf.ExtensionRegistryLite extensionRegistry) throws com.google.protobuf.InvalidProtocolBufferException
{    return new FlumeEvent(input, extensionRegistry);}
0
public com.google.protobuf.Parser<FlumeEvent> getParserForType()
{    return PARSER;}
0
public java.util.List<org.apache.flume.channel.file.proto.ProtosFactory.FlumeEventHeader> getHeadersList()
{    return headers_;}
0
public java.util.List<? extends org.apache.flume.channel.file.proto.ProtosFactory.FlumeEventHeaderOrBuilder> getHeadersOrBuilderList()
{    return headers_;}
0
public int getHeadersCount()
{    return headers_.size();}
0
public org.apache.flume.channel.file.proto.ProtosFactory.FlumeEventHeader getHeaders(int index)
{    return headers_.get(index);}
0
public org.apache.flume.channel.file.proto.ProtosFactory.FlumeEventHeaderOrBuilder getHeadersOrBuilder(int index)
{    return headers_.get(index);}
0
public boolean hasBody()
{    return ((bitField0_ & 0x00000001) == 0x00000001);}
0
public com.google.protobuf.ByteString getBody()
{    return body_;}
0
private void initFields()
{    headers_ = java.util.Collections.emptyList();    body_ = com.google.protobuf.ByteString.EMPTY;}
0
public final boolean isInitialized()
{    byte isInitialized = memoizedIsInitialized;    if (isInitialized != -1)        return isInitialized == 1;    if (!hasBody()) {        memoizedIsInitialized = 0;        return false;    }    for (int i = 0; i < getHeadersCount(); i++) {        if (!getHeaders(i).isInitialized()) {            memoizedIsInitialized = 0;            return false;        }    }    memoizedIsInitialized = 1;    return true;}
0
public void writeTo(com.google.protobuf.CodedOutputStream output) throws java.io.IOException
{    getSerializedSize();    for (int i = 0; i < headers_.size(); i++) {        output.writeMessage(1, headers_.get(i));    }    if (((bitField0_ & 0x00000001) == 0x00000001)) {        output.writeBytes(2, body_);    }    getUnknownFields().writeTo(output);}
0
public int getSerializedSize()
{    int size = memoizedSerializedSize;    if (size != -1)        return size;    size = 0;    for (int i = 0; i < headers_.size(); i++) {        size += com.google.protobuf.CodedOutputStream.computeMessageSize(1, headers_.get(i));    }    if (((bitField0_ & 0x00000001) == 0x00000001)) {        size += com.google.protobuf.CodedOutputStream.computeBytesSize(2, body_);    }    size += getUnknownFields().getSerializedSize();    memoizedSerializedSize = size;    return size;}
0
protected java.lang.Object writeReplace() throws java.io.ObjectStreamException
{    return super.writeReplace();}
0
public static org.apache.flume.channel.file.proto.ProtosFactory.FlumeEvent parseFrom(com.google.protobuf.ByteString data) throws com.google.protobuf.InvalidProtocolBufferException
{    return PARSER.parseFrom(data);}
0
public static org.apache.flume.channel.file.proto.ProtosFactory.FlumeEvent parseFrom(com.google.protobuf.ByteString data, com.google.protobuf.ExtensionRegistryLite extensionRegistry) throws com.google.protobuf.InvalidProtocolBufferException
{    return PARSER.parseFrom(data, extensionRegistry);}
0
public static org.apache.flume.channel.file.proto.ProtosFactory.FlumeEvent parseFrom(byte[] data) throws com.google.protobuf.InvalidProtocolBufferException
{    return PARSER.parseFrom(data);}
0
public static org.apache.flume.channel.file.proto.ProtosFactory.FlumeEvent parseFrom(byte[] data, com.google.protobuf.ExtensionRegistryLite extensionRegistry) throws com.google.protobuf.InvalidProtocolBufferException
{    return PARSER.parseFrom(data, extensionRegistry);}
0
public static org.apache.flume.channel.file.proto.ProtosFactory.FlumeEvent parseFrom(java.io.InputStream input) throws java.io.IOException
{    return PARSER.parseFrom(input);}
0
public static org.apache.flume.channel.file.proto.ProtosFactory.FlumeEvent parseFrom(java.io.InputStream input, com.google.protobuf.ExtensionRegistryLite extensionRegistry) throws java.io.IOException
{    return PARSER.parseFrom(input, extensionRegistry);}
0
public static org.apache.flume.channel.file.proto.ProtosFactory.FlumeEvent parseDelimitedFrom(java.io.InputStream input) throws java.io.IOException
{    return PARSER.parseDelimitedFrom(input);}
0
public static org.apache.flume.channel.file.proto.ProtosFactory.FlumeEvent parseDelimitedFrom(java.io.InputStream input, com.google.protobuf.ExtensionRegistryLite extensionRegistry) throws java.io.IOException
{    return PARSER.parseDelimitedFrom(input, extensionRegistry);}
0
public static org.apache.flume.channel.file.proto.ProtosFactory.FlumeEvent parseFrom(com.google.protobuf.CodedInputStream input) throws java.io.IOException
{    return PARSER.parseFrom(input);}
0
public static org.apache.flume.channel.file.proto.ProtosFactory.FlumeEvent parseFrom(com.google.protobuf.CodedInputStream input, com.google.protobuf.ExtensionRegistryLite extensionRegistry) throws java.io.IOException
{    return PARSER.parseFrom(input, extensionRegistry);}
0
public static Builder newBuilder()
{    return Builder.create();}
0
public Builder newBuilderForType()
{    return newBuilder();}
0
public static Builder newBuilder(org.apache.flume.channel.file.proto.ProtosFactory.FlumeEvent prototype)
{    return newBuilder().mergeFrom(prototype);}
0
public Builder toBuilder()
{    return newBuilder(this);}
0
protected Builder newBuilderForType(com.google.protobuf.GeneratedMessage.BuilderParent parent)
{    Builder builder = new Builder(parent);    return builder;}
0
public static final com.google.protobuf.Descriptors.Descriptor getDescriptor()
{    return org.apache.flume.channel.file.proto.ProtosFactory.internal_static_FlumeEvent_descriptor;}
0
protected com.google.protobuf.GeneratedMessage.FieldAccessorTable internalGetFieldAccessorTable()
{    return org.apache.flume.channel.file.proto.ProtosFactory.internal_static_FlumeEvent_fieldAccessorTable.ensureFieldAccessorsInitialized(org.apache.flume.channel.file.proto.ProtosFactory.FlumeEvent.class, org.apache.flume.channel.file.proto.ProtosFactory.FlumeEvent.Builder.class);}
0
private void maybeForceBuilderInitialization()
{    if (com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders) {        getHeadersFieldBuilder();    }}
0
private static Builder create()
{    return new Builder();}
0
public Builder clear()
{    super.clear();    if (headersBuilder_ == null) {        headers_ = java.util.Collections.emptyList();        bitField0_ = (bitField0_ & ~0x00000001);    } else {        headersBuilder_.clear();    }    body_ = com.google.protobuf.ByteString.EMPTY;    bitField0_ = (bitField0_ & ~0x00000002);    return this;}
0
public Builder clone()
{    return create().mergeFrom(buildPartial());}
0
public com.google.protobuf.Descriptors.Descriptor getDescriptorForType()
{    return org.apache.flume.channel.file.proto.ProtosFactory.internal_static_FlumeEvent_descriptor;}
0
public org.apache.flume.channel.file.proto.ProtosFactory.FlumeEvent getDefaultInstanceForType()
{    return org.apache.flume.channel.file.proto.ProtosFactory.FlumeEvent.getDefaultInstance();}
0
public org.apache.flume.channel.file.proto.ProtosFactory.FlumeEvent build()
{    org.apache.flume.channel.file.proto.ProtosFactory.FlumeEvent result = buildPartial();    if (!result.isInitialized()) {        throw newUninitializedMessageException(result);    }    return result;}
0
public org.apache.flume.channel.file.proto.ProtosFactory.FlumeEvent buildPartial()
{    org.apache.flume.channel.file.proto.ProtosFactory.FlumeEvent result = new org.apache.flume.channel.file.proto.ProtosFactory.FlumeEvent(this);    int from_bitField0_ = bitField0_;    int to_bitField0_ = 0;    if (headersBuilder_ == null) {        if (((bitField0_ & 0x00000001) == 0x00000001)) {            headers_ = java.util.Collections.unmodifiableList(headers_);            bitField0_ = (bitField0_ & ~0x00000001);        }        result.headers_ = headers_;    } else {        result.headers_ = headersBuilder_.build();    }    if (((from_bitField0_ & 0x00000002) == 0x00000002)) {        to_bitField0_ |= 0x00000001;    }    result.body_ = body_;    result.bitField0_ = to_bitField0_;    onBuilt();    return result;}
0
public Builder mergeFrom(com.google.protobuf.Message other)
{    if (other instanceof org.apache.flume.channel.file.proto.ProtosFactory.FlumeEvent) {        return mergeFrom((org.apache.flume.channel.file.proto.ProtosFactory.FlumeEvent) other);    } else {        super.mergeFrom(other);        return this;    }}
0
public Builder mergeFrom(org.apache.flume.channel.file.proto.ProtosFactory.FlumeEvent other)
{    if (other == org.apache.flume.channel.file.proto.ProtosFactory.FlumeEvent.getDefaultInstance())        return this;    if (headersBuilder_ == null) {        if (!other.headers_.isEmpty()) {            if (headers_.isEmpty()) {                headers_ = other.headers_;                bitField0_ = (bitField0_ & ~0x00000001);            } else {                ensureHeadersIsMutable();                headers_.addAll(other.headers_);            }            onChanged();        }    } else {        if (!other.headers_.isEmpty()) {            if (headersBuilder_.isEmpty()) {                headersBuilder_.dispose();                headersBuilder_ = null;                headers_ = other.headers_;                bitField0_ = (bitField0_ & ~0x00000001);                headersBuilder_ = com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders ? getHeadersFieldBuilder() : null;            } else {                headersBuilder_.addAllMessages(other.headers_);            }        }    }    if (other.hasBody()) {        setBody(other.getBody());    }    this.mergeUnknownFields(other.getUnknownFields());    return this;}
0
public final boolean isInitialized()
{    if (!hasBody()) {        return false;    }    for (int i = 0; i < getHeadersCount(); i++) {        if (!getHeaders(i).isInitialized()) {            return false;        }    }    return true;}
0
public Builder mergeFrom(com.google.protobuf.CodedInputStream input, com.google.protobuf.ExtensionRegistryLite extensionRegistry) throws java.io.IOException
{    org.apache.flume.channel.file.proto.ProtosFactory.FlumeEvent parsedMessage = null;    try {        parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);    } catch (com.google.protobuf.InvalidProtocolBufferException e) {        parsedMessage = (org.apache.flume.channel.file.proto.ProtosFactory.FlumeEvent) e.getUnfinishedMessage();        throw e;    } finally {        if (parsedMessage != null) {            mergeFrom(parsedMessage);        }    }    return this;}
0
private void ensureHeadersIsMutable()
{    if (!((bitField0_ & 0x00000001) == 0x00000001)) {        headers_ = new java.util.ArrayList<org.apache.flume.channel.file.proto.ProtosFactory.FlumeEventHeader>(headers_);        bitField0_ |= 0x00000001;    }}
0
public java.util.List<org.apache.flume.channel.file.proto.ProtosFactory.FlumeEventHeader> getHeadersList()
{    if (headersBuilder_ == null) {        return java.util.Collections.unmodifiableList(headers_);    } else {        return headersBuilder_.getMessageList();    }}
0
public int getHeadersCount()
{    if (headersBuilder_ == null) {        return headers_.size();    } else {        return headersBuilder_.getCount();    }}
0
public org.apache.flume.channel.file.proto.ProtosFactory.FlumeEventHeader getHeaders(int index)
{    if (headersBuilder_ == null) {        return headers_.get(index);    } else {        return headersBuilder_.getMessage(index);    }}
0
public Builder setHeaders(int index, org.apache.flume.channel.file.proto.ProtosFactory.FlumeEventHeader value)
{    if (headersBuilder_ == null) {        if (value == null) {            throw new NullPointerException();        }        ensureHeadersIsMutable();        headers_.set(index, value);        onChanged();    } else {        headersBuilder_.setMessage(index, value);    }    return this;}
0
public Builder setHeaders(int index, org.apache.flume.channel.file.proto.ProtosFactory.FlumeEventHeader.Builder builderForValue)
{    if (headersBuilder_ == null) {        ensureHeadersIsMutable();        headers_.set(index, builderForValue.build());        onChanged();    } else {        headersBuilder_.setMessage(index, builderForValue.build());    }    return this;}
0
public Builder addHeaders(org.apache.flume.channel.file.proto.ProtosFactory.FlumeEventHeader value)
{    if (headersBuilder_ == null) {        if (value == null) {            throw new NullPointerException();        }        ensureHeadersIsMutable();        headers_.add(value);        onChanged();    } else {        headersBuilder_.addMessage(value);    }    return this;}
0
public Builder addHeaders(int index, org.apache.flume.channel.file.proto.ProtosFactory.FlumeEventHeader value)
{    if (headersBuilder_ == null) {        if (value == null) {            throw new NullPointerException();        }        ensureHeadersIsMutable();        headers_.add(index, value);        onChanged();    } else {        headersBuilder_.addMessage(index, value);    }    return this;}
0
public Builder addHeaders(org.apache.flume.channel.file.proto.ProtosFactory.FlumeEventHeader.Builder builderForValue)
{    if (headersBuilder_ == null) {        ensureHeadersIsMutable();        headers_.add(builderForValue.build());        onChanged();    } else {        headersBuilder_.addMessage(builderForValue.build());    }    return this;}
0
public Builder addHeaders(int index, org.apache.flume.channel.file.proto.ProtosFactory.FlumeEventHeader.Builder builderForValue)
{    if (headersBuilder_ == null) {        ensureHeadersIsMutable();        headers_.add(index, builderForValue.build());        onChanged();    } else {        headersBuilder_.addMessage(index, builderForValue.build());    }    return this;}
0
public Builder addAllHeaders(java.lang.Iterable<? extends org.apache.flume.channel.file.proto.ProtosFactory.FlumeEventHeader> values)
{    if (headersBuilder_ == null) {        ensureHeadersIsMutable();        super.addAll(values, headers_);        onChanged();    } else {        headersBuilder_.addAllMessages(values);    }    return this;}
0
public Builder clearHeaders()
{    if (headersBuilder_ == null) {        headers_ = java.util.Collections.emptyList();        bitField0_ = (bitField0_ & ~0x00000001);        onChanged();    } else {        headersBuilder_.clear();    }    return this;}
0
public Builder removeHeaders(int index)
{    if (headersBuilder_ == null) {        ensureHeadersIsMutable();        headers_.remove(index);        onChanged();    } else {        headersBuilder_.remove(index);    }    return this;}
0
public org.apache.flume.channel.file.proto.ProtosFactory.FlumeEventHeader.Builder getHeadersBuilder(int index)
{    return getHeadersFieldBuilder().getBuilder(index);}
0
public org.apache.flume.channel.file.proto.ProtosFactory.FlumeEventHeaderOrBuilder getHeadersOrBuilder(int index)
{    if (headersBuilder_ == null) {        return headers_.get(index);    } else {        return headersBuilder_.getMessageOrBuilder(index);    }}
0
public java.util.List<? extends org.apache.flume.channel.file.proto.ProtosFactory.FlumeEventHeaderOrBuilder> getHeadersOrBuilderList()
{    if (headersBuilder_ != null) {        return headersBuilder_.getMessageOrBuilderList();    } else {        return java.util.Collections.unmodifiableList(headers_);    }}
0
public org.apache.flume.channel.file.proto.ProtosFactory.FlumeEventHeader.Builder addHeadersBuilder()
{    return getHeadersFieldBuilder().addBuilder(org.apache.flume.channel.file.proto.ProtosFactory.FlumeEventHeader.getDefaultInstance());}
0
public org.apache.flume.channel.file.proto.ProtosFactory.FlumeEventHeader.Builder addHeadersBuilder(int index)
{    return getHeadersFieldBuilder().addBuilder(index, org.apache.flume.channel.file.proto.ProtosFactory.FlumeEventHeader.getDefaultInstance());}
0
public java.util.List<org.apache.flume.channel.file.proto.ProtosFactory.FlumeEventHeader.Builder> getHeadersBuilderList()
{    return getHeadersFieldBuilder().getBuilderList();}
0
private com.google.protobuf.RepeatedFieldBuilder<org.apache.flume.channel.file.proto.ProtosFactory.FlumeEventHeader, org.apache.flume.channel.file.proto.ProtosFactory.FlumeEventHeader.Builder, org.apache.flume.channel.file.proto.ProtosFactory.FlumeEventHeaderOrBuilder> getHeadersFieldBuilder()
{    if (headersBuilder_ == null) {        headersBuilder_ = new com.google.protobuf.RepeatedFieldBuilder<org.apache.flume.channel.file.proto.ProtosFactory.FlumeEventHeader, org.apache.flume.channel.file.proto.ProtosFactory.FlumeEventHeader.Builder, org.apache.flume.channel.file.proto.ProtosFactory.FlumeEventHeaderOrBuilder>(headers_, ((bitField0_ & 0x00000001) == 0x00000001), getParentForChildren(), isClean());        headers_ = null;    }    return headersBuilder_;}
0
public boolean hasBody()
{    return ((bitField0_ & 0x00000002) == 0x00000002);}
0
public com.google.protobuf.ByteString getBody()
{    return body_;}
0
public Builder setBody(com.google.protobuf.ByteString value)
{    if (value == null) {        throw new NullPointerException();    }    bitField0_ |= 0x00000002;    body_ = value;    onChanged();    return this;}
0
public Builder clearBody()
{    bitField0_ = (bitField0_ & ~0x00000002);    body_ = getDefaultInstance().getBody();    onChanged();    return this;}
0
public static FlumeEventHeader getDefaultInstance()
{    return defaultInstance;}
0
public FlumeEventHeader getDefaultInstanceForType()
{    return defaultInstance;}
0
public final com.google.protobuf.UnknownFieldSet getUnknownFields()
{    return this.unknownFields;}
0
public static final com.google.protobuf.Descriptors.Descriptor getDescriptor()
{    return org.apache.flume.channel.file.proto.ProtosFactory.internal_static_FlumeEventHeader_descriptor;}
0
protected com.google.protobuf.GeneratedMessage.FieldAccessorTable internalGetFieldAccessorTable()
{    return org.apache.flume.channel.file.proto.ProtosFactory.internal_static_FlumeEventHeader_fieldAccessorTable.ensureFieldAccessorsInitialized(org.apache.flume.channel.file.proto.ProtosFactory.FlumeEventHeader.class, org.apache.flume.channel.file.proto.ProtosFactory.FlumeEventHeader.Builder.class);}
0
public FlumeEventHeader parsePartialFrom(com.google.protobuf.CodedInputStream input, com.google.protobuf.ExtensionRegistryLite extensionRegistry) throws com.google.protobuf.InvalidProtocolBufferException
{    return new FlumeEventHeader(input, extensionRegistry);}
0
public com.google.protobuf.Parser<FlumeEventHeader> getParserForType()
{    return PARSER;}
0
public boolean hasKey()
{    return ((bitField0_ & 0x00000001) == 0x00000001);}
0
public java.lang.String getKey()
{    java.lang.Object ref = key_;    if (ref instanceof java.lang.String) {        return (java.lang.String) ref;    } else {        com.google.protobuf.ByteString bs = (com.google.protobuf.ByteString) ref;        java.lang.String s = bs.toStringUtf8();        if (bs.isValidUtf8()) {            key_ = s;        }        return s;    }}
0
public com.google.protobuf.ByteString getKeyBytes()
{    java.lang.Object ref = key_;    if (ref instanceof java.lang.String) {        com.google.protobuf.ByteString b = com.google.protobuf.ByteString.copyFromUtf8((java.lang.String) ref);        key_ = b;        return b;    } else {        return (com.google.protobuf.ByteString) ref;    }}
0
public boolean hasValue()
{    return ((bitField0_ & 0x00000002) == 0x00000002);}
0
public java.lang.String getValue()
{    java.lang.Object ref = value_;    if (ref instanceof java.lang.String) {        return (java.lang.String) ref;    } else {        com.google.protobuf.ByteString bs = (com.google.protobuf.ByteString) ref;        java.lang.String s = bs.toStringUtf8();        if (bs.isValidUtf8()) {            value_ = s;        }        return s;    }}
0
public com.google.protobuf.ByteString getValueBytes()
{    java.lang.Object ref = value_;    if (ref instanceof java.lang.String) {        com.google.protobuf.ByteString b = com.google.protobuf.ByteString.copyFromUtf8((java.lang.String) ref);        value_ = b;        return b;    } else {        return (com.google.protobuf.ByteString) ref;    }}
0
private void initFields()
{    key_ = "";    value_ = "";}
0
public final boolean isInitialized()
{    byte isInitialized = memoizedIsInitialized;    if (isInitialized != -1)        return isInitialized == 1;    if (!hasKey()) {        memoizedIsInitialized = 0;        return false;    }    memoizedIsInitialized = 1;    return true;}
0
public void writeTo(com.google.protobuf.CodedOutputStream output) throws java.io.IOException
{    getSerializedSize();    if (((bitField0_ & 0x00000001) == 0x00000001)) {        output.writeBytes(1, getKeyBytes());    }    if (((bitField0_ & 0x00000002) == 0x00000002)) {        output.writeBytes(2, getValueBytes());    }    getUnknownFields().writeTo(output);}
0
public int getSerializedSize()
{    int size = memoizedSerializedSize;    if (size != -1)        return size;    size = 0;    if (((bitField0_ & 0x00000001) == 0x00000001)) {        size += com.google.protobuf.CodedOutputStream.computeBytesSize(1, getKeyBytes());    }    if (((bitField0_ & 0x00000002) == 0x00000002)) {        size += com.google.protobuf.CodedOutputStream.computeBytesSize(2, getValueBytes());    }    size += getUnknownFields().getSerializedSize();    memoizedSerializedSize = size;    return size;}
0
protected java.lang.Object writeReplace() throws java.io.ObjectStreamException
{    return super.writeReplace();}
0
public static org.apache.flume.channel.file.proto.ProtosFactory.FlumeEventHeader parseFrom(com.google.protobuf.ByteString data) throws com.google.protobuf.InvalidProtocolBufferException
{    return PARSER.parseFrom(data);}
0
public static org.apache.flume.channel.file.proto.ProtosFactory.FlumeEventHeader parseFrom(com.google.protobuf.ByteString data, com.google.protobuf.ExtensionRegistryLite extensionRegistry) throws com.google.protobuf.InvalidProtocolBufferException
{    return PARSER.parseFrom(data, extensionRegistry);}
0
public static org.apache.flume.channel.file.proto.ProtosFactory.FlumeEventHeader parseFrom(byte[] data) throws com.google.protobuf.InvalidProtocolBufferException
{    return PARSER.parseFrom(data);}
0
public static org.apache.flume.channel.file.proto.ProtosFactory.FlumeEventHeader parseFrom(byte[] data, com.google.protobuf.ExtensionRegistryLite extensionRegistry) throws com.google.protobuf.InvalidProtocolBufferException
{    return PARSER.parseFrom(data, extensionRegistry);}
0
public static org.apache.flume.channel.file.proto.ProtosFactory.FlumeEventHeader parseFrom(java.io.InputStream input) throws java.io.IOException
{    return PARSER.parseFrom(input);}
0
public static org.apache.flume.channel.file.proto.ProtosFactory.FlumeEventHeader parseFrom(java.io.InputStream input, com.google.protobuf.ExtensionRegistryLite extensionRegistry) throws java.io.IOException
{    return PARSER.parseFrom(input, extensionRegistry);}
0
public static org.apache.flume.channel.file.proto.ProtosFactory.FlumeEventHeader parseDelimitedFrom(java.io.InputStream input) throws java.io.IOException
{    return PARSER.parseDelimitedFrom(input);}
0
public static org.apache.flume.channel.file.proto.ProtosFactory.FlumeEventHeader parseDelimitedFrom(java.io.InputStream input, com.google.protobuf.ExtensionRegistryLite extensionRegistry) throws java.io.IOException
{    return PARSER.parseDelimitedFrom(input, extensionRegistry);}
0
public static org.apache.flume.channel.file.proto.ProtosFactory.FlumeEventHeader parseFrom(com.google.protobuf.CodedInputStream input) throws java.io.IOException
{    return PARSER.parseFrom(input);}
0
public static org.apache.flume.channel.file.proto.ProtosFactory.FlumeEventHeader parseFrom(com.google.protobuf.CodedInputStream input, com.google.protobuf.ExtensionRegistryLite extensionRegistry) throws java.io.IOException
{    return PARSER.parseFrom(input, extensionRegistry);}
0
public static Builder newBuilder()
{    return Builder.create();}
0
public Builder newBuilderForType()
{    return newBuilder();}
0
public static Builder newBuilder(org.apache.flume.channel.file.proto.ProtosFactory.FlumeEventHeader prototype)
{    return newBuilder().mergeFrom(prototype);}
0
public Builder toBuilder()
{    return newBuilder(this);}
0
protected Builder newBuilderForType(com.google.protobuf.GeneratedMessage.BuilderParent parent)
{    Builder builder = new Builder(parent);    return builder;}
0
public static final com.google.protobuf.Descriptors.Descriptor getDescriptor()
{    return org.apache.flume.channel.file.proto.ProtosFactory.internal_static_FlumeEventHeader_descriptor;}
0
protected com.google.protobuf.GeneratedMessage.FieldAccessorTable internalGetFieldAccessorTable()
{    return org.apache.flume.channel.file.proto.ProtosFactory.internal_static_FlumeEventHeader_fieldAccessorTable.ensureFieldAccessorsInitialized(org.apache.flume.channel.file.proto.ProtosFactory.FlumeEventHeader.class, org.apache.flume.channel.file.proto.ProtosFactory.FlumeEventHeader.Builder.class);}
0
private void maybeForceBuilderInitialization()
{    if (com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders) {    }}
0
private static Builder create()
{    return new Builder();}
0
public Builder clear()
{    super.clear();    key_ = "";    bitField0_ = (bitField0_ & ~0x00000001);    value_ = "";    bitField0_ = (bitField0_ & ~0x00000002);    return this;}
0
public Builder clone()
{    return create().mergeFrom(buildPartial());}
0
public com.google.protobuf.Descriptors.Descriptor getDescriptorForType()
{    return org.apache.flume.channel.file.proto.ProtosFactory.internal_static_FlumeEventHeader_descriptor;}
0
public org.apache.flume.channel.file.proto.ProtosFactory.FlumeEventHeader getDefaultInstanceForType()
{    return org.apache.flume.channel.file.proto.ProtosFactory.FlumeEventHeader.getDefaultInstance();}
0
public org.apache.flume.channel.file.proto.ProtosFactory.FlumeEventHeader build()
{    org.apache.flume.channel.file.proto.ProtosFactory.FlumeEventHeader result = buildPartial();    if (!result.isInitialized()) {        throw newUninitializedMessageException(result);    }    return result;}
0
public org.apache.flume.channel.file.proto.ProtosFactory.FlumeEventHeader buildPartial()
{    org.apache.flume.channel.file.proto.ProtosFactory.FlumeEventHeader result = new org.apache.flume.channel.file.proto.ProtosFactory.FlumeEventHeader(this);    int from_bitField0_ = bitField0_;    int to_bitField0_ = 0;    if (((from_bitField0_ & 0x00000001) == 0x00000001)) {        to_bitField0_ |= 0x00000001;    }    result.key_ = key_;    if (((from_bitField0_ & 0x00000002) == 0x00000002)) {        to_bitField0_ |= 0x00000002;    }    result.value_ = value_;    result.bitField0_ = to_bitField0_;    onBuilt();    return result;}
0
public Builder mergeFrom(com.google.protobuf.Message other)
{    if (other instanceof org.apache.flume.channel.file.proto.ProtosFactory.FlumeEventHeader) {        return mergeFrom((org.apache.flume.channel.file.proto.ProtosFactory.FlumeEventHeader) other);    } else {        super.mergeFrom(other);        return this;    }}
0
public Builder mergeFrom(org.apache.flume.channel.file.proto.ProtosFactory.FlumeEventHeader other)
{    if (other == org.apache.flume.channel.file.proto.ProtosFactory.FlumeEventHeader.getDefaultInstance())        return this;    if (other.hasKey()) {        bitField0_ |= 0x00000001;        key_ = other.key_;        onChanged();    }    if (other.hasValue()) {        bitField0_ |= 0x00000002;        value_ = other.value_;        onChanged();    }    this.mergeUnknownFields(other.getUnknownFields());    return this;}
0
public final boolean isInitialized()
{    if (!hasKey()) {        return false;    }    return true;}
0
public Builder mergeFrom(com.google.protobuf.CodedInputStream input, com.google.protobuf.ExtensionRegistryLite extensionRegistry) throws java.io.IOException
{    org.apache.flume.channel.file.proto.ProtosFactory.FlumeEventHeader parsedMessage = null;    try {        parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);    } catch (com.google.protobuf.InvalidProtocolBufferException e) {        parsedMessage = (org.apache.flume.channel.file.proto.ProtosFactory.FlumeEventHeader) e.getUnfinishedMessage();        throw e;    } finally {        if (parsedMessage != null) {            mergeFrom(parsedMessage);        }    }    return this;}
0
public boolean hasKey()
{    return ((bitField0_ & 0x00000001) == 0x00000001);}
0
public java.lang.String getKey()
{    java.lang.Object ref = key_;    if (!(ref instanceof java.lang.String)) {        java.lang.String s = ((com.google.protobuf.ByteString) ref).toStringUtf8();        key_ = s;        return s;    } else {        return (java.lang.String) ref;    }}
0
public com.google.protobuf.ByteString getKeyBytes()
{    java.lang.Object ref = key_;    if (ref instanceof String) {        com.google.protobuf.ByteString b = com.google.protobuf.ByteString.copyFromUtf8((java.lang.String) ref);        key_ = b;        return b;    } else {        return (com.google.protobuf.ByteString) ref;    }}
0
public Builder setKey(java.lang.String value)
{    if (value == null) {        throw new NullPointerException();    }    bitField0_ |= 0x00000001;    key_ = value;    onChanged();    return this;}
0
public Builder clearKey()
{    bitField0_ = (bitField0_ & ~0x00000001);    key_ = getDefaultInstance().getKey();    onChanged();    return this;}
0
public Builder setKeyBytes(com.google.protobuf.ByteString value)
{    if (value == null) {        throw new NullPointerException();    }    bitField0_ |= 0x00000001;    key_ = value;    onChanged();    return this;}
0
public boolean hasValue()
{    return ((bitField0_ & 0x00000002) == 0x00000002);}
0
public java.lang.String getValue()
{    java.lang.Object ref = value_;    if (!(ref instanceof java.lang.String)) {        java.lang.String s = ((com.google.protobuf.ByteString) ref).toStringUtf8();        value_ = s;        return s;    } else {        return (java.lang.String) ref;    }}
0
public com.google.protobuf.ByteString getValueBytes()
{    java.lang.Object ref = value_;    if (ref instanceof String) {        com.google.protobuf.ByteString b = com.google.protobuf.ByteString.copyFromUtf8((java.lang.String) ref);        value_ = b;        return b;    } else {        return (com.google.protobuf.ByteString) ref;    }}
0
public Builder setValue(java.lang.String value)
{    if (value == null) {        throw new NullPointerException();    }    bitField0_ |= 0x00000002;    value_ = value;    onChanged();    return this;}
0
public Builder clearValue()
{    bitField0_ = (bitField0_ & ~0x00000002);    value_ = getDefaultInstance().getValue();    onChanged();    return this;}
0
public Builder setValueBytes(com.google.protobuf.ByteString value)
{    if (value == null) {        throw new NullPointerException();    }    bitField0_ |= 0x00000002;    value_ = value;    onChanged();    return this;}
0
public static com.google.protobuf.Descriptors.FileDescriptor getDescriptor()
{    return descriptor;}
0
public com.google.protobuf.ExtensionRegistry assignDescriptors(com.google.protobuf.Descriptors.FileDescriptor root)
{    descriptor = root;    internal_static_Checkpoint_descriptor = getDescriptor().getMessageTypes().get(0);    internal_static_Checkpoint_fieldAccessorTable = new com.google.protobuf.GeneratedMessage.FieldAccessorTable(internal_static_Checkpoint_descriptor, new java.lang.String[] { "Version", "WriteOrderID", "QueueSize", "QueueHead", "ActiveLogs" });    internal_static_ActiveLog_descriptor = getDescriptor().getMessageTypes().get(1);    internal_static_ActiveLog_fieldAccessorTable = new com.google.protobuf.GeneratedMessage.FieldAccessorTable(internal_static_ActiveLog_descriptor, new java.lang.String[] { "LogFileID", "Count" });    internal_static_LogFileMetaData_descriptor = getDescriptor().getMessageTypes().get(2);    internal_static_LogFileMetaData_fieldAccessorTable = new com.google.protobuf.GeneratedMessage.FieldAccessorTable(internal_static_LogFileMetaData_descriptor, new java.lang.String[] { "Version", "LogFileID", "CheckpointPosition", "CheckpointWriteOrderID", "Encryption", "BackupCheckpointPosition", "BackupCheckpointWriteOrderID" });    internal_static_LogFileEncryption_descriptor = getDescriptor().getMessageTypes().get(3);    internal_static_LogFileEncryption_fieldAccessorTable = new com.google.protobuf.GeneratedMessage.FieldAccessorTable(internal_static_LogFileEncryption_descriptor, new java.lang.String[] { "CipherProvider", "KeyAlias", "Parameters" });    internal_static_TransactionEventHeader_descriptor = getDescriptor().getMessageTypes().get(4);    internal_static_TransactionEventHeader_fieldAccessorTable = new com.google.protobuf.GeneratedMessage.FieldAccessorTable(internal_static_TransactionEventHeader_descriptor, new java.lang.String[] { "Type", "TransactionID", "WriteOrderID" });    internal_static_Put_descriptor = getDescriptor().getMessageTypes().get(5);    internal_static_Put_fieldAccessorTable = new com.google.protobuf.GeneratedMessage.FieldAccessorTable(internal_static_Put_descriptor, new java.lang.String[] { "Event", "Checksum" });    internal_static_Take_descriptor = getDescriptor().getMessageTypes().get(6);    internal_static_Take_fieldAccessorTable = new com.google.protobuf.GeneratedMessage.FieldAccessorTable(internal_static_Take_descriptor, new java.lang.String[] { "FileID", "Offset" });    internal_static_Rollback_descriptor = getDescriptor().getMessageTypes().get(7);    internal_static_Rollback_fieldAccessorTable = new com.google.protobuf.GeneratedMessage.FieldAccessorTable(internal_static_Rollback_descriptor, new java.lang.String[] {});    internal_static_Commit_descriptor = getDescriptor().getMessageTypes().get(8);    internal_static_Commit_fieldAccessorTable = new com.google.protobuf.GeneratedMessage.FieldAccessorTable(internal_static_Commit_descriptor, new java.lang.String[] { "Type" });    internal_static_TransactionEventFooter_descriptor = getDescriptor().getMessageTypes().get(9);    internal_static_TransactionEventFooter_fieldAccessorTable = new com.google.protobuf.GeneratedMessage.FieldAccessorTable(internal_static_TransactionEventFooter_descriptor, new java.lang.String[] {});    internal_static_FlumeEvent_descriptor = getDescriptor().getMessageTypes().get(10);    internal_static_FlumeEvent_fieldAccessorTable = new com.google.protobuf.GeneratedMessage.FieldAccessorTable(internal_static_FlumeEvent_descriptor, new java.lang.String[] { "Headers", "Body" });    internal_static_FlumeEventHeader_descriptor = getDescriptor().getMessageTypes().get(11);    internal_static_FlumeEventHeader_fieldAccessorTable = new com.google.protobuf.GeneratedMessage.FieldAccessorTable(internal_static_FlumeEventHeader_descriptor, new java.lang.String[] { "Key", "Value" });    return null;}
0
 FlumeEvent getEvent()
{    return event;}
0
public void readFields(DataInput in) throws IOException
{    super.readFields(in);    event = FlumeEvent.from(in);}
0
public void write(DataOutput out) throws IOException
{    super.write(out);    event.write(out);}
0
 void writeProtos(OutputStream out) throws IOException
{    ProtosFactory.Put.Builder putBuilder = ProtosFactory.Put.newBuilder();    ProtosFactory.FlumeEvent.Builder eventBuilder = ProtosFactory.FlumeEvent.newBuilder();    Map<String, String> headers = event.getHeaders();    ProtosFactory.FlumeEventHeader.Builder headerBuilder = ProtosFactory.FlumeEventHeader.newBuilder();    if (headers != null) {        for (String key : headers.keySet()) {            String value = headers.get(key);            headerBuilder.clear();            headerBuilder.setKey(key);            if (value != null) {                headerBuilder.setValue(value);            }            eventBuilder.addHeaders(headerBuilder.build());        }    }    eventBuilder.setBody(ByteString.copyFrom(event.getBody()));    ProtosFactory.FlumeEvent protoEvent = eventBuilder.build();    putBuilder.setEvent(protoEvent);    putBuilder.setChecksum(calculateChecksum(event.getBody()));    putBuilder.build().writeDelimitedTo(out);}
0
 void readProtos(InputStream in) throws IOException, CorruptEventException
{    ProtosFactory.Put put = Preconditions.checkNotNull(ProtosFactory.Put.parseDelimitedFrom(in), "Put cannot be null");    Map<String, String> headers = Maps.newHashMap();    ProtosFactory.FlumeEvent protosEvent = put.getEvent();    for (ProtosFactory.FlumeEventHeader header : protosEvent.getHeadersList()) {        headers.put(header.getKey(), header.getValue());    }    byte[] eventBody = protosEvent.getBody().toByteArray();    if (put.hasChecksum()) {        long eventBodyChecksum = calculateChecksum(eventBody);        if (eventBodyChecksum != put.getChecksum()) {            throw new CorruptEventException("Expected checksum for event was " + eventBodyChecksum + " but the checksum of the event is " + put.getChecksum());        }    }        event = new FlumeEvent(headers, eventBody);}
0
protected long calculateChecksum(byte[] body)
{    checksum.reset();    checksum.update(body, 0, body.length);    return checksum.getValue();}
0
public short getRecordType()
{    return Type.PUT.get();}
0
public String toString()
{    StringBuilder builder = new StringBuilder();    builder.append("Put [event=");    builder.append(event);    builder.append(", getLogWriteOrderID()=");    builder.append(getLogWriteOrderID());    builder.append(", getTransactionID()=");    builder.append(getTransactionID());    builder.append("]");    return builder.toString();}
0
public int getReadCount()
{    return readCount;}
0
public int getPutCount()
{    return putCount;}
0
public int getTakeCount()
{    return takeCount;}
0
public int getCommitCount()
{    return commitCount;}
0
public int getRollbackCount()
{    return rollbackCount;}
0
 void replayLogv1(List<File> logs) throws Exception
{    int total = 0;    int count = 0;    MultiMap transactionMap = new MultiValueMap();        SetMultimap<Long, Long> inflightPuts = queue.deserializeInflightPuts();    for (Long txnID : inflightPuts.keySet()) {        Set<Long> eventPointers = inflightPuts.get(txnID);        for (Long eventPointer : eventPointers) {            transactionMap.put(txnID, FlumeEventPointer.fromLong(eventPointer));        }    }    SetMultimap<Long, Long> inflightTakes = queue.deserializeInflightTakes();        for (File log : logs) {                LogFile.SequentialReader reader = null;        try {            reader = LogFileFactory.getSequentialReader(log, encryptionKeyProvider, fsyncPerTransaction);            reader.skipToLastCheckpointPosition(queue.getLogWriteOrderID());            LogRecord entry;            FlumeEventPointer ptr;                                    int fileId = reader.getLogFileID();            while ((entry = reader.next()) != null) {                int offset = entry.getOffset();                TransactionEventRecord record = entry.getEvent();                short type = record.getRecordType();                long trans = record.getTransactionID();                readCount++;                if (record.getLogWriteOrderID() > lastCheckpoint) {                    if (type == TransactionEventRecord.Type.PUT.get()) {                        putCount++;                        ptr = new FlumeEventPointer(fileId, offset);                        transactionMap.put(trans, ptr);                    } else if (type == TransactionEventRecord.Type.TAKE.get()) {                        takeCount++;                        Take take = (Take) record;                        ptr = new FlumeEventPointer(take.getFileID(), take.getOffset());                        transactionMap.put(trans, ptr);                    } else if (type == TransactionEventRecord.Type.ROLLBACK.get()) {                        rollbackCount++;                        transactionMap.remove(trans);                    } else if (type == TransactionEventRecord.Type.COMMIT.get()) {                        commitCount++;                        @SuppressWarnings("unchecked")                        Collection<FlumeEventPointer> pointers = (Collection<FlumeEventPointer>) transactionMap.remove(trans);                        if (((Commit) record).getType() == TransactionEventRecord.Type.TAKE.get()) {                            if (inflightTakes.containsKey(trans)) {                                if (pointers == null) {                                    pointers = Sets.newHashSet();                                }                                Set<Long> takes = inflightTakes.removeAll(trans);                                Iterator<Long> it = takes.iterator();                                while (it.hasNext()) {                                    Long take = it.next();                                    pointers.add(FlumeEventPointer.fromLong(take));                                }                            }                        }                        if (pointers != null && pointers.size() > 0) {                            processCommit(((Commit) record).getType(), pointers);                            count += pointers.size();                        }                    } else {                        Preconditions.checkArgument(false, "Unknown record type: " + Integer.toHexString(type));                    }                } else {                    skipCount++;                }            }                        if (LOG.isDebugEnabled()) {                            }        } catch (EOFException e) {                    } finally {            total += count;            count = 0;            if (reader != null) {                reader.close();            }        }    }            int uncommittedTakes = 0;    for (Long inflightTxnId : inflightTakes.keySet()) {        Set<Long> inflightUncommittedTakes = inflightTakes.get(inflightTxnId);        for (Long inflightUncommittedTake : inflightUncommittedTakes) {            queue.addHead(FlumeEventPointer.fromLong(inflightUncommittedTake));            uncommittedTakes++;        }    }    inflightTakes.clear();    count += uncommittedTakes;    int pendingTakesSize = pendingTakes.size();    if (pendingTakesSize > 0) {        String msg = "Pending takes " + pendingTakesSize + " exist after the end of replay";        if (LOG.isDebugEnabled()) {            for (Long pointer : pendingTakes) {                            }        } else {                    }    }    }
1
 void replayLog(List<File> logs) throws Exception
{    int count = 0;    MultiMap transactionMap = new MultiValueMap();        long transactionIDSeed = lastCheckpoint, writeOrderIDSeed = lastCheckpoint;                SetMultimap<Long, Long> inflightPuts = queue.deserializeInflightPuts();    for (Long txnID : inflightPuts.keySet()) {        Set<Long> eventPointers = inflightPuts.get(txnID);        for (Long eventPointer : eventPointers) {            transactionMap.put(txnID, FlumeEventPointer.fromLong(eventPointer));        }    }    SetMultimap<Long, Long> inflightTakes = queue.deserializeInflightTakes();    try {        for (File log : logs) {                        try {                LogFile.SequentialReader reader = LogFileFactory.getSequentialReader(log, encryptionKeyProvider, fsyncPerTransaction);                reader.skipToLastCheckpointPosition(queue.getLogWriteOrderID());                Preconditions.checkState(!readers.containsKey(reader.getLogFileID()), "Readers " + readers + " already contains " + reader.getLogFileID());                readers.put(reader.getLogFileID(), reader);                LogRecord logRecord = reader.next();                if (logRecord == null) {                    readers.remove(reader.getLogFileID());                    reader.close();                } else {                    logRecordBuffer.add(logRecord);                }            } catch (EOFException e) {                            }        }        LogRecord entry = null;        FlumeEventPointer ptr = null;        while ((entry = next()) != null) {                                    int fileId = entry.getFileID();            int offset = entry.getOffset();            TransactionEventRecord record = entry.getEvent();            short type = record.getRecordType();            long trans = record.getTransactionID();            transactionIDSeed = Math.max(transactionIDSeed, trans);            writeOrderIDSeed = Math.max(writeOrderIDSeed, record.getLogWriteOrderID());            readCount++;            if (readCount % 10000 == 0 && readCount > 0) {                            }            if (record.getLogWriteOrderID() > lastCheckpoint) {                if (type == TransactionEventRecord.Type.PUT.get()) {                    putCount++;                    ptr = new FlumeEventPointer(fileId, offset);                    transactionMap.put(trans, ptr);                } else if (type == TransactionEventRecord.Type.TAKE.get()) {                    takeCount++;                    Take take = (Take) record;                    ptr = new FlumeEventPointer(take.getFileID(), take.getOffset());                    transactionMap.put(trans, ptr);                } else if (type == TransactionEventRecord.Type.ROLLBACK.get()) {                    rollbackCount++;                    transactionMap.remove(trans);                } else if (type == TransactionEventRecord.Type.COMMIT.get()) {                    commitCount++;                    @SuppressWarnings("unchecked")                    Collection<FlumeEventPointer> pointers = (Collection<FlumeEventPointer>) transactionMap.remove(trans);                    if (((Commit) record).getType() == TransactionEventRecord.Type.TAKE.get()) {                        if (inflightTakes.containsKey(trans)) {                            if (pointers == null) {                                pointers = Sets.newHashSet();                            }                            Set<Long> takes = inflightTakes.removeAll(trans);                            Iterator<Long> it = takes.iterator();                            while (it.hasNext()) {                                Long take = it.next();                                pointers.add(FlumeEventPointer.fromLong(take));                            }                        }                    }                    if (pointers != null && pointers.size() > 0) {                        processCommit(((Commit) record).getType(), pointers);                        count += pointers.size();                    }                } else {                    Preconditions.checkArgument(false, "Unknown record type: " + Integer.toHexString(type));                }            } else {                skipCount++;            }        }                queue.replayComplete();    } finally {        TransactionIDOracle.setSeed(transactionIDSeed);        WriteOrderOracle.setSeed(writeOrderIDSeed);        for (LogFile.SequentialReader reader : readers.values()) {            if (reader != null) {                reader.close();            }        }    }            int uncommittedTakes = 0;    for (Long inflightTxnId : inflightTakes.keySet()) {        Set<Long> inflightUncommittedTakes = inflightTakes.get(inflightTxnId);        for (Long inflightUncommittedTake : inflightUncommittedTakes) {            queue.addHead(FlumeEventPointer.fromLong(inflightUncommittedTake));            uncommittedTakes++;        }    }    inflightTakes.clear();    count += uncommittedTakes;    int pendingTakesSize = pendingTakes.size();    if (pendingTakesSize > 0) {            }}
1
private LogRecord next() throws IOException, CorruptEventException
{    LogRecord resultLogRecord = logRecordBuffer.poll();    if (resultLogRecord != null) {                LogFile.SequentialReader reader = readers.get(resultLogRecord.getFileID());        LogRecord nextLogRecord;        if ((nextLogRecord = reader.next()) != null) {            logRecordBuffer.add(nextLogRecord);        }    }    return resultLogRecord;}
0
private void processCommit(short type, Collection<FlumeEventPointer> pointers)
{    if (type == TransactionEventRecord.Type.PUT.get()) {        for (FlumeEventPointer pointer : pointers) {            if (!queue.addTail(pointer)) {                throw new IllegalStateException("Unable to add " + pointer + ". Queue depth = " + queue.getSize() + ", Capacity = " + queue.getCapacity());            }            if (pendingTakes.remove(pointer.toLong())) {                Preconditions.checkState(queue.remove(pointer), "Take was pending and pointer was successfully added to the" + " queue but could not be removed: " + pointer);            }        }    } else if (type == TransactionEventRecord.Type.TAKE.get()) {        for (FlumeEventPointer pointer : pointers) {            boolean removed = queue.remove(pointer);            if (!removed) {                pendingTakes.add(pointer.toLong());            }        }    } else {        Preconditions.checkArgument(false, "Unknown record type: " + Integer.toHexString(type));    }}
0
public void readFields(DataInput in) throws IOException
{    super.readFields(in);}
0
public void write(DataOutput out) throws IOException
{    super.write(out);}
0
 void writeProtos(OutputStream out) throws IOException
{    ProtosFactory.Rollback.Builder rollbackBuilder = ProtosFactory.Rollback.newBuilder();    rollbackBuilder.build().writeDelimitedTo(out);}
0
 void readProtos(InputStream in) throws IOException
{    @SuppressWarnings("unused")    ProtosFactory.Rollback rollback = Preconditions.checkNotNull(ProtosFactory.Rollback.parseDelimitedFrom(in), "Rollback cannot be null");}
0
 short getRecordType()
{    return Type.ROLLBACK.get();}
0
public String toString()
{    StringBuilder builder = new StringBuilder();    builder.append("Rollback [getLogWriteOrderID()=");    builder.append(getLogWriteOrderID());    builder.append(", getTransactionID()=");    builder.append(getTransactionID());    builder.append("]");    return builder.toString();}
0
 static File getMetaDataTempFile(File metaDataFile)
{    String metaDataFileName = metaDataFile.getName() + METADATA_TMP_FILENAME;    return new File(metaDataFile.getParentFile(), metaDataFileName);}
0
 static File getMetaDataFile(File file)
{    String metaDataFileName = file.getName() + METADATA_FILENAME;    return new File(file.getParentFile(), metaDataFileName);}
0
 static File getOldMetaDataFile(File file)
{    String oldMetaDataFileName = file.getName() + OLD_METADATA_FILENAME;    return new File(file.getParentFile(), oldMetaDataFileName);}
0
 static boolean deleteAllFiles(File checkpointDir, @Nullable Set<String> excludes)
{    if (!checkpointDir.isDirectory()) {        return false;    }    File[] files = checkpointDir.listFiles();    if (files == null) {        return false;    }    StringBuilder builder;    if (files.length == 0) {        return true;    } else {        builder = new StringBuilder("Deleted the following files: ");    }    if (excludes == null) {        excludes = Collections.emptySet();    }    for (File file : files) {        if (excludes.contains(file.getName())) {                        continue;        }        if (!FileUtils.deleteQuietly(file)) {                                    return false;        }        builder.append(", ").append(file.getName());    }    builder.append(".");        return true;}
1
public static boolean copyFile(File from, File to) throws IOException
{    Preconditions.checkNotNull(from, "Source file is null, file copy failed.");    Preconditions.checkNotNull(to, "Destination file is null, " + "file copy failed.");    Preconditions.checkState(from.exists(), "Source file: " + from.toString() + " does not exist.");    Preconditions.checkState(!to.exists(), "Destination file: " + to.toString() + " unexpectedly exists.");    BufferedInputStream in = null;        RandomAccessFile out = null;    try {        in = new BufferedInputStream(new FileInputStream(from));        out = new RandomAccessFile(to, "rw");        byte[] buf = new byte[FILE_BUFFER_SIZE];        int total = 0;        while (true) {            int read = in.read(buf);            if (read == -1) {                break;            }            out.write(buf, 0, read);            total += read;        }        out.getFD().sync();        Preconditions.checkState(total == from.length(), "The size of the origin file and destination file are not equal.");        return true;    } catch (Exception ex) {                Throwables.propagate(ex);    } finally {        Throwable th = null;        try {            if (in != null) {                in.close();            }        } catch (Throwable ex) {                        th = ex;        }        try {            if (out != null) {                out.close();            }        } catch (IOException ex) {                        Throwables.propagate(ex);        }        if (th != null) {            Throwables.propagate(th);        }    }        throw new IOException("Copying file: " + from.toString() + " to: " + to.toString() + " may have failed.");}
1
public static boolean compressFile(File uncompressed, File compressed) throws IOException
{    Preconditions.checkNotNull(uncompressed, "Source file is null, compression failed.");    Preconditions.checkNotNull(compressed, "Destination file is null, compression failed.");    Preconditions.checkState(uncompressed.exists(), "Source file: " + uncompressed.toString() + " does not exist.");    Preconditions.checkState(!compressed.exists(), "Compressed file: " + compressed.toString() + " unexpectedly " + "exists.");    BufferedInputStream in = null;    FileOutputStream out = null;    SnappyOutputStream snappyOut = null;    try {        in = new BufferedInputStream(new FileInputStream(uncompressed));        out = new FileOutputStream(compressed);        snappyOut = new SnappyOutputStream(out);        byte[] buf = new byte[FILE_BUFFER_SIZE];        while (true) {            int read = in.read(buf);            if (read == -1) {                break;            }            snappyOut.write(buf, 0, read);        }        out.getFD().sync();        return true;    } catch (Exception ex) {                Throwables.propagate(ex);    } finally {        Throwable th = null;        try {            if (in != null) {                in.close();            }        } catch (Throwable ex) {                        th = ex;        }        try {            if (snappyOut != null) {                snappyOut.close();            }        } catch (IOException ex) {                        Throwables.propagate(ex);        }        if (th != null) {            Throwables.propagate(th);        }    }        throw new IOException("Copying file: " + uncompressed.toString() + " to: " + compressed.toString() + " may have failed.");}
1
public static boolean decompressFile(File compressed, File decompressed) throws IOException
{    Preconditions.checkNotNull(compressed, "Source file is null, decompression failed.");    Preconditions.checkNotNull(decompressed, "Destination file is " + "null, decompression failed.");    Preconditions.checkState(compressed.exists(), "Source file: " + compressed.toString() + " does not exist.");    Preconditions.checkState(!decompressed.exists(), "Decompressed file: " + decompressed.toString() + " unexpectedly exists.");    BufferedInputStream in = null;    SnappyInputStream snappyIn = null;    FileOutputStream out = null;    try {        in = new BufferedInputStream(new FileInputStream(compressed));        snappyIn = new SnappyInputStream(in);        out = new FileOutputStream(decompressed);        byte[] buf = new byte[FILE_BUFFER_SIZE];        while (true) {            int read = snappyIn.read(buf);            if (read == -1) {                break;            }            out.write(buf, 0, read);        }        out.getFD().sync();        return true;    } catch (Exception ex) {                Throwables.propagate(ex);    } finally {        Throwable th = null;        try {            if (in != null) {                in.close();            }        } catch (Throwable ex) {                        th = ex;        }        try {            if (snappyIn != null) {                snappyIn.close();            }        } catch (IOException ex) {                        Throwables.propagate(ex);        }        if (th != null) {            Throwables.propagate(th);        }    }        throw new IOException("Decompressing file: " + compressed.toString() + " to: " + decompressed.toString() + " may have failed.");}
1
 int getOffset()
{    return offset;}
0
 int getFileID()
{    return fileID;}
0
public void readFields(DataInput in) throws IOException
{    super.readFields(in);    offset = in.readInt();    fileID = in.readInt();}
0
public void write(DataOutput out) throws IOException
{    super.write(out);    out.writeInt(offset);    out.writeInt(fileID);}
0
 void writeProtos(OutputStream out) throws IOException
{    ProtosFactory.Take.Builder takeBuilder = ProtosFactory.Take.newBuilder();    takeBuilder.setFileID(fileID);    takeBuilder.setOffset(offset);    takeBuilder.build().writeDelimitedTo(out);}
0
 void readProtos(InputStream in) throws IOException
{    ProtosFactory.Take take = Preconditions.checkNotNull(ProtosFactory.Take.parseDelimitedFrom(in), "Take cannot be null");    fileID = take.getFileID();    offset = take.getOffset();}
0
 short getRecordType()
{    return Type.TAKE.get();}
0
public String toString()
{    StringBuilder builder = new StringBuilder();    builder.append("Take [offset=");    builder.append(offset);    builder.append(", fileID=");    builder.append(fileID);    builder.append(", getLogWriteOrderID()=");    builder.append(getLogWriteOrderID());    builder.append(", getTransactionID()=");    builder.append(getTransactionID());    builder.append("]");    return builder.toString();}
0
public void readFields(DataInput in) throws IOException
{}
0
public void write(DataOutput out) throws IOException
{}
0
 long getLogWriteOrderID()
{    return logWriteOrderID;}
0
 long getTransactionID()
{    return transactionID;}
0
public short get()
{    return id;}
0
 static ByteBuffer toByteBufferV2(TransactionEventRecord record)
{    ByteArrayOutputStream byteOutput = new ByteArrayOutputStream(512);    DataOutputStream dataOutput = new DataOutputStream(byteOutput);    try {        dataOutput.writeInt(MAGIC_HEADER);        dataOutput.writeShort(record.getRecordType());        dataOutput.writeLong(record.getTransactionID());        dataOutput.writeLong(record.getLogWriteOrderID());        record.write(dataOutput);        dataOutput.flush();                return ByteBuffer.wrap(byteOutput.toByteArray());    } catch (IOException e) {                throw Throwables.propagate(e);    } finally {        if (dataOutput != null) {            try {                dataOutput.close();            } catch (IOException e) {                            }        }    }}
1
 static TransactionEventRecord fromDataInputV2(DataInput in) throws IOException
{    int header = in.readInt();    if (header != MAGIC_HEADER) {        throw new IOException("Header " + Integer.toHexString(header) + " is not the required value: " + Integer.toHexString(MAGIC_HEADER));    }    short type = in.readShort();    long transactionID = in.readLong();    long writeOrderID = in.readLong();    TransactionEventRecord entry = newRecordForType(type, transactionID, writeOrderID);    entry.readFields(in);    return entry;}
0
 static ByteBuffer toByteBuffer(TransactionEventRecord record)
{    ByteArrayOutputStream byteOutput = new ByteArrayOutputStream(512);    try {        ProtosFactory.TransactionEventHeader.Builder headerBuilder = ProtosFactory.TransactionEventHeader.newBuilder();        headerBuilder.setType(record.getRecordType());        headerBuilder.setTransactionID(record.getTransactionID());        headerBuilder.setWriteOrderID(record.getLogWriteOrderID());        headerBuilder.build().writeDelimitedTo(byteOutput);        record.writeProtos(byteOutput);        ProtosFactory.TransactionEventFooter footer = ProtosFactory.TransactionEventFooter.newBuilder().build();        footer.writeDelimitedTo(byteOutput);        return ByteBuffer.wrap(byteOutput.toByteArray());    } catch (IOException e) {        throw Throwables.propagate(e);    } finally {        if (byteOutput != null) {            try {                byteOutput.close();            } catch (IOException e) {                            }        }    }}
1
 static TransactionEventRecord fromByteArray(byte[] buffer) throws IOException, CorruptEventException
{    ByteArrayInputStream in = new ByteArrayInputStream(buffer);    try {        ProtosFactory.TransactionEventHeader header = Preconditions.checkNotNull(ProtosFactory.TransactionEventHeader.parseDelimitedFrom(in), "Header cannot be null");        short type = (short) header.getType();        long transactionID = header.getTransactionID();        long writeOrderID = header.getWriteOrderID();        TransactionEventRecord transactionEvent = newRecordForType(type, transactionID, writeOrderID);        transactionEvent.readProtos(in);        @SuppressWarnings("unused")        ProtosFactory.TransactionEventFooter footer = Preconditions.checkNotNull(ProtosFactory.TransactionEventFooter.parseDelimitedFrom(in), "Footer cannot be null");        return transactionEvent;    } catch (InvalidProtocolBufferException ex) {        throw new CorruptEventException("Could not parse event from data file.", ex);    } finally {        try {            in.close();        } catch (IOException e) {                    }    }}
1
 static String getName(short type)
{    Constructor<? extends TransactionEventRecord> constructor = TYPES.get(type);    Preconditions.checkNotNull(constructor, "Unknown action " + Integer.toHexString(type));    return constructor.getDeclaringClass().getSimpleName();}
0
private static TransactionEventRecord newRecordForType(short type, long transactionID, long writeOrderID)
{    Constructor<? extends TransactionEventRecord> constructor = TYPES.get(type);    Preconditions.checkNotNull(constructor, "Unknown action " + Integer.toHexString(type));    try {        return constructor.newInstance(transactionID, writeOrderID);    } catch (Exception e) {        throw Throwables.propagate(e);    }}
0
public static void setSeed(long highest)
{    long previous;    while (highest > (previous = TRANSACTION_ID.get())) {        TRANSACTION_ID.compareAndSet(previous, highest);    }}
0
public static long next()
{    return TRANSACTION_ID.incrementAndGet();}
0
public static void writeVInt(DataOutput stream, int i) throws IOException
{    writeVLong(stream, i);}
0
public static void writeVLong(DataOutput stream, long i) throws IOException
{    if (i >= -112 && i <= 127) {        stream.writeByte((byte) i);        return;    }    int len = -112;    if (i < 0) {                i ^= -1L;        len = -120;    }    long tmp = i;    while (tmp != 0) {        tmp = tmp >> 8;        len--;    }    stream.writeByte((byte) len);    len = (len < -120) ? -(len + 120) : -(len + 112);    for (int idx = len; idx != 0; idx--) {        int shiftbits = (idx - 1) * 8;        long mask = 0xFFL << shiftbits;        stream.writeByte((byte) ((i & mask) >> shiftbits));    }}
0
public static long readVLong(DataInput stream) throws IOException
{    byte firstByte = stream.readByte();    int len = decodeVIntSize(firstByte);    if (len == 1) {        return firstByte;    }    long i = 0;    for (int idx = 0; idx < len - 1; idx++) {        byte b = stream.readByte();        i = i << 8;        i = i | (b & 0xFF);    }    return (isNegativeVInt(firstByte) ? (i ^ -1L) : i);}
0
public static int readVInt(DataInput stream) throws IOException
{    long n = readVLong(stream);    if ((n > Integer.MAX_VALUE) || (n < Integer.MIN_VALUE)) {        throw new IOException("value too long to fit in integer");    }    return (int) n;}
0
public static boolean isNegativeVInt(byte value)
{    return value < -120 || (value >= -112 && value < 0);}
0
public static int decodeVIntSize(byte value)
{    if (value >= -112) {        return 1;    } else if (value < -120) {        return -119 - value;    }    return -111 - value;}
0
public static void setSeed(long highest)
{    long previous;    while (highest > (previous = WRITER_ORDERER.get())) {        WRITER_ORDERER.compareAndSet(previous, highest);    }}
0
public static long next()
{    return WRITER_ORDERER.incrementAndGet();}
0
public void run()
{    run = true;    while (run && count < until) {        boolean error = true;        try {            if (Sink.Status.READY.equals(sink.process())) {                count++;                error = false;            }        } catch (Exception ex) {            errors.add(ex);        }        if (error) {            try {                Thread.sleep(1000L);            } catch (InterruptedException e) {            }        }    }}
0
public void shutdown()
{    run = false;}
0
public int getCount()
{    return count;}
0
public List<Exception> getErrors()
{    return errors;}
0
public void run()
{    run = true;    while (run && count < until) {        boolean error = true;        try {            if (PollableSource.Status.READY.equals(source.process())) {                count++;                error = false;            }        } catch (Exception ex) {            errors.add(ex);        }        if (error) {            try {                Thread.sleep(1000L);            } catch (InterruptedException e) {            }        }    }}
0
public void shutdown()
{    run = false;}
0
public int getCount()
{    return count;}
0
public List<Exception> getErrors()
{    return errors;}
0
public void test() throws Exception
{    testBasic();    testEmpty();    testNullPlainText();    testNullCipherText();}
0
public void testBasic() throws Exception
{    String expected = "mn state fair is the place to be";    byte[] cipherText = encryptor.encrypt(expected.getBytes(Charsets.UTF_8));    byte[] clearText = decryptor.decrypt(cipherText);    Assert.assertEquals(expected, new String(clearText, Charsets.UTF_8));}
0
public void testEmpty() throws Exception
{    String expected = "";    byte[] cipherText = encryptor.encrypt(new byte[] {});    byte[] clearText = decryptor.decrypt(cipherText);    Assert.assertEquals(expected, new String(clearText));}
0
public void testNullPlainText() throws Exception
{    try {        encryptor.encrypt(null);        Assert.fail();    } catch (NullPointerException e) {        }}
0
public void testNullCipherText() throws Exception
{    try {        decryptor.decrypt(null);        Assert.fail();    } catch (NullPointerException e) {        }}
0
private static Key newKey()
{    KeyGenerator keyGen;    try {        keyGen = KeyGenerator.getInstance("AES");        Key key = keyGen.generateKey();        return key;    } catch (Exception e) {        throw Throwables.propagate(e);    }}
0
public static void createKeyStore(File keyStoreFile, File keyStorePasswordFile, Map<String, File> keyAliasPassword) throws Exception
{    KeyStore ks = KeyStore.getInstance("jceks");    ks.load(null);    List<String> keysWithSeperatePasswords = Lists.newArrayList();    for (String alias : keyAliasPassword.keySet()) {        Key key = newKey();        char[] password = null;        File passwordFile = keyAliasPassword.get(alias);        if (passwordFile == null) {            password = Files.toString(keyStorePasswordFile, Charsets.UTF_8).toCharArray();        } else {            keysWithSeperatePasswords.add(alias);            password = Files.toString(passwordFile, Charsets.UTF_8).toCharArray();        }        ks.setKeyEntry(alias, key, password, null);    }    char[] keyStorePassword = Files.toString(keyStorePasswordFile, Charsets.UTF_8).toCharArray();    FileOutputStream outputStream = new FileOutputStream(keyStoreFile);    ks.store(outputStream, keyStorePassword);    outputStream.close();}
0
public static Map<String, File> configureTestKeyStore(File baseDir, File keyStoreFile) throws IOException
{    Map<String, File> result = Maps.newHashMap();    if (System.getProperty("java.vendor").contains("IBM")) {        Resources.copy(Resources.getResource("ibm-test.keystore"), new FileOutputStream(keyStoreFile));    } else {        Resources.copy(Resources.getResource("sun-test.keystore"), new FileOutputStream(keyStoreFile));    }    /* Commands below:     * keytool -genseckey -alias key-0 -keypass keyPassword -keyalg AES \     *   -keysize 128 -validity 9000 -keystore src/test/resources/test.keystore \     *   -storetype jceks -storepass keyStorePassword     * keytool -genseckey -alias key-1 -keyalg AES -keysize 128 -validity 9000 \     *   -keystore src/test/resources/test.keystore -storetype jceks \     *   -storepass keyStorePassword     */        result.put("key-0", TestUtils.writeStringToFile(baseDir, "key-0", "keyPassword"));    result.put("key-1", null);    return result;}
0
public static Map<String, String> configureForKeyStore(File keyStoreFile, File keyStorePasswordFile, Map<String, File> keyAliasPassword) throws Exception
{    Map<String, String> context = Maps.newHashMap();    List<String> keys = Lists.newArrayList();    Joiner joiner = Joiner.on(".");    for (String alias : keyAliasPassword.keySet()) {        File passwordFile = keyAliasPassword.get(alias);        if (passwordFile == null) {            keys.add(alias);        } else {            String propertyName = joiner.join(EncryptionConfiguration.KEY_PROVIDER, EncryptionConfiguration.JCE_FILE_KEYS, alias, EncryptionConfiguration.JCE_FILE_KEY_PASSWORD_FILE);            keys.add(alias);            context.put(propertyName, passwordFile.getAbsolutePath());        }    }    context.put(joiner.join(EncryptionConfiguration.KEY_PROVIDER, EncryptionConfiguration.JCE_FILE_KEY_STORE_FILE), keyStoreFile.getAbsolutePath());    if (keyStorePasswordFile != null) {        context.put(joiner.join(EncryptionConfiguration.KEY_PROVIDER, EncryptionConfiguration.JCE_FILE_KEY_STORE_PASSWORD_FILE), keyStorePasswordFile.getAbsolutePath());    }    context.put(joiner.join(EncryptionConfiguration.KEY_PROVIDER, EncryptionConfiguration.JCE_FILE_KEYS), Joiner.on(" ").join(keys));    return context;}
0
public void setup() throws Exception
{    KeyGenerator keyGen = KeyGenerator.getInstance("AES");    key = keyGen.generateKey();    encryptor = CipherProviderFactory.getEncrypter(CipherProviderType.AESCTRNOPADDING.name(), key);    decryptor = CipherProviderFactory.getDecrypter(CipherProviderType.AESCTRNOPADDING.name(), key, encryptor.getParameters());    cipherProviderTestSuite = new CipherProviderTestSuite(encryptor, decryptor);}
0
public void test() throws Exception
{    cipherProviderTestSuite.test();}
0
public void setup() throws Exception
{    super.setup();    keyStorePasswordFile = new File(baseDir, "keyStorePasswordFile");    Files.write("keyStorePassword", keyStorePasswordFile, Charsets.UTF_8);    keyStoreFile = new File(baseDir, "keyStoreFile");    Assert.assertTrue(keyStoreFile.createNewFile());    keyAliasPassword = Maps.newHashMap();    keyAliasPassword.putAll(EncryptionTestUtils.configureTestKeyStore(baseDir, keyStoreFile));}
0
public void teardown()
{    super.teardown();}
0
private Map<String, String> getOverrides() throws Exception
{    Map<String, String> overrides = Maps.newHashMap();    overrides.put(FileChannelConfiguration.CAPACITY, String.valueOf(100));    overrides.put(FileChannelConfiguration.TRANSACTION_CAPACITY, String.valueOf(100));    return overrides;}
0
private Map<String, String> getOverridesForEncryption() throws Exception
{    Map<String, String> overrides = getOverrides();    Map<String, String> encryptionProps = EncryptionTestUtils.configureForKeyStore(keyStoreFile, keyStorePasswordFile, keyAliasPassword);    encryptionProps.put(EncryptionConfiguration.KEY_PROVIDER, KeyProviderType.JCEKSFILE.name());    encryptionProps.put(EncryptionConfiguration.CIPHER_PROVIDER, CipherProviderType.AESCTRNOPADDING.name());    encryptionProps.put(EncryptionConfiguration.ACTIVE_KEY, "key-1");    for (String key : encryptionProps.keySet()) {        overrides.put(EncryptionConfiguration.ENCRYPTION_PREFIX + "." + key, encryptionProps.get(key));    }    return overrides;}
0
public void testThreadedConsume() throws Exception
{    int numThreads = 20;    Map<String, String> overrides = getOverridesForEncryption();    overrides.put(FileChannelConfiguration.CAPACITY, String.valueOf(10000));    overrides.put(FileChannelConfiguration.TRANSACTION_CAPACITY, String.valueOf(100));    channel = createFileChannel(overrides);    channel.start();    Assert.assertTrue(channel.isOpen());    Executor executor = Executors.newFixedThreadPool(numThreads);    Set<String> in = fillChannel(channel, "threaded-consume");    final AtomicBoolean error = new AtomicBoolean(false);    final CountDownLatch startLatch = new CountDownLatch(numThreads);    final CountDownLatch stopLatch = new CountDownLatch(numThreads);    final Set<String> out = Collections.synchronizedSet(new HashSet<String>());    for (int i = 0; i < numThreads; i++) {        executor.execute(new Runnable() {            @Override            public void run() {                try {                    startLatch.countDown();                    startLatch.await();                    out.addAll(takeEvents(channel, 10));                } catch (Throwable t) {                    error.set(true);                                    } finally {                    stopLatch.countDown();                }            }        });    }    stopLatch.await();    Assert.assertFalse(error.get());    compareInputAndOut(in, out);}
1
public void run()
{    try {        startLatch.countDown();        startLatch.await();        out.addAll(takeEvents(channel, 10));    } catch (Throwable t) {        error.set(true);            } finally {        stopLatch.countDown();    }}
1
public void testThreadedProduce() throws Exception
{    int numThreads = 20;    Map<String, String> overrides = getOverridesForEncryption();    overrides.put(FileChannelConfiguration.CAPACITY, String.valueOf(10000));    overrides.put(FileChannelConfiguration.TRANSACTION_CAPACITY, String.valueOf(100));    channel = createFileChannel(overrides);    channel.start();    Assert.assertTrue(channel.isOpen());    Executor executor = Executors.newFixedThreadPool(numThreads);    final AtomicBoolean error = new AtomicBoolean(false);    final CountDownLatch startLatch = new CountDownLatch(numThreads);    final CountDownLatch stopLatch = new CountDownLatch(numThreads);    final Set<String> in = Collections.synchronizedSet(new HashSet<String>());    for (int i = 0; i < numThreads; i++) {        executor.execute(new Runnable() {            @Override            public void run() {                try {                    startLatch.countDown();                    startLatch.await();                    in.addAll(putEvents(channel, "thread-produce", 10, 10000, true));                } catch (Throwable t) {                    error.set(true);                                    } finally {                    stopLatch.countDown();                }            }        });    }    stopLatch.await();    Set<String> out = consumeChannel(channel);    Assert.assertFalse(error.get());    compareInputAndOut(in, out);}
1
public void run()
{    try {        startLatch.countDown();        startLatch.await();        in.addAll(putEvents(channel, "thread-produce", 10, 10000, true));    } catch (Throwable t) {        error.set(true);            } finally {        stopLatch.countDown();    }}
1
public void testConfiguration() throws Exception
{    Map<String, String> overrides = Maps.newHashMap();    overrides.put("encryption.activeKey", "key-1");    overrides.put("encryption.cipherProvider", "AESCTRNOPADDING");    overrides.put("encryption.keyProvider", "JCEKSFILE");    overrides.put("encryption.keyProvider.keyStoreFile", keyStoreFile.getAbsolutePath());    overrides.put("encryption.keyProvider.keyStorePasswordFile", keyStorePasswordFile.getAbsolutePath());    overrides.put("encryption.keyProvider.keys", "key-0 key-1");    overrides.put("encryption.keyProvider.keys.key-0.passwordFile", keyAliasPassword.get("key-0").getAbsolutePath());    channel = createFileChannel(overrides);    channel.start();    Assert.assertTrue(channel.isOpen());    Set<String> in = fillChannel(channel, "restart");    channel.stop();    channel = TestUtils.createFileChannel(checkpointDir.getAbsolutePath(), dataDir, overrides);    channel.start();    Assert.assertTrue(channel.isOpen());    Set<String> out = consumeChannel(channel);    compareInputAndOut(in, out);}
0
public void testBasicEncyrptionDecryption() throws Exception
{    Map<String, String> overrides = getOverridesForEncryption();    channel = createFileChannel(overrides);    channel.start();    Assert.assertTrue(channel.isOpen());    Set<String> in = fillChannel(channel, "restart");    channel.stop();    channel = TestUtils.createFileChannel(checkpointDir.getAbsolutePath(), dataDir, overrides);    channel.start();    Assert.assertTrue(channel.isOpen());    Set<String> out = consumeChannel(channel);    compareInputAndOut(in, out);}
0
public void testEncryptedChannelWithoutEncryptionConfigFails() throws Exception
{    Map<String, String> overrides = getOverridesForEncryption();    channel = createFileChannel(overrides);    channel.start();    Assert.assertTrue(channel.isOpen());    fillChannel(channel, "will-not-restart");    channel.stop();    Map<String, String> noEncryptionOverrides = getOverrides();    channel = createFileChannel(noEncryptionOverrides);    channel.start();    if (channel.isOpen()) {        try {            takeEvents(channel, 1, 1);            Assert.fail("Channel was opened and take did not throw exception");        } catch (ChannelException ex) {                }    }}
0
public void testUnencyrptedAndEncryptedLogs() throws Exception
{    Map<String, String> noEncryptionOverrides = getOverrides();    channel = createFileChannel(noEncryptionOverrides);    channel.start();    Assert.assertTrue(channel.isOpen());    Set<String> in = fillChannel(channel, "unencrypted-and-encrypted");    int numEventsToRemove = in.size() / 2;    for (int i = 0; i < numEventsToRemove; i++) {        Assert.assertTrue(in.removeAll(takeEvents(channel, 1, 1)));    }        channel.stop();    Map<String, String> overrides = getOverridesForEncryption();    channel = createFileChannel(overrides);    channel.start();    Assert.assertTrue(channel.isOpen());    in.addAll(fillChannel(channel, "unencrypted-and-encrypted"));    Set<String> out = consumeChannel(channel);    compareInputAndOut(in, out);}
0
public void testBadKeyProviderInvalidValue() throws Exception
{    Map<String, String> overrides = getOverridesForEncryption();    overrides.put(Joiner.on(".").join(EncryptionConfiguration.ENCRYPTION_PREFIX, EncryptionConfiguration.KEY_PROVIDER), "invalid");    try {        channel = createFileChannel(overrides);        Assert.fail();    } catch (FlumeException ex) {        Assert.assertEquals("java.lang.ClassNotFoundException: invalid", ex.getMessage());    }}
0
public void testBadKeyProviderInvalidClass() throws Exception
{    Map<String, String> overrides = getOverridesForEncryption();    overrides.put(Joiner.on(".").join(EncryptionConfiguration.ENCRYPTION_PREFIX, EncryptionConfiguration.KEY_PROVIDER), String.class.getName());    try {        channel = createFileChannel(overrides);        Assert.fail();    } catch (FlumeException ex) {        Assert.assertEquals("Unable to instantiate Builder from java.lang.String", ex.getMessage());    }}
0
public void testBadCipherProviderInvalidValue() throws Exception
{    Map<String, String> overrides = getOverridesForEncryption();    overrides.put(Joiner.on(".").join(EncryptionConfiguration.ENCRYPTION_PREFIX, EncryptionConfiguration.CIPHER_PROVIDER), "invalid");    channel = createFileChannel(overrides);    channel.start();    Assert.assertFalse(channel.isOpen());}
0
public void testBadCipherProviderInvalidClass() throws Exception
{    Map<String, String> overrides = getOverridesForEncryption();    overrides.put(Joiner.on(".").join(EncryptionConfiguration.ENCRYPTION_PREFIX, EncryptionConfiguration.CIPHER_PROVIDER), String.class.getName());    channel = createFileChannel(overrides);    channel.start();    Assert.assertFalse(channel.isOpen());}
0
public void testMissingKeyStoreFile() throws Exception
{    Map<String, String> overrides = getOverridesForEncryption();    overrides.put(Joiner.on(".").join(EncryptionConfiguration.ENCRYPTION_PREFIX, EncryptionConfiguration.KEY_PROVIDER, EncryptionConfiguration.JCE_FILE_KEY_STORE_FILE), "/path/does/not/exist");    try {        channel = createFileChannel(overrides);        Assert.fail();    } catch (RuntimeException ex) {        Assert.assertTrue("Exception message is incorrect: " + ex.getMessage(), ex.getMessage().startsWith("java.io.FileNotFoundException: /path/does/not/exist "));    }}
0
public void testMissingKeyStorePasswordFile() throws Exception
{    Map<String, String> overrides = getOverridesForEncryption();    overrides.put(Joiner.on(".").join(EncryptionConfiguration.ENCRYPTION_PREFIX, EncryptionConfiguration.KEY_PROVIDER, EncryptionConfiguration.JCE_FILE_KEY_STORE_PASSWORD_FILE), "/path/does/not/exist");    try {        channel = createFileChannel(overrides);        Assert.fail();    } catch (RuntimeException ex) {        Assert.assertTrue("Exception message is incorrect: " + ex.getMessage(), ex.getMessage().startsWith("java.io.FileNotFoundException: /path/does/not/exist "));    }}
0
public void testBadKeyStorePassword() throws Exception
{    Files.write("invalid", keyStorePasswordFile, Charsets.UTF_8);    Map<String, String> overrides = getOverridesForEncryption();    try {        channel = TestUtils.createFileChannel(checkpointDir.getAbsolutePath(), dataDir, overrides);        Assert.fail();    } catch (RuntimeException ex) {        Assert.assertEquals("java.io.IOException: Keystore was tampered with, or " + "password was incorrect", ex.getMessage());    }}
0
public void testBadKeyAlias() throws Exception
{    Map<String, String> overrides = getOverridesForEncryption();    overrides.put(EncryptionConfiguration.ENCRYPTION_PREFIX + "." + EncryptionConfiguration.ACTIVE_KEY, "invalid");    channel = TestUtils.createFileChannel(checkpointDir.getAbsolutePath(), dataDir, overrides);    channel.start();    Assert.assertFalse(channel.isOpen());}
0
public void setup() throws Exception
{    baseDir = Files.createTempDir();    keyStorePasswordFile = new File(baseDir, "keyStorePasswordFile");    Files.write("keyStorePassword", keyStorePasswordFile, Charsets.UTF_8);    keyAliasPassword = Maps.newHashMap();    keyStoreFile = new File(baseDir, "keyStoreFile");    Assert.assertTrue(keyStoreFile.createNewFile());}
0
public void cleanup()
{    FileUtils.deleteQuietly(baseDir);}
0
private void initializeForKey(Key key)
{    encryptor = new AESCTRNoPaddingProvider.EncryptorBuilder().setKey(key).build();    decryptor = new AESCTRNoPaddingProvider.DecryptorBuilder().setKey(key).setParameters(encryptor.getParameters()).build();}
0
public void testWithNewKeyStore() throws Exception
{    createNewKeyStore();    EncryptionTestUtils.createKeyStore(keyStoreFile, keyStorePasswordFile, keyAliasPassword);    Context context = new Context(EncryptionTestUtils.configureForKeyStore(keyStoreFile, keyStorePasswordFile, keyAliasPassword));    Context keyProviderContext = new Context(context.getSubProperties(EncryptionConfiguration.KEY_PROVIDER + "."));    KeyProvider keyProvider = KeyProviderFactory.getInstance(KeyProviderType.JCEKSFILE.name(), keyProviderContext);    testKeyProvider(keyProvider);}
0
public void testWithExistingKeyStore() throws Exception
{    keyAliasPassword.putAll(EncryptionTestUtils.configureTestKeyStore(baseDir, keyStoreFile));    Context context = new Context(EncryptionTestUtils.configureForKeyStore(keyStoreFile, keyStorePasswordFile, keyAliasPassword));    Context keyProviderContext = new Context(context.getSubProperties(EncryptionConfiguration.KEY_PROVIDER + "."));    KeyProvider keyProvider = KeyProviderFactory.getInstance(KeyProviderType.JCEKSFILE.name(), keyProviderContext);    testKeyProvider(keyProvider);}
0
private void createNewKeyStore() throws Exception
{    for (int i = 0; i < 10; i++) {                if (i % 2 == 0) {            String alias = "test-" + i;            String password = String.valueOf(i);            keyAliasPassword.put(alias, TestUtils.writeStringToFile(baseDir, alias, password));        }    }}
0
private void testKeyProvider(KeyProvider keyProvider)
{    for (String alias : keyAliasPassword.keySet()) {        Key key = keyProvider.getKey(alias);        initializeForKey(key);        String expected = "some text here " + alias;        byte[] cipherText = encryptor.encrypt(expected.getBytes(Charsets.UTF_8));        byte[] clearText = decryptor.decrypt(cipherText);        Assert.assertEquals(expected, new String(clearText, Charsets.UTF_8));    }}
0
public void setup() throws IOException
{    file = File.createTempFile("Checkpoint", "");    inflightPuts = File.createTempFile("inflightPuts", "");    inflightTakes = File.createTempFile("inflightTakes", "");    queueSet = File.createTempFile("queueset", "");    Assert.assertTrue(file.isFile());    Assert.assertTrue(file.canWrite());}
0
public void cleanup()
{    file.delete();}
0
public void testSerialization() throws Exception
{    EventQueueBackingStore backingStore = new EventQueueBackingStoreFileV2(file, 1, "test", new FileChannelCounter("test"));    FlumeEventPointer ptrIn = new FlumeEventPointer(10, 20);    FlumeEventQueue queueIn = new FlumeEventQueue(backingStore, inflightTakes, inflightPuts, queueSet);    queueIn.addHead(ptrIn);    FlumeEventQueue queueOut = new FlumeEventQueue(backingStore, inflightTakes, inflightPuts, queueSet);    Assert.assertEquals(0, queueOut.getLogWriteOrderID());    queueIn.checkpoint(false);    FlumeEventQueue queueOut2 = new FlumeEventQueue(backingStore, inflightTakes, inflightPuts, queueSet);    FlumeEventPointer ptrOut = queueOut2.removeHead(0L);    Assert.assertEquals(ptrIn, ptrOut);    Assert.assertTrue(queueOut2.getLogWriteOrderID() > 0);}
0
public void setup() throws Exception
{    super.setup();}
0
public void teardown()
{    super.teardown();}
0
public void testFastReplay() throws Exception
{    Map<String, String> overrides = Maps.newHashMap();    overrides.put(FileChannelConfiguration.CAPACITY, String.valueOf(50));    overrides.put(FileChannelConfiguration.TRANSACTION_CAPACITY, String.valueOf(50));    channel = createFileChannel(overrides);    channel.start();    Assert.assertTrue(channel.isOpen());    Set<String> in = fillChannel(channel, "checkpointBulder");    channel.stop();    File checkpointFile = new File(checkpointDir, "checkpoint");    File metaDataFile = Serialization.getMetaDataFile(checkpointFile);    File inflightTakesFile = new File(checkpointDir, "inflighttakes");    File inflightPutsFile = new File(checkpointDir, "inflightputs");    File queueSetDir = new File(checkpointDir, "queueset");    Assert.assertTrue(checkpointFile.delete());    Assert.assertTrue(metaDataFile.delete());    Assert.assertTrue(inflightTakesFile.delete());    Assert.assertTrue(inflightPutsFile.delete());    EventQueueBackingStore backingStore = EventQueueBackingStoreFactory.get(checkpointFile, 50, "test", new FileChannelCounter("test"));    FlumeEventQueue queue = new FlumeEventQueue(backingStore, inflightTakesFile, inflightPutsFile, queueSetDir);    CheckpointRebuilder checkpointRebuilder = new CheckpointRebuilder(getAllLogs(dataDirs), queue, true);    Assert.assertTrue(checkpointRebuilder.rebuild());    channel = createFileChannel(overrides);    channel.start();    Assert.assertTrue(channel.isOpen());    Set<String> out = consumeChannel(channel);    compareInputAndOut(in, out);}
0
public void setup() throws IOException
{    baseDir = Files.createTempDir();    checkpoint = new File(baseDir, "checkpoint");    inflightTakes = new File(baseDir, "takes");    inflightPuts = new File(baseDir, "puts");    queueSetDir = new File(baseDir, "queueset");    TestUtils.copyDecompressed("fileformat-v2-checkpoint.gz", checkpoint);}
0
public void teardown()
{    FileUtils.deleteQuietly(baseDir);}
0
public void testWithNoFlag() throws Exception
{    verify(EventQueueBackingStoreFactory.get(checkpoint, 10, "test", new FileChannelCounter("test")), Serialization.VERSION_3, pointersInTestCheckpoint);}
0
public void testWithFlag() throws Exception
{    verify(EventQueueBackingStoreFactory.get(checkpoint, 10, "test", new FileChannelCounter("test"), true), Serialization.VERSION_3, pointersInTestCheckpoint);}
0
public void testNoUprade() throws Exception
{    verify(EventQueueBackingStoreFactory.get(checkpoint, 10, "test", new FileChannelCounter("test"), false), Serialization.VERSION_2, pointersInTestCheckpoint);}
0
public void testDecreaseCapacity() throws Exception
{    Assert.assertTrue(checkpoint.delete());    EventQueueBackingStore backingStore = EventQueueBackingStoreFactory.get(checkpoint, 10, "test", new FileChannelCounter("test"));    backingStore.close();    EventQueueBackingStoreFactory.get(checkpoint, 9, "test", new FileChannelCounter("test"));    Assert.fail();}
0
public void testIncreaseCapacity() throws Exception
{    Assert.assertTrue(checkpoint.delete());    EventQueueBackingStore backingStore = EventQueueBackingStoreFactory.get(checkpoint, 10, "test", new FileChannelCounter("test"));    backingStore.close();    EventQueueBackingStoreFactory.get(checkpoint, 11, "test", new FileChannelCounter("test"));    Assert.fail();}
0
public void testNewCheckpoint() throws Exception
{    Assert.assertTrue(checkpoint.delete());    verify(EventQueueBackingStoreFactory.get(checkpoint, 10, "test", new FileChannelCounter("test"), false), Serialization.VERSION_3, Collections.<Long>emptyList());}
0
public void testCheckpointBadVersion() throws Exception
{    RandomAccessFile writer = new RandomAccessFile(checkpoint, "rw");    try {        EventQueueBackingStore backingStore = EventQueueBackingStoreFactory.get(checkpoint, 10, "test", new FileChannelCounter("test"));        backingStore.close();        writer.seek(EventQueueBackingStoreFile.INDEX_VERSION * Serialization.SIZE_OF_LONG);        writer.writeLong(94L);        writer.getFD().sync();        backingStore = EventQueueBackingStoreFactory.get(checkpoint, 10, "test", new FileChannelCounter("test"));    } finally {        writer.close();    }}
0
public void testIncompleteCheckpoint() throws Exception
{    RandomAccessFile writer = new RandomAccessFile(checkpoint, "rw");    try {        EventQueueBackingStore backingStore = EventQueueBackingStoreFactory.get(checkpoint, 10, "test", new FileChannelCounter("test"));        backingStore.close();        writer.seek(EventQueueBackingStoreFile.INDEX_CHECKPOINT_MARKER * Serialization.SIZE_OF_LONG);        writer.writeLong(EventQueueBackingStoreFile.CHECKPOINT_INCOMPLETE);        writer.getFD().sync();        backingStore = EventQueueBackingStoreFactory.get(checkpoint, 10, "test", new FileChannelCounter("test"));    } finally {        writer.close();    }}
0
public void testCheckpointVersionNotEqualToMeta() throws Exception
{    RandomAccessFile writer = new RandomAccessFile(checkpoint, "rw");    try {        EventQueueBackingStore backingStore = EventQueueBackingStoreFactory.get(checkpoint, 10, "test", new FileChannelCounter("test"));        backingStore.close();        writer.seek(EventQueueBackingStoreFile.INDEX_VERSION * Serialization.SIZE_OF_LONG);        writer.writeLong(2L);        writer.getFD().sync();        backingStore = EventQueueBackingStoreFactory.get(checkpoint, 10, "test", new FileChannelCounter("test"));    } finally {        writer.close();    }}
0
public void testCheckpointVersionNotEqualToMeta2() throws Exception
{    FileOutputStream os = null;    try {        EventQueueBackingStore backingStore = EventQueueBackingStoreFactory.get(checkpoint, 10, "test", new FileChannelCounter("test"));        backingStore.close();        Assert.assertTrue(checkpoint.exists());        Assert.assertTrue(Serialization.getMetaDataFile(checkpoint).length() != 0);        FileInputStream is = new FileInputStream(Serialization.getMetaDataFile(checkpoint));        ProtosFactory.Checkpoint meta = ProtosFactory.Checkpoint.parseDelimitedFrom(is);        Assert.assertNotNull(meta);        is.close();        os = new FileOutputStream(Serialization.getMetaDataFile(checkpoint));        meta.toBuilder().setVersion(2).build().writeDelimitedTo(os);        os.flush();        backingStore = EventQueueBackingStoreFactory.get(checkpoint, 10, "test", new FileChannelCounter("test"));    } finally {        os.close();    }}
0
public void testCheckpointOrderIdNotEqualToMeta() throws Exception
{    RandomAccessFile writer = new RandomAccessFile(checkpoint, "rw");    try {        EventQueueBackingStore backingStore = EventQueueBackingStoreFactory.get(checkpoint, 10, "test", new FileChannelCounter("test"));        backingStore.close();        writer.seek(EventQueueBackingStoreFile.INDEX_WRITE_ORDER_ID * Serialization.SIZE_OF_LONG);        writer.writeLong(2L);        writer.getFD().sync();        backingStore = EventQueueBackingStoreFactory.get(checkpoint, 10, "test", new FileChannelCounter("test"));    } finally {        writer.close();    }}
0
public void testCheckpointOrderIdNotEqualToMeta2() throws Exception
{    FileOutputStream os = null;    try {        EventQueueBackingStore backingStore = EventQueueBackingStoreFactory.get(checkpoint, 10, "test", new FileChannelCounter("test"));        backingStore.close();        Assert.assertTrue(checkpoint.exists());        Assert.assertTrue(Serialization.getMetaDataFile(checkpoint).length() != 0);        FileInputStream is = new FileInputStream(Serialization.getMetaDataFile(checkpoint));        ProtosFactory.Checkpoint meta = ProtosFactory.Checkpoint.parseDelimitedFrom(is);        Assert.assertNotNull(meta);        is.close();        os = new FileOutputStream(Serialization.getMetaDataFile(checkpoint));        meta.toBuilder().setWriteOrderID(1).build().writeDelimitedTo(os);        os.flush();        backingStore = EventQueueBackingStoreFactory.get(checkpoint, 10, "test", new FileChannelCounter("test"));    } finally {        os.close();    }}
0
public void testTruncateMeta() throws Exception
{    EventQueueBackingStore backingStore = EventQueueBackingStoreFactory.get(checkpoint, 10, "test", new FileChannelCounter("test"));    backingStore.close();    Assert.assertTrue(checkpoint.exists());    File metaFile = Serialization.getMetaDataFile(checkpoint);    Assert.assertTrue(metaFile.length() != 0);    RandomAccessFile writer = new RandomAccessFile(metaFile, "rw");    writer.setLength(0);    writer.getFD().sync();    writer.close();    backingStore = EventQueueBackingStoreFactory.get(checkpoint, 10, "test", new FileChannelCounter("test"));}
0
public void testCorruptMeta() throws Throwable
{    EventQueueBackingStore backingStore = EventQueueBackingStoreFactory.get(checkpoint, 10, "test", new FileChannelCounter("test"));    backingStore.close();    Assert.assertTrue(checkpoint.exists());    File metaFile = Serialization.getMetaDataFile(checkpoint);    Assert.assertTrue(metaFile.length() != 0);    RandomAccessFile writer = new RandomAccessFile(metaFile, "rw");    writer.seek(10);    writer.writeLong(new Random().nextLong());    writer.getFD().sync();    writer.close();    try {        backingStore = EventQueueBackingStoreFactory.get(checkpoint, 10, "test", new FileChannelCounter("test"));    } catch (BadCheckpointException ex) {        throw ex.getCause();    }}
0
private void verify(EventQueueBackingStore backingStore, long expectedVersion, List<Long> expectedPointers) throws Exception
{    FlumeEventQueue queue = new FlumeEventQueue(backingStore, inflightTakes, inflightPuts, queueSetDir);    List<Long> actualPointers = Lists.newArrayList();    FlumeEventPointer ptr;    while ((ptr = queue.removeHead(0L)) != null) {        actualPointers.add(ptr.toLong());    }    Assert.assertEquals(expectedPointers, actualPointers);    Assert.assertEquals(10, backingStore.getCapacity());    DataInputStream in = new DataInputStream(new FileInputStream(checkpoint));    long actualVersion = in.readLong();    Assert.assertEquals(expectedVersion, actualVersion);    in.close();}
0
public void testPutEvent()
{    FlumeEvent event = new FlumeEvent(null, new byte[5]);    Put put = new Put(1L, 1L, event);    Event returnEvent = EventUtils.getEventFromTransactionEvent(put);    Assert.assertNotNull(returnEvent);    Assert.assertEquals(5, returnEvent.getBody().length);}
0
public void testInvalidEvent()
{    Take take = new Take(1L, 1L);    Event returnEvent = EventUtils.getEventFromTransactionEvent(take);    Assert.assertNull(returnEvent);}
0
public void setup() throws Exception
{    super.setup();}
0
public void teardown()
{    super.teardown();}
0
public void testNegativeCapacities()
{    Map<String, String> parms = Maps.newHashMap();    parms.put(FileChannelConfiguration.CAPACITY, "-3");    parms.put(FileChannelConfiguration.TRANSACTION_CAPACITY, "-1");    parms.put(FileChannelConfiguration.CHECKPOINT_INTERVAL, "-2");    FileChannel channel = createFileChannel(parms);    Assert.assertTrue(field("capacity").ofType(Integer.class).in(channel).get() > 0);    Assert.assertTrue(field("transactionCapacity").ofType(Integer.class).in(channel).get() > 0);    Assert.assertTrue(field("checkpointInterval").ofType(Long.class).in(channel).get() > 0);}
0
public void testFailAfterTakeBeforeCommit() throws Throwable
{    final FileChannel channel = createFileChannel();    channel.start();    final Set<String> eventSet = putEvents(channel, "testTakeFailBeforeCommit", 5, 5);    Transaction tx = channel.getTransaction();    takeWithoutCommit(channel, tx, 2);            Executors.newSingleThreadExecutor().submit(new Runnable() {        @Override        public void run() {            Transaction tx = channel.getTransaction();            takeWithoutCommit(channel, tx, 3);        }    }).get();    forceCheckpoint(channel);    channel.stop();        try {        Executors.newSingleThreadExecutor().submit(new Runnable() {            @Override            public void run() {                FileChannel channel = createFileChannel();                channel.start();                Set<String> output = null;                try {                    output = takeEvents(channel, 5);                } catch (Exception e) {                    Throwables.propagate(e);                }                compareInputAndOut(eventSet, output);                channel.stop();            }        }).get();    } catch (ExecutionException e) {        throw e.getCause();    }}
0
public void run()
{    Transaction tx = channel.getTransaction();    takeWithoutCommit(channel, tx, 3);}
0
public void run()
{    FileChannel channel = createFileChannel();    channel.start();    Set<String> output = null;    try {        output = takeEvents(channel, 5);    } catch (Exception e) {        Throwables.propagate(e);    }    compareInputAndOut(eventSet, output);    channel.stop();}
0
public void testFailAfterPutCheckpointCommit() throws Throwable
{    final Map<String, String> overrides = Maps.newHashMap();    overrides.put(FileChannelConfiguration.CHECKPOINT_INTERVAL, "10000");    final FileChannel channel = createFileChannel(overrides);    channel.start();    Transaction tx = channel.getTransaction();    final Set<String> input = putWithoutCommit(channel, tx, "failAfterPut", 3);            final CountDownLatch latch = new CountDownLatch(1);    Executors.newSingleThreadExecutor().submit(new Runnable() {        @Override        public void run() {            Transaction tx = channel.getTransaction();            input.addAll(putWithoutCommit(channel, tx, "failAfterPut", 3));            try {                latch.await();                tx.commit();            } catch (InterruptedException e) {                tx.rollback();                Throwables.propagate(e);            } finally {                tx.close();            }        }    });    forceCheckpoint(channel);    tx.commit();    tx.close();    latch.countDown();    Thread.sleep(2000);    channel.stop();    final Set<String> out = Sets.newHashSet();        try {        Executors.newSingleThreadExecutor().submit(new Runnable() {            @Override            public void run() {                try {                    FileChannel channel = createFileChannel();                    channel.start();                    out.addAll(takeEvents(channel, 6));                    channel.stop();                } catch (Exception ex) {                    Throwables.propagate(ex);                }            }        }).get();    } catch (ExecutionException e) {        throw e.getCause();    }}
0
public void run()
{    Transaction tx = channel.getTransaction();    input.addAll(putWithoutCommit(channel, tx, "failAfterPut", 3));    try {        latch.await();        tx.commit();    } catch (InterruptedException e) {        tx.rollback();        Throwables.propagate(e);    } finally {        tx.close();    }}
0
public void run()
{    try {        FileChannel channel = createFileChannel();        channel.start();        out.addAll(takeEvents(channel, 6));        channel.stop();    } catch (Exception ex) {        Throwables.propagate(ex);    }}
0
public void testReconfigure() throws Exception
{    channel.start();    Assert.assertTrue(channel.isOpen());    Set<String> in = Sets.newHashSet();    try {        while (true) {            in.addAll(putEvents(channel, "reconfig", 1, 1));        }    } catch (ChannelException e) {        Assert.assertEquals("The channel has reached it's capacity. " + "This might be the result of a sink on the channel having too " + "low of batch size, a downstream system running slower than " + "normal, or that the channel capacity is just too low. [channel=" + channel.getName() + "]", e.getMessage());    }    Configurables.configure(channel, createContext());    Set<String> out = takeEvents(channel, 1, Integer.MAX_VALUE);    compareInputAndOut(in, out);}
0
public void testPut() throws Exception
{    channel.start();    Assert.assertTrue(channel.isOpen());        int found = takeEvents(channel, 1, 5).size();    Assert.assertEquals(0, found);    Set<String> expected = Sets.newHashSet();    expected.addAll(putEvents(channel, "unbatched", 1, 5));    expected.addAll(putEvents(channel, "batched", 5, 5));    Set<String> actual = takeEvents(channel, 1);    compareInputAndOut(expected, actual);}
0
public void testPutConvertsNullValueToEmptyStrInHeader() throws Exception
{    channel.start();    Event event = EventBuilder.withBody("test body".getBytes(Charsets.UTF_8), Collections.<String, String>singletonMap(TEST_KEY, null));    Transaction txPut = channel.getTransaction();    txPut.begin();    channel.put(event);    txPut.commit();    txPut.close();    Transaction txTake = channel.getTransaction();    txTake.begin();    Event eventTaken = channel.take();    Assert.assertArrayEquals(event.getBody(), eventTaken.getBody());    Assert.assertEquals("", eventTaken.getHeaders().get(TEST_KEY));    txTake.commit();    txTake.close();}
0
public void testCommitAfterNoPutTake() throws Exception
{    channel.start();    Assert.assertTrue(channel.isOpen());    Transaction transaction;    transaction = channel.getTransaction();    transaction.begin();    transaction.commit();    transaction.close();        channel.stop();    channel = createFileChannel();    channel.start();    Assert.assertTrue(channel.isOpen());    transaction = channel.getTransaction();    transaction.begin();    Assert.assertNull(channel.take());    transaction.commit();    transaction.close();}
0
public void testCapacity() throws Exception
{    Map<String, String> overrides = Maps.newHashMap();    overrides.put(FileChannelConfiguration.CAPACITY, String.valueOf(5));    overrides.put(FileChannelConfiguration.TRANSACTION_CAPACITY, String.valueOf(5));    channel = createFileChannel(overrides);    channel.start();    Assert.assertTrue(channel.isOpen());    fillChannel(channel, "fillup");            Transaction transaction;    transaction = channel.getTransaction();    transaction.begin();    Event event = channel.take();    Assert.assertNotNull(event);    transaction.rollback();    transaction.close();        Assert.assertEquals(0, fillChannel(channel, "capacity").size());        Assert.assertEquals(5, takeEvents(channel, 1, 5).size());}
0
public void testRaceFoundInFLUME1432() throws Exception
{        Map<String, String> overrides = Maps.newHashMap();    overrides.put(FileChannelConfiguration.KEEP_ALIVE, String.valueOf(10L));    overrides.put(FileChannelConfiguration.CAPACITY, String.valueOf(10L));    overrides.put(FileChannelConfiguration.TRANSACTION_CAPACITY, String.valueOf(10L));    channel = createFileChannel(overrides);    channel.start();    Assert.assertTrue(channel.isOpen());    fillChannel(channel, "fillup");        Future<String> put = Executors.newSingleThreadExecutor().submit(new Callable<String>() {        @Override        public String call() throws Exception {            Set<String> result = putEvents(channel, "blocked-put", 1, 1);            Assert.assertTrue(result.toString(), result.size() == 1);            Iterator<String> iter = result.iterator();            return iter.next();        }    });        Thread.sleep(1000L);        Set<String> result = takeEvents(channel, 1, 1);    Assert.assertTrue(result.toString(), result.size() == 1);    String putmsg = put.get();    Assert.assertNotNull(putmsg);    String takemsg = result.iterator().next();    Assert.assertNotNull(takemsg);        channel.stop();    channel = createFileChannel(overrides);            channel.start();    Assert.assertTrue(channel.isOpen());}
1
public String call() throws Exception
{    Set<String> result = putEvents(channel, "blocked-put", 1, 1);    Assert.assertTrue(result.toString(), result.size() == 1);    Iterator<String> iter = result.iterator();    return iter.next();}
0
public void testThreaded() throws IOException, InterruptedException
{    channel.start();    Assert.assertTrue(channel.isOpen());    int numThreads = 10;    final CountDownLatch producerStopLatch = new CountDownLatch(numThreads);    final CountDownLatch consumerStopLatch = new CountDownLatch(numThreads);    final List<Exception> errors = Collections.synchronizedList(new ArrayList<Exception>());    final Set<String> expected = Collections.synchronizedSet(new HashSet<String>());    final Set<String> actual = Collections.synchronizedSet(new HashSet<String>());    for (int i = 0; i < numThreads; i++) {        final int id = i;        Thread t = new Thread() {            @Override            public void run() {                try {                    if (id % 2 == 0) {                        expected.addAll(putEvents(channel, Integer.toString(id), 1, 5));                    } else {                        expected.addAll(putEvents(channel, Integer.toString(id), 5, 5));                    }                                    } catch (Exception e) {                                        errors.add(e);                } finally {                    producerStopLatch.countDown();                }            }        };        t.setDaemon(true);        t.start();    }    for (int i = 0; i < numThreads; i++) {        final int id = i;        Thread t = new Thread() {            @Override            public void run() {                try {                    while (!producerStopLatch.await(1, TimeUnit.SECONDS) || expected.size() > actual.size()) {                        if (id % 2 == 0) {                            actual.addAll(takeEvents(channel, 1, Integer.MAX_VALUE));                        } else {                            actual.addAll(takeEvents(channel, 5, Integer.MAX_VALUE));                        }                    }                    if (actual.isEmpty()) {                                            } else {                                            }                } catch (Exception e) {                                        errors.add(e);                } finally {                    consumerStopLatch.countDown();                }            }        };        t.setDaemon(true);        t.start();    }    Assert.assertTrue("Timed out waiting for producers", producerStopLatch.await(30, TimeUnit.SECONDS));    Assert.assertTrue("Timed out waiting for consumer", consumerStopLatch.await(30, TimeUnit.SECONDS));    Assert.assertEquals(Collections.EMPTY_LIST, errors);    compareInputAndOut(expected, actual);}
1
public void run()
{    try {        if (id % 2 == 0) {            expected.addAll(putEvents(channel, Integer.toString(id), 1, 5));        } else {            expected.addAll(putEvents(channel, Integer.toString(id), 5, 5));        }            } catch (Exception e) {                errors.add(e);    } finally {        producerStopLatch.countDown();    }}
1
public void run()
{    try {        while (!producerStopLatch.await(1, TimeUnit.SECONDS) || expected.size() > actual.size()) {            if (id % 2 == 0) {                actual.addAll(takeEvents(channel, 1, Integer.MAX_VALUE));            } else {                actual.addAll(takeEvents(channel, 5, Integer.MAX_VALUE));            }        }        if (actual.isEmpty()) {                    } else {                    }    } catch (Exception e) {                errors.add(e);    } finally {        consumerStopLatch.countDown();    }}
1
public void testLocking() throws IOException
{    channel.start();    Assert.assertTrue(channel.isOpen());    FileChannel fileChannel = createFileChannel();    fileChannel.start();    Assert.assertTrue(!fileChannel.isOpen());}
0
public void testTakeTransactionCrossingCheckpoint() throws Exception
{    Map<String, String> overrides = Maps.newHashMap();    overrides.put(FileChannelConfiguration.CHECKPOINT_INTERVAL, "10000");    channel = createFileChannel(overrides);    channel.start();    Assert.assertTrue(channel.isOpen());    Set<String> in = fillChannel(channel, "restart");    Set<String> out = Sets.newHashSet();        Transaction tx = channel.getTransaction();    out.addAll(takeWithoutCommit(channel, tx, 1));            forceCheckpoint(channel);    tx.commit();    tx.close();    channel.stop();    channel = createFileChannel(overrides);    channel.start();    Assert.assertTrue(channel.isOpen());        Set<String> out2 = takeEvents(channel, 1, Integer.MAX_VALUE);    channel.stop();    in.removeAll(out);    compareInputAndOut(in, out2);}
0
public void testPutForceCheckpointCommitReplay() throws Exception
{    Map<String, String> overrides = Maps.newHashMap();    overrides.put(FileChannelConfiguration.CAPACITY, String.valueOf(2));    overrides.put(FileChannelConfiguration.TRANSACTION_CAPACITY, String.valueOf(2));    overrides.put(FileChannelConfiguration.CHECKPOINT_INTERVAL, "10000");    FileChannel channel = createFileChannel(overrides);    channel.start();        Transaction tx = channel.getTransaction();    Set<String> in = putWithoutCommit(channel, tx, "putWithoutCommit", 1);    forceCheckpoint(channel);    tx.commit();    tx.close();    channel.stop();    channel = createFileChannel(overrides);    channel.start();    Assert.assertTrue(channel.isOpen());    Set<String> out = takeEvents(channel, 1);    compareInputAndOut(in, out);    channel.stop();}
0
public void testPutCheckpointCommitCheckpointReplay() throws Exception
{    Map<String, String> overrides = Maps.newHashMap();    overrides.put(FileChannelConfiguration.CAPACITY, String.valueOf(2));    overrides.put(FileChannelConfiguration.TRANSACTION_CAPACITY, String.valueOf(2));    overrides.put(FileChannelConfiguration.CHECKPOINT_INTERVAL, "10000");    FileChannel channel = createFileChannel(overrides);    channel.start();        Transaction tx = channel.getTransaction();    Set<String> in = putWithoutCommit(channel, tx, "doubleCheckpoint", 1);    forceCheckpoint(channel);    tx.commit();    tx.close();    forceCheckpoint(channel);    channel.stop();    channel = createFileChannel(overrides);    channel.start();    Assert.assertTrue(channel.isOpen());    Set<String> out = takeEvents(channel, 5);    compareInputAndOut(in, out);    channel.stop();}
0
public void testReferenceCounts() throws Exception
{    Map<String, String> overrides = Maps.newHashMap();    overrides.put(FileChannelConfiguration.CHECKPOINT_INTERVAL, "10000");    overrides.put(FileChannelConfiguration.MAX_FILE_SIZE, "150");    final FileChannel channel = createFileChannel(overrides);    channel.start();    putEvents(channel, "testing-reference-counting", 1, 15);    Transaction tx = channel.getTransaction();    takeWithoutCommit(channel, tx, 10);    forceCheckpoint(channel);    tx.rollback();            final Set<String> takenEvents = Sets.newHashSet();    Executors.newSingleThreadExecutor().submit(new Runnable() {        @Override        public void run() {            try {                takenEvents.addAll(takeEvents(channel, 15));            } catch (Exception ex) {                Throwables.propagate(ex);            }        }    }).get();    Assert.assertEquals(15, takenEvents.size());}
0
public void run()
{    try {        takenEvents.addAll(takeEvents(channel, 15));    } catch (Exception ex) {        Throwables.propagate(ex);    }}
0
public void testRollbackIncompleteTransaction() throws Exception
{    Map<String, String> overrides = Maps.newHashMap();    overrides.put(FileChannelConfiguration.CHECKPOINT_INTERVAL, String.valueOf(Integer.MAX_VALUE));    final FileChannel channel = createFileChannel(overrides);    channel.start();    FileBackedTransaction tx = (FileBackedTransaction) channel.getTransaction();    InflightEventWrapper inflightPuts = field("inflightPuts").ofType(InflightEventWrapper.class).in(field("queue").ofType(FlumeEventQueue.class).in(tx).get()).get();    tx.begin();    for (int i = 0; i < 100; i++) {        channel.put(EventBuilder.withBody("TestEvent".getBytes()));    }    Assert.assertFalse(inflightPuts.getFileIDs().isEmpty());    Assert.assertFalse(inflightPuts.getInFlightPointers().isEmpty());    tx.rollback();    tx.close();    Assert.assertTrue(inflightPuts.getFileIDs().isEmpty());    Assert.assertTrue(inflightPuts.getInFlightPointers().isEmpty());    Assert.assertTrue(channel.getDepth() == 0);    Set<String> in = putEvents(channel, "testing-rollbacks", 100, 100);    tx = (FileBackedTransaction) channel.getTransaction();    InflightEventWrapper inflightTakes = field("inflightTakes").ofType(InflightEventWrapper.class).in(field("queue").ofType(FlumeEventQueue.class).in(tx).get()).get();    tx.begin();    for (int i = 0; i < 100; i++) {        channel.take();    }    Assert.assertFalse(inflightTakes.getFileIDs().isEmpty());    Assert.assertFalse(inflightTakes.getInFlightPointers().isEmpty());    tx.rollback();    tx.close();    Assert.assertTrue(inflightTakes.getFileIDs().isEmpty());    Assert.assertTrue(inflightTakes.getInFlightPointers().isEmpty());    Assert.assertTrue(channel.getDepth() == in.size());}
0
public void testChannelDiesOnCorruptEventFsync() throws Exception
{    testChannelDiesOnCorruptEvent(true);}
0
public void testChannelDiesOnCorruptEventNoFsync() throws Exception
{    testChannelDiesOnCorruptEvent(false);}
0
private void testChannelDiesOnCorruptEvent(boolean fsyncPerTxn) throws Exception
{    Map<String, String> overrides = new HashMap<String, String>();    overrides.put(FileChannelConfiguration.FSYNC_PER_TXN, String.valueOf(fsyncPerTxn));    final FileChannel channel = createFileChannel(overrides);    channel.start();    putEvents(channel, "test-corrupt-event", 100, 100);    for (File dataDir : dataDirs) {        File[] files = dataDir.listFiles(new FilenameFilter() {            @Override            public boolean accept(File dir, String name) {                if (!name.endsWith("meta") && !name.contains("lock")) {                    return true;                }                return false;            }        });        if (files != null && files.length > 0) {            for (int j = 0; j < files.length; j++) {                RandomAccessFile fileToCorrupt = new RandomAccessFile(files[0], "rw");                fileToCorrupt.seek(50);                fileToCorrupt.writeByte(234);                fileToCorrupt.close();            }        }    }    Set<String> events;    try {        events = consumeChannel(channel, true);    } catch (IllegalStateException ex) {                                Assert.assertTrue(ex.getMessage().contains("Log is closed"));        throw ex;    }    if (fsyncPerTxn) {        Assert.fail();    } else {                        Assert.assertEquals(99, events.size());    }}
0
public boolean accept(File dir, String name)
{    if (!name.endsWith("meta") && !name.contains("lock")) {        return true;    }    return false;}
0
public void testFileChannelCounterIsOpen()
{    FileChannel channel = createFileChannel();    FileChannelCounter counter = channel.getChannelCounter();    Assert.assertEquals(counter.isOpen(), false);    channel.start();    Assert.assertEquals(counter.isOpen(), true);    channel.stop();    Assert.assertEquals(counter.isOpen(), false);}
0
public void setup() throws Exception
{    baseDir = Files.createTempDir();    checkpointDir = new File(baseDir, "chkpt");    backupDir = new File(baseDir, "backup");    uncompressedBackupCheckpoint = new File(backupDir, "checkpoint");    compressedBackupCheckpoint = new File(backupDir, "checkpoint.snappy");    Assert.assertTrue(checkpointDir.mkdirs() || checkpointDir.isDirectory());    Assert.assertTrue(backupDir.mkdirs() || backupDir.isDirectory());    dataDirs = new File[dataDirCount];    dataDir = "";    for (int i = 0; i < dataDirs.length; i++) {        dataDirs[i] = new File(baseDir, "data" + (i + 1));        Assert.assertTrue(dataDirs[i].mkdirs() || dataDirs[i].isDirectory());        dataDir += dataDirs[i].getAbsolutePath() + ",";    }    dataDir = dataDir.substring(0, dataDir.length() - 1);    channel = createFileChannel();}
0
public void teardown()
{    if (channel != null && channel.isOpen()) {        channel.stop();    }    FileUtils.deleteQuietly(baseDir);}
0
protected Context createContext()
{    return createContext(new HashMap<String, String>());}
0
protected Context createContext(Map<String, String> overrides)
{    return TestUtils.createFileChannelContext(checkpointDir.getAbsolutePath(), dataDir, backupDir.getAbsolutePath(), overrides);}
0
protected FileChannel createFileChannel()
{    return createFileChannel(new HashMap<String, String>());}
0
protected FileChannel createFileChannel(Map<String, String> overrides)
{    return TestUtils.createFileChannel(checkpointDir.getAbsolutePath(), dataDir, backupDir.getAbsolutePath(), overrides);}
0
public void testEventTakePutErrorCount() throws Exception
{    final long usableSpaceRefreshInterval = 1;    FileChannel channel = Mockito.spy(createFileChannel());    Mockito.when(channel.createLogBuilder()).then(new Answer<Log.Builder>() {        @Override        public Log.Builder answer(InvocationOnMock invocation) throws Throwable {            Log.Builder ret = (Log.Builder) invocation.callRealMethod();            ret.setUsableSpaceRefreshInterval(usableSpaceRefreshInterval);            return ret;        }    });    channel.start();    FileChannelCounter channelCounter = channel.getChannelCounter();    Transaction tx = channel.getTransaction();    tx.begin();    channel.put(EventBuilder.withBody("test1".getBytes()));    channel.put(EventBuilder.withBody("test2".getBytes()));    tx.commit();    tx.close();    assertEquals(2, channelCounter.getEventPutAttemptCount());    assertEquals(2, channelCounter.getEventPutSuccessCount());    assertEquals(0, channelCounter.getEventPutErrorCount());    tx = channel.getTransaction();    tx.begin();    channel.take();    tx.commit();    tx.close();    assertEquals(1, channelCounter.getEventTakeAttemptCount());    assertEquals(1, channelCounter.getEventTakeSuccessCount());    assertEquals(0, channelCounter.getEventTakeErrorCount());    FileUtils.deleteDirectory(baseDir);    Thread.sleep(2 * usableSpaceRefreshInterval);    tx = channel.getTransaction();    tx.begin();    ChannelException putException = null;    try {        channel.put(EventBuilder.withBody("test".getBytes()));    } catch (ChannelException ex) {        putException = ex;    }    assertNotNull(putException);    assertTrue(putException.getCause() instanceof IOException);    assertEquals(3, channelCounter.getEventPutAttemptCount());    assertEquals(2, channelCounter.getEventPutSuccessCount());    assertEquals(1, channelCounter.getEventPutErrorCount());    ChannelException takeException = null;    try {                channel.take();    } catch (ChannelException ex) {        takeException = ex;    }    assertNotNull(takeException);    assertTrue(takeException.getCause() instanceof IOException);    assertEquals(2, channelCounter.getEventTakeAttemptCount());    assertEquals(1, channelCounter.getEventTakeSuccessCount());    assertEquals(1, channelCounter.getEventTakeErrorCount());}
0
public Log.Builder answer(InvocationOnMock invocation) throws Throwable
{    Log.Builder ret = (Log.Builder) invocation.callRealMethod();    ret.setUsableSpaceRefreshInterval(usableSpaceRefreshInterval);    return ret;}
0
public void testCorruptEventTaken() throws Exception
{    FileChannel channel = createFileChannel(Collections.singletonMap(FileChannelConfiguration.FSYNC_PER_TXN, "false"));    channel.start();    FileChannelCounter channelCounter = channel.getChannelCounter();    Transaction tx = channel.getTransaction();    tx.begin();    channel.put(EventBuilder.withBody("test".getBytes()));    tx.commit();    tx.close();    byte[] data = FileUtils.readFileToByteArray(new File(dataDirs[0], "log-1"));        data[0] = LogFile.OP_EOF;    FileUtils.writeByteArrayToFile(new File(dataDirs[0], "log-1"), data);    tx = channel.getTransaction();    tx.begin();    try {        channel.take();    } catch (Throwable t) {                                        Assert.fail("No exception should be thrown as fsyncPerTransaction is false");    }    assertEquals(1, channelCounter.getEventTakeAttemptCount());    assertEquals(0, channelCounter.getEventTakeSuccessCount());    assertEquals(1, channelCounter.getEventTakeErrorCount());}
0
public void testCheckpointWriteErrorCount() throws Exception
{    int checkpointInterval = 1500;    final FileChannel channel = createFileChannel(Collections.singletonMap(FileChannelConfiguration.CHECKPOINT_INTERVAL, String.valueOf(checkpointInterval)));    channel.start();    Transaction tx = channel.getTransaction();    tx.begin();    channel.put(EventBuilder.withBody("test".getBytes()));    tx.commit();    tx.close();    final long beforeCheckpointWrite = System.currentTimeMillis();        assertEventuallyTrue("checkpoint should have been written", new BooleanPredicate() {        @Override        public boolean get() {            return new File(checkpointDir, "checkpoint").lastModified() > beforeCheckpointWrite;        }    }, checkpointInterval * 3);    assertEquals(0, channel.getChannelCounter().getCheckpointWriteErrorCount());    FileUtils.deleteDirectory(baseDir);        assertEventuallyTrue("checkpointWriterErrorCount should be 1", new BooleanPredicate() {        @Override        public boolean get() {            return channel.getChannelCounter().getCheckpointWriteErrorCount() == 1;        }    }, checkpointInterval * 3);}
0
public boolean get()
{    return new File(checkpointDir, "checkpoint").lastModified() > beforeCheckpointWrite;}
0
public boolean get()
{    return channel.getChannelCounter().getCheckpointWriteErrorCount() == 1;}
0
public void testHealthy() throws Exception
{    FileChannel channel = createFileChannel();    assertEquals(0, channel.getChannelCounter().getUnhealthy());    assertEquals(1, channel.getChannelCounter().getClosed());    assertFalse(channel.getChannelCounter().isOpen());    channel.start();    assertEquals(0, channel.getChannelCounter().getUnhealthy());    assertEquals(0, channel.getChannelCounter().getClosed());    assertTrue(channel.getChannelCounter().isOpen());}
0
public void testUnhealthy() throws Exception
{    FileChannel channel = createFileChannel();    assertEquals(0, channel.getChannelCounter().getUnhealthy());    assertEquals(1, channel.getChannelCounter().getClosed());    assertFalse(channel.getChannelCounter().isOpen());    FileUtils.write(new File(dataDirs[0], "log-1"), "invalid data file content");    channel.start();    assertEquals(1, channel.getChannelCounter().getUnhealthy());    assertEquals(1, channel.getChannelCounter().getClosed());    assertFalse(channel.getChannelCounter().isOpen());}
0
public void testCheckpointBackupWriteErrorShouldIncreaseCounter() throws IOException, InterruptedException
{    FileChannelCounter fileChannelCounter = new FileChannelCounter("test");    File checkpointFile = File.createTempFile("checkpoint", ".tmp");    File backupDir = Files.createTempDirectory("checkpoint").toFile();    backupDir.deleteOnExit();    checkpointFile.deleteOnExit();    EventQueueBackingStoreFileV3 backingStoreFileV3 = new EventQueueBackingStoreFileV3(checkpointFile, 1, "test", fileChannelCounter, backupDir, true, false);        backingStoreFileV3.checkpoint();        assertEventuallyTrue("checkpoint backup write failure should increase counter to 1", new BooleanPredicate() {        @Override        public boolean get() {            return fileChannelCounter.getCheckpointBackupWriteErrorCount() == 1;        }    }, 100);}
0
public boolean get()
{    return fileChannelCounter.getCheckpointBackupWriteErrorCount() == 1;}
0
public void testCheckpointBackupWriteErrorShouldIncreaseCounter2() throws Exception
{    int checkpointInterval = 1500;    Map config = new HashMap();    config.put(FileChannelConfiguration.CHECKPOINT_INTERVAL, String.valueOf(checkpointInterval));    config.put(FileChannelConfiguration.USE_DUAL_CHECKPOINTS, "true");    final FileChannel channel = createFileChannel(Collections.unmodifiableMap(config));    channel.start();    Transaction tx = channel.getTransaction();    tx.begin();    channel.put(EventBuilder.withBody("test".getBytes()));    tx.commit();    tx.close();    final long beforeCheckpointWrite = System.currentTimeMillis();        assertEventuallyTrue("checkpoint backup should have been written", new BooleanPredicate() {        @Override        public boolean get() {            return new File(backupDir, "checkpoint").lastModified() > beforeCheckpointWrite;        }    }, checkpointInterval * 3);    assertEquals(0, channel.getChannelCounter().getCheckpointBackupWriteErrorCount());    FileUtils.deleteDirectory(backupDir);    tx = channel.getTransaction();    tx.begin();    channel.put(EventBuilder.withBody("test2".getBytes()));    tx.commit();    tx.close();        assertEventuallyTrue("checkpointBackupWriteErrorCount should be 1", new BooleanPredicate() {        @Override        public boolean get() {            return channel.getChannelCounter().getCheckpointBackupWriteErrorCount() >= 1;        }    }, checkpointInterval * 3);}
0
public boolean get()
{    return new File(backupDir, "checkpoint").lastModified() > beforeCheckpointWrite;}
0
public boolean get()
{    return channel.getChannelCounter().getCheckpointBackupWriteErrorCount() >= 1;}
0
private static void assertEventuallyTrue(String description, BooleanPredicate expression, long timeoutMillis) throws InterruptedException
{    long start = System.currentTimeMillis();    while (System.currentTimeMillis() < start + timeoutMillis) {        if (expression.get())            break;        Thread.sleep(timeoutMillis / 10);    }    assertTrue(description, expression.get());}
0
public void setup() throws Exception
{    super.setup();}
0
public void teardown()
{    super.teardown();}
0
public void testFileFormatV2postFLUME1432() throws Exception
{    TestUtils.copyDecompressed("fileformat-v2-checkpoint.gz", new File(checkpointDir, "checkpoint"));    for (int i = 0; i < dataDirs.length; i++) {        int fileIndex = i + 1;        TestUtils.copyDecompressed("fileformat-v2-log-" + fileIndex + ".gz", new File(dataDirs[i], "log-" + fileIndex));    }    Map<String, String> overrides = Maps.newHashMap();    overrides.put(FileChannelConfiguration.CAPACITY, String.valueOf(10));    overrides.put(FileChannelConfiguration.TRANSACTION_CAPACITY, String.valueOf(10));    channel = createFileChannel(overrides);    channel.start();    Assert.assertTrue(channel.isOpen());    Set<String> events = takeEvents(channel, 1);    Set<String> expected = new HashSet<String>();    expected.addAll(Arrays.asList((new String[] { "2684", "2685", "2686", "2687", "2688", "2689", "2690", "2691" })));    compareInputAndOut(expected, events);}
0
public void testFileFormatV2PreFLUME1432LogReplayV1() throws Exception
{    doTestFileFormatV2PreFLUME1432(true);}
0
public void testFileFormatV2PreFLUME1432LogReplayV2() throws Exception
{    doTestFileFormatV2PreFLUME1432(false);}
0
public void doTestFileFormatV2PreFLUME1432(boolean useLogReplayV1) throws Exception
{    TestUtils.copyDecompressed("fileformat-v2-pre-FLUME-1432-checkpoint.gz", new File(checkpointDir, "checkpoint"));    for (int i = 0; i < dataDirs.length; i++) {        int fileIndex = i + 1;        TestUtils.copyDecompressed("fileformat-v2-pre-FLUME-1432-log-" + fileIndex + ".gz", new File(dataDirs[i], "log-" + fileIndex));    }    Map<String, String> overrides = Maps.newHashMap();    overrides.put(FileChannelConfiguration.CAPACITY, String.valueOf(10000));    overrides.put(FileChannelConfiguration.USE_LOG_REPLAY_V1, String.valueOf(useLogReplayV1));    channel = createFileChannel(overrides);    channel.start();    Assert.assertTrue(channel.isOpen());    Set<String> events = takeEvents(channel, 1);    Assert.assertEquals(50, events.size());}
0
public void setup() throws Exception
{    super.setup();}
0
public void teardown()
{    super.teardown();}
0
protected FileChannel createFileChannel(Map<String, String> overrides)
{        overrides.put(FileChannelConfiguration.CHECKPOINT_INTERVAL, "6000000");    return TestUtils.createFileChannel(checkpointDir.getAbsolutePath(), dataDir, backupDir.getAbsolutePath(), overrides);}
0
public void testRestartLogReplayV1() throws Exception
{    doTestRestart(true, false, false, false);}
0
public void testRestartLogReplayV2() throws Exception
{    doTestRestart(false, false, false, false);}
0
public void testFastReplayV1() throws Exception
{    doTestRestart(true, true, true, true);}
0
public void testFastReplayV2() throws Exception
{    doTestRestart(false, true, true, true);}
0
public void testFastReplayNegativeTestV1() throws Exception
{    doTestRestart(true, true, false, true);}
0
public void testFastReplayNegativeTestV2() throws Exception
{    doTestRestart(false, true, false, true);}
0
public void testNormalReplayV1() throws Exception
{    doTestRestart(true, true, true, false);}
0
public void testNormalReplayV2() throws Exception
{    doTestRestart(false, true, true, false);}
0
public void doTestRestart(boolean useLogReplayV1, boolean forceCheckpoint, boolean deleteCheckpoint, boolean useFastReplay) throws Exception
{    Map<String, String> overrides = Maps.newHashMap();    overrides.put(FileChannelConfiguration.USE_LOG_REPLAY_V1, String.valueOf(useLogReplayV1));    overrides.put(FileChannelConfiguration.USE_FAST_REPLAY, String.valueOf(useFastReplay));    channel = createFileChannel(overrides);    channel.start();    Assert.assertTrue(channel.isOpen());    Set<String> in = fillChannel(channel, "restart");    if (forceCheckpoint) {        forceCheckpoint(channel);    }    channel.stop();    if (deleteCheckpoint) {        File checkpoint = new File(checkpointDir, "checkpoint");        Assert.assertTrue(checkpoint.delete());        File checkpointMetaData = Serialization.getMetaDataFile(checkpoint);        Assert.assertTrue(checkpointMetaData.delete());    }    channel = createFileChannel(overrides);    channel.start();    Assert.assertTrue(channel.isOpen());    Set<String> out = consumeChannel(channel);    compareInputAndOut(in, out);}
0
public void testRestartWhenMetaDataExistsButCheckpointDoesNot() throws Exception
{    doTestRestartWhenMetaDataExistsButCheckpointDoesNot(false);}
0
public void testRestartWhenMetaDataExistsButCheckpointDoesNotWithBackup() throws Exception
{    doTestRestartWhenMetaDataExistsButCheckpointDoesNot(true);}
0
private void doTestRestartWhenMetaDataExistsButCheckpointDoesNot(boolean backup) throws Exception
{    Map<String, String> overrides = Maps.newHashMap();    overrides.put(FileChannelConfiguration.USE_DUAL_CHECKPOINTS, String.valueOf(backup));    channel = createFileChannel(overrides);    channel.start();    Assert.assertTrue(channel.isOpen());    Set<String> in = putEvents(channel, "restart", 10, 100);    Assert.assertEquals(100, in.size());    forceCheckpoint(channel);    if (backup) {        Thread.sleep(2000);    }    channel.stop();    File checkpoint = new File(checkpointDir, "checkpoint");    Assert.assertTrue(checkpoint.delete());    File checkpointMetaData = Serialization.getMetaDataFile(checkpoint);    Assert.assertTrue(checkpointMetaData.exists());    channel = createFileChannel(overrides);    channel.start();    Assert.assertTrue(channel.isOpen());    Assert.assertTrue(checkpoint.exists());    Assert.assertTrue(checkpointMetaData.exists());    Assert.assertTrue(!backup || channel.checkpointBackupRestored());    Set<String> out = consumeChannel(channel);    compareInputAndOut(in, out);}
0
public void testRestartWhenCheckpointExistsButMetaDoesNot() throws Exception
{    doTestRestartWhenCheckpointExistsButMetaDoesNot(false);}
0
public void testRestartWhenCheckpointExistsButMetaDoesNotWithBackup() throws Exception
{    doTestRestartWhenCheckpointExistsButMetaDoesNot(true);}
0
private void doTestRestartWhenCheckpointExistsButMetaDoesNot(boolean backup) throws Exception
{    Map<String, String> overrides = Maps.newHashMap();    overrides.put(FileChannelConfiguration.USE_DUAL_CHECKPOINTS, String.valueOf(backup));    channel = createFileChannel(overrides);    channel.start();    Assert.assertTrue(channel.isOpen());    Set<String> in = putEvents(channel, "restart", 10, 100);    Assert.assertEquals(100, in.size());    forceCheckpoint(channel);    if (backup) {        Thread.sleep(2000);    }    channel.stop();    File checkpoint = new File(checkpointDir, "checkpoint");    File checkpointMetaData = Serialization.getMetaDataFile(checkpoint);    Assert.assertTrue(checkpointMetaData.delete());    Assert.assertTrue(checkpoint.exists());    channel = createFileChannel(overrides);    channel.start();    Assert.assertTrue(channel.isOpen());    Assert.assertTrue(checkpoint.exists());    Assert.assertTrue(checkpointMetaData.exists());    Assert.assertTrue(!backup || channel.checkpointBackupRestored());    Set<String> out = consumeChannel(channel);    compareInputAndOut(in, out);}
0
public void testRestartWhenNoCheckpointExists() throws Exception
{    doTestRestartWhenNoCheckpointExists(false);}
0
public void testRestartWhenNoCheckpointExistsWithBackup() throws Exception
{    doTestRestartWhenNoCheckpointExists(true);}
0
private void doTestRestartWhenNoCheckpointExists(boolean backup) throws Exception
{    Map<String, String> overrides = Maps.newHashMap();    overrides.put(FileChannelConfiguration.USE_DUAL_CHECKPOINTS, String.valueOf(backup));    channel = createFileChannel(overrides);    channel.start();    Assert.assertTrue(channel.isOpen());    Set<String> in = putEvents(channel, "restart", 10, 100);    Assert.assertEquals(100, in.size());    forceCheckpoint(channel);    if (backup) {        Thread.sleep(2000);    }    channel.stop();    File checkpoint = new File(checkpointDir, "checkpoint");    File checkpointMetaData = Serialization.getMetaDataFile(checkpoint);    Assert.assertTrue(checkpointMetaData.delete());    Assert.assertTrue(checkpoint.delete());    channel = createFileChannel(overrides);    channel.start();    Assert.assertTrue(channel.isOpen());    Assert.assertTrue(checkpoint.exists());    Assert.assertTrue(checkpointMetaData.exists());    Assert.assertTrue(!backup || channel.checkpointBackupRestored());    Set<String> out = consumeChannel(channel);    compareInputAndOut(in, out);}
0
public void testBadCheckpointVersion() throws Exception
{    doTestBadCheckpointVersion(false);}
0
public void testBadCheckpointVersionWithBackup() throws Exception
{    doTestBadCheckpointVersion(true);}
0
private void doTestBadCheckpointVersion(boolean backup) throws Exception
{    Map<String, String> overrides = Maps.newHashMap();    overrides.put(FileChannelConfiguration.USE_DUAL_CHECKPOINTS, String.valueOf(backup));    channel = createFileChannel(overrides);    channel.start();    Assert.assertTrue(channel.isOpen());    Set<String> in = putEvents(channel, "restart", 10, 100);    Assert.assertEquals(100, in.size());    forceCheckpoint(channel);    if (backup) {        Thread.sleep(2000);    }    channel.stop();    File checkpoint = new File(checkpointDir, "checkpoint");    RandomAccessFile writer = new RandomAccessFile(checkpoint, "rw");    writer.seek(EventQueueBackingStoreFile.INDEX_VERSION * Serialization.SIZE_OF_LONG);    writer.writeLong(2L);    writer.getFD().sync();    writer.close();    channel = createFileChannel(overrides);    channel.start();    Assert.assertTrue(channel.isOpen());    Assert.assertTrue(!backup || channel.checkpointBackupRestored());    Set<String> out = consumeChannel(channel);    compareInputAndOut(in, out);}
0
public void testBadCheckpointMetaVersion() throws Exception
{    doTestBadCheckpointMetaVersion(false);}
0
public void testBadCheckpointMetaVersionWithBackup() throws Exception
{    doTestBadCheckpointMetaVersion(true);}
0
private void doTestBadCheckpointMetaVersion(boolean backup) throws Exception
{    Map<String, String> overrides = Maps.newHashMap();    overrides.put(FileChannelConfiguration.USE_DUAL_CHECKPOINTS, String.valueOf(backup));    channel = createFileChannel(overrides);    channel.start();    Assert.assertTrue(channel.isOpen());    Set<String> in = putEvents(channel, "restart", 10, 100);    Assert.assertEquals(100, in.size());    forceCheckpoint(channel);    if (backup) {        Thread.sleep(2000);    }    channel.stop();    File checkpoint = new File(checkpointDir, "checkpoint");    FileInputStream is = new FileInputStream(Serialization.getMetaDataFile(checkpoint));    ProtosFactory.Checkpoint meta = ProtosFactory.Checkpoint.parseDelimitedFrom(is);    Assert.assertNotNull(meta);    is.close();    FileOutputStream os = new FileOutputStream(Serialization.getMetaDataFile(checkpoint));    meta.toBuilder().setVersion(2).build().writeDelimitedTo(os);    os.flush();    channel = createFileChannel(overrides);    channel.start();    Assert.assertTrue(channel.isOpen());    Assert.assertTrue(!backup || channel.checkpointBackupRestored());    Set<String> out = consumeChannel(channel);    compareInputAndOut(in, out);}
0
public void testDifferingOrderIDCheckpointAndMetaVersion() throws Exception
{    doTestDifferingOrderIDCheckpointAndMetaVersion(false);}
0
public void testDifferingOrderIDCheckpointAndMetaVersionWithBackup() throws Exception
{    doTestDifferingOrderIDCheckpointAndMetaVersion(true);}
0
private void doTestDifferingOrderIDCheckpointAndMetaVersion(boolean backup) throws Exception
{    Map<String, String> overrides = Maps.newHashMap();    overrides.put(FileChannelConfiguration.USE_DUAL_CHECKPOINTS, String.valueOf(backup));    channel = createFileChannel(overrides);    channel.start();    Assert.assertTrue(channel.isOpen());    Set<String> in = putEvents(channel, "restart", 10, 100);    Assert.assertEquals(100, in.size());    forceCheckpoint(channel);    if (backup) {        Thread.sleep(2000);    }    channel.stop();    File checkpoint = new File(checkpointDir, "checkpoint");    FileInputStream is = new FileInputStream(Serialization.getMetaDataFile(checkpoint));    ProtosFactory.Checkpoint meta = ProtosFactory.Checkpoint.parseDelimitedFrom(is);    Assert.assertNotNull(meta);    is.close();    FileOutputStream os = new FileOutputStream(Serialization.getMetaDataFile(checkpoint));    meta.toBuilder().setWriteOrderID(12).build().writeDelimitedTo(os);    os.flush();    channel = createFileChannel(overrides);    channel.start();    Assert.assertTrue(channel.isOpen());    Assert.assertTrue(!backup || channel.checkpointBackupRestored());    Set<String> out = consumeChannel(channel);    compareInputAndOut(in, out);}
0
public void testIncompleteCheckpoint() throws Exception
{    doTestIncompleteCheckpoint(false);}
0
public void testIncompleteCheckpointWithCheckpoint() throws Exception
{    doTestIncompleteCheckpoint(true);}
0
private void doTestIncompleteCheckpoint(boolean backup) throws Exception
{    Map<String, String> overrides = Maps.newHashMap();    overrides.put(FileChannelConfiguration.USE_DUAL_CHECKPOINTS, String.valueOf(backup));    channel = createFileChannel(overrides);    channel.start();    Assert.assertTrue(channel.isOpen());    Set<String> in = putEvents(channel, "restart", 10, 100);    Assert.assertEquals(100, in.size());    forceCheckpoint(channel);    if (backup) {        Thread.sleep(2000);    }    channel.stop();    File checkpoint = new File(checkpointDir, "checkpoint");    RandomAccessFile writer = new RandomAccessFile(checkpoint, "rw");    writer.seek(EventQueueBackingStoreFile.INDEX_CHECKPOINT_MARKER * Serialization.SIZE_OF_LONG);    writer.writeLong(EventQueueBackingStoreFile.CHECKPOINT_INCOMPLETE);    writer.getFD().sync();    writer.close();    channel = createFileChannel(overrides);    channel.start();    Assert.assertTrue(channel.isOpen());    Assert.assertTrue(!backup || channel.checkpointBackupRestored());    Set<String> out = consumeChannel(channel);    compareInputAndOut(in, out);}
0
public void testCorruptInflightPuts() throws Exception
{    doTestCorruptInflights("inflightputs", false);}
0
public void testCorruptInflightPutsWithBackup() throws Exception
{    doTestCorruptInflights("inflightputs", true);}
0
public void testCorruptInflightTakes() throws Exception
{    doTestCorruptInflights("inflighttakes", false);}
0
public void testCorruptInflightTakesWithBackup() throws Exception
{    doTestCorruptInflights("inflighttakes", true);}
0
public void testFastReplayWithCheckpoint() throws Exception
{    testFastReplay(false, true);}
0
public void testFastReplayWithBadCheckpoint() throws Exception
{    testFastReplay(true, true);}
0
public void testNoFastReplayWithCheckpoint() throws Exception
{    testFastReplay(false, false);}
0
public void testNoFastReplayWithBadCheckpoint() throws Exception
{    testFastReplay(true, false);}
0
private void testFastReplay(boolean shouldCorruptCheckpoint, boolean useFastReplay) throws Exception
{    Map<String, String> overrides = Maps.newHashMap();    overrides.put(FileChannelConfiguration.USE_FAST_REPLAY, String.valueOf(useFastReplay));    channel = createFileChannel(overrides);    channel.start();    Assert.assertTrue(channel.isOpen());    Set<String> in = putEvents(channel, "restart", 10, 100);    Assert.assertEquals(100, in.size());    forceCheckpoint(channel);    channel.stop();    if (shouldCorruptCheckpoint) {        File checkpoint = new File(checkpointDir, "checkpoint");        RandomAccessFile writer = new RandomAccessFile(Serialization.getMetaDataFile(checkpoint), "rw");        writer.seek(10);        writer.writeLong(new Random().nextLong());        writer.getFD().sync();        writer.close();    }    channel = createFileChannel(overrides);    channel.start();    Assert.assertTrue(channel.isOpen());    Set<String> out = consumeChannel(channel);    if (useFastReplay && shouldCorruptCheckpoint) {        Assert.assertTrue(channel.didFastReplay());    } else {        Assert.assertFalse(channel.didFastReplay());    }    compareInputAndOut(in, out);}
0
private void doTestCorruptInflights(String name, boolean backup) throws Exception
{    Map<String, String> overrides = Maps.newHashMap();    overrides.put(FileChannelConfiguration.USE_DUAL_CHECKPOINTS, String.valueOf(backup));    channel = createFileChannel(overrides);    channel.start();    Assert.assertTrue(channel.isOpen());    final Set<String> in1 = putEvents(channel, "restart-", 10, 100);    Assert.assertEquals(100, in1.size());    Executors.newSingleThreadScheduledExecutor().submit(new Runnable() {        @Override        public void run() {            Transaction tx = channel.getTransaction();            Set<String> out1 = takeWithoutCommit(channel, tx, 100);            Assert.assertEquals(100, out1.size());        }    });    Transaction tx = channel.getTransaction();    Set<String> in2 = putWithoutCommit(channel, tx, "restart", 100);    Assert.assertEquals(100, in2.size());    forceCheckpoint(channel);    if (backup) {        Thread.sleep(2000);    }    tx.commit();    tx.close();    channel.stop();    File inflight = new File(checkpointDir, name);    RandomAccessFile writer = new RandomAccessFile(inflight, "rw");    writer.write(new Random().nextInt());    writer.close();    channel = createFileChannel(overrides);    channel.start();    Assert.assertTrue(channel.isOpen());    Assert.assertTrue(!backup || channel.checkpointBackupRestored());    Set<String> out = consumeChannel(channel);    in1.addAll(in2);    compareInputAndOut(in1, out);}
0
public void run()
{    Transaction tx = channel.getTransaction();    Set<String> out1 = takeWithoutCommit(channel, tx, 100);    Assert.assertEquals(100, out1.size());}
0
public void testTruncatedCheckpointMeta() throws Exception
{    doTestTruncatedCheckpointMeta(false);}
0
public void testTruncatedCheckpointMetaWithBackup() throws Exception
{    doTestTruncatedCheckpointMeta(true);}
0
private void doTestTruncatedCheckpointMeta(boolean backup) throws Exception
{    Map<String, String> overrides = Maps.newHashMap();    overrides.put(FileChannelConfiguration.USE_DUAL_CHECKPOINTS, String.valueOf(backup));    channel = createFileChannel(overrides);    channel.start();    Assert.assertTrue(channel.isOpen());    Set<String> in = putEvents(channel, "restart", 10, 100);    Assert.assertEquals(100, in.size());    forceCheckpoint(channel);    if (backup) {        Thread.sleep(2000);    }    channel.stop();    File checkpoint = new File(checkpointDir, "checkpoint");    RandomAccessFile writer = new RandomAccessFile(Serialization.getMetaDataFile(checkpoint), "rw");    writer.setLength(0);    writer.getFD().sync();    writer.close();    channel = createFileChannel(overrides);    channel.start();    Assert.assertTrue(channel.isOpen());    Assert.assertTrue(!backup || channel.checkpointBackupRestored());    Set<String> out = consumeChannel(channel);    compareInputAndOut(in, out);}
0
public void testCorruptCheckpointMeta() throws Exception
{    doTestCorruptCheckpointMeta(false);}
0
public void testCorruptCheckpointMetaWithBackup() throws Exception
{    doTestCorruptCheckpointMeta(true);}
0
private void doTestCorruptCheckpointMeta(boolean backup) throws Exception
{    Map<String, String> overrides = Maps.newHashMap();    overrides.put(FileChannelConfiguration.USE_DUAL_CHECKPOINTS, String.valueOf(backup));    channel = createFileChannel(overrides);    channel.start();    Assert.assertTrue(channel.isOpen());    Set<String> in = putEvents(channel, "restart", 10, 100);    Assert.assertEquals(100, in.size());    forceCheckpoint(channel);    if (backup) {        Thread.sleep(2000);    }    channel.stop();    File checkpoint = new File(checkpointDir, "checkpoint");    RandomAccessFile writer = new RandomAccessFile(Serialization.getMetaDataFile(checkpoint), "rw");    writer.seek(10);    writer.writeLong(new Random().nextLong());    writer.getFD().sync();    writer.close();    channel = createFileChannel(overrides);    channel.start();    Assert.assertTrue(channel.isOpen());    Assert.assertTrue(!backup || channel.checkpointBackupRestored());    Set<String> out = consumeChannel(channel);    compareInputAndOut(in, out);}
0
private void checkIfBackupUsed(boolean backup)
{    boolean backupRestored = channel.checkpointBackupRestored();    if (backup) {        Assert.assertTrue(backupRestored);    } else {        Assert.assertFalse(backupRestored);    }}
0
public void testCorruptCheckpointVersionMostSignificant4Bytes() throws Exception
{    Map<String, String> overrides = Maps.newHashMap();    channel = createFileChannel(overrides);    channel.start();    Assert.assertTrue(channel.isOpen());    Set<String> in = putEvents(channel, "restart", 10, 100);    Assert.assertEquals(100, in.size());    forceCheckpoint(channel);    channel.stop();    File checkpoint = new File(checkpointDir, "checkpoint");    RandomAccessFile writer = new RandomAccessFile(checkpoint, "rw");    writer.seek(EventQueueBackingStoreFile.INDEX_VERSION * Serialization.SIZE_OF_LONG);    writer.write(new byte[] { (byte) 1, (byte) 5 });    writer.getFD().sync();    writer.close();    channel = createFileChannel(overrides);    channel.start();    Assert.assertTrue(channel.isOpen());    Set<String> out = consumeChannel(channel);    Assert.assertTrue(channel.didFullReplayDueToBadCheckpointException());    compareInputAndOut(in, out);}
0
public void testCorruptCheckpointCompleteMarkerMostSignificant4Bytes() throws Exception
{    Map<String, String> overrides = Maps.newHashMap();    channel = createFileChannel(overrides);    channel.start();    Assert.assertTrue(channel.isOpen());    Set<String> in = putEvents(channel, "restart", 10, 100);    Assert.assertEquals(100, in.size());    forceCheckpoint(channel);    channel.stop();    File checkpoint = new File(checkpointDir, "checkpoint");    RandomAccessFile writer = new RandomAccessFile(checkpoint, "rw");    writer.seek(EventQueueBackingStoreFile.INDEX_CHECKPOINT_MARKER * Serialization.SIZE_OF_LONG);    writer.write(new byte[] { (byte) 1, (byte) 5 });    writer.getFD().sync();    writer.close();    channel = createFileChannel(overrides);    channel.start();    Assert.assertTrue(channel.isOpen());    Set<String> out = consumeChannel(channel);    Assert.assertTrue(channel.didFullReplayDueToBadCheckpointException());    compareInputAndOut(in, out);}
0
public void testWithExtraLogs() throws Exception
{    Map<String, String> overrides = Maps.newHashMap();    overrides.put(FileChannelConfiguration.CAPACITY, "10");    overrides.put(FileChannelConfiguration.TRANSACTION_CAPACITY, "10");    channel = createFileChannel(overrides);    channel.start();    Assert.assertTrue(channel.isOpen());    Set<String> in = fillChannel(channel, "extralogs");    for (int i = 0; i < dataDirs.length; i++) {        File file = new File(dataDirs[i], Log.PREFIX + (1000 + i));        Assert.assertTrue(file.createNewFile());        Assert.assertTrue(file.length() == 0);        File metaDataFile = Serialization.getMetaDataFile(file);        File metaDataTempFile = Serialization.getMetaDataTempFile(metaDataFile);        Assert.assertTrue(metaDataTempFile.createNewFile());    }    channel.stop();    channel = createFileChannel(overrides);    channel.start();    Assert.assertTrue(channel.isOpen());    Set<String> out = consumeChannel(channel);    compareInputAndOut(in, out);}
0
public void testBackupUsedEnsureNoFullReplayWithoutCompression() throws Exception
{    testBackupUsedEnsureNoFullReplay(false);}
0
public void testBackupUsedEnsureNoFullReplayWithCompression() throws Exception
{    testBackupUsedEnsureNoFullReplay(true);}
0
private void testBackupUsedEnsureNoFullReplay(boolean compressedBackup) throws Exception
{    File dataDir = Files.createTempDir();    File tempBackup = Files.createTempDir();    Map<String, String> overrides = Maps.newHashMap();    overrides.put(FileChannelConfiguration.DATA_DIRS, dataDir.getAbsolutePath());    overrides.put(FileChannelConfiguration.USE_DUAL_CHECKPOINTS, "true");    overrides.put(FileChannelConfiguration.COMPRESS_BACKUP_CHECKPOINT, String.valueOf(compressedBackup));    channel = createFileChannel(overrides);    channel.start();    Assert.assertTrue(channel.isOpen());    Set<String> in = putEvents(channel, "restart", 10, 100);    Assert.assertEquals(100, in.size());    Thread.sleep(5000);    forceCheckpoint(channel);    Thread.sleep(5000);    in = putEvents(channel, "restart", 10, 100);    takeEvents(channel, 10, 100);    Assert.assertEquals(100, in.size());    for (File file : backupDir.listFiles()) {        if (file.getName().equals(Log.FILE_LOCK)) {            continue;        }        Files.copy(file, new File(tempBackup, file.getName()));    }    forceCheckpoint(channel);    channel.stop();    Serialization.deleteAllFiles(checkpointDir, Log.EXCLUDES);                    Serialization.deleteAllFiles(backupDir, Log.EXCLUDES);    for (File file : tempBackup.listFiles()) {        if (file.getName().equals(Log.FILE_LOCK)) {            continue;        }        Files.copy(file, new File(backupDir, file.getName()));    }    channel = createFileChannel(overrides);    channel.start();    Assert.assertTrue(channel.isOpen());    checkIfBackupUsed(true);    Assert.assertEquals(100, channel.getLog().getPutCount());    Assert.assertEquals(20, channel.getLog().getCommittedCount());    Assert.assertEquals(100, channel.getLog().getTakeCount());    Assert.assertEquals(0, channel.getLog().getRollbackCount());        Assert.assertEquals(220, channel.getLog().getReadCount());    consumeChannel(channel);    FileUtils.deleteQuietly(dataDir);    FileUtils.deleteQuietly(tempBackup);}
0
public void testDataFilesRequiredByBackupNotDeleted() throws Exception
{    Map<String, String> overrides = Maps.newHashMap();    overrides.put(FileChannelConfiguration.USE_DUAL_CHECKPOINTS, "true");    overrides.put(FileChannelConfiguration.MAX_FILE_SIZE, "1000");    channel = createFileChannel(overrides);    channel.start();    String prefix = "abcdefghijklmnopqrstuvwxyzabcdefghijklmnopqrstuvwxyz";    Assert.assertTrue(channel.isOpen());    putEvents(channel, prefix, 10, 100);    Set<String> origFiles = Sets.newHashSet();    for (File dir : dataDirs) {        origFiles.addAll(Lists.newArrayList(dir.list()));    }    forceCheckpoint(channel);    takeEvents(channel, 10, 50);    long beforeSecondCheckpoint = System.currentTimeMillis();    forceCheckpoint(channel);    Set<String> newFiles = Sets.newHashSet();    int olderThanCheckpoint = 0;    int totalMetaFiles = 0;    for (File dir : dataDirs) {        File[] metadataFiles = dir.listFiles(new FilenameFilter() {            @Override            public boolean accept(File dir, String name) {                if (name.endsWith(".meta")) {                    return true;                }                return false;            }        });        totalMetaFiles = metadataFiles.length;        for (File metadataFile : metadataFiles) {            if (metadataFile.lastModified() < beforeSecondCheckpoint) {                olderThanCheckpoint++;            }        }        newFiles.addAll(Lists.newArrayList(dir.list()));    }    /*     * Files which are not required by the new checkpoint should not have been     * modified by the checkpoint.     */    Assert.assertTrue(olderThanCheckpoint > 0);    Assert.assertTrue(totalMetaFiles != olderThanCheckpoint);    /*     * All files needed by original checkpoint should still be there.     */    Assert.assertTrue(newFiles.containsAll(origFiles));    takeEvents(channel, 10, 50);    forceCheckpoint(channel);    newFiles = Sets.newHashSet();    for (File dir : dataDirs) {        newFiles.addAll(Lists.newArrayList(dir.list()));    }    Assert.assertTrue(!newFiles.containsAll(origFiles));}
0
public boolean accept(File dir, String name)
{    if (name.endsWith(".meta")) {        return true;    }    return false;}
0
public void testSlowBackup() throws Throwable
{    Map<String, String> overrides = Maps.newHashMap();    overrides.put(FileChannelConfiguration.USE_DUAL_CHECKPOINTS, "true");    overrides.put(FileChannelConfiguration.MAX_FILE_SIZE, "1000");    channel = createFileChannel(overrides);    channel.start();    Assert.assertTrue(channel.isOpen());    Set<String> in = putEvents(channel, "restart", 10, 100);    Assert.assertEquals(100, in.size());    slowdownBackup(channel);    forceCheckpoint(channel);    in = putEvents(channel, "restart", 10, 100);    takeEvents(channel, 10, 100);    Assert.assertEquals(100, in.size());    try {        forceCheckpoint(channel);    } catch (ReflectionError ex) {        throw ex.getCause();    } finally {        channel.stop();    }}
0
public void testCompressBackup() throws Throwable
{    Map<String, String> overrides = Maps.newHashMap();    overrides.put(FileChannelConfiguration.USE_DUAL_CHECKPOINTS, "true");    overrides.put(FileChannelConfiguration.MAX_FILE_SIZE, "1000");    overrides.put(FileChannelConfiguration.COMPRESS_BACKUP_CHECKPOINT, "true");    channel = createFileChannel(overrides);    channel.start();    Assert.assertTrue(channel.isOpen());    putEvents(channel, "restart", 10, 100);    forceCheckpoint(channel);        Thread.sleep(2000);    Assert.assertTrue(compressedBackupCheckpoint.exists());    Serialization.decompressFile(compressedBackupCheckpoint, uncompressedBackupCheckpoint);    File checkpoint = new File(checkpointDir, "checkpoint");    Assert.assertTrue(FileUtils.contentEquals(checkpoint, uncompressedBackupCheckpoint));    channel.stop();}
0
public void testToggleCheckpointCompressionFromTrueToFalse() throws Exception
{    restartToggleCompression(true);}
0
public void testToggleCheckpointCompressionFromFalseToTrue() throws Exception
{    restartToggleCompression(false);}
0
public void restartToggleCompression(boolean originalCheckpointCompressed) throws Exception
{    Map<String, String> overrides = Maps.newHashMap();    overrides.put(FileChannelConfiguration.USE_DUAL_CHECKPOINTS, "true");    overrides.put(FileChannelConfiguration.MAX_FILE_SIZE, "1000");    overrides.put(FileChannelConfiguration.TRANSACTION_CAPACITY, "1000");    overrides.put(FileChannelConfiguration.CAPACITY, "1000");    overrides.put(FileChannelConfiguration.COMPRESS_BACKUP_CHECKPOINT, String.valueOf(originalCheckpointCompressed));    channel = createFileChannel(overrides);    channel.start();    Assert.assertTrue(channel.isOpen());    Set<String> in = fillChannel(channel, "restart");    forceCheckpoint(channel);    Thread.sleep(2000);    Assert.assertEquals(compressedBackupCheckpoint.exists(), originalCheckpointCompressed);    Assert.assertEquals(uncompressedBackupCheckpoint.exists(), !originalCheckpointCompressed);    channel.stop();    File checkpoint = new File(checkpointDir, "checkpoint");    Assert.assertTrue(checkpoint.delete());    File checkpointMetaData = Serialization.getMetaDataFile(checkpoint);    Assert.assertTrue(checkpointMetaData.delete());    overrides.put(FileChannelConfiguration.COMPRESS_BACKUP_CHECKPOINT, String.valueOf(!originalCheckpointCompressed));    channel = createFileChannel(overrides);    channel.start();    Assert.assertTrue(channel.isOpen());    Set<String> out = consumeChannel(channel);    compareInputAndOut(in, out);    forceCheckpoint(channel);    Thread.sleep(2000);    Assert.assertEquals(compressedBackupCheckpoint.exists(), !originalCheckpointCompressed);    Assert.assertEquals(uncompressedBackupCheckpoint.exists(), originalCheckpointCompressed);}
0
private static void slowdownBackup(FileChannel channel)
{    Log log = field("log").ofType(Log.class).in(channel).get();    FlumeEventQueue queue = field("queue").ofType(FlumeEventQueue.class).in(log).get();    EventQueueBackingStore backingStore = field("backingStore").ofType(EventQueueBackingStore.class).in(queue).get();    field("slowdownBackup").ofType(Boolean.class).in(backingStore).set(true);}
0
public void setup() throws Exception
{    super.setup();}
0
public void teardown()
{    super.teardown();}
0
public void testRollbackAfterNoPutTake() throws Exception
{    channel.start();    Assert.assertTrue(channel.isOpen());    Transaction transaction;    transaction = channel.getTransaction();    transaction.begin();    transaction.rollback();    transaction.close();        channel.stop();    channel = createFileChannel();    channel.start();    Assert.assertTrue(channel.isOpen());    transaction = channel.getTransaction();    transaction.begin();    Assert.assertNull(channel.take());    transaction.commit();    transaction.close();}
0
public void testRollbackSimulatedCrash() throws Exception
{    channel.start();    Assert.assertTrue(channel.isOpen());    int numEvents = 50;    Set<String> in = putEvents(channel, "rollback", 1, numEvents);    Transaction transaction;        transaction = channel.getTransaction();    transaction.begin();    channel.put(EventBuilder.withBody("rolled back".getBytes(Charsets.UTF_8)));    transaction.rollback();    transaction.close();        channel.stop();    channel = createFileChannel();    channel.start();    Assert.assertTrue(channel.isOpen());        Set<String> out = takeEvents(channel, 1, numEvents);    compareInputAndOut(in, out);}
0
public void testRollbackSimulatedCrashWithSink() throws Exception
{    channel.start();    Assert.assertTrue(channel.isOpen());    int numEvents = 100;    LoggerSink sink = new LoggerSink();    sink.setChannel(channel);        CountingSinkRunner runner = new CountingSinkRunner(sink, numEvents - 1);    runner.start();    putEvents(channel, "rollback", 10, numEvents);    Transaction transaction;        transaction = channel.getTransaction();    transaction.begin();    byte[] bytes = "rolled back".getBytes(Charsets.UTF_8);    channel.put(EventBuilder.withBody(bytes));    transaction.rollback();    transaction.close();    while (runner.isAlive()) {        Thread.sleep(10L);    }    Assert.assertEquals(numEvents - 1, runner.getCount());    for (Exception ex : runner.getErrors()) {            }    Assert.assertEquals(Collections.EMPTY_LIST, runner.getErrors());        channel.stop();    channel = createFileChannel();    channel.start();    Assert.assertTrue(channel.isOpen());    Set<String> out = takeEvents(channel, 1, 1);    Assert.assertEquals(1, out.size());    String s = out.iterator().next();    Assert.assertTrue(s, s.startsWith("rollback-90-9"));}
1
public void testBasics()
{    Map<String, String> headers = Maps.newHashMap();    headers.put("key", "value");    byte[] body = "flume".getBytes(Charsets.UTF_8);    FlumeEvent event = new FlumeEvent(headers, body);    Assert.assertEquals(headers, event.getHeaders());    Assert.assertTrue(Arrays.equals(body, event.getBody()));}
0
public void testSerialization() throws IOException
{    Map<String, String> headers = Maps.newHashMap();    headers.put("key", "value");    byte[] body = "flume".getBytes(Charsets.UTF_8);    FlumeEvent in = new FlumeEvent(headers, body);    FlumeEvent out = FlumeEvent.from(TestUtils.toDataInput(in));    Assert.assertEquals(headers, out.getHeaders());    Assert.assertTrue(Arrays.equals(body, out.getBody()));    in.setHeaders(null);    in.setBody(null);    out = FlumeEvent.from(TestUtils.toDataInput(in));    Assert.assertEquals(Maps.newHashMap(), out.getHeaders());    Assert.assertNull(out.getBody());}
0
public void testGetter()
{    FlumeEventPointer pointer = new FlumeEventPointer(1, 1);    Assert.assertEquals(1, pointer.getFileID());    Assert.assertEquals(1, pointer.getOffset());}
0
public void testEquals()
{    FlumeEventPointer pointerA = new FlumeEventPointer(1, 1);    FlumeEventPointer pointerB = new FlumeEventPointer(1, 1);    Assert.assertEquals(pointerA, pointerB);    Assert.assertEquals(pointerB, pointerA);    pointerA = new FlumeEventPointer(1, 1);    pointerB = new FlumeEventPointer(2, 2);    Assert.assertFalse(pointerA.equals(pointerB));    Assert.assertFalse(pointerB.equals(pointerA));}
0
public void testHashCode()
{    FlumeEventPointer pointerA = new FlumeEventPointer(1, 1);    FlumeEventPointer pointerB = new FlumeEventPointer(1, 1);    Assert.assertEquals(pointerA.hashCode(), pointerB.hashCode());    pointerA = new FlumeEventPointer(1, 1);    pointerB = new FlumeEventPointer(2, 2);    Assert.assertFalse(pointerA.hashCode() == pointerB.hashCode());}
0
public void testPack()
{    FlumeEventPointer pointerA = new FlumeEventPointer(1, 1);    FlumeEventPointer pointerB = new FlumeEventPointer(1, 2);    Assert.assertEquals(4294967297L, pointerA.toLong());    Assert.assertEquals(4294967298L, pointerB.toLong());    Assert.assertEquals(pointerA, FlumeEventPointer.fromLong(pointerA.toLong()));    Assert.assertEquals(pointerB, FlumeEventPointer.fromLong(pointerB.toLong()));}
0
 File getCheckpoint()
{    return checkpoint;}
0
 File getInflightPuts()
{    return inflightPuts;}
0
 File getInflightTakes()
{    return inflightTakes;}
0
 File getQueueSetDir()
{    return queueSetDir;}
0
 void delete()
{    FileUtils.deleteQuietly(baseDir);}
0
public static Collection<Object[]> data() throws Exception
{    Object[][] data = new Object[][] { { new EventQueueBackingStoreSupplier() {        @Override        public EventQueueBackingStore get() throws Exception {            Assert.assertTrue(baseDir.isDirectory() || baseDir.mkdirs());            return new EventQueueBackingStoreFileV2(getCheckpoint(), 1000, "test", new FileChannelCounter("test"));        }    } }, { new EventQueueBackingStoreSupplier() {        @Override        public EventQueueBackingStore get() throws Exception {            Assert.assertTrue(baseDir.isDirectory() || baseDir.mkdirs());            return new EventQueueBackingStoreFileV3(getCheckpoint(), 1000, "test", new FileChannelCounter("test"));        }    } } };    return Arrays.asList(data);}
0
public EventQueueBackingStore get() throws Exception
{    Assert.assertTrue(baseDir.isDirectory() || baseDir.mkdirs());    return new EventQueueBackingStoreFileV2(getCheckpoint(), 1000, "test", new FileChannelCounter("test"));}
0
public EventQueueBackingStore get() throws Exception
{    Assert.assertTrue(baseDir.isDirectory() || baseDir.mkdirs());    return new EventQueueBackingStoreFileV3(getCheckpoint(), 1000, "test", new FileChannelCounter("test"));}
0
public void setup() throws Exception
{    backingStore = backingStoreSupplier.get();}
0
public void cleanup() throws IOException
{    if (backingStore != null) {        backingStore.close();    }    backingStoreSupplier.delete();}
0
public void testCapacity() throws Exception
{    backingStore.close();    File checkpoint = backingStoreSupplier.getCheckpoint();    Assert.assertTrue(checkpoint.delete());    backingStore = new EventQueueBackingStoreFileV2(checkpoint, 1, "test", new FileChannelCounter("test"));    queue = new FlumeEventQueue(backingStore, backingStoreSupplier.getInflightTakes(), backingStoreSupplier.getInflightPuts(), backingStoreSupplier.getQueueSetDir());    Assert.assertTrue(queue.addTail(pointer1));    Assert.assertFalse(queue.addTail(pointer2));}
0
public void testInvalidCapacityZero() throws Exception
{    backingStore.close();    File checkpoint = backingStoreSupplier.getCheckpoint();    Assert.assertTrue(checkpoint.delete());    backingStore = new EventQueueBackingStoreFileV2(checkpoint, 0, "test", new FileChannelCounter("test"));    queue = new FlumeEventQueue(backingStore, backingStoreSupplier.getInflightTakes(), backingStoreSupplier.getInflightPuts(), backingStoreSupplier.getQueueSetDir());}
0
public void testInvalidCapacityNegative() throws Exception
{    backingStore.close();    File checkpoint = backingStoreSupplier.getCheckpoint();    Assert.assertTrue(checkpoint.delete());    backingStore = new EventQueueBackingStoreFileV2(checkpoint, -1, "test", new FileChannelCounter("test"));    queue = new FlumeEventQueue(backingStore, backingStoreSupplier.getInflightTakes(), backingStoreSupplier.getInflightPuts(), backingStoreSupplier.getQueueSetDir());}
0
public void testQueueIsEmptyAfterCreation() throws Exception
{    queue = new FlumeEventQueue(backingStore, backingStoreSupplier.getInflightTakes(), backingStoreSupplier.getInflightPuts(), backingStoreSupplier.getQueueSetDir());    Assert.assertNull(queue.removeHead(0L));}
0
public void addTail1() throws Exception
{    queue = new FlumeEventQueue(backingStore, backingStoreSupplier.getInflightTakes(), backingStoreSupplier.getInflightPuts(), backingStoreSupplier.getQueueSetDir());    Assert.assertTrue(queue.addTail(pointer1));    Assert.assertEquals(pointer1, queue.removeHead(0));    Assert.assertEquals(Sets.newHashSet(), queue.getFileIDs());}
0
public void addTail2() throws Exception
{    queue = new FlumeEventQueue(backingStore, backingStoreSupplier.getInflightTakes(), backingStoreSupplier.getInflightPuts(), backingStoreSupplier.getQueueSetDir());    Assert.assertTrue(queue.addTail(pointer1));    Assert.assertTrue(queue.addTail(pointer2));    Assert.assertEquals(Sets.newHashSet(1, 2), queue.getFileIDs());    Assert.assertEquals(pointer1, queue.removeHead(0));    Assert.assertEquals(Sets.newHashSet(2), queue.getFileIDs());}
0
public void addTailLarge() throws Exception
{    queue = new FlumeEventQueue(backingStore, backingStoreSupplier.getInflightTakes(), backingStoreSupplier.getInflightPuts(), backingStoreSupplier.getQueueSetDir());    int size = 500;    Set<Integer> fileIDs = Sets.newHashSet();    for (int i = 1; i <= size; i++) {        Assert.assertTrue(queue.addTail(new FlumeEventPointer(i, i)));        fileIDs.add(i);        Assert.assertEquals(fileIDs, queue.getFileIDs());    }    for (int i = 1; i <= size; i++) {        Assert.assertEquals(new FlumeEventPointer(i, i), queue.removeHead(0));        fileIDs.remove(i);        Assert.assertEquals(fileIDs, queue.getFileIDs());    }    Assert.assertEquals(Sets.newHashSet(), queue.getFileIDs());}
0
public void addHead1() throws Exception
{    queue = new FlumeEventQueue(backingStore, backingStoreSupplier.getInflightTakes(), backingStoreSupplier.getInflightPuts(), backingStoreSupplier.getQueueSetDir());    Assert.assertTrue(queue.addHead(pointer1));    Assert.assertEquals(Sets.newHashSet(1), queue.getFileIDs());    Assert.assertEquals(pointer1, queue.removeHead(0));    Assert.assertEquals(Sets.newHashSet(), queue.getFileIDs());}
0
public void addHead2() throws Exception
{    queue = new FlumeEventQueue(backingStore, backingStoreSupplier.getInflightTakes(), backingStoreSupplier.getInflightPuts(), backingStoreSupplier.getQueueSetDir());    queue.replayComplete();    Assert.assertTrue(queue.addHead(pointer1));    Assert.assertTrue(queue.addHead(pointer2));    Assert.assertEquals(Sets.newHashSet(1, 2), queue.getFileIDs());    Assert.assertEquals(pointer2, queue.removeHead(0));    Assert.assertEquals(Sets.newHashSet(1), queue.getFileIDs());}
0
public void addHeadLarge() throws Exception
{    queue = new FlumeEventQueue(backingStore, backingStoreSupplier.getInflightTakes(), backingStoreSupplier.getInflightPuts(), backingStoreSupplier.getQueueSetDir());    queue.replayComplete();    int size = 500;    Set<Integer> fileIDs = Sets.newHashSet();    for (int i = 1; i <= size; i++) {        Assert.assertTrue(queue.addHead(new FlumeEventPointer(i, i)));        fileIDs.add(i);        Assert.assertEquals(fileIDs, queue.getFileIDs());    }    for (int i = size; i > 0; i--) {        Assert.assertEquals(new FlumeEventPointer(i, i), queue.removeHead(0));        fileIDs.remove(i);        Assert.assertEquals(fileIDs, queue.getFileIDs());    }    Assert.assertEquals(Sets.newHashSet(), queue.getFileIDs());}
0
public void addTailRemove1() throws Exception
{    queue = new FlumeEventQueue(backingStore, backingStoreSupplier.getInflightTakes(), backingStoreSupplier.getInflightPuts(), backingStoreSupplier.getQueueSetDir());    Assert.assertTrue(queue.addTail(pointer1));    Assert.assertEquals(Sets.newHashSet(1), queue.getFileIDs());    Assert.assertTrue(queue.remove(pointer1));    queue.replayComplete();    Assert.assertEquals(Sets.newHashSet(), queue.getFileIDs());    Assert.assertNull(queue.removeHead(0));    Assert.assertEquals(Sets.newHashSet(), queue.getFileIDs());}
0
public void addTailRemove2() throws Exception
{    queue = new FlumeEventQueue(backingStore, backingStoreSupplier.getInflightTakes(), backingStoreSupplier.getInflightPuts(), backingStoreSupplier.getQueueSetDir());    Assert.assertTrue(queue.addTail(pointer1));    Assert.assertTrue(queue.addTail(pointer2));    Assert.assertTrue(queue.remove(pointer1));    queue.replayComplete();    Assert.assertEquals(pointer2, queue.removeHead(0));}
0
public void addHeadRemove1() throws Exception
{    queue = new FlumeEventQueue(backingStore, backingStoreSupplier.getInflightTakes(), backingStoreSupplier.getInflightPuts(), backingStoreSupplier.getQueueSetDir());    queue.addHead(pointer1);    Assert.assertTrue(queue.remove(pointer1));    Assert.assertNull(queue.removeHead(0));}
0
public void addHeadRemove2() throws Exception
{    queue = new FlumeEventQueue(backingStore, backingStoreSupplier.getInflightTakes(), backingStoreSupplier.getInflightPuts(), backingStoreSupplier.getQueueSetDir());    Assert.assertTrue(queue.addHead(pointer1));    Assert.assertTrue(queue.addHead(pointer2));    Assert.assertTrue(queue.remove(pointer1));    queue.replayComplete();    Assert.assertEquals(pointer2, queue.removeHead(0));}
0
public void testUnknownPointerDoesNotCauseSearch() throws Exception
{    queue = new FlumeEventQueue(backingStore, backingStoreSupplier.getInflightTakes(), backingStoreSupplier.getInflightPuts(), backingStoreSupplier.getQueueSetDir());    Assert.assertTrue(queue.addHead(pointer1));    Assert.assertTrue(queue.addHead(pointer2));        Assert.assertFalse(queue.remove(pointer3));    Assert.assertTrue(queue.remove(pointer1));    Assert.assertTrue(queue.remove(pointer2));    queue.replayComplete();    Assert.assertEquals(2, queue.getSearchCount());}
0
public void testRemoveAfterReplayComplete() throws Exception
{    queue = new FlumeEventQueue(backingStore, backingStoreSupplier.getInflightTakes(), backingStoreSupplier.getInflightPuts(), backingStoreSupplier.getQueueSetDir());    queue.replayComplete();    queue.remove(pointer1);}
0
public void testWrappingCorrectly() throws Exception
{    queue = new FlumeEventQueue(backingStore, backingStoreSupplier.getInflightTakes(), backingStoreSupplier.getInflightPuts(), backingStoreSupplier.getQueueSetDir());    int size = Integer.MAX_VALUE;    for (int i = 1; i <= size; i++) {        if (!queue.addHead(new FlumeEventPointer(i, i))) {            break;        }    }    for (int i = queue.getSize() / 2; i > 0; i--) {        Assert.assertNotNull(queue.removeHead(0));    }        for (int i = 1; i <= size; i++) {        if (!queue.addHead(new FlumeEventPointer(i, i))) {            break;        }    }}
0
public void testInflightPuts() throws Exception
{    queue = new FlumeEventQueue(backingStore, backingStoreSupplier.getInflightTakes(), backingStoreSupplier.getInflightPuts(), backingStoreSupplier.getQueueSetDir());    long txnID1 = new Random().nextInt(Integer.MAX_VALUE - 1);    long txnID2 = txnID1 + 1;    queue.addWithoutCommit(new FlumeEventPointer(1, 1), txnID1);    queue.addWithoutCommit(new FlumeEventPointer(2, 1), txnID1);    queue.addWithoutCommit(new FlumeEventPointer(2, 2), txnID2);    queue.checkpoint(true);    TimeUnit.SECONDS.sleep(3L);    queue = new FlumeEventQueue(backingStore, backingStoreSupplier.getInflightTakes(), backingStoreSupplier.getInflightPuts(), backingStoreSupplier.getQueueSetDir());    SetMultimap<Long, Long> deserializedMap = queue.deserializeInflightPuts();    Assert.assertTrue(deserializedMap.get(txnID1).contains(new FlumeEventPointer(1, 1).toLong()));    Assert.assertTrue(deserializedMap.get(txnID1).contains(new FlumeEventPointer(2, 1).toLong()));    Assert.assertTrue(deserializedMap.get(txnID2).contains(new FlumeEventPointer(2, 2).toLong()));}
0
public void testInflightTakes() throws Exception
{    queue = new FlumeEventQueue(backingStore, backingStoreSupplier.getInflightTakes(), backingStoreSupplier.getInflightPuts(), backingStoreSupplier.getQueueSetDir());    long txnID1 = new Random().nextInt(Integer.MAX_VALUE - 1);    long txnID2 = txnID1 + 1;    queue.addTail(new FlumeEventPointer(1, 1));    queue.addTail(new FlumeEventPointer(2, 1));    queue.addTail(new FlumeEventPointer(2, 2));    queue.removeHead(txnID1);    queue.removeHead(txnID2);    queue.removeHead(txnID2);    queue.checkpoint(true);    TimeUnit.SECONDS.sleep(3L);    queue = new FlumeEventQueue(backingStore, backingStoreSupplier.getInflightTakes(), backingStoreSupplier.getInflightPuts(), backingStoreSupplier.getQueueSetDir());    SetMultimap<Long, Long> deserializedMap = queue.deserializeInflightTakes();    Assert.assertTrue(deserializedMap.get(txnID1).contains(new FlumeEventPointer(1, 1).toLong()));    Assert.assertTrue(deserializedMap.get(txnID2).contains(new FlumeEventPointer(2, 1).toLong()));    Assert.assertTrue(deserializedMap.get(txnID2).contains(new FlumeEventPointer(2, 2).toLong()));}
0
public void testCorruptInflightPuts() throws Exception
{    RandomAccessFile inflight = null;    try {        queue = new FlumeEventQueue(backingStore, backingStoreSupplier.getInflightTakes(), backingStoreSupplier.getInflightPuts(), backingStoreSupplier.getQueueSetDir());        long txnID1 = new Random().nextInt(Integer.MAX_VALUE - 1);        long txnID2 = txnID1 + 1;        queue.addWithoutCommit(new FlumeEventPointer(1, 1), txnID1);        queue.addWithoutCommit(new FlumeEventPointer(2, 1), txnID1);        queue.addWithoutCommit(new FlumeEventPointer(2, 2), txnID2);        queue.checkpoint(true);        TimeUnit.SECONDS.sleep(3L);        inflight = new RandomAccessFile(backingStoreSupplier.getInflightPuts(), "rw");        inflight.seek(0);        inflight.writeInt(new Random().nextInt());        queue = new FlumeEventQueue(backingStore, backingStoreSupplier.getInflightTakes(), backingStoreSupplier.getInflightPuts(), backingStoreSupplier.getQueueSetDir());        SetMultimap<Long, Long> deserializedMap = queue.deserializeInflightPuts();        Assert.assertTrue(deserializedMap.get(txnID1).contains(new FlumeEventPointer(1, 1).toLong()));        Assert.assertTrue(deserializedMap.get(txnID1).contains(new FlumeEventPointer(2, 1).toLong()));        Assert.assertTrue(deserializedMap.get(txnID2).contains(new FlumeEventPointer(2, 2).toLong()));    } finally {        inflight.close();    }}
0
public void testCorruptInflightTakes() throws Exception
{    RandomAccessFile inflight = null;    try {        queue = new FlumeEventQueue(backingStore, backingStoreSupplier.getInflightTakes(), backingStoreSupplier.getInflightPuts(), backingStoreSupplier.getQueueSetDir());        long txnID1 = new Random().nextInt(Integer.MAX_VALUE - 1);        long txnID2 = txnID1 + 1;        queue.addWithoutCommit(new FlumeEventPointer(1, 1), txnID1);        queue.addWithoutCommit(new FlumeEventPointer(2, 1), txnID1);        queue.addWithoutCommit(new FlumeEventPointer(2, 2), txnID2);        queue.checkpoint(true);        TimeUnit.SECONDS.sleep(3L);        inflight = new RandomAccessFile(backingStoreSupplier.getInflightTakes(), "rw");        inflight.seek(0);        inflight.writeInt(new Random().nextInt());        queue = new FlumeEventQueue(backingStore, backingStoreSupplier.getInflightTakes(), backingStoreSupplier.getInflightPuts(), backingStoreSupplier.getQueueSetDir());        SetMultimap<Long, Long> deserializedMap = queue.deserializeInflightTakes();        Assert.assertTrue(deserializedMap.get(txnID1).contains(new FlumeEventPointer(1, 1).toLong()));        Assert.assertTrue(deserializedMap.get(txnID1).contains(new FlumeEventPointer(2, 1).toLong()));        Assert.assertTrue(deserializedMap.get(txnID2).contains(new FlumeEventPointer(2, 2).toLong()));    } finally {        inflight.close();    }}
0
public void setup()
{    baseDir = Files.createTempDir();    checkpointDir = new File(baseDir, "chkpt");    Assert.assertTrue(checkpointDir.mkdirs() || checkpointDir.isDirectory());    dataDirs = new File[3];    dataDir = "";    for (int i = 0; i < dataDirs.length; i++) {        dataDirs[i] = new File(baseDir, "data" + (i + 1));        Assert.assertTrue(dataDirs[i].mkdirs() || dataDirs[i].isDirectory());        dataDir += dataDirs[i].getAbsolutePath() + ",";    }    dataDir = dataDir.substring(0, dataDir.length() - 1);}
0
public void teardown()
{    if (channel != null && channel.isOpen()) {        channel.stop();    }    FileUtils.deleteQuietly(baseDir);}
0
public void testIntegration() throws IOException, InterruptedException
{            Context context = new Context();    context.put(FileChannelConfiguration.CHECKPOINT_DIR, checkpointDir.getAbsolutePath());    context.put(FileChannelConfiguration.DATA_DIRS, dataDir);    context.put(FileChannelConfiguration.CAPACITY, String.valueOf(10000));        context.put(FileChannelConfiguration.CHECKPOINT_INTERVAL, "5000");    context.put(FileChannelConfiguration.MAX_FILE_SIZE, String.valueOf(1024 * 1024 * 5));        channel = new FileChannel();    channel.setName("FileChannel-" + UUID.randomUUID());    Configurables.configure(channel, context);    channel.start();    Assert.assertTrue(channel.isOpen());    SequenceGeneratorSource source = new SequenceGeneratorSource();    CountingSourceRunner sourceRunner = new CountingSourceRunner(source, channel);    source.configure(context);    source.start();    NullSink sink = new NullSink();    sink.setChannel(channel);    CountingSinkRunner sinkRunner = new CountingSinkRunner(sink);    sinkRunner.start();    sourceRunner.start();    TimeUnit.SECONDS.sleep(30);        sourceRunner.shutdown();    while (sourceRunner.isAlive()) {        Thread.sleep(10L);    }        while (channel.getDepth() > 0) {        Thread.sleep(10L);    }        sinkRunner.shutdown();        TimeUnit.SECONDS.sleep(5);    List<File> logs = Lists.newArrayList();    for (int i = 0; i < dataDirs.length; i++) {        logs.addAll(LogUtils.getLogs(dataDirs[i]));    }        for (File logFile : logs) {            }            for (Exception ex : sourceRunner.getErrors()) {            }    for (Exception ex : sinkRunner.getErrors()) {            }    Assert.assertEquals(sinkRunner.getCount(), sinkRunner.getCount());    Assert.assertEquals(Collections.EMPTY_LIST, sinkRunner.getErrors());    Assert.assertEquals(Collections.EMPTY_LIST, sourceRunner.getErrors());}
1
public void setup() throws IOException
{    transactionID = 0;    checkpointDir = Files.createTempDir();    FileUtils.forceDeleteOnExit(checkpointDir);    Assert.assertTrue(checkpointDir.isDirectory());    dataDirs = new File[3];    for (int i = 0; i < dataDirs.length; i++) {        dataDirs[i] = Files.createTempDir();        Assert.assertTrue(dataDirs[i].isDirectory());    }    log = new Log.Builder().setCheckpointInterval(1L).setMaxFileSize(MAX_FILE_SIZE).setQueueSize(CAPACITY).setCheckpointDir(checkpointDir).setLogDirs(dataDirs).setCheckpointOnClose(false).setChannelName("testlog").setChannelCounter(new FileChannelCounter("testlog")).build();    log.replay();}
0
public void cleanup() throws Exception
{    if (log != null) {        log.close();    }    FileUtils.deleteQuietly(checkpointDir);    for (int i = 0; i < dataDirs.length; i++) {        FileUtils.deleteQuietly(dataDirs[i]);    }}
0
public void testPutGet() throws IOException, InterruptedException, NoopRecordException, CorruptEventException
{    FlumeEvent eventIn = TestUtils.newPersistableEvent();    long transactionID = ++this.transactionID;    FlumeEventPointer eventPointer = log.put(transactionID, eventIn);        log.commitPut(transactionID);        FlumeEvent eventOut = log.get(eventPointer);    Assert.assertNotNull(eventOut);    Assert.assertEquals(eventIn.getHeaders(), eventOut.getHeaders());    Assert.assertArrayEquals(eventIn.getBody(), eventOut.getBody());}
0
public void testRoll() throws IOException, InterruptedException, NoopRecordException, CorruptEventException
{    log.shutdownWorker();    Thread.sleep(1000);    for (int i = 0; i < 1000; i++) {        FlumeEvent eventIn = TestUtils.newPersistableEvent();        long transactionID = ++this.transactionID;        FlumeEventPointer eventPointer = log.put(transactionID, eventIn);                FlumeEvent eventOut = log.get(eventPointer);        Assert.assertNotNull(eventOut);        Assert.assertEquals(eventIn.getHeaders(), eventOut.getHeaders());        Assert.assertArrayEquals(eventIn.getBody(), eventOut.getBody());    }    int logCount = 0;    for (File dataDir : dataDirs) {        for (File logFile : dataDir.listFiles()) {            if (logFile.getName().startsWith("log-")) {                logCount++;            }        }    }        Assert.assertEquals(186, logCount);}
0
public void testPutCommit() throws IOException, InterruptedException, NoopRecordException, CorruptEventException
{    FlumeEvent eventIn = TestUtils.newPersistableEvent();    long transactionID = ++this.transactionID;    FlumeEventPointer eventPointerIn = log.put(transactionID, eventIn);    log.commitPut(transactionID);    log.close();    log = new Log.Builder().setCheckpointInterval(Long.MAX_VALUE).setMaxFileSize(FileChannelConfiguration.DEFAULT_MAX_FILE_SIZE).setQueueSize(CAPACITY).setCheckpointDir(checkpointDir).setLogDirs(dataDirs).setChannelName("testlog").setChannelCounter(new FileChannelCounter("testlog")).build();    log.replay();    takeAndVerify(eventPointerIn, eventIn);}
0
public void testPutRollback() throws IOException, InterruptedException
{    FlumeEvent eventIn = TestUtils.newPersistableEvent();    long transactionID = ++this.transactionID;    log.put(transactionID, eventIn);        log.rollback(transactionID);    log.close();    log = new Log.Builder().setCheckpointInterval(Long.MAX_VALUE).setMaxFileSize(FileChannelConfiguration.DEFAULT_MAX_FILE_SIZE).setQueueSize(CAPACITY).setCheckpointDir(checkpointDir).setLogDirs(dataDirs).setChannelName("testlog").setChannelCounter(new FileChannelCounter("testlog")).build();    log.replay();    FlumeEventQueue queue = log.getFlumeEventQueue();    Assert.assertNull(queue.removeHead(transactionID));}
0
public void testMinimumRequiredSpaceTooSmallOnStartup() throws IOException, InterruptedException
{    log.close();    log = new Log.Builder().setCheckpointInterval(Long.MAX_VALUE).setMaxFileSize(FileChannelConfiguration.DEFAULT_MAX_FILE_SIZE).setQueueSize(CAPACITY).setCheckpointDir(checkpointDir).setLogDirs(dataDirs).setChannelName("testlog").setMinimumRequiredSpace(Long.MAX_VALUE).setChannelCounter(new FileChannelCounter("testlog")).build();    try {        log.replay();        Assert.fail();    } catch (IOException e) {        Assert.assertTrue(e.getMessage(), e.getMessage().startsWith("Usable space exhausted"));    }}
0
public void testMinimumRequiredSpaceTooSmallForPut() throws IOException, InterruptedException
{    try {        doTestMinimumRequiredSpaceTooSmallForPut();    } catch (IOException e) {                doTestMinimumRequiredSpaceTooSmallForPut();    } catch (AssertionError e) {                doTestMinimumRequiredSpaceTooSmallForPut();    }}
1
public void doTestMinimumRequiredSpaceTooSmallForPut() throws IOException, InterruptedException
{    long minimumRequiredSpace = checkpointDir.getUsableSpace() - (10L * 1024L * 1024L);    log.close();    log = new Log.Builder().setCheckpointInterval(Long.MAX_VALUE).setMaxFileSize(FileChannelConfiguration.DEFAULT_MAX_FILE_SIZE).setQueueSize(CAPACITY).setCheckpointDir(checkpointDir).setLogDirs(dataDirs).setChannelName("testlog").setMinimumRequiredSpace(minimumRequiredSpace).setUsableSpaceRefreshInterval(1L).setChannelCounter(new FileChannelCounter("testlog")).build();    log.replay();    File filler = new File(checkpointDir, "filler");    byte[] buffer = new byte[64 * 1024];    FileOutputStream out = new FileOutputStream(filler);    while (checkpointDir.getUsableSpace() > minimumRequiredSpace) {        out.write(buffer);    }    out.close();    try {        FlumeEvent eventIn = TestUtils.newPersistableEvent();        long transactionID = ++this.transactionID;        log.put(transactionID, eventIn);        Assert.fail();    } catch (IOException e) {        Assert.assertTrue(e.getMessage(), e.getMessage().startsWith("Usable space exhausted"));    }}
0
public void testPutTakeCommit() throws IOException, InterruptedException
{    FlumeEvent eventIn = TestUtils.newPersistableEvent();    long putTransactionID = ++transactionID;    FlumeEventPointer eventPointer = log.put(putTransactionID, eventIn);    log.commitPut(putTransactionID);    long takeTransactionID = ++transactionID;    log.take(takeTransactionID, eventPointer);    log.commitTake(takeTransactionID);    log.close();    new Log.Builder().setCheckpointInterval(Long.MAX_VALUE).setMaxFileSize(FileChannelConfiguration.DEFAULT_MAX_FILE_SIZE).setQueueSize(1).setCheckpointDir(checkpointDir).setLogDirs(dataDirs).setChannelName("testlog").setChannelCounter(new FileChannelCounter("testlog")).build();    log.replay();    FlumeEventQueue queue = log.getFlumeEventQueue();    Assert.assertNull(queue.removeHead(0));}
0
public void testPutTakeRollbackLogReplayV1() throws IOException, InterruptedException, NoopRecordException, CorruptEventException
{    doPutTakeRollback(true);}
0
public void testPutTakeRollbackLogReplayV2() throws IOException, InterruptedException, NoopRecordException, CorruptEventException
{    doPutTakeRollback(false);}
0
public void doPutTakeRollback(boolean useLogReplayV1) throws IOException, InterruptedException, NoopRecordException, CorruptEventException
{    FlumeEvent eventIn = TestUtils.newPersistableEvent();    long putTransactionID = ++transactionID;    FlumeEventPointer eventPointerIn = log.put(putTransactionID, eventIn);    log.commitPut(putTransactionID);    long takeTransactionID = ++transactionID;    log.take(takeTransactionID, eventPointerIn);    log.rollback(takeTransactionID);    log.close();    new Log.Builder().setCheckpointInterval(Long.MAX_VALUE).setMaxFileSize(FileChannelConfiguration.DEFAULT_MAX_FILE_SIZE).setQueueSize(1).setCheckpointDir(checkpointDir).setLogDirs(dataDirs).setChannelName("testlog").setUseLogReplayV1(useLogReplayV1).setChannelCounter(new FileChannelCounter("testlog")).build();    log.replay();    takeAndVerify(eventPointerIn, eventIn);}
0
public void testCommitNoPut() throws IOException, InterruptedException
{    long putTransactionID = ++transactionID;    log.commitPut(putTransactionID);    log.close();    new Log.Builder().setCheckpointInterval(Long.MAX_VALUE).setMaxFileSize(FileChannelConfiguration.DEFAULT_MAX_FILE_SIZE).setQueueSize(1).setCheckpointDir(checkpointDir).setLogDirs(dataDirs).setChannelName("testlog").setChannelCounter(new FileChannelCounter("testlog")).build();    log.replay();    FlumeEventQueue queue = log.getFlumeEventQueue();    FlumeEventPointer eventPointerOut = queue.removeHead(0);    Assert.assertNull(eventPointerOut);}
0
public void testCommitNoTake() throws IOException, InterruptedException
{    long putTransactionID = ++transactionID;    log.commitTake(putTransactionID);    log.close();    new Log.Builder().setCheckpointInterval(Long.MAX_VALUE).setMaxFileSize(FileChannelConfiguration.DEFAULT_MAX_FILE_SIZE).setQueueSize(1).setCheckpointDir(checkpointDir).setLogDirs(dataDirs).setChannelName("testlog").setChannelCounter(new FileChannelCounter("testlog")).build();    log.replay();    FlumeEventQueue queue = log.getFlumeEventQueue();    FlumeEventPointer eventPointerOut = queue.removeHead(0);    Assert.assertNull(eventPointerOut);}
0
public void testRollbackNoPutTake() throws IOException, InterruptedException
{    long putTransactionID = ++transactionID;    log.rollback(putTransactionID);    log.close();    new Log.Builder().setCheckpointInterval(Long.MAX_VALUE).setMaxFileSize(FileChannelConfiguration.DEFAULT_MAX_FILE_SIZE).setQueueSize(1).setCheckpointDir(checkpointDir).setLogDirs(dataDirs).setChannelName("testlog").setChannelCounter(new FileChannelCounter("testlog")).build();    log.replay();    FlumeEventQueue queue = log.getFlumeEventQueue();    FlumeEventPointer eventPointerOut = queue.removeHead(0);    Assert.assertNull(eventPointerOut);}
0
public void testGetLogs() throws IOException
{    File logDir = dataDirs[0];    List<File> expected = Lists.newArrayList();    for (int i = 0; i < 10; i++) {        File log = new File(logDir, Log.PREFIX + i);        expected.add(log);        Assert.assertTrue(log.isFile() || log.createNewFile());        File metaDataFile = Serialization.getMetaDataFile(log);        File metaDataTempFile = Serialization.getMetaDataTempFile(metaDataFile);        File logGzip = new File(logDir, Log.PREFIX + i + ".gz");        Assert.assertTrue(metaDataFile.isFile() || metaDataFile.createNewFile());        Assert.assertTrue(metaDataTempFile.isFile() || metaDataTempFile.createNewFile());        Assert.assertTrue(log.isFile() || logGzip.createNewFile());    }    List<File> actual = LogUtils.getLogs(logDir);    LogUtils.sort(actual);    LogUtils.sort(expected);    Assert.assertEquals(expected, actual);}
0
public void testReplayFailsWithAllEmptyLogMetaDataNormalReplay() throws IOException, InterruptedException
{    doTestReplayFailsWithAllEmptyLogMetaData(false);}
0
public void testReplayFailsWithAllEmptyLogMetaDataFastReplay() throws IOException, InterruptedException
{    doTestReplayFailsWithAllEmptyLogMetaData(true);}
0
public void doTestReplayFailsWithAllEmptyLogMetaData(boolean useFastReplay) throws IOException, InterruptedException
{        log.close();    log = new Log.Builder().setCheckpointInterval(1L).setMaxFileSize(MAX_FILE_SIZE).setQueueSize(CAPACITY).setCheckpointDir(checkpointDir).setLogDirs(dataDirs).setChannelName("testlog").setUseFastReplay(useFastReplay).setChannelCounter(new FileChannelCounter("testlog")).build();    log.replay();    FlumeEvent eventIn = TestUtils.newPersistableEvent();    long transactionID = ++this.transactionID;    log.put(transactionID, eventIn);    log.commitPut(transactionID);    log.close();    if (useFastReplay) {        FileUtils.deleteQuietly(checkpointDir);        Assert.assertTrue(checkpointDir.mkdir());    }    List<File> logFiles = Lists.newArrayList();    for (int i = 0; i < dataDirs.length; i++) {        logFiles.addAll(LogUtils.getLogs(dataDirs[i]));    }    Assert.assertTrue(logFiles.size() > 0);    for (File logFile : logFiles) {        File logFileMeta = Serialization.getMetaDataFile(logFile);        Assert.assertTrue(logFileMeta.delete());        Assert.assertTrue(logFileMeta.createNewFile());    }    log = new Log.Builder().setCheckpointInterval(1L).setMaxFileSize(MAX_FILE_SIZE).setQueueSize(CAPACITY).setCheckpointDir(checkpointDir).setLogDirs(dataDirs).setChannelName("testlog").setUseFastReplay(useFastReplay).setChannelCounter(new FileChannelCounter("testlog")).build();    try {        log.replay();        Assert.fail();    } catch (IllegalStateException expected) {        String msg = expected.getMessage();        Assert.assertNotNull(msg);        Assert.assertTrue(msg, msg.contains(".meta is empty, but log"));    }}
0
public void testReplaySucceedsWithUnusedEmptyLogMetaDataNormalReplay() throws IOException, InterruptedException, NoopRecordException, CorruptEventException
{    FlumeEvent eventIn = TestUtils.newPersistableEvent();    long transactionID = ++this.transactionID;    FlumeEventPointer eventPointer = log.put(transactionID, eventIn);        log.commitPut(transactionID);    log.close();    log = new Log.Builder().setCheckpointInterval(1L).setMaxFileSize(MAX_FILE_SIZE).setQueueSize(CAPACITY).setCheckpointDir(checkpointDir).setLogDirs(dataDirs).setChannelName("testlog").setChannelCounter(new FileChannelCounter("testlog")).build();    doTestReplaySucceedsWithUnusedEmptyLogMetaData(eventIn, eventPointer);}
0
public void testReplaySucceedsWithUnusedEmptyLogMetaDataFastReplay() throws IOException, InterruptedException, NoopRecordException, CorruptEventException
{    FlumeEvent eventIn = TestUtils.newPersistableEvent();    long transactionID = ++this.transactionID;    FlumeEventPointer eventPointer = log.put(transactionID, eventIn);        log.commitPut(transactionID);    log.close();    checkpointDir = Files.createTempDir();    FileUtils.forceDeleteOnExit(checkpointDir);    Assert.assertTrue(checkpointDir.isDirectory());    log = new Log.Builder().setCheckpointInterval(1L).setMaxFileSize(MAX_FILE_SIZE).setQueueSize(CAPACITY).setCheckpointDir(checkpointDir).setLogDirs(dataDirs).setChannelName("testlog").setUseFastReplay(true).setChannelCounter(new FileChannelCounter("testlog")).build();    doTestReplaySucceedsWithUnusedEmptyLogMetaData(eventIn, eventPointer);}
0
public void doTestReplaySucceedsWithUnusedEmptyLogMetaData(FlumeEvent eventIn, FlumeEventPointer eventPointer) throws IOException, InterruptedException, NoopRecordException, CorruptEventException
{    for (int i = 0; i < dataDirs.length; i++) {        for (File logFile : LogUtils.getLogs(dataDirs[i])) {            if (logFile.length() == 0L) {                File logFileMeta = Serialization.getMetaDataFile(logFile);                Assert.assertTrue(logFileMeta.delete());                Assert.assertTrue(logFileMeta.createNewFile());            }        }    }    log.replay();    FlumeEvent eventOut = log.get(eventPointer);    Assert.assertNotNull(eventOut);    Assert.assertEquals(eventIn.getHeaders(), eventOut.getHeaders());    Assert.assertArrayEquals(eventIn.getBody(), eventOut.getBody());}
0
public void testCachedFSUsableSpace() throws Exception
{    File fs = mock(File.class);    when(fs.getUsableSpace()).thenReturn(Long.MAX_VALUE);    LogFile.CachedFSUsableSpace cachedFS = new LogFile.CachedFSUsableSpace(fs, 1000L);    Assert.assertEquals(cachedFS.getUsableSpace(), Long.MAX_VALUE);    cachedFS.decrement(Integer.MAX_VALUE);    Assert.assertEquals(cachedFS.getUsableSpace(), Long.MAX_VALUE - Integer.MAX_VALUE);    try {        cachedFS.decrement(-1);        Assert.fail();    } catch (IllegalArgumentException expected) {    }    when(fs.getUsableSpace()).thenReturn(Long.MAX_VALUE - 1L);    Thread.sleep(1100);    Assert.assertEquals(cachedFS.getUsableSpace(), Long.MAX_VALUE - 1L);}
0
public void testCheckpointOnClose() throws Exception
{    log.close();    log = new Log.Builder().setCheckpointInterval(1L).setMaxFileSize(MAX_FILE_SIZE).setQueueSize(CAPACITY).setCheckpointDir(checkpointDir).setLogDirs(dataDirs).setCheckpointOnClose(true).setChannelName("testLog").setChannelCounter(new FileChannelCounter("testlog")).build();    log.replay();        FlumeEvent eventIn = TestUtils.newPersistableEvent();    log.put(transactionID, eventIn);    log.commitPut(transactionID);        File checkPointMetaFile = FileUtils.listFiles(checkpointDir, new String[] { "meta" }, false).iterator().next();    long before = FileUtils.checksumCRC32(checkPointMetaFile);        log.close();        long after = FileUtils.checksumCRC32(checkPointMetaFile);    Assert.assertFalse(before == after);}
0
private void takeAndVerify(FlumeEventPointer eventPointerIn, FlumeEvent eventIn) throws IOException, InterruptedException, NoopRecordException, CorruptEventException
{    FlumeEventQueue queue = log.getFlumeEventQueue();    FlumeEventPointer eventPointerOut = queue.removeHead(0);    Assert.assertNotNull(eventPointerOut);    Assert.assertNull(queue.removeHead(0));    Assert.assertEquals(eventPointerIn, eventPointerOut);    Assert.assertEquals(eventPointerIn.hashCode(), eventPointerOut.hashCode());    FlumeEvent eventOut = log.get(eventPointerOut);    Assert.assertNotNull(eventOut);    Assert.assertEquals(eventIn.getHeaders(), eventOut.getHeaders());    Assert.assertArrayEquals(eventIn.getBody(), eventOut.getBody());}
0
public void setup() throws IOException
{    fileID = 1;    transactionID = 1L;    dataDir = Files.createTempDir();    dataFile = new File(dataDir, String.valueOf(fileID));    Assert.assertTrue(dataDir.isDirectory());    logFileWriter = LogFileFactory.getWriter(dataFile, fileID, Integer.MAX_VALUE, null, null, null, Long.MAX_VALUE, true, 0);}
0
public void cleanup() throws IOException
{    try {        if (logFileWriter != null) {            logFileWriter.close();        }    } finally {        FileUtils.deleteQuietly(dataDir);    }}
0
public void testWriterRefusesToOverwriteFile() throws IOException
{    Assert.assertTrue(dataFile.isFile() || dataFile.createNewFile());    try {        LogFileFactory.getWriter(dataFile, fileID, Integer.MAX_VALUE, null, null, null, Long.MAX_VALUE, true, 0);        Assert.fail();    } catch (IllegalStateException e) {        Assert.assertEquals("File already exists " + dataFile.getAbsolutePath(), e.getMessage());    }}
0
public void testWriterFailsWithDirectory() throws IOException
{    FileUtils.deleteQuietly(dataFile);    Assert.assertFalse(dataFile.exists());    Assert.assertTrue(dataFile.mkdirs());    try {        LogFileFactory.getWriter(dataFile, fileID, Integer.MAX_VALUE, null, null, null, Long.MAX_VALUE, true, 0);        Assert.fail();    } catch (IllegalStateException e) {        Assert.assertEquals("File already exists " + dataFile.getAbsolutePath(), e.getMessage());    }}
0
public void testPutGet() throws InterruptedException, IOException
{    final List<Throwable> errors = Collections.synchronizedList(new ArrayList<Throwable>());    ExecutorService executorService = Executors.newFixedThreadPool(10);    CompletionService<Void> completionService = new ExecutorCompletionService<Void>(executorService);    final LogFile.RandomReader logFileReader = LogFileFactory.getRandomReader(dataFile, null, true);    for (int i = 0; i < 1000; i++) {                synchronized (errors) {            for (Throwable throwable : errors) {                Throwables.propagateIfInstanceOf(throwable, AssertionError.class);            }                        for (Throwable throwable : errors) {                Throwables.propagate(throwable);            }        }        final FlumeEvent eventIn = TestUtils.newPersistableEvent();        final Put put = new Put(++transactionID, WriteOrderOracle.next(), eventIn);        ByteBuffer bytes = TransactionEventRecord.toByteBuffer(put);        FlumeEventPointer ptr = logFileWriter.put(bytes);        final int offset = ptr.getOffset();        completionService.submit(new Runnable() {            @Override            public void run() {                try {                    FlumeEvent eventOut = logFileReader.get(offset);                    Assert.assertEquals(eventIn.getHeaders(), eventOut.getHeaders());                    Assert.assertTrue(Arrays.equals(eventIn.getBody(), eventOut.getBody()));                } catch (Throwable throwable) {                    synchronized (errors) {                        errors.add(throwable);                    }                }            }        }, null);    }    for (int i = 0; i < 1000; i++) {        completionService.take();    }        for (Throwable throwable : errors) {        Throwables.propagateIfInstanceOf(throwable, AssertionError.class);    }        for (Throwable throwable : errors) {        Throwables.propagate(throwable);    }}
0
public void run()
{    try {        FlumeEvent eventOut = logFileReader.get(offset);        Assert.assertEquals(eventIn.getHeaders(), eventOut.getHeaders());        Assert.assertTrue(Arrays.equals(eventIn.getBody(), eventOut.getBody()));    } catch (Throwable throwable) {        synchronized (errors) {            errors.add(throwable);        }    }}
0
public void testReader() throws InterruptedException, IOException, CorruptEventException
{    Map<Integer, Put> puts = Maps.newHashMap();    for (int i = 0; i < 1000; i++) {        FlumeEvent eventIn = TestUtils.newPersistableEvent();        Put put = new Put(++transactionID, WriteOrderOracle.next(), eventIn);        ByteBuffer bytes = TransactionEventRecord.toByteBuffer(put);        FlumeEventPointer ptr = logFileWriter.put(bytes);        puts.put(ptr.getOffset(), put);    }    LogFile.SequentialReader reader = LogFileFactory.getSequentialReader(dataFile, null, true);    LogRecord entry;    while ((entry = reader.next()) != null) {        Integer offset = entry.getOffset();        TransactionEventRecord record = entry.getEvent();        Put put = puts.get(offset);        FlumeEvent eventIn = put.getEvent();        Assert.assertEquals(put.getTransactionID(), record.getTransactionID());        Assert.assertTrue(record instanceof Put);        FlumeEvent eventOut = ((Put) record).getEvent();        Assert.assertEquals(eventIn.getHeaders(), eventOut.getHeaders());        Assert.assertTrue(Arrays.equals(eventIn.getBody(), eventOut.getBody()));    }}
0
public void testReaderOldMetaFile() throws InterruptedException, IOException, CorruptEventException
{    Map<Integer, Put> puts = Maps.newHashMap();    for (int i = 0; i < 1000; i++) {        FlumeEvent eventIn = TestUtils.newPersistableEvent();        Put put = new Put(++transactionID, WriteOrderOracle.next(), eventIn);        ByteBuffer bytes = TransactionEventRecord.toByteBuffer(put);        FlumeEventPointer ptr = logFileWriter.put(bytes);        puts.put(ptr.getOffset(), put);    }        File metadataFile = Serialization.getMetaDataFile(dataFile);    File oldMetadataFile = Serialization.getOldMetaDataFile(dataFile);    if (!metadataFile.renameTo(oldMetadataFile)) {        Assert.fail("Renaming to meta.old failed");    }    LogFile.SequentialReader reader = LogFileFactory.getSequentialReader(dataFile, null, true);    Assert.assertTrue(metadataFile.exists());    Assert.assertFalse(oldMetadataFile.exists());    LogRecord entry;    while ((entry = reader.next()) != null) {        Integer offset = entry.getOffset();        TransactionEventRecord record = entry.getEvent();        Put put = puts.get(offset);        FlumeEvent eventIn = put.getEvent();        Assert.assertEquals(put.getTransactionID(), record.getTransactionID());        Assert.assertTrue(record instanceof Put);        FlumeEvent eventOut = ((Put) record).getEvent();        Assert.assertEquals(eventIn.getHeaders(), eventOut.getHeaders());        Assert.assertTrue(Arrays.equals(eventIn.getBody(), eventOut.getBody()));    }}
0
public void testReaderTempMetaFile() throws InterruptedException, IOException, CorruptEventException
{    Map<Integer, Put> puts = Maps.newHashMap();    for (int i = 0; i < 1000; i++) {        FlumeEvent eventIn = TestUtils.newPersistableEvent();        Put put = new Put(++transactionID, WriteOrderOracle.next(), eventIn);        ByteBuffer bytes = TransactionEventRecord.toByteBuffer(put);        FlumeEventPointer ptr = logFileWriter.put(bytes);        puts.put(ptr.getOffset(), put);    }        File metadataFile = Serialization.getMetaDataFile(dataFile);    File tempMetadataFile = Serialization.getMetaDataTempFile(dataFile);    File oldMetadataFile = Serialization.getOldMetaDataFile(dataFile);        oldMetadataFile.createNewFile();    if (!metadataFile.renameTo(tempMetadataFile)) {        Assert.fail("Renaming to meta.temp failed");    }    LogFile.SequentialReader reader = LogFileFactory.getSequentialReader(dataFile, null, true);    Assert.assertTrue(metadataFile.exists());    Assert.assertFalse(tempMetadataFile.exists());    Assert.assertFalse(oldMetadataFile.exists());    LogRecord entry;    while ((entry = reader.next()) != null) {        Integer offset = entry.getOffset();        TransactionEventRecord record = entry.getEvent();        Put put = puts.get(offset);        FlumeEvent eventIn = put.getEvent();        Assert.assertEquals(put.getTransactionID(), record.getTransactionID());        Assert.assertTrue(record instanceof Put);        FlumeEvent eventOut = ((Put) record).getEvent();        Assert.assertEquals(eventIn.getHeaders(), eventOut.getHeaders());        Assert.assertTrue(Arrays.equals(eventIn.getBody(), eventOut.getBody()));    }}
0
public void testWriteDelimitedTo() throws IOException
{    if (dataFile.isFile()) {        Assert.assertTrue(dataFile.delete());    }    Assert.assertTrue(dataFile.createNewFile());    ProtosFactory.LogFileMetaData.Builder metaDataBuilder = ProtosFactory.LogFileMetaData.newBuilder();    metaDataBuilder.setVersion(1);    metaDataBuilder.setLogFileID(2);    metaDataBuilder.setCheckpointPosition(3);    metaDataBuilder.setCheckpointWriteOrderID(4);    LogFileV3.writeDelimitedTo(metaDataBuilder.build(), dataFile);    ProtosFactory.LogFileMetaData metaData = ProtosFactory.LogFileMetaData.parseDelimitedFrom(new FileInputStream(dataFile));    Assert.assertEquals(1, metaData.getVersion());    Assert.assertEquals(2, metaData.getLogFileID());    Assert.assertEquals(3, metaData.getCheckpointPosition());    Assert.assertEquals(4, metaData.getCheckpointWriteOrderID());}
0
public void testPutGetCorruptEvent() throws Exception
{    final LogFile.RandomReader logFileReader = LogFileFactory.getRandomReader(dataFile, null, true);    final FlumeEvent eventIn = TestUtils.newPersistableEvent(2500);    final Put put = new Put(++transactionID, WriteOrderOracle.next(), eventIn);    ByteBuffer bytes = TransactionEventRecord.toByteBuffer(put);    FlumeEventPointer ptr = logFileWriter.put(bytes);    logFileWriter.commit(TransactionEventRecord.toByteBuffer(new Commit(transactionID, WriteOrderOracle.next())));    logFileWriter.sync();    final int offset = ptr.getOffset();    RandomAccessFile writer = new RandomAccessFile(dataFile, "rw");    writer.seek(offset + 1500);    writer.write((byte) 45);    writer.write((byte) 12);    writer.getFD().sync();    logFileReader.get(offset);        Assert.fail();}
0
public void testPutGetNoopEvent() throws Exception
{    final LogFile.RandomReader logFileReader = LogFileFactory.getRandomReader(dataFile, null, true);    final FlumeEvent eventIn = TestUtils.newPersistableEvent(2500);    final Put put = new Put(++transactionID, WriteOrderOracle.next(), eventIn);    ByteBuffer bytes = TransactionEventRecord.toByteBuffer(put);    FlumeEventPointer ptr = logFileWriter.put(bytes);    logFileWriter.commit(TransactionEventRecord.toByteBuffer(new Commit(transactionID, WriteOrderOracle.next())));    logFileWriter.sync();    final int offset = ptr.getOffset();    LogFile.OperationRecordUpdater updater = new LogFile.OperationRecordUpdater(dataFile);    updater.markRecordAsNoop(offset);    logFileReader.get(offset);        Assert.fail();}
0
public void testOperationRecordUpdater() throws Exception
{    File tempDir = Files.createTempDir();    File temp = new File(tempDir, "temp");    final RandomAccessFile tempFile = new RandomAccessFile(temp, "rw");    for (int i = 0; i < 5000; i++) {        tempFile.write(LogFile.OP_RECORD);    }    tempFile.seek(0);    LogFile.OperationRecordUpdater recordUpdater = new LogFile.OperationRecordUpdater(temp);        for (int i = 0; i < 5000; i += 10) {        recordUpdater.markRecordAsNoop(i);    }    recordUpdater.close();    tempFile.seek(0);        for (int i = 0; i < 5000; i += 10) {        tempFile.seek(i);        Assert.assertEquals(LogFile.OP_NOOP, tempFile.readByte());    }}
0
public void testOpRecordUpdaterWithFlumeEvents() throws Exception
{    final FlumeEvent eventIn = TestUtils.newPersistableEvent(2500);    final Put put = new Put(++transactionID, WriteOrderOracle.next(), eventIn);    ByteBuffer bytes = TransactionEventRecord.toByteBuffer(put);    FlumeEventPointer ptr = logFileWriter.put(bytes);    logFileWriter.commit(TransactionEventRecord.toByteBuffer(new Commit(transactionID, WriteOrderOracle.next())));    logFileWriter.sync();    final int offset = ptr.getOffset();    LogFile.OperationRecordUpdater updater = new LogFile.OperationRecordUpdater(dataFile);    updater.markRecordAsNoop(offset);    RandomAccessFile fileReader = new RandomAccessFile(dataFile, "rw");    Assert.assertEquals(LogFile.OP_NOOP, fileReader.readByte());}
0
public void testGroupCommit() throws Exception
{    final FlumeEvent eventIn = TestUtils.newPersistableEvent(250);    final CyclicBarrier barrier = new CyclicBarrier(20);    ExecutorService executorService = Executors.newFixedThreadPool(20);    ExecutorCompletionService<Void> completionService = new ExecutorCompletionService<Void>(executorService);    final LogFile.Writer writer = logFileWriter;    final AtomicLong txnId = new AtomicLong(++transactionID);    for (int i = 0; i < 20; i++) {        completionService.submit(new Callable<Void>() {            @Override            public Void call() {                try {                    Put put = new Put(txnId.incrementAndGet(), WriteOrderOracle.next(), eventIn);                    ByteBuffer bytes = TransactionEventRecord.toByteBuffer(put);                    writer.put(bytes);                    writer.commit(TransactionEventRecord.toByteBuffer(new Commit(txnId.get(), WriteOrderOracle.next())));                    barrier.await();                    writer.sync();                } catch (Exception ex) {                    Throwables.propagate(ex);                }                return null;            }        });    }    for (int i = 0; i < 20; i++) {        completionService.take().get();    }        Assert.assertTrue(logFileWriter.position() >= 5000);    Assert.assertEquals(1, writer.getSyncCount());    Assert.assertTrue(logFileWriter.getLastCommitPosition() == logFileWriter.getLastSyncPosition());    executorService.shutdown();}
0
public Void call()
{    try {        Put put = new Put(txnId.incrementAndGet(), WriteOrderOracle.next(), eventIn);        ByteBuffer bytes = TransactionEventRecord.toByteBuffer(put);        writer.put(bytes);        writer.commit(TransactionEventRecord.toByteBuffer(new Commit(txnId.get(), WriteOrderOracle.next())));        barrier.await();        writer.sync();    } catch (Exception ex) {        Throwables.propagate(ex);    }    return null;}
0
public void testConstructor()
{    long now = System.currentTimeMillis();    Commit commit = new Commit(now, now + 1);    LogRecord logRecord = new LogRecord(1, 2, commit);    Assert.assertTrue(now == commit.getTransactionID());    Assert.assertTrue(now + 1 == commit.getLogWriteOrderID());    Assert.assertTrue(1 == logRecord.getFileID());    Assert.assertTrue(2 == logRecord.getOffset());    Assert.assertTrue(commit == logRecord.getEvent());}
0
public void testSortOrder()
{        long now = System.currentTimeMillis();    List<LogRecord> records = Lists.newArrayList();    for (int i = 0; i < 3; i++) {        Commit commit = new Commit((long) i, now - i);        LogRecord logRecord = new LogRecord(1, i, commit);        records.add(logRecord);    }    LogRecord logRecord;    logRecord = Collections.min(records);    Assert.assertTrue(String.valueOf(logRecord.getOffset()), 2 == logRecord.getOffset());    records.remove(logRecord);    logRecord = Collections.min(records);    Assert.assertTrue(String.valueOf(logRecord.getOffset()), 1 == logRecord.getOffset());    records.remove(logRecord);    logRecord = Collections.min(records);    Assert.assertTrue(String.valueOf(logRecord.getOffset()), 0 == logRecord.getOffset());    records.remove(logRecord);}
0
public void testTypes() throws IOException
{    Put put = new Put(System.currentTimeMillis(), WriteOrderOracle.next());    Assert.assertEquals(TransactionEventRecord.Type.PUT.get(), put.getRecordType());    Take take = new Take(System.currentTimeMillis(), WriteOrderOracle.next());    Assert.assertEquals(TransactionEventRecord.Type.TAKE.get(), take.getRecordType());    Rollback rollback = new Rollback(System.currentTimeMillis(), WriteOrderOracle.next());    Assert.assertEquals(TransactionEventRecord.Type.ROLLBACK.get(), rollback.getRecordType());    Commit commit = new Commit(System.currentTimeMillis(), WriteOrderOracle.next());    Assert.assertEquals(TransactionEventRecord.Type.COMMIT.get(), commit.getRecordType());}
0
public void testPutSerialization() throws IOException
{    Put in = new Put(System.currentTimeMillis(), WriteOrderOracle.next(), new FlumeEvent(new HashMap<String, String>(), new byte[0]));    Put out = (Put) TransactionEventRecord.fromDataInputV2(toDataInput(in));    Assert.assertEquals(in.getClass(), out.getClass());    Assert.assertEquals(in.getRecordType(), out.getRecordType());    Assert.assertEquals(in.getTransactionID(), out.getTransactionID());    Assert.assertEquals(in.getLogWriteOrderID(), out.getLogWriteOrderID());    Assert.assertEquals(in.getEvent().getHeaders(), out.getEvent().getHeaders());    Assert.assertTrue(Arrays.equals(in.getEvent().getBody(), out.getEvent().getBody()));}
0
public void testTakeSerialization() throws IOException
{    Take in = new Take(System.currentTimeMillis(), WriteOrderOracle.next(), 10, 20);    Take out = (Take) TransactionEventRecord.fromDataInputV2(toDataInput(in));    Assert.assertEquals(in.getClass(), out.getClass());    Assert.assertEquals(in.getRecordType(), out.getRecordType());    Assert.assertEquals(in.getTransactionID(), out.getTransactionID());    Assert.assertEquals(in.getLogWriteOrderID(), out.getLogWriteOrderID());    Assert.assertEquals(in.getFileID(), out.getFileID());    Assert.assertEquals(in.getOffset(), out.getOffset());}
0
public void testRollbackSerialization() throws IOException
{    Rollback in = new Rollback(System.currentTimeMillis(), WriteOrderOracle.next());    Rollback out = (Rollback) TransactionEventRecord.fromDataInputV2(toDataInput(in));    Assert.assertEquals(in.getClass(), out.getClass());    Assert.assertEquals(in.getRecordType(), out.getRecordType());    Assert.assertEquals(in.getTransactionID(), out.getTransactionID());    Assert.assertEquals(in.getLogWriteOrderID(), out.getLogWriteOrderID());}
0
public void testCommitSerialization() throws IOException
{    Commit in = new Commit(System.currentTimeMillis(), WriteOrderOracle.next());    Commit out = (Commit) TransactionEventRecord.fromDataInputV2(toDataInput(in));    Assert.assertEquals(in.getClass(), out.getClass());    Assert.assertEquals(in.getRecordType(), out.getRecordType());    Assert.assertEquals(in.getTransactionID(), out.getTransactionID());    Assert.assertEquals(in.getLogWriteOrderID(), out.getLogWriteOrderID());}
0
public void testBadHeader() throws IOException
{    Put in = new Put(System.currentTimeMillis(), WriteOrderOracle.next(), new FlumeEvent(new HashMap<String, String>(), new byte[0]));    try {        TransactionEventRecord.fromDataInputV2(toDataInput(0, in));        Assert.fail();    } catch (IOException e) {        Assert.assertEquals("Header 0 is not the required value: deadbeef", e.getMessage());    }}
0
public void testBadType() throws IOException
{    TransactionEventRecord in = mock(TransactionEventRecord.class);    when(in.getRecordType()).thenReturn(Short.MIN_VALUE);    try {        TransactionEventRecord.fromDataInputV2(toDataInput(in));        Assert.fail();    } catch (NullPointerException e) {        Assert.assertEquals("Unknown action ffff8000", e.getMessage());    }}
0
private DataInput toDataInput(TransactionEventRecord record) throws IOException
{    ByteBuffer buffer = TransactionEventRecord.toByteBufferV2(record);    ByteArrayInputStream byteInput = new ByteArrayInputStream(buffer.array());    DataInputStream dataInput = new DataInputStream(byteInput);    return dataInput;}
0
private DataInput toDataInput(int header, TransactionEventRecord record) throws IOException
{    ByteArrayOutputStream byteOutput = new ByteArrayOutputStream();    DataOutputStream dataOutput = new DataOutputStream(byteOutput);    dataOutput.writeInt(header);    dataOutput.writeShort(record.getRecordType());    dataOutput.writeLong(record.getTransactionID());    dataOutput.writeLong(record.getLogWriteOrderID());    record.write(dataOutput);    ByteArrayInputStream byteInput = new ByteArrayInputStream(byteOutput.toByteArray());    DataInputStream dataInput = new DataInputStream(byteInput);    return dataInput;}
0
public void testTypes() throws IOException
{    Put put = new Put(System.currentTimeMillis(), WriteOrderOracle.next());    Assert.assertEquals(TransactionEventRecord.Type.PUT.get(), put.getRecordType());    Take take = new Take(System.currentTimeMillis(), WriteOrderOracle.next());    Assert.assertEquals(TransactionEventRecord.Type.TAKE.get(), take.getRecordType());    Rollback rollback = new Rollback(System.currentTimeMillis(), WriteOrderOracle.next());    Assert.assertEquals(TransactionEventRecord.Type.ROLLBACK.get(), rollback.getRecordType());    Commit commit = new Commit(System.currentTimeMillis(), WriteOrderOracle.next());    Assert.assertEquals(TransactionEventRecord.Type.COMMIT.get(), commit.getRecordType());}
0
public void testPutSerialization() throws IOException, CorruptEventException
{    Map<String, String> headers = new HashMap<String, String>();    headers.put("key", "value");    Put in = new Put(System.currentTimeMillis(), WriteOrderOracle.next(), new FlumeEvent(headers, new byte[0]));    Put out = (Put) TransactionEventRecord.fromByteArray(toByteArray(in));    Assert.assertEquals(in.getClass(), out.getClass());    Assert.assertEquals(in.getRecordType(), out.getRecordType());    Assert.assertEquals(in.getTransactionID(), out.getTransactionID());    Assert.assertEquals(in.getLogWriteOrderID(), out.getLogWriteOrderID());    Assert.assertEquals(in.getEvent().getHeaders(), out.getEvent().getHeaders());    Assert.assertEquals(headers, in.getEvent().getHeaders());    Assert.assertEquals(headers, out.getEvent().getHeaders());    Assert.assertTrue(Arrays.equals(in.getEvent().getBody(), out.getEvent().getBody()));}
0
public void testPutSerializationNullHeader() throws IOException, CorruptEventException
{    Put in = new Put(System.currentTimeMillis(), WriteOrderOracle.next(), new FlumeEvent(null, new byte[0]));    Put out = (Put) TransactionEventRecord.fromByteArray(toByteArray(in));    Assert.assertEquals(in.getClass(), out.getClass());    Assert.assertEquals(in.getRecordType(), out.getRecordType());    Assert.assertEquals(in.getTransactionID(), out.getTransactionID());    Assert.assertEquals(in.getLogWriteOrderID(), out.getLogWriteOrderID());    Assert.assertNull(in.getEvent().getHeaders());    Assert.assertNotNull(out.getEvent().getHeaders());    Assert.assertTrue(Arrays.equals(in.getEvent().getBody(), out.getEvent().getBody()));}
0
public void testTakeSerialization() throws IOException, CorruptEventException
{    Take in = new Take(System.currentTimeMillis(), WriteOrderOracle.next(), 10, 20);    Take out = (Take) TransactionEventRecord.fromByteArray(toByteArray(in));    Assert.assertEquals(in.getClass(), out.getClass());    Assert.assertEquals(in.getRecordType(), out.getRecordType());    Assert.assertEquals(in.getTransactionID(), out.getTransactionID());    Assert.assertEquals(in.getLogWriteOrderID(), out.getLogWriteOrderID());    Assert.assertEquals(in.getFileID(), out.getFileID());    Assert.assertEquals(in.getOffset(), out.getOffset());}
0
public void testRollbackSerialization() throws IOException, CorruptEventException
{    Rollback in = new Rollback(System.currentTimeMillis(), WriteOrderOracle.next());    Rollback out = (Rollback) TransactionEventRecord.fromByteArray(toByteArray(in));    Assert.assertEquals(in.getClass(), out.getClass());    Assert.assertEquals(in.getRecordType(), out.getRecordType());    Assert.assertEquals(in.getTransactionID(), out.getTransactionID());    Assert.assertEquals(in.getLogWriteOrderID(), out.getLogWriteOrderID());}
0
public void testCommitSerialization() throws IOException, CorruptEventException
{    Commit in = new Commit(System.currentTimeMillis(), WriteOrderOracle.next());    Commit out = (Commit) TransactionEventRecord.fromByteArray(toByteArray(in));    Assert.assertEquals(in.getClass(), out.getClass());    Assert.assertEquals(in.getRecordType(), out.getRecordType());    Assert.assertEquals(in.getTransactionID(), out.getTransactionID());    Assert.assertEquals(in.getLogWriteOrderID(), out.getLogWriteOrderID());}
0
public void testBadType() throws IOException, CorruptEventException
{    TransactionEventRecord in = mock(TransactionEventRecord.class);    when(in.getRecordType()).thenReturn(Short.MIN_VALUE);    try {        TransactionEventRecord.fromByteArray(toByteArray(in));        Assert.fail();    } catch (NullPointerException e) {        Assert.assertEquals("Unknown action ffff8000", e.getMessage());    }}
0
private byte[] toByteArray(TransactionEventRecord record) throws IOException
{    ByteBuffer buffer = TransactionEventRecord.toByteBuffer(record);    return buffer.array();}
0
public void testSetSeed()
{    long current = TransactionIDOracle.next();    current += Integer.MAX_VALUE;    TransactionIDOracle.setSeed(current);    Assert.assertTrue(TransactionIDOracle.next() > System.currentTimeMillis());}
0
public static FlumeEvent newPersistableEvent()
{    Map<String, String> headers = Maps.newHashMap();    String timestamp = String.valueOf(System.currentTimeMillis());    headers.put("timestamp", timestamp);    FlumeEvent event = new FlumeEvent(headers, timestamp.getBytes());    return event;}
0
public static FlumeEvent newPersistableEvent(int size)
{    Map<String, String> headers = Maps.newHashMap();    String timestamp = String.valueOf(System.currentTimeMillis());    headers.put("timestamp", timestamp);    byte[] data = new byte[size];    Arrays.fill(data, (byte) 54);    FlumeEvent event = new FlumeEvent(headers, data);    return event;}
0
public static DataInput toDataInput(Writable writable) throws IOException
{    ByteArrayOutputStream byteOutput = new ByteArrayOutputStream();    DataOutputStream dataOutput = new DataOutputStream(byteOutput);    writable.write(dataOutput);    ByteArrayInputStream byteInput = new ByteArrayInputStream(byteOutput.toByteArray());    DataInputStream dataInput = new DataInputStream(byteInput);    return dataInput;}
0
public static void compareInputAndOut(Set<String> in, Set<String> out)
{    Assert.assertNotNull(in);    Assert.assertNotNull(out);    Assert.assertEquals(in.size(), out.size());    Assert.assertTrue(in.equals(out));}
0
public static Set<String> putWithoutCommit(Channel channel, Transaction tx, String prefix, int number)
{    Set<String> events = Sets.newHashSet();    tx.begin();    for (int i = 0; i < number; i++) {        String eventData = (prefix + UUID.randomUUID()).toString();        Event event = EventBuilder.withBody(eventData.getBytes());        channel.put(event);        events.add(eventData);    }    return events;}
0
public static Set<String> takeWithoutCommit(Channel channel, Transaction tx, int number)
{    Set<String> events = Sets.newHashSet();    tx.begin();    for (int i = 0; i < number; i++) {        Event e = channel.take();        if (e == null) {            break;        }        events.add(new String(e.getBody()));    }    return events;}
0
public static List<File> getAllLogs(File[] dataDirs)
{    List<File> result = Lists.newArrayList();    for (File dataDir : dataDirs) {        result.addAll(LogUtils.getLogs(dataDir));    }    return result;}
0
public static void forceCheckpoint(FileChannel channel)
{    Log log = field("log").ofType(Log.class).in(channel).get();    Assert.assertTrue("writeCheckpoint returned false", method("writeCheckpoint").withReturnType(Boolean.class).withParameterTypes(Boolean.class).in(log).invoke(true));}
0
public static Set<String> takeEvents(Channel channel, int batchSize) throws Exception
{    return takeEvents(channel, batchSize, false);}
0
public static Set<String> takeEvents(Channel channel, int batchSize, boolean checkForCorruption) throws Exception
{    return takeEvents(channel, batchSize, Integer.MAX_VALUE, checkForCorruption);}
0
public static Set<String> takeEvents(Channel channel, int batchSize, int numEvents) throws Exception
{    return takeEvents(channel, batchSize, numEvents, false);}
0
public static Set<String> takeEvents(Channel channel, int batchSize, int numEvents, boolean checkForCorruption) throws Exception
{    Set<String> result = Sets.newHashSet();    for (int i = 0; i < numEvents; i += batchSize) {        Transaction transaction = channel.getTransaction();        try {            transaction.begin();            for (int j = 0; j < batchSize; j++) {                Event event;                try {                    event = channel.take();                } catch (ChannelException ex) {                    Throwable th = ex;                    String msg;                    if (checkForCorruption) {                        msg = "Corrupt event found. Please run File Channel";                        th = ex.getCause();                    } else {                        msg = "Take list for FileBackedTransaction, capacity";                    }                    Assert.assertTrue(th.getMessage().startsWith(msg));                    if (checkForCorruption) {                        throw (Exception) th;                    }                    transaction.commit();                    return result;                }                if (event == null) {                    transaction.commit();                    return result;                }                result.add(new String(event.getBody(), Charsets.UTF_8));            }            transaction.commit();        } catch (Throwable ex) {            transaction.rollback();            throw new RuntimeException(ex);        } finally {            transaction.close();        }    }    return result;}
0
public static Set<String> consumeChannel(Channel channel) throws Exception
{    return consumeChannel(channel, false);}
0
public static Set<String> consumeChannel(Channel channel, boolean checkForCorruption) throws Exception
{    Set<String> result = Sets.newHashSet();    int[] batchSizes = new int[] { 1000, 100, 10, 1 };    for (int i = 0; i < batchSizes.length; i++) {        while (true) {            Set<String> batch = takeEvents(channel, batchSizes[i], checkForCorruption);            if (batch.isEmpty()) {                break;            }            result.addAll(batch);        }    }    return result;}
0
public static Set<String> fillChannel(Channel channel, String prefix) throws Exception
{    Set<String> result = Sets.newHashSet();    int[] batchSizes = new int[] { 1000, 100, 10, 1 };    for (int i = 0; i < batchSizes.length; i++) {        try {            while (true) {                Set<String> batch = putEvents(channel, prefix, batchSizes[i], Integer.MAX_VALUE, true);                if (batch.isEmpty()) {                    break;                }                result.addAll(batch);            }        } catch (ChannelException e) {            Assert.assertTrue(("The channel has reached it's capacity. This might " + "be the result of a sink on the channel having too low of batch " + "size, a downstream system running slower than normal, or that " + "the channel capacity is just too low. [channel=" + channel.getName() + "]").equals(e.getMessage()) || e.getMessage().startsWith("Put queue for FileBackedTransaction of capacity "));        }    }    return result;}
0
public static Set<String> putEvents(Channel channel, String prefix, int batchSize, int numEvents) throws Exception
{    return putEvents(channel, prefix, batchSize, numEvents, false);}
0
public static Set<String> putEvents(Channel channel, String prefix, int batchSize, int numEvents, boolean untilCapacityIsReached) throws Exception
{    Set<String> result = Sets.newHashSet();    for (int i = 0; i < numEvents; i += batchSize) {        Transaction transaction = channel.getTransaction();        transaction.begin();        try {            Set<String> batch = Sets.newHashSet();            for (int j = 0; j < batchSize; j++) {                String s = prefix + "-" + i + "-" + j + "-" + UUID.randomUUID();                Event event = EventBuilder.withBody(s.getBytes(Charsets.UTF_8));                channel.put(event);                batch.add(s);            }            transaction.commit();            result.addAll(batch);        } catch (Exception ex) {            transaction.rollback();            if (untilCapacityIsReached && ex instanceof ChannelException && ("The channel has reached it's capacity. " + "This might be the result of a sink on the channel having too " + "low of batch size, a downstream system running slower than " + "normal, or that the channel capacity is just too low. " + "[channel=" + channel.getName() + "]").equals(ex.getMessage())) {                break;            }            throw ex;        } finally {            transaction.close();        }    }    return result;}
0
public static void copyDecompressed(String resource, File output) throws IOException
{    URL input = Resources.getResource(resource);    FileOutputStream fos = new FileOutputStream(output);    GZIPInputStream gzis = new GZIPInputStream(input.openStream());    ByteStreams.copy(gzis, fos);    fos.close();    gzis.close();}
0
public static Context createFileChannelContext(String checkpointDir, String dataDir, String backupDir, Map<String, String> overrides)
{    Context context = new Context();    context.put(FileChannelConfiguration.CHECKPOINT_DIR, checkpointDir);    if (backupDir != null) {        context.put(FileChannelConfiguration.BACKUP_CHECKPOINT_DIR, backupDir);    }    context.put(FileChannelConfiguration.DATA_DIRS, dataDir);    context.put(FileChannelConfiguration.KEEP_ALIVE, String.valueOf(1));    context.put(FileChannelConfiguration.CAPACITY, String.valueOf(10000));    context.putAll(overrides);    return context;}
0
public static FileChannel createFileChannel(String checkpointDir, String dataDir, Map<String, String> overrides)
{    return createFileChannel(checkpointDir, dataDir, null, overrides);}
0
public static FileChannel createFileChannel(String checkpointDir, String dataDir, String backupDir, Map<String, String> overrides)
{    FileChannel channel = new FileChannel();    channel.setName("FileChannel-" + UUID.randomUUID());    Context context = createFileChannelContext(checkpointDir, dataDir, backupDir, overrides);    Configurables.configure(channel, context);    return channel;}
0
public static File writeStringToFile(File baseDir, String name, String text) throws IOException
{    File passwordFile = new File(baseDir, name);    Files.write(text, passwordFile, Charsets.UTF_8);    return passwordFile;}
0
public void testSetSeed()
{    long current = WriteOrderOracle.next();    current += Integer.MAX_VALUE;    WriteOrderOracle.setSeed(current);    Assert.assertTrue(WriteOrderOracle.next() > System.currentTimeMillis());}
0
public String toString()
{    return getName();}
0
public String getName()
{    return name;}
0
public String getValidationQuery()
{    return validationQuery;}
0
public static DatabaseType getByName(String dbName)
{    DatabaseType type = null;    try {        type = DatabaseType.valueOf(dbName.trim().toUpperCase(Locale.ENGLISH));    } catch (IllegalArgumentException ex) {        type = DatabaseType.OTHER;    }    return type;}
0
public boolean schemaExists()
{    Connection connection = null;    Statement stmt = null;    try {        connection = dataSource.getConnection();        stmt = connection.createStatement();        ResultSet rset = stmt.executeQuery(QUREY_SYSCHEMA_FLUME);        if (!rset.next()) {                        return false;        }        String flumeSchemaId = rset.getString(1);                connection.commit();    } catch (SQLException ex) {        try {            connection.rollback();        } catch (SQLException ex2) {                    }        throw new JdbcChannelException("Unable to query schema", ex);    } finally {        if (stmt != null) {            try {                stmt.close();            } catch (SQLException ex) {                            }        }        if (connection != null) {            try {                connection.close();            } catch (SQLException ex) {                            }        }    }    return true;}
1
public void createSchemaObjects(boolean createForeignKeys, boolean createIndex)
{    runQuery(QUERY_CREATE_SCHEMA_FLUME);    runQuery(QUERY_CREATE_TABLE_FL_EVENT);    if (createForeignKeys) {        runQuery(QUERY_CREATE_TABLE_FL_PLSPILL_FK);        runQuery(QUERY_CREATE_TABLE_FL_HEADER_FK);        runQuery(QUERY_CREATE_TABLE_FL_NMSPILL_FK);        runQuery(QUERY_CREATE_TABLE_FL_VLSPILL_FK);    } else {        runQuery(QUERY_CREATE_TABLE_FL_PLSPILL_NOFK);        runQuery(QUERY_CREATE_TABLE_FL_HEADER_NOFK);        runQuery(QUERY_CREATE_TABLE_FL_NMSPILL_NOFK);        runQuery(QUERY_CREATE_TABLE_FL_VLSPILL_NOFK);    }    if (createIndex) {        runQuery(QUERY_CREATE_INDEX_FLE_CHANNEL);        runQuery(QUERY_CREATE_INDEX_FLH_EVENT);        runQuery(QUERY_CREATE_INDEX_FLP_EVENT);        runQuery(QUERY_CREATE_INDEX_FLN_HEADER);        runQuery(QUERY_CREATE_INDEX_FLV_HEADER);    }}
0
public void validateSchema()
{    verifyTableStructure(SCHEMA_FLUME, TABLE_FL_EVENT_NAME, COLUMN_FLE_ID, COLUMN_FLE_PAYLOAD, COLUMN_FLE_CHANNEL, COLUMN_FLE_SPILL);    verifyTableStructure(SCHEMA_FLUME, TABLE_FL_PLSPILL_NAME, COLUMN_FLP_EVENT, COLUMN_FLP_SPILL);    verifyTableStructure(SCHEMA_FLUME, TABLE_FL_HEADER_NAME, COLUMN_FLH_ID, COLUMN_FLH_EVENT, COLUMN_FLH_NAME, COLUMN_FLH_VALUE, COLUMN_FLH_NMSPILL, COLUMN_FLH_VLSPILL);    verifyTableStructure(SCHEMA_FLUME, TABLE_FL_NMSPILL_NAME, COLUMN_FLN_HEADER, COLUMN_FLN_SPILL);    verifyTableStructure(SCHEMA_FLUME, TABLE_FL_VLSPILL_NAME, COLUMN_FLV_HEADER, COLUMN_FLV_SPILL);}
0
private void verifyTableStructure(String schemaName, String tableName, String... columns)
{    Set<String> columnNames = new HashSet<String>();    Connection connection = null;    PreparedStatement pStmt = null;    try {        connection = dataSource.getConnection();        pStmt = connection.prepareStatement(COLUMN_LOOKUP_QUERY);        pStmt.setString(1, tableName);        pStmt.setString(2, schemaName);        ResultSet rset = pStmt.executeQuery();        while (rset.next()) {            columnNames.add(rset.getString(1));        }        connection.commit();    } catch (SQLException ex) {        try {            connection.rollback();        } catch (SQLException ex2) {                    }        throw new JdbcChannelException("Unable to run query: " + COLUMN_LOOKUP_QUERY + ": 1=" + tableName + ", 2=" + schemaName, ex);    } finally {        if (pStmt != null) {            try {                pStmt.close();            } catch (SQLException ex) {                            }            if (connection != null) {                try {                    connection.close();                } catch (SQLException ex) {                                    }            }        }    }    Set<String> columnDiff = new HashSet<String>();    columnDiff.addAll(columnNames);        StringBuilder sb = new StringBuilder("{");    boolean first = true;    for (String column : columns) {        columnDiff.remove(column);        if (first) {            first = false;        } else {            sb.append(", ");        }        sb.append(column);    }    sb.append("}");    String expectedColumns = sb.toString();    if (LOGGER.isDebugEnabled()) {            }    if (columnNames.size() != columns.length || columnDiff.size() != 0) {        throw new JdbcChannelException("Expected table " + schemaName + "." + tableName + " to have columns: " + expectedColumns + ". Instead " + "found columns: " + columnNames);    }}
1
private void runQuery(String query)
{    Connection connection = null;    Statement stmt = null;    try {        connection = dataSource.getConnection();        stmt = connection.createStatement();        if (stmt.execute(query)) {            ResultSet rset = stmt.getResultSet();            int count = 0;            while (rset.next()) {                count++;            }                    } else {            int updateCount = stmt.getUpdateCount();                    }        connection.commit();    } catch (SQLException ex) {        try {            connection.rollback();        } catch (SQLException ex2) {                    }        throw new JdbcChannelException("Unable to run query: " + query, ex);    } finally {        if (stmt != null) {            try {                stmt.close();            } catch (SQLException ex) {                            }            if (connection != null) {                try {                    connection.close();                } catch (SQLException ex) {                                    }            }        }    }}
1
public void storeEvent(PersistableEvent pe, Connection connection)
{        byte[] basePayload = pe.getBasePayload();    byte[] spillPayload = pe.getSpillPayload();    boolean hasSpillPayload = (spillPayload != null);    String channelName = pe.getChannelName();        PreparedStatement baseEventStmt = null;    PreparedStatement spillEventStmt = null;    PreparedStatement baseHeaderStmt = null;    PreparedStatement headerNameSpillStmt = null;    PreparedStatement headerValueSpillStmt = null;    try {        baseEventStmt = connection.prepareStatement(STMT_INSERT_EVENT_BASE, Statement.RETURN_GENERATED_KEYS);        baseEventStmt.setBytes(1, basePayload);        baseEventStmt.setString(2, channelName);        baseEventStmt.setBoolean(3, hasSpillPayload);        int baseEventCount = baseEventStmt.executeUpdate();        if (baseEventCount != 1) {            throw new JdbcChannelException("Invalid update count on base " + "event insert: " + baseEventCount);        }                ResultSet eventIdResult = baseEventStmt.getGeneratedKeys();        if (!eventIdResult.next()) {            throw new JdbcChannelException("Unable to retrieive inserted event-id");        }        long eventId = eventIdResult.getLong(1);        pe.setEventId(eventId);                if (hasSpillPayload) {            spillEventStmt = connection.prepareStatement(STMT_INSERT_EVENT_SPILL);            spillEventStmt.setLong(1, eventId);            spillEventStmt.setBinaryStream(2, new ByteArrayInputStream(spillPayload), spillPayload.length);            int spillEventCount = spillEventStmt.executeUpdate();            if (spillEventCount != 1) {                throw new JdbcChannelException("Invalid update count on spill " + "event insert: " + spillEventCount);            }        }                List<HeaderEntry> headers = pe.getHeaderEntries();        if (headers != null && headers.size() > 0) {            List<HeaderEntry> headerWithNameSpill = new ArrayList<HeaderEntry>();            List<HeaderEntry> headerWithValueSpill = new ArrayList<HeaderEntry>();            baseHeaderStmt = connection.prepareStatement(STMT_INSERT_HEADER_BASE, Statement.RETURN_GENERATED_KEYS);            Iterator<HeaderEntry> it = headers.iterator();            while (it.hasNext()) {                HeaderEntry entry = it.next();                SpillableString name = entry.getName();                SpillableString value = entry.getValue();                baseHeaderStmt.setLong(1, eventId);                baseHeaderStmt.setString(2, name.getBase());                baseHeaderStmt.setString(3, value.getBase());                baseHeaderStmt.setBoolean(4, name.hasSpill());                baseHeaderStmt.setBoolean(5, value.hasSpill());                int updateCount = baseHeaderStmt.executeUpdate();                if (updateCount != 1) {                    throw new JdbcChannelException("Unexpected update header count: " + updateCount);                }                ResultSet headerIdResultSet = baseHeaderStmt.getGeneratedKeys();                if (!headerIdResultSet.next()) {                    throw new JdbcChannelException("Unable to retrieve inserted header id");                }                long headerId = headerIdResultSet.getLong(1);                entry.setId(headerId);                if (name.hasSpill()) {                    headerWithNameSpill.add(entry);                }                if (value.hasSpill()) {                    headerWithValueSpill.add(entry);                }            }                        if (headerWithNameSpill.size() > 0) {                                headerNameSpillStmt = connection.prepareStatement(STMT_INSERT_HEADER_NAME_SPILL);                for (HeaderEntry entry : headerWithNameSpill) {                    String nameSpill = entry.getName().getSpill();                    headerNameSpillStmt.setLong(1, entry.getId());                    headerNameSpillStmt.setString(2, nameSpill);                    headerNameSpillStmt.addBatch();                }                int[] nameSpillUpdateCount = headerNameSpillStmt.executeBatch();                if (nameSpillUpdateCount.length != headerWithNameSpill.size()) {                    throw new JdbcChannelException("Unexpected update count for header " + "name spills: expected " + headerWithNameSpill.size() + ", " + "found " + nameSpillUpdateCount.length);                }                for (int i = 0; i < nameSpillUpdateCount.length; i++) {                    if (nameSpillUpdateCount[i] != 1) {                        throw new JdbcChannelException("Unexpected update count for " + "header name spill at position " + i + ", value: " + nameSpillUpdateCount[i]);                    }                }            }                        if (headerWithValueSpill.size() > 0) {                                headerValueSpillStmt = connection.prepareStatement(STMT_INSERT_HEADER_VALUE_SPILL);                for (HeaderEntry entry : headerWithValueSpill) {                    String valueSpill = entry.getValue().getSpill();                    headerValueSpillStmt.setLong(1, entry.getId());                    headerValueSpillStmt.setString(2, valueSpill);                    headerValueSpillStmt.addBatch();                }                int[] valueSpillUpdateCount = headerValueSpillStmt.executeBatch();                if (valueSpillUpdateCount.length != headerWithValueSpill.size()) {                    throw new JdbcChannelException("Unexpected update count for header " + "value spills: expected " + headerWithValueSpill.size() + ", " + "found " + valueSpillUpdateCount.length);                }                for (int i = 0; i < valueSpillUpdateCount.length; i++) {                    if (valueSpillUpdateCount[i] != 1) {                        throw new JdbcChannelException("Unexpected update count for " + "header value spill at position " + i + ", value: " + valueSpillUpdateCount[i]);                    }                }            }        }    } catch (SQLException ex) {        throw new JdbcChannelException("Unable to persist event: " + pe, ex);    } finally {        if (baseEventStmt != null) {            try {                baseEventStmt.close();            } catch (SQLException ex) {                            }        }        if (spillEventStmt != null) {            try {                spillEventStmt.close();            } catch (SQLException ex) {                            }        }        if (baseHeaderStmt != null) {            try {                baseHeaderStmt.close();            } catch (SQLException ex) {                            }        }        if (headerNameSpillStmt != null) {            try {                headerNameSpillStmt.close();            } catch (SQLException ex) {                            }        }        if (headerValueSpillStmt != null) {            try {                headerValueSpillStmt.close();            } catch (SQLException ex) {                            }        }    }    }
1
public PersistableEvent fetchAndDeleteEvent(String channel, Connection connection)
{    PersistableEvent.Builder peBuilder = null;    PreparedStatement baseEventFetchStmt = null;    PreparedStatement spillEventFetchStmt = null;    InputStream payloadInputStream = null;    PreparedStatement baseHeaderFetchStmt = null;    PreparedStatement nameSpillHeaderStmt = null;    PreparedStatement valueSpillHeaderStmt = null;    PreparedStatement deleteSpillEventStmt = null;    PreparedStatement deleteNameSpillHeaderStmt = null;    PreparedStatement deleteValueSpillHeaderStmt = null;    PreparedStatement deleteBaseHeaderStmt = null;    PreparedStatement deleteBaseEventStmt = null;    try {        baseEventFetchStmt = connection.prepareStatement(STMT_FETCH_PAYLOAD_BASE);        baseEventFetchStmt.setString(1, channel);        ResultSet rsetBaseEvent = baseEventFetchStmt.executeQuery();        if (!rsetBaseEvent.next()) {                                    return null;        }                long eventId = rsetBaseEvent.getLong(1);        peBuilder = new PersistableEvent.Builder(channel, eventId);        peBuilder.setBasePayload(rsetBaseEvent.getBytes(2));        boolean hasSpill = rsetBaseEvent.getBoolean(3);        if (hasSpill) {            spillEventFetchStmt = connection.prepareStatement(STMT_FETCH_PAYLOAD_SPILL);            spillEventFetchStmt.setLong(1, eventId);            ResultSet rsetSpillEvent = spillEventFetchStmt.executeQuery();            if (!rsetSpillEvent.next()) {                throw new JdbcChannelException("Payload spill expected but not " + "found for event: " + eventId);            }            Blob payloadSpillBlob = rsetSpillEvent.getBlob(1);            payloadInputStream = payloadSpillBlob.getBinaryStream();            ByteArrayOutputStream spillStream = new ByteArrayOutputStream();            byte[] buffer = new byte[1024];            int length = 0;            while ((length = payloadInputStream.read(buffer)) != -1) {                spillStream.write(buffer, 0, length);            }            peBuilder.setSpillPayload(spillStream.toByteArray());                        deleteSpillEventStmt = connection.prepareStatement(STMT_DELETE_EVENT_SPILL);            deleteSpillEventStmt.setLong(1, eventId);            int updateCount = deleteSpillEventStmt.executeUpdate();            if (updateCount != 1) {                throw new JdbcChannelException("Unexpected row count for spill " + "delete: " + updateCount);            }        }        if (rsetBaseEvent.next()) {            throw new JdbcChannelException("More than expected events retrieved");        }                List<Long> nameSpillHeaders = null;        List<Long> valueSpillHeaders = null;        baseHeaderFetchStmt = connection.prepareStatement(STMT_FETCH_HEADER_BASE);        baseHeaderFetchStmt.setLong(1, eventId);                int headerCount = 0;        ResultSet rsetBaseHeader = baseHeaderFetchStmt.executeQuery();        while (rsetBaseHeader.next()) {            headerCount++;            long headerId = rsetBaseHeader.getLong(1);            String baseName = rsetBaseHeader.getString(2);            String baseValue = rsetBaseHeader.getString(3);            boolean hasNameSpill = rsetBaseHeader.getBoolean(4);            boolean hasValueSpill = rsetBaseHeader.getBoolean(5);            peBuilder.setHeader(headerId, baseName, baseValue);            if (hasNameSpill) {                if (nameSpillHeaders == null) {                    nameSpillHeaders = new ArrayList<Long>();                }                nameSpillHeaders.add(headerId);            }            if (hasValueSpill) {                if (valueSpillHeaders == null) {                    valueSpillHeaders = new ArrayList<Long>();                }                valueSpillHeaders.add(headerId);            }        }        if (nameSpillHeaders != null) {            nameSpillHeaderStmt = connection.prepareStatement(STMT_FETCH_HEADER_NAME_SPILL);            deleteNameSpillHeaderStmt = connection.prepareStatement(STMT_DELETE_HEADER_NAME_SPILL);            for (long headerId : nameSpillHeaders) {                nameSpillHeaderStmt.setLong(1, headerId);                ResultSet rsetHeaderNameSpill = nameSpillHeaderStmt.executeQuery();                if (!rsetHeaderNameSpill.next()) {                    throw new JdbcChannelException("Name spill was set for header " + headerId + " but was not found");                }                String nameSpill = rsetHeaderNameSpill.getString(1);                peBuilder.setHeaderNameSpill(headerId, nameSpill);                deleteNameSpillHeaderStmt.setLong(1, headerId);                deleteNameSpillHeaderStmt.addBatch();            }                        int[] headerNameSpillDelete = deleteNameSpillHeaderStmt.executeBatch();            if (headerNameSpillDelete.length != nameSpillHeaders.size()) {                throw new JdbcChannelException("Unexpected number of header name " + "spill deletes: expected " + nameSpillHeaders.size() + ", found: " + headerNameSpillDelete.length);            }            for (int numRowsAffected : headerNameSpillDelete) {                if (numRowsAffected != 1) {                    throw new JdbcChannelException("Unexpected number of deleted rows " + "for header name spill deletes: " + numRowsAffected);                }            }        }        if (valueSpillHeaders != null) {            valueSpillHeaderStmt = connection.prepareStatement(STMT_FETCH_HEADER_VALUE_SPILL);            deleteValueSpillHeaderStmt = connection.prepareStatement(STMT_DELETE_HEADER_VALUE_SPILL);            for (long headerId : valueSpillHeaders) {                valueSpillHeaderStmt.setLong(1, headerId);                ResultSet rsetHeaderValueSpill = valueSpillHeaderStmt.executeQuery();                if (!rsetHeaderValueSpill.next()) {                    throw new JdbcChannelException("Value spill was set for header " + headerId + " but was not found");                }                String valueSpill = rsetHeaderValueSpill.getString(1);                peBuilder.setHeaderValueSpill(headerId, valueSpill);                deleteValueSpillHeaderStmt.setLong(1, headerId);                deleteValueSpillHeaderStmt.addBatch();            }                        int[] headerValueSpillDelete = deleteValueSpillHeaderStmt.executeBatch();            if (headerValueSpillDelete.length != valueSpillHeaders.size()) {                throw new JdbcChannelException("Unexpected number of header value " + "spill deletes: expected " + valueSpillHeaders.size() + ", found: " + headerValueSpillDelete.length);            }            for (int numRowsAffected : headerValueSpillDelete) {                if (numRowsAffected != 1) {                    throw new JdbcChannelException("Unexpected number of deleted rows " + "for header value spill deletes: " + numRowsAffected);                }            }        }                if (headerCount > 0) {            deleteBaseHeaderStmt = connection.prepareStatement(STMT_DELETE_HEADER_BASE);            deleteBaseHeaderStmt.setLong(1, eventId);            int rowCount = deleteBaseHeaderStmt.executeUpdate();            if (rowCount != headerCount) {                throw new JdbcChannelException("Unexpected base header delete count: " + "expected: " + headerCount + ", found: " + rowCount);            }        }                deleteBaseEventStmt = connection.prepareStatement(STMT_DELETE_EVENT_BASE);        deleteBaseEventStmt.setLong(1, eventId);        int rowCount = deleteBaseEventStmt.executeUpdate();        if (rowCount != 1) {            throw new JdbcChannelException("Unexpected row count for delete of " + "event-id: " + eventId + ", count: " + rowCount);        }    } catch (SQLException ex) {        throw new JdbcChannelException("Unable to retrieve event", ex);    } catch (IOException ex) {        throw new JdbcChannelException("Unable to read data", ex);    } finally {        if (payloadInputStream != null) {            try {                payloadInputStream.close();            } catch (IOException ex) {                            }        }        if (baseEventFetchStmt != null) {            try {                baseEventFetchStmt.close();            } catch (SQLException ex) {                            }        }        if (spillEventFetchStmt != null) {            try {                spillEventFetchStmt.close();            } catch (SQLException ex) {                            }        }        if (deleteSpillEventStmt != null) {            try {                deleteSpillEventStmt.close();            } catch (SQLException ex) {                            }        }        if (baseHeaderFetchStmt != null) {            try {                baseHeaderFetchStmt.close();            } catch (SQLException ex) {                            }        }        if (nameSpillHeaderStmt != null) {            try {                nameSpillHeaderStmt.close();            } catch (SQLException ex) {                            }        }        if (valueSpillHeaderStmt != null) {            try {                valueSpillHeaderStmt.close();            } catch (SQLException ex) {                            }        }        if (deleteNameSpillHeaderStmt != null) {            try {                deleteNameSpillHeaderStmt.close();            } catch (SQLException ex) {                            }        }        if (deleteValueSpillHeaderStmt != null) {            try {                deleteValueSpillHeaderStmt.close();            } catch (SQLException ex) {                            }        }        if (deleteBaseHeaderStmt != null) {            try {                deleteBaseHeaderStmt.close();            } catch (SQLException ex) {                            }        }        if (deleteBaseEventStmt != null) {            try {                deleteBaseEventStmt.close();            } catch (SQLException ex) {                            }        }    }    return peBuilder.build();}
1
public long getChannelSize(Connection connection)
{    long size = 0L;    Statement stmt = null;    try {        stmt = connection.createStatement();        stmt.execute(QUERY_CHANNEL_SIZE);        ResultSet rset = stmt.getResultSet();        if (!rset.next()) {            throw new JdbcChannelException("Failed to determine channel size: " + "Query (" + QUERY_CHANNEL_SIZE + ") did not produce any results");        }        size = rset.getLong(1);        connection.commit();    } catch (SQLException ex) {        try {            connection.rollback();        } catch (SQLException ex2) {                    }        throw new JdbcChannelException("Unable to run query: " + QUERY_CHANNEL_SIZE, ex);    } finally {        if (stmt != null) {            try {                stmt.close();            } catch (SQLException ex) {                            }        }    }    return size;}
1
public void initialize(Context context)
{        initializeSystemProperties(context);    initializeDataSource(context);    initializeSchema(context);    initializeChannelState(context);}
1
private void initializeSystemProperties(Context context)
{    Map<String, String> sysProps = new HashMap<String, String>();    Map<String, String> sysPropsOld = context.getSubProperties(ConfigurationConstants.OLD_CONFIG_JDBC_SYSPROP_PREFIX);    if (sysPropsOld.size() > 0) {                sysProps.putAll(sysPropsOld);    }    Map<String, String> sysPropsNew = context.getSubProperties(ConfigurationConstants.CONFIG_JDBC_SYSPROP_PREFIX);        if (sysPropsNew.size() > 0) {        sysProps.putAll(sysPropsNew);    }    for (String key : sysProps.keySet()) {        String value = sysProps.get(key);        if (key != null && value != null) {            System.setProperty(key, value);        }    }}
1
private void initializeChannelState(Context context)
{    String maxCapacityStr = getConfigurationString(context, ConfigurationConstants.CONFIG_MAX_CAPACITY, ConfigurationConstants.OLD_CONFIG_MAX_CAPACITY, "0");    long maxCapacitySpecified = 0;    try {        maxCapacitySpecified = Long.parseLong(maxCapacityStr);    } catch (NumberFormatException nfe) {            }    if (maxCapacitySpecified > 0) {        this.maxCapacity = maxCapacitySpecified;            } else {            }    if (maxCapacity > 0) {                JdbcTransactionImpl tx = null;        try {            tx = getTransaction();            tx.begin();            Connection conn = tx.getConnection();            currentSize.set(schemaHandler.getChannelSize(conn));            tx.commit();        } catch (Exception ex) {            tx.rollback();            throw new JdbcChannelException("Failed to initialize current size", ex);        } finally {            if (tx != null) {                tx.close();            }        }        long currentSizeLong = currentSize.get();        if (currentSizeLong > maxCapacity) {                    }            }}
1
private void initializeSchema(Context context)
{    String createSchemaFlag = getConfigurationString(context, ConfigurationConstants.CONFIG_CREATE_SCHEMA, ConfigurationConstants.OLD_CONFIG_CREATE_SCHEMA, "true");    boolean createSchema = Boolean.valueOf(createSchemaFlag);            schemaHandler = SchemaHandlerFactory.getHandler(databaseType, dataSource);    if (!schemaHandler.schemaExists()) {        if (!createSchema) {            throw new JdbcChannelException("Schema does not exist and " + "auto-generation is disabled. Please enable auto-generation of " + "schema and try again.");        }        String createIndexFlag = getConfigurationString(context, ConfigurationConstants.CONFIG_CREATE_INDEX, ConfigurationConstants.OLD_CONFIG_CREATE_INDEX, "true");        String createForeignKeysFlag = getConfigurationString(context, ConfigurationConstants.CONFIG_CREATE_FK, ConfigurationConstants.OLD_CONFIG_CREATE_FK, "true");        boolean createIndex = Boolean.valueOf(createIndexFlag);        if (!createIndex) {                    }        boolean createForeignKeys = Boolean.valueOf(createForeignKeysFlag);        if (createForeignKeys) {                    } else {                    }                schemaHandler.createSchemaObjects(createForeignKeys, createIndex);    }        schemaHandler.validateSchema();}
1
public void close()
{    try {        connectionPool.close();    } catch (Exception ex) {        throw new JdbcChannelException("Unable to close connection pool", ex);    }    if (databaseType.equals(DatabaseType.DERBY) && driverClassName.equals(EMBEDDED_DERBY_DRIVER_CLASSNAME)) {                if (connectUrl.startsWith("jdbc:derby:")) {            int index = connectUrl.indexOf(";");            String baseUrl = null;            if (index != -1) {                baseUrl = connectUrl.substring(0, index + 1);            } else {                baseUrl = connectUrl + ";";            }            String shutDownUrl = baseUrl + "shutdown=true";                        try {                DriverManager.getConnection(shutDownUrl);            } catch (SQLException ex) {                                if (ex.getErrorCode() != 45000) {                    throw new JdbcChannelException("Unable to shutdown embedded Derby: " + shutDownUrl + " Error Code: " + ex.getErrorCode(), ex);                }                            }        } else {                    }    }    dataSource = null;    txFactory = null;    schemaHandler = null;}
1
public void persistEvent(String channel, Event event)
{    PersistableEvent persistableEvent = new PersistableEvent(channel, event);    JdbcTransactionImpl tx = null;    try {        tx = getTransaction();        tx.begin();        if (maxCapacity > 0) {            long currentSizeLong = currentSize.get();            if (currentSizeLong >= maxCapacity) {                throw new JdbcChannelException("Channel capacity reached: " + "maxCapacity: " + maxCapacity + ", currentSize: " + currentSizeLong);            }        }                schemaHandler.storeEvent(persistableEvent, tx.getConnection());        tx.incrementPersistedEventCount();        tx.commit();    } catch (Exception ex) {        tx.rollback();        throw new JdbcChannelException("Failed to persist event", ex);    } finally {        if (tx != null) {            tx.close();        }    }    }
1
public Event removeEvent(String channelName)
{    PersistableEvent result = null;    JdbcTransactionImpl tx = null;    try {        tx = getTransaction();        tx.begin();                result = schemaHandler.fetchAndDeleteEvent(channelName, tx.getConnection());        if (result != null) {            tx.incrementRemovedEventCount();        }        tx.commit();    } catch (Exception ex) {        tx.rollback();        throw new JdbcChannelException("Failed to persist event", ex);    } finally {        if (tx != null) {            tx.close();        }    }    if (result != null) {            } else {            }    return result;}
1
public JdbcTransactionImpl getTransaction()
{    return txFactory.get();}
0
private void initializeDataSource(Context context)
{    driverClassName = getConfigurationString(context, ConfigurationConstants.CONFIG_JDBC_DRIVER_CLASS, ConfigurationConstants.OLD_CONFIG_JDBC_DRIVER_CLASS, null);    connectUrl = getConfigurationString(context, ConfigurationConstants.CONFIG_URL, ConfigurationConstants.OLD_CONFIG_URL, null);    String userName = getConfigurationString(context, ConfigurationConstants.CONFIG_USERNAME, ConfigurationConstants.OLD_CONFIG_USERNAME, null);    String password = getConfigurationString(context, ConfigurationConstants.CONFIG_PASSWORD, ConfigurationConstants.OLD_CONFIG_PASSWORD, null);    String jdbcPropertiesFile = getConfigurationString(context, ConfigurationConstants.CONFIG_JDBC_PROPS_FILE, ConfigurationConstants.OLD_CONFIG_JDBC_PROPS_FILE, null);    String dbTypeName = getConfigurationString(context, ConfigurationConstants.CONFIG_DATABASE_TYPE, ConfigurationConstants.OLD_CONFIG_DATABASE_TYPE, null);        if (connectUrl == null || connectUrl.trim().length() == 0) {                driverClassName = DEFAULT_DRIVER_CLASSNAME;        userName = DEFAULT_USERNAME;        password = DEFAULT_PASSWORD;        dbTypeName = DEFAULT_DBTYPE;        String homePath = System.getProperty("user.home").replace('\\', '/');        String defaultDbDir = homePath + "/.flume/jdbc-channel";        File dbDir = new File(defaultDbDir);        String canonicalDbDirPath = null;        try {            canonicalDbDirPath = dbDir.getCanonicalPath();        } catch (IOException ex) {            throw new JdbcChannelException("Unable to find canonical path of dir: " + defaultDbDir, ex);        }        if (!dbDir.exists()) {            if (!dbDir.mkdirs()) {                throw new JdbcChannelException("unable to create directory: " + canonicalDbDirPath);            }        }        connectUrl = "jdbc:derby:" + canonicalDbDirPath + "/db;create=true";                jdbcPropertiesFile = null;            }        databaseType = DatabaseType.getByName(dbTypeName);    switch(databaseType) {        case DERBY:        case MYSQL:            break;        default:            throw new JdbcChannelException("Database " + databaseType + " not supported at this time");    }        if (driverClassName == null || driverClassName.trim().length() == 0) {        throw new JdbcChannelException("No jdbc driver specified");    }    try {        Class.forName(driverClassName);    } catch (ClassNotFoundException ex) {        throw new JdbcChannelException("Unable to load driver: " + driverClassName, ex);    }        Properties jdbcProps = new Properties();    if (jdbcPropertiesFile != null && jdbcPropertiesFile.trim().length() > 0) {        File jdbcPropsFile = new File(jdbcPropertiesFile.trim());        if (!jdbcPropsFile.exists()) {            throw new JdbcChannelException("Jdbc properties file does not exist: " + jdbcPropertiesFile);        }        InputStream inStream = null;        try {            inStream = new FileInputStream(jdbcPropsFile);            jdbcProps.load(inStream);        } catch (IOException ex) {            throw new JdbcChannelException("Unable to load jdbc properties " + "from file: " + jdbcPropertiesFile, ex);        } finally {            if (inStream != null) {                try {                    inStream.close();                } catch (IOException ex) {                                    }            }        }    }    if (userName != null) {        Object oldUser = jdbcProps.put("user", userName);        if (oldUser != null) {                    }    }    if (password != null) {        Object oldPass = jdbcProps.put("password", password);        if (oldPass != null) {                    }    }    if (LOGGER.isDebugEnabled()) {        StringBuilder sb = new StringBuilder("JDBC Properties {");        boolean first = true;        Enumeration<?> propertyKeys = jdbcProps.propertyNames();        while (propertyKeys.hasMoreElements()) {            if (first) {                first = false;            } else {                sb.append(", ");            }            String key = (String) propertyKeys.nextElement();            sb.append(key).append("=");            if (key.equalsIgnoreCase("password")) {                sb.append("*******");            } else {                sb.append(jdbcProps.get(key));            }        }        sb.append("}");            }        String txIsolation = getConfigurationString(context, ConfigurationConstants.CONFIG_TX_ISOLATION_LEVEL, ConfigurationConstants.OLD_CONFIG_TX_ISOLATION_LEVEL, TransactionIsolation.READ_COMMITTED.getName());    TransactionIsolation txIsolationLevel = TransactionIsolation.getByName(txIsolation);            ConnectionFactory connFactory = new DriverManagerConnectionFactory(connectUrl, jdbcProps);    connectionPool = new GenericObjectPool();    String maxActiveConnections = getConfigurationString(context, ConfigurationConstants.CONFIG_MAX_CONNECTIONS, ConfigurationConstants.OLD_CONFIG_MAX_CONNECTIONS, "10");    int maxActive = 10;    if (maxActiveConnections != null && maxActiveConnections.length() > 0) {        try {            maxActive = Integer.parseInt(maxActiveConnections);        } catch (NumberFormatException nfe) {                    }    }        connectionPool.setMaxActive(maxActive);    statementPool = new GenericKeyedObjectPoolFactory(null);        new PoolableConnectionFactory(connFactory, connectionPool, statementPool, databaseType.getValidationQuery(), false, false, txIsolationLevel.getCode());    dataSource = new PoolingDataSource(connectionPool);    txFactory = new JdbcTransactionFactory(dataSource, this);}
1
protected void updateCurrentChannelSize(long delta)
{    long currentSizeLong = currentSize.addAndGet(delta);    }
1
private String getConfigurationString(Context context, String key, String oldKey, String defaultValue)
{    String oldValue = context.getString(oldKey);    if (oldValue != null && oldValue.length() > 0) {            }    String value = context.getString(key);    if (value == null) {        if (oldValue != null) {            value = oldValue;        } else {            value = defaultValue;        }    }    return value;}
1
protected JdbcTransactionImpl initialValue()
{    return new JdbcTransactionImpl(dataSource, this, providerImpl);}
0
public void begin()
{    if (!active) {        throw new JdbcChannelException("Inactive transaction");    }    if (count == 0) {                try {            connection = dataSource.getConnection();        } catch (SQLException ex) {            throw new JdbcChannelException("Unable to lease connection", ex);        }                try {            connection.clearWarnings();        } catch (SQLException ex) {                    }    }    count++;    LOGGER.trace("Tx count-begin: " + count + ", rollback: " + rollback);}
1
public void commit()
{    if (!active) {        throw new JdbcChannelException("Inactive transaction");    }    if (rollback) {        throw new JdbcChannelException("Cannot commit transaction marked for rollback");    }    LOGGER.trace("Tx count-commit: " + count + ", rollback: " + rollback);}
0
public void rollback()
{    if (!active) {        throw new JdbcChannelException("Inactive transaction");    }        rollback = true;    LOGGER.trace("Tx count-rollback: " + count + ", rollback: " + rollback);}
1
public void close()
{    if (!active) {        throw new JdbcChannelException("Inactive transaction");    }    count--;        if (count == 0) {        active = false;        try {            if (rollback) {                                connection.rollback();            } else {                                connection.commit();                                providerImpl.updateCurrentChannelSize(this.persistedEventCount - this.removedEventCount);                this.persistedEventCount = 0;                this.removedEventCount = 0;            }        } catch (SQLException ex) {            throw new JdbcChannelException("Unable to finalize transaction", ex);        } finally {            if (connection != null) {                                try {                    SQLWarning warning = connection.getWarnings();                    if (warning != null) {                        StringBuilder sb = new StringBuilder("Connection warnigns: ");                        boolean first = true;                        while (warning != null) {                            if (first) {                                first = false;                            } else {                                sb.append("; ");                            }                            sb.append("[").append(warning.getErrorCode()).append("] ");                            sb.append(warning.getMessage());                        }                                            }                } catch (SQLException ex) {                                    }                                try {                    connection.close();                } catch (SQLException ex) {                                    }            }                        txFactory.remove();                        connection = null;            txFactory = null;        }    }}
1
protected Connection getConnection()
{    if (!active) {        throw new JdbcChannelException("Inactive transaction");    }    return connection;}
0
protected void incrementRemovedEventCount()
{    removedEventCount++;}
0
protected void incrementPersistedEventCount()
{    persistedEventCount++;}
0
public boolean schemaExists()
{        return false;}
0
public void validateSchema()
{}
0
public void storeEvent(PersistableEvent pe, Connection connection)
{}
0
public PersistableEvent fetchAndDeleteEvent(String channel, Connection connection)
{        return null;}
0
public long getChannelSize(Connection connection)
{        return 0;}
0
public void createSchemaObjects(boolean createForeignKeys, boolean createIndex)
{}
0
public String getChannelName()
{    return channel;}
0
public byte[] getBasePayload()
{    return this.basePayload;}
0
public byte[] getSpillPayload()
{    return this.spillPayload;}
0
protected void setEventId(long eventId)
{    this.eventId = eventId;}
0
protected long getEventId()
{    return this.eventId;}
0
public List<HeaderEntry> getHeaderEntries()
{    return headers;}
0
public String getNameString()
{    return name.getString();}
0
public SpillableString getName()
{    return name;}
0
public String getValueString()
{    return value.getString();}
0
public SpillableString getValue()
{    return value;}
0
protected void setId(long headerId)
{    this.headerId = headerId;}
0
public long getId()
{    return headerId;}
0
public String getBase()
{    return base;}
0
public String getSpill()
{    return spill;}
0
public String getString()
{    if (spill == null) {        return base;    }    return base + spill;}
0
public boolean hasSpill()
{    return spill != null;}
0
public void setHeaders(Map<String, String> headers)
{    throw new UnsupportedOperationException("Cannot update headers of " + "persistable event");}
0
public byte[] getBody()
{    byte[] result = null;    if (spillPayload == null) {        result = Arrays.copyOf(basePayload, basePayload.length);    } else {        result = new byte[basePayload.length + spillPayload.length];        System.arraycopy(basePayload, 0, result, 0, basePayload.length);        System.arraycopy(spillPayload, 0, result, basePayload.length, spillPayload.length);    }    return result;}
0
public void setBody(byte[] body)
{    throw new UnsupportedOperationException("Cannot update payload of " + "persistable event");}
0
public Map<String, String> getHeaders()
{    Map<String, String> headerMap = null;    if (headers != null) {        headerMap = new HashMap<String, String>();        for (HeaderEntry entry : headers) {            headerMap.put(entry.getNameString(), entry.getValueString());        }    }    return headerMap;}
0
public Builder setEventId(long eventId)
{    bEventId = eventId;    return this;}
0
public Builder setChannel(String channel)
{    bChannelName = channel;    return this;}
0
public Builder setBasePayload(byte[] basePayload)
{    bBasePayload = basePayload;    return this;}
0
public Builder setSpillPayload(byte[] spillPayload)
{    bSpillPayload = spillPayload;    return this;}
0
public Builder setHeader(long headerId, String baseName, String baseValue)
{    if (bHeaderParts == null) {        bHeaderParts = new HashMap<Long, HeaderPart>();    }    HeaderPart hp = new HeaderPart(baseName, baseValue);    if (bHeaderParts.put(headerId, hp) != null) {        throw new JdbcChannelException("Duplicate header found: " + "headerId: " + headerId + ", baseName: " + baseName + ", " + "baseValue: " + baseValue);    }    return this;}
0
public Builder setHeaderNameSpill(long headerId, String nameSpill)
{    HeaderPart hp = bHeaderParts.get(headerId);    if (hp == null) {        throw new JdbcChannelException("Header not found for spill: " + headerId);    }    hp.setSpillName(nameSpill);    return this;}
0
public Builder setHeaderValueSpill(long headerId, String valueSpill)
{    HeaderPart hp = bHeaderParts.get(headerId);    if (hp == null) {        throw new JdbcChannelException("Header not found for spill: " + headerId);    }    hp.setSpillValue(valueSpill);    return this;}
0
public PersistableEvent build()
{    List<HeaderEntry> bHeaders = new ArrayList<HeaderEntry>();    if (bHeaderParts != null) {        for (long headerId : bHeaderParts.keySet()) {            HeaderPart part = bHeaderParts.get(headerId);            bHeaders.add(part.getEntry(headerId));        }    }    PersistableEvent pe = new PersistableEvent(bEventId, bChannelName, bBasePayload, bSpillPayload, bHeaders);    bEventId = 0L;    bChannelName = null;    bBasePayload = null;    bSpillPayload = null;    bHeaderParts = null;    return pe;}
0
 String getBaseName()
{    return hBaseName;}
0
 String getBaseValue()
{    return hBaseValue;}
0
 String getSpillName()
{    return hSpillName;}
0
 String getSpillValue()
{    return hSpillValue;}
0
 void setSpillName(String spillName)
{    hSpillName = spillName;}
0
 void setSpillValue(String spillValue)
{    hSpillValue = spillValue;}
0
 HeaderEntry getEntry(long headerId)
{    return new HeaderEntry(headerId, hBaseName, hSpillName, hBaseValue, hSpillValue);}
0
public static SchemaHandler getHandler(DatabaseType dbType, DataSource dataSource)
{    SchemaHandler handler = null;    switch(dbType) {        case DERBY:            handler = new DerbySchemaHandler(dataSource);            break;        case MYSQL:            handler = new MySQLSchemaHandler(dataSource);            break;        default:            throw new JdbcChannelException("Database " + dbType + " not supported yet");    }    return handler;}
0
public void put(Event event) throws ChannelException
{    getProvider().persistEvent(getName(), event);}
0
public Event take() throws ChannelException
{    return getProvider().removeEvent(getName());}
0
public Transaction getTransaction()
{    return getProvider().getTransaction();}
0
public void stop()
{    JdbcChannelProviderFactory.releaseProvider(getName());    provider = null;    super.stop();}
0
private JdbcChannelProvider getProvider()
{    return provider;}
0
public void configure(Context context)
{    provider = JdbcChannelProviderFactory.getProvider(context, getName());    }
1
public static synchronized JdbcChannelProvider getProvider(Context context, String name)
{    if (PROVIDER == null) {        PROVIDER = new JdbcChannelProviderImpl();        PROVIDER.initialize(context);    }    if (!INSTANCES.add(name)) {        throw new JdbcChannelException("Attempt to initialize multiple " + "channels with same name: " + name);    }    return PROVIDER;}
0
public static synchronized void releaseProvider(String name)
{    if (!INSTANCES.remove(name)) {        throw new JdbcChannelException("Attempt to release non-existant channel: " + name);    }    if (INSTANCES.size() == 0) {                PROVIDER.close();        PROVIDER = null;    }}
0
public int getCode()
{    return code;}
0
public String getName()
{    return name;}
0
public String toString()
{    return getName();}
0
public static TransactionIsolation getByName(String name)
{    return valueOf(name.trim().toUpperCase(Locale.ENGLISH));}
0
public void setUp() throws IOException
{    derbyCtx.clear();    derbyCtx.put(ConfigurationConstants.CONFIG_CREATE_SCHEMA, "true");    derbyCtx.put(ConfigurationConstants.CONFIG_DATABASE_TYPE, "DERBY");    derbyCtx.put(ConfigurationConstants.CONFIG_JDBC_DRIVER_CLASS, "org.apache.derby.jdbc.EmbeddedDriver");    derbyCtx.put(ConfigurationConstants.CONFIG_PASSWORD, "");    derbyCtx.put(ConfigurationConstants.CONFIG_USERNAME, "sa");    File tmpDir = new File("target/test");    tmpDir.mkdirs();    File derbyLogFile = new File(tmpDir, "derbytest.log");    String derbyLogFilePath = derbyLogFile.getCanonicalPath();    derbyCtx.put(ConfigurationConstants.CONFIG_JDBC_SYSPROP_PREFIX + "derby.stream.error.file", derbyLogFilePath);        File tempFile = File.createTempFile("temp", "_db", tmpDir);    String absFileName = tempFile.getCanonicalPath();    tempFile.delete();    derbyDbDir = new File(absFileName + "_dir");    if (!derbyDbDir.exists()) {        derbyDbDir.mkdirs();    }    derbyCtx.put(ConfigurationConstants.CONFIG_URL, "jdbc:derby:memory:" + derbyDbDir.getCanonicalPath() + "/db;create=true");    configureChannel(derbyCtx);    }
1
public void testDerbyChannelCapacity()
{    provider = new JdbcChannelProviderImpl();    derbyCtx.put(ConfigurationConstants.CONFIG_MAX_CAPACITY, "10");    provider.initialize(derbyCtx);    Set<MockEvent> events = new HashSet<MockEvent>();    for (int i = 1; i < 12; i++) {        events.add(MockEventUtils.generateMockEvent(i, i, i, 61 % i, 1));    }    Iterator<MockEvent> meIt = events.iterator();    int count = 0;    while (meIt.hasNext()) {        count++;        MockEvent me = meIt.next();        String chName = me.getChannel();        try {            provider.persistEvent(chName, me);            if (count == 11) {                Assert.fail();            }        } catch (JdbcChannelException ex) {                        Assert.assertEquals(11, count);        }                Event e = provider.removeEvent(chName);        Assert.assertNotNull(e);                provider.persistEvent(chName, me);    }}
0
public void testDerbySetup()
{    provider = new JdbcChannelProviderImpl();    provider.initialize(derbyCtx);    Transaction tx1 = provider.getTransaction();    tx1.begin();    Transaction tx2 = provider.getTransaction();    Assert.assertSame(tx1, tx2);    tx2.begin();    tx2.close();    tx1.close();    Transaction tx3 = provider.getTransaction();    Assert.assertNotSame(tx1, tx3);    tx3.begin();    tx3.close();    provider.close();    provider = null;}
0
public void testEventWithSimulatedSourceAndSinks() throws Exception
{    provider = new JdbcChannelProviderImpl();    provider.initialize(derbyCtx);    Map<String, List<MockEvent>> eventMap = new HashMap<String, List<MockEvent>>();    for (int i = 1; i < 121; i++) {        MockEvent me = MockEventUtils.generateMockEvent(i, i, i, 61 % i, 10);        List<MockEvent> meList = eventMap.get(me.getChannel());        if (meList == null) {            meList = new ArrayList<MockEvent>();            eventMap.put(me.getChannel(), meList);        }        meList.add(me);    }    List<MockSource> sourceList = new ArrayList<MockSource>();    List<MockSink> sinkList = new ArrayList<MockSink>();    for (String channel : eventMap.keySet()) {        List<MockEvent> meList = eventMap.get(channel);        sourceList.add(new MockSource(channel, meList, provider));        sinkList.add(new MockSink(channel, meList, provider));    }    ExecutorService sourceExecutor = Executors.newFixedThreadPool(10);    ExecutorService sinkExecutor = Executors.newFixedThreadPool(10);    List<Future<Integer>> srcResults = sourceExecutor.invokeAll(sourceList, 300, TimeUnit.SECONDS);    Thread.sleep(MockEventUtils.generateSleepInterval(3000));    List<Future<Integer>> sinkResults = sinkExecutor.invokeAll(sinkList, 300, TimeUnit.SECONDS);    int srcCount = 0;    for (Future<Integer> srcOutput : srcResults) {        srcCount += srcOutput.get();    }    Assert.assertEquals(120, srcCount);    int sinkCount = 0;    for (Future<Integer> sinkOutput : sinkResults) {        sinkCount += sinkOutput.get();    }    Assert.assertEquals(120, sinkCount);}
0
public void testPeristingEvents()
{    provider = new JdbcChannelProviderImpl();    provider.initialize(derbyCtx);    Map<String, List<MockEvent>> eventMap = new HashMap<String, List<MockEvent>>();    Set<MockEvent> events = new HashSet<MockEvent>();    for (int i = 1; i < 81; i++) {        events.add(MockEventUtils.generateMockEvent(i, i, i, 61 % i, 5));    }    Iterator<MockEvent> meIt = events.iterator();    while (meIt.hasNext()) {        MockEvent me = meIt.next();        String chName = me.getChannel();        List<MockEvent> eventList = eventMap.get(chName);        if (eventList == null) {            eventList = new ArrayList<MockEvent>();            eventMap.put(chName, eventList);        }        eventList.add(me);        provider.persistEvent(me.getChannel(), me);    }    for (String chName : eventMap.keySet()) {        List<MockEvent> meList = eventMap.get(chName);        Iterator<MockEvent> it = meList.iterator();        while (it.hasNext()) {            MockEvent me = it.next();            Event event = provider.removeEvent(chName);            assertEquals(me, event);        }                Event nullEvent = provider.removeEvent(chName);        Assert.assertNull(nullEvent);    }    provider.close();    provider = null;}
0
private static void assertEquals(Event e1, Event e2)
{    byte[] pl1 = e1.getBody();    byte[] pl2 = e2.getBody();    Assert.assertArrayEquals(pl1, pl2);    Map<String, String> h1 = e1.getHeaders();    Map<String, String> h2 = e2.getHeaders();    if (h1 == null || h1.size() == 0) {        Assert.assertTrue(h2 == null || h2.size() == 0);    } else {        Assert.assertTrue(h1.size() == h2.size());        for (String key : h1.keySet()) {            Assert.assertTrue(h2.containsKey(key));            String v1 = h1.get(key);            String v2 = h2.remove(key);            Assert.assertEquals(v1, v2);        }        Assert.assertTrue(h2.size() == 0);    }}
0
public void tearDown() throws IOException
{    if (provider != null) {        try {            provider.close();        } catch (Exception ex) {                    }    }    provider = null;}
1
public Integer call() throws Exception
{        if (events == null) {        return 0;    }    Iterator<MockEvent> it = events.iterator();    while (it.hasNext()) {        MockEvent me = it.next();        Event event = null;        while (event == null) {            event = provider.removeEvent(channel);            if (event == null) {                                try {                    Thread.sleep(MockEventUtils.generateSleepInterval(1000));                } catch (InterruptedException ex) {                    Thread.currentThread().interrupt();                }            } else {                            }        }        BaseJdbcChannelProviderTest.assertEquals(me, event);    }        return events.size();}
1
public Integer call() throws Exception
{        if (events == null) {        return 0;    }    Iterator<MockEvent> it = events.iterator();    while (it.hasNext()) {        MockEvent me = it.next();        Assert.assertEquals(channel, me.getChannel());        provider.persistEvent(channel, me);        try {            Thread.sleep(MockEventUtils.generateSleepInterval(1000));        } catch (InterruptedException ex) {            Thread.currentThread().interrupt();        }    }        return events.size();}
1
public Map<String, String> getHeaders()
{    return headers;}
0
public void setHeaders(Map<String, String> headers)
{}
0
public byte[] getBody()
{    return payload;}
0
public void setBody(byte[] body)
{}
0
public String getChannel()
{    return channel;}
0
public static byte[] generatePayload(int size)
{    byte[] result = new byte[size];    RANDOM.nextBytes(result);    return result;}
0
public static String generateHeaderString(int size)
{    StringBuilder sb = new StringBuilder();    for (int i = 0; i < size; i++) {        int x = Math.abs(RANDOM.nextInt());        int y = x % CHARS.length;        sb.append(CHARS[y]);    }    return sb.toString();}
0
public static MockEvent generateMockEvent(int payloadMargin, int headerNameMargin, int headerValueMargin, int numHeaders, int numChannels)
{    int chIndex = 0;    if (numChannels > 1) {        chIndex = Math.abs(RANDOM.nextInt()) % numChannels;    }    String channel = "test-" + chIndex;    StringBuilder sb = new StringBuilder("New Event[payload size:");    int plTh = ConfigurationConstants.PAYLOAD_LENGTH_THRESHOLD;    int plSize = Math.abs(RANDOM.nextInt()) % plTh + payloadMargin;    sb.append(plSize).append(", numHeaders:").append(numHeaders);    sb.append(", channel:").append(channel);    byte[] payload = generatePayload(plSize);    int nmTh = ConfigurationConstants.HEADER_NAME_LENGTH_THRESHOLD;    int vlTh = ConfigurationConstants.HEADER_VALUE_LENGTH_THRESHOLD;    Map<String, String> headers = new HashMap<String, String>();    for (int i = 0; i < numHeaders; i++) {        int nmSize = Math.abs(RANDOM.nextInt()) % nmTh + headerNameMargin;        int vlSize = Math.abs(RANDOM.nextInt()) % vlTh + headerValueMargin;        String name = generateHeaderString(nmSize);        String value = generateHeaderString(vlSize);        headers.put(name, value);        sb.append("{nm:").append(nmSize).append(",vl:").append(vlSize);        sb.append("} ");    }        return new MockEvent(payload, headers, channel);}
1
public static int generateSleepInterval(int upperBound)
{    return Math.abs(RANDOM.nextInt(upperBound));}
0
public void setUp()
{    enumMap.clear();    enumMap.put(DBTYPE_OTHER, DatabaseType.OTHER);    enumMap.put(DBTYPE_DERBY, DatabaseType.DERBY);    enumMap.put(DBTYPE_MYSQL, DatabaseType.MYSQL);    enumMap.put(DBTYPE_PGSQL, DatabaseType.POSTGRESQL);    enumMap.put(DBTYPE_ORACLE, DatabaseType.ORACLE);}
0
public void testDatabaseTypeLookup()
{    for (String key : enumMap.keySet()) {        DatabaseType type = enumMap.get(key);        DatabaseType lookupType = DatabaseType.valueOf(key);        String lookupTypeName = lookupType.getName();        Assert.assertEquals(lookupTypeName, lookupType.toString());        Assert.assertSame(type, lookupType);        Assert.assertEquals(key, lookupTypeName);        DatabaseType lookupType2 = DatabaseType.getByName(key.toLowerCase(Locale.ENGLISH));        Assert.assertSame(type, lookupType2);    }}
0
public void testUnknonwnDatabaseTypeLookup()
{    String[] invalidTypes = new String[] { "foo", "bar", "abcd" };    for (String key : invalidTypes) {        DatabaseType type = DatabaseType.getByName(key);        Assert.assertSame(type, DatabaseType.OTHER);    }}
0
public void testCreateQueries()
{    Assert.assertEquals(DerbySchemaHandler.QUERY_CREATE_SCHEMA_FLUME, EXPECTED_QUERY_CREATE_SCHEMA_FLUME);    Assert.assertEquals(DerbySchemaHandler.QUERY_CREATE_TABLE_FL_EVENT, EXPECTED_QUERY_CREATE_TABLE_FL_EVENT);    Assert.assertEquals(DerbySchemaHandler.QUERY_CREATE_INDEX_FLE_CHANNEL, EXPECTED_QUERY_CREATE_INDEX_FLE_CHANNEL);    Assert.assertEquals(DerbySchemaHandler.QUERY_CREATE_TABLE_FL_PLSPILL_FK, EXPECTED_QUERY_CREATE_TABLE_FL_PLSPILL_FK);    Assert.assertEquals(DerbySchemaHandler.QUERY_CREATE_TABLE_FL_PLSPILL_NOFK, EXPECTED_QUERY_CREATE_TABLE_FL_PLSPILL_NOFK);    Assert.assertEquals(DerbySchemaHandler.QUERY_CREATE_INDEX_FLP_EVENT, EXPECTED_QUERY_CREATE_INDEX_FLP_EVENT);    Assert.assertEquals(DerbySchemaHandler.QUERY_CREATE_TABLE_FL_HEADER_FK, EXPECTED_QUERY_CREATE_TABLE_FL_HEADER_FK);    Assert.assertEquals(DerbySchemaHandler.QUERY_CREATE_TABLE_FL_HEADER_NOFK, EXPECTED_QUERY_CREATE_TABLE_FL_HEADER_NOFK);    Assert.assertEquals(DerbySchemaHandler.QUERY_CREATE_INDEX_FLH_EVENT, EXPECTED_QUERY_CREATE_INDEX_FLH_EVENT);    Assert.assertEquals(DerbySchemaHandler.QUERY_CREATE_TABLE_FL_NMSPILL_FK, EXPECTED_QUERY_CREATE_TABLE_FL_NMSPILL_FK);    Assert.assertEquals(DerbySchemaHandler.QUERY_CREATE_TABLE_FL_NMSPILL_NOFK, EXPECTED_QUERY_CREATE_TABLE_FL_NMSPILL_NOFK);    Assert.assertEquals(DerbySchemaHandler.QUERY_CREATE_INDEX_FLN_HEADER, EXPECTED_QUERY_CREATE_INDEX_FLN_HEADER);    Assert.assertEquals(DerbySchemaHandler.QUERY_CREATE_TABLE_FL_VLSPILL_FK, EXPECTED_QUERY_CREATE_TABLE_FL_VLSPILL_FK);    Assert.assertEquals(DerbySchemaHandler.QUERY_CREATE_TABLE_FL_VLSPILL_NOFK, EXPECTED_QUERY_CREATE_TABLE_FL_VLSPILL_NOFK);    Assert.assertEquals(DerbySchemaHandler.QUERY_CREATE_INDEX_FLV_HEADER, EXPECTED_QUERY_CREATE_INDEX_FLV_HEADER);    Assert.assertEquals(DerbySchemaHandler.COLUMN_LOOKUP_QUERY, EXPECTED_COLUMN_LOOKUP_QUERY);    Assert.assertEquals(DerbySchemaHandler.QUERY_CHANNEL_SIZE, EXPECTED_QUERY_CHANNEL_SIZE);    Assert.assertEquals(DerbySchemaHandler.STMT_INSERT_EVENT_BASE, EXPECTED_STMT_INSERT_EVENT_BASE);    Assert.assertEquals(DerbySchemaHandler.STMT_INSERT_EVENT_SPILL, EXPECTED_STMT_INSERT_EVENT_SPILL);    Assert.assertEquals(DerbySchemaHandler.STMT_INSERT_HEADER_BASE, EXPECTED_STMT_INSERT_HEADER_BASE);    Assert.assertEquals(DerbySchemaHandler.STMT_INSERT_HEADER_NAME_SPILL, EXPECTED_STMT_INSERT_HEADER_NAME_SPILL);    Assert.assertEquals(DerbySchemaHandler.STMT_INSERT_HEADER_VALUE_SPILL, EXPECTED_STMT_INSERT_HEADER_VALUE_SPILL);    Assert.assertEquals(DerbySchemaHandler.STMT_FETCH_PAYLOAD_BASE, EXPECTED_STMT_FETCH_PAYLOAD_BASE);    Assert.assertEquals(DerbySchemaHandler.STMT_FETCH_PAYLOAD_SPILL, EXPECTED_STMT_FETCH_PAYLOAD_SPILL);    Assert.assertEquals(DerbySchemaHandler.STMT_FETCH_HEADER_BASE, EXPECTED_STMT_FETCH_HEADER_BASE);    Assert.assertEquals(DerbySchemaHandler.STMT_FETCH_HEADER_NAME_SPILL, EXPECTED_STMT_FETCH_HEADER_NAME_SPILL);    Assert.assertEquals(DerbySchemaHandler.STMT_FETCH_HEADER_VALUE_SPILL, EXPECTED_STMT_FETCH_HEADER_VALUE_SPILL);    Assert.assertEquals(DerbySchemaHandler.STMT_DELETE_HEADER_VALUE_SPILL, EXPECTED_STMT_DELETE_HEADER_VALUE_SPILL);    Assert.assertEquals(DerbySchemaHandler.STMT_DELETE_HEADER_NAME_SPILL, EXPECTED_STMT_DELETE_HEADER_NAME_SPILL);    Assert.assertEquals(DerbySchemaHandler.STMT_DELETE_EVENT_SPILL, EXPECTED_STMT_DELETE_EVENT_SPILL);    Assert.assertEquals(DerbySchemaHandler.STMT_DELETE_HEADER_BASE, EXPECTED_STMT_DELETE_HEADER_BASE);    Assert.assertEquals(DerbySchemaHandler.STMT_DELETE_EVENT_BASE, EXPECTED_STMT_DELETE_EVENT_BASE);}
0
protected void configureChannel(Context context)
{}
0
protected void configureChannel(Context context)
{    context.put(ConfigurationConstants.CONFIG_CREATE_FK, "false");}
0
public void testMarshalling()
{    int nameLimit = ConfigurationConstants.HEADER_NAME_LENGTH_THRESHOLD;    int valLimit = ConfigurationConstants.HEADER_VALUE_LENGTH_THRESHOLD;    byte[] s1 = MockEventUtils.generatePayload(1);    runTest(s1, null);    byte[] s2 = MockEventUtils.generatePayload(2);    runTest(s2, new HashMap<String, String>());    int th = ConfigurationConstants.PAYLOAD_LENGTH_THRESHOLD;    byte[] s3 = MockEventUtils.generatePayload(th - 2);    Map<String, String> m3 = new HashMap<String, String>();    m3.put(MockEventUtils.generateHeaderString(1), MockEventUtils.generateHeaderString(1));    runTest(s3, m3);    byte[] s4 = MockEventUtils.generatePayload(th - 1);    Map<String, String> m4 = new HashMap<String, String>();    m4.put(MockEventUtils.generateHeaderString(nameLimit - 21), "w");    m4.put(MockEventUtils.generateHeaderString(nameLimit - 2), "x");    m4.put(MockEventUtils.generateHeaderString(nameLimit - 1), "y");    m4.put(MockEventUtils.generateHeaderString(nameLimit), "z");    m4.put(MockEventUtils.generateHeaderString(nameLimit + 1), "a");    m4.put(MockEventUtils.generateHeaderString(nameLimit + 2), "b");    m4.put(MockEventUtils.generateHeaderString(nameLimit + 21), "c");    runTest(s4, m4);    byte[] s5 = MockEventUtils.generatePayload(th);    Map<String, String> m5 = new HashMap<String, String>();    m5.put("w", MockEventUtils.generateHeaderString(valLimit - 21));    m5.put("x", MockEventUtils.generateHeaderString(valLimit - 2));    m5.put("y", MockEventUtils.generateHeaderString(valLimit - 1));    m5.put("z", MockEventUtils.generateHeaderString(valLimit));    m5.put("a", MockEventUtils.generateHeaderString(valLimit + 1));    m5.put("b", MockEventUtils.generateHeaderString(valLimit + 2));    m5.put("c", MockEventUtils.generateHeaderString(valLimit + 21));    runTest(s5, m5);    byte[] s6 = MockEventUtils.generatePayload(th + 1);    Map<String, String> m6 = new HashMap<String, String>();    m6.put(MockEventUtils.generateHeaderString(nameLimit - 21), MockEventUtils.generateHeaderString(valLimit - 21));    m6.put(MockEventUtils.generateHeaderString(nameLimit - 2), MockEventUtils.generateHeaderString(valLimit - 2));    m6.put(MockEventUtils.generateHeaderString(nameLimit - 1), MockEventUtils.generateHeaderString(valLimit - 1));    m6.put(MockEventUtils.generateHeaderString(nameLimit), MockEventUtils.generateHeaderString(valLimit));    m6.put(MockEventUtils.generateHeaderString(nameLimit + 1), MockEventUtils.generateHeaderString(valLimit + 1));    m6.put(MockEventUtils.generateHeaderString(nameLimit + 2), MockEventUtils.generateHeaderString(valLimit + 2));    m6.put(MockEventUtils.generateHeaderString(nameLimit + 21), MockEventUtils.generateHeaderString(valLimit + 21));    runTest(s6, m6);    byte[] s7 = MockEventUtils.generatePayload(th + 2);    runTest(s7, null);    byte[] s8 = MockEventUtils.generatePayload(th + 27);    runTest(s8, null);}
0
private void runTest(byte[] payload, Map<String, String> headers)
{    PersistableEvent pe = new PersistableEvent("test", new MockEvent(payload, headers, null));    Assert.assertArrayEquals(payload, pe.getBody());    Map<String, String> h = pe.getHeaders();    if (h == null) {        Assert.assertTrue(headers == null || headers.size() == 0);    } else {        Assert.assertTrue(headers.size() == h.size());        for (String key : h.keySet()) {            Assert.assertTrue(headers.containsKey(key));            String value = h.get(key);            String expectedValue = headers.remove(key);            Assert.assertEquals(expectedValue, value);        }        Assert.assertTrue(headers.size() == 0);    }}
0
public void setUp()
{    enumMap.clear();    enumMap.put(TX_READ_UNCOMMITTED, TransactionIsolation.READ_UNCOMMITTED);    enumMap.put(TX_READ_COMMITTED, TransactionIsolation.READ_COMMITTED);    enumMap.put(TX_REPEATABLE_READ, TransactionIsolation.REPEATABLE_READ);    enumMap.put(TX_SERIALIZABLE, TransactionIsolation.SERIALIZABLE);}
0
public void testReverseLookup()
{    for (String key : enumMap.keySet()) {        TransactionIsolation txIsolation = enumMap.get(key);        TransactionIsolation lookupTxIsolation = TransactionIsolation.valueOf(key);        String lookupTxIsolationName = lookupTxIsolation.getName();        Assert.assertEquals(lookupTxIsolationName, lookupTxIsolation.toString());        Assert.assertSame(txIsolation, lookupTxIsolation);        Assert.assertEquals(key, lookupTxIsolationName);        TransactionIsolation lookupTxIsolation2 = TransactionIsolation.getByName(key.toLowerCase(Locale.ENGLISH));        Assert.assertSame(txIsolation, lookupTxIsolation2);    }}
0
public ConsumerAndRecords initialValue()
{    return createConsumerAndRecords();}
0
public void start()
{            if (migrateZookeeperOffsets && zookeeperConnect != null && !zookeeperConnect.isEmpty()) {        migrateOffsets();    }    producer = new KafkaProducer<String, byte[]>(producerProps);            counter.start();    super.start();}
1
public void stop()
{    for (ConsumerAndRecords c : consumers) {        try {            decommissionConsumerAndRecords(c);        } catch (Exception ex) {                    }    }    producer.close();    counter.stop();    super.stop();    }
1
protected BasicTransactionSemantics createTransaction()
{    return new KafkaTransaction();}
0
public void configure(Context ctx)
{        translateOldProps(ctx);    topicStr = ctx.getString(TOPIC_CONFIG);    if (topicStr == null || topicStr.isEmpty()) {        topicStr = DEFAULT_TOPIC;            }    topic.set(topicStr);    groupId = ctx.getString(KAFKA_CONSUMER_PREFIX + ConsumerConfig.GROUP_ID_CONFIG);    if (groupId == null || groupId.isEmpty()) {        groupId = DEFAULT_GROUP_ID;            }    String bootStrapServers = ctx.getString(BOOTSTRAP_SERVERS_CONFIG);    if (bootStrapServers == null || bootStrapServers.isEmpty()) {        throw new ConfigurationException("Bootstrap Servers must be specified");    }    setProducerProps(ctx, bootStrapServers);    setConsumerProps(ctx, bootStrapServers);    parseAsFlumeEvent = ctx.getBoolean(PARSE_AS_FLUME_EVENT, DEFAULT_PARSE_AS_FLUME_EVENT);    pollTimeout = ctx.getLong(POLL_TIMEOUT, DEFAULT_POLL_TIMEOUT);    staticPartitionId = ctx.getInteger(STATIC_PARTITION_CONF);    partitionHeader = ctx.getString(PARTITION_HEADER_NAME);    migrateZookeeperOffsets = ctx.getBoolean(MIGRATE_ZOOKEEPER_OFFSETS, DEFAULT_MIGRATE_ZOOKEEPER_OFFSETS);    zookeeperConnect = ctx.getString(ZOOKEEPER_CONNECT_FLUME_KEY);    if (logger.isDebugEnabled() && LogPrivacyUtil.allowLogPrintConfig()) {            }    if (counter == null) {        counter = new KafkaChannelCounter(getName());    }}
1
private void translateOldProps(Context ctx)
{    if (!(ctx.containsKey(TOPIC_CONFIG))) {        ctx.put(TOPIC_CONFIG, ctx.getString("topic"));            }        if (!(ctx.containsKey(BOOTSTRAP_SERVERS_CONFIG))) {        String brokerList = ctx.getString(BROKER_LIST_FLUME_KEY);        if (brokerList == null || brokerList.isEmpty()) {            throw new ConfigurationException("Bootstrap Servers must be specified");        } else {            ctx.put(BOOTSTRAP_SERVERS_CONFIG, brokerList);                    }    }        if (!(ctx.containsKey(KAFKA_CONSUMER_PREFIX + ConsumerConfig.GROUP_ID_CONFIG))) {        String oldGroupId = ctx.getString(GROUP_ID_FLUME);        if (oldGroupId != null && !oldGroupId.isEmpty()) {            ctx.put(KAFKA_CONSUMER_PREFIX + ConsumerConfig.GROUP_ID_CONFIG, oldGroupId);                    }    }    if (!(ctx.containsKey((KAFKA_CONSUMER_PREFIX + ConsumerConfig.AUTO_OFFSET_RESET_CONFIG)))) {        Boolean oldReadSmallest = ctx.getBoolean(READ_SMALLEST_OFFSET);        String auto;        if (oldReadSmallest != null) {            if (oldReadSmallest) {                auto = "earliest";            } else {                auto = "latest";            }            ctx.put(KAFKA_CONSUMER_PREFIX + ConsumerConfig.AUTO_OFFSET_RESET_CONFIG, auto);                    }    }}
1
private void setProducerProps(Context ctx, String bootStrapServers)
{    producerProps.clear();    producerProps.put(ProducerConfig.ACKS_CONFIG, DEFAULT_ACKS);    producerProps.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, DEFAULT_KEY_SERIALIZER);    producerProps.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, DEFAULT_VALUE_SERIAIZER);        producerProps.putAll(ctx.getSubProperties(KAFKA_PRODUCER_PREFIX));    producerProps.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, bootStrapServers);    KafkaSSLUtil.addGlobalSSLParameters(producerProps);}
0
protected Properties getProducerProps()
{    return producerProps;}
0
private void setConsumerProps(Context ctx, String bootStrapServers)
{    consumerProps.clear();    consumerProps.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, DEFAULT_KEY_DESERIALIZER);    consumerProps.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, DEFAULT_VALUE_DESERIAIZER);    consumerProps.put(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG, DEFAULT_AUTO_OFFSET_RESET);        consumerProps.putAll(ctx.getSubProperties(KAFKA_CONSUMER_PREFIX));        consumerProps.put(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, bootStrapServers);    consumerProps.put(ConsumerConfig.GROUP_ID_CONFIG, groupId);    consumerProps.put(ConsumerConfig.ENABLE_AUTO_COMMIT_CONFIG, false);    KafkaSSLUtil.addGlobalSSLParameters(consumerProps);}
0
protected Properties getConsumerProps()
{    return consumerProps;}
0
private synchronized ConsumerAndRecords createConsumerAndRecords()
{    try {        KafkaConsumer<String, byte[]> consumer = new KafkaConsumer<String, byte[]>(consumerProps);        ConsumerAndRecords car = new ConsumerAndRecords(consumer, channelUUID);                car.consumer.subscribe(Arrays.asList(topic.get()), new ChannelRebalanceListener(rebalanceFlag));        car.offsets = new HashMap<TopicPartition, OffsetAndMetadata>();        consumers.add(car);        return car;    } catch (Exception e) {        throw new FlumeException("Unable to connect to Kafka", e);    }}
1
private void migrateOffsets()
{    try (KafkaZkClient zkClient = KafkaZkClient.apply(zookeeperConnect, JaasUtils.isZkSecurityEnabled(), ZK_SESSION_TIMEOUT, ZK_CONNECTION_TIMEOUT, 10, Time.SYSTEM, "kafka.server", "SessionExpireListener");        KafkaConsumer<String, byte[]> consumer = new KafkaConsumer<>(consumerProps)) {        Map<TopicPartition, OffsetAndMetadata> kafkaOffsets = getKafkaOffsets(consumer);        if (kafkaOffsets == null) {                        return;        }        if (!kafkaOffsets.isEmpty()) {                                    return;        }                Map<TopicPartition, OffsetAndMetadata> zookeeperOffsets = getZookeeperOffsets(zkClient, consumer);        if (zookeeperOffsets.isEmpty()) {                        return;        }                        consumer.commitSync(zookeeperOffsets);                Map<TopicPartition, OffsetAndMetadata> newKafkaOffsets = getKafkaOffsets(consumer);                if (newKafkaOffsets == null || !newKafkaOffsets.keySet().containsAll(zookeeperOffsets.keySet())) {            throw new FlumeException("Offsets could not be committed");        }    }}
1
private Map<TopicPartition, OffsetAndMetadata> getKafkaOffsets(KafkaConsumer<String, byte[]> client)
{    Map<TopicPartition, OffsetAndMetadata> offsets = null;    List<PartitionInfo> partitions = client.partitionsFor(topicStr);    if (partitions != null) {        offsets = new HashMap<>();        for (PartitionInfo partition : partitions) {            TopicPartition key = new TopicPartition(topicStr, partition.partition());            OffsetAndMetadata offsetAndMetadata = client.committed(key);            if (offsetAndMetadata != null) {                offsets.put(key, offsetAndMetadata);            }        }    }    return offsets;}
0
private Map<TopicPartition, OffsetAndMetadata> getZookeeperOffsets(KafkaZkClient zkClient, KafkaConsumer<String, byte[]> consumer)
{    Map<TopicPartition, OffsetAndMetadata> offsets = new HashMap<>();    List<PartitionInfo> partitions = consumer.partitionsFor(topicStr);    for (PartitionInfo partition : partitions) {        TopicPartition topicPartition = new TopicPartition(topicStr, partition.partition());        Option<Object> optionOffset = zkClient.getConsumerOffset(groupId, topicPartition);        if (optionOffset.nonEmpty()) {            Long offset = (Long) optionOffset.get();            OffsetAndMetadata offsetAndMetadata = new OffsetAndMetadata(offset);            offsets.put(topicPartition, offsetAndMetadata);        }    }    return offsets;}
0
private void decommissionConsumerAndRecords(ConsumerAndRecords c)
{    c.consumer.wakeup();    c.consumer.close();}
0
 void registerThread()
{    try {        consumerAndRecords.get();    } catch (Exception e) {                e.printStackTrace();    }}
1
protected void doBegin() throws InterruptedException
{    rebalanceFlag.set(false);}
0
protected void doPut(Event event) throws InterruptedException
{    type = TransactionType.PUT;    if (!producerRecords.isPresent()) {        producerRecords = Optional.of(new LinkedList<ProducerRecord<String, byte[]>>());    }    String key = event.getHeaders().get(KEY_HEADER);    Integer partitionId = null;    try {        if (staticPartitionId != null) {            partitionId = staticPartitionId;        }                if (partitionHeader != null) {            String headerVal = event.getHeaders().get(partitionHeader);            if (headerVal != null) {                partitionId = Integer.parseInt(headerVal);            }        }        if (partitionId != null) {            producerRecords.get().add(new ProducerRecord<String, byte[]>(topic.get(), partitionId, key, serializeValue(event, parseAsFlumeEvent)));        } else {            producerRecords.get().add(new ProducerRecord<String, byte[]>(topic.get(), key, serializeValue(event, parseAsFlumeEvent)));        }        counter.incrementEventPutAttemptCount();    } catch (NumberFormatException e) {        throw new ChannelException("Non integer partition id specified", e);    } catch (Exception e) {        throw new ChannelException("Error while serializing event", e);    }}
0
protected Event doTake() throws InterruptedException
{    logger.trace("Starting event take");    type = TransactionType.TAKE;    try {        if (!(consumerAndRecords.get().uuid.equals(channelUUID))) {                        decommissionConsumerAndRecords(consumerAndRecords.get());            consumerAndRecords.remove();        }    } catch (Exception ex) {            }    if (!events.isPresent()) {        events = Optional.of(new LinkedList<Event>());    }    Event e;        if (rebalanceFlag.get()) {                return null;    }    if (!consumerAndRecords.get().failedEvents.isEmpty()) {        e = consumerAndRecords.get().failedEvents.removeFirst();    } else {        if (logger.isTraceEnabled()) {            logger.trace("Assignment during take: {}", consumerAndRecords.get().consumer.assignment().toString());        }        try {            long startTime = System.nanoTime();            if (!consumerAndRecords.get().recordIterator.hasNext()) {                consumerAndRecords.get().poll();            }            if (consumerAndRecords.get().recordIterator.hasNext()) {                ConsumerRecord<String, byte[]> record = consumerAndRecords.get().recordIterator.next();                e = deserializeValue(record.value(), parseAsFlumeEvent);                TopicPartition tp = new TopicPartition(record.topic(), record.partition());                OffsetAndMetadata oam = new OffsetAndMetadata(record.offset() + 1, batchUUID);                consumerAndRecords.get().saveOffsets(tp, oam);                                if (record.key() != null) {                    e.getHeaders().put(KEY_HEADER, record.key());                }                long endTime = System.nanoTime();                counter.addToKafkaEventGetTimer((endTime - startTime) / (1000 * 1000));                if (logger.isDebugEnabled()) {                                    }            } else {                return null;            }            counter.incrementEventTakeAttemptCount();        } catch (Exception ex) {                        throw new ChannelException("Error while getting events from Kafka", ex);        }    }    eventTaken = true;    events.get().add(e);    return e;}
1
protected void doCommit() throws InterruptedException
{    logger.trace("Starting commit");    if (type.equals(TransactionType.NONE)) {        return;    }    if (type.equals(TransactionType.PUT)) {        if (!kafkaFutures.isPresent()) {            kafkaFutures = Optional.of(new LinkedList<Future<RecordMetadata>>());        }        try {            long batchSize = producerRecords.get().size();            long startTime = System.nanoTime();            int index = 0;            for (ProducerRecord<String, byte[]> record : producerRecords.get()) {                index++;                kafkaFutures.get().add(producer.send(record, new ChannelCallback(index, startTime)));            }                        producer.flush();            for (Future<RecordMetadata> future : kafkaFutures.get()) {                future.get();            }            long endTime = System.nanoTime();            counter.addToKafkaEventSendTimer((endTime - startTime) / (1000 * 1000));            counter.addToEventPutSuccessCount(batchSize);            producerRecords.get().clear();            kafkaFutures.get().clear();        } catch (Exception ex) {                        throw new ChannelException("Commit failed as send to Kafka failed", ex);        }    } else {                if (consumerAndRecords.get().failedEvents.isEmpty() && eventTaken) {            logger.trace("About to commit batch");            long startTime = System.nanoTime();            consumerAndRecords.get().commitOffsets();            long endTime = System.nanoTime();            counter.addToKafkaCommitTimer((endTime - startTime) / (1000 * 1000));            if (logger.isDebugEnabled()) {                            }        }        int takes = events.get().size();        if (takes > 0) {            counter.addToEventTakeSuccessCount(takes);            events.get().clear();        }    }}
1
protected void doRollback() throws InterruptedException
{    if (type.equals(TransactionType.NONE)) {        return;    }    if (type.equals(TransactionType.PUT)) {        producerRecords.get().clear();        kafkaFutures.get().clear();    } else {        counter.addToRollbackCounter(events.get().size());        consumerAndRecords.get().failedEvents.addAll(events.get());        events.get().clear();    }}
0
private byte[] serializeValue(Event event, boolean parseAsFlumeEvent) throws IOException
{    byte[] bytes;    if (parseAsFlumeEvent) {        if (!tempOutStream.isPresent()) {            tempOutStream = Optional.of(new ByteArrayOutputStream());        }        if (!writer.isPresent()) {            writer = Optional.of(new SpecificDatumWriter<AvroFlumeEvent>(AvroFlumeEvent.class));        }        tempOutStream.get().reset();        AvroFlumeEvent e = new AvroFlumeEvent(toCharSeqMap(event.getHeaders()), ByteBuffer.wrap(event.getBody()));        encoder = EncoderFactory.get().directBinaryEncoder(tempOutStream.get(), encoder);        writer.get().write(e, encoder);        encoder.flush();        bytes = tempOutStream.get().toByteArray();    } else {        bytes = event.getBody();    }    return bytes;}
0
private Event deserializeValue(byte[] value, boolean parseAsFlumeEvent) throws IOException
{    Event e;    if (parseAsFlumeEvent) {        ByteArrayInputStream in = new ByteArrayInputStream(value);        decoder = DecoderFactory.get().directBinaryDecoder(in, decoder);        if (!reader.isPresent()) {            reader = Optional.of(new SpecificDatumReader<AvroFlumeEvent>(AvroFlumeEvent.class));        }        AvroFlumeEvent event = reader.get().read(null, decoder);        e = EventBuilder.withBody(event.getBody().array(), toStringMap(event.getHeaders()));    } else {        e = EventBuilder.withBody(value, Collections.EMPTY_MAP);    }    return e;}
0
private static Map<CharSequence, CharSequence> toCharSeqMap(Map<String, String> stringMap)
{    Map<CharSequence, CharSequence> charSeqMap = new HashMap<CharSequence, CharSequence>();    for (Map.Entry<String, String> entry : stringMap.entrySet()) {        charSeqMap.put(entry.getKey(), entry.getValue());    }    return charSeqMap;}
0
private static Map<String, String> toStringMap(Map<CharSequence, CharSequence> charSeqMap)
{    Map<String, String> stringMap = new HashMap<String, String>();    for (Map.Entry<CharSequence, CharSequence> entry : charSeqMap.entrySet()) {        stringMap.put(entry.getKey().toString(), entry.getValue().toString());    }    return stringMap;}
0
private void poll()
{    logger.trace("Polling with timeout: {}ms channel-{}", pollTimeout, getName());    try {        records = consumer.poll(Duration.ofMillis(pollTimeout));        recordIterator = records.iterator();            } catch (WakeupException e) {        logger.trace("Consumer woken up for channel {}.", getName());    }}
1
private void commitOffsets()
{    try {        consumer.commitSync(offsets);    } catch (Exception e) {            } finally {        logger.trace("About to clear offsets map.");        offsets.clear();    }}
1
private String getOffsetMapString()
{    StringBuilder sb = new StringBuilder();    sb.append(getName()).append(" current offsets map: ");    for (TopicPartition tp : offsets.keySet()) {        sb.append("p").append(tp.partition()).append("-").append(offsets.get(tp).offset()).append(" ");    }    return sb.toString();}
0
private String getCommittedOffsetsString()
{    StringBuilder sb = new StringBuilder();    sb.append(getName()).append(" committed: ");    for (TopicPartition tp : consumer.assignment()) {        try {            sb.append("[").append(tp).append(",").append(consumer.committed(tp).offset()).append("] ");        } catch (NullPointerException npe) {                    }    }    return sb.toString();}
1
private void saveOffsets(TopicPartition tp, OffsetAndMetadata oam)
{    offsets.put(tp, oam);    if (logger.isTraceEnabled()) {        logger.trace(getOffsetMapString());    }}
0
public void onCompletion(RecordMetadata metadata, Exception exception)
{    if (exception != null) {        log.trace("Error sending message to Kafka due to " + exception.getMessage());    }    if (log.isDebugEnabled()) {        long batchElapsedTime = System.currentTimeMillis() - startTime;        if (metadata != null) {                    }    }}
1
public void onPartitionsRevoked(Collection<TopicPartition> partitions)
{    for (TopicPartition partition : partitions) {                rebalanceFlag.set(true);    }}
1
public void onPartitionsAssigned(Collection<TopicPartition> partitions)
{    for (TopicPartition partition : partitions) {            }}
1
public void testProps() throws Exception
{    Context context = new Context();    context.put("kafka.producer.some-parameter", "1");    context.put("kafka.consumer.another-parameter", "1");    context.put(BOOTSTRAP_SERVERS_CONFIG, testUtil.getKafkaServerUrl());    context.put(TOPIC_CONFIG, topic);    final KafkaChannel channel = new KafkaChannel();    Configurables.configure(channel, context);    Properties consumerProps = channel.getConsumerProps();    Properties producerProps = channel.getProducerProps();    Assert.assertEquals(producerProps.getProperty("some-parameter"), "1");    Assert.assertEquals(consumerProps.getProperty("another-parameter"), "1");}
0
public void testOldConfig() throws Exception
{    Context context = new Context();    context.put(BROKER_LIST_FLUME_KEY, testUtil.getKafkaServerUrl());    context.put(GROUP_ID_FLUME, "flume-something");    context.put(READ_SMALLEST_OFFSET, "true");    context.put("topic", topic);    final KafkaChannel channel = new KafkaChannel();    Configurables.configure(channel, context);    Properties consumerProps = channel.getConsumerProps();    Properties producerProps = channel.getProducerProps();    Assert.assertEquals(producerProps.getProperty(CommonClientConfigs.BOOTSTRAP_SERVERS_CONFIG), testUtil.getKafkaServerUrl());    Assert.assertEquals(consumerProps.getProperty(ConsumerConfig.GROUP_ID_CONFIG), "flume-something");    Assert.assertEquals(consumerProps.getProperty(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG), "earliest");}
0
public void testStopAndStart() throws Exception
{    doTestStopAndStart(false, false);}
0
public void testStopAndStartWithRollback() throws Exception
{    doTestStopAndStart(true, true);}
0
public void testStopAndStartWithRollbackAndNoRetry() throws Exception
{    doTestStopAndStart(true, false);}
0
public void testNullKeyNoHeader() throws Exception
{    doTestNullKeyNoHeader();}
0
public void testDefaultSettingsOnReConfigure() throws Exception
{    String sampleProducerProp = "compression.type";    String sampleProducerVal = "snappy";    String sampleConsumerProp = "fetch.min.bytes";    String sampleConsumerVal = "99";    Context context = prepareDefaultContext(false);    context.put(KafkaChannelConfiguration.KAFKA_PRODUCER_PREFIX + sampleProducerProp, sampleProducerVal);    context.put(KafkaChannelConfiguration.KAFKA_CONSUMER_PREFIX + sampleConsumerProp, sampleConsumerVal);    final KafkaChannel channel = createChannel(context);    Assert.assertEquals(sampleProducerVal, channel.getProducerProps().getProperty(sampleProducerProp));    Assert.assertEquals(sampleConsumerVal, channel.getConsumerProps().getProperty(sampleConsumerProp));    context = prepareDefaultContext(false);    channel.configure(context);    Assert.assertNull(channel.getProducerProps().getProperty(sampleProducerProp));    Assert.assertNull(channel.getConsumerProps().getProperty(sampleConsumerProp));}
0
private void doTestNullKeyNoHeader() throws Exception
{    final KafkaChannel channel = startChannel(false);    Properties props = channel.getProducerProps();    KafkaProducer<String, byte[]> producer = new KafkaProducer<>(props);    for (int i = 0; i < 50; i++) {        ProducerRecord<String, byte[]> data = new ProducerRecord<>(topic, null, String.valueOf(i).getBytes());        producer.send(data).get();    }    ExecutorCompletionService<Void> submitterSvc = new ExecutorCompletionService<>(Executors.newCachedThreadPool());    List<Event> events = pullEvents(channel, submitterSvc, 50, false, false);    wait(submitterSvc, 5);    List<String> finals = new ArrayList<>(50);    for (int i = 0; i < 50; i++) {        finals.add(i, events.get(i).getHeaders().get(KEY_HEADER));    }    for (int i = 0; i < 50; i++) {        Assert.assertTrue(finals.get(i) == null);    }    channel.stop();}
0
private void doTestStopAndStart(boolean rollback, boolean retryAfterRollback) throws Exception
{    final KafkaChannel channel = startChannel(true);    ExecutorService underlying = Executors.newCachedThreadPool();    ExecutorCompletionService<Void> submitterSvc = new ExecutorCompletionService<>(underlying);    final List<List<Event>> events = createBaseList();    putEvents(channel, events, submitterSvc);    wait(submitterSvc, 5);    channel.stop();    final KafkaChannel channel2 = startChannel(true);    int total = 50;    if (rollback && !retryAfterRollback) {        total = 40;    }    final List<Event> eventsPulled = pullEvents(channel2, submitterSvc, total, rollback, retryAfterRollback);    wait(submitterSvc, 5);    channel2.stop();    if (!retryAfterRollback && rollback) {        final KafkaChannel channel3 = startChannel(true);        int expectedRemaining = 50 - eventsPulled.size();        final List<Event> eventsPulled2 = pullEvents(channel3, submitterSvc, expectedRemaining, false, false);        wait(submitterSvc, 5);        Assert.assertEquals(expectedRemaining, eventsPulled2.size());        eventsPulled.addAll(eventsPulled2);        channel3.stop();    }    underlying.shutdownNow();    verify(eventsPulled);}
0
public void testMetricsCount() throws Exception
{    final KafkaChannel channel = startChannel(true);    ExecutorService underlying = Executors.newCachedThreadPool();    ExecutorCompletionService<Void> submitterSvc = new ExecutorCompletionService<Void>(underlying);    final List<List<Event>> events = createBaseList();    putEvents(channel, events, submitterSvc);    takeEventsWithCommittingTxn(channel, 50);    KafkaChannelCounter counter = (KafkaChannelCounter) Whitebox.getInternalState(channel, "counter");    Assert.assertEquals(50, counter.getEventPutAttemptCount());    Assert.assertEquals(50, counter.getEventPutSuccessCount());    Assert.assertEquals(50, counter.getEventTakeAttemptCount());    Assert.assertEquals(50, counter.getEventTakeSuccessCount());    channel.stop();}
0
private void takeEventsWithCommittingTxn(KafkaChannel channel, long eventsCount)
{    List<Event> takeEventsList = new ArrayList<>();    Transaction txn = channel.getTransaction();    txn.begin();    while (takeEventsList.size() < eventsCount) {        Event event = channel.take();        if (event != null) {            takeEventsList.add(event);        }    }    txn.commit();    txn.close();}
0
public static void setupClass() throws Exception
{    testUtil.prepare();    Thread.sleep(2500);}
0
public void setup() throws Exception
{    topic = findUnusedTopic();    createTopic(topic, DEFAULT_TOPIC_PARTITIONS);    Thread.sleep(2500);}
0
public static void tearDown()
{    testUtil.tearDown();}
0
 String findUnusedTopic()
{    String newTopic = null;    boolean topicFound = false;    while (!topicFound) {        newTopic = RandomStringUtils.randomAlphabetic(8);        if (!usedTopics.contains(newTopic)) {            usedTopics.add(newTopic);            topicFound = true;        }    }    return newTopic;}
0
 static void createTopic(String topicName, int numPartitions)
{    testUtil.createTopics(Collections.singletonList(topicName), numPartitions);}
0
 static void deleteTopic(String topicName)
{    testUtil.deleteTopic(topicName);}
0
 KafkaChannel startChannel(boolean parseAsFlume) throws Exception
{    Context context = prepareDefaultContext(parseAsFlume);    KafkaChannel channel = createChannel(context);    channel.start();    return channel;}
0
 Context prepareDefaultContext(boolean parseAsFlume)
{        Context context = new Context();    context.put(BOOTSTRAP_SERVERS_CONFIG, testUtil.getKafkaServerUrl());    context.put(PARSE_AS_FLUME_EVENT, String.valueOf(parseAsFlume));    context.put(TOPIC_CONFIG, topic);    context.put(KAFKA_CONSUMER_PREFIX + "max.poll.interval.ms", "10000");    return context;}
0
 KafkaChannel createChannel(Context context) throws Exception
{    final KafkaChannel channel = new KafkaChannel();    Configurables.configure(channel, context);    return channel;}
0
 List<Event> pullEvents(final KafkaChannel channel, ExecutorCompletionService<Void> submitterSvc, final int total, final boolean testRollbacks, final boolean retryAfterRollback)
{    final List<Event> eventsPulled = Collections.synchronizedList(new ArrayList<Event>(50));    final CyclicBarrier barrier = new CyclicBarrier(5);    final AtomicInteger counter = new AtomicInteger(0);    final AtomicInteger rolledBackCount = new AtomicInteger(0);    final AtomicBoolean startedGettingEvents = new AtomicBoolean(false);    final AtomicBoolean rolledBack = new AtomicBoolean(false);    for (int k = 0; k < 5; k++) {        final int index = k;        submitterSvc.submit(new Callable<Void>() {            @Override            public Void call() throws Exception {                Transaction tx = null;                final List<Event> eventsLocal = Lists.newLinkedList();                channel.registerThread();                Thread.sleep(1000);                barrier.await();                while (counter.get() < (total - rolledBackCount.get())) {                    if (tx == null) {                        tx = channel.getTransaction();                        tx.begin();                    }                    try {                        Event e = channel.take();                        if (e != null) {                            startedGettingEvents.set(true);                            eventsLocal.add(e);                        } else {                            if (testRollbacks && index == 4 && (!rolledBack.get()) && startedGettingEvents.get()) {                                tx.rollback();                                tx.close();                                tx = null;                                rolledBack.set(true);                                final int eventsLocalSize = eventsLocal.size();                                eventsLocal.clear();                                if (!retryAfterRollback) {                                    rolledBackCount.set(eventsLocalSize);                                    return null;                                }                            } else {                                tx.commit();                                tx.close();                                tx = null;                                eventsPulled.addAll(eventsLocal);                                counter.getAndAdd(eventsLocal.size());                                eventsLocal.clear();                            }                        }                    } catch (Exception ex) {                        eventsLocal.clear();                        if (tx != null) {                            tx.rollback();                            tx.close();                        }                        tx = null;                        ex.printStackTrace();                    }                }                                return null;            }        });    }    return eventsPulled;}
0
public Void call() throws Exception
{    Transaction tx = null;    final List<Event> eventsLocal = Lists.newLinkedList();    channel.registerThread();    Thread.sleep(1000);    barrier.await();    while (counter.get() < (total - rolledBackCount.get())) {        if (tx == null) {            tx = channel.getTransaction();            tx.begin();        }        try {            Event e = channel.take();            if (e != null) {                startedGettingEvents.set(true);                eventsLocal.add(e);            } else {                if (testRollbacks && index == 4 && (!rolledBack.get()) && startedGettingEvents.get()) {                    tx.rollback();                    tx.close();                    tx = null;                    rolledBack.set(true);                    final int eventsLocalSize = eventsLocal.size();                    eventsLocal.clear();                    if (!retryAfterRollback) {                        rolledBackCount.set(eventsLocalSize);                        return null;                    }                } else {                    tx.commit();                    tx.close();                    tx = null;                    eventsPulled.addAll(eventsLocal);                    counter.getAndAdd(eventsLocal.size());                    eventsLocal.clear();                }            }        } catch (Exception ex) {            eventsLocal.clear();            if (tx != null) {                tx.rollback();                tx.close();            }            tx = null;            ex.printStackTrace();        }    }        return null;}
0
 void wait(ExecutorCompletionService<Void> submitterSvc, int max) throws Exception
{    int completed = 0;    while (completed < max) {        submitterSvc.take();        completed++;    }}
0
 List<List<Event>> createBaseList()
{    final List<List<Event>> events = new ArrayList<>();    for (int i = 0; i < 5; i++) {        List<Event> eventList = new ArrayList<>(10);        events.add(eventList);        for (int j = 0; j < 10; j++) {            Map<String, String> hdrs = new HashMap<>();            String v = (String.valueOf(i) + " - " + String.valueOf(j));            hdrs.put("header", v);            eventList.add(EventBuilder.withBody(v.getBytes(), hdrs));        }    }    return events;}
0
 void putEvents(final KafkaChannel channel, final List<List<Event>> events, ExecutorCompletionService<Void> submitterSvc)
{    for (int i = 0; i < 5; i++) {        final int index = i;        submitterSvc.submit(new Callable<Void>() {            @Override            public Void call() {                Transaction tx = channel.getTransaction();                tx.begin();                List<Event> eventsToPut = events.get(index);                for (int j = 0; j < 10; j++) {                    channel.put(eventsToPut.get(j));                }                try {                    tx.commit();                } finally {                    tx.close();                }                return null;            }        });    }}
0
public Void call()
{    Transaction tx = channel.getTransaction();    tx.begin();    List<Event> eventsToPut = events.get(index);    for (int j = 0; j < 10; j++) {        channel.put(eventsToPut.get(j));    }    try {        tx.commit();    } finally {        tx.close();    }    return null;}
0
 void verify(List<Event> eventsPulled)
{    Assert.assertFalse(eventsPulled.isEmpty());    Assert.assertEquals(50, eventsPulled.size());    Set<String> eventStrings = new HashSet<>();    for (Event e : eventsPulled) {        Assert.assertEquals(e.getHeaders().get("header"), new String(e.getBody()));        eventStrings.add(e.getHeaders().get("header"));    }    for (int i = 0; i < 5; i++) {        for (int j = 0; j < 10; j++) {            String v = String.valueOf(i) + " - " + String.valueOf(j);            Assert.assertTrue(eventStrings.contains(v));            eventStrings.remove(v);        }    }    Assert.assertTrue(eventStrings.isEmpty());}
0
public void testOffsetsNotCommittedOnStop() throws Exception
{    String message = "testOffsetsNotCommittedOnStop-" + System.nanoTime();    KafkaChannel channel = startChannel(false);    KafkaProducer<String, byte[]> producer = new KafkaProducer<>(channel.getProducerProps());    ProducerRecord<String, byte[]> data = new ProducerRecord<>(topic, "header-" + message, message.getBytes());    producer.send(data).get();    producer.flush();    producer.close();    Event event = takeEventWithoutCommittingTxn(channel);    Assert.assertNotNull(event);    Assert.assertTrue(Arrays.equals(message.getBytes(), event.getBody()));        channel.stop();    channel = startChannel(false);        event = takeEventWithoutCommittingTxn(channel);    Assert.assertNotNull(event);    Assert.assertTrue(Arrays.equals(message.getBytes(), event.getBody()));}
0
public void testMigrateOffsetsNone() throws Exception
{    doTestMigrateZookeeperOffsets(false, false, "testMigrateOffsets-none");}
0
public void testMigrateOffsetsZookeeper() throws Exception
{    doTestMigrateZookeeperOffsets(true, false, "testMigrateOffsets-zookeeper");}
0
public void testMigrateOffsetsKafka() throws Exception
{    doTestMigrateZookeeperOffsets(false, true, "testMigrateOffsets-kafka");}
0
public void testMigrateOffsetsBoth() throws Exception
{    doTestMigrateZookeeperOffsets(true, true, "testMigrateOffsets-both");}
0
private Event takeEventWithoutCommittingTxn(KafkaChannel channel)
{    for (int i = 0; i < 10; i++) {        Transaction txn = channel.getTransaction();        txn.begin();        Event event = channel.take();        if (event != null) {            return event;        } else {            txn.commit();            txn.close();        }    }    return null;}
0
private void doTestMigrateZookeeperOffsets(boolean hasZookeeperOffsets, boolean hasKafkaOffsets, String group) throws Exception
{        topic = findUnusedTopic();    createTopic(topic, 1);    Context context = prepareDefaultContext(false);    context.put(ZOOKEEPER_CONNECT_FLUME_KEY, testUtil.getZkUrl());    context.put(GROUP_ID_FLUME, group);    final KafkaChannel channel = createChannel(context);        Long fifthOffset = 0L;    Long tenthOffset = 0L;    Properties props = channel.getProducerProps();    KafkaProducer<String, byte[]> producer = new KafkaProducer<>(props);    for (int i = 1; i <= 50; i++) {        ProducerRecord<String, byte[]> data = new ProducerRecord<>(topic, null, String.valueOf(i).getBytes());        RecordMetadata recordMetadata = producer.send(data).get();        if (i == 5) {            fifthOffset = recordMetadata.offset();        }        if (i == 10) {            tenthOffset = recordMetadata.offset();        }    }        if (hasZookeeperOffsets) {        KafkaZkClient zkClient = KafkaZkClient.apply(testUtil.getZkUrl(), JaasUtils.isZkSecurityEnabled(), 30000, 30000, 10, Time.SYSTEM, "kafka.server", "SessionExpireListener");        zkClient.getConsumerOffset(group, new TopicPartition(topic, 0));        Long offset = tenthOffset + 1;        zkClient.setOrCreateConsumerOffset(group, new TopicPartition(topic, 0), offset);        zkClient.close();    }        if (hasKafkaOffsets) {        Map<TopicPartition, OffsetAndMetadata> offsets = new HashMap<>();        offsets.put(new TopicPartition(topic, 0), new OffsetAndMetadata(fifthOffset + 1));        KafkaConsumer<String, byte[]> consumer = new KafkaConsumer<>(channel.getConsumerProps());        consumer.commitSync(offsets);        consumer.close();    }        channel.start();    ExecutorCompletionService<Void> submitterSvc = new ExecutorCompletionService<>(Executors.newCachedThreadPool());    List<Event> events = pullEvents(channel, submitterSvc, 20, false, false);    wait(submitterSvc, 5);    List<Integer> finals = new ArrayList<>(40);    for (Event event : events) {        finals.add(Integer.parseInt(new String(event.getBody())));    }    channel.stop();    if (!hasKafkaOffsets && !hasZookeeperOffsets) {                Assert.assertTrue("Channel should read the the first message", finals.contains(1));    } else if (hasKafkaOffsets && hasZookeeperOffsets) {                Assert.assertFalse("Channel should not read the 5th message", finals.contains(5));        Assert.assertTrue("Channel should read the 6th message", finals.contains(6));    } else if (hasKafkaOffsets) {                Assert.assertFalse("Channel should not read the 5th message", finals.contains(5));        Assert.assertTrue("Channel should read the 6th message", finals.contains(6));    } else {                Assert.assertFalse("Channel should not read the 10th message", finals.contains(10));        Assert.assertTrue("Channel should read the 11th message", finals.contains(11));    }}
0
public void testMigrateZookeeperOffsetsWhenTopicNotExists() throws Exception
{    topic = findUnusedTopic();    Context context = prepareDefaultContext(false);    context.put(ZOOKEEPER_CONNECT_FLUME_KEY, testUtil.getZkUrl());    context.put(GROUP_ID_FLUME, "testMigrateOffsets-nonExistingTopic");    KafkaChannel channel = createChannel(context);    channel.start();    Assert.assertEquals(LifecycleState.START, channel.getLifecycleState());    channel.stop();}
0
public void testParseAsFlumeEventFalse() throws Exception
{    doParseAsFlumeEventFalse(false);}
0
public void testParseAsFlumeEventFalseCheckHeader() throws Exception
{    doParseAsFlumeEventFalse(true);}
0
public void testParseAsFlumeEventFalseAsSource() throws Exception
{    doParseAsFlumeEventFalseAsSource(false);}
0
public void testParseAsFlumeEventFalseAsSourceCheckHeader() throws Exception
{    doParseAsFlumeEventFalseAsSource(true);}
0
private void doParseAsFlumeEventFalse(Boolean checkHeaders) throws Exception
{    final KafkaChannel channel = startChannel(false);    Properties props = channel.getProducerProps();    KafkaProducer<String, byte[]> producer = new KafkaProducer<>(props);    for (int i = 0; i < 50; i++) {        ProducerRecord<String, byte[]> data = new ProducerRecord<>(topic, String.valueOf(i) + "-header", String.valueOf(i).getBytes());        producer.send(data).get();    }    ExecutorCompletionService<Void> submitterSvc = new ExecutorCompletionService<>(Executors.newCachedThreadPool());    List<Event> events = pullEvents(channel, submitterSvc, 50, false, false);    wait(submitterSvc, 5);    Map<Integer, String> finals = new HashMap<>();    for (int i = 0; i < 50; i++) {        finals.put(Integer.parseInt(new String(events.get(i).getBody())), events.get(i).getHeaders().get(KEY_HEADER));    }    for (int i = 0; i < 50; i++) {        Assert.assertTrue(finals.keySet().contains(i));        if (checkHeaders) {            Assert.assertTrue(finals.containsValue(String.valueOf(i) + "-header"));        }        finals.remove(i);    }    Assert.assertTrue(finals.isEmpty());    channel.stop();}
0
private void doParseAsFlumeEventFalseAsSource(Boolean checkHeaders) throws Exception
{    final KafkaChannel channel = startChannel(false);    List<String> msgs = new ArrayList<>();    Map<String, String> headers = new HashMap<>();    for (int i = 0; i < 50; i++) {        msgs.add(String.valueOf(i));    }    Transaction tx = channel.getTransaction();    tx.begin();    for (int i = 0; i < msgs.size(); i++) {        headers.put(KEY_HEADER, String.valueOf(i) + "-header");        channel.put(EventBuilder.withBody(msgs.get(i).getBytes(), headers));    }    tx.commit();    ExecutorCompletionService<Void> submitterSvc = new ExecutorCompletionService<>(Executors.newCachedThreadPool());    List<Event> events = pullEvents(channel, submitterSvc, 50, false, false);    wait(submitterSvc, 5);    Map<Integer, String> finals = new HashMap<>();    for (int i = 0; i < 50; i++) {        finals.put(Integer.parseInt(new String(events.get(i).getBody())), events.get(i).getHeaders().get(KEY_HEADER));    }    for (int i = 0; i < 50; i++) {        Assert.assertTrue(finals.keySet().contains(i));        if (checkHeaders) {            Assert.assertTrue(finals.containsValue(String.valueOf(i) + "-header"));        }        finals.remove(i);    }    Assert.assertTrue(finals.isEmpty());    channel.stop();}
0
public void testPartitionHeaderSet() throws Exception
{    doPartitionHeader(PartitionTestScenario.PARTITION_ID_HEADER_ONLY);}
0
public void testPartitionHeaderNotSet() throws Exception
{    doPartitionHeader(PartitionTestScenario.NO_PARTITION_HEADERS);}
0
public void testStaticPartitionAndHeaderSet() throws Exception
{    doPartitionHeader(PartitionTestScenario.STATIC_HEADER_AND_PARTITION_ID);}
0
public void testStaticPartitionHeaderNotSet() throws Exception
{    doPartitionHeader(PartitionTestScenario.STATIC_HEADER_ONLY);}
0
public void testPartitionHeaderMissing() throws Exception
{    doPartitionErrors(PartitionOption.NOTSET);}
0
public void testPartitionHeaderOutOfRange() throws Exception
{    doPartitionErrors(PartitionOption.VALIDBUTOUTOFRANGE);}
0
public void testPartitionHeaderInvalid() throws Exception
{    doPartitionErrors(PartitionOption.NOTANUMBER);}
0
private void doPartitionHeader(PartitionTestScenario scenario) throws Exception
{    final int numPtns = DEFAULT_TOPIC_PARTITIONS;    final int numMsgs = numPtns * 10;    final Integer staticPtn = DEFAULT_TOPIC_PARTITIONS - 2;    Context context = prepareDefaultContext(false);    if (scenario == PartitionTestScenario.PARTITION_ID_HEADER_ONLY || scenario == PartitionTestScenario.STATIC_HEADER_AND_PARTITION_ID) {        context.put(PARTITION_HEADER_NAME, "partition-header");    }    if (scenario == PartitionTestScenario.STATIC_HEADER_AND_PARTITION_ID || scenario == PartitionTestScenario.STATIC_HEADER_ONLY) {        context.put(STATIC_PARTITION_CONF, staticPtn.toString());    }    final KafkaChannel channel = createChannel(context);    channel.start();            Map<Integer, List<Event>> partitionMap = new HashMap<>(numPtns);    for (int i = 0; i < numPtns; i++) {        partitionMap.put(i, new ArrayList<Event>());    }    Transaction tx = channel.getTransaction();    tx.begin();    List<Event> orderedEvents = KafkaPartitionTestUtil.generateSkewedMessageList(scenario, numMsgs, partitionMap, numPtns, staticPtn);    for (Event event : orderedEvents) {        channel.put(event);    }    tx.commit();    Map<Integer, List<byte[]>> resultsMap = KafkaPartitionTestUtil.retrieveRecordsFromPartitions(topic, numPtns, channel.getConsumerProps());    KafkaPartitionTestUtil.checkResultsAgainstSkew(scenario, partitionMap, resultsMap, staticPtn, numMsgs);    channel.stop();}
0
private void doPartitionErrors(PartitionOption option) throws Exception
{    Context context = prepareDefaultContext(false);    context.put(PARTITION_HEADER_NAME, KafkaPartitionTestUtil.PARTITION_HEADER);    String tempTopic = findUnusedTopic();    createTopic(tempTopic, 5);    final KafkaChannel channel = createChannel(context);    channel.start();    Transaction tx = channel.getTransaction();    tx.begin();    Map<String, String> headers = new HashMap<>();    switch(option) {        case VALIDBUTOUTOFRANGE:            headers.put(KafkaPartitionTestUtil.PARTITION_HEADER, String.valueOf(DEFAULT_TOPIC_PARTITIONS + 2));            break;        case NOTSET:            headers.put("wrong-header", "2");            break;        case NOTANUMBER:            headers.put(KafkaPartitionTestUtil.PARTITION_HEADER, "not-a-number");            break;        default:            break;    }    Event event = EventBuilder.withBody(String.valueOf(9).getBytes(), headers);    channel.put(event);    tx.commit();    deleteTopic(tempTopic);}
0
public void testSuccess() throws Exception
{    doTestSuccessRollback(false, false);}
0
public void testSuccessInterleave() throws Exception
{    doTestSuccessRollback(false, true);}
0
public void testRollbacks() throws Exception
{    doTestSuccessRollback(true, false);}
0
public void testRollbacksInterleave() throws Exception
{    doTestSuccessRollback(true, true);}
0
private void doTestSuccessRollback(final boolean rollback, final boolean interleave) throws Exception
{    final KafkaChannel channel = startChannel(true);    writeAndVerify(rollback, channel, interleave);    channel.stop();}
0
private void writeAndVerify(final boolean testRollbacks, final KafkaChannel channel, final boolean interleave) throws Exception
{    final List<List<Event>> events = createBaseList();    ExecutorCompletionService<Void> submitterSvc = new ExecutorCompletionService<Void>(Executors.newCachedThreadPool());    putEvents(channel, events, submitterSvc);    if (interleave) {        wait(submitterSvc, 5);    }    ExecutorCompletionService<Void> submitterSvc2 = new ExecutorCompletionService<Void>(Executors.newCachedThreadPool());    final List<Event> eventsPulled = pullEvents(channel, submitterSvc2, 50, testRollbacks, true);    if (!interleave) {        wait(submitterSvc, 5);    }    wait(submitterSvc2, 5);    verify(eventsPulled);}
0
protected int getTotalStored()
{    return totalStored.availablePermits();}
0
public int getMemoryCapacity()
{    return memoryCapacity;}
0
public int getOverflowTimeout()
{    return overflowTimeout;}
0
public int getMaxMemQueueSize()
{    return maxMemQueueSize;}
0
protected Integer getOverflowCapacity()
{    return overflowCapacity;}
0
protected boolean isOverflowDisabled()
{    return overflowDisabled;}
0
public int queueSize()
{    synchronized (queueLock) {        return memQueue.size();    }}
0
public void add(int amount)
{    value += amount;}
0
public int intValue()
{    return value;}
0
public String dump()
{    StringBuilder sb = new StringBuilder();    sb.append("  [ ");    for (MutableInteger i : queue) {        sb.append(i.intValue());        sb.append(" ");    }    sb.append("]");    return sb.toString();}
0
public void putPrimary(Integer eventCount)
{    totalPuts += eventCount;    if ((queue.peekLast() == null) || queue.getLast().intValue() < 0) {        queue.addLast(new MutableInteger(eventCount));    } else {        queue.getLast().add(eventCount);    }}
0
public void putFirstPrimary(Integer eventCount)
{    if ((queue.peekFirst() == null) || queue.getFirst().intValue() < 0) {        queue.addFirst(new MutableInteger(eventCount));    } else {        queue.getFirst().add(eventCount);    }}
0
public void putOverflow(Integer eventCount)
{    totalPuts += eventCount;    if ((queue.peekLast() == null) || queue.getLast().intValue() > 0) {        queue.addLast(new MutableInteger(-eventCount));    } else {        queue.getLast().add(-eventCount);    }    overflowCounter += eventCount;}
0
public void putFirstOverflow(Integer eventCount)
{    if ((queue.peekFirst() == null) || queue.getFirst().intValue() > 0) {        queue.addFirst(new MutableInteger(-eventCount));    } else {        queue.getFirst().add(-eventCount);    }    overflowCounter += eventCount;}
0
public int front()
{    return queue.getFirst().intValue();}
0
public boolean isEmpty()
{    return queue.isEmpty();}
0
public void takePrimary(int takeCount)
{    MutableInteger headValue = queue.getFirst();        if (headValue.intValue() < takeCount) {        throw new IllegalStateException("Cannot take " + takeCount + " from " + headValue.intValue() + " in DrainOrder Queue");    }    headValue.add(-takeCount);    if (headValue.intValue() == 0) {        queue.removeFirst();    }}
0
public void takeOverflow(int takeCount)
{    MutableInteger headValue = queue.getFirst();    if (headValue.intValue() > -takeCount) {        throw new IllegalStateException("Cannot take " + takeCount + " from " + headValue.intValue() + " in DrainOrder Queue head ");    }    headValue.add(takeCount);    if (headValue.intValue() == 0) {        queue.removeFirst();    }    overflowCounter -= takeCount;}
0
public void begin()
{    super.begin();}
0
public void close()
{    if (overflowTakeTx != null) {        overflowTakeTx.close();    }    if (overflowPutTx != null) {        overflowPutTx.close();    }    super.close();}
0
protected void doPut(Event event) throws InterruptedException
{    channelCounter.incrementEventPutAttemptCount();    putCalled = true;    int eventByteSize = (int) Math.ceil(estimateEventSize(event) / avgEventSize);    if (!putList.offer(event)) {        throw new ChannelFullException("Put queue in " + getName() + " channel's Transaction having capacity " + putList.size() + " full, consider reducing batch size of sources");    }    putListByteCount += eventByteSize;}
0
protected Event doTake() throws InterruptedException
{    channelCounter.incrementEventTakeAttemptCount();    if (!totalStored.tryAcquire(overflowTimeout, TimeUnit.SECONDS)) {                return null;    }    boolean takeSuceeded = false;    try {        Event event;        synchronized (queueLock) {            int drainOrderTop = drainOrder.front();            if (!takeCalled) {                takeCalled = true;                if (drainOrderTop < 0) {                    useOverflow = true;                    overflowTakeTx = getOverflowTx();                    overflowTakeTx.begin();                }            }            if (useOverflow) {                if (drainOrderTop > 0) {                                                            return null;                }                event = overflowTakeTx.take();                ++takeCount;                drainOrder.takeOverflow(1);            } else {                if (drainOrderTop < 0) {                                                            return null;                }                event = memQueue.poll();                ++takeCount;                drainOrder.takePrimary(1);                Preconditions.checkNotNull(event, "Queue.poll returned NULL despite" + " semaphore signalling existence of entry");            }        }        int eventByteSize = (int) Math.ceil(estimateEventSize(event) / avgEventSize);        if (!useOverflow) {                        takeList.offer(event);        }        takeListByteCount += eventByteSize;        takeSuceeded = true;        return event;    } finally {        if (!takeSuceeded) {            totalStored.release();        }    }}
1
protected void doCommit() throws InterruptedException
{    if (putCalled) {        putCommit();        if (LOGGER.isDebugEnabled()) {                    }    } else if (takeCalled) {        takeCommit();        if (LOGGER.isDebugEnabled()) {                    }    }}
1
private void takeCommit()
{    if (takeCount > largestTakeTxSize) {        largestTakeTxSize = takeCount;    }    synchronized (queueLock) {        if (overflowTakeTx != null) {            overflowTakeTx.commit();        }        double memoryPercentFree = (memoryCapacity == 0) ? 0 : (memoryCapacity - memQueue.size() + takeCount) / (double) memoryCapacity;        if (overflowActivated && memoryPercentFree >= overflowDeactivationThreshold) {            overflowActivated = false;                    }        channelCounter.setChannelSize(getTotalStored());    }    if (!useOverflow) {        memQueRemaining.release(takeCount);        bytesRemaining.release(takeListByteCount);    }    channelCounter.addToEventTakeSuccessCount(takeCount);}
1
private void putCommit() throws InterruptedException
{        int timeout = overflowActivated ? 0 : overflowTimeout;    if (memoryCapacity != 0) {                if (!memQueRemaining.tryAcquire(putList.size(), timeout, TimeUnit.SECONDS)) {            if (overflowDisabled) {                throw new ChannelFullException("Spillable Memory Channel's " + "memory capacity has been reached and overflow is " + "disabled. Consider increasing memoryCapacity.");            }            overflowActivated = true;            useOverflow = true;                } else if (!bytesRemaining.tryAcquire(putListByteCount, overflowTimeout, TimeUnit.SECONDS)) {            memQueRemaining.release(putList.size());            if (overflowDisabled) {                throw new ChannelFullException("Spillable Memory Channel's " + "memory capacity has been reached.  " + (bytesRemaining.availablePermits() * (int) avgEventSize) + " bytes are free and overflow is disabled. Consider " + "increasing byteCapacity or capacity.");            }            overflowActivated = true;            useOverflow = true;        }    } else {        useOverflow = true;    }    if (putList.size() > largestPutTxSize) {        largestPutTxSize = putList.size();    }    if (useOverflow) {        commitPutsToOverflow();    } else {        commitPutsToPrimary();    }}
0
private void commitPutsToOverflow() throws InterruptedException
{    overflowPutTx = getOverflowTx();    overflowPutTx.begin();    for (Event event : putList) {        overflowPutTx.put(event);    }    commitPutsToOverflow_core(overflowPutTx);    totalStored.release(putList.size());    overflowPutCount += putList.size();    channelCounter.addToEventPutSuccessCount(putList.size());}
0
private void commitPutsToOverflow_core(Transaction overflowPutTx) throws InterruptedException
{        for (int i = 0; i < 2; ++i) {        try {            synchronized (queueLock) {                overflowPutTx.commit();                drainOrder.putOverflow(putList.size());                channelCounter.setChannelSize(memQueue.size() + drainOrder.overflowCounter);                break;            }        } catch (ChannelFullException e) {                        if (i == 0) {                Thread.sleep(overflowTimeout * 1000);            } else {                throw e;            }        }    }}
0
private void commitPutsToPrimary()
{    synchronized (queueLock) {        for (Event e : putList) {            if (!memQueue.offer(e)) {                throw new ChannelException("Unable to insert event into memory " + "queue in spite of spare capacity, this is very unexpected");            }        }        drainOrder.putPrimary(putList.size());        maxMemQueueSize = (memQueue.size() > maxMemQueueSize) ? memQueue.size() : maxMemQueueSize;        channelCounter.setChannelSize(memQueue.size() + drainOrder.overflowCounter);    }        totalStored.release(putList.size());    channelCounter.addToEventPutSuccessCount(putList.size());}
0
protected void doRollback()
{        if (putCalled) {        if (overflowPutTx != null) {            overflowPutTx.rollback();        }        if (!useOverflow) {            bytesRemaining.release(putListByteCount);            putList.clear();        }        putListByteCount = 0;    } else if (takeCalled) {        synchronized (queueLock) {            if (overflowTakeTx != null) {                overflowTakeTx.rollback();            }            if (useOverflow) {                drainOrder.putFirstOverflow(takeCount);            } else {                int remainingCapacity = memoryCapacity - memQueue.size();                Preconditions.checkState(remainingCapacity >= takeCount, "Not enough space in memory queue to rollback takes. This" + " should never happen, please report");                while (!takeList.isEmpty()) {                    memQueue.addFirst(takeList.removeLast());                }                drainOrder.putFirstPrimary(takeCount);            }        }        totalStored.release(takeCount);    } else {        overflowTakeTx.rollback();    }    channelCounter.setChannelSize(memQueue.size() + drainOrder.overflowCounter);}
1
public void configure(Context context)
{    if (    getLifecycleState() == LifecycleState.START || getLifecycleState() == LifecycleState.ERROR) {        stop();    }    if (totalStored == null) {        totalStored = new Semaphore(0);    }    if (channelCounter == null) {        channelCounter = new ChannelCounter(getName());    }        Integer newMemoryCapacity;    try {        newMemoryCapacity = context.getInteger(MEMORY_CAPACITY, defaultMemoryCapacity);        if (newMemoryCapacity == null) {            newMemoryCapacity = defaultMemoryCapacity;        }        if (newMemoryCapacity < 0) {            throw new NumberFormatException(MEMORY_CAPACITY + " must be >= 0");        }    } catch (NumberFormatException e) {        newMemoryCapacity = defaultMemoryCapacity;            }    try {        resizePrimaryQueue(newMemoryCapacity);    } catch (InterruptedException e) {        Thread.currentThread().interrupt();    }        try {        Integer newOverflowTimeout = context.getInteger(OVERFLOW_TIMEOUT, defaultOverflowTimeout);        overflowTimeout = (newOverflowTimeout != null) ? newOverflowTimeout : defaultOverflowTimeout;    } catch (NumberFormatException e) {                overflowTimeout = defaultOverflowTimeout;    }    try {        Integer newThreshold = context.getInteger(OVERFLOW_DEACTIVATION_THRESHOLD);        overflowDeactivationThreshold = (newThreshold != null) ? newThreshold / 100.0 : defaultOverflowDeactivationThreshold / 100.0;    } catch (NumberFormatException e) {                overflowDeactivationThreshold = defaultOverflowDeactivationThreshold / 100.0;    }        try {        byteCapacityBufferPercentage = context.getInteger(BYTE_CAPACITY_BUFFER_PERCENTAGE, defaultByteCapacityBufferPercentage);    } catch (NumberFormatException e) {                byteCapacityBufferPercentage = defaultByteCapacityBufferPercentage;    }    try {        avgEventSize = context.getInteger(AVG_EVENT_SIZE, defaultAvgEventSize);    } catch (NumberFormatException e) {                avgEventSize = defaultAvgEventSize;    }    try {        byteCapacity = (int) ((context.getLong(BYTE_CAPACITY, defaultByteCapacity) * (1 - byteCapacityBufferPercentage * .01)) / avgEventSize);        if (byteCapacity < 1) {            byteCapacity = Integer.MAX_VALUE;        }    } catch (NumberFormatException e) {                byteCapacity = (int) ((defaultByteCapacity * (1 - byteCapacityBufferPercentage * .01)) / avgEventSize);    }    if (bytesRemaining == null) {        bytesRemaining = new Semaphore(byteCapacity);        lastByteCapacity = byteCapacity;    } else {        if (byteCapacity > lastByteCapacity) {            bytesRemaining.release(byteCapacity - lastByteCapacity);            lastByteCapacity = byteCapacity;        } else {            try {                if (!bytesRemaining.tryAcquire(lastByteCapacity - byteCapacity, overflowTimeout, TimeUnit.SECONDS)) {                                    } else {                    lastByteCapacity = byteCapacity;                }            } catch (InterruptedException e) {                Thread.currentThread().interrupt();            }        }    }    try {                overflowCapacity = context.getInteger(OVERFLOW_CAPACITY, defaultOverflowCapacity);                if (memoryCapacity < 1 && overflowCapacity < 1) {                        overflowCapacity = defaultOverflowCapacity;        }        overflowDisabled = (overflowCapacity < 1);        if (overflowDisabled) {            overflowActivated = false;        }    } catch (NumberFormatException e) {        overflowCapacity = defaultOverflowCapacity;    }            context.put(KEEP_ALIVE, "0");        context.put(CAPACITY, Integer.toString(overflowCapacity));    super.configure(context);}
1
private void resizePrimaryQueue(int newMemoryCapacity) throws InterruptedException
{    if (memQueue != null && memoryCapacity == newMemoryCapacity) {        return;    }    if (memoryCapacity > newMemoryCapacity) {        int diff = memoryCapacity - newMemoryCapacity;        if (!memQueRemaining.tryAcquire(diff, overflowTimeout, TimeUnit.SECONDS)) {                        return;        }        synchronized (queueLock) {            ArrayDeque<Event> newQueue = new ArrayDeque<Event>(newMemoryCapacity);            newQueue.addAll(memQueue);            memQueue = newQueue;            memoryCapacity = newMemoryCapacity;        }    } else {                synchronized (queueLock) {            ArrayDeque<Event> newQueue = new ArrayDeque<Event>(newMemoryCapacity);            if (memQueue != null) {                newQueue.addAll(memQueue);            }            memQueue = newQueue;            if (memQueRemaining == null) {                memQueRemaining = new Semaphore(newMemoryCapacity);            } else {                int diff = newMemoryCapacity - memoryCapacity;                memQueRemaining.release(diff);            }            memoryCapacity = newMemoryCapacity;        }    }}
1
public synchronized void start()
{    super.start();    int overFlowCount = super.getDepth();    if (drainOrder.isEmpty()) {        drainOrder.putOverflow(overFlowCount);        totalStored.release(overFlowCount);    }    channelCounter.start();    int totalCount = overFlowCount + memQueue.size();    channelCounter.setChannelCapacity(memoryCapacity + getOverflowCapacity());    channelCounter.setChannelSize(totalCount);}
0
public synchronized void stop()
{    if (getLifecycleState() == LifecycleState.STOP) {        return;    }    channelCounter.setChannelSize(memQueue.size() + drainOrder.overflowCounter);    channelCounter.stop();    super.stop();}
0
protected BasicTransactionSemantics createTransaction()
{    return new SpillableMemoryTransaction(channelCounter);}
0
private BasicTransactionSemantics getOverflowTx()
{    return super.createTransaction();}
0
private long estimateEventSize(Event event)
{    byte[] body = event.getBody();    if (body != null && body.length != 0) {        return body.length;    }        return 1;}
0
private void configureChannel(Map<String, String> overrides)
{    Context context = new Context();    File checkPointDir = fileChannelDir.newFolder("checkpoint");    File dataDir = fileChannelDir.newFolder("data");    context.put(FileChannelConfiguration.CHECKPOINT_DIR, checkPointDir.getAbsolutePath());    context.put(FileChannelConfiguration.DATA_DIRS, dataDir.getAbsolutePath());        context.put(FileChannelConfiguration.CHECKPOINT_INTERVAL, "5000");    if (overrides != null) {        context.putAll(overrides);    }    Configurables.configure(channel, context);}
0
private void reconfigureChannel(Map<String, String> overrides)
{    configureChannel(overrides);    channel.stop();    channel.start();}
0
private void startChannel(Map<String, String> params)
{    configureChannel(params);    channel.start();}
0
private void restartChannel(Map<String, String> params)
{    channel.stop();    setUp();    startChannel(params);}
0
public void setUp()
{    channel = new SpillableMemoryChannel();    channel.setName("spillChannel-" + UUID.randomUUID());}
0
public void tearDown()
{    channel.stop();}
0
private static void putN(int first, int count, AbstractChannel channel)
{    for (int i = 0; i < count; ++i) {        channel.put(EventBuilder.withBody(String.valueOf(first++).getBytes()));    }}
0
private static void takeNull(AbstractChannel channel)
{    channel.take();}
0
private static void takeN(int first, int count, AbstractChannel channel)
{    int last = first + count;    for (int i = first; i < last; ++i) {        Event e = channel.take();        if (e == null) {            throw new NullFound(i);        }        Event expected = EventBuilder.withBody(String.valueOf(i).getBytes());        Assert.assertArrayEquals(e.getBody(), expected.getBody());    }}
0
private static int takeN_NoCheck(int batchSize, AbstractChannel channel)
{    int i = 0;    for (; i < batchSize; ++i) {        Event e = channel.take();        if (e == null) {            try {                Thread.sleep(0);            } catch (InterruptedException ex) {            /* ignore */            }            return i;        }    }    return i;}
0
private static void transactionalPutN(int first, int count, AbstractChannel channel)
{    Transaction tx = channel.getTransaction();    tx.begin();    try {        putN(first, count, channel);        tx.commit();    } catch (RuntimeException e) {        tx.rollback();        throw e;    } finally {        tx.close();    }}
0
private static void transactionalTakeN(int first, int count, AbstractChannel channel)
{    Transaction tx = channel.getTransaction();    tx.begin();    try {        takeN(first, count, channel);        tx.commit();    } catch (NullFound e) {        tx.commit();        throw e;    } catch (AssertionError e) {        tx.rollback();        throw e;    } catch (RuntimeException e) {        tx.rollback();        throw e;    } finally {        tx.close();    }}
0
private static int transactionalTakeN_NoCheck(int count, AbstractChannel channel)
{    Transaction tx = channel.getTransaction();    tx.begin();    try {        int eventCount = takeN_NoCheck(count, channel);        tx.commit();        return eventCount;    } catch (RuntimeException e) {        tx.rollback();        throw e;    } finally {        tx.close();    }}
0
private static void transactionalTakeNull(int count, AbstractChannel channel)
{    Transaction tx = channel.getTransaction();    tx.begin();    try {        for (int i = 0; i < count; ++i) {            takeNull(channel);        }        tx.commit();    } catch (AssertionError e) {        tx.rollback();        throw e;    } catch (RuntimeException e) {        tx.rollback();        throw e;    } finally {        tx.close();    }}
0
private Thread makePutThread(String threadName, final int first, final int count, final int batchSize, final AbstractChannel channel)
{    return new Thread(threadName) {        public void run() {            int maxdepth = 0;            StopWatch watch = new StopWatch();            for (int i = first; i < first + count; i = i + batchSize) {                transactionalPutN(i, batchSize, channel);            }            watch.elapsed();        }    };}
0
public void run()
{    int maxdepth = 0;    StopWatch watch = new StopWatch();    for (int i = first; i < first + count; i = i + batchSize) {        transactionalPutN(i, batchSize, channel);    }    watch.elapsed();}
0
private static Thread makeTakeThread(String threadName, final int first, final int count, final int batchSize, final AbstractChannel channel)
{    return new Thread(threadName) {        public void run() {            StopWatch watch = new StopWatch();            for (int i = first; i < first + count; ) {                try {                    transactionalTakeN(i, batchSize, channel);                    i = i + batchSize;                } catch (NullFound e) {                    i = e.expectedValue;                }            }            watch.elapsed();        }    };}
0
public void run()
{    StopWatch watch = new StopWatch();    for (int i = first; i < first + count; ) {        try {            transactionalTakeN(i, batchSize, channel);            i = i + batchSize;        } catch (NullFound e) {            i = e.expectedValue;        }    }    watch.elapsed();}
0
private static Thread makeTakeThread_noCheck(String threadName, final int totalEvents, final int batchSize, final AbstractChannel channel)
{    return new Thread(threadName) {        public void run() {            int batchSz = batchSize;            StopWatch watch = new StopWatch();            int i = 0, attempts = 0;            while (i < totalEvents) {                int remaining = totalEvents - i;                batchSz = (remaining > batchSz) ? batchSz : remaining;                int takenCount = transactionalTakeN_NoCheck(batchSz, channel);                if (takenCount < batchSz) {                    try {                        Thread.sleep(20);                    } catch (InterruptedException ex) {                    /* ignore */                    }                }                i += takenCount;                ++attempts;                if (attempts > totalEvents * 3) {                    throw new TooManyNulls(attempts);                }            }            watch.elapsed(" items = " + i + ", attempts = " + attempts);        }    };}
0
public void run()
{    int batchSz = batchSize;    StopWatch watch = new StopWatch();    int i = 0, attempts = 0;    while (i < totalEvents) {        int remaining = totalEvents - i;        batchSz = (remaining > batchSz) ? batchSz : remaining;        int takenCount = transactionalTakeN_NoCheck(batchSz, channel);        if (takenCount < batchSz) {            try {                Thread.sleep(20);            } catch (InterruptedException ex) {            /* ignore */            }        }        i += takenCount;        ++attempts;        if (attempts > totalEvents * 3) {            throw new TooManyNulls(attempts);        }    }    watch.elapsed(" items = " + i + ", attempts = " + attempts);}
0
public void testPutTake()
{    Map<String, String> params = new HashMap<String, String>();    params.put("memoryCapacity", "5");    params.put("overflowCapacity", "5");    params.put(FileChannelConfiguration.TRANSACTION_CAPACITY, "5");    startChannel(params);    Transaction tx = channel.getTransaction();    tx.begin();    putN(0, 2, channel);    tx.commit();    tx.close();    tx = channel.getTransaction();    tx.begin();    takeN(0, 2, channel);    tx.commit();    tx.close();}
0
public void testCapacityDisableOverflow()
{    Map<String, String> params = new HashMap<String, String>();    params.put("memoryCapacity", "2");        params.put("overflowCapacity", "0");    params.put("overflowTimeout", "0");    startChannel(params);    transactionalPutN(0, 2, channel);    boolean threw = false;    try {        transactionalPutN(2, 1, channel);    } catch (ChannelException e) {        threw = true;    }    Assert.assertTrue("Expecting ChannelFullException to be thrown", threw);    transactionalTakeN(0, 2, channel);    Transaction tx = channel.getTransaction();    tx.begin();    Assert.assertNull(channel.take());    tx.commit();    tx.close();}
0
public void testCapacityWithOverflow()
{    Map<String, String> params = new HashMap<String, String>();    params.put("memoryCapacity", "2");    params.put("overflowCapacity", "4");    params.put(FileChannelConfiguration.TRANSACTION_CAPACITY, "3");    params.put("overflowTimeout", "0");    startChannel(params);    transactionalPutN(1, 2, channel);    transactionalPutN(3, 2, channel);    transactionalPutN(5, 2, channel);    boolean threw = false;    try {                transactionalPutN(7, 2, channel);    } catch (ChannelFullException e) {        threw = true;    }    Assert.assertTrue("Expecting ChannelFullException to be thrown", threw);    transactionalTakeN(1, 2, channel);    transactionalTakeN(3, 2, channel);    transactionalTakeN(5, 2, channel);}
0
public void testRestart()
{    Map<String, String> params = new HashMap<String, String>();    params.put("memoryCapacity", "2");    params.put("overflowCapacity", "10");    params.put(FileChannelConfiguration.TRANSACTION_CAPACITY, "4");    params.put("overflowTimeout", "0");    startChannel(params);    transactionalPutN(1, 2, channel);        transactionalPutN(3, 2, channel);    restartChannel(params);        transactionalTakeN(3, 2, channel);}
0
public void testBasicStart()
{    Map<String, String> params = new HashMap<String, String>();    params.put("memoryCapacity", "10000000");    params.put("overflowCapacity", "20000000");    params.put(FileChannelConfiguration.TRANSACTION_CAPACITY, "10");    params.put("overflowTimeout", "1");    startChannel(params);    transactionalPutN(1, 5, channel);    transactionalPutN(6, 5, channel);        transactionalPutN(11, 5, channel);    transactionalTakeN(1, 10, channel);    transactionalTakeN(11, 5, channel);}
0
public void testOverflow()
{    Map<String, String> params = new HashMap<String, String>();    params.put("memoryCapacity", "10");    params.put("overflowCapacity", "20");    params.put(FileChannelConfiguration.TRANSACTION_CAPACITY, "10");    params.put("overflowTimeout", "1");    startChannel(params);    transactionalPutN(1, 5, channel);    transactionalPutN(6, 5, channel);        transactionalPutN(11, 5, channel);    transactionalTakeN(1, 10, channel);    transactionalTakeN(11, 5, channel);}
0
public void testDrainOrder()
{    Map<String, String> params = new HashMap<String, String>();    params.put("memoryCapacity", "10");    params.put("overflowCapacity", "10");    params.put(FileChannelConfiguration.TRANSACTION_CAPACITY, "5");    params.put("overflowTimeout", "1");    startChannel(params);    transactionalPutN(1, 5, channel);    transactionalPutN(6, 5, channel);        transactionalPutN(11, 5, channel);        transactionalPutN(16, 5, channel);    transactionalTakeN(1, 1, channel);    transactionalTakeN(2, 5, channel);    transactionalTakeN(7, 4, channel);    transactionalPutN(20, 2, channel);    transactionalPutN(22, 3, channel);        transactionalTakeN(11, 3, channel);        transactionalTakeN(14, 5, channel);        transactionalTakeN(19, 2, channel);}
0
public void testByteCapacity()
{    Map<String, String> params = new HashMap<String, String>();    params.put("memoryCapacity", "1000");        params.put("byteCapacity", "100");    params.put("avgEventSize", "10");    params.put("overflowCapacity", "20");    params.put(FileChannelConfiguration.TRANSACTION_CAPACITY, "10");    params.put("overflowTimeout", "1");    startChannel(params);        transactionalPutN(1, 8, channel);    transactionalPutN(9, 10, channel);        transactionalPutN(19, 10, channel);    boolean threw = false;    try {                transactionalPutN(11, 1, channel);    } catch (ChannelFullException e) {        threw = true;    }    Assert.assertTrue("byteCapacity did not throw as expected", threw);}
0
public void testDrainingOnChannelBoundary()
{    Map<String, String> params = new HashMap<String, String>();    params.put("memoryCapacity", "5");    params.put("overflowCapacity", "15");    params.put(FileChannelConfiguration.TRANSACTION_CAPACITY, "10");    params.put("overflowTimeout", "1");    startChannel(params);    transactionalPutN(1, 5, channel);        transactionalPutN(6, 5, channel);        transactionalPutN(11, 5, channel);        transactionalPutN(16, 5, channel);    transactionalTakeN(1, 3, channel);    Transaction tx = channel.getTransaction();    tx.begin();    takeN(4, 2, channel);        takeNull(channel);    tx.commit();    tx.close();        transactionalTakeN(6, 5, channel);        transactionalTakeN(11, 5, channel);        transactionalTakeN(16, 2, channel);    transactionalPutN(21, 5, channel);    tx = channel.getTransaction();    tx.begin();        takeN(18, 3, channel);        takeNull(channel);    tx.commit();    tx.close();    transactionalTakeN(21, 5, channel);}
0
public void testRollBack()
{    Map<String, String> params = new HashMap<String, String>();    params.put("memoryCapacity", "100");    params.put("overflowCapacity", "900");    params.put(FileChannelConfiguration.TRANSACTION_CAPACITY, "900");    params.put("overflowTimeout", "0");    startChannel(params);        transactionalPutN(1, 5, channel);    Transaction tx = channel.getTransaction();    tx.begin();    putN(6, 5, channel);    tx.rollback();    tx.close();    transactionalTakeN(1, 5, channel);    transactionalTakeNull(2, channel);        transactionalPutN(11, 5, channel);    transactionalTakeN(11, 5, channel);        transactionalPutN(16, 5, channel);    tx = channel.getTransaction();    tx.begin();    takeN(16, 5, channel);    takeNull(channel);    tx.rollback();    tx.close();    transactionalTakeN_NoCheck(5, channel);        transactionalPutN(21, 5, channel);    transactionalTakeN(21, 5, channel);}
0
public void testReconfigure()
{        Map<String, String> params = new HashMap<String, String>();    params.put("memoryCapacity", "10");    params.put("overflowCapacity", "0");    params.put("overflowTimeout", "0");    startChannel(params);    Assert.assertTrue("overflowTimeout setting did not reconfigure correctly", channel.getOverflowTimeout() == 0);    Assert.assertTrue("memoryCapacity did not reconfigure correctly", channel.getMemoryCapacity() == 10);    Assert.assertTrue("overflowCapacity did not reconfigure correctly", channel.isOverflowDisabled());    transactionalPutN(1, 10, channel);    boolean threw = false;    try {                transactionalPutN(11, 10, channel);    } catch (ChannelException e) {        threw = true;    }    Assert.assertTrue("Expected the channel to fill up and throw an exception, " + "but it did not throw", threw);        params = new HashMap<String, String>();    params.put("memoryCapacity", "20");    params.put("overflowCapacity", "0");    reconfigureChannel(params);    Assert.assertTrue("overflowTimeout setting did not reconfigure correctly", channel.getOverflowTimeout() == SpillableMemoryChannel.defaultOverflowTimeout);    Assert.assertTrue("memoryCapacity did not reconfigure correctly", channel.getMemoryCapacity() == 20);    Assert.assertTrue("overflowCapacity did not reconfigure correctly", channel.isOverflowDisabled());        transactionalTakeN(1, 10, channel);    transactionalPutN(11, 10, channel);    transactionalPutN(21, 10, channel);    threw = false;    try {                transactionalPutN(31, 10, channel);    } catch (ChannelException e) {        threw = true;    }    Assert.assertTrue("Expected the channel to fill up and throw an exception, " + "but it did not throw", threw);    transactionalTakeN(11, 10, channel);    transactionalTakeN(21, 10, channel);        params = new HashMap<String, String>();    reconfigureChannel(params);    Assert.assertTrue("overflowTimeout setting did not reconfigure correctly", channel.getOverflowTimeout() == SpillableMemoryChannel.defaultOverflowTimeout);    Assert.assertTrue("memoryCapacity did not reconfigure correctly", channel.getMemoryCapacity() == SpillableMemoryChannel.defaultMemoryCapacity);    Assert.assertTrue("overflowCapacity did not reconfigure correctly", channel.getOverflowCapacity() == SpillableMemoryChannel.defaultOverflowCapacity);    Assert.assertFalse("overflowCapacity did not reconfigure correctly", channel.isOverflowDisabled());        params = new HashMap<String, String>();    params.put("memoryCapacity", "10");    params.put("overflowCapacity", "10");    params.put("transactionCapacity", "5");    params.put("overflowTimeout", "1");    reconfigureChannel(params);    transactionalPutN(1, 5, channel);    transactionalPutN(6, 5, channel);    transactionalPutN(11, 5, channel);    transactionalPutN(16, 5, channel);    threw = false;    try {                transactionalPutN(21, 5, channel);    } catch (ChannelException e) {        threw = true;    }    Assert.assertTrue("Expected the last insertion to fail, but it didn't.", threw);        params = new HashMap<String, String>();    params.put("memoryCapacity", "10");    params.put("overflowCapacity", "20");    params.put("transactionCapacity", "10");    params.put("overflowTimeout", "1");    reconfigureChannel(params);        transactionalPutN(21, 5, channel);    transactionalTakeN(1, 10, channel);    transactionalTakeN(11, 5, channel);    transactionalTakeN(16, 5, channel);    transactionalTakeN(21, 5, channel);}
0
public void testParallelSingleSourceAndSink() throws InterruptedException
{    Map<String, String> params = new HashMap<String, String>();    params.put("memoryCapacity", "1000020");    params.put("overflowCapacity", "0");    params.put("overflowTimeout", "3");    startChannel(params);        Thread sourceThd = makePutThread("src", 1, 500000, 100, channel);    Thread sinkThd = makeTakeThread("sink", 1, 500000, 100, channel);    StopWatch watch = new StopWatch();    sinkThd.start();    sourceThd.start();    sourceThd.join();    sinkThd.join();    watch.elapsed();    System.out.println("Max Queue size " + channel.getMaxMemQueueSize());}
0
public void testCounters() throws InterruptedException
{    Map<String, String> params = new HashMap<String, String>();    params.put("memoryCapacity", "5000");    params.put("overflowCapacity", "5000");    params.put("transactionCapacity", "5000");    params.put("overflowTimeout", "0");    startChannel(params);    Assert.assertTrue("channel.channelCounter should have started", channel.channelCounter.getStartTime() > 0);        Thread sourceThd = makePutThread("src", 1, 5000, 2500, channel);    sourceThd.start();    sourceThd.join();    Assert.assertEquals(5000, channel.getTotalStored());    Assert.assertEquals(5000, channel.channelCounter.getChannelSize());    Assert.assertEquals(5000, channel.channelCounter.getEventPutAttemptCount());    Assert.assertEquals(5000, channel.channelCounter.getEventPutSuccessCount());        Thread sinkThd = makeTakeThread("sink", 1, 5000, 1000, channel);    sinkThd.start();    sinkThd.join();    Assert.assertEquals(0, channel.getTotalStored());    Assert.assertEquals(0, channel.channelCounter.getChannelSize());    Assert.assertEquals(5000, channel.channelCounter.getEventTakeAttemptCount());    Assert.assertEquals(5000, channel.channelCounter.getEventTakeSuccessCount());        sourceThd = makePutThread("src", 1, 10000, 1000, channel);    sourceThd.start();    sourceThd.join();    Assert.assertEquals(10000, channel.getTotalStored());    Assert.assertEquals(10000, channel.channelCounter.getChannelSize());    Assert.assertEquals(15000, channel.channelCounter.getEventPutAttemptCount());    Assert.assertEquals(15000, channel.channelCounter.getEventPutSuccessCount());        sinkThd = makeTakeThread("sink", 1, 5000, 1000, channel);    sinkThd.start();    sinkThd.join();    Assert.assertEquals(5000, channel.getTotalStored());    Assert.assertEquals(5000, channel.channelCounter.getChannelSize());    Assert.assertEquals(10000, channel.channelCounter.getEventTakeAttemptCount());    Assert.assertEquals(10000, channel.channelCounter.getEventTakeSuccessCount());        transactionalTakeN(5001, 1000, channel);    transactionalTakeN(6001, 1000, channel);    transactionalTakeN(7001, 1000, channel);    transactionalTakeN(8001, 1000, channel);    transactionalTakeN(9001, 1000, channel);    Assert.assertEquals(0, channel.getTotalStored());    Assert.assertEquals(0, channel.channelCounter.getChannelSize());    Assert.assertEquals(15000, channel.channelCounter.getEventTakeAttemptCount());    Assert.assertEquals(15000, channel.channelCounter.getEventTakeSuccessCount());        sourceThd = makePutThread("src1", 1, 5000, 1000, channel);    Thread sourceThd2 = makePutThread("src2", 1, 5000, 500, channel);    sinkThd = makeTakeThread_noCheck("sink1", 5000, 1000, channel);    sourceThd.start();    sourceThd2.start();    sinkThd.start();    sourceThd.join();    sourceThd2.join();    sinkThd.join();    Assert.assertEquals(5000, channel.getTotalStored());    Assert.assertEquals(5000, channel.channelCounter.getChannelSize());    Thread sinkThd2 = makeTakeThread_noCheck("sink2", 2500, 500, channel);    Thread sinkThd3 = makeTakeThread_noCheck("sink3", 2500, 1000, channel);    sinkThd2.start();    sinkThd3.start();    sinkThd2.join();    sinkThd3.join();    Assert.assertEquals(0, channel.getTotalStored());    Assert.assertEquals(0, channel.channelCounter.getChannelSize());    Assert.assertEquals(25000, channel.channelCounter.getEventTakeSuccessCount());    Assert.assertEquals(25000, channel.channelCounter.getEventPutSuccessCount());    Assert.assertTrue("TakeAttempt channel counter value larger than expected", 25000 <= channel.channelCounter.getEventTakeAttemptCount());    Assert.assertTrue("PutAttempt channel counter value larger than expected", 25000 <= channel.channelCounter.getEventPutAttemptCount());}
0
public ArrayList<Thread> createSourceThreads(int count, int totalEvents, int batchSize)
{    ArrayList<Thread> sourceThds = new ArrayList<Thread>();    for (int i = 0; i < count; ++i) {        sourceThds.add(makePutThread("src" + i, 1, totalEvents / count, batchSize, channel));    }    return sourceThds;}
0
public ArrayList<Thread> createSinkThreads(int count, int totalEvents, int batchSize)
{    ArrayList<Thread> sinkThreads = new ArrayList<Thread>(count);    for (int i = 0; i < count; ++i) {        sinkThreads.add(makeTakeThread_noCheck("sink" + i, totalEvents / count, batchSize, channel));    }    return sinkThreads;}
0
public void startThreads(ArrayList<Thread> threads)
{    for (Thread thread : threads) {        thread.start();    }}
0
public void joinThreads(ArrayList<Thread> threads) throws InterruptedException
{    for (Thread thread : threads) {        try {            thread.join();        } catch (InterruptedException e) {            System.out.println("Interrupted while waiting on " + thread.getName());            throw e;        }    }}
0
public void testParallelMultipleSourcesAndSinks() throws InterruptedException
{    int sourceCount = 8;    int sinkCount = 8;    int eventCount = 1000000;    int batchSize = 100;    Map<String, String> params = new HashMap<String, String>();    params.put("memoryCapacity", "0");    params.put("overflowCapacity", String.valueOf(eventCount));    params.put("overflowTimeout", "3");    startChannel(params);    ArrayList<Thread> sinks = createSinkThreads(sinkCount, eventCount, batchSize);    ArrayList<Thread> sources = createSourceThreads(sourceCount, eventCount, batchSize);    StopWatch watch = new StopWatch();    startThreads(sinks);    startThreads(sources);    joinThreads(sources);    joinThreads(sinks);    watch.elapsed();    System.out.println("Max Queue size " + channel.getMaxMemQueueSize());    Assert.assertEquals(eventCount, channel.drainOrder.totalPuts);    Assert.assertEquals("Channel not fully drained", 0, channel.getTotalStored());    System.out.println("testParallelMultipleSourcesAndSinks done");}
0
public void elapsed()
{    elapsed(null);}
0
public void elapsed(String suffix)
{    long elapsed = System.currentTimeMillis() - startTime;    if (suffix == null) {        suffix = "";    } else {        suffix = "{ " + suffix + " }";    }    if (elapsed < 10000) {        System.out.println(Thread.currentThread().getName() + " : [ " + elapsed + " ms ].        " + suffix);    } else {        System.out.println(Thread.currentThread().getName() + " : [ " + elapsed / 1000 + " sec ].       " + suffix);    }}
0
public void setHosts(String hostNames)
{    this.hosts = hostNames;}
0
public void setSelector(String selector)
{    this.selector = selector;}
0
public void setMaxBackoff(String maxBackoff)
{    this.maxBackoff = maxBackoff;}
0
private Properties getProperties(String hosts, String selector, String maxBackoff, long timeout) throws FlumeException
{    if (StringUtils.isEmpty(hosts)) {        throw new FlumeException("hosts must not be null");    }    Properties props = new Properties();    String[] hostsAndPorts = hosts.split("\\s+");    StringBuilder names = new StringBuilder();    for (int i = 0; i < hostsAndPorts.length; i++) {        String hostAndPort = hostsAndPorts[i];        String name = "h" + i;        props.setProperty(RpcClientConfigurationConstants.CONFIG_HOSTS_PREFIX + name, hostAndPort);        names.append(name).append(" ");    }    props.put(RpcClientConfigurationConstants.CONFIG_HOSTS, names.toString());    props.put(RpcClientConfigurationConstants.CONFIG_CLIENT_TYPE, ClientType.DEFAULT_LOADBALANCE.toString());    if (!StringUtils.isEmpty(selector)) {        props.put(RpcClientConfigurationConstants.CONFIG_HOST_SELECTOR, selector);    }    if (!StringUtils.isEmpty(maxBackoff)) {        long millis = Long.parseLong(maxBackoff.trim());        if (millis <= 0) {            throw new FlumeException("Misconfigured max backoff, value must be greater than 0");        }        props.put(RpcClientConfigurationConstants.CONFIG_BACKOFF, String.valueOf(true));        props.put(RpcClientConfigurationConstants.CONFIG_MAX_BACKOFF, maxBackoff);    }    props.setProperty(RpcClientConfigurationConstants.CONFIG_CONNECT_TIMEOUT, String.valueOf(timeout));    props.setProperty(RpcClientConfigurationConstants.CONFIG_REQUEST_TIMEOUT, String.valueOf(timeout));    return props;}
0
private List<Event> parseEvents(LoggingEvent loggingEvent)
{    Map<String, String> headers = new HashMap<>();    headers.put(Log4jAvroHeaders.LOGGER_NAME.toString(), loggingEvent.getLoggerName());    headers.put(Log4jAvroHeaders.TIMESTAMP.toString(), String.valueOf(loggingEvent.timeStamp));    headers.put(Log4jAvroHeaders.ADDRESS.toString(), clientAddress);                headers.put(Log4jAvroHeaders.LOG_LEVEL.toString(), String.valueOf(loggingEvent.getLevel().toInt()));    Map<String, String> headersWithEncoding = null;    Collection<?> messages;    if (loggingEvent.getMessage() instanceof Collection) {        messages = (Collection) loggingEvent.getMessage();    } else {        messages = Collections.singleton(loggingEvent.getMessage());    }    List<Event> events = new LinkedList<>();    for (Object message : messages) {        if (message instanceof GenericRecord) {            GenericRecord record = (GenericRecord) message;            populateAvroHeaders(headers, record.getSchema());            events.add(EventBuilder.withBody(serialize(record, record.getSchema()), headers));        } else if (message instanceof SpecificRecord || avroReflectionEnabled) {            Schema schema = ReflectData.get().getSchema(message.getClass());            populateAvroHeaders(headers, schema);            events.add(EventBuilder.withBody(serialize(message, schema), headers));        } else {            String msg;            if (layout != null) {                LoggingEvent singleLoggingEvent = new LoggingEvent(loggingEvent.getFQNOfLoggerClass(), loggingEvent.getLogger(), loggingEvent.getTimeStamp(), loggingEvent.getLevel(), message, loggingEvent.getThreadName(), loggingEvent.getThrowableInformation(), loggingEvent.getNDC(), loggingEvent.getLocationInformation(), loggingEvent.getProperties());                msg = layout.format(singleLoggingEvent);            } else {                msg = message.toString();            }            if (headersWithEncoding == null) {                headersWithEncoding = new HashMap<>(headers);                headersWithEncoding.put(Log4jAvroHeaders.MESSAGE_ENCODING.toString(), "UTF8");            }            events.add(EventBuilder.withBody(msg, Charset.forName("UTF8"), headersWithEncoding));        }    }    return events;}
0
private byte[] serialize(Object datum, Schema datumSchema) throws FlumeException
{    if (schema == null || !datumSchema.equals(schema)) {        schema = datumSchema;        out = new ByteArrayOutputStream();        writer = new ReflectDatumWriter<>(schema);        encoder = EncoderFactory.get().binaryEncoder(out, null);    }    out.reset();    try {        writer.write(datum, encoder);        encoder.flush();        return out.toByteArray();    } catch (IOException e) {        throw new FlumeException(e);    }}
0
public boolean requiresLayout()
{        return true;}
0
public void setHostname(String hostname)
{    this.hostname = hostname;}
0
public void setPort(int port)
{    this.port = port;}
0
public void setUnsafeMode(boolean unsafeMode)
{    this.unsafeMode = unsafeMode;}
0
public boolean getUnsafeMode()
{    return unsafeMode;}
0
public void setTimeout(long timeout)
{    this.timeout = timeout;}
0
public long getTimeout()
{    return this.timeout;}
0
public void setAvroReflectionEnabled(boolean avroReflectionEnabled)
{    this.avroReflectionEnabled = avroReflectionEnabled;}
0
public void setAvroSchemaUrl(String avroSchemaUrl)
{    this.avroSchemaUrl = avroSchemaUrl;}
0
private void reconnect() throws FlumeException
{    close();    activateOptions();}
0
public String getName()
{    return headerName;}
0
public String toString()
{    return getName();}
0
public static Log4jAvroHeaders getByName(String headerName)
{    Log4jAvroHeaders hdrs = null;    try {        hdrs = Log4jAvroHeaders.valueOf(headerName.toLowerCase(Locale.ENGLISH).trim());    } catch (IllegalArgumentException e) {        hdrs = Log4jAvroHeaders.OTHER;    }    return hdrs;}
0
private static List<Integer> getFreePorts(int numberOfPorts) throws IOException
{    List<Integer> ports = new ArrayList<>(numberOfPorts);    for (int index = 0; index < numberOfPorts; ++index) {        try (ServerSocket socket = new ServerSocket(0)) {            ports.add(socket.getLocalPort());        }    }    return ports;}
0
private static String toHostList(List<Integer> ports)
{    List<String> addresses = new ArrayList<String>(ports.size());    for (Integer port : ports) {        addresses.add("localhost:" + port);    }    String hostList = StringUtils.join(addresses, " ");    return hostList;}
0
public void initiate() throws InterruptedException
{    ch = new MemoryChannel();    configureChannel();}
0
private void configureChannel()
{    Configurables.configure(ch, new Context());    List<Channel> channels = new ArrayList<Channel>();    channels.add(ch);    rcs = new ReplicatingChannelSelector();    rcs.setChannels(channels);}
0
public void cleanUp()
{    for (Source source : sources) {        source.stop();    }}
0
public void testLog4jAppenderRoundRobin() throws IOException
{    int numberOfMsgs = 1000;    int expectedPerSource = 500;    String propertiesFile = "flume-loadbalancinglog4jtest.properties";    startSources(propertiesFile, false, getFreePorts(2));    sendAndAssertMessages(numberOfMsgs);    for (CountingAvroSource source : sources) {        Assert.assertEquals(expectedPerSource, source.appendCount.get());    }}
0
public void testLog4jAppenderRandom() throws IOException
{    int numberOfMsgs = 1000;    String propertiesFile = "flume-loadbalancing-rnd-log4jtest.properties";    startSources(propertiesFile, false, getFreePorts(10));    sendAndAssertMessages(numberOfMsgs);    int total = 0;    Set<Integer> counts = new HashSet<Integer>();    for (CountingAvroSource source : sources) {        total += source.appendCount.intValue();        counts.add(source.appendCount.intValue());    }        Assert.assertTrue("Very unusual distribution " + counts.size(), counts.size() > 2);    Assert.assertTrue("Missing events", total == numberOfMsgs);}
0
public void testRandomBackoff() throws Exception
{    String propertiesFile = "flume-loadbalancing-backoff-log4jtest.properties";    startSources(propertiesFile, false, getFreePorts(3));    sources.get(0).setFail();    sources.get(2).setFail();    sendAndAssertMessages(50);    Assert.assertEquals(50, sources.get(1).appendCount.intValue());    Assert.assertEquals(0, sources.get(0).appendCount.intValue());    Assert.assertEquals(0, sources.get(2).appendCount.intValue());    sources.get(0).setOk();        sources.get(1).setFail();    try {        send(1);                Assert.fail("Expected EventDeliveryException");    } catch (FlumeException e) {        Assert.assertTrue(e.getCause() instanceof EventDeliveryException);    }        Thread.sleep(2500);    sendAndAssertMessages(50);    Assert.assertEquals(50, sources.get(0).appendCount.intValue());    Assert.assertEquals(50, sources.get(1).appendCount.intValue());    Assert.assertEquals(0, sources.get(2).appendCount.intValue());}
0
public void testRandomBackoffUnsafeMode() throws Exception
{    String propertiesFile = "flume-loadbalancing-backoff-log4jtest.properties";    startSources(propertiesFile, true, getFreePorts(3));    sources.get(0).setFail();    sources.get(1).setFail();    sources.get(2).setFail();    sendAndAssertFail();}
0
public void testTimeout() throws Throwable
{    String propertiesFile = "flume-loadbalancinglog4jtest.properties";    ch = new TestLog4jAppender.SlowMemoryChannel(2000);    configureChannel();    slowDown = true;    startSources(propertiesFile, false, getFreePorts(3));    int level = 20000;    String msg = "This is log message number" + String.valueOf(level);    try {        fixture.log(Level.toLevel(level), msg);    } catch (FlumeException ex) {        throw ex.getCause();    }}
0
public void testRandomBackoffNotUnsafeMode() throws Throwable
{    String propertiesFile = "flume-loadbalancing-backoff-log4jtest.properties";    startSources(propertiesFile, false, getFreePorts(3));    sources.get(0).setFail();    sources.get(1).setFail();    sources.get(2).setFail();    try {        sendAndAssertFail();    } catch (FlumeException ex) {        throw ex.getCause();    }}
0
private void send(int numberOfMsgs) throws EventDeliveryException
{    for (int count = 0; count < numberOfMsgs; count++) {        int level = count % 5;        String msg = "This is log message number" + String.valueOf(count);        fixture.log(Level.toLevel(level), msg);    }}
0
private void sendAndAssertFail() throws IOException
{    int level = 20000;    String msg = "This is log message number" + String.valueOf(level);    fixture.log(Level.toLevel(level), msg);    Transaction transaction = ch.getTransaction();    transaction.begin();    Event event = ch.take();    Assert.assertNull(event);    transaction.commit();    transaction.close();}
0
private void sendAndAssertMessages(int numberOfMsgs) throws IOException
{    for (int count = 0; count < numberOfMsgs; count++) {        int level = count % 5;        String msg = "This is log message number" + String.valueOf(count);        fixture.log(Level.toLevel(level), msg);        Transaction transaction = ch.getTransaction();        transaction.begin();        Event event = ch.take();        Assert.assertNotNull(event);        Assert.assertEquals(new String(event.getBody(), "UTF8"), msg);        Map<String, String> hdrs = event.getHeaders();        Assert.assertNotNull(hdrs.get(Log4jAvroHeaders.TIMESTAMP.toString()));        Assert.assertNotNull(hdrs.get(Log4jAvroHeaders.ADDRESS.toString()));        Assert.assertEquals(Level.toLevel(level), Level.toLevel(hdrs.get(Log4jAvroHeaders.LOG_LEVEL.toString())));        Assert.assertEquals(fixture.getName(), hdrs.get(Log4jAvroHeaders.LOGGER_NAME.toString()));        Assert.assertEquals("UTF8", hdrs.get(Log4jAvroHeaders.MESSAGE_ENCODING.toString()));                System.out.println("Got body: " + new String(event.getBody(), "UTF8"));        transaction.commit();        transaction.close();    }}
0
private void startSources(String log4jProps, boolean unsafeMode, List<Integer> ports) throws IOException
{    for (int port : ports) {        CountingAvroSource source = new CountingAvroSource(port);        Context context = new Context();        context.put("port", String.valueOf(port));        context.put("bind", "0.0.0.0");        Configurables.configure(source, context);        sources.add(source);        source.setChannelProcessor(new ChannelProcessor(rcs));    }    for (Source source : sources) {        source.start();    }                        Reader reader = new InputStreamReader(getClass().getResourceAsStream("/" + log4jProps));    Properties props = new Properties();    props.load(reader);    props.setProperty("log4j.appender.out2.Hosts", toHostList(ports));    props.setProperty("log4j.appender.out2.UnsafeMode", String.valueOf(unsafeMode));    if (slowDown) {        props.setProperty("log4j.appender.out2.Timeout", String.valueOf(1000));    }    PropertyConfigurator.configure(props);    fixture = LogManager.getLogger(TestLoadBalancingLog4jAppender.class);}
0
public void setOk()
{    this.isFail = false;}
0
public void setFail()
{    this.isFail = true;}
0
public String getName()
{    return "testing..." + port2;}
0
public Status append(AvroFlumeEvent avroEvent)
{    if (isFail) {        return Status.FAILED;    }    appendCount.incrementAndGet();    return super.append(avroEvent);}
0
public Status appendBatch(List<AvroFlumeEvent> events)
{    if (isFail) {        return Status.FAILED;    }    appendCount.addAndGet(events.size());    return super.appendBatch(events);}
0
private static int getFreePort() throws Exception
{    try (ServerSocket socket = new ServerSocket(0)) {        return socket.getLocalPort();    }}
0
public void initiate() throws Exception
{    int port = getFreePort();    source = Mockito.spy(new AvroSource());    ch = new MemoryChannel();    Configurables.configure(ch, new Context());    Context context = new Context();    context.put("port", String.valueOf(port));    context.put("bind", "localhost");    Configurables.configure(source, context);    File TESTFILE = new File(TestLog4jAppender.class.getClassLoader().getResource("flume-log4jtest.properties").getFile());    FileReader reader = new FileReader(TESTFILE);    props = new Properties();    props.load(reader);    props.put("log4j.appender.out2.Port", String.valueOf(port));    reader.close();}
0
private void configureSource()
{    ChannelSelector rcs = new ReplicatingChannelSelector();    rcs.setChannels(Collections.singletonList(ch));    source.setChannelProcessor(new ChannelProcessor(rcs));    source.start();}
0
public void testLog4jAppender() throws IOException
{    configureSource();    PropertyConfigurator.configure(props);    Logger logger = LogManager.getLogger(TestLog4jAppender.class);    for (int count = 0; count <= 1000; count++) {        /*       * Log4j internally defines levels as multiples of 10000. So if we       * create levels directly using count, the level will be set as the       * default.       */        int level = ((count % 5) + 1) * 10000;        String msg = "This is log message number" + String.valueOf(count);        logger.log(Level.toLevel(level), msg);        Transaction transaction = ch.getTransaction();        transaction.begin();        Event event = ch.take();        Assert.assertNotNull(event);        Assert.assertEquals(new String(event.getBody(), "UTF8"), msg);        Map<String, String> hdrs = event.getHeaders();        Assert.assertNotNull(hdrs.get(Log4jAvroHeaders.TIMESTAMP.toString()));        Assert.assertEquals(Level.toLevel(level), Level.toLevel(Integer.valueOf(hdrs.get(Log4jAvroHeaders.LOG_LEVEL.toString()))));        Assert.assertNotNull(hdrs.get(Log4jAvroHeaders.ADDRESS.toString()));        Assert.assertEquals(logger.getName(), hdrs.get(Log4jAvroHeaders.LOGGER_NAME.toString()));        Assert.assertEquals("UTF8", hdrs.get(Log4jAvroHeaders.MESSAGE_ENCODING.toString()));        transaction.commit();        transaction.close();    }}
0
private void testBatchedSending(int numEvents)
{    configureSource();    PropertyConfigurator.configure(props);    Logger logger = LogManager.getLogger(getClass());    List<String> events = IntStream.range(0, numEvents).mapToObj(String::valueOf).collect(Collectors.toList());        Transaction tx = ch.getTransaction();    tx.begin();    for (String s : events) {        Event e = ch.take();        Assert.assertNotNull(e);        Assert.assertEquals(s, new String(e.getBody()));    }    Assert.assertNull("There should be no more events in the channel", ch.take());}
1
public void testLogBatch()
{    testBatchedSending(5);    Mockito.verify(source, Mockito.times(1)).appendBatch(Mockito.anyList());    Mockito.verify(source, Mockito.times(0)).append(Mockito.any(AvroFlumeEvent.class));}
0
public void testLogSingleMessage()
{    configureSource();    PropertyConfigurator.configure(props);    Logger logger = LogManager.getLogger(getClass());        Transaction tx = ch.getTransaction();    tx.begin();    Event e = ch.take();    Assert.assertNotNull(e);    Assert.assertEquals("test", new String(e.getBody()));    Mockito.verify(source, Mockito.times(0)).appendBatch(Mockito.anyList());    Mockito.verify(source, Mockito.times(1)).append(Mockito.any(AvroFlumeEvent.class));}
1
public void testLogSingleMessageInCollection()
{    testBatchedSending(1);    Mockito.verify(source, Mockito.times(0)).appendBatch(Mockito.anyList());    Mockito.verify(source, Mockito.times(1)).append(Mockito.any(AvroFlumeEvent.class));}
0
public void testLogEmptyBatch()
{    testBatchedSending(0);    Mockito.verify(source, Mockito.times(0)).appendBatch(Mockito.anyList());    Mockito.verify(source, Mockito.times(0)).append(Mockito.any(AvroFlumeEvent.class));}
0
public void testLog4jAppenderFailureUnsafeMode() throws Throwable
{    configureSource();    props.setProperty("log4j.appender.out2.UnsafeMode", String.valueOf(true));    PropertyConfigurator.configure(props);    Logger logger = LogManager.getLogger(TestLog4jAppender.class);    source.stop();    sendAndAssertFail(logger);}
0
public void testLog4jAppenderFailureNotUnsafeMode() throws Throwable
{    configureSource();    PropertyConfigurator.configure(props);    Logger logger = LogManager.getLogger(TestLog4jAppender.class);    source.stop();    sendAndAssertFail(logger);}
0
private void sendAndAssertFail(Logger logger) throws Throwable
{    /*     * Log4j internally defines levels as multiples of 10000. So if we     * create levels directly using count, the level will be set as the     * default.     */    int level = 20000;    try {        logger.log(Level.toLevel(level), "Test Msg");    } catch (FlumeException ex) {        ex.printStackTrace();        throw ex.getCause();    }    Transaction transaction = ch.getTransaction();    transaction.begin();    Event event = ch.take();    Assert.assertNull(event);    transaction.commit();    transaction.close();}
0
public void testLayout() throws IOException
{    configureSource();    props.put("log4j.appender.out2.layout", "org.apache.log4j.PatternLayout");    props.put("log4j.appender.out2.layout.ConversionPattern", "%-5p [%t]: %m%n");    PropertyConfigurator.configure(props);    Logger logger = LogManager.getLogger(TestLog4jAppender.class);    Thread.currentThread().setName("Log4jAppenderTest");    for (int count = 0; count <= 100; count++) {        /*       * Log4j internally defines levels as multiples of 10000. So if we       * create levels directly using count, the level will be set as the       * default.       */        int level = ((count % 5) + 1) * 10000;        String msg = "This is log message number" + String.valueOf(count);        logger.log(Level.toLevel(level), msg);        Transaction transaction = ch.getTransaction();        transaction.begin();        Event event = ch.take();        Assert.assertNotNull(event);        StringBuilder builder = new StringBuilder();        builder.append("[").append("Log4jAppenderTest").append("]: ").append(msg);                String eventBody = new String(event.getBody(), "UTF-8");        String eventLevel = eventBody.split("\\s+")[0];        Assert.assertEquals(Level.toLevel(level).toString(), eventLevel);        Assert.assertEquals(new String(event.getBody(), "UTF8").trim().substring(eventLevel.length()).trim(), builder.toString());        Map<String, String> hdrs = event.getHeaders();        Assert.assertNotNull(hdrs.get(Log4jAvroHeaders.TIMESTAMP.toString()));        Assert.assertEquals(Level.toLevel(level), Level.toLevel(Integer.parseInt(hdrs.get(Log4jAvroHeaders.LOG_LEVEL.toString()))));        Assert.assertEquals(logger.getName(), hdrs.get(Log4jAvroHeaders.LOGGER_NAME.toString()));        Assert.assertEquals("UTF8", hdrs.get(Log4jAvroHeaders.MESSAGE_ENCODING.toString()));        transaction.commit();        transaction.close();    }}
0
public void testSlowness() throws Throwable
{    ch = new SlowMemoryChannel(2000);    Configurables.configure(ch, new Context());    configureSource();    props.put("log4j.appender.out2.Timeout", "1000");    props.put("log4j.appender.out2.layout", "org.apache.log4j.PatternLayout");    props.put("log4j.appender.out2.layout.ConversionPattern", "%-5p [%t]: %m%n");    PropertyConfigurator.configure(props);    Logger logger = LogManager.getLogger(TestLog4jAppender.class);    Thread.currentThread().setName("Log4jAppenderTest");    int level = 10000;    String msg = "This is log message number" + String.valueOf(1);    try {        logger.log(Level.toLevel(level), msg);    } catch (FlumeException ex) {        throw ex.getCause();    }}
0
public void testSlownessUnsafeMode() throws Throwable
{    props.setProperty("log4j.appender.out2.UnsafeMode", String.valueOf(true));    testSlowness();}
0
public void cleanUp()
{    source.stop();    ch.stop();    props.clear();}
0
public void put(Event e)
{    try {        TimeUnit.MILLISECONDS.sleep(slowTime);    } catch (Exception ex) {        throw new RuntimeException(ex);    }    super.put(e);}
0
private static int getFreePort() throws Exception
{    try (ServerSocket socket = new ServerSocket(0)) {        return socket.getLocalPort();    }}
0
public void setUp() throws Exception
{    URL schemaUrl = getClass().getClassLoader().getResource("myrecord.avsc");    Files.copy(Resources.newInputStreamSupplier(schemaUrl), new File("/tmp/myrecord.avsc"));    port = getFreePort();    source = new AvroSource();    ch = new MemoryChannel();    Configurables.configure(ch, new Context());    Context context = new Context();    context.put("port", String.valueOf(port));    context.put("bind", "localhost");    Configurables.configure(source, context);    List<Channel> channels = new ArrayList<>();    channels.add(ch);    ChannelSelector rcs = new ReplicatingChannelSelector();    rcs.setChannels(channels);    source.setChannelProcessor(new ChannelProcessor(rcs));    source.start();}
0
private void loadProperties(String file) throws IOException
{    File TESTFILE = new File(TestLog4jAppenderWithAvro.class.getClassLoader().getResource(file).getFile());    FileReader reader = new FileReader(TESTFILE);    props = new Properties();    props.load(reader);    reader.close();}
0
public void testAvroGeneric() throws IOException
{    loadProperties("flume-log4jtest-avro-generic.properties");    props.put("log4j.appender.out2.Port", String.valueOf(port));    PropertyConfigurator.configure(props);    Logger logger = LogManager.getLogger(TestLog4jAppenderWithAvro.class);    String msg = "This is log message number " + String.valueOf(0);    Schema schema = new Schema.Parser().parse(getClass().getClassLoader().getResource("myrecord.avsc").openStream());    GenericRecordBuilder builder = new GenericRecordBuilder(schema);    GenericRecord record = builder.set("message", msg).build();        Transaction transaction = ch.getTransaction();    transaction.begin();    Event event = ch.take();    Assert.assertNotNull(event);    GenericDatumReader<GenericRecord> reader = new GenericDatumReader<>(schema);    BinaryDecoder decoder = DecoderFactory.get().binaryDecoder(event.getBody(), null);    GenericRecord recordFromEvent = reader.read(null, decoder);    Assert.assertEquals(msg, recordFromEvent.get("message").toString());    Map<String, String> hdrs = event.getHeaders();    Assert.assertNull(hdrs.get(Log4jAvroHeaders.MESSAGE_ENCODING.toString()));    Assert.assertEquals("Schema URL should be set", "file:///tmp/myrecord.avsc", hdrs.get(Log4jAvroHeaders.AVRO_SCHEMA_URL.toString()));    Assert.assertNull("Schema string should not be set", hdrs.get(Log4jAvroHeaders.AVRO_SCHEMA_LITERAL.toString()));    transaction.commit();    transaction.close();}
1
public void testAvroReflect() throws IOException
{    loadProperties("flume-log4jtest-avro-reflect.properties");    props.put("log4j.appender.out2.Port", String.valueOf(port));    PropertyConfigurator.configure(props);    Logger logger = LogManager.getLogger(TestLog4jAppenderWithAvro.class);    String msg = "This is log message number " + String.valueOf(0);    AppEvent appEvent = new AppEvent();    appEvent.setMessage(msg);        Transaction transaction = ch.getTransaction();    transaction.begin();    Event event = ch.take();    Assert.assertNotNull(event);    Schema schema = ReflectData.get().getSchema(appEvent.getClass());    ReflectDatumReader<AppEvent> reader = new ReflectDatumReader<>(AppEvent.class);    BinaryDecoder decoder = DecoderFactory.get().binaryDecoder(event.getBody(), null);    AppEvent recordFromEvent = reader.read(null, decoder);    Assert.assertEquals(msg, recordFromEvent.getMessage());    Map<String, String> hdrs = event.getHeaders();    Assert.assertNull(hdrs.get(Log4jAvroHeaders.MESSAGE_ENCODING.toString()));    Assert.assertNull("Schema URL should not be set", hdrs.get(Log4jAvroHeaders.AVRO_SCHEMA_URL.toString()));    Assert.assertEquals("Schema string should be set", schema.toString(), hdrs.get(Log4jAvroHeaders.AVRO_SCHEMA_LITERAL.toString()));    transaction.commit();    transaction.close();}
1
public void testDifferentEventTypesInBatchWithAvroReflect() throws IOException
{    loadProperties("flume-log4jtest-avro-reflect.properties");    props.put("log4j.appender.out2.Port", String.valueOf(port));    PropertyConfigurator.configure(props);    Logger logger = LogManager.getLogger(getClass());    List<Object> events = Arrays.asList("string", new AppEvent("appEvent"));        Transaction transaction = ch.getTransaction();    transaction.begin();    for (Object o : events) {        Event e = ch.take();        Assert.assertNotNull(e);        ReflectDatumReader<?> reader = new ReflectDatumReader<>(o.getClass());        BinaryDecoder decoder = DecoderFactory.get().binaryDecoder(e.getBody(), null);        Object readObject = reader.read(null, decoder);        Assert.assertEquals(o, readObject);        Map<String, String> hdrs = e.getHeaders();        Assert.assertNull(hdrs.get(Log4jAvroHeaders.MESSAGE_ENCODING.toString()));        Assert.assertNull("Schema URL should not be set", hdrs.get(Log4jAvroHeaders.AVRO_SCHEMA_URL.toString()));        Assert.assertEquals("Schema string should be set", ReflectData.get().getSchema(readObject.getClass()).toString(), hdrs.get(Log4jAvroHeaders.AVRO_SCHEMA_LITERAL.toString()));    }    Assert.assertNull("There should be no more events in the channel", ch.take());}
1
public void testDifferentEventTypesInBatchWithAvroGeneric() throws IOException
{    loadProperties("flume-log4jtest-avro-generic.properties");    props.put("log4j.appender.out2.Port", String.valueOf(port));    PropertyConfigurator.configure(props);    Logger logger = LogManager.getLogger(getClass());    String msg = "Avro log message";    Schema schema = new Schema.Parser().parse(getClass().getClassLoader().getResource("myrecord.avsc").openStream());    GenericRecordBuilder builder = new GenericRecordBuilder(schema);    GenericRecord record = builder.set("message", msg).build();    List<Object> events = Arrays.asList("string", record);        Transaction transaction = ch.getTransaction();    transaction.begin();    Event event = ch.take();    Assert.assertNotNull(event);    Assert.assertEquals("string", new String(event.getBody()));    event = ch.take();    Assert.assertNotNull(event);    GenericDatumReader<GenericRecord> reader = new GenericDatumReader<>(schema);    BinaryDecoder decoder = DecoderFactory.get().binaryDecoder(event.getBody(), null);    GenericRecord recordFromEvent = reader.read(null, decoder);    Assert.assertEquals(msg, recordFromEvent.get("message").toString());    Map<String, String> hdrs = event.getHeaders();    Assert.assertNull(hdrs.get(Log4jAvroHeaders.MESSAGE_ENCODING.toString()));    Assert.assertEquals("Schema URL should be set", "file:///tmp/myrecord.avsc", hdrs.get(Log4jAvroHeaders.AVRO_SCHEMA_URL.toString()));    Assert.assertNull("Schema string should not be set", hdrs.get(Log4jAvroHeaders.AVRO_SCHEMA_LITERAL.toString()));    transaction.commit();    transaction.close();}
1
public void cleanUp()
{    source.stop();    ch.stop();    props.clear();}
0
public String getMessage()
{    return message;}
0
public void setMessage(String message)
{    this.message = message;}
0
public boolean equals(Object o)
{    if (this == o)        return true;    if (o == null || getClass() != o.getClass())        return false;    AppEvent appEvent = (AppEvent) o;    return message != null ? message.equals(appEvent.message) : appEvent.message == null;}
0
public int hashCode()
{    return message != null ? message.hashCode() : 0;}
0
public void setName(String name)
{    this.name = name;}
0
public String getName()
{    return name;}
0
public String filter(String key)
{    return System.getenv(key);}
0
public void initializeWithConfiguration(Map<String, String> configuration)
{}
0
public void filter()
{    environmentVariables.set(MY_PASSWORD_KEY, FILTERED);    environmentVariables.set(MY_PASSWORD_KEY_2, FILTERED_2);    ConfigFilter configFilter = new EnvironmentVariableConfigFilter();    assertEquals(FILTERED, configFilter.filter(MY_PASSWORD_KEY));    assertEquals(FILTERED_2, configFilter.filter(MY_PASSWORD_KEY_2));}
0
public void filterUnknownKey()
{    ConfigFilter configFilter = new EnvironmentVariableConfigFilter();    assertNull(configFilter.filter("unknown"));}
0
public String filter(String key)
{    try {        return execCommand(key);    } catch (InterruptedException | IllegalStateException | IOException ex) {            }    return null;}
1
public void initializeWithConfiguration(Map<String, String> configuration)
{    String charsetName = configuration.getOrDefault(CHARSET_KEY, CHARSET_DEFAULT);    try {        charset = Charset.forName(charsetName);    } catch (UnsupportedCharsetException ex) {        throw new RuntimeException("Unsupported charset: " + charsetName, ex);    }    command = configuration.get(COMMAND_KEY);    if (command == null) {        throw new IllegalArgumentException(COMMAND_KEY + " must be set for " + "ExternalProcessConfigFilter");    }}
0
private String execCommand(String key) throws IOException, InterruptedException
{    String[] split = command.split("\\s+");    int newLength = split.length + 1;    String[] commandParts = Arrays.copyOf(split, newLength);    commandParts[newLength - 1] = key;    Process p = Runtime.getRuntime().exec(commandParts);    p.waitFor();    if (p.exitValue() != 0) {        String stderr;        try {            stderr = getResultFromStream(p.getErrorStream());        } catch (Throwable t) {            stderr = null;        }        throw new IllegalStateException(String.format("Process (%s) exited with non-zero (%s) status code. Sterr: %s", this.command, p.exitValue(), stderr));    }    return getResultFromStream(p.getInputStream());}
0
private String getResultFromStream(InputStream inputStream)
{    try (Scanner scanner = new Scanner(inputStream, charset.name())) {        String result = null;        if (scanner.hasNextLine()) {            result = scanner.nextLine();            if (scanner.hasNextLine()) {                            }        } else {                    }        return result;    }}
1
public void setUp()
{    configFilter = new ExternalProcessConfigFilter();}
0
public void filterOk()
{    String file = Thread.currentThread().getContextClassLoader().getResource("test.sh").getFile();    File testExecutable = new File(file);    testExecutable.setExecutable(true);    HashMap<String, String> configuration = new HashMap<>();    configuration.put("command", file);    configFilter.initializeWithConfiguration(configuration);    assertEquals(FILTERED, configFilter.filter(MY_PASSWORD_KEY));    assertEquals(FILTERED_2, configFilter.filter(MY_PASSWORD_KEY_2));}
0
public void filterError()
{    String file = Thread.currentThread().getContextClassLoader().getResource("test_error.sh").getFile();    File testExecutable = new File(file);    testExecutable.setExecutable(true);    HashMap<String, String> configuration = new HashMap<>();    configuration.put("command", file);    configFilter.initializeWithConfiguration(configuration);    assertNull(configFilter.filter(MY_PASSWORD_KEY));}
0
public void initializeWithConfiguration(Map<String, String> configuration)
{        hadoopConfiguration = new Configuration();    hadoopConfiguration.set(HADOOP_SECURITY + CREDENTIAL_PROVIDER_PATH, configuration.get(CREDENTIAL_PROVIDER_PATH));    String passwordFile = configuration.get(PASSWORD_FILE_CONFIG_KEY);    if (passwordFile != null && !passwordFile.isEmpty()) {        checkPasswordFile(passwordFile);        hadoopConfiguration.set(HADOOP_SECURITY + PASSWORD_FILE_CONFIG_KEY, passwordFile);    }}
1
private void checkPasswordFile(String passwordFile)
{    if (Thread.currentThread().getContextClassLoader().getResource(passwordFile) == null) {            }}
1
public String filter(String key)
{    char[] result = null;    try {        result = hadoopConfiguration.getPassword(key);    } catch (IOException e) {            }    return result == null ? null : new String(result);}
1
public static void setUpClass() throws Exception
{    generateTempFileNames();    fillCredStoreWithDefaultPassword();    fillCredStoreWithPasswordFile();    fillCredStoreWithEnvironmentVariablePassword();}
0
public static void tearDown()
{    fileDefault.deleteOnExit();    fileEnvPassword.deleteOnExit();    fileFilePassword.deleteOnExit();}
0
public void setUp()
{    String[] objects = System.getenv().keySet().toArray(new String[0]);    environmentVariables.clear(objects);    configFilter = new HadoopCredentialStoreConfigFilter();}
0
public void filterDefaultPasswordFile()
{    HashMap<String, String> configuration = new HashMap<>();    configuration.put(CREDENTIAL_PROVIDER_PATH, providerPathDefault);    configFilter.initializeWithConfiguration(configuration);    assertEquals("filtered_default", configFilter.filter("password"));}
0
public void filterWithEnvPassword()
{    environmentVariables.set("HADOOP_CREDSTORE_PASSWORD", "envSecret");    HashMap<String, String> configuration = new HashMap<>();    configuration.put(CREDENTIAL_PROVIDER_PATH, providerPathEnv);    configFilter.initializeWithConfiguration(configuration);    assertEquals("filtered_env", configFilter.filter("password"));}
0
public void filterWithPasswordFile()
{    HashMap<String, String> configuration = new HashMap<>();    configuration.put(CREDENTIAL_PROVIDER_PATH, providerPathPwdFile);    configuration.put(PASSWORD_FILE_CONFIG_KEY, "test-password.txt");    configFilter.initializeWithConfiguration(configuration);    assertEquals("filtered_file", configFilter.filter("password"));}
0
public void filterWithEnvNoPassword()
{    HashMap<String, String> configuration = new HashMap<>();    configuration.put(CREDENTIAL_PROVIDER_PATH, providerPathEnv);    configFilter.initializeWithConfiguration(configuration);    assertNull(configFilter.filter("password"));}
0
public void filterErrorWithPasswordFileWrongPassword()
{    HashMap<String, String> configuration = new HashMap<>();    configuration.put(CREDENTIAL_PROVIDER_PATH, providerPathPwdFile);    configuration.put(PASSWORD_FILE_CONFIG_KEY, "test-password2.txt");    configFilter.initializeWithConfiguration(configuration);    assertNull(configFilter.filter("password"));}
0
public void filterErrorWithPasswordFileNoPasswordFile()
{    HashMap<String, String> configuration = new HashMap<>();    configuration.put(CREDENTIAL_PROVIDER_PATH, providerPathPwdFile);    configFilter.initializeWithConfiguration(configuration);    assertNull(configFilter.filter("password"));}
0
public void filterErrorWithNoProvider()
{    HashMap<String, String> configuration = new HashMap<>();    configFilter.initializeWithConfiguration(configuration);}
0
private static void fillCredStoreWithEnvironmentVariablePassword() throws Exception
{    environmentVariables.set("HADOOP_CREDSTORE_PASSWORD", "envSecret");    runCommand("create password -value filtered_env -provider " + providerPathEnv, new Configuration());}
0
private static void fillCredStoreWithPasswordFile() throws Exception
{    Configuration conf = new Configuration();    conf.set(HADOOP_SECURITY + PASSWORD_FILE_CONFIG_KEY, "test-password.txt");    runCommand("create password -value filtered_file -provider " + providerPathPwdFile, conf);}
0
private static void fillCredStoreWithDefaultPassword() throws Exception
{    runCommand("create password -value filtered_default -provider " + providerPathDefault, new Configuration());}
0
private static void generateTempFileNames() throws IOException
{    fileDefault = Files.createTempFile("test-default-pwd-", ".jceks").toFile();    boolean deleted = fileDefault.delete();    fileEnvPassword = Files.createTempFile("test-env-pwd-", ".jceks").toFile();    deleted &= fileEnvPassword.delete();    fileFilePassword = Files.createTempFile("test-file-pwd-", ".jceks").toFile();    deleted &= fileFilePassword.delete();    if (!deleted) {        fail("Could not delete temporary files");    }    providerPathDefault = "jceks://file/" + fileDefault.getAbsolutePath();    providerPathEnv = "jceks://file/" + fileEnvPassword.getAbsolutePath();    providerPathPwdFile = "jceks://file/" + fileFilePassword.getAbsolutePath();}
0
private static void runCommand(String c, Configuration conf) throws Exception
{    ToolRunner.run(conf, new CredentialShell(), c.split(" "));}
0
public String getChannelConfigurationType()
{    return channelConfigurationType;}
0
public ChannelConfiguration getConfiguration(String name) throws ConfigurationException
{    if (this == OTHER) {        return new ChannelConfiguration(name);    }    Class<? extends ChannelConfiguration> clazz;    ChannelConfiguration instance = null;    try {        if (channelConfigurationType != null) {            clazz = (Class<? extends ChannelConfiguration>) Class.forName(channelConfigurationType);            instance = clazz.getConstructor(String.class).newInstance(name);        } else {            return new ChannelConfiguration(name);        }    } catch (ClassNotFoundException e) {                instance = new ChannelConfiguration(name);                instance.setNotFoundConfigClass();    } catch (Exception e) {        throw new ConfigurationException(e);    }    return instance;}
0
public Set<String> getChannels()
{    return channelNames;}
0
public void setChannels(Set<String> channelNames)
{    this.channelNames = channelNames;}
0
public String getChannelSelectorConfigurationType()
{    return this.selectorType;}
0
public ChannelSelectorConfiguration getConfiguration(String name) throws ConfigurationException
{    if (this == OTHER) {        return new ChannelSelectorConfiguration(name);    }    Class<? extends ChannelSelectorConfiguration> clazz;    ChannelSelectorConfiguration instance = null;    try {                if (this.selectorType != null) {            clazz = (Class<? extends ChannelSelectorConfiguration>) Class.forName(this.selectorType);            instance = clazz.getConstructor(String.class).newInstance(name);        } else {            return new ChannelSelectorConfiguration(name);        }    } catch (ClassNotFoundException e) {                instance = new ChannelSelectorConfiguration(name);                instance.setNotFoundConfigClass();    } catch (Exception e) {        throw new ConfigurationException("Configuration error!", e);    }    return instance;}
0
public String getChannelSelectorClassName()
{    return channelSelectorClassName;}
0
public String getClassName()
{    return channelSelectorClassName;}
0
public String getChannelClassName()
{    return channelClassName;}
0
public String getClassName()
{    return channelClassName;}
0
public boolean isNotFoundConfigClass()
{    return notFoundConfigClass;}
0
public void setNotFoundConfigClass()
{    this.notFoundConfigClass = true;}
0
public List<FlumeConfigurationError> getErrors()
{    return errors;}
0
public void configure(Context context) throws ConfigurationException
{    failIfConfigured();    String confType = context.getString(BasicConfigurationConstants.CONFIG_TYPE);    if (confType != null && !confType.isEmpty()) {        this.type = confType;    }        if (this.type == null || this.type.isEmpty()) {        errors.add(new FlumeConfigurationError(componentName, BasicConfigurationConstants.CONFIG_TYPE, FlumeConfigurationErrorType.ATTRS_MISSING, ErrorOrWarning.ERROR));        throw new ConfigurationException("Component has no type. Cannot configure. " + componentName);    }}
0
protected void failIfConfigured() throws ConfigurationException
{    if (configured) {        throw new ConfigurationException("Already configured component." + componentName);    }}
0
public String getType()
{    return type;}
0
public void setType(String type)
{    this.type = type;}
0
public String toString()
{    return toString(0);}
0
public String toString(int indentCount)
{    StringBuilder indentSb = new StringBuilder();    for (int i = 0; i < indentCount; i++) {        indentSb.append(FlumeConfiguration.INDENTSTEP);    }    String indent = indentSb.toString();    StringBuilder sb = new StringBuilder(indent);    sb.append("ComponentConfiguration[").append(componentName).append("]");    sb.append(FlumeConfiguration.NEWLINE).append(indent).append(FlumeConfiguration.INDENTSTEP).append("CONFIG: ");    sb.append(FlumeConfiguration.NEWLINE).append(indent).append(FlumeConfiguration.INDENTSTEP);    return sb.toString();}
0
public String getComponentName()
{    return componentName;}
0
protected void setConfigured()
{    configured = true;}
0
public String getComponentType()
{    return componentType;}
0
public static ComponentConfiguration create(String name, String type, ComponentType component) throws ConfigurationException
{    Class<? extends ComponentConfiguration> confType = null;    if (type == null) {        throw new ConfigurationException("Cannot create component without knowing its type!");    }    try {        confType = (Class<? extends ComponentConfiguration>) Class.forName(type);        return confType.getConstructor(String.class).newInstance(type);    } catch (Exception ignored) {        try {            type = type.toUpperCase(Locale.ENGLISH);            switch(component) {                case SOURCE:                    return SourceConfigurationType.valueOf(type.toUpperCase(Locale.ENGLISH)).getConfiguration(name);                case CONFIG_FILTER:                    return ConfigFilterConfigurationType.valueOf(type.toUpperCase(Locale.ENGLISH)).getConfiguration(name);                case SINK:                    return SinkConfigurationType.valueOf(type.toUpperCase(Locale.ENGLISH)).getConfiguration(name);                case CHANNEL:                    return ChannelConfigurationType.valueOf(type.toUpperCase(Locale.ENGLISH)).getConfiguration(name);                case SINK_PROCESSOR:                    return SinkProcessorConfigurationType.valueOf(type.toUpperCase(Locale.ENGLISH)).getConfiguration(name);                case CHANNELSELECTOR:                    return ChannelSelectorConfigurationType.valueOf(type.toUpperCase(Locale.ENGLISH)).getConfiguration(name);                case SINKGROUP:                    return new SinkGroupConfiguration(name);                default:                    throw new ConfigurationException("Cannot create configuration. Unknown Type specified: " + type);            }        } catch (ConfigurationException e) {            throw e;        } catch (Exception e) {            throw new ConfigurationException("Could not create configuration! " + " Due to " + e.getClass().getSimpleName() + ": " + e.getMessage(), e);        }    }}
0
public String getConfigFilterConfigurationType()
{    return configurationName;}
0
public ConfigFilterConfiguration getConfiguration(String name) throws ConfigurationException
{    if (this == OTHER) {        return new ConfigFilterConfiguration(name);    }    Class<? extends ConfigFilterConfiguration> clazz;    ConfigFilterConfiguration instance = null;    try {        if (configurationName != null) {            clazz = (Class<? extends ConfigFilterConfiguration>) Class.forName(configurationName);            instance = clazz.getConstructor(String.class).newInstance(name);        } else {            return new ConfigFilterConfiguration(name);        }    } catch (ClassNotFoundException e) {                instance = new ConfigFilterConfiguration(name);                instance.setNotFoundConfigClass();    } catch (Exception e) {        throw new ConfigurationException("Couldn't create configuration", e);    }    return instance;}
0
public String getClassName()
{    return className;}
0
public static ConfigFilter create(String name, String type) throws FlumeException
{    Preconditions.checkNotNull(name, "name");    Preconditions.checkNotNull(type, "type");        Class<? extends ConfigFilter> aClass = getClass(type);    try {        ConfigFilter configFilter = aClass.newInstance();        configFilter.setName(name);        return configFilter;    } catch (Exception ex) {        throw new FlumeException("Unable to create configfilter: " + name + ", type: " + type + ", class: " + aClass.getName(), ex);    }}
1
public static Class<? extends ConfigFilter> getClass(String type) throws FlumeException
{    String classname = type;    ConfigFilterType srcType = ConfigFilterType.OTHER;    try {        srcType = ConfigFilterType.valueOf(type.toUpperCase(Locale.ENGLISH));    } catch (IllegalArgumentException ex) {            }    if (srcType != ConfigFilterType.OTHER) {        classname = srcType.getClassName();    }    try {        return (Class<? extends ConfigFilter>) Class.forName(classname);    } catch (Exception ex) {        throw new FlumeException("Unable to load configfilter type: " + type + ", class: " + classname, ex);    }}
1
public List<FlumeConfigurationError> getConfigurationErrors()
{    return errors;}
0
public AgentConfiguration getConfigurationFor(String hostname)
{    return agentConfigMap.get(hostname);}
0
private void validateConfiguration()
{    Set<Entry<String, AgentConfiguration>> entries = agentConfigMap.entrySet();    Iterator<Entry<String, AgentConfiguration>> it = entries.iterator();    while (it.hasNext()) {        Entry<String, AgentConfiguration> next = it.next();        String agentName = next.getKey();        AgentConfiguration aconf = next.getValue();        if (!aconf.isValid()) {                        addError(agentName, AGENT_CONFIGURATION_INVALID, ERROR);            it.remove();        }                            }    }
1
private boolean addRawProperty(String rawName, String rawValue)
{        if (rawName == null || rawValue == null) {        addError("", AGENT_NAME_MISSING, ERROR);        return false;    }        String name = rawName.trim();    String value = rawValue.trim();        if (value.isEmpty()) {        addError(name, PROPERTY_VALUE_NULL, ERROR);        return false;    }    int index = name.indexOf('.');        if (index == -1) {        addError(name, AGENT_NAME_MISSING, ERROR);        return false;    }    String agentName = name.substring(0, index);        if (agentName.isEmpty()) {        addError(name, AGENT_NAME_MISSING, ERROR);        return false;    }    String configKey = name.substring(index + 1);        if (configKey.isEmpty()) {        addError(name, PROPERTY_NAME_NULL, ERROR);        return false;    }    AgentConfiguration aconf = agentConfigMap.get(agentName);    if (aconf == null) {        aconf = new AgentConfiguration(agentName, errors);        agentConfigMap.put(agentName, aconf);    }        return aconf.addProperty(configKey, value);}
0
private void addError(String component, FlumeConfigurationErrorType errorType, ErrorOrWarning level)
{    errors.add(new FlumeConfigurationError(component, "", errorType, level));}
0
public Map<String, ComponentConfiguration> getChannelConfigMap()
{    return channelConfigMap;}
0
public Map<String, ComponentConfiguration> getSourceConfigMap()
{    return sourceConfigMap;}
0
public Map<String, ComponentConfiguration> getConfigFilterConfigMap()
{    return configFilterConfigMap;}
0
public Map<String, ComponentConfiguration> getSinkConfigMap()
{    return sinkConfigMap;}
0
public Map<String, ComponentConfiguration> getSinkGroupConfigMap()
{    return sinkgroupConfigMap;}
0
public Map<String, Context> getConfigFilterContext()
{    return configFilterContextMap;}
0
public Map<String, Context> getSourceContext()
{    return sourceContextMap;}
0
public Map<String, Context> getSinkContext()
{    return sinkContextMap;}
0
public Map<String, Context> getChannelContext()
{    return channelContextMap;}
0
public Set<String> getSinkSet()
{    return sinkSet;}
0
public Set<String> getConfigFilterSet()
{    return configFilterSet;}
0
public Set<String> getSourceSet()
{    return sourceSet;}
0
public Set<String> getChannelSet()
{    return channelSet;}
0
public Set<String> getSinkgroupSet()
{    return sinkgroupSet;}
0
private boolean isValid()
{        if (LOGGER.isDebugEnabled() && LogPrivacyUtil.allowLogPrintConfig()) {            }    configFilterSet = validateConfigFilterSet();    createConfigFilters();    runFiltersThroughConfigs();        if (channels == null || channels.trim().isEmpty()) {                addError(CONFIG_CHANNELS, PROPERTY_VALUE_NULL, ERROR);        return false;    }    channelSet = new HashSet<>(Arrays.asList(channels.split("\\s+")));    channelSet = validateChannels(channelSet);    if (channelSet.isEmpty()) {                addError(CONFIG_CHANNELS, PROPERTY_VALUE_NULL, ERROR);        return false;    }    sourceSet = validateSources(channelSet);    sinkSet = validateSinks(channelSet);    sinkgroupSet = validateGroups(sinkSet);        if (sourceSet.isEmpty() && sinkSet.isEmpty()) {                addError(CONFIG_SOURCES, PROPERTY_VALUE_NULL, ERROR);        addError(CONFIG_SINKS, PROPERTY_VALUE_NULL, ERROR);        return false;    }        this.configFilters = getSpaceDelimitedList(configFilterSet);    sources = getSpaceDelimitedList(sourceSet);    channels = getSpaceDelimitedList(channelSet);    sinks = getSpaceDelimitedList(sinkSet);    sinkgroups = getSpaceDelimitedList(sinkgroupSet);    if (LOGGER.isDebugEnabled() && LogPrivacyUtil.allowLogPrintConfig()) {                    }    return true;}
1
private void runFiltersThroughConfigs()
{    runFiltersOnContextMaps(sourceContextMap, channelContextMap, sinkContextMap, sinkGroupContextMap);}
0
private void runFiltersOnContextMaps(Map<String, Context>... maps)
{    for (Map<String, Context> map : maps) {        for (Context context : map.values()) {            for (String key : context.getParameters().keySet()) {                filterValue(context, key);            }        }    }}
0
private void createConfigFilters()
{    for (String name : configFilterSet) {        Context context = configFilterContextMap.get(name);        ComponentConfiguration componentConfiguration = configFilterConfigMap.get(name);        try {            if (context != null) {                ConfigFilter configFilter = ConfigFilterFactory.create(name, context.getString(BasicConfigurationConstants.CONFIG_TYPE));                configFilter.initializeWithConfiguration(context.getParameters());                configFiltersInstances.add(configFilter);                configFilterPatternCache.put(configFilter.getName(), createConfigFilterPattern(configFilter));            } else if (componentConfiguration != null) {                ConfigFilter configFilter = ConfigFilterFactory.create(componentConfiguration.getComponentName(), componentConfiguration.getType());                configFiltersInstances.add(configFilter);                configFilterPatternCache.put(configFilter.getName(), createConfigFilterPattern(configFilter));            }        } catch (Exception e) {                    }    }}
1
private Pattern createConfigFilterPattern(ConfigFilter configFilter)
{        return Pattern.compile(    "\\$\\{" +     Pattern.quote(configFilter.getName()) +     "\\[(|'|\")" +     "(?<key>[-_a-zA-Z0-9]+)" +     "\\1\\]" +     "\\}");}
0
private void filterValue(Context c, String contextKey)
{    for (ConfigFilter configFilter : configFiltersInstances) {        try {            Pattern pattern = configFilterPatternCache.get(configFilter.getName());            String currentValue = c.getString(contextKey);            Matcher matcher = pattern.matcher(currentValue);            String filteredValue = currentValue;            while (matcher.find()) {                String key = matcher.group("key");                                String filtered = configFilter.filter(key);                if (filtered == null) {                    continue;                }                String fullMatch = matcher.group();                filteredValue = filteredValue.replace(fullMatch, filtered);            }            c.put(contextKey, filteredValue);        } catch (Exception e) {            e.printStackTrace();                    }    }}
1
private void addError(String key, FlumeConfigurationErrorType errorType, ErrorOrWarning level)
{    errorList.add(new FlumeConfigurationError(agentName, key, errorType, level));}
0
private ChannelType getKnownChannel(String type)
{    return getKnownComponent(type, ChannelType.values());}
0
private SinkType getKnownSink(String type)
{    return getKnownComponent(type, SinkType.values());}
0
private SourceType getKnownSource(String type)
{    return getKnownComponent(type, SourceType.values());}
0
private ConfigFilterType getKnownConfigFilter(String type)
{    return getKnownComponent(type, ConfigFilterType.values());}
0
private T getKnownComponent(String type, T[] values)
{    for (T value : values) {        if (value.toString().equalsIgnoreCase(type))            return value;        String src = value.getClassName();        if (src != null && src.equalsIgnoreCase(type))            return value;    }    return null;}
0
private Set<String> validateChannels(Set<String> channelSet)
{    Iterator<String> iter = channelSet.iterator();    Map<String, Context> newContextMap = new HashMap<>();    ChannelConfiguration conf = null;    /*       * The logic for the following code:       *       * Is it a known component?       *  -Yes: Get the ChannelType and set the string name of that to       *        config and set configSpecified to true.       *  -No.Look for config type for the given component:       *      -Config Found:       *        Set config to the type mentioned, set configSpecified to true       *      -No Config found:       *        Set config to OTHER, configSpecified to false,       *        do basic validation. Leave the context in the       *        contextMap to process later. Setting it to other returns       *        a vanilla configuration(Source/Sink/Channel Configuration),       *        which does basic syntactic validation. This object is not       *        put into the map, so the context is retained which can be       *        picked up - this is meant for older classes which don't       *        implement ConfigurableComponent.       */    while (iter.hasNext()) {        String channelName = iter.next();        Context channelContext = channelContextMap.get(channelName);                if (channelContext != null) {                        ChannelType chType = getKnownChannel(channelContext.getString(BasicConfigurationConstants.CONFIG_TYPE));            boolean configSpecified = false;            String config = null;                        if (chType == null) {                config = channelContext.getString(CONFIG_CONFIG);                if (config == null || config.isEmpty()) {                    config = "OTHER";                } else {                    configSpecified = true;                }            } else {                config = chType.toString().toUpperCase(Locale.ENGLISH);                configSpecified = true;            }            try {                conf = (ChannelConfiguration) ComponentConfigurationFactory.create(channelName, config, ComponentType.CHANNEL);                                if (conf != null) {                    conf.configure(channelContext);                }                if ((configSpecified && conf.isNotFoundConfigClass()) || !configSpecified) {                    newContextMap.put(channelName, channelContext);                } else if (configSpecified) {                    channelConfigMap.put(channelName, conf);                }                if (conf != null) {                    errorList.addAll(conf.getErrors());                }            } catch (ConfigurationException e) {                                if (conf != null)                    errorList.addAll(conf.getErrors());                iter.remove();                            }        } else {            iter.remove();            addError(channelName, CONFIG_ERROR, ERROR);        }    }    channelContextMap = newContextMap;    Set<String> tempchannelSet = new HashSet<String>();    tempchannelSet.addAll(channelConfigMap.keySet());    tempchannelSet.addAll(channelContextMap.keySet());    channelSet.retainAll(tempchannelSet);    return channelSet;}
1
private Set<String> validateConfigFilterSet()
{    if (configFilters == null || configFilters.isEmpty()) {                return new HashSet<>();    }    Set<String> configFilterSet = new HashSet<>(Arrays.asList(configFilters.split("\\s+")));    Map<String, Context> newContextMap = new HashMap<>();    Iterator<String> iter = configFilterSet.iterator();    ConfigFilterConfiguration conf = null;    while (iter.hasNext()) {        String configFilterName = iter.next();        Context configFilterContext = configFilterContextMap.get(configFilterName);        if (configFilterContext != null) {                        ConfigFilterType chType = getKnownConfigFilter(configFilterContext.getString(BasicConfigurationConstants.CONFIG_TYPE));            boolean configSpecified = false;            String config = null;                        if (chType == null) {                config = configFilterContext.getString(CONFIG_CONFIG);                if (config == null || config.isEmpty()) {                    config = "OTHER";                } else {                    configSpecified = true;                }            } else {                config = chType.toString().toUpperCase(Locale.ENGLISH);                configSpecified = true;            }            try {                conf = (ConfigFilterConfiguration) ComponentConfigurationFactory.create(configFilterName, config, ComponentType.CONFIG_FILTER);                                if (conf != null) {                    conf.configure(configFilterContext);                }                if ((configSpecified && conf.isNotFoundConfigClass()) || !configSpecified) {                    newContextMap.put(configFilterName, configFilterContext);                } else if (configSpecified) {                    configFilterConfigMap.put(configFilterName, conf);                }                if (conf != null) {                    errorList.addAll(conf.getErrors());                }            } catch (ConfigurationException e) {                if (conf != null)                    errorList.addAll(conf.getErrors());                iter.remove();                            }        } else {            iter.remove();            addError(configFilterName, CONFIG_ERROR, ERROR);                    }    }    configFilterContextMap = newContextMap;    Set<String> tempchannelSet = new HashSet<String>();    tempchannelSet.addAll(configFilterConfigMap.keySet());    tempchannelSet.addAll(configFilterContextMap.keySet());    configFilterSet.retainAll(tempchannelSet);    return configFilterSet;}
1
private Set<String> validateSources(Set<String> channelSet)
{        if (sources == null || sources.isEmpty()) {                addError(CONFIG_SOURCES, PROPERTY_VALUE_NULL, WARNING);        return new HashSet<String>();    }    Set<String> sourceSet = new HashSet<String>(Arrays.asList(sources.split("\\s+")));    Map<String, Context> newContextMap = new HashMap<String, Context>();    Iterator<String> iter = sourceSet.iterator();    SourceConfiguration srcConf = null;    /*       * The logic for the following code:       *       * Is it a known component?       *  -Yes: Get the SourceType and set the string name of that to       *        config and set configSpecified to true.       *  -No.Look for config type for the given component:       *      -Config Found:       *        Set config to the type mentioned, set configSpecified to true       *      -No Config found:       *        Set config to OTHER, configSpecified to false,       *        do basic validation. Leave the context in the       *        contextMap to process later. Setting it to other returns       *        a vanilla configuration(Source/Sink/Channel Configuration),       *        which does basic syntactic validation. This object is not       *        put into the map, so the context is retained which can be       *        picked up - this is meant for older classes which don't       *        implement ConfigurableComponent.       */    while (iter.hasNext()) {        String sourceName = iter.next();        Context srcContext = sourceContextMap.get(sourceName);        String config = null;        boolean configSpecified = false;        if (srcContext != null) {            SourceType srcType = getKnownSource(srcContext.getString(BasicConfigurationConstants.CONFIG_TYPE));            if (srcType == null) {                config = srcContext.getString(CONFIG_CONFIG);                if (config == null || config.isEmpty()) {                    config = "OTHER";                } else {                    configSpecified = true;                }            } else {                config = srcType.toString().toUpperCase(Locale.ENGLISH);                configSpecified = true;            }            try {                                                srcConf = (SourceConfiguration) ComponentConfigurationFactory.create(sourceName, config, ComponentType.SOURCE);                if (srcConf != null) {                    srcConf.configure(srcContext);                    Set<String> channels = new HashSet<String>();                    if (srcConf.getChannels() != null) {                        channels.addAll(srcConf.getChannels());                    }                    channels.retainAll(channelSet);                    if (channels.isEmpty()) {                        throw new ConfigurationException("No Channels configured for " + sourceName);                    }                    srcContext.put(CONFIG_CHANNELS, this.getSpaceDelimitedList(channels));                }                if ((configSpecified && srcConf.isNotFoundConfigClass()) || !configSpecified) {                    newContextMap.put(sourceName, srcContext);                } else if (configSpecified) {                    sourceConfigMap.put(sourceName, srcConf);                }                if (srcConf != null)                    errorList.addAll(srcConf.getErrors());            } catch (ConfigurationException e) {                if (srcConf != null)                    errorList.addAll(srcConf.getErrors());                iter.remove();                            }        } else {            iter.remove();            addError(sourceName, CONFIG_ERROR, ERROR);                    }    }            sourceContextMap = newContextMap;    Set<String> tempsourceSet = new HashSet<String>();    tempsourceSet.addAll(sourceContextMap.keySet());    tempsourceSet.addAll(sourceConfigMap.keySet());    sourceSet.retainAll(tempsourceSet);    return sourceSet;}
1
private Set<String> validateSinks(Set<String> channelSet)
{            Map<String, Context> newContextMap = new HashMap<String, Context>();    Set<String> sinkSet;    SinkConfiguration sinkConf = null;    if (sinks == null || sinks.isEmpty()) {                addError(CONFIG_SINKS, PROPERTY_VALUE_NULL, WARNING);        return new HashSet<String>();    } else {        sinkSet = new HashSet<String>(Arrays.asList(sinks.split("\\s+")));    }    Iterator<String> iter = sinkSet.iterator();    /*       * The logic for the following code:       *       * Is it a known component?       *  -Yes: Get the SinkType and set the string name of that to       *        config and set configSpecified to true.       *  -No.Look for config type for the given component:       *      -Config Found:       *        Set config to the type mentioned, set configSpecified to true       *      -No Config found:       *        Set config to OTHER, configSpecified to false,       *        do basic validation. Leave the context in the       *        contextMap to process later. Setting it to other returns       *        a vanilla configuration(Source/Sink/Channel Configuration),       *        which does basic syntactic validation. This object is not       *        put into the map, so the context is retained which can be       *        picked up - this is meant for older classes which don't       *        implement ConfigurableComponent.       */    while (iter.hasNext()) {        String sinkName = iter.next();        Context sinkContext = sinkContextMap.get(sinkName.trim());        if (sinkContext == null) {            iter.remove();                        addError(sinkName, CONFIG_ERROR, ERROR);        } else {            String config = null;            boolean configSpecified = false;            SinkType sinkType = getKnownSink(sinkContext.getString(BasicConfigurationConstants.CONFIG_TYPE));            if (sinkType == null) {                config = sinkContext.getString(CONFIG_CONFIG);                if (config == null || config.isEmpty()) {                    config = "OTHER";                } else {                    configSpecified = true;                }            } else {                config = sinkType.toString().toUpperCase(Locale.ENGLISH);                configSpecified = true;            }            try {                                sinkConf = (SinkConfiguration) ComponentConfigurationFactory.create(sinkName, config, ComponentType.SINK);                if (sinkConf != null) {                    sinkConf.configure(sinkContext);                }                if (!channelSet.contains(sinkConf.getChannel())) {                    throw new ConfigurationException("Channel " + sinkConf.getChannel() + " not in active set.");                }                if ((configSpecified && sinkConf.isNotFoundConfigClass()) || !configSpecified) {                    newContextMap.put(sinkName, sinkContext);                } else if (configSpecified) {                    sinkConfigMap.put(sinkName, sinkConf);                }                if (sinkConf != null)                    errorList.addAll(sinkConf.getErrors());            } catch (ConfigurationException e) {                iter.remove();                if (sinkConf != null)                    errorList.addAll(sinkConf.getErrors());                            }        }        }    sinkContextMap = newContextMap;    Set<String> tempSinkset = new HashSet<String>();    tempSinkset.addAll(sinkConfigMap.keySet());    tempSinkset.addAll(sinkContextMap.keySet());    sinkSet.retainAll(tempSinkset);    return sinkSet;}
1
private Set<String> validateGroups(Set<String> sinkSet)
{    Set<String> sinkgroupSet = stringToSet(sinkgroups, " ");    Map<String, String> usedSinks = new HashMap<String, String>();    Iterator<String> iter = sinkgroupSet.iterator();    SinkGroupConfiguration conf;    while (iter.hasNext()) {        String sinkgroupName = iter.next();        Context context = this.sinkGroupContextMap.get(sinkgroupName);        if (context != null) {            try {                conf = (SinkGroupConfiguration) ComponentConfigurationFactory.create(sinkgroupName, "sinkgroup", ComponentType.SINKGROUP);                conf.configure(context);                Set<String> groupSinks = validGroupSinks(sinkSet, usedSinks, conf);                if (conf != null)                    errorList.addAll(conf.getErrors());                if (groupSinks != null && !groupSinks.isEmpty()) {                    List<String> sinkArray = new ArrayList<String>();                    sinkArray.addAll(groupSinks);                    conf.setSinks(sinkArray);                    sinkgroupConfigMap.put(sinkgroupName, conf);                } else {                    addError(sinkgroupName, CONFIG_ERROR, ERROR);                    if (conf != null)                        errorList.addAll(conf.getErrors());                    throw new ConfigurationException("No available sinks for sinkgroup: " + sinkgroupName + ". Sinkgroup will be removed");                }            } catch (ConfigurationException e) {                iter.remove();                addError(sinkgroupName, CONFIG_ERROR, ERROR);                            }        } else {            iter.remove();            addError(sinkgroupName, CONFIG_ERROR, ERROR);                    }    }    sinkgroupSet.retainAll(sinkgroupConfigMap.keySet());    return sinkgroupSet;}
1
private Set<String> validGroupSinks(Set<String> sinkSet, Map<String, String> usedSinks, SinkGroupConfiguration groupConf)
{    Set<String> groupSinks = Collections.synchronizedSet(new HashSet<String>(groupConf.getSinks()));    if (groupSinks.isEmpty())        return null;    Iterator<String> sinkIt = groupSinks.iterator();    while (sinkIt.hasNext()) {        String curSink = sinkIt.next();        if (usedSinks.containsKey(curSink)) {                        addError(groupConf.getComponentName(), PROPERTY_PART_OF_ANOTHER_GROUP, ERROR);            sinkIt.remove();            continue;        } else if (!sinkSet.contains(curSink)) {                        addError(curSink, INVALID_PROPERTY, ERROR);            sinkIt.remove();            continue;        } else {            usedSinks.put(curSink, groupConf.getComponentName());        }    }    return groupSinks;}
1
private String getSpaceDelimitedList(Set<String> entries)
{    if (entries.isEmpty()) {        return null;    }    StringBuilder sb = new StringBuilder();    for (String entry : entries) {        sb.append(" ").append(entry);    }    return sb.toString().trim();}
0
private static Set<String> stringToSet(String target, String delim)
{    Set<String> out = new HashSet<String>();    if (target == null || target.trim().length() == 0) {        return out;    }    StringTokenizer t = new StringTokenizer(target, delim);    while (t.hasMoreTokens()) {        out.add(t.nextToken());    }    return out;}
0
public String getPrevalidationConfig()
{    StringBuilder sb = new StringBuilder("AgentConfiguration[");    sb.append(agentName).append("]").append(NEWLINE);    sb.append("CONFIG_FILTERS: ").append(configFilterContextMap).append(NEWLINE);    sb.append("SOURCES: ").append(sourceContextMap).append(NEWLINE);    sb.append("CHANNELS: ").append(channelContextMap).append(NEWLINE);    sb.append("SINKS: ").append(sinkContextMap).append(NEWLINE);    return sb.toString();}
0
public String getPostvalidationConfig()
{    StringBuilder sb = new StringBuilder("AgentConfiguration created without Configuration stubs " + "for which only basic syntactical validation was performed[");    sb.append(agentName).append("]").append(NEWLINE);    if (!sourceContextMap.isEmpty() || !sinkContextMap.isEmpty() || !channelContextMap.isEmpty()) {        if (!sourceContextMap.isEmpty()) {            sb.append("SOURCES: ").append(sourceContextMap).append(NEWLINE);        }        if (!channelContextMap.isEmpty()) {            sb.append("CHANNELS: ").append(channelContextMap).append(NEWLINE);        }        if (!sinkContextMap.isEmpty()) {            sb.append("SINKS: ").append(sinkContextMap).append(NEWLINE);        }    }    if (!sourceConfigMap.isEmpty() || !sinkConfigMap.isEmpty() || !channelConfigMap.isEmpty()) {        sb.append("AgentConfiguration created with Configuration stubs " + "for which full validation was performed[");        sb.append(agentName).append("]").append(NEWLINE);        if (!sourceConfigMap.isEmpty()) {            sb.append("SOURCES: ").append(sourceConfigMap).append(NEWLINE);        }        if (!channelConfigMap.isEmpty()) {            sb.append("CHANNELS: ").append(channelConfigMap).append(NEWLINE);        }        if (!sinkConfigMap.isEmpty()) {            sb.append("SINKS: ").append(sinkConfigMap).append(NEWLINE);        }    }    return sb.toString();}
0
private boolean addProperty(String key, String value)
{        if (CONFIG_CONFIGFILTERS.equals(key)) {        if (configFilters == null) {            configFilters = value;            return true;        } else {                        addError(CONFIG_CONFIGFILTERS, DUPLICATE_PROPERTY, ERROR);            return false;        }    }        if (CONFIG_SOURCES.equals(key)) {        if (sources == null) {            sources = value;            return true;        } else {                        addError(CONFIG_SOURCES, DUPLICATE_PROPERTY, ERROR);            return false;        }    }        if (CONFIG_SINKS.equals(key)) {        if (sinks == null) {            sinks = value;                        return true;        } else {                        addError(CONFIG_SINKS, DUPLICATE_PROPERTY, ERROR);            return false;        }    }        if (CONFIG_CHANNELS.equals(key)) {        if (channels == null) {            channels = value;            return true;        } else {                        addError(CONFIG_CHANNELS, DUPLICATE_PROPERTY, ERROR);            return false;        }    }        if (CONFIG_SINKGROUPS.equals(key)) {        if (sinkgroups == null) {            sinkgroups = value;            return true;        } else {                        addError(CONFIG_SINKGROUPS, DUPLICATE_PROPERTY, ERROR);            return false;        }    }    if (addAsSourceConfig(key, value) || addAsChannelValue(key, value) || addAsSinkConfig(key, value) || addAsSinkGroupConfig(key, value) || addAsConfigFilterConfig(key, value)) {        return true;    }        addError(key, INVALID_PROPERTY, ERROR);    return false;}
1
private boolean addAsConfigFilterConfig(String key, String value)
{    return addComponentConfig(key, value, CONFIG_CONFIGFILTERS_PREFIX, configFilterContextMap);}
0
private boolean addAsSinkGroupConfig(String key, String value)
{    return addComponentConfig(key, value, CONFIG_SINKGROUPS_PREFIX, sinkGroupContextMap);}
0
private boolean addAsSinkConfig(String key, String value)
{    return addComponentConfig(key, value, CONFIG_SINKS_PREFIX, sinkContextMap);}
0
private boolean addAsChannelValue(String key, String value)
{    return addComponentConfig(key, value, CONFIG_CHANNELS_PREFIX, channelContextMap);}
0
private boolean addAsSourceConfig(String key, String value)
{    return addComponentConfig(key, value, CONFIG_SOURCES_PREFIX, sourceContextMap);}
0
private boolean addComponentConfig(String key, String value, String configPrefix, Map<String, Context> contextMap)
{    ComponentNameAndConfigKey parsed = parseConfigKey(key, configPrefix);    if (parsed != null) {        String name = parsed.getComponentName().trim();                Context context = contextMap.get(name);        if (context == null) {                        context = new Context();            contextMap.put(name, context);        }        context.put(parsed.getConfigKey(), value);        return true;    }    return false;}
1
private ComponentNameAndConfigKey parseConfigKey(String key, String prefix)
{        if (!key.startsWith(prefix)) {        return null;    }            int index = key.indexOf('.', prefix.length() + 1);    if (index == -1) {        return null;    }    String name = key.substring(prefix.length(), index);    String configKey = key.substring(prefix.length() + name.length() + 1);        if (name.isEmpty() || configKey.isEmpty()) {        return null;    }    return new ComponentNameAndConfigKey(name, configKey);}
0
public String getComponentName()
{    return componentName;}
0
public String getConfigKey()
{    return configKey;}
0
public String getComponentName()
{    return componentName;}
0
public String getKey()
{    return key;}
0
public FlumeConfigurationErrorType getErrorType()
{    return errorType;}
0
public ErrorOrWarning getErrorOrWarning()
{    return error;}
0
public String getError()
{    return error;}
0
public static boolean allowLogRawData()
{    return Boolean.getBoolean(LOG_RAWDATA_PROP);}
0
public static boolean allowLogPrintConfig()
{    return Boolean.getBoolean(LOG_PRINTCONFIG_PROP);}
0
public String getChannel()
{    return channel;}
0
public void getChannel(String channel)
{    this.channel = channel;}
0
public void configure(Context context) throws ConfigurationException
{    super.configure(context);    this.channel = context.getString("channel");    if (this.channel == null || this.channel.isEmpty()) {        errors.add(new FlumeConfigurationError(componentName, "channel", FlumeConfigurationErrorType.PROPERTY_VALUE_NULL, ErrorOrWarning.ERROR));        throw new ConfigurationException("No channel configured for sink: " + this.getComponentName());    }}
0
public String toString(int indentCount)
{    String basicStr = super.toString(indentCount);    StringBuilder sb = new StringBuilder();    sb.append(basicStr).append(FlumeConfiguration.INDENTSTEP).append("CHANNEL:").append(this.channel).append(FlumeConfiguration.NEWLINE);    return sb.toString();}
0
public String getSinkConfigurationType()
{    return this.sinkConfigurationName;}
0
public SinkConfiguration getConfiguration(String name) throws ConfigurationException
{    if (this == OTHER) {        return new SinkConfiguration(name);    }    Class<? extends SinkConfiguration> clazz;    SinkConfiguration instance = null;    try {        if (sinkConfigurationName != null) {            clazz = (Class<? extends SinkConfiguration>) Class.forName(sinkConfigurationName);            instance = clazz.getConstructor(String.class).newInstance(name);        } else {            return new SinkConfiguration(name);        }    } catch (ClassNotFoundException e) {                instance = new SinkConfiguration(name);                instance.setNotFoundConfigClass();    } catch (Exception e) {        throw new ConfigurationException("Couldn't create configuration", e);    }    return instance;}
0
public void setSinks(List<String> sinks)
{    this.sinks = sinks;}
0
public List<String> getSinks()
{    return sinks;}
0
public void configure(Context context) throws ConfigurationException
{    super.configure(context);    sinks = Arrays.asList(context.getString(BasicConfigurationConstants.CONFIG_SINKS).split("\\s+"));    Map<String, String> params = context.getSubProperties(BasicConfigurationConstants.CONFIG_SINK_PROCESSOR_PREFIX);    processorContext = new Context();    processorContext.putAll(params);    SinkProcessorType spType = getKnownSinkProcessor(processorContext.getString(BasicConfigurationConstants.CONFIG_TYPE));    if (spType != null) {        processorConf = (SinkProcessorConfiguration) ComponentConfigurationFactory.create(this.getComponentName() + "-processor", spType.toString(), ComponentType.SINK_PROCESSOR);        if (processorConf != null) {            processorConf.setSinks(new HashSet<String>(sinks));            processorConf.configure(processorContext);        }    }    setConfigured();}
0
public Context getProcessorContext()
{    return processorContext;}
0
public void setProcessorContext(Context processorContext)
{    this.processorContext = processorContext;}
0
public SinkProcessorConfiguration getSinkProcessorConfiguration()
{    return processorConf;}
0
public void setSinkProcessorConfiguration(SinkProcessorConfiguration conf)
{    this.processorConf = conf;}
0
private SinkProcessorType getKnownSinkProcessor(String type)
{    SinkProcessorType[] values = SinkProcessorType.values();    for (SinkProcessorType value : values) {        if (value.toString().equalsIgnoreCase(type))            return value;        String sinkProcessClassName = value.getClassName();        if (sinkProcessClassName != null && sinkProcessClassName.equalsIgnoreCase(type)) {            return value;        }    }    return null;}
0
public void configure(Context context) throws ConfigurationException
{}
0
public Set<String> getSinks()
{    return sinks;}
0
public void setSinks(Set<String> sinks)
{    this.sinks = sinks;}
0
public String getSinkProcessorConfigurationType()
{    return processorClassName;}
0
public SinkProcessorConfiguration getConfiguration(String name) throws ConfigurationException
{    Class<? extends SinkProcessorConfiguration> clazz;    SinkProcessorConfiguration instance = null;    try {        if (processorClassName != null) {            clazz = (Class<? extends SinkProcessorConfiguration>) Class.forName(processorClassName);            instance = clazz.getConstructor(String.class).newInstance(name);        } else {            return new SinkProcessorConfiguration(name);        }    } catch (ClassNotFoundException e) {                instance = new SinkProcessorConfiguration(name);                instance.setNotFoundConfigClass();    } catch (Exception e) {        throw new ConfigurationException("Could not instantiate configuration!", e);    }    return instance;}
0
public String getSinkProcessorClassName()
{    return processorClassName;}
0
public String getClassName()
{    return processorClassName;}
0
public String getSinkClassName()
{    return sinkClassName;}
0
public String getClassName()
{    return sinkClassName;}
0
public Set<String> getChannels()
{    return channels;}
0
public ChannelSelectorConfiguration getSelectorConfiguration()
{    return selectorConf;}
0
public void configure(Context context) throws ConfigurationException
{    super.configure(context);    try {        String channelList = context.getString(BasicConfigurationConstants.CONFIG_CHANNELS);        if (channelList != null) {            this.channels = new HashSet<String>(Arrays.asList(channelList.split("\\s+")));        }        if (channels.isEmpty()) {            errors.add(new FlumeConfigurationError(componentName, ComponentType.CHANNEL.getComponentType(), FlumeConfigurationErrorType.PROPERTY_VALUE_NULL, ErrorOrWarning.ERROR));            throw new ConfigurationException("No channels set for " + this.getComponentName());        }        Map<String, String> selectorParams = context.getSubProperties(BasicConfigurationConstants.CONFIG_SOURCE_CHANNELSELECTOR_PREFIX);        String selType;        if (selectorParams != null && !selectorParams.isEmpty()) {            selType = selectorParams.get(BasicConfigurationConstants.CONFIG_TYPE);        } else {            selType = ChannelSelectorConfigurationType.REPLICATING.toString();        }        if (selType == null || selType.isEmpty()) {            selType = ChannelSelectorConfigurationType.REPLICATING.toString();        }        ChannelSelectorType selectorType = this.getKnownChannelSelector(selType);        Context selectorContext = new Context();        selectorContext.putAll(selectorParams);        String config = null;        if (selectorType == null) {            config = selectorContext.getString(BasicConfigurationConstants.CONFIG_CONFIG);            if (config == null || config.isEmpty()) {                config = "OTHER";            }        } else {            config = selectorType.toString().toUpperCase(Locale.ENGLISH);        }        this.selectorConf = (ChannelSelectorConfiguration) ComponentConfigurationFactory.create(ComponentType.CHANNELSELECTOR.getComponentType(), config, ComponentType.CHANNELSELECTOR);        selectorConf.setChannels(channels);        selectorConf.configure(selectorContext);    } catch (Exception e) {        errors.add(new FlumeConfigurationError(componentName, ComponentType.CHANNELSELECTOR.getComponentType(), FlumeConfigurationErrorType.CONFIG_ERROR, ErrorOrWarning.ERROR));        throw new ConfigurationException("Failed to configure component!", e);    }}
0
public String toString(int indentCount)
{    String basicStr = super.toString(indentCount);    StringBuilder sb = new StringBuilder();    sb.append(basicStr).append("CHANNELS:");    for (String channel : this.channels) {        sb.append(FlumeConfiguration.INDENTSTEP).append(channel).append(FlumeConfiguration.NEWLINE);    }    return sb.toString();}
0
private ChannelSelectorType getKnownChannelSelector(String type)
{    ChannelSelectorType[] values = ChannelSelectorType.values();    for (ChannelSelectorType value : values) {        if (value.toString().equalsIgnoreCase(type))            return value;        String clName = value.getClassName();        if (clName != null && clName.equalsIgnoreCase(type))            return value;    }    return null;}
0
public String getSourceConfigurationType()
{    return this.srcConfigurationName;}
0
public SourceConfiguration getConfiguration(String name) throws ConfigurationException
{    if (this == OTHER) {        return new SourceConfiguration(name);    }    Class<? extends SourceConfiguration> clazz = null;    SourceConfiguration instance = null;    try {        if (srcConfigurationName != null) {            clazz = (Class<? extends SourceConfiguration>) Class.forName(srcConfigurationName);            instance = clazz.getConstructor(String.class).newInstance(name);        } else {                        instance = new SourceConfiguration(name);                        instance.setNotFoundConfigClass();        }    } catch (ClassNotFoundException e) {                instance = new SourceConfiguration(name);                instance.setNotFoundConfigClass();    } catch (Exception e) {        throw new ConfigurationException("Error creating configuration", e);    }    return instance;}
0
public String getSourceClassName()
{    return sourceClassName;}
0
public String getClassName()
{    return sourceClassName;}
0
public Map<String, String> getParameters()
{    synchronized (parameters) {        return ImmutableMap.copyOf(parameters);    }}
0
public void clear()
{    parameters.clear();}
0
public Map<String, String> getSubProperties(String prefix)
{    Preconditions.checkArgument(prefix.endsWith("."), "The given prefix does not end with a period (" + prefix + ")");    Map<String, String> result = Maps.newHashMap();    synchronized (parameters) {        for (Entry<String, String> entry : parameters.entrySet()) {            String key = entry.getKey();            if (key.startsWith(prefix)) {                String name = key.substring(prefix.length());                result.put(name, entry.getValue());            }        }    }    return ImmutableMap.copyOf(result);}
0
public void putAll(Map<String, String> map)
{    parameters.putAll(map);}
0
public void put(String key, String value)
{    parameters.put(key, value);}
0
public boolean containsKey(String key)
{    return parameters.containsKey(key);}
0
public Boolean getBoolean(String key, Boolean defaultValue)
{    String value = get(key);    if (value != null) {        return Boolean.valueOf(Boolean.parseBoolean(value.trim()));    }    return defaultValue;}
0
public Boolean getBoolean(String key)
{    return getBoolean(key, null);}
0
public Integer getInteger(String key, Integer defaultValue)
{    String value = get(key);    if (value != null) {        return Integer.valueOf(Integer.parseInt(value.trim()));    }    return defaultValue;}
0
public Integer getInteger(String key)
{    return getInteger(key, null);}
0
public Long getLong(String key, Long defaultValue)
{    String value = get(key);    if (value != null) {        return Long.valueOf(Long.parseLong(value.trim()));    }    return defaultValue;}
0
public Long getLong(String key)
{    return getLong(key, null);}
0
public String getString(String key, String defaultValue)
{    return get(key, defaultValue);}
0
public String getString(String key)
{    return get(key);}
0
public Float getFloat(String key, Float defaultValue)
{    String value = get(key);    if (value != null) {        return Float.parseFloat(value.trim());    }    return defaultValue;}
0
public Float getFloat(String key)
{    return getFloat(key, null);}
0
public Double getDouble(String key, Double defaultValue)
{    String value = get(key);    if (value != null) {        return Double.parseDouble(value.trim());    }    return defaultValue;}
0
public Double getDouble(String key)
{    return getDouble(key, null);}
0
private String get(String key, String defaultValue)
{    String result = parameters.get(key);    if (result != null) {        return result;    }    return defaultValue;}
0
private String get(String key)
{    return get(key, null);}
0
public String toString()
{    return "{ parameters:" + parameters + " }";}
0
public String filter(String key)
{    if (key.equals("null")) {        return null;    }    if (key.equals("throw")) {        throw new IllegalStateException("Test exception");    }    return "filtered_" + key;}
0
public void initializeWithConfiguration(Map<String, String> configuration)
{}
0
public void testFLUME1847() throws Exception
{    Context context = new Context();    context.put("type", "something");    SourceConfiguration sourceConfig = new SourceConfiguration("src");    sourceConfig.configure(context);}
0
public static void setupClass()
{    PROPERTIES.put(SOURCES, "s1 s2");    PROPERTIES.put(SOURCES + ".s1.type", "s1_type");    PROPERTIES.put(SOURCES + ".s1.channels", "c1");    PROPERTIES.put(SOURCES + ".s2.type", "jms");    PROPERTIES.put(SOURCES + ".s2.channels", "c2");    PROPERTIES.put(CHANNELS, "c1 c2");    PROPERTIES.put(CHANNELS + ".c1.type", "c1_type");    PROPERTIES.put(CHANNELS + ".c2.type", "memory");    PROPERTIES.put(SINKS, "k1 k2");    PROPERTIES.put(SINKS + ".k1.type", "k1_type");    PROPERTIES.put(SINKS + ".k2.type", "null");    PROPERTIES.put(SINKS + ".k1.channel", "c1");    PROPERTIES.put(SINKS + ".k2.channel", "c2");    PROPERTIES.put(AGENT + ".sinkgroups", "g1");    PROPERTIES.put(AGENT + ".sinkgroups.g1.sinks", "k1 k2");    PROPERTIES.put(AGENT + ".configfilters", "f1 f2");    PROPERTIES.put(AGENT + ".configfilters.f1.type", "f1_type");    PROPERTIES.put(AGENT + ".configfilters.f2.type", "env");}
0
public void testConfigHasNoErrors()
{    FlumeConfiguration configuration = new FlumeConfiguration(PROPERTIES);    assertTrue(configuration.getConfigurationErrors().isEmpty());}
0
public void testSourcesAdded()
{    FlumeConfiguration configuration = new FlumeConfiguration(PROPERTIES);    Set<String> sourceSet = configuration.getConfigurationFor(AGENT).getSourceSet();    assertEquals(new HashSet<>(Arrays.asList("s1", "s2")), sourceSet);}
0
public void testFiltersAdded()
{    FlumeConfiguration configuration = new FlumeConfiguration(PROPERTIES);    Set<String> configFilterSet = configuration.getConfigurationFor(AGENT).getConfigFilterSet();    assertEquals(new HashSet<>(Arrays.asList("f1", "f2")), configFilterSet);}
0
public void testSinksAdded()
{    FlumeConfiguration configuration = new FlumeConfiguration(PROPERTIES);    Set<String> sinkSet = configuration.getConfigurationFor(AGENT).getSinkSet();    assertEquals(new HashSet<>(Arrays.asList("k1", "k2")), sinkSet);}
0
public void testChannelsAdded()
{    FlumeConfiguration configuration = new FlumeConfiguration(PROPERTIES);    Set<String> channelSet = configuration.getConfigurationFor(AGENT).getChannelSet();    assertEquals(new HashSet<>(Arrays.asList("c1", "c2")), channelSet);}
0
public void testSinkGroupsAdded()
{    FlumeConfiguration configuration = new FlumeConfiguration(PROPERTIES);    Set<String> sinkSet = configuration.getConfigurationFor(AGENT).getSinkgroupSet();    assertEquals(new HashSet<>(Arrays.asList("g1")), sinkSet);}
0
public void testConfigFiltersMappedCorrectly()
{    FlumeConfiguration configuration = new FlumeConfiguration(PROPERTIES);    Map<String, Context> contextMap = configuration.getConfigurationFor(AGENT).getConfigFilterContext();    assertEquals("f1_type", contextMap.get("f1").getString("type"));}
0
public void testSourcesMappedCorrectly()
{    FlumeConfiguration configuration = new FlumeConfiguration(PROPERTIES);    Map<String, Context> contextMap = configuration.getConfigurationFor(AGENT).getSourceContext();    assertEquals("s1_type", contextMap.get("s1").getString("type"));}
0
public void testSinksMappedCorrectly()
{    FlumeConfiguration configuration = new FlumeConfiguration(PROPERTIES);    Map<String, Context> contextMap = configuration.getConfigurationFor(AGENT).getSinkContext();    assertEquals("k1_type", contextMap.get("k1").getString("type"));}
0
public void testChannelsMappedCorrectly()
{    FlumeConfiguration configuration = new FlumeConfiguration(PROPERTIES);    Map<String, Context> contextMap = configuration.getConfigurationFor(AGENT).getChannelContext();    assertEquals("c1_type", contextMap.get("c1").getString("type"));}
0
public void testChannelsConfigMappedCorrectly()
{    FlumeConfiguration configuration = new FlumeConfiguration(PROPERTIES);    Map<String, ComponentConfiguration> configMap = configuration.getConfigurationFor(AGENT).getChannelConfigMap();    assertEquals("memory", configMap.get("c2").getType());}
0
public void testConfigFilterConfigMappedCorrectly()
{    FlumeConfiguration configuration = new FlumeConfiguration(PROPERTIES);    Map<String, ComponentConfiguration> configMap = configuration.getConfigurationFor(AGENT).getConfigFilterConfigMap();    assertEquals("env", configMap.get("f2").getType());}
0
public void testSourceConfigMappedCorrectly()
{    FlumeConfiguration configuration = new FlumeConfiguration(PROPERTIES);    Map<String, ComponentConfiguration> configMap = configuration.getConfigurationFor(AGENT).getSourceConfigMap();    assertEquals("jms", configMap.get("s2").getType());}
0
public void testSinkConfigMappedCorrectly()
{    FlumeConfiguration configuration = new FlumeConfiguration(PROPERTIES);    Map<String, ComponentConfiguration> configMap = configuration.getConfigurationFor(AGENT).getSinkConfigMap();    assertEquals("null", configMap.get("k2").getType());}
0
public void testSinkgroupConfigMappedCorrectly()
{    FlumeConfiguration configuration = new FlumeConfiguration(PROPERTIES);    Map<String, ComponentConfiguration> configMap = configuration.getConfigurationFor(AGENT).getSinkGroupConfigMap();    assertEquals("Sinkgroup", configMap.get("g1").getType());}
0
public void testNoChannelIsInvalid()
{    Map<String, String> properties = new HashMap<>(PROPERTIES);    properties.put(CHANNELS, "");    FlumeConfiguration flumeConfiguration = new FlumeConfiguration(properties);    assertFalse(flumeConfiguration.getConfigurationErrors().isEmpty());    assertNull(flumeConfiguration.getConfigurationFor(AGENT));}
0
public void testNoSourcesIsValid()
{    Map<String, String> properties = new HashMap<>(PROPERTIES);    properties.remove(SOURCES);    properties.remove(SOURCES + ".s1.type");    properties.remove(SOURCES + ".s1.channels");    properties.remove(SOURCES + ".s2.type");    properties.remove(SOURCES + ".s2.channels");    FlumeConfiguration flumeConfiguration = new FlumeConfiguration(properties);    assertConfigHasNoError(flumeConfiguration);    assertNotNull(flumeConfiguration.getConfigurationFor(AGENT));}
0
public void testNoSinksIsValid()
{    Map<String, String> properties = new HashMap<>(PROPERTIES);    properties.remove(SINKS);    properties.remove(SINKS + ".k1.type", "k1_type");    properties.remove(SINKS + ".k2.type", "null");    properties.remove(SINKS + ".k1.channel", "c1");    properties.remove(SINKS + ".k2.channel", "c2");    properties.remove(AGENT + ".sinkgroups", "g1");    properties.remove(AGENT + ".sinkgroups.g1.sinks", "k1 k2");    FlumeConfiguration flumeConfiguration = new FlumeConfiguration(properties);    assertConfigHasNoError(flumeConfiguration);    assertNotNull(flumeConfiguration.getConfigurationFor(AGENT));}
0
private void assertConfigHasNoError(FlumeConfiguration configuration)
{    List<FlumeConfigurationError> configurationErrors = configuration.getConfigurationErrors();    long count = 0L;    for (FlumeConfigurationError e : configurationErrors) {        if (e.getErrorOrWarning() == ERROR) {            count++;        }    }    assertTrue(count == 0);}
0
public void testNoSourcesAndNoSinksIsInvalid()
{    Map<String, String> properties = new HashMap<>(PROPERTIES);    properties.put(SOURCES, "");    properties.put(SINKS, "");    FlumeConfiguration flumeConfiguration = new FlumeConfiguration(properties);    assertFalse(flumeConfiguration.getConfigurationErrors().isEmpty());    assertNull(flumeConfiguration.getConfigurationFor(AGENT));}
0
public void testFLUME1743()
{    Properties properties = new Properties();    properties.put("agent1.channels", "ch0");    properties.put("agent1.channels.ch0.type", "memory");    properties.put("agent1.sources", "src0");    properties.put("agent1.sources.src0.type", "multiport_syslogtcp");    properties.put("agent1.sources.src0.channels", "ch0");    properties.put("agent1.sources.src0.host", "localhost");    properties.put("agent1.sources.src0.ports", "10001 10002 10003");    properties.put("agent1.sources.src0.portHeader", "port");    properties.put("agent1.sinks", "sink0");    properties.put("agent1.sinks.sink0.type", "null");    properties.put("agent1.sinks.sink0.channel", "ch0");    properties.put("agent1.configfilters", "f1");    properties.put("agent1.configfilters.f1.type", "env");    FlumeConfiguration conf = new FlumeConfiguration(properties);    AgentConfiguration agentConfiguration = conf.getConfigurationFor("agent1");    Assert.assertEquals(String.valueOf(agentConfiguration.getSourceSet()), 1, agentConfiguration.getSourceSet().size());    Assert.assertEquals(String.valueOf(agentConfiguration.getChannelSet()), 1, agentConfiguration.getChannelSet().size());    Assert.assertEquals(String.valueOf(agentConfiguration.getSinkSet()), 1, agentConfiguration.getSinkSet().size());    Assert.assertTrue(agentConfiguration.getSourceSet().contains("src0"));    Assert.assertTrue(agentConfiguration.getChannelSet().contains("ch0"));    Assert.assertTrue(agentConfiguration.getSinkSet().contains("sink0"));    Assert.assertTrue(agentConfiguration.getConfigFilterSet().contains("f1"));}
0
public void testFlumeConfigAdsErrorOnNullName()
{    HashMap<String, String> properties = new HashMap<>();    properties.put(null, "something");    FlumeConfiguration config = new FlumeConfiguration(properties);    List<FlumeConfigurationError> configurationErrors = config.getConfigurationErrors();    assertEquals(1, configurationErrors.size());    assertError(configurationErrors.get(0), AGENT_NAME_MISSING, "", "", ERROR);}
0
public void testFlumeConfigAddsErrorOnNullValue()
{    HashMap<String, String> properties = new HashMap<>();    properties.put("something", null);    FlumeConfiguration config = new FlumeConfiguration(properties);    List<FlumeConfigurationError> configurationErrors = config.getConfigurationErrors();    assertEquals(1, configurationErrors.size());    assertError(configurationErrors.get(0), AGENT_NAME_MISSING, "", "", ERROR);}
0
public void testFlumeConfigAddsErrorOnEmptyValue()
{    HashMap<String, String> properties = new HashMap<>();    properties.put("something", "");    FlumeConfiguration config = new FlumeConfiguration(properties);    List<FlumeConfigurationError> configurationErrors = config.getConfigurationErrors();    assertEquals(1, configurationErrors.size());    assertError(configurationErrors.get(0), PROPERTY_VALUE_NULL, "something", "", ERROR);}
0
public void testFlumeConfigAddsErrorOnNoAgentNameValue()
{    HashMap<String, String> properties = new HashMap<>();    properties.put("something", "value");    FlumeConfiguration config = new FlumeConfiguration(properties);    List<FlumeConfigurationError> configurationErrors = config.getConfigurationErrors();    assertEquals(1, configurationErrors.size());    assertError(configurationErrors.get(0), AGENT_NAME_MISSING, "something", "", ERROR);}
0
public void testFlumeConfigAddsErrorOnEmptyAgentNameValue()
{    Properties properties = new Properties();    properties.put(".something", "value");    FlumeConfiguration config = new FlumeConfiguration(properties);    List<FlumeConfigurationError> configurationErrors = config.getConfigurationErrors();    assertEquals(1, configurationErrors.size());    assertError(configurationErrors.get(0), AGENT_NAME_MISSING, ".something", "", ERROR);}
0
public void testFlumeConfigAddsErrorOnEmptyPropertyName()
{    HashMap<String, String> properties = new HashMap<>();    properties.put("agent.", "something");    FlumeConfiguration config = new FlumeConfiguration(properties);    List<FlumeConfigurationError> configurationErrors = config.getConfigurationErrors();    assertEquals(1, configurationErrors.size());    assertError(configurationErrors.get(0), PROPERTY_NAME_NULL, "agent.", "", ERROR);}
0
public void testFlumeConfigAddsErrorOnInvalidConfig()
{    HashMap<String, String> properties = new HashMap<>();    properties.put("agent.channels", "c1");    properties.put("agent.channel.c1", "cc1");    FlumeConfiguration config = new FlumeConfiguration(properties);    List<FlumeConfigurationError> configurationErrors = config.getConfigurationErrors();    assertEquals(4, configurationErrors.size());    assertError(configurationErrors.get(0), INVALID_PROPERTY, "agent", "channel.c1", ERROR);    assertError(configurationErrors.get(1), CONFIG_ERROR, "agent", "c1", ERROR);    assertError(configurationErrors.get(2), PROPERTY_VALUE_NULL, "agent", "channels", ERROR);    assertError(configurationErrors.get(3), AGENT_CONFIGURATION_INVALID, "agent", "", ERROR);}
0
private void assertError(FlumeConfigurationError error, FlumeConfigurationErrorType agentNameMissing, String componentName, String key, ErrorOrWarning eow)
{    assertEquals(agentNameMissing, error.getErrorType());    assertEquals("ComponentName mismatch.", componentName, error.getComponentName());    assertEquals("Key mismatch.", key, error.getKey());    assertEquals(eow, error.getErrorOrWarning());}
0
public void testFlumeConfigFilterWorks()
{    Properties properties = new Properties();    properties.put("agent1.channels", "ch0");    properties.put("agent1.channels.ch0.type", "file");    properties.put("agent1.channels.ch0.param1", "${f1['param']}");    properties.put("agent1.channels.ch0.param2", "${f1['param\"]}");    properties.put("agent1.channels.ch0.param3", "${f1['null']}");    properties.put("agent1.channels.ch0.param4", "${f1['throw']}");    properties.put("agent1.sources", "src0");    properties.put("agent1.sources.src0.type", "multiport_syslogtcp");    properties.put("agent1.sources.src0.channels", "ch0");    properties.put("agent1.sources.src0.host", "${f1[host]}");    properties.put("agent1.sources.src0.ports", "10001 10002 10003");    properties.put("agent1.sources.src0.portHeader", "${f2[\"port\"]}-${f1['header']}");    properties.put("agent1.sinks", "sink0");    properties.put("agent1.sinks.sink0.type", "thrift");    properties.put("agent1.sinks.sink0.param", "${f2['param']}");    properties.put("agent1.sinks.sink0.channel", "ch0");    properties.put("agent1.configfilters", "f1 f2");    properties.put("agent1.configfilters.f1.type", "org.apache.flume.conf.configfilter.MockConfigFilter");    properties.put("agent1.configfilters.f2.type", "org.apache.flume.conf.configfilter.MockConfigFilter");    FlumeConfiguration conf = new FlumeConfiguration(properties);    AgentConfiguration agentConfiguration = conf.getConfigurationFor("agent1");    Context src0 = agentConfiguration.getSourceContext().get("src0");    assertEquals("filtered_host", src0.getString("host"));    assertEquals("filtered_port-filtered_header", src0.getString("portHeader"));    Context sink0 = agentConfiguration.getSinkContext().get("sink0");    assertEquals("filtered_param", sink0.getString("param"));    Context ch0 = agentConfiguration.getChannelContext().get("ch0");    assertEquals("filtered_param", ch0.getString("param1"));    assertEquals("${f1['param\"]}", ch0.getString("param2"));    assertEquals("${f1['null']}", ch0.getString("param3"));    assertEquals("${f1['throw']}", ch0.getString("param4"));}
0
public synchronized void setName(String name)
{    this.name = name;}
0
public synchronized void start()
{    lifecycleState = LifecycleState.START;}
0
public synchronized void stop()
{    lifecycleState = LifecycleState.STOP;}
0
public synchronized LifecycleState getLifecycleState()
{    return lifecycleState;}
0
public synchronized String getName()
{    return name;}
0
public void configure(Context context)
{}
0
public String toString()
{    return this.getClass().getName() + "{name: " + name + "}";}
0
public List<Channel> getAllChannels()
{    return channels;}
0
public void setChannels(List<Channel> channels)
{    this.channels = channels;}
0
public synchronized void setName(String name)
{    this.name = name;}
0
public synchronized String getName()
{    return name;}
0
protected Map<String, Channel> getChannelNameMap()
{    Map<String, Channel> channelNameMap = new HashMap<String, Channel>();    for (Channel ch : getAllChannels()) {        channelNameMap.put(ch.getName(), ch);    }    return channelNameMap;}
0
protected List<Channel> getChannelListFromNames(String channels, Map<String, Channel> channelNameMap)
{    List<Channel> configuredChannels = new ArrayList<Channel>();    if (channels == null || channels.isEmpty()) {        return configuredChannels;    }    String[] chNames = channels.split(" ");    for (String name : chNames) {        Channel ch = channelNameMap.get(name);        if (ch != null) {            configuredChannels.add(ch);        } else {            throw new FlumeException("Selector channel not found: " + name);        }    }    return configuredChannels;}
0
protected void initialize()
{}
0
public void put(Event event) throws ChannelException
{    BasicTransactionSemantics transaction = currentTransaction.get();    Preconditions.checkState(transaction != null, "No transaction exists for this thread");    transaction.put(event);}
0
public Event take() throws ChannelException
{    BasicTransactionSemantics transaction = currentTransaction.get();    Preconditions.checkState(transaction != null, "No transaction exists for this thread");    return transaction.take();}
0
public Transaction getTransaction()
{    if (!initialized) {        synchronized (this) {            if (!initialized) {                initialize();                initialized = true;            }        }    }    BasicTransactionSemantics transaction = currentTransaction.get();    if (transaction == null || transaction.getState().equals(BasicTransactionSemantics.State.CLOSED)) {        transaction = createTransaction();        currentTransaction.set(transaction);    }    return transaction;}
0
protected void doBegin() throws InterruptedException
{}
0
protected void doClose()
{}
0
protected void put(Event event)
{    Preconditions.checkState(Thread.currentThread().getId() == initialThreadId, "put() called from different thread than getTransaction()!");    Preconditions.checkState(state.equals(State.OPEN), "put() called when transaction is %s!", state);    Preconditions.checkArgument(event != null, "put() called with null event!");    try {        doPut(event);    } catch (InterruptedException e) {        Thread.currentThread().interrupt();        throw new ChannelException(e.toString(), e);    }}
0
protected Event take()
{    Preconditions.checkState(Thread.currentThread().getId() == initialThreadId, "take() called from different thread than getTransaction()!");    Preconditions.checkState(state.equals(State.OPEN), "take() called when transaction is %s!", state);    try {        return doTake();    } catch (InterruptedException e) {        Thread.currentThread().interrupt();        return null;    }}
0
protected State getState()
{    return state;}
0
public void begin()
{    Preconditions.checkState(Thread.currentThread().getId() == initialThreadId, "begin() called from different thread than getTransaction()!");    Preconditions.checkState(state.equals(State.NEW), "begin() called when transaction is " + state + "!");    try {        doBegin();    } catch (InterruptedException e) {        Thread.currentThread().interrupt();        throw new ChannelException(e.toString(), e);    }    state = State.OPEN;}
0
public void commit()
{    Preconditions.checkState(Thread.currentThread().getId() == initialThreadId, "commit() called from different thread than getTransaction()!");    Preconditions.checkState(state.equals(State.OPEN), "commit() called when transaction is %s!", state);    try {        doCommit();    } catch (InterruptedException e) {        Thread.currentThread().interrupt();        throw new ChannelException(e.toString(), e);    }    state = State.COMPLETED;}
0
public void rollback()
{    Preconditions.checkState(Thread.currentThread().getId() == initialThreadId, "rollback() called from different thread than getTransaction()!");    Preconditions.checkState(state.equals(State.OPEN), "rollback() called when transaction is %s!", state);    state = State.COMPLETED;    try {        doRollback();    } catch (InterruptedException e) {        Thread.currentThread().interrupt();        throw new ChannelException(e.toString(), e);    }}
0
public void close()
{    Preconditions.checkState(Thread.currentThread().getId() == initialThreadId, "close() called from different thread than getTransaction()!");    Preconditions.checkState(state.equals(State.NEW) || state.equals(State.COMPLETED), "close() called when transaction is %s" + " - you must either commit or rollback first", state);    state = State.CLOSED;    doClose();}
0
public String toString()
{    StringBuilder builder = new StringBuilder();    builder.append("BasicTransactionSemantics: {");    builder.append(" state:").append(state);    builder.append(" initialThreadId:").append(initialThreadId);    builder.append(" }");    return builder.toString();}
0
public void initialize()
{    interceptorChain.initialize();}
0
public void close()
{    interceptorChain.close();}
0
public void configure(Context context)
{    configureInterceptors(context);}
0
private void configureInterceptors(Context context)
{    List<Interceptor> interceptors = Lists.newLinkedList();    String interceptorListStr = context.getString("interceptors", "");    if (interceptorListStr.isEmpty()) {        return;    }    String[] interceptorNames = interceptorListStr.split("\\s+");    Context interceptorContexts = new Context(context.getSubProperties("interceptors."));        InterceptorBuilderFactory factory = new InterceptorBuilderFactory();    for (String interceptorName : interceptorNames) {        Context interceptorContext = new Context(interceptorContexts.getSubProperties(interceptorName + "."));        String type = interceptorContext.getString("type");        if (type == null) {                        throw new FlumeException("Interceptor.Type not specified for " + interceptorName);        }        try {            Interceptor.Builder builder = factory.newInstance(type);            builder.configure(interceptorContext);            interceptors.add(builder.build());        } catch (ClassNotFoundException e) {                        throw new FlumeException("Interceptor.Builder not found.", e);        } catch (InstantiationException e) {                        throw new FlumeException("Interceptor.Builder not constructable.", e);        } catch (IllegalAccessException e) {                        throw new FlumeException("Unable to access Interceptor.Builder.", e);        }    }    interceptorChain.setInterceptors(interceptors);}
1
public ChannelSelector getSelector()
{    return selector;}
0
public void processEventBatch(List<Event> events)
{    Preconditions.checkNotNull(events, "Event list must not be null");    events = interceptorChain.intercept(events);    Map<Channel, List<Event>> reqChannelQueue = new LinkedHashMap<Channel, List<Event>>();    Map<Channel, List<Event>> optChannelQueue = new LinkedHashMap<Channel, List<Event>>();    for (Event event : events) {        List<Channel> reqChannels = selector.getRequiredChannels(event);        for (Channel ch : reqChannels) {            List<Event> eventQueue = reqChannelQueue.get(ch);            if (eventQueue == null) {                eventQueue = new ArrayList<Event>();                reqChannelQueue.put(ch, eventQueue);            }            eventQueue.add(event);        }        List<Channel> optChannels = selector.getOptionalChannels(event);        for (Channel ch : optChannels) {            List<Event> eventQueue = optChannelQueue.get(ch);            if (eventQueue == null) {                eventQueue = new ArrayList<Event>();                optChannelQueue.put(ch, eventQueue);            }            eventQueue.add(event);        }    }        for (Channel reqChannel : reqChannelQueue.keySet()) {        Transaction tx = reqChannel.getTransaction();        Preconditions.checkNotNull(tx, "Transaction object must not be null");        try {            tx.begin();            List<Event> batch = reqChannelQueue.get(reqChannel);            for (Event event : batch) {                reqChannel.put(event);            }            tx.commit();        } catch (Throwable t) {            tx.rollback();            if (t instanceof Error) {                                throw (Error) t;            } else if (t instanceof ChannelException) {                throw (ChannelException) t;            } else {                throw new ChannelException("Unable to put batch on required " + "channel: " + reqChannel, t);            }        } finally {            if (tx != null) {                tx.close();            }        }    }        for (Channel optChannel : optChannelQueue.keySet()) {        Transaction tx = optChannel.getTransaction();        Preconditions.checkNotNull(tx, "Transaction object must not be null");        try {            tx.begin();            List<Event> batch = optChannelQueue.get(optChannel);            for (Event event : batch) {                optChannel.put(event);            }            tx.commit();        } catch (Throwable t) {            tx.rollback();                        if (t instanceof Error) {                throw (Error) t;            }        } finally {            if (tx != null) {                tx.close();            }        }    }}
1
public void processEvent(Event event)
{    event = interceptorChain.intercept(event);    if (event == null) {        return;    }        List<Channel> requiredChannels = selector.getRequiredChannels(event);    for (Channel reqChannel : requiredChannels) {        Transaction tx = reqChannel.getTransaction();        Preconditions.checkNotNull(tx, "Transaction object must not be null");        try {            tx.begin();            reqChannel.put(event);            tx.commit();        } catch (Throwable t) {            tx.rollback();            if (t instanceof Error) {                                throw (Error) t;            } else if (t instanceof ChannelException) {                throw (ChannelException) t;            } else {                throw new ChannelException("Unable to put event on required " + "channel: " + reqChannel, t);            }        } finally {            if (tx != null) {                tx.close();            }        }    }        List<Channel> optionalChannels = selector.getOptionalChannels(event);    for (Channel optChannel : optionalChannels) {        Transaction tx = null;        try {            tx = optChannel.getTransaction();            tx.begin();            optChannel.put(event);            tx.commit();        } catch (Throwable t) {            tx.rollback();                        if (t instanceof Error) {                throw (Error) t;            }        } finally {            if (tx != null) {                tx.close();            }        }    }}
1
public static ChannelSelector create(List<Channel> channels, Map<String, String> config)
{    ChannelSelector selector = getSelectorForType(config.get(BasicConfigurationConstants.CONFIG_TYPE));    selector.setChannels(channels);    Context context = new Context();    context.putAll(config);    Configurables.configure(selector, context);    return selector;}
0
public static ChannelSelector create(List<Channel> channels, ChannelSelectorConfiguration conf)
{    String type = ChannelSelectorType.REPLICATING.toString();    if (conf != null) {        type = conf.getType();    }    ChannelSelector selector = getSelectorForType(type);    selector.setChannels(channels);    Configurables.configure(selector, conf);    return selector;}
0
private static ChannelSelector getSelectorForType(String type)
{    if (type == null || type.trim().length() == 0) {        return new ReplicatingChannelSelector();    }    String selectorClassName = type;    ChannelSelectorType selectorType = ChannelSelectorType.OTHER;    try {        selectorType = ChannelSelectorType.valueOf(type.toUpperCase(Locale.ENGLISH));    } catch (IllegalArgumentException ex) {            }    if (!selectorType.equals(ChannelSelectorType.OTHER)) {        selectorClassName = selectorType.getChannelSelectorClassName();    }    ChannelSelector selector = null;    try {        @SuppressWarnings("unchecked")        Class<? extends ChannelSelector> selectorClass = (Class<? extends ChannelSelector>) Class.forName(selectorClassName);        selector = selectorClass.newInstance();    } catch (Exception ex) {        throw new FlumeException("Unable to load selector type: " + type + ", class: " + selectorClassName, ex);    }    return selector;}
1
public static void put(final Channel channel, final Event event) throws ChannelException
{    transact(channel, new Runnable() {        @Override        public void run() {            channel.put(event);        }    });}
0
public void run()
{    channel.put(event);}
0
public static void put(final Channel channel, final Collection<Event> events) throws ChannelException
{    transact(channel, new Runnable() {        @Override        public void run() {            for (Event event : events) {                channel.put(event);            }        }    });}
0
public void run()
{    for (Event event : events) {        channel.put(event);    }}
0
public static Event take(final Channel channel) throws ChannelException
{    return transact(channel, new Callable<Event>() {        @Override        public Event call() {            return channel.take();        }    });}
0
public Event call()
{    return channel.take();}
0
public static List<Event> take(final Channel channel, final int max) throws ChannelException
{    return transact(channel, new Callable<List<Event>>() {        @Override        public List<Event> call() {            List<Event> events = new ArrayList<Event>(max);            while (events.size() < max) {                Event event = channel.take();                if (event == null) {                    break;                }                events.add(event);            }            return events;        }    });}
0
public List<Event> call()
{    List<Event> events = new ArrayList<Event>(max);    while (events.size() < max) {        Event event = channel.take();        if (event == null) {            break;        }        events.add(event);    }    return events;}
0
public static void transact(Channel channel, Runnable transactor) throws ChannelException
{    transact(channel, Executors.callable(transactor));}
0
public static T transact(Channel channel, Callable<T> transactor) throws ChannelException
{    Transaction transaction = channel.getTransaction();    boolean committed = false;    boolean interrupted = false;    try {        transaction.begin();        T value = transactor.call();        transaction.commit();        committed = true;        return value;    } catch (Throwable e) {        interrupted = Thread.currentThread().isInterrupted();        try {            transaction.rollback();        } catch (Throwable e2) {                    }        if (e instanceof InterruptedException) {            interrupted = true;        } else if (e instanceof Error) {            throw (Error) e;        } else if (e instanceof RuntimeException) {            throw (RuntimeException) e;        }        throw new ChannelException(e);    } finally {        interrupted = interrupted || Thread.currentThread().isInterrupted();        try {            transaction.close();        } catch (Throwable e) {            if (committed) {                if (e instanceof Error) {                    throw (Error) e;                } else if (e instanceof RuntimeException) {                    throw (RuntimeException) e;                } else {                    throw new ChannelException(e);                }            } else {                            }        } finally {            if (interrupted) {                Thread.currentThread().interrupt();            }        }    }}
1
public Channel create(String name, String type) throws FlumeException
{    Preconditions.checkNotNull(name, "name");    Preconditions.checkNotNull(type, "type");        Class<? extends Channel> channelClass = getClass(type);    try {        return channelClass.newInstance();    } catch (Exception ex) {        throw new FlumeException("Unable to create channel: " + name + ", type: " + type + ", class: " + channelClass.getName(), ex);    }}
1
public Class<? extends Channel> getClass(String type) throws FlumeException
{    String channelClassName = type;    ChannelType channelType = ChannelType.OTHER;    try {        channelType = ChannelType.valueOf(type.toUpperCase(Locale.ENGLISH));    } catch (IllegalArgumentException ex) {            }    if (!channelType.equals(ChannelType.OTHER)) {        channelClassName = channelType.getChannelClassName();    }    try {        return (Class<? extends Channel>) Class.forName(channelClassName);    } catch (Exception ex) {        throw new FlumeException("Unable to load channel type: " + type + ", class: " + channelClassName, ex);    }}
1
protected void doPut(Event event) throws InterruptedException
{    channelCounter.incrementEventPutAttemptCount();    int eventByteSize = (int) Math.ceil(estimateEventSize(event) / byteCapacitySlotSize);    if (!putList.offer(event)) {        throw new ChannelException("Put queue for MemoryTransaction of capacity " + putList.size() + " full, consider committing more frequently, " + "increasing capacity or increasing thread count");    }    putByteCounter += eventByteSize;}
0
protected Event doTake() throws InterruptedException
{    channelCounter.incrementEventTakeAttemptCount();    if (takeList.remainingCapacity() == 0) {        throw new ChannelException("Take list for MemoryTransaction, capacity " + takeList.size() + " full, consider committing more frequently, " + "increasing capacity, or increasing thread count");    }    if (!queueStored.tryAcquire(keepAlive, TimeUnit.SECONDS)) {        return null;    }    Event event;    synchronized (queueLock) {        event = queue.poll();    }    Preconditions.checkNotNull(event, "Queue.poll returned NULL despite semaphore " + "signalling existence of entry");    takeList.put(event);    int eventByteSize = (int) Math.ceil(estimateEventSize(event) / byteCapacitySlotSize);    takeByteCounter += eventByteSize;    return event;}
0
protected void doCommit() throws InterruptedException
{    int remainingChange = takeList.size() - putList.size();    if (remainingChange < 0) {        if (!bytesRemaining.tryAcquire(putByteCounter, keepAlive, TimeUnit.SECONDS)) {            throw new ChannelException("Cannot commit transaction. Byte capacity " + "allocated to store event body " + byteCapacity * byteCapacitySlotSize + "reached. Please increase heap space/byte capacity allocated to " + "the channel as the sinks may not be keeping up with the sources");        }        if (!queueRemaining.tryAcquire(-remainingChange, keepAlive, TimeUnit.SECONDS)) {            bytesRemaining.release(putByteCounter);            throw new ChannelFullException("Space for commit to queue couldn't be acquired." + " Sinks are likely not keeping up with sources, or the buffer size is too tight");        }    }    int puts = putList.size();    int takes = takeList.size();    synchronized (queueLock) {        if (puts > 0) {            while (!putList.isEmpty()) {                if (!queue.offer(putList.removeFirst())) {                    throw new RuntimeException("Queue add failed, this shouldn't be able to happen");                }            }        }        putList.clear();        takeList.clear();    }    bytesRemaining.release(takeByteCounter);    takeByteCounter = 0;    putByteCounter = 0;    queueStored.release(puts);    if (remainingChange > 0) {        queueRemaining.release(remainingChange);    }    if (puts > 0) {        channelCounter.addToEventPutSuccessCount(puts);    }    if (takes > 0) {        channelCounter.addToEventTakeSuccessCount(takes);    }    channelCounter.setChannelSize(queue.size());}
0
protected void doRollback()
{    int takes = takeList.size();    synchronized (queueLock) {        Preconditions.checkState(queue.remainingCapacity() >= takeList.size(), "Not enough space in memory channel " + "queue to rollback takes. This should never happen, please report");        while (!takeList.isEmpty()) {            queue.addFirst(takeList.removeLast());        }        putList.clear();    }    putByteCounter = 0;    takeByteCounter = 0;    queueStored.release(takes);    channelCounter.setChannelSize(queue.size());}
0
public void configure(Context context)
{    Integer capacity = null;    try {        capacity = context.getInteger("capacity", defaultCapacity);    } catch (NumberFormatException e) {        capacity = defaultCapacity;            }    if (capacity <= 0) {        capacity = defaultCapacity;            }    try {        transCapacity = context.getInteger("transactionCapacity", defaultTransCapacity);    } catch (NumberFormatException e) {        transCapacity = defaultTransCapacity;            }    if (transCapacity <= 0) {        transCapacity = defaultTransCapacity;            }    Preconditions.checkState(transCapacity <= capacity, "Transaction Capacity of Memory Channel cannot be higher than " + "the capacity.");    try {        byteCapacityBufferPercentage = context.getInteger("byteCapacityBufferPercentage", defaultByteCapacityBufferPercentage);    } catch (NumberFormatException e) {        byteCapacityBufferPercentage = defaultByteCapacityBufferPercentage;    }    try {        byteCapacity = (int) ((context.getLong("byteCapacity", defaultByteCapacity).longValue() * (1 - byteCapacityBufferPercentage * .01)) / byteCapacitySlotSize);        if (byteCapacity < 1) {            byteCapacity = Integer.MAX_VALUE;        }    } catch (NumberFormatException e) {        byteCapacity = (int) ((defaultByteCapacity * (1 - byteCapacityBufferPercentage * .01)) / byteCapacitySlotSize);    }    try {        keepAlive = context.getInteger("keep-alive", defaultKeepAlive);    } catch (NumberFormatException e) {        keepAlive = defaultKeepAlive;    }    if (queue != null) {        try {            resizeQueue(capacity);        } catch (InterruptedException e) {            Thread.currentThread().interrupt();        }    } else {        synchronized (queueLock) {            queue = new LinkedBlockingDeque<Event>(capacity);            queueRemaining = new Semaphore(capacity);            queueStored = new Semaphore(0);        }    }    if (bytesRemaining == null) {        bytesRemaining = new Semaphore(byteCapacity);        lastByteCapacity = byteCapacity;    } else {        if (byteCapacity > lastByteCapacity) {            bytesRemaining.release(byteCapacity - lastByteCapacity);            lastByteCapacity = byteCapacity;        } else {            try {                if (!bytesRemaining.tryAcquire(lastByteCapacity - byteCapacity, keepAlive, TimeUnit.SECONDS)) {                                    } else {                    lastByteCapacity = byteCapacity;                }            } catch (InterruptedException e) {                Thread.currentThread().interrupt();            }        }    }    if (channelCounter == null) {        channelCounter = new ChannelCounter(getName());    }}
1
private void resizeQueue(int capacity) throws InterruptedException
{    int oldCapacity;    synchronized (queueLock) {        oldCapacity = queue.size() + queue.remainingCapacity();    }    if (oldCapacity == capacity) {        return;    } else if (oldCapacity > capacity) {        if (!queueRemaining.tryAcquire(oldCapacity - capacity, keepAlive, TimeUnit.SECONDS)) {                    } else {            synchronized (queueLock) {                LinkedBlockingDeque<Event> newQueue = new LinkedBlockingDeque<Event>(capacity);                newQueue.addAll(queue);                queue = newQueue;            }        }    } else {        synchronized (queueLock) {            LinkedBlockingDeque<Event> newQueue = new LinkedBlockingDeque<Event>(capacity);            newQueue.addAll(queue);            queue = newQueue;        }        queueRemaining.release(capacity - oldCapacity);    }}
1
public synchronized void start()
{    channelCounter.start();    channelCounter.setChannelSize(queue.size());    channelCounter.setChannelCapacity(Long.valueOf(queue.size() + queue.remainingCapacity()));    super.start();}
0
public synchronized void stop()
{    channelCounter.setChannelSize(queue.size());    channelCounter.stop();    super.stop();}
0
protected BasicTransactionSemantics createTransaction()
{    return new MemoryTransaction(transCapacity, channelCounter);}
0
private long estimateEventSize(Event event)
{    byte[] body = event.getBody();    if (body != null && body.length != 0) {        return body.length;    }        return 1;}
0
 int getBytesRemainingValue()
{    return bytesRemaining.availablePermits();}
0
public long getTransactionCapacity()
{    return transCapacity;}
0
public List<Channel> getRequiredChannels(Event event)
{    String headerValue = event.getHeaders().get(headerName);    if (headerValue == null || headerValue.trim().length() == 0) {        return defaultChannels;    }    List<Channel> channels = channelMapping.get(headerValue);        if (channels == null) {        channels = defaultChannels;    }    return channels;}
0
public List<Channel> getOptionalChannels(Event event)
{    String hdr = event.getHeaders().get(headerName);    List<Channel> channels = optionalChannels.get(hdr);    if (channels == null) {        channels = EMPTY_LIST;    }    return channels;}
0
public void configure(Context context)
{    this.headerName = context.getString(CONFIG_MULTIPLEX_HEADER_NAME, DEFAULT_MULTIPLEX_HEADER);    Map<String, Channel> channelNameMap = getChannelNameMap();    defaultChannels = getChannelListFromNames(context.getString(CONFIG_DEFAULT_CHANNEL), channelNameMap);    Map<String, String> mapConfig = context.getSubProperties(CONFIG_PREFIX_MAPPING);    channelMapping = new HashMap<String, List<Channel>>();    for (String headerValue : mapConfig.keySet()) {        List<Channel> configuredChannels = getChannelListFromNames(mapConfig.get(headerValue), channelNameMap);                if (configuredChannels.size() == 0) {            throw new FlumeException("No channel configured for when " + "header value is: " + headerValue);        }        if (channelMapping.put(headerValue, configuredChannels) != null) {            throw new FlumeException("Selector channel configured twice");        }    }            Map<String, String> optionalChannelsMapping = context.getSubProperties(CONFIG_PREFIX_OPTIONAL + ".");    optionalChannels = new HashMap<String, List<Channel>>();    for (String hdr : optionalChannelsMapping.keySet()) {        List<Channel> confChannels = getChannelListFromNames(optionalChannelsMapping.get(hdr), channelNameMap);        if (confChannels.isEmpty()) {            confChannels = EMPTY_LIST;        }                        List<Channel> reqdChannels = channelMapping.get(hdr);                if (reqdChannels == null || reqdChannels.isEmpty()) {            reqdChannels = defaultChannels;        }        for (Channel c : reqdChannels) {            if (confChannels.contains(c)) {                confChannels.remove(c);            }        }        if (optionalChannels.put(hdr, confChannels) != null) {            throw new FlumeException("Selector channel configured twice");        }    }}
0
public void configure(Context context)
{    Integer capacity = context.getInteger("capacity");    keepAlive = context.getInteger("keep-alive");    if (capacity == null) {        capacity = defaultCapacity;    }    if (keepAlive == null) {        keepAlive = defaultKeepAlive;    }    queue = new ArrayBlockingQueue<Event>(capacity);    if (channelCounter == null) {        channelCounter = new ChannelCounter(getName());    }}
0
public void start()
{    channelCounter.start();    channelCounter.setChannelSize(queue.size());    channelCounter.setChannelSize(Long.valueOf(queue.size() + queue.remainingCapacity()));    super.start();}
0
public void stop()
{    channelCounter.setChannelSize(queue.size());    channelCounter.stop();    super.stop();}
0
public void put(Event event)
{    Preconditions.checkState(queue != null, "No queue defined (Did you forget to configure me?");    channelCounter.incrementEventPutAttemptCount();    try {        queue.put(event);    } catch (InterruptedException ex) {        throw new ChannelException("Failed to put(" + event + ")", ex);    }    channelCounter.addToEventPutSuccessCount(1);    channelCounter.setChannelSize(queue.size());}
0
public Event take()
{    Preconditions.checkState(queue != null, "No queue defined (Did you forget to configure me?");    channelCounter.incrementEventTakeAttemptCount();    try {        Event e = queue.poll(keepAlive, TimeUnit.SECONDS);        channelCounter.addToEventTakeSuccessCount(1);        channelCounter.setChannelSize(queue.size());        return e;    } catch (InterruptedException ex) {        throw new ChannelException("Failed to take()", ex);    }}
0
public Transaction getTransaction()
{    return NoOpTransaction.sharedInstance();}
0
public static Transaction sharedInstance()
{    if (sharedInstance == null) {        sharedInstance = new NoOpTransaction();    }    return sharedInstance;}
0
public void begin()
{}
0
public void commit()
{}
0
public void rollback()
{}
0
public void close()
{}
0
public List<Channel> getRequiredChannels(Event event)
{    /*     * Seems like there are lot of components within flume that do not call     * configure method. It is conceiveable that custom component tests too     * do that. So in that case, revert to old behavior.     */    if (requiredChannels == null) {        return getAllChannels();    }    return requiredChannels;}
0
public List<Channel> getOptionalChannels(Event event)
{    return optionalChannels;}
0
public void configure(Context context)
{    String optionalList = context.getString(CONFIG_OPTIONAL);    requiredChannels = new ArrayList<Channel>(getAllChannels());    Map<String, Channel> channelNameMap = getChannelNameMap();    if (optionalList != null && !optionalList.isEmpty()) {        for (String optional : optionalList.split("\\s+")) {            Channel optionalChannel = channelNameMap.get(optional);            requiredChannels.remove(optionalChannel);            if (!optionalChannels.contains(optionalChannel)) {                optionalChannels.add(optionalChannel);            }        }    }}
0
public static void main(String[] args)
{    SSLUtil.initGlobalSSLParameters();    AvroCLIClient client = new AvroCLIClient();    try {        if (client.parseCommandLine(args)) {            client.run();        }    } catch (ParseException e) {            } catch (IOException e) {            } catch (FlumeException e) {            } catch (EventDeliveryException e) {            }    }
1
private void parseHeaders(CommandLine commandLine)
{    String headerFile = commandLine.getOptionValue("headerFile");    FileInputStream fs = null;    try {        if (headerFile != null) {            fs = new FileInputStream(headerFile);            Properties properties = new Properties();            properties.load(fs);            for (Map.Entry<Object, Object> propertiesEntry : properties.entrySet()) {                String key = (String) propertiesEntry.getKey();                String value = (String) propertiesEntry.getValue();                                headers.put(key, value);            }        }    } catch (Exception e) {                return;    } finally {        if (fs != null) {            try {                fs.close();            } catch (Exception e) {                                return;            }        }    }}
1
private boolean parseCommandLine(String[] args) throws ParseException
{    Options options = new Options();    options.addOption("P", "rpcProps", true, "RPC client properties file with " + "server connection params").addOption("p", "port", true, "port of the avro source").addOption("H", "host", true, "hostname of the avro source").addOption("F", "filename", true, "file to stream to avro source").addOption(null, "dirname", true, "directory to stream to avro source").addOption("R", "headerFile", true, ("file containing headers as " + "key/value pairs on each new line")).addOption("h", "help", false, "display help text");    CommandLineParser parser = new GnuParser();    CommandLine commandLine = parser.parse(options, args);    if (commandLine.hasOption('h')) {        new HelpFormatter().printHelp("flume-ng avro-client", "", options, "The --dirname option assumes that a spooling directory exists " + "where immutable log files are dropped.", true);        return false;    }    if (commandLine.hasOption("filename") && commandLine.hasOption("dirname")) {        throw new ParseException("--filename and --dirname options cannot be used simultaneously");    }    if (!commandLine.hasOption("port") && !commandLine.hasOption("host") && !commandLine.hasOption("rpcProps")) {        throw new ParseException("Either --rpcProps or both --host and --port " + "must be specified.");    }    if (commandLine.hasOption("rpcProps")) {        rpcClientPropsFile = commandLine.getOptionValue("rpcProps");        Preconditions.checkNotNull(rpcClientPropsFile, "RPC client properties " + "file must be specified after --rpcProps argument.");        Preconditions.checkArgument(new File(rpcClientPropsFile).exists(), "RPC client properties file %s does not exist!", rpcClientPropsFile);    }    if (rpcClientPropsFile == null) {        if (!commandLine.hasOption("port")) {            throw new ParseException("You must specify a port to connect to with --port");        }        port = Integer.parseInt(commandLine.getOptionValue("port"));        if (!commandLine.hasOption("host")) {            throw new ParseException("You must specify a hostname to connect to with --host");        }        hostname = commandLine.getOptionValue("host");    }    fileName = commandLine.getOptionValue("filename");    dirName = commandLine.getOptionValue("dirname");    if (commandLine.hasOption("headerFile")) {        parseHeaders(commandLine);    }    return true;}
0
private void run() throws IOException, FlumeException, EventDeliveryException
{    EventReader reader = null;    RpcClient rpcClient;    if (rpcClientPropsFile != null) {        rpcClient = RpcClientFactory.getInstance(new File(rpcClientPropsFile));    } else {        rpcClient = RpcClientFactory.getDefaultInstance(hostname, port, BATCH_SIZE);    }    try {        if (fileName != null) {            reader = new SimpleTextLineEventReader(new FileReader(new File(fileName)));        } else if (dirName != null) {            reader = new ReliableSpoolingFileEventReader.Builder().spoolDirectory(new File(dirName)).sourceCounter(new SourceCounter("avrocli")).build();        } else {            reader = new SimpleTextLineEventReader(new InputStreamReader(System.in));        }        long lastCheck = System.currentTimeMillis();        long sentBytes = 0;        int batchSize = rpcClient.getBatchSize();        List<Event> events;        while (!(events = reader.readEvents(batchSize)).isEmpty()) {            for (Event event : events) {                event.setHeaders(headers);                sentBytes += event.getBody().length;                sent++;                long now = System.currentTimeMillis();                if (now >= lastCheck + 5000) {                                        lastCheck = now;                }            }            rpcClient.appendBatch(events);            if (reader instanceof ReliableEventReader) {                ((ReliableEventReader) reader).commit();            }        }            } finally {        if (reader != null) {                        reader.close();        }                rpcClient.close();    }}
1
private List<File> getCandidateFiles(final Path directory)
{    Preconditions.checkNotNull(directory);    final List<File> candidateFiles = new ArrayList<>();    try {        final Set<Path> trackerDirCompletedFiles = getTrackerDirCompletedFiles();        Files.walkFileTree(directory, new SimpleFileVisitor<Path>() {            @Override            public FileVisitResult preVisitDirectory(Path dir, BasicFileAttributes attrs) throws IOException {                if (directory.equals(dir)) {                                        return FileVisitResult.CONTINUE;                }                String directoryName = dir.getFileName().toString();                if (!recursiveDirectorySearch || directoryName.startsWith(".") || ignorePattern.matcher(directoryName).matches()) {                    return FileVisitResult.SKIP_SUBTREE;                }                return FileVisitResult.CONTINUE;            }            @Override            public FileVisitResult visitFile(Path candidate, BasicFileAttributes attrs) throws IOException {                String fileName = candidate.getFileName().toString();                if (!fileName.endsWith(completedSuffix) && !isFileInTrackerDir(trackerDirCompletedFiles, candidate) && !fileName.startsWith(".") && includePattern.matcher(fileName).matches() && !ignorePattern.matcher(fileName).matches()) {                    candidateFiles.add(candidate.toFile());                }                return FileVisitResult.CONTINUE;            }        });    } catch (IOException e) {                sourceCounter.incrementGenericProcessingFail();    }    return candidateFiles;}
1
public FileVisitResult preVisitDirectory(Path dir, BasicFileAttributes attrs) throws IOException
{    if (directory.equals(dir)) {                return FileVisitResult.CONTINUE;    }    String directoryName = dir.getFileName().toString();    if (!recursiveDirectorySearch || directoryName.startsWith(".") || ignorePattern.matcher(directoryName).matches()) {        return FileVisitResult.SKIP_SUBTREE;    }    return FileVisitResult.CONTINUE;}
0
public FileVisitResult visitFile(Path candidate, BasicFileAttributes attrs) throws IOException
{    String fileName = candidate.getFileName().toString();    if (!fileName.endsWith(completedSuffix) && !isFileInTrackerDir(trackerDirCompletedFiles, candidate) && !fileName.startsWith(".") && includePattern.matcher(fileName).matches() && !ignorePattern.matcher(fileName).matches()) {        candidateFiles.add(candidate.toFile());    }    return FileVisitResult.CONTINUE;}
0
private Set<Path> getTrackerDirCompletedFiles() throws IOException
{    final Set<Path> completedFiles = new HashSet<>();    if (TrackingPolicy.TRACKER_DIR != trackingPolicy) {        return completedFiles;    }    Path trackerDirPath = trackerDirectory.toPath();    Files.walkFileTree(trackerDirPath, new SimpleFileVisitor<Path>() {        @Override        public FileVisitResult visitFile(Path candidate, BasicFileAttributes attrs) throws IOException {            String fileName = candidate.getFileName().toString();            if (fileName.endsWith(completedSuffix)) {                completedFiles.add(candidate.toAbsolutePath());            }            return FileVisitResult.CONTINUE;        }    });    return completedFiles;}
0
public FileVisitResult visitFile(Path candidate, BasicFileAttributes attrs) throws IOException
{    String fileName = candidate.getFileName().toString();    if (fileName.endsWith(completedSuffix)) {        completedFiles.add(candidate.toAbsolutePath());    }    return FileVisitResult.CONTINUE;}
0
private boolean isFileInTrackerDir(Set<Path> completedFiles, Path path)
{    Path relPath = getRelPathToSpoolDir(path);    Path trackerPath = Paths.get(trackerDirectoryAbsolutePath, relPath.toString() + completedSuffix);    return completedFiles.contains(trackerPath);}
0
private Path getRelPathToSpoolDir(Path path)
{    return spoolDirPath.relativize(path.toAbsolutePath());}
0
 int getListFilesCount()
{    return listFilesCount;}
0
public String getLastFileRead()
{    if (!lastFileRead.isPresent()) {        return null;    }    return lastFileRead.get().getFile().getAbsolutePath();}
0
public Event readEvent() throws IOException
{    List<Event> events = readEvents(1);    if (!events.isEmpty()) {        return events.get(0);    } else {        return null;    }}
0
public List<Event> readEvents(int numEvents) throws IOException
{    if (!committed) {        if (!currentFile.isPresent()) {            throw new IllegalStateException("File should not roll when " + "commit is outstanding.");        }                currentFile.get().getDeserializer().reset();    } else {                if (!currentFile.isPresent()) {            currentFile = getNextFile();        }                if (!currentFile.isPresent()) {            return Collections.emptyList();        }    }    List<Event> events = readDeserializerEvents(numEvents);    /* It's possible that the last read took us just up to a file boundary.     * If so, try to roll to the next file, if there is one.     * Loop until events is not empty or there is no next file in case of 0 byte files */    while (events.isEmpty()) {                retireCurrentFile();        currentFile = getNextFile();        if (!currentFile.isPresent()) {            return Collections.emptyList();        }        events = readDeserializerEvents(numEvents);    }    fillHeader(events);    committed = false;    lastFileRead = currentFile;    return events;}
1
private List<Event> readDeserializerEvents(int numEvents) throws IOException
{    EventDeserializer des = currentFile.get().getDeserializer();    List<Event> events = des.readEvents(numEvents);    if (events.isEmpty() && firstTimeRead) {        events.add(EventBuilder.withBody(new byte[0]));    }    firstTimeRead = false;    return events;}
0
private void fillHeader(List<Event> events)
{    if (annotateFileName) {        String filename = currentFile.get().getFile().getAbsolutePath();        for (Event event : events) {            event.getHeaders().put(fileNameHeader, filename);        }    }    if (annotateBaseName) {        String basename = currentFile.get().getFile().getName();        for (Event event : events) {            event.getHeaders().put(baseNameHeader, basename);        }    }}
0
public void close() throws IOException
{    if (currentFile.isPresent()) {        currentFile.get().getDeserializer().close();        currentFile = Optional.absent();    }}
0
public void commit() throws IOException
{    if (!committed && currentFile.isPresent()) {        currentFile.get().getDeserializer().mark();        committed = true;    }}
0
private void retireCurrentFile() throws IOException
{    Preconditions.checkState(currentFile.isPresent());    File fileToRoll = new File(currentFile.get().getFile().getAbsolutePath());    currentFile.get().getDeserializer().close();        if (fileToRoll.lastModified() != currentFile.get().getLastModified()) {        String message = "File has been modified since being read: " + fileToRoll;        throw new IllegalStateException(message);    }    if (fileToRoll.length() != currentFile.get().getLength()) {        String message = "File has changed size since being read: " + fileToRoll;        throw new IllegalStateException(message);    }    if (deletePolicy.equalsIgnoreCase(DeletePolicy.NEVER.name())) {        if (trackingPolicy == TrackingPolicy.RENAME) {            rollCurrentFile(fileToRoll);        } else {            rollCurrentFileInTrackerDir(fileToRoll);        }    } else if (deletePolicy.equalsIgnoreCase(DeletePolicy.IMMEDIATE.name())) {        deleteCurrentFile(fileToRoll);    } else {                throw new IllegalArgumentException("Unsupported delete policy: " + deletePolicy);    }}
0
private void rollCurrentFile(File fileToRoll) throws IOException
{    File dest = new File(fileToRoll.getPath() + completedSuffix);            if (dest.exists() && PlatformDetect.isWindows()) {        /*       * If we are here, it means the completed file already exists. In almost       * every case this means the user is violating an assumption of Flume       * (that log files are placed in the spooling directory with unique       * names). However, there is a corner case on Windows systems where the       * file was already rolled but the rename was not atomic. If that seems       * likely, we let it pass with only a warning.       */        if (com.google.common.io.Files.equal(currentFile.get().getFile(), dest)) {                        boolean deleted = fileToRoll.delete();            if (!deleted) {                                sourceCounter.incrementGenericProcessingFail();            }        } else {            String message = "File name has been re-used with different" + " files. Spooling assumptions violated for " + dest;            throw new IllegalStateException(message);        }        } else if (dest.exists()) {        String message = "File name has been re-used with different" + " files. Spooling assumptions violated for " + dest;        throw new IllegalStateException(message);        } else {        boolean renamed = fileToRoll.renameTo(dest);        if (renamed) {                                    deleteMetaFile();        } else {            /* If we are here then the file cannot be renamed for a reason other         * than that the destination file exists (actually, that remains         * possible w/ small probability due to TOC-TOU conditions).*/            String message = "Unable to move " + fileToRoll + " to " + dest + ". This will likely cause duplicate events. Please verify that " + "flume has sufficient permissions to perform these operations.";            throw new FlumeException(message);        }    }}
1
private void rollCurrentFileInTrackerDir(File fileToRoll) throws IOException
{    Path path = fileToRoll.toPath();    Path relToRoll = getRelPathToSpoolDir(path);    File dest = new File(trackerDirectory.getPath(), relToRoll + completedSuffix);        if (dest.exists()) {        String message = "File name has been re-used with different" + " files. Spooling assumptions violated for " + dest;        throw new IllegalStateException(message);    }            dest.getParentFile().mkdirs();    if (!dest.createNewFile()) {        throw new IOException("Could not create tracker file: " + dest);    }}
1
private void deleteCurrentFile(File fileToDelete) throws IOException
{        if (!fileToDelete.exists()) {                return;    }    if (!fileToDelete.delete()) {        throw new IOException("Unable to delete spool file: " + fileToDelete);    }        deleteMetaFile();}
1
private Optional<FileInfo> getNextFile()
{    List<File> candidateFiles = Collections.emptyList();    if (consumeOrder != ConsumeOrder.RANDOM || candidateFileIter == null || !candidateFileIter.hasNext()) {        candidateFiles = getCandidateFiles(spoolDirectory.toPath());        listFilesCount++;        candidateFileIter = candidateFiles.iterator();    }    if (!candidateFileIter.hasNext()) {                return Optional.absent();    }    File selectedFile = candidateFileIter.next();    if (consumeOrder == ConsumeOrder.RANDOM) {                return openFile(selectedFile);    } else if (consumeOrder == ConsumeOrder.YOUNGEST) {        for (File candidateFile : candidateFiles) {            long compare = selectedFile.lastModified() - candidateFile.lastModified();            if (compare == 0) {                                selectedFile = smallerLexicographical(selectedFile, candidateFile);            } else if (compare < 0) {                                selectedFile = candidateFile;            }        }    } else {                for (File candidateFile : candidateFiles) {            long compare = selectedFile.lastModified() - candidateFile.lastModified();            if (compare == 0) {                                selectedFile = smallerLexicographical(selectedFile, candidateFile);            } else if (compare > 0) {                                selectedFile = candidateFile;            }        }    }    firstTimeRead = true;    return openFile(selectedFile);}
0
private File smallerLexicographical(File f1, File f2)
{    if (f1.getName().compareTo(f2.getName()) < 0) {        return f1;    }    return f2;}
0
private Optional<FileInfo> openFile(File file)
{    try {                String nextPath = file.getPath();        PositionTracker tracker = DurablePositionTracker.getInstance(metaFile, nextPath);        if (!tracker.getTarget().equals(nextPath)) {            tracker.close();            deleteMetaFile();            tracker = DurablePositionTracker.getInstance(metaFile, nextPath);        }                Preconditions.checkState(tracker.getTarget().equals(nextPath), "Tracker target %s does not equal expected filename %s", tracker.getTarget(), nextPath);        ResettableInputStream in = new ResettableFileInputStream(file, tracker, ResettableFileInputStream.DEFAULT_BUF_SIZE, inputCharset, decodeErrorPolicy);        EventDeserializer deserializer = EventDeserializerFactory.getInstance(deserializerType, deserializerContext, in);        return Optional.of(new FileInfo(file, deserializer));    } catch (FileNotFoundException e) {                        return Optional.absent();    } catch (IOException e) {                sourceCounter.incrementGenericProcessingFail();        return Optional.absent();    }}
1
private void deleteMetaFile() throws IOException
{    if (metaFile.exists() && !metaFile.delete()) {        throw new IOException("Unable to delete old meta file " + metaFile);    }}
0
public long getLength()
{    return length;}
0
public long getLastModified()
{    return lastModified;}
0
public EventDeserializer getDeserializer()
{    return deserializer;}
0
public File getFile()
{    return file;}
0
public Builder spoolDirectory(File directory)
{    this.spoolDirectory = directory;    return this;}
0
public Builder completedSuffix(String completedSuffix)
{    this.completedSuffix = completedSuffix;    return this;}
0
public Builder includePattern(String includePattern)
{    this.includePattern = includePattern;    return this;}
0
public Builder ignorePattern(String ignorePattern)
{    this.ignorePattern = ignorePattern;    return this;}
0
public Builder trackerDirPath(String trackerDirPath)
{    this.trackerDirPath = trackerDirPath;    return this;}
0
public Builder annotateFileName(Boolean annotateFileName)
{    this.annotateFileName = annotateFileName;    return this;}
0
public Builder fileNameHeader(String fileNameHeader)
{    this.fileNameHeader = fileNameHeader;    return this;}
0
public Builder annotateBaseName(Boolean annotateBaseName)
{    this.annotateBaseName = annotateBaseName;    return this;}
0
public Builder baseNameHeader(String baseNameHeader)
{    this.baseNameHeader = baseNameHeader;    return this;}
0
public Builder deserializerType(String deserializerType)
{    this.deserializerType = deserializerType;    return this;}
0
public Builder deserializerContext(Context deserializerContext)
{    this.deserializerContext = deserializerContext;    return this;}
0
public Builder deletePolicy(String deletePolicy)
{    this.deletePolicy = deletePolicy;    return this;}
0
public Builder trackingPolicy(String trackingPolicy)
{    this.trackingPolicy = trackingPolicy;    return this;}
0
public Builder inputCharset(String inputCharset)
{    this.inputCharset = inputCharset;    return this;}
0
public Builder recursiveDirectorySearch(boolean recursiveDirectorySearch)
{    this.recursiveDirectorySearch = recursiveDirectorySearch;    return this;}
0
public Builder decodeErrorPolicy(DecodeErrorPolicy decodeErrorPolicy)
{    this.decodeErrorPolicy = decodeErrorPolicy;    return this;}
0
public Builder consumeOrder(ConsumeOrder consumeOrder)
{    this.consumeOrder = consumeOrder;    return this;}
0
public Builder sourceCounter(SourceCounter sourceCounter)
{    this.sourceCounter = sourceCounter;    return this;}
0
public ReliableSpoolingFileEventReader build() throws IOException
{    return new ReliableSpoolingFileEventReader(spoolDirectory, completedSuffix, includePattern, ignorePattern, trackerDirPath, annotateFileName, fileNameHeader, annotateBaseName, baseNameHeader, deserializerType, deserializerContext, deletePolicy, trackingPolicy, inputCharset, decodeErrorPolicy, consumeOrder, recursiveDirectorySearch, sourceCounter);}
0
public Event readEvent() throws IOException
{    String line = reader.readLine();    if (line != null) {        return EventBuilder.withBody(line, Charsets.UTF_8);    } else {        return null;    }}
0
public List<Event> readEvents(int n) throws IOException
{    List<Event> events = Lists.newLinkedList();    while (events.size() < n) {        Event event = readEvent();        if (event != null) {            events.add(event);        } else {            break;        }    }    return events;}
0
public void close() throws IOException
{    reader.close();}
0
public static boolean configure(Object target, Context context)
{    if (target instanceof Configurable) {        ((Configurable) target).configure(context);        return true;    }    return false;}
0
public static boolean configure(Object target, ComponentConfiguration conf)
{    if (target instanceof ConfigurableComponent) {        ((ConfigurableComponent) target).configure(conf);        return true;    }    return false;}
0
public static void ensureRequiredNonNull(Context context, String... keys)
{    for (String key : keys) {        if (!context.getParameters().containsKey(key) || context.getParameters().get(key) == null) {            throw new IllegalArgumentException("Required parameter " + key + " must exist and may not be null");        }    }}
0
public static void ensureOptionalNonNull(Context context, String... keys)
{    for (String key : keys) {        if (context.getParameters().containsKey(key) && context.getParameters().get(key) == null) {            throw new IllegalArgumentException("Optional parameter " + key + " may not be null");        }    }}
0
public synchronized Long get(String name)
{    return getCounter(name).get();}
0
public synchronized Long incrementAndGet(String name)
{    return getCounter(name).incrementAndGet();}
0
public synchronized Long addAndGet(String name, Long delta)
{    return getCounter(name).addAndGet(delta);}
0
public synchronized void add(CounterGroup counterGroup)
{    synchronized (counterGroup) {        for (Entry<String, AtomicLong> entry : counterGroup.getCounters().entrySet()) {            addAndGet(entry.getKey(), entry.getValue().get());        }    }}
0
public synchronized void set(String name, Long value)
{    getCounter(name).set(value);}
0
public synchronized AtomicLong getCounter(String name)
{    if (!counters.containsKey(name)) {        counters.put(name, new AtomicLong());    }    return counters.get(name);}
0
public synchronized String toString()
{    return "{ name:" + name + " counters:" + counters + " }";}
0
public synchronized String getName()
{    return name;}
0
public synchronized void setName(String name)
{    this.name = name;}
0
public synchronized HashMap<String, AtomicLong> getCounters()
{    return counters;}
0
public synchronized void setCounters(HashMap<String, AtomicLong> counters)
{    this.counters = counters;}
0
public static String dumpEvent(Event event)
{    return dumpEvent(event, DEFAULT_MAX_BYTES);}
0
public static String dumpEvent(Event event, int maxBytes)
{    StringBuilder buffer = new StringBuilder();    if (event == null || event.getBody() == null) {        buffer.append("null");    } else if (event.getBody().length == 0) {        } else {        byte[] body = event.getBody();        byte[] data = Arrays.copyOf(body, Math.min(body.length, maxBytes));        ByteArrayOutputStream out = new ByteArrayOutputStream();        try {            HexDump.dump(data, 0, out, 0);            String hexDump = new String(out.toByteArray());                        if (hexDump.startsWith(HEXDUMP_OFFSET)) {                hexDump = hexDump.substring(HEXDUMP_OFFSET.length());            }            buffer.append(hexDump);        } catch (Exception e) {            if (LOGGER.isInfoEnabled()) {                            }            buffer.append("...Exception while dumping: ").append(e.getMessage());        }        String result = buffer.toString();        if (result.endsWith(EOL) && buffer.length() > EOL.length()) {            buffer.delete(buffer.length() - EOL.length(), buffer.length()).toString();        }    }    return "{ headers:" + event.getHeaders() + " body:" + buffer + " }";}
1
public static boolean containsTag(String in)
{    return tagPattern.matcher(in).find();}
0
public static String expandShorthand(char c)
{        switch(c) {        case 'a':            return "weekday_short";        case 'A':            return "weekday_full";        case 'b':            return "monthname_short";        case 'B':            return "monthname_full";        case 'c':            return "datetime";        case 'd':                        return "day_of_month_xx";        case 'e':                        return "day_of_month_x";        case 'D':                        return "date_short";        case 'H':            return "hour_24_xx";        case 'I':            return "hour_12_xx";        case 'j':                        return "day_of_year_xxx";        case 'k':                        return "hour_24";        case 'l':                        return "hour_12";        case 'm':            return "month_xx";        case 'n':                        return "month_x";        case 'M':            return "minute_xx";        case 'p':            return "am_pm";        case 's':            return "unix_seconds";        case 'S':            return "seconds_xx";        case 't':                        return "unix_millis";        case 'y':            return "year_xx";        case 'Y':            return "year_xxxx";        case 'z':            return "timezone_delta";        default:                        return "" + c;    }}
0
public static String replaceShorthand(char c, Map<String, String> headers)
{    return replaceShorthand(c, headers, false, 0, 0);}
0
public static String replaceShorthand(char c, Map<String, String> headers, boolean needRounding, int unit, int roundDown)
{    return replaceShorthand(c, headers, null, needRounding, unit, roundDown, false);}
0
public static String replaceShorthand(char c, Map<String, String> headers, TimeZone timeZone, boolean needRounding, int unit, int roundDown, boolean useLocalTimestamp)
{    long ts = 0;    if (useLocalTimestamp) {        ts = clock.currentTimeMillis();    }    return replaceShorthand(c, headers, timeZone, needRounding, unit, roundDown, false, ts);}
0
protected HashMap<String, SimpleDateFormat> initialValue()
{    return new HashMap<String, SimpleDateFormat>();}
0
protected static SimpleDateFormat getSimpleDateFormat(String string)
{    HashMap<String, SimpleDateFormat> localCache = simpleDateFormatCache.get();    SimpleDateFormat simpleDateFormat = localCache.get(string);    if (simpleDateFormat == null) {        simpleDateFormat = new SimpleDateFormat(string);        localCache.put(string, simpleDateFormat);        simpleDateFormatCache.set(localCache);    }    return simpleDateFormat;}
0
protected static String replaceStaticString(String key)
{    String replacementString = "";    switch(key.toLowerCase()) {        case "localhost":            replacementString = InetAddressCache.hostName;            break;        case "ip":            replacementString = InetAddressCache.hostAddress;            break;        case "fqdn":            replacementString = InetAddressCache.canonicalHostName;            break;        default:            throw new RuntimeException("The static escape string '" + key + "'" + " was provided but does not match any of (localhost,IP,FQDN)");    }    return replacementString;}
0
protected static String replaceShorthand(char c, Map<String, String> headers, TimeZone timeZone, boolean needRounding, int unit, int roundDown, boolean useLocalTimestamp, long ts)
{    String timestampHeader = null;    try {        if (!useLocalTimestamp) {            timestampHeader = headers.get("timestamp");            Preconditions.checkNotNull(timestampHeader, "Expected timestamp in " + "the Flume event headers, but it was null");            ts = Long.valueOf(timestampHeader);        } else {            timestampHeader = String.valueOf(ts);        }    } catch (NumberFormatException e) {        throw new RuntimeException("Flume wasn't able to parse timestamp header" + " in the event to resolve time based bucketing. Please check that" + " you're correctly populating timestamp header (for example using" + " TimestampInterceptor source interceptor).", e);    }    if (needRounding) {        ts = roundDown(roundDown, unit, ts, timeZone);    }        String formatString = "";    switch(c) {        case '%':            return "%";        case 'a':            formatString = "EEE";            break;        case 'A':            formatString = "EEEE";            break;        case 'b':            formatString = "MMM";            break;        case 'B':            formatString = "MMMM";            break;        case 'c':            formatString = "EEE MMM d HH:mm:ss yyyy";            break;        case 'd':            formatString = "dd";            break;        case 'e':            formatString = "d";            break;        case 'D':            formatString = "MM/dd/yy";            break;        case 'H':            formatString = "HH";            break;        case 'I':            formatString = "hh";            break;        case 'j':            formatString = "DDD";            break;        case 'k':            formatString = "H";            break;        case 'l':            formatString = "h";            break;        case 'm':            formatString = "MM";            break;        case 'M':            formatString = "mm";            break;        case 'n':            formatString = "M";            break;        case 'p':            formatString = "a";            break;        case 's':            return "" + (ts / 1000);        case 'S':            formatString = "ss";            break;        case 't':                        return timestampHeader;        case 'y':            formatString = "yy";            break;        case 'Y':            formatString = "yyyy";            break;        case 'z':            formatString = "ZZZ";            break;        default:                        return "";    }    SimpleDateFormat format = getSimpleDateFormat(formatString);    if (timeZone != null) {        format.setTimeZone(timeZone);    } else {        format.setTimeZone(TimeZone.getDefault());    }    Date date = new Date(ts);    return format.format(date);}
0
private static long roundDown(int roundDown, int unit, long ts, TimeZone timeZone)
{    long timestamp = ts;    if (roundDown <= 0) {        roundDown = 1;    }    switch(unit) {        case Calendar.SECOND:            timestamp = TimestampRoundDownUtil.roundDownTimeStampSeconds(ts, roundDown, timeZone);            break;        case Calendar.MINUTE:            timestamp = TimestampRoundDownUtil.roundDownTimeStampMinutes(ts, roundDown, timeZone);            break;        case Calendar.HOUR_OF_DAY:            timestamp = TimestampRoundDownUtil.roundDownTimeStampHours(ts, roundDown, timeZone);            break;        default:            timestamp = ts;            break;    }    return timestamp;}
0
public static String escapeString(String in, Map<String, String> headers)
{    return escapeString(in, headers, false, 0, 0);}
0
public static String escapeString(String in, Map<String, String> headers, boolean needRounding, int unit, int roundDown)
{    return escapeString(in, headers, null, needRounding, unit, roundDown, false);}
0
public static String escapeString(String in, Map<String, String> headers, TimeZone timeZone, boolean needRounding, int unit, int roundDown, boolean useLocalTimeStamp)
{    long ts = clock.currentTimeMillis();    Matcher matcher = tagPattern.matcher(in);    StringBuffer sb = new StringBuffer();    while (matcher.find()) {        String replacement = "";                if (matcher.group(2) != null) {            replacement = headers.get(matcher.group(2));            if (replacement == null) {                replacement = "";                        }                } else if (matcher.group(3) != null) {            replacement = replaceStaticString(matcher.group(3));        } else {                                                Preconditions.checkState(matcher.group(1) != null && matcher.group(1).length() == 1, "Expected to match single character tag in string " + in);            char c = matcher.group(1).charAt(0);            replacement = replaceShorthand(c, headers, timeZone, needRounding, unit, roundDown, useLocalTimeStamp, ts);        }                                                                                replacement = replacement.replaceAll("\\\\", "\\\\\\\\");        replacement = replacement.replaceAll("\\$", "\\\\\\$");        matcher.appendReplacement(sb, replacement);    }    matcher.appendTail(sb);    return sb.toString();}
0
public static Map<String, String> getEscapeMapping(String in, Map<String, String> headers)
{    return getEscapeMapping(in, headers, false, 0, 0);}
0
public static Map<String, String> getEscapeMapping(String in, Map<String, String> headers, boolean needRounding, int unit, int roundDown)
{    Map<String, String> mapping = new HashMap<String, String>();    Matcher matcher = tagPattern.matcher(in);    while (matcher.find()) {        String replacement = "";                if (matcher.group(2) != null) {            replacement = headers.get(matcher.group(2));            if (replacement == null) {                replacement = "";                        }            mapping.put(matcher.group(2), replacement);        } else {                                                Preconditions.checkState(matcher.group(1) != null && matcher.group(1).length() == 1, "Expected to match single character tag in string " + in);            char c = matcher.group(1).charAt(0);            replacement = replaceShorthand(c, headers, needRounding, unit, roundDown);            mapping.put(expandShorthand(c), replacement);        }    }    return mapping;}
0
public static void setClock(Clock clk)
{    clock = clk;}
0
public static Clock getClock()
{    return clock;}
0
public File nextFile()
{    StringBuilder sb = new StringBuilder();    sb.append(filePrefix).append(seriesTimestamp).append("-");    sb.append(fileIndex.incrementAndGet());    if (extension.length() > 0) {        sb.append(".").append(extension);    }    currentFile = new File(baseDirectory, sb.toString());    return currentFile;}
0
public File getCurrentFile()
{    if (currentFile == null) {        return nextFile();    }    return currentFile;}
0
public void rotate()
{    currentFile = null;}
0
public File getBaseDirectory()
{    return baseDirectory;}
0
public void setBaseDirectory(File baseDirectory)
{    this.baseDirectory = baseDirectory;}
0
public long getSeriesTimestamp()
{    return seriesTimestamp;}
0
public String getPrefix()
{    return filePrefix;}
0
public String getExtension()
{    return extension;}
0
public AtomicInteger getFileIndex()
{    return fileIndex;}
0
public PathManager build(Context context)
{    return new DefaultPathManager(context);}
0
public static PathManager getInstance(String managerType, Context context)
{    Preconditions.checkNotNull(managerType, "path manager type must not be null");        PathManagerType type;    try {        type = PathManagerType.valueOf(managerType.toUpperCase(Locale.ENGLISH));    } catch (IllegalArgumentException e) {                type = PathManagerType.OTHER;    }    Class<? extends PathManager.Builder> builderClass = type.getBuilderClass();        if (builderClass == null) {        try {            Class c = Class.forName(managerType);            if (c != null && PathManager.Builder.class.isAssignableFrom(c)) {                builderClass = (Class<? extends PathManager.Builder>) c;            } else {                String errMessage = "Unable to instantiate Builder from " + managerType + ": does not appear to implement " + PathManager.Builder.class.getName();                throw new FlumeException(errMessage);            }        } catch (ClassNotFoundException ex) {                        throw new FlumeException(ex);        }    }        PathManager.Builder builder;    try {        builder = builderClass.newInstance();    } catch (InstantiationException ex) {        String errMessage = "Cannot instantiate builder: " + managerType;                throw new FlumeException(errMessage, ex);    } catch (IllegalAccessException ex) {        String errMessage = "Cannot instantiate builder: " + managerType;                throw new FlumeException(errMessage, ex);    }    return builder.build(context);}
1
public Class<? extends PathManager.Builder> getBuilderClass()
{    return builderClass;}
0
public File nextFile()
{    StringBuilder sb = new StringBuilder();    String date = formatter.print(LocalDateTime.now());    if (!date.equals(lastRoll)) {        getFileIndex().set(0);        lastRoll = date;    }    sb.append(getPrefix()).append(date).append("-");    sb.append(getFileIndex().incrementAndGet());    if (getExtension().length() > 0) {        sb.append(".").append(getExtension());    }    currentFile = new File(getBaseDirectory(), sb.toString());    return currentFile;}
0
public PathManager build(Context context)
{    return new RollTimePathManager(context);}
0
public byte[] format(Event event)
{    String body = event.getBody().length > 0 ? new String(event.getBody()) : "";    return (body + "\n").getBytes();}
0
public long getChannelSize()
{    return get(COUNTER_CHANNEL_SIZE);}
0
public void setChannelSize(long newSize)
{    set(COUNTER_CHANNEL_SIZE, newSize);}
0
public long getEventPutAttemptCount()
{    return get(COUNTER_EVENT_PUT_ATTEMPT);}
0
public long incrementEventPutAttemptCount()
{    return increment(COUNTER_EVENT_PUT_ATTEMPT);}
0
public long getEventTakeAttemptCount()
{    return get(COUNTER_EVENT_TAKE_ATTEMPT);}
0
public long incrementEventTakeAttemptCount()
{    return increment(COUNTER_EVENT_TAKE_ATTEMPT);}
0
public long getEventPutSuccessCount()
{    return get(COUNTER_EVENT_PUT_SUCCESS);}
0
public long addToEventPutSuccessCount(long delta)
{    return addAndGet(COUNTER_EVENT_PUT_SUCCESS, delta);}
0
public long getEventTakeSuccessCount()
{    return get(COUNTER_EVENT_TAKE_SUCCESS);}
0
public long addToEventTakeSuccessCount(long delta)
{    return addAndGet(COUNTER_EVENT_TAKE_SUCCESS, delta);}
0
public void setChannelCapacity(long capacity)
{    set(COUNTER_CHANNEL_CAPACITY, capacity);}
0
public long getChannelCapacity()
{    return get(COUNTER_CHANNEL_CAPACITY);}
0
public double getChannelFillPercentage()
{    long capacity = getChannelCapacity();    if (capacity != 0L) {        return (getChannelSize() / (double) capacity) * 100;    }    return Double.MAX_VALUE;}
0
protected void xdr_string(String s)
{    byte[] bytes = s.getBytes();    int len = bytes.length;    xdr_int(len);    System.arraycopy(bytes, 0, buffer, offset, len);    offset += len;    pad();}
0
private void pad()
{    int newOffset = ((offset + 3) / 4) * 4;    while (offset < newOffset) {        buffer[offset++] = 0;    }}
0
protected void xdr_int(int i)
{    buffer[offset++] = (byte) ((i >> 24) & 0xff);    buffer[offset++] = (byte) ((i >> 16) & 0xff);    buffer[offset++] = (byte) ((i >> 8) & 0xff);    buffer[offset++] = (byte) (i & 0xff);}
0
public synchronized void sendToGangliaNodes()
{    DatagramPacket packet;    for (SocketAddress addr : addresses) {        try {            packet = new DatagramPacket(buffer, offset, addr);            socket.send(packet);        } catch (Exception ex) {                    }    }    offset = 0;}
1
public void start()
{    try {        socket = new DatagramSocket();        hostname = InetAddress.getLocalHost().getHostName();    } catch (SocketException ex) {                throw new FlumeException("Could not create socket for metrics collection.", ex);    } catch (Exception ex2) {            }    for (HostInfo host : hosts) {        addresses.add(new InetSocketAddress(host.getHostName(), host.getPortNumber()));    }    collectorRunnable.server = this;    if (service.isShutdown() || service.isTerminated()) {        service = Executors.newSingleThreadScheduledExecutor();    }    service.scheduleWithFixedDelay(collectorRunnable, 0, pollFrequency, TimeUnit.SECONDS);}
1
public void stop()
{    service.shutdown();    while (!service.isTerminated()) {        try {                        service.awaitTermination(500, TimeUnit.MILLISECONDS);        } catch (InterruptedException ex) {                        service.shutdownNow();        }    }    addresses.clear();}
1
public void setPollFrequency(int pollFrequency)
{    this.pollFrequency = pollFrequency;}
0
public int getPollFrequency()
{    return pollFrequency;}
0
public void setIsGanglia3(boolean isGanglia3)
{    this.isGanglia3 = isGanglia3;}
0
public boolean isGanglia3()
{    return this.isGanglia3;}
0
protected void createGangliaMessage(String name, String value)
{        name = hostname + "." + name;    xdr_int(0);    String type = "string";    try {        Float.parseFloat(value);        type = "float";    } catch (NumberFormatException ex) {        }        xdr_string(type);    xdr_string(name);    xdr_string(value);    xdr_string(DEFAULT_UNITS);    xdr_int(DEFAULT_SLOPE);    xdr_int(DEFAULT_TMAX);    xdr_int(DEFAULT_DMAX);}
1
protected void createGangliaMessage31(String name, String value)
{            xdr_int(128);        xdr_string(hostname);        xdr_string(name);        xdr_int(0);    String type = "string";    try {        Float.parseFloat(value);        type = "float";    } catch (NumberFormatException ex) {        }        xdr_string(type);        xdr_string(name);        xdr_string(DEFAULT_UNITS);        xdr_int(DEFAULT_SLOPE);        xdr_int(DEFAULT_TMAX);        xdr_int(DEFAULT_DMAX);    xdr_int(1);    /*Num of the entries in extra_value field for Ganglia 3.1.x*/    xdr_string("GROUP");    /*Group attribute*/    xdr_string("flume");    /*Group value*/    this.sendToGangliaNodes();                        xdr_int(133);        xdr_string(hostname);        xdr_string(name);        xdr_int(0);        xdr_string("%s");        xdr_string(value);}
1
public void configure(Context context)
{    this.pollFrequency = context.getInteger(this.CONF_POLL_FREQUENCY, 60);    String localHosts = context.getString(this.CONF_HOSTS);    if (localHosts == null || localHosts.isEmpty()) {        throw new ConfigurationException("Hosts list cannot be empty.");    }    this.hosts = this.getHostsFromString(localHosts);    this.isGanglia3 = context.getBoolean(this.CONF_ISGANGLIA3, false);}
0
private List<HostInfo> getHostsFromString(String hosts) throws FlumeException
{    List<HostInfo> hostInfoList = new ArrayList<HostInfo>();    String[] hostsAndPorts = hosts.split(",");    int i = 0;    for (String host : hostsAndPorts) {        String[] hostAndPort = host.split(":");        if (hostAndPort.length < 2) {                        continue;        }        try {            hostInfoList.add(new HostInfo("ganglia_host-" + String.valueOf(i), hostAndPort[0], Integer.parseInt(hostAndPort[1])));        } catch (Exception e) {                        continue;        }    }    if (hostInfoList.isEmpty()) {        throw new FlumeException("No valid ganglia hosts defined!");    }    return hostInfoList;}
1
public void run()
{    try {        Map<String, Map<String, String>> metricsMap = JMXPollUtil.getAllMBeans();        for (String component : metricsMap.keySet()) {            Map<String, String> attributeMap = metricsMap.get(component);            for (String attribute : attributeMap.keySet()) {                if (isGanglia3) {                    server.createGangliaMessage(GANGLIA_CONTEXT + component + "." + attribute, attributeMap.get(attribute));                } else {                    server.createGangliaMessage31(GANGLIA_CONTEXT + component + "." + attribute, attributeMap.get(attribute));                }                server.sendToGangliaNodes();            }        }    } catch (Throwable t) {            }}
1
public void start()
{    jettyServer = new Server();            HttpConfiguration httpConfiguration = new HttpConfiguration();    ServerConnector connector = new ServerConnector(jettyServer, new HttpConnectionFactory(httpConfiguration));    connector.setReuseAddress(true);    connector.setPort(port);    jettyServer.addConnector(connector);    jettyServer.setHandler(new HTTPMetricsHandler());    try {        jettyServer.start();        while (!jettyServer.isStarted()) {            Thread.sleep(500);        }    } catch (Exception ex) {            }}
1
public void stop()
{    try {        jettyServer.stop();        jettyServer.join();    } catch (Exception ex) {            }}
1
public void configure(Context context)
{    port = context.getInteger(CONFIG_PORT, DEFAULT_PORT);}
0
public void handle(String target, Request r1, HttpServletRequest request, HttpServletResponse response) throws IOException, ServletException
{        if (request.getMethod().equalsIgnoreCase("TRACE") || request.getMethod().equalsIgnoreCase("OPTIONS")) {        response.sendError(HttpServletResponse.SC_FORBIDDEN);        response.flushBuffer();        ((Request) request).setHandled(true);        return;    }    if (target.equals("/")) {        response.setContentType("text/html;charset=utf-8");        response.setStatus(HttpServletResponse.SC_OK);        response.getWriter().write("For Flume metrics please click" + " <a href = \"./metrics\"> here</a>.");        response.flushBuffer();        ((Request) request).setHandled(true);        return;    } else if (target.equalsIgnoreCase("/metrics")) {        response.setContentType("application/json;charset=utf-8");        response.setStatus(HttpServletResponse.SC_OK);        Map<String, Map<String, String>> metricsMap = JMXPollUtil.getAllMBeans();        String json = gson.toJson(metricsMap, mapType);        response.getWriter().write(json);        response.flushBuffer();        ((Request) request).setHandled(true);        return;    }    response.sendError(HttpServletResponse.SC_NOT_FOUND);    response.flushBuffer();}
0
public long addToKafkaEventGetTimer(long delta)
{    return addAndGet(TIMER_KAFKA_EVENT_GET, delta);}
0
public long addToKafkaEventSendTimer(long delta)
{    return addAndGet(TIMER_KAFKA_EVENT_SEND, delta);}
0
public long addToKafkaCommitTimer(long delta)
{    return addAndGet(TIMER_KAFKA_COMMIT, delta);}
0
public long addToRollbackCounter(long delta)
{    return addAndGet(COUNT_ROLLBACK, delta);}
0
public long getKafkaEventGetTimer()
{    return get(TIMER_KAFKA_EVENT_GET);}
0
public long getKafkaEventSendTimer()
{    return get(TIMER_KAFKA_EVENT_SEND);}
0
public long getKafkaCommitTimer()
{    return get(TIMER_KAFKA_COMMIT);}
0
public long getRollbackCount()
{    return get(COUNT_ROLLBACK);}
0
public long addToKafkaEventSendTimer(long delta)
{    return addAndGet(TIMER_KAFKA_EVENT_SEND, delta);}
0
public long incrementRollbackCount()
{    return increment(COUNT_ROLLBACK);}
0
public long getKafkaEventSendTimer()
{    return get(TIMER_KAFKA_EVENT_SEND);}
0
public long getRollbackCount()
{    return get(COUNT_ROLLBACK);}
0
public long addToKafkaEventGetTimer(long delta)
{    return addAndGet(TIMER_KAFKA_EVENT_GET, delta);}
0
public long addToKafkaCommitTimer(long delta)
{    return addAndGet(TIMER_KAFKA_COMMIT, delta);}
0
public long incrementKafkaEmptyCount()
{    return increment(COUNTER_KAFKA_EMPTY);}
0
public long getKafkaCommitTimer()
{    return get(TIMER_KAFKA_COMMIT);}
0
public long getKafkaEventGetTimer()
{    return get(TIMER_KAFKA_EVENT_GET);}
0
public long getKafkaEmptyCount()
{    return get(COUNTER_KAFKA_EMPTY);}
0
public void start()
{    register();    stopTime.set(0L);    for (String counter : counterMap.keySet()) {        counterMap.get(counter).set(0L);    }    startTime.set(System.currentTimeMillis());    }
1
 void register()
{    if (!registered) {        try {            ObjectName objName = new ObjectName("org.apache.flume." + type.name().toLowerCase(Locale.ENGLISH) + ":type=" + this.name);            if (ManagementFactory.getPlatformMBeanServer().isRegistered(objName)) {                                ManagementFactory.getPlatformMBeanServer().unregisterMBean(objName);                            }            ManagementFactory.getPlatformMBeanServer().registerMBean(this, objName);                        registered = true;        } catch (Exception ex) {                    }    }}
1
public void stop()
{        stopTime.set(System.currentTimeMillis());                final String typePrefix = type.name().toLowerCase(Locale.ENGLISH);                        final List<String> mapKeys = new ArrayList<String>(counterMap.keySet());    Collections.sort(mapKeys);        for (final String counterMapKey : mapKeys) {                final long counterMapValue = get(counterMapKey);            }}
1
public long getStartTime()
{    return startTime.get();}
0
public long getStopTime()
{    return stopTime.get();}
0
public final String toString()
{    StringBuilder sb = new StringBuilder(type.name()).append(":");    sb.append(name).append("{");    boolean first = true;    Iterator<String> counterIterator = counterMap.keySet().iterator();    while (counterIterator.hasNext()) {        if (first) {            first = false;        } else {            sb.append(", ");        }        String counterName = counterIterator.next();        sb.append(counterName).append("=").append(get(counterName));    }    sb.append("}");    return sb.toString();}
0
protected long get(String counter)
{    return counterMap.get(counter).get();}
0
protected void set(String counter, long value)
{    counterMap.get(counter).set(value);}
0
protected long addAndGet(String counter, long delta)
{    return counterMap.get(counter).addAndGet(delta);}
0
protected long increment(String counter)
{    return counterMap.get(counter).incrementAndGet();}
0
public String getType()
{    return type.name();}
0
public Class<? extends MonitorService> getMonitorClass()
{    return this.monitoringClass;}
0
public long getConnectionCreatedCount()
{    return get(COUNTER_CONNECTION_CREATED);}
0
public long incrementConnectionCreatedCount()
{    return increment(COUNTER_CONNECTION_CREATED);}
0
public long getConnectionClosedCount()
{    return get(COUNTER_CONNECTION_CLOSED);}
0
public long incrementConnectionClosedCount()
{    return increment(COUNTER_CONNECTION_CLOSED);}
0
public long getConnectionFailedCount()
{    return get(COUNTER_CONNECTION_FAILED);}
0
public long incrementConnectionFailedCount()
{    return increment(COUNTER_CONNECTION_FAILED);}
0
public long getBatchEmptyCount()
{    return get(COUNTER_BATCH_EMPTY);}
0
public long incrementBatchEmptyCount()
{    return increment(COUNTER_BATCH_EMPTY);}
0
public long getBatchUnderflowCount()
{    return get(COUNTER_BATCH_UNDERFLOW);}
0
public long incrementBatchUnderflowCount()
{    return increment(COUNTER_BATCH_UNDERFLOW);}
0
public long getBatchCompleteCount()
{    return get(COUNTER_BATCH_COMPLETE);}
0
public long incrementBatchCompleteCount()
{    return increment(COUNTER_BATCH_COMPLETE);}
0
public long getEventDrainAttemptCount()
{    return get(COUNTER_EVENT_DRAIN_ATTEMPT);}
0
public long incrementEventDrainAttemptCount()
{    return increment(COUNTER_EVENT_DRAIN_ATTEMPT);}
0
public long addToEventDrainAttemptCount(long delta)
{    return addAndGet(COUNTER_EVENT_DRAIN_ATTEMPT, delta);}
0
public long getEventDrainSuccessCount()
{    return get(COUNTER_EVENT_DRAIN_SUCCESS);}
0
public long incrementEventDrainSuccessCount()
{    return increment(COUNTER_EVENT_DRAIN_SUCCESS);}
0
public long addToEventDrainSuccessCount(long delta)
{    return addAndGet(COUNTER_EVENT_DRAIN_SUCCESS, delta);}
0
public long incrementEventWriteFail()
{    return increment(COUNTER_EVENT_WRITE_FAIL);}
0
public long getEventWriteFail()
{    return get(COUNTER_EVENT_WRITE_FAIL);}
0
public long incrementChannelReadFail()
{    return increment(COUNTER_CHANNEL_READ_FAIL);}
0
public long getChannelReadFail()
{    return get(COUNTER_CHANNEL_READ_FAIL);}
0
public long incrementEventWriteOrChannelFail(Throwable t)
{    if (t instanceof ChannelException) {        return incrementChannelReadFail();    }    return incrementEventWriteFail();}
0
public long getEventReceivedCount()
{    return get(COUNTER_EVENTS_RECEIVED);}
0
public long incrementEventReceivedCount()
{    return increment(COUNTER_EVENTS_RECEIVED);}
0
public long addToEventReceivedCount(long delta)
{    return addAndGet(COUNTER_EVENTS_RECEIVED, delta);}
0
public long getEventAcceptedCount()
{    return get(COUNTER_EVENTS_ACCEPTED);}
0
public long incrementEventAcceptedCount()
{    return increment(COUNTER_EVENTS_ACCEPTED);}
0
public long addToEventAcceptedCount(long delta)
{    return addAndGet(COUNTER_EVENTS_ACCEPTED, delta);}
0
public long getAppendReceivedCount()
{    return get(COUNTER_APPEND_RECEIVED);}
0
public long incrementAppendReceivedCount()
{    return increment(COUNTER_APPEND_RECEIVED);}
0
public long getAppendAcceptedCount()
{    return get(COUNTER_APPEND_ACCEPTED);}
0
public long incrementAppendAcceptedCount()
{    return increment(COUNTER_APPEND_ACCEPTED);}
0
public long getAppendBatchReceivedCount()
{    return get(COUNTER_APPEND_BATCH_RECEIVED);}
0
public long incrementAppendBatchReceivedCount()
{    return increment(COUNTER_APPEND_BATCH_RECEIVED);}
0
public long getAppendBatchAcceptedCount()
{    return get(COUNTER_APPEND_BATCH_ACCEPTED);}
0
public long incrementAppendBatchAcceptedCount()
{    return increment(COUNTER_APPEND_BATCH_ACCEPTED);}
0
public long getOpenConnectionCount()
{    return get(COUNTER_OPEN_CONNECTION_COUNT);}
0
public void setOpenConnectionCount(long openConnectionCount)
{    set(COUNTER_OPEN_CONNECTION_COUNT, openConnectionCount);}
0
public long incrementEventReadFail()
{    return increment(COUNTER_EVENT_READ_FAIL);}
0
public long getEventReadFail()
{    return get(COUNTER_EVENT_READ_FAIL);}
0
public long incrementChannelWriteFail()
{    return increment(COUNTER_CHANNEL_WRITE_FAIL);}
0
public long getChannelWriteFail()
{    return get(COUNTER_CHANNEL_WRITE_FAIL);}
0
public long incrementGenericProcessingFail()
{    return increment(COUNTER_GENERIC_PROCESSING_FAIL);}
0
public long getGenericProcessingFail()
{    return get(COUNTER_GENERIC_PROCESSING_FAIL);}
0
public long incrementEventReadOrChannelFail(Throwable t)
{    if (t instanceof ChannelException) {        return incrementChannelWriteFail();    }    return incrementEventReadFail();}
0
public static Map<String, Map<String, String>> getAllMBeans()
{    Map<String, Map<String, String>> mbeanMap = Maps.newHashMap();    Set<ObjectInstance> queryMBeans = null;    try {        queryMBeans = mbeanServer.queryMBeans(null, null);    } catch (Exception ex) {                Throwables.propagate(ex);    }    for (ObjectInstance obj : queryMBeans) {        try {            if (!obj.getObjectName().toString().startsWith("org.apache.flume")) {                continue;            }            MBeanAttributeInfo[] attrs = mbeanServer.getMBeanInfo(obj.getObjectName()).getAttributes();            String[] strAtts = new String[attrs.length];            for (int i = 0; i < strAtts.length; i++) {                strAtts[i] = attrs[i].getName();            }            AttributeList attrList = mbeanServer.getAttributes(obj.getObjectName(), strAtts);            String component = obj.getObjectName().toString().substring(obj.getObjectName().toString().indexOf('=') + 1);            Map<String, String> attrMap = Maps.newHashMap();            for (Object attr : attrList) {                Attribute localAttr = (Attribute) attr;                if (localAttr.getName().equalsIgnoreCase("type")) {                    component = localAttr.getValue() + "." + component;                }                attrMap.put(localAttr.getName(), localAttr.getValue().toString());            }            mbeanMap.put(component, attrMap);        } catch (Exception e) {                    }    }    return mbeanMap;}
1
public void initialize()
{}
0
public Event intercept(Event event)
{    Map<String, String> headers = event.getHeaders();    if (preserveExisting && headers.containsKey(header)) {        return event;    }    if (host != null) {        headers.put(header, host);    }    return event;}
0
public List<Event> intercept(List<Event> events)
{    for (Event event : events) {        intercept(event);    }    return events;}
0
public void close()
{}
0
public Interceptor build()
{    return new HostInterceptor(preserveExisting, useIP, header);}
0
public void configure(Context context)
{    preserveExisting = context.getBoolean(PRESERVE, PRESERVE_DFLT);    useIP = context.getBoolean(USE_IP, USE_IP_DFLT);    header = context.getString(HOST_HEADER, HOST);}
0
private static Class<? extends Builder> lookup(String name)
{    try {        return InterceptorType.valueOf(name.toUpperCase(Locale.ENGLISH)).getBuilderClass();    } catch (IllegalArgumentException e) {        return null;    }}
0
public static Builder newInstance(String name) throws ClassNotFoundException, InstantiationException, IllegalAccessException
{    Class<? extends Builder> clazz = lookup(name);    if (clazz == null) {        clazz = (Class<? extends Builder>) Class.forName(name);    }    return clazz.newInstance();}
0
public void setInterceptors(List<Interceptor> interceptors)
{    this.interceptors = interceptors;}
0
public Event intercept(Event event)
{    for (Interceptor interceptor : interceptors) {        if (event == null) {            return null;        }        event = interceptor.intercept(event);    }    return event;}
0
public List<Event> intercept(List<Event> events)
{    for (Interceptor interceptor : interceptors) {        if (events.isEmpty()) {            return events;        }        events = interceptor.intercept(events);        Preconditions.checkNotNull(events, "Event list returned null from interceptor %s", interceptor);    }    return events;}
0
public void initialize()
{    Iterator<Interceptor> iter = interceptors.iterator();    while (iter.hasNext()) {        Interceptor interceptor = iter.next();        interceptor.initialize();    }}
0
public void close()
{    Iterator<Interceptor> iter = interceptors.iterator();    while (iter.hasNext()) {        Interceptor interceptor = iter.next();        interceptor.close();    }}
0
public Class<? extends Interceptor.Builder> getBuilderClass()
{    return builderClass;}
0
public void initialize()
{}
0
public void close()
{}
0
public Event intercept(Event event)
{    Matcher matcher = regex.matcher(new String(event.getBody(), Charsets.UTF_8));    Map<String, String> headers = event.getHeaders();    if (matcher.find()) {        for (int group = 0, count = matcher.groupCount(); group < count; group++) {            int groupIndex = group + 1;            if (groupIndex > serializers.size()) {                if (logger.isDebugEnabled()) {                                    }                break;            }            NameAndSerializer serializer = serializers.get(group);            if (logger.isDebugEnabled()) {                            }            headers.put(serializer.headerName, serializer.serializer.serialize(matcher.group(groupIndex)));        }    }    return event;}
1
public List<Event> intercept(List<Event> events)
{    List<Event> intercepted = Lists.newArrayListWithCapacity(events.size());    for (Event event : events) {        Event interceptedEvent = intercept(event);        if (interceptedEvent != null) {            intercepted.add(interceptedEvent);        }    }    return intercepted;}
0
public void configure(Context context)
{    String regexString = context.getString(REGEX);    Preconditions.checkArgument(!StringUtils.isEmpty(regexString), "Must supply a valid regex string");    regex = Pattern.compile(regexString);    regex.pattern();    regex.matcher("").groupCount();    configureSerializers(context);}
0
private void configureSerializers(Context context)
{    String serializerListStr = context.getString(SERIALIZERS);    Preconditions.checkArgument(!StringUtils.isEmpty(serializerListStr), "Must supply at least one name and serializer");    String[] serializerNames = serializerListStr.split("\\s+");    Context serializerContexts = new Context(context.getSubProperties(SERIALIZERS + "."));    serializerList = Lists.newArrayListWithCapacity(serializerNames.length);    for (String serializerName : serializerNames) {        Context serializerContext = new Context(serializerContexts.getSubProperties(serializerName + "."));        String type = serializerContext.getString("type", "DEFAULT");        String name = serializerContext.getString("name");        Preconditions.checkArgument(!StringUtils.isEmpty(name), "Supplied name cannot be empty.");        if ("DEFAULT".equals(type)) {            serializerList.add(new NameAndSerializer(name, defaultSerializer));        } else {            serializerList.add(new NameAndSerializer(name, getCustomSerializer(type, serializerContext)));        }    }}
0
private RegexExtractorInterceptorSerializer getCustomSerializer(String clazzName, Context context)
{    try {        RegexExtractorInterceptorSerializer serializer = (RegexExtractorInterceptorSerializer) Class.forName(clazzName).newInstance();        serializer.configure(context);        return serializer;    } catch (Exception e) {                Throwables.propagate(e);    }    return defaultSerializer;}
1
public Interceptor build()
{    Preconditions.checkArgument(regex != null, "Regex pattern was misconfigured");    Preconditions.checkArgument(serializerList.size() > 0, "Must supply a valid group match id list");    return new RegexExtractorInterceptor(regex, serializerList);}
0
public void configure(Context context)
{    String pattern = context.getString("pattern");    Preconditions.checkArgument(!StringUtils.isEmpty(pattern), "Must configure with a valid pattern");    formatter = DateTimeFormat.forPattern(pattern);}
0
public String serialize(String value)
{    DateTime dateTime = formatter.parseDateTime(value);    return Long.toString(dateTime.getMillis());}
0
public void configure(ComponentConfiguration conf)
{}
0
public String serialize(String value)
{    return value;}
0
public void configure(Context context)
{}
0
public void configure(ComponentConfiguration conf)
{}
0
public void initialize()
{}
0
public Event intercept(Event event)
{    if (!excludeEvents) {        if (regex.matcher(new String(event.getBody())).find()) {            return event;        } else {            return null;        }    } else {        if (regex.matcher(new String(event.getBody())).find()) {            return null;        } else {            return event;        }    }}
0
public List<Event> intercept(List<Event> events)
{    List<Event> out = Lists.newArrayList();    for (Event event : events) {        Event outEvent = intercept(event);        if (outEvent != null) {            out.add(outEvent);        }    }    return out;}
0
public void close()
{}
0
public void configure(Context context)
{    String regexString = context.getString(REGEX, DEFAULT_REGEX);    regex = Pattern.compile(regexString);    excludeEvents = context.getBoolean(EXCLUDE_EVENTS, DEFAULT_EXCLUDE_EVENTS);}
0
public Interceptor build()
{        return new RegexFilteringInterceptor(regex, excludeEvents);}
1
public void initialize()
{}
0
public void close()
{}
0
public List<Event> intercept(final List<Event> events)
{    for (final Event event : events) {        intercept(event);    }    return events;}
0
public Event intercept(final Event event)
{    assert event != null : "Missing Flume event while intercepting";    try {        final Map<String, String> headers = event.getHeaders();                if (withName != null && headers.remove(withName) != null) {            LOG.trace("Removed header \"{}\" for event: {}", withName, event);        }                if (fromList != null || matchRegex != null) {            final Iterator<String> headerIterator = headers.keySet().iterator();            List<String> removedHeaders = new LinkedList<>();            while (headerIterator.hasNext()) {                final String currentHeader = headerIterator.next();                if (fromList != null && fromList.contains(currentHeader)) {                    headerIterator.remove();                    removedHeaders.add(currentHeader);                } else if (matchRegex != null) {                    final Matcher matcher = matchRegex.matcher(currentHeader);                    if (matcher.matches()) {                        headerIterator.remove();                        removedHeaders.add(currentHeader);                    }                }            }            if (!removedHeaders.isEmpty() && LogPrivacyUtil.allowLogRawData()) {                LOG.trace("Removed headers \"{}\" for event: {}", removedHeaders, event);            }        }    } catch (final Exception e) {            }    return event;}
1
public Interceptor build()
{    if (LOG.isDebugEnabled()) {            }    return new RemoveHeaderInterceptor(withName, fromList, listSeparator, matchRegex);}
1
public void configure(final Context context)
{    withName = context.getString(WITH_NAME);    fromList = context.getString(FROM_LIST);    listSeparator = context.getString(LIST_SEPARATOR, LIST_SEPARATOR_DEFAULT);    final String matchRegexStr = context.getString(MATCH_REGEX);    if (matchRegexStr != null) {        matchRegex = Pattern.compile(matchRegexStr);    }}
0
public void initialize()
{}
0
public void close()
{}
0
public Event intercept(Event event)
{    String origBody = new String(event.getBody(), charset);    Matcher matcher = searchPattern.matcher(origBody);    String newBody = matcher.replaceAll(replaceString);    event.setBody(newBody.getBytes(charset));    return event;}
0
public List<Event> intercept(List<Event> events)
{    for (Event event : events) {        intercept(event);    }    return events;}
0
public void configure(Context context)
{    String searchPattern = context.getString(SEARCH_PAT_KEY);    Preconditions.checkArgument(!StringUtils.isEmpty(searchPattern), "Must supply a valid search pattern " + SEARCH_PAT_KEY + " (may not be empty)");    replaceString = context.getString(REPLACE_STRING_KEY);        if (replaceString == null) {        replaceString = "";    }    searchRegex = Pattern.compile(searchPattern);    if (context.containsKey(CHARSET_KEY)) {                charset = Charset.forName(context.getString(CHARSET_KEY));    }}
0
public Interceptor build()
{    Preconditions.checkNotNull(searchRegex, "Regular expression search pattern required");    Preconditions.checkNotNull(replaceString, "Replacement string required");    return new SearchAndReplaceInterceptor(searchRegex, replaceString, charset);}
0
public void initialize()
{}
0
public Event intercept(Event event)
{    Map<String, String> headers = event.getHeaders();    if (preserveExisting && headers.containsKey(key)) {        return event;    }    headers.put(key, value);    return event;}
0
public List<Event> intercept(List<Event> events)
{    for (Event event : events) {        intercept(event);    }    return events;}
0
public void close()
{}
0
public void configure(Context context)
{    preserveExisting = context.getBoolean(Constants.PRESERVE, Constants.PRESERVE_DEFAULT);    key = context.getString(Constants.KEY, Constants.KEY_DEFAULT);    value = context.getString(Constants.VALUE, Constants.VALUE_DEFAULT);}
0
public Interceptor build()
{        return new StaticInterceptor(preserveExisting, key, value);}
1
public void initialize()
{}
0
public Event intercept(Event event)
{    Map<String, String> headers = event.getHeaders();    if (preserveExisting && headers.containsKey(header)) {        } else {        long now = System.currentTimeMillis();        headers.put(header, Long.toString(now));    }    return event;}
0
public List<Event> intercept(List<Event> events)
{    for (Event event : events) {        intercept(event);    }    return events;}
0
public void close()
{}
0
public Interceptor build()
{    return new TimestampInterceptor(preserveExisting, header);}
0
public void configure(Context context)
{    preserveExisting = context.getBoolean(CONFIG_PRESERVE, DEFAULT_PRESERVE);    header = context.getString(CONFIG_HEADER_NAME, DEFAULT_HEADER_NAME);}
0
public static boolean waitForState(LifecycleAware delegate, LifecycleState state) throws InterruptedException
{    return waitForState(delegate, state, 0);}
0
public static boolean waitForState(LifecycleAware delegate, LifecycleState state, long timeout) throws InterruptedException
{    return waitForOneOf(delegate, new LifecycleState[] { state }, timeout);}
0
public static boolean waitForOneOf(LifecycleAware delegate, LifecycleState[] states) throws InterruptedException
{    return waitForOneOf(delegate, states, 0);}
0
public static boolean waitForOneOf(LifecycleAware delegate, LifecycleState[] states, long timeout) throws InterruptedException
{    if (logger.isDebugEnabled()) {            }    long sleepInterval = Math.max(shortestSleepDuration, timeout / maxNumberOfChecks);    long deadLine = System.currentTimeMillis() + timeout;    do {        for (LifecycleState state : states) {            if (delegate.getLifecycleState().equals(state)) {                return true;            }        }        Thread.sleep(sleepInterval);    } while (timeout == 0 || System.currentTimeMillis() < deadLine);        return false;}
1
public static void stopAll(List<LifecycleAware> services) throws InterruptedException
{    for (LifecycleAware service : services) {        waitForOneOf(service, LifecycleState.STOP_OR_ERROR);    }}
0
public synchronized void start()
{        monitorService.scheduleWithFixedDelay(purger, 2, 2, TimeUnit.HOURS);    lifecycleState = LifecycleState.START;    }
1
public synchronized void stop()
{        if (monitorService != null) {        monitorService.shutdown();        try {            monitorService.awaitTermination(10, TimeUnit.SECONDS);        } catch (InterruptedException e) {                    }        if (!monitorService.isTerminated()) {            monitorService.shutdownNow();            try {                while (!monitorService.isTerminated()) {                    monitorService.awaitTermination(10, TimeUnit.SECONDS);                }            } catch (InterruptedException e) {                            }        }    }    for (final Entry<LifecycleAware, Supervisoree> entry : supervisedProcesses.entrySet()) {        if (entry.getKey().getLifecycleState().equals(LifecycleState.START)) {            entry.getValue().status.desiredState = LifecycleState.STOP;            entry.getKey().stop();        }    }    /* If we've failed, preserve the error state. */    if (lifecycleState.equals(LifecycleState.START)) {        lifecycleState = LifecycleState.STOP;    }    supervisedProcesses.clear();    monitorFutures.clear();    }
1
public synchronized void fail()
{    lifecycleState = LifecycleState.ERROR;}
0
public synchronized void supervise(LifecycleAware lifecycleAware, SupervisorPolicy policy, LifecycleState desiredState)
{    if (this.monitorService.isShutdown() || this.monitorService.isTerminated() || this.monitorService.isTerminating()) {        throw new FlumeException("Supervise called on " + lifecycleAware + " " + "after shutdown has been initiated. " + lifecycleAware + " will not" + " be started");    }    Preconditions.checkState(!supervisedProcesses.containsKey(lifecycleAware), "Refusing to supervise " + lifecycleAware + " more than once");    if (logger.isDebugEnabled()) {            }    Supervisoree process = new Supervisoree();    process.status = new Status();    process.policy = policy;    process.status.desiredState = desiredState;    process.status.error = false;    MonitorRunnable monitorRunnable = new MonitorRunnable();    monitorRunnable.lifecycleAware = lifecycleAware;    monitorRunnable.supervisoree = process;    monitorRunnable.monitorService = monitorService;    supervisedProcesses.put(lifecycleAware, process);    ScheduledFuture<?> future = monitorService.scheduleWithFixedDelay(monitorRunnable, 0, 3, TimeUnit.SECONDS);    monitorFutures.put(lifecycleAware, future);}
1
public synchronized void unsupervise(LifecycleAware lifecycleAware)
{    Preconditions.checkState(supervisedProcesses.containsKey(lifecycleAware), "Unaware of " + lifecycleAware + " - can not unsupervise");        synchronized (lifecycleAware) {        Supervisoree supervisoree = supervisedProcesses.get(lifecycleAware);        supervisoree.status.discard = true;        this.setDesiredState(lifecycleAware, LifecycleState.STOP);                lifecycleAware.stop();    }    supervisedProcesses.remove(lifecycleAware);            monitorFutures.get(lifecycleAware).cancel(false);        needToPurge = true;    monitorFutures.remove(lifecycleAware);}
1
public synchronized void setDesiredState(LifecycleAware lifecycleAware, LifecycleState desiredState)
{    Preconditions.checkState(supervisedProcesses.containsKey(lifecycleAware), "Unaware of " + lifecycleAware + " - can not set desired state to " + desiredState);        Supervisoree supervisoree = supervisedProcesses.get(lifecycleAware);    supervisoree.status.desiredState = desiredState;}
1
public synchronized LifecycleState getLifecycleState()
{    return lifecycleState;}
0
public synchronized boolean isComponentInErrorState(LifecycleAware component)
{    return supervisedProcesses.get(component).status.error;}
0
public void run()
{        long now = System.currentTimeMillis();    try {        if (supervisoree.status.firstSeen == null) {                        supervisoree.status.firstSeen = now;        }        supervisoree.status.lastSeen = now;        synchronized (lifecycleAware) {            if (supervisoree.status.discard) {                                                return;            } else if (supervisoree.status.error) {                                return;            }            supervisoree.status.lastSeenState = lifecycleAware.getLifecycleState();            if (!lifecycleAware.getLifecycleState().equals(supervisoree.status.desiredState)) {                                switch(supervisoree.status.desiredState) {                    case START:                        try {                            lifecycleAware.start();                        } catch (Throwable e) {                                                        if (e instanceof Error) {                                                                supervisoree.status.desiredState = LifecycleState.STOP;                                try {                                    lifecycleAware.stop();                                                                    } catch (Throwable e1) {                                                                        supervisoree.status.error = true;                                    if (e1 instanceof Error) {                                        throw (Error) e1;                                    }                                                                                                }                            }                            supervisoree.status.failures++;                        }                        break;                    case STOP:                        try {                            lifecycleAware.stop();                        } catch (Throwable e) {                                                        if (e instanceof Error) {                                throw (Error) e;                            }                            supervisoree.status.failures++;                        }                        break;                    default:                                        }                if (!supervisoree.policy.isValid(lifecycleAware, supervisoree.status)) {                                    }            }        }    } catch (Throwable t) {            }    }
1
public void run()
{    if (needToPurge) {        monitorService.purge();        needToPurge = false;    }}
0
public String toString()
{    return "{ lastSeen:" + lastSeen + " lastSeenState:" + lastSeenState + " desiredState:" + desiredState + " firstSeen:" + firstSeen + " failures:" + failures + " discard:" + discard + " error:" + error + " }";}
0
 boolean isValid(LifecycleAware object, Status status)
{    return true;}
0
 boolean isValid(LifecycleAware object, Status status)
{    return status.failures == 0;}
0
public String toString()
{    return "{ status:" + status + " policy:" + policy + " }";}
0
public void configure(Context context)
{    int syncIntervalBytes = context.getInteger(SYNC_INTERVAL_BYTES, DEFAULT_SYNC_INTERVAL_BYTES);    String compressionCodec = context.getString(COMPRESSION_CODEC, DEFAULT_COMPRESSION_CODEC);    writer = new ReflectDatumWriter<T>(getSchema());    dataFileWriter = new DataFileWriter<T>(writer);    dataFileWriter.setSyncInterval(syncIntervalBytes);    try {        CodecFactory codecFactory = CodecFactory.fromString(compressionCodec);        dataFileWriter.setCodec(codecFactory);    } catch (AvroRuntimeException e) {            }}
1
public void afterCreate() throws IOException
{        dataFileWriter.create(getSchema(), getOutputStream());}
0
public void afterReopen() throws IOException
{        throw new UnsupportedOperationException("Avro API doesn't support append");}
0
public void write(Event event) throws IOException
{    T destType = convert(event);    dataFileWriter.append(destType);}
0
public void flush() throws IOException
{    dataFileWriter.flush();}
0
public void beforeClose() throws IOException
{}
0
public boolean supportsReopen()
{    return false;}
0
private void initialize() throws IOException, NoSuchAlgorithmException
{    SeekableResettableInputBridge in = new SeekableResettableInputBridge(ris);    long pos = in.tell();    in.seek(0L);    fileReader = new DataFileReader<GenericRecord>(in, new GenericDatumReader<GenericRecord>());    fileReader.sync(pos);    schema = fileReader.getSchema();    datumWriter = new GenericDatumWriter(schema);    out = new ByteArrayOutputStream();    encoder = EncoderFactory.get().binaryEncoder(out, encoder);    schemaHash = SchemaNormalization.parsingFingerprint("CRC-64-AVRO", schema);    schemaHashString = Hex.encodeHexString(schemaHash);}
0
public Event readEvent() throws IOException
{    if (fileReader.hasNext()) {        record = fileReader.next(record);        out.reset();        datumWriter.write(record, encoder);        encoder.flush();                Event event = EventBuilder.withBody(out.toByteArray());        if (schemaType == AvroSchemaType.HASH) {            event.getHeaders().put(AVRO_SCHEMA_HEADER_HASH, schemaHashString);        } else {            event.getHeaders().put(AVRO_SCHEMA_HEADER_LITERAL, schema.toString());        }        return event;    }    return null;}
0
public List<Event> readEvents(int numEvents) throws IOException
{    List<Event> events = Lists.newArrayList();    for (int i = 0; i < numEvents && fileReader.hasNext(); i++) {        Event event = readEvent();        if (event != null) {            events.add(event);        }    }    return events;}
0
public void mark() throws IOException
{    long pos = fileReader.previousSync() - DataFileConstants.SYNC_SIZE;    if (pos < 0)        pos = 0;    ((RemoteMarkable) ris).markPosition(pos);}
0
public void reset() throws IOException
{    long pos = ((RemoteMarkable) ris).getMarkPosition();    fileReader.sync(pos);}
0
public void close() throws IOException
{    ris.close();}
0
public EventDeserializer build(Context context, ResettableInputStream in)
{    if (!(in instanceof RemoteMarkable)) {        throw new IllegalArgumentException("Cannot use this deserializer " + "without a RemoteMarkable input stream");    }    AvroEventDeserializer deserializer = new AvroEventDeserializer(context, in);    try {        deserializer.initialize();    } catch (Exception e) {        throw new FlumeException("Cannot instantiate deserializer", e);    }    return deserializer;}
0
public void seek(long p) throws IOException
{    ris.seek(p);}
0
public long tell() throws IOException
{    return ris.tell();}
0
public long length() throws IOException
{    if (ris instanceof LengthMeasurable) {        return ((LengthMeasurable) ris).length();    } else {                return Long.MAX_VALUE;    }}
0
public int read(byte[] b, int off, int len) throws IOException
{    return ris.read(b, off, len);}
0
public void close() throws IOException
{    ris.close();}
0
public boolean supportsReopen()
{    return true;}
0
public void afterCreate()
{}
0
public void afterReopen()
{}
0
public void beforeClose()
{}
0
public void write(Event e) throws IOException
{    out.write(e.getBody());    if (appendNewline) {        out.write('\n');    }}
0
public void flush() throws IOException
{}
0
public EventSerializer build(Context context, OutputStream out)
{    BodyTextEventSerializer s = new BodyTextEventSerializer(out, context);    return s;}
0
public static DurablePositionTracker getInstance(File trackerFile, String target) throws IOException
{    if (!trackerFile.exists()) {        return new DurablePositionTracker(trackerFile, target);    }        DurablePositionTracker oldTracker = new DurablePositionTracker(trackerFile, target);    String existingTarget = oldTracker.getTarget();    long targetPosition = oldTracker.getPosition();    oldTracker.close();    File tmpMeta = File.createTempFile(trackerFile.getName(), ".tmp", trackerFile.getParentFile());    tmpMeta.delete();    DurablePositionTracker tmpTracker = new DurablePositionTracker(tmpMeta, existingTarget);    tmpTracker.storePosition(targetPosition);    tmpTracker.close();        if (PlatformDetect.isWindows()) {        if (!trackerFile.delete()) {            throw new IOException("Unable to delete existing meta file " + trackerFile);        }    }        if (!tmpMeta.renameTo(trackerFile)) {        throw new IOException("Unable to rename " + tmpMeta + " to " + trackerFile);    }        DurablePositionTracker newTracker = new DurablePositionTracker(trackerFile, existingTarget);    return newTracker;}
0
private void initReader() throws IOException
{    long syncPos = trackerFile.length() - 256L;    if (syncPos < 0)        syncPos = 0L;    reader.sync(syncPos);    while (reader.hasNext()) {        reader.next(metaCache);    }}
0
public synchronized void storePosition(long position) throws IOException
{    metaCache.setOffset(position);    writer.append(metaCache);    writer.sync();    writer.flush();}
0
public synchronized long getPosition()
{    return metaCache.getOffset();}
0
public String getTarget()
{    return target;}
0
public void close() throws IOException
{    if (isOpen) {        writer.close();        reader.close();        isOpen = false;    }}
0
public static EventDeserializer getInstance(String deserializerType, Context context, ResettableInputStream in)
{    Preconditions.checkNotNull(deserializerType, "serializer type must not be null");        EventDeserializerType type;    try {        type = EventDeserializerType.valueOf(deserializerType.toUpperCase(Locale.ENGLISH));    } catch (IllegalArgumentException e) {                type = EventDeserializerType.OTHER;    }    Class<? extends EventDeserializer.Builder> builderClass = type.getBuilderClass();        if (builderClass == null) {        try {            Class c = Class.forName(deserializerType);            if (c != null && EventDeserializer.Builder.class.isAssignableFrom(c)) {                builderClass = (Class<? extends EventDeserializer.Builder>) c;            } else {                String errMessage = "Unable to instantiate Builder from " + deserializerType + ": does not appear to implement " + EventDeserializer.Builder.class.getName();                throw new FlumeException(errMessage);            }        } catch (ClassNotFoundException ex) {                        throw new FlumeException(ex);        }    }        EventDeserializer.Builder builder;    try {        builder = builderClass.newInstance();    } catch (InstantiationException ex) {        String errMessage = "Cannot instantiate builder: " + deserializerType;                throw new FlumeException(errMessage, ex);    } catch (IllegalAccessException ex) {        String errMessage = "Cannot instantiate builder: " + deserializerType;                throw new FlumeException(errMessage, ex);    }    return builder.build(context, in);}
1
public Class<? extends EventDeserializer.Builder> getBuilderClass()
{    return builderClass;}
0
public static EventSerializer getInstance(String serializerType, Context context, OutputStream out)
{    Preconditions.checkNotNull(serializerType, "serializer type must not be null");        EventSerializerType type;    try {        type = EventSerializerType.valueOf(serializerType.toUpperCase(Locale.ENGLISH));    } catch (IllegalArgumentException e) {                type = EventSerializerType.OTHER;    }    Class<? extends EventSerializer.Builder> builderClass = type.getBuilderClass();        if (builderClass == null) {        try {            Class c = Class.forName(serializerType);            if (c != null && EventSerializer.Builder.class.isAssignableFrom(c)) {                builderClass = (Class<? extends EventSerializer.Builder>) c;            } else {                String errMessage = "Unable to instantiate Builder from " + serializerType + ": does not appear to implement " + EventSerializer.Builder.class.getName();                throw new FlumeException(errMessage);            }        } catch (ClassNotFoundException ex) {                        throw new FlumeException(ex);        }    }        EventSerializer.Builder builder;    try {        builder = builderClass.newInstance();    } catch (InstantiationException ex) {        String errMessage = "Cannot instantiate builder: " + serializerType;                throw new FlumeException(errMessage, ex);    } catch (IllegalAccessException ex) {        String errMessage = "Cannot instantiate builder: " + serializerType;                throw new FlumeException(errMessage, ex);    }    return builder.build(context, out);}
1
public Class<? extends EventSerializer.Builder> getBuilderClass()
{    return builderClass;}
0
protected Schema getSchema()
{    return SCHEMA;}
0
protected OutputStream getOutputStream()
{    return out;}
0
protected Event convert(Event event)
{    return event;}
0
public EventSerializer build(Context context, OutputStream out)
{    FlumeEventAvroEventSerializer writer = new FlumeEventAvroEventSerializer(out);    writer.configure(context);    return writer;}
0
public boolean supportsReopen()
{    return true;}
0
public void afterCreate()
{}
0
public void afterReopen()
{}
0
public void beforeClose()
{}
0
public void write(Event e) throws IOException
{    out.write((e.getHeaders() + " ").getBytes());    out.write(e.getBody());    if (appendNewline) {        out.write('\n');    }}
0
public void flush() throws IOException
{}
0
public EventSerializer build(Context context, OutputStream out)
{    HeaderAndBodyTextEventSerializer s = new HeaderAndBodyTextEventSerializer(out, context);    return s;}
0
public Event readEvent() throws IOException
{    ensureOpen();    String line = readLine();    if (line == null) {        return null;    } else {        return EventBuilder.withBody(line, outputCharset);    }}
0
public List<Event> readEvents(int numEvents) throws IOException
{    ensureOpen();    List<Event> events = Lists.newLinkedList();    for (int i = 0; i < numEvents; i++) {        Event event = readEvent();        if (event != null) {            events.add(event);        } else {            break;        }    }    return events;}
0
public void mark() throws IOException
{    ensureOpen();    in.mark();}
0
public void reset() throws IOException
{    ensureOpen();    in.reset();}
0
public void close() throws IOException
{    if (isOpen) {        reset();        in.close();        isOpen = false;    }}
0
private void ensureOpen()
{    if (!isOpen) {        throw new IllegalStateException("Serializer has been closed");    }}
0
private String readLine() throws IOException
{    StringBuilder sb = new StringBuilder();    int c;    int readChars = 0;    while ((c = in.readChar()) != -1) {        readChars++;                if (c == '\n') {            break;        }        sb.append((char) c);        if (readChars >= maxLineLength) {                        break;        }    }    if (readChars > 0) {        return sb.toString();    } else {        return null;    }}
1
public EventDeserializer build(Context context, ResettableInputStream in)
{    return new LineDeserializer(context, in);}
0
public synchronized int read() throws IOException
{    int len = read(byteBuf, 0, 1);    if (len == -1) {        return -1;        } else if (len == 0) {        return -1;    } else {        return byteBuf[0] & 0xFF;    }}
0
public synchronized int read(byte[] b, int off, int len) throws IOException
{    logger.trace("read(buf, {}, {})", off, len);    if (position >= fileSize) {        return -1;    }    if (!buf.hasRemaining()) {        refillBuf();    }    int rem = buf.remaining();    if (len > rem) {        len = rem;    }    buf.get(b, off, len);    incrPosition(len, true);    return len;}
0
public synchronized int readChar() throws IOException
{        if (hasLowSurrogate) {        hasLowSurrogate = false;        return lowSurrogate;    }        if (buf.remaining() < maxCharWidth) {        buf.clear();        buf.flip();        refillBuf();    }    int start = buf.position();    charBuf.clear();    charBuf.limit(1);    boolean isEndOfInput = false;    if (position >= fileSize) {        isEndOfInput = true;    }    CoderResult res = decoder.decode(buf, charBuf, isEndOfInput);    if (res.isMalformed() || res.isUnmappable()) {        res.throwException();    }    int delta = buf.position() - start;    charBuf.flip();        if (charBuf.hasRemaining()) {        char c = charBuf.get();        incrPosition(delta, true);        return c;    }        if (buf.hasRemaining()) {        charBuf.clear();                charBuf.limit(2);                res = decoder.decode(buf, charBuf, isEndOfInput);        if (res.isMalformed() || res.isUnmappable()) {            res.throwException();        }        charBuf.flip();                if (charBuf.remaining() == 2) {            char highSurrogate = charBuf.get();                        lowSurrogate = charBuf.get();                        if (!Character.isHighSurrogate(highSurrogate) || !Character.isLowSurrogate(lowSurrogate)) {                                            }            hasLowSurrogate = true;                        delta = buf.position() - start;            incrPosition(delta, true);                        return highSurrogate;        }    }        incrPosition(delta, false);    return -1;}
1
private void refillBuf() throws IOException
{    buf.compact();        chan.position(position);    chan.read(buf);    buf.flip();}
0
public void mark() throws IOException
{    tracker.storePosition(tell());}
0
public void markPosition(long position) throws IOException
{    tracker.storePosition(position);}
0
public long getMarkPosition() throws IOException
{    return tracker.getPosition();}
0
public void reset() throws IOException
{    seek(tracker.getPosition());}
0
public long length() throws IOException
{    return file.length();}
0
public long tell() throws IOException
{    logger.trace("Tell position: {}", syncPosition);    return syncPosition;}
0
public synchronized void seek(long newPos) throws IOException
{    logger.trace("Seek to position: {}", newPos);        long relativeChange = newPos - position;        if (relativeChange == 0)        return;    long newBufPos = buf.position() + relativeChange;    if (newBufPos >= 0 && newBufPos < buf.limit()) {                buf.position((int) newBufPos);    } else {                buf.clear();        buf.flip();    }        decoder.reset();        chan.position(newPos);        position = syncPosition = newPos;}
0
private void incrPosition(int incr, boolean updateSyncPosition)
{    position += incr;    if (updateSyncPosition) {        syncPosition = position;    }}
0
public void close() throws IOException
{    tracker.close();    in.close();}
0
public void configure(Context context)
{    clientProps = new Properties();    hostname = context.getString("hostname");    port = context.getInteger("port");    Preconditions.checkState(hostname != null, "No hostname specified");    Preconditions.checkState(port != null, "No port specified");    clientProps.setProperty(RpcClientConfigurationConstants.CONFIG_HOSTS, "h1");    clientProps.setProperty(RpcClientConfigurationConstants.CONFIG_HOSTS_PREFIX + "h1", hostname + ":" + port);    for (Entry<String, String> entry : context.getParameters().entrySet()) {        clientProps.setProperty(entry.getKey(), entry.getValue());    }    batchSize = AbstractRpcClient.parseBatchSize(clientProps);    if (sinkCounter == null) {        sinkCounter = new SinkCounter(getName());    }    cxnResetInterval = context.getInteger("reset-connection-interval", DEFAULT_CXN_RESET_INTERVAL);    if (cxnResetInterval == DEFAULT_CXN_RESET_INTERVAL) {            }}
1
private void createConnection() throws FlumeException
{    if (client == null) {                try {            resetConnectionFlag = new AtomicBoolean(false);            client = initializeRpcClient(clientProps);            Preconditions.checkNotNull(client, "Rpc Client could not be " + "initialized. " + getName() + " could not be started");            sinkCounter.incrementConnectionCreatedCount();            if (cxnResetInterval > 0) {                cxnResetExecutor.schedule(new Runnable() {                    @Override                    public void run() {                        resetConnectionFlag.set(true);                    }                }, cxnResetInterval, TimeUnit.SECONDS);            }        } catch (Exception ex) {            sinkCounter.incrementConnectionFailedCount();            if (ex instanceof FlumeException) {                throw (FlumeException) ex;            } else {                throw new FlumeException(ex);            }        }            }}
1
public void run()
{    resetConnectionFlag.set(true);}
0
private void resetConnection()
{    try {        destroyConnection();        createConnection();    } catch (Throwable throwable) {                    }}
1
private void destroyConnection()
{    if (client != null) {                try {            client.close();            sinkCounter.incrementConnectionClosedCount();        } catch (FlumeException e) {            sinkCounter.incrementConnectionFailedCount();                    }    }    client = null;}
1
private void verifyConnection() throws FlumeException
{    if (client == null) {        createConnection();    } else if (!client.isActive()) {        destroyConnection();        createConnection();    }}
0
public void start()
{        sinkCounter.start();    try {        createConnection();    } catch (FlumeException e) {                /* Try to prevent leaking resources. */        destroyConnection();    }    super.start();    }
1
public void stop()
{        destroyConnection();    cxnResetExecutor.shutdown();    try {        if (cxnResetExecutor.awaitTermination(5, TimeUnit.SECONDS)) {            cxnResetExecutor.shutdownNow();        }    } catch (Exception ex) {            }    sinkCounter.stop();    super.stop();    }
1
public String toString()
{    return "RpcSink " + getName() + " { host: " + hostname + ", port: " + port + " }";}
0
public Status process() throws EventDeliveryException
{    Status status = Status.READY;    Channel channel = getChannel();    Transaction transaction = channel.getTransaction();    if (resetConnectionFlag.get()) {        resetConnection();                                resetConnectionFlag.set(false);    }    try {        transaction.begin();        verifyConnection();        List<Event> batch = Lists.newLinkedList();        for (int i = 0; i < client.getBatchSize(); i++) {            Event event = channel.take();            if (event == null) {                break;            }            batch.add(event);        }        int size = batch.size();        int batchSize = client.getBatchSize();        if (size == 0) {            sinkCounter.incrementBatchEmptyCount();            status = Status.BACKOFF;        } else {            if (size < batchSize) {                sinkCounter.incrementBatchUnderflowCount();            } else {                sinkCounter.incrementBatchCompleteCount();            }            sinkCounter.addToEventDrainAttemptCount(size);            client.appendBatch(batch);        }        transaction.commit();        sinkCounter.addToEventDrainSuccessCount(size);    } catch (Throwable t) {        transaction.rollback();        if (t instanceof Error) {            throw (Error) t;        } else if (t instanceof ChannelException) {                        sinkCounter.incrementChannelReadFail();            status = Status.BACKOFF;        } else {            sinkCounter.incrementEventWriteFail();            destroyConnection();            throw new EventDeliveryException("Failed to send events", t);        }    } finally {        transaction.close();    }    return status;}
1
 RpcClient getUnderlyingClient()
{    return client;}
0
public long getBatchSize()
{    return batchSize;}
0
public synchronized void start()
{    Preconditions.checkState(channel != null, "No channel configured");    lifecycleState = LifecycleState.START;}
0
public synchronized void stop()
{    lifecycleState = LifecycleState.STOP;}
0
public synchronized Channel getChannel()
{    return channel;}
0
public synchronized void setChannel(Channel channel)
{    this.channel = channel;}
0
public synchronized LifecycleState getLifecycleState()
{    return lifecycleState;}
0
public synchronized void setName(String name)
{    this.name = name;}
0
public synchronized String getName()
{    return name;}
0
public String toString()
{    return this.getClass().getName() + "{name:" + name + ", channel:" + channel.getName() + "}";}
0
public void start()
{    for (Sink s : sinkList) {        s.start();    }    state = LifecycleState.START;}
0
public void stop()
{    for (Sink s : sinkList) {        s.stop();    }    state = LifecycleState.STOP;}
0
public LifecycleState getLifecycleState()
{    return state;}
0
public void setSinks(List<Sink> sinks)
{    List<Sink> list = new ArrayList<Sink>();    list.addAll(sinks);    sinkList = Collections.unmodifiableList(list);}
0
protected List<Sink> getSinks()
{    return sinkList;}
0
public void configure(Context context)
{    Long timeOut = context.getLong("maxTimeOut");    if (timeOut != null) {        maxTimeOut = timeOut;    }}
0
public void start()
{    state = LifecycleState.START;}
0
public void stop()
{    state = LifecycleState.STOP;}
0
public LifecycleState getLifecycleState()
{    return state;}
0
public void setSinks(List<Sink> sinks)
{    sinkList = new ArrayList<Sink>();    sinkList.addAll(sinks);}
0
protected List<Sink> getSinks()
{    return sinkList;}
0
public void informSinkFailed(Sink failedSink)
{}
0
protected RpcClient initializeRpcClient(Properties props)
{        return RpcClientFactory.getInstance(props);}
1
public Sink create(String name, String type) throws FlumeException
{    Preconditions.checkNotNull(name, "name");    Preconditions.checkNotNull(type, "type");        Class<? extends Sink> sinkClass = getClass(type);    try {        Sink sink = sinkClass.newInstance();        sink.setName(name);        return sink;    } catch (Exception ex) {        throw new FlumeException("Unable to create sink: " + name + ", type: " + type + ", class: " + sinkClass.getName(), ex);    }}
1
public Class<? extends Sink> getClass(String type) throws FlumeException
{    String sinkClassName = type;    SinkType sinkType = SinkType.OTHER;    try {        sinkType = SinkType.valueOf(type.toUpperCase(Locale.ENGLISH));    } catch (IllegalArgumentException ex) {            }    if (!sinkType.equals(SinkType.OTHER)) {        sinkClassName = sinkType.getSinkClassName();    }    try {        return (Class<? extends Sink>) Class.forName(sinkClassName);    } catch (Exception ex) {        throw new FlumeException("Unable to load sink type: " + type + ", class: " + sinkClassName, ex);    }}
1
public void start()
{    Preconditions.checkNotNull(sink, "DefaultSinkProcessor sink not set");    sink.start();    lifecycleState = LifecycleState.START;}
0
public void stop()
{    Preconditions.checkNotNull(sink, "DefaultSinkProcessor sink not set");    sink.stop();    lifecycleState = LifecycleState.STOP;}
0
public LifecycleState getLifecycleState()
{    return lifecycleState;}
0
public void configure(Context context)
{}
0
public Status process() throws EventDeliveryException
{    return sink.process();}
0
public void setSinks(List<Sink> sinks)
{    Preconditions.checkNotNull(sinks);    Preconditions.checkArgument(sinks.size() == 1, "DefaultSinkPolicy can " + "only handle one sink, " + "try using a policy that supports multiple sinks");    sink = sinks.get(0);}
0
public void configure(ComponentConfiguration conf)
{}
0
public int compareTo(FailedSink arg0)
{    return refresh.compareTo(arg0.refresh);}
0
public Long getRefresh()
{    return refresh;}
0
public Sink getSink()
{    return sink;}
0
public Integer getPriority()
{    return priority;}
0
public void incFails()
{    sequentialFailures++;    adjustRefresh();    }
1
private void adjustRefresh()
{    refresh = System.currentTimeMillis() + Math.min(maxPenalty, (1 << sequentialFailures) * FAILURE_PENALTY);}
0
public void configure(Context context)
{    liveSinks = new TreeMap<Integer, Sink>();    failedSinks = new PriorityQueue<FailedSink>();    Integer nextPrio = 0;    String maxPenaltyStr = context.getString(MAX_PENALTY_PREFIX);    if (maxPenaltyStr == null) {        maxPenalty = DEFAULT_MAX_PENALTY;    } else {        try {            maxPenalty = Integer.parseInt(maxPenaltyStr);        } catch (NumberFormatException e) {                        maxPenalty = DEFAULT_MAX_PENALTY;        }    }    for (Entry<String, Sink> entry : sinks.entrySet()) {        String priStr = PRIORITY_PREFIX + entry.getKey();        Integer priority;        try {            priority = Integer.parseInt(context.getString(priStr));        } catch (Exception e) {            priority = --nextPrio;        }        if (!liveSinks.containsKey(priority)) {            liveSinks.put(priority, sinks.get(entry.getKey()));        } else {                    }    }    activeSink = liveSinks.get(liveSinks.lastKey());}
1
public Status process() throws EventDeliveryException
{        Long now = System.currentTimeMillis();    while (!failedSinks.isEmpty() && failedSinks.peek().getRefresh() < now) {        FailedSink cur = failedSinks.poll();        Status s;        try {            s = cur.getSink().process();            if (s == Status.READY) {                liveSinks.put(cur.getPriority(), cur.getSink());                activeSink = liveSinks.get(liveSinks.lastKey());                            } else {                                failedSinks.add(cur);            }            return s;        } catch (Exception e) {            cur.incFails();            failedSinks.add(cur);        }    }    Status ret = null;    while (activeSink != null) {        try {            ret = activeSink.process();            return ret;        } catch (Exception e) {                        activeSink = moveActiveToDeadAndGetNext();        }    }    throw new EventDeliveryException("All sinks failed to process, " + "nothing left to failover to");}
1
private Sink moveActiveToDeadAndGetNext()
{    Integer key = liveSinks.lastKey();    failedSinks.add(new FailedSink(key, activeSink, 1));    liveSinks.remove(key);    if (liveSinks.isEmpty())        return null;    if (liveSinks.lastKey() != null) {        return liveSinks.get(liveSinks.lastKey());    } else {        return null;    }}
0
public void setSinks(List<Sink> sinks)
{        super.setSinks(sinks);    this.sinks = new HashMap<String, Sink>();    for (Sink sink : sinks) {        this.sinks.put(sink.getName(), sink);    }}
0
public void configure(Context context)
{    Preconditions.checkState(getSinks().size() > 1, "The LoadBalancingSinkProcessor cannot be used for a single sink. " + "Please configure more than one sinks and try again.");    String selectorTypeName = context.getString(CONFIG_SELECTOR, SELECTOR_NAME_ROUND_ROBIN);    Boolean shouldBackOff = context.getBoolean(CONFIG_BACKOFF, false);    selector = null;    if (selectorTypeName.equalsIgnoreCase(SELECTOR_NAME_ROUND_ROBIN)) {        selector = new RoundRobinSinkSelector(shouldBackOff);    } else if (selectorTypeName.equalsIgnoreCase(SELECTOR_NAME_RANDOM)) {        selector = new RandomOrderSinkSelector(shouldBackOff);    } else {        try {            @SuppressWarnings("unchecked")            Class<? extends SinkSelector> klass = (Class<? extends SinkSelector>) Class.forName(selectorTypeName);            selector = klass.newInstance();        } catch (Exception ex) {            throw new FlumeException("Unable to instantiate sink selector: " + selectorTypeName, ex);        }    }    selector.setSinks(getSinks());    selector.configure(new Context(context.getSubProperties(CONFIG_SELECTOR_PREFIX)));    }
1
public void start()
{    super.start();    selector.start();}
0
public void stop()
{    super.stop();    selector.stop();}
0
public Status process() throws EventDeliveryException
{    Status status = null;    Iterator<Sink> sinkIterator = selector.createSinkIterator();    while (sinkIterator.hasNext()) {        Sink sink = sinkIterator.next();        try {            status = sink.process();            break;        } catch (Exception ex) {            selector.informSinkFailed(sink);                    }    }    if (status == null) {        throw new EventDeliveryException("All configured sinks have failed");    }    return status;}
1
public void configure(Context context)
{    super.configure(context);    if (maxTimeOut != 0) {        selector.setMaxTimeOut(maxTimeOut);    }}
0
public Iterator<Sink> createSinkIterator()
{    return selector.createIterator();}
0
public void setSinks(List<Sink> sinks)
{    selector.setObjects(sinks);}
0
public void informSinkFailed(Sink failedSink)
{    selector.informFailure(failedSink);}
0
public void configure(Context context)
{    super.configure(context);    if (maxTimeOut != 0) {        selector.setMaxTimeOut(maxTimeOut);    }}
0
public void setSinks(List<Sink> sinks)
{    selector.setObjects(sinks);}
0
public Iterator<Sink> createSinkIterator()
{    return selector.createIterator();}
0
public void informSinkFailed(Sink failedSink)
{    selector.informFailure(failedSink);}
0
public void configure(Context context)
{    String strMaxBytes = context.getString(MAX_BYTES_DUMP_KEY);    if (!Strings.isNullOrEmpty(strMaxBytes)) {        try {            maxBytesToLog = Integer.parseInt(strMaxBytes);        } catch (NumberFormatException e) {                        maxBytesToLog = DEFAULT_MAX_BYTE_DUMP;        }    }}
1
public Status process() throws EventDeliveryException
{    Status result = Status.READY;    Channel channel = getChannel();    Transaction transaction = channel.getTransaction();    Event event = null;    try {        transaction.begin();        event = channel.take();        if (event != null) {            if (logger.isInfoEnabled()) {                            }        } else {                        result = Status.BACKOFF;        }        transaction.commit();    } catch (Exception ex) {        transaction.rollback();        throw new EventDeliveryException("Failed to log event: " + event, ex);    } finally {        transaction.close();    }    return result;}
1
public void configure(Context context)
{    batchSize = context.getInteger("batchSize", DFLT_BATCH_SIZE);        Preconditions.checkArgument(batchSize > 0, "Batch size must be > 0");    logEveryNEvents = context.getInteger("logEveryNEvents", DFLT_LOG_EVERY_N_EVENTS);        Preconditions.checkArgument(logEveryNEvents > 0, "logEveryNEvents must be > 0");}
1
public Status process() throws EventDeliveryException
{    Status status = Status.READY;    Channel channel = getChannel();    Transaction transaction = channel.getTransaction();    Event event = null;    long eventCounter = counterGroup.get("events.success");    try {        transaction.begin();        int i = 0;        for (i = 0; i < batchSize; i++) {            event = channel.take();            if (++eventCounter % logEveryNEvents == 0) {                            }            if (event == null) {                status = Status.BACKOFF;                break;            }        }        transaction.commit();        counterGroup.addAndGet("events.success", (long) Math.min(batchSize, i));        counterGroup.incrementAndGet("transaction.success");    } catch (Exception ex) {        transaction.rollback();        counterGroup.incrementAndGet("transaction.failed");                throw new EventDeliveryException("Failed to deliver event: " + event, ex);    } finally {        transaction.close();    }    return status;}
1
public void start()
{        counterGroup.setName(this.getName());    super.start();    }
1
public void stop()
{        super.stop();    }
1
public String toString()
{    return "NullSink " + getName() + " { batchSize: " + batchSize + " }";}
0
public long getBatchSize()
{    return batchSize;}
0
public void configure(Context context)
{    String pathManagerType = context.getString("sink.pathManager", "DEFAULT");    String directory = context.getString("sink.directory");    String rollInterval = context.getString("sink.rollInterval");    serializerType = context.getString("sink.serializer", "TEXT");    serializerContext = new Context(context.getSubProperties("sink." + EventSerializer.CTX_PREFIX));    Context pathManagerContext = new Context(context.getSubProperties("sink." + PathManager.CTX_PREFIX));    pathController = PathManagerFactory.getInstance(pathManagerType, pathManagerContext);    Preconditions.checkArgument(directory != null, "Directory may not be null");    Preconditions.checkNotNull(serializerType, "Serializer type is undefined");    if (rollInterval == null) {        this.rollInterval = defaultRollInterval;    } else {        this.rollInterval = Long.parseLong(rollInterval);    }    batchSize = context.getInteger("sink.batchSize", defaultBatchSize);    this.directory = new File(directory);    if (sinkCounter == null) {        sinkCounter = new SinkCounter(getName());    }}
0
public void start()
{        sinkCounter.start();    super.start();    pathController.setBaseDirectory(directory);    if (rollInterval > 0) {        rollService = Executors.newScheduledThreadPool(1, new ThreadFactoryBuilder().setNameFormat("rollingFileSink-roller-" + Thread.currentThread().getId() + "-%d").build());        /*       * Every N seconds, mark that it's time to rotate. We purposefully do NOT       * touch anything other than the indicator flag to avoid error handling       * issues (e.g. IO exceptions occuring in two different threads.       * Resist the urge to actually perform rotation in a separate thread!       */        rollService.scheduleAtFixedRate(new Runnable() {            @Override            public void run() {                                shouldRotate = true;            }        }, rollInterval, rollInterval, TimeUnit.SECONDS);    } else {            }    }
1
public void run()
{        shouldRotate = true;}
1
public Status process() throws EventDeliveryException
{    if (shouldRotate) {                if (outputStream != null) {                        try {                serializer.flush();                serializer.beforeClose();                outputStream.close();                sinkCounter.incrementConnectionClosedCount();                shouldRotate = false;            } catch (IOException e) {                sinkCounter.incrementConnectionFailedCount();                throw new EventDeliveryException("Unable to rotate file " + pathController.getCurrentFile() + " while delivering event", e);            } finally {                serializer = null;                outputStream = null;            }            pathController.rotate();        }    }    if (outputStream == null) {        File currentFile = pathController.getCurrentFile();                try {            outputStream = new BufferedOutputStream(new FileOutputStream(currentFile));            serializer = EventSerializerFactory.getInstance(serializerType, serializerContext, outputStream);            serializer.afterCreate();            sinkCounter.incrementConnectionCreatedCount();        } catch (IOException e) {            sinkCounter.incrementConnectionFailedCount();            throw new EventDeliveryException("Failed to open file " + pathController.getCurrentFile() + " while delivering event", e);        }    }    Channel channel = getChannel();    Transaction transaction = channel.getTransaction();    Event event = null;    Status result = Status.READY;    try {        transaction.begin();        int eventAttemptCounter = 0;        for (int i = 0; i < batchSize; i++) {            event = channel.take();            if (event != null) {                sinkCounter.incrementEventDrainAttemptCount();                eventAttemptCounter++;                serializer.write(event);            /*           * FIXME: Feature: Rotate on size and time by checking bytes written and           * setting shouldRotate = true if we're past a threshold.           */            /*           * FIXME: Feature: Control flush interval based on time or number of           * events. For now, we're super-conservative and flush on each write.           */            } else {                                result = Status.BACKOFF;                break;            }        }        serializer.flush();        outputStream.flush();        transaction.commit();        sinkCounter.addToEventDrainSuccessCount(eventAttemptCounter);    } catch (Exception ex) {        sinkCounter.incrementEventWriteOrChannelFail(ex);        transaction.rollback();        throw new EventDeliveryException("Failed to process transaction", ex);    } finally {        transaction.close();    }    return result;}
1
public void stop()
{        sinkCounter.stop();    super.stop();    if (outputStream != null) {                try {            serializer.flush();            serializer.beforeClose();            outputStream.close();            sinkCounter.incrementConnectionClosedCount();        } catch (IOException e) {            sinkCounter.incrementConnectionFailedCount();                    } finally {            outputStream = null;            serializer = null;        }    }    if (rollInterval > 0) {        rollService.shutdown();        while (!rollService.isTerminated()) {            try {                rollService.awaitTermination(1, TimeUnit.SECONDS);            } catch (InterruptedException e) {                            }        }    }    }
1
public File getDirectory()
{    return directory;}
0
public void setDirectory(File directory)
{    this.directory = directory;}
0
public long getRollInterval()
{    return rollInterval;}
0
public void setRollInterval(long rollInterval)
{    this.rollInterval = rollInterval;}
0
public long getBatchSize()
{    return batchSize;}
0
public void configure(Context context)
{    conf = new SinkGroupConfiguration("sinkgrp");    try {        conf.configure(context);    } catch (ConfigurationException e) {        throw new FlumeException("Invalid Configuration!", e);    }    configure(conf);}
0
public SinkProcessor getProcessor()
{    return processor;}
0
public void configure(ComponentConfiguration conf)
{    this.conf = (SinkGroupConfiguration) conf;    processor = SinkProcessorFactory.getProcessor(this.conf.getProcessorContext(), sinks);}
0
public static SinkProcessor getProcessor(Context context, List<Sink> sinks)
{    Preconditions.checkNotNull(context);    Preconditions.checkNotNull(sinks);    Preconditions.checkArgument(!sinks.isEmpty());    Map<String, String> params = context.getParameters();    SinkProcessor processor;    String typeStr = params.get(TYPE);    SinkProcessorType type = SinkProcessorType.OTHER;    String processorClassName = typeStr;    try {        type = SinkProcessorType.valueOf(typeStr.toUpperCase(Locale.ENGLISH));    } catch (Exception ex) {            }    if (!type.equals(SinkProcessorType.OTHER)) {        processorClassName = type.getSinkProcessorClassName();    }        Class<? extends SinkProcessor> processorClass = null;    try {        processorClass = (Class<? extends SinkProcessor>) Class.forName(processorClassName);    } catch (Exception ex) {        throw new FlumeException("Unable to load sink processor type: " + typeStr + ", class: " + type.getSinkProcessorClassName(), ex);    }    try {        processor = processorClass.newInstance();    } catch (Exception e) {        throw new FlumeException("Unable to create sink processor, type: " + typeStr + ", class: " + processorClassName, e);    }    processor.setSinks(sinks);    Configurables.configure(processor, context);    return processor;}
1
public static SinkProcessor getProcessor(ComponentConfiguration conf, List<Sink> sinks)
{    String typeStr = conf.getType();    SinkProcessor processor;    SinkProcessorType type = SinkProcessorType.DEFAULT;    try {        type = SinkProcessorType.valueOf(typeStr.toUpperCase(Locale.ENGLISH));    } catch (Exception ex) {            }    Class<? extends SinkProcessor> processorClass = null;    try {        processorClass = (Class<? extends SinkProcessor>) Class.forName(type.getSinkProcessorClassName());    } catch (Exception ex) {        throw new FlumeException("Unable to load sink processor type: " + typeStr + ", class: " + type.getSinkProcessorClassName(), ex);    }    try {        processor = processorClass.newInstance();    } catch (Exception e) {        throw new FlumeException("Unable to create processor, type: " + typeStr + ", class: " + type.getSinkProcessorClassName(), e);    }    processor.setSinks(sinks);    Configurables.configure(processor, conf);    return processor;}
1
protected RpcClient initializeRpcClient(Properties props)
{            props.setProperty(RpcClientConfigurationConstants.CONFIG_CONNECTION_POOL_SIZE, String.valueOf(1));    boolean enableKerberos = Boolean.parseBoolean(props.getProperty(RpcClientConfigurationConstants.KERBEROS_KEY, "false"));    if (enableKerberos) {        return SecureRpcClientFactory.getThriftInstance(props);    } else {        props.setProperty(RpcClientConfigurationConstants.CONFIG_CLIENT_TYPE, RpcClientFactory.ClientType.THRIFT.name());        return RpcClientFactory.getInstance(props);    }}
0
public SinkProcessor getPolicy()
{    return policy;}
0
public void setSink(SinkProcessor policy)
{    this.policy = policy;}
0
public void start()
{    SinkProcessor policy = getPolicy();    policy.start();    runner = new PollingRunner();    runner.policy = policy;    runner.counterGroup = counterGroup;    runner.shouldStop = new AtomicBoolean();    runnerThread = new Thread(runner);    runnerThread.setName("SinkRunner-PollingRunner-" + policy.getClass().getSimpleName());    runnerThread.start();    lifecycleState = LifecycleState.START;}
0
public void stop()
{    if (runnerThread != null) {        runner.shouldStop.set(true);        runnerThread.interrupt();        while (runnerThread.isAlive()) {            try {                                runnerThread.join(500);            } catch (InterruptedException e) {                            }        }    }    getPolicy().stop();    lifecycleState = LifecycleState.STOP;}
1
public String toString()
{    return "SinkRunner: { policy:" + getPolicy() + " counterGroup:" + counterGroup + " }";}
0
public LifecycleState getLifecycleState()
{    return lifecycleState;}
0
public void run()
{        while (!shouldStop.get()) {        try {            if (policy.process().equals(Sink.Status.BACKOFF)) {                counterGroup.incrementAndGet("runner.backoffs");                Thread.sleep(Math.min(counterGroup.incrementAndGet("runner.backoffs.consecutive") * backoffSleepIncrement, maxBackoffSleep));            } else {                counterGroup.set("runner.backoffs.consecutive", 0L);            }        } catch (InterruptedException e) {                        counterGroup.incrementAndGet("runner.interruptions");        } catch (Exception e) {                        if (e instanceof EventDeliveryException) {                counterGroup.incrementAndGet("runner.deliveryErrors");            } else {                counterGroup.incrementAndGet("runner.errors");            }            try {                Thread.sleep(maxBackoffSleep);            } catch (InterruptedException ex) {                Thread.currentThread().interrupt();            }        }    }    }
1
public Status process() throws EventDeliveryException
{    Exception exception = getStartException();    if (exception != null) {        throw new FlumeException("Source had error configuring or starting", exception);    }    if (!isStarted()) {        throw new EventDeliveryException("Source is not started.  It is in '" + getLifecycleState() + "' state");    }    return doProcess();}
0
public synchronized void configure(Context context)
{    super.configure(context);    backoffSleepIncrement = context.getLong(PollableSourceConstants.BACKOFF_SLEEP_INCREMENT, PollableSourceConstants.DEFAULT_BACKOFF_SLEEP_INCREMENT);    maxBackoffSleep = context.getLong(PollableSourceConstants.MAX_BACKOFF_SLEEP, PollableSourceConstants.DEFAULT_MAX_BACKOFF_SLEEP);}
0
public long getBackOffSleepIncrement()
{    return backoffSleepIncrement;}
0
public long getMaxBackOffSleepInterval()
{    return maxBackoffSleep;}
0
public synchronized void start()
{    Preconditions.checkState(channelProcessor != null, "No channel processor configured");    lifecycleState = LifecycleState.START;}
0
public synchronized void stop()
{    lifecycleState = LifecycleState.STOP;}
0
public synchronized void setChannelProcessor(ChannelProcessor cp)
{    channelProcessor = cp;}
0
public synchronized ChannelProcessor getChannelProcessor()
{    return channelProcessor;}
0
public synchronized LifecycleState getLifecycleState()
{    return lifecycleState;}
0
public synchronized void setName(String name)
{    this.name = name;}
0
public synchronized String getName()
{    return name;}
0
public String toString()
{    return this.getClass().getName() + "{name:" + name + ",state:" + lifecycleState + "}";}
0
public void configure(Context context)
{    configureSsl(context);    Configurables.ensureRequiredNonNull(context, PORT_KEY, BIND_KEY);    port = context.getInteger(PORT_KEY);    bindAddress = context.getString(BIND_KEY);    compressionType = context.getString(COMPRESSION_TYPE, "none");    try {        maxThreads = context.getInteger(THREADS, 0);    } catch (NumberFormatException e) {            }    enableIpFilter = context.getBoolean(IP_FILTER_KEY, false);    if (enableIpFilter) {        patternRuleConfigDefinition = context.getString(IP_FILTER_RULES_KEY);        if (patternRuleConfigDefinition == null || patternRuleConfigDefinition.trim().isEmpty()) {            throw new FlumeException("ipFilter is configured with true but ipFilterRules is not defined:" + " ");        }        String[] patternRuleDefinitions = patternRuleConfigDefinition.split(",");        rules = new ArrayList<IpFilterRule>(patternRuleDefinitions.length);        for (String patternRuleDefinition : patternRuleDefinitions) {            rules.add(generateRule(patternRuleDefinition));        }    }    if (sourceCounter == null) {        sourceCounter = new SourceCounter(getName());    }}
1
public void start()
{        try {        Responder responder = new SpecificResponder(AvroSourceProtocol.class, this);        socketChannelFactory = initSocketChannelFactory();        ChannelPipelineFactory pipelineFactory = initChannelPipelineFactory();        server = new NettyServer(responder, new InetSocketAddress(bindAddress, port), socketChannelFactory, pipelineFactory, null);    } catch (org.jboss.netty.channel.ChannelException nce) {                stop();        throw new FlumeException("Failed to set up server socket", nce);    }    connectionCountUpdater = Executors.newSingleThreadScheduledExecutor();    server.start();    sourceCounter.start();    super.start();    final NettyServer srv = (NettyServer) server;    connectionCountUpdater.scheduleWithFixedDelay(() -> sourceCounter.setOpenConnectionCount(Long.valueOf(srv.getNumActiveConnections())), 0, 60, TimeUnit.SECONDS);    }
1
private NioServerSocketChannelFactory initSocketChannelFactory()
{    NioServerSocketChannelFactory socketChannelFactory;    if (maxThreads <= 0) {        socketChannelFactory = new NioServerSocketChannelFactory(Executors.newCachedThreadPool(new ThreadFactoryBuilder().setNameFormat("Avro " + NettyTransceiver.class.getSimpleName() + " Boss-%d").build()), Executors.newCachedThreadPool(new ThreadFactoryBuilder().setNameFormat("Avro " + NettyTransceiver.class.getSimpleName() + "  I/O Worker-%d").build()));    } else {        socketChannelFactory = new NioServerSocketChannelFactory(Executors.newCachedThreadPool(new ThreadFactoryBuilder().setNameFormat("Avro " + NettyTransceiver.class.getSimpleName() + " Boss-%d").build()), Executors.newFixedThreadPool(maxThreads, new ThreadFactoryBuilder().setNameFormat("Avro " + NettyTransceiver.class.getSimpleName() + "  I/O Worker-%d").build()));    }    return socketChannelFactory;}
0
private ChannelPipelineFactory initChannelPipelineFactory()
{    ChannelPipelineFactory pipelineFactory;    boolean enableCompression = compressionType.equalsIgnoreCase("deflate");    if (enableCompression || isSslEnabled() || enableIpFilter) {        pipelineFactory = new AdvancedChannelPipelineFactory(enableCompression, enableIpFilter, patternRuleConfigDefinition, getSslEngineSupplier(false));    } else {        pipelineFactory = Channels::pipeline;    }    return pipelineFactory;}
0
public void stop()
{        if (server != null) {        server.close();        try {            server.join();            server = null;        } catch (InterruptedException e) {                        Thread.currentThread().interrupt();        }    }    if (socketChannelFactory != null) {        socketChannelFactory.releaseExternalResources();        socketChannelFactory = null;    }    sourceCounter.stop();    if (connectionCountUpdater != null) {        connectionCountUpdater.shutdownNow();        connectionCountUpdater = null;    }    super.stop();    }
1
public String toString()
{    return "Avro source " + getName() + ": { bindAddress: " + bindAddress + ", port: " + port + " }";}
0
private static Map<String, String> toStringMap(Map<CharSequence, CharSequence> charSeqMap)
{    Map<String, String> stringMap = new HashMap<String, String>();    for (Map.Entry<CharSequence, CharSequence> entry : charSeqMap.entrySet()) {        stringMap.put(entry.getKey().toString(), entry.getValue().toString());    }    return stringMap;}
0
public Status append(AvroFlumeEvent avroEvent)
{    if (logger.isDebugEnabled()) {        if (LogPrivacyUtil.allowLogRawData()) {                    } else {                    }    }    sourceCounter.incrementAppendReceivedCount();    sourceCounter.incrementEventReceivedCount();    Event event = EventBuilder.withBody(avroEvent.getBody().array(), toStringMap(avroEvent.getHeaders()));    try {        getChannelProcessor().processEvent(event);    } catch (ChannelException ex) {                sourceCounter.incrementChannelWriteFail();        return Status.FAILED;    }    sourceCounter.incrementAppendAcceptedCount();    sourceCounter.incrementEventAcceptedCount();    return Status.OK;}
1
public Status appendBatch(List<AvroFlumeEvent> events)
{        sourceCounter.incrementAppendBatchReceivedCount();    sourceCounter.addToEventReceivedCount(events.size());    List<Event> batch = new ArrayList<Event>();    for (AvroFlumeEvent avroEvent : events) {        Event event = EventBuilder.withBody(avroEvent.getBody().array(), toStringMap(avroEvent.getHeaders()));        batch.add(event);    }    try {        getChannelProcessor().processEventBatch(batch);    } catch (Throwable t) {                sourceCounter.incrementChannelWriteFail();        if (t instanceof Error) {            throw (Error) t;        }        return Status.FAILED;    }    sourceCounter.incrementAppendBatchAcceptedCount();    sourceCounter.addToEventAcceptedCount(events.size());    return Status.OK;}
1
private PatternRule generateRule(String patternRuleDefinition) throws FlumeException
{    patternRuleDefinition = patternRuleDefinition.trim();        int firstColonIndex = patternRuleDefinition.indexOf(":");    if (firstColonIndex == -1) {        throw new FlumeException("Invalid ipFilter patternRule '" + patternRuleDefinition + "' should look like <'allow'  or 'deny'>:<'ip' or " + "'name'>:<pattern>");    } else {        String ruleAccessFlag = patternRuleDefinition.substring(0, firstColonIndex);        int secondColonIndex = patternRuleDefinition.indexOf(":", firstColonIndex + 1);        if ((!ruleAccessFlag.equals("allow") && !ruleAccessFlag.equals("deny")) || secondColonIndex == -1) {            throw new FlumeException("Invalid ipFilter patternRule '" + patternRuleDefinition + "' should look like <'allow'  or 'deny'>:<'ip' or " + "'name'>:<pattern>");        }        String patternTypeFlag = patternRuleDefinition.substring(firstColonIndex + 1, secondColonIndex);        if ((!patternTypeFlag.equals("ip") && !patternTypeFlag.equals("name"))) {            throw new FlumeException("Invalid ipFilter patternRule '" + patternRuleDefinition + "' should look like <'allow'  or 'deny'>:<'ip' or " + "'name'>:<pattern>");        }        boolean isAllow = ruleAccessFlag.equals("allow");        String patternRuleString = (patternTypeFlag.equals("ip") ? "i" : "n") + ":" + patternRuleDefinition.substring(secondColonIndex + 1);                return new PatternRule(isAllow, patternRuleString);    }}
1
public ChannelPipeline getPipeline() throws Exception
{    ChannelPipeline pipeline = Channels.pipeline();    if (enableCompression) {        ZlibEncoder encoder = new ZlibEncoder(6);        pipeline.addFirst("deflater", encoder);        pipeline.addFirst("inflater", new ZlibDecoder());    }    sslEngineSupplier.get().ifPresent(sslEngine -> {                                        pipeline.addFirst("ssl", new SslHandler(sslEngine));    });    if (enableIpFilter) {                IpFilterRuleHandler ipFilterHandler = new IpFilterRuleHandler();        ipFilterHandler.addAll(rules);                pipeline.addFirst("ipFilter", ipFilterHandler);    }    return pipeline;}
1
public synchronized void configure(Context context)
{    if (isStarted()) {        throw new IllegalStateException("Configure called when started");    } else {        try {            exception = null;            setLifecycleState(LifecycleState.IDLE);            doConfigure(context);        } catch (Exception e) {            exception = e;            setLifecycleState(LifecycleState.ERROR);                        Throwables.propagate(e);        }    }}
0
public synchronized void start()
{    if (exception != null) {            } else {        try {            Preconditions.checkState(channelProcessor != null, "No channel processor configured");            doStart();            setLifecycleState(LifecycleState.START);        } catch (Exception e) {                        exception = e;            setLifecycleState(LifecycleState.ERROR);        }    }}
1
public synchronized void stop()
{    try {        doStop();        setLifecycleState(LifecycleState.STOP);    } catch (Exception e) {                setLifecycleState(LifecycleState.ERROR);    }}
1
public synchronized void setChannelProcessor(ChannelProcessor cp)
{    channelProcessor = cp;}
0
public synchronized ChannelProcessor getChannelProcessor()
{    return channelProcessor;}
0
public synchronized void setName(String name)
{    this.name = name;}
0
public synchronized String getName()
{    return name;}
0
public synchronized LifecycleState getLifecycleState()
{    return lifecycleState;}
0
public String toString()
{    return this.getClass().getName() + "{name:" + name + ",state:" + lifecycleState + "}";}
0
protected boolean isStarted()
{    return getLifecycleState() == LifecycleState.START;}
0
protected Exception getStartException()
{    return exception;}
0
protected synchronized void setLifecycleState(LifecycleState lifecycleState)
{    this.lifecycleState = lifecycleState;}
0
public Source create(String name, String type) throws FlumeException
{    Preconditions.checkNotNull(name, "name");    Preconditions.checkNotNull(type, "type");        Class<? extends Source> sourceClass = getClass(type);    try {        Source source = sourceClass.newInstance();        source.setName(name);        return source;    } catch (Exception ex) {        throw new FlumeException("Unable to create source: " + name + ", type: " + type + ", class: " + sourceClass.getName(), ex);    }}
1
public Class<? extends Source> getClass(String type) throws FlumeException
{    String sourceClassName = type;    SourceType srcType = SourceType.OTHER;    try {        srcType = SourceType.valueOf(type.toUpperCase(Locale.ENGLISH));    } catch (IllegalArgumentException ex) {            }    if (!srcType.equals(SourceType.OTHER)) {        sourceClassName = srcType.getSourceClassName();    }    try {        return (Class<? extends Source>) Class.forName(sourceClassName);    } catch (Exception ex) {        throw new FlumeException("Unable to load source type: " + type + ", class: " + sourceClassName, ex);    }}
1
public void start()
{    Source source = getSource();    ChannelProcessor cp = source.getChannelProcessor();    cp.initialize();    source.start();    lifecycleState = LifecycleState.START;}
0
public void stop()
{    Source source = getSource();    source.stop();    ChannelProcessor cp = source.getChannelProcessor();    cp.close();    lifecycleState = LifecycleState.STOP;}
0
public String toString()
{    return "EventDrivenSourceRunner: { source:" + getSource() + " }";}
0
public LifecycleState getLifecycleState()
{    return lifecycleState;}
0
public void start()
{            sourceCounter.start();    executor = Executors.newSingleThreadExecutor();    runner = new ExecRunnable(shell, command, getChannelProcessor(), sourceCounter, restart, restartThrottle, logStderr, bufferCount, batchTimeout, charset);        runnerFuture = executor.submit(runner);        super.start();    }
1
public void stop()
{        if (runner != null) {        runner.setRestart(false);        runner.kill();    }    if (runnerFuture != null) {                runnerFuture.cancel(true);            }    executor.shutdown();    while (!executor.isTerminated()) {                try {            executor.awaitTermination(500, TimeUnit.MILLISECONDS);        } catch (InterruptedException e) {                        Thread.currentThread().interrupt();        }    }    sourceCounter.stop();    super.stop();    }
1
public void configure(Context context)
{    command = context.getString("command");    Preconditions.checkState(command != null, "The parameter command must be specified");    restartThrottle = context.getLong(ExecSourceConfigurationConstants.CONFIG_RESTART_THROTTLE, ExecSourceConfigurationConstants.DEFAULT_RESTART_THROTTLE);    restart = context.getBoolean(ExecSourceConfigurationConstants.CONFIG_RESTART, ExecSourceConfigurationConstants.DEFAULT_RESTART);    logStderr = context.getBoolean(ExecSourceConfigurationConstants.CONFIG_LOG_STDERR, ExecSourceConfigurationConstants.DEFAULT_LOG_STDERR);    bufferCount = context.getInteger(ExecSourceConfigurationConstants.CONFIG_BATCH_SIZE, ExecSourceConfigurationConstants.DEFAULT_BATCH_SIZE);    batchTimeout = context.getLong(ExecSourceConfigurationConstants.CONFIG_BATCH_TIME_OUT, ExecSourceConfigurationConstants.DEFAULT_BATCH_TIME_OUT);    charset = Charset.forName(context.getString(ExecSourceConfigurationConstants.CHARSET, ExecSourceConfigurationConstants.DEFAULT_CHARSET));    shell = context.getString(ExecSourceConfigurationConstants.CONFIG_SHELL, null);    if (sourceCounter == null) {        sourceCounter = new SourceCounter(getName());    }}
0
public long getBatchSize()
{    return bufferCount;}
0
public void run()
{    do {        String exitCode = "unknown";        BufferedReader reader = null;        String line = null;        final List<Event> eventList = new ArrayList<Event>();        timedFlushService = Executors.newSingleThreadScheduledExecutor(new ThreadFactoryBuilder().setNameFormat("timedFlushExecService" + Thread.currentThread().getId() + "-%d").build());        try {            if (shell != null) {                String[] commandArgs = formulateShellCommand(shell, command);                process = Runtime.getRuntime().exec(commandArgs);            } else {                String[] commandArgs = command.split("\\s+");                process = new ProcessBuilder(commandArgs).start();            }            reader = new BufferedReader(new InputStreamReader(process.getInputStream(), charset));                        StderrReader stderrReader = new StderrReader(new BufferedReader(new InputStreamReader(process.getErrorStream(), charset)), logStderr);            stderrReader.setName("StderrReader-[" + command + "]");            stderrReader.setDaemon(true);            stderrReader.start();            future = timedFlushService.scheduleWithFixedDelay(new Runnable() {                @Override                public void run() {                    try {                        synchronized (eventList) {                            if (!eventList.isEmpty() && timeout()) {                                flushEventBatch(eventList);                            }                        }                    } catch (Exception e) {                                                if (e instanceof InterruptedException) {                            Thread.currentThread().interrupt();                        }                    }                }            }, batchTimeout, batchTimeout, TimeUnit.MILLISECONDS);            while ((line = reader.readLine()) != null) {                sourceCounter.incrementEventReceivedCount();                synchronized (eventList) {                    eventList.add(EventBuilder.withBody(line.getBytes(charset)));                    if (eventList.size() >= bufferCount || timeout()) {                        flushEventBatch(eventList);                    }                }            }            synchronized (eventList) {                if (!eventList.isEmpty()) {                    flushEventBatch(eventList);                }            }        } catch (Exception e) {                        if (e instanceof InterruptedException) {                Thread.currentThread().interrupt();            }        } finally {            if (reader != null) {                try {                    reader.close();                } catch (IOException ex) {                                    }            }            exitCode = String.valueOf(kill());        }        if (restart) {                        try {                Thread.sleep(restartThrottle);            } catch (InterruptedException e) {                Thread.currentThread().interrupt();            }        } else {                    }    } while (restart);}
1
public void run()
{    try {        synchronized (eventList) {            if (!eventList.isEmpty() && timeout()) {                flushEventBatch(eventList);            }        }    } catch (Exception e) {                if (e instanceof InterruptedException) {            Thread.currentThread().interrupt();        }    }}
1
private void flushEventBatch(List<Event> eventList)
{    channelProcessor.processEventBatch(eventList);    sourceCounter.addToEventAcceptedCount(eventList.size());    eventList.clear();    lastPushToChannel = systemClock.currentTimeMillis();}
0
private boolean timeout()
{    return (systemClock.currentTimeMillis() - lastPushToChannel) >= batchTimeout;}
0
private static String[] formulateShellCommand(String shell, String command)
{    String[] shellArgs = shell.split("\\s+");    String[] result = new String[shellArgs.length + 1];    System.arraycopy(shellArgs, 0, result, 0, shellArgs.length);    result[shellArgs.length] = command;    return result;}
0
public int kill()
{    if (process != null) {        synchronized (process) {            process.destroy();            try {                int exitValue = process.waitFor();                                if (future != null) {                    future.cancel(true);                }                if (timedFlushService != null) {                    timedFlushService.shutdown();                    while (!timedFlushService.isTerminated()) {                        try {                            timedFlushService.awaitTermination(500, TimeUnit.MILLISECONDS);                        } catch (InterruptedException e) {                                                        Thread.currentThread().interrupt();                        }                    }                }                return exitValue;            } catch (InterruptedException ex) {                Thread.currentThread().interrupt();            }        }        return Integer.MIN_VALUE;    }    return Integer.MIN_VALUE / 2;}
1
public void setRestart(boolean restart)
{    this.restart = restart;}
0
public void run()
{    try {        int i = 0;        String line = null;        while ((line = input.readLine()) != null) {            if (logStderr) {                                                                            }        }    } catch (IOException e) {            } finally {        try {            if (input != null) {                input.close();            }        } catch (IOException ex) {                    }    }}
1
public List<Event> getEvents(HttpServletRequest request) throws Exception
{    Map<String, String> headers = new HashMap<String, String>();    InputStream inputStream = request.getInputStream();    Map<String, String[]> parameters = request.getParameterMap();    for (String parameter : parameters.keySet()) {        String value = parameters.get(parameter)[0];        if (LOG.isDebugEnabled() && LogPrivacyUtil.allowLogRawData()) {                    }        headers.put(parameter, value);    }    for (String header : mandatoryHeaders) {        Preconditions.checkArgument(headers.containsKey(header), "Please specify " + header + " parameter in the request.");    }    ByteArrayOutputStream outputStream = new ByteArrayOutputStream();    try {        IOUtils.copy(inputStream, outputStream);                Event event = EventBuilder.withBody(outputStream.toByteArray(), headers);        event.setHeaders(headers);        List<Event> eventList = new ArrayList<Event>();        eventList.add(event);        return eventList;    } finally {        outputStream.close();        inputStream.close();    }}
1
public void configure(Context context)
{    this.commaSeparatedHeaders = context.getString(MANDATORY_PARAMETERS, DEFAULT_MANDATORY_PARAMETERS);    this.mandatoryHeaders = commaSeparatedHeaders.split(PARAMETER_SEPARATOR);}
0
public void configure(Context context)
{    configureSsl(context);    sourceContext = context;    try {        port = context.getInteger(HTTPSourceConfigurationConstants.CONFIG_PORT);        host = context.getString(HTTPSourceConfigurationConstants.CONFIG_BIND, HTTPSourceConfigurationConstants.DEFAULT_BIND);        Preconditions.checkState(host != null && !host.isEmpty(), "HTTPSource hostname specified is empty");        Preconditions.checkNotNull(port, "HTTPSource requires a port number to be" + " specified");        String handlerClassName = context.getString(HTTPSourceConfigurationConstants.CONFIG_HANDLER, HTTPSourceConfigurationConstants.DEFAULT_HANDLER).trim();        @SuppressWarnings("unchecked")        Class<? extends HTTPSourceHandler> clazz = (Class<? extends HTTPSourceHandler>) Class.forName(handlerClassName);        handler = clazz.getDeclaredConstructor().newInstance();        Map<String, String> subProps = context.getSubProperties(HTTPSourceConfigurationConstants.CONFIG_HANDLER_PREFIX);        handler.configure(new Context(subProps));    } catch (ClassNotFoundException ex) {                Throwables.propagate(ex);    } catch (ClassCastException ex) {                Throwables.propagate(ex);    } catch (Exception ex) {                Throwables.propagate(ex);    }    if (sourceCounter == null) {        sourceCounter = new SourceCounter(getName());    }}
1
public void start()
{    Preconditions.checkState(srv == null, "Running HTTP Server found in source: " + getName() + " before I started one." + "Will not attempt to start.");    QueuedThreadPool threadPool = new QueuedThreadPool();    if (sourceContext.getSubProperties("QueuedThreadPool.").size() > 0) {        FlumeBeanConfigurator.setConfigurationFields(threadPool, sourceContext);    }    srv = new Server(threadPool);        MBeanContainer mbContainer = new MBeanContainer(ManagementFactory.getPlatformMBeanServer());    srv.addEventListener(mbContainer);    srv.addBean(mbContainer);    HttpConfiguration httpConfiguration = new HttpConfiguration();    httpConfiguration.addCustomizer(new SecureRequestCustomizer());    FlumeBeanConfigurator.setConfigurationFields(httpConfiguration, sourceContext);    ServerConnector connector = getSslContextSupplier().get().map(sslContext -> {        SslContextFactory sslCtxFactory = new SslContextFactory();        sslCtxFactory.setSslContext(sslContext);        sslCtxFactory.setExcludeProtocols(getExcludeProtocols().toArray(new String[] {}));        sslCtxFactory.setIncludeProtocols(getIncludeProtocols().toArray(new String[] {}));        sslCtxFactory.setExcludeCipherSuites(getExcludeCipherSuites().toArray(new String[] {}));        sslCtxFactory.setIncludeCipherSuites(getIncludeCipherSuites().toArray(new String[] {}));        FlumeBeanConfigurator.setConfigurationFields(sslCtxFactory, sourceContext);        httpConfiguration.setSecurePort(port);        httpConfiguration.setSecureScheme("https");        return new ServerConnector(srv, new SslConnectionFactory(sslCtxFactory, HttpVersion.HTTP_1_1.asString()), new HttpConnectionFactory(httpConfiguration));    }).orElse(new ServerConnector(srv, new HttpConnectionFactory(httpConfiguration)));    connector.setPort(port);    connector.setHost(host);    connector.setReuseAddress(true);    FlumeBeanConfigurator.setConfigurationFields(connector, sourceContext);    srv.addConnector(connector);    try {        ServletContextHandler context = new ServletContextHandler(ServletContextHandler.SESSIONS);        context.setContextPath("/");        srv.setHandler(context);        context.addServlet(new ServletHolder(new FlumeHTTPServlet()), "/");        context.setSecurityHandler(HTTPServerConstraintUtil.enforceConstraints());        srv.start();    } catch (Exception ex) {                Throwables.propagate(ex);    }    Preconditions.checkArgument(srv.isRunning());    sourceCounter.start();    super.start();}
1
public void stop()
{    try {        srv.stop();        srv.join();        srv = null;    } catch (Exception ex) {            }    sourceCounter.stop();    }
1
public void doPost(HttpServletRequest request, HttpServletResponse response) throws IOException
{        List<Event> events = Collections.emptyList();    try {        events = handler.getEvents(request);    } catch (HTTPBadRequestException ex) {                sourceCounter.incrementEventReadFail();        response.sendError(HttpServletResponse.SC_BAD_REQUEST, "Bad request from client. " + ex.getMessage());        return;    } catch (Exception ex) {                sourceCounter.incrementEventReadFail();        response.sendError(HttpServletResponse.SC_INTERNAL_SERVER_ERROR, "Deserializer threw unexpected exception. " + ex.getMessage());        return;    }    sourceCounter.incrementAppendBatchReceivedCount();    sourceCounter.addToEventReceivedCount(events.size());    try {        getChannelProcessor().processEventBatch(events);    } catch (ChannelException ex) {                sourceCounter.incrementChannelWriteFail();        response.sendError(HttpServletResponse.SC_SERVICE_UNAVAILABLE, "Error appending event to channel. Channel might be full." + ex.getMessage());        return;    } catch (Exception ex) {                sourceCounter.incrementGenericProcessingFail();        response.sendError(HttpServletResponse.SC_INTERNAL_SERVER_ERROR, "Unexpected error while appending event to channel. " + ex.getMessage());        return;    }    response.setCharacterEncoding(request.getCharacterEncoding());    response.setStatus(HttpServletResponse.SC_OK);    response.flushBuffer();    sourceCounter.incrementAppendBatchAcceptedCount();    sourceCounter.addToEventAcceptedCount(events.size());}
1
public void doGet(HttpServletRequest request, HttpServletResponse response) throws IOException
{    doPost(request, response);}
0
protected void configureSsl(Context context)
{    handleDeprecatedParameter(context, "ssl", "enableSSL");    handleDeprecatedParameter(context, "exclude-protocols", "excludeProtocols");    handleDeprecatedParameter(context, "keystore-password", "keystorePassword");    super.configureSsl(context);}
0
private void handleDeprecatedParameter(Context context, String newParam, String oldParam)
{    if (!context.containsKey(newParam) && context.containsKey(oldParam)) {        context.put(newParam, context.getString(oldParam));    }}
0
public List<Event> getEvents(HttpServletRequest request) throws Exception
{    BufferedReader reader = request.getReader();    String charset = request.getCharacterEncoding();        if (charset == null) {                charset = "UTF-8";    } else if (!(charset.equalsIgnoreCase("utf-8") || charset.equalsIgnoreCase("utf-16") || charset.equalsIgnoreCase("utf-32"))) {                throw new UnsupportedCharsetException("JSON handler supports UTF-8, " + "UTF-16 and UTF-32 only.");    }    /*     * Gson throws Exception if the data is not parseable to JSON.     * Need not catch it since the source will catch it and return error.     */    List<Event> eventList = new ArrayList<Event>(0);    try {        eventList = gson.fromJson(reader, listType);    } catch (JsonSyntaxException ex) {        throw new HTTPBadRequestException("Request has invalid JSON Syntax.", ex);    }    for (Event e : eventList) {        ((JSONEvent) e).setCharset(charset);    }    return getSimpleEvents(eventList);}
1
public void configure(Context context)
{}
0
private List<Event> getSimpleEvents(List<Event> events)
{    List<Event> newEvents = new ArrayList<Event>(events.size());    for (Event e : events) {        newEvents.add(EventBuilder.withBody(e.getBody(), e.getHeaders()));    }    return newEvents;}
0
public void configure(Context context)
{    configureSsl(context);    String portsStr = context.getString(SyslogSourceConfigurationConstants.CONFIG_PORTS);    Preconditions.checkNotNull(portsStr, "Must define config " + "parameter for MultiportSyslogTCPSource: ports");    for (String portStr : portsStr.split("\\s+")) {        Integer port = Integer.parseInt(portStr);        ports.add(port);    }    host = context.getString(SyslogSourceConfigurationConstants.CONFIG_HOST);    numProcessors = context.getInteger(SyslogSourceConfigurationConstants.CONFIG_NUMPROCESSORS);    maxEventSize = context.getInteger(SyslogSourceConfigurationConstants.CONFIG_EVENTSIZE, SyslogUtils.DEFAULT_SIZE);    String defaultCharsetStr = context.getString(SyslogSourceConfigurationConstants.CONFIG_CHARSET, SyslogSourceConfigurationConstants.DEFAULT_CHARSET);    try {        defaultCharset = Charset.forName(defaultCharsetStr);    } catch (Exception ex) {        throw new IllegalArgumentException("Unable to parse charset " + "string (" + defaultCharsetStr + ") from port configuration.", ex);    }    defaultDecoder = new ThreadSafeDecoder(defaultCharset);        portCharsets.clear();    {        Map<String, String> portCharsetCfg = context.getSubProperties(SyslogSourceConfigurationConstants.CONFIG_PORT_CHARSET_PREFIX);        for (Map.Entry<String, String> entry : portCharsetCfg.entrySet()) {            String portStr = entry.getKey();            String charsetStr = entry.getValue();            Integer port = Integer.parseInt(portStr);            Preconditions.checkNotNull(port, "Invalid port number in config");            try {                Charset charset = Charset.forName(charsetStr);                portCharsets.put(port, new ThreadSafeDecoder(charset));            } catch (Exception ex) {                throw new IllegalArgumentException("Unable to parse charset " + "string (" + charsetStr + ") from port configuration.", ex);            }        }    }    batchSize = context.getInteger(SyslogSourceConfigurationConstants.CONFIG_BATCHSIZE, SyslogSourceConfigurationConstants.DEFAULT_BATCHSIZE);    portHeader = context.getString(SyslogSourceConfigurationConstants.CONFIG_PORT_HEADER);    clientIPHeader = context.getString(SyslogSourceConfigurationConstants.CONFIG_CLIENT_IP_HEADER);    clientHostnameHeader = context.getString(SyslogSourceConfigurationConstants.CONFIG_CLIENT_HOSTNAME_HEADER);    readBufferSize = context.getInteger(SyslogSourceConfigurationConstants.CONFIG_READBUF_SIZE, SyslogSourceConfigurationConstants.DEFAULT_READBUF_SIZE);    keepFields = SyslogUtils.chooseFieldsToKeep(context.getString(SyslogSourceConfigurationConstants.CONFIG_KEEP_FIELDS, SyslogSourceConfigurationConstants.DEFAULT_KEEP_FIELDS));    if (sourceCounter == null) {        sourceCounter = new SourceCounter(getName());    }}
0
public void start()
{            if (numProcessors != null) {        acceptor = new NioSocketAcceptor(numProcessors);    } else {        acceptor = new NioSocketAcceptor();    }    getSslContextSupplier().get().ifPresent(sslContext -> {        SslFilter filter = new SslFilter(sslContext);        SSLParameters sslParameters = sslContext.getDefaultSSLParameters();        filter.setEnabledProtocols(getFilteredProtocols(sslParameters));        filter.setEnabledCipherSuites(getFilteredCipherSuites(sslParameters));        acceptor.getFilterChain().addFirst("ssl", filter);    });    acceptor.setReuseAddress(true);    acceptor.getSessionConfig().setReadBufferSize(readBufferSize);    acceptor.getSessionConfig().setIdleTime(IdleStatus.BOTH_IDLE, 10);    acceptor.setHandler(new MultiportSyslogHandler(maxEventSize, batchSize, getChannelProcessor(), sourceCounter, portHeader, clientIPHeader, clientHostnameHeader, defaultDecoder, portCharsets, keepFields));    for (int port : ports) {        InetSocketAddress addr;        if (host != null) {            addr = new InetSocketAddress(host, port);        } else {            addr = new InetSocketAddress(port);        }        try {                                    acceptor.bind(addr);        } catch (IOException ex) {                    }    }    sourceCounter.start();    super.start();    }
1
public void stop()
{        acceptor.unbind();    acceptor.dispose();    sourceCounter.stop();    super.stop();    }
1
public String toString()
{    return "Multiport Syslog TCP source " + getName();}
0
public long getBatchSize()
{    return batchSize;}
0
public void exceptionCaught(IoSession session, Throwable cause) throws Exception
{        sourceCounter.incrementGenericProcessingFail();    if (cause instanceof Error) {        Throwables.propagate(cause);    }}
1
public void sessionCreated(IoSession session)
{                    session.setAttribute(SAVED_BUF, IoBuffer.allocate(maxEventSize, false));}
1
public void sessionOpened(IoSession session)
{        }
1
public void sessionClosed(IoSession session)
{    }
1
public void messageReceived(IoSession session, Object message)
{    IoBuffer buf = (IoBuffer) message;    IoBuffer savedBuf = (IoBuffer) session.getAttribute(SAVED_BUF);    ParsedBuffer parsedLine = new ParsedBuffer();    List<Event> events = Lists.newArrayList();        CharsetDecoder decoder = defaultDecoder.get();    int port = ((InetSocketAddress) session.getLocalAddress()).getPort();    if (portCharsets.containsKey(port)) {        decoder = portCharsets.get(port).get();    }        while (buf.hasRemaining()) {        events.clear();                for (int num = 0; num < batchSize && buf.hasRemaining(); num++) {            if (lineSplitter.parseLine(buf, savedBuf, parsedLine)) {                Event event = parseEvent(parsedLine, decoder);                if (portHeader != null) {                    event.getHeaders().put(portHeader, String.valueOf(port));                }                if (clientIPHeader != null) {                    event.getHeaders().put(clientIPHeader, SyslogUtils.getIP(session.getRemoteAddress()));                }                if (clientHostnameHeader != null) {                    event.getHeaders().put(clientHostnameHeader, SyslogUtils.getHostname(session.getRemoteAddress()));                }                events.add(event);            } else {                logger.trace("Parsed null event");            }        }                if (events.isEmpty()) {            logger.trace("Empty set!");            return;        }        int numEvents = events.size();        sourceCounter.addToEventReceivedCount(numEvents);                try {            channelProcessor.processEventBatch(events);            sourceCounter.addToEventAcceptedCount(numEvents);        } catch (Throwable t) {                        sourceCounter.incrementEventReadOrChannelFail(t);            if (t instanceof Error) {                Throwables.propagate(t);            }        }    }}
1
 Event parseEvent(ParsedBuffer parsedBuf, CharsetDecoder decoder)
{    String msg = null;    try {        msg = parsedBuf.buffer.getString(decoder);    } catch (Throwable t) {                sourceCounter.incrementEventReadFail();        if (t instanceof Error) {            Throwables.propagate(t);        }                byte[] bytes = new byte[parsedBuf.buffer.remaining()];        parsedBuf.buffer.get(bytes);        Event event = EventBuilder.withBody(bytes);        event.getHeaders().put(SyslogUtils.EVENT_STATUS, SyslogUtils.SyslogStatus.INVALID.getSyslogStatus());        return event;    }    if (logger.isTraceEnabled()) {        if (LogPrivacyUtil.allowLogRawData()) {            logger.trace("Seen raw event: {}", msg);        } else {            logger.trace("Seen raw event.");        }    }    Event event;    try {        event = syslogParser.parseMessage(msg, decoder.charset(), keepFields);        if (parsedBuf.incomplete) {            event.getHeaders().put(SyslogUtils.EVENT_STATUS, SyslogUtils.SyslogStatus.INCOMPLETE.getSyslogStatus());        }    } catch (IllegalArgumentException ex) {        event = EventBuilder.withBody(msg, decoder.charset());        event.getHeaders().put(SyslogUtils.EVENT_STATUS, SyslogUtils.SyslogStatus.INVALID.getSyslogStatus());                sourceCounter.incrementEventReadFail();    }    return event;}
1
public boolean parseLine(IoBuffer buf, IoBuffer savedBuf, ParsedBuffer parsedBuf)
{        parsedBuf.buffer = null;    parsedBuf.incomplete = false;    byte curByte;    buf.mark();        int msgPos = savedBuf.position();    boolean seenNewline = false;    while (!seenNewline && buf.hasRemaining() && msgPos < maxLineLength) {        curByte = buf.get();                if (curByte == NEWLINE) {            seenNewline = true;        }        msgPos++;    }        if (seenNewline) {        int end = buf.position();        buf.reset();        int start = buf.position();        if (savedBuf.position() > 0) {                        byte[] tmp = new byte[end - start];            buf.get(tmp);            savedBuf.put(tmp);            int len = savedBuf.position() - 1;            savedBuf.flip();            parsedBuf.buffer = savedBuf.getSlice(len);            savedBuf.clear();        } else {            parsedBuf.buffer = buf.getSlice(end - start - 1);                        buf.get();        }        return true;        } else {                if (msgPos == maxLineLength) {            int end = buf.position();            buf.reset();            int start = buf.position();            if (savedBuf.position() > 0) {                                byte[] tmp = new byte[end - start];                buf.get(tmp);                savedBuf.put(tmp);                savedBuf.flip();                parsedBuf.buffer = savedBuf.getSlice(msgPos);                savedBuf.clear();            } else {                                parsedBuf.buffer = buf.getSlice(msgPos);            }                        parsedBuf.incomplete = true;            return true;                } else if (!buf.hasRemaining()) {            int end = buf.position();            buf.reset();            int start = buf.position();            byte[] tmp = new byte[end - start];            buf.get(tmp);            savedBuf.put(tmp);            return false;                } else {            throw new IllegalStateException("unexpected buffer state: " + "msgPos=" + msgPos + ", buf.hasRemaining=" + buf.hasRemaining() + ", savedBuf.hasRemaining=" + savedBuf.hasRemaining() + ", seenNewline=" + seenNewline + ", maxLen=" + maxLineLength);        }    }}
1
protected CharsetDecoder initialValue()
{    return charset.newDecoder();}
0
public void configure(Context context)
{    String hostKey = NetcatSourceConfigurationConstants.CONFIG_HOSTNAME;    String portKey = NetcatSourceConfigurationConstants.CONFIG_PORT;    String ackEventKey = NetcatSourceConfigurationConstants.CONFIG_ACKEVENT;    Configurables.ensureRequiredNonNull(context, hostKey, portKey);    hostName = context.getString(hostKey);    port = context.getInteger(portKey);    ackEveryEvent = context.getBoolean(ackEventKey, true);    maxLineLength = context.getInteger(NetcatSourceConfigurationConstants.CONFIG_MAX_LINE_LENGTH, NetcatSourceConfigurationConstants.DEFAULT_MAX_LINE_LENGTH);    sourceEncoding = context.getString(NetcatSourceConfigurationConstants.CONFIG_SOURCE_ENCODING, NetcatSourceConfigurationConstants.DEFAULT_ENCODING);}
0
public void start()
{        counterGroup.incrementAndGet("open.attempts");    try {        SocketAddress bindPoint = new InetSocketAddress(hostName, port);        serverSocket = ServerSocketChannel.open();        serverSocket.socket().setReuseAddress(true);        serverSocket.socket().bind(bindPoint);            } catch (IOException e) {        counterGroup.incrementAndGet("open.errors");                stop();        throw new FlumeException(e);    }    handlerService = Executors.newCachedThreadPool(new ThreadFactoryBuilder().setNameFormat("netcat-handler-%d").build());    AcceptHandler acceptRunnable = new AcceptHandler(maxLineLength);    acceptThreadShouldStop.set(false);    acceptRunnable.counterGroup = counterGroup;    acceptRunnable.handlerService = handlerService;    acceptRunnable.shouldStop = acceptThreadShouldStop;    acceptRunnable.ackEveryEvent = ackEveryEvent;    acceptRunnable.source = this;    acceptRunnable.serverSocket = serverSocket;    acceptRunnable.sourceEncoding = sourceEncoding;    acceptThread = new Thread(acceptRunnable);    acceptThread.start();        super.start();}
1
public void stop()
{        acceptThreadShouldStop.set(true);    if (acceptThread != null) {                while (acceptThread.isAlive()) {            try {                                acceptThread.interrupt();                acceptThread.join(500);            } catch (InterruptedException e) {                                Thread.currentThread().interrupt();            }        }            }    if (serverSocket != null) {        try {            serverSocket.close();        } catch (IOException e) {                        return;        }    }    if (handlerService != null) {        handlerService.shutdown();                        try {            handlerService.awaitTermination(500, TimeUnit.MILLISECONDS);        } catch (InterruptedException e) {                        Thread.currentThread().interrupt();        }        if (!handlerService.isShutdown()) {            handlerService.shutdownNow();        }            }        super.stop();}
1
public void run()
{        while (!shouldStop.get()) {        try {            SocketChannel socketChannel = serverSocket.accept();            NetcatSocketHandler request = new NetcatSocketHandler(maxLineLength);            request.socketChannel = socketChannel;            request.counterGroup = counterGroup;            request.source = source;            request.ackEveryEvent = ackEveryEvent;            request.sourceEncoding = sourceEncoding;            handlerService.submit(request);            counterGroup.incrementAndGet("accept.succeeded");        } catch (ClosedByInterruptException e) {                } catch (IOException e) {                        counterGroup.incrementAndGet("accept.failed");        }    }    }
1
public void run()
{        Event event = null;    try {        Reader reader = Channels.newReader(socketChannel, sourceEncoding);        Writer writer = Channels.newWriter(socketChannel, sourceEncoding);        CharBuffer buffer = CharBuffer.allocate(maxLineLength);                buffer.flip();        while (true) {                        int charsRead = fill(buffer, reader);                                    int eventsProcessed = processEvents(buffer, writer);                        if (charsRead == -1) {                                break;            } else if (charsRead == 0 && eventsProcessed == 0) {                if (buffer.remaining() == buffer.capacity()) {                                                                                                                                                                counterGroup.incrementAndGet("events.failed");                    writer.write("FAILED: Event exceeds the maximum length (" + buffer.capacity() + " chars, including newline)\n");                    writer.flush();                    break;                }            }        }        socketChannel.close();        counterGroup.incrementAndGet("sessions.completed");    } catch (IOException e) {        counterGroup.incrementAndGet("sessions.broken");        try {            socketChannel.close();        } catch (IOException ex) {                    }    }    }
1
private int processEvents(CharBuffer buffer, Writer writer) throws IOException
{    int numProcessed = 0;    boolean foundNewLine = true;    while (foundNewLine) {        foundNewLine = false;        int limit = buffer.limit();        for (int pos = buffer.position(); pos < limit; pos++) {            if (buffer.get(pos) == '\n') {                                                buffer.limit(pos);                ByteBuffer bytes = Charsets.UTF_8.encode(buffer);                                buffer.limit(limit);                                byte[] body = new byte[bytes.remaining()];                bytes.get(body);                Event event = EventBuilder.withBody(body);                                ChannelException ex = null;                try {                    source.getChannelProcessor().processEvent(event);                } catch (ChannelException chEx) {                    ex = chEx;                }                if (ex == null) {                    counterGroup.incrementAndGet("events.processed");                    numProcessed++;                    if (true == ackEveryEvent) {                        writer.write("OK\n");                    }                } else {                    counterGroup.incrementAndGet("events.failed");                                        writer.write("FAILED: " + ex.getMessage() + "\n");                }                writer.flush();                                                buffer.position(pos + 1);                foundNewLine = true;                break;            }        }    }    return numProcessed;}
1
private int fill(CharBuffer buffer, Reader reader) throws IOException
{        buffer.compact();        int charsRead = reader.read(buffer);    counterGroup.addAndGet("characters.received", Long.valueOf(charsRead));        buffer.flip();    return charsRead;}
0
private Event extractEvent(ChannelBuffer in, SocketAddress remoteAddress)
{    Map<String, String> headers = new HashMap<String, String>();    headers.put(remoteHostHeader, remoteAddress.toString());    byte b = 0;    ByteArrayOutputStream baos = new ByteArrayOutputStream();    Event e = null;    boolean doneReading = false;    try {        while (!doneReading && in.readable()) {            b = in.readByte();                        if (b == '\n') {                doneReading = true;            } else {                baos.write(b);            }        }        e = EventBuilder.withBody(baos.toByteArray(), headers);    } finally {        }    return e;}
0
public void messageReceived(ChannelHandlerContext ctx, MessageEvent mEvent)
{    try {        Event e = extractEvent((ChannelBuffer) mEvent.getMessage(), mEvent.getRemoteAddress());        if (e == null) {            return;        }        getChannelProcessor().processEvent(e);        counterGroup.incrementAndGet("events.success");    } catch (ChannelException ex) {        counterGroup.incrementAndGet("events.dropped");            } catch (RuntimeException ex) {        counterGroup.incrementAndGet("events.dropped");            }}
1
public void start()
{        ConnectionlessBootstrap serverBootstrap = new ConnectionlessBootstrap(new OioDatagramChannelFactory(Executors.newCachedThreadPool()));    final NetcatHandler handler = new NetcatHandler();    serverBootstrap.setOption("receiveBufferSizePredictorFactory", new AdaptiveReceiveBufferSizePredictorFactory(DEFAULT_MIN_SIZE, DEFAULT_INITIAL_SIZE, maxsize));    serverBootstrap.setPipelineFactory(new ChannelPipelineFactory() {        @Override        public ChannelPipeline getPipeline() {            return Channels.pipeline(handler);        }    });    if (host == null) {        nettyChannel = serverBootstrap.bind(new InetSocketAddress(port));    } else {        nettyChannel = serverBootstrap.bind(new InetSocketAddress(host, port));    }    super.start();}
0
public ChannelPipeline getPipeline()
{    return Channels.pipeline(handler);}
0
public void stop()
{            if (nettyChannel != null) {        nettyChannel.close();        try {            nettyChannel.getCloseFuture().await(60, TimeUnit.SECONDS);        } catch (InterruptedException e) {                    } finally {            nettyChannel = null;        }    }    super.stop();}
1
public void configure(Context context)
{    Configurables.ensureRequiredNonNull(context, CONFIG_PORT);    port = context.getInteger(CONFIG_PORT);    host = context.getString(CONFIG_HOST);    remoteHostHeader = context.getString(REMOTE_ADDRESS_HEADER);}
0
public int getSourcePort()
{    SocketAddress localAddress = nettyChannel.getLocalAddress();    if (localAddress instanceof InetSocketAddress) {        InetSocketAddress addr = (InetSocketAddress) localAddress;        return addr.getPort();    }    return 0;}
0
public void start()
{    PollableSource source = (PollableSource) getSource();    ChannelProcessor cp = source.getChannelProcessor();    cp.initialize();    source.start();    runner = new PollingRunner();    runner.source = source;    runner.counterGroup = counterGroup;    runner.shouldStop = shouldStop;    runnerThread = new Thread(runner);    runnerThread.setName(getClass().getSimpleName() + "-" + source.getClass().getSimpleName() + "-" + source.getName());    runnerThread.start();    lifecycleState = LifecycleState.START;}
0
public void stop()
{    runner.shouldStop.set(true);    try {        runnerThread.interrupt();        runnerThread.join();    } catch (InterruptedException e) {                Thread.currentThread().interrupt();    }    Source source = getSource();    source.stop();    ChannelProcessor cp = source.getChannelProcessor();    cp.close();    lifecycleState = LifecycleState.STOP;}
1
public String toString()
{    return "PollableSourceRunner: { source:" + getSource() + " counterGroup:" + counterGroup + " }";}
0
public LifecycleState getLifecycleState()
{    return lifecycleState;}
0
public void run()
{        while (!shouldStop.get()) {        counterGroup.incrementAndGet("runner.polls");        try {            if (source.process().equals(PollableSource.Status.BACKOFF)) {                counterGroup.incrementAndGet("runner.backoffs");                Thread.sleep(Math.min(counterGroup.incrementAndGet("runner.backoffs.consecutive") * source.getBackOffSleepIncrement(), source.getMaxBackOffSleepInterval()));            } else {                counterGroup.set("runner.backoffs.consecutive", 0L);            }        } catch (InterruptedException e) {                        counterGroup.incrementAndGet("runner.interruptions");        } catch (EventDeliveryException e) {                        counterGroup.incrementAndGet("runner.deliveryErrors");        } catch (Exception e) {            counterGroup.incrementAndGet("runner.errors");                        try {                Thread.sleep(source.getMaxBackOffSleepInterval());            } catch (InterruptedException ex) {                Thread.currentThread().interrupt();            }        }    }    }
1
protected void doConfigure(Context context) throws FlumeException
{    batchSize = context.getInteger("batchSize", 1);    totalEvents = context.getLong("totalEvents", Long.MAX_VALUE);    Preconditions.checkArgument(batchSize > 0, "batchSize was %s but expected positive", batchSize);    if (sourceCounter == null) {        sourceCounter = new SourceCounter(getName());    }}
0
protected Status doProcess() throws EventDeliveryException
{    Status status = Status.READY;    long eventsSentTX = eventsSent;    try {        if (batchSize == 1) {            if (eventsSentTX < totalEvents) {                getChannelProcessor().processEvent(EventBuilder.withBody(String.valueOf(eventsSentTX++).getBytes()));                sourceCounter.incrementEventAcceptedCount();            } else {                status = Status.BACKOFF;            }        } else {            List<Event> batchArrayList = new ArrayList<>(batchSize);            for (int i = 0; i < batchSize; i++) {                if (eventsSentTX < totalEvents) {                    batchArrayList.add(i, EventBuilder.withBody(String.valueOf(eventsSentTX++).getBytes()));                } else {                    status = Status.BACKOFF;                    break;                }            }            if (!batchArrayList.isEmpty()) {                getChannelProcessor().processEventBatch(batchArrayList);                sourceCounter.incrementAppendBatchAcceptedCount();                sourceCounter.addToEventAcceptedCount(batchArrayList.size());            }        }        eventsSent = eventsSentTX;    } catch (ChannelException ex) {                sourceCounter.incrementChannelWriteFail();    }    return status;}
1
protected void doStart() throws FlumeException
{        sourceCounter.start();    }
1
protected void doStop() throws FlumeException
{        sourceCounter.stop();    }
1
public long getBatchSize()
{    return batchSize;}
0
public static RateLimiter create(double permitsPerSecond)
{    /*     * The default RateLimiter configuration can save the unused permits of up to one second.     * This is to avoid unnecessary stalls in situations like this: A RateLimiter of 1qps,     * and 4 threads, all calling acquire() at these moments:     *     * T0 at 0 seconds     * T1 at 1.05 seconds     * T2 at 2 seconds     * T3 at 3 seconds     *     * Due to the slight delay of T1, T2 would have to sleep till 2.05 seconds,     * and T3 would also have to sleep till 3.05 seconds.     */    return create(SleepingStopwatch.createFromSystemTimer(), permitsPerSecond);}
0
 static RateLimiter create(SleepingStopwatch stopwatch, double permitsPerSecond)
{    RateLimiter rateLimiter = new SmoothBursty(stopwatch, 1.0);    rateLimiter.setRate(permitsPerSecond);    return rateLimiter;}
0
public static RateLimiter create(double permitsPerSecond, long warmupPeriod, TimeUnit unit)
{    checkArgument(warmupPeriod >= 0, "warmupPeriod must not be negative: %s", warmupPeriod);    return create(SleepingStopwatch.createFromSystemTimer(), permitsPerSecond, warmupPeriod, unit);}
0
 static RateLimiter create(SleepingStopwatch stopwatch, double permitsPerSecond, long warmupPeriod, TimeUnit unit)
{    RateLimiter rateLimiter = new SmoothWarmingUp(stopwatch, warmupPeriod, unit);    rateLimiter.setRate(permitsPerSecond);    return rateLimiter;}
0
private Object mutex()
{    Object mutex = mutexDoNotUseDirectly;    if (mutex == null) {        synchronized (this) {            mutex = mutexDoNotUseDirectly;            if (mutex == null) {                mutexDoNotUseDirectly = mutex = new Object();            }        }    }    return mutex;}
0
public final void setRate(double permitsPerSecond)
{    checkArgument(permitsPerSecond > 0.0 && !Double.isNaN(permitsPerSecond), "rate must be positive");    synchronized (mutex()) {        doSetRate(permitsPerSecond, stopwatch.readMicros());    }}
0
public final double getRate()
{    synchronized (mutex()) {        return doGetRate();    }}
0
public double acquire()
{    return acquire(1);}
0
public double acquire(int permits)
{    long microsToWait = reserve(permits);    stopwatch.sleepMicrosUninterruptibly(microsToWait);    return 1.0 * microsToWait / SECONDS.toMicros(1L);}
0
 final long reserve(int permits)
{    checkPermits(permits);    synchronized (mutex()) {        return reserveAndGetWaitLength(permits, stopwatch.readMicros());    }}
0
public boolean tryAcquire(long timeout, TimeUnit unit)
{    return tryAcquire(1, timeout, unit);}
0
public boolean tryAcquire(int permits)
{    return tryAcquire(permits, 0, MICROSECONDS);}
0
public boolean tryAcquire()
{    return tryAcquire(1, 0, MICROSECONDS);}
0
public boolean tryAcquire(int permits, long timeout, TimeUnit unit)
{    long timeoutMicros = max(unit.toMicros(timeout), 0);    checkPermits(permits);    long microsToWait;    synchronized (mutex()) {        long nowMicros = stopwatch.readMicros();        if (!canAcquire(nowMicros, timeoutMicros)) {            return false;        } else {            microsToWait = reserveAndGetWaitLength(permits, nowMicros);        }    }    stopwatch.sleepMicrosUninterruptibly(microsToWait);    return true;}
0
private boolean canAcquire(long nowMicros, long timeoutMicros)
{    return queryEarliestAvailable(nowMicros) - timeoutMicros <= nowMicros;}
0
 final long reserveAndGetWaitLength(int permits, long nowMicros)
{    long momentAvailable = reserveEarliestAvailable(permits, nowMicros);    return max(momentAvailable - nowMicros, 0);}
0
public String toString()
{    return String.format("RateLimiter[stableRate=%3.1fqps]", getRate());}
0
 static final SleepingStopwatch createFromSystemTimer()
{    return new SleepingStopwatch() {        final Stopwatch stopwatch = Stopwatch.createStarted();        @Override        long readMicros() {            return stopwatch.elapsed(MICROSECONDS);        }        @Override        void sleepMicrosUninterruptibly(long micros) {            if (micros > 0) {                Uninterruptibles.sleepUninterruptibly(micros, MICROSECONDS);            }        }    };}
0
 long readMicros()
{    return stopwatch.elapsed(MICROSECONDS);}
0
 void sleepMicrosUninterruptibly(long micros)
{    if (micros > 0) {        Uninterruptibles.sleepUninterruptibly(micros, MICROSECONDS);    }}
0
private static int checkPermits(int permits)
{    checkArgument(permits > 0, "Requested permits (%s) must be positive", permits);    return permits;}
0
 void doSetRate(double permitsPerSecond, double stableIntervalMicros)
{    double oldMaxPermits = maxPermits;    maxPermits = warmupPeriodMicros / stableIntervalMicros;    halfPermits = maxPermits / 2.0;        double coldIntervalMicros = stableIntervalMicros * 3.0;    slope = (coldIntervalMicros - stableIntervalMicros) / halfPermits;    if (oldMaxPermits == Double.POSITIVE_INFINITY) {                storedPermits = 0.0;    } else {        storedPermits = (oldMaxPermits == 0.0) ?         maxPermits : storedPermits * maxPermits / oldMaxPermits;    }}
0
 long storedPermitsToWaitTime(double storedPermits, double permitsToTake)
{    double availablePermitsAboveHalf = storedPermits - halfPermits;    long micros = 0;        if (availablePermitsAboveHalf > 0.0) {        double permitsAboveHalfToTake = min(availablePermitsAboveHalf, permitsToTake);        micros = (long) (permitsAboveHalfToTake * (permitsToTime(availablePermitsAboveHalf) + permitsToTime(availablePermitsAboveHalf - permitsAboveHalfToTake)) / 2.0);        permitsToTake -= permitsAboveHalfToTake;    }        micros += (stableIntervalMicros * permitsToTake);    return micros;}
0
private double permitsToTime(double permits)
{    return stableIntervalMicros + permits * slope;}
0
 void doSetRate(double permitsPerSecond, double stableIntervalMicros)
{    double oldMaxPermits = this.maxPermits;    maxPermits = maxBurstSeconds * permitsPerSecond;    if (oldMaxPermits == Double.POSITIVE_INFINITY) {                storedPermits = maxPermits;    } else {        storedPermits = (oldMaxPermits == 0.0) ?         0.0 : storedPermits * maxPermits / oldMaxPermits;    }}
0
 long storedPermitsToWaitTime(double storedPermits, double permitsToTake)
{    return 0L;}
0
 final void doSetRate(double permitsPerSecond, long nowMicros)
{    resync(nowMicros);    double stableIntervalMicros = SECONDS.toMicros(1L) / permitsPerSecond;    this.stableIntervalMicros = stableIntervalMicros;    doSetRate(permitsPerSecond, stableIntervalMicros);}
0
 final double doGetRate()
{    return SECONDS.toMicros(1L) / stableIntervalMicros;}
0
 final long queryEarliestAvailable(long nowMicros)
{    return nextFreeTicketMicros;}
0
 final long reserveEarliestAvailable(int requiredPermits, long nowMicros)
{    resync(nowMicros);    long returnValue = nextFreeTicketMicros;    double storedPermitsToSpend = min(requiredPermits, this.storedPermits);    double freshPermits = requiredPermits - storedPermitsToSpend;    long waitMicros = storedPermitsToWaitTime(this.storedPermits, storedPermitsToSpend) + (long) (freshPermits * stableIntervalMicros);    this.nextFreeTicketMicros = nextFreeTicketMicros + waitMicros;    this.storedPermits -= storedPermitsToSpend;    return returnValue;}
0
private void resync(long nowMicros)
{        if (nowMicros > nextFreeTicketMicros) {        storedPermits = min(maxPermits, storedPermits + (nowMicros - nextFreeTicketMicros) / stableIntervalMicros);        nextFreeTicketMicros = nowMicros;    }}
0
public static Stopwatch createUnstarted()
{    return new Stopwatch();}
0
public static Stopwatch createUnstarted(Ticker ticker)
{    return new Stopwatch(ticker);}
0
public static Stopwatch createStarted()
{    return new Stopwatch().start();}
0
public static Stopwatch createStarted(Ticker ticker)
{    return new Stopwatch(ticker).start();}
0
public boolean isRunning()
{    return isRunning;}
0
public Stopwatch start()
{    checkState(!isRunning, "This stopwatch is already running.");    isRunning = true;    startTick = ticker.read();    return this;}
0
public Stopwatch stop()
{    long tick = ticker.read();    checkState(isRunning, "This stopwatch is already stopped.");    isRunning = false;    elapsedNanos += tick - startTick;    return this;}
0
public Stopwatch reset()
{    elapsedNanos = 0;    isRunning = false;    return this;}
0
private long elapsedNanos()
{    return isRunning ? ticker.read() - startTick + elapsedNanos : elapsedNanos;}
0
public long elapsed(TimeUnit desiredUnit)
{    return desiredUnit.convert(elapsedNanos(), NANOSECONDS);}
0
public String toString()
{    long nanos = elapsedNanos();    TimeUnit unit = chooseUnit(nanos);    double value = (double) nanos / NANOSECONDS.convert(1, unit);        return String.format("%.4g %s", value, abbreviate(unit));}
0
private static TimeUnit chooseUnit(long nanos)
{    if (DAYS.convert(nanos, NANOSECONDS) > 0) {        return DAYS;    }    if (HOURS.convert(nanos, NANOSECONDS) > 0) {        return HOURS;    }    if (MINUTES.convert(nanos, NANOSECONDS) > 0) {        return MINUTES;    }    if (SECONDS.convert(nanos, NANOSECONDS) > 0) {        return SECONDS;    }    if (MILLISECONDS.convert(nanos, NANOSECONDS) > 0) {        return MILLISECONDS;    }    if (MICROSECONDS.convert(nanos, NANOSECONDS) > 0) {        return MICROSECONDS;    }    return NANOSECONDS;}
0
private static String abbreviate(TimeUnit unit)
{    switch(unit) {        case NANOSECONDS:            return "ns";        case MICROSECONDS:                        return "\u03bcs";        case MILLISECONDS:            return "ms";        case SECONDS:            return "s";        case MINUTES:            return "min";        case HOURS:            return "h";        case DAYS:            return "d";        default:            throw new AssertionError();    }}
0
public static void awaitUninterruptibly(CountDownLatch latch)
{    boolean interrupted = false;    try {        while (true) {            try {                latch.await();                return;            } catch (InterruptedException e) {                interrupted = true;            }        }    } finally {        if (interrupted) {            Thread.currentThread().interrupt();        }    }}
0
public static boolean awaitUninterruptibly(CountDownLatch latch, long timeout, TimeUnit unit)
{    boolean interrupted = false;    try {        long remainingNanos = unit.toNanos(timeout);        long end = System.nanoTime() + remainingNanos;        while (true) {            try {                                return latch.await(remainingNanos, NANOSECONDS);            } catch (InterruptedException e) {                interrupted = true;                remainingNanos = end - System.nanoTime();            }        }    } finally {        if (interrupted) {            Thread.currentThread().interrupt();        }    }}
0
public static void joinUninterruptibly(Thread toJoin)
{    boolean interrupted = false;    try {        while (true) {            try {                toJoin.join();                return;            } catch (InterruptedException e) {                interrupted = true;            }        }    } finally {        if (interrupted) {            Thread.currentThread().interrupt();        }    }}
0
public static void joinUninterruptibly(Thread toJoin, long timeout, TimeUnit unit)
{    Preconditions.checkNotNull(toJoin);    boolean interrupted = false;    try {        long remainingNanos = unit.toNanos(timeout);        long end = System.nanoTime() + remainingNanos;        while (true) {            try {                                NANOSECONDS.timedJoin(toJoin, remainingNanos);                return;            } catch (InterruptedException e) {                interrupted = true;                remainingNanos = end - System.nanoTime();            }        }    } finally {        if (interrupted) {            Thread.currentThread().interrupt();        }    }}
0
public static V getUninterruptibly(Future<V> future) throws ExecutionException
{    boolean interrupted = false;    try {        while (true) {            try {                return future.get();            } catch (InterruptedException e) {                interrupted = true;            }        }    } finally {        if (interrupted) {            Thread.currentThread().interrupt();        }    }}
0
public static V getUninterruptibly(Future<V> future, long timeout, TimeUnit unit) throws ExecutionException, TimeoutException
{    boolean interrupted = false;    try {        long remainingNanos = unit.toNanos(timeout);        long end = System.nanoTime() + remainingNanos;        while (true) {            try {                                return future.get(remainingNanos, NANOSECONDS);            } catch (InterruptedException e) {                interrupted = true;                remainingNanos = end - System.nanoTime();            }        }    } finally {        if (interrupted) {            Thread.currentThread().interrupt();        }    }}
0
public static E takeUninterruptibly(BlockingQueue<E> queue)
{    boolean interrupted = false;    try {        while (true) {            try {                return queue.take();            } catch (InterruptedException e) {                interrupted = true;            }        }    } finally {        if (interrupted) {            Thread.currentThread().interrupt();        }    }}
0
public static void putUninterruptibly(BlockingQueue<E> queue, E element)
{    boolean interrupted = false;    try {        while (true) {            try {                queue.put(element);                return;            } catch (InterruptedException e) {                interrupted = true;            }        }    } finally {        if (interrupted) {            Thread.currentThread().interrupt();        }    }}
0
public static void sleepUninterruptibly(long sleepFor, TimeUnit unit)
{    boolean interrupted = false;    try {        long remainingNanos = unit.toNanos(sleepFor);        long end = System.nanoTime() + remainingNanos;        while (true) {            try {                                NANOSECONDS.sleep(remainingNanos);                return;            } catch (InterruptedException e) {                interrupted = true;                remainingNanos = end - System.nanoTime();            }        }    } finally {        if (interrupted) {            Thread.currentThread().interrupt();        }    }}
0
public static boolean tryAcquireUninterruptibly(Semaphore semaphore, long timeout, TimeUnit unit)
{    return tryAcquireUninterruptibly(semaphore, 1, timeout, unit);}
0
public static boolean tryAcquireUninterruptibly(Semaphore semaphore, int permits, long timeout, TimeUnit unit)
{    boolean interrupted = false;    try {        long remainingNanos = unit.toNanos(timeout);        long end = System.nanoTime() + remainingNanos;        while (true) {            try {                                return semaphore.tryAcquire(permits, remainingNanos, NANOSECONDS);            } catch (InterruptedException e) {                interrupted = true;                remainingNanos = end - System.nanoTime();            }        }    } finally {        if (interrupted) {            Thread.currentThread().interrupt();        }    }}
0
public synchronized void start()
{        executor = Executors.newSingleThreadScheduledExecutor();    File directory = new File(spoolDirectory);    try {        reader = new ReliableSpoolingFileEventReader.Builder().spoolDirectory(directory).completedSuffix(completedSuffix).includePattern(includePattern).ignorePattern(ignorePattern).trackerDirPath(trackerDirPath).annotateFileName(fileHeader).fileNameHeader(fileHeaderKey).annotateBaseName(basenameHeader).baseNameHeader(basenameHeaderKey).deserializerType(deserializerType).deserializerContext(deserializerContext).deletePolicy(deletePolicy).inputCharset(inputCharset).decodeErrorPolicy(decodeErrorPolicy).consumeOrder(consumeOrder).recursiveDirectorySearch(recursiveDirectorySearch).trackingPolicy(trackingPolicy).sourceCounter(sourceCounter).build();    } catch (IOException ioe) {        throw new FlumeException("Error instantiating spooling event parser", ioe);    }    Runnable runner = new SpoolDirectoryRunnable(reader, sourceCounter);    executor.scheduleWithFixedDelay(runner, 0, pollDelay, TimeUnit.MILLISECONDS);    super.start();        sourceCounter.start();}
1
public synchronized void stop()
{    executor.shutdown();    try {        executor.awaitTermination(10L, TimeUnit.SECONDS);    } catch (InterruptedException ex) {            }    executor.shutdownNow();    super.stop();    sourceCounter.stop();    }
1
public String toString()
{    return "Spool Directory source " + getName() + ": { spoolDir: " + spoolDirectory + " }";}
0
public synchronized void configure(Context context)
{    spoolDirectory = context.getString(SPOOL_DIRECTORY);    Preconditions.checkState(spoolDirectory != null, "Configuration must specify a spooling directory");    completedSuffix = context.getString(SPOOLED_FILE_SUFFIX, DEFAULT_SPOOLED_FILE_SUFFIX);    deletePolicy = context.getString(DELETE_POLICY, DEFAULT_DELETE_POLICY);    fileHeader = context.getBoolean(FILENAME_HEADER, DEFAULT_FILE_HEADER);    fileHeaderKey = context.getString(FILENAME_HEADER_KEY, DEFAULT_FILENAME_HEADER_KEY);    basenameHeader = context.getBoolean(BASENAME_HEADER, DEFAULT_BASENAME_HEADER);    basenameHeaderKey = context.getString(BASENAME_HEADER_KEY, DEFAULT_BASENAME_HEADER_KEY);    batchSize = context.getInteger(BATCH_SIZE, DEFAULT_BATCH_SIZE);    inputCharset = context.getString(INPUT_CHARSET, DEFAULT_INPUT_CHARSET);    decodeErrorPolicy = DecodeErrorPolicy.valueOf(context.getString(DECODE_ERROR_POLICY, DEFAULT_DECODE_ERROR_POLICY).toUpperCase(Locale.ENGLISH));    includePattern = context.getString(INCLUDE_PAT, DEFAULT_INCLUDE_PAT);    ignorePattern = context.getString(IGNORE_PAT, DEFAULT_IGNORE_PAT);    trackerDirPath = context.getString(TRACKER_DIR, DEFAULT_TRACKER_DIR);    deserializerType = context.getString(DESERIALIZER, DEFAULT_DESERIALIZER);    deserializerContext = new Context(context.getSubProperties(DESERIALIZER + "."));    consumeOrder = ConsumeOrder.valueOf(context.getString(CONSUME_ORDER, DEFAULT_CONSUME_ORDER.toString()).toUpperCase(Locale.ENGLISH));    pollDelay = context.getInteger(POLL_DELAY, DEFAULT_POLL_DELAY);    recursiveDirectorySearch = context.getBoolean(RECURSIVE_DIRECTORY_SEARCH, DEFAULT_RECURSIVE_DIRECTORY_SEARCH);            Integer bufferMaxLineLength = context.getInteger(BUFFER_MAX_LINE_LENGTH);    if (bufferMaxLineLength != null && deserializerType != null && deserializerType.equalsIgnoreCase(DEFAULT_DESERIALIZER)) {        deserializerContext.put(LineDeserializer.MAXLINE_KEY, bufferMaxLineLength.toString());    }    maxBackoff = context.getInteger(MAX_BACKOFF, DEFAULT_MAX_BACKOFF);    if (sourceCounter == null) {        sourceCounter = new SourceCounter(getName());    }    trackingPolicy = context.getString(TRACKING_POLICY, DEFAULT_TRACKING_POLICY);}
0
protected boolean hasFatalError()
{    return hasFatalError;}
0
protected void setBackOff(boolean backoff)
{    this.backoff = backoff;}
0
protected boolean didHitChannelException()
{    return hitChannelException;}
0
protected boolean didHitChannelFullException()
{    return hitChannelFullException;}
0
protected SourceCounter getSourceCounter()
{    return sourceCounter;}
0
protected boolean getRecursiveDirectorySearch()
{    return recursiveDirectorySearch;}
0
public long getBatchSize()
{    return batchSize;}
0
public void run()
{    int backoffInterval = 250;    boolean readingEvents = false;    try {        while (!Thread.interrupted()) {            readingEvents = true;            List<Event> events = reader.readEvents(batchSize);            readingEvents = false;            if (events.isEmpty()) {                break;            }            sourceCounter.addToEventReceivedCount(events.size());            sourceCounter.incrementAppendBatchReceivedCount();            try {                getChannelProcessor().processEventBatch(events);                reader.commit();            } catch (ChannelFullException ex) {                                sourceCounter.incrementChannelWriteFail();                hitChannelFullException = true;                backoffInterval = waitAndGetNewBackoffInterval(backoffInterval);                continue;            } catch (ChannelException ex) {                                sourceCounter.incrementChannelWriteFail();                hitChannelException = true;                backoffInterval = waitAndGetNewBackoffInterval(backoffInterval);                continue;            }            backoffInterval = 250;            sourceCounter.addToEventAcceptedCount(events.size());            sourceCounter.incrementAppendBatchAcceptedCount();        }    } catch (Throwable t) {                if (readingEvents) {            sourceCounter.incrementEventReadFail();        } else {            sourceCounter.incrementGenericProcessingFail();        }        hasFatalError = true;        Throwables.propagate(t);    }}
1
private int waitAndGetNewBackoffInterval(int backoffInterval) throws InterruptedException
{    if (backoff) {        TimeUnit.MILLISECONDS.sleep(backoffInterval);        backoffInterval = backoffInterval << 1;        backoffInterval = backoffInterval >= maxBackoff ? maxBackoff : backoffInterval;    }    return backoffInterval;}
0
public String getKeystore()
{    return keystore;}
0
public String getKeystorePassword()
{    return keystorePassword;}
0
public String getKeystoreType()
{    return keystoreType;}
0
public Set<String> getExcludeProtocols()
{    return excludeProtocols;}
0
public Set<String> getIncludeProtocols()
{    return includeProtocols;}
0
public Set<String> getExcludeCipherSuites()
{    return excludeCipherSuites;}
0
public Set<String> getIncludeCipherSuites()
{    return includeCipherSuites;}
0
public boolean isSslEnabled()
{    return sslEnabled;}
0
protected void configureSsl(Context context)
{    sslEnabled = context.getBoolean(SSL_ENABLED_KEY, SSL_ENABLED_DEFAULT_VALUE);    keystore = context.getString(KEYSTORE_KEY, SSLUtil.getGlobalKeystorePath());    keystorePassword = context.getString(KEYSTORE_PASSWORD_KEY, SSLUtil.getGlobalKeystorePassword());    keystoreType = context.getString(KEYSTORE_TYPE_KEY, SSLUtil.getGlobalKeystoreType(KEYSTORE_TYPE_DEFAULT_VALUE));    parseList(context.getString(EXCLUDE_PROTOCOLS, SSLUtil.getGlobalExcludeProtocols()), excludeProtocols);    parseList(context.getString(INCLUDE_PROTOCOLS, SSLUtil.getGlobalIncludeProtocols()), includeProtocols);    parseList(context.getString(EXCLUDE_CIPHER_SUITES, SSLUtil.getGlobalExcludeCipherSuites()), excludeCipherSuites);    parseList(context.getString(INCLUDE_CIPHER_SUITES, SSLUtil.getGlobalIncludeCipherSuites()), includeCipherSuites);    if (sslEnabled) {        Objects.requireNonNull(keystore, KEYSTORE_KEY + " must be specified when SSL is enabled");        Objects.requireNonNull(keystorePassword, KEYSTORE_PASSWORD_KEY + " must be specified when SSL is enabled");        try {            KeyStore ks = KeyStore.getInstance(keystoreType);            ks.load(new FileInputStream(keystore), keystorePassword.toCharArray());        } catch (Exception ex) {            throw new FlumeException("Source " + getName() + " configured with invalid keystore: " + keystore, ex);        }    }}
0
private Optional<SSLContext> getSslContext()
{    if (sslEnabled) {        try {            KeyStore ks = KeyStore.getInstance(keystoreType);            ks.load(new FileInputStream(keystore), keystorePassword.toCharArray());                        String algorithm = KeyManagerFactory.getDefaultAlgorithm();                        KeyManagerFactory kmf = KeyManagerFactory.getInstance(algorithm);            kmf.init(ks, keystorePassword.toCharArray());            SSLContext serverContext = SSLContext.getInstance("TLS");            serverContext.init(kmf.getKeyManagers(), null, null);            return Optional.of(serverContext);        } catch (Exception e) {            throw new Error("Failed to initialize the server-side SSLContext", e);        }    } else {        return Optional.empty();    }}
0
private Optional<SSLEngine> getSslEngine(boolean useClientMode)
{    return getSslContext().map(sslContext -> {        SSLEngine sslEngine = sslContext.createSSLEngine();        sslEngine.setUseClientMode(useClientMode);        sslEngine.setEnabledProtocols(getFilteredProtocols(sslEngine.getEnabledProtocols()));        sslEngine.setEnabledCipherSuites(getFilteredCipherSuites(sslEngine.getEnabledCipherSuites()));        return sslEngine;    });}
0
protected Supplier<Optional<SSLContext>> getSslContextSupplier()
{    return this::getSslContext;}
0
protected Supplier<Optional<SSLEngine>> getSslEngineSupplier(boolean useClientMode)
{    return () -> getSslEngine(useClientMode);}
0
protected String[] getFilteredProtocols(SSLParameters sslParameters)
{    return getFilteredProtocols(sslParameters.getProtocols());}
0
private String[] getFilteredProtocols(String[] enabledProtocols)
{    return Stream.of(enabledProtocols).filter(o -> includeProtocols.isEmpty() || includeProtocols.contains(o)).filter(o -> !excludeProtocols.contains(o)).toArray(String[]::new);}
0
protected String[] getFilteredCipherSuites(SSLParameters sslParameters)
{    return getFilteredCipherSuites(sslParameters.getCipherSuites());}
0
private String[] getFilteredCipherSuites(String[] enabledCipherSuites)
{    return Stream.of(enabledCipherSuites).filter(o -> includeCipherSuites.isEmpty() || includeCipherSuites.contains(o)).filter(o -> !excludeCipherSuites.contains(o)).toArray(String[]::new);}
0
private void parseList(String value, Set<String> set)
{    if (Objects.nonNull(value)) {        set.addAll(Arrays.asList(value.split(" ")));    }}
0
protected void doConfigure(Context context) throws FlumeException
{    /* Limit on the total number of events. */    maxTotalEvents = context.getLong("maxTotalEvents", -1L);    /* Limit on the total number of successful events. */    maxSuccessfulEvents = context.getLong("maxSuccessfulEvents", -1L);    /* Set max events in a batch submission */    batchSize = context.getInteger("batchSize", 1);    /* Size of events to be generated. */    int size = context.getInteger("size", 500);    int rateLimit = context.getInteger("maxEventsPerSecond", 0);    if (rateLimit > 0) {        limiter = RateLimiter.create(rateLimit);    } else {        limiter = null;    }    prepEventData(size);}
0
private void prepEventData(int bufferSize)
{    buffer = new byte[bufferSize];    Arrays.fill(buffer, Byte.MAX_VALUE);    if (batchSize > 1) {                eventBatchList = new ArrayList<Event>();        for (int i = 0; i < batchSize; i++) {            eventBatchList.add(EventBuilder.withBody(buffer));        }    } else {                event = EventBuilder.withBody(buffer);    }}
0
protected Status doProcess() throws EventDeliveryException
{    long totalEventSent = counterGroup.addAndGet("events.total", lastSent);    if ((maxTotalEvents >= 0 && totalEventSent >= maxTotalEvents) || (maxSuccessfulEvents >= 0 && counterGroup.get("events.successful") >= maxSuccessfulEvents)) {        return Status.BACKOFF;    }    try {        lastSent = batchSize;        if (batchSize == 1) {            if (limiter != null) {                limiter.acquire();            }            getChannelProcessor().processEvent(event);        } else {            long eventsLeft = maxTotalEvents - totalEventSent;            if (maxTotalEvents >= 0 && eventsLeft < batchSize) {                eventBatchListToProcess = eventBatchList.subList(0, (int) eventsLeft);            } else {                eventBatchListToProcess = eventBatchList;            }            lastSent = eventBatchListToProcess.size();            if (limiter != null) {                                limiter.acquire((int) lastSent);            }            getChannelProcessor().processEventBatch(eventBatchListToProcess);        }        counterGroup.addAndGet("events.successful", lastSent);    } catch (ChannelException ex) {        counterGroup.addAndGet("events.failed", lastSent);        return Status.BACKOFF;    }    return Status.READY;}
0
protected void doStart() throws FlumeException
{    }
1
protected void doStop() throws FlumeException
{    }
1
public long getBatchSize()
{    return batchSize;}
0
public Long load(String key) throws Exception
{    return timeParser.parseMillis(key);}
0
public Event parseMessage(String msg, Charset charset, Set<String> keepFields)
{    Map<String, String> headers = Maps.newHashMap();    int msgLen = msg.length();    int curPos = 0;    Preconditions.checkArgument(msg.charAt(curPos) == '<', "Bad format: invalid priority: cannot find open bracket '<' (%s)", msg);    int endBracketPos = msg.indexOf('>');    Preconditions.checkArgument(endBracketPos > 0 && endBracketPos <= 6, "Bad format: invalid priority: cannot find end bracket '>' (%s)", msg);    String priority = msg.substring(1, endBracketPos);    int pri = Integer.parseInt(priority);    int facility = pri / 8;    int severity = pri % 8;        headers.put(SyslogUtils.SYSLOG_PRIORITY, priority);        headers.put(SyslogUtils.SYSLOG_FACILITY, String.valueOf(facility));    headers.put(SyslogUtils.SYSLOG_SEVERITY, String.valueOf(severity));    Preconditions.checkArgument(msgLen > endBracketPos + 1, "Bad format: no data except priority (%s)", msg);        curPos = endBracketPos + 1;        String version = null;    if (msgLen > curPos + 2 && "1 ".equals(msg.substring(curPos, curPos + 2))) {        version = msg.substring(curPos, curPos + 1);        headers.put(SyslogUtils.SYSLOG_VERSION, version);        curPos += 2;    }        long ts;    String tsString;    char dateStartChar = msg.charAt(curPos);    try {                if (dateStartChar == '-') {            tsString = Character.toString(dateStartChar);            ts = System.currentTimeMillis();            if (msgLen <= curPos + 2) {                throw new IllegalArgumentException("bad syslog format (missing hostname)");            }                        curPos += 2;                } else if (dateStartChar >= 'A' && dateStartChar <= 'Z') {            if (msgLen <= curPos + RFC3164_LEN) {                throw new IllegalArgumentException("bad timestamp format");            }            tsString = msg.substring(curPos, curPos + RFC3164_LEN);            ts = parseRfc3164Time(tsString);            curPos += RFC3164_LEN + 1;                } else {            int nextSpace = msg.indexOf(' ', curPos);            if (nextSpace == -1) {                throw new IllegalArgumentException("bad timestamp format");            }            tsString = msg.substring(curPos, nextSpace);            ts = parseRfc5424Date(tsString);            curPos = nextSpace + 1;        }    } catch (IllegalArgumentException ex) {        throw new IllegalArgumentException("Unable to parse message: " + msg, ex);    }    headers.put("timestamp", String.valueOf(ts));        int nextSpace = msg.indexOf(' ', curPos);    if (nextSpace == -1) {        throw new IllegalArgumentException("bad syslog format (missing hostname)");    }            String hostname = new String(msg.substring(curPos, nextSpace));    headers.put("host", hostname);        String data = "";    if (msgLen > nextSpace + 1 && !SyslogUtils.keepAllFields(keepFields)) {        curPos = nextSpace + 1;        data = msg.substring(curPos);        data = SyslogUtils.addFieldsToBody(keepFields, data, priority, version, tsString, hostname);    } else {        data = msg;    }    Event event = EventBuilder.withBody(data, charset, headers);    return event;}
0
protected long parseRfc5424Date(String msg)
{    Long ts = null;    int curPos = 0;    int msgLen = msg.length();    Preconditions.checkArgument(msgLen > RFC5424_PREFIX_LEN, "Bad format: Not a valid RFC5424 timestamp: %s", msg);    String timestampPrefix = msg.substring(curPos, RFC5424_PREFIX_LEN);    try {        ts = timestampCache.get(timestampPrefix);    } catch (ExecutionException ex) {        throw new IllegalArgumentException("bad timestamp format", ex);    }    curPos += RFC5424_PREFIX_LEN;    Preconditions.checkArgument(ts != null, "Parsing error: timestamp is null");        if (msg.charAt(curPos) == '.') {                boolean foundEnd = false;        int endMillisPos = curPos + 1;        if (msgLen <= endMillisPos) {            throw new IllegalArgumentException("bad timestamp format (no TZ)");        }                while (!foundEnd) {            char curDigit = msg.charAt(endMillisPos);            if (curDigit >= '0' && curDigit <= '9') {                endMillisPos++;            } else {                foundEnd = true;            }        }                final int fractionalPositions = endMillisPos - (curPos + 1);        if (fractionalPositions > 0) {            long milliseconds = Long.parseLong(msg.substring(curPos + 1, endMillisPos));            if (fractionalPositions > 3) {                milliseconds /= Math.pow(10, (fractionalPositions - 3));            } else if (fractionalPositions < 3) {                milliseconds *= Math.pow(10, (3 - fractionalPositions));            }            ts += milliseconds;        } else {            throw new IllegalArgumentException("Bad format: Invalid timestamp (fractional portion): " + msg);        }        curPos = endMillisPos;    }        char tzFirst = msg.charAt(curPos);        if (tzFirst == 'Z') {        } else if (tzFirst == '+' || tzFirst == '-') {        Preconditions.checkArgument(msgLen > curPos + 5, "Bad format: Invalid timezone (%s)", msg);        int polarity;        if (tzFirst == '+') {            polarity = +1;        } else {            polarity = -1;        }        char[] h = new char[5];        for (int i = 0; i < 5; i++) {            h[i] = msg.charAt(curPos + 1 + i);        }        if (h[0] >= '0' && h[0] <= '9' && h[1] >= '0' && h[1] <= '9' && h[2] == ':' && h[3] >= '0' && h[3] <= '9' && h[4] >= '0' && h[4] <= '9') {            int hourOffset = Integer.parseInt(msg.substring(curPos + 1, curPos + 3));            int minOffset = Integer.parseInt(msg.substring(curPos + 4, curPos + 6));            ts -= polarity * ((hourOffset * 60) + minOffset) * 60000;        } else {            throw new IllegalArgumentException("Bad format: Invalid timezone: " + msg);        }    }    return ts;}
0
protected long parseRfc3164Time(String ts)
{    DateTime now = DateTime.now();    int year = now.getYear();    ts = TWO_SPACES.matcher(ts).replaceFirst(" ");    DateTime date;    try {        date = rfc3164Format.parseDateTime(ts);    } catch (IllegalArgumentException e) {                return 0;    }    if (date != null) {        DateTime fixed = date.withYear(year);                if (fixed.isAfter(now) && fixed.minusMonths(1).isAfter(now)) {            fixed = date.minusYears(1);                } else if (fixed.isBefore(now) && fixed.plusMonths(1).isBefore(now)) {            fixed = date.plusYears(1);        }        date = fixed;    }    if (date == null) {        return 0;    }    return date.getMillis();}
1
public void setEventSize(int eventSize)
{    syslogUtils.setEventSize(eventSize);}
0
public void setKeepFields(Set<String> keepFields)
{    syslogUtils.setKeepFields(keepFields);}
0
public void setFormater(Map<String, String> prop)
{    syslogUtils.addFormats(prop);}
0
public void setClientIPHeader(String clientIPHeader)
{    this.clientIPHeader = clientIPHeader;}
0
public void setClientHostnameHeader(String clientHostnameHeader)
{    this.clientHostnameHeader = clientHostnameHeader;}
0
public void messageReceived(ChannelHandlerContext ctx, MessageEvent mEvent)
{    ChannelBuffer buff = (ChannelBuffer) mEvent.getMessage();    while (buff.readable()) {        Event e = syslogUtils.extractEvent(buff);        if (e == null) {                        continue;        }        if (clientIPHeader != null) {            e.getHeaders().put(clientIPHeader, SyslogUtils.getIP(ctx.getChannel().getRemoteAddress()));        }        if (clientHostnameHeader != null) {            e.getHeaders().put(clientHostnameHeader, SyslogUtils.getHostname(ctx.getChannel().getRemoteAddress()));        }        sourceCounter.incrementEventReceivedCount();        try {            getChannelProcessor().processEvent(e);            sourceCounter.incrementEventAcceptedCount();        } catch (ChannelException ex) {                        sourceCounter.incrementChannelWriteFail();        } catch (RuntimeException ex) {                        sourceCounter.incrementEventReadFail();            return;        }    }}
1
public void start()
{    ChannelFactory factory = new NioServerSocketChannelFactory(Executors.newCachedThreadPool(), Executors.newCachedThreadPool());    ServerBootstrap serverBootstrap = new ServerBootstrap(factory);    serverBootstrap.setPipelineFactory(new PipelineFactory(eventSize, formaterProp, keepFields, clientIPHeader, clientHostnameHeader, getSslEngineSupplier(false)));        if (host == null) {        nettyChannel = serverBootstrap.bind(new InetSocketAddress(port));    } else {        nettyChannel = serverBootstrap.bind(new InetSocketAddress(host, port));    }    sourceCounter.start();    super.start();}
1
public void stop()
{            if (nettyChannel != null) {        nettyChannel.close();        try {            nettyChannel.getCloseFuture().await(60, TimeUnit.SECONDS);        } catch (InterruptedException e) {                    } finally {            nettyChannel = null;        }    }    sourceCounter.stop();    super.stop();}
1
public void configure(Context context)
{    configureSsl(context);    Configurables.ensureRequiredNonNull(context, SyslogSourceConfigurationConstants.CONFIG_PORT);    port = context.getInteger(SyslogSourceConfigurationConstants.CONFIG_PORT);    host = context.getString(SyslogSourceConfigurationConstants.CONFIG_HOST);    eventSize = context.getInteger("eventSize", SyslogUtils.DEFAULT_SIZE);    formaterProp = context.getSubProperties(SyslogSourceConfigurationConstants.CONFIG_FORMAT_PREFIX);    keepFields = SyslogUtils.chooseFieldsToKeep(context.getString(SyslogSourceConfigurationConstants.CONFIG_KEEP_FIELDS, SyslogSourceConfigurationConstants.DEFAULT_KEEP_FIELDS));    clientIPHeader = context.getString(SyslogSourceConfigurationConstants.CONFIG_CLIENT_IP_HEADER);    clientHostnameHeader = context.getString(SyslogSourceConfigurationConstants.CONFIG_CLIENT_HOSTNAME_HEADER);    if (sourceCounter == null) {        sourceCounter = new SourceCounter(getName());    }}
0
 InetSocketAddress getBoundAddress()
{    SocketAddress localAddress = nettyChannel.getLocalAddress();    if (!(localAddress instanceof InetSocketAddress)) {        throw new IllegalArgumentException("Not bound to an internet address");    }    return (InetSocketAddress) localAddress;}
0
 SourceCounter getSourceCounter()
{    return sourceCounter;}
0
public ChannelPipeline getPipeline()
{    syslogTcpHandler handler = new syslogTcpHandler();    handler.setEventSize(eventSize);    handler.setFormater(formaterProp);    handler.setKeepFields(keepFields);    handler.setClientIPHeader(clientIPHeader);    handler.setClientHostnameHeader(clientHostnameHeader);    ChannelPipeline pipeline = Channels.pipeline(handler);    sslEngineSupplier.get().ifPresent(sslEngine -> {        pipeline.addFirst("ssl", new SslHandler(sslEngine));    });    return pipeline;}
0
public void setFormater(Map<String, String> prop)
{    syslogUtils.addFormats(prop);}
0
public void setKeepFields(Set<String> keepFields)
{    syslogUtils.setKeepFields(keepFields);}
0
public void setClientIPHeader(String clientIPHeader)
{    this.clientIPHeader = clientIPHeader;}
0
public void setClientHostnameHeader(String clientHostnameHeader)
{    this.clientHostnameHeader = clientHostnameHeader;}
0
public void messageReceived(ChannelHandlerContext ctx, MessageEvent mEvent)
{    try {        syslogUtils.setEventSize(maxsize);        Event e = syslogUtils.extractEvent((ChannelBuffer) mEvent.getMessage());        if (e == null) {            return;        }        if (clientIPHeader != null) {            e.getHeaders().put(clientIPHeader, SyslogUtils.getIP(mEvent.getRemoteAddress()));        }        if (clientHostnameHeader != null) {            e.getHeaders().put(clientHostnameHeader, SyslogUtils.getHostname(mEvent.getRemoteAddress()));        }        sourceCounter.incrementEventReceivedCount();        getChannelProcessor().processEvent(e);        sourceCounter.incrementEventAcceptedCount();    } catch (ChannelException ex) {                sourceCounter.incrementChannelWriteFail();        return;    } catch (RuntimeException ex) {                sourceCounter.incrementEventReadFail();        return;    }}
1
public void start()
{        ConnectionlessBootstrap serverBootstrap = new ConnectionlessBootstrap(new OioDatagramChannelFactory(Executors.newCachedThreadPool()));    final syslogHandler handler = new syslogHandler();    handler.setFormater(formaterProp);    handler.setKeepFields(keepFields);    handler.setClientIPHeader(clientIPHeader);    handler.setClientHostnameHeader(clientHostnameHeader);    serverBootstrap.setOption("receiveBufferSizePredictorFactory", new AdaptiveReceiveBufferSizePredictorFactory(DEFAULT_MIN_SIZE, DEFAULT_INITIAL_SIZE, maxsize));    serverBootstrap.setPipelineFactory(new ChannelPipelineFactory() {        @Override        public ChannelPipeline getPipeline() {            return Channels.pipeline(handler);        }    });    if (host == null) {        nettyChannel = serverBootstrap.bind(new InetSocketAddress(port));    } else {        nettyChannel = serverBootstrap.bind(new InetSocketAddress(host, port));    }    sourceCounter.start();    super.start();}
0
public ChannelPipeline getPipeline()
{    return Channels.pipeline(handler);}
0
public void stop()
{            if (nettyChannel != null) {        nettyChannel.close();        try {            nettyChannel.getCloseFuture().await(60, TimeUnit.SECONDS);        } catch (InterruptedException e) {                    } finally {            nettyChannel = null;        }    }    sourceCounter.stop();    super.stop();}
1
public void configure(Context context)
{    Configurables.ensureRequiredNonNull(context, SyslogSourceConfigurationConstants.CONFIG_PORT);    port = context.getInteger(SyslogSourceConfigurationConstants.CONFIG_PORT);    host = context.getString(SyslogSourceConfigurationConstants.CONFIG_HOST);    formaterProp = context.getSubProperties(SyslogSourceConfigurationConstants.CONFIG_FORMAT_PREFIX);    keepFields = SyslogUtils.chooseFieldsToKeep(context.getString(SyslogSourceConfigurationConstants.CONFIG_KEEP_FIELDS, SyslogSourceConfigurationConstants.DEFAULT_KEEP_FIELDS));    clientIPHeader = context.getString(SyslogSourceConfigurationConstants.CONFIG_CLIENT_IP_HEADER);    clientHostnameHeader = context.getString(SyslogSourceConfigurationConstants.CONFIG_CLIENT_HOSTNAME_HEADER);    if (sourceCounter == null) {        sourceCounter = new SourceCounter(getName());    }}
0
 InetSocketAddress getBoundAddress()
{    SocketAddress localAddress = nettyChannel.getLocalAddress();    if (!(localAddress instanceof InetSocketAddress)) {        throw new IllegalArgumentException("Not bound to an internet address");    }    return (InetSocketAddress) localAddress;}
0
 SourceCounter getSourceCounter()
{    return sourceCounter;}
0
public static boolean keepAllFields(Set<String> keepFields)
{    if (keepFields == null) {        return false;    }    return keepFields.contains(KEEP_FIELDS_ALL);}
0
public static Set<String> chooseFieldsToKeep(String keepFields)
{    if (keepFields == null) {        return null;    }    keepFields = keepFields.trim().toLowerCase(Locale.ENGLISH);    if (keepFields.equals("false") || keepFields.equals("none")) {        return null;    }    if (keepFields.equals("true") || keepFields.equals("all")) {        Set<String> fieldsToKeep = new HashSet<String>(1);        fieldsToKeep.add(KEEP_FIELDS_ALL);        return fieldsToKeep;    }    Set<String> fieldsToKeep = new HashSet<String>(DEFAULT_FIELDS_TO_KEEP.length);    for (String field : DEFAULT_FIELDS_TO_KEEP) {        if (keepFields.indexOf(field) != -1) {            fieldsToKeep.add(field);        }    }    return fieldsToKeep;}
0
public static String addFieldsToBody(Set<String> keepFields, String body, String priority, String version, String timestamp, String hostname)
{        if (keepFields != null) {        if (keepFields.contains(SyslogSourceConfigurationConstants.CONFIG_KEEP_FIELDS_HOSTNAME)) {            body = hostname + " " + body;        }        if (keepFields.contains(SyslogSourceConfigurationConstants.CONFIG_KEEP_FIELDS_TIMESTAMP)) {            body = timestamp + " " + body;        }        if (keepFields.contains(SyslogSourceConfigurationConstants.CONFIG_KEEP_FIELDS_VERSION)) {            if (version != null && !version.isEmpty()) {                body = version + " " + body;            }        }        if (keepFields.contains(SyslogSourceConfigurationConstants.CONFIG_KEEP_FIELDS_PRIORITY)) {            body = "<" + priority + ">" + body;        }    }    return body;}
0
public static String getIP(SocketAddress socketAddress)
{    try {        InetSocketAddress inetSocketAddress = (InetSocketAddress) socketAddress;        String ip = inetSocketAddress.getAddress().getHostAddress();        if (ip != null) {            return ip;        } else {            throw new NullPointerException("The returned IP is null");        }    } catch (Exception e) {            }        return "";}
1
public static String getHostname(SocketAddress socketAddress)
{    try {        InetSocketAddress inetSocketAddress = (InetSocketAddress) socketAddress;        String hostname = inetSocketAddress.getHostName();        if (hostname != null) {            return hostname;        } else {            throw new NullPointerException("The returned hostname is null");        }    } catch (Exception e) {            }        return "";}
1
public void addFormats(Map<String, String> formatProp)
{    if (formatProp.isEmpty() || !formatProp.containsKey(SyslogSourceConfigurationConstants.CONFIG_REGEX)) {        return;    }    SyslogFormatter fmt1 = new SyslogFormatter();    fmt1.regexPattern = Pattern.compile(formatProp.get(SyslogSourceConfigurationConstants.CONFIG_REGEX));    if (formatProp.containsKey(SyslogSourceConfigurationConstants.CONFIG_SEARCH)) {        fmt1.searchPattern.add(formatProp.get(SyslogSourceConfigurationConstants.CONFIG_SEARCH));    }    if (formatProp.containsKey(SyslogSourceConfigurationConstants.CONFIG_REPLACE)) {        fmt1.replacePattern.add(formatProp.get(SyslogSourceConfigurationConstants.CONFIG_REPLACE));    }    if (formatProp.containsKey(SyslogSourceConfigurationConstants.CONFIG_DATEFORMAT)) {        fmt1.dateFormat.add(new SimpleDateFormat(formatProp.get(SyslogSourceConfigurationConstants.CONFIG_DATEFORMAT)));    }    formats.add(0, fmt1);}
0
private void initHeaderFormats()
{        SyslogFormatter fmt1 = new SyslogFormatter();    fmt1.regexPattern = Pattern.compile(SYSLOG_MSG_RFC5424_0);        fmt1.searchPattern.add("Z");    fmt1.replacePattern.add("+0000");        fmt1.searchPattern.add("([+-])(\\d{2})[:](\\d{2})");    fmt1.replacePattern.add("$1$2$3");        fmt1.searchPattern.add("(T\\d{2}:\\d{2}:\\d{2}\\.\\d{3})(\\d*)");    fmt1.replacePattern.add("$1");    fmt1.dateFormat.add(new SimpleDateFormat(SYSLOG_TIMESTAMP_FORMAT_RFC5424_1, Locale.ENGLISH));    fmt1.dateFormat.add(new SimpleDateFormat(SYSLOG_TIMESTAMP_FORMAT_RFC5424_2, Locale.ENGLISH));    fmt1.dateFormat.add(new SimpleDateFormat(SYSLOG_TIMESTAMP_FORMAT_RFC5424_3, Locale.ENGLISH));    fmt1.dateFormat.add(new SimpleDateFormat(SYSLOG_TIMESTAMP_FORMAT_RFC5424_4, Locale.ENGLISH));    fmt1.addYear = false;        SyslogFormatter fmt2 = new SyslogFormatter();    fmt2.regexPattern = Pattern.compile(SYSLOG_MSG_RFC3164_0);        fmt2.searchPattern.add("  ");    fmt2.replacePattern.add(" ");    fmt2.dateFormat.add(new SimpleDateFormat(SYSLOG_TIMESTAMP_FORMAT_RFC3164_1, Locale.ENGLISH));    fmt2.addYear = true;    formats.add(fmt1);    formats.add(fmt2);}
0
public String getSyslogStatus()
{    return this.syslogStatus;}
0
 Event buildEvent()
{    try {        byte[] body;        int pri = 0;        int sev = 0;        int facility = 0;        if (!isBadEvent) {            pri = Integer.parseInt(prio.toString());            sev = pri % 8;            facility = pri / 8;            formatHeaders();        }        Map<String, String> headers = new HashMap<String, String>();        headers.put(SYSLOG_FACILITY, String.valueOf(facility));        headers.put(SYSLOG_SEVERITY, String.valueOf(sev));        if ((priority != null) && (priority.length() > 0)) {            headers.put("priority", priority);        }        if ((version != null) && (version.length() > 0)) {            headers.put("version", version);        }        if ((timeStamp != null) && timeStamp.length() > 0) {            headers.put("timestamp", timeStamp);        }        if ((hostName != null) && (hostName.length() > 0)) {            headers.put("host", hostName);        }        if (isBadEvent) {                        headers.put(EVENT_STATUS, SyslogStatus.INVALID.getSyslogStatus());        } else if (isIncompleteEvent) {                        headers.put(EVENT_STATUS, SyslogStatus.INCOMPLETE.getSyslogStatus());        }        if (!keepAllFields(keepFields)) {            if ((msgBody != null) && (msgBody.length() > 0)) {                body = msgBody.getBytes();            } else {                                body = baos.toByteArray();            }        } else {            body = baos.toByteArray();        }                return EventBuilder.withBody(body, headers);    } finally {        reset();    }}
1
private void formatHeaders()
{    String eventStr = baos.toString();    String timeStampString = null;    for (int p = 0; p < formats.size(); p++) {        SyslogFormatter fmt = formats.get(p);        Pattern pattern = fmt.regexPattern;        Matcher matcher = pattern.matcher(eventStr);        if (!matcher.matches()) {            continue;        }        MatchResult res = matcher.toMatchResult();        for (int grp = 1; grp <= res.groupCount(); grp++) {            String value = res.group(grp);            if (grp == SYSLOG_TIMESTAMP_POS) {                timeStampString = value;                                if (value != null) {                    for (int sp = 0; sp < fmt.searchPattern.size(); sp++) {                        value = value.replaceAll(fmt.searchPattern.get(sp), fmt.replacePattern.get(sp));                    }                                        if (fmt.addYear) {                        value = clock.instant().atOffset(ZoneOffset.UTC).get(ChronoField.YEAR) + value;                    }                                        for (int dt = 0; dt < fmt.dateFormat.size(); dt++) {                        try {                            Date parsedDate = fmt.dateFormat.get(dt).parse(value);                            /*                 * Some code to try and add some smarts to the year insertion.                 * Original code just added the current year which was okay-ish, but around                 * January 1st becomes pretty nave.                 * The current year is added above. This code, if the year has been added does                 * the following:                 * 1. Compute what the computed time, but one month in the past would be.                 * 2. Compute what the computed time, but eleven months in the future would be.                 * If the computed time is more than one month in the future then roll it back a                 * year. If the computed time is more than eleven months in the past then roll it                 * forward a year. This gives us a 12 month rolling window (11 months in the past,                 * 1 month in the future) of timestamps.                 */                            if (fmt.addYear) {                                Calendar calParsed = Calendar.getInstance();                                calParsed.setTime(parsedDate);                                Calendar calMinusOneMonth = Calendar.getInstance();                                calMinusOneMonth.setTime(parsedDate);                                calMinusOneMonth.add(Calendar.MONTH, -1);                                Calendar calPlusElevenMonths = Calendar.getInstance();                                calPlusElevenMonths.setTime(parsedDate);                                calPlusElevenMonths.add(Calendar.MONTH, +11);                                long currentTimeMillis = clock.millis();                                if (calParsed.getTimeInMillis() > currentTimeMillis && calMinusOneMonth.getTimeInMillis() > currentTimeMillis) {                                                                        Calendar c1 = Calendar.getInstance();                                    c1.setTime(parsedDate);                                    c1.add(Calendar.YEAR, -1);                                    parsedDate = c1.getTime();                                } else if (calParsed.getTimeInMillis() < currentTimeMillis && calPlusElevenMonths.getTimeInMillis() < currentTimeMillis) {                                                                        Calendar c1 = Calendar.getInstance();                                    c1.setTime(parsedDate);                                    c1.add(Calendar.YEAR, +1);                                    parsedDate = c1.getTime();                                }                            }                            timeStamp = String.valueOf(parsedDate.getTime());                                                        break;                        } catch (ParseException e) {                                                        continue;                        }                    }                }            } else if (grp == SYSLOG_HOSTNAME_POS) {                hostName = value;            } else if (grp == SYSLOG_PRIORITY_POS) {                priority = value;            } else if (grp == SYSLOG_VERSION_POS) {                version = value;            } else if (grp == SYSLOG_BODY_POS) {                msgBody = addFieldsToBody(keepFields, value, priority, version, timeStampString, hostName);            }        }                break;    }}
0
private void reset()
{    baos.reset();    m = Mode.START;    prio.delete(0, prio.length());    isBadEvent = false;    isIncompleteEvent = false;    hostName = null;    timeStamp = null;    msgBody = null;}
0
public Event extractEvent(ChannelBuffer in)
{    /* for protocol debugging    ByteBuffer bb = in.toByteBuffer();    int remaining = bb.remaining();    byte[] buf = new byte[remaining];    bb.get(buf);    HexDump.dump(buf, 0, System.out, 0);    */    byte b = 0;    Event e = null;    boolean doneReading = false;    try {        while (!doneReading && in.readable()) {            b = in.readByte();            switch(m) {                case START:                    if (b == '<') {                        baos.write(b);                        m = Mode.PRIO;                    } else if (b == '\n') {                                                                                                                                                                                            } else {                        isBadEvent = true;                        baos.write(b);                                                m = Mode.DATA;                    }                    break;                case PRIO:                    baos.write(b);                    if (b == '>') {                        if (prio.length() == 0) {                            isBadEvent = true;                        }                        m = Mode.DATA;                    } else {                        char ch = (char) b;                        prio.append(ch);                                                if (!Character.isDigit(ch) || prio.length() > 3) {                            isBadEvent = true;                                                        m = Mode.DATA;                        }                    }                    break;                case DATA:                                        if (b == '\n') {                        e = buildEvent();                        doneReading = true;                    } else {                        baos.write(b);                    }                    if (baos.size() == this.maxSize && !doneReading) {                        isIncompleteEvent = true;                        e = buildEvent();                        doneReading = true;                    }                    break;            }        }                if (e == null && isUdp) {            doneReading = true;            e = buildEvent();        }    } finally {        }    return e;}
1
public Integer getEventSize()
{    return maxSize;}
0
public void setEventSize(Integer eventSize)
{    this.maxSize = eventSize;}
0
public void setKeepFields(Set<String> keepFields)
{    this.keepFields = keepFields;}
0
public void configure(Context context)
{    configureSsl(context);        port = context.getInteger(CONFIG_PORT);    Preconditions.checkNotNull(port, "Port must be specified for Thrift " + "Source.");    bindAddress = context.getString(CONFIG_BIND);    Preconditions.checkNotNull(bindAddress, "Bind address must be specified " + "for Thrift Source.");    try {        maxThreads = context.getInteger(CONFIG_THREADS, 0);        maxThreads = (maxThreads <= 0) ? Integer.MAX_VALUE : maxThreads;    } catch (NumberFormatException e) {            }    if (sourceCounter == null) {        sourceCounter = new SourceCounter(getName());    }    protocol = context.getString(CONFIG_PROTOCOL);    if (protocol == null) {                protocol = COMPACT_PROTOCOL;    }    Preconditions.checkArgument((protocol.equalsIgnoreCase(BINARY_PROTOCOL) || protocol.equalsIgnoreCase(COMPACT_PROTOCOL)), "binary or compact are the only valid Thrift protocol types to " + "choose from.");    principal = context.getString(AGENT_PRINCIPAL);    String keytab = context.getString(AGENT_KEYTAB);    enableKerberos = context.getBoolean(KERBEROS_KEY, false);    this.flumeAuth = FlumeAuthenticationUtil.getAuthenticator(principal, keytab);    if (enableKerberos) {        if (!flumeAuth.isAuthenticated()) {            throw new FlumeException("Authentication failed in Kerberos mode for " + "principal " + principal + " keytab " + keytab);        }        flumeAuth.startCredentialRefresher();    }}
1
public void start()
{            server = getTThreadedSelectorServer();        if (server == null) {        server = getTThreadPoolServer();    }    servingExecutor = Executors.newSingleThreadExecutor(new ThreadFactoryBuilder().setNameFormat("Flume Thrift Source I/O Boss").build());    /**     * Start serving.     */    servingExecutor.submit(new Runnable() {        @Override        public void run() {            flumeAuth.execute(new PrivilegedAction<Object>() {                @Override                public Object run() {                    server.serve();                    return null;                }            });        }    });    long timeAfterStart = System.currentTimeMillis();    while (!server.isServing()) {        try {            if (System.currentTimeMillis() - timeAfterStart >= 10000) {                throw new FlumeException("Thrift server failed to start!");            }            TimeUnit.MILLISECONDS.sleep(1000);        } catch (InterruptedException e) {            Thread.currentThread().interrupt();            throw new FlumeException("Interrupted while waiting for Thrift server" + " to start.", e);        }    }    sourceCounter.start();        super.start();}
1
public void run()
{    flumeAuth.execute(new PrivilegedAction<Object>() {        @Override        public Object run() {            server.serve();            return null;        }    });}
0
public Object run()
{    server.serve();    return null;}
0
private TServerTransport getSSLServerTransport()
{    try {        TServerTransport transport;        TSSLTransportFactory.TSSLTransportParameters params = new TSSLTransportFactory.TSSLTransportParameters();        params.setKeyStore(getKeystore(), getKeystorePassword(), KeyManagerFactory.getDefaultAlgorithm(), getKeystoreType());        transport = TSSLTransportFactory.getServerSocket(port, 120000, InetAddress.getByName(bindAddress), params);        ServerSocket serverSock = ((TServerSocket) transport).getServerSocket();        if (serverSock instanceof SSLServerSocket) {            SSLServerSocket sslServerSock = (SSLServerSocket) serverSock;            SSLParameters sslParameters = sslServerSock.getSSLParameters();            sslServerSock.setEnabledCipherSuites(getFilteredCipherSuites(sslParameters));            sslServerSock.setEnabledProtocols(getFilteredProtocols(sslParameters));        }        return transport;    } catch (Throwable throwable) {        throw new FlumeException("Cannot start Thrift source.", throwable);    }}
0
private TServerTransport getTServerTransport()
{    try {        return new TServerSocket(new InetSocketAddress(bindAddress, port));    } catch (Throwable throwable) {        throw new FlumeException("Cannot start Thrift source.", throwable);    }}
0
private TProtocolFactory getProtocolFactory()
{    if (protocol.equals(BINARY_PROTOCOL)) {                return new TBinaryProtocol.Factory();    } else {                return new TCompactProtocol.Factory();    }}
1
private TServer getTThreadedSelectorServer()
{    if (isSslEnabled() || enableKerberos) {        return null;    }    Class<?> serverClass;    Class<?> argsClass;    TServer.AbstractServerArgs args;    try {        serverClass = Class.forName("org.apache.thrift" + ".server.TThreadedSelectorServer");        argsClass = Class.forName("org.apache.thrift" + ".server.TThreadedSelectorServer$Args");        TServerTransport serverTransport = new TNonblockingServerSocket(new InetSocketAddress(bindAddress, port));        ExecutorService sourceService;        ThreadFactory threadFactory = new ThreadFactoryBuilder().setNameFormat("Flume Thrift IPC Thread %d").build();        if (maxThreads == 0) {            sourceService = Executors.newCachedThreadPool(threadFactory);        } else {            sourceService = Executors.newFixedThreadPool(maxThreads, threadFactory);        }        args = (TNonblockingServer.AbstractNonblockingServerArgs) argsClass.getConstructor(TNonblockingServerTransport.class).newInstance(serverTransport);        Method m = argsClass.getDeclaredMethod("executorService", ExecutorService.class);        m.invoke(args, sourceService);        populateServerParams(args);        /*       * Both THsHaServer and TThreadedSelectorServer allows us to pass in       * the executor service to use - unfortunately the "executorService"       * method does not exist in the parent abstract Args class,       * so use reflection to pass the executor in.       *       */        server = (TServer) serverClass.getConstructor(argsClass).newInstance(args);    } catch (ClassNotFoundException e) {        return null;    } catch (Throwable ex) {        throw new FlumeException("Cannot start Thrift Source.", ex);    }    return server;}
0
private TServer getTThreadPoolServer()
{    TServerTransport serverTransport;    if (isSslEnabled()) {        serverTransport = getSSLServerTransport();    } else {        serverTransport = getTServerTransport();    }    TThreadPoolServer.Args serverArgs = new TThreadPoolServer.Args(serverTransport);    serverArgs.maxWorkerThreads(maxThreads);    populateServerParams(serverArgs);    return new TThreadPoolServer(serverArgs);}
0
private void populateServerParams(TServer.AbstractServerArgs args)
{        args.protocolFactory(getProtocolFactory());        if (enableKerberos) {        args.transportFactory(getSASLTransportFactory());    } else {        args.transportFactory(new TFastFramedTransport.Factory());    }        args.processor(new ThriftSourceProtocol.Processor<ThriftSourceHandler>(new ThriftSourceHandler()));}
0
private TTransportFactory getSASLTransportFactory()
{    String[] names;    try {        names = FlumeAuthenticationUtil.splitKerberosName(principal);    } catch (IOException e) {        throw new FlumeException("Error while trying to resolve Principal name - " + principal, e);    }    Map<String, String> saslProperties = new HashMap<String, String>();    saslProperties.put(Sasl.QOP, "auth");    TSaslServerTransport.Factory saslTransportFactory = new TSaslServerTransport.Factory();    saslTransportFactory.addServerDefinition("GSSAPI", names[0], names[1], saslProperties, FlumeAuthenticationUtil.getSaslGssCallbackHandler());    return saslTransportFactory;}
0
public void stop()
{    if (server != null && server.isServing()) {        server.stop();    }    if (servingExecutor != null) {        servingExecutor.shutdown();        try {            if (!servingExecutor.awaitTermination(5, TimeUnit.SECONDS)) {                servingExecutor.shutdownNow();            }        } catch (InterruptedException e) {            throw new FlumeException("Interrupted while waiting for server to be " + "shutdown.");        }    }    sourceCounter.stop();    super.stop();}
0
public Status append(ThriftFlumeEvent event) throws TException
{    Event flumeEvent = EventBuilder.withBody(event.getBody(), event.getHeaders());    sourceCounter.incrementAppendReceivedCount();    sourceCounter.incrementEventReceivedCount();    try {        getChannelProcessor().processEvent(flumeEvent);    } catch (ChannelException ex) {                sourceCounter.incrementChannelWriteFail();        return Status.FAILED;    }    sourceCounter.incrementAppendAcceptedCount();    sourceCounter.incrementEventAcceptedCount();    return Status.OK;}
1
public Status appendBatch(List<ThriftFlumeEvent> events) throws TException
{    sourceCounter.incrementAppendBatchReceivedCount();    sourceCounter.addToEventReceivedCount(events.size());    List<Event> flumeEvents = Lists.newArrayList();    for (ThriftFlumeEvent event : events) {        flumeEvents.add(EventBuilder.withBody(event.getBody(), event.getHeaders()));    }    try {        getChannelProcessor().processEventBatch(flumeEvents);    } catch (ChannelException ex) {                sourceCounter.incrementChannelWriteFail();        return Status.FAILED;    }    sourceCounter.incrementAppendBatchAcceptedCount();    sourceCounter.addToEventAcceptedCount(events.size());    return Status.OK;}
1
public static SourceRunner forSource(Source source)
{    SourceRunner runner = null;    if (source instanceof PollableSource) {        runner = new PollableSourceRunner();        ((PollableSourceRunner) runner).setSource((PollableSource) source);    } else if (source instanceof EventDrivenSource) {        runner = new EventDrivenSourceRunner();        ((EventDrivenSourceRunner) runner).setSource((EventDrivenSource) source);    } else {        throw new IllegalArgumentException("No known runner type for source " + source);    }    return runner;}
0
public Source getSource()
{    return source;}
0
public void setSource(Source source)
{    this.source = source;}
0
public long currentTimeMillis()
{    return System.currentTimeMillis();}
0
public static ByteBuffer allocate(int size)
{    Preconditions.checkArgument(size > 0, "Size must be greater than zero");    long maxDirectMemory = getDirectMemorySize();    long allocatedCurrently = allocated.get();        try {        ByteBuffer result = ByteBuffer.allocateDirect(size);        allocated.addAndGet(size);        return result;    } catch (OutOfMemoryError error) {                throw error;    }}
1
public static void clean(ByteBuffer buffer) throws Exception
{    Preconditions.checkArgument(buffer.isDirect(), "buffer isn't direct!");    Method cleanerMethod = buffer.getClass().getMethod("cleaner");    cleanerMethod.setAccessible(true);    Object cleaner = cleanerMethod.invoke(buffer);    Method cleanMethod = cleaner.getClass().getMethod("clean");    cleanMethod.setAccessible(true);    cleanMethod.invoke(cleaner);    allocated.getAndAdd(-buffer.capacity());    long maxDirectMemory = getDirectMemorySize();    }
1
public static long getDirectMemorySize()
{    RuntimeMXBean RuntimemxBean = ManagementFactory.getRuntimeMXBean();    List<String> arguments = Lists.reverse(RuntimemxBean.getInputArguments());        long multiplier = 1;    for (String s : arguments) {        if (s.contains(MAX_DIRECT_MEMORY_PARAM)) {            String memSize = s.toLowerCase(Locale.ENGLISH).replace(MAX_DIRECT_MEMORY_PARAM.toLowerCase(Locale.ENGLISH), "").trim();            if (memSize.contains("k")) {                multiplier = 1024;            } else if (memSize.contains("m")) {                multiplier = 1048576;            } else if (memSize.contains("g")) {                multiplier = 1073741824;            }            memSize = memSize.replaceAll("[^\\d]", "");            long retValue = Long.parseLong(memSize);            return retValue * multiplier;        }    }    return DEFAULT_SIZE;}
0
private static long getDefaultDirectMemorySize()
{    try {        Class<?> VM = Class.forName("sun.misc.VM");        Method maxDirectMemory = VM.getDeclaredMethod("maxDirectMemory", (Class<?>) null);        Object result = maxDirectMemory.invoke(null, (Object[]) null);        if (result != null && result instanceof Long) {            return (Long) result;        }    } catch (Exception e) {            }        return Runtime.getRuntime().maxMemory();}
1
public static void setConfigurationFields(Object configurable, Map<String, String> properties) throws ConfigurationException
{    Class<?> clazz = configurable.getClass();    for (Method method : clazz.getMethods()) {        String methodName = method.getName();        if (methodName.startsWith("set") && method.getParameterTypes().length == 1) {            String fieldName = methodName.substring(3);            String value = properties.get(StringUtils.uncapitalize(fieldName));            if (value != null) {                Class<?> fieldType = method.getParameterTypes()[0];                ;                try {                    if (fieldType.equals(String.class)) {                        method.invoke(configurable, value);                    } else if (fieldType.equals(boolean.class)) {                        method.invoke(configurable, Boolean.parseBoolean(value));                    } else if (fieldType.equals(short.class)) {                        method.invoke(configurable, Short.parseShort(value));                    } else if (fieldType.equals(long.class)) {                        method.invoke(configurable, Long.parseLong(value));                    } else if (fieldType.equals(float.class)) {                        method.invoke(configurable, Float.parseFloat(value));                    } else if (fieldType.equals(int.class)) {                        method.invoke(configurable, Integer.parseInt(value));                    } else if (fieldType.equals(double.class)) {                        method.invoke(configurable, Double.parseDouble(value));                    } else if (fieldType.equals(char.class)) {                        method.invoke(configurable, value.charAt(0));                    } else if (fieldType.equals(byte.class)) {                        method.invoke(configurable, Byte.parseByte(value));                    } else if (fieldType.equals(String[].class)) {                        method.invoke(configurable, (Object) value.split("\\s+"));                    } else {                        throw new ConfigurationException("Unable to configure component due to an unsupported type on field: " + fieldName);                    }                } catch (Exception ex) {                    if (ex instanceof ConfigurationException) {                        throw (ConfigurationException) ex;                    } else {                        throw new ConfigurationException("Unable to configure component: ", ex);                    }                }            }        }    }}
0
public static void setConfigurationFields(Object configurable, Context context) throws ConfigurationException
{    Class<?> clazz = configurable.getClass();    Map<String, String> properties = context.getSubProperties(clazz.getSimpleName() + ".");    setConfigurationFields(configurable, properties);}
0
public static void setConfigurationFields(Object configurable, Context context, String subPropertiesPrefix) throws ConfigurationException
{    Map<String, String> properties = context.getSubProperties(subPropertiesPrefix);    setConfigurationFields(configurable, properties);}
0
public static void main(String[] args)
{    if (args.length == 0) {        for (Object prop : System.getProperties().keySet()) {            System.out.println(prop + "=" + System.getProperty((String) prop, ""));        }    } else {        for (String prop : args) {            System.out.println(prop + "=" + System.getProperty(prop, ""));        }    }}
0
public static ConstraintSecurityHandler enforceConstraints()
{    Constraint c = new Constraint();    c.setAuthenticate(true);    ConstraintMapping cmt = new ConstraintMapping();    cmt.setConstraint(c);    cmt.setMethod("TRACE");    cmt.setPathSpec("/*");    ConstraintMapping cmo = new ConstraintMapping();    cmo.setConstraint(c);    cmo.setMethod("OPTIONS");    cmo.setPathSpec("/*");    ConstraintSecurityHandler sh = new ConstraintSecurityHandler();    sh.setConstraintMappings(new ConstraintMapping[] { cmt, cmo });    return sh;}
0
public static boolean isWindows()
{    String os = System.getProperty("os.name");    boolean isWin = (os.toLowerCase(Locale.ENGLISH).indexOf("win") >= 0);    return isWin;}
0
public static long roundDownTimeStampSeconds(long timestamp, int roundDownSec) throws IllegalStateException
{    return roundDownTimeStampSeconds(timestamp, roundDownSec, null);}
0
public static long roundDownTimeStampSeconds(long timestamp, int roundDownSec, TimeZone timeZone) throws IllegalStateException
{    Preconditions.checkArgument(roundDownSec > 0 && roundDownSec <= 60, "RoundDownSec must be > 0 and <=60");    Calendar cal = roundDownField(timestamp, Calendar.SECOND, roundDownSec, timeZone);    cal.set(Calendar.MILLISECOND, 0);    return cal.getTimeInMillis();}
0
public static long roundDownTimeStampMinutes(long timestamp, int roundDownMins) throws IllegalStateException
{    return roundDownTimeStampMinutes(timestamp, roundDownMins, null);}
0
public static long roundDownTimeStampMinutes(long timestamp, int roundDownMins, TimeZone timeZone) throws IllegalStateException
{    Preconditions.checkArgument(roundDownMins > 0 && roundDownMins <= 60, "RoundDown must be > 0 and <=60");    Calendar cal = roundDownField(timestamp, Calendar.MINUTE, roundDownMins, timeZone);    cal.set(Calendar.SECOND, 0);    cal.set(Calendar.MILLISECOND, 0);    return cal.getTimeInMillis();}
0
public static long roundDownTimeStampHours(long timestamp, int roundDownHours) throws IllegalStateException
{    return roundDownTimeStampHours(timestamp, roundDownHours, null);}
0
public static long roundDownTimeStampHours(long timestamp, int roundDownHours, TimeZone timeZone) throws IllegalStateException
{    Preconditions.checkArgument(roundDownHours > 0 && roundDownHours <= 24, "RoundDown must be > 0 and <=24");    Calendar cal = roundDownField(timestamp, Calendar.HOUR_OF_DAY, roundDownHours, timeZone);    cal.set(Calendar.MINUTE, 0);    cal.set(Calendar.SECOND, 0);    cal.set(Calendar.MILLISECOND, 0);    return cal.getTimeInMillis();}
0
private static Calendar roundDownField(long timestamp, int field, int roundDown, TimeZone timeZone)
{    Preconditions.checkArgument(timestamp > 0, "Timestamp must be positive");    Calendar cal = (timeZone == null) ? Calendar.getInstance() : Calendar.getInstance(timeZone);    cal.setTimeInMillis(timestamp);    int fieldVal = cal.get(field);    int remainder = (fieldVal % roundDown);    cal.set(field, fieldVal - remainder);    return cal;}
0
 static Package getPackage()
{    return myPackage;}
0
public static String getVersion()
{    return version != null ? version.version() : "Unknown";}
0
public static String getRevision()
{    if (version != null && version.revision() != null && !version.revision().isEmpty()) {        return version.revision();    }    return "Unknown";}
0
public static String getBranch()
{    return version != null ? version.branch() : "Unknown";}
0
public static String getDate()
{    return version != null ? version.date() : "Unknown";}
0
public static String getUser()
{    return version != null ? version.user() : "Unknown";}
0
public static String getUrl()
{    return version != null ? version.url() : "Unknown";}
0
public static String getSrcChecksum()
{    return version != null ? version.srcChecksum() : "Unknown";}
0
public static String getBuildVersion()
{    return VersionInfo.getVersion() + " from " + VersionInfo.getRevision() + " by " + VersionInfo.getUser() + " on " + VersionInfo.getDate() + " source checksum " + VersionInfo.getSrcChecksum();}
0
public static void main(String[] args)
{    System.out.println("Flume " + getVersion());    System.out.println("Source code repository: " + "https://git-wip-us.apache.org/repos/asf/flume.git");    System.out.println("Revision: " + getRevision());    System.out.println("Compiled by " + getUser() + " on " + getDate());    System.out.println("From source with checksum " + getSrcChecksum());}
0
public Mode getMode()
{    return mode;}
0
public void setMode(Mode mode)
{    this.mode = mode;}
0
public boolean wasLastTransactionCommitted()
{    return lastTransactionCommitted;}
0
public boolean wasLastTransactionRolledBack()
{    return lastTransactionRolledBack;}
0
public boolean wasLastTransactionClosed()
{    return lastTransactionClosed;}
0
protected BasicTransactionSemantics createTransaction()
{    return new TestTransaction();}
0
protected void doMode() throws InterruptedException
{    switch(mode) {        case THROW_ERROR:            throw new TestError();        case THROW_RUNTIME:            throw new TestRuntimeException();        case THROW_CHANNEL:            throw new ChannelException("test");        case SLEEP:            Thread.sleep(300000);            break;    }}
0
protected void doBegin() throws InterruptedException
{    doMode();}
0
protected void doPut(Event event) throws InterruptedException
{    doMode();    synchronized (queue) {        queue.add(event);    }}
0
protected Event doTake() throws InterruptedException
{    doMode();    synchronized (queue) {        return queue.poll();    }}
0
protected void doCommit() throws InterruptedException
{    doMode();    lastTransactionCommitted = true;}
0
protected void doRollback() throws InterruptedException
{    lastTransactionRolledBack = true;    doMode();}
0
protected void doClose()
{    lastTransactionClosed = true;    Preconditions.checkState(mode != TestChannel.Mode.SLEEP, "doClose() can't throw InterruptedException, so why SLEEP?");    try {        doMode();    } catch (InterruptedException e) {        Assert.fail();    }}
0
protected void testException(Class<? extends Throwable> exceptionClass, Runnable test)
{    try {        test.run();        Assert.fail();    } catch (Throwable e) {        if (exceptionClass == InterruptedException.class && e instanceof ChannelException && e.getCause() instanceof InterruptedException) {            Assert.assertTrue(Thread.interrupted());        } else if (!exceptionClass.isInstance(e)) {            throw new AssertionError(e);        }    }}
0
protected void testIllegalArgument(Runnable test)
{    testException(IllegalArgumentException.class, test);}
0
protected void testIllegalState(Runnable test)
{    testException(IllegalStateException.class, test);}
0
protected void testWrongThread(final Runnable test) throws Exception
{    executor.submit(new Runnable() {        @Override        public void run() {            testIllegalState(test);        }    }).get();}
0
public void run()
{    testIllegalState(test);}
0
protected void testMode(TestChannel.Mode mode, Runnable test)
{    TestChannel.Mode oldMode = channel.getMode();    try {        channel.setMode(mode);        test.run();    } finally {        channel.setMode(oldMode);    }}
0
protected void testException(TestChannel.Mode mode, final Class<? extends Throwable> exceptionClass, final Runnable test)
{    testMode(mode, new Runnable() {        @Override        public void run() {            testException(exceptionClass, test);        }    });}
0
public void run()
{    testException(exceptionClass, test);}
0
protected void testError(Runnable test)
{    testException(TestChannel.Mode.THROW_ERROR, TestError.class, test);}
0
protected void testRuntimeException(Runnable test)
{    testException(TestChannel.Mode.THROW_RUNTIME, TestRuntimeException.class, test);}
0
protected void testChannelException(Runnable test)
{    testException(TestChannel.Mode.THROW_CHANNEL, ChannelException.class, test);}
0
protected void testInterrupt(final Runnable test)
{    testMode(TestChannel.Mode.SLEEP, new Runnable() {        @Override        public void run() {            testException(InterruptedException.class, new Runnable() {                @Override                public void run() {                    interruptTest(test);                }            });        }    });}
0
public void run()
{    testException(InterruptedException.class, new Runnable() {        @Override        public void run() {            interruptTest(test);        }    });}
0
public void run()
{    interruptTest(test);}
0
protected void interruptTest(final Runnable test)
{    final Thread mainThread = Thread.currentThread();    Future<?> future = executor.submit(new Runnable() {        @Override        public void run() {            try {                Thread.sleep(500);            } catch (InterruptedException e) {            }            mainThread.interrupt();        }    });    test.run();    try {        future.get();    } catch (Exception e) {        throw new AssertionError(e);    }}
0
public void run()
{    try {        Thread.sleep(500);    } catch (InterruptedException e) {    }    mainThread.interrupt();}
0
protected void testExceptions(Runnable test) throws Exception
{    testWrongThread(test);    testBasicExceptions(test);    testInterrupt(test);}
0
protected void testBasicExceptions(Runnable test) throws Exception
{    testError(test);    testRuntimeException(test);    testChannelException(test);}
0
public void before()
{    Preconditions.checkState(channel == null, "test cleanup failed!");    Preconditions.checkState(executor == null, "test cleanup failed!");    channel = new TestChannel();    executor = Executors.newCachedThreadPool();}
0
public void after()
{    channel = null;    executor.shutdown();    executor = null;}
0
public static Channel createMockChannel(String name)
{    Channel ch = new MockChannel();    ch.setName(name);    return ch;}
0
public void put(Event event) throws ChannelException
{    events.add(event);}
0
public Event take() throws ChannelException
{    return (events.size() > 0) ? events.get(0) : null;}
0
public Transaction getTransaction()
{    return new MockTransaction();}
0
public void begin()
{}
0
public void commit()
{}
0
public void rollback()
{}
0
public void close()
{}
0
public Map<String, String> getHeaders()
{    return headers;}
0
public void setHeaders(Map<String, String> headers)
{    this.headers = new HashMap<String, String>();    this.headers.putAll(headers);}
0
public byte[] getBody()
{    return body;}
0
public void setBody(byte[] body)
{    this.body = new byte[body.length];    System.arraycopy(body, 0, this.body, 0, body.length);}
0
public void testHappyPath()
{    for (int i = 0; i < events.size(); ++i) {        Transaction transaction = channel.getTransaction();        transaction.begin();        channel.put(events.get(i));        transaction.commit();        transaction.close();    }    for (int i = 0; i < events.size(); ++i) {        Transaction transaction = channel.getTransaction();        transaction.begin();        Assert.assertSame(events.get(i), channel.take());        transaction.commit();        transaction.close();    }}
0
public void testMultiThreadedHappyPath() throws Exception
{    final int testLength = 1000;    Future<?> producer = executor.submit(new Runnable() {        @Override        public void run() {            try {                Thread.sleep(500);                for (int i = 0; i < testLength; ++i) {                    Transaction transaction = channel.getTransaction();                    transaction.begin();                    channel.put(events.get(i % events.size()));                    transaction.commit();                    transaction.close();                    Thread.sleep(1);                }                Thread.sleep(500);            } catch (InterruptedException e) {                Assert.fail();            }        }    });    int i = 0;    while (!producer.isDone()) {        Transaction transaction = channel.getTransaction();        transaction.begin();        Event event = channel.take();        if (event != null) {            Assert.assertSame(events.get(i % events.size()), event);            ++i;        }        transaction.commit();        transaction.close();    }    Assert.assertEquals(testLength, i);    producer.get();}
0
public void run()
{    try {        Thread.sleep(500);        for (int i = 0; i < testLength; ++i) {            Transaction transaction = channel.getTransaction();            transaction.begin();            channel.put(events.get(i % events.size()));            transaction.commit();            transaction.close();            Thread.sleep(1);        }        Thread.sleep(500);    } catch (InterruptedException e) {        Assert.fail();    }}
0
public void testGetTransaction() throws Exception
{    final Transaction transaction = channel.getTransaction();    executor.submit(new Runnable() {        @Override        public void run() {            Assert.assertNotSame(transaction, channel.getTransaction());        }    }).get();    Assert.assertSame(transaction, channel.getTransaction());    transaction.begin();    executor.submit(new Runnable() {        @Override        public void run() {            Assert.assertNotSame(transaction, channel.getTransaction());        }    }).get();    Assert.assertSame(transaction, channel.getTransaction());    transaction.commit();    executor.submit(new Runnable() {        @Override        public void run() {            Assert.assertNotSame(transaction, channel.getTransaction());        }    }).get();    Assert.assertSame(transaction, channel.getTransaction());    transaction.close();    executor.submit(new Runnable() {        @Override        public void run() {            Assert.assertNotSame(transaction, channel.getTransaction());        }    }).get();    Assert.assertNotSame(transaction, channel.getTransaction());}
0
public void run()
{    Assert.assertNotSame(transaction, channel.getTransaction());}
0
public void run()
{    Assert.assertNotSame(transaction, channel.getTransaction());}
0
public void run()
{    Assert.assertNotSame(transaction, channel.getTransaction());}
0
public void run()
{    Assert.assertNotSame(transaction, channel.getTransaction());}
0
public void testBegin() throws Exception
{    final Transaction transaction = channel.getTransaction();    testExceptions(new Runnable() {        @Override        public void run() {            transaction.begin();        }    });    transaction.begin();    testIllegalState(new Runnable() {        @Override        public void run() {            transaction.begin();        }    });    transaction.commit();    testIllegalState(new Runnable() {        @Override        public void run() {            transaction.begin();        }    });    transaction.close();    testIllegalState(new Runnable() {        @Override        public void run() {            transaction.begin();        }    });}
0
public void run()
{    transaction.begin();}
0
public void run()
{    transaction.begin();}
0
public void run()
{    transaction.begin();}
0
public void run()
{    transaction.begin();}
0
public void testPut1() throws Exception
{    testIllegalState(new Runnable() {        @Override        public void run() {            channel.put(events.get(0));        }    });    Transaction transaction = channel.getTransaction();    testIllegalState(new Runnable() {        @Override        public void run() {            channel.put(events.get(0));        }    });    transaction.begin();    channel.put(events.get(0));    testIllegalArgument(new Runnable() {        @Override        public void run() {            channel.put(null);        }    });    testExceptions(new Runnable() {        @Override        public void run() {            channel.put(events.get(0));        }    });    transaction.commit();    testIllegalState(new Runnable() {        @Override        public void run() {            channel.put(events.get(0));        }    });    transaction.close();    testIllegalState(new Runnable() {        @Override        public void run() {            channel.put(events.get(0));        }    });}
0
public void run()
{    channel.put(events.get(0));}
0
public void run()
{    channel.put(events.get(0));}
0
public void run()
{    channel.put(null);}
0
public void run()
{    channel.put(events.get(0));}
0
public void run()
{    channel.put(events.get(0));}
0
public void run()
{    channel.put(events.get(0));}
0
public void testPut2() throws Exception
{    Transaction transaction = channel.getTransaction();    transaction.begin();    channel.put(events.get(0));    transaction.rollback();    testIllegalState(new Runnable() {        @Override        public void run() {            channel.put(events.get(0));        }    });    transaction.close();    testIllegalState(new Runnable() {        @Override        public void run() {            channel.put(events.get(0));        }    });}
0
public void run()
{    channel.put(events.get(0));}
0
public void run()
{    channel.put(events.get(0));}
0
public void testPut3() throws Exception
{    Transaction transaction = channel.getTransaction();    transaction.begin();    channel.put(events.get(0));    final Transaction finalTransaction = transaction;    testChannelException(new Runnable() {        @Override        public void run() {            finalTransaction.commit();        }    });    transaction.rollback();    testIllegalState(new Runnable() {        @Override        public void run() {            channel.put(events.get(0));        }    });    transaction.close();    testIllegalState(new Runnable() {        @Override        public void run() {            channel.put(events.get(0));        }    });}
0
public void run()
{    finalTransaction.commit();}
0
public void run()
{    channel.put(events.get(0));}
0
public void run()
{    channel.put(events.get(0));}
0
public void testTake1() throws Exception
{    testIllegalState(new Runnable() {        @Override        public void run() {            channel.take();        }    });    Transaction transaction = channel.getTransaction();    testIllegalState(new Runnable() {        @Override        public void run() {            channel.take();        }    });    transaction.begin();    Assert.assertNull(channel.take());    for (int i = 0; i < 1000; ++i) {        channel.put(events.get(i % events.size()));    }    Assert.assertNotNull(channel.take());    testWrongThread(new Runnable() {        @Override        public void run() {            channel.take();        }    });    testBasicExceptions(new Runnable() {        @Override        public void run() {            channel.take();        }    });    testMode(TestChannel.Mode.SLEEP, new Runnable() {        @Override        public void run() {            interruptTest(new Runnable() {                @Override                public void run() {                    Assert.assertNull(channel.take());                    Assert.assertTrue(Thread.interrupted());                }            });        }    });    Assert.assertNotNull(channel.take());    transaction.commit();    testIllegalState(new Runnable() {        @Override        public void run() {            channel.take();        }    });    transaction.close();    testIllegalState(new Runnable() {        @Override        public void run() {            channel.take();        }    });}
0
public void run()
{    channel.take();}
0
public void run()
{    channel.take();}
0
public void run()
{    channel.take();}
0
public void run()
{    channel.take();}
0
public void run()
{    interruptTest(new Runnable() {        @Override        public void run() {            Assert.assertNull(channel.take());            Assert.assertTrue(Thread.interrupted());        }    });}
0
public void run()
{    Assert.assertNull(channel.take());    Assert.assertTrue(Thread.interrupted());}
0
public void run()
{    channel.take();}
0
public void run()
{    channel.take();}
0
public void testTake2() throws Exception
{    Transaction transaction = channel.getTransaction();    transaction.begin();    channel.take();    transaction.rollback();    testIllegalState(new Runnable() {        @Override        public void run() {            channel.take();        }    });    transaction.close();    testIllegalState(new Runnable() {        @Override        public void run() {            channel.take();        }    });}
0
public void run()
{    channel.take();}
0
public void run()
{    channel.take();}
0
public void testTake3() throws Exception
{    Transaction transaction = channel.getTransaction();    transaction.begin();    channel.take();    final Transaction finalTransaction = transaction;    testChannelException(new Runnable() {        @Override        public void run() {            finalTransaction.commit();        }    });    transaction.rollback();    testIllegalState(new Runnable() {        @Override        public void run() {            channel.take();        }    });    transaction.close();    testIllegalState(new Runnable() {        @Override        public void run() {            channel.take();        }    });}
0
public void run()
{    finalTransaction.commit();}
0
public void run()
{    channel.take();}
0
public void run()
{    channel.take();}
0
public void testCommit1() throws Exception
{    final Transaction transaction = channel.getTransaction();    testIllegalState(new Runnable() {        @Override        public void run() {            transaction.commit();        }    });    transaction.begin();    testExceptions(new Runnable() {        @Override        public void run() {            transaction.commit();        }    });    transaction.commit();    testIllegalState(new Runnable() {        @Override        public void run() {            transaction.commit();        }    });    transaction.close();    testIllegalState(new Runnable() {        @Override        public void run() {            transaction.commit();        }    });}
0
public void run()
{    transaction.commit();}
0
public void run()
{    transaction.commit();}
0
public void run()
{    transaction.commit();}
0
public void run()
{    transaction.commit();}
0
public void testCommit2() throws Exception
{    final Transaction transaction = channel.getTransaction();    transaction.begin();    transaction.rollback();    testIllegalState(new Runnable() {        @Override        public void run() {            transaction.commit();        }    });    transaction.close();    testIllegalState(new Runnable() {        @Override        public void run() {            transaction.commit();        }    });}
0
public void run()
{    transaction.commit();}
0
public void run()
{    transaction.commit();}
0
public void testRollback1() throws Exception
{    final Transaction transaction = channel.getTransaction();    testIllegalState(new Runnable() {        @Override        public void run() {            transaction.rollback();        }    });    transaction.begin();    testWrongThread(new Runnable() {        @Override        public void run() {            transaction.rollback();        }    });    transaction.rollback();    testIllegalState(new Runnable() {        @Override        public void run() {            transaction.rollback();        }    });    transaction.close();    testIllegalState(new Runnable() {        @Override        public void run() {            transaction.rollback();        }    });}
0
public void run()
{    transaction.rollback();}
0
public void run()
{    transaction.rollback();}
0
public void run()
{    transaction.rollback();}
0
public void run()
{    transaction.rollback();}
0
public void testRollback2() throws Exception
{    final Transaction transaction = channel.getTransaction();    testIllegalState(new Runnable() {        @Override        public void run() {            transaction.rollback();        }    });    transaction.begin();    testError(new Runnable() {        @Override        public void run() {            transaction.rollback();        }    });    testIllegalState(new Runnable() {        @Override        public void run() {            transaction.rollback();        }    });    transaction.close();    testIllegalState(new Runnable() {        @Override        public void run() {            transaction.rollback();        }    });}
0
public void run()
{    transaction.rollback();}
0
public void run()
{    transaction.rollback();}
0
public void run()
{    transaction.rollback();}
0
public void run()
{    transaction.rollback();}
0
public void testRollback3() throws Exception
{    final Transaction transaction = channel.getTransaction();    testIllegalState(new Runnable() {        @Override        public void run() {            transaction.rollback();        }    });    transaction.begin();    testRuntimeException(new Runnable() {        @Override        public void run() {            transaction.rollback();        }    });    testIllegalState(new Runnable() {        @Override        public void run() {            transaction.rollback();        }    });    transaction.close();    testIllegalState(new Runnable() {        @Override        public void run() {            transaction.rollback();        }    });}
0
public void run()
{    transaction.rollback();}
0
public void run()
{    transaction.rollback();}
0
public void run()
{    transaction.rollback();}
0
public void run()
{    transaction.rollback();}
0
public void testRollback4() throws Exception
{    final Transaction transaction = channel.getTransaction();    testIllegalState(new Runnable() {        @Override        public void run() {            transaction.rollback();        }    });    transaction.begin();    testChannelException(new Runnable() {        @Override        public void run() {            transaction.rollback();        }    });    testIllegalState(new Runnable() {        @Override        public void run() {            transaction.rollback();        }    });    transaction.close();    testIllegalState(new Runnable() {        @Override        public void run() {            transaction.rollback();        }    });}
0
public void run()
{    transaction.rollback();}
0
public void run()
{    transaction.rollback();}
0
public void run()
{    transaction.rollback();}
0
public void run()
{    transaction.rollback();}
0
public void testRollback5() throws Exception
{    final Transaction transaction = channel.getTransaction();    testIllegalState(new Runnable() {        @Override        public void run() {            transaction.rollback();        }    });    transaction.begin();    testInterrupt(new Runnable() {        @Override        public void run() {            transaction.rollback();        }    });    testIllegalState(new Runnable() {        @Override        public void run() {            transaction.rollback();        }    });    transaction.close();    testIllegalState(new Runnable() {        @Override        public void run() {            transaction.rollback();        }    });}
0
public void run()
{    transaction.rollback();}
0
public void run()
{    transaction.rollback();}
0
public void run()
{    transaction.rollback();}
0
public void run()
{    transaction.rollback();}
0
public void testRollback6() throws Exception
{    final Transaction transaction = channel.getTransaction();    transaction.begin();    transaction.commit();    testIllegalState(new Runnable() {        @Override        public void run() {            transaction.rollback();        }    });    transaction.close();    testIllegalState(new Runnable() {        @Override        public void run() {            transaction.rollback();        }    });}
0
public void run()
{    transaction.rollback();}
0
public void run()
{    transaction.rollback();}
0
public void testRollback7() throws Exception
{    final Transaction transaction = channel.getTransaction();    transaction.begin();    testExceptions(new Runnable() {        @Override        public void run() {            transaction.commit();        }    });    transaction.rollback();    testIllegalState(new Runnable() {        @Override        public void run() {            transaction.rollback();        }    });    transaction.close();    testIllegalState(new Runnable() {        @Override        public void run() {            transaction.rollback();        }    });}
0
public void run()
{    transaction.commit();}
0
public void run()
{    transaction.rollback();}
0
public void run()
{    transaction.rollback();}
0
public void testClose1() throws Exception
{    final Transaction transaction = channel.getTransaction();    testError(new Runnable() {        @Override        public void run() {            transaction.close();        }    });    testIllegalState(new Runnable() {        @Override        public void run() {            transaction.close();        }    });}
0
public void run()
{    transaction.close();}
0
public void run()
{    transaction.close();}
0
public void testClose2() throws Exception
{    final Transaction transaction = channel.getTransaction();    testRuntimeException(new Runnable() {        @Override        public void run() {            transaction.close();        }    });    testIllegalState(new Runnable() {        @Override        public void run() {            transaction.close();        }    });}
0
public void run()
{    transaction.close();}
0
public void run()
{    transaction.close();}
0
public void testClose3() throws Exception
{    final Transaction transaction = channel.getTransaction();    testChannelException(new Runnable() {        @Override        public void run() {            transaction.close();        }    });    testIllegalState(new Runnable() {        @Override        public void run() {            transaction.close();        }    });}
0
public void run()
{    transaction.close();}
0
public void run()
{    transaction.close();}
0
public void testClose4() throws Exception
{    final Transaction transaction = channel.getTransaction();    transaction.begin();    testIllegalState(new Runnable() {        @Override        public void run() {            transaction.close();        }    });}
0
public void run()
{    transaction.close();}
0
public void testClose5() throws Exception
{    final Transaction transaction = channel.getTransaction();    transaction.begin();    testChannelException(new Runnable() {        @Override        public void run() {            transaction.commit();        }    });    testIllegalState(new Runnable() {        @Override        public void run() {            transaction.close();        }    });}
0
public void run()
{    transaction.commit();}
0
public void run()
{    transaction.close();}
0
public void testExceptionFromGetTransaction()
{        Channel ch = mock(Channel.class);    when(ch.getTransaction()).thenThrow(new ChannelException("doh!"));    ChannelSelector sel = new ReplicatingChannelSelector();    sel.setChannels(Lists.newArrayList(ch));    ChannelProcessor proc = new ChannelProcessor(sel);    List<Event> events = Lists.newArrayList();    events.add(EventBuilder.withBody("event 1", Charsets.UTF_8));    proc.processEventBatch(events);}
0
public void testNullFromGetTransaction()
{        Channel ch = mock(Channel.class);    when(ch.getTransaction()).thenReturn(null);    ChannelSelector sel = new ReplicatingChannelSelector();    sel.setChannels(Lists.newArrayList(ch));    ChannelProcessor proc = new ChannelProcessor(sel);    List<Event> events = Lists.newArrayList();    events.add(EventBuilder.withBody("event 1", Charsets.UTF_8));    boolean threw = false;    try {        proc.processEventBatch(events);    } catch (NullPointerException ex) {        threw = true;        Assert.assertNotNull("NPE must be manually thrown", ex.getMessage());    }    Assert.assertTrue("Must throw NPE", threw);}
0
public void testRequiredAndOptionalChannels()
{    Context context = new Context();    ArrayList<Channel> channels = new ArrayList<Channel>();    for (int i = 0; i < 4; i++) {        Channel ch = new MemoryChannel();        ch.setName("ch" + i);        Configurables.configure(ch, context);        channels.add(ch);    }    ChannelSelector selector = new ReplicatingChannelSelector();    selector.setChannels(channels);    context = new Context();    context.put(ReplicatingChannelSelector.CONFIG_OPTIONAL, "ch2 ch3");    Configurables.configure(selector, context);    ChannelProcessor processor = new ChannelProcessor(selector);    context = new Context();    Configurables.configure(processor, context);    Event event1 = EventBuilder.withBody("event 1", Charsets.UTF_8);    processor.processEvent(event1);    try {        Thread.sleep(3000);    } catch (InterruptedException e) {    }    for (Channel channel : channels) {        Transaction transaction = channel.getTransaction();        transaction.begin();        Event event_ch = channel.take();        Assert.assertEquals(event1, event_ch);        transaction.commit();        transaction.close();    }    List<Event> events = Lists.newArrayList();    for (int i = 0; i < 100; i++) {        events.add(EventBuilder.withBody("event " + i, Charsets.UTF_8));    }    processor.processEventBatch(events);    try {        Thread.sleep(3000);    } catch (InterruptedException e) {    }    for (Channel channel : channels) {        Transaction transaction = channel.getTransaction();        transaction.begin();        for (int i = 0; i < 100; i++) {            Event event_ch = channel.take();            Assert.assertNotNull(event_ch);        }        transaction.commit();        transaction.close();    }}
0
public void testHappyPath1()
{    ChannelUtils.put(channel, events.get(0));    Assert.assertTrue(channel.wasLastTransactionCommitted());    Assert.assertFalse(channel.wasLastTransactionRolledBack());    Assert.assertTrue(channel.wasLastTransactionClosed());}
0
public void testHappyPath2()
{    ChannelUtils.take(channel);    Assert.assertTrue(channel.wasLastTransactionCommitted());    Assert.assertFalse(channel.wasLastTransactionRolledBack());    Assert.assertTrue(channel.wasLastTransactionClosed());}
0
public void testHappyPath3()
{    ChannelUtils.put(channel, events.get(0));    Assert.assertSame(events.get(0), ChannelUtils.take(channel));}
0
public void testHappyPath4()
{    for (int i = 0; i < events.size(); ++i) {        ChannelUtils.put(channel, events.get(i));    }    for (int i = 0; i < events.size(); ++i) {        Assert.assertSame(events.get(i), ChannelUtils.take(channel));    }}
0
public void testHappyPath5()
{    int rounds = 10;    for (int i = 0; i < rounds; ++i) {        ChannelUtils.put(channel, events);    }    for (int i = 0; i < rounds; ++i) {        List<Event> takenEvents = ChannelUtils.take(channel, events.size());        Assert.assertTrue(takenEvents.size() == events.size());        for (int j = 0; j < events.size(); ++j) {            Assert.assertSame(events.get(j), takenEvents.get(j));        }    }}
0
private void testTransact(final TestChannel.Mode mode, Class<? extends Throwable> exceptionClass, final Runnable test)
{    testException(exceptionClass, new Runnable() {        @Override        public void run() {            ChannelUtils.transact(channel, new Runnable() {                @Override                public void run() {                    testMode(mode, test);                }            });        }    });    Assert.assertFalse(channel.wasLastTransactionCommitted());    Assert.assertTrue(channel.wasLastTransactionRolledBack());    Assert.assertTrue(channel.wasLastTransactionClosed());}
0
public void run()
{    ChannelUtils.transact(channel, new Runnable() {        @Override        public void run() {            testMode(mode, test);        }    });}
0
public void run()
{    testMode(mode, test);}
0
private void testTransact(TestChannel.Mode mode, Class<? extends Throwable> exceptionClass)
{    testTransact(mode, exceptionClass, new Runnable() {        @Override        public void run() {            channel.put(events.get(0));        }    });}
0
public void run()
{    channel.put(events.get(0));}
0
public void testError()
{    testTransact(TestChannel.Mode.THROW_ERROR, TestError.class);}
0
public void testRuntimeException()
{    testTransact(TestChannel.Mode.THROW_RUNTIME, TestRuntimeException.class);}
0
public void testChannelException()
{    testTransact(TestChannel.Mode.THROW_CHANNEL, ChannelException.class);}
0
public void testInterrupt() throws Exception
{    testTransact(TestChannel.Mode.SLEEP, InterruptedException.class, new Runnable() {        @Override        public void run() {            interruptTest(new Runnable() {                @Override                public void run() {                    channel.put(events.get(0));                }            });        }    });}
0
public void run()
{    interruptTest(new Runnable() {        @Override        public void run() {            channel.put(events.get(0));        }    });}
0
public void run()
{    channel.put(events.get(0));}
0
public void setUp()
{    channel = new MemoryChannel();}
0
public void testPutTake() throws InterruptedException, EventDeliveryException
{    Event event = EventBuilder.withBody("test event".getBytes());    Context context = new Context();    Configurables.configure(channel, context);    Transaction transaction = channel.getTransaction();    Assert.assertNotNull(transaction);    transaction.begin();    channel.put(event);    transaction.commit();    transaction.close();    transaction = channel.getTransaction();    Assert.assertNotNull(transaction);    transaction.begin();    Event event2 = channel.take();    Assert.assertEquals(event, event2);    transaction.commit();}
0
public void testPutAcceptsNullValueInHeader()
{    Configurables.configure(channel, new Context());    Event event = EventBuilder.withBody("test body".getBytes(Charsets.UTF_8), Collections.<String, String>singletonMap("test_key", null));    Transaction txPut = channel.getTransaction();    txPut.begin();    channel.put(event);    txPut.commit();    txPut.close();    Transaction txTake = channel.getTransaction();    txTake.begin();    Event eventTaken = channel.take();    Assert.assertEquals(event, eventTaken);    txTake.commit();}
0
public void testChannelResize()
{    Context context = new Context();    Map<String, String> parms = new HashMap<String, String>();    parms.put("capacity", "5");    parms.put("transactionCapacity", "5");    context.putAll(parms);    Configurables.configure(channel, context);    Transaction transaction = channel.getTransaction();    transaction.begin();    for (int i = 0; i < 5; i++) {        channel.put(EventBuilder.withBody(String.format("test event %d", i).getBytes()));    }    transaction.commit();    transaction.close();    /*     * Verify overflow semantics     */    transaction = channel.getTransaction();    boolean overflowed = false;    try {        transaction.begin();        channel.put(EventBuilder.withBody("overflow event".getBytes()));        transaction.commit();    } catch (ChannelException e) {        overflowed = true;        transaction.rollback();    } finally {        transaction.close();    }    Assert.assertTrue(overflowed);    /*     * Reconfigure capacity down and add another event, shouldn't result in exception     */    parms.put("capacity", "6");    context.putAll(parms);    Configurables.configure(channel, context);    transaction = channel.getTransaction();    transaction.begin();    channel.put(EventBuilder.withBody("extended capacity event".getBytes()));    transaction.commit();    transaction.close();    /*     * Attempt to reconfigure capacity to below current entry count and verify     * it wasn't carried out     */    parms.put("capacity", "2");    parms.put("transactionCapacity", "2");    context.putAll(parms);    Configurables.configure(channel, context);    for (int i = 0; i < 6; i++) {        transaction = channel.getTransaction();        transaction.begin();        Assert.assertNotNull(channel.take());        transaction.commit();        transaction.close();    }}
0
public void testTransactionPutCapacityOverload()
{    Context context = new Context();    Map<String, String> parms = new HashMap<String, String>();    parms.put("capacity", "5");    parms.put("transactionCapacity", "2");    context.putAll(parms);    Configurables.configure(channel, context);    Transaction transaction = channel.getTransaction();    transaction.begin();    channel.put(EventBuilder.withBody("test".getBytes()));    channel.put(EventBuilder.withBody("test".getBytes()));        channel.put(EventBuilder.withBody("test".getBytes()));    Assert.fail();}
0
public void testCapacityOverload()
{    Context context = new Context();    Map<String, String> parms = new HashMap<String, String>();    parms.put("capacity", "5");    parms.put("transactionCapacity", "3");    context.putAll(parms);    Configurables.configure(channel, context);    Transaction transaction = channel.getTransaction();    transaction.begin();    channel.put(EventBuilder.withBody("test".getBytes()));    channel.put(EventBuilder.withBody("test".getBytes()));    channel.put(EventBuilder.withBody("test".getBytes()));    transaction.commit();    transaction.close();    transaction = channel.getTransaction();    transaction.begin();    channel.put(EventBuilder.withBody("test".getBytes()));    channel.put(EventBuilder.withBody("test".getBytes()));    channel.put(EventBuilder.withBody("test".getBytes()));        transaction.commit();    Assert.fail();}
0
public void testCapacityBufferEmptyingAfterTakeCommit()
{    Context context = new Context();    Map<String, String> parms = new HashMap<String, String>();    parms.put("capacity", "3");    parms.put("transactionCapacity", "3");    context.putAll(parms);    Configurables.configure(channel, context);    Transaction tx = channel.getTransaction();    tx.begin();    channel.put(EventBuilder.withBody("test".getBytes()));    channel.put(EventBuilder.withBody("test".getBytes()));    channel.put(EventBuilder.withBody("test".getBytes()));    tx.commit();    tx.close();    tx = channel.getTransaction();    tx.begin();    channel.take();    channel.take();    tx.commit();    tx.close();    tx = channel.getTransaction();    tx.begin();    channel.put(EventBuilder.withBody("test".getBytes()));    channel.put(EventBuilder.withBody("test".getBytes()));    tx.commit();    tx.close();}
0
public void testCapacityBufferEmptyingAfterRollback()
{    Context context = new Context();    Map<String, String> parms = new HashMap<String, String>();    parms.put("capacity", "3");    parms.put("transactionCapacity", "3");    context.putAll(parms);    Configurables.configure(channel, context);    Transaction tx = channel.getTransaction();    tx.begin();    channel.put(EventBuilder.withBody("test".getBytes()));    channel.put(EventBuilder.withBody("test".getBytes()));    channel.put(EventBuilder.withBody("test".getBytes()));    tx.rollback();    tx.close();    tx = channel.getTransaction();    tx.begin();    channel.put(EventBuilder.withBody("test".getBytes()));    channel.put(EventBuilder.withBody("test".getBytes()));    channel.put(EventBuilder.withBody("test".getBytes()));    tx.commit();    tx.close();}
0
public void testByteCapacityOverload()
{    Context context = new Context();    Map<String, String> parms = new HashMap<String, String>();    parms.put("byteCapacity", "2000");    parms.put("byteCapacityBufferPercentage", "20");    context.putAll(parms);    Configurables.configure(channel, context);    byte[] eventBody = new byte[405];    Transaction transaction = channel.getTransaction();    transaction.begin();    channel.put(EventBuilder.withBody(eventBody));    channel.put(EventBuilder.withBody(eventBody));    channel.put(EventBuilder.withBody(eventBody));    transaction.commit();    transaction.close();    transaction = channel.getTransaction();    transaction.begin();    channel.put(EventBuilder.withBody(eventBody));    channel.put(EventBuilder.withBody(eventBody));        transaction.commit();    Assert.fail();}
0
public void testByteCapacityAfterRollback()
{    Context ctx = new Context(ImmutableMap.of("byteCapacity", "1000"));    Configurables.configure(channel, ctx);    Assert.assertEquals(8, channel.getBytesRemainingValue());    Event e = new SimpleEvent();    Transaction t = channel.getTransaction();    t.begin();    channel.put(e);    t.rollback();    Assert.assertEquals(8, channel.getBytesRemainingValue());}
0
public void testByteCapacityBufferEmptyingAfterTakeCommit()
{    Context context = new Context();    Map<String, String> parms = new HashMap<String, String>();    parms.put("byteCapacity", "2000");    parms.put("byteCapacityBufferPercentage", "20");    context.putAll(parms);    Configurables.configure(channel, context);    byte[] eventBody = new byte[405];    Transaction tx = channel.getTransaction();    tx.begin();    channel.put(EventBuilder.withBody(eventBody));    channel.put(EventBuilder.withBody(eventBody));    channel.put(EventBuilder.withBody(eventBody));    channel.put(EventBuilder.withBody(eventBody));    try {        channel.put(EventBuilder.withBody(eventBody));        throw new RuntimeException("Put was able to overflow byte capacity.");    } catch (ChannelException ce) {        }    tx.commit();    tx.close();    tx = channel.getTransaction();    tx.begin();    channel.take();    channel.take();    tx.commit();    tx.close();    tx = channel.getTransaction();    tx.begin();    channel.put(EventBuilder.withBody(eventBody));    channel.put(EventBuilder.withBody(eventBody));    try {        channel.put(EventBuilder.withBody(eventBody));        throw new RuntimeException("Put was able to overflow byte capacity.");    } catch (ChannelException ce) {        }    tx.commit();    tx.close();}
0
public void testByteCapacityBufferEmptyingAfterRollback()
{    Context context = new Context();    Map<String, String> parms = new HashMap<String, String>();    parms.put("byteCapacity", "2000");    parms.put("byteCapacityBufferPercentage", "20");    context.putAll(parms);    Configurables.configure(channel, context);    byte[] eventBody = new byte[405];    Transaction tx = channel.getTransaction();    tx.begin();    channel.put(EventBuilder.withBody(eventBody));    channel.put(EventBuilder.withBody(eventBody));    channel.put(EventBuilder.withBody(eventBody));    tx.rollback();    tx.close();    tx = channel.getTransaction();    tx.begin();    channel.put(EventBuilder.withBody(eventBody));    channel.put(EventBuilder.withBody(eventBody));    channel.put(EventBuilder.withBody(eventBody));    tx.commit();    tx.close();}
0
public void testByteCapacityBufferChangeConfig()
{    Context context = new Context();    Map<String, String> parms = new HashMap<String, String>();    parms.put("byteCapacity", "2000");    parms.put("byteCapacityBufferPercentage", "20");    context.putAll(parms);    Configurables.configure(channel, context);    byte[] eventBody = new byte[405];    Transaction tx = channel.getTransaction();    tx.begin();    channel.put(EventBuilder.withBody(eventBody));    tx.commit();    tx.close();    channel.stop();    parms.put("byteCapacity", "1500");    context.putAll(parms);    Configurables.configure(channel, context);    channel.start();    tx = channel.getTransaction();    tx.begin();    channel.put(EventBuilder.withBody(eventBody));    try {        channel.put(EventBuilder.withBody(eventBody));        tx.commit();        Assert.fail();    } catch (ChannelException e) {                tx.rollback();    } finally {        tx.close();    }    channel.stop();    parms.put("byteCapacity", "250");    parms.put("byteCapacityBufferPercentage", "20");    context.putAll(parms);    Configurables.configure(channel, context);    channel.start();    tx = channel.getTransaction();    tx.begin();    channel.put(EventBuilder.withBody(eventBody));    tx.commit();    tx.close();    channel.stop();    parms.put("byteCapacity", "300");    context.putAll(parms);    Configurables.configure(channel, context);    channel.start();    tx = channel.getTransaction();    tx.begin();    try {        for (int i = 0; i < 2; i++) {            channel.put(EventBuilder.withBody(eventBody));        }        tx.commit();        Assert.fail();    } catch (ChannelException e) {                tx.rollback();    } finally {        tx.close();    }    channel.stop();    parms.put("byteCapacity", "3300");    context.putAll(parms);    Configurables.configure(channel, context);    channel.start();    tx = channel.getTransaction();    tx.begin();    try {        for (int i = 0; i < 15; i++) {            channel.put(EventBuilder.withBody(eventBody));        }        tx.commit();        Assert.fail();    } catch (ChannelException e) {                tx.rollback();    } finally {        tx.close();    }    channel.stop();    parms.put("byteCapacity", "4000");    context.putAll(parms);    Configurables.configure(channel, context);    channel.start();    tx = channel.getTransaction();    tx.begin();    try {        for (int i = 0; i < 25; i++) {            channel.put(EventBuilder.withBody(eventBody));        }        tx.commit();        Assert.fail();    } catch (ChannelException e) {                tx.rollback();    } finally {        tx.close();    }    channel.stop();}
0
public void testNullEmptyEvent()
{    Context context = new Context();    Map<String, String> parms = new HashMap<String, String>();    parms.put("byteCapacity", "2000");    parms.put("byteCapacityBufferPercentage", "20");    context.putAll(parms);    Configurables.configure(channel, context);    Transaction tx = channel.getTransaction();    tx.begin();        channel.put(EventBuilder.withBody(null));    tx.commit();    tx.close();    tx = channel.getTransaction();    tx.begin();    channel.put(EventBuilder.withBody(new byte[0]));    tx.commit();    tx.close();}
0
public void testNegativeCapacities()
{    Context context = new Context();    Map<String, String> parms = new HashMap<String, String>();    parms.put("capacity", "-3");    parms.put("transactionCapacity", "-1");    context.putAll(parms);    Configurables.configure(channel, context);    Assert.assertTrue(field("queue").ofType(LinkedBlockingDeque.class).in(channel).get().remainingCapacity() > 0);    Assert.assertTrue(field("transCapacity").ofType(Integer.class).in(channel).get() > 0);}
0
public void setUp()
{}
0
public void testTransactionConcurrency() throws InterruptedException
{    final Channel channel = new MemoryChannel();    barrier = new CyclicBarrier(2);    Configurables.configure(channel, new Context());    Thread t1 = new Thread(new Runnable() {        @Override        public void run() {            Transaction tx = channel.getTransaction();            tx.begin();            channel.put(EventBuilder.withBody("first event".getBytes()));            try {                barrier.await();                barrier.await();                tx.rollback();                barrier.await();                tx.close();                                barrier.await();            } catch (InterruptedException e) {                Assert.fail();            } catch (BrokenBarrierException e) {                Assert.fail();            }        }    });    Thread t2 = new Thread(new Runnable() {        @Override        public void run() {            Transaction tx = channel.getTransaction();            try {                barrier.await();                tx.begin();                channel.put(EventBuilder.withBody("second event".getBytes()));                barrier.await();                barrier.await();                tx.commit();                tx.close();                                barrier.await();            } catch (InterruptedException e) {                Assert.fail();            } catch (BrokenBarrierException e) {                Assert.fail();            }        }    });    t1.start();    t2.start();    t1.join(1000);    if (t1.isAlive()) {        Assert.fail("Thread1 failed to finish");        t1.interrupt();    }    t2.join(1000);    if (t2.isAlive()) {        Assert.fail("Thread2 failed to finish");        t2.interrupt();    }    Transaction tx = channel.getTransaction();    tx.begin();    Event e = channel.take();    Assert.assertEquals("second event", new String(e.getBody()));    Assert.assertNull(channel.take());    tx.commit();    tx.close();}
0
public void run()
{    Transaction tx = channel.getTransaction();    tx.begin();    channel.put(EventBuilder.withBody("first event".getBytes()));    try {        barrier.await();        barrier.await();        tx.rollback();        barrier.await();        tx.close();                barrier.await();    } catch (InterruptedException e) {        Assert.fail();    } catch (BrokenBarrierException e) {        Assert.fail();    }}
0
public void run()
{    Transaction tx = channel.getTransaction();    try {        barrier.await();        tx.begin();        channel.put(EventBuilder.withBody("second event".getBytes()));        barrier.await();        barrier.await();        tx.commit();        tx.close();                barrier.await();    } catch (InterruptedException e) {        Assert.fail();    } catch (BrokenBarrierException e) {        Assert.fail();    }}
0
public void testManyThreads() throws InterruptedException
{    final Channel channel = new MemoryChannel();    Context context = new Context();    context.put("keep-alive", "1");        context.put("capacity", "5000");                context.put("transactionCapacity", "5000");    Configurables.configure(channel, context);    final ConcurrentHashMap<String, AtomicInteger> committedPuts = new ConcurrentHashMap<String, AtomicInteger>();    final int threadCount = 100;    final CountDownLatch startGate = new CountDownLatch(1);    final CountDownLatch endGate = new CountDownLatch(threadCount);    for (int i = 0; i < threadCount; i++) {        Thread t = new Thread() {            @Override            public void run() {                Long tid = Thread.currentThread().getId();                String strtid = tid.toString();                Random rng = new Random(tid);                try {                    startGate.await();                } catch (InterruptedException e1) {                    Thread.currentThread().interrupt();                }                for (int j = 0; j < 10; j++) {                    int events = rng.nextInt(5) + 1;                    Transaction tx = channel.getTransaction();                    tx.begin();                    for (int k = 0; k < events; k++) {                        channel.put(EventBuilder.withBody(strtid.getBytes()));                    }                    if (rng.nextBoolean()) {                        tx.commit();                        AtomicInteger tcount = committedPuts.get(strtid);                        if (tcount == null) {                            committedPuts.put(strtid, new AtomicInteger(events));                        } else {                            tcount.addAndGet(events);                        }                    } else {                        tx.rollback();                    }                    tx.close();                }                endGate.countDown();            }        };        t.start();    }    startGate.countDown();    endGate.await();    if (committedPuts.isEmpty()) {        Assert.fail();    }        Transaction tx = channel.getTransaction();    tx.begin();    Event e;    while ((e = channel.take()) != null) {        String index = new String(e.getBody());        AtomicInteger remain = committedPuts.get(index);        int post = remain.decrementAndGet();        if (post == 0) {            committedPuts.remove(index);        }    }    tx.commit();    tx.close();    if (!committedPuts.isEmpty()) {        Assert.fail();    }}
0
public void run()
{    Long tid = Thread.currentThread().getId();    String strtid = tid.toString();    Random rng = new Random(tid);    try {        startGate.await();    } catch (InterruptedException e1) {        Thread.currentThread().interrupt();    }    for (int j = 0; j < 10; j++) {        int events = rng.nextInt(5) + 1;        Transaction tx = channel.getTransaction();        tx.begin();        for (int k = 0; k < events; k++) {            channel.put(EventBuilder.withBody(strtid.getBytes()));        }        if (rng.nextBoolean()) {            tx.commit();            AtomicInteger tcount = committedPuts.get(strtid);            if (tcount == null) {                committedPuts.put(strtid, new AtomicInteger(events));            } else {                tcount.addAndGet(events);            }        } else {            tx.rollback();        }        tx.close();    }    endGate.countDown();}
0
public void testConcurrentSinksAndSources() throws InterruptedException
{    final Channel channel = new MemoryChannel();    Context context = new Context();    context.put("keep-alive", "1");        context.put("capacity", "100");                context.put("transactionCapacity", "100");    Configurables.configure(channel, context);    final ConcurrentHashMap<String, AtomicInteger> committedPuts = new ConcurrentHashMap<String, AtomicInteger>();    final ConcurrentHashMap<String, AtomicInteger> committedTakes = new ConcurrentHashMap<String, AtomicInteger>();    final int threadCount = 100;    final CountDownLatch startGate = new CountDownLatch(1);    final CountDownLatch endGate = new CountDownLatch(threadCount);        for (int i = 0; i < threadCount / 2; i++) {        Thread t = new Thread() {            @Override            public void run() {                Long tid = Thread.currentThread().getId();                String strtid = tid.toString();                Random rng = new Random(tid);                try {                    startGate.await();                } catch (InterruptedException e1) {                    Thread.currentThread().interrupt();                }                for (int j = 0; j < 10; j++) {                    int events = rng.nextInt(5) + 1;                    Transaction tx = channel.getTransaction();                    tx.begin();                    for (int k = 0; k < events; k++) {                        channel.put(EventBuilder.withBody(strtid.getBytes()));                    }                    if (rng.nextBoolean()) {                        try {                            tx.commit();                            AtomicInteger tcount = committedPuts.get(strtid);                            if (tcount == null) {                                committedPuts.put(strtid, new AtomicInteger(events));                            } else {                                tcount.addAndGet(events);                            }                        } catch (ChannelException e) {                            System.out.print("puts commit failed");                            tx.rollback();                        }                    } else {                        tx.rollback();                    }                    tx.close();                }                endGate.countDown();            }        };                t.start();        final Integer takeMapLock = 0;        t = new Thread() {            @Override            public void run() {                Random rng = new Random(Thread.currentThread().getId());                try {                    startGate.await();                } catch (InterruptedException e1) {                    Thread.currentThread().interrupt();                }                for (int j = 0; j < 10; j++) {                    int events = rng.nextInt(5) + 1;                    Transaction tx = channel.getTransaction();                    tx.begin();                    Event[] taken = new Event[events];                    int k;                    for (k = 0; k < events; k++) {                        taken[k] = channel.take();                        if (taken[k] == null)                            break;                    }                    if (rng.nextBoolean()) {                        try {                            tx.commit();                            for (Event e : taken) {                                if (e == null)                                    break;                                String index = new String(e.getBody());                                synchronized (takeMapLock) {                                    AtomicInteger remain = committedTakes.get(index);                                    if (remain == null) {                                        committedTakes.put(index, new AtomicInteger(1));                                    } else {                                        remain.incrementAndGet();                                    }                                }                            }                        } catch (ChannelException e) {                            System.out.print("takes commit failed");                            tx.rollback();                        }                    } else {                        tx.rollback();                    }                    tx.close();                }                endGate.countDown();            }        };                t.start();    }    startGate.countDown();    if (!endGate.await(20, TimeUnit.SECONDS)) {        Assert.fail("Not all threads ended succesfully");    }        Transaction tx = channel.getTransaction();    tx.begin();    Event e;        while ((e = channel.take()) != null) {        String index = new String(e.getBody());        AtomicInteger remain = committedPuts.get(index);        int post = remain.decrementAndGet();        if (post == 0) {            committedPuts.remove(index);        }    }    tx.commit();    tx.close();        for (Entry<String, AtomicInteger> takes : committedTakes.entrySet()) {        AtomicInteger count = committedPuts.get(takes.getKey());        if (count == null) {            Assert.fail("Putted data doesn't exist");        }        if (count.get() != takes.getValue().get()) {            Assert.fail(String.format("Mismatched put and take counts expected %d had %d", count.get(), takes.getValue().get()));        }        committedPuts.remove(takes.getKey());    }    if (!committedPuts.isEmpty()) {        Assert.fail("Puts still has entries remaining");    }}
0
public void run()
{    Long tid = Thread.currentThread().getId();    String strtid = tid.toString();    Random rng = new Random(tid);    try {        startGate.await();    } catch (InterruptedException e1) {        Thread.currentThread().interrupt();    }    for (int j = 0; j < 10; j++) {        int events = rng.nextInt(5) + 1;        Transaction tx = channel.getTransaction();        tx.begin();        for (int k = 0; k < events; k++) {            channel.put(EventBuilder.withBody(strtid.getBytes()));        }        if (rng.nextBoolean()) {            try {                tx.commit();                AtomicInteger tcount = committedPuts.get(strtid);                if (tcount == null) {                    committedPuts.put(strtid, new AtomicInteger(events));                } else {                    tcount.addAndGet(events);                }            } catch (ChannelException e) {                System.out.print("puts commit failed");                tx.rollback();            }        } else {            tx.rollback();        }        tx.close();    }    endGate.countDown();}
0
public void run()
{    Random rng = new Random(Thread.currentThread().getId());    try {        startGate.await();    } catch (InterruptedException e1) {        Thread.currentThread().interrupt();    }    for (int j = 0; j < 10; j++) {        int events = rng.nextInt(5) + 1;        Transaction tx = channel.getTransaction();        tx.begin();        Event[] taken = new Event[events];        int k;        for (k = 0; k < events; k++) {            taken[k] = channel.take();            if (taken[k] == null)                break;        }        if (rng.nextBoolean()) {            try {                tx.commit();                for (Event e : taken) {                    if (e == null)                        break;                    String index = new String(e.getBody());                    synchronized (takeMapLock) {                        AtomicInteger remain = committedTakes.get(index);                        if (remain == null) {                            committedTakes.put(index, new AtomicInteger(1));                        } else {                            remain.incrementAndGet();                        }                    }                }            } catch (ChannelException e) {                System.out.print("takes commit failed");                tx.rollback();            }        } else {            tx.rollback();        }        tx.close();    }    endGate.countDown();}
0
public void setUp()
{    channel = new MemoryChannel();}
0
public void testCommit() throws InterruptedException, EventDeliveryException
{    Event event;    Event event2;    Context context = new Context();    int putCounter = 0;    context.put("keep-alive", "1");    context.put("capacity", "100");    context.put("transactionCapacity", "50");    Configurables.configure(channel, context);    Transaction transaction = channel.getTransaction();    Assert.assertNotNull(transaction);    transaction.begin();    for (putCounter = 0; putCounter < 10; putCounter++) {        event = EventBuilder.withBody(("test event" + putCounter).getBytes());        channel.put(event);    }    transaction.commit();    transaction.close();    transaction = channel.getTransaction();    Assert.assertNotNull(transaction);    transaction = channel.getTransaction();    transaction.begin();    for (int i = 0; i < 10; i++) {        event2 = channel.take();        Assert.assertNotNull("lost an event", event2);        Assert.assertArrayEquals(event2.getBody(), ("test event" + i).getBytes());        }    event2 = channel.take();    Assert.assertNull("extra event found", event2);    transaction.commit();    transaction.close();}
0
public void testRollBack() throws InterruptedException, EventDeliveryException
{    Event event;    Event event2;    Context context = new Context();    int putCounter = 0;    context.put("keep-alive", "1");    Configurables.configure(channel, context);    Transaction transaction = channel.getTransaction();    Assert.assertNotNull(transaction);        transaction.begin();    for (putCounter = 0; putCounter < 10; putCounter++) {        event = EventBuilder.withBody(("test event" + putCounter).getBytes());        channel.put(event);    }    transaction.rollback();    transaction.close();        transaction = channel.getTransaction();    transaction.begin();    event2 = channel.take();    Assert.assertNull("extra event found", event2);    transaction.commit();    transaction.close();        transaction = channel.getTransaction();    transaction.begin();    for (putCounter = 0; putCounter < 10; putCounter++) {        event = EventBuilder.withBody(("test event" + putCounter).getBytes());        channel.put(event);    }    transaction.commit();    transaction.close();    transaction = channel.getTransaction();    Assert.assertNotNull(transaction);        transaction.begin();    for (int i = 0; i < 10; i++) {        event2 = channel.take();        Assert.assertNotNull("lost an event", event2);        Assert.assertArrayEquals(event2.getBody(), ("test event" + i).getBytes());    }    event2 = channel.take();    Assert.assertNull("extra event found", event2);    transaction.rollback();    transaction.close();        transaction = channel.getTransaction();    transaction.begin();    for (int i = 0; i < 10; i++) {        event2 = channel.take();        Assert.assertNotNull("lost an event", event2);        Assert.assertArrayEquals(event2.getBody(), ("test event" + i).getBytes());    }    event2 = channel.take();    Assert.assertNull("extra event found", event2);    transaction.rollback();    transaction.close();}
0
public void testReEntTxn() throws InterruptedException, EventDeliveryException
{    Event event;    Event event2;    Context context = new Context();    int putCounter = 0;    context.put("keep-alive", "1");    Configurables.configure(channel, context);    Transaction transaction = channel.getTransaction();    Assert.assertNotNull(transaction);        transaction.begin();    for (putCounter = 0; putCounter < 10; putCounter++) {                transaction.begin();        event = EventBuilder.withBody(("test event" + putCounter).getBytes());        channel.put(event);                transaction.commit();    }    transaction.commit();    transaction.close();    transaction = channel.getTransaction();    Assert.assertNotNull(transaction);    transaction.begin();    for (int i = 0; i < 10; i++) {        event2 = channel.take();        Assert.assertNotNull("lost an event", event2);        Assert.assertArrayEquals(event2.getBody(), ("test event" + i).getBytes());        }    event2 = channel.take();    Assert.assertNull("extra event found", event2);    transaction.commit();    transaction.close();}
0
public void testReEntTxnRollBack() throws InterruptedException, EventDeliveryException
{    Event event;    Event event2;    Context context = new Context();    int putCounter = 0;    context.put("keep-alive", "1");    Configurables.configure(channel, context);    Transaction transaction = channel.getTransaction();    Assert.assertNotNull(transaction);        transaction.begin();    for (putCounter = 0; putCounter < 10; putCounter++) {        event = EventBuilder.withBody(("test event" + putCounter).getBytes());        channel.put(event);    }    transaction.rollback();    transaction.close();        transaction = channel.getTransaction();    transaction.begin();    event2 = channel.take();    Assert.assertNull("extra event found", event2);    transaction.commit();    transaction.close();        transaction = channel.getTransaction();    transaction.begin();    for (putCounter = 0; putCounter < 10; putCounter++) {        event = EventBuilder.withBody(("test event" + putCounter).getBytes());        channel.put(event);    }    transaction.commit();    transaction.close();    transaction = channel.getTransaction();    Assert.assertNotNull(transaction);        transaction.begin();    for (int i = 0; i < 10; i++) {                transaction.begin();        event2 = channel.take();        Assert.assertNotNull("lost an event", event2);        Assert.assertArrayEquals(event2.getBody(), ("test event" + i).getBytes());                transaction.commit();    }    event2 = channel.take();    Assert.assertNull("extra event found", event2);    transaction.rollback();    transaction.close();        transaction = channel.getTransaction();    transaction.begin();    for (int i = 0; i < 10; i++) {        event2 = channel.take();        Assert.assertNotNull("lost an event", event2);        Assert.assertArrayEquals(event2.getBody(), ("test event" + i).getBytes());    }    event2 = channel.take();    Assert.assertNull("extra event found", event2);    transaction.rollback();    transaction.close();}
0
public void setUp() throws Exception
{    channels.clear();    channels.add(MockChannel.createMockChannel("ch1"));    channels.add(MockChannel.createMockChannel("ch2"));    channels.add(MockChannel.createMockChannel("ch3"));    config.put("type", "multiplexing");    config.put("header", "myheader");    config.put("optional.foo", "ch2 ch3");    config.put("optional.xyz", "ch1 ch3");    config.put("optional.zebra", "ch1 ch2");}
0
public void testSelection() throws Exception
{    config.put("mapping.foo", "ch1 ch2");    config.put("mapping.bar", "ch2 ch3");    config.put("mapping.xyz", "ch1 ch2 ch3");    config.put("default", "ch1 ch3");    selector = ChannelSelectorFactory.create(channels, config);    Assert.assertTrue(selector instanceof MultiplexingChannelSelector);    Event event1 = new MockEvent();    Map<String, String> header1 = new HashMap<String, String>();        header1.put("myheader", "foo");    event1.setHeaders(header1);    List<Channel> reqCh1 = selector.getRequiredChannels(event1);    Assert.assertEquals(2, reqCh1.size());    Assert.assertTrue(reqCh1.get(0).getName().equals("ch1"));    Assert.assertTrue(reqCh1.get(1).getName().equals("ch2"));    List<Channel> optCh1 = selector.getOptionalChannels(event1);    Assert.assertTrue(optCh1.size() == 1);        Assert.assertTrue(optCh1.get(0).getName().equals("ch3"));    Event event2 = new MockEvent();    Map<String, String> header2 = new HashMap<String, String>();        header2.put("myheader", "bar");    event2.setHeaders(header2);    List<Channel> reqCh2 = selector.getRequiredChannels(event2);    Assert.assertEquals(2, reqCh2.size());    Assert.assertTrue(reqCh2.get(0).getName().equals("ch2"));    Assert.assertTrue(reqCh2.get(1).getName().equals("ch3"));    List<Channel> optCh2 = selector.getOptionalChannels(event2);    Assert.assertTrue(optCh2.isEmpty());    Event event3 = new MockEvent();    Map<String, String> header3 = new HashMap<String, String>();        header3.put("myheader", "xyz");    event3.setHeaders(header3);    List<Channel> reqCh3 = selector.getRequiredChannels(event3);    Assert.assertEquals(3, reqCh3.size());    Assert.assertTrue(reqCh3.get(0).getName().equals("ch1"));    Assert.assertTrue(reqCh3.get(1).getName().equals("ch2"));    Assert.assertTrue(reqCh3.get(2).getName().equals("ch3"));    List<Channel> optCh3 = selector.getOptionalChannels(event3);        Assert.assertTrue(optCh3.size() == 0);}
0
public void testNoSelection() throws Exception
{    config.put("mapping.foo", "ch1 ch2");    config.put("mapping.bar", "ch2 ch3");    config.put("mapping.xyz", "ch1 ch2 ch3");    config.put("default", "ch1 ch3");    selector = ChannelSelectorFactory.create(channels, config);    Assert.assertTrue(selector instanceof MultiplexingChannelSelector);    Event noHeaderEvent = new MockEvent();    List<Channel> reqCh1 = selector.getRequiredChannels(noHeaderEvent);    List<Channel> optCh1 = selector.getOptionalChannels(noHeaderEvent);    Assert.assertEquals(2, reqCh1.size());    Assert.assertTrue(reqCh1.get(0).getName().equals("ch1"));    Assert.assertTrue(reqCh1.get(1).getName().equals("ch3"));    Assert.assertTrue(optCh1.isEmpty());    Map<String, String> header2 = new HashMap<String, String>();    header2.put("someheader", "foo");    Event invalidHeaderEvent = new MockEvent();    invalidHeaderEvent.setHeaders(header2);    List<Channel> reqCh2 = selector.getRequiredChannels(invalidHeaderEvent);    List<Channel> optCh2 = selector.getOptionalChannels(invalidHeaderEvent);    Assert.assertEquals(2, reqCh2.size());    Assert.assertTrue(reqCh2.get(0).getName().equals("ch1"));    Assert.assertTrue(reqCh2.get(1).getName().equals("ch3"));    Assert.assertTrue(optCh2.isEmpty());    Map<String, String> header3 = new HashMap<String, String>();    header3.put("myheader", "bar1");    Event unmatchedHeaderEvent = new MockEvent();    unmatchedHeaderEvent.setHeaders(header3);    List<Channel> reqCh3 = selector.getRequiredChannels(unmatchedHeaderEvent);    List<Channel> optCh3 = selector.getOptionalChannels(unmatchedHeaderEvent);    Assert.assertEquals(2, reqCh3.size());    Assert.assertTrue(reqCh3.get(0).getName().equals("ch1"));    Assert.assertTrue(reqCh3.get(1).getName().equals("ch3"));    Assert.assertTrue(optCh3.isEmpty());    Map<String, String> header4 = new HashMap<String, String>();    header4.put("myheader", "zebra");    Event zebraEvent = new MockEvent();    zebraEvent.setHeaders(header4);    List<Channel> reqCh4 = selector.getRequiredChannels(zebraEvent);    List<Channel> optCh4 = selector.getOptionalChannels(zebraEvent);    Assert.assertEquals(2, reqCh4.size());    Assert.assertTrue(reqCh4.get(0).getName().equals("ch1"));    Assert.assertTrue(reqCh4.get(1).getName().equals("ch3"));        Assert.assertTrue(optCh4.size() == 1);    Assert.assertTrue(optCh4.get(0).getName().equals("ch2"));    List<Channel> allChannels = selector.getAllChannels();    Assert.assertTrue(allChannels.size() == 3);    Assert.assertTrue(allChannels.get(0).getName().equals("ch1"));    Assert.assertTrue(allChannels.get(1).getName().equals("ch2"));    Assert.assertTrue(allChannels.get(2).getName().equals("ch3"));}
0
public void testNoDefault()
{    config.put("mapping.foo", "ch1 ch2");    config.put("mapping.bar", "ch2 ch3");    config.put("mapping.xyz", "ch1 ch2 ch3");    config.put("mapping.zebra", "ch2");    config.put("optional.zebra", "ch1 ch3");    selector = ChannelSelectorFactory.create(channels, config);    Assert.assertTrue(selector instanceof MultiplexingChannelSelector);    Event event1 = new MockEvent();    Map<String, String> header1 = new HashMap<String, String>();        header1.put("myheader", "foo");    event1.setHeaders(header1);    List<Channel> reqCh1 = selector.getRequiredChannels(event1);    Assert.assertEquals(2, reqCh1.size());    Assert.assertEquals("ch1", reqCh1.get(0).getName());    Assert.assertEquals("ch2", reqCh1.get(1).getName());    List<Channel> optCh1 = selector.getOptionalChannels(event1);    Assert.assertTrue(optCh1.size() == 1);        Assert.assertEquals("ch3", optCh1.get(0).getName());    Event event2 = new MockEvent();    Map<String, String> header2 = new HashMap<String, String>();        header2.put("myheader", "bar");    event2.setHeaders(header2);    List<Channel> reqCh2 = selector.getRequiredChannels(event2);    Assert.assertEquals(2, reqCh2.size());    Assert.assertEquals("ch2", reqCh2.get(0).getName());    Assert.assertEquals("ch3", reqCh2.get(1).getName());    List<Channel> optCh2 = selector.getOptionalChannels(event2);    Assert.assertTrue(optCh2.isEmpty());    Event event3 = new MockEvent();    Map<String, String> header3 = new HashMap<String, String>();        header3.put("myheader", "xyz");    event3.setHeaders(header3);    List<Channel> reqCh3 = selector.getRequiredChannels(event3);    Assert.assertEquals(3, reqCh3.size());    Assert.assertEquals("ch1", reqCh3.get(0).getName());    Assert.assertEquals("ch2", reqCh3.get(1).getName());    Assert.assertEquals("ch3", reqCh3.get(2).getName());    List<Channel> optCh3 = selector.getOptionalChannels(event3);        Assert.assertTrue(optCh3.isEmpty());    Event event4 = new MockEvent();    Map<String, String> header4 = new HashMap<String, String>();    header4.put("myheader", "zebra");    event4.setHeaders(header4);    List<Channel> reqCh4 = selector.getRequiredChannels(event4);    Assert.assertEquals(1, reqCh4.size());    Assert.assertEquals("ch2", reqCh4.get(0).getName());    List<Channel> optCh4 = selector.getOptionalChannels(event4);    Assert.assertEquals(2, optCh4.size());    Assert.assertEquals("ch1", optCh4.get(0).getName());    Assert.assertEquals("ch3", optCh4.get(1).getName());}
0
public void testNoMandatory()
{    config.put("default", "ch3");    config.put("optional.foo", "ch1 ch2");    config.put("optional.zebra", "ch2 ch3");    selector = ChannelSelectorFactory.create(channels, config);    Assert.assertTrue(selector instanceof MultiplexingChannelSelector);    Event event1 = new MockEvent();    Map<String, String> header1 = new HashMap<String, String>();        header1.put("myheader", "foo");    event1.setHeaders(header1);    List<Channel> reqCh1 = selector.getRequiredChannels(event1);    Assert.assertEquals(1, reqCh1.size());    Assert.assertEquals("ch3", reqCh1.get(0).getName());    List<Channel> optCh1 = selector.getOptionalChannels(event1);    Assert.assertEquals(2, optCh1.size());        Assert.assertEquals("ch1", optCh1.get(0).getName());    Assert.assertEquals("ch2", optCh1.get(1).getName());    Event event4 = new MockEvent();    Map<String, String> header4 = new HashMap<String, String>();    header4.put("myheader", "zebra");    event4.setHeaders(header4);    List<Channel> reqCh4 = selector.getRequiredChannels(event4);    Assert.assertEquals(1, reqCh4.size());    Assert.assertTrue(reqCh4.get(0).getName().equals("ch3"));    List<Channel> optCh4 = selector.getOptionalChannels(event4);            Assert.assertEquals(1, optCh4.size());    Assert.assertEquals("ch2", optCh4.get(0).getName());}
0
public void testOnlyOptional()
{    config.put("optional.foo", "ch1 ch2");    config.put("optional.zebra", "ch2 ch3");    selector = ChannelSelectorFactory.create(channels, config);    Assert.assertTrue(selector instanceof MultiplexingChannelSelector);    Event event1 = new MockEvent();    Map<String, String> header1 = new HashMap<String, String>();        header1.put("myheader", "foo");    event1.setHeaders(header1);    List<Channel> reqCh1 = selector.getRequiredChannels(event1);    Assert.assertTrue(reqCh1.isEmpty());    List<Channel> optCh1 = selector.getOptionalChannels(event1);    Assert.assertEquals(2, optCh1.size());        Event event4 = new MockEvent();    Map<String, String> header4 = new HashMap<String, String>();    header4.put("myheader", "zebra");    event4.setHeaders(header4);    List<Channel> reqCh4 = selector.getRequiredChannels(event4);    Assert.assertTrue(reqCh4.isEmpty());    List<Channel> optCh4 = selector.getOptionalChannels(event4);    Assert.assertEquals(2, optCh4.size());    Assert.assertEquals("ch2", optCh4.get(0).getName());    Assert.assertEquals("ch3", optCh4.get(1).getName());}
0
public void setUp() throws Exception
{    channels.clear();    channels.add(MockChannel.createMockChannel("ch1"));    channels.add(MockChannel.createMockChannel("ch2"));    channels.add(MockChannel.createMockChannel("ch3"));    channels.add(MockChannel.createMockChannel("ch4"));    selector = ChannelSelectorFactory.create(channels, new HashMap<String, String>());}
0
public void testReplicatingSelector() throws Exception
{    selector.configure(new Context());    List<Channel> channels = selector.getRequiredChannels(new MockEvent());    Assert.assertNotNull(channels);    Assert.assertEquals(4, channels.size());    Assert.assertEquals("ch1", channels.get(0).getName());    Assert.assertEquals("ch2", channels.get(1).getName());    Assert.assertEquals("ch3", channels.get(2).getName());    Assert.assertEquals("ch4", channels.get(3).getName());    List<Channel> optCh = selector.getOptionalChannels(new MockEvent());    Assert.assertEquals(0, optCh.size());}
0
public void testOptionalChannels() throws Exception
{    Context context = new Context();    context.put(ReplicatingChannelSelector.CONFIG_OPTIONAL, "ch1");    Configurables.configure(selector, context);    List<Channel> channels = selector.getRequiredChannels(new MockEvent());    Assert.assertNotNull(channels);    Assert.assertEquals(3, channels.size());    Assert.assertEquals("ch2", channels.get(0).getName());    Assert.assertEquals("ch3", channels.get(1).getName());    Assert.assertEquals("ch4", channels.get(2).getName());    List<Channel> optCh = selector.getOptionalChannels(new MockEvent());    Assert.assertEquals(1, optCh.size());    Assert.assertEquals("ch1", optCh.get(0).getName());}
0
public void testMultipleOptionalChannels() throws Exception
{    Context context = new Context();    context.put(ReplicatingChannelSelector.CONFIG_OPTIONAL, "ch1 ch4");    Configurables.configure(selector, context);    List<Channel> channels = selector.getRequiredChannels(new MockEvent());    Assert.assertNotNull(channels);    Assert.assertEquals(2, channels.size());    Assert.assertEquals("ch2", channels.get(0).getName());    Assert.assertEquals("ch3", channels.get(1).getName());    List<Channel> optCh = selector.getOptionalChannels(new MockEvent());    Assert.assertEquals(2, optCh.size());    Assert.assertEquals("ch1", optCh.get(0).getName());    Assert.assertEquals("ch4", optCh.get(1).getName());}
0
public void testMultipleOptionalChannelsSameChannelTwice() throws Exception
{    Context context = new Context();    context.put(ReplicatingChannelSelector.CONFIG_OPTIONAL, "ch1 ch4 ch1");    Configurables.configure(selector, context);    List<Channel> channels = selector.getRequiredChannels(new MockEvent());    Assert.assertNotNull(channels);    Assert.assertEquals(2, channels.size());    Assert.assertEquals("ch2", channels.get(0).getName());    Assert.assertEquals("ch3", channels.get(1).getName());    List<Channel> optCh = selector.getOptionalChannels(new MockEvent());    Assert.assertEquals(2, optCh.size());    Assert.assertEquals("ch1", optCh.get(0).getName());    Assert.assertEquals("ch4", optCh.get(1).getName());}
0
public void before()
{    tmpDir = Files.createTempDir();}
0
public void after()
{    for (File f : tmpDir.listFiles()) {        f.delete();    }    tmpDir.delete();}
0
public void testSimpleRead() throws IOException
{    File f1 = new File(tmpDir.getAbsolutePath() + "/file1");    Files.write("file1line1\nfile1line2\nfile1line3\nfile1line4\n" + "file1line5\nfile1line6\nfile1line7\nfile1line8\n", f1, Charsets.UTF_8);    SimpleTextLineEventReader reader = new SimpleTextLineEventReader(new FileReader(f1));    assertEquals("file1line1", bodyAsString(reader.readEvent()));    assertEquals("file1line2", bodyAsString(reader.readEvent()));    assertEquals("file1line3", bodyAsString(reader.readEvent()));    assertEquals("file1line4", bodyAsString(reader.readEvent()));    assertEquals("file1line5", bodyAsString(reader.readEvent()));    assertEquals("file1line6", bodyAsString(reader.readEvent()));    assertEquals("file1line7", bodyAsString(reader.readEvent()));    assertEquals("file1line8", bodyAsString(reader.readEvent()));    assertEquals(null, reader.readEvent());}
0
public void testBatchedReadsWithinAFile() throws IOException
{    File f1 = new File(tmpDir.getAbsolutePath() + "/file1");    Files.write("file1line1\nfile1line2\nfile1line3\nfile1line4\n" + "file1line5\nfile1line6\nfile1line7\nfile1line8\n", f1, Charsets.UTF_8);    SimpleTextLineEventReader reader = new SimpleTextLineEventReader(new FileReader(f1));    List<String> out = bodiesAsStrings(reader.readEvents(5));        assertEquals(5, out.size());    assertTrue(out.contains("file1line1"));    assertTrue(out.contains("file1line2"));    assertTrue(out.contains("file1line3"));    assertTrue(out.contains("file1line4"));    assertTrue(out.contains("file1line5"));}
0
public void testBatchedReadsAtFileBoundary() throws IOException
{    File f1 = new File(tmpDir.getAbsolutePath() + "/file1");    Files.write("file1line1\nfile1line2\nfile1line3\nfile1line4\n" + "file1line5\nfile1line6\nfile1line7\nfile1line8\n", f1, Charsets.UTF_8);    SimpleTextLineEventReader reader = new SimpleTextLineEventReader(new FileReader(f1));    List<String> out = bodiesAsStrings(reader.readEvents(10));        assertEquals(8, out.size());    assertTrue(out.contains("file1line1"));    assertTrue(out.contains("file1line2"));    assertTrue(out.contains("file1line3"));    assertTrue(out.contains("file1line4"));    assertTrue(out.contains("file1line5"));    assertTrue(out.contains("file1line6"));    assertTrue(out.contains("file1line7"));    assertTrue(out.contains("file1line8"));}
0
public void setup() throws IOException, InterruptedException
{    if (!WORK_DIR.isDirectory()) {        Files.createParentDirs(new File(WORK_DIR, "dummy"));    }        for (int i = 0; i < 4; i++) {        File fileName = new File(WORK_DIR, "file" + i);        StringBuilder sb = new StringBuilder();                for (int j = 0; j < i; j++) {            sb.append("file" + i + "line" + j + "\n");        }        Files.write(sb.toString(), fileName, Charsets.UTF_8);    }        Thread.sleep(1500L);    Files.write("\n", new File(WORK_DIR, "emptylineFile"), Charsets.UTF_8);}
0
public void tearDown()
{    deleteDir(WORK_DIR);}
0
private void deleteDir(File dir)
{        try {        FileUtils.deleteDirectory(dir);    } catch (IOException e) {            }}
1
private void processEventsWithReader(ReliableEventReader reader, int nEvents) throws IOException
{    List<Event> events;    do {        events = reader.readEvents(nEvents);        reader.commit();    } while (!events.isEmpty());}
0
private boolean checkLeftFilesInDir(File dir, String[] files)
{    List<File> actualFiles = listFiles(dir);    Set<String> expectedFiles = new HashSet<String>(Arrays.asList(files));        if (actualFiles.size() != expectedFiles.size()) {        return false;    }        for (File f : actualFiles) {        expectedFiles.remove(f.getName());    }    return expectedFiles.isEmpty();}
0
public void testIncludePattern() throws IOException
{    ReliableEventReader reader = new ReliableSpoolingFileEventReader.Builder().spoolDirectory(WORK_DIR).includePattern("^file2$").deletePolicy(DeletePolicy.IMMEDIATE.toString()).sourceCounter(new SourceCounter("test")).build();    String[] beforeFiles = { "file0", "file1", "file2", "file3", "emptylineFile" };    Assert.assertTrue("Expected " + beforeFiles.length + " files in working dir", checkLeftFilesInDir(WORK_DIR, beforeFiles));    processEventsWithReader(reader, 10);    String[] afterFiles = { "file0", "file1", "file3", "emptylineFile" };    Assert.assertTrue("Expected " + afterFiles.length + " files left in working dir", checkLeftFilesInDir(WORK_DIR, afterFiles));    Assert.assertTrue("Expected no files left in tracker dir", checkLeftFilesInDir(TRACKER_DIR, new String[0]));}
0
public void testIgnorePattern() throws IOException
{    ReliableEventReader reader = new ReliableSpoolingFileEventReader.Builder().spoolDirectory(WORK_DIR).ignorePattern("^file2$").deletePolicy(DeletePolicy.IMMEDIATE.toString()).sourceCounter(new SourceCounter("test")).build();    String[] beforeFiles = { "file0", "file1", "file2", "file3", "emptylineFile" };    Assert.assertTrue("Expected " + beforeFiles.length + " files in working dir", checkLeftFilesInDir(WORK_DIR, beforeFiles));    processEventsWithReader(reader, 10);    String[] files = { "file2" };    Assert.assertTrue("Expected " + files.length + " files left in working dir", checkLeftFilesInDir(WORK_DIR, files));    Assert.assertTrue("Expected no files left in tracker dir", checkLeftFilesInDir(TRACKER_DIR, new String[0]));}
0
public void testIncludeExcludePatternNoConflict() throws IOException
{                                ReliableEventReader reader = new ReliableSpoolingFileEventReader.Builder().spoolDirectory(WORK_DIR).ignorePattern("^file[013]$").includePattern("^file2$").deletePolicy(DeletePolicy.IMMEDIATE.toString()).sourceCounter(new SourceCounter("test")).build();    String[] beforeFiles = { "file0", "file1", "file2", "file3", "emptylineFile" };    Assert.assertTrue("Expected " + beforeFiles.length + " files in working dir", checkLeftFilesInDir(WORK_DIR, beforeFiles));    processEventsWithReader(reader, 10);    String[] files = { "file0", "file1", "file3", "emptylineFile" };    Assert.assertTrue("Expected " + files.length + " files left in working dir", checkLeftFilesInDir(WORK_DIR, files));    Assert.assertTrue("Expected no files left in tracker dir", checkLeftFilesInDir(TRACKER_DIR, new String[0]));}
0
public void testIncludeExcludePatternConflict() throws IOException
{                        ReliableEventReader reader = new ReliableSpoolingFileEventReader.Builder().spoolDirectory(WORK_DIR).ignorePattern("^file2$").includePattern("^file2$").deletePolicy(DeletePolicy.IMMEDIATE.toString()).sourceCounter(new SourceCounter("test")).build();    String[] beforeFiles = { "file0", "file1", "file2", "file3", "emptylineFile" };    Assert.assertTrue("Expected " + beforeFiles.length + " files in working dir", checkLeftFilesInDir(WORK_DIR, beforeFiles));    processEventsWithReader(reader, 10);    String[] files = { "file0", "file1", "file2", "file3", "emptylineFile" };    Assert.assertTrue("Expected " + files.length + " files left in working dir", checkLeftFilesInDir(WORK_DIR, files));    Assert.assertTrue("Expected no files left in tracker dir", checkLeftFilesInDir(TRACKER_DIR, new String[0]));}
0
public void testRepeatedCallsWithCommitAlways() throws IOException
{    ReliableEventReader reader = new ReliableSpoolingFileEventReader.Builder().spoolDirectory(WORK_DIR).sourceCounter(new SourceCounter("test")).build();    final int expectedLines = 1 + 1 + 2 + 3 + 1;    int seenLines = 0;    for (int i = 0; i < 10; i++) {        List<Event> events = reader.readEvents(10);        seenLines += events.size();        reader.commit();    }    Assert.assertEquals(expectedLines, seenLines);}
0
public void testRepeatedCallsWithCommitOnSuccess() throws IOException
{    String trackerDirPath = SpoolDirectorySourceConfigurationConstants.DEFAULT_TRACKER_DIR;    File trackerDir = new File(WORK_DIR, trackerDirPath);    ReliableEventReader reader = new ReliableSpoolingFileEventReader.Builder().spoolDirectory(WORK_DIR).trackerDirPath(trackerDirPath).sourceCounter(new SourceCounter("test")).build();    final int expectedLines = 1 + 1 + 2 + 3 + 1;    int seenLines = 0;    for (int i = 0; i < 10; i++) {        List<Event> events = reader.readEvents(10);        int numEvents = events.size();        if (numEvents > 0) {            seenLines += numEvents;            reader.commit();                        File[] files = trackerDir.listFiles();            Assert.assertNotNull(files);            Assert.assertTrue("Expected tracker files in tracker dir " + trackerDir.getAbsolutePath(), files.length > 0);        }    }    Assert.assertEquals(expectedLines, seenLines);}
0
public void testFileDeletion() throws IOException
{    ReliableEventReader reader = new ReliableSpoolingFileEventReader.Builder().spoolDirectory(WORK_DIR).deletePolicy(DeletePolicy.IMMEDIATE.name()).sourceCounter(new SourceCounter("test")).build();    List<File> before = listFiles(WORK_DIR);    Assert.assertEquals("Expected 5, not: " + before, 5, before.size());    List<Event> events;    do {        events = reader.readEvents(10);        reader.commit();    } while (!events.isEmpty());    List<File> after = listFiles(WORK_DIR);    Assert.assertEquals("Expected 0, not: " + after, 0, after.size());    List<File> trackerFiles = listFiles(new File(WORK_DIR, SpoolDirectorySourceConfigurationConstants.DEFAULT_TRACKER_DIR));    Assert.assertEquals("Expected 0, not: " + trackerFiles, 0, trackerFiles.size());}
0
public void testNullConsumeOrder() throws IOException
{    new ReliableSpoolingFileEventReader.Builder().spoolDirectory(WORK_DIR).consumeOrder(null).sourceCounter(new SourceCounter("test")).build();}
0
public void testConsumeFileRandomly() throws IOException
{    ReliableEventReader reader = new ReliableSpoolingFileEventReader.Builder().spoolDirectory(WORK_DIR).consumeOrder(ConsumeOrder.RANDOM).sourceCounter(new SourceCounter("test")).build();    File fileName = new File(WORK_DIR, "new-file");    FileUtils.write(fileName, "New file created in the end. Shoud be read randomly.\n");    Set<String> actual = Sets.newHashSet();    readEventsForFilesInDir(WORK_DIR, reader, actual);    Set<String> expected = Sets.newHashSet();    createExpectedFromFilesInSetup(expected);    expected.add("");    expected.add("New file created in the end. Shoud be read randomly.");    Assert.assertEquals(expected, actual);}
0
public void testConsumeFileRandomlyNewFile() throws Exception
{        if (SystemUtils.IS_OS_WINDOWS) {        return;    }    final ReliableEventReader reader = new ReliableSpoolingFileEventReader.Builder().spoolDirectory(WORK_DIR).consumeOrder(ConsumeOrder.RANDOM).sourceCounter(new SourceCounter("test")).build();    File fileName = new File(WORK_DIR, "new-file");    FileUtils.write(fileName, "New file created in the end. Shoud be read randomly.\n");    Set<String> expected = Sets.newHashSet();    int totalFiles = WORK_DIR.listFiles().length;    final Set<String> actual = Sets.newHashSet();    ExecutorService executor = Executors.newSingleThreadExecutor();    final Semaphore semaphore1 = new Semaphore(0);    final Semaphore semaphore2 = new Semaphore(0);    Future<Void> wait = executor.submit(new Callable<Void>() {        @Override        public Void call() throws Exception {            readEventsForFilesInDir(WORK_DIR, reader, actual, semaphore1, semaphore2);            return null;        }    });    semaphore1.acquire();    File finalFile = new File(WORK_DIR, "t-file");    FileUtils.write(finalFile, "Last file");    semaphore2.release();    wait.get();    int listFilesCount = ((ReliableSpoolingFileEventReader) reader).getListFilesCount();    finalFile.delete();    createExpectedFromFilesInSetup(expected);    expected.add("");    expected.add("New file created in the end. Shoud be read randomly.");    expected.add("Last file");    Assert.assertTrue(listFilesCount < (totalFiles + 2));    Assert.assertEquals(expected, actual);}
0
public Void call() throws Exception
{    readEventsForFilesInDir(WORK_DIR, reader, actual, semaphore1, semaphore2);    return null;}
0
public void testConsumeFileOldest() throws IOException, InterruptedException
{    ReliableEventReader reader = new ReliableSpoolingFileEventReader.Builder().spoolDirectory(WORK_DIR).consumeOrder(ConsumeOrder.OLDEST).sourceCounter(new SourceCounter("test")).build();    File file1 = new File(WORK_DIR, "new-file1");    File file2 = new File(WORK_DIR, "new-file2");    File file3 = new File(WORK_DIR, "new-file3");    Thread.sleep(1000L);    FileUtils.write(file2, "New file2 created.\n");    Thread.sleep(1000L);    FileUtils.write(file1, "New file1 created.\n");    Thread.sleep(1000L);    FileUtils.write(file3, "New file3 created.\n");        List<String> actual = Lists.newLinkedList();    readEventsForFilesInDir(WORK_DIR, reader, actual);    List<String> expected = Lists.newLinkedList();    createExpectedFromFilesInSetup(expected);        expected.add("");    expected.add("New file2 created.");    expected.add("New file1 created.");    expected.add("New file3 created.");    Assert.assertEquals(expected, actual);}
0
public void testConsumeFileYoungest() throws IOException, InterruptedException
{    ReliableEventReader reader = new ReliableSpoolingFileEventReader.Builder().spoolDirectory(WORK_DIR).consumeOrder(ConsumeOrder.YOUNGEST).sourceCounter(new SourceCounter("test")).build();    File file1 = new File(WORK_DIR, "new-file1");    File file2 = new File(WORK_DIR, "new-file2");    File file3 = new File(WORK_DIR, "new-file3");    Thread.sleep(1000L);    FileUtils.write(file2, "New file2 created.\n");    Thread.sleep(1000L);    FileUtils.write(file3, "New file3 created.\n");    Thread.sleep(1000L);    FileUtils.write(file1, "New file1 created.\n");        List<String> actual = Lists.newLinkedList();    readEventsForFilesInDir(WORK_DIR, reader, actual);    List<String> expected = Lists.newLinkedList();    createExpectedFromFilesInSetup(expected);    Collections.sort(expected);        expected.add(0, "");    expected.add(0, "New file2 created.");    expected.add(0, "New file3 created.");    expected.add(0, "New file1 created.");    Assert.assertEquals(expected, actual);}
0
public void testConsumeFileOldestWithLexicographicalComparision() throws IOException, InterruptedException
{    ReliableEventReader reader = new ReliableSpoolingFileEventReader.Builder().spoolDirectory(WORK_DIR).consumeOrder(ConsumeOrder.OLDEST).sourceCounter(new SourceCounter("test")).build();    File file1 = new File(WORK_DIR, "new-file1");    File file2 = new File(WORK_DIR, "new-file2");    File file3 = new File(WORK_DIR, "new-file3");    Thread.sleep(1000L);    FileUtils.write(file3, "New file3 created.\n");    FileUtils.write(file2, "New file2 created.\n");    FileUtils.write(file1, "New file1 created.\n");    file1.setLastModified(file3.lastModified());    file1.setLastModified(file2.lastModified());            List<String> actual = Lists.newLinkedList();    readEventsForFilesInDir(WORK_DIR, reader, actual);    List<String> expected = Lists.newLinkedList();    createExpectedFromFilesInSetup(expected);        expected.add("");    expected.add("New file1 created.");    expected.add("New file2 created.");    expected.add("New file3 created.");    Assert.assertEquals(expected, actual);}
0
public void testConsumeFileYoungestWithLexicographicalComparision() throws IOException, InterruptedException
{    ReliableEventReader reader = new ReliableSpoolingFileEventReader.Builder().spoolDirectory(WORK_DIR).consumeOrder(ConsumeOrder.YOUNGEST).sourceCounter(new SourceCounter("test")).build();    File file1 = new File(WORK_DIR, "new-file1");    File file2 = new File(WORK_DIR, "new-file2");    File file3 = new File(WORK_DIR, "new-file3");    Thread.sleep(1000L);    FileUtils.write(file1, "New file1 created.\n");    FileUtils.write(file2, "New file2 created.\n");    FileUtils.write(file3, "New file3 created.\n");    file1.setLastModified(file3.lastModified());    file1.setLastModified(file2.lastModified());            List<String> actual = Lists.newLinkedList();    readEventsForFilesInDir(WORK_DIR, reader, actual);    List<String> expected = Lists.newLinkedList();    createExpectedFromFilesInSetup(expected);        expected.add(0, "");    expected.add(0, "New file3 created.");    expected.add(0, "New file2 created.");    expected.add(0, "New file1 created.");    Assert.assertEquals(expected, actual);}
0
public void testLargeNumberOfFilesOLDEST() throws IOException
{    templateTestForRecursiveDirs(ConsumeOrder.OLDEST, null, 3, 3, 37, TrackingPolicy.RENAME);}
0
public void testLargeNumberOfFilesYOUNGEST() throws IOException
{    templateTestForRecursiveDirs(ConsumeOrder.YOUNGEST, Comparator.reverseOrder(), 3, 3, 37, TrackingPolicy.RENAME);}
0
public void testLargeNumberOfFilesRANDOM() throws IOException
{    templateTestForRecursiveDirs(ConsumeOrder.RANDOM, null, 3, 3, 37, TrackingPolicy.RENAME);}
0
public void testLargeNumberOfFilesOLDESTTrackerDir() throws IOException
{    templateTestForRecursiveDirs(ConsumeOrder.OLDEST, null, 3, 3, 10, TrackingPolicy.TRACKER_DIR);}
0
public void testLargeNumberOfFilesYOUNGESTTrackerDir() throws IOException
{    templateTestForRecursiveDirs(ConsumeOrder.YOUNGEST, Comparator.reverseOrder(), 3, 3, 10, TrackingPolicy.TRACKER_DIR);}
0
public void testLargeNumberOfFilesRANDOMTrackerDir() throws IOException
{    templateTestForRecursiveDirs(ConsumeOrder.RANDOM, null, 3, 3, 10, TrackingPolicy.TRACKER_DIR);}
0
public void testZeroByteTrackerFile() throws IOException
{    String trackerDirPath = SpoolDirectorySourceConfigurationConstants.DEFAULT_TRACKER_DIR;    File trackerDir = new File(WORK_DIR, trackerDirPath);    if (!trackerDir.exists()) {        trackerDir.mkdir();    }    File trackerFile = new File(trackerDir, ReliableSpoolingFileEventReader.metaFileName);    if (trackerFile.exists()) {        trackerFile.delete();    }    trackerFile.createNewFile();    ReliableEventReader reader = new ReliableSpoolingFileEventReader.Builder().spoolDirectory(WORK_DIR).trackerDirPath(trackerDirPath).sourceCounter(new SourceCounter("test")).build();    final int expectedLines = 1;    int seenLines = 0;    List<Event> events = reader.readEvents(10);    int numEvents = events.size();    if (numEvents > 0) {        seenLines += numEvents;        reader.commit();    }        Assert.assertEquals(expectedLines, seenLines);}
0
private void templateTestForRecursiveDirs(ConsumeOrder order, Comparator<Long> comparator, int depth, int dirNum, int fileNum, TrackingPolicy trackingPolicy) throws IOException
{    File dir = null;    try {        dir = new File("target/test/work/" + this.getClass().getSimpleName() + "_large");        Files.createParentDirs(new File(dir, "dummy"));        ReliableEventReader reader = new ReliableSpoolingFileEventReader.Builder().spoolDirectory(dir).consumeOrder(order).trackingPolicy(trackingPolicy.toString()).recursiveDirectorySearch(true).sourceCounter(new SourceCounter("test")).build();        Map<Long, List<String>> expected;        if (comparator == null) {            expected = new TreeMap<Long, List<String>>();        } else {            expected = new TreeMap<Long, List<String>>(comparator);        }        createMultilevelFiles(dir, 0, depth, dirNum, fileNum, expected, new MutableLong(0L));        Collection<String> expectedColl;        int index = 0;        if (order == ConsumeOrder.RANDOM) {            expectedColl = Sets.newHashSet();        } else {            expectedColl = new ArrayList<>();        }        for (Entry<Long, List<String>> entry : expected.entrySet()) {            Collections.sort(entry.getValue());            expectedColl.addAll(entry.getValue());        }        int expNum = expectedColl.size();        int actualNum = 0;        for (int i = 0; i < expNum; i++) {            List<Event> events;            events = reader.readEvents(10);            for (Event e : events) {                actualNum++;                if (order == ConsumeOrder.RANDOM) {                    Assert.assertTrue(expectedColl.remove(new String(e.getBody())));                } else {                    String exp = ((ArrayList<String>) expectedColl).get(index);                    String actual = new String(e.getBody());                    Assert.assertEquals(exp, actual);                    index++;                }            }            reader.commit();        }        Assert.assertEquals(expNum, actualNum);    } finally {        deleteDir(dir);    }}
0
private void createMultilevelFiles(File dir, int currDepth, int maxDepth, int dirNum, int fileNum, Map<Long, List<String>> expected, MutableLong id) throws IOException
{    if (currDepth == maxDepth) {        createFiles(dir, fileNum, expected, id);    } else {        for (int i = 0; i < dirNum; i++) {            File nextDir = new File(dir, "dir-" + i);            nextDir.mkdirs();            createMultilevelFiles(nextDir, currDepth + 1, maxDepth, dirNum, fileNum, expected, id);        }    }}
0
private void createFiles(File dir, int fileNum, Map<Long, List<String>> expected, MutableLong id) throws IOException
{    for (int i = 0; i < fileNum; i++) {        File f = new File(dir, "file-" + id);        String data = f.getPath();        Files.write(data, f, Charsets.UTF_8);        long lastMod = id.longValue() * 10000L;        f.setLastModified(lastMod);        if (expected.containsKey(f.lastModified())) {            expected.get(f.lastModified()).add(data);        } else {            expected.put(f.lastModified(), Lists.newArrayList(data));        }        id.increment();    }}
0
private void readEventsForFilesInDir(File dir, ReliableEventReader reader, Collection<String> actual) throws IOException
{    readEventsForFilesInDir(dir, reader, actual, null, null);}
0
private void readEventsForFilesInDir(File dir, ReliableEventReader reader, Collection<String> actual, Semaphore semaphore1, Semaphore semaphore2) throws IOException
{    List<Event> events;    boolean executed = false;    for (int i = 0; i < listFiles(dir).size(); i++) {        events = reader.readEvents(10);        for (Event e : events) {            actual.add(new String(e.getBody()));        }        reader.commit();        try {            if (!executed) {                executed = true;                if (semaphore1 != null) {                    semaphore1.release();                }                if (semaphore2 != null) {                    semaphore2.acquire();                }            }        } catch (Exception ex) {            throw new IOException(ex);        }    }}
0
private void createExpectedFromFilesInSetup(Collection<String> expected)
{    expected.add("");    for (int i = 0; i < 4; i++) {        for (int j = 0; j < i; j++) {            expected.add("file" + i + "line" + j);        }    }}
0
private static List<File> listFiles(File dir)
{    List<File> files = Lists.newArrayList(dir.listFiles(new FileFilter() {        @Override        public boolean accept(File pathname) {            return !pathname.isDirectory();        }    }));    return files;}
0
public boolean accept(File pathname)
{    return !pathname.isDirectory();}
0
 static String bodyAsString(Event event)
{    return new String(event.getBody());}
0
 static List<String> bodiesAsStrings(List<Event> events)
{    List<String> bodies = Lists.newArrayListWithCapacity(events.size());    for (Event event : events) {        bodies.add(bodyAsString(event));    }    return bodies;}
0
private ReliableSpoolingFileEventReader getParser(int maxLineLength)
{    Context ctx = new Context();    ctx.put(LineDeserializer.MAXLINE_KEY, Integer.toString(maxLineLength));    ReliableSpoolingFileEventReader parser;    try {        parser = new ReliableSpoolingFileEventReader.Builder().spoolDirectory(tmpDir).completedSuffix(completedSuffix).deserializerContext(ctx).sourceCounter(new SourceCounter("dummy")).build();    } catch (IOException ioe) {        throw Throwables.propagate(ioe);    }    return parser;}
0
private ReliableSpoolingFileEventReader getParser()
{    return getParser(bufferMaxLineLength);}
0
private FileFilter directoryFilter()
{    return new FileFilter() {        public boolean accept(File candidate) {            if (candidate.isDirectory()) {                return false;            }            return true;        }    };}
0
public boolean accept(File candidate)
{    if (candidate.isDirectory()) {        return false;    }    return true;}
0
public void setUp()
{    tmpDir = Files.createTempDir();}
0
public void tearDown()
{    for (File f : tmpDir.listFiles()) {        if (f.isDirectory()) {            for (File sdf : f.listFiles()) {                sdf.delete();            }        }        f.delete();    }    tmpDir.delete();}
0
public void testBasicSpooling() throws IOException
{    File f1 = new File(tmpDir.getAbsolutePath() + "/file1");    File f2 = new File(tmpDir.getAbsolutePath() + "/file2");    File f3 = new File(tmpDir.getAbsolutePath() + "/file3");    Files.write("file1line1\nfile1line2\n", f1, Charsets.UTF_8);    Files.write("file2line1\nfile2line2\n", f2, Charsets.UTF_8);    Files.write("file3line1\nfile3line2\n", f3, Charsets.UTF_8);    ReliableSpoolingFileEventReader parser = getParser();    List<String> out = Lists.newArrayList();    for (int i = 0; i < 6; i++) {                String body = bodyAsString(parser.readEvent());                out.add(body);        parser.commit();    }        assertTrue(out.contains("file1line1"));    assertTrue(out.contains("file1line2"));    assertTrue(out.contains("file2line1"));    assertTrue(out.contains("file2line2"));    assertTrue(out.contains("file3line1"));    assertTrue(out.contains("file3line2"));    List<File> outFiles = Lists.newArrayList(tmpDir.listFiles(directoryFilter()));    assertEquals(3, outFiles.size());        assertTrue(outFiles.contains(new File(tmpDir + "/file1" + completedSuffix)));    assertTrue(outFiles.contains(new File(tmpDir + "/file2" + completedSuffix)));    assertTrue(outFiles.contains(new File(tmpDir + "/file3")));}
1
public void testInitiallyEmptyDirectory() throws IOException
{    ReliableSpoolingFileEventReader parser = getParser();    assertNull(parser.readEvent());    assertEquals(0, parser.readEvents(10).size());    File f1 = new File(tmpDir.getAbsolutePath() + "/file1");    Files.write("file1line1\nfile1line2\n", f1, Charsets.UTF_8);    List<String> out = bodiesAsStrings(parser.readEvents(2));    parser.commit();        assertTrue(out.contains("file1line1"));    assertTrue(out.contains("file1line2"));    File f2 = new File(tmpDir.getAbsolutePath() + "/file2");    Files.write("file2line1\nfile2line2\n", f2, Charsets.UTF_8);        parser.readEvent();    parser.commit();    List<File> outFiles = Lists.newArrayList(tmpDir.listFiles(directoryFilter()));    assertEquals(2, outFiles.size());    assertTrue(outFiles.contains(new File(tmpDir + "/file1" + completedSuffix)));    assertTrue(outFiles.contains(new File(tmpDir + "/file2")));}
0
public void testFileChangesDuringRead() throws IOException
{    File f1 = new File(tmpDir.getAbsolutePath() + "/file1");    Files.write("file1line1\nfile1line2\n", f1, Charsets.UTF_8);    ReliableSpoolingFileEventReader parser1 = getParser();    List<String> out = Lists.newArrayList();    out.addAll(bodiesAsStrings(parser1.readEvents(2)));    parser1.commit();    assertEquals(2, out.size());    assertTrue(out.contains("file1line1"));    assertTrue(out.contains("file1line2"));    Files.append("file1line3\n", f1, Charsets.UTF_8);    out.add(bodyAsString(parser1.readEvent()));    parser1.commit();    out.add(bodyAsString(parser1.readEvent()));    parser1.commit();}
0
public void testDestinationExistsAndSameFileWindows() throws IOException
{    System.setProperty("os.name", "Some version of Windows");    File f1 = new File(tmpDir.getAbsolutePath() + "/file1");    File f1Completed = new File(tmpDir.getAbsolutePath() + "/file1" + completedSuffix);    Files.write("file1line1\nfile1line2\n", f1, Charsets.UTF_8);    Files.write("file1line1\nfile1line2\n", f1Completed, Charsets.UTF_8);    ReliableSpoolingFileEventReader parser = getParser();    List<String> out = Lists.newArrayList();    for (int i = 0; i < 2; i++) {        out.add(bodyAsString(parser.readEvent()));        parser.commit();    }    File f2 = new File(tmpDir.getAbsolutePath() + "/file2");    Files.write("file2line1\nfile2line2\n", f2, Charsets.UTF_8);    for (int i = 0; i < 2; i++) {        out.add(bodyAsString(parser.readEvent()));        parser.commit();    }        assertEquals(4, out.size());    assertTrue(out.contains("file1line1"));    assertTrue(out.contains("file1line2"));    assertTrue(out.contains("file2line1"));    assertTrue(out.contains("file2line2"));        List<File> outFiles = Lists.newArrayList(tmpDir.listFiles(directoryFilter()));    assertEquals(2, outFiles.size());    assertTrue(outFiles.contains(new File(tmpDir + "/file2")));    assertTrue(outFiles.contains(new File(tmpDir + "/file1" + completedSuffix)));}
0
public void testDestinationExistsAndSameFileNotOnWindows() throws IOException
{    System.setProperty("os.name", "Some version of Linux");    File f1 = new File(tmpDir.getAbsolutePath() + "/file1");    File f1Completed = new File(tmpDir.getAbsolutePath() + "/file1" + completedSuffix);    Files.write("file1line1\nfile1line2\n", f1, Charsets.UTF_8);    Files.write("file1line1\nfile1line2\n", f1Completed, Charsets.UTF_8);    ReliableSpoolingFileEventReader parser = getParser();    List<String> out = Lists.newArrayList();    for (int i = 0; i < 2; i++) {        out.add(bodyAsString(parser.readEvent()));        parser.commit();    }    File f2 = new File(tmpDir.getAbsolutePath() + "/file2");    Files.write("file2line1\nfile2line2\n", f2, Charsets.UTF_8);    for (int i = 0; i < 2; i++) {        out.add(bodyAsString(parser.readEvent()));        parser.commit();    }}
0
public void testBasicCommitFailure() throws IOException
{    File f1 = new File(tmpDir.getAbsolutePath() + "/file1");    Files.write("file1line1\nfile1line2\nfile1line3\nfile1line4\n" + "file1line5\nfile1line6\nfile1line7\nfile1line8\n" + "file1line9\nfile1line10\nfile1line11\nfile1line12\n", f1, Charsets.UTF_8);    ReliableSpoolingFileEventReader parser = getParser();    List<String> out1 = bodiesAsStrings(parser.readEvents(4));    assertTrue(out1.contains("file1line1"));    assertTrue(out1.contains("file1line2"));    assertTrue(out1.contains("file1line3"));    assertTrue(out1.contains("file1line4"));    List<String> out2 = bodiesAsStrings(parser.readEvents(4));    assertTrue(out2.contains("file1line1"));    assertTrue(out2.contains("file1line2"));    assertTrue(out2.contains("file1line3"));    assertTrue(out2.contains("file1line4"));    parser.commit();    List<String> out3 = bodiesAsStrings(parser.readEvents(4));    assertTrue(out3.contains("file1line5"));    assertTrue(out3.contains("file1line6"));    assertTrue(out3.contains("file1line7"));    assertTrue(out3.contains("file1line8"));    parser.commit();    List<String> out4 = bodiesAsStrings(parser.readEvents(4));    assertEquals(4, out4.size());    assertTrue(out4.contains("file1line9"));    assertTrue(out4.contains("file1line10"));    assertTrue(out4.contains("file1line11"));    assertTrue(out4.contains("file1line12"));}
0
public void testBasicCommitFailureAndBufferSizeChanges() throws IOException
{    File f1 = new File(tmpDir.getAbsolutePath() + "/file1");    Files.write("file1line1\nfile1line2\nfile1line3\nfile1line4\n" + "file1line5\nfile1line6\nfile1line7\nfile1line8\n" + "file1line9\nfile1line10\nfile1line11\nfile1line12\n", f1, Charsets.UTF_8);    ReliableSpoolingFileEventReader parser = getParser();    List<String> out1 = bodiesAsStrings(parser.readEvents(5));    assertTrue(out1.contains("file1line1"));    assertTrue(out1.contains("file1line2"));    assertTrue(out1.contains("file1line3"));    assertTrue(out1.contains("file1line4"));    assertTrue(out1.contains("file1line5"));    List<String> out2 = bodiesAsStrings(parser.readEvents(2));    assertTrue(out2.contains("file1line1"));    assertTrue(out2.contains("file1line2"));    parser.commit();    List<String> out3 = bodiesAsStrings(parser.readEvents(2));    assertTrue(out3.contains("file1line3"));    assertTrue(out3.contains("file1line4"));    parser.commit();    List<String> out4 = bodiesAsStrings(parser.readEvents(2));    assertTrue(out4.contains("file1line5"));    assertTrue(out4.contains("file1line6"));    parser.commit();    List<String> out5 = bodiesAsStrings(parser.readEvents(2));    assertTrue(out5.contains("file1line7"));    assertTrue(out5.contains("file1line8"));    parser.commit();    List<String> out6 = bodiesAsStrings(parser.readEvents(15));    assertTrue(out6.contains("file1line9"));    assertTrue(out6.contains("file1line10"));    assertTrue(out6.contains("file1line11"));    assertTrue(out6.contains("file1line12"));}
0
public void testDestinationExistsAndDifferentFile() throws IOException
{    File f1 = new File(tmpDir.getAbsolutePath() + "/file1");    File f1Completed = new File(tmpDir.getAbsolutePath() + "/file1" + completedSuffix);    Files.write("file1line1\nfile1line2\n", f1, Charsets.UTF_8);    Files.write("file1line1\nfile1XXXe2\n", f1Completed, Charsets.UTF_8);    ReliableSpoolingFileEventReader parser = getParser();    List<String> out = Lists.newArrayList();    for (int i = 0; i < 2; i++) {        out.add(bodyAsString(parser.readEvent()));        parser.commit();    }    File f2 = new File(tmpDir.getAbsolutePath() + "/file2");    Files.write("file2line1\nfile2line2\n", f2, Charsets.UTF_8);    for (int i = 0; i < 2; i++) {        out.add(bodyAsString(parser.readEvent()));        parser.commit();    }}
0
public void testBehaviorWithEmptyFile() throws IOException
{    File f1 = new File(tmpDir.getAbsolutePath() + "/file0");    Files.touch(f1);    ReliableSpoolingFileEventReader parser = getParser();    File f2 = new File(tmpDir.getAbsolutePath() + "/file1");    Files.write("file1line1\nfile1line2\nfile1line3\nfile1line4\n" + "file1line5\nfile1line6\nfile1line7\nfile1line8\n", f2, Charsets.UTF_8);        Event event = parser.readEvent();    assertEquals(0, event.getBody().length);    List<String> out = bodiesAsStrings(parser.readEvents(8));    parser.commit();    assertEquals(8, out.size());    assertTrue(out.contains("file1line1"));    assertTrue(out.contains("file1line2"));    assertTrue(out.contains("file1line3"));    assertTrue(out.contains("file1line4"));    assertTrue(out.contains("file1line5"));    assertTrue(out.contains("file1line6"));    assertTrue(out.contains("file1line7"));    assertTrue(out.contains("file1line8"));    assertNull(parser.readEvent());        List<File> outFiles = Lists.newArrayList(tmpDir.listFiles(directoryFilter()));    assertEquals(2, outFiles.size());    assertTrue("Outfiles should have file0 & file1: " + outFiles, outFiles.contains(new File(tmpDir + "/file0" + completedSuffix)));    assertTrue("Outfiles should have file0 & file1: " + outFiles, outFiles.contains(new File(tmpDir + "/file1" + completedSuffix)));}
0
public void testBatchedReadsWithinAFile() throws IOException
{    File f1 = new File(tmpDir.getAbsolutePath() + "/file1");    Files.write("file1line1\nfile1line2\nfile1line3\nfile1line4\n" + "file1line5\nfile1line6\nfile1line7\nfile1line8\n", f1, Charsets.UTF_8);    ReliableSpoolingFileEventReader parser = getParser();    List<String> out = bodiesAsStrings(parser.readEvents(5));    parser.commit();        assertEquals(5, out.size());    assertTrue(out.contains("file1line1"));    assertTrue(out.contains("file1line2"));    assertTrue(out.contains("file1line3"));    assertTrue(out.contains("file1line4"));    assertTrue(out.contains("file1line5"));}
0
public void testBatchedReadsAcrossFileBoundary() throws IOException
{    File f1 = new File(tmpDir.getAbsolutePath() + "/file1");    Files.write("file1line1\nfile1line2\nfile1line3\nfile1line4\n" + "file1line5\nfile1line6\nfile1line7\nfile1line8\n", f1, Charsets.UTF_8);    ReliableSpoolingFileEventReader parser = getParser();    List<String> out1 = bodiesAsStrings(parser.readEvents(5));    parser.commit();    File f2 = new File(tmpDir.getAbsolutePath() + "/file2");    Files.write("file2line1\nfile2line2\nfile2line3\nfile2line4\n" + "file2line5\nfile2line6\nfile2line7\nfile2line8\n", f2, Charsets.UTF_8);    List<String> out2 = bodiesAsStrings(parser.readEvents(5));    parser.commit();    List<String> out3 = bodiesAsStrings(parser.readEvents(5));    parser.commit();        assertEquals(5, out1.size());    assertTrue(out1.contains("file1line1"));    assertTrue(out1.contains("file1line2"));    assertTrue(out1.contains("file1line3"));    assertTrue(out1.contains("file1line4"));    assertTrue(out1.contains("file1line5"));        assertEquals(3, out2.size());    assertTrue(out2.contains("file1line6"));    assertTrue(out2.contains("file1line7"));    assertTrue(out2.contains("file1line8"));        assertEquals(5, out3.size());    assertTrue(out3.contains("file2line1"));    assertTrue(out3.contains("file2line2"));    assertTrue(out3.contains("file2line3"));    assertTrue(out3.contains("file2line4"));    assertTrue(out3.contains("file2line5"));        List<File> outFiles = Lists.newArrayList(tmpDir.listFiles(directoryFilter()));    assertEquals(2, outFiles.size());    assertTrue(outFiles.contains(new File(tmpDir + "/file1" + completedSuffix)));    assertTrue(outFiles.contains(new File(tmpDir + "/file2")));}
0
public void testEmptyDirectoryAfterCommittingFile() throws IOException
{    File f1 = new File(tmpDir.getAbsolutePath() + "/file1");    Files.write("file1line1\nfile1line2\n", f1, Charsets.UTF_8);    ReliableSpoolingFileEventReader parser = getParser();    List<String> allLines = bodiesAsStrings(parser.readEvents(2));    assertEquals(2, allLines.size());    parser.commit();    List<String> empty = bodiesAsStrings(parser.readEvents(10));    assertEquals(0, empty.size());}
0
public void testLineExceedsMaxLineLength() throws IOException
{    final int maxLineLength = 12;    File f1 = new File(tmpDir.getAbsolutePath() + "/file1");    Files.write("file1line1\nfile1line2\nfile1line3\nfile1line4\n" + "file1line5\nfile1line6\nfile1line7\nfile1line8\n" + "reallyreallyreallyreallyLongLineHerefile1line9\n" + "file1line10\nfile1line11\nfile1line12\nfile1line13\n", f1, Charsets.UTF_8);    ReliableSpoolingFileEventReader parser = getParser(maxLineLength);    List<String> out1 = bodiesAsStrings(parser.readEvents(5));    assertTrue(out1.contains("file1line1"));    assertTrue(out1.contains("file1line2"));    assertTrue(out1.contains("file1line3"));    assertTrue(out1.contains("file1line4"));    assertTrue(out1.contains("file1line5"));    parser.commit();    List<String> out2 = bodiesAsStrings(parser.readEvents(4));    assertTrue(out2.contains("file1line6"));    assertTrue(out2.contains("file1line7"));    assertTrue(out2.contains("file1line8"));    assertTrue(out2.contains("reallyreally"));    parser.commit();    List<String> out3 = bodiesAsStrings(parser.readEvents(5));    assertTrue(out3.contains("reallyreally"));    assertTrue(out3.contains("LongLineHere"));    assertTrue(out3.contains("file1line9"));    assertTrue(out3.contains("file1line10"));    assertTrue(out3.contains("file1line11"));    parser.commit();    List<String> out4 = bodiesAsStrings(parser.readEvents(5));    assertTrue(out4.contains("file1line12"));    assertTrue(out4.contains("file1line13"));    assertEquals(2, out4.size());    parser.commit();    assertEquals(0, parser.readEvents(5).size());}
0
public void testNameCorrespondsToLatestRead() throws IOException
{    File f1 = new File(tmpDir.getAbsolutePath() + "/file1");    Files.write("file1line1\nfile1line2\nfile1line3\nfile1line4\n" + "file1line5\nfile1line6\nfile1line7\nfile1line8\n", f1, Charsets.UTF_8);    ReliableSpoolingFileEventReader parser = getParser();    parser.readEvents(5);    parser.commit();    assertNotNull(parser.getLastFileRead());    assertTrue(parser.getLastFileRead().endsWith("file1"));    File f2 = new File(tmpDir.getAbsolutePath() + "/file2");    Files.write("file2line1\nfile2line2\nfile2line3\nfile2line4\n" + "file2line5\nfile2line6\nfile2line7\nfile2line8\n", f2, Charsets.UTF_8);    parser.readEvents(5);    parser.commit();    assertNotNull(parser.getLastFileRead());    assertTrue(parser.getLastFileRead().endsWith("file1"));    parser.readEvents(5);    parser.commit();    assertNotNull(parser.getLastFileRead());    assertTrue(parser.getLastFileRead().endsWith("file2"));    parser.readEvents(5);    assertTrue(parser.getLastFileRead().endsWith("file2"));    parser.readEvents(5);    assertTrue(parser.getLastFileRead().endsWith("file2"));}
0
public void testPrintable()
{    SimpleEvent event = new SimpleEvent();    event.setBody("Some text".getBytes());    String eventDump = EventHelper.dumpEvent(event);    System.out.println(eventDump);    Assert.assertTrue(eventDump, eventDump.contains("Some text"));}
0
public void testNonPrintable()
{    SimpleEvent event = new SimpleEvent();    byte[] body = new byte[5];    event.setBody(body);    String eventDump = EventHelper.dumpEvent(event);    Assert.assertTrue(eventDump, eventDump.contains("....."));}
0
public void setUp()
{    cal = createCalendar(2012, 5, 23, 13, 46, 33, 234, null);    headers = new HashMap<>();    headers.put("timestamp", String.valueOf(cal.getTimeInMillis()));    Calendar calWithTimeZone = createCalendar(2012, 5, 23, 13, 46, 33, 234, CUSTOM_TIMEZONE);    headersWithTimeZone = new HashMap<>();    headersWithTimeZone.put("timestamp", String.valueOf(calWithTimeZone.getTimeInMillis()));}
0
public void testDateFormatCache()
{    TimeZone utcTimeZone = TimeZone.getTimeZone("UTC");    String test = "%c";    BucketPath.escapeString(test, headers, utcTimeZone, false, Calendar.HOUR_OF_DAY, 12, false);    String escapedString = BucketPath.escapeString(test, headers, false, Calendar.HOUR_OF_DAY, 12);    System.out.println("Escaped String: " + escapedString);    SimpleDateFormat format = new SimpleDateFormat("EEE MMM d HH:mm:ss yyyy");    Date d = new Date(cal.getTimeInMillis());    String expectedString = format.format(d);    System.out.println("Expected String: " + expectedString);    Assert.assertEquals(expectedString, escapedString);}
0
public void testDateFormatHours()
{    String test = "%c";    String escapedString = BucketPath.escapeString(test, headers, true, Calendar.HOUR_OF_DAY, 12);    System.out.println("Escaped String: " + escapedString);    Calendar cal2 = createCalendar(2012, 5, 23, 12, 0, 0, 0, null);    SimpleDateFormat format = new SimpleDateFormat("EEE MMM d HH:mm:ss yyyy");    Date d = new Date(cal2.getTimeInMillis());    String expectedString = format.format(d);    System.out.println("Expected String: " + expectedString);    Assert.assertEquals(expectedString, escapedString);}
0
public void testDateFormatHoursTimeZone()
{    String test = "%c";    String escapedString = BucketPath.escapeString(test, headersWithTimeZone, CUSTOM_TIMEZONE, true, Calendar.HOUR_OF_DAY, 12, false);    System.out.println("Escaped String: " + escapedString);    Calendar cal2 = createCalendar(2012, 5, 23, 12, 0, 0, 0, CUSTOM_TIMEZONE);    SimpleDateFormat format = new SimpleDateFormat("EEE MMM d HH:mm:ss yyyy");    format.setTimeZone(CUSTOM_TIMEZONE);    Date d = new Date(cal2.getTimeInMillis());    String expectedString = format.format(d);    System.out.println("Expected String: " + expectedString);    Assert.assertEquals(expectedString, escapedString);}
0
public void testDateFormatMinutes()
{    String test = "%s";    String escapedString = BucketPath.escapeString(test, headers, true, Calendar.MINUTE, 5);    System.out.println("Escaped String: " + escapedString);    Calendar cal2 = createCalendar(2012, 5, 23, 13, 45, 0, 0, null);    String expectedString = String.valueOf(cal2.getTimeInMillis() / 1000);    System.out.println("Expected String: " + expectedString);    Assert.assertEquals(expectedString, escapedString);}
0
public void testDateFormatMinutesTimeZone()
{    String test = "%s";    String escapedString = BucketPath.escapeString(test, headersWithTimeZone, CUSTOM_TIMEZONE, true, Calendar.MINUTE, 5, false);    System.out.println("Escaped String: " + escapedString);    Calendar cal2 = createCalendar(2012, 5, 23, 13, 45, 0, 0, CUSTOM_TIMEZONE);    String expectedString = String.valueOf(cal2.getTimeInMillis() / 1000);    System.out.println("Expected String: " + expectedString);    Assert.assertEquals(expectedString, escapedString);}
0
public void testDateFormatSeconds()
{    String test = "%s";    String escapedString = BucketPath.escapeString(test, headers, true, Calendar.SECOND, 5);    System.out.println("Escaped String: " + escapedString);    Calendar cal2 = createCalendar(2012, 5, 23, 13, 46, 30, 0, null);    String expectedString = String.valueOf(cal2.getTimeInMillis() / 1000);    System.out.println("Expected String: " + expectedString);    Assert.assertEquals(expectedString, escapedString);}
0
public void testDateFormatSecondsTimeZone()
{    String test = "%s";    String escapedString = BucketPath.escapeString(test, headersWithTimeZone, CUSTOM_TIMEZONE, true, Calendar.SECOND, 5, false);    System.out.println("Escaped String: " + escapedString);    Calendar cal2 = createCalendar(2012, 5, 23, 13, 46, 30, 0, CUSTOM_TIMEZONE);    String expectedString = String.valueOf(cal2.getTimeInMillis() / 1000);    System.out.println("Expected String: " + expectedString);    Assert.assertEquals(expectedString, escapedString);}
0
public void testNoRounding()
{    String test = "%c";    String escapedString = BucketPath.escapeString(test, headers, false, Calendar.HOUR_OF_DAY, 12);    System.out.println("Escaped String: " + escapedString);    SimpleDateFormat format = new SimpleDateFormat("EEE MMM d HH:mm:ss yyyy");    Date d = new Date(cal.getTimeInMillis());    String expectedString = format.format(d);    System.out.println("Expected String: " + expectedString);    Assert.assertEquals(expectedString, escapedString);}
0
public void testNoPadding()
{    Calendar calender;    Map<String, String> calender_timestamp;    calender = Calendar.getInstance();        calender.set(2014, (5 - 1), 3, 13, 46, 33);    calender_timestamp = new HashMap<String, String>();    calender_timestamp.put("timestamp", String.valueOf(calender.getTimeInMillis()));    SimpleDateFormat format = new SimpleDateFormat("M-d");        String test = "%n-%e";    String escapedString = BucketPath.escapeString(test, calender_timestamp, false, Calendar.HOUR_OF_DAY, 12);    Date d = new Date(calender.getTimeInMillis());    String expectedString = format.format(d);        calender.set(2014, (11 - 1), 13, 13, 46, 33);    calender_timestamp.put("timestamp", String.valueOf(calender.getTimeInMillis()));    escapedString += " " + BucketPath.escapeString(test, calender_timestamp, false, Calendar.HOUR_OF_DAY, 12);    System.out.println("Escaped String: " + escapedString);    d = new Date(calender.getTimeInMillis());    expectedString += " " + format.format(d);    System.out.println("Expected String: " + expectedString);    Assert.assertEquals(expectedString, escapedString);}
0
public void testDateFormatTimeZone()
{    TimeZone utcTimeZone = TimeZone.getTimeZone("UTC");    String test = "%c";    String escapedString = BucketPath.escapeString(test, headers, utcTimeZone, false, Calendar.HOUR_OF_DAY, 12, false);    System.out.println("Escaped String: " + escapedString);    SimpleDateFormat format = new SimpleDateFormat("EEE MMM d HH:mm:ss yyyy");    format.setTimeZone(utcTimeZone);    Date d = new Date(cal.getTimeInMillis());    String expectedString = format.format(d);    System.out.println("Expected String: " + expectedString);    Assert.assertEquals(expectedString, escapedString);}
0
public void testDateRace()
{    Clock mockClock = mock(Clock.class);    DateTimeFormatter parser = ISODateTimeFormat.dateTimeParser();    long two = parser.parseMillis("2013-04-21T02:59:59-00:00");    long three = parser.parseMillis("2013-04-21T03:00:00-00:00");    when(mockClock.currentTimeMillis()).thenReturn(two, three);        Clock origClock = BucketPath.getClock();    BucketPath.setClock(mockClock);    String pat = "%H:%M";    String escaped = BucketPath.escapeString(pat, new HashMap<String, String>(), TimeZone.getTimeZone("UTC"), true, Calendar.MINUTE, 10, true);        BucketPath.setClock(origClock);    Assert.assertEquals("Race condition detected", "02:50", escaped);}
0
private static Calendar createCalendar(int year, int month, int day, int hour, int minute, int second, int ms, @Nullable TimeZone timeZone)
{    Calendar cal = (timeZone == null) ? Calendar.getInstance() : Calendar.getInstance(timeZone);    cal.set(year, month, day, hour, minute, second);    cal.set(Calendar.MILLISECOND, ms);    return cal;}
0
public void testStaticEscapeStrings()
{    Map<String, String> staticStrings;    staticStrings = new HashMap<>();    try {        InetAddress addr = InetAddress.getLocalHost();        staticStrings.put("localhost", addr.getHostName());        staticStrings.put("IP", addr.getHostAddress());        staticStrings.put("FQDN", addr.getCanonicalHostName());    } catch (UnknownHostException e) {        Assert.fail("Test failed due to UnkownHostException");    }    TimeZone utcTimeZone = TimeZone.getTimeZone("UTC");    String filePath = "%[localhost]/%[IP]/%[FQDN]";    String realPath = BucketPath.escapeString(filePath, headers, utcTimeZone, false, Calendar.HOUR_OF_DAY, 12, false);    String[] args = realPath.split("\\/");    Assert.assertEquals(args[0], staticStrings.get("localhost"));    Assert.assertEquals(args[1], staticStrings.get("IP"));    Assert.assertEquals(args[2], staticStrings.get("FQDN"));    StringBuilder s = new StringBuilder();    s.append("Expected String: ").append(staticStrings.get("localhost"));    s.append("/").append(staticStrings.get("IP")).append("/");    s.append(staticStrings.get("FQDN"));    System.out.println(s);    System.out.println("Escaped String: " + realPath);}
0
public void testStaticEscapeStringsNoKey()
{    Map<String, String> staticStrings;    staticStrings = new HashMap<>();    TimeZone utcTimeZone = TimeZone.getTimeZone("UTC");    String filePath = "%[abcdefg]/%[IP]/%[FQDN]";    String realPath = BucketPath.escapeString(filePath, headers, utcTimeZone, false, Calendar.HOUR_OF_DAY, 12, false);}
0
private static int getFreePort() throws Exception
{    try (ServerSocket socket = new ServerSocket(0)) {        return socket.getLocalPort();    }}
0
public void testJSON() throws Exception
{    memChannel.setName("memChannel");    pmemChannel.setName("pmemChannel");    Context c = new Context();    Configurables.configure(memChannel, c);    Configurables.configure(pmemChannel, c);    memChannel.start();    pmemChannel.start();    Transaction txn = memChannel.getTransaction();    txn.begin();    memChannel.put(EventBuilder.withBody("blah".getBytes()));    memChannel.put(EventBuilder.withBody("blah".getBytes()));    txn.commit();    txn.close();    txn = memChannel.getTransaction();    txn.begin();    memChannel.take();    txn.commit();    txn.close();    Transaction txn2 = pmemChannel.getTransaction();    txn2.begin();    pmemChannel.put(EventBuilder.withBody("blah".getBytes()));    pmemChannel.put(EventBuilder.withBody("blah".getBytes()));    txn2.commit();    txn2.close();    txn2 = pmemChannel.getTransaction();    txn2.begin();    pmemChannel.take();    txn2.commit();    txn2.close();    testWithPort(getFreePort());    memChannel.stop();    pmemChannel.stop();}
0
private void testWithPort(int port) throws Exception
{    MonitorService srv = new HTTPMetricsServer();    Context context = new Context();    context.put(HTTPMetricsServer.CONFIG_PORT, String.valueOf(port));    srv.configure(context);    srv.start();    Thread.sleep(1000);    URL url = new URL("http://0.0.0.0:" + String.valueOf(port) + "/metrics");    HttpURLConnection conn = (HttpURLConnection) url.openConnection();    conn.setRequestMethod("GET");    BufferedReader reader = new BufferedReader(new InputStreamReader(conn.getInputStream()));    String line;    String result = "";    while ((line = reader.readLine()) != null) {        result += line;    }    reader.close();    Map<String, Map<String, String>> mbeans = gson.fromJson(result, mapType);    Assert.assertNotNull(mbeans);    Map<String, String> memBean = mbeans.get("CHANNEL.memChannel");    Assert.assertNotNull(memBean);    JMXTestUtils.checkChannelCounterParams(memBean);    Map<String, String> pmemBean = mbeans.get("CHANNEL.pmemChannel");    Assert.assertNotNull(pmemBean);    JMXTestUtils.checkChannelCounterParams(pmemBean);    srv.stop();}
0
public void testTrace() throws Exception
{    doTestForbiddenMethods(getFreePort(), "TRACE");}
0
public void testOptions() throws Exception
{    doTestForbiddenMethods(getFreePort(), "OPTIONS");}
0
public void doTestForbiddenMethods(int port, String method) throws Exception
{    MonitorService srv = new HTTPMetricsServer();    Context context = new Context();    context.put(HTTPMetricsServer.CONFIG_PORT, String.valueOf(port));    srv.configure(context);    srv.start();    Thread.sleep(1000);    URL url = new URL("http://0.0.0.0:" + String.valueOf(port) + "/metrics");    HttpURLConnection conn = (HttpURLConnection) url.openConnection();    conn.setRequestMethod(method);    Assert.assertEquals(HttpServletResponse.SC_FORBIDDEN, conn.getResponseCode());    srv.stop();}
0
public void setUp() throws Exception
{    counter = new KafkaSourceCounter("test");}
0
public void testAddToKafkaEventGetTimer() throws Exception
{    Assert.assertEquals(1L, counter.addToKafkaEventGetTimer(1L));}
0
public void testAddToKafkaCommitTimer() throws Exception
{    Assert.assertEquals(1L, counter.addToKafkaCommitTimer(1L));}
0
public void testIncrementKafkaEmptyCount() throws Exception
{    Assert.assertEquals(1L, counter.incrementKafkaEmptyCount());}
0
public void testGetKafkaCommitTimer() throws Exception
{    Assert.assertEquals(0, counter.getKafkaCommitTimer());}
0
public void testGetKafkaEventGetTimer() throws Exception
{    Assert.assertEquals(0, counter.getKafkaEventGetTimer());}
0
public void testGetKafkaEmptyCount() throws Exception
{    Assert.assertEquals(0, counter.getKafkaEmptyCount());}
0
public void setUp()
{    mbServer = ManagementFactory.getPlatformMBeanServer();    random = new Random(System.nanoTime());}
0
public void testSinkCounter() throws Exception
{    String name = getRandomName();    SinkCounter skc = new SinkCounter(name);    skc.register();    ObjectName on = new ObjectName(SINK_OBJ_NAME_PREFIX + name);    assertSkCounterState(on, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L);    skc.start();    long start1 = getStartTime(on);    Assert.assertTrue("StartTime", start1 != 0L);    Assert.assertTrue("StopTime", getStopTime(on) == 0L);    int connCreated = random.nextInt(MAX_BOUNDS);    int connClosed = random.nextInt(MAX_BOUNDS);    int connFailed = random.nextInt(MAX_BOUNDS);    int batchEmpty = random.nextInt(MAX_BOUNDS);    int batchUnderflow = random.nextInt(MAX_BOUNDS);    int batchComplete = random.nextInt(MAX_BOUNDS);    int eventDrainAttempt = random.nextInt(MAX_BOUNDS);    int eventDrainSuccess = random.nextInt(MAX_BOUNDS);    for (int i = 0; i < connCreated; i++) {        skc.incrementConnectionCreatedCount();    }    for (int i = 0; i < connClosed; i++) {        skc.incrementConnectionClosedCount();    }    for (int i = 0; i < connFailed; i++) {        skc.incrementConnectionFailedCount();    }    for (int i = 0; i < batchEmpty; i++) {        skc.incrementBatchEmptyCount();    }    for (int i = 0; i < batchUnderflow; i++) {        skc.incrementBatchUnderflowCount();    }    for (int i = 0; i < batchComplete; i++) {        skc.incrementBatchCompleteCount();    }    for (int i = 0; i < eventDrainAttempt; i++) {        skc.incrementEventDrainAttemptCount();    }    for (int i = 0; i < eventDrainSuccess; i++) {        skc.incrementEventDrainSuccessCount();    }    assertSkCounterState(on, connCreated, connClosed, connFailed, batchEmpty, batchUnderflow, batchComplete, eventDrainAttempt, eventDrainSuccess);    skc.stop();    Assert.assertTrue("StartTime", getStartTime(on) != 0L);    Assert.assertTrue("StopTime", getStopTime(on) != 0L);    assertSkCounterState(on, connCreated, connClosed, connFailed, batchEmpty, batchUnderflow, batchComplete, eventDrainAttempt, eventDrainSuccess);        Thread.sleep(5L);    skc.start();    Assert.assertTrue("StartTime", getStartTime(on) != 0L);    Assert.assertTrue("StartTime", getStartTime(on) > start1);    Assert.assertTrue("StopTime", getStopTime(on) == 0L);    assertSkCounterState(on, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L);    int eventDrainAttempt2 = random.nextInt(MAX_BOUNDS);    int eventDrainSuccess2 = random.nextInt(MAX_BOUNDS);    skc.addToEventDrainAttemptCount(eventDrainAttempt2);    skc.addToEventDrainSuccessCount(eventDrainSuccess2);    assertSkCounterState(on, 0L, 0L, 0L, 0L, 0L, 0L, eventDrainAttempt2, eventDrainSuccess2);}
0
public void testChannelCounter() throws Exception
{    String name = getRandomName();    ChannelCounter chc = new ChannelCounter(name);    chc.register();    ObjectName on = new ObjectName(CHANNEL_OBJ_NAME_PREFIX + name);    assertChCounterState(on, 0L, 0L, 0L, 0L, 0L);    Assert.assertTrue("StartTime", getStartTime(on) == 0L);    Assert.assertTrue("StopTime", getStopTime(on) == 0L);    chc.start();    long start1 = getStartTime(on);    Assert.assertTrue("StartTime", start1 != 0L);    Assert.assertTrue("StopTime", getStopTime(on) == 0L);    int numChannelSize = random.nextInt(MAX_BOUNDS);    int numEventPutAttempt = random.nextInt(MAX_BOUNDS);    int numEventTakeAttempt = random.nextInt(MAX_BOUNDS);    int numEventPutSuccess = random.nextInt(MAX_BOUNDS);    int numEventTakeSuccess = random.nextInt(MAX_BOUNDS);    chc.setChannelSize(numChannelSize);    for (int i = 0; i < numEventPutAttempt; i++) {        chc.incrementEventPutAttemptCount();    }    for (int i = 0; i < numEventTakeAttempt; i++) {        chc.incrementEventTakeAttemptCount();    }    chc.addToEventPutSuccessCount(numEventPutSuccess);    chc.addToEventTakeSuccessCount(numEventTakeSuccess);    assertChCounterState(on, numChannelSize, numEventPutAttempt, numEventTakeAttempt, numEventPutSuccess, numEventTakeSuccess);    chc.stop();    Assert.assertTrue("StartTime", getStartTime(on) != 0L);    Assert.assertTrue("StopTime", getStopTime(on) != 0L);    assertChCounterState(on, numChannelSize, numEventPutAttempt, numEventTakeAttempt, numEventPutSuccess, numEventTakeSuccess);        Thread.sleep(5L);    chc.start();    Assert.assertTrue("StartTime", getStartTime(on) != 0L);    Assert.assertTrue("StartTime", getStartTime(on) > start1);    Assert.assertTrue("StopTime", getStopTime(on) == 0L);    assertChCounterState(on, 0L, 0L, 0L, 0L, 0L);}
0
public void testSourceCounter() throws Exception
{    String name = getRandomName();    SourceCounter srcc = new SourceCounter(name);    srcc.register();    ObjectName on = new ObjectName(SOURCE_OBJ_NAME_PREFIX + name);    assertSrcCounterState(on, 0L, 0L, 0L, 0L, 0L, 0L);    Assert.assertTrue("StartTime", getStartTime(on) == 0L);    Assert.assertTrue("StopTime", getStopTime(on) == 0L);    srcc.start();    long start1 = getStartTime(on);    Assert.assertTrue("StartTime", start1 != 0L);    Assert.assertTrue("StopTime", getStopTime(on) == 0L);    int numEventReceived = random.nextInt(MAX_BOUNDS);    int numEventAccepted = random.nextInt(MAX_BOUNDS);    int numAppendReceived = random.nextInt(MAX_BOUNDS);    int numAppendAccepted = random.nextInt(MAX_BOUNDS);    int numAppendBatchReceived = random.nextInt(MAX_BOUNDS);    int numAppendBatchAccepted = random.nextInt(MAX_BOUNDS);    srcc.addToEventReceivedCount(numEventReceived);    srcc.addToEventAcceptedCount(numEventAccepted);    for (int i = 0; i < numAppendReceived; i++) {        srcc.incrementAppendReceivedCount();    }    for (int i = 0; i < numAppendAccepted; i++) {        srcc.incrementAppendAcceptedCount();    }    for (int i = 0; i < numAppendBatchReceived; i++) {        srcc.incrementAppendBatchReceivedCount();    }    for (int i = 0; i < numAppendBatchAccepted; i++) {        srcc.incrementAppendBatchAcceptedCount();    }    assertSrcCounterState(on, numEventReceived, numEventAccepted, numAppendReceived, numAppendAccepted, numAppendBatchReceived, numAppendBatchAccepted);    srcc.stop();    Assert.assertTrue("StartTime", getStartTime(on) != 0L);    Assert.assertTrue("StopTime", getStopTime(on) != 0L);    assertSrcCounterState(on, numEventReceived, numEventAccepted, numAppendReceived, numAppendAccepted, numAppendBatchReceived, numAppendBatchAccepted);        Thread.sleep(5L);    srcc.start();    Assert.assertTrue("StartTime", getStartTime(on) != 0L);    Assert.assertTrue("StartTime", getStartTime(on) > start1);    Assert.assertTrue("StopTime", getStopTime(on) == 0L);    assertSrcCounterState(on, 0L, 0L, 0L, 0L, 0L, 0L);    int numEventReceived2 = random.nextInt(MAX_BOUNDS);    int numEventAccepted2 = random.nextInt(MAX_BOUNDS);    for (int i = 0; i < numEventReceived2; i++) {        srcc.incrementEventReceivedCount();    }    for (int i = 0; i < numEventAccepted2; i++) {        srcc.incrementEventAcceptedCount();    }    assertSrcCounterState(on, numEventReceived2, numEventAccepted2, 0L, 0L, 0L, 0L);}
0
public void testRegisterTwice() throws Exception
{    String name = "re-register-" + getRandomName();    SourceCounter c1 = new SourceCounter(name);    c1.register();    ObjectName on = new ObjectName(SOURCE_OBJ_NAME_PREFIX + name);    Assert.assertEquals("StartTime", 0L, getStartTime(on));    Assert.assertEquals("StopTime", 0L, getStopTime(on));    c1.start();    c1.stop();    Assert.assertTrue("StartTime", getStartTime(on) > 0L);    Assert.assertTrue("StopTime", getStopTime(on) > 0L);    SourceCounter c2 = new SourceCounter(name);    c2.register();    Assert.assertEquals("StartTime", 0L, getStartTime(on));    Assert.assertEquals("StopTime", 0L, getStopTime(on));}
0
private void assertSrcCounterState(ObjectName on, long eventReceivedCount, long eventAcceptedCount, long appendReceivedCount, long appendAcceptedCount, long appendBatchReceivedCount, long appendBatchAcceptedCount) throws Exception
{    Assert.assertEquals("SrcEventReceived", getSrcEventReceivedCount(on), eventReceivedCount);    Assert.assertEquals("SrcEventAccepted", getSrcEventAcceptedCount(on), eventAcceptedCount);    Assert.assertEquals("SrcAppendReceived", getSrcAppendReceivedCount(on), appendReceivedCount);    Assert.assertEquals("SrcAppendAccepted", getSrcAppendAcceptedCount(on), appendAcceptedCount);    Assert.assertEquals("SrcAppendBatchReceived", getSrcAppendBatchReceivedCount(on), appendBatchReceivedCount);    Assert.assertEquals("SrcAppendBatchAccepted", getSrcAppendBatchAcceptedCount(on), appendBatchAcceptedCount);}
0
private void assertChCounterState(ObjectName on, long channelSize, long eventPutAttempt, long eventTakeAttempt, long eventPutSuccess, long eventTakeSuccess) throws Exception
{    Assert.assertEquals("ChChannelSize", getChChannelSize(on), channelSize);    Assert.assertEquals("ChEventPutAttempt", getChEventPutAttempt(on), eventPutAttempt);    Assert.assertEquals("ChEventTakeAttempt", getChEventTakeAttempt(on), eventTakeAttempt);    Assert.assertEquals("ChEventPutSuccess", getChEventPutSuccess(on), eventPutSuccess);    Assert.assertEquals("ChEventTakeSuccess", getChEventTakeSuccess(on), eventTakeSuccess);}
0
private void assertSkCounterState(ObjectName on, long connCreated, long connClosed, long connFailed, long batchEmpty, long batchUnderflow, long batchComplete, long eventDrainAttempt, long eventDrainSuccess) throws Exception
{    Assert.assertEquals("SkConnCreated", getSkConnectionCreated(on), connCreated);    Assert.assertEquals("SkConnClosed", getSkConnectionClosed(on), connClosed);    Assert.assertEquals("SkConnFailed", getSkConnectionFailed(on), connFailed);    Assert.assertEquals("SkBatchEmpty", getSkBatchEmpty(on), batchEmpty);    Assert.assertEquals("SkBatchUnderflow", getSkBatchUnderflow(on), batchUnderflow);    Assert.assertEquals("SkBatchComplete", getSkBatchComplete(on), batchComplete);    Assert.assertEquals("SkEventDrainAttempt", getSkEventDrainAttempt(on), eventDrainAttempt);    Assert.assertEquals("SkEventDrainSuccess", getSkEventDrainSuccess(on), eventDrainSuccess);}
0
private long getStartTime(ObjectName on) throws Exception
{    return getLongAttribute(on, ATTR_START_TIME);}
0
private long getStopTime(ObjectName on) throws Exception
{    return getLongAttribute(on, ATTR_STOP_TIME);}
0
private long getSkConnectionCreated(ObjectName on) throws Exception
{    return getLongAttribute(on, SK_ATTR_CONN_CREATED);}
0
private long getSkConnectionClosed(ObjectName on) throws Exception
{    return getLongAttribute(on, SK_ATTR_CONN_CLOSED);}
0
private long getSkConnectionFailed(ObjectName on) throws Exception
{    return getLongAttribute(on, SK_ATTR_CONN_FAILED);}
0
private long getSkBatchEmpty(ObjectName on) throws Exception
{    return getLongAttribute(on, SK_ATTR_BATCH_EMPTY);}
0
private long getSkBatchUnderflow(ObjectName on) throws Exception
{    return getLongAttribute(on, SK_ATTR_BATCH_UNDERFLOW);}
0
private long getSkBatchComplete(ObjectName on) throws Exception
{    return getLongAttribute(on, SK_ATTR_BATCH_COMPLETE);}
0
private long getSkEventDrainAttempt(ObjectName on) throws Exception
{    return getLongAttribute(on, SK_ATTR_EVENT_DRAIN_ATTEMPT);}
0
private long getSkEventDrainSuccess(ObjectName on) throws Exception
{    return getLongAttribute(on, SK_ATTR_EVENT_DRAIN_SUCCESS);}
0
private long getChChannelSize(ObjectName on) throws Exception
{    return getLongAttribute(on, CH_ATTR_CHANNEL_SIZE);}
0
private long getChEventPutAttempt(ObjectName on) throws Exception
{    return getLongAttribute(on, CH_ATTR_EVENT_PUT_ATTEMPT);}
0
private long getChEventTakeAttempt(ObjectName on) throws Exception
{    return getLongAttribute(on, CH_ATTR_EVENT_TAKE_ATTEMPT);}
0
private long getChEventPutSuccess(ObjectName on) throws Exception
{    return getLongAttribute(on, CH_ATTR_EVENT_PUT_SUCCESS);}
0
private long getChEventTakeSuccess(ObjectName on) throws Exception
{    return getLongAttribute(on, CH_ATTR_EVENT_TAKE_SUCCESS);}
0
private long getSrcAppendBatchAcceptedCount(ObjectName on) throws Exception
{    return getLongAttribute(on, SRC_ATTR_APPEND_BATCH_ACCEPTED_COUNT);}
0
private long getSrcAppendBatchReceivedCount(ObjectName on) throws Exception
{    return getLongAttribute(on, SRC_ATTR_APPEND_BATCH_RECEVIED_COUNT);}
0
private long getSrcAppendAcceptedCount(ObjectName on) throws Exception
{    return getLongAttribute(on, SRC_ATTR_APPEND_ACCEPTED_COUNT);}
0
private long getSrcAppendReceivedCount(ObjectName on) throws Exception
{    return getLongAttribute(on, SRC_ATTR_APPEND_RECEVIED_COUNT);}
0
private long getSrcEventAcceptedCount(ObjectName on) throws Exception
{    return getLongAttribute(on, SRC_ATTR_EVENT_ACCEPTED_COUNT);}
0
private long getSrcEventReceivedCount(ObjectName on) throws Exception
{    return getLongAttribute(on, SRC_ATTR_EVENT_RECEVIED_COUNT);}
0
private long getLongAttribute(ObjectName on, String attr) throws Exception
{    Object result = getAttribute(on, attr);    return ((Long) result).longValue();}
0
private Object getAttribute(ObjectName objName, String attrName) throws Exception
{    return mbServer.getAttribute(objName, attrName);}
0
private String getRandomName()
{    return "random-" + System.nanoTime();}
0
public static void checkChannelCounterParams(Map<String, String> attrs)
{    Assert.assertNotNull(attrs.get("StartTime"));    Assert.assertNotNull(attrs.get("StopTime"));    Assert.assertTrue(Long.parseLong(attrs.get("ChannelSize")) != 0);    Assert.assertTrue(Long.parseLong(attrs.get("EventPutAttemptCount")) == 2);    Assert.assertTrue(Long.parseLong(attrs.get("EventTakeAttemptCount")) == 1);    Assert.assertTrue(Long.parseLong(attrs.get("EventPutSuccessCount")) == 2);    Assert.assertTrue(Long.parseLong(attrs.get("EventTakeSuccessCount")) == 1);}
0
public void testJMXPoll()
{    memChannel.setName("memChannel");    pmemChannel.setName("pmemChannel");    Context c = new Context();    Configurables.configure(memChannel, c);    Configurables.configure(pmemChannel, c);    memChannel.start();    pmemChannel.start();    Transaction txn = memChannel.getTransaction();    txn.begin();    memChannel.put(EventBuilder.withBody("blah".getBytes()));    memChannel.put(EventBuilder.withBody("blah".getBytes()));    txn.commit();    txn.close();    txn = memChannel.getTransaction();    txn.begin();    memChannel.take();    txn.commit();    txn.close();    Transaction txn2 = pmemChannel.getTransaction();    txn2.begin();    pmemChannel.put(EventBuilder.withBody("blah".getBytes()));    pmemChannel.put(EventBuilder.withBody("blah".getBytes()));    txn2.commit();    txn2.close();    txn2 = pmemChannel.getTransaction();    txn2.begin();    pmemChannel.take();    txn2.commit();    txn2.close();    Map<String, Map<String, String>> mbeans = JMXPollUtil.getAllMBeans();    Assert.assertNotNull(mbeans);    Map<String, String> memBean = mbeans.get("CHANNEL.memChannel");    Assert.assertNotNull(memBean);    JMXTestUtils.checkChannelCounterParams(memBean);    Map<String, String> pmemBean = mbeans.get("CHANNEL.pmemChannel");    Assert.assertNotNull(pmemBean);    JMXTestUtils.checkChannelCounterParams(pmemBean);    memChannel.stop();    pmemChannel.stop();}
0
public void initialize()
{}
0
public Event intercept(Event event)
{    Map<String, String> headers = event.getHeaders();    if (headers.containsKey("Bad-Words")) {        headers.remove("Bad-Words");    }    return event;}
0
public List<Event> intercept(List<Event> events)
{    for (Event e : events) {        intercept(e);    }    return events;}
0
public void close()
{}
0
public Interceptor build()
{    return new CensoringInterceptor();}
0
public void configure(Context context)
{}
0
private Event buildEventWithHeader()
{    return EventBuilder.withBody("My test event".getBytes(), ImmutableMap.of(HEADER1, HEADER1, HEADER2, HEADER2, HEADER3, HEADER3, HEADER4, HEADER4, HEADER5, HEADER5));}
0
private Event buildEventWithoutHeader()
{    return EventBuilder.withBody("My test event".getBytes());}
0
public void testBadConfig() throws Exception
{    new RemoveHeaderIntBuilder().fromList(HEADER1, "(").build();}
0
public void testWithName() throws IllegalAccessException, ClassNotFoundException, InstantiationException
{    final Interceptor removeHeaderInterceptor = new RemoveHeaderIntBuilder().withName(HEADER4).build();    final Event event1 = buildEventWithHeader();    Assert.assertEquals(HEADER1, event1.getHeaders().get(HEADER1));    Assert.assertEquals(HEADER2, event1.getHeaders().get(HEADER2));    Assert.assertEquals(HEADER3, event1.getHeaders().get(HEADER3));    Assert.assertEquals(HEADER4, event1.getHeaders().get(HEADER4));    Assert.assertEquals(HEADER5, event1.getHeaders().get(HEADER5));    removeHeaderInterceptor.intercept(event1);    Assert.assertEquals(HEADER1, event1.getHeaders().get(HEADER1));    Assert.assertEquals(HEADER2, event1.getHeaders().get(HEADER2));    Assert.assertEquals(HEADER3, event1.getHeaders().get(HEADER3));    Assert.assertNull(event1.getHeaders().get(HEADER4));    Assert.assertEquals(HEADER5, event1.getHeaders().get(HEADER5));    final Event event2 = buildEventWithoutHeader();    Assert.assertTrue(event2.getHeaders().isEmpty());    removeHeaderInterceptor.intercept(event2);    Assert.assertTrue(event2.getHeaders().isEmpty());}
0
public void testFromListWithDefaultSeparator1() throws Exception
{    final Interceptor removeHeaderInterceptor = new RemoveHeaderIntBuilder().fromList(HEADER4 + MY_SEPARATOR + HEADER2).build();    final Event event1 = buildEventWithHeader();    Assert.assertEquals(HEADER1, event1.getHeaders().get(HEADER1));    Assert.assertEquals(HEADER2, event1.getHeaders().get(HEADER2));    Assert.assertEquals(HEADER3, event1.getHeaders().get(HEADER3));    Assert.assertEquals(HEADER4, event1.getHeaders().get(HEADER4));    Assert.assertEquals(HEADER5, event1.getHeaders().get(HEADER5));    removeHeaderInterceptor.intercept(event1);    Assert.assertEquals(HEADER1, event1.getHeaders().get(HEADER1));    Assert.assertEquals(HEADER2, event1.getHeaders().get(HEADER2));    Assert.assertEquals(HEADER3, event1.getHeaders().get(HEADER3));    Assert.assertEquals(HEADER4, event1.getHeaders().get(HEADER4));    Assert.assertEquals(HEADER5, event1.getHeaders().get(HEADER5));    final Event event2 = buildEventWithoutHeader();    Assert.assertTrue(event2.getHeaders().isEmpty());    removeHeaderInterceptor.intercept(event2);    Assert.assertTrue(event2.getHeaders().isEmpty());}
0
public void testFromListWithDefaultSeparator2() throws Exception
{    final Interceptor removeHeaderInterceptor = new RemoveHeaderIntBuilder().fromList(HEADER4 + DEFAULT_SEPARATOR + HEADER2).build();    final Event event1 = buildEventWithHeader();    Assert.assertEquals(HEADER1, event1.getHeaders().get(HEADER1));    Assert.assertEquals(HEADER2, event1.getHeaders().get(HEADER2));    Assert.assertEquals(HEADER3, event1.getHeaders().get(HEADER3));    Assert.assertEquals(HEADER4, event1.getHeaders().get(HEADER4));    Assert.assertEquals(HEADER5, event1.getHeaders().get(HEADER5));    removeHeaderInterceptor.intercept(event1);    Assert.assertEquals(HEADER1, event1.getHeaders().get(HEADER1));    Assert.assertNull(event1.getHeaders().get(HEADER2));    Assert.assertEquals(HEADER3, event1.getHeaders().get(HEADER3));    Assert.assertNull(event1.getHeaders().get(HEADER4));    Assert.assertEquals(HEADER5, event1.getHeaders().get(HEADER5));    final Event event2 = buildEventWithoutHeader();    Assert.assertTrue(event2.getHeaders().isEmpty());    removeHeaderInterceptor.intercept(event2);    Assert.assertTrue(event2.getHeaders().isEmpty());}
0
public void testFromListWithCustomSeparator1() throws Exception
{    final Interceptor removeHeaderInterceptor = new RemoveHeaderIntBuilder().fromList(HEADER4 + MY_SEPARATOR + HEADER2, MY_SEPARATOR).build();    final Event event1 = buildEventWithHeader();    Assert.assertEquals(HEADER1, event1.getHeaders().get(HEADER1));    Assert.assertEquals(HEADER2, event1.getHeaders().get(HEADER2));    Assert.assertEquals(HEADER3, event1.getHeaders().get(HEADER3));    Assert.assertEquals(HEADER4, event1.getHeaders().get(HEADER4));    Assert.assertEquals(HEADER5, event1.getHeaders().get(HEADER5));    removeHeaderInterceptor.intercept(event1);    Assert.assertEquals(HEADER1, event1.getHeaders().get(HEADER1));    Assert.assertNull(event1.getHeaders().get(HEADER2));    Assert.assertEquals(HEADER3, event1.getHeaders().get(HEADER3));    Assert.assertNull(event1.getHeaders().get(HEADER4));    Assert.assertEquals(HEADER5, event1.getHeaders().get(HEADER5));    final Event event2 = buildEventWithoutHeader();    Assert.assertTrue(event2.getHeaders().isEmpty());    removeHeaderInterceptor.intercept(event2);    Assert.assertTrue(event2.getHeaders().isEmpty());}
0
public void testFromListWithCustomSeparator2() throws Exception
{    final Interceptor removeHeaderInterceptor = new RemoveHeaderIntBuilder().fromList(HEADER4 + DEFAULT_SEPARATOR + HEADER2, MY_SEPARATOR).build();    final Event event1 = buildEventWithHeader();    Assert.assertEquals(HEADER1, event1.getHeaders().get(HEADER1));    Assert.assertEquals(HEADER2, event1.getHeaders().get(HEADER2));    Assert.assertEquals(HEADER3, event1.getHeaders().get(HEADER3));    Assert.assertEquals(HEADER4, event1.getHeaders().get(HEADER4));    Assert.assertEquals(HEADER5, event1.getHeaders().get(HEADER5));    removeHeaderInterceptor.intercept(event1);    Assert.assertEquals(HEADER1, event1.getHeaders().get(HEADER1));    Assert.assertEquals(HEADER2, event1.getHeaders().get(HEADER2));    Assert.assertEquals(HEADER3, event1.getHeaders().get(HEADER3));    Assert.assertEquals(HEADER4, event1.getHeaders().get(HEADER4));    Assert.assertEquals(HEADER5, event1.getHeaders().get(HEADER5));    final Event event2 = buildEventWithoutHeader();    Assert.assertTrue(event2.getHeaders().isEmpty());    removeHeaderInterceptor.intercept(event2);    Assert.assertTrue(event2.getHeaders().isEmpty());}
0
public void testMatchRegex() throws Exception
{    final Interceptor removeHeaderInterceptor = new RemoveHeaderIntBuilder().matchRegex("my-header1.*").build();    final Event event1 = buildEventWithHeader();    Assert.assertEquals(HEADER1, event1.getHeaders().get(HEADER1));    Assert.assertEquals(HEADER2, event1.getHeaders().get(HEADER2));    Assert.assertEquals(HEADER3, event1.getHeaders().get(HEADER3));    Assert.assertEquals(HEADER4, event1.getHeaders().get(HEADER4));    Assert.assertEquals(HEADER5, event1.getHeaders().get(HEADER5));    removeHeaderInterceptor.intercept(event1);    Assert.assertNull(event1.getHeaders().get(HEADER1));    Assert.assertNull(event1.getHeaders().get(HEADER2));    Assert.assertNull(event1.getHeaders().get(HEADER3));    Assert.assertEquals(HEADER4, event1.getHeaders().get(HEADER4));    Assert.assertEquals(HEADER5, event1.getHeaders().get(HEADER5));    final Event event2 = buildEventWithoutHeader();    Assert.assertTrue(event2.getHeaders().isEmpty());    removeHeaderInterceptor.intercept(event2);    Assert.assertTrue(event2.getHeaders().isEmpty());}
0
public void testAll() throws Exception
{    final Interceptor removeHeaderInterceptor = new RemoveHeaderIntBuilder().matchRegex("my-header2.*").fromList(HEADER1 + MY_SEPARATOR + HEADER3, MY_SEPARATOR).withName(HEADER2).build();    final Event event1 = buildEventWithHeader();    Assert.assertEquals(HEADER1, event1.getHeaders().get(HEADER1));    Assert.assertEquals(HEADER2, event1.getHeaders().get(HEADER2));    Assert.assertEquals(HEADER3, event1.getHeaders().get(HEADER3));    Assert.assertEquals(HEADER4, event1.getHeaders().get(HEADER4));    Assert.assertEquals(HEADER5, event1.getHeaders().get(HEADER5));    removeHeaderInterceptor.intercept(event1);    Assert.assertTrue(event1.getHeaders().isEmpty());    final Event event2 = buildEventWithoutHeader();    Assert.assertTrue(event2.getHeaders().isEmpty());    removeHeaderInterceptor.intercept(event2);    Assert.assertTrue(event2.getHeaders().isEmpty());}
0
 RemoveHeaderIntBuilder withName(final String str)
{    contextMap.put(RemoveHeaderInterceptor.WITH_NAME, str);    return this;}
0
 RemoveHeaderIntBuilder fromList(final String str)
{    contextMap.put(RemoveHeaderInterceptor.FROM_LIST, str);    return this;}
0
 RemoveHeaderIntBuilder fromList(final String str, final String separator)
{    fromList(str);    contextMap.put(RemoveHeaderInterceptor.LIST_SEPARATOR, separator);    return this;}
0
 RemoveHeaderIntBuilder matchRegex(final String str)
{    contextMap.put(RemoveHeaderInterceptor.MATCH_REGEX, str);    return this;}
0
public Interceptor build() throws InstantiationException, IllegalAccessException, ClassNotFoundException
{    Interceptor.Builder builder = InterceptorBuilderFactory.newInstance(InterceptorType.REMOVE_HEADER.toString());    builder.configure(new Context(contextMap));    return builder.build();}
0
public void testCensor()
{    MemoryChannel memCh = new MemoryChannel();    memCh.configure(new Context());    memCh.start();    ChannelSelector cs = new ReplicatingChannelSelector();    cs.setChannels(Lists.<Channel>newArrayList(memCh));    ChannelProcessor cp = new ChannelProcessor(cs);        Map<String, String> cfgMap = Maps.newHashMap();    cfgMap.put("interceptors", "a");    String builderClass = CensoringInterceptor.Builder.class.getName();    cfgMap.put("interceptors.a.type", builderClass);    Context ctx = new Context(cfgMap);        cp.configure(ctx);    cp.initialize();    Map<String, String> headers = Maps.newHashMap();    String badWord = "scribe";    headers.put("Bad-Words", badWord);    Event event1 = EventBuilder.withBody("test", Charsets.UTF_8, headers);    Assert.assertEquals(badWord, event1.getHeaders().get("Bad-Words"));    cp.processEvent(event1);    Transaction tx = memCh.getTransaction();    tx.begin();    Event event1a = memCh.take();    Assert.assertNull(event1a.getHeaders().get("Bad-Words"));    tx.commit();    tx.close();        cp.close();    memCh.stop();}
0
public void testBasic() throws Exception
{    Interceptor.Builder builder = InterceptorBuilderFactory.newInstance(InterceptorType.HOST.toString());    Interceptor interceptor = builder.build();    Event eventBeforeIntercept = EventBuilder.withBody("test event", Charsets.UTF_8);    Assert.assertNull(eventBeforeIntercept.getHeaders().get(Constants.HOST));    Event eventAfterIntercept = interceptor.intercept(eventBeforeIntercept);    String actualHost = eventAfterIntercept.getHeaders().get(Constants.HOST);    Assert.assertNotNull(actualHost);}
0
public void testCustomHeader() throws Exception
{    Interceptor.Builder builder = InterceptorBuilderFactory.newInstance(InterceptorType.HOST.toString());    Context ctx = new Context();    ctx.put("preserveExisting", "false");    ctx.put("hostHeader", "hostname");    builder.configure(ctx);    Interceptor interceptor = builder.build();    Event eventBeforeIntercept = EventBuilder.withBody("test event", Charsets.UTF_8);    Assert.assertNull(eventBeforeIntercept.getHeaders().get("hostname"));    Event eventAfterIntercept = interceptor.intercept(eventBeforeIntercept);    String actualHost = eventAfterIntercept.getHeaders().get("hostname");    Assert.assertNotNull(actualHost);    Assert.assertEquals(InetAddress.getLocalHost().getHostAddress(), actualHost);}
0
public void testPreserve() throws Exception
{    Context ctx = new Context();    ctx.put("preserveExisting", "true");    Interceptor.Builder builder = InterceptorBuilderFactory.newInstance(InterceptorType.HOST.toString());    builder.configure(ctx);    Interceptor interceptor = builder.build();    final String ORIGINAL_HOST = "originalhost";    Event eventBeforeIntercept = EventBuilder.withBody("test event", Charsets.UTF_8);    eventBeforeIntercept.getHeaders().put(Constants.HOST, ORIGINAL_HOST);    Assert.assertEquals(ORIGINAL_HOST, eventBeforeIntercept.getHeaders().get(Constants.HOST));    String expectedHost = ORIGINAL_HOST;    Event eventAfterIntercept = interceptor.intercept(eventBeforeIntercept);    String actualHost = eventAfterIntercept.getHeaders().get(Constants.HOST);    Assert.assertNotNull(actualHost);    Assert.assertEquals(expectedHost, actualHost);}
0
public void testClobber() throws Exception
{    Context ctx = new Context();        ctx.put("preserveExisting", "false");    Interceptor.Builder builder = InterceptorBuilderFactory.newInstance(InterceptorType.HOST.toString());    builder.configure(ctx);    Interceptor interceptor = builder.build();    final String ORIGINAL_HOST = "originalhost";    Event eventBeforeIntercept = EventBuilder.withBody("test event", Charsets.UTF_8);    eventBeforeIntercept.getHeaders().put(Constants.HOST, ORIGINAL_HOST);    Assert.assertEquals(ORIGINAL_HOST, eventBeforeIntercept.getHeaders().get(Constants.HOST));    String expectedHost = InetAddress.getLocalHost().getHostAddress();    Event eventAfterIntercept = interceptor.intercept(eventBeforeIntercept);    String actualHost = eventAfterIntercept.getHeaders().get(Constants.HOST);    Assert.assertNotNull(actualHost);    Assert.assertEquals(expectedHost, actualHost);}
0
public void testUseIP() throws Exception
{    Context ctx = new Context();        ctx.put("useIP", "true");    Interceptor.Builder builder = InterceptorBuilderFactory.newInstance(InterceptorType.HOST.toString());    builder.configure(ctx);    Interceptor interceptor = builder.build();    final String ORIGINAL_HOST = "originalhost";    Event eventBeforeIntercept = EventBuilder.withBody("test event", Charsets.UTF_8);    eventBeforeIntercept.getHeaders().put(Constants.HOST, ORIGINAL_HOST);    Assert.assertEquals(ORIGINAL_HOST, eventBeforeIntercept.getHeaders().get(Constants.HOST));    String expectedHost = InetAddress.getLocalHost().getHostAddress();    Event eventAfterIntercept = interceptor.intercept(eventBeforeIntercept);    String actualHost = eventAfterIntercept.getHeaders().get(Constants.HOST);    Assert.assertNotNull(actualHost);    Assert.assertEquals(expectedHost, actualHost);}
0
public void testUseHostname() throws Exception
{    Context ctx = new Context();    ctx.put("useIP", "false");    Interceptor.Builder builder = InterceptorBuilderFactory.newInstance(InterceptorType.HOST.toString());    builder.configure(ctx);    Interceptor interceptor = builder.build();    final String ORIGINAL_HOST = "originalhost";    Event eventBeforeIntercept = EventBuilder.withBody("test event", Charsets.UTF_8);    eventBeforeIntercept.getHeaders().put(Constants.HOST, ORIGINAL_HOST);    Assert.assertEquals(ORIGINAL_HOST, eventBeforeIntercept.getHeaders().get(Constants.HOST));    String expectedHost = InetAddress.getLocalHost().getCanonicalHostName();    Event eventAfterIntercept = interceptor.intercept(eventBeforeIntercept);    String actualHost = eventAfterIntercept.getHeaders().get(Constants.HOST);    Assert.assertNotNull(actualHost);    Assert.assertEquals(expectedHost, actualHost);}
0
public void init() throws Exception
{    fixtureBuilder = InterceptorBuilderFactory.newInstance(InterceptorType.REGEX_EXTRACTOR.toString());}
0
public void shouldNotAllowConfigurationWithoutRegex() throws Exception
{    try {        fixtureBuilder.build();        Assert.fail();    } catch (IllegalArgumentException ex) {        }}
0
public void shouldNotAllowConfigurationWithIllegalRegex() throws Exception
{    try {        Context context = new Context();        context.put(RegexExtractorInterceptor.REGEX, "?&?&&&?&?&?&&&??");        fixtureBuilder.configure(context);        fixtureBuilder.build();        Assert.fail();    } catch (IllegalArgumentException ex) {        }}
0
public void shouldNotAllowConfigurationWithoutMatchIds() throws Exception
{    try {        Context context = new Context();        context.put(RegexExtractorInterceptor.REGEX, ".*");        context.put(RegexExtractorInterceptor.SERIALIZERS, "");        fixtureBuilder.configure(context);        fixtureBuilder.build();        Assert.fail();    } catch (IllegalArgumentException ex) {        }}
0
public void shouldNotAllowMisconfiguredSerializers() throws Exception
{    try {        Context context = new Context();        context.put(RegexExtractorInterceptor.REGEX, "(\\d):(\\d):(\\d)");        context.put(RegexExtractorInterceptor.SERIALIZERS, ",,,");        fixtureBuilder.configure(context);        fixtureBuilder.build();        Assert.fail();    } catch (IllegalArgumentException ex) {        }}
0
public void shouldNotAllowEmptyNames() throws Exception
{    try {        String space = " ";        Context context = new Context();        context.put(RegexExtractorInterceptor.REGEX, "(\\d):(\\d):(\\d)");        context.put(RegexExtractorInterceptor.SERIALIZERS, Joiner.on(',').join(space, space, space));        fixtureBuilder.configure(context);        fixtureBuilder.build();        Assert.fail();    } catch (IllegalArgumentException ex) {        }}
0
public void shouldExtractAddHeadersForAllMatchGroups() throws Exception
{    Context context = new Context();    context.put(RegexExtractorInterceptor.REGEX, "(\\d):(\\d):(\\d)");    context.put(RegexExtractorInterceptor.SERIALIZERS, "s1 s2 s3");    context.put(RegexExtractorInterceptor.SERIALIZERS + ".s1.name", "Num1");    context.put(RegexExtractorInterceptor.SERIALIZERS + ".s2.name", "Num2");    context.put(RegexExtractorInterceptor.SERIALIZERS + ".s3.name", "Num3");    fixtureBuilder.configure(context);    Interceptor fixture = fixtureBuilder.build();    Event event = EventBuilder.withBody("1:2:3.4foobar5", Charsets.UTF_8);    Event expected = EventBuilder.withBody("1:2:3.4foobar5", Charsets.UTF_8);    expected.getHeaders().put("Num1", "1");    expected.getHeaders().put("Num2", "2");    expected.getHeaders().put("Num3", "3");    Event actual = fixture.intercept(event);    Assert.assertArrayEquals(expected.getBody(), actual.getBody());    Assert.assertEquals(expected.getHeaders(), actual.getHeaders());}
0
public void shouldExtractAddHeadersForAllMatchGroupsIgnoringMissingIds() throws Exception
{    String body = "2012-10-17 14:34:44,338";    Context context = new Context();        context.put(RegexExtractorInterceptor.REGEX, "^(\\d\\d\\d\\d-\\d\\d-\\d\\d\\s\\d\\d:\\d\\d)(:\\d\\d,\\d\\d\\d)");    context.put(RegexExtractorInterceptor.SERIALIZERS, "s1");    context.put(RegexExtractorInterceptor.SERIALIZERS + ".s1.name", "timestamp");    fixtureBuilder.configure(context);    Interceptor fixture = fixtureBuilder.build();    Event event = EventBuilder.withBody(body, Charsets.UTF_8);    Event expected = EventBuilder.withBody(body, Charsets.UTF_8);    expected.getHeaders().put("timestamp", "2012-10-17 14:34");    Event actual = fixture.intercept(event);    Assert.assertArrayEquals(expected.getBody(), actual.getBody());    Assert.assertEquals(expected.getHeaders(), actual.getHeaders());}
0
public void shouldExtractAddHeadersUsingSpecifiedSerializer() throws Exception
{    long now = (System.currentTimeMillis() / 60000L) * 60000L;    String pattern = "yyyy-MM-dd HH:mm:ss,SSS";    DateTimeFormatter formatter = DateTimeFormat.forPattern(pattern);    String body = formatter.print(now);    System.out.println(body);    Context context = new Context();        context.put(RegexExtractorInterceptor.REGEX, "^(\\d\\d\\d\\d-\\d\\d-\\d\\d\\s\\d\\d:\\d\\d)(:\\d\\d,\\d\\d\\d)");    context.put(RegexExtractorInterceptor.SERIALIZERS, "s1 s2");    String millisSerializers = RegexExtractorInterceptorMillisSerializer.class.getName();    context.put(RegexExtractorInterceptor.SERIALIZERS + ".s1.type", millisSerializers);    context.put(RegexExtractorInterceptor.SERIALIZERS + ".s1.name", "timestamp");    context.put(RegexExtractorInterceptor.SERIALIZERS + ".s1.pattern", "yyyy-MM-dd HH:mm");        context.put(RegexExtractorInterceptor.SERIALIZERS + ".s2.name", "data");    fixtureBuilder.configure(context);    Interceptor fixture = fixtureBuilder.build();    Event event = EventBuilder.withBody(body, Charsets.UTF_8);    Event expected = EventBuilder.withBody(body, Charsets.UTF_8);    expected.getHeaders().put("timestamp", String.valueOf(now));    expected.getHeaders().put("data", ":00,000");    Event actual = fixture.intercept(event);    Assert.assertArrayEquals(expected.getBody(), actual.getBody());    Assert.assertEquals(expected.getHeaders(), actual.getHeaders());}
0
public void shouldRequirePatternInConfiguration()
{    try {        RegexExtractorInterceptorMillisSerializer fixture = new RegexExtractorInterceptorMillisSerializer();        fixture.configure(new Context());        Assert.fail();    } catch (IllegalArgumentException ex) {        }}
0
public void shouldRequireValidPatternInConfiguration()
{    try {        RegexExtractorInterceptorMillisSerializer fixture = new RegexExtractorInterceptorMillisSerializer();        Context context = new Context();        context.put("pattern", "ABCDEFG");        fixture.configure(context);        Assert.fail();    } catch (IllegalArgumentException ex) {        }}
0
public void shouldReturnMillisFromPattern()
{    RegexExtractorInterceptorMillisSerializer fixture = new RegexExtractorInterceptorMillisSerializer();    Context context = new Context();    String pattern = "yyyy-MM-dd HH:mm:ss";    context.put("pattern", pattern);    fixture.configure(context);    DateTimeFormatter formatter = DateTimeFormat.forPattern(pattern);    long time = (System.currentTimeMillis() / 1000L) * 1000L;    Assert.assertEquals(String.valueOf(time), fixture.serialize(formatter.print(time)));}
0
public void shouldReturnSameValue()
{    RegexExtractorInterceptorPassThroughSerializer fixture = new RegexExtractorInterceptorPassThroughSerializer();    fixture.configure(new Context());    String input = "testing (1,2,3,4)";    Assert.assertEquals(input, fixture.serialize(input));}
0
public void testDefaultBehavior() throws ClassNotFoundException, InstantiationException, IllegalAccessException
{    Interceptor.Builder builder = InterceptorBuilderFactory.newInstance(InterceptorType.REGEX_FILTER.toString());    builder.configure(new Context());    Interceptor interceptor = builder.build();    Event event = EventBuilder.withBody("test", Charsets.UTF_8);    Event filteredEvent = interceptor.intercept(event);    Assert.assertNotNull(filteredEvent);    Assert.assertEquals(event, filteredEvent);}
0
public void testInclusion() throws ClassNotFoundException, InstantiationException, IllegalAccessException
{    Interceptor.Builder builder = InterceptorBuilderFactory.newInstance(InterceptorType.REGEX_FILTER.toString());    Context ctx = new Context();    ctx.put(Constants.REGEX, "(INFO.*)|(WARNING.*)");    ctx.put(Constants.EXCLUDE_EVENTS, "false");    builder.configure(ctx);    Interceptor interceptor = builder.build();    Event shouldPass1 = EventBuilder.withBody("INFO: some message", Charsets.UTF_8);    Assert.assertNotNull(interceptor.intercept(shouldPass1));    Event shouldPass2 = EventBuilder.withBody("WARNING: some message", Charsets.UTF_8);    Assert.assertNotNull(interceptor.intercept(shouldPass2));    Event shouldNotPass = EventBuilder.withBody("DEBUG: some message", Charsets.UTF_8);    Assert.assertNull(interceptor.intercept(shouldNotPass));    builder.configure(ctx);}
0
public void testExclusion() throws ClassNotFoundException, InstantiationException, IllegalAccessException
{    Interceptor.Builder builder = InterceptorBuilderFactory.newInstance(InterceptorType.REGEX_FILTER.toString());    Context ctx = new Context();    ctx.put(Constants.REGEX, ".*DEBUG.*");    ctx.put(Constants.EXCLUDE_EVENTS, "true");    builder.configure(ctx);    Interceptor interceptor = builder.build();    Event shouldPass1 = EventBuilder.withBody("INFO: some message", Charsets.UTF_8);    Assert.assertNotNull(interceptor.intercept(shouldPass1));    Event shouldPass2 = EventBuilder.withBody("WARNING: some message", Charsets.UTF_8);    Assert.assertNotNull(interceptor.intercept(shouldPass2));    Event shouldNotPass = EventBuilder.withBody("this message has DEBUG in it", Charsets.UTF_8);    Assert.assertNull(interceptor.intercept(shouldNotPass));    builder.configure(ctx);}
0
private void testSearchReplace(Context context, String input, String output) throws Exception
{    Interceptor.Builder builder = InterceptorBuilderFactory.newInstance(InterceptorType.SEARCH_REPLACE.toString());    builder.configure(context);    Interceptor interceptor = builder.build();    Event event = EventBuilder.withBody(input, Charsets.UTF_8);    event = interceptor.intercept(event);    String val = new String(event.getBody(), Charsets.UTF_8);    assertEquals(output, val);    }
1
public void testRemovePrefix() throws Exception
{    Context context = new Context();    context.put("searchPattern", "^prefix");    context.put("replaceString", "");    testSearchReplace(context, "prefix non-prefix suffix", " non-prefix suffix");}
0
public void testSyslogStripPriority() throws Exception
{    final String input = "<13>Feb  5 17:32:18 10.0.0.99 Use the BFG!";    final String output = "Feb  5 17:32:18 10.0.0.99 Use the BFG!";    Context context = new Context();    context.put("searchPattern", "^<[0-9]+>");    context.put("replaceString", "");    testSearchReplace(context, input, output);}
0
public void testCapturedGroups() throws Exception
{    final String input = "The quick brown fox jumped over the lazy dog.";    final String output = "The hungry dog ate the careless fox.";    Context context = new Context();    context.put("searchPattern", "The quick brown ([a-z]+) jumped over the lazy ([a-z]+).");    context.put("replaceString", "The hungry $2 ate the careless $1.");    testSearchReplace(context, input, output);}
0
public void testRepeatedRemoval() throws Exception
{    final String input = "Email addresses: test@test.com and foo@test.com";    final String output = "Email addresses: REDACTED and REDACTED";    Context context = new Context();    context.put("searchPattern", "[A-Za-z0-9_.]+@[A-Za-z0-9_-]+\\.com");    context.put("replaceString", "REDACTED");    testSearchReplace(context, input, output);}
0
public void testReplaceEmpty() throws Exception
{    final String input = "Abc123@test.com";    final String output = "@test.com";    Context context = new Context();    context.put("searchPattern", "^[A-Za-z0-9_]+");    testSearchReplace(context, input, output);    context.put("replaceString", "");    testSearchReplace(context, input, output);}
0
public void testDefaultKeyValue() throws ClassNotFoundException, InstantiationException, IllegalAccessException
{    Interceptor.Builder builder = InterceptorBuilderFactory.newInstance(InterceptorType.STATIC.toString());    builder.configure(new Context());    Interceptor interceptor = builder.build();    Event event = EventBuilder.withBody("test", Charsets.UTF_8);    Assert.assertNull(event.getHeaders().get(Constants.KEY));    event = interceptor.intercept(event);    String val = event.getHeaders().get(Constants.KEY);    Assert.assertNotNull(val);    Assert.assertEquals(Constants.VALUE, val);}
0
public void testCustomKeyValue() throws ClassNotFoundException, InstantiationException, IllegalAccessException
{    Interceptor.Builder builder = InterceptorBuilderFactory.newInstance(InterceptorType.STATIC.toString());    Context ctx = new Context();    ctx.put(Constants.KEY, "myKey");    ctx.put(Constants.VALUE, "myVal");    builder.configure(ctx);    Interceptor interceptor = builder.build();    Event event = EventBuilder.withBody("test", Charsets.UTF_8);    Assert.assertNull(event.getHeaders().get("myKey"));    event = interceptor.intercept(event);    String val = event.getHeaders().get("myKey");    Assert.assertNotNull(val);    Assert.assertEquals("myVal", val);}
0
public void testReplace() throws ClassNotFoundException, InstantiationException, IllegalAccessException
{    Interceptor.Builder builder = InterceptorBuilderFactory.newInstance(InterceptorType.STATIC.toString());    Context ctx = new Context();    ctx.put(Constants.PRESERVE, "false");    ctx.put(Constants.VALUE, "replacement value");    builder.configure(ctx);    Interceptor interceptor = builder.build();    Event event = EventBuilder.withBody("test", Charsets.UTF_8);    event.getHeaders().put(Constants.KEY, "incumbent value");    Assert.assertNotNull(event.getHeaders().get(Constants.KEY));    event = interceptor.intercept(event);    String val = event.getHeaders().get(Constants.KEY);    Assert.assertNotNull(val);    Assert.assertEquals("replacement value", val);}
0
public void testPreserve() throws ClassNotFoundException, InstantiationException, IllegalAccessException
{    Interceptor.Builder builder = InterceptorBuilderFactory.newInstance(InterceptorType.STATIC.toString());    Context ctx = new Context();    ctx.put(Constants.PRESERVE, "true");    ctx.put(Constants.VALUE, "replacement value");    builder.configure(ctx);    Interceptor interceptor = builder.build();    Event event = EventBuilder.withBody("test", Charsets.UTF_8);    event.getHeaders().put(Constants.KEY, "incumbent value");    Assert.assertNotNull(event.getHeaders().get(Constants.KEY));    event = interceptor.intercept(event);    String val = event.getHeaders().get(Constants.KEY);    Assert.assertNotNull(val);    Assert.assertEquals("incumbent value", val);}
0
public void testBasic() throws ClassNotFoundException, InstantiationException, IllegalAccessException
{    Interceptor.Builder builder = InterceptorBuilderFactory.newInstance(InterceptorType.TIMESTAMP.toString());    Interceptor interceptor = builder.build();    Event event = EventBuilder.withBody("test event", Charsets.UTF_8);    Assert.assertNull(event.getHeaders().get(Constants.DEFAULT_HEADER_NAME));    Long now = System.currentTimeMillis();    event = interceptor.intercept(event);    String timestampStr = event.getHeaders().get(Constants.DEFAULT_HEADER_NAME);    Assert.assertNotNull(timestampStr);    Assert.assertTrue(Long.parseLong(timestampStr) >= now);}
0
public void testPreserve() throws ClassNotFoundException, InstantiationException, IllegalAccessException
{    Context ctx = new Context();    ctx.put("preserveExisting", "true");    Interceptor.Builder builder = InterceptorBuilderFactory.newInstance(InterceptorType.TIMESTAMP.toString());    builder.configure(ctx);    Interceptor interceptor = builder.build();    long originalTs = 1L;    Event event = EventBuilder.withBody("test event", Charsets.UTF_8);    event.getHeaders().put(Constants.DEFAULT_HEADER_NAME, Long.toString(originalTs));    Assert.assertEquals(Long.toString(originalTs), event.getHeaders().get(Constants.DEFAULT_HEADER_NAME));    event = interceptor.intercept(event);    String timestampStr = event.getHeaders().get(Constants.DEFAULT_HEADER_NAME);    Assert.assertNotNull(timestampStr);    Assert.assertTrue(Long.parseLong(timestampStr) == originalTs);}
0
public void testClobber() throws ClassNotFoundException, InstantiationException, IllegalAccessException
{    Context ctx = new Context();        ctx.put("preserveExisting", "false");    Interceptor.Builder builder = InterceptorBuilderFactory.newInstance(InterceptorType.TIMESTAMP.toString());    builder.configure(ctx);    Interceptor interceptor = builder.build();    long originalTs = 1L;    Event event = EventBuilder.withBody("test event", Charsets.UTF_8);    event.getHeaders().put(Constants.DEFAULT_HEADER_NAME, Long.toString(originalTs));    Assert.assertEquals(Long.toString(originalTs), event.getHeaders().get(Constants.DEFAULT_HEADER_NAME));    Long now = System.currentTimeMillis();    event = interceptor.intercept(event);    String timestampStr = event.getHeaders().get(Constants.DEFAULT_HEADER_NAME);    Assert.assertNotNull(timestampStr);    Assert.assertTrue(Long.parseLong(timestampStr) >= now);}
0
public void testCustomHeader() throws Exception
{    Context ctx = new Context();    ctx.put(TimestampInterceptor.Constants.CONFIG_HEADER_NAME, "timestampHeader");    Interceptor.Builder builder = InterceptorBuilderFactory.newInstance(InterceptorType.TIMESTAMP.toString());    builder.configure(ctx);    Interceptor interceptor = builder.build();    long originalTs = 1L;    Event event = EventBuilder.withBody("test event", Charsets.UTF_8);    event.getHeaders().put(Constants.DEFAULT_HEADER_NAME, Long.toString(originalTs));    Long now = System.currentTimeMillis();    event = interceptor.intercept(event);    Assert.assertEquals(Long.toString(originalTs), event.getHeaders().get(Constants.DEFAULT_HEADER_NAME));    String timestampStr = event.getHeaders().get("timestampHeader");    Assert.assertNotNull(timestampStr);    Assert.assertTrue(Long.parseLong(timestampStr) >= now);}
0
public void testWaitForState() throws LifecycleException, InterruptedException
{    LifecycleAware delegate = new SleeperLifecycleDelegate();    Assert.assertTrue(delegate.getLifecycleState().equals(LifecycleState.IDLE));    delegate.start();    boolean reached = LifecycleController.waitForState(delegate, LifecycleState.START, 2000);    Assert.assertEquals(true, reached);    Assert.assertEquals(LifecycleState.START, delegate.getLifecycleState());    delegate.stop();    reached = LifecycleController.waitForState(delegate, LifecycleState.STOP, 2000);    Assert.assertEquals(true, reached);    Assert.assertEquals(LifecycleState.STOP, delegate.getLifecycleState());    delegate.start();    reached = LifecycleController.waitForState(delegate, LifecycleState.IDLE, 500);    Assert.assertEquals(false, reached);    Assert.assertEquals(LifecycleState.START, delegate.getLifecycleState());}
0
public void testWaitForOneOf() throws LifecycleException, InterruptedException
{    LifecycleAware delegate = new SleeperLifecycleDelegate();    Assert.assertEquals(LifecycleState.IDLE, delegate.getLifecycleState());    delegate.start();    boolean reached = LifecycleController.waitForOneOf(delegate, new LifecycleState[] { LifecycleState.STOP, LifecycleState.START }, 2000);    Assert.assertTrue("Matched a state change", reached);    Assert.assertEquals(LifecycleState.START, delegate.getLifecycleState());}
0
public void start()
{    try {        Thread.sleep(sleepTime);    } catch (InterruptedException e) {        }    state = LifecycleState.START;}
0
public void stop()
{    try {        Thread.sleep(sleepTime);    } catch (InterruptedException e) {        }    state = LifecycleState.STOP;}
0
public LifecycleState getLifecycleState()
{    return state;}
0
public long getSleepTime()
{    return sleepTime;}
0
public void setSleepTime(long sleepTime)
{    this.sleepTime = sleepTime;}
0
public void setUp()
{    supervisor = new LifecycleSupervisor();}
0
public void testLifecycle() throws LifecycleException, InterruptedException
{    supervisor.start();    supervisor.stop();}
0
public void testSupervise() throws LifecycleException, InterruptedException
{    supervisor.start();    /* Attempt to supervise a known-to-fail config. */    /*     * LogicalNode node = new LogicalNode(); SupervisorPolicy policy = new     * SupervisorPolicy.OnceOnlyPolicy(); supervisor.supervise(node, policy,     * LifecycleState.START);     */    CountingLifecycleAware node = new CountingLifecycleAware();    SupervisorPolicy policy = new SupervisorPolicy.OnceOnlyPolicy();    supervisor.supervise(node, policy, LifecycleState.START);    Thread.sleep(10000);    node = new CountingLifecycleAware();    policy = new SupervisorPolicy.OnceOnlyPolicy();    supervisor.supervise(node, policy, LifecycleState.START);    Thread.sleep(5000);    supervisor.stop();}
0
public void testSuperviseBroken() throws LifecycleException, InterruptedException
{    supervisor.start();    /* Attempt to supervise a known-to-fail config. */    LifecycleAware node = new LifecycleAware() {        @Override        public void stop() {        }        @Override        public void start() {            throw new NullPointerException("Boom!");        }        @Override        public LifecycleState getLifecycleState() {            return LifecycleState.IDLE;        }    };    SupervisorPolicy policy = new SupervisorPolicy.OnceOnlyPolicy();    supervisor.supervise(node, policy, LifecycleState.START);    Thread.sleep(5000);    supervisor.stop();}
0
public void stop()
{}
0
public void start()
{    throw new NullPointerException("Boom!");}
0
public LifecycleState getLifecycleState()
{    return LifecycleState.IDLE;}
0
public void testSuperviseSupervisor() throws LifecycleException, InterruptedException
{    supervisor.start();    LifecycleSupervisor supervisor2 = new LifecycleSupervisor();    CountingLifecycleAware node = new CountingLifecycleAware();    SupervisorPolicy policy = new SupervisorPolicy.OnceOnlyPolicy();    supervisor2.supervise(node, policy, LifecycleState.START);    supervisor.supervise(supervisor2, new SupervisorPolicy.AlwaysRestartPolicy(), LifecycleState.START);    Thread.sleep(10000);    supervisor.stop();}
0
public void testUnsuperviseServce() throws LifecycleException, InterruptedException
{    supervisor.start();    LifecycleAware service = new CountingLifecycleAware();    SupervisorPolicy policy = new SupervisorPolicy.OnceOnlyPolicy();    supervisor.supervise(service, policy, LifecycleState.START);    supervisor.unsupervise(service);    service.stop();    supervisor.stop();}
0
public void testStopServce() throws LifecycleException, InterruptedException
{    supervisor.start();    CountingLifecycleAware service = new CountingLifecycleAware();    SupervisorPolicy policy = new SupervisorPolicy.OnceOnlyPolicy();    Assert.assertEquals(Long.valueOf(0), service.counterGroup.get("start"));    Assert.assertEquals(Long.valueOf(0), service.counterGroup.get("stop"));    supervisor.supervise(service, policy, LifecycleState.START);    Thread.sleep(3200);    Assert.assertEquals(Long.valueOf(1), service.counterGroup.get("start"));    Assert.assertEquals(Long.valueOf(0), service.counterGroup.get("stop"));    supervisor.setDesiredState(service, LifecycleState.STOP);    Thread.sleep(3200);    Assert.assertEquals(Long.valueOf(1), service.counterGroup.get("start"));    Assert.assertEquals(Long.valueOf(1), service.counterGroup.get("stop"));    supervisor.stop();}
0
public void start()
{    counterGroup.incrementAndGet("start");    lifecycleState = LifecycleState.START;}
0
public void stop()
{    counterGroup.incrementAndGet("stop");    lifecycleState = LifecycleState.STOP;}
0
public LifecycleState getLifecycleState()
{    return lifecycleState;}
0
public int readChar() throws IOException
{    if (curPos >= str.length()) {        return -1;    }    return str.charAt(curPos++);}
0
public void mark() throws IOException
{    markPos = curPos;}
0
public void reset() throws IOException
{    curPos = markPos;}
0
public void seek(long position) throws IOException
{    throw new UnsupportedOperationException("Unimplemented in test class");}
0
public long tell() throws IOException
{    throw new UnsupportedOperationException("Unimplemented in test class");}
0
public int read() throws IOException
{    throw new UnsupportedOperationException("This test class doesn't return " + "bytes!");}
0
public int read(byte[] b, int off, int len) throws IOException
{    throw new UnsupportedOperationException("This test class doesn't return " + "bytes!");}
0
public void close() throws IOException
{}
0
protected OutputStream getOutputStream()
{    return out;}
0
protected Schema getSchema()
{    return schema;}
0
protected SyslogEvent convert(Event event)
{    SyslogEvent sle = new SyslogEvent();            String msg = new String(event.getBody(), Charsets.UTF_8);        int seek = 0;                Map<String, String> headers = event.getHeaders();    boolean fromSyslogSource = false;    if (headers.containsKey(SyslogUtils.SYSLOG_FACILITY)) {        fromSyslogSource = true;        int facility = Integer.parseInt(headers.get(SyslogUtils.SYSLOG_FACILITY));        sle.setFacility(facility);    }    if (headers.containsKey(SyslogUtils.SYSLOG_SEVERITY)) {        fromSyslogSource = true;        int severity = Integer.parseInt(headers.get(SyslogUtils.SYSLOG_SEVERITY));        sle.setSeverity(severity);    }        if (!fromSyslogSource) {        if (msg.charAt(0) == '<') {            int end = msg.indexOf(">");            if (end > -1) {                seek = end + 1;                String priStr = msg.substring(1, end);                int priority = Integer.parseInt(priStr);                int severity = priority % 8;                int facility = (priority - severity) / 8;                sle.setFacility(facility);                sle.setSeverity(severity);            }        }    }        String timestampStr = msg.substring(seek, seek + 15);    long ts = parseRfc3164Date(timestampStr);    if (ts != 0) {        sle.setTimestamp(ts);                seek += 15 + 1;    }        int nextSpace = msg.indexOf(' ', seek);    if (nextSpace > -1) {        String hostname = msg.substring(seek, nextSpace);        sle.setHostname(hostname);        seek = nextSpace + 1;    }        String actualMessage = msg.substring(seek);    sle.setMessage(actualMessage);    if (logger.isDebugEnabled() && LogPrivacyUtil.allowLogRawData()) {            }    return sle;}
1
private static long parseRfc3164Date(String in)
{    DateTime date = null;    try {        date = dateFmt1.parseDateTime(in);    } catch (IllegalArgumentException e) {                    }    if (date == null) {        try {            date = dateFmt2.parseDateTime(in);        } catch (IllegalArgumentException e) {                                }    }        if (date != null) {        DateTime now = new DateTime();        int year = now.getYear();        DateTime corrected = date.withYear(year);                if (corrected.isAfter(now) && corrected.minusMonths(1).isAfter(now)) {            corrected = date.minusYears(1);                } else if (corrected.isBefore(now) && corrected.plusMonths(1).isBefore(now)) {            corrected = date.plusYears(1);        }        date = corrected;    }    if (date == null) {        return 0;    }    return date.getMillis();}
1
public EventSerializer build(Context context, OutputStream out)
{    SyslogAvroEventSerializer writer = null;    try {        writer = new SyslogAvroEventSerializer(out);        writer.configure(context);    } catch (IOException e) {            }    return writer;}
1
public void setFacility(int f)
{    facility = f;}
0
public int getFacility()
{    return facility;}
0
public void setSeverity(int s)
{    severity = s;}
0
public int getSeverity()
{    return severity;}
0
public void setTimestamp(long t)
{    timestamp = t;}
0
public long getTimestamp()
{    return timestamp;}
0
public void setHostname(String h)
{    hostname = h;}
0
public String getHostname()
{    return hostname;}
0
public void setMessage(String m)
{    message = m;}
0
public String getMessage()
{    return message;}
0
public String toString()
{    StringBuilder builder = new StringBuilder();    builder.append("{ Facility: ").append(facility).append(", ");    builder.append(" Severity: ").append(severity).append(", ");    builder.append(" Timestamp: ").append(timestamp).append(", ");    builder.append(" Hostname: ").append(hostname).append(", ");    builder.append(" Message: \"").append(message).append("\" }");    return builder.toString();}
0
public void resetTest() throws IOException
{    File tempFile = newTestFile(true);    String target = tempFile.getAbsolutePath();        TransientPositionTracker tracker = new TransientPositionTracker(target);    AvroEventDeserializer.Builder desBuilder = new AvroEventDeserializer.Builder();    EventDeserializer deserializer = desBuilder.build(new Context(), new ResettableFileInputStream(tempFile, tracker));    BinaryDecoder decoder = null;    DatumReader<GenericRecord> reader = new GenericDatumReader<GenericRecord>(schema);    decoder = DecoderFactory.get().binaryDecoder(deserializer.readEvent().getBody(), decoder);    assertEquals("bar", reader.read(null, decoder).get("foo").toString());    deserializer.reset();    decoder = DecoderFactory.get().binaryDecoder(deserializer.readEvent().getBody(), decoder);    assertEquals("bar", reader.read(null, decoder).get("foo").toString());    deserializer.mark();    decoder = DecoderFactory.get().binaryDecoder(deserializer.readEvent().getBody(), decoder);    assertEquals("baz", reader.read(null, decoder).get("foo").toString());    deserializer.reset();    decoder = DecoderFactory.get().binaryDecoder(deserializer.readEvent().getBody(), decoder);    assertEquals("baz", reader.read(null, decoder).get("foo").toString());    assertNull(deserializer.readEvent());}
1
public void testSchemaHash() throws IOException, NoSuchAlgorithmException
{    File tempFile = newTestFile(true);    String target = tempFile.getAbsolutePath();        TransientPositionTracker tracker = new TransientPositionTracker(target);    Context context = new Context();    context.put(AvroEventDeserializer.CONFIG_SCHEMA_TYPE_KEY, AvroEventDeserializer.AvroSchemaType.HASH.toString());    ResettableInputStream in = new ResettableFileInputStream(tempFile, tracker);    EventDeserializer des = new AvroEventDeserializer.Builder().build(context, in);    Event event = des.readEvent();    String eventSchemaHash = event.getHeaders().get(AvroEventDeserializer.AVRO_SCHEMA_HEADER_HASH);    String expectedSchemaHash = Hex.encodeHexString(SchemaNormalization.parsingFingerprint("CRC-64-AVRO", schema));    Assert.assertEquals(expectedSchemaHash, eventSchemaHash);}
1
public void testSchemaLiteral() throws IOException
{    File tempFile = newTestFile(true);    String target = tempFile.getAbsolutePath();        TransientPositionTracker tracker = new TransientPositionTracker(target);    Context context = new Context();    context.put(AvroEventDeserializer.CONFIG_SCHEMA_TYPE_KEY, AvroEventDeserializer.AvroSchemaType.LITERAL.toString());    ResettableInputStream in = new ResettableFileInputStream(tempFile, tracker);    EventDeserializer des = new AvroEventDeserializer.Builder().build(context, in);    Event event = des.readEvent();    String eventSchema = event.getHeaders().get(AvroEventDeserializer.AVRO_SCHEMA_HEADER_LITERAL);    Assert.assertEquals(schema.toString(), eventSchema);}
1
private File newTestFile(boolean deleteOnExit) throws IOException
{    File tempFile = File.createTempFile("testDirectFile", "tmp");    if (deleteOnExit) {        tempFile.deleteOnExit();    }    DataFileWriter<GenericRecord> writer = new DataFileWriter<GenericRecord>(new GenericDatumWriter<GenericRecord>(schema));    writer.create(schema, tempFile);    GenericRecordBuilder recordBuilder;    recordBuilder = new GenericRecordBuilder(schema);    recordBuilder.set("foo", "bar");    GenericRecord record = recordBuilder.build();    writer.append(record);    writer.sync();    recordBuilder = new GenericRecordBuilder(schema);    recordBuilder.set("foo", "baz");    record = recordBuilder.build();    writer.append(record);    writer.sync();    writer.flush();    writer.close();    return tempFile;}
0
public void testWithNewline() throws FileNotFoundException, IOException
{    OutputStream out = new FileOutputStream(testFile);    EventSerializer serializer = EventSerializerFactory.getInstance("text", new Context(), out);    serializer.afterCreate();    serializer.write(EventBuilder.withBody("event 1", Charsets.UTF_8));    serializer.write(EventBuilder.withBody("event 2", Charsets.UTF_8));    serializer.write(EventBuilder.withBody("event 3", Charsets.UTF_8));    serializer.flush();    serializer.beforeClose();    out.flush();    out.close();    BufferedReader reader = new BufferedReader(new FileReader(testFile));    Assert.assertEquals("event 1", reader.readLine());    Assert.assertEquals("event 2", reader.readLine());    Assert.assertEquals("event 3", reader.readLine());    Assert.assertNull(reader.readLine());    reader.close();    FileUtils.forceDelete(testFile);}
0
public void testNoNewline() throws FileNotFoundException, IOException
{    OutputStream out = new FileOutputStream(testFile);    Context context = new Context();    context.put("appendNewline", "false");    EventSerializer serializer = EventSerializerFactory.getInstance("text", context, out);    serializer.afterCreate();    serializer.write(EventBuilder.withBody("event 1\n", Charsets.UTF_8));    serializer.write(EventBuilder.withBody("event 2\n", Charsets.UTF_8));    serializer.write(EventBuilder.withBody("event 3\n", Charsets.UTF_8));    serializer.flush();    serializer.beforeClose();    out.flush();    out.close();    BufferedReader reader = new BufferedReader(new FileReader(testFile));    Assert.assertEquals("event 1", reader.readLine());    Assert.assertEquals("event 2", reader.readLine());    Assert.assertEquals("event 3", reader.readLine());    Assert.assertNull(reader.readLine());    reader.close();    FileUtils.forceDelete(testFile);}
0
public void testBasicTracker() throws IOException
{    File metaFile = File.createTempFile(getClass().getName(), ".meta");    metaFile.delete();    File dataFile = File.createTempFile(getClass().getName(), ".data");    Files.write("line 1\nline2\n", dataFile, Charsets.UTF_8);    final long NEW_POS = 7;    PositionTracker tracker;    tracker = new DurablePositionTracker(metaFile, dataFile.toString());    Assert.assertEquals(0, tracker.getPosition());    tracker.storePosition(NEW_POS);    Assert.assertEquals(NEW_POS, tracker.getPosition());    tracker.close();        tracker = new DurablePositionTracker(metaFile, "foobar");    Assert.assertEquals(NEW_POS, tracker.getPosition());    Assert.assertEquals(dataFile.getAbsolutePath(), tracker.getTarget());}
0
public void testGoodTrackerFile() throws IOException, URISyntaxException
{    String fileName = "/TestResettableFileInputStream_1.avro";    File trackerFile = new File(getClass().getResource(fileName).toURI());    Assert.assertTrue(trackerFile.exists());    PositionTracker tracker;    tracker = new DurablePositionTracker(trackerFile, "foo");        Assert.assertEquals(62, tracker.getPosition());}
0
public void testPartialTrackerFile() throws IOException, URISyntaxException
{    String fileName = "/TestResettableFileInputStream_1.truncated.avro";    File trackerFile = new File(getClass().getResource(fileName).toURI());    Assert.assertTrue(trackerFile.exists());    PositionTracker tracker;    tracker = new DurablePositionTracker(trackerFile, "foo");        Assert.assertEquals(25, tracker.getPosition());}
0
public void testAvroSerializer() throws FileNotFoundException, IOException
{    createAvroFile(TESTFILE, null);    validateAvroFile(TESTFILE);    FileUtils.forceDelete(TESTFILE);}
0
public void testAvroSerializerNullCompression() throws FileNotFoundException, IOException
{    createAvroFile(TESTFILE, "null");    validateAvroFile(TESTFILE);    FileUtils.forceDelete(TESTFILE);}
0
public void testAvroSerializerDeflateCompression() throws FileNotFoundException, IOException
{    createAvroFile(TESTFILE, "deflate");    validateAvroFile(TESTFILE);    FileUtils.forceDelete(TESTFILE);}
0
public void testAvroSerializerSnappyCompression() throws FileNotFoundException, IOException
{        Assume.assumeTrue(!"Mac OS X".equals(System.getProperty("os.name")) || !System.getProperty("java.version").startsWith("1.7."));    createAvroFile(TESTFILE, "snappy");    validateAvroFile(TESTFILE);    FileUtils.forceDelete(TESTFILE);}
0
public void createAvroFile(File file, String codec) throws FileNotFoundException, IOException
{    if (file.exists()) {        FileUtils.forceDelete(file);    }        OutputStream out = new FileOutputStream(file);    Context ctx = new Context();    if (codec != null) {        ctx.put("compressionCodec", codec);    }    EventSerializer.Builder builder = new FlumeEventAvroEventSerializer.Builder();    EventSerializer serializer = builder.build(ctx, out);    serializer.afterCreate();    serializer.write(EventBuilder.withBody("yo man!", Charsets.UTF_8));    serializer.write(EventBuilder.withBody("2nd event!", Charsets.UTF_8));    serializer.write(EventBuilder.withBody("last one!", Charsets.UTF_8));    serializer.flush();    serializer.beforeClose();    out.flush();    out.close();}
0
public void validateAvroFile(File file) throws IOException
{        DatumReader<GenericRecord> reader = new GenericDatumReader<GenericRecord>();    DataFileReader<GenericRecord> fileReader = new DataFileReader<GenericRecord>(file, reader);    GenericRecord record = new GenericData.Record(fileReader.getSchema());    int numEvents = 0;    while (fileReader.hasNext()) {        fileReader.next(record);        ByteBuffer body = (ByteBuffer) record.get("body");        CharsetDecoder decoder = Charsets.UTF_8.newDecoder();        String bodyStr = decoder.decode(body).toString();        System.out.println(bodyStr);        numEvents++;    }    fileReader.close();    Assert.assertEquals("Should have found a total of 3 events", 3, numEvents);}
0
public void testWithNewline() throws FileNotFoundException, IOException
{    Map<String, String> headers = new HashMap<String, String>();    headers.put("header1", "value1");    headers.put("header2", "value2");    OutputStream out = new FileOutputStream(testFile);    EventSerializer serializer = EventSerializerFactory.getInstance("header_and_text", new Context(), out);    serializer.afterCreate();    serializer.write(EventBuilder.withBody("event 1", Charsets.UTF_8, headers));    serializer.write(EventBuilder.withBody("event 2", Charsets.UTF_8, headers));    serializer.write(EventBuilder.withBody("event 3", Charsets.UTF_8, headers));    serializer.flush();    serializer.beforeClose();    out.flush();    out.close();    BufferedReader reader = new BufferedReader(new FileReader(testFile));    Assert.assertEquals("{header2=value2, header1=value1} event 1", reader.readLine());    Assert.assertEquals("{header2=value2, header1=value1} event 2", reader.readLine());    Assert.assertEquals("{header2=value2, header1=value1} event 3", reader.readLine());    Assert.assertNull(reader.readLine());    reader.close();    FileUtils.forceDelete(testFile);}
0
public void testNoNewline() throws FileNotFoundException, IOException
{    Map<String, String> headers = new HashMap<String, String>();    headers.put("header1", "value1");    headers.put("header2", "value2");    OutputStream out = new FileOutputStream(testFile);    Context context = new Context();    context.put("appendNewline", "false");    EventSerializer serializer = EventSerializerFactory.getInstance("header_and_text", context, out);    serializer.afterCreate();    serializer.write(EventBuilder.withBody("event 1\n", Charsets.UTF_8, headers));    serializer.write(EventBuilder.withBody("event 2\n", Charsets.UTF_8, headers));    serializer.write(EventBuilder.withBody("event 3\n", Charsets.UTF_8, headers));    serializer.flush();    serializer.beforeClose();    out.flush();    out.close();    BufferedReader reader = new BufferedReader(new FileReader(testFile));    Assert.assertEquals("{header2=value2, header1=value1} event 1", reader.readLine());    Assert.assertEquals("{header2=value2, header1=value1} event 2", reader.readLine());    Assert.assertEquals("{header2=value2, header1=value1} event 3", reader.readLine());    Assert.assertNull(reader.readLine());    reader.close();    FileUtils.forceDelete(testFile);}
0
public void setup()
{    StringBuilder sb = new StringBuilder();    sb.append("line 1\n");    sb.append("line 2\n");    mini = sb.toString();}
0
public void testSimple() throws IOException
{    ResettableInputStream in = new ResettableTestStringInputStream(mini);    EventDeserializer des = new LineDeserializer(new Context(), in);    validateMiniParse(des);}
0
public void testSimpleViaBuilder() throws IOException
{    ResettableInputStream in = new ResettableTestStringInputStream(mini);    EventDeserializer.Builder builder = new LineDeserializer.Builder();    EventDeserializer des = builder.build(new Context(), in);    validateMiniParse(des);}
0
public void testSimpleViaFactory() throws IOException
{    ResettableInputStream in = new ResettableTestStringInputStream(mini);    EventDeserializer des;    des = EventDeserializerFactory.getInstance("LINE", new Context(), in);    validateMiniParse(des);}
0
public void testBatch() throws IOException
{    ResettableInputStream in = new ResettableTestStringInputStream(mini);    EventDeserializer des = new LineDeserializer(new Context(), in);    List<Event> events;        events = des.readEvents(1);    Assert.assertEquals(1, events.size());    assertEventBodyEquals("line 1", events.get(0));        events = des.readEvents(10);    Assert.assertEquals(1, events.size());    assertEventBodyEquals("line 2", events.get(0));    des.mark();    des.close();}
0
public void testMaxLineLength() throws IOException
{    String longLine = "abcdefghijklmnopqrstuvwxyz\n";    Context ctx = new Context();    ctx.put(LineDeserializer.MAXLINE_KEY, "10");    ResettableInputStream in = new ResettableTestStringInputStream(longLine);    EventDeserializer des = new LineDeserializer(ctx, in);    assertEventBodyEquals("abcdefghij", des.readEvent());    assertEventBodyEquals("klmnopqrst", des.readEvent());    assertEventBodyEquals("uvwxyz", des.readEvent());    Assert.assertNull(des.readEvent());}
0
private void assertEventBodyEquals(String expected, Event event)
{    String bodyStr = new String(event.getBody(), Charsets.UTF_8);    Assert.assertEquals(expected, bodyStr);}
0
private void validateMiniParse(EventDeserializer des) throws IOException
{    Event evt;    evt = des.readEvent();    Assert.assertEquals(new String(evt.getBody()), "line 1");    des.mark();    evt = des.readEvent();    Assert.assertEquals(new String(evt.getBody()), "line 2");        des.reset();    evt = des.readEvent();    Assert.assertEquals("Line 2 should be repeated, " + "because we reset() the stream", new String(evt.getBody()), "line 2");    evt = des.readEvent();    Assert.assertNull("Event should be null because there are no lines " + "left to read", evt);    des.mark();    des.close();}
0
public void setup() throws Exception
{    Files.createParentDirs(new File(WORK_DIR, "dummy"));    file = File.createTempFile(getClass().getSimpleName(), ".txt", WORK_DIR);        meta = File.createTempFile(getClass().getSimpleName(), ".avro", WORK_DIR);            meta.delete();}
1
public void tearDown() throws Exception
{    if (CLEANUP) {        meta.delete();        file.delete();    }}
0
public void testBasicRead() throws IOException
{    String output = singleLineFileInit(file, Charsets.UTF_8);    PositionTracker tracker = new DurablePositionTracker(meta, file.getPath());    ResettableInputStream in = new ResettableFileInputStream(file, tracker);    String result = readLine(in, output.length());    assertEquals(output, result);    String afterEOF = readLine(in, output.length());    assertNull(afterEOF);    in.close();}
0
public void testReadByte() throws IOException
{    byte[] bytes = new byte[255];    for (int i = 0; i < 255; i++) {        bytes[i] = (byte) i;    }    Files.write(bytes, file);    PositionTracker tracker = new DurablePositionTracker(meta, file.getPath());    ResettableInputStream in = new ResettableFileInputStream(file, tracker);    for (int i = 0; i < 255; i++) {        assertEquals(i, in.read());    }    assertEquals(-1, in.read());    in.close();}
0
public void testMultiByteCharRead() throws IOException
{    ByteArrayOutputStream out = new ByteArrayOutputStream();    out.write("1234567".getBytes(Charsets.UTF_8));        generateUtf83ByteSequence(out);        Files.write(out.toByteArray(), file);    ResettableInputStream in = initInputStream(8, Charsets.UTF_8, DecodeErrorPolicy.FAIL);    String result = readLine(in, 8);    assertEquals("1234567\u0A93\n", result);}
0
public void testUtf8SurrogatePairRead() throws IOException
{    ByteArrayOutputStream out = new ByteArrayOutputStream();    out.write("1234567".getBytes(Charsets.UTF_8));    generateUtf8SurrogatePairSequence(out);            Files.write(out.toByteArray(), file);    ResettableInputStream in = initInputStream(8, Charsets.UTF_8, DecodeErrorPolicy.FAIL);    String result = readLine(in, 9);    assertEquals("1234567\uD83D\uDE18\n", result);}
0
public void testUtf16BOMAndSurrogatePairRead() throws IOException
{    ByteArrayOutputStream out = new ByteArrayOutputStream();    generateUtf16SurrogatePairSequence(out);            Files.write(out.toByteArray(), file);    ResettableInputStream in = initInputStream(8, Charsets.UTF_16, DecodeErrorPolicy.FAIL);    String result = readLine(in, 2);    assertEquals("\uD83D\uDE18\n", result);}
0
public void testShiftJisSurrogateCharRead() throws IOException
{    ByteArrayOutputStream out = new ByteArrayOutputStream();    out.write("1234567".getBytes(Charset.forName("Shift_JIS")));        generateShiftJis2ByteSequence(out);        Files.write(out.toByteArray(), file);    ResettableInputStream in = initInputStream(8, Charset.forName("Shift_JIS"), DecodeErrorPolicy.FAIL);    String result = readLine(in, 8);    assertEquals("1234567\u4E9C\n", result);}
0
public void testUtf8DecodeErrorHandlingFailMalformed() throws IOException
{    ResettableInputStream in = initUtf8DecodeTest(DecodeErrorPolicy.FAIL);    while (in.readChar() != -1) {        }    fail("Expected MalformedInputException!");}
0
public void testUtf8DecodeErrorHandlingIgnore() throws IOException
{    ResettableInputStream in = initUtf8DecodeTest(DecodeErrorPolicy.IGNORE);    int c;    StringBuilder sb = new StringBuilder();    while ((c = in.readChar()) != -1) {        sb.append((char) c);    }    assertEquals("Latin1: ()\nLong: ()\nNonUnicode: ()\n", sb.toString());}
0
public void testUtf8DecodeErrorHandlingReplace() throws IOException
{    ResettableInputStream in = initUtf8DecodeTest(DecodeErrorPolicy.REPLACE);    int c;    StringBuilder sb = new StringBuilder();    while ((c = in.readChar()) != -1) {        sb.append((char) c);    }    String preJdk8ExpectedStr = "Latin1: (X)\nLong: (XXX)\nNonUnicode: (X)\n";    String expectedStr = "Latin1: (X)\nLong: (XXX)\nNonUnicode: (XXXXX)\n";    String javaVersionStr = System.getProperty("java.version");    double javaVersion = Double.parseDouble(javaVersionStr.substring(0, 3));    if (javaVersion < 1.8) {        assertTrue(preJdk8ExpectedStr.replaceAll("X", "\ufffd").equals(sb.toString()));    } else {        assertTrue(expectedStr.replaceAll("X", "\ufffd").equals(sb.toString()));    }}
0
public void testLatin1DecodeErrorHandlingFailMalformed() throws IOException
{    ByteArrayOutputStream out = new ByteArrayOutputStream();    generateLatin1InvalidSequence(out);    Files.write(out.toByteArray(), file);    ResettableInputStream in = initInputStream(DecodeErrorPolicy.FAIL);    while (in.readChar() != -1) {        }    fail("Expected MalformedInputException!");}
0
public void testLatin1DecodeErrorHandlingReplace() throws IOException
{    ByteArrayOutputStream out = new ByteArrayOutputStream();    generateLatin1InvalidSequence(out);    Files.write(out.toByteArray(), file);    ResettableInputStream in = initInputStream(DecodeErrorPolicy.REPLACE);    int c;    StringBuilder sb = new StringBuilder();    while ((c = in.readChar()) != -1) {        sb.append((char) c);    }    assertEquals("Invalid: (X)\n".replaceAll("X", "\ufffd"), sb.toString());}
0
public void testReset() throws IOException
{    String output = singleLineFileInit(file, Charsets.UTF_8);    PositionTracker tracker = new DurablePositionTracker(meta, file.getPath());    ResettableInputStream in = new ResettableFileInputStream(file, tracker);    String result1 = readLine(in, output.length());    assertEquals(output, result1);    in.reset();    String result2 = readLine(in, output.length());    assertEquals(output, result2);    String result3 = readLine(in, output.length());    assertNull("Should be null: " + result3, result3);    in.close();}
0
public void testMarkReset() throws IOException
{    List<String> expected = multiLineFileInit(file, Charsets.UTF_8);    int MAX_LEN = 100;    PositionTracker tracker = new DurablePositionTracker(meta, file.getPath());    ResettableInputStream in = new ResettableFileInputStream(file, tracker);    String result0 = readLine(in, MAX_LEN);    assertEquals(expected.get(0), result0);    in.reset();    String result0a = readLine(in, MAX_LEN);    assertEquals(expected.get(0), result0a);    in.mark();    String result1 = readLine(in, MAX_LEN);    assertEquals(expected.get(1), result1);    in.reset();    String result1a = readLine(in, MAX_LEN);    assertEquals(expected.get(1), result1a);    in.mark();    in.close();}
0
public void testMarkResetWithSurrogatePairs() throws IOException
{    ByteArrayOutputStream out = new ByteArrayOutputStream();    out.write("foo".getBytes(Charsets.UTF_8));    generateUtf8SurrogatePairSequence(out);    out.write("bar".getBytes(Charsets.UTF_8));    Files.write(out.toByteArray(), file);    PositionTracker tracker = new DurablePositionTracker(meta, file.getPath());    ResettableInputStream in = new ResettableFileInputStream(file, tracker);    Assert.assertEquals('f', in.readChar());    Assert.assertEquals('o', in.readChar());    in.mark();    Assert.assertEquals('o', in.readChar());        Assert.assertEquals('\ud83d', in.readChar());        in.reset();            Assert.assertEquals('\ude18', in.readChar());        Assert.assertEquals('o', in.readChar());        Assert.assertEquals('\ud83d', in.readChar());            in.mark();        in.reset();            Assert.assertEquals('\ude18', in.readChar());    Assert.assertEquals('b', in.readChar());    Assert.assertEquals('a', in.readChar());        in.reset();    Assert.assertEquals('b', in.readChar());    Assert.assertEquals('a', in.readChar());    Assert.assertEquals('r', in.readChar());    Assert.assertEquals(-1, in.readChar());    in.close();        tracker.close();}
0
public void testResume() throws IOException
{    List<String> expected = multiLineFileInit(file, Charsets.UTF_8);    int MAX_LEN = 100;    PositionTracker tracker = new DurablePositionTracker(meta, file.getPath());    ResettableInputStream in = new ResettableFileInputStream(file, tracker);    String result0 = readLine(in, MAX_LEN);    String result1 = readLine(in, MAX_LEN);    in.mark();    String result2 = readLine(in, MAX_LEN);    Assert.assertEquals(expected.get(2), result2);    String result3 = readLine(in, MAX_LEN);    Assert.assertEquals(expected.get(3), result3);    in.close();        tracker.close();        tracker = new DurablePositionTracker(meta, file.getPath());    in = new ResettableFileInputStream(file, tracker);    String result2a = readLine(in, MAX_LEN);    String result3a = readLine(in, MAX_LEN);    Assert.assertEquals(result2, result2a);    Assert.assertEquals(result3, result3a);}
0
public void testResumeWithSurrogatePairs() throws IOException
{    ByteArrayOutputStream out = new ByteArrayOutputStream();    out.write("foo".getBytes(Charsets.UTF_8));    generateUtf8SurrogatePairSequence(out);    out.write("bar".getBytes(Charsets.UTF_8));    Files.write(out.toByteArray(), file);    PositionTracker tracker = new DurablePositionTracker(meta, file.getPath());    ResettableInputStream in = new ResettableFileInputStream(file, tracker);    Assert.assertEquals('f', in.readChar());    Assert.assertEquals('o', in.readChar());    in.mark();    Assert.assertEquals('o', in.readChar());        Assert.assertEquals('\ud83d', in.readChar());        in.reset();            in.close();        tracker.close();        tracker = new DurablePositionTracker(meta, file.getPath());    in = new ResettableFileInputStream(file, tracker);        Assert.assertEquals('o', in.readChar());        Assert.assertEquals('\ud83d', in.readChar());            in.mark();            in.close();        tracker.close();        tracker = new DurablePositionTracker(meta, file.getPath());    in = new ResettableFileInputStream(file, tracker);        Assert.assertEquals('b', in.readChar());    Assert.assertEquals('a', in.readChar());    Assert.assertEquals('r', in.readChar());    Assert.assertEquals(-1, in.readChar());    in.close();        tracker.close();}
0
public void testSeek() throws IOException
{    int NUM_LINES = 1000;    int LINE_LEN = 1000;    generateData(file, Charsets.UTF_8, NUM_LINES, LINE_LEN);    PositionTracker tracker = new DurablePositionTracker(meta, file.getPath());    ResettableInputStream in = new ResettableFileInputStream(file, tracker, 10 * LINE_LEN, Charsets.UTF_8, DecodeErrorPolicy.FAIL);    String line = "";    for (int i = 0; i < 9; i++) {        line = readLine(in, LINE_LEN);    }    int lineNum = Integer.parseInt(line.substring(0, 10));    assertEquals(8, lineNum);        long pos = in.tell();        in.seek(pos - 2 * LINE_LEN);    line = readLine(in, LINE_LEN);    lineNum = Integer.parseInt(line.substring(0, 10));    assertEquals(7, lineNum);        in.seek(in.tell() + LINE_LEN);    line = readLine(in, LINE_LEN);    lineNum = Integer.parseInt(line.substring(0, 10));    assertEquals(9, lineNum);        in.seek(in.tell() + 20 * LINE_LEN);    line = readLine(in, LINE_LEN);    lineNum = Integer.parseInt(line.substring(0, 10));    assertEquals(30, lineNum);        in.seek(in.tell() - 25 * LINE_LEN);    line = readLine(in, LINE_LEN);    lineNum = Integer.parseInt(line.substring(0, 10));    assertEquals(6, lineNum);        in.seek(100 * LINE_LEN);        in.seek(0);    in.seek(9 * LINE_LEN);    assertEquals(9, Integer.parseInt(readLine(in, LINE_LEN).substring(0, 10)));    assertEquals(10, Integer.parseInt(readLine(in, LINE_LEN).substring(0, 10)));    assertEquals(11, Integer.parseInt(readLine(in, LINE_LEN).substring(0, 10)));}
0
private ResettableInputStream initUtf8DecodeTest(DecodeErrorPolicy policy) throws IOException
{    writeBigBadUtf8Sequence(file);    return initInputStream(policy);}
0
private ResettableInputStream initInputStream(DecodeErrorPolicy policy) throws IOException
{    return initInputStream(2048, Charsets.UTF_8, policy);}
0
private ResettableInputStream initInputStream(int bufferSize, Charset charset, DecodeErrorPolicy policy) throws IOException
{    PositionTracker tracker = new DurablePositionTracker(meta, file.getPath());    ResettableInputStream in = new ResettableFileInputStream(file, tracker, bufferSize, charset, policy);    return in;}
0
private void writeBigBadUtf8Sequence(File file) throws IOException
{    ByteArrayOutputStream out = new ByteArrayOutputStream();    generateUtf8Latin1Sequence(out);    generateUtf8OverlyLongSequence(out);    generateUtf8NonUnicodeSequence(out);    Files.write(out.toByteArray(), file);}
0
private void generateUtf8OverlyLongSequence(OutputStream out) throws IOException
{    out.write("Long: (".getBytes(Charsets.UTF_8));        out.write(new byte[] { (byte) 0xe0, (byte) 0x80, (byte) 0xaf });    out.write(")\n".getBytes(Charsets.UTF_8));}
0
private void generateUtf8NonUnicodeSequence(OutputStream out) throws IOException
{    out.write("NonUnicode: (".getBytes(Charsets.UTF_8));        out.write(new byte[] { (byte) 0xf8, (byte) 0xa1, (byte) 0xa1, (byte) 0xa1, (byte) 0xa1 });    out.write(")\n".getBytes(Charsets.UTF_8));}
0
private void generateUtf8Latin1Sequence(OutputStream out) throws IOException
{    out.write("Latin1: (".getBytes(Charsets.UTF_8));        out.write(new byte[] { (byte) 0xe9 });    out.write(")\n".getBytes(Charsets.UTF_8));}
0
private void generateLatin1InvalidSequence(OutputStream out) throws IOException
{    out.write("Invalid: (".getBytes(Charsets.UTF_8));        out.write(new byte[] { (byte) 0x81 });    out.write(")\n".getBytes(Charsets.UTF_8));}
0
private void generateUtf8SurrogatePairSequence(OutputStream out) throws IOException
{        out.write(new byte[] { (byte) 0xF0, (byte) 0x9F, (byte) 0x98, (byte) 0x98 });}
0
private void generateUtf16SurrogatePairSequence(OutputStream out) throws IOException
{        out.write(new byte[] { (byte) 0xFE, (byte) 0xFF });        out.write(new byte[] { (byte) 0xD8, (byte) 0x3D, (byte) 0xDE, (byte) 0x18 });}
0
private void generateUtf83ByteSequence(OutputStream out) throws IOException
{        out.write(new byte[] { (byte) 0xe0, (byte) 0xaa, (byte) 0x93 });}
0
private void generateShiftJis2ByteSequence(OutputStream out) throws IOException
{        out.write(new byte[] { (byte) 0x88, (byte) 0x9f });}
0
private static String readLine(ResettableInputStream in, int maxLength) throws IOException
{    StringBuilder s = new StringBuilder();    int c;    int i = 1;    while ((c = in.readChar()) != -1) {                if (c == '\n') {            break;        }                s.append((char) c);        if (i++ > maxLength) {            System.out.println("Output: >" + s + "<");            throw new RuntimeException("Too far!");        }    }    if (s.length() > 0) {        s.append('\n');        return s.toString();    } else {        return null;    }}
0
private static String singleLineFileInit(File file, Charset charset) throws IOException
{    String output = "This is gonna be great!\n";    Files.write(output.getBytes(charset), file);    return output;}
0
private static List<String> multiLineFileInit(File file, Charset charset) throws IOException
{    List<String> lines = Lists.newArrayList();    lines.add("1. On the planet of Mars\n");    lines.add("2. They have clothes just like ours,\n");    lines.add("3. And they have the same shoes and same laces,\n");    lines.add("4. And they have the same charms and same graces...\n");    StringBuilder sb = new StringBuilder();    for (String line : lines) {        sb.append(line);    }    Files.write(sb.toString().getBytes(charset), file);    return lines;}
0
private static void generateData(File file, Charset charset, int numLines, int lineLen) throws IOException
{    OutputStream out = new BufferedOutputStream(new FileOutputStream(file));    StringBuilder junk = new StringBuilder();    for (int x = 0; x < lineLen - 13; x++) {        junk.append('x');    }    String payload = junk.toString();    StringBuilder builder = new StringBuilder();    for (int i = 0; i < numLines; i++) {        builder.append(String.format("%010d: %s\n", i, payload));        if (i % 1000 == 0 && i != 0) {            out.write(builder.toString().getBytes(charset));            builder.setLength(0);        }    }    out.write(builder.toString().getBytes(charset));    out.close();    Assert.assertEquals(lineLen * numLines, file.length());}
0
private static List<Event> generateSyslogEvents()
{    List<Event> list = Lists.newArrayList();    Event e;        e = EventBuilder.withBody("Apr  7 01:00:00 host Msg 01", Charsets.UTF_8);    e.getHeaders().put(SyslogUtils.SYSLOG_FACILITY, "1");    e.getHeaders().put(SyslogUtils.SYSLOG_SEVERITY, "2");    list.add(e);        e = EventBuilder.withBody("Apr 22 01:00:00 host Msg 02", Charsets.UTF_8);    e.getHeaders().put(SyslogUtils.SYSLOG_FACILITY, "1");    e.getHeaders().put(SyslogUtils.SYSLOG_SEVERITY, "3");    list.add(e);        e = EventBuilder.withBody("<8>Apr 22 01:00:00 host Msg 03", Charsets.UTF_8);    list.add(e);    return list;}
0
public void test() throws FileNotFoundException, IOException
{        Assume.assumeTrue(!"Mac OS X".equals(System.getProperty("os.name")) || !System.getProperty("java.version").startsWith("1.7."));            OutputStream out = new FileOutputStream(testFile);    String builderName = SyslogAvroEventSerializer.Builder.class.getName();    Context ctx = new Context();    ctx.put("syncInterval", "4096");    ctx.put("compressionCodec", "snappy");    EventSerializer serializer = EventSerializerFactory.getInstance(builderName, ctx, out);        serializer.afterCreate();    List<Event> events = generateSyslogEvents();    for (Event e : events) {        serializer.write(e);    }    serializer.flush();    serializer.beforeClose();    out.flush();    out.close();        DatumReader<GenericRecord> reader = new GenericDatumReader<GenericRecord>();    DataFileReader<GenericRecord> fileReader = new DataFileReader<GenericRecord>(testFile, reader);    GenericRecord record = new GenericData.Record(fileReader.getSchema());    int numEvents = 0;    while (fileReader.hasNext()) {        fileReader.next(record);        int facility = (Integer) record.get("facility");        int severity = (Integer) record.get("severity");        long timestamp = (Long) record.get("timestamp");        String hostname = record.get("hostname").toString();        String message = record.get("message").toString();        Assert.assertEquals("Facility should be 1", 1, facility);        System.out.println(timestamp + ": " + message);        numEvents++;    }    fileReader.close();    Assert.assertEquals("Should have found a total of 3 events", 3, numEvents);    FileUtils.forceDelete(testFile);}
0
public void storePosition(long position) throws IOException
{    this.position = position;}
0
public long getPosition()
{    return position;}
0
public String getTarget()
{    return target;}
0
public void close() throws IOException
{}
0
public Iterator<Sink> createSinkIterator()
{    return getSinks().iterator();}
0
public void configure(Context context)
{    super.configure(context);    if (context.getString(SET_ME) == null) {        throw new RuntimeException("config key " + SET_ME + " not specified");    }}
0
public void test()
{    Context context = new Context();    context.put("type", FailoverSinkProcessor.class.getName());    context.put("priority.sink1", "1");    context.put("priority.sink2", "2");    SinkFactory sf = new DefaultSinkFactory();    List<Sink> sinks = new ArrayList<Sink>();    sinks.add(sf.create("sink1", "avro"));    sinks.add(sf.create("sink2", "avro"));    SinkProcessor sp = SinkProcessorFactory.getProcessor(context, sinks);    context.put("type", "failover");    SinkProcessor sp2 = SinkProcessorFactory.getProcessor(context, sinks);    Assert.assertEquals(sp.getClass(), sp2.getClass());}
0
public void testInstantiatingLoadBalancingSinkProcessor()
{    Context context = new Context();    context.put("type", LoadBalancingSinkProcessor.class.getName());    context.put("selector", "random");    SinkFactory sf = new DefaultSinkFactory();    List<Sink> sinks = new ArrayList<Sink>();    sinks.add(sf.create("sink1", "avro"));    sinks.add(sf.create("sink2", "avro"));    SinkProcessor sp = SinkProcessorFactory.getProcessor(context, sinks);    context.put("type", "load_balance");    SinkProcessor sp2 = SinkProcessorFactory.getProcessor(context, sinks);    Assert.assertEquals(sp.getClass(), sp2.getClass());}
0
public void setUp()
{    setUp("none", 0);}
0
public void setUp(String compressionType, int compressionLevel)
{    if (sink != null) {        throw new RuntimeException("double setup");    }    sink = new AvroSink();    channel = new MemoryChannel();    Context context = createBaseContext();    if (compressionType.equals("deflate")) {        context.put("compression-type", compressionType);        context.put("compression-level", Integer.toString(compressionLevel));    }    sink.setChannel(channel);    Configurables.configure(sink, context);    Configurables.configure(channel, context);}
0
private Context createBaseContext()
{    Context context = new Context();    context.put("hostname", hostname);    context.put("port", String.valueOf(port));    context.put("batch-size", String.valueOf(2));    context.put("connect-timeout", String.valueOf(2000L));    context.put("request-timeout", String.valueOf(3000L));    return context;}
0
private Server createServer(AvroSourceProtocol protocol) throws IllegalAccessException, InstantiationException
{    Server server = new NettyServer(new SpecificResponder(AvroSourceProtocol.class, protocol), new InetSocketAddress(hostname, port));    return server;}
0
public void testLifecycle() throws InterruptedException, InstantiationException, IllegalAccessException
{    setUp();    Server server = createServer(new MockAvroServer());    server.start();    sink.start();    Assert.assertTrue(LifecycleController.waitForOneOf(sink, LifecycleState.START_OR_ERROR, 5000));    sink.stop();    Assert.assertTrue(LifecycleController.waitForOneOf(sink, LifecycleState.STOP_OR_ERROR, 5000));    server.close();}
0
public void testProcess() throws InterruptedException, EventDeliveryException, InstantiationException, IllegalAccessException
{    setUp();    Event event = EventBuilder.withBody("test event 1", Charsets.UTF_8);    Server server = createServer(new MockAvroServer());    server.start();    sink.start();    Assert.assertTrue(LifecycleController.waitForOneOf(sink, LifecycleState.START_OR_ERROR, 5000));    Transaction transaction = channel.getTransaction();    transaction.begin();    for (int i = 0; i < 10; i++) {        channel.put(event);    }    transaction.commit();    transaction.close();    for (int i = 0; i < 5; i++) {        Sink.Status status = sink.process();        Assert.assertEquals(Sink.Status.READY, status);    }    Assert.assertEquals(Sink.Status.BACKOFF, sink.process());    sink.stop();    Assert.assertTrue(LifecycleController.waitForOneOf(sink, LifecycleState.STOP_OR_ERROR, 5000));    server.close();}
0
public void testChannelException() throws InterruptedException, EventDeliveryException, InstantiationException, IllegalAccessException
{    setUp();    Server server = createServer(new MockAvroServer());    server.start();    sink.start();    Channel channel = Mockito.mock(Channel.class);    Mockito.when(channel.take()).thenThrow(new ChannelException("dummy"));    Transaction transaction = Mockito.mock(BasicTransactionSemantics.class);    Mockito.when(channel.getTransaction()).thenReturn(transaction);    sink.setChannel(channel);    Sink.Status status = sink.process();    sink.stop();    server.close();    SinkCounter sinkCounter = (SinkCounter) Whitebox.getInternalState(sink, "sinkCounter");    Assert.assertEquals(1, sinkCounter.getChannelReadFail());}
0
public void testTimeout() throws InterruptedException, EventDeliveryException, InstantiationException, IllegalAccessException
{    setUp();    Event event = EventBuilder.withBody("foo", Charsets.UTF_8);    AtomicLong delay = new AtomicLong();    Server server = createServer(new DelayMockAvroServer(delay));    server.start();    sink.start();    Assert.assertTrue(LifecycleController.waitForOneOf(sink, LifecycleState.START_OR_ERROR, 5000));    Transaction txn = channel.getTransaction();    txn.begin();    for (int i = 0; i < 4; i++) {        channel.put(event);    }    txn.commit();    txn.close();            delay.set(3000L);    boolean threw = false;    try {        sink.process();    } catch (EventDeliveryException ex) {                threw = true;    }    Assert.assertTrue("Must throw due to connect timeout", threw);        delay.set(0);    sink.process();            delay.set(4000L);    threw = false;    try {        sink.process();    } catch (EventDeliveryException ex) {                threw = true;    }    Assert.assertTrue("Must throw due to request timeout", threw);    sink.stop();    Assert.assertTrue(LifecycleController.waitForOneOf(sink, LifecycleState.STOP_OR_ERROR, 5000));    server.close();    SinkCounter sinkCounter = (SinkCounter) Whitebox.getInternalState(sink, "sinkCounter");    Assert.assertEquals(2, sinkCounter.getEventWriteFail());}
1
public void testFailedConnect() throws InterruptedException, EventDeliveryException, InstantiationException, IllegalAccessException
{    setUp();    Event event = EventBuilder.withBody("test event 1", Charset.forName("UTF8"));    Server server = createServer(new MockAvroServer());    server.start();    sink.start();    Assert.assertTrue(LifecycleController.waitForOneOf(sink, LifecycleState.START_OR_ERROR, 5000));        Thread.sleep(500L);    server.close();        Thread.sleep(500L);    Transaction transaction = channel.getTransaction();    transaction.begin();    for (int i = 0; i < 10; i++) {        channel.put(event);    }    transaction.commit();    transaction.close();    for (int i = 0; i < 5; i++) {        boolean threwException = false;        try {            sink.process();        } catch (EventDeliveryException e) {            threwException = true;        }        Assert.assertTrue("Must throw EventDeliveryException if disconnected", threwException);    }    server = createServer(new MockAvroServer());    server.start();    for (int i = 0; i < 5; i++) {        Sink.Status status = sink.process();        Assert.assertEquals(Sink.Status.READY, status);    }    Assert.assertEquals(Sink.Status.BACKOFF, sink.process());    sink.stop();    Assert.assertTrue(LifecycleController.waitForOneOf(sink, LifecycleState.STOP_OR_ERROR, 5000));    server.close();    SinkCounter sinkCounter = (SinkCounter) Whitebox.getInternalState(sink, "sinkCounter");    Assert.assertEquals(5, sinkCounter.getEventWriteFail());    Assert.assertEquals(4, sinkCounter.getConnectionFailedCount());}
0
public void testReset() throws Exception
{    setUp();    Server server = createServer(new MockAvroServer());    server.start();    Context context = new Context();    context.put("hostname", hostname);    context.put("port", String.valueOf(port));    context.put("batch-size", String.valueOf(2));    context.put("connect-timeout", String.valueOf(2000L));    context.put("request-timeout", String.valueOf(3000L));    context.put("reset-connection-interval", String.valueOf("5"));    sink.setChannel(channel);    Configurables.configure(sink, context);    sink.start();    RpcClient firstClient = sink.getUnderlyingClient();    Thread.sleep(6000);    Transaction t = channel.getTransaction();    t.begin();    channel.put(EventBuilder.withBody("This is a test", Charset.defaultCharset()));    t.commit();    t.close();    sink.process();        Assert.assertFalse(firstClient == sink.getUnderlyingClient());    sink.stop();    context.put("hostname", hostname);    context.put("port", String.valueOf(port));    context.put("batch-size", String.valueOf(2));    context.put("connect-timeout", String.valueOf(2000L));    context.put("request-timeout", String.valueOf(3000L));    context.put("reset-connection-interval", String.valueOf("0"));    sink.setChannel(channel);    Configurables.configure(sink, context);    sink.start();    firstClient = sink.getUnderlyingClient();    Thread.sleep(6000);        Assert.assertTrue(firstClient == sink.getUnderlyingClient());    sink.stop();    context.clear();    context.put("hostname", hostname);    context.put("port", String.valueOf(port));    context.put("batch-size", String.valueOf(2));    context.put("connect-timeout", String.valueOf(2000L));    context.put("request-timeout", String.valueOf(3000L));    sink.setChannel(channel);    Configurables.configure(sink, context);    sink.start();    firstClient = sink.getUnderlyingClient();    Thread.sleep(6000);        Assert.assertTrue(firstClient == sink.getUnderlyingClient());    sink.stop();    server.close();}
0
public void testSslProcessTrustAllCerts() throws InterruptedException, EventDeliveryException, InstantiationException, IllegalAccessException
{    setUp();    Context context = createBaseContext();    context.put("ssl", String.valueOf(true));    context.put("trust-all-certs", String.valueOf(true));    Configurables.configure(sink, context);    doTestSslProcess();}
0
public void testSslProcessWithComponentTruststore() throws InterruptedException, EventDeliveryException, InstantiationException, IllegalAccessException
{    setUp();    Context context = createBaseContext();    context.put("ssl", String.valueOf(true));    context.put("truststore", "src/test/resources/truststore.jks");    context.put("truststore-password", "password");    Configurables.configure(sink, context);    doTestSslProcess();}
0
public void testSslProcessWithComponentTruststoreNoPassword() throws InterruptedException, EventDeliveryException, InstantiationException, IllegalAccessException
{    setUp();    Context context = createBaseContext();    context.put("ssl", String.valueOf(true));    context.put("truststore", "src/test/resources/truststore.jks");    Configurables.configure(sink, context);    doTestSslProcess();}
0
public void testSslProcessWithGlobalTruststore() throws InterruptedException, EventDeliveryException, InstantiationException, IllegalAccessException
{    setUp();    System.setProperty("javax.net.ssl.trustStore", "src/test/resources/truststore.jks");    System.setProperty("javax.net.ssl.trustStorePassword", "password");    Context context = createBaseContext();    context.put("ssl", String.valueOf(true));    Configurables.configure(sink, context);    doTestSslProcess();    System.clearProperty("javax.net.ssl.trustStore");    System.clearProperty("javax.net.ssl.trustStorePassword");}
0
public void testSslProcessWithGlobalTruststoreNoPassword() throws InterruptedException, EventDeliveryException, InstantiationException, IllegalAccessException
{    setUp();    System.setProperty("javax.net.ssl.trustStore", "src/test/resources/truststore.jks");    Context context = createBaseContext();    context.put("ssl", String.valueOf(true));    Configurables.configure(sink, context);    doTestSslProcess();    System.clearProperty("javax.net.ssl.trustStore");}
0
private void doTestSslProcess() throws InterruptedException, EventDeliveryException, InstantiationException, IllegalAccessException
{    Server server = createSslServer(new MockAvroServer());    server.start();    sink.start();    Assert.assertTrue(LifecycleController.waitForOneOf(sink, LifecycleState.START_OR_ERROR, 5000));    Transaction transaction = channel.getTransaction();    transaction.begin();    Event event = EventBuilder.withBody("test event 1", Charsets.UTF_8);    for (int i = 0; i < 10; i++) {        channel.put(event);    }    transaction.commit();    transaction.close();    for (int i = 0; i < 5; i++) {        Sink.Status status = sink.process();        Assert.assertEquals(Sink.Status.READY, status);    }    Assert.assertEquals(Sink.Status.BACKOFF, sink.process());    sink.stop();    Assert.assertTrue(LifecycleController.waitForOneOf(sink, LifecycleState.STOP_OR_ERROR, 5000));    server.close();}
0
public void testSslWithCompression() throws InterruptedException, EventDeliveryException, InstantiationException, IllegalAccessException
{    setUp("deflate", 6);    boolean bound = false;    AvroSource source;    Channel sourceChannel;    int selectedPort;    source = new AvroSource();    sourceChannel = new MemoryChannel();    Configurables.configure(sourceChannel, new Context());    List<Channel> channels = new ArrayList<Channel>();    channels.add(sourceChannel);    ChannelSelector rcs = new ReplicatingChannelSelector();    rcs.setChannels(channels);    source.setChannelProcessor(new ChannelProcessor(rcs));    Context context = new Context();    context.put("port", port.toString());    context.put("bind", hostname);    context.put("threads", "50");    context.put("compression-type", "deflate");    context.put("ssl", String.valueOf(true));    context.put("keystore", "src/test/resources/server.p12");    context.put("keystore-password", "password");    context.put("keystore-type", "PKCS12");    Configurables.configure(source, context);    source.start();    Assert.assertTrue("Reached start or error", LifecycleController.waitForOneOf(source, LifecycleState.START_OR_ERROR));    Assert.assertEquals("Server is started", LifecycleState.START, source.getLifecycleState());    Event event = EventBuilder.withBody("Hello avro", Charset.forName("UTF8"));    context = createBaseContext();    context.put("ssl", String.valueOf(true));    context.put("trust-all-certs", String.valueOf(true));    context.put("compression-type", "deflate");    context.put("compression-level", Integer.toString(6));    Configurables.configure(sink, context);    sink.start();    Transaction sickTransaction = channel.getTransaction();    sickTransaction.begin();    for (int i = 0; i < 10; i++) {        channel.put(event);    }    sickTransaction.commit();    sickTransaction.close();    for (int i = 0; i < 5; i++) {        Sink.Status status = sink.process();                Assert.assertEquals(Sink.Status.READY, status);    }    sink.stop();    Transaction sourceTransaction = sourceChannel.getTransaction();    sourceTransaction.begin();    Event sourceEvent = sourceChannel.take();    Assert.assertNotNull(sourceEvent);    Assert.assertEquals("Channel contained our event", "Hello avro", new String(sourceEvent.getBody()));    sourceTransaction.commit();    sourceTransaction.close();        source.stop();    Assert.assertTrue("Reached stop or error", LifecycleController.waitForOneOf(source, LifecycleState.STOP_OR_ERROR));    Assert.assertEquals("Server is stopped", LifecycleState.STOP, source.getLifecycleState());}
1
public void testSslSinkWithNonSslServer() throws InterruptedException, InstantiationException, IllegalAccessException
{    setUp();    Server server = createServer(new MockAvroServer());    server.start();    Context context = createBaseContext();    context.put("ssl", String.valueOf(true));    context.put("trust-all-certs", String.valueOf(true));    Configurables.configure(sink, context);    boolean failed = doRequestWhenFailureExpected();    server.close();    if (!failed) {        Assert.fail("SSL-enabled sink successfully connected to a non-SSL-enabled server, " + "that's wrong.");    }    SinkCounter sinkCounter = (SinkCounter) Whitebox.getInternalState(sink, "sinkCounter");    Assert.assertEquals(1, sinkCounter.getEventWriteFail());}
0
public void testSslSinkWithNonTrustedCert() throws InterruptedException, InstantiationException, IllegalAccessException
{    setUp();    Server server = createSslServer(new MockAvroServer());    server.start();    Context context = createBaseContext();    context.put("ssl", String.valueOf(true));    Configurables.configure(sink, context);    boolean failed = doRequestWhenFailureExpected();    server.close();    if (!failed) {        Assert.fail("SSL-enabled sink successfully connected to a server with an " + "untrusted certificate when it should have failed");    }    SinkCounter sinkCounter = (SinkCounter) Whitebox.getInternalState(sink, "sinkCounter");    Assert.assertEquals(1, sinkCounter.getEventWriteFail());}
0
private boolean doRequestWhenFailureExpected() throws InterruptedException
{    sink.start();    Assert.assertTrue(LifecycleController.waitForOneOf(sink, LifecycleState.START_OR_ERROR, 5000));    Transaction transaction = channel.getTransaction();    transaction.begin();    Event event = EventBuilder.withBody("test event 1", Charsets.UTF_8);    channel.put(event);    transaction.commit();    transaction.close();    boolean failed;    try {        sink.process();        failed = false;    } catch (EventDeliveryException ex) {                failed = true;    }    sink.stop();    Assert.assertTrue(LifecycleController.waitForOneOf(sink, LifecycleState.STOP_OR_ERROR, 5000));    return failed;}
1
public void testRequestWithNoCompression() throws InterruptedException, IOException, EventDeliveryException
{    doRequest(false, false, 6);}
0
public void testRequestWithCompressionOnClientAndServerOnLevel0() throws InterruptedException, IOException, EventDeliveryException
{    doRequest(true, true, 0);}
0
public void testRequestWithCompressionOnClientAndServerOnLevel1() throws InterruptedException, IOException, EventDeliveryException
{    doRequest(true, true, 1);}
0
public void testRequestWithCompressionOnClientAndServerOnLevel6() throws InterruptedException, IOException, EventDeliveryException
{    doRequest(true, true, 6);}
0
public void testRequestWithCompressionOnClientAndServerOnLevel9() throws InterruptedException, IOException, EventDeliveryException
{    doRequest(true, true, 9);}
0
private void doRequest(boolean serverEnableCompression, boolean clientEnableCompression, int compressionLevel) throws InterruptedException, IOException, EventDeliveryException
{    if (clientEnableCompression) {        setUp("deflate", compressionLevel);    } else {        setUp("none", compressionLevel);    }    boolean bound = false;    AvroSource source;    Channel sourceChannel;    int selectedPort;    source = new AvroSource();    sourceChannel = new MemoryChannel();    Configurables.configure(sourceChannel, new Context());    List<Channel> channels = new ArrayList<Channel>();    channels.add(sourceChannel);    ChannelSelector rcs = new ReplicatingChannelSelector();    rcs.setChannels(channels);    source.setChannelProcessor(new ChannelProcessor(rcs));    Context context = new Context();    context.put("port", port.toString());    context.put("bind", hostname);    context.put("threads", "50");    if (serverEnableCompression) {        context.put("compression-type", "deflate");    } else {        context.put("compression-type", "none");    }    Configurables.configure(source, context);    source.start();    Assert.assertTrue("Reached start or error", LifecycleController.waitForOneOf(source, LifecycleState.START_OR_ERROR));    Assert.assertEquals("Server is started", LifecycleState.START, source.getLifecycleState());    Event event = EventBuilder.withBody("Hello avro", Charset.forName("UTF8"));    sink.start();    Transaction sickTransaction = channel.getTransaction();    sickTransaction.begin();    for (int i = 0; i < 10; i++) {        channel.put(event);    }    sickTransaction.commit();    sickTransaction.close();    for (int i = 0; i < 5; i++) {        Sink.Status status = sink.process();                Assert.assertEquals(Sink.Status.READY, status);    }    sink.stop();    Transaction sourceTransaction = sourceChannel.getTransaction();    sourceTransaction.begin();    Event sourceEvent = sourceChannel.take();    Assert.assertNotNull(sourceEvent);    Assert.assertEquals("Channel contained our event", "Hello avro", new String(sourceEvent.getBody()));    sourceTransaction.commit();    sourceTransaction.close();        source.stop();    Assert.assertTrue("Reached stop or error", LifecycleController.waitForOneOf(source, LifecycleState.STOP_OR_ERROR));    Assert.assertEquals("Server is stopped", LifecycleState.STOP, source.getLifecycleState());}
1
public Status append(AvroFlumeEvent event) throws AvroRemoteException
{        return Status.OK;}
1
public Status appendBatch(List<AvroFlumeEvent> events) throws AvroRemoteException
{        return Status.OK;}
1
private void sleep() throws AvroRemoteException
{    try {        Thread.sleep(delay.get());    } catch (InterruptedException e) {        throw new AvroRemoteException("Interrupted while sleeping", e);    }}
0
private Server createSslServer(AvroSourceProtocol protocol) throws IllegalAccessException, InstantiationException
{    Server server = new NettyServer(new SpecificResponder(AvroSourceProtocol.class, protocol), new InetSocketAddress(hostname, port), new NioServerSocketChannelFactory(Executors.newCachedThreadPool(), Executors.newCachedThreadPool()), new SSLChannelPipelineFactory(), null);    return server;}
0
private SSLContext createServerSSLContext()
{    try {        KeyStore ks = KeyStore.getInstance(keystoreType);        ks.load(new FileInputStream(keystore), keystorePassword.toCharArray());                KeyManagerFactory kmf = KeyManagerFactory.getInstance(getAlgorithm());        kmf.init(ks, keystorePassword.toCharArray());        SSLContext serverContext = SSLContext.getInstance("TLS");        serverContext.init(kmf.getKeyManagers(), null, null);        return serverContext;    } catch (Exception e) {        throw new Error("Failed to initialize the server-side SSLContext", e);    }}
0
private String getAlgorithm()
{    String algorithm = Security.getProperty("ssl.KeyManagerFactory.algorithm");    if (algorithm == null) {        algorithm = "SunX509";    }    return algorithm;}
0
public ChannelPipeline getPipeline() throws Exception
{    ChannelPipeline pipeline = Channels.pipeline();    SSLEngine sslEngine = createServerSSLContext().createSSLEngine();    sslEngine.setUseClientMode(false);    pipeline.addLast("ssl", new SslHandler(sslEngine));    return pipeline;}
0
public void setUp()
{    sinkFactory = new DefaultSinkFactory();}
0
public void testDuplicateCreate()
{    Sink avroSink1 = sinkFactory.create("avroSink1", "avro");    Sink avroSink2 = sinkFactory.create("avroSink2", "avro");    Assert.assertNotNull(avroSink1);    Assert.assertNotNull(avroSink2);    Assert.assertNotSame(avroSink1, avroSink2);    Assert.assertTrue(avroSink1 instanceof AvroSink);    Assert.assertTrue(avroSink2 instanceof AvroSink);    Sink s1 = sinkFactory.create("avroSink1", "avro");    Sink s2 = sinkFactory.create("avroSink2", "avro");    Assert.assertNotSame(avroSink1, s1);    Assert.assertNotSame(avroSink2, s2);}
0
private void verifySinkCreation(String name, String type, Class<?> typeClass) throws Exception
{    Sink sink = sinkFactory.create(name, type);    Assert.assertNotNull(sink);    Assert.assertTrue(typeClass.isInstance(sink));}
0
public void testSinkCreation() throws Exception
{    verifySinkCreation("null-sink", "null", NullSink.class);    verifySinkCreation("logger-sink", "logger", LoggerSink.class);    verifySinkCreation("file-roll-sink", "file_roll", RollingFileSink.class);    verifySinkCreation("avro-sink", "avro", AvroSink.class);}
0
public void start()
{    state = LifecycleState.START;}
0
public void stop()
{    state = LifecycleState.STOP;}
0
public LifecycleState getLifecycleState()
{    return state;}
0
public void setName(String name)
{    this.name = name;}
0
public String getName()
{    return name;}
0
public void setChannel(Channel channel)
{    this.channel = channel;}
0
public Channel getChannel()
{    return channel;}
0
public synchronized void setRemaining(int remaining)
{    this.remaining = remaining;}
0
public Status process() throws EventDeliveryException
{    synchronized (this) {        if (remaining <= 0) {            throw new EventDeliveryException("can't consume more");        }    }    Transaction tx = channel.getTransaction();    tx.begin();    Event e = channel.take();    tx.commit();    tx.close();    if (e != null) {        synchronized (this) {            remaining--;        }        written++;    }    return Status.READY;}
0
public Integer getWritten()
{    return written;}
0
public void testFailover() throws InterruptedException
{    Channel ch = new MemoryChannel();    ConsumeXSink s1 = new ConsumeXSink(10);    s1.setChannel(ch);    s1.setName("s1");    ConsumeXSink s2 = new ConsumeXSink(50);    s2.setChannel(ch);    s2.setName("s2");    ConsumeXSink s3 = new ConsumeXSink(100);    s3.setChannel(ch);    s3.setName("s3");    Context context = new Context();    Configurables.configure(s1, context);    Configurables.configure(s2, context);    Configurables.configure(s3, context);    Configurables.configure(ch, context);    ch.start();    List<Sink> sinks = new LinkedList<Sink>();    sinks.add(s1);    sinks.add(s2);    sinks.add(s3);    SinkGroup group = new SinkGroup(sinks);    Map<String, String> params = new HashMap<String, String>();    params.put("sinks", "s1 s2 s3");    params.put("processor.type", "failover");    params.put("processor.priority.s1", "3");    params.put("processor.priority.s2", "2");    params.put("processor.priority.s3", "1");    params.put("processor.maxpenalty", "10000");    context.putAll(params);    Configurables.configure(group, context);    SinkRunner runner = new SinkRunner(group.getProcessor());    runner.start();    Assert.assertEquals(LifecycleState.START, s1.getLifecycleState());    Assert.assertEquals(LifecycleState.START, s2.getLifecycleState());    Assert.assertEquals(LifecycleState.START, s3.getLifecycleState());    for (int i = 0; i < 15; i++) {        Transaction tx = ch.getTransaction();        tx.begin();        ch.put(EventBuilder.withBody("test".getBytes()));        tx.commit();        tx.close();    }    Thread.sleep(100);    Assert.assertEquals(new Integer(10), s1.getWritten());    Assert.assertEquals(new Integer(5), s2.getWritten());    for (int i = 0; i < 50; i++) {        Transaction tx = ch.getTransaction();        tx.begin();        ch.put(EventBuilder.withBody("test".getBytes()));        tx.commit();        tx.close();    }    Thread.sleep(100);    Assert.assertEquals(new Integer(50), s2.getWritten());    Assert.assertEquals(new Integer(5), s3.getWritten());        s2.setRemaining(20);        Thread.sleep(5000);    for (int i = 0; i < 100; i++) {        Transaction tx = ch.getTransaction();        tx.begin();        ch.put(EventBuilder.withBody("test".getBytes()));        tx.commit();        tx.close();    }    Thread.sleep(1000);    Assert.assertEquals(new Integer(10), s1.getWritten());    Assert.assertEquals(new Integer(70), s2.getWritten());    Assert.assertEquals(new Integer(85), s3.getWritten());    runner.stop();    ch.stop();}
0
private Context getContext(String selectorType, boolean backoff)
{    Map<String, String> p = new HashMap<String, String>();    p.put("selector", selectorType);    p.put("backoff", String.valueOf(backoff));    Context ctx = new Context(p);    return ctx;}
0
private Context getContext(String selectorType)
{    Map<String, String> p = new HashMap<String, String>();    p.put("selector", selectorType);    Context ctx = new Context(p);    return ctx;}
0
private LoadBalancingSinkProcessor getProcessor(String selectorType, List<Sink> sinks, boolean backoff)
{    return getProcessor(sinks, getContext(selectorType, backoff));}
0
private LoadBalancingSinkProcessor getProcessor(List<Sink> sinks, Context ctx)
{    LoadBalancingSinkProcessor lbsp = new LoadBalancingSinkProcessor();    lbsp.setSinks(sinks);    lbsp.configure(ctx);    lbsp.start();    return lbsp;}
0
public void testDefaultConfiguration() throws Exception
{        Channel ch = new MockChannel();    int n = 100;    int numEvents = 3 * n;    for (int i = 0; i < numEvents; i++) {        ch.put(new MockEvent("test" + i));    }    MockSink s1 = new MockSink(1);    s1.setChannel(ch);    MockSink s2 = new MockSink(2);    s2.setChannel(ch);    MockSink s3 = new MockSink(3);    s3.setChannel(ch);    List<Sink> sinks = new ArrayList<Sink>();    sinks.add(s1);    sinks.add(s2);    sinks.add(s3);    LoadBalancingSinkProcessor lbsp = getProcessor(sinks, new Context());    Status s = Status.READY;    while (s != Status.BACKOFF) {        s = lbsp.process();    }    Assert.assertTrue(s1.getEvents().size() == n);    Assert.assertTrue(s2.getEvents().size() == n);    Assert.assertTrue(s3.getEvents().size() == n);}
0
public void testRandomOneActiveSink() throws Exception
{    Channel ch = new MockChannel();    int n = 10;    int numEvents = n;    for (int i = 0; i < numEvents; i++) {        ch.put(new MockEvent("test" + i));    }    MockSink s1 = new MockSink(1);    s1.setChannel(ch);        s1.setFail(true);    MockSink s2 = new MockSink(2);    s2.setChannel(ch);    MockSink s3 = new MockSink(3);    s3.setChannel(ch);        s3.setFail(true);    List<Sink> sinks = new ArrayList<Sink>();    sinks.add(s1);    sinks.add(s2);    sinks.add(s3);    LoadBalancingSinkProcessor lbsp = getProcessor("random", sinks, false);    Sink.Status s = Sink.Status.READY;    while (s != Sink.Status.BACKOFF) {        s = lbsp.process();    }    Assert.assertTrue(s1.getEvents().size() == 0);    Assert.assertTrue(s2.getEvents().size() == n);    Assert.assertTrue(s3.getEvents().size() == 0);}
0
public void testRandomBackoff() throws Exception
{    Channel ch = new MockChannel();    int n = 100;    int numEvents = n;    for (int i = 0; i < numEvents; i++) {        ch.put(new MockEvent("test" + i));    }    MockSink s1 = new MockSink(1);    s1.setChannel(ch);        s1.setFail(true);    MockSink s2 = new MockSink(2);    s2.setChannel(ch);    MockSink s3 = new MockSink(3);    s3.setChannel(ch);        s3.setFail(true);    List<Sink> sinks = new ArrayList<Sink>();    sinks.add(s1);    sinks.add(s2);    sinks.add(s3);    LoadBalancingSinkProcessor lbsp = getProcessor("random", sinks, true);        for (int i = 0; i < 50; i++) {                lbsp.process();    }    Assert.assertEquals(50, s2.getEvents().size());    s2.setFail(true);        s1.setFail(false);    try {        lbsp.process();                Assert.fail("Expected EventDeliveryException");    } catch (EventDeliveryException e) {        }        Thread.sleep(2100);    Sink.Status s = Sink.Status.READY;    while (s != Sink.Status.BACKOFF) {        s = lbsp.process();    }    Assert.assertEquals(50, s1.getEvents().size());    Assert.assertEquals(50, s2.getEvents().size());    Assert.assertEquals(0, s3.getEvents().size());}
0
public void testRandomPersistentFailure() throws Exception
{    Channel ch = new MockChannel();    int n = 100;    int numEvents = 3 * n;    for (int i = 0; i < numEvents; i++) {        ch.put(new MockEvent("test" + i));    }    MockSink s1 = new MockSink(1);    s1.setChannel(ch);    MockSink s2 = new MockSink(2);    s2.setChannel(ch);        s2.setFail(true);    MockSink s3 = new MockSink(3);    s3.setChannel(ch);    List<Sink> sinks = new ArrayList<Sink>();    sinks.add(s1);    sinks.add(s2);    sinks.add(s3);    LoadBalancingSinkProcessor lbsp = getProcessor("random", sinks, false);    Status s = Status.READY;    while (s != Status.BACKOFF) {        s = lbsp.process();    }    Assert.assertTrue(s2.getEvents().size() == 0);    Assert.assertTrue(s1.getEvents().size() + s3.getEvents().size() == 3 * n);}
0
public void testRandomNoFailure() throws Exception
{    Channel ch = new MockChannel();    int n = 10000;    int numEvents = n;    for (int i = 0; i < numEvents; i++) {        ch.put(new MockEvent("test" + i));    }    MockSink s1 = new MockSink(1);    s1.setChannel(ch);    MockSink s2 = new MockSink(2);    s2.setChannel(ch);    MockSink s3 = new MockSink(3);    s3.setChannel(ch);    MockSink s4 = new MockSink(4);    s4.setChannel(ch);    MockSink s5 = new MockSink(5);    s5.setChannel(ch);    MockSink s6 = new MockSink(6);    s6.setChannel(ch);    MockSink s7 = new MockSink(7);    s7.setChannel(ch);    MockSink s8 = new MockSink(8);    s8.setChannel(ch);    MockSink s9 = new MockSink(9);    s9.setChannel(ch);    MockSink s0 = new MockSink(0);    s0.setChannel(ch);    List<Sink> sinks = new ArrayList<Sink>();    sinks.add(s1);    sinks.add(s2);    sinks.add(s3);    sinks.add(s4);    sinks.add(s5);    sinks.add(s6);    sinks.add(s7);    sinks.add(s8);    sinks.add(s9);    sinks.add(s0);    LoadBalancingSinkProcessor lbsp = getProcessor("random", sinks, false);    Status s = Status.READY;    while (s != Status.BACKOFF) {        s = lbsp.process();    }    Set<Integer> sizeSet = new HashSet<Integer>();    int sum = 0;    for (Sink ms : sinks) {        int count = ((MockSink) ms).getEvents().size();        sum += count;        sizeSet.add(count);    }        Assert.assertEquals(n, sum);                        Assert.assertTrue("Miraculous distribution", sizeSet.size() > 1);}
0
public void testRoundRobinOneActiveSink() throws Exception
{    Channel ch = new MockChannel();    int n = 10;    int numEvents = n;    for (int i = 0; i < numEvents; i++) {        ch.put(new MockEvent("test" + i));    }    MockSink s1 = new MockSink(1);    s1.setChannel(ch);        s1.setFail(true);    MockSink s2 = new MockSink(2);    s2.setChannel(ch);    MockSink s3 = new MockSink(3);    s3.setChannel(ch);        s3.setFail(true);    List<Sink> sinks = new ArrayList<Sink>();    sinks.add(s1);    sinks.add(s2);    sinks.add(s3);    LoadBalancingSinkProcessor lbsp = getProcessor("round_robin", sinks, false);    Sink.Status s = Sink.Status.READY;    while (s != Sink.Status.BACKOFF) {        s = lbsp.process();    }    Assert.assertTrue(s1.getEvents().size() == 0);    Assert.assertTrue(s2.getEvents().size() == n);    Assert.assertTrue(s3.getEvents().size() == 0);}
0
public void testRoundRobinPersistentFailure() throws Exception
{    Channel ch = new MockChannel();    int n = 100;    int numEvents = 3 * n;    for (int i = 0; i < numEvents; i++) {        ch.put(new MockEvent("test" + i));    }    MockSink s1 = new MockSink(1);    s1.setChannel(ch);    MockSink s2 = new MockSink(2);    s2.setChannel(ch);        s2.setFail(true);    MockSink s3 = new MockSink(3);    s3.setChannel(ch);    List<Sink> sinks = new ArrayList<Sink>();    sinks.add(s1);    sinks.add(s2);    sinks.add(s3);    LoadBalancingSinkProcessor lbsp = getProcessor("round_robin", sinks, false);    Status s = Status.READY;    while (s != Status.BACKOFF) {        s = lbsp.process();    }    Assert.assertTrue(s1.getEvents().size() == n);    Assert.assertTrue(s2.getEvents().size() == 0);    Assert.assertTrue(s3.getEvents().size() == 2 * n);}
0
public void testRoundRobinBackoffInitialFailure() throws EventDeliveryException
{    Channel ch = new MockChannel();    int n = 100;    int numEvents = 3 * n;    for (int i = 0; i < numEvents; i++) {        ch.put(new MockEvent("test" + i));    }    MockSink s1 = new MockSink(1);    s1.setChannel(ch);    MockSink s2 = new MockSink(2);    s2.setChannel(ch);    MockSink s3 = new MockSink(3);    s3.setChannel(ch);    List<Sink> sinks = new ArrayList<Sink>();    sinks.add(s1);    sinks.add(s2);    sinks.add(s3);    LoadBalancingSinkProcessor lbsp = getProcessor("round_robin", sinks, true);    Status s = Status.READY;    for (int i = 0; i < 3 && s != Status.BACKOFF; i++) {        s = lbsp.process();    }    s2.setFail(true);    for (int i = 0; i < 3 && s != Status.BACKOFF; i++) {        s = lbsp.process();    }    s2.setFail(false);    while (s != Status.BACKOFF) {        s = lbsp.process();    }    Assert.assertEquals((3 * n) / 2, s1.getEvents().size());    Assert.assertEquals(1, s2.getEvents().size());    Assert.assertEquals((3 * n) / 2 - 1, s3.getEvents().size());}
0
public void testRoundRobinBackoffIncreasingBackoffs() throws EventDeliveryException, InterruptedException
{    Channel ch = new MockChannel();    int n = 100;    int numEvents = 3 * n;    for (int i = 0; i < numEvents; i++) {        ch.put(new MockEvent("test" + i));    }    MockSink s1 = new MockSink(1);    s1.setChannel(ch);    MockSink s2 = new MockSink(2);    s2.setChannel(ch);    s2.setFail(true);    MockSink s3 = new MockSink(3);    s3.setChannel(ch);    List<Sink> sinks = new ArrayList<Sink>();    sinks.add(s1);    sinks.add(s2);    sinks.add(s3);    LoadBalancingSinkProcessor lbsp = getProcessor("round_robin", sinks, true);    Status s = Status.READY;    for (int i = 0; i < 3 && s != Status.BACKOFF; i++) {        s = lbsp.process();    }    Assert.assertEquals(0, s2.getEvents().size());    Thread.sleep(2100);        for (int i = 0; i < 3 && s != Status.BACKOFF; i++) {        s = lbsp.process();    }    Assert.assertEquals(0, s2.getEvents().size());    s2.setFail(false);    Thread.sleep(2100);        for (int i = 0; i < 3 && s != Status.BACKOFF; i++) {        s = lbsp.process();    }    Assert.assertEquals(0, s2.getEvents().size());        Thread.sleep(2100);    while (s != Status.BACKOFF) {        s = lbsp.process();    }    Assert.assertEquals(n + 2, s1.getEvents().size());    Assert.assertEquals(n - 3, s2.getEvents().size());    Assert.assertEquals(n + 1, s3.getEvents().size());}
0
public void testRoundRobinBackoffFailureRecovery() throws EventDeliveryException, InterruptedException
{    Channel ch = new MockChannel();    int n = 100;    int numEvents = 3 * n;    for (int i = 0; i < numEvents; i++) {        ch.put(new MockEvent("test" + i));    }    MockSink s1 = new MockSink(1);    s1.setChannel(ch);    MockSink s2 = new MockSink(2);    s2.setChannel(ch);    s2.setFail(true);    MockSink s3 = new MockSink(3);    s3.setChannel(ch);    List<Sink> sinks = new ArrayList<Sink>();    sinks.add(s1);    sinks.add(s2);    sinks.add(s3);    LoadBalancingSinkProcessor lbsp = getProcessor("round_robin", sinks, true);    Status s = Status.READY;    for (int i = 0; i < 3 && s != Status.BACKOFF; i++) {        s = lbsp.process();    }    s2.setFail(false);    Thread.sleep(2001);    while (s != Status.BACKOFF) {        s = lbsp.process();    }    Assert.assertEquals(n + 1, s1.getEvents().size());    Assert.assertEquals(n - 1, s2.getEvents().size());    Assert.assertEquals(n, s3.getEvents().size());}
0
public void testRoundRobinNoFailure() throws Exception
{    Channel ch = new MockChannel();    int n = 100;    int numEvents = 3 * n;    for (int i = 0; i < numEvents; i++) {        ch.put(new MockEvent("test" + i));    }    MockSink s1 = new MockSink(1);    s1.setChannel(ch);    MockSink s2 = new MockSink(2);    s2.setChannel(ch);    MockSink s3 = new MockSink(3);    s3.setChannel(ch);    List<Sink> sinks = new ArrayList<Sink>();    sinks.add(s1);    sinks.add(s2);    sinks.add(s3);    LoadBalancingSinkProcessor lbsp = getProcessor("round_robin", sinks, false);    Status s = Status.READY;    while (s != Status.BACKOFF) {        s = lbsp.process();    }    Assert.assertTrue(s1.getEvents().size() == n);    Assert.assertTrue(s2.getEvents().size() == n);    Assert.assertTrue(s3.getEvents().size() == n);}
0
public void testCustomSelector() throws Exception
{    Channel ch = new MockChannel();    int n = 10;    int numEvents = n;    for (int i = 0; i < numEvents; i++) {        ch.put(new MockEvent("test" + i));    }    MockSink s1 = new MockSink(1);    s1.setChannel(ch);        s1.setFail(true);    MockSink s2 = new MockSink(2);    s2.setChannel(ch);    MockSink s3 = new MockSink(3);    s3.setChannel(ch);    List<Sink> sinks = new ArrayList<Sink>();    sinks.add(s1);    sinks.add(s2);    sinks.add(s3);        Context ctx = getContext(FixedOrderSelector.class.getCanonicalName());    ctx.put("selector." + FixedOrderSelector.SET_ME, "foo");    LoadBalancingSinkProcessor lbsp = getProcessor(sinks, ctx);    Sink.Status s = Sink.Status.READY;    while (s != Sink.Status.BACKOFF) {        s = lbsp.process();    }    Assert.assertTrue(s1.getEvents().size() == 0);    Assert.assertTrue(s2.getEvents().size() == n);    Assert.assertTrue(s3.getEvents().size() == 0);}
0
 List<Event> getEvents()
{    return events;}
0
 int getId()
{    return id;}
0
 void setFail(boolean bFail)
{    fail = bFail;}
0
public Status process() throws EventDeliveryException
{    if (fail) {        throw new EventDeliveryException("failed");    }    Event e = this.getChannel().take();    if (e == null) {        return Status.BACKOFF;    }    events.add(e);    return Status.READY;}
0
public void put(Event event) throws ChannelException
{    events.add(event);}
0
public Event take() throws ChannelException
{    if (events.size() > 0) {        return events.remove(0);    }    return null;}
0
public Transaction getTransaction()
{    return null;}
0
public Map<String, String> getHeaders()
{    return EMPTY_HEADERS;}
0
public void setHeaders(Map<String, String> headers)
{    throw new UnsupportedOperationException();}
0
public byte[] getBody()
{    return body;}
0
public void setBody(byte[] body)
{    this.body = body;}
0
public void setUp()
{    sink = new LoggerSink();}
0
public void testAppend() throws InterruptedException, LifecycleException, EventDeliveryException
{    Channel channel = new PseudoTxnMemoryChannel();    Context context = new Context();    Configurables.configure(channel, context);    Configurables.configure(sink, context);    sink.setChannel(channel);    sink.start();    for (int i = 0; i < 10; i++) {        Event event = EventBuilder.withBody(("Test " + i).getBytes());        channel.put(event);        sink.process();    }    sink.stop();}
0
public void testAppendWithCustomSize() throws InterruptedException, LifecycleException, EventDeliveryException
{    Channel channel = new PseudoTxnMemoryChannel();    Context context = new Context();    context.put(LoggerSink.MAX_BYTES_DUMP_KEY, String.valueOf(30));    Configurables.configure(channel, context);    Configurables.configure(sink, context);    sink.setChannel(channel);    sink.start();    for (int i = 0; i < 10; i++) {        Event event = EventBuilder.withBody((Strings.padStart("Test " + i, 30, 'P')).getBytes());        channel.put(event);        sink.process();    }    sink.stop();}
0
public void setUp()
{    tmpDir = new File("/tmp/flume-rfs-" + System.currentTimeMillis() + "-" + Thread.currentThread().getId());    sink = new RollingFileSink();    sink.setChannel(new MemoryChannel());    tmpDir.mkdirs();}
0
public void tearDown()
{    tmpDir.delete();}
0
public void testLifecycle()
{    Context context = new Context();    context.put("sink.directory", tmpDir.getPath());    Configurables.configure(sink, context);    sink.start();    sink.stop();}
0
public void testAppend() throws InterruptedException, EventDeliveryException, IOException
{    Context context = new Context();    context.put("sink.directory", tmpDir.getPath());    context.put("sink.rollInterval", "1");    context.put("sink.batchSize", "1");    doTest(context);}
0
public void testAppend2() throws InterruptedException, EventDeliveryException, IOException
{    Context context = new Context();    context.put("sink.directory", tmpDir.getPath());    context.put("sink.rollInterval", "0");    context.put("sink.batchSize", "1");    doTest(context);}
0
public void testAppend3() throws InterruptedException, EventDeliveryException, IOException
{    File tmpDir = new File("target/tmpLog");    tmpDir.mkdirs();    cleanDirectory(tmpDir);    Context context = new Context();    context.put("sink.directory", "target/tmpLog");    context.put("sink.rollInterval", "0");    context.put("sink.batchSize", "1");    context.put("sink.pathManager.prefix", "test3-");    context.put("sink.pathManager.extension", "txt");    doTest(context);}
0
public void testRollTime() throws InterruptedException, EventDeliveryException, IOException
{    File tmpDir = new File("target/tempLog");    tmpDir.mkdirs();    cleanDirectory(tmpDir);    Context context = new Context();    context.put("sink.directory", "target/tempLog/");    context.put("sink.rollInterval", "1");    context.put("sink.batchSize", "1");    context.put("sink.pathManager", "rolltime");    context.put("sink.pathManager.prefix", "test4-");    context.put("sink.pathManager.extension", "txt");    doTest(context);}
0
public void testChannelException() throws InterruptedException, IOException
{    Context context = new Context();    context.put("sink.directory", tmpDir.getPath());    context.put("sink.rollInterval", "0");    context.put("sink.batchSize", "1");    Channel channel = Mockito.mock(Channel.class);    Mockito.when(channel.take()).thenThrow(new ChannelException("dummy"));    Transaction transaction = Mockito.mock(BasicTransactionSemantics.class);    Mockito.when(channel.getTransaction()).thenReturn(transaction);    try {        doTest(context, channel);    } catch (EventDeliveryException e) {        }    SinkCounter sinkCounter = (SinkCounter) Whitebox.getInternalState(sink, "sinkCounter");    Assert.assertEquals(1, sinkCounter.getChannelReadFail());}
0
private void doTest(Context context) throws EventDeliveryException, InterruptedException, IOException
{    doTest(context, null);}
0
private void doTest(Context context, Channel channel) throws EventDeliveryException, InterruptedException, IOException
{    Configurables.configure(sink, context);    if (channel == null) {        channel = new PseudoTxnMemoryChannel();        Configurables.configure(channel, context);    }    sink.setChannel(channel);    sink.start();    for (int i = 0; i < 10; i++) {        Event event = new SimpleEvent();        event.setBody(("Test event " + i).getBytes());        channel.put(event);        sink.process();        Thread.sleep(500);    }    sink.stop();    for (String file : sink.getDirectory().list()) {        BufferedReader reader = new BufferedReader(new FileReader(new File(sink.getDirectory(), file)));        String lastLine = null;        String currentLine = null;        while ((currentLine = reader.readLine()) != null) {            lastLine = currentLine;                    }        reader.close();    }}
1
private void cleanDirectory(File dir)
{    File[] files = dir.listFiles();    for (File file : files) {        file.delete();    }}
0
public void testTransCapBatchSizeCompatibility() throws EventDeliveryException
{    Context context = new Context();    context.put("sink.directory", tmpDir.getPath());    context.put("sink.rollInterval", "0");    context.put("sink.batchSize", "1000");    Configurables.configure(sink, context);    context.put("capacity", "50");    context.put("transactionCapacity", "5");    Channel channel = new MemoryChannel();    Configurables.configure(channel, context);    sink.setChannel(channel);    sink.start();    try {        for (int j = 0; j < 10; j++) {            Transaction tx = channel.getTransaction();            tx.begin();            for (int i = 0; i < 5; i++) {                Event event = new SimpleEvent();                event.setBody(("Test event " + i).getBytes());                channel.put(event);            }            tx.commit();            tx.close();        }        sink.process();    } finally {        sink.stop();    }}
0
public void setUp() throws Exception
{    sink = new ThriftSink();    channel = new MemoryChannel();    hostname = "0.0.0.0";    try (ServerSocket socket = new ServerSocket(0)) {        port = socket.getLocalPort();    }    Context context = createBaseContext();    context.put(ThriftRpcClient.CONFIG_PROTOCOL, ThriftRpcClient.COMPACT_PROTOCOL);    sink.setChannel(channel);    Configurables.configure(sink, context);    Configurables.configure(channel, context);}
0
private Context createBaseContext()
{    Context context = new Context();    context.put("hostname", hostname);    context.put("port", String.valueOf(port));    context.put("batch-size", String.valueOf(2));    context.put("connect-timeout", String.valueOf(2000L));    context.put("request-timeout", String.valueOf(2000L));    return context;}
0
public void tearDown() throws Exception
{    channel.stop();    sink.stop();    src.stop();}
0
public void testProcess() throws Exception
{    Event event = EventBuilder.withBody("test event 1", Charsets.UTF_8);    src = new ThriftTestingSource(ThriftTestingSource.HandlerType.OK.name(), port, ThriftRpcClient.COMPACT_PROTOCOL);    channel.start();    sink.start();    Transaction transaction = channel.getTransaction();    transaction.begin();    for (int i = 0; i < 11; i++) {        channel.put(event);    }    transaction.commit();    transaction.close();    for (int i = 0; i < 6; i++) {        Sink.Status status = sink.process();        Assert.assertEquals(Sink.Status.READY, status);    }    Assert.assertEquals(Sink.Status.BACKOFF, sink.process());    sink.stop();    Assert.assertEquals(11, src.flumeEvents.size());    Assert.assertEquals(6, src.batchCount);    Assert.assertEquals(0, src.individualCount);}
0
public void testTimeout() throws Exception
{    AtomicLong delay = new AtomicLong();    src = new ThriftTestingSource(ThriftTestingSource.HandlerType.ALTERNATE.name(), port, ThriftRpcClient.COMPACT_PROTOCOL);    src.setDelay(delay);    delay.set(2500);    Event event = EventBuilder.withBody("foo", Charsets.UTF_8);    sink.start();    Transaction txn = channel.getTransaction();    txn.begin();    for (int i = 0; i < 4; i++) {        channel.put(event);    }    txn.commit();    txn.close();        boolean threw = false;    try {        sink.process();    } catch (EventDeliveryException ex) {        threw = true;    }    Assert.assertTrue("Must throw due to connect timeout", threw);        delay.set(0);    sink.process();            delay.set(2500L);    threw = false;    try {        sink.process();    } catch (EventDeliveryException ex) {        threw = true;    }    Assert.assertTrue("Must throw due to request timeout", threw);    sink.stop();}
0
public void testFailedConnect() throws Exception
{    Event event = EventBuilder.withBody("test event 1", Charset.forName("UTF8"));    sink.start();        Thread.sleep(500L);        Thread.sleep(500L);    Transaction transaction = channel.getTransaction();    transaction.begin();    for (int i = 0; i < 10; i++) {        channel.put(event);    }    transaction.commit();    transaction.close();    for (int i = 0; i < 5; i++) {        boolean threwException = false;        try {            sink.process();        } catch (EventDeliveryException e) {            threwException = true;        }        Assert.assertTrue("Must throw EventDeliveryException if disconnected", threwException);    }    src = new ThriftTestingSource(ThriftTestingSource.HandlerType.OK.name(), port, ThriftRpcClient.COMPACT_PROTOCOL);    for (int i = 0; i < 5; i++) {        Sink.Status status = sink.process();        Assert.assertEquals(Sink.Status.READY, status);    }    Assert.assertEquals(Sink.Status.BACKOFF, sink.process());    sink.stop();}
0
public void testSslProcessWithComponentTruststore() throws Exception
{    Context context = createBaseContext();    context.put("ssl", String.valueOf(true));    context.put("truststore", "src/test/resources/truststorefile.jks");    context.put("truststore-password", "password");    Configurables.configure(sink, context);    doTestSslProcess();}
0
public void testSslProcessWithComponentTruststoreNoPassword() throws Exception
{    Context context = createBaseContext();    context.put("ssl", String.valueOf(true));    context.put("truststore", "src/test/resources/truststorefile.jks");    Configurables.configure(sink, context);    doTestSslProcess();}
0
public void testSslProcessWithGlobalTruststore() throws Exception
{    System.setProperty("javax.net.ssl.trustStore", "src/test/resources/truststorefile.jks");    System.setProperty("javax.net.ssl.trustStorePassword", "password");    Context context = createBaseContext();    context.put("ssl", String.valueOf(true));    Configurables.configure(sink, context);    doTestSslProcess();    System.clearProperty("javax.net.ssl.trustStore");    System.clearProperty("javax.net.ssl.trustStorePassword");}
0
public void testSslProcessWithGlobalTruststoreNoPassword() throws Exception
{    System.setProperty("javax.net.ssl.trustStore", "src/test/resources/truststorefile.jks");    Context context = createBaseContext();    context.put("ssl", String.valueOf(true));    Configurables.configure(sink, context);    doTestSslProcess();    System.clearProperty("javax.net.ssl.trustStore");}
0
private void doTestSslProcess() throws Exception
{    src = new ThriftTestingSource(ThriftTestingSource.HandlerType.OK.name(), port, ThriftRpcClient.COMPACT_PROTOCOL, "src/test/resources/keystorefile.jks", "password", KeyManagerFactory.getDefaultAlgorithm(), "JKS");    channel.start();    sink.start();    Transaction transaction = channel.getTransaction();    transaction.begin();    Event event = EventBuilder.withBody("test event 1", Charsets.UTF_8);    for (int i = 0; i < 11; i++) {        channel.put(event);    }    transaction.commit();    transaction.close();    for (int i = 0; i < 6; i++) {        Sink.Status status = sink.process();        Assert.assertEquals(Sink.Status.READY, status);    }    Assert.assertEquals(Sink.Status.BACKOFF, sink.process());    sink.stop();    Assert.assertEquals(11, src.flumeEvents.size());    Assert.assertEquals(6, src.batchCount);    Assert.assertEquals(0, src.individualCount);}
0
public void testSslSinkWithNonSslServer() throws Exception
{    src = new ThriftTestingSource(ThriftTestingSource.HandlerType.OK.name(), port, ThriftRpcClient.COMPACT_PROTOCOL);    Context context = createBaseContext();    context.put("ssl", String.valueOf(true));    context.put("truststore", "src/test/resources/truststorefile.jks");    context.put("truststore-password", "password");    Configurables.configure(sink, context);    boolean failed = doRequestWhenFailureExpected();    if (!failed) {        Assert.fail("SSL-enabled sink successfully connected to a non-SSL-enabled server, " + "that's wrong.");    }}
0
public void testSslSinkWithNonTrustedCert() throws Exception
{    src = new ThriftTestingSource(ThriftTestingSource.HandlerType.OK.name(), port, ThriftRpcClient.COMPACT_PROTOCOL, "src/test/resources/keystorefile.jks", "password", KeyManagerFactory.getDefaultAlgorithm(), "JKS");    Context context = createBaseContext();    context.put("ssl", String.valueOf(true));    Configurables.configure(sink, context);    boolean failed = doRequestWhenFailureExpected();    if (!failed) {        Assert.fail("SSL-enabled sink successfully connected to a server with an " + "untrusted certificate when it should have failed");    }}
0
private boolean doRequestWhenFailureExpected() throws Exception
{    channel.start();    sink.start();    Assert.assertTrue(LifecycleController.waitForOneOf(sink, LifecycleState.START_OR_ERROR, 5000));    Transaction transaction = channel.getTransaction();    transaction.begin();    Event event = EventBuilder.withBody("test event 1", Charsets.UTF_8);    channel.put(event);    transaction.commit();    transaction.close();    boolean failed;    try {        Sink.Status status = sink.process();        failed = false;    } catch (EventDeliveryException ex) {                failed = true;    }    sink.stop();    Assert.assertTrue(LifecycleController.waitForOneOf(sink, LifecycleState.STOP_OR_ERROR, 5000));    return failed;}
0
public String getAuthType()
{    throw new UnsupportedOperationException("Not supported yet.");}
0
public Cookie[] getCookies()
{    throw new UnsupportedOperationException("Not supported yet.");}
0
public long getDateHeader(String name)
{    throw new UnsupportedOperationException("Not supported yet.");}
0
public String getHeader(String name)
{    throw new UnsupportedOperationException("Not supported yet.");}
0
public Enumeration<String> getHeaders(String name)
{    throw new UnsupportedOperationException("Not supported yet.");}
0
public Enumeration<String> getHeaderNames()
{    throw new UnsupportedOperationException("Not supported yet.");}
0
public int getIntHeader(String name)
{    throw new UnsupportedOperationException("Not supported yet.");}
0
public String getMethod()
{    throw new UnsupportedOperationException("Not supported yet.");}
0
public String getPathInfo()
{    throw new UnsupportedOperationException("Not supported yet.");}
0
public String getPathTranslated()
{    throw new UnsupportedOperationException("Not supported yet.");}
0
public String getContextPath()
{    throw new UnsupportedOperationException("Not supported yet.");}
0
public String getQueryString()
{    throw new UnsupportedOperationException("Not supported yet.");}
0
public String getRemoteUser()
{    throw new UnsupportedOperationException("Not supported yet.");}
0
public boolean isUserInRole(String role)
{    throw new UnsupportedOperationException("Not supported yet.");}
0
public Principal getUserPrincipal()
{    throw new UnsupportedOperationException("Not supported yet.");}
0
public String getRequestedSessionId()
{    throw new UnsupportedOperationException("Not supported yet.");}
0
public String getRequestURI()
{    throw new UnsupportedOperationException("Not supported yet.");}
0
public StringBuffer getRequestURL()
{    throw new UnsupportedOperationException("Not supported yet.");}
0
public String getServletPath()
{    throw new UnsupportedOperationException("Not supported yet.");}
0
public HttpSession getSession(boolean create)
{    throw new UnsupportedOperationException("Not supported yet.");}
0
public HttpSession getSession()
{    throw new UnsupportedOperationException("Not supported yet.");}
0
public boolean isRequestedSessionIdValid()
{    throw new UnsupportedOperationException("Not supported yet.");}
0
public boolean isRequestedSessionIdFromCookie()
{    throw new UnsupportedOperationException("Not supported yet.");}
0
public boolean isRequestedSessionIdFromURL()
{    throw new UnsupportedOperationException("Not supported yet.");}
0
public boolean isRequestedSessionIdFromUrl()
{    throw new UnsupportedOperationException("Not supported yet.");}
0
public Object getAttribute(String name)
{    throw new UnsupportedOperationException("Not supported yet.");}
0
public Enumeration<String> getAttributeNames()
{    throw new UnsupportedOperationException("Not supported yet.");}
0
public String getCharacterEncoding()
{    return charset;}
0
public void setCharacterEncoding(String env) throws UnsupportedEncodingException
{    this.charset = env;}
0
public int getContentLength()
{    throw new UnsupportedOperationException("Not supported yet.");}
0
public String getContentType()
{    throw new UnsupportedOperationException("Not supported yet.");}
0
public ServletInputStream getInputStream() throws IOException
{    throw new UnsupportedOperationException("Not supported yet.");}
0
public String getParameter(String name)
{    throw new UnsupportedOperationException("Not supported yet.");}
0
public Enumeration<String> getParameterNames()
{    throw new UnsupportedOperationException("Not supported yet.");}
0
public String[] getParameterValues(String name)
{    throw new UnsupportedOperationException("Not supported yet.");}
0
public Map<String, String[]> getParameterMap()
{    throw new UnsupportedOperationException("Not supported yet.");}
0
public String getProtocol()
{    throw new UnsupportedOperationException("Not supported yet.");}
0
public String getScheme()
{    throw new UnsupportedOperationException("Not supported yet.");}
0
public String getServerName()
{    throw new UnsupportedOperationException("Not supported yet.");}
0
public int getServerPort()
{    throw new UnsupportedOperationException("Not supported yet.");}
0
public BufferedReader getReader() throws IOException
{    return reader;}
0
public String getRemoteAddr()
{    throw new UnsupportedOperationException("Not supported yet.");}
0
public String getRemoteHost()
{    throw new UnsupportedOperationException("Not supported yet.");}
0
public void setAttribute(String name, Object o)
{    throw new UnsupportedOperationException("Not supported yet.");}
0
public void removeAttribute(String name)
{    throw new UnsupportedOperationException("Not supported yet.");}
0
public Locale getLocale()
{    throw new UnsupportedOperationException("Not supported yet.");}
0
public Enumeration<Locale> getLocales()
{    throw new UnsupportedOperationException("Not supported yet.");}
0
public boolean isSecure()
{    throw new UnsupportedOperationException("Not supported yet.");}
0
public RequestDispatcher getRequestDispatcher(String path)
{    throw new UnsupportedOperationException("Not supported yet.");}
0
public String getRealPath(String path)
{    throw new UnsupportedOperationException("Not supported yet.");}
0
public int getRemotePort()
{    throw new UnsupportedOperationException("Not supported yet.");}
0
public String getLocalName()
{    throw new UnsupportedOperationException("Not supported yet.");}
0
public String getLocalAddr()
{    throw new UnsupportedOperationException("Not supported yet.");}
0
public int getLocalPort()
{    throw new UnsupportedOperationException("Not supported yet.");}
0
public AsyncContext getAsyncContext()
{    throw new UnsupportedOperationException("Not supported yet.");}
0
public long getContentLengthLong()
{    throw new UnsupportedOperationException("Not supported yet.");}
0
public DispatcherType getDispatcherType()
{    throw new UnsupportedOperationException("Not supported yet.");}
0
public ServletContext getServletContext()
{    throw new UnsupportedOperationException("Not supported yet.");}
0
public boolean isAsyncStarted()
{    throw new UnsupportedOperationException("Not supported yet.");}
0
public boolean isAsyncSupported()
{    throw new UnsupportedOperationException("Not supported yet.");}
0
public AsyncContext startAsync() throws IllegalStateException
{    throw new UnsupportedOperationException("Not supported yet.");}
0
public AsyncContext startAsync(ServletRequest arg0, ServletResponse arg1) throws IllegalStateException
{    throw new UnsupportedOperationException("Not supported yet.");}
0
public boolean authenticate(HttpServletResponse arg0) throws IOException, ServletException
{    throw new UnsupportedOperationException("Not supported yet.");}
0
public String changeSessionId()
{    throw new UnsupportedOperationException("Not supported yet.");}
0
public Part getPart(String arg0) throws IOException, ServletException
{    throw new UnsupportedOperationException("Not supported yet.");}
0
public Collection<Part> getParts() throws IOException, ServletException
{    throw new UnsupportedOperationException("Not supported yet.");}
0
public void login(String arg0, String arg1) throws ServletException
{    throw new UnsupportedOperationException("Not supported yet.");}
0
public void logout() throws ServletException
{    throw new UnsupportedOperationException("Not supported yet.");}
0
public T upgrade(Class<T> arg0) throws IOException, ServletException
{    throw new UnsupportedOperationException("Not supported yet.");}
0
public void setUp()
{    handler = new BLOBHandler();}
0
public void testCSVData() throws Exception
{    Map requestParameterMap = new HashMap();    requestParameterMap.put("param1", new String[] { "value1" });    requestParameterMap.put("param2", new String[] { "value2" });    HttpServletRequest req = mock(HttpServletRequest.class);    final String csvData = "a,b,c";    ServletInputStream servletInputStream = new DelegatingServletInputStream(new ByteArrayInputStream(csvData.getBytes()));    when(req.getInputStream()).thenReturn(servletInputStream);    when(req.getParameterMap()).thenReturn(requestParameterMap);    Context context = mock(Context.class);    when(context.getString(BLOBHandler.MANDATORY_PARAMETERS, BLOBHandler.DEFAULT_MANDATORY_PARAMETERS)).thenReturn("param1,param2");    handler.configure(context);    List<Event> deserialized = handler.getEvents(req);    assertEquals(1, deserialized.size());    Event e = deserialized.get(0);    assertEquals(new String(e.getBody()), csvData);    assertEquals(e.getHeaders().get("param1"), "value1");    assertEquals(e.getHeaders().get("param2"), "value2");}
0
public void testTabData() throws Exception
{    Map requestParameterMap = new HashMap();    requestParameterMap.put("param1", new String[] { "value1" });    HttpServletRequest req = mock(HttpServletRequest.class);    final String tabData = "a\tb\tc";    ServletInputStream servletInputStream = new DelegatingServletInputStream(new ByteArrayInputStream(tabData.getBytes()));    when(req.getInputStream()).thenReturn(servletInputStream);    when(req.getParameterMap()).thenReturn(requestParameterMap);    Context context = mock(Context.class);    when(context.getString(BLOBHandler.MANDATORY_PARAMETERS, BLOBHandler.DEFAULT_MANDATORY_PARAMETERS)).thenReturn("param1");    handler.configure(context);    List<Event> deserialized = handler.getEvents(req);    assertEquals(1, deserialized.size());    Event e = deserialized.get(0);    assertEquals(new String(e.getBody()), tabData);    assertEquals(e.getHeaders().get("param1"), "value1");}
0
public void testMissingParameters() throws Exception
{    Map requestParameterMap = new HashMap();    HttpServletRequest req = mock(HttpServletRequest.class);    final String tabData = "a\tb\tc";    ServletInputStream servletInputStream = new DelegatingServletInputStream(new ByteArrayInputStream(tabData.getBytes()));    when(req.getInputStream()).thenReturn(servletInputStream);    when(req.getParameterMap()).thenReturn(requestParameterMap);    Context context = mock(Context.class);    when(context.getString(BLOBHandler.MANDATORY_PARAMETERS, BLOBHandler.DEFAULT_MANDATORY_PARAMETERS)).thenReturn("param1");    handler.configure(context);    handler.getEvents(req);}
0
public final InputStream getSourceStream()
{    return this.sourceStream;}
0
public int read() throws IOException
{    return this.sourceStream.read();}
0
public void close() throws IOException
{    super.close();    this.sourceStream.close();}
0
public boolean isFinished()
{    throw new UnsupportedOperationException("Not supported yet.");}
0
public boolean isReady()
{    throw new UnsupportedOperationException("Not supported yet.");}
0
public void setReadListener(ReadListener arg0)
{    throw new UnsupportedOperationException("Not supported yet.");}
0
private static int findFreePort() throws IOException
{    ServerSocket socket = new ServerSocket(0);    int port = socket.getLocalPort();    socket.close();    return port;}
0
private static Context getDefaultNonSecureContext(int port) throws IOException
{    Context ctx = new Context();    ctx.put(HTTPSourceConfigurationConstants.CONFIG_BIND, "0.0.0.0");    ctx.put(HTTPSourceConfigurationConstants.CONFIG_PORT, String.valueOf(port));    ctx.put("QueuedThreadPool.MaxThreads", "100");    return ctx;}
0
private static Context getDefaultSecureContext(int port) throws IOException
{    Context sslContext = new Context();    sslContext.put(HTTPSourceConfigurationConstants.CONFIG_PORT, String.valueOf(port));    sslContext.put(HTTPSourceConfigurationConstants.SSL_ENABLED, "true");    sslContext.put(HTTPSourceConfigurationConstants.SSL_KEYSTORE_PASSWORD, "password");    sslContext.put(HTTPSourceConfigurationConstants.SSL_KEYSTORE, "src/test/resources/jettykeystore");    return sslContext;}
0
private static Context getDefaultSecureContextGlobalKeystore(int port) throws IOException
{    System.setProperty("javax.net.ssl.keyStore", "src/test/resources/jettykeystore");    System.setProperty("javax.net.ssl.keyStorePassword", "password");    Context sslContext = new Context();    sslContext.put(HTTPSourceConfigurationConstants.CONFIG_PORT, String.valueOf(port));    sslContext.put(HTTPSourceConfigurationConstants.SSL_ENABLED, "true");    return sslContext;}
0
public static void setUpClass() throws Exception
{    httpSource = new HTTPSource();    httpChannel = new MemoryChannel();    httpPort = findFreePort();    configureSourceAndChannel(httpSource, httpChannel, getDefaultNonSecureContext(httpPort));    httpChannel.start();    httpSource.start();    httpsSource = new HTTPSource();    httpsChannel = new MemoryChannel();    httpsPort = findFreePort();    configureSourceAndChannel(httpsSource, httpsChannel, getDefaultSecureContext(httpsPort));    httpsChannel.start();    httpsSource.start();    httpsGlobalKeystoreSource = new HTTPSource();    httpsGlobalKeystoreChannel = new MemoryChannel();    httpsGlobalKeystorePort = findFreePort();    configureSourceAndChannel(httpsGlobalKeystoreSource, httpsGlobalKeystoreChannel, getDefaultSecureContextGlobalKeystore(httpsGlobalKeystorePort));    httpsGlobalKeystoreChannel.start();    httpsGlobalKeystoreSource.start();    System.clearProperty("javax.net.ssl.keyStore");    System.clearProperty("javax.net.ssl.keyStorePassword");}
0
private static void configureSourceAndChannel(HTTPSource source, Channel channel, Context context)
{    Context channelContext = new Context();    channelContext.put("capacity", "100");    Configurables.configure(channel, channelContext);    Configurables.configure(source, context);    ChannelSelector rcs1 = new ReplicatingChannelSelector();    rcs1.setChannels(Collections.singletonList(channel));    source.setChannelProcessor(new ChannelProcessor(rcs1));}
0
public static void tearDownClass() throws Exception
{    httpSource.stop();    httpChannel.stop();    httpsSource.stop();    httpsChannel.stop();    httpsGlobalKeystoreSource.stop();    httpsGlobalKeystoreChannel.stop();}
0
public void setUp()
{    HttpClientBuilder builder = HttpClientBuilder.create();    httpClient = builder.build();    postRequest = new HttpPost("http://0.0.0.0:" + httpPort);}
0
public void testSimple() throws IOException, InterruptedException
{    StringEntity input = new StringEntity("[{\"headers\":{\"a\": \"b\"},\"body\": \"random_body\"}," + "{\"headers\":{\"e\": \"f\"},\"body\": \"random_body2\"}]");            input.setContentType("application/json");    postRequest.setEntity(input);    HttpResponse response = httpClient.execute(postRequest);    Assert.assertEquals(HttpServletResponse.SC_OK, response.getStatusLine().getStatusCode());    Transaction tx = httpChannel.getTransaction();    tx.begin();    Event e = httpChannel.take();    Assert.assertNotNull(e);    Assert.assertEquals("b", e.getHeaders().get("a"));    Assert.assertEquals("random_body", new String(e.getBody(), "UTF-8"));    e = httpChannel.take();    Assert.assertNotNull(e);    Assert.assertEquals("f", e.getHeaders().get("e"));    Assert.assertEquals("random_body2", new String(e.getBody(), "UTF-8"));    tx.commit();    tx.close();}
0
public void testTrace() throws Exception
{    doTestForbidden(new HttpTrace("http://0.0.0.0:" + httpPort));}
0
public void testOptions() throws Exception
{    doTestForbidden(new HttpOptions("http://0.0.0.0:" + httpPort));}
0
private void doTestForbidden(HttpRequestBase request) throws Exception
{    HttpResponse response = httpClient.execute(request);    Assert.assertEquals(HttpServletResponse.SC_FORBIDDEN, response.getStatusLine().getStatusCode());}
0
public void testSimpleUTF16() throws IOException, InterruptedException
{    StringEntity input = new StringEntity("[{\"headers\":{\"a\": \"b\"},\"body\": \"random_body\"}," + "{\"headers\":{\"e\": \"f\"},\"body\": \"random_body2\"}]", "UTF-16");    input.setContentType("application/json; charset=utf-16");    postRequest.setEntity(input);    HttpResponse response = httpClient.execute(postRequest);    Assert.assertEquals(HttpServletResponse.SC_OK, response.getStatusLine().getStatusCode());    Transaction tx = httpChannel.getTransaction();    tx.begin();    Event e = httpChannel.take();    Assert.assertNotNull(e);    Assert.assertEquals("b", e.getHeaders().get("a"));    Assert.assertEquals("random_body", new String(e.getBody(), "UTF-16"));    e = httpChannel.take();    Assert.assertNotNull(e);    Assert.assertEquals("f", e.getHeaders().get("e"));    Assert.assertEquals("random_body2", new String(e.getBody(), "UTF-16"));    tx.commit();    tx.close();}
0
public void testInvalid() throws Exception
{    StringEntity input = new StringEntity("[{\"a\": \"b\",[\"d\":\"e\"],\"body\": \"random_body\"}," + "{\"e\": \"f\",\"body\": \"random_body2\"}]");    input.setContentType("application/json");    postRequest.setEntity(input);    HttpResponse response = httpClient.execute(postRequest);    Assert.assertEquals(HttpServletResponse.SC_BAD_REQUEST, response.getStatusLine().getStatusCode());    SourceCounter sc = (SourceCounter) Whitebox.getInternalState(httpSource, "sourceCounter");    Assert.assertEquals(1, sc.getEventReadFail());}
0
public void testBigBatchDeserializarionUTF8() throws Exception
{    testBatchWithVariousEncoding("UTF-8");}
0
public void testBigBatchDeserializarionUTF16() throws Exception
{    testBatchWithVariousEncoding("UTF-16");}
0
public void testBigBatchDeserializarionUTF32() throws Exception
{    testBatchWithVariousEncoding("UTF-32");}
0
public void testCounterGenericFail() throws Exception
{    ChannelProcessor cp = Mockito.mock(ChannelProcessor.class);    doThrow(new RuntimeException("dummy")).when(cp).processEventBatch(anyListOf(Event.class));    ChannelProcessor oldCp = httpSource.getChannelProcessor();    httpSource.setChannelProcessor(cp);    testBatchWithVariousEncoding("UTF-8");    SourceCounter sc = (SourceCounter) Whitebox.getInternalState(httpSource, "sourceCounter");    Assert.assertEquals(1, sc.getGenericProcessingFail());    httpSource.setChannelProcessor(oldCp);}
0
public void testSingleEvent() throws Exception
{    StringEntity input = new StringEntity("[{\"headers\" : {\"a\": \"b\"},\"body\":" + " \"random_body\"}]");    input.setContentType("application/json");    postRequest.setEntity(input);    httpClient.execute(postRequest);    Transaction tx = httpChannel.getTransaction();    tx.begin();    Event e = httpChannel.take();    Assert.assertNotNull(e);    Assert.assertEquals("b", e.getHeaders().get("a"));    Assert.assertEquals("random_body", new String(e.getBody(), "UTF-8"));    tx.commit();    tx.close();}
0
public void testConfigurables() throws Exception
{    StringEntity input = new StringEntity("[{\"headers\" : {\"a\": \"b\"},\"body\":" + " \"random_body\"}]");    input.setContentType("application/json");    postRequest.setEntity(input);    HttpResponse resp = httpClient.execute(postRequest);        Assert.assertTrue(resp.getHeaders("X-Powered-By").length == 0);    Assert.assertTrue(resp.getHeaders("Server").length == 1);    Transaction tx = httpChannel.getTransaction();    tx.begin();    Event e = httpChannel.take();    Assert.assertNotNull(e);    tx.commit();    tx.close();    Assert.assertTrue(findMBeans("org.eclipse.jetty.util.thread:type=queuedthreadpool,*", "maxThreads", 123).size() == 0);    Assert.assertTrue(findMBeans("org.eclipse.jetty.server:type=serverconnector,*", "acceptQueueSize", 22).size() == 0);    int newPort = findFreePort();    Context configuredSourceContext = getDefaultNonSecureContext(newPort);    configuredSourceContext.put("HttpConfiguration.sendServerVersion", "false");    configuredSourceContext.put("HttpConfiguration.sendXPoweredBy", "true");    configuredSourceContext.put("ServerConnector.acceptQueueSize", "22");    configuredSourceContext.put("QueuedThreadPool.maxThreads", "123");    HTTPSource newSource = new HTTPSource();    Channel newChannel = new MemoryChannel();    configureSourceAndChannel(newSource, newChannel, configuredSourceContext);    newChannel.start();    newSource.start();    HttpPost newPostRequest = new HttpPost("http://0.0.0.0:" + newPort);    resp = httpClient.execute(newPostRequest);    Assert.assertTrue(resp.getHeaders("X-Powered-By").length > 0);    Assert.assertTrue(resp.getHeaders("Server").length == 0);    Assert.assertTrue(findMBeans("org.eclipse.jetty.util.thread:type=queuedthreadpool,*", "maxThreads", 123).size() == 1);    Assert.assertTrue(findMBeans("org.eclipse.jetty.server:type=serverconnector,*", "acceptQueueSize", 22).size() == 1);    newSource.stop();    newChannel.stop();        newPort = findFreePort();    configuredSourceContext = getDefaultSecureContext(newPort);    configuredSourceContext.put("SslContextFactory.IncludeProtocols", "abc def");    newSource = new HTTPSource();    newChannel = new MemoryChannel();    configureSourceAndChannel(newSource, newChannel, configuredSourceContext);    newChannel.start();    newSource.start();    newPostRequest = new HttpPost("http://0.0.0.0:" + newPort);    try {        doTestHttps(null, newPort, httpsChannel);                Assert.assertTrue(false);    } catch (AssertionError ex) {        }    newSource.stop();    newChannel.stop();}
0
public void testFullChannel() throws Exception
{    HttpResponse response = putWithEncoding("UTF-8", 150).response;    Assert.assertEquals(HttpServletResponse.SC_SERVICE_UNAVAILABLE, response.getStatusLine().getStatusCode());    SourceCounter sc = (SourceCounter) Whitebox.getInternalState(httpSource, "sourceCounter");    Assert.assertEquals(1, sc.getChannelWriteFail());}
0
public void testFail() throws Exception
{    HTTPSourceHandler handler = field("handler").ofType(HTTPSourceHandler.class).in(httpSource).get();            field("handler").ofType(HTTPSourceHandler.class).in(httpSource).set(null);    HttpResponse response = putWithEncoding("UTF-8", 1).response;    Assert.assertEquals(HttpServletResponse.SC_INTERNAL_SERVER_ERROR, response.getStatusLine().getStatusCode());        field("handler").ofType(HTTPSourceHandler.class).in(httpSource).set(handler);}
0
public void testMBeans() throws Exception
{    MBeanServer mbeanServer = ManagementFactory.getPlatformMBeanServer();    ObjectName objectName = new ObjectName("org.eclipse.jetty.*:*");    Set<ObjectInstance> queryMBeans = mbeanServer.queryMBeans(objectName, null);    Assert.assertTrue(queryMBeans.size() > 0);}
0
public void testHandlerThrowingException() throws Exception
{            HttpResponse response = putWithEncoding("ISO-8859-1", 150).response;    Assert.assertEquals(HttpServletResponse.SC_INTERNAL_SERVER_ERROR, response.getStatusLine().getStatusCode());}
0
private Set<ObjectInstance> findMBeans(String name, String attribute, int value) throws MalformedObjectNameException
{    MBeanServer mbeanServer = ManagementFactory.getPlatformMBeanServer();    ObjectName objectName = new ObjectName(name);    QueryExp q = Query.eq(Query.attr(attribute), Query.value(value));    return mbeanServer.queryMBeans(objectName, q);}
0
private ResultWrapper putWithEncoding(String encoding, int n) throws Exception
{    Type listType = new TypeToken<List<JSONEvent>>() {    }.getType();    List<JSONEvent> events = new ArrayList<JSONEvent>();    Random rand = new Random();    for (int i = 0; i < n; i++) {        Map<String, String> input = Maps.newHashMap();        for (int j = 0; j < 10; j++) {            input.put(String.valueOf(i) + String.valueOf(j), String.valueOf(i));        }        JSONEvent e = new JSONEvent();        e.setHeaders(input);        e.setBody(String.valueOf(rand.nextGaussian()).getBytes(encoding));        events.add(e);    }    Gson gson = new Gson();    String json = gson.toJson(events, listType);    StringEntity input = new StringEntity(json);    input.setContentType("application/json; charset=" + encoding);    postRequest.setEntity(input);    HttpResponse resp = httpClient.execute(postRequest);    return new ResultWrapper(resp, events);}
0
public void testHttps() throws Exception
{    doTestHttps(null, httpsPort, httpsChannel);}
0
public void testHttpsSSLv3() throws Exception
{    doTestHttps("SSLv3", httpsPort, httpsChannel);}
0
public void testHttpsGlobalKeystore() throws Exception
{    doTestHttps(null, httpsGlobalKeystorePort, httpsGlobalKeystoreChannel);}
0
private void doTestHttps(String protocol, int port, Channel channel) throws Exception
{    Type listType = new TypeToken<List<JSONEvent>>() {    }.getType();    List<JSONEvent> events = new ArrayList<JSONEvent>();    Random rand = new Random();    for (int i = 0; i < 10; i++) {        Map<String, String> input = Maps.newHashMap();        for (int j = 0; j < 10; j++) {            input.put(String.valueOf(i) + String.valueOf(j), String.valueOf(i));        }        input.put("MsgNum", String.valueOf(i));        JSONEvent e = new JSONEvent();        e.setHeaders(input);        e.setBody(String.valueOf(rand.nextGaussian()).getBytes("UTF-8"));        events.add(e);    }    Gson gson = new Gson();    String json = gson.toJson(events, listType);    HttpsURLConnection httpsURLConnection = null;    Transaction transaction = null;    try {        TrustManager[] trustAllCerts = { new X509TrustManager() {            @Override            public void checkClientTrusted(java.security.cert.X509Certificate[] x509Certificates, String s) throws CertificateException {                        }            @Override            public void checkServerTrusted(java.security.cert.X509Certificate[] x509Certificates, String s) throws CertificateException {                        }            public java.security.cert.X509Certificate[] getAcceptedIssuers() {                return null;            }        } };        SSLContext sc = null;        javax.net.ssl.SSLSocketFactory factory = null;        if (System.getProperty("java.vendor").contains("IBM")) {            sc = SSLContext.getInstance("SSL_TLS");        } else {            sc = SSLContext.getInstance("SSL");        }        HostnameVerifier hv = new HostnameVerifier() {            public boolean verify(String arg0, SSLSession arg1) {                return true;            }        };        sc.init(null, trustAllCerts, new SecureRandom());        if (protocol != null) {            factory = new DisabledProtocolsSocketFactory(sc.getSocketFactory(), protocol);        } else {            factory = sc.getSocketFactory();        }        HttpsURLConnection.setDefaultSSLSocketFactory(factory);        HttpsURLConnection.setDefaultHostnameVerifier(NoopHostnameVerifier.INSTANCE);        URL sslUrl = new URL("https://0.0.0.0:" + port);        httpsURLConnection = (HttpsURLConnection) sslUrl.openConnection();        httpsURLConnection.setDoInput(true);        httpsURLConnection.setDoOutput(true);        httpsURLConnection.setRequestMethod("POST");        httpsURLConnection.getOutputStream().write(json.getBytes());        int statusCode = httpsURLConnection.getResponseCode();        Assert.assertEquals(200, statusCode);        transaction = channel.getTransaction();        transaction.begin();        for (int i = 0; i < 10; i++) {            Event e = channel.take();            Assert.assertNotNull(e);            Assert.assertEquals(String.valueOf(i), e.getHeaders().get("MsgNum"));        }    } finally {        if (transaction != null) {            transaction.commit();            transaction.close();        }        httpsURLConnection.disconnect();    }}
0
public void checkClientTrusted(java.security.cert.X509Certificate[] x509Certificates, String s) throws CertificateException
{}
0
public void checkServerTrusted(java.security.cert.X509Certificate[] x509Certificates, String s) throws CertificateException
{}
0
public java.security.cert.X509Certificate[] getAcceptedIssuers()
{    return null;}
0
public boolean verify(String arg0, SSLSession arg1)
{    return true;}
0
public void testHttpsSourceNonHttpsClient() throws Exception
{    Type listType = new TypeToken<List<JSONEvent>>() {    }.getType();    List<JSONEvent> events = new ArrayList<JSONEvent>();    Random rand = new Random();    for (int i = 0; i < 10; i++) {        Map<String, String> input = Maps.newHashMap();        for (int j = 0; j < 10; j++) {            input.put(String.valueOf(i) + String.valueOf(j), String.valueOf(i));        }        input.put("MsgNum", String.valueOf(i));        JSONEvent e = new JSONEvent();        e.setHeaders(input);        e.setBody(String.valueOf(rand.nextGaussian()).getBytes("UTF-8"));        events.add(e);    }    Gson gson = new Gson();    String json = gson.toJson(events, listType);    HttpURLConnection httpURLConnection = null;    try {        URL url = new URL("http://0.0.0.0:" + httpsPort);        httpURLConnection = (HttpURLConnection) url.openConnection();        httpURLConnection.setDoInput(true);        httpURLConnection.setDoOutput(true);        httpURLConnection.setRequestMethod("POST");        httpURLConnection.getOutputStream().write(json.getBytes());        httpURLConnection.getResponseCode();        Assert.fail("HTTP Client cannot connect to HTTPS source");    } catch (Exception exception) {        Assert.assertTrue("Exception expected", true);    } finally {        httpURLConnection.disconnect();    }}
0
private void takeWithEncoding(String encoding, int n, List<JSONEvent> events) throws Exception
{    Transaction tx = httpChannel.getTransaction();    tx.begin();    Event e = null;    int i = 0;    while (true) {        e = httpChannel.take();        if (e == null) {            break;        }        Event current = events.get(i++);        Assert.assertEquals(new String(current.getBody(), encoding), new String(e.getBody(), encoding));        Assert.assertEquals(current.getHeaders(), e.getHeaders());    }    Assert.assertEquals(n, events.size());    tx.commit();    tx.close();}
0
private void testBatchWithVariousEncoding(String encoding) throws Exception
{    testBatchWithVariousEncoding(encoding, 50);}
0
private void testBatchWithVariousEncoding(String encoding, int n) throws Exception
{    List<JSONEvent> events = putWithEncoding(encoding, n).events;    takeWithEncoding(encoding, n, events);}
0
public String[] getDefaultCipherSuites()
{    return socketFactory.getDefaultCipherSuites();}
0
public String[] getSupportedCipherSuites()
{    return socketFactory.getSupportedCipherSuites();}
0
public Socket createSocket(Socket socket, String s, int i, boolean b) throws IOException
{    SSLSocket sc = (SSLSocket) socketFactory.createSocket(socket, s, i, b);    sc.setEnabledProtocols(protocols);    return sc;}
0
public Socket createSocket(String s, int i) throws IOException, UnknownHostException
{    SSLSocket sc = (SSLSocket) socketFactory.createSocket(s, i);    sc.setEnabledProtocols(protocols);    return sc;}
0
public Socket createSocket(String s, int i, InetAddress inetAddress, int i2) throws IOException, UnknownHostException
{    SSLSocket sc = (SSLSocket) socketFactory.createSocket(s, i, inetAddress, i2);    sc.setEnabledProtocols(protocols);    return sc;}
0
public Socket createSocket(InetAddress inetAddress, int i) throws IOException
{    SSLSocket sc = (SSLSocket) socketFactory.createSocket(inetAddress, i);    sc.setEnabledProtocols(protocols);    return sc;}
0
public Socket createSocket(InetAddress inetAddress, int i, InetAddress inetAddress2, int i2) throws IOException
{    SSLSocket sc = (SSLSocket) socketFactory.createSocket(inetAddress, i, inetAddress2, i2);    sc.setEnabledProtocols(protocols);    return sc;}
0
public void setUp()
{    handler = new JSONHandler();}
0
public void testMultipleEvents() throws Exception
{    String json = "[{\"headers\":{\"a\": \"b\"},\"body\": \"random_body\"}," + "{\"headers\":{\"e\": \"f\"},\"body\": \"random_body2\"}]";    HttpServletRequest req = new FlumeHttpServletRequestWrapper(json);    List<Event> deserialized = handler.getEvents(req);    Event e = deserialized.get(0);    Assert.assertEquals("b", e.getHeaders().get("a"));    Assert.assertEquals("random_body", new String(e.getBody(), "UTF-8"));    e = deserialized.get(1);    Assert.assertEquals("f", e.getHeaders().get("e"));    Assert.assertEquals("random_body2", new String(e.getBody(), "UTF-8"));}
0
public void testMultipleEventsUTF16() throws Exception
{    String json = "[{\"headers\":{\"a\": \"b\"},\"body\": \"random_body\"}," + "{\"headers\":{\"e\": \"f\"},\"body\": \"random_body2\"}]";    HttpServletRequest req = new FlumeHttpServletRequestWrapper(json, "UTF-16");    List<Event> deserialized = handler.getEvents(req);    Event e = deserialized.get(0);    Assert.assertEquals("b", e.getHeaders().get("a"));    Assert.assertEquals("random_body", new String(e.getBody(), "UTF-16"));    e = deserialized.get(1);    Assert.assertEquals("f", e.getHeaders().get("e"));    Assert.assertEquals("random_body2", new String(e.getBody(), "UTF-16"));}
0
public void testMultipleEventsUTF32() throws Exception
{    String json = "[{\"headers\":{\"a\": \"b\"},\"body\": \"random_body\"}," + "{\"headers\":{\"e\": \"f\"},\"body\": \"random_body2\"}]";    HttpServletRequest req = new FlumeHttpServletRequestWrapper(json, "UTF-32");    List<Event> deserialized = handler.getEvents(req);    Event e = deserialized.get(0);    Assert.assertEquals("b", e.getHeaders().get("a"));    Assert.assertEquals("random_body", new String(e.getBody(), "UTF-32"));    e = deserialized.get(1);    Assert.assertEquals("f", e.getHeaders().get("e"));    Assert.assertEquals("random_body2", new String(e.getBody(), "UTF-32"));}
0
public void testMultipleEventsUTF8() throws Exception
{    String json = "[{\"headers\":{\"a\": \"b\"},\"body\": \"random_body\"}," + "{\"headers\":{\"e\": \"f\"},\"body\": \"random_body2\"}]";    HttpServletRequest req = new FlumeHttpServletRequestWrapper(json, "UTF-8");    List<Event> deserialized = handler.getEvents(req);    Event e = deserialized.get(0);    Assert.assertEquals("b", e.getHeaders().get("a"));    Assert.assertEquals("random_body", new String(e.getBody(), "UTF-8"));    e = deserialized.get(1);    Assert.assertEquals("f", e.getHeaders().get("e"));    Assert.assertEquals("random_body2", new String(e.getBody(), "UTF-8"));}
0
public void testEscapedJSON() throws Exception
{        String json = "[{\"headers\":{\"a\": \"b\"}}," + "{\"headers\":{\"e\": \"f\"},\"body\": \"rand\\\"om_body2\"}]";    HttpServletRequest req = new FlumeHttpServletRequestWrapper(json);    List<Event> deserialized = handler.getEvents(req);    Event e = deserialized.get(0);    Assert.assertEquals("b", e.getHeaders().get("a"));    Assert.assertTrue(e.getBody().length == 0);    e = deserialized.get(1);    Assert.assertEquals("f", e.getHeaders().get("e"));    Assert.assertEquals("rand\"om_body2", new String(e.getBody(), "UTF-8"));}
0
public void testNoBody() throws Exception
{    String json = "[{\"headers\" : {\"a\": \"b\"}}," + "{\"headers\" : {\"e\": \"f\"},\"body\": \"random_body2\"}]";    HttpServletRequest req = new FlumeHttpServletRequestWrapper(json);    List<Event> deserialized = handler.getEvents(req);    Event e = deserialized.get(0);    Assert.assertEquals("b", e.getHeaders().get("a"));    Assert.assertTrue(e.getBody().length == 0);    e = deserialized.get(1);    Assert.assertEquals("f", e.getHeaders().get("e"));    Assert.assertEquals("random_body2", new String(e.getBody(), "UTF-8"));}
0
public void testSingleHTMLEvent() throws Exception
{    String json = "[{\"headers\": {\"a\": \"b\"}," + "\"body\": \"<html><body>test</body></html>\"}]";    HttpServletRequest req = new FlumeHttpServletRequestWrapper(json);    List<Event> deserialized = handler.getEvents(req);    Event e = deserialized.get(0);    Assert.assertEquals("b", e.getHeaders().get("a"));    Assert.assertEquals("<html><body>test</body></html>", new String(e.getBody(), "UTF-8"));}
0
public void testSingleEvent() throws Exception
{    String json = "[{\"headers\" : {\"a\": \"b\"},\"body\": \"random_body\"}]";    HttpServletRequest req = new FlumeHttpServletRequestWrapper(json);    List<Event> deserialized = handler.getEvents(req);    Event e = deserialized.get(0);    Assert.assertEquals("b", e.getHeaders().get("a"));    Assert.assertEquals("random_body", new String(e.getBody(), "UTF-8"));}
0
public void testBadEvent() throws Exception
{    String json = "{[\"a\": \"b\"],\"body\": \"random_body\"}";    HttpServletRequest req = new FlumeHttpServletRequestWrapper(json);    handler.getEvents(req);    Assert.fail();}
0
public void testError() throws Exception
{    String json = "[{\"headers\" : {\"a\": \"b\"},\"body\": \"random_body\"}]";    HttpServletRequest req = new FlumeHttpServletRequestWrapper(json, "ISO-8859-1");    handler.getEvents(req);    Assert.fail();}
0
public void testSingleEventInArray() throws Exception
{    String json = "[{\"headers\": {\"a\": \"b\"},\"body\": \"random_body\"}]";    HttpServletRequest req = new FlumeHttpServletRequestWrapper(json);    List<Event> deserialized = handler.getEvents(req);    Event e = deserialized.get(0);    Assert.assertEquals("b", e.getHeaders().get("a"));    Assert.assertEquals("random_body", new String(e.getBody(), "UTF-8"));}
0
public void testMultipleLargeEvents() throws Exception
{    String json = "[{\"headers\" : {\"a\": \"b\", \"a2\": \"b2\"," + "\"a3\": \"b3\",\"a4\": \"b4\"},\"body\": \"random_body\"}," + "{\"headers\" :{\"e\": \"f\",\"e2\": \"f2\"," + "\"e3\": \"f3\",\"e4\": \"f4\",\"e5\": \"f5\"}," + "\"body\": \"random_body2\"}," + "{\"headers\" :{\"q1\": \"b\",\"q2\": \"b2\",\"q3\": \"b3\",\"q4\": \"b4\"}," + "\"body\": \"random_bodyq\"}]";    HttpServletRequest req = new FlumeHttpServletRequestWrapper(json);    List<Event> deserialized = handler.getEvents(req);    Event e = deserialized.get(0);    Assert.assertNotNull(e);    Assert.assertEquals("b", e.getHeaders().get("a"));    Assert.assertEquals("b2", e.getHeaders().get("a2"));    Assert.assertEquals("b3", e.getHeaders().get("a3"));    Assert.assertEquals("b4", e.getHeaders().get("a4"));    Assert.assertEquals("random_body", new String(e.getBody(), "UTF-8"));    e = deserialized.get(1);    Assert.assertNotNull(e);    Assert.assertEquals("f", e.getHeaders().get("e"));    Assert.assertEquals("f2", e.getHeaders().get("e2"));    Assert.assertEquals("f3", e.getHeaders().get("e3"));    Assert.assertEquals("f4", e.getHeaders().get("e4"));    Assert.assertEquals("f5", e.getHeaders().get("e5"));    Assert.assertEquals("random_body2", new String(e.getBody(), "UTF-8"));    e = deserialized.get(2);    Assert.assertNotNull(e);    Assert.assertEquals("b", e.getHeaders().get("q1"));    Assert.assertEquals("b2", e.getHeaders().get("q2"));    Assert.assertEquals("b3", e.getHeaders().get("q3"));    Assert.assertEquals("b4", e.getHeaders().get("q4"));    Assert.assertEquals("random_bodyq", new String(e.getBody(), "UTF-8"));}
0
public void testDeserializarion() throws Exception
{    Type listType = new TypeToken<List<JSONEvent>>() {    }.getType();    List<JSONEvent> events = Lists.newArrayList();    Random rand = new Random();    for (int i = 1; i < 10; i++) {        Map<String, String> input = Maps.newHashMap();        for (int j = 1; j < 10; j++) {            input.put(String.valueOf(i) + String.valueOf(j), String.valueOf(i));        }        JSONEvent e = new JSONEvent();        e.setBody(String.valueOf(rand.nextGaussian()).getBytes("UTF-8"));        e.setHeaders(input);        events.add(e);    }    Gson gson = new Gson();    List<Event> deserialized = handler.getEvents(new FlumeHttpServletRequestWrapper(gson.toJson(events, listType)));    int i = 0;    for (Event e : deserialized) {        Event current = events.get(i++);        Assert.assertEquals(new String(current.getBody(), "UTF-8"), new String(e.getBody(), "UTF-8"));        Assert.assertEquals(current.getHeaders(), e.getHeaders());    }}
0
public void start()
{}
0
public void stop()
{}
0
public LifecycleState getLifecycleState()
{    return null;}
0
public void setChannelProcessor(ChannelProcessor cp)
{}
0
public void setName(String name)
{    this.name = name;}
0
public String getName()
{    return name;}
0
public ChannelProcessor getChannelProcessor()
{    return null;}
0
public void testSimple()
{    RateLimiter limiter = RateLimiter.create(stopwatch, 5.0);        limiter.acquire();        limiter.acquire();        limiter.acquire();    assertEvents("R0.00", "R0.20", "R0.20");}
0
public void testImmediateTryAcquire()
{    RateLimiter r = RateLimiter.create(1);    assertTrue("Unable to acquire initial permit", r.tryAcquire());    assertFalse("Capable of acquiring secondary permit", r.tryAcquire());}
0
public void testSimpleRateUpdate()
{    RateLimiter limiter = RateLimiter.create(5.0, 5, SECONDS);    assertEquals(5.0, limiter.getRate());    limiter.setRate(10.0);    assertEquals(10.0, limiter.getRate());    try {        limiter.setRate(0.0);        fail();    } catch (IllegalArgumentException expected) {    }    try {        limiter.setRate(-10.0);        fail();    } catch (IllegalArgumentException expected) {    }}
0
public void testAcquireParameterValidation()
{    RateLimiter limiter = RateLimiter.create(999);    try {        limiter.acquire(0);        fail();    } catch (IllegalArgumentException expected) {    }    try {        limiter.acquire(-1);        fail();    } catch (IllegalArgumentException expected) {    }    try {        limiter.tryAcquire(0);        fail();    } catch (IllegalArgumentException expected) {    }    try {        limiter.tryAcquire(-1);        fail();    } catch (IllegalArgumentException expected) {    }    try {        limiter.tryAcquire(0, 1, SECONDS);        fail();    } catch (IllegalArgumentException expected) {    }    try {        limiter.tryAcquire(-1, 1, SECONDS);        fail();    } catch (IllegalArgumentException expected) {    }}
0
public void testSimpleWithWait()
{    RateLimiter limiter = RateLimiter.create(stopwatch, 5.0);        limiter.acquire();        stopwatch.sleepMillis(200);        limiter.acquire();        limiter.acquire();    assertEvents("R0.00", "U0.20", "R0.00", "R0.20");}
0
public void testSimpleAcquireReturnValues()
{    RateLimiter limiter = RateLimiter.create(stopwatch, 5.0);        assertEquals(0.0, limiter.acquire(), EPSILON);        stopwatch.sleepMillis(200);        assertEquals(0.0, limiter.acquire(), EPSILON);        assertEquals(0.2, limiter.acquire(), EPSILON);    assertEvents("R0.00", "U0.20", "R0.00", "R0.20");}
0
public void testSimpleAcquireEarliestAvailableIsInPast()
{    RateLimiter limiter = RateLimiter.create(stopwatch, 5.0);    assertEquals(0.0, limiter.acquire(), EPSILON);    stopwatch.sleepMillis(400);    assertEquals(0.0, limiter.acquire(), EPSILON);    assertEquals(0.0, limiter.acquire(), EPSILON);    assertEquals(0.2, limiter.acquire(), EPSILON);}
0
public void testOneSecondBurst()
{    RateLimiter limiter = RateLimiter.create(stopwatch, 5.0);        stopwatch.sleepMillis(1000);        stopwatch.sleepMillis(1000);        limiter.acquire(1);        limiter.acquire(1);        limiter.acquire(3);        limiter.acquire(1);        limiter.acquire();    assertEvents("U1.00", "U1.00",     "R0.00",     "R0.00",     "R0.00",     "R0.00", "R0.20");}
0
public void testCreateWarmupParameterValidation()
{    RateLimiter.create(1.0, 1, NANOSECONDS);    RateLimiter.create(1.0, 0, NANOSECONDS);    try {        RateLimiter.create(0.0, 1, NANOSECONDS);        fail();    } catch (IllegalArgumentException expected) {    }    try {        RateLimiter.create(1.0, -1, NANOSECONDS);        fail();    } catch (IllegalArgumentException expected) {    }}
0
public void testWarmUp()
{    RateLimiter limiter = RateLimiter.create(stopwatch, 2.0, 4000, MILLISECONDS);    for (int i = 0; i < 8; i++) {                limiter.acquire();    }        stopwatch.sleepMillis(500);        stopwatch.sleepMillis(4000);    for (int i = 0; i < 8; i++) {                limiter.acquire();    }        stopwatch.sleepMillis(500);        stopwatch.sleepMillis(2000);    for (int i = 0; i < 8; i++) {                limiter.acquire();    }    assertEvents(    "R0.00, R1.38, R1.13, R0.88, R0.63, R0.50, R0.50, R0.50",     "U0.50",     "U4.00",     "R0.00, R1.38, R1.13, R0.88, R0.63, R0.50, R0.50, R0.50",     "U0.50",     "U2.00",     "R0.00, R0.50, R0.50, R0.50, R0.50, R0.50, R0.50, R0.50");}
0
public void testWarmUpAndUpdate()
{    RateLimiter limiter = RateLimiter.create(stopwatch, 2.0, 4000, MILLISECONDS);    for (int i = 0; i < 8; i++) {                limiter.acquire();    }        stopwatch.sleepMillis(4500);    for (int i = 0; i < 3; i++) {                        limiter.acquire();    }        limiter.setRate(4.0);        limiter.acquire();    for (int i = 0; i < 4; i++) {                limiter.acquire();    }        stopwatch.sleepMillis(4250);    for (int i = 0; i < 11; i++) {                limiter.acquire();    }        assertEvents(    "R0.00, R1.38, R1.13, R0.88, R0.63, R0.50, R0.50, R0.50",     "U4.50",     "R0.00, R1.38, R1.13",     "R0.88",     "R0.34, R0.28, R0.25, R0.25",     "U4.25",     "R0.00, R0.72, R0.66, R0.59, R0.53, R0.47, R0.41",     "R0.34, R0.28, R0.25, R0.25");}
0
public void testBurstyAndUpdate()
{    RateLimiter rateLimiter = RateLimiter.create(stopwatch, 1.0);        rateLimiter.acquire(1);        rateLimiter.acquire(1);        rateLimiter.setRate(2.0);        rateLimiter.acquire(1);        rateLimiter.acquire(2);        rateLimiter.acquire(4);        rateLimiter.acquire(1);    assertEvents("R0.00", "R1.00", "R1.00", "R0.50", "R1.00", "R2.00");}
0
public void testTryAcquire_noWaitAllowed()
{    RateLimiter limiter = RateLimiter.create(stopwatch, 5.0);    assertTrue(limiter.tryAcquire(0, SECONDS));    assertFalse(limiter.tryAcquire(0, SECONDS));    assertFalse(limiter.tryAcquire(0, SECONDS));    stopwatch.sleepMillis(100);    assertFalse(limiter.tryAcquire(0, SECONDS));}
0
public void testTryAcquire_someWaitAllowed()
{    RateLimiter limiter = RateLimiter.create(stopwatch, 5.0);    assertTrue(limiter.tryAcquire(0, SECONDS));    assertTrue(limiter.tryAcquire(200, MILLISECONDS));    assertFalse(limiter.tryAcquire(100, MILLISECONDS));    stopwatch.sleepMillis(100);    assertTrue(limiter.tryAcquire(100, MILLISECONDS));}
0
public void testTryAcquire_overflow()
{    RateLimiter limiter = RateLimiter.create(stopwatch, 5.0);    assertTrue(limiter.tryAcquire(0, MICROSECONDS));    stopwatch.sleepMillis(100);    assertTrue(limiter.tryAcquire(Long.MAX_VALUE, MICROSECONDS));}
0
public void testTryAcquire_negative()
{    RateLimiter limiter = RateLimiter.create(stopwatch, 5.0);    assertTrue(limiter.tryAcquire(5, 0, SECONDS));    stopwatch.sleepMillis(900);    assertFalse(limiter.tryAcquire(1, Long.MIN_VALUE, SECONDS));    stopwatch.sleepMillis(100);    assertTrue(limiter.tryAcquire(1, -1, SECONDS));}
0
public void testSimpleWeights()
{    RateLimiter rateLimiter = RateLimiter.create(stopwatch, 1.0);        rateLimiter.acquire(1);        rateLimiter.acquire(1);        rateLimiter.acquire(2);        rateLimiter.acquire(4);        rateLimiter.acquire(8);        rateLimiter.acquire(1);    assertEvents("R0.00", "R1.00", "R1.00", "R2.00", "R4.00", "R8.00");}
0
public void testInfinity_Bursty()
{    RateLimiter limiter = RateLimiter.create(stopwatch, Double.POSITIVE_INFINITY);    limiter.acquire(Integer.MAX_VALUE / 4);    limiter.acquire(Integer.MAX_VALUE / 2);    limiter.acquire(Integer.MAX_VALUE);        assertEvents("R0.00", "R0.00", "R0.00");    limiter.setRate(2.0);    limiter.acquire();    limiter.acquire();    limiter.acquire();    limiter.acquire();    limiter.acquire();    assertEvents(    "R0.00", "R0.00",     "R0.00",     "R0.50", "R0.50");    limiter.setRate(Double.POSITIVE_INFINITY);    limiter.acquire();    limiter.acquire();    limiter.acquire();        assertEvents("R0.50", "R0.00", "R0.00");}
0
public void testInfinity_BustyTimeElapsed()
{    RateLimiter limiter = RateLimiter.create(stopwatch, Double.POSITIVE_INFINITY);    stopwatch.instant += 1000000;    limiter.setRate(2.0);    for (int i = 0; i < 5; i++) {        limiter.acquire();    }    assertEvents(    "R0.00", "R0.00",     "R0.00",     "R0.50", "R0.50");}
0
public void testInfinity_WarmUp()
{    RateLimiter limiter = RateLimiter.create(stopwatch, Double.POSITIVE_INFINITY, 10, SECONDS);    limiter.acquire(Integer.MAX_VALUE / 4);    limiter.acquire(Integer.MAX_VALUE / 2);    limiter.acquire(Integer.MAX_VALUE);    assertEvents("R0.00", "R0.00", "R0.00");    limiter.setRate(1.0);    limiter.acquire();    limiter.acquire();    limiter.acquire();    assertEvents("R0.00", "R1.00", "R1.00");    limiter.setRate(Double.POSITIVE_INFINITY);    limiter.acquire();    limiter.acquire();    limiter.acquire();    assertEvents("R1.00", "R0.00", "R0.00");}
0
public void testInfinity_WarmUpTimeElapsed()
{    RateLimiter limiter = RateLimiter.create(stopwatch, Double.POSITIVE_INFINITY, 10, SECONDS);    stopwatch.instant += 1000000;    limiter.setRate(1.0);    for (int i = 0; i < 5; i++) {        limiter.acquire();    }    assertEvents("R0.00", "R1.00", "R1.00", "R1.00", "R1.00");}
0
public void testWeNeverGetABurstMoreThanOneSec()
{    RateLimiter limiter = RateLimiter.create(stopwatch, 1.0);    int[] rates = { 1000, 1, 10, 1000000, 10, 1 };    for (int rate : rates) {        int oneSecWorthOfWork = rate;        stopwatch.sleepMillis(rate * 1000);        limiter.setRate(rate);        long burst = measureTotalTimeMillis(limiter, oneSecWorthOfWork, new Random());                assertTrue(burst <= 1000);        long afterBurst = measureTotalTimeMillis(limiter, oneSecWorthOfWork, new Random());                assertTrue(afterBurst >= 1000);    }}
0
public void testTimeToWarmUpIsHonouredEvenWithWeights()
{    Random random = new Random();    int maxPermits = 10;    double[] qpsToTest = { 4.0, 2.0, 1.0, 0.5, 0.1 };    for (int trial = 0; trial < 100; trial++) {        for (double qps : qpsToTest) {                                                long warmupMillis = (long) ((2 * maxPermits / qps) * 1000.0);            RateLimiter rateLimiter = RateLimiter.create(stopwatch, qps, warmupMillis, MILLISECONDS);            assertEquals(warmupMillis, measureTotalTimeMillis(rateLimiter, maxPermits, random));        }    }}
0
private long measureTotalTimeMillis(RateLimiter rateLimiter, int permits, Random random)
{    long startTime = stopwatch.instant;    while (permits > 0) {        int nextPermitsToAcquire = Math.max(1, random.nextInt(permits));        permits -= nextPermitsToAcquire;        rateLimiter.acquire(nextPermitsToAcquire);    }        rateLimiter.acquire(1);    return NANOSECONDS.toMillis(stopwatch.instant - startTime);}
0
private void assertEvents(String... events)
{    assertEquals(Arrays.toString(events), stopwatch.readEventsAndClear());}
0
public long readMicros()
{    return NANOSECONDS.toMicros(instant);}
0
 void sleepMillis(int millis)
{    sleepMicros("U", MILLISECONDS.toMicros(millis));}
0
 void sleepMicros(String caption, long micros)
{    instant += MICROSECONDS.toNanos(micros);    events.add(caption + String.format("%3.2f", (micros / 1000000.0)));}
0
 void sleepMicrosUninterruptibly(long micros)
{    sleepMicros("R", micros);}
0
 String readEventsAndClear()
{    try {        return events.toString();    } finally {        events.clear();    }}
0
public String toString()
{    return events.toString();}
0
public void setUp()
{    source = spy(new AbstractPollableSource() {        @Override        protected Status doProcess() throws EventDeliveryException {            return Status.BACKOFF;        }        @Override        protected void doConfigure(Context context) throws FlumeException {            throw new FlumeException("dummy");        }        @Override        protected void doStart() throws FlumeException {        }        @Override        protected void doStop() throws FlumeException {        }    });}
0
protected Status doProcess() throws EventDeliveryException
{    return Status.BACKOFF;}
0
protected void doConfigure(Context context) throws FlumeException
{    throw new FlumeException("dummy");}
0
protected void doStart() throws FlumeException
{}
0
protected void doStop() throws FlumeException
{}
0
public void testExceptionStartup() throws Exception
{    source.configure(new Context());}
0
public void testNotStarted() throws Exception
{    source.process();}
0
public void voidBackOffConfig()
{    source = spy(new AbstractPollableSource() {        @Override        protected Status doProcess() throws EventDeliveryException {            return Status.BACKOFF;        }        @Override        protected void doConfigure(Context context) throws FlumeException {        }        @Override        protected void doStart() throws FlumeException {        }        @Override        protected void doStop() throws FlumeException {        }    });    HashMap<String, String> inputConfigs = new HashMap<String, String>();    inputConfigs.put(PollableSourceConstants.BACKOFF_SLEEP_INCREMENT, "42");    inputConfigs.put(PollableSourceConstants.MAX_BACKOFF_SLEEP, "4242");    Context context = new Context(inputConfigs);    source.configure(context);    Assert.assertEquals("BackOffSleepIncrement should equal 42 but it equals " + source.getBackOffSleepIncrement(), 42L, source.getBackOffSleepIncrement());    Assert.assertEquals("BackOffSleepIncrement should equal 42 but it equals " + source.getMaxBackOffSleepInterval(), 4242L, source.getMaxBackOffSleepInterval());}
0
protected Status doProcess() throws EventDeliveryException
{    return Status.BACKOFF;}
0
protected void doConfigure(Context context) throws FlumeException
{}
0
protected void doStart() throws FlumeException
{}
0
protected void doStop() throws FlumeException
{}
0
public void voidBackOffConfigDefaults()
{    source = spy(new AbstractPollableSource() {        @Override        protected Status doProcess() throws EventDeliveryException {            return Status.BACKOFF;        }        @Override        protected void doConfigure(Context context) throws FlumeException {        }        @Override        protected void doStart() throws FlumeException {        }        @Override        protected void doStop() throws FlumeException {        }    });    HashMap<String, String> inputConfigs = new HashMap<String, String>();    Assert.assertEquals("BackOffSleepIncrement should equal " + PollableSourceConstants.DEFAULT_BACKOFF_SLEEP_INCREMENT + " but it equals " + source.getBackOffSleepIncrement(), PollableSourceConstants.DEFAULT_BACKOFF_SLEEP_INCREMENT, source.getBackOffSleepIncrement());    Assert.assertEquals("BackOffSleepIncrement should equal " + PollableSourceConstants.DEFAULT_MAX_BACKOFF_SLEEP + " but it equals " + source.getMaxBackOffSleepInterval(), PollableSourceConstants.DEFAULT_MAX_BACKOFF_SLEEP, source.getMaxBackOffSleepInterval());}
0
protected Status doProcess() throws EventDeliveryException
{    return Status.BACKOFF;}
0
protected void doConfigure(Context context) throws FlumeException
{}
0
protected void doStart() throws FlumeException
{}
0
protected void doStop() throws FlumeException
{}
0
public void setUp() throws UnknownHostException
{    localhost = InetAddress.getByName("127.0.0.1");    source = new AvroSource();    channel = new MemoryChannel();    Configurables.configure(channel, new Context());    List<Channel> channels = new ArrayList<Channel>();    channels.add(channel);    ChannelSelector rcs = new ReplicatingChannelSelector();    rcs.setChannels(channels);    source.setChannelProcessor(new ChannelProcessor(rcs));}
0
public void testLifecycle() throws InterruptedException, IOException
{    Context context = new Context();    context.put("port", String.valueOf(selectedPort = getFreePort()));    context.put("bind", "0.0.0.0");    Configurables.configure(source, context);    source.start();    Assert.assertTrue("Reached start or error", LifecycleController.waitForOneOf(source, LifecycleState.START_OR_ERROR));    Assert.assertEquals("Server is started", LifecycleState.START, source.getLifecycleState());    source.stop();    Assert.assertTrue("Reached stop or error", LifecycleController.waitForOneOf(source, LifecycleState.STOP_OR_ERROR));    Assert.assertEquals("Server is stopped", LifecycleState.STOP, source.getLifecycleState());}
0
public void testSourceStoppedOnFlumeExceptionIfPortUsed() throws InterruptedException, IOException
{    final String loopbackIPv4 = "127.0.0.1";    final int port = 10500;        try (ServerSocketChannel dummyServerSocket = ServerSocketChannel.open()) {        dummyServerSocket.socket().setReuseAddress(true);        dummyServerSocket.socket().bind(new InetSocketAddress(loopbackIPv4, port));        Context context = new Context();        context.put("port", String.valueOf(port));        context.put("bind", loopbackIPv4);        Configurables.configure(source, context);        try {            source.start();            Assert.fail("Expected an exception during startup caused by binding on a used port");        } catch (FlumeException e) {                        Assert.assertTrue("Expected a server socket setup related root cause", e.getMessage().contains("server socket"));        }    }            Assert.assertEquals("Server is stopped", LifecycleState.STOP, source.getLifecycleState());}
1
public void testInvalidAddress() throws InterruptedException, IOException
{    final String invalidHost = "invalid.host";    final int port = 10501;    Context context = new Context();    context.put("port", String.valueOf(port));    context.put("bind", invalidHost);    Configurables.configure(source, context);    try {        source.start();        Assert.fail("Expected an exception during startup caused by binding on a invalid host");    } catch (FlumeException e) {                Assert.assertTrue("Expected a server socket setup related root cause", e.getMessage().contains("server socket"));    }            Assert.assertEquals("Server is stopped", LifecycleState.STOP, source.getLifecycleState());}
1
public void testRequestWithNoCompression() throws InterruptedException, IOException
{    doRequest(false, false, 6);}
0
public void testRequestWithCompressionOnClientAndServerOnLevel0() throws InterruptedException, IOException
{    doRequest(true, true, 0);}
0
public void testRequestWithCompressionOnClientAndServerOnLevel1() throws InterruptedException, IOException
{    doRequest(true, true, 1);}
0
public void testRequestWithCompressionOnClientAndServerOnLevel6() throws InterruptedException, IOException
{    doRequest(true, true, 6);}
0
public void testRequestWithCompressionOnClientAndServerOnLevel9() throws InterruptedException, IOException
{    doRequest(true, true, 9);}
0
public void testRequestWithCompressionOnServerOnly() throws InterruptedException, IOException
{        doRequest(true, false, 6);}
0
public void testRequestWithCompressionOnClientOnly() throws InterruptedException, IOException
{        doRequest(false, true, 6);}
0
private void doRequest(boolean serverEnableCompression, boolean clientEnableCompression, int compressionLevel) throws InterruptedException, IOException
{    Context context = new Context();    context.put("port", String.valueOf(selectedPort = getFreePort()));    context.put("bind", "0.0.0.0");    context.put("threads", "50");    if (serverEnableCompression) {        context.put("compression-type", "deflate");    } else {        context.put("compression-type", "none");    }    Configurables.configure(source, context);    source.start();    Assert.assertTrue("Reached start or error", LifecycleController.waitForOneOf(source, LifecycleState.START_OR_ERROR));    Assert.assertEquals("Server is started", LifecycleState.START, source.getLifecycleState());    AvroSourceProtocol client;    NettyTransceiver nettyTransceiver;    if (clientEnableCompression) {        nettyTransceiver = new NettyTransceiver(new InetSocketAddress(selectedPort), new CompressionChannelFactory(compressionLevel));        client = SpecificRequestor.getClient(AvroSourceProtocol.class, nettyTransceiver);    } else {        nettyTransceiver = new NettyTransceiver(new InetSocketAddress(selectedPort));        client = SpecificRequestor.getClient(AvroSourceProtocol.class, nettyTransceiver);    }    AvroFlumeEvent avroEvent = new AvroFlumeEvent();    avroEvent.setHeaders(new HashMap<CharSequence, CharSequence>());    avroEvent.setBody(ByteBuffer.wrap("Hello avro".getBytes()));    Status status = client.append(avroEvent);    Assert.assertEquals(Status.OK, status);    Transaction transaction = channel.getTransaction();    transaction.begin();    Event event = channel.take();    Assert.assertNotNull(event);    Assert.assertEquals("Channel contained our event", "Hello avro", new String(event.getBody()));    transaction.commit();    transaction.close();        nettyTransceiver.close();    source.stop();    Assert.assertTrue("Reached stop or error", LifecycleController.waitForOneOf(source, LifecycleState.STOP_OR_ERROR));    Assert.assertEquals("Server is stopped", LifecycleState.STOP, source.getLifecycleState());}
1
private static int getFreePort() throws IOException
{    try (ServerSocket socket = new ServerSocket(0)) {        return socket.getLocalPort();    }}
0
public SocketChannel newChannel(ChannelPipeline pipeline)
{    try {        ZlibEncoder encoder = new ZlibEncoder(compressionLevel);        pipeline.addFirst("deflater", encoder);        pipeline.addFirst("inflater", new ZlibDecoder());        return super.newChannel(pipeline);    } catch (Exception ex) {        throw new RuntimeException("Cannot create Compression channel", ex);    }}
0
public void testSslRequestWithComponentKeystore() throws InterruptedException, IOException
{    Context context = new Context();    context.put("port", String.valueOf(selectedPort = getFreePort()));    context.put("bind", "0.0.0.0");    context.put("ssl", "true");    context.put("keystore", "src/test/resources/server.p12");    context.put("keystore-password", "password");    context.put("keystore-type", "PKCS12");    Configurables.configure(source, context);    doSslRequest();}
0
public void testSslRequestWithGlobalKeystore() throws InterruptedException, IOException
{    System.setProperty("javax.net.ssl.keyStore", "src/test/resources/server.p12");    System.setProperty("javax.net.ssl.keyStorePassword", "password");    System.setProperty("javax.net.ssl.keyStoreType", "PKCS12");    Context context = new Context();    context.put("port", String.valueOf(selectedPort = getFreePort()));    context.put("bind", "0.0.0.0");    context.put("ssl", "true");    Configurables.configure(source, context);    doSslRequest();    System.clearProperty("javax.net.ssl.keyStore");    System.clearProperty("javax.net.ssl.keyStorePassword");}
0
private void doSslRequest() throws InterruptedException, IOException
{    source.start();    Assert.assertTrue("Reached start or error", LifecycleController.waitForOneOf(source, LifecycleState.START_OR_ERROR));    Assert.assertEquals("Server is started", LifecycleState.START, source.getLifecycleState());    AvroSourceProtocol client = SpecificRequestor.getClient(AvroSourceProtocol.class, new NettyTransceiver(new InetSocketAddress(selectedPort), new SSLChannelFactory()));    AvroFlumeEvent avroEvent = new AvroFlumeEvent();    avroEvent.setHeaders(new HashMap<CharSequence, CharSequence>());    avroEvent.setBody(ByteBuffer.wrap("Hello avro ssl".getBytes()));    Status status = client.append(avroEvent);    Assert.assertEquals(Status.OK, status);    Transaction transaction = channel.getTransaction();    transaction.begin();    Event event = channel.take();    Assert.assertNotNull(event);    Assert.assertEquals("Channel contained our event", "Hello avro ssl", new String(event.getBody()));    transaction.commit();    transaction.close();        source.stop();    Assert.assertTrue("Reached stop or error", LifecycleController.waitForOneOf(source, LifecycleState.STOP_OR_ERROR));    Assert.assertEquals("Server is stopped", LifecycleState.STOP, source.getLifecycleState());}
1
public SocketChannel newChannel(ChannelPipeline pipeline)
{    try {        SSLContext sslContext = SSLContext.getInstance("TLS");        sslContext.init(null, new TrustManager[] { new PermissiveTrustManager() }, null);        SSLEngine sslEngine = sslContext.createSSLEngine();        sslEngine.setUseClientMode(true);                        pipeline.addFirst("ssl", new SslHandler(sslEngine));        return super.newChannel(pipeline);    } catch (Exception ex) {        throw new RuntimeException("Cannot create SSL channel", ex);    }}
0
public void checkClientTrusted(X509Certificate[] certs, String s)
{}
0
public void checkServerTrusted(X509Certificate[] certs, String s)
{}
0
public X509Certificate[] getAcceptedIssuers()
{    return new X509Certificate[0];}
0
public void testValidIpFilterAllows() throws InterruptedException, IOException
{    doIpFilterTest(localhost, "allow:name:localhost,deny:ip:*", true, false);    doIpFilterTest(localhost, "allow:ip:" + localhost.getHostAddress() + ",deny:ip:*", true, false);    doIpFilterTest(localhost, "allow:ip:*", true, false);    doIpFilterTest(localhost, "allow:ip:" + localhost.getHostAddress().substring(0, 3) + "*,deny:ip:*", true, false);    doIpFilterTest(localhost, "allow:ip:127.0.0.2,allow:ip:" + localhost.getHostAddress().substring(0, 3) + "*,deny:ip:*", true, false);    doIpFilterTest(localhost, "allow:name:localhost,deny:ip:*", true, true);    doIpFilterTest(localhost, "allow:ip:*", true, true);}
0
public void testValidIpFilterDenys() throws InterruptedException, IOException
{    doIpFilterTest(localhost, "deny:ip:*", false, false);    doIpFilterTest(localhost, "deny:name:localhost", false, false);    doIpFilterTest(localhost, "deny:ip:" + localhost.getHostAddress() + ",allow:ip:*", false, false);    doIpFilterTest(localhost, "deny:ip:*", false, false);    doIpFilterTest(localhost, "allow:ip:45.2.2.2,deny:ip:*", false, false);    doIpFilterTest(localhost, "deny:ip:" + localhost.getHostAddress().substring(0, 3) + "*,allow:ip:*", false, false);    doIpFilterTest(localhost, "deny:ip:*", false, true);}
0
public void testInvalidIpFilter() throws InterruptedException, IOException
{    doIpFilterTest(localhost, "deny:ip:*", false, false);    doIpFilterTest(localhost, "allow:name:localhost", true, false);    doIpFilterTest(localhost, "deny:ip:127.0.0.2,allow:ip:*,deny:ip:" + localhost.getHostAddress(), true, false);    doIpFilterTest(localhost, "deny:ip:" + localhost.getHostAddress().substring(0, 3) + "*,allow:ip:*", false, false);        Consumer<Exception> exceptionChecker = (Exception ex) -> {                        Assert.assertTrue("Expected an ipFilterRules related exception", ex.getMessage().contains("ipFilter"));    };    try {        doIpFilterTest(localhost, null, false, false);        Assert.fail("The null ipFilterRules config should have thrown an exception.");    } catch (FlumeException e) {        exceptionChecker.accept(e);    }    try {        doIpFilterTest(localhost, "", true, false);        Assert.fail("The empty string ipFilterRules config should have thrown " + "an exception");    } catch (FlumeException e) {        exceptionChecker.accept(e);    }    try {        doIpFilterTest(localhost, "homer:ip:45.4.23.1", true, false);        Assert.fail("Bad ipFilterRules config should have thrown an exception.");    } catch (FlumeException e) {        exceptionChecker.accept(e);    }    try {        doIpFilterTest(localhost, "allow:sleeps:45.4.23.1", true, false);        Assert.fail("Bad ipFilterRules config should have thrown an exception.");    } catch (FlumeException e) {        exceptionChecker.accept(e);    }}
1
public void doIpFilterTest(InetAddress dest, String ruleDefinition, boolean eventShouldBeAllowed, boolean testWithSSL) throws InterruptedException, IOException
{    Context context = new Context();    context.put("port", String.valueOf(selectedPort = getFreePort()));    context.put("bind", "0.0.0.0");    context.put("ipFilter", "true");    if (ruleDefinition != null) {        context.put("ipFilterRules", ruleDefinition);    }    if (testWithSSL) {                context.put("ssl", "true");        context.put("keystore", "src/test/resources/server.p12");        context.put("keystore-password", "password");        context.put("keystore-type", "PKCS12");    }        Configurables.configure(source, context);    source.start();    Assert.assertTrue("Reached start or error", LifecycleController.waitForOneOf(source, LifecycleState.START_OR_ERROR));    Assert.assertEquals("Server is started", LifecycleState.START, source.getLifecycleState());    AvroSourceProtocol client;    NettyTransceiver nettyTransceiver = null;    try {        if (testWithSSL) {            nettyTransceiver = new NettyTransceiver(new InetSocketAddress(dest, selectedPort), new SSLChannelFactory());            client = SpecificRequestor.getClient(AvroSourceProtocol.class, nettyTransceiver);        } else {            nettyTransceiver = new NettyTransceiver(new InetSocketAddress(dest, selectedPort));            client = SpecificRequestor.getClient(AvroSourceProtocol.class, nettyTransceiver);        }        AvroFlumeEvent avroEvent = new AvroFlumeEvent();        avroEvent.setHeaders(new HashMap<CharSequence, CharSequence>());        avroEvent.setBody(ByteBuffer.wrap("Hello avro ipFilter".getBytes()));                Status status = client.append(avroEvent);                Assert.assertEquals(Status.OK, status);    } catch (IOException e) {        Assert.assertTrue("Should have been allowed: " + ruleDefinition, !eventShouldBeAllowed);        return;    } finally {        if (nettyTransceiver != null) {            nettyTransceiver.close();        }        source.stop();    }    Assert.assertTrue("Should have been denied: " + ruleDefinition, eventShouldBeAllowed);    Transaction transaction = channel.getTransaction();    transaction.begin();    Event event = channel.take();    Assert.assertNotNull(event);    Assert.assertEquals("Channel contained our event", "Hello avro ipFilter", new String(event.getBody()));    transaction.commit();    transaction.close();        Assert.assertTrue("Reached stop or error", LifecycleController.waitForOneOf(source, LifecycleState.STOP_OR_ERROR));    Assert.assertEquals("Server is stopped", LifecycleState.STOP, source.getLifecycleState());}
1
public void testErrorCounterChannelWriteFail() throws Exception
{    Context context = new Context();    context.put("port", String.valueOf(selectedPort = getFreePort()));    context.put("bind", "0.0.0.0");    source.configure(context);    ChannelProcessor cp = Mockito.mock(ChannelProcessor.class);    doThrow(new ChannelException("dummy")).when(cp).processEvent(any(Event.class));    doThrow(new ChannelException("dummy")).when(cp).processEventBatch(anyListOf(Event.class));    source.setChannelProcessor(cp);    source.start();    AvroFlumeEvent avroEvent = new AvroFlumeEvent();    avroEvent.setHeaders(new HashMap<CharSequence, CharSequence>());    avroEvent.setBody(ByteBuffer.wrap("Hello avro ssl".getBytes()));    source.append(avroEvent);    source.appendBatch(Arrays.asList(avroEvent));    SourceCounter sc = (SourceCounter) Whitebox.getInternalState(source, "sourceCounter");    Assert.assertEquals(2, sc.getChannelWriteFail());    source.stop();}
0
public void setUp()
{    context = new Context();    channelProcessor = mock(ChannelProcessor.class);}
0
public DoNothingSource spyAndConfigure(DoNothingSource source)
{    source = spy(source);    source.setChannelProcessor(channelProcessor);    source.configure(context);    return source;}
0
public void testDoConfigureThrowsException() throws Exception
{    source = spy(new DoNothingSource() {        @Override        protected void doConfigure(Context context) throws FlumeException {            throw new FlumeException("dummy");        }    });    source.setChannelProcessor(channelProcessor);    try {        source.configure(context);        Assert.fail();    } catch (FlumeException expected) {    }    Assert.assertFalse(source.isStarted());    Assert.assertEquals(LifecycleState.ERROR, source.getLifecycleState());    Assert.assertNotNull(source.getStartException());}
0
protected void doConfigure(Context context) throws FlumeException
{    throw new FlumeException("dummy");}
0
public void testDoStartThrowsException() throws Exception
{    source = spyAndConfigure(new DoNothingSource() {        @Override        protected void doStart() throws FlumeException {            throw new FlumeException("dummy");        }    });    source.start();    Assert.assertFalse(source.isStarted());    Assert.assertEquals(LifecycleState.ERROR, source.getLifecycleState());    Assert.assertNotNull(source.getStartException());}
0
protected void doStart() throws FlumeException
{    throw new FlumeException("dummy");}
0
public void testDoStopThrowsException() throws Exception
{    source = spyAndConfigure(new DoNothingSource() {        @Override        protected void doStop() throws FlumeException {            throw new FlumeException("dummy");        }    });    source.start();    source.stop();    Assert.assertFalse(source.isStarted());    Assert.assertEquals(LifecycleState.ERROR, source.getLifecycleState());    Assert.assertNull(source.getStartException());}
0
protected void doStop() throws FlumeException
{    throw new FlumeException("dummy");}
0
public void testConfigureCalledWhenStarted() throws Exception
{    source = spyAndConfigure(new DoNothingSource());    source.start();    try {        source.configure(context);        Assert.fail();    } catch (IllegalStateException expected) {    }    Assert.assertTrue(source.isStarted());    Assert.assertNull(source.getStartException());}
0
protected void doConfigure(Context context) throws FlumeException
{}
0
protected void doStart() throws FlumeException
{}
0
protected void doStop() throws FlumeException
{}
0
public void setUp()
{    sourceFactory = new DefaultSourceFactory();}
0
public void testDuplicateCreate()
{    Source avroSource1 = sourceFactory.create("avroSource1", "avro");    Source avroSource2 = sourceFactory.create("avroSource2", "avro");    Assert.assertNotNull(avroSource1);    Assert.assertNotNull(avroSource2);    Assert.assertNotSame(avroSource1, avroSource2);    Assert.assertTrue(avroSource1 instanceof AvroSource);    Assert.assertTrue(avroSource2 instanceof AvroSource);    Source s1 = sourceFactory.create("avroSource1", "avro");    Source s2 = sourceFactory.create("avroSource2", "avro");    Assert.assertNotSame(avroSource1, s1);    Assert.assertNotSame(avroSource2, s2);}
0
private void verifySourceCreation(String name, String type, Class<?> typeClass) throws Exception
{    Source src = sourceFactory.create(name, type);    Assert.assertNotNull(src);    Assert.assertTrue(typeClass.isInstance(src));}
0
public void testSourceCreation() throws Exception
{    verifySourceCreation("seq-src", "seq", SequenceGeneratorSource.class);    verifySourceCreation("netcat-src", "netcat", NetcatSource.class);    verifySourceCreation("netcat-udp-src", "netcatudp", NetcatUdpSource.class);    verifySourceCreation("exec-src", "exec", ExecSource.class);    verifySourceCreation("avro-src", "avro", AvroSource.class);    verifySourceCreation("syslogtcp-src", "syslogtcp", SyslogTcpSource.class);    verifySourceCreation("multiport_syslogtcp-src", "multiport_syslogtcp", MultiportSyslogTCPSource.class);    verifySourceCreation("syslogudp-src", "syslogudp", SyslogUDPSource.class);    verifySourceCreation("spooldir-src", "spooldir", SpoolDirectorySource.class);    verifySourceCreation("http-src", "http", HTTPSource.class);    verifySourceCreation("thrift-src", "thrift", ThriftSource.class);    verifySourceCreation("custom-src", MockSource.class.getCanonicalName(), MockSource.class);}
0
public void setUp()
{    context.put("keep-alive", "1");    context.put("capacity", "1000");    context.put("transactionCapacity", "1000");    Configurables.configure(channel, context);    rcs.setChannels(Lists.newArrayList(channel));    source = new ExecSource();    source.setChannelProcessor(new ChannelProcessor(rcs));}
0
public void tearDown()
{    source.stop();        ObjectName objName = null;    try {        objName = new ObjectName("org.apache.flume.source" + ":type=" + source.getName());        ManagementFactory.getPlatformMBeanServer().unregisterMBean(objName);    } catch (Exception ex) {        System.out.println("Failed to unregister the monitored counter: " + objName + ex.getMessage());    }}
0
public void testProcess() throws InterruptedException, LifecycleException, EventDeliveryException, IOException
{        File inputFile = File.createTempFile("input", null);    File ouputFile = File.createTempFile("ouput", null);    FileUtils.forceDeleteOnExit(inputFile);    FileUtils.forceDeleteOnExit(ouputFile);        FileOutputStream outputStream1 = new FileOutputStream(inputFile);    for (int i = 0; i < 10; i++) {        outputStream1.write(RandomStringUtils.randomAlphanumeric(200).getBytes());        outputStream1.write('\n');    }    outputStream1.close();    String command = SystemUtils.IS_OS_WINDOWS ? String.format("cmd /c type %s", inputFile.getAbsolutePath()) : String.format("cat %s", inputFile.getAbsolutePath());    context.put("command", command);    context.put("keep-alive", "1");    context.put("capacity", "1000");    context.put("transactionCapacity", "1000");    Configurables.configure(source, context);    source.start();    Thread.sleep(2000);    Transaction transaction = channel.getTransaction();    transaction.begin();    Event event;    FileOutputStream outputStream = new FileOutputStream(ouputFile);    while ((event = channel.take()) != null) {        outputStream.write(event.getBody());        outputStream.write('\n');    }    outputStream.close();    transaction.commit();    transaction.close();    Assert.assertEquals(FileUtils.checksumCRC32(inputFile), FileUtils.checksumCRC32(ouputFile));}
0
public void testShellCommandSimple() throws InterruptedException, LifecycleException, EventDeliveryException, IOException
{    if (SystemUtils.IS_OS_WINDOWS) {        runTestShellCmdHelper("powershell -ExecutionPolicy Unrestricted -command", "1..5", new String[] { "1", "2", "3", "4", "5" });    } else {        runTestShellCmdHelper("/bin/bash -c", "seq 5", new String[] { "1", "2", "3", "4", "5" });    }}
0
public void testShellCommandBackTicks() throws InterruptedException, LifecycleException, EventDeliveryException, IOException
{        if (SystemUtils.IS_OS_WINDOWS) {        runTestShellCmdHelper("powershell -ExecutionPolicy Unrestricted -command", "$(1..5)", new String[] { "1", "2", "3", "4", "5" });    } else {        runTestShellCmdHelper("/bin/bash -c", "echo `seq 5`", new String[] { "1 2 3 4 5" });        runTestShellCmdHelper("/bin/bash -c", "echo $(seq 5)", new String[] { "1 2 3 4 5" });    }}
0
public void testShellCommandComplex() throws InterruptedException, LifecycleException, EventDeliveryException, IOException
{        String[] expected = { "1234", "abcd", "ijk", "xyz", "zzz" };        if (SystemUtils.IS_OS_WINDOWS) {        runTestShellCmdHelper("powershell -ExecutionPolicy Unrestricted -command", "'zzz','1234','xyz','abcd','ijk' | sort", expected);    } else {        runTestShellCmdHelper("/bin/bash -c", "echo zzz 1234 xyz abcd ijk | xargs -n1 echo | sort -f", expected);    }}
0
public void testShellCommandScript() throws InterruptedException, LifecycleException, EventDeliveryException, IOException
{        if (SystemUtils.IS_OS_WINDOWS) {        runTestShellCmdHelper("powershell -ExecutionPolicy Unrestricted -command", "foreach ($i in 1..5) { $i }", new String[] { "1", "2", "3", "4", "5" });                runTestShellCmdHelper("powershell -ExecutionPolicy Unrestricted -command", "if(2+2 -gt 3) { 'good' } else { 'not good' } ", new String[] { "good" });    } else {        runTestShellCmdHelper("/bin/bash -c", "for i in {1..5}; do echo $i;done", new String[] { "1", "2", "3", "4", "5" });                runTestShellCmdHelper("/bin/bash -c", "if ((2+2>3)); " + "then  echo good; else echo not good; fi", new String[] { "good" });    }}
0
public void testShellCommandEmbeddingAndEscaping() throws InterruptedException, LifecycleException, EventDeliveryException, IOException
{        String fileName = SystemUtils.IS_OS_WINDOWS ? "src\\test\\resources\\test_command.ps1" : "src/test/resources/test_command.txt";    BufferedReader reader = new BufferedReader(new FileReader(fileName));    try {        String shell = SystemUtils.IS_OS_WINDOWS ? "powershell -ExecutionPolicy Unrestricted -command" : "/bin/bash -c";        String command1 = reader.readLine();        Assert.assertNotNull(command1);        String[] output1 = new String[] { "'1'", "\"2\"", "\\3", "\\4" };        runTestShellCmdHelper(shell, command1, output1);        String command2 = reader.readLine();        Assert.assertNotNull(command2);        String[] output2 = new String[] { "1", "2", "3", "4", "5" };        runTestShellCmdHelper(shell, command2, output2);        String command3 = reader.readLine();        Assert.assertNotNull(command3);        String[] output3 = new String[] { "2", "3", "4", "5", "6" };        runTestShellCmdHelper(shell, command3, output3);    } finally {        reader.close();    }}
0
public void testMonitoredCounterGroup() throws InterruptedException, LifecycleException, EventDeliveryException, IOException
{        if (SystemUtils.IS_OS_WINDOWS) {        runTestShellCmdHelper("powershell -ExecutionPolicy Unrestricted -command", "foreach ($i in 1..5) { $i }", new String[] { "1", "2", "3", "4", "5" });    } else {        runTestShellCmdHelper("/bin/bash -c", "for i in {1..5}; do echo $i;done", new String[] { "1", "2", "3", "4", "5" });    }    ObjectName objName = null;    try {        objName = new ObjectName("org.apache.flume.source" + ":type=" + source.getName());        MBeanServer mbeanServer = ManagementFactory.getPlatformMBeanServer();        String[] strAtts = { "Type", "EventReceivedCount", "EventAcceptedCount" };        AttributeList attrList = mbeanServer.getAttributes(objName, strAtts);        Assert.assertNotNull(attrList.get(0));        Assert.assertEquals("Expected Value: Type", "Type", ((Attribute) attrList.get(0)).getName());        Assert.assertEquals("Expected Value: SOURCE", "SOURCE", ((Attribute) attrList.get(0)).getValue());        Assert.assertNotNull(attrList.get(1));        Assert.assertEquals("Expected Value: EventReceivedCount", "EventReceivedCount", ((Attribute) attrList.get(1)).getName());        Assert.assertEquals("Expected Value: 5", "5", ((Attribute) attrList.get(1)).getValue().toString());        Assert.assertNotNull(attrList.get(2));        Assert.assertEquals("Expected Value: EventAcceptedCount", "EventAcceptedCount", ((Attribute) attrList.get(2)).getName());        Assert.assertEquals("Expected Value: 5", "5", ((Attribute) attrList.get(2)).getValue().toString());    } catch (Exception ex) {        System.out.println("Unable to retreive the monitored counter: " + objName + ex.getMessage());    }}
0
public void testBatchTimeout() throws InterruptedException, LifecycleException, EventDeliveryException, IOException
{    String filePath = "/tmp/flume-execsource." + Thread.currentThread().getId();    String eventBody = "TestMessage";    FileOutputStream outputStream = new FileOutputStream(filePath);    context.put(ExecSourceConfigurationConstants.CONFIG_BATCH_SIZE, "50000");    context.put(ExecSourceConfigurationConstants.CONFIG_BATCH_TIME_OUT, "750");    context.put("shell", SystemUtils.IS_OS_WINDOWS ? "powershell -ExecutionPolicy Unrestricted -command" : "/bin/bash -c");    context.put("command", SystemUtils.IS_OS_WINDOWS ? "Get-Content " + filePath + " | Select-Object -Last 10" : ("tail -f " + filePath));    Configurables.configure(source, context);    source.start();    Transaction transaction = channel.getTransaction();    transaction.begin();    for (int lineNumber = 0; lineNumber < 3; lineNumber++) {        outputStream.write((eventBody).getBytes());        outputStream.write(String.valueOf(lineNumber).getBytes());        outputStream.write('\n');        outputStream.flush();    }    outputStream.close();    Thread.sleep(1500);    for (int i = 0; i < 3; i++) {        Event event = channel.take();        assertNotNull(event);        assertNotNull(event.getBody());        assertEquals(eventBody + String.valueOf(i), new String(event.getBody()));    }    transaction.commit();    transaction.close();    source.stop();    File file = new File(filePath);    FileUtils.forceDelete(file);}
0
private void runTestShellCmdHelper(String shell, String command, String[] expectedOutput) throws InterruptedException, LifecycleException, EventDeliveryException, IOException
{    context.put("shell", shell);    context.put("command", command);    Configurables.configure(source, context);    source.start();            Thread.sleep(2500);    Transaction transaction = channel.getTransaction();    transaction.begin();    try {        List<String> output = Lists.newArrayList();        Event event;        while ((event = channel.take()) != null) {            output.add(new String(event.getBody(), Charset.defaultCharset()));        }        transaction.commit();        Assert.assertArrayEquals(expectedOutput, output.toArray(new String[] {}));    } finally {        transaction.close();        source.stop();    }}
0
public void testRestart() throws InterruptedException, LifecycleException, EventDeliveryException, IOException
{    context.put(ExecSourceConfigurationConstants.CONFIG_RESTART_THROTTLE, "10");    context.put(ExecSourceConfigurationConstants.CONFIG_RESTART, "true");    context.put("command", SystemUtils.IS_OS_WINDOWS ? "cmd /c echo flume" : "echo flume");    Configurables.configure(source, context);    source.start();    Transaction transaction = channel.getTransaction();    transaction.begin();    long start = System.currentTimeMillis();    for (int i = 0; i < 5; i++) {        Event event = channel.take();        assertNotNull(event);        assertNotNull(event.getBody());        assertEquals("flume", new String(event.getBody(), Charsets.UTF_8));    }        assertTrue(System.currentTimeMillis() - start < 10000L);    transaction.commit();    transaction.close();    source.stop();}
0
public void testShutdown() throws Exception
{        int seconds = 272;        boolean searchForCommand = true;    while (searchForCommand) {        searchForCommand = false;        String command = SystemUtils.IS_OS_WINDOWS ? "cmd /c sleep " + seconds : "sleep " + seconds;        String searchTxt = SystemUtils.IS_OS_WINDOWS ? "sleep.exe" : "\b" + command + "\b";        Pattern pattern = Pattern.compile(searchTxt);        for (String line : exec(SystemUtils.IS_OS_WINDOWS ? "cmd /c tasklist /FI \"SESSIONNAME eq Console\"" : "ps -ef")) {            if (pattern.matcher(line).find()) {                seconds++;                searchForCommand = true;                break;            }        }    }            String command = "sleep " + seconds;    Pattern pattern = Pattern.compile("\b" + command + "\b");    context.put(ExecSourceConfigurationConstants.CONFIG_RESTART, "false");    context.put("command", command);    Configurables.configure(source, context);    source.start();    Thread.sleep(1000L);    source.stop();    Thread.sleep(1000L);    for (String line : exec(SystemUtils.IS_OS_WINDOWS ? "cmd /c tasklist /FI \"SESSIONNAME eq Console\"" : "ps -ef")) {        if (pattern.matcher(line).find()) {            Assert.fail("Found [" + line + "]");        }    }}
0
private static List<String> exec(String command) throws Exception
{    String[] commandArgs = command.split("\\s+");    Process process = new ProcessBuilder(commandArgs).start();    BufferedReader reader = null;    try {        reader = new BufferedReader(new InputStreamReader(process.getInputStream()));        List<String> result = Lists.newArrayList();        String line;        while ((line = reader.readLine()) != null) {            result.add(line);        }        return result;    } finally {        process.destroy();        if (reader != null) {            reader.close();        }        int exit = process.waitFor();        if (exit != 0) {            throw new IllegalStateException("Command [" + command + "] exited with " + exit);        }    }}
0
private static final int getFreePort() throws IOException
{    try (ServerSocket socket = new ServerSocket(0)) {        return socket.getLocalPort();    }}
0
private byte[] getEvent(int counter)
{        String msg1 = "<10>" + stamp1 + " " + host1 + " " + data1 + " " + String.valueOf(counter) + "\n";    return msg1.getBytes();}
0
private List<Integer> testNPorts(MultiportSyslogTCPSource source, Channel channel, List<Event> channelEvents, int numPorts, ChannelProcessor channelProcessor, BiConsumer<Integer, byte[]> eventSenderFuncton, Context additionalContext) throws IOException
{    Context channelContext = new Context();    channelContext.put("capacity", String.valueOf(2000));    channelContext.put("transactionCapacity", String.valueOf(2000));    Configurables.configure(channel, channelContext);    if (channelProcessor == null) {        List<Channel> channels = Lists.newArrayList();        channels.add(channel);        ChannelSelector rcs = new ReplicatingChannelSelector();        rcs.setChannels(channels);        source.setChannelProcessor(new ChannelProcessor(rcs));    } else {        source.setChannelProcessor(channelProcessor);    }    List<Integer> portList = new ArrayList<>(numPorts);    while (portList.size() < numPorts) {        int port = getFreePort();        if (!portList.contains(port)) {            portList.add(port);        }    }    StringBuilder ports = new StringBuilder();    for (int i = 0; i < numPorts; i++) {        ports.append(String.valueOf(portList.get(i))).append(" ");    }    Context context = new Context();    context.put(SyslogSourceConfigurationConstants.CONFIG_PORTS, ports.toString().trim());    context.put("portHeader", "port");    context.putAll(additionalContext.getParameters());    source.configure(context);    source.start();    for (int i = 0; i < numPorts; i++) {        eventSenderFuncton.accept(portList.get(i), getEvent(i));    }    Transaction txn = channel.getTransaction();    txn.begin();    for (int i = 0; i < numPorts; i++) {        Event e = channel.take();        if (e == null) {            throw new NullPointerException("Event is null");        }        channelEvents.add(e);    }    try {        txn.commit();    } catch (Throwable t) {        txn.rollback();    } finally {        txn.close();    }    return portList;}
0
public void testMultiplePorts() throws IOException, ParseException
{    MultiportSyslogTCPSource source = new MultiportSyslogTCPSource();    Channel channel = new MemoryChannel();    List<Event> channelEvents = new ArrayList<>();    int numPorts = 1000;    List<Integer> portList = testNPorts(source, channel, channelEvents, numPorts, null, getSimpleEventSender(), new Context());        processEvents(channelEvents, numPorts, portList);    source.stop();}
0
public void testMultiplePortsSSL() throws Exception
{    SSLContext sslContext = SSLContext.getInstance("TLS");    sslContext.init(null, new TrustManager[] { new X509TrustManager() {        @Override        public void checkClientTrusted(X509Certificate[] certs, String s) {                }        @Override        public void checkServerTrusted(X509Certificate[] certs, String s) {                }        @Override        public X509Certificate[] getAcceptedIssuers() {            return new X509Certificate[0];        }    } }, null);    SocketFactory socketFactory = sslContext.getSocketFactory();    Context context = new Context();    context.put("ssl", "true");    context.put("keystore", "src/test/resources/server.p12");    context.put("keystore-password", "password");    context.put("keystore-type", "PKCS12");    MultiportSyslogTCPSource source = new MultiportSyslogTCPSource();    Channel channel = new MemoryChannel();    List<Event> channelEvents = new ArrayList<>();    int numPorts = 10;    List<Integer> portList = testNPorts(source, channel, channelEvents, numPorts, null, getSSLEventSender(socketFactory), context);        processEvents(channelEvents, numPorts, portList);    source.stop();}
0
public void checkClientTrusted(X509Certificate[] certs, String s)
{}
0
public void checkServerTrusted(X509Certificate[] certs, String s)
{}
0
public X509Certificate[] getAcceptedIssuers()
{    return new X509Certificate[0];}
0
private BiConsumer<Integer, byte[]> getSSLEventSender(SocketFactory socketFactory)
{    return (port, event) -> {        try {            Socket syslogSocket = socketFactory.createSocket(InetAddress.getLocalHost(), port);            syslogSocket.getOutputStream().write(event);            syslogSocket.close();        } catch (Exception e) {            e.printStackTrace();        }    };}
0
private BiConsumer<Integer, byte[]> getSimpleEventSender()
{    return (Integer port, byte[] event) -> {        try {            Socket syslogSocket = new Socket(InetAddress.getLocalHost(), port);            syslogSocket.getOutputStream().write(event);            syslogSocket.close();        } catch (IOException e) {            e.printStackTrace();        }    };}
0
private void processEvents(List<Event> channelEvents, int numPorts, List<Integer> portList)
{    for (int i = 0; i < numPorts; i++) {        Iterator<Event> iter = channelEvents.iterator();        while (iter.hasNext()) {            Event e = iter.next();            Map<String, String> headers = e.getHeaders();                        Integer port = null;            if (headers.containsKey("port")) {                port = Integer.parseInt(headers.get("port"));            }            iter.remove();            Assert.assertEquals("Timestamps must match", String.valueOf(time.getMillis()), headers.get("timestamp"));            String host2 = headers.get("host");            Assert.assertEquals(host1, host2);            if (port != null) {                int num = portList.indexOf(port);                Assert.assertEquals(data1 + " " + String.valueOf(num), new String(e.getBody()));            }        }    }}
0
public void testFragmented() throws CharacterCodingException
{    final int maxLen = 100;    IoBuffer savedBuf = IoBuffer.allocate(maxLen);    String origMsg = "<1>- - blah blam foo\n";    IoBuffer buf1 = IoBuffer.wrap(origMsg.substring(0, 11).getBytes(Charsets.UTF_8));    IoBuffer buf2 = IoBuffer.wrap(origMsg.substring(11, 16).getBytes(Charsets.UTF_8));    IoBuffer buf3 = IoBuffer.wrap(origMsg.substring(16, 21).getBytes(Charsets.UTF_8));    LineSplitter lineSplitter = new LineSplitter(maxLen);    ParsedBuffer parsedLine = new ParsedBuffer();    Assert.assertFalse("Incomplete line should not be parsed", lineSplitter.parseLine(buf1, savedBuf, parsedLine));    Assert.assertFalse("Incomplete line should not be parsed", lineSplitter.parseLine(buf2, savedBuf, parsedLine));    Assert.assertTrue("Completed line should be parsed", lineSplitter.parseLine(buf3, savedBuf, parsedLine));        Assert.assertEquals(origMsg.trim(), parsedLine.buffer.getString(Charsets.UTF_8.newDecoder()));    parsedLine.buffer.rewind();    MultiportSyslogHandler handler = new MultiportSyslogHandler(maxLen, 100, null, null, null, null, null, new ThreadSafeDecoder(Charsets.UTF_8), new ConcurrentHashMap<Integer, ThreadSafeDecoder>(), null);    Event event = handler.parseEvent(parsedLine, Charsets.UTF_8.newDecoder());    String body = new String(event.getBody(), Charsets.UTF_8);    Assert.assertEquals("Event body incorrect", origMsg.trim().substring(7), body);}
0
public void testCharsetParsing() throws FileNotFoundException, IOException
{    String header = "<10>2012-08-11T01:01:01Z localhost ";    String enBody = "Yarf yarf yarf";    String enMsg = header + enBody;    String frBody = "Comment " + "\u00EA" + "tes-vous?";    String frMsg = header + frBody;    String esBody = "Cmo ests?";    String esMsg = header + esBody;        MultiportSyslogHandler handler = new MultiportSyslogHandler(1000, 10, new ChannelProcessor(new ReplicatingChannelSelector()), new SourceCounter("test"), null, null, null, new ThreadSafeDecoder(Charsets.UTF_8), new ConcurrentHashMap<Integer, ThreadSafeDecoder>(), null);    ParsedBuffer parsedBuf = new ParsedBuffer();    parsedBuf.incomplete = false;        String[] bodies = { enBody, esBody, frBody };    String[] msgs = { enMsg, esMsg, frMsg };    Charset[] charsets = { Charsets.UTF_8, Charsets.ISO_8859_1 };    for (Charset charset : charsets) {        for (int i = 0; i < msgs.length; i++) {            String msg = msgs[i];            String body = bodies[i];            parsedBuf.buffer = IoBuffer.wrap(msg.getBytes(charset));            Event evt = handler.parseEvent(parsedBuf, charset.newDecoder());            String result = new String(evt.getBody(), charset);                        Assert.assertEquals(charset + " parse error: " + msg, body, result);            Assert.assertNull(evt.getHeaders().get(SyslogUtils.EVENT_STATUS));        }    }            byte[] badUtf8Seq = enMsg.getBytes(Charsets.ISO_8859_1);    int badMsgLen = badUtf8Seq.length;        badUtf8Seq[badMsgLen - 2] = (byte) 0xFE;        badUtf8Seq[badMsgLen - 1] = (byte) 0xFF;    parsedBuf.buffer = IoBuffer.wrap(badUtf8Seq);    Event evt = handler.parseEvent(parsedBuf, Charsets.UTF_8.newDecoder());    Assert.assertEquals("event body: " + new String(evt.getBody(), Charsets.ISO_8859_1) + " and my default charset = " + Charset.defaultCharset() + " with event = " + evt, SyslogUtils.SyslogStatus.INVALID.getSyslogStatus(), evt.getHeaders().get(SyslogUtils.EVENT_STATUS));    Assert.assertArrayEquals("Raw message data should be kept in body of event", badUtf8Seq, evt.getBody());    SourceCounter sc = (SourceCounter) Whitebox.getInternalState(handler, "sourceCounter");    Assert.assertEquals(1, sc.getEventReadFail());}
0
public void testHandlerGenericFail() throws Exception
{        MultiportSyslogHandler handler = new MultiportSyslogHandler(1000, 10, new ChannelProcessor(new ReplicatingChannelSelector()), new SourceCounter("test"), null, null, null, new ThreadSafeDecoder(Charsets.UTF_8), new ConcurrentHashMap<Integer, ThreadSafeDecoder>(), null);    handler.exceptionCaught(null, new RuntimeException("dummy"));    SourceCounter sc = (SourceCounter) Whitebox.getInternalState(handler, "sourceCounter");    Assert.assertEquals(1, sc.getGenericProcessingFail());}
0
private static Event takeEvent(Channel channel)
{    Transaction txn = channel.getTransaction();    txn.begin();    Event evt = channel.take();    txn.commit();    txn.close();    return evt;}
0
public void testPortCharsetHandling() throws UnknownHostException, Exception
{            InetAddress localAddr = InetAddress.getLocalHost();    DefaultIoSessionDataStructureFactory dsFactory = new DefaultIoSessionDataStructureFactory();        int port1 = 10001;    NioSession session1 = mock(NioSession.class);    session1.setAttributeMap(dsFactory.getAttributeMap(session1));    SocketAddress sockAddr1 = new InetSocketAddress(localAddr, port1);    when(session1.getLocalAddress()).thenReturn(sockAddr1);        int port2 = 10002;    NioSession session2 = mock(NioSession.class);    session2.setAttributeMap(dsFactory.getAttributeMap(session2));    SocketAddress sockAddr2 = new InetSocketAddress(localAddr, port2);    when(session2.getLocalAddress()).thenReturn(sockAddr2);        ConcurrentMap<Integer, ThreadSafeDecoder> portCharsets = new ConcurrentHashMap<Integer, ThreadSafeDecoder>();    portCharsets.put(port1, new ThreadSafeDecoder(Charsets.ISO_8859_1));    portCharsets.put(port2, new ThreadSafeDecoder(Charsets.UTF_8));                MemoryChannel chan = new MemoryChannel();    chan.configure(new Context());    chan.start();    ReplicatingChannelSelector sel = new ReplicatingChannelSelector();    sel.setChannels(Lists.<Channel>newArrayList(chan));    ChannelProcessor chanProc = new ChannelProcessor(sel);        MultiportSyslogHandler handler = new MultiportSyslogHandler(1000, 10, chanProc, new SourceCounter("test"), null, null, null, new ThreadSafeDecoder(Charsets.UTF_8), portCharsets, null);        handler.sessionCreated(session1);    handler.sessionCreated(session2);                String header = "<10>2012-08-17T02:14:00-07:00 192.168.1.110 ";        String dangerousChars = "";            String msg;    IoBuffer buf;    Event evt;        msg = header + dangerousChars + "\n";    buf = IoBuffer.wrap(msg.getBytes(Charsets.ISO_8859_1));    handler.messageReceived(session1, buf);    evt = takeEvent(chan);    Assert.assertNotNull("Event vanished!", evt);    Assert.assertNull(evt.getHeaders().get(SyslogUtils.EVENT_STATUS));        msg = header + dangerousChars + "\n";    buf = IoBuffer.wrap(msg.getBytes(Charsets.ISO_8859_1));    handler.messageReceived(session2, buf);    evt = takeEvent(chan);    Assert.assertNotNull("Event vanished!", evt);    Assert.assertEquals("Expected invalid event due to character encoding", SyslogUtils.SyslogStatus.INVALID.getSyslogStatus(), evt.getHeaders().get(SyslogUtils.EVENT_STATUS));        msg = header + dangerousChars + "\n";    buf = IoBuffer.wrap(msg.getBytes(Charsets.UTF_8));    handler.messageReceived(session2, buf);    evt = takeEvent(chan);    Assert.assertNotNull("Event vanished!", evt);    Assert.assertNull(evt.getHeaders().get(SyslogUtils.EVENT_STATUS));    SourceCounter sc = (SourceCounter) Whitebox.getInternalState(handler, "sourceCounter");    Assert.assertEquals(1, sc.getEventReadFail());}
0
public void testErrorCounterChannelWriteFail() throws Exception
{    MultiportSyslogTCPSource source = new MultiportSyslogTCPSource();    Channel channel = new MemoryChannel();    List<Event> channelEvents = new ArrayList<>();    ChannelProcessor cp = Mockito.mock(ChannelProcessor.class);    doThrow(new ChannelException("dummy")).doNothing().when(cp).processEventBatch(anyListOf(Event.class));    try {        testNPorts(source, channel, channelEvents, 1, cp, getSimpleEventSender(), new Context());    } catch (Exception e) {        }    SourceCounter sc = (SourceCounter) Whitebox.getInternalState(source, "sourceCounter");    Assert.assertEquals(1, sc.getChannelWriteFail());    source.stop();}
0
public void testClientHeaders() throws IOException
{    String testClientIPHeader = "testClientIPHeader";    String testClientHostnameHeader = "testClientHostnameHeader";    MultiportSyslogTCPSource source = new MultiportSyslogTCPSource();    Channel channel = new MemoryChannel();    Configurables.configure(channel, new Context());    List<Channel> channels = Lists.newArrayList();    channels.add(channel);    ChannelSelector rcs = new ReplicatingChannelSelector();    rcs.setChannels(channels);    source.setChannelProcessor(new ChannelProcessor(rcs));    int port = getFreePort();    Context context = new Context();    context.put("host", InetAddress.getLoopbackAddress().getHostAddress());    context.put("ports", String.valueOf(port));    context.put("clientIPHeader", testClientIPHeader);    context.put("clientHostnameHeader", testClientHostnameHeader);    source.configure(context);    source.start();        Socket syslogSocket = new Socket(InetAddress.getLoopbackAddress().getHostAddress(), port);    syslogSocket.getOutputStream().write(getEvent(0));    Event e = takeEvent(channel);    source.stop();    Map<String, String> headers = e.getHeaders();    checkHeader(headers, testClientIPHeader, InetAddress.getLoopbackAddress().getHostAddress());    checkHeader(headers, testClientHostnameHeader, InetAddress.getLoopbackAddress().getHostName());}
0
private static void checkHeader(Map<String, String> headers, String headerName, String expectedValue)
{    assertTrue("Missing event header: " + headerName, headers.containsKey(headerName));    assertEquals("Event header value does not match: " + headerName, expectedValue, headers.get(headerName));}
0
private static int getFreePort()
{    try (ServerSocket socket = new ServerSocket(0)) {        return socket.getLocalPort();    } catch (IOException e) {        throw new AssertionError("Can not find free port.", e);    }}
0
public void setUp() throws UnknownHostException
{    localhost = InetAddress.getByName("127.0.0.1");    source = new NetcatSource();    channel = new MemoryChannel();    Configurables.configure(channel, new Context());    List<Channel> channels = new ArrayList<Channel>();    channels.add(channel);    ChannelSelector rcs = new ReplicatingChannelSelector();    rcs.setChannels(channels);    source.setChannelProcessor(new ChannelProcessor(rcs));}
0
public void testUTF16BEencoding() throws InterruptedException, IOException
{    String encoding = "UTF-16BE";    startSource(encoding, "false", "1", "512");    Socket netcatSocket = new Socket(localhost, selectedPort);    try {                for (int i = 0; i < 20; i++) {            sendEvent(netcatSocket, english, encoding);            Assert.assertArrayEquals("Channel contained our event", english.getBytes(defaultCharset), getFlumeEvent());        }                for (int i = 0; i < 20; i++) {            sendEvent(netcatSocket, french, encoding);            Assert.assertArrayEquals("Channel contained our event", french.getBytes(defaultCharset), getFlumeEvent());        }    } finally {        netcatSocket.close();        stopSource();    }}
0
public void testUTF16LEencoding() throws InterruptedException, IOException
{    String encoding = "UTF-16LE";    startSource(encoding, "false", "1", "512");    Socket netcatSocket = new Socket(localhost, selectedPort);    try {                for (int i = 0; i < 20; i++) {            sendEvent(netcatSocket, english, encoding);            Assert.assertArrayEquals("Channel contained our event", english.getBytes(defaultCharset), getFlumeEvent());        }                for (int i = 0; i < 20; i++) {            sendEvent(netcatSocket, french, encoding);            Assert.assertArrayEquals("Channel contained our event", french.getBytes(defaultCharset), getFlumeEvent());        }    } finally {        netcatSocket.close();        stopSource();    }}
0
public void testUTF8encoding() throws InterruptedException, IOException
{    String encoding = "UTF-8";    startSource(encoding, "false", "1", "512");    Socket netcatSocket = new Socket(localhost, selectedPort);    try {                for (int i = 0; i < 20; i++) {            sendEvent(netcatSocket, english, encoding);            Assert.assertArrayEquals("Channel contained our event", english.getBytes(defaultCharset), getFlumeEvent());        }                for (int i = 0; i < 20; i++) {            sendEvent(netcatSocket, french, encoding);            Assert.assertArrayEquals("Channel contained our event", french.getBytes(defaultCharset), getFlumeEvent());        }    } finally {        netcatSocket.close();        stopSource();    }}
0
public void testIS88591encoding() throws InterruptedException, IOException
{    String encoding = "ISO-8859-1";    startSource(encoding, "false", "1", "512");    Socket netcatSocket = new Socket(localhost, selectedPort);    try {                for (int i = 0; i < 20; i++) {            sendEvent(netcatSocket, english, encoding);            Assert.assertArrayEquals("Channel contained our event", english.getBytes(defaultCharset), getFlumeEvent());        }                for (int i = 0; i < 20; i++) {            sendEvent(netcatSocket, french, encoding);            Assert.assertArrayEquals("Channel contained our event", french.getBytes(defaultCharset), getFlumeEvent());        }    } finally {        netcatSocket.close();        stopSource();    }}
0
public void testAck() throws InterruptedException, IOException
{    String encoding = "UTF-8";    String ackEvent = "OK";    startSource(encoding, "true", "1", "512");    Socket netcatSocket = new Socket(localhost, selectedPort);    LineIterator inputLineIterator = IOUtils.lineIterator(netcatSocket.getInputStream(), encoding);    try {                for (int i = 0; i < 20; i++) {            sendEvent(netcatSocket, english, encoding);            Assert.assertArrayEquals("Channel contained our event", english.getBytes(defaultCharset), getFlumeEvent());            Assert.assertEquals("Socket contained the Ack", ackEvent, inputLineIterator.nextLine());        }                for (int i = 0; i < 20; i++) {            sendEvent(netcatSocket, french, encoding);            Assert.assertArrayEquals("Channel contained our event", french.getBytes(defaultCharset), getFlumeEvent());            Assert.assertEquals("Socket contained the Ack", ackEvent, inputLineIterator.nextLine());        }    } finally {        netcatSocket.close();        stopSource();    }}
0
public void testMaxLineLength() throws InterruptedException, IOException
{    String encoding = "UTF-8";    startSource(encoding, "false", "1", "10");    Socket netcatSocket = new Socket(localhost, selectedPort);    try {        sendEvent(netcatSocket, "123456789", encoding);        Assert.assertArrayEquals("Channel contained our event", "123456789".getBytes(defaultCharset), getFlumeEvent());        sendEvent(netcatSocket, english, encoding);        Assert.assertEquals("Channel does not contain an event", null, getRawFlumeEvent());    } finally {        netcatSocket.close();        stopSource();    }}
0
public void testMaxLineLengthwithAck() throws InterruptedException, IOException
{    String encoding = "UTF-8";    String ackEvent = "OK";    String ackErrorEvent = "FAILED: Event exceeds the maximum length (10 chars, including newline)";    startSource(encoding, "true", "1", "10");    Socket netcatSocket = new Socket(localhost, selectedPort);    LineIterator inputLineIterator = IOUtils.lineIterator(netcatSocket.getInputStream(), encoding);    try {        sendEvent(netcatSocket, "123456789", encoding);        Assert.assertArrayEquals("Channel contained our event", "123456789".getBytes(defaultCharset), getFlumeEvent());        Assert.assertEquals("Socket contained the Ack", ackEvent, inputLineIterator.nextLine());        sendEvent(netcatSocket, english, encoding);        Assert.assertEquals("Channel does not contain an event", null, getRawFlumeEvent());        Assert.assertEquals("Socket contained the Error Ack", ackErrorEvent, inputLineIterator.nextLine());    } finally {        netcatSocket.close();        stopSource();    }}
0
public void testSourceStoppedOnFlumeException() throws InterruptedException, IOException
{    boolean isFlumeExceptionThrown = false;        try (ServerSocketChannel dummyServerSocket = ServerSocketChannel.open()) {        dummyServerSocket.socket().setReuseAddress(true);        dummyServerSocket.socket().bind(new InetSocketAddress("0.0.0.0", 10500));        Context context = new Context();        context.put("port", String.valueOf(10500));        context.put("bind", "0.0.0.0");        context.put("ack-every-event", "false");        Configurables.configure(source, context);        source.start();    } catch (FlumeException fe) {        isFlumeExceptionThrown = true;    }            Assert.assertTrue("Flume exception is thrown as port already in use", isFlumeExceptionThrown);    Assert.assertEquals("Server is stopped", LifecycleState.STOP, source.getLifecycleState());}
0
private void startSource(String encoding, String ack, String batchSize, String maxLineLength) throws InterruptedException
{    Context context = new Context();    context.put("port", String.valueOf(selectedPort = getFreePort()));    context.put("bind", "0.0.0.0");    context.put("ack-every-event", ack);    context.put("encoding", encoding);    context.put("batch-size", batchSize);    context.put("max-line-length", maxLineLength);    Configurables.configure(source, context);    source.start();    Assert.assertTrue("Reached start or error", LifecycleController.waitForOneOf(source, LifecycleState.START_OR_ERROR));    Assert.assertEquals("Server is started", LifecycleState.START, source.getLifecycleState());}
0
private void sendEvent(Socket socket, String content, String encoding) throws IOException
{    OutputStream output = socket.getOutputStream();    IOUtils.write(content + IOUtils.LINE_SEPARATOR_UNIX, output, encoding);    output.flush();}
0
private byte[] getFlumeEvent()
{    Transaction transaction = channel.getTransaction();    transaction.begin();    Event event = channel.take();    Assert.assertNotNull(event);    try {        transaction.commit();    } catch (Throwable t) {        transaction.rollback();    } finally {        transaction.close();    }        return event.getBody();}
1
private Event getRawFlumeEvent()
{    Transaction transaction = channel.getTransaction();    transaction.begin();    Event event = channel.take();    try {        transaction.commit();    } catch (Throwable t) {        transaction.rollback();    } finally {        transaction.close();    }        return event;}
1
private void stopSource() throws InterruptedException
{    source.stop();    Assert.assertTrue("Reached stop or error", LifecycleController.waitForOneOf(source, LifecycleState.STOP_OR_ERROR));    Assert.assertEquals("Server is stopped", LifecycleState.STOP, source.getLifecycleState());    }
1
private void init()
{    source = new NetcatUdpSource();    channel = new MemoryChannel();    Configurables.configure(channel, new Context());    List<Channel> channels = new ArrayList<Channel>();    channels.add(channel);    ChannelSelector rcs = new ReplicatingChannelSelector();    rcs.setChannels(channels);    source.setChannelProcessor(new ChannelProcessor(rcs));    Context context = new Context();    context.put("port", String.valueOf(TEST_NETCAT_PORT));    source.configure(context);}
0
private void runUdpTest(String data1) throws IOException
{    init();    source.start();        DatagramSocket socket;    DatagramPacket datagramPacket;    datagramPacket = new DatagramPacket(data1.getBytes(), data1.getBytes().length, InetAddress.getLocalHost(), source.getSourcePort());    for (int i = 0; i < 10; i++) {        socket = new DatagramSocket();        socket.send(datagramPacket);        socket.close();    }    List<Event> channelEvents = new ArrayList<Event>();    Transaction txn = channel.getTransaction();    txn.begin();    for (int i = 0; i < 10; i++) {        Event e = channel.take();        Assert.assertNotNull(e);        channelEvents.add(e);    }    try {        txn.commit();    } catch (Throwable t) {        txn.rollback();    } finally {        txn.close();    }    source.stop();    for (Event e : channelEvents) {        Assert.assertNotNull(e);        String str = new String(e.getBody(), Charsets.UTF_8);                Assert.assertArrayEquals(data1.getBytes(), e.getBody());    }}
1
public void testLargePayload() throws Exception
{    init();    source.start();        byte[] largePayload = getPayload(1000).getBytes();    DatagramSocket socket;    DatagramPacket datagramPacket;    datagramPacket = new DatagramPacket(largePayload, 1000, InetAddress.getLocalHost(), source.getSourcePort());    for (int i = 0; i < 10; i++) {        socket = new DatagramSocket();        socket.send(datagramPacket);        socket.close();    }    List<Event> channelEvents = new ArrayList<Event>();    Transaction txn = channel.getTransaction();    txn.begin();    for (int i = 0; i < 10; i++) {        Event e = channel.take();        Assert.assertNotNull(e);        channelEvents.add(e);    }    try {        txn.commit();    } catch (Throwable t) {        txn.rollback();    } finally {        txn.close();    }    source.stop();    for (Event e : channelEvents) {        Assert.assertNotNull(e);        Assert.assertArrayEquals(largePayload, e.getBody());    }}
0
public void testShortString() throws IOException
{    runUdpTest(shortString);}
0
public void testMediumString() throws IOException
{    runUdpTest(mediumString);}
0
private String getPayload(int length)
{    StringBuilder payload = new StringBuilder(length);    for (int n = 0; n < length; ++n) {        payload.append("x");    }    return payload.toString();}
0
public void setUp()
{    sourceRunner = new PollableSourceRunner();}
0
public void testLifecycle() throws InterruptedException
{    final Channel channel = new MemoryChannel();    final CountDownLatch latch = new CountDownLatch(50);    Configurables.configure(channel, new Context());    final ChannelSelector cs = new ReplicatingChannelSelector();    cs.setChannels(Lists.newArrayList(channel));    PollableSource source = new PollableSource() {        private String name;        private ChannelProcessor cp = new ChannelProcessor(cs);        @Override        public Status process() throws EventDeliveryException {            Transaction transaction = channel.getTransaction();            try {                transaction.begin();                Event event = EventBuilder.withBody(String.valueOf("Event " + latch.getCount()).getBytes());                latch.countDown();                if (latch.getCount() % 20 == 0) {                    throw new EventDeliveryException("I don't like event:" + event);                }                channel.put(event);                transaction.commit();                return Status.READY;            } catch (EventDeliveryException e) {                                transaction.rollback();                return Status.BACKOFF;            } finally {                transaction.close();            }        }        @Override        public long getBackOffSleepIncrement() {            return PollableSourceConstants.DEFAULT_BACKOFF_SLEEP_INCREMENT;        }        @Override        public long getMaxBackOffSleepInterval() {            return PollableSourceConstants.DEFAULT_MAX_BACKOFF_SLEEP;        }        @Override        public void start() {                }        @Override        public void stop() {                }        @Override        public LifecycleState getLifecycleState() {                        return null;        }        @Override        public void setName(String name) {            this.name = name;        }        @Override        public String getName() {            return name;        }        @Override        public void setChannelProcessor(ChannelProcessor channelProcessor) {            cp = channelProcessor;        }        @Override        public ChannelProcessor getChannelProcessor() {            return cp;        }    };    sourceRunner.setSource(source);    sourceRunner.start();    latch.await();    sourceRunner.stop();}
1
public Status process() throws EventDeliveryException
{    Transaction transaction = channel.getTransaction();    try {        transaction.begin();        Event event = EventBuilder.withBody(String.valueOf("Event " + latch.getCount()).getBytes());        latch.countDown();        if (latch.getCount() % 20 == 0) {            throw new EventDeliveryException("I don't like event:" + event);        }        channel.put(event);        transaction.commit();        return Status.READY;    } catch (EventDeliveryException e) {                transaction.rollback();        return Status.BACKOFF;    } finally {        transaction.close();    }}
1
public long getBackOffSleepIncrement()
{    return PollableSourceConstants.DEFAULT_BACKOFF_SLEEP_INCREMENT;}
0
public long getMaxBackOffSleepInterval()
{    return PollableSourceConstants.DEFAULT_MAX_BACKOFF_SLEEP;}
0
public void start()
{}
0
public void stop()
{}
0
public LifecycleState getLifecycleState()
{        return null;}
0
public void setName(String name)
{    this.name = name;}
0
public String getName()
{    return name;}
0
public void setChannelProcessor(ChannelProcessor channelProcessor)
{    cp = channelProcessor;}
0
public ChannelProcessor getChannelProcessor()
{    return cp;}
0
public void setUp()
{    source = new SequenceGeneratorSource();    source.setName(TestSequenceGeneratorSource.class.getCanonicalName());}
0
public void testLifecycle() throws org.apache.flume.EventDeliveryException
{    final int DOPROCESS_LOOPS = 5;    Context context = new Context();    Configurables.configure(source, context);    ChannelProcessor cp = Mockito.mock(ChannelProcessor.class);    source.setChannelProcessor(cp);    source.start();    for (int i = 0; i < DOPROCESS_LOOPS; i++) {        source.process();    }    source.stop();}
0
public void testSingleEvents() throws EventDeliveryException
{    final int BATCH_SIZE = 1;    final int TOTAL_EVENTS = 5;    final int DOPROCESS_LOOPS = 10;    Context context = new Context();    context.put("batchSize", Integer.toString(BATCH_SIZE));    context.put("totalEvents", Integer.toString(TOTAL_EVENTS));    Configurables.configure(source, context);    ChannelProcessor cp = Mockito.mock(ChannelProcessor.class);    Mockito.doNothing().doThrow(    ChannelException.class).doNothing().when(cp).processEvent(Mockito.any(Event.class));    source.setChannelProcessor(cp);    source.start();    for (int i = 0; i < DOPROCESS_LOOPS; i++) {        source.process();    }    ArgumentCaptor<Event> argumentCaptor = ArgumentCaptor.forClass(Event.class);    Mockito.verify(cp, Mockito.times(6)).processEvent(argumentCaptor.capture());    Mockito.verify(cp, Mockito.never()).processEventBatch(Mockito.anyListOf(Event.class));    verifyEventSequence(TOTAL_EVENTS, argumentCaptor.getAllValues());}
0
public void testBatch() throws EventDeliveryException
{    final int BATCH_SIZE = 3;    final int TOTAL_EVENTS = 5;    final int DOPROCESS_LOOPS = 10;    Context context = new Context();    context.put("batchSize", Integer.toString(BATCH_SIZE));    context.put("totalEvents", Integer.toString(TOTAL_EVENTS));    Configurables.configure(source, context);    ChannelProcessor cp = Mockito.mock(ChannelProcessor.class);    Mockito.doNothing().doThrow(    ChannelException.class).doNothing().when(cp).processEventBatch(Mockito.anyListOf(Event.class));    source.setChannelProcessor(cp);    source.start();    for (int i = 0; i < DOPROCESS_LOOPS; i++) {        source.process();    }    ArgumentCaptor<List<Event>> argumentCaptor = ArgumentCaptor.forClass((Class) List.class);    Mockito.verify(cp, Mockito.never()).processEvent(Mockito.any(Event.class));    Mockito.verify(cp, Mockito.times(3)).processEventBatch(argumentCaptor.capture());    List<List<Event>> eventBatches = argumentCaptor.getAllValues();    verifyEventSequence(TOTAL_EVENTS, flatOutBatches(eventBatches));}
0
private static void verifyEventSequence(int expectedTotalEvents, List<Event> actualEvents)
{    Set<Integer> uniqueEvents = new LinkedHashSet<>();    for (Event e : actualEvents) {        uniqueEvents.add(Integer.parseInt(new String(e.getBody())));    }    List<Integer> sortedFilteredEvents = new ArrayList<>(uniqueEvents);    Collections.sort(sortedFilteredEvents);    Assert.assertEquals("mismatching number of events", expectedTotalEvents, sortedFilteredEvents.size());    for (int i = 0; i < sortedFilteredEvents.size(); ++i) {        Assert.assertEquals("missing or unexpected event body", i, (int) sortedFilteredEvents.get(i));    }}
0
private static List<Event> flatOutBatches(List<List<Event>> eventBatches)
{    List<Event> events = new ArrayList<>();    for (List<Event> le : eventBatches) {        for (Event e : le) {            events.add(e);        }    }    return events;}
0
public void setUp()
{    source = new SpoolDirectorySource();    channel = new MemoryChannel();    Configurables.configure(channel, new Context());    List<Channel> channels = new ArrayList<Channel>();    channels.add(channel);    ChannelSelector rcs = new ReplicatingChannelSelector();    rcs.setChannels(channels);    source.setChannelProcessor(new ChannelProcessor(rcs));    tmpDir = Files.createTempDir();}
0
public void tearDown()
{    deleteFiles(tmpDir);    tmpDir.delete();}
0
private void deleteFiles(File directory)
{    for (File f : directory.listFiles()) {        if (f.isDirectory()) {            deleteFiles(f);            f.delete();        } else {            f.delete();        }    }}
0
public void testInvalidSortOrder()
{    Context context = new Context();    context.put(SpoolDirectorySourceConfigurationConstants.SPOOL_DIRECTORY, tmpDir.getAbsolutePath());    context.put(SpoolDirectorySourceConfigurationConstants.CONSUME_ORDER, "undefined");    Configurables.configure(source, context);}
0
public void testValidSortOrder()
{    Context context = new Context();    context.put(SpoolDirectorySourceConfigurationConstants.SPOOL_DIRECTORY, tmpDir.getAbsolutePath());    context.put(SpoolDirectorySourceConfigurationConstants.CONSUME_ORDER, "oLdESt");    Configurables.configure(source, context);    context.put(SpoolDirectorySourceConfigurationConstants.CONSUME_ORDER, "yoUnGest");    Configurables.configure(source, context);    context.put(SpoolDirectorySourceConfigurationConstants.CONSUME_ORDER, "rAnDom");    Configurables.configure(source, context);}
0
public void testPutFilenameHeader() throws IOException, InterruptedException
{    Context context = new Context();    File f1 = new File(tmpDir.getAbsolutePath() + "/file1");    Files.write("file1line1\nfile1line2\nfile1line3\nfile1line4\n" + "file1line5\nfile1line6\nfile1line7\nfile1line8\n", f1, Charsets.UTF_8);    context.put(SpoolDirectorySourceConfigurationConstants.SPOOL_DIRECTORY, tmpDir.getAbsolutePath());    context.put(SpoolDirectorySourceConfigurationConstants.FILENAME_HEADER, "true");    context.put(SpoolDirectorySourceConfigurationConstants.FILENAME_HEADER_KEY, "fileHeaderKeyTest");    Configurables.configure(source, context);    source.start();    while (source.getSourceCounter().getEventAcceptedCount() < 8) {        Thread.sleep(10);    }    Transaction txn = channel.getTransaction();    txn.begin();    Event e = channel.take();    Assert.assertNotNull("Event must not be null", e);    Assert.assertNotNull("Event headers must not be null", e.getHeaders());    Assert.assertNotNull(e.getHeaders().get("fileHeaderKeyTest"));    Assert.assertEquals(f1.getAbsolutePath(), e.getHeaders().get("fileHeaderKeyTest"));    txn.commit();    txn.close();}
0
public void testPutBasenameHeader() throws IOException, InterruptedException
{    Context context = new Context();    File f1 = new File(tmpDir.getAbsolutePath() + "/file1");    Files.write("file1line1\nfile1line2\nfile1line3\nfile1line4\n" + "file1line5\nfile1line6\nfile1line7\nfile1line8\n", f1, Charsets.UTF_8);    context.put(SpoolDirectorySourceConfigurationConstants.SPOOL_DIRECTORY, tmpDir.getAbsolutePath());    context.put(SpoolDirectorySourceConfigurationConstants.BASENAME_HEADER, "true");    context.put(SpoolDirectorySourceConfigurationConstants.BASENAME_HEADER_KEY, "basenameHeaderKeyTest");    Configurables.configure(source, context);    source.start();    while (source.getSourceCounter().getEventAcceptedCount() < 8) {        Thread.sleep(10);    }    Transaction txn = channel.getTransaction();    txn.begin();    Event e = channel.take();    Assert.assertNotNull("Event must not be null", e);    Assert.assertNotNull("Event headers must not be null", e.getHeaders());    Assert.assertNotNull(e.getHeaders().get("basenameHeaderKeyTest"));    Assert.assertEquals(f1.getName(), e.getHeaders().get("basenameHeaderKeyTest"));    txn.commit();    txn.close();}
0
public void testRecursion_SetToTrue() throws IOException, InterruptedException
{    File subDir = new File(tmpDir, "directorya/directoryb/directoryc");    boolean directoriesCreated = subDir.mkdirs();    Assert.assertTrue("source directories must be created", directoriesCreated);    final String FILE_NAME = "recursion_file.txt";    File f1 = new File(subDir, FILE_NAME);    String origBody = "file1line1\nfile1line2\nfile1line3\nfile1line4\n" + "file1line5\nfile1line6\nfile1line7\nfile1line8\n";    Files.write(origBody, f1, Charsets.UTF_8);    Context context = new Context();    context.put(SpoolDirectorySourceConfigurationConstants.RECURSIVE_DIRECTORY_SEARCH,     "true");    context.put(SpoolDirectorySourceConfigurationConstants.SPOOL_DIRECTORY,     tmpDir.getAbsolutePath());    context.put(SpoolDirectorySourceConfigurationConstants.FILENAME_HEADER,     "true");    Configurables.configure(source, context);    source.start();    Assert.assertTrue("Recursion setting in source is correct", source.getRecursiveDirectorySearch());    Transaction txn = channel.getTransaction();    txn.begin();    long startTime = System.currentTimeMillis();    Event e = null;    while (System.currentTimeMillis() - startTime < 300 && e == null) {        e = channel.take();        Thread.sleep(10);    }    Assert.assertNotNull("Event must not be null", e);    Assert.assertNotNull("Event headers must not be null", e.getHeaders());    Assert.assertTrue("File header value did not end with expected filename", e.getHeaders().get("file").endsWith(FILE_NAME));    ByteArrayOutputStream baos = new ByteArrayOutputStream();    do {                baos.write(e.getBody());                baos.write('\n');        e = channel.take();    } while (e != null);    Assert.assertEquals("Event body is correct", Arrays.toString(origBody.getBytes()), Arrays.toString(baos.toByteArray()));    txn.commit();    txn.close();}
0
public void testRecursion_SetToFalse() throws IOException, InterruptedException
{    Context context = new Context();    File subDir = new File(tmpDir, "directory");    boolean directoriesCreated = subDir.mkdirs();    Assert.assertTrue("source directories must be created", directoriesCreated);    File f1 = new File(subDir.getAbsolutePath() + "/file1.txt");    Files.write("file1line1\nfile1line2\nfile1line3\nfile1line4\n" + "file1line5\nfile1line6\nfile1line7\nfile1line8\n", f1, Charsets.UTF_8);    context.put(SpoolDirectorySourceConfigurationConstants.RECURSIVE_DIRECTORY_SEARCH, "false");    context.put(SpoolDirectorySourceConfigurationConstants.SPOOL_DIRECTORY, tmpDir.getAbsolutePath());    context.put(SpoolDirectorySourceConfigurationConstants.FILENAME_HEADER, "true");    context.put(SpoolDirectorySourceConfigurationConstants.FILENAME_HEADER_KEY, "fileHeaderKeyTest");    Configurables.configure(source, context);    source.start();        Assert.assertFalse("Recursion setting in source is not set to false (this" + "test does not want recursion enabled)", source.getRecursiveDirectorySearch());    Transaction txn = channel.getTransaction();    txn.begin();    long startTime = System.currentTimeMillis();    Event e = null;    while (System.currentTimeMillis() - startTime < 300 && e == null) {        e = channel.take();        Thread.sleep(10);    }    Assert.assertNull("Event must be null", e);    txn.commit();    txn.close();}
0
public void testLifecycle() throws IOException, InterruptedException
{    Context context = new Context();    File f1 = new File(tmpDir.getAbsolutePath() + "/file1");    Files.write("file1line1\nfile1line2\nfile1line3\nfile1line4\n" + "file1line5\nfile1line6\nfile1line7\nfile1line8\n", f1, Charsets.UTF_8);    context.put(SpoolDirectorySourceConfigurationConstants.SPOOL_DIRECTORY, tmpDir.getAbsolutePath());    Configurables.configure(source, context);    for (int i = 0; i < 10; i++) {        source.start();        Assert.assertTrue("Reached start or error", LifecycleController.waitForOneOf(source, LifecycleState.START_OR_ERROR));        Assert.assertEquals("Server is started", LifecycleState.START, source.getLifecycleState());        source.stop();        Assert.assertTrue("Reached stop or error", LifecycleController.waitForOneOf(source, LifecycleState.STOP_OR_ERROR));        Assert.assertEquals("Server is stopped", LifecycleState.STOP, source.getLifecycleState());    }}
0
public void testReconfigure() throws InterruptedException, IOException
{    final int NUM_RECONFIGS = 20;    for (int i = 0; i < NUM_RECONFIGS; i++) {        Context context = new Context();        File file = new File(tmpDir.getAbsolutePath() + "/file-" + i);        Files.write("File " + i, file, Charsets.UTF_8);        context.put(SpoolDirectorySourceConfigurationConstants.SPOOL_DIRECTORY, tmpDir.getAbsolutePath());        Configurables.configure(source, context);        source.start();        Thread.sleep(TimeUnit.SECONDS.toMillis(1));        Transaction txn = channel.getTransaction();        txn.begin();        try {            Event event = channel.take();            String content = new String(event.getBody(), Charsets.UTF_8);            Assert.assertEquals("File " + i, content);            txn.commit();        } catch (Throwable t) {            txn.rollback();        } finally {            txn.close();        }        source.stop();        Assert.assertFalse("Fatal error on iteration " + i, source.hasFatalError());    }}
0
public void testSourceDoesNotDieOnFullChannel() throws Exception
{    Context chContext = new Context();    chContext.put("capacity", "2");    chContext.put("transactionCapacity", "2");    chContext.put("keep-alive", "0");    channel.stop();    Configurables.configure(channel, chContext);    channel.start();    Context context = new Context();    File f1 = new File(tmpDir.getAbsolutePath() + "/file1");    Files.write("file1line1\nfile1line2\nfile1line3\nfile1line4\n" + "file1line5\nfile1line6\nfile1line7\nfile1line8\n", f1, Charsets.UTF_8);    context.put(SpoolDirectorySourceConfigurationConstants.SPOOL_DIRECTORY, tmpDir.getAbsolutePath());    context.put(SpoolDirectorySourceConfigurationConstants.BATCH_SIZE, "2");    Configurables.configure(source, context);    source.setBackOff(false);    source.start();        long startTime = System.currentTimeMillis();    while (System.currentTimeMillis() - startTime < 5000 && !source.didHitChannelFullException()) {        Thread.sleep(10);    }    Assert.assertTrue("Expected to hit ChannelFullException, but did not!", source.didHitChannelFullException());    List<String> dataOut = Lists.newArrayList();    for (int i = 0; i < 8; ) {        Transaction tx = channel.getTransaction();        tx.begin();        Event e = channel.take();        if (e != null) {            dataOut.add(new String(e.getBody(), "UTF-8"));            i++;        }        e = channel.take();        if (e != null) {            dataOut.add(new String(e.getBody(), "UTF-8"));            i++;        }        tx.commit();        tx.close();    }    Assert.assertEquals(8, dataOut.size());    source.stop();}
0
public void testEndWithZeroByteFiles() throws IOException, InterruptedException
{    Context context = new Context();    File f1 = new File(tmpDir.getAbsolutePath() + "/file1");    Files.write("file1line1\n", f1, Charsets.UTF_8);    File f2 = new File(tmpDir.getAbsolutePath() + "/file2");    File f3 = new File(tmpDir.getAbsolutePath() + "/file3");    File f4 = new File(tmpDir.getAbsolutePath() + "/file4");    Files.touch(f2);    Files.touch(f3);    Files.touch(f4);    context.put(SpoolDirectorySourceConfigurationConstants.SPOOL_DIRECTORY, tmpDir.getAbsolutePath());    Configurables.configure(source, context);    source.start();        Thread.sleep(5000);    Assert.assertFalse("Server did not error", source.hasFatalError());    Assert.assertEquals("Four messages were read", 4, source.getSourceCounter().getEventAcceptedCount());    source.stop();}
0
public void testWithAllEmptyFiles() throws InterruptedException, IOException
{    Context context = new Context();    File[] f = new File[10];    for (int i = 0; i < 10; i++) {        f[i] = new File(tmpDir.getAbsolutePath() + "/file" + i);        Files.write(new byte[0], f[i]);    }    context.put(SpoolDirectorySourceConfigurationConstants.SPOOL_DIRECTORY, tmpDir.getAbsolutePath());    context.put(SpoolDirectorySourceConfigurationConstants.FILENAME_HEADER, "true");    context.put(SpoolDirectorySourceConfigurationConstants.FILENAME_HEADER_KEY, "fileHeaderKeyTest");    Configurables.configure(source, context);    source.start();    Thread.sleep(10);    for (int i = 0; i < 10; i++) {        Transaction txn = channel.getTransaction();        txn.begin();        Event e = channel.take();        Assert.assertNotNull("Event must not be null", e);        Assert.assertNotNull("Event headers must not be null", e.getHeaders());        Assert.assertNotNull(e.getHeaders().get("fileHeaderKeyTest"));        Assert.assertEquals(f[i].getAbsolutePath(), e.getHeaders().get("fileHeaderKeyTest"));        Assert.assertArrayEquals(new byte[0], e.getBody());        txn.commit();        txn.close();    }    source.stop();}
0
public void testWithEmptyAndDataFiles() throws InterruptedException, IOException
{    Context context = new Context();    File f1 = new File(tmpDir.getAbsolutePath() + "/file1");    Files.write("some data".getBytes(), f1);    File f2 = new File(tmpDir.getAbsolutePath() + "/file2");    Files.write(new byte[0], f2);    context.put(SpoolDirectorySourceConfigurationConstants.SPOOL_DIRECTORY, tmpDir.getAbsolutePath());    Configurables.configure(source, context);    source.start();    Thread.sleep(10);    for (int i = 0; i < 2; i++) {        Transaction txn = channel.getTransaction();        txn.begin();        Event e = channel.take();        txn.commit();        txn.close();    }    Transaction txn = channel.getTransaction();    txn.begin();    Assert.assertNull(channel.take());    txn.commit();    txn.close();    source.stop();}
0
private SourceCounter errorCounterCommonInit()
{    SourceCounter sc = new SourceCounter("dummy");    sc.start();    Context context = new Context();    context.put(SpoolDirectorySourceConfigurationConstants.SPOOL_DIRECTORY, tmpDir.getAbsolutePath());    Configurables.configure(source, context);    return sc;}
0
public void testErrorCounters() throws Exception
{    SourceCounter sc = errorCounterCommonInit();    ChannelProcessor cp = Mockito.mock(ChannelProcessor.class);    Mockito.doThrow(new ChannelException("dummy")).doThrow(new ChannelFullException("dummy")).doThrow(new RuntimeException("runtime")).when(cp).processEventBatch(Matchers.anyListOf(Event.class));    source.setChannelProcessor(cp);    ReliableSpoolingFileEventReader reader = Mockito.mock(ReliableSpoolingFileEventReader.class);    List<Event> events = new ArrayList<>();    events.add(Mockito.mock(Event.class));    Mockito.doReturn(events).doReturn(events).doReturn(events).doThrow(new IOException("dummy")).when(reader).readEvents(Mockito.anyInt());    Runnable runner = source.new SpoolDirectoryRunnable(reader, sc);    try {        runner.run();    } catch (Exception ex) {        }    Assert.assertEquals(2, sc.getChannelWriteFail());    Assert.assertEquals(1, sc.getGenericProcessingFail());}
0
public void testErrorCounterEventReadFail() throws Exception
{    SourceCounter sc = errorCounterCommonInit();    ChannelProcessor cp = Mockito.mock(ChannelProcessor.class);    source.setChannelProcessor(cp);    ReliableSpoolingFileEventReader reader = Mockito.mock(ReliableSpoolingFileEventReader.class);    List<Event> events = new ArrayList<>();    events.add(Mockito.mock(Event.class));    Mockito.doReturn(events).doThrow(new IOException("dummy")).when(reader).readEvents(Mockito.anyInt());    Runnable runner = source.new SpoolDirectoryRunnable(reader, sc);    try {        runner.run();    } catch (Exception ex) {        }    Assert.assertEquals(1, sc.getEventReadFail());}
0
public void setUp()
{    mockProcessor = mock(ChannelProcessor.class);}
0
private Event getEvent(StressSource source)
{    return field("event").ofType(Event.class).in(source).get();}
0
private List<Event> getLastProcessedEventList(StressSource source)
{    return field("eventBatchListToProcess").ofType(List.class).in(source).get();}
0
private CounterGroup getCounterGroup(StressSource source)
{    return field("counterGroup").ofType(CounterGroup.class).in(source).get();}
0
public void testMaxTotalEvents() throws InterruptedException, EventDeliveryException
{    StressSource source = new StressSource();    source.setChannelProcessor(mockProcessor);    Context context = new Context();    context.put("maxTotalEvents", "35");    source.configure(context);    source.start();    for (int i = 0; i < 50; i++) {        source.process();    }    verify(mockProcessor, times(35)).processEvent(getEvent(source));}
0
public void testRateLimitedEventsNoBatch() throws InterruptedException, EventDeliveryException
{    StressSource source = new StressSource();    source.setChannelProcessor(mockProcessor);    Context context = new Context();    context.put("maxTotalEvents", "20");    context.put("maxEventsPerSecond", "20");    source.configure(context);    long startTime = System.currentTimeMillis();    source.start();    for (int i = 0; i < 20; i++) {        source.process();    }    long finishTime = System.currentTimeMillis();        Assert.assertTrue(finishTime - startTime < 1300);    Assert.assertTrue(finishTime - startTime > 700);    source.stop();}
0
public void testNonRateLimitedEventsNoBatch() throws InterruptedException, EventDeliveryException
{    StressSource source = new StressSource();    source.setChannelProcessor(mockProcessor);    Context context = new Context();        context = new Context();    context.put("maxTotalEvents", "20");    context.put("maxEventsPerSecond", "0");    source.configure(context);    long startTime = System.currentTimeMillis();    source.start();    for (int i = 0; i <= 20; i++) {        source.process();    }    long finishTime = System.currentTimeMillis();    Assert.assertTrue(finishTime - startTime < 70);}
0
public void testRateLimitedEventsBatch() throws InterruptedException, EventDeliveryException
{    StressSource source = new StressSource();    source.setChannelProcessor(mockProcessor);    Context context = new Context();    context.put("maxTotalEvents", "20");    context.put("maxEventsPerSecond", "20");    context.put("batchSize", "3");    source.configure(context);    long startTime = System.currentTimeMillis();    source.start();    for (int i = 0; i < 20; i++) {        source.process();    }    long finishTime = System.currentTimeMillis();        Assert.assertTrue(finishTime - startTime < 1300);    Assert.assertTrue(finishTime - startTime > 700);    source.stop();}
0
public void testNonRateLimitedEventsBatch() throws InterruptedException, EventDeliveryException
{    StressSource source = new StressSource();    source.setChannelProcessor(mockProcessor);    Context context = new Context();        context.put("maxTotalEvents", "20");    context.put("maxEventsPerSecond", "0");    source.configure(context);    long startTime = System.currentTimeMillis();    source.start();    for (int i = 0; i <= 20; i++) {        source.process();    }    long finishTime = System.currentTimeMillis();    Assert.assertTrue(finishTime - startTime < 70);}
0
public void testBatchEvents() throws InterruptedException, EventDeliveryException
{    StressSource source = new StressSource();    source.setChannelProcessor(mockProcessor);    Context context = new Context();    context.put("maxTotalEvents", "35");    context.put("batchSize", "10");    source.configure(context);    source.start();    for (int i = 0; i < 50; i++) {        if (source.process() == Status.BACKOFF) {            TestCase.assertTrue("Source should have sent all events in 4 batches", i == 4);            break;        }        if (i < 3) {            verify(mockProcessor, times(i + 1)).processEventBatch(getLastProcessedEventList(source));        } else {            verify(mockProcessor, times(1)).processEventBatch(getLastProcessedEventList(source));        }    }    long successfulEvents = getCounterGroup(source).get("events.successful");    TestCase.assertTrue("Number of successful events should be 35 but was " + successfulEvents, successfulEvents == 35);    long failedEvents = getCounterGroup(source).get("events.failed");    TestCase.assertTrue("Number of failure events should be 0 but was " + failedEvents, failedEvents == 0);}
0
public void testBatchEventsWithoutMatTotalEvents() throws InterruptedException, EventDeliveryException
{    StressSource source = new StressSource();    source.setChannelProcessor(mockProcessor);    Context context = new Context();    context.put("batchSize", "10");    source.configure(context);    source.start();    for (int i = 0; i < 10; i++) {        Assert.assertFalse("StressSource with no maxTotalEvents should not return " + Status.BACKOFF, source.process() == Status.BACKOFF);    }    verify(mockProcessor, times(10)).processEventBatch(getLastProcessedEventList(source));    long successfulEvents = getCounterGroup(source).get("events.successful");    TestCase.assertTrue("Number of successful events should be 100 but was " + successfulEvents, successfulEvents == 100);    long failedEvents = getCounterGroup(source).get("events.failed");    TestCase.assertTrue("Number of failure events should be 0 but was " + failedEvents, failedEvents == 0);}
0
public void testMaxSuccessfulEvents() throws InterruptedException, EventDeliveryException
{    StressSource source = new StressSource();    source.setChannelProcessor(mockProcessor);    Context context = new Context();    context.put("maxSuccessfulEvents", "35");    source.configure(context);    source.start();    for (int i = 0; i < 10; i++) {        source.process();    }        doThrow(new ChannelException("stub")).when(mockProcessor).processEvent(getEvent(source));    source.process();    doNothing().when(mockProcessor).processEvent(getEvent(source));    for (int i = 0; i < 10; i++) {        source.process();    }        doThrow(new ChannelException("stub")).when(mockProcessor).processEvent(getEvent(source));    source.process();    doNothing().when(mockProcessor).processEvent(getEvent(source));    for (int i = 0; i < 50; i++) {        source.process();    }            verify(mockProcessor, times(37)).processEvent(getEvent(source));}
0
public void testRfc5424DateParsing()
{    final String[] examples = { "1985-04-12T23:20:50.52Z", "1985-04-12T19:20:50.52-04:00", "2003-10-11T22:14:15.003Z", "2003-08-24T05:14:15.000003-07:00", "2012-04-13T11:11:11-08:00", "2012-04-13T08:08:08.0001+00:00", "2012-04-13T08:08:08.251+00:00" };    SyslogParser parser = new SyslogParser();    DateTimeFormatter jodaParser = ISODateTimeFormat.dateTimeParser();    for (String ex : examples) {        Assert.assertEquals("Problem parsing date string: " + ex, jodaParser.parseMillis(ex), parser.parseRfc5424Date(ex));    }}
0
public void testMessageParsing()
{    SyslogParser parser = new SyslogParser();    Charset charset = Charsets.UTF_8;    List<String> messages = Lists.newArrayList();        messages.add("<34>Oct 11 22:14:15 mymachine su: 'su root' failed for " + "lonvick on /dev/pts/8");    messages.add("<13>Feb  5 17:32:18 10.0.0.99 Use the BFG!");    messages.add("<165>Aug 24 05:34:00 CST 1987 mymachine myproc[10]: %% " + "It's time to make the do-nuts.  %%  Ingredients: Mix=OK, Jelly=OK # " + "Devices: Mixer=OK, Jelly_Injector=OK, Frier=OK # Transport: " + "Conveyer1=OK, Conveyer2=OK # %%");    messages.add("<0>Oct 22 10:52:12 scapegoat 1990 Oct 22 10:52:01 TZ-6 " + "scapegoat.dmz.example.org 10.1.2.3 sched[0]: That's All Folks!");        messages.add("<34>1 2003-10-11T22:14:15.003Z mymachine.example.com su - " + "ID47 - BOM'su root' failed for lonvick on /dev/pts/8");    messages.add("<165>1 2003-08-24T05:14:15.000003-07:00 192.0.2.1 myproc " + "8710 - - %% It's time to make the do-nuts.");        messages.add("<13>2003-08-24T05:14:15Z localhost snarf?");    messages.add("<13>2012-08-16T14:34:03-08:00 127.0.0.1 test shnap!");        for (String msg : messages) {        Set<String> keepFields = new HashSet<String>();        Event event = parser.parseMessage(msg, charset, keepFields);        Assert.assertNull("Failure to parse known-good syslog message", event.getHeaders().get(SyslogUtils.EVENT_STATUS));    }        for (String msg : messages) {        Set<String> keepFields = new HashSet<String>();        keepFields.add(SyslogUtils.KEEP_FIELDS_ALL);        Event event = parser.parseMessage(msg, charset, keepFields);        Assert.assertArrayEquals(event.getBody(), msg.getBytes());        Assert.assertNull("Failure to parse known-good syslog message", event.getHeaders().get(SyslogUtils.EVENT_STATUS));    }        for (String msg : messages) {        Set<String> keepFields = new HashSet<String>();        keepFields.add(SyslogSourceConfigurationConstants.CONFIG_KEEP_FIELDS_HOSTNAME);        Event event = parser.parseMessage(msg, charset, keepFields);        Assert.assertTrue("Failure to persist hostname", new String(event.getBody()).contains(event.getHeaders().get("host")));        Assert.assertNull("Failure to parse known-good syslog message", event.getHeaders().get(SyslogUtils.EVENT_STATUS));    }}
0
private void init(String keepFields)
{    init(keepFields, new Context());}
0
private void init(String keepFields, Context context)
{    source = new SyslogTcpSource();    channel = new MemoryChannel();    Configurables.configure(channel, new Context());    List<Channel> channels = new ArrayList<>();    channels.add(channel);    ChannelSelector rcs = new ReplicatingChannelSelector();    rcs.setChannels(channels);    source.setChannelProcessor(new ChannelProcessor(rcs));    context.put("host", InetAddress.getLoopbackAddress().getHostAddress());    context.put("port", String.valueOf(TEST_SYSLOG_PORT));    context.put("keepFields", keepFields);    source.configure(context);}
0
private void initSsl()
{    Context context = new Context();    context.put("ssl", "true");    context.put("keystore", "src/test/resources/server.p12");    context.put("keystore-password", "password");    context.put("keystore-type", "PKCS12");    init("none", context);}
0
private void runKeepFieldsTest(String keepFields) throws IOException
{    init(keepFields);    source.start();        InetSocketAddress addr = source.getBoundAddress();    for (int i = 0; i < 10; i++) {        try (Socket syslogSocket = new Socket(addr.getAddress(), addr.getPort())) {            syslogSocket.getOutputStream().write(bodyWithTandH.getBytes());        }    }    List<Event> channelEvents = new ArrayList<>();    Transaction txn = channel.getTransaction();    txn.begin();    for (int i = 0; i < 10; i++) {        Event e = channel.take();        if (e == null) {            throw new NullPointerException("Event is null");        }        channelEvents.add(e);    }    try {        txn.commit();    } catch (Throwable t) {        txn.rollback();    } finally {        txn.close();    }    source.stop();    for (Event e : channelEvents) {        Assert.assertNotNull(e);        String str = new String(e.getBody(), Charsets.UTF_8);                if (keepFields.equals("true") || keepFields.equals("all")) {            Assert.assertArrayEquals(bodyWithTandH.trim().getBytes(), e.getBody());        } else if (keepFields.equals("false") || keepFields.equals("none")) {            Assert.assertArrayEquals(data1.getBytes(), e.getBody());        } else if (keepFields.equals("hostname")) {            Assert.assertArrayEquals(bodyWithHostname.getBytes(), e.getBody());        } else if (keepFields.equals("timestamp")) {            Assert.assertArrayEquals(bodyWithTimestamp.getBytes(), e.getBody());        }    }}
1
public void testKeepFields() throws IOException
{    runKeepFieldsTest("all");        runKeepFieldsTest("true");}
0
public void testRemoveFields() throws IOException
{    runKeepFieldsTest("none");        runKeepFieldsTest("false");}
0
public void testKeepHostname() throws IOException
{    runKeepFieldsTest("hostname");}
0
public void testKeepTimestamp() throws IOException
{    runKeepFieldsTest("timestamp");}
0
public void testSourceCounter() throws IOException
{    runKeepFieldsTest("all");    assertEquals(10, source.getSourceCounter().getEventAcceptedCount());    assertEquals(10, source.getSourceCounter().getEventReceivedCount());}
0
public void testSourceCounterChannelFail() throws Exception
{    init("true");    errorCounterCommon(new ChannelException("dummy"));    for (int i = 0; i < 10 && source.getSourceCounter().getChannelWriteFail() == 0; i++) {        Thread.sleep(100);    }    assertEquals(1, source.getSourceCounter().getChannelWriteFail());}
0
public void testSourceCounterEventFail() throws Exception
{    init("true");    errorCounterCommon(new RuntimeException("dummy"));    for (int i = 0; i < 10 && source.getSourceCounter().getEventReadFail() == 0; i++) {        Thread.sleep(100);    }    assertEquals(1, source.getSourceCounter().getEventReadFail());}
0
private void errorCounterCommon(Exception e) throws IOException
{    ChannelProcessor cp = Mockito.mock(ChannelProcessor.class);    doThrow(e).when(cp).processEvent(any(Event.class));    source.setChannelProcessor(cp);    source.start();        InetSocketAddress addr = source.getBoundAddress();    try (Socket syslogSocket = new Socket(addr.getAddress(), addr.getPort())) {        syslogSocket.getOutputStream().write(bodyWithTandH.getBytes());    }}
0
public void testSSLMessages() throws Exception
{    initSsl();    source.start();    InetSocketAddress address = source.getBoundAddress();    SSLContext sslContext = SSLContext.getInstance("TLS");    sslContext.init(null, new TrustManager[] { new X509TrustManager() {        @Override        public void checkClientTrusted(X509Certificate[] certs, String s) {                }        @Override        public void checkServerTrusted(X509Certificate[] certs, String s) {                }        @Override        public X509Certificate[] getAcceptedIssuers() {            return new X509Certificate[0];        }    } }, null);    SocketFactory socketFactory = sslContext.getSocketFactory();    Socket socket = socketFactory.createSocket();    socket.connect(address);    OutputStream outputStream = socket.getOutputStream();    outputStream.write(bodyWithTandH.getBytes());    socket.close();        Transaction transaction = channel.getTransaction();    transaction.begin();    Event event = channel.take();    assertEquals(new String(event.getBody()), data1);    transaction.commit();    transaction.close();}
0
public void checkClientTrusted(X509Certificate[] certs, String s)
{}
0
public void checkServerTrusted(X509Certificate[] certs, String s)
{}
0
public X509Certificate[] getAcceptedIssuers()
{    return new X509Certificate[0];}
0
public void testClientHeaders() throws IOException
{    String testClientIPHeader = "testClientIPHeader";    String testClientHostnameHeader = "testClientHostnameHeader";    Context context = new Context();    context.put("clientIPHeader", testClientIPHeader);    context.put("clientHostnameHeader", testClientHostnameHeader);    init("none", context);    source.start();        InetSocketAddress addr = source.getBoundAddress();    Socket syslogSocket = new Socket(addr.getAddress(), addr.getPort());    syslogSocket.getOutputStream().write(bodyWithTandH.getBytes());    Transaction txn = channel.getTransaction();    txn.begin();    Event e = channel.take();    try {        txn.commit();    } catch (Throwable t) {        txn.rollback();    } finally {        txn.close();    }    source.stop();    Map<String, String> headers = e.getHeaders();    checkHeader(headers, testClientIPHeader, InetAddress.getLoopbackAddress().getHostAddress());    checkHeader(headers, testClientHostnameHeader, InetAddress.getLoopbackAddress().getHostName());}
0
private static void checkHeader(Map<String, String> headers, String headerName, String expectedValue)
{    assertTrue("Missing event header: " + headerName, headers.containsKey(headerName));    assertEquals("Event header value does not match: " + headerName, expectedValue, headers.get(headerName));}
0
private void init(String keepFields)
{    init(keepFields, new Context());}
0
private void init(String keepFields, Context context)
{    source = new SyslogUDPSource();    channel = new MemoryChannel();    Configurables.configure(channel, new Context());    List<Channel> channels = new ArrayList<Channel>();    channels.add(channel);    ChannelSelector rcs = new ReplicatingChannelSelector();    rcs.setChannels(channels);    source.setChannelProcessor(new ChannelProcessor(rcs));    context.put("host", InetAddress.getLoopbackAddress().getHostAddress());    context.put("port", String.valueOf(TEST_SYSLOG_PORT));    context.put("keepFields", keepFields);    source.configure(context);}
0
private void runKeepFieldsTest(String keepFields) throws IOException
{    init(keepFields);    source.start();        DatagramPacket datagramPacket = createDatagramPacket(bodyWithTandH.getBytes());    for (int i = 0; i < 10; i++) {        sendDatagramPacket(datagramPacket);    }    List<Event> channelEvents = new ArrayList<>();    Transaction txn = channel.getTransaction();    txn.begin();    for (int i = 0; i < 10; i++) {        Event e = channel.take();        Assert.assertNotNull(e);        channelEvents.add(e);    }    commitAndCloseTransaction(txn);    source.stop();    for (Event e : channelEvents) {        Assert.assertNotNull(e);        String str = new String(e.getBody(), Charsets.UTF_8);                if (keepFields.equals("true") || keepFields.equals("all")) {            Assert.assertArrayEquals(bodyWithTandH.trim().getBytes(), e.getBody());        } else if (keepFields.equals("false") || keepFields.equals("none")) {            Assert.assertArrayEquals(data1.getBytes(), e.getBody());        } else if (keepFields.equals("hostname")) {            Assert.assertArrayEquals(bodyWithHostname.getBytes(), e.getBody());        } else if (keepFields.equals("timestamp")) {            Assert.assertArrayEquals(bodyWithTimestamp.getBytes(), e.getBody());        }    }}
1
public void testLargePayload() throws Exception
{    init("true");    source.start();        byte[] largePayload = getPayload(1000).getBytes();    DatagramPacket datagramPacket = createDatagramPacket(largePayload);    for (int i = 0; i < 10; i++) {        sendDatagramPacket(datagramPacket);    }    List<Event> channelEvents = new ArrayList<>();    Transaction txn = channel.getTransaction();    txn.begin();    for (int i = 0; i < 10; i++) {        Event e = channel.take();        Assert.assertNotNull(e);        channelEvents.add(e);    }    commitAndCloseTransaction(txn);    source.stop();    for (Event e : channelEvents) {        Assert.assertNotNull(e);        Assert.assertArrayEquals(largePayload, e.getBody());    }}
0
public void testKeepFields() throws IOException
{    runKeepFieldsTest("all");        runKeepFieldsTest("true");}
0
public void testRemoveFields() throws IOException
{    runKeepFieldsTest("none");        runKeepFieldsTest("false");}
0
public void testKeepHostname() throws IOException
{    runKeepFieldsTest("hostname");}
0
public void testKeepTimestamp() throws IOException
{    runKeepFieldsTest("timestamp");}
0
public void testSourceCounter() throws Exception
{    init("true");    doCounterCommon();        for (int i = 0; i < 10 && source.getSourceCounter().getEventAcceptedCount() == 0; i++) {        Thread.sleep(100);    }    Assert.assertEquals(1, source.getSourceCounter().getEventAcceptedCount());    Assert.assertEquals(1, source.getSourceCounter().getEventReceivedCount());}
0
private void doCounterCommon() throws IOException, InterruptedException
{    source.start();    DatagramPacket datagramPacket = createDatagramPacket("test".getBytes());    sendDatagramPacket(datagramPacket);}
0
public void testSourceCounterChannelFail() throws Exception
{    init("true");    ChannelProcessor cp = Mockito.mock(ChannelProcessor.class);    doThrow(new ChannelException("dummy")).when(cp).processEvent(any(Event.class));    source.setChannelProcessor(cp);    doCounterCommon();    for (int i = 0; i < 10 && source.getSourceCounter().getChannelWriteFail() == 0; i++) {        Thread.sleep(100);    }    Assert.assertEquals(1, source.getSourceCounter().getChannelWriteFail());}
0
public void testSourceCounterReadFail() throws Exception
{    init("true");    ChannelProcessor cp = Mockito.mock(ChannelProcessor.class);    doThrow(new RuntimeException("dummy")).when(cp).processEvent(any(Event.class));    source.setChannelProcessor(cp);    doCounterCommon();    for (int i = 0; i < 10 && source.getSourceCounter().getEventReadFail() == 0; i++) {        Thread.sleep(100);    }    Assert.assertEquals(1, source.getSourceCounter().getEventReadFail());}
0
private DatagramPacket createDatagramPacket(byte[] payload)
{    InetSocketAddress addr = source.getBoundAddress();    return new DatagramPacket(payload, payload.length, addr.getAddress(), addr.getPort());}
0
private void sendDatagramPacket(DatagramPacket datagramPacket) throws IOException
{    try (DatagramSocket syslogSocket = new DatagramSocket()) {        syslogSocket.send(datagramPacket);    }}
0
private void commitAndCloseTransaction(Transaction txn)
{    try {        txn.commit();    } catch (Throwable t) {                txn.rollback();    } finally {        txn.close();    }}
1
private String getPayload(int length)
{    StringBuilder payload = new StringBuilder(length);    for (int n = 0; n < length; ++n) {        payload.append("x");    }    return payload.toString();}
0
public void testClientHeaders() throws IOException
{    String testClientIPHeader = "testClientIPHeader";    String testClientHostnameHeader = "testClientHostnameHeader";    Context context = new Context();    context.put("clientIPHeader", testClientIPHeader);    context.put("clientHostnameHeader", testClientHostnameHeader);    init("none", context);    source.start();    DatagramPacket datagramPacket = createDatagramPacket(bodyWithTandH.getBytes());    sendDatagramPacket(datagramPacket);    Transaction txn = channel.getTransaction();    txn.begin();    Event e = channel.take();    commitAndCloseTransaction(txn);    source.stop();    Map<String, String> headers = e.getHeaders();    checkHeader(headers, testClientIPHeader, InetAddress.getLoopbackAddress().getHostAddress());    checkHeader(headers, testClientHostnameHeader, InetAddress.getLoopbackAddress().getHostName());}
0
private static void checkHeader(Map<String, String> headers, String headerName, String expectedValue)
{    assertTrue("Missing event header: " + headerName, headers.containsKey(headerName));    assertEquals("Event header value does not match: " + headerName, expectedValue, headers.get(headerName));}
0
public void TestHeader0() throws ParseException
{    String stamp1 = "2012-04-13T11:11:11";    String format1 = "yyyy-MM-dd'T'HH:mm:ssZ";    String host1 = "ubuntu-11.cloudera.com";    String data1 = "some msg";        String msg1 = "<10>" + stamp1 + "+08:00" + " " + host1 + " " + data1 + "\n";    checkHeader(msg1, stamp1 + "+0800", format1, host1, data1);}
0
public void TestHeader1() throws ParseException
{    String stamp1 = "2012-04-13T11:11:11";    String format1 = "yyyy-MM-dd'T'HH:mm:ss";    String host1 = "ubuntu-11.cloudera.com";    String data1 = "some msg";    String msg1 = "<10>1 " + stamp1 + " " + host1 + " " + data1 + "\n";    checkHeader(msg1, stamp1, format1, host1, data1);}
0
public void TestHeader2() throws ParseException
{    String stamp1 = "2012-04-13T11:11:11";    String format1 = "yyyy-MM-dd'T'HH:mm:ssZ";    String host1 = "ubuntu-11.cloudera.com";    String data1 = "some msg";        String msg1 = "<10>1 " + stamp1 + "Z" + " " + host1 + " " + data1 + "\n";    checkHeader(msg1, stamp1 + "+0000", format1, host1, data1);}
0
public void TestHeader3() throws ParseException
{    String stamp1 = "2012-04-13T11:11:11";    String format1 = "yyyy-MM-dd'T'HH:mm:ssZ";    String host1 = "ubuntu-11.cloudera.com";    String data1 = "some msg";        String msg1 = "<10>1 " + stamp1 + "+08:00" + " " + host1 + " " + data1 + "\n";    checkHeader(msg1, stamp1 + "+0800", format1, host1, data1);}
0
public void TestHeader4() throws ParseException
{    String host1 = "ubuntu-11.cloudera.com";    String data1 = "some msg";        String msg1 = "<10>1 " + "-" + " " + host1 + " " + data1 + "\n";    checkHeader(msg1, null, null, host1, data1);}
0
public void TestHeader5() throws ParseException
{    String stamp1 = "2012-04-13T11:11:11";    String format1 = "yyyy-MM-dd'T'HH:mm:ss";    String host1 = "-";    String data1 = "some msg";        String msg1 = "<10>1 " + stamp1 + " " + host1 + " " + data1 + "\n";    checkHeader(msg1, stamp1, format1, null, data1);}
0
public void TestHeader6() throws ParseException
{    String stamp1 = "2012-04-13T11:11:11";    String format1 = "yyyy-MM-dd'T'HH:mm:ssZ";    String host1 = "-";    String data1 = "some msg";        String msg1 = "<10>1 " + stamp1 + "Z" + " " + host1 + " " + data1 + "\n";    checkHeader(msg1, stamp1 + "+0000", format1, null, data1);}
0
public void TestHeader7() throws ParseException
{    String stamp1 = "2012-04-13T11:11:11";    String format1 = "yyyy-MM-dd'T'HH:mm:ssZ";    String host1 = "-";    String data1 = "some msg";        String msg1 = "<10>1 " + stamp1 + "+08:00" + " " + host1 + " " + data1 + "\n";    checkHeader(msg1, stamp1 + "+0800", format1, null, data1);}
0
public void TestHeader8() throws ParseException
{    String stamp1 = "2012-04-13T11:11:11.999";    String format1 = "yyyy-MM-dd'T'HH:mm:ss.S";    String host1 = "ubuntu-11.cloudera.com";    String data1 = "some msg";    String msg1 = "<10>1 " + stamp1 + " " + host1 + " " + data1 + "\n";    checkHeader(msg1, stamp1, format1, host1, data1);}
0
public void TestHeader9() throws ParseException
{    SimpleDateFormat sdf = new SimpleDateFormat("MMM  d hh:MM:ss", Locale.ENGLISH);    Calendar cal = Calendar.getInstance();    String year = String.valueOf(cal.get(Calendar.YEAR));    String stamp1 = sdf.format(cal.getTime());    String format1 = "yyyyMMM d HH:mm:ss";    String host1 = "ubuntu-11.cloudera.com";    String data1 = "some msg";        String msg1 = "<10>" + stamp1 + " " + host1 + " " + data1 + "\n";    checkHeader(msg1, year + stamp1, format1, host1, data1);}
0
public void TestHeader10() throws ParseException
{    SimpleDateFormat sdf = new SimpleDateFormat("MMM  d hh:MM:ss", Locale.ENGLISH);    Calendar cal = Calendar.getInstance();    String year = String.valueOf(cal.get(Calendar.YEAR));    String stamp1 = sdf.format(cal.getTime());    String format1 = "yyyyMMM d HH:mm:ss";    String host1 = "ubuntu-11.cloudera.com";    String data1 = "some msg";        String msg1 = "<10>" + stamp1 + " " + host1 + " " + data1 + "\n";    checkHeader(msg1, year + stamp1, format1, host1, data1);}
0
public void TestHeader11() throws ParseException
{            String inputStamp = "2014-10-03T17:20:01.123456-07:00";    String outputStamp = "2014-10-03T17:20:01.123-07:00";    String format1 = "yyyy-MM-dd'T'HH:mm:ss.S";    String host1 = "ubuntu-11.cloudera.com";    String data1 = "some msg";    String msg1 = "<10>" + inputStamp + " " + host1 + " " + data1 + "\n";    checkHeader(msg1, outputStamp, format1, host1, data1);}
0
public void TestRfc3164HeaderApacheLogWithNulls() throws ParseException
{    SimpleDateFormat sdf = new SimpleDateFormat("MMM  d hh:MM:ss", Locale.ENGLISH);    Calendar cal = Calendar.getInstance();    String year = String.valueOf(cal.get(Calendar.YEAR));    String stamp1 = sdf.format(cal.getTime());    String format1 = "yyyyMMM d HH:mm:ss";    String host1 = "ubuntu-11.cloudera.com";    String data1 = "- hyphen_null_breaks_5424_pattern [07/Jun/2012:14:46:44 -0600]";    String msg1 = "<10>" + stamp1 + " " + host1 + " " + data1 + "\n";    checkHeader(msg1, year + stamp1, format1, host1, data1);}
0
public void TestRfc3164Dates() throws ParseException
{        for (int monthOffset = 0; monthOffset <= 13; monthOffset++) {        Clock mockClock = Clock.fixed(LocalDateTime.now().plusMonths(monthOffset).toInstant(ZoneOffset.UTC), Clock.systemDefaultZone().getZone());                for (int i = -10; i <= 1; i++) {            SimpleDateFormat sdf = new SimpleDateFormat("MMM  d hh:MM:ss", Locale.ENGLISH);            Date date = new Date(mockClock.millis());            Calendar cal = Calendar.getInstance();            cal.setTime(date);            cal.add(Calendar.MONTH, i);                        if (i == 1) {                cal.add(Calendar.DAY_OF_MONTH, -1);            }            String stamp1 = sdf.format(cal.getTime());            String year = String.valueOf(cal.get(Calendar.YEAR));            String format1 = "yyyyMMM d HH:mm:ss";            String host1 = "ubuntu-11.cloudera.com";            String data1 = "some msg";                        String msg1 = "<10>" + stamp1 + " " + host1 + " " + data1 + "\n";            checkHeader(msg1, year + stamp1, format1, host1, data1, mockClock);        }    }}
0
public static void checkHeader(String keepFields, String msg1, String stamp1, String format1, String host1, String data1, Clock clock) throws ParseException
{    SyslogUtils util;    if (keepFields == null || keepFields.isEmpty()) {        util = new SyslogUtils(SyslogUtils.DEFAULT_SIZE, new HashSet<String>(), false, clock);    } else {        util = new SyslogUtils(SyslogUtils.DEFAULT_SIZE, SyslogUtils.chooseFieldsToKeep(keepFields), false, clock);    }    ChannelBuffer buff = ChannelBuffers.buffer(200);    buff.writeBytes(msg1.getBytes());    Event e = util.extractEvent(buff);    if (e == null) {        throw new NullPointerException("Event is null");    }    Map<String, String> headers2 = e.getHeaders();    if (stamp1 == null) {        Assert.assertFalse(headers2.containsKey("timestamp"));    } else {        SimpleDateFormat formater = new SimpleDateFormat(format1, Locale.ENGLISH);        Assert.assertEquals(String.valueOf(formater.parse(stamp1).getTime()), headers2.get("timestamp"));    }    if (host1 == null) {        Assert.assertFalse(headers2.containsKey("host"));    } else {        String host2 = headers2.get("host");        Assert.assertEquals(host2, host1);    }    Assert.assertEquals(data1, new String(e.getBody()));}
0
public static void checkHeader(String keepFields, String msg1, String stamp1, String format1, String host1, String data1) throws ParseException
{    checkHeader(keepFields, msg1, stamp1, format1, host1, data1, Clock.system(Clock.systemDefaultZone().getZone()));}
0
public static void checkHeader(String msg1, String stamp1, String format1, String host1, String data1, Clock clock) throws ParseException
{    checkHeader("none", msg1, stamp1, format1, host1, data1, clock);}
0
public static void checkHeader(String msg1, String stamp1, String format1, String host1, String data1) throws ParseException
{    checkHeader("none", msg1, stamp1, format1, host1, data1, Clock.system(Clock.systemDefaultZone().getZone()));}
0
public void testExtractBadEvent1()
{    String badData1 = "<10F> bad bad data\n";    SyslogUtils util = new SyslogUtils(false);    ChannelBuffer buff = ChannelBuffers.buffer(100);    buff.writeBytes(badData1.getBytes());    Event e = util.extractEvent(buff);    if (e == null) {        throw new NullPointerException("Event is null");    }    Map<String, String> headers = e.getHeaders();    Assert.assertEquals("0", headers.get(SyslogUtils.SYSLOG_FACILITY));    Assert.assertEquals("0", headers.get(SyslogUtils.SYSLOG_SEVERITY));    Assert.assertEquals(SyslogUtils.SyslogStatus.INVALID.getSyslogStatus(), headers.get(SyslogUtils.EVENT_STATUS));    Assert.assertEquals(badData1.trim(), new String(e.getBody()).trim());}
0
public void testExtractBadEvent2()
{    String badData1 = "hi guys! <10> bad bad data\n";    SyslogUtils util = new SyslogUtils(false);    ChannelBuffer buff = ChannelBuffers.buffer(100);    buff.writeBytes(badData1.getBytes());    Event e = util.extractEvent(buff);    if (e == null) {        throw new NullPointerException("Event is null");    }    Map<String, String> headers = e.getHeaders();    Assert.assertEquals("0", headers.get(SyslogUtils.SYSLOG_FACILITY));    Assert.assertEquals("0", headers.get(SyslogUtils.SYSLOG_SEVERITY));    Assert.assertEquals(SyslogUtils.SyslogStatus.INVALID.getSyslogStatus(), headers.get(SyslogUtils.EVENT_STATUS));    Assert.assertEquals(badData1.trim(), new String(e.getBody()).trim());}
0
public void testExtractBadEvent3()
{    String badData1 = "<> bad bad data\n";    SyslogUtils util = new SyslogUtils(false);    ChannelBuffer buff = ChannelBuffers.buffer(100);    buff.writeBytes(badData1.getBytes());    Event e = util.extractEvent(buff);    if (e == null) {        throw new NullPointerException("Event is null");    }    Map<String, String> headers = e.getHeaders();    Assert.assertEquals("0", headers.get(SyslogUtils.SYSLOG_FACILITY));    Assert.assertEquals("0", headers.get(SyslogUtils.SYSLOG_SEVERITY));    Assert.assertEquals(SyslogUtils.SyslogStatus.INVALID.getSyslogStatus(), headers.get(SyslogUtils.EVENT_STATUS));    Assert.assertEquals(badData1.trim(), new String(e.getBody()).trim());}
0
public void testExtractBadEvent4()
{    String badData1 = "<123123123123123123123123123123> bad bad data\n";    SyslogUtils util = new SyslogUtils(false);    ChannelBuffer buff = ChannelBuffers.buffer(100);    buff.writeBytes(badData1.getBytes());    Event e = util.extractEvent(buff);    if (e == null) {        throw new NullPointerException("Event is null");    }    Map<String, String> headers = e.getHeaders();    Assert.assertEquals("0", headers.get(SyslogUtils.SYSLOG_FACILITY));    Assert.assertEquals("0", headers.get(SyslogUtils.SYSLOG_SEVERITY));    Assert.assertEquals(SyslogUtils.SyslogStatus.INVALID.getSyslogStatus(), headers.get(SyslogUtils.EVENT_STATUS));    Assert.assertEquals(badData1.trim(), new String(e.getBody()).trim());}
0
public void testExtractGoodEvent()
{    String priority = "<10>";    String goodData1 = "Good good good data\n";    SyslogUtils util = new SyslogUtils(false);    ChannelBuffer buff = ChannelBuffers.buffer(100);    buff.writeBytes((priority + goodData1).getBytes());    Event e = util.extractEvent(buff);    if (e == null) {        throw new NullPointerException("Event is null");    }    Map<String, String> headers = e.getHeaders();    Assert.assertEquals("1", headers.get(SyslogUtils.SYSLOG_FACILITY));    Assert.assertEquals("2", headers.get(SyslogUtils.SYSLOG_SEVERITY));    Assert.assertEquals(null, headers.get(SyslogUtils.EVENT_STATUS));    Assert.assertEquals(priority + goodData1.trim(), new String(e.getBody()).trim());}
0
public void testBadEventGoodEvent()
{    String badData1 = "hi guys! <10F> bad bad data\n";    SyslogUtils util = new SyslogUtils(false);    ChannelBuffer buff = ChannelBuffers.buffer(100);    buff.writeBytes(badData1.getBytes());    String priority = "<10>";    String goodData1 = "Good good good data\n";    buff.writeBytes((priority + goodData1).getBytes());    Event e = util.extractEvent(buff);    if (e == null) {        throw new NullPointerException("Event is null");    }    Map<String, String> headers = e.getHeaders();    Assert.assertEquals("0", headers.get(SyslogUtils.SYSLOG_FACILITY));    Assert.assertEquals("0", headers.get(SyslogUtils.SYSLOG_SEVERITY));    Assert.assertEquals(SyslogUtils.SyslogStatus.INVALID.getSyslogStatus(), headers.get(SyslogUtils.EVENT_STATUS));    Assert.assertEquals(badData1.trim(), new String(e.getBody()).trim());    Event e2 = util.extractEvent(buff);    if (e2 == null) {        throw new NullPointerException("Event is null");    }    Map<String, String> headers2 = e2.getHeaders();    Assert.assertEquals("1", headers2.get(SyslogUtils.SYSLOG_FACILITY));    Assert.assertEquals("2", headers2.get(SyslogUtils.SYSLOG_SEVERITY));    Assert.assertEquals(null, headers2.get(SyslogUtils.EVENT_STATUS));    Assert.assertEquals(priority + goodData1.trim(), new String(e2.getBody()).trim());}
0
public void testGoodEventBadEvent()
{    String badData1 = "hi guys! <10F> bad bad data\n";    String priority = "<10>";    String goodData1 = "Good good good data\n";    SyslogUtils util = new SyslogUtils(false);    ChannelBuffer buff = ChannelBuffers.buffer(100);    buff.writeBytes((priority + goodData1).getBytes());    buff.writeBytes(badData1.getBytes());    Event e2 = util.extractEvent(buff);    if (e2 == null) {        throw new NullPointerException("Event is null");    }    Map<String, String> headers2 = e2.getHeaders();    Assert.assertEquals("1", headers2.get(SyslogUtils.SYSLOG_FACILITY));    Assert.assertEquals("2", headers2.get(SyslogUtils.SYSLOG_SEVERITY));    Assert.assertEquals(null, headers2.get(SyslogUtils.EVENT_STATUS));    Assert.assertEquals(priority + goodData1.trim(), new String(e2.getBody()).trim());    Event e = util.extractEvent(buff);    if (e == null) {        throw new NullPointerException("Event is null");    }    Map<String, String> headers = e.getHeaders();    Assert.assertEquals("0", headers.get(SyslogUtils.SYSLOG_FACILITY));    Assert.assertEquals("0", headers.get(SyslogUtils.SYSLOG_SEVERITY));    Assert.assertEquals(SyslogUtils.SyslogStatus.INVALID.getSyslogStatus(), headers.get(SyslogUtils.EVENT_STATUS));    Assert.assertEquals(badData1.trim(), new String(e.getBody()).trim());}
0
public void testBadEventBadEvent()
{    String badData1 = "hi guys! <10F> bad bad data\n";    SyslogUtils util = new SyslogUtils(false);    ChannelBuffer buff = ChannelBuffers.buffer(100);    buff.writeBytes(badData1.getBytes());    String badData2 = "hi guys! <20> bad bad data\n";    buff.writeBytes((badData2).getBytes());    Event e = util.extractEvent(buff);    if (e == null) {        throw new NullPointerException("Event is null");    }    Map<String, String> headers = e.getHeaders();    Assert.assertEquals("0", headers.get(SyslogUtils.SYSLOG_FACILITY));    Assert.assertEquals("0", headers.get(SyslogUtils.SYSLOG_SEVERITY));    Assert.assertEquals(SyslogUtils.SyslogStatus.INVALID.getSyslogStatus(), headers.get(SyslogUtils.EVENT_STATUS));    Assert.assertEquals(badData1.trim(), new String(e.getBody()).trim());    Event e2 = util.extractEvent(buff);    if (e2 == null) {        throw new NullPointerException("Event is null");    }    Map<String, String> headers2 = e2.getHeaders();    Assert.assertEquals("0", headers2.get(SyslogUtils.SYSLOG_FACILITY));    Assert.assertEquals("0", headers2.get(SyslogUtils.SYSLOG_SEVERITY));    Assert.assertEquals(SyslogUtils.SyslogStatus.INVALID.getSyslogStatus(), headers2.get(SyslogUtils.EVENT_STATUS));    Assert.assertEquals(badData2.trim(), new String(e2.getBody()).trim());}
0
public void testGoodEventGoodEvent()
{    String priority = "<10>";    String goodData1 = "Good good good data\n";    SyslogUtils util = new SyslogUtils(false);    ChannelBuffer buff = ChannelBuffers.buffer(100);    buff.writeBytes((priority + goodData1).getBytes());    String priority2 = "<20>";    String goodData2 = "Good really good data\n";    buff.writeBytes((priority2 + goodData2).getBytes());    Event e = util.extractEvent(buff);    if (e == null) {        throw new NullPointerException("Event is null");    }    Map<String, String> headers = e.getHeaders();    Assert.assertEquals("1", headers.get(SyslogUtils.SYSLOG_FACILITY));    Assert.assertEquals("2", headers.get(SyslogUtils.SYSLOG_SEVERITY));    Assert.assertEquals(null, headers.get(SyslogUtils.EVENT_STATUS));    Assert.assertEquals(priority + goodData1.trim(), new String(e.getBody()).trim());    Event e2 = util.extractEvent(buff);    if (e2 == null) {        throw new NullPointerException("Event is null");    }    Map<String, String> headers2 = e2.getHeaders();    Assert.assertEquals("2", headers2.get(SyslogUtils.SYSLOG_FACILITY));    Assert.assertEquals("4", headers2.get(SyslogUtils.SYSLOG_SEVERITY));    Assert.assertEquals(null, headers.get(SyslogUtils.EVENT_STATUS));    Assert.assertEquals(priority2 + goodData2.trim(), new String(e2.getBody()).trim());}
0
public void testExtractBadEventLarge()
{    String badData1 = "<10> bad bad data bad bad\n";        SyslogUtils util = new SyslogUtils(5, null, false);    ChannelBuffer buff = ChannelBuffers.buffer(100);    buff.writeBytes(badData1.getBytes());    Event e = util.extractEvent(buff);    if (e == null) {        throw new NullPointerException("Event is null");    }    Map<String, String> headers = e.getHeaders();    Assert.assertEquals("1", headers.get(SyslogUtils.SYSLOG_FACILITY));    Assert.assertEquals("2", headers.get(SyslogUtils.SYSLOG_SEVERITY));    Assert.assertEquals(SyslogUtils.SyslogStatus.INCOMPLETE.getSyslogStatus(), headers.get(SyslogUtils.EVENT_STATUS));    Assert.assertEquals("<10> bad b".trim(), new String(e.getBody()).trim());    Event e2 = util.extractEvent(buff);    if (e2 == null) {        throw new NullPointerException("Event is null");    }    Map<String, String> headers2 = e2.getHeaders();    Assert.assertEquals("0", headers2.get(SyslogUtils.SYSLOG_FACILITY));    Assert.assertEquals("0", headers2.get(SyslogUtils.SYSLOG_SEVERITY));    Assert.assertEquals(SyslogUtils.SyslogStatus.INVALID.getSyslogStatus(), headers2.get(SyslogUtils.EVENT_STATUS));    Assert.assertEquals("ad data ba".trim(), new String(e2.getBody()).trim());}
0
public void testKeepFields() throws Exception
{    String stamp1 = "2012-04-13T11:11:11";    String format1 = "yyyy-MM-dd'T'HH:mm:ssZ";    String host1 = "ubuntu-11.cloudera.com";    String data1 = "some msg";        String msg1 = "<10>1 " + stamp1 + "+08:00" + " " + host1 + " " + data1 + "\n";    checkHeader("none", msg1, stamp1 + "+0800", format1, host1, data1);    checkHeader("false", msg1, stamp1 + "+0800", format1, host1, data1);    String data2 = "ubuntu-11.cloudera.com some msg";    checkHeader("hostname", msg1, stamp1 + "+0800", format1, host1, data2);    String data3 = "2012-04-13T11:11:11+08:00 ubuntu-11.cloudera.com some msg";    checkHeader("timestamp hostname", msg1, stamp1 + "+0800", format1, host1, data3);    String data4 = "<10>2012-04-13T11:11:11+08:00 ubuntu-11.cloudera.com some msg";    checkHeader("priority timestamp hostname", msg1, stamp1 + "+0800", format1, host1, data4);    String data5 = "<10>1 2012-04-13T11:11:11+08:00 ubuntu-11.cloudera.com some msg";    checkHeader("priority version timestamp hostname", msg1, stamp1 + "+0800", format1, host1, data5);    checkHeader("all", msg1, stamp1 + "+0800", format1, host1, data5);    checkHeader("true", msg1, stamp1 + "+0800", format1, host1, data5);}
0
public void testGetIPWhenSuccessful()
{    SocketAddress socketAddress = new InetSocketAddress("localhost", 2000);    String ip = SyslogUtils.getIP(socketAddress);    assertEquals("127.0.0.1", ip);}
0
public void testGetIPWhenInputIsNull()
{    SocketAddress socketAddress = null;    String ip = SyslogUtils.getIP(socketAddress);    assertEquals("", ip);}
0
public void testGetIPWhenInputIsNotInetSocketAddress()
{    SocketAddress socketAddress = new SocketAddress() {    };    String ip = SyslogUtils.getIP(socketAddress);    assertEquals("", ip);}
0
public void testGetHostnameWhenSuccessful()
{    SocketAddress socketAddress = new InetSocketAddress("127.0.0.1", 2000);    String hostname = SyslogUtils.getHostname(socketAddress);    assertEquals("localhost", hostname);}
0
public void testGetHostnameWhenInputIsNull()
{    SocketAddress socketAddress = null;    String hostname = SyslogUtils.getHostname(socketAddress);    assertEquals("", hostname);}
0
public void testGetHostnameWhenInputIsNotInetSocketAddress()
{    SocketAddress socketAddress = new SocketAddress() {    };    String hostname = SyslogUtils.getHostname(socketAddress);    assertEquals("", hostname);}
0
public void setUp() throws IOException
{    try (ServerSocket socket = new ServerSocket(0)) {        port = socket.getLocalPort();    }    props.clear();    props.setProperty("hosts", "h1");    props.setProperty("hosts.h1", "0.0.0.0:" + String.valueOf(port));    props.setProperty(RpcClientConfigurationConstants.CONFIG_BATCH_SIZE, "10");    props.setProperty(RpcClientConfigurationConstants.CONFIG_REQUEST_TIMEOUT, "2000");    channel = new MemoryChannel();    source = new ThriftSource();}
0
public void stop() throws Exception
{    source.stop();}
0
private void configureSource()
{    List<Channel> channels = new ArrayList<Channel>();    channels.add(channel);    ChannelSelector rcs = new ReplicatingChannelSelector();    rcs.setChannels(channels);    source.setChannelProcessor(new ChannelProcessor(rcs));}
0
public void testAppendSSLWithComponentKeystore() throws Exception
{    Context context = new Context();    channel.configure(context);    configureSource();    context.put(ThriftSource.CONFIG_BIND, "0.0.0.0");    context.put(ThriftSource.CONFIG_PORT, String.valueOf(port));    context.put("ssl", "true");    context.put("keystore", "src/test/resources/keystorefile.jks");    context.put("keystore-password", "password");    context.put("keystore-type", "JKS");    Configurables.configure(source, context);    doAppendSSL();}
0
public void testAppendSSLWithGlobalKeystore() throws Exception
{    System.setProperty("javax.net.ssl.keyStore", "src/test/resources/keystorefile.jks");    System.setProperty("javax.net.ssl.keyStorePassword", "password");    System.setProperty("javax.net.ssl.keyStoreType", "JKS");    Context context = new Context();    channel.configure(context);    configureSource();    context.put(ThriftSource.CONFIG_BIND, "0.0.0.0");    context.put(ThriftSource.CONFIG_PORT, String.valueOf(port));    context.put("ssl", "true");    Configurables.configure(source, context);    doAppendSSL();    System.clearProperty("javax.net.ssl.keyStore");    System.clearProperty("javax.net.ssl.keyStorePassword");    System.clearProperty("javax.net.ssl.keyStoreType");}
0
private void doAppendSSL() throws EventDeliveryException
{    Properties sslprops = (Properties) props.clone();    sslprops.put("ssl", "true");    sslprops.put("truststore", "src/test/resources/truststorefile.jks");    sslprops.put("truststore-password", "password");    client = RpcClientFactory.getThriftInstance(sslprops);    source.start();    for (int i = 0; i < 30; i++) {        client.append(EventBuilder.withBody(String.valueOf(i).getBytes()));    }    Transaction transaction = channel.getTransaction();    transaction.begin();    for (int i = 0; i < 30; i++) {        Event event = channel.take();        Assert.assertNotNull(event);        Assert.assertEquals(String.valueOf(i), new String(event.getBody()));    }    transaction.commit();    transaction.close();}
0
public void testAppend() throws Exception
{    client = RpcClientFactory.getThriftInstance(props);    Context context = new Context();    channel.configure(context);    configureSource();    context.put(ThriftSource.CONFIG_BIND, "0.0.0.0");    context.put(ThriftSource.CONFIG_PORT, String.valueOf(port));    Configurables.configure(source, context);    source.start();    for (int i = 0; i < 30; i++) {        client.append(EventBuilder.withBody(String.valueOf(i).getBytes()));    }    Transaction transaction = channel.getTransaction();    transaction.begin();    for (int i = 0; i < 30; i++) {        Event event = channel.take();        Assert.assertNotNull(event);        Assert.assertEquals(String.valueOf(i), new String(event.getBody()));    }    transaction.commit();    transaction.close();}
0
public void testAppendBatch() throws Exception
{    client = RpcClientFactory.getThriftInstance(props);    Context context = new Context();    context.put("capacity", "1000");    context.put("transactionCapacity", "1000");    channel.configure(context);    configureSource();    context.put(ThriftSource.CONFIG_BIND, "0.0.0.0");    context.put(ThriftSource.CONFIG_PORT, String.valueOf(port));    Configurables.configure(source, context);    source.start();    for (int i = 0; i < 30; i++) {        List<Event> events = Lists.newArrayList();        for (int j = 0; j < 10; j++) {            Map<String, String> hdrs = Maps.newHashMap();            hdrs.put("time", String.valueOf(System.currentTimeMillis()));            events.add(EventBuilder.withBody(String.valueOf(i).getBytes(), hdrs));        }        client.appendBatch(events);    }    Transaction transaction = channel.getTransaction();    transaction.begin();    long after = System.currentTimeMillis();    List<Integer> events = Lists.newArrayList();    for (int i = 0; i < 300; i++) {        Event event = channel.take();        Assert.assertNotNull(event);        Assert.assertTrue(Long.valueOf(event.getHeaders().get("time")) <= after);        events.add(Integer.parseInt(new String(event.getBody())));    }    transaction.commit();    transaction.close();    Collections.sort(events);    int index = 0;        for (int i = 0; i < 30; i++) {        for (int j = 0; j < 10; j++) {            Assert.assertEquals(i, events.get(index++).intValue());        }    }}
0
public void testAppendBigBatch() throws Exception
{    client = RpcClientFactory.getThriftInstance(props);    Context context = new Context();    context.put("capacity", "3000");    context.put("transactionCapacity", "3000");    channel.configure(context);    configureSource();    context.put(ThriftSource.CONFIG_BIND, "0.0.0.0");    context.put(ThriftSource.CONFIG_PORT, String.valueOf(port));    Configurables.configure(source, context);    source.start();    for (int i = 0; i < 5; i++) {        List<Event> events = Lists.newArrayList();        for (int j = 0; j < 500; j++) {            Map<String, String> hdrs = Maps.newHashMap();            hdrs.put("time", String.valueOf(System.currentTimeMillis()));            events.add(EventBuilder.withBody(String.valueOf(i).getBytes(), hdrs));        }        client.appendBatch(events);    }    Transaction transaction = channel.getTransaction();    transaction.begin();    long after = System.currentTimeMillis();    List<Integer> events = Lists.newArrayList();    for (int i = 0; i < 2500; i++) {        Event event = channel.take();        Assert.assertNotNull(event);        Assert.assertTrue(Long.valueOf(event.getHeaders().get("time")) < after);        events.add(Integer.parseInt(new String(event.getBody())));    }    transaction.commit();    transaction.close();    Collections.sort(events);    int index = 0;        for (int i = 0; i < 5; i++) {        for (int j = 0; j < 500; j++) {            Assert.assertEquals(i, events.get(index++).intValue());        }    }}
0
public void testMultipleClients() throws Exception
{    ExecutorService submitter = Executors.newCachedThreadPool();    client = RpcClientFactory.getThriftInstance(props);    Context context = new Context();    context.put("capacity", "1000");    context.put("transactionCapacity", "1000");    channel.configure(context);    configureSource();    context.put(ThriftSource.CONFIG_BIND, "0.0.0.0");    context.put(ThriftSource.CONFIG_PORT, String.valueOf(port));    Configurables.configure(source, context);    source.start();    ExecutorCompletionService<Void> completionService = new ExecutorCompletionService<>(submitter);    for (int i = 0; i < 30; i++) {        completionService.submit(new SubmitHelper(i), null);    }    for (int i = 0; i < 30; i++) {        completionService.take();    }    Transaction transaction = channel.getTransaction();    transaction.begin();    long after = System.currentTimeMillis();    List<Integer> events = Lists.newArrayList();    for (int i = 0; i < 300; i++) {        Event event = channel.take();        Assert.assertNotNull(event);        Assert.assertTrue(Long.valueOf(event.getHeaders().get("time")) < after);        events.add(Integer.parseInt(new String(event.getBody())));    }    transaction.commit();    transaction.close();    Collections.sort(events);    int index = 0;        for (int i = 0; i < 30; i++) {        for (int j = 0; j < 10; j++) {            Assert.assertEquals(i, events.get(index++).intValue());        }    }}
0
public void testErrorCounterChannelWriteFail() throws Exception
{    client = RpcClientFactory.getThriftInstance(props);    Context context = new Context();    context.put(ThriftSource.CONFIG_BIND, "0.0.0.0");    context.put(ThriftSource.CONFIG_PORT, String.valueOf(port));    source.configure(context);    ChannelProcessor cp = Mockito.mock(ChannelProcessor.class);    doThrow(new ChannelException("dummy")).when(cp).processEvent(any(Event.class));    doThrow(new ChannelException("dummy")).when(cp).processEventBatch(anyListOf(Event.class));    source.setChannelProcessor(cp);    source.start();    Event event = EventBuilder.withBody("hello".getBytes());    try {        client.append(event);    } catch (EventDeliveryException e) {        }    try {        client.appendBatch(Arrays.asList(event));    } catch (EventDeliveryException e) {        }    SourceCounter sc = (SourceCounter) Whitebox.getInternalState(source, "sourceCounter");    Assert.assertEquals(2, sc.getChannelWriteFail());    source.stop();}
0
public void run()
{    List<Event> events = Lists.newArrayList();    for (int j = 0; j < 10; j++) {        Map<String, String> hdrs = Maps.newHashMap();        hdrs.put("time", String.valueOf(System.currentTimeMillis()));        events.add(EventBuilder.withBody(String.valueOf(i).getBytes(), hdrs));    }    try {        client.appendBatch(events);    } catch (EventDeliveryException e) {        throw new FlumeException(e);    }}
0
public void setUp()
{    context = new Context();}
0
public void testPutGet()
{    assertEquals("Context is empty", 0, context.getParameters().size());    context.put("test", "value");    assertEquals("value", context.getString("test"));    context.clear();    assertNull(context.getString("test"));    assertEquals("value", context.getString("test", "value"));    context.put("test", "true");    assertEquals(new Boolean(true), context.getBoolean("test"));    context.clear();    assertNull(context.getBoolean("test"));    assertEquals(new Boolean(true), context.getBoolean("test", true));    context.put("test", "1");    assertEquals(new Integer(1), context.getInteger("test"));    context.clear();    assertNull(context.getInteger("test"));    assertEquals(new Integer(1), context.getInteger("test", 1));    context.put("test", String.valueOf(Long.MAX_VALUE));    assertEquals(new Long(Long.MAX_VALUE), context.getLong("test"));    context.clear();    assertNull(context.getLong("test"));    assertEquals(new Long(Long.MAX_VALUE), context.getLong("test", Long.MAX_VALUE));    context.put("test", "0.1");    assertEquals(new Float(0.1), context.getFloat("test"));    context.clear();    assertNull(context.getFloat("test"));    assertEquals(new Float(1.1), context.getFloat("test", 1.1F));    context.put("test", "0.1");    assertEquals(new Double(0.1), context.getDouble("test"));    context.clear();    assertNull(context.getDouble("test"));    assertEquals(new Double(1.1), context.getDouble("test", 1.1));}
0
public void testSubProperties()
{    context.put("my.key", "1");    context.put("otherKey", "otherValue");    assertEquals(ImmutableMap.of("key", "1"), context.getSubProperties("my."));}
0
public void testClear()
{    context.put("test", "1");    context.clear();    assertNull(context.getInteger("test"));}
0
public void testPutAll()
{    context.putAll(ImmutableMap.of("test", "1"));    assertEquals("1", context.getString("test"));}
0
public void setUp()
{    counterGroup = new CounterGroup();}
0
public void testGetCounter()
{    AtomicLong counter = counterGroup.getCounter("test");    Assert.assertNotNull(counter);    Assert.assertEquals(0, counter.get());}
0
public void testGet()
{    long value = counterGroup.get("test");    Assert.assertEquals(0, value);}
0
public void testIncrementAndGet()
{    long value = counterGroup.incrementAndGet("test");    Assert.assertEquals(1, value);}
0
public void testAddAndGet()
{    long value = counterGroup.addAndGet("test", 13L);    Assert.assertEquals(13, value);}
0
public void testIntConfiguration()
{    Map<String, String> props = new HashMap<String, String>();    Random random = new Random();    int intValue = random.nextInt(Integer.MAX_VALUE - 1) + 1;    props.put(testPrefix + "testInt", Integer.toString(intValue));    Context context = new Context(props);    TestBean bean = new TestBean();    Assert.assertEquals(0, bean.getTestInt());    FlumeBeanConfigurator.setConfigurationFields(bean, context);    Assert.assertEquals(intValue, bean.getTestInt());}
0
public void testShortConfiguration()
{    Map<String, String> props = new HashMap<String, String>();    Random random = new Random();    short shortValue = (short) (random.nextInt(Short.MAX_VALUE - 1) + 1);    props.put(testPrefix + "testShort", Short.toString(shortValue));    Context context = new Context(props);    TestBean bean = new TestBean();    Assert.assertEquals(0, bean.getTestShort());    FlumeBeanConfigurator.setConfigurationFields(bean, context);    Assert.assertEquals(shortValue, bean.getTestShort());}
0
public void testLongConfiguration()
{    Map<String, String> props = new HashMap<String, String>();    long longValue = ThreadLocalRandom.current().nextLong(Integer.MAX_VALUE, Long.MAX_VALUE);    props.put(testPrefix + "testLong", Long.toString(longValue));    Context context = new Context(props);    TestBean bean = new TestBean();    Assert.assertEquals(0, bean.getTestLong());    FlumeBeanConfigurator.setConfigurationFields(bean, context);    Assert.assertEquals(longValue, bean.getTestLong());}
0
public void testByteConfiguration()
{    Map<String, String> props = new HashMap<String, String>();    Random random = new Random();    byte byteValue = (byte) (random.nextInt(Byte.MAX_VALUE - 1) + 1);    props.put(testPrefix + "testByte", Byte.toString(byteValue));    Context context = new Context(props);    TestBean bean = new TestBean();    Assert.assertEquals(0, bean.getTestByte());    FlumeBeanConfigurator.setConfigurationFields(bean, context);    Assert.assertEquals(byteValue, bean.getTestByte());}
0
public void testBooleanConfiguration()
{    Map<String, String> props = new HashMap<String, String>();    props.put(testPrefix + "testBoolean", "true");    Context context = new Context(props);    TestBean bean = new TestBean();    Assert.assertEquals(false, bean.getTestBoolean());    FlumeBeanConfigurator.setConfigurationFields(bean, context);    Assert.assertEquals(true, bean.getTestBoolean());}
0
public void testDoubleConfiguration()
{    Map<String, String> props = new HashMap<String, String>();    Random random = new Random();    double doubleValue = random.nextDouble();    props.put(testPrefix + "testDouble", Double.toString(doubleValue));    Context context = new Context(props);    TestBean bean = new TestBean();    Assert.assertEquals(0.0d, bean.getTestDouble());    FlumeBeanConfigurator.setConfigurationFields(bean, context);    Assert.assertEquals(doubleValue, bean.getTestDouble());}
0
public void testFloatConfiguration()
{    Map<String, String> props = new HashMap<String, String>();    Random random = new Random();    float floatValue = random.nextFloat();    props.put(testPrefix + "testFloat", Float.toString(floatValue));    Context context = new Context(props);    TestBean bean = new TestBean();    Assert.assertEquals(0.0f, bean.getTestFloat());    FlumeBeanConfigurator.setConfigurationFields(bean, context);    Assert.assertEquals(floatValue, bean.getTestFloat());}
0
public void testStringConfiguration()
{    Map<String, String> props = new HashMap<String, String>();    String stringValue = UUID.randomUUID().toString();    props.put(testPrefix + "testString", stringValue);    Context context = new Context(props);    TestBean bean = new TestBean();    Assert.assertEquals("", bean.getTestString());    FlumeBeanConfigurator.setConfigurationFields(bean, context);    Assert.assertEquals(stringValue, bean.getTestString());}
0
public void testPrivateConfiguration()
{    Map<String, String> props = new HashMap<String, String>();    Random random = new Random();    int intValue = random.nextInt(Integer.MAX_VALUE - 1) + 1;    props.put(testPrefix + "privateInt", Integer.toString(intValue));    Context context = new Context(props);    TestBean bean = new TestBean();    Assert.assertEquals(0, bean.getPrivateInt());    FlumeBeanConfigurator.setConfigurationFields(bean, context);    Assert.assertTrue(bean.getPrivateInt() != intValue);}
0
public int getTestInt()
{    return testInt;}
0
public void setTestInt(int testInt)
{    this.testInt = testInt;}
0
public short getTestShort()
{    return testShort;}
0
public void setTestShort(short testShort)
{    this.testShort = testShort;}
0
public long getTestLong()
{    return testLong;}
0
public void setTestLong(long testLong)
{    this.testLong = testLong;}
0
public byte getTestByte()
{    return testByte;}
0
public void setTestByte(byte testByte)
{    this.testByte = testByte;}
0
public boolean getTestBoolean()
{    return testBoolean;}
0
public void setTestBoolean(boolean testBoolean)
{    this.testBoolean = testBoolean;}
0
public float getTestFloat()
{    return testFloat;}
0
public void setTestFloat(float testFloat)
{    this.testFloat = testFloat;}
0
public double getTestDouble()
{    return testDouble;}
0
public void setTestDouble(double testDouble)
{    this.testDouble = testDouble;}
0
public String getTestString()
{    return testString;}
0
public void setTestString(String testString)
{    this.testString = testString;}
0
private int getPrivateInt()
{    return privateInt;}
0
private void setPrivateInt(int privateInt)
{    this.privateInt = privateInt;}
0
public void testRoundDownTimeStampSeconds()
{    Calendar cal = BASE_CALENDAR_WITH_DEFAULT_TIMEZONE;    Calendar cal2 = createCalendar(2012, 5, 15, 15, 12, 0, 0, null);    long timeToVerify = cal2.getTimeInMillis();    long ret = TimestampRoundDownUtil.roundDownTimeStampSeconds(cal.getTimeInMillis(), 60);    System.out.println("Cal 1: " + cal.toString());    System.out.println("Cal 2: " + cal2.toString());    Assert.assertEquals(timeToVerify, ret);}
0
public void testRoundDownTimeStampSecondsWithTimeZone()
{    Calendar cal = BASE_CALENDAR_WITH_CUSTOM_TIMEZONE;    Calendar cal2 = createCalendar(2012, 5, 15, 15, 12, 0, 0, CUSTOM_TIMEZONE);    long timeToVerify = cal2.getTimeInMillis();    long withoutTimeZone = TimestampRoundDownUtil.roundDownTimeStampSeconds(cal.getTimeInMillis(), 60);    long withTimeZone = TimestampRoundDownUtil.roundDownTimeStampSeconds(cal.getTimeInMillis(), 60, CUSTOM_TIMEZONE);    assertThat(withoutTimeZone, not(equalTo(timeToVerify)));    Assert.assertEquals(withTimeZone, timeToVerify);}
0
public void testRoundDownTimeStampMinutes()
{    Calendar cal = BASE_CALENDAR_WITH_DEFAULT_TIMEZONE;    Calendar cal2 = createCalendar(2012, 5, 15, 15, 10, 0, 0, null);    long timeToVerify = cal2.getTimeInMillis();    long ret = TimestampRoundDownUtil.roundDownTimeStampMinutes(cal.getTimeInMillis(), 5);    System.out.println("Cal 1: " + cal.toString());    System.out.println("Cal 2: " + cal2.toString());    Assert.assertEquals(timeToVerify, ret);}
0
public void testRoundDownTimeStampMinutesWithTimeZone()
{    Calendar cal = BASE_CALENDAR_WITH_CUSTOM_TIMEZONE;    Calendar cal2 = createCalendar(2012, 5, 15, 15, 10, 0, 0, CUSTOM_TIMEZONE);    long timeToVerify = cal2.getTimeInMillis();    long withoutTimeZone = TimestampRoundDownUtil.roundDownTimeStampMinutes(cal.getTimeInMillis(), 5);    long withTimeZone = TimestampRoundDownUtil.roundDownTimeStampMinutes(cal.getTimeInMillis(), 5, CUSTOM_TIMEZONE);    assertThat(withoutTimeZone, not(equalTo(timeToVerify)));    Assert.assertEquals(withTimeZone, timeToVerify);}
0
public void testRoundDownTimeStampHours()
{    Calendar cal = BASE_CALENDAR_WITH_DEFAULT_TIMEZONE;    Calendar cal2 = createCalendar(2012, 5, 15, 14, 0, 0, 0, null);    long timeToVerify = cal2.getTimeInMillis();    long ret = TimestampRoundDownUtil.roundDownTimeStampHours(cal.getTimeInMillis(), 2);    System.out.println("Cal 1: " + ret);    System.out.println("Cal 2: " + cal2.toString());    Assert.assertEquals(timeToVerify, ret);}
0
public void testRoundDownTimeStampHoursWithTimeZone()
{    Calendar cal = BASE_CALENDAR_WITH_CUSTOM_TIMEZONE;    Calendar cal2 = createCalendar(2012, 5, 15, 14, 0, 0, 0, CUSTOM_TIMEZONE);    long timeToVerify = cal2.getTimeInMillis();    long withoutTimeZone = TimestampRoundDownUtil.roundDownTimeStampHours(cal.getTimeInMillis(), 2);    long withTimeZone = TimestampRoundDownUtil.roundDownTimeStampHours(cal.getTimeInMillis(), 2, CUSTOM_TIMEZONE);    assertThat(withoutTimeZone, not(equalTo(timeToVerify)));    Assert.assertEquals(withTimeZone, timeToVerify);}
0
private static Calendar createCalendar(int year, int month, int day, int hour, int minute, int second, int ms, @Nullable TimeZone timeZone)
{    Calendar cal = (timeZone == null) ? Calendar.getInstance() : Calendar.getInstance(timeZone);    cal.set(year, month, day, hour, minute, second);    cal.set(Calendar.MILLISECOND, ms);    return cal;}
0
public void testVersionInfoUnknown()
{                        assertTrue("getVersion returned Unknown", !VersionInfo.getVersion().equals("Unknown"));    assertTrue("getUser returned Unknown", !VersionInfo.getUser().equals("Unknown"));    assertTrue("getUrl returned Unknown", !VersionInfo.getUrl().equals("Unknown"));    assertTrue("getSrcChecksum returned Unknown", !VersionInfo.getSrcChecksum().equals("Unknown"));        assertTrue("getBuildVersion returned unexpected format", VersionInfo.getBuildVersion().matches(".+from.+by.+on.+source checksum.+"));        assertNotNull("getRevision returned null", VersionInfo.getRevision());    assertNotNull("getBranch returned null", VersionInfo.getBranch());}
1
public void configure(Map<String, String> properties) throws FlumeException
{    if (state == State.STARTED) {        throw new IllegalStateException("Cannot be configured while started");    }    doConfigure(properties);    state = State.STOPPED;}
0
public void start() throws FlumeException
{    if (state == State.STARTED) {        throw new IllegalStateException("Cannot be started while started");    } else if (state == State.NEW) {        throw new IllegalStateException("Cannot be started before being " + "configured");    }            Source source = Preconditions.checkNotNull(sourceRunner.getSource(), "Source runner returned null source");    if (source instanceof EmbeddedSource) {        embeddedSource = (EmbeddedSource) source;    } else {        throw new IllegalStateException("Unknown source type: " + source.getClass().getName());    }    doStart();    state = State.STARTED;}
0
public void stop() throws FlumeException
{    if (state != State.STARTED) {        throw new IllegalStateException("Cannot be stopped unless started");    }    supervisor.stop();    embeddedSource = null;    state = State.STOPPED;}
0
private void doConfigure(Map<String, String> properties)
{    properties = EmbeddedAgentConfiguration.configure(name, properties);    if (LOGGER.isDebugEnabled() && LogPrivacyUtil.allowLogPrintConfig()) {                for (String key : new TreeSet<String>(properties.keySet())) {                    }    }    MaterializedConfiguration conf = configurationProvider.get(name, properties);    Map<String, SourceRunner> sources = conf.getSourceRunners();    if (sources.size() != 1) {        throw new FlumeException("Expected one source and got " + sources.size());    }    Map<String, Channel> channels = conf.getChannels();    if (channels.size() != 1) {        throw new FlumeException("Expected one channel and got " + channels.size());    }    Map<String, SinkRunner> sinks = conf.getSinkRunners();    if (sinks.size() != 1) {        throw new FlumeException("Expected one sink group and got " + sinks.size());    }    this.sourceRunner = sources.values().iterator().next();    this.channel = channels.values().iterator().next();    this.sinkRunner = sinks.values().iterator().next();}
1
public void put(Event event) throws EventDeliveryException
{    if (state != State.STARTED) {        throw new IllegalStateException("Cannot put events unless started");    }    try {        embeddedSource.put(event);    } catch (ChannelException ex) {        throw new EventDeliveryException("Embedded agent " + name + ": Unable to process event: " + ex.getMessage(), ex);    }}
0
public void putAll(List<Event> events) throws EventDeliveryException
{    if (state != State.STARTED) {        throw new IllegalStateException("Cannot put events unless started");    }    try {        embeddedSource.putAll(events);    } catch (ChannelException ex) {        throw new EventDeliveryException("Embedded agent " + name + ": Unable to process event: " + ex.getMessage(), ex);    }}
0
private void doStart()
{    boolean error = true;    try {        channel.start();        sinkRunner.start();        sourceRunner.start();        supervisor.supervise(channel, new SupervisorPolicy.AlwaysRestartPolicy(), LifecycleState.START);        supervisor.supervise(sinkRunner, new SupervisorPolicy.AlwaysRestartPolicy(), LifecycleState.START);        supervisor.supervise(sourceRunner, new SupervisorPolicy.AlwaysRestartPolicy(), LifecycleState.START);        error = false;    } finally {        if (error) {            stopLogError(sourceRunner);            stopLogError(channel);            stopLogError(sinkRunner);            supervisor.stop();        }    }}
0
private void stopLogError(LifecycleAware lifeCycleAware)
{    try {        if (LifecycleState.START.equals(lifeCycleAware.getLifecycleState())) {            lifeCycleAware.stop();        }    } catch (Exception e) {            }}
1
private static void validate(String name, Map<String, String> properties) throws FlumeException
{    if (properties.containsKey(SOURCE_TYPE)) {        checkAllowed(ALLOWED_SOURCES, properties.get(SOURCE_TYPE));    }    checkRequired(properties, CHANNEL_TYPE);    checkAllowed(ALLOWED_CHANNELS, properties.get(CHANNEL_TYPE));    checkRequired(properties, SINKS);    String sinkNames = properties.get(SINKS);    for (String sink : sinkNames.split("\\s+")) {        if (DISALLOWED_SINK_NAMES.contains(sink.toLowerCase(Locale.ENGLISH))) {            throw new FlumeException("Sink name " + sink + " is one of the" + " disallowed sink names: " + DISALLOWED_SINK_NAMES);        }        String key = join(sink, TYPE);        checkRequired(properties, key);        checkAllowed(ALLOWED_SINKS, properties.get(key));    }    checkRequired(properties, SINK_PROCESSOR_TYPE);    checkAllowed(ALLOWED_SINK_PROCESSORS, properties.get(SINK_PROCESSOR_TYPE));}
0
 static Map<String, String> configure(String name, Map<String, String> properties) throws FlumeException
{    validate(name, properties);        properties = new HashMap<String, String>(properties);    if (!properties.containsKey(SOURCE_TYPE) || SOURCE_TYPE_EMBEDDED_ALIAS.equalsIgnoreCase(properties.get(SOURCE_TYPE))) {        properties.put(SOURCE_TYPE, SOURCE_TYPE_EMBEDDED);    }    String sinkNames = properties.remove(SINKS);    String strippedName = name.replaceAll("\\s+", "");    String sourceName = "source-" + strippedName;    String channelName = "channel-" + strippedName;    String sinkGroupName = "sink-group-" + strippedName;    /*     * Now we are going to process the user supplied configuration     * and generate an agent configuration. This is only to supply     * a simpler client api than passing in an entire agent configuration.     */        Map<String, String> result = Maps.newHashMap();    /*     * First we are going to setup all the root level pointers. I.E     * point the agent at the components, sink group at sinks, and     * source at the channel.     */        result.put(join(name, BasicConfigurationConstants.CONFIG_SOURCES), sourceName);        result.put(join(name, BasicConfigurationConstants.CONFIG_CHANNELS), channelName);        result.put(join(name, BasicConfigurationConstants.CONFIG_SINKS), sinkNames);        result.put(join(name, BasicConfigurationConstants.CONFIG_SINKGROUPS), sinkGroupName);        result.put(join(name, BasicConfigurationConstants.CONFIG_SINKGROUPS, sinkGroupName, SINKS), sinkNames);        result.put(join(name, BasicConfigurationConstants.CONFIG_SOURCES, sourceName, BasicConfigurationConstants.CONFIG_CHANNELS), channelName);            Set<String> userProvidedKeys = new HashSet<String>(properties.keySet());    /*     * Second process the sink configuration and point the sinks     * at the channel.     */    for (String sink : sinkNames.split("\\s+")) {        for (String key : userProvidedKeys) {            String value = properties.get(key);            if (key.startsWith(sink + SEPERATOR)) {                properties.remove(key);                result.put(join(name, BasicConfigurationConstants.CONFIG_SINKS, key), value);            }        }                result.put(join(name, BasicConfigurationConstants.CONFIG_SINKS, sink, BasicConfigurationConstants.CONFIG_CHANNEL), channelName);    }    /*     * Third, process all remaining configuration items, prefixing them     * correctly and then passing them on to the agent.     */    userProvidedKeys = new HashSet<String>(properties.keySet());    for (String key : userProvidedKeys) {        String value = properties.get(key);        if (key.startsWith(SOURCE_PREFIX)) {                        key = key.replaceFirst(SOURCE, sourceName);            result.put(join(name, BasicConfigurationConstants.CONFIG_SOURCES, key), value);        } else if (key.startsWith(CHANNEL_PREFIX)) {                        key = key.replaceFirst(CHANNEL, channelName);            result.put(join(name, BasicConfigurationConstants.CONFIG_CHANNELS, key), value);        } else if (key.startsWith(SINK_PROCESSOR_PREFIX)) {                        result.put(join(name, BasicConfigurationConstants.CONFIG_SINKGROUPS, sinkGroupName, key), value);        } else {                        throw new FlumeException("Unknown configuration " + key);        }    }    return result;}
0
private static void checkAllowed(String[] allowedTypes, String type)
{    boolean isAllowed = false;    type = type.trim();    for (String allowedType : allowedTypes) {        if (allowedType.equalsIgnoreCase(type)) {            isAllowed = true;            break;        }    }    if (!isAllowed) {        throw new FlumeException("Component type of " + type + " is not in " + "allowed types of " + Arrays.toString(allowedTypes));    }}
0
private static void checkRequired(Map<String, String> properties, String name)
{    if (!properties.containsKey(name)) {        throw new FlumeException("Required parameter not found " + name);    }}
0
private static String join(String... parts)
{    return JOINER.join(parts);}
0
public void configure(Context context)
{}
0
public void put(Event event) throws ChannelException
{    getChannelProcessor().processEvent(event);}
0
public void putAll(List<Event> events) throws ChannelException
{    getChannelProcessor().processEventBatch(events);}
0
 MaterializedConfiguration get(String name, Map<String, String> properties)
{    MemoryConfigurationProvider confProvider = new MemoryConfigurationProvider(name, properties);    return confProvider.getConfiguration();}
0
protected FlumeConfiguration getFlumeConfiguration()
{    return new FlumeConfiguration(properties);}
0
public void setUp() throws Exception
{    headers = Maps.newHashMap();    headers.put("key1", "value1");    body = "body".getBytes(Charsets.UTF_8);    int port = findFreePort();    eventCollector = new EventCollector();    Responder responder = new SpecificResponder(AvroSourceProtocol.class, eventCollector);    nettyServer = new NettyServer(responder, new InetSocketAddress(HOSTNAME, port));    nettyServer.start();        Thread.sleep(1000L);    properties = Maps.newHashMap();    properties.put("channel.type", "memory");    properties.put("channel.capacity", "200");    properties.put("sinks", "sink1 sink2");    properties.put("sink1.type", "avro");    properties.put("sink2.type", "avro");    properties.put("sink1.hostname", HOSTNAME);    properties.put("sink1.port", String.valueOf(port));    properties.put("sink2.hostname", HOSTNAME);    properties.put("sink2.port", String.valueOf(port));    properties.put("processor.type", "load_balance");    agent = new EmbeddedAgent("test-" + serialNumber.incrementAndGet());}
0
public void tearDown() throws Exception
{    if (agent != null) {        try {            agent.stop();        } catch (Exception e) {                    }    }    if (nettyServer != null) {        try {            nettyServer.close();        } catch (Exception e) {                    }    }}
1
public void testPut() throws Exception
{    agent.configure(properties);    agent.start();    agent.put(EventBuilder.withBody(body, headers));    Event event;    while ((event = eventCollector.poll()) == null) {        Thread.sleep(500L);    }    Assert.assertNotNull(event);    Assert.assertArrayEquals(body, event.getBody());    Assert.assertEquals(headers, event.getHeaders());}
0
public void testPutAll() throws Exception
{    List<Event> events = Lists.newArrayList();    events.add(EventBuilder.withBody(body, headers));    agent.configure(properties);    agent.start();    agent.putAll(events);    Event event;    while ((event = eventCollector.poll()) == null) {        Thread.sleep(500L);    }    Assert.assertNotNull(event);    Assert.assertArrayEquals(body, event.getBody());    Assert.assertEquals(headers, event.getHeaders());}
0
public void testPutWithInterceptors() throws Exception
{    properties.put("source.interceptors", "i1");    properties.put("source.interceptors.i1.type", "static");    properties.put("source.interceptors.i1.key", "key2");    properties.put("source.interceptors.i1.value", "value2");    agent.configure(properties);    agent.start();    agent.put(EventBuilder.withBody(body, headers));    Event event;    while ((event = eventCollector.poll()) == null) {        Thread.sleep(500L);    }    Assert.assertNotNull(event);    Assert.assertArrayEquals(body, event.getBody());    Map<String, String> newHeaders = new HashMap<String, String>(headers);    newHeaders.put("key2", "value2");    Assert.assertEquals(newHeaders, event.getHeaders());}
0
public void testEmbeddedAgentName() throws Exception
{    EmbeddedAgent embedAgent = new EmbeddedAgent("test 1 2" + serialNumber.incrementAndGet());    List<Event> events = Lists.newArrayList();    events.add(EventBuilder.withBody(body, headers));    embedAgent.configure(properties);    embedAgent.start();    embedAgent.putAll(events);    Event event;    while ((event = eventCollector.poll()) == null) {        Thread.sleep(500L);    }    Assert.assertNotNull(event);    Assert.assertArrayEquals(body, event.getBody());    Assert.assertEquals(headers, event.getHeaders());    if (embedAgent != null) {        try {            embedAgent.stop();        } catch (Exception e) {                    }    }}
1
public Event poll()
{    AvroFlumeEvent avroEvent = eventQueue.poll();    if (avroEvent != null) {        return EventBuilder.withBody(avroEvent.getBody().array(), toStringMap(avroEvent.getHeaders()));    }    return null;}
0
public Status append(AvroFlumeEvent event) throws AvroRemoteException
{    eventQueue.add(event);    return Status.OK;}
0
public Status appendBatch(List<AvroFlumeEvent> events) throws AvroRemoteException
{    Preconditions.checkState(eventQueue.addAll(events));    return Status.OK;}
0
private static Map<String, String> toStringMap(Map<CharSequence, CharSequence> charSeqMap)
{    Map<String, String> stringMap = new HashMap<String, String>();    for (Map.Entry<CharSequence, CharSequence> entry : charSeqMap.entrySet()) {        stringMap.put(entry.getKey().toString(), entry.getValue().toString());    }    return stringMap;}
0
private static int findFreePort() throws IOException
{    try (ServerSocket socket = new ServerSocket(0)) {        return socket.getLocalPort();    }}
0
public void setUp() throws Exception
{    properties = Maps.newHashMap();    properties.put("source.type", EmbeddedAgentConfiguration.SOURCE_TYPE_EMBEDDED);    properties.put("channel.type", "memory");    properties.put("channel.capacity", "200");    properties.put("sinks", "sink1 sink2");    properties.put("sink1.type", "avro");    properties.put("sink2.type", "avro");    properties.put("sink1.hostname", "sink1.host");    properties.put("sink1.port", "2");    properties.put("sink2.hostname", "sink2.host");    properties.put("sink2.port", "2");    properties.put("processor.type", "load_balance");    properties.put("source.interceptors", "i1");    properties.put("source.interceptors.i1.type", "timestamp");}
0
public void testFullSourceType() throws Exception
{    doTestExcepted(EmbeddedAgentConfiguration.configure("test1", properties));}
0
public void testMissingSourceType() throws Exception
{    Assert.assertNotNull(properties.remove("source.type"));    doTestExcepted(EmbeddedAgentConfiguration.configure("test1", properties));}
0
public void testShortSourceType() throws Exception
{    properties.put("source.type", "EMBEDDED");    doTestExcepted(EmbeddedAgentConfiguration.configure("test1", properties));}
0
public void doTestExcepted(Map<String, String> actual) throws Exception
{    Map<String, String> expected = Maps.newHashMap();    expected.put("test1.channels", "channel-test1");    expected.put("test1.channels.channel-test1.capacity", "200");    expected.put("test1.channels.channel-test1.type", "memory");    expected.put("test1.sinkgroups", "sink-group-test1");    expected.put("test1.sinkgroups.sink-group-test1.processor.type", "load_balance");    expected.put("test1.sinkgroups.sink-group-test1.sinks", "sink1 sink2");    expected.put("test1.sinks", "sink1 sink2");    expected.put("test1.sinks.sink1.channel", "channel-test1");    expected.put("test1.sinks.sink1.hostname", "sink1.host");    expected.put("test1.sinks.sink1.port", "2");    expected.put("test1.sinks.sink1.type", "avro");    expected.put("test1.sinks.sink2.channel", "channel-test1");    expected.put("test1.sinks.sink2.hostname", "sink2.host");    expected.put("test1.sinks.sink2.port", "2");    expected.put("test1.sinks.sink2.type", "avro");    expected.put("test1.sources", "source-test1");    expected.put("test1.sources.source-test1.channels", "channel-test1");    expected.put("test1.sources.source-test1.type", EmbeddedAgentConfiguration.SOURCE_TYPE_EMBEDDED);    expected.put("test1.sources.source-test1.interceptors", "i1");    expected.put("test1.sources.source-test1.interceptors.i1.type", "timestamp");    Assert.assertEquals(expected, actual);}
0
public void testBadSource() throws Exception
{    properties.put("source.type", "exec");    EmbeddedAgentConfiguration.configure("test1", properties);}
0
public void testBadChannel() throws Exception
{    properties.put("channel.type", "jdbc");    EmbeddedAgentConfiguration.configure("test1", properties);}
0
public void testBadSink() throws Exception
{    properties.put("sink1.type", "hbase");    EmbeddedAgentConfiguration.configure("test1", properties);}
0
public void testBadSinkProcessor() throws Exception
{    properties.put("processor.type", "bad");    EmbeddedAgentConfiguration.configure("test1", properties);}
0
public void testNoChannel() throws Exception
{    properties.remove("channel.type");    EmbeddedAgentConfiguration.configure("test1", properties);}
0
public void testNoSink() throws Exception
{    properties.remove("sink2.type");    EmbeddedAgentConfiguration.configure("test1", properties);}
0
public void testNoSinkProcessor() throws Exception
{    properties.remove("processor.type");    EmbeddedAgentConfiguration.configure("test1", properties);}
0
public void testBadKey() throws Exception
{    properties.put("bad.key.name", "bad");    EmbeddedAgentConfiguration.configure("test1", properties);}
0
public void testSinkNamedLikeSource() throws Exception
{    properties.put("sinks", "source");    EmbeddedAgentConfiguration.configure("test1", properties);}
0
public void testSinkNamedLikeChannel() throws Exception
{    properties.put("sinks", "channel");    EmbeddedAgentConfiguration.configure("test1", properties);}
0
public void testSinkNamedLikeProcessor() throws Exception
{    properties.put("sinks", "processor");    EmbeddedAgentConfiguration.configure("test1", properties);}
0
public void setUp() throws Exception
{    properties = Maps.newHashMap();    properties.put("source.type", EmbeddedAgentConfiguration.SOURCE_TYPE_EMBEDDED);    properties.put("channel.type", "memory");    properties.put("sinks", "sink1 sink2");    properties.put("sink1.type", "avro");    properties.put("sink2.type", "avro");    properties.put("processor.type", "load_balance");    sourceRunner = mock(SourceRunner.class);    channel = mock(Channel.class);    sinkRunner = mock(SinkRunner.class);    source = mock(EmbeddedSource.class);    when(sourceRunner.getSource()).thenReturn(source);    when(sourceRunner.getLifecycleState()).thenReturn(LifecycleState.START);    when(channel.getLifecycleState()).thenReturn(LifecycleState.START);    when(sinkRunner.getLifecycleState()).thenReturn(LifecycleState.START);    config = new MaterializedConfiguration() {        @Override        public Map<String, SourceRunner> getSourceRunners() {            Map<String, SourceRunner> result = Maps.newHashMap();            result.put("source", sourceRunner);            return ImmutableMap.copyOf(result);        }        @Override        public Map<String, SinkRunner> getSinkRunners() {            Map<String, SinkRunner> result = Maps.newHashMap();            result.put("sink", sinkRunner);            return ImmutableMap.copyOf(result);        }        @Override        public Map<String, Channel> getChannels() {            Map<String, Channel> result = Maps.newHashMap();            result.put("channel", channel);            return ImmutableMap.copyOf(result);        }        @Override        public void addSourceRunner(String name, SourceRunner sourceRunner) {            throw new UnsupportedOperationException();        }        @Override        public void addSinkRunner(String name, SinkRunner sinkRunner) {            throw new UnsupportedOperationException();        }        @Override        public void addChannel(String name, Channel channel) {            throw new UnsupportedOperationException();        }    };    agent = new EmbeddedAgent(new MaterializedConfigurationProvider() {        public MaterializedConfiguration get(String name, Map<String, String> properties) {            return config;        }    }, "dummy");}
0
public Map<String, SourceRunner> getSourceRunners()
{    Map<String, SourceRunner> result = Maps.newHashMap();    result.put("source", sourceRunner);    return ImmutableMap.copyOf(result);}
0
public Map<String, SinkRunner> getSinkRunners()
{    Map<String, SinkRunner> result = Maps.newHashMap();    result.put("sink", sinkRunner);    return ImmutableMap.copyOf(result);}
0
public Map<String, Channel> getChannels()
{    Map<String, Channel> result = Maps.newHashMap();    result.put("channel", channel);    return ImmutableMap.copyOf(result);}
0
public void addSourceRunner(String name, SourceRunner sourceRunner)
{    throw new UnsupportedOperationException();}
0
public void addSinkRunner(String name, SinkRunner sinkRunner)
{    throw new UnsupportedOperationException();}
0
public void addChannel(String name, Channel channel)
{    throw new UnsupportedOperationException();}
0
public MaterializedConfiguration get(String name, Map<String, String> properties)
{    return config;}
0
public void testStart()
{    agent.configure(properties);    agent.start();    verify(sourceRunner, times(1)).start();    verify(channel, times(1)).start();    verify(sinkRunner, times(1)).start();}
0
public void testStop()
{    agent.configure(properties);    agent.start();    agent.stop();    verify(sourceRunner, times(1)).stop();    verify(channel, times(1)).stop();    verify(sinkRunner, times(1)).stop();}
0
public void testStartSourceThrowsException()
{    doThrow(new LocalRuntimeException()).when(sourceRunner).start();    startExpectingLocalRuntimeException();}
0
public void testStartChannelThrowsException()
{    doThrow(new LocalRuntimeException()).when(channel).start();    startExpectingLocalRuntimeException();}
0
public void testStartSinkThrowsException()
{    doThrow(new LocalRuntimeException()).when(sinkRunner).start();    startExpectingLocalRuntimeException();}
0
private void startExpectingLocalRuntimeException()
{    agent.configure(properties);    try {        agent.start();        Assert.fail();    } catch (LocalRuntimeException e) {        }    verify(sourceRunner, times(1)).stop();    verify(channel, times(1)).stop();    verify(sinkRunner, times(1)).stop();}
0
public void testPut() throws EventDeliveryException
{    Event event = new SimpleEvent();    agent.configure(properties);    agent.start();    agent.put(event);    verify(source, times(1)).put(event);}
0
public void testPutAll() throws EventDeliveryException
{    Event event = new SimpleEvent();    List<Event> events = Lists.newArrayList();    events.add(event);    agent.configure(properties);    agent.start();    agent.putAll(events);    verify(source, times(1)).putAll(events);}
0
public void testPutNotStarted() throws EventDeliveryException
{    Event event = new SimpleEvent();    agent.configure(properties);    agent.put(event);}
0
public void testPutAllNotStarted() throws EventDeliveryException
{    Event event = new SimpleEvent();    List<Event> events = Lists.newArrayList();    events.add(event);    agent.configure(properties);    agent.putAll(events);}
0
public void setUp() throws Exception
{    agent = new EmbeddedAgent("dummy");    properties = Maps.newHashMap();    properties.put("source.type", EmbeddedAgentConfiguration.SOURCE_TYPE_EMBEDDED);    properties.put("channel.type", "memory");    properties.put("sinks", "sink1 sink2");    properties.put("sink1.type", "avro");    properties.put("sink2.type", "avro");    properties.put("sink1.hostname", HOSTNAME);    properties.put("sink1.port", "0");    properties.put("sink2.hostname", HOSTNAME);    properties.put("sink2.port", "0");    properties.put("processor.type", "load_balance");}
0
public void testConfigureWithBadSourceType()
{    properties.put(EmbeddedAgentConfiguration.SOURCE_TYPE, "bad");    agent.configure(properties);}
0
public void testConfigureWhileStarted()
{    try {        agent.configure(properties);        agent.start();    } catch (Exception e) {        Throwables.propagate(e);    }    agent.configure(properties);}
0
public void testConfigureMultipleTimes()
{    agent.configure(properties);    agent.configure(properties);}
0
public void testStartWhileStarted()
{    try {        agent.configure(properties);        agent.start();    } catch (Exception e) {        Throwables.propagate(e);    }    agent.start();}
0
public void testStartUnconfigured()
{    agent.start();}
0
public void testStopBeforeConfigure()
{    agent.stop();}
0
public void testStoppedWhileStopped()
{    try {        agent.configure(properties);    } catch (Exception e) {        Throwables.propagate(e);    }    agent.stop();}
0
public void testStopAfterStop()
{    try {        agent.configure(properties);        agent.start();        agent.stop();    } catch (Exception e) {        Throwables.propagate(e);    }    agent.stop();}
0
public void testStopAfterConfigure()
{    try {        agent.configure(properties);    } catch (Exception e) {        Throwables.propagate(e);    }    agent.stop();}
0
public void start()
{        res = new SpecificResponder(FlumeOGEventAvroServer.class, this);    try {        http = new HttpServer(res, host, port);    } catch (IOException eI) {                return;    }    http.start();    super.start();}
1
public void stop()
{    http.close();    super.stop();}
0
public Void append(AvroFlumeOGEvent evt) throws AvroRemoteException
{    counterGroup.incrementAndGet("rpc.received");    Map<String, String> headers = new HashMap<String, String>();        headers.put(HOST, evt.getHost().toString());    headers.put(TIMESTAMP, evt.getTimestamp().toString());    headers.put(PRIORITY, evt.getPriority().toString());    headers.put(NANOS, evt.getNanos().toString());    for (Entry<CharSequence, ByteBuffer> entry : evt.getFields().entrySet()) {        headers.put(entry.getKey().toString(), entry.getValue().toString());    }    headers.put(OG_EVENT, "yes");    Event event = EventBuilder.withBody(evt.getBody().array(), headers);    try {        getChannelProcessor().processEvent(event);        counterGroup.incrementAndGet("rpc.events");    } catch (ChannelException ex) {        return null;    }    counterGroup.incrementAndGet("rpc.successful");    return null;}
0
public void configure(Context context)
{    port = Integer.parseInt(context.getString("port"));    host = context.getString("host");}
0
public void setUp() throws Exception
{    source = new AvroLegacySource();    channel = new MemoryChannel();    Configurables.configure(channel, new Context());    List<Channel> channels = new ArrayList<Channel>();    channels.add(channel);    ChannelSelector rcs = new ReplicatingChannelSelector();    rcs.setChannels(channels);    source.setChannelProcessor(new ChannelProcessor(rcs));    try (ServerSocket socket = new ServerSocket(0)) {        selectedPort = socket.getLocalPort();    }}
0
public void testLifecycle() throws InterruptedException
{    Context context = new Context();    context.put("port", String.valueOf(selectedPort));    context.put("host", "0.0.0.0");    Configurables.configure(source, context);    source.start();    Assert.assertTrue("Reached start or error", LifecycleController.waitForOneOf(source, LifecycleState.START_OR_ERROR));    Assert.assertEquals("Server is started", LifecycleState.START, source.getLifecycleState());    source.stop();    Assert.assertTrue("Reached stop or error", LifecycleController.waitForOneOf(source, LifecycleState.STOP_OR_ERROR));    Assert.assertEquals("Server is stopped", LifecycleState.STOP, source.getLifecycleState());}
0
public void testRequest() throws InterruptedException, IOException
{    Context context = new Context();    context.put("port", String.valueOf(selectedPort));    context.put("host", "0.0.0.0");    Configurables.configure(source, context);    source.start();    Assert.assertTrue("Reached start or error", LifecycleController.waitForOneOf(source, LifecycleState.START_OR_ERROR));    Assert.assertEquals("Server is started", LifecycleState.START, source.getLifecycleState());        URL url = new URL("http", "0.0.0.0", selectedPort, "/");    Transceiver http = new HttpTransceiver(url);    FlumeOGEventAvroServer client = SpecificRequestor.getClient(FlumeOGEventAvroServer.class, http);    AvroFlumeOGEvent avroEvent = AvroFlumeOGEvent.newBuilder().setHost("foo").setPriority(Priority.INFO).setNanos(0).setTimestamp(1).setFields(new HashMap<CharSequence, ByteBuffer>()).setBody(ByteBuffer.wrap("foo".getBytes())).build();    client.append(avroEvent);        Transaction transaction = channel.getTransaction();    transaction.begin();    Event event = channel.take();    Assert.assertNotNull(event);    Assert.assertEquals("Channel contained our event", "foo", new String(event.getBody()));    transaction.commit();    transaction.close();    source.stop();    Assert.assertTrue("Reached stop or error", LifecycleController.waitForOneOf(source, LifecycleState.STOP_OR_ERROR));    Assert.assertEquals("Server is stopped", LifecycleState.STOP, source.getLifecycleState());}
0
public int getValue()
{    return value;}
0
public static EventStatus findByValue(int value)
{    switch(value) {        case 0:            return ACK;        case 1:            return COMMITED;        case 2:            return ERR;        default:            return null;    }}
0
public int getValue()
{    return value;}
0
public static Priority findByValue(int value)
{    switch(value) {        case 0:            return FATAL;        case 1:            return ERROR;        case 2:            return WARN;        case 3:            return INFO;        case 4:            return DEBUG;        case 5:            return TRACE;        default:            return null;    }}
0
public static _Fields findByThriftId(int fieldId)
{    switch(fieldId) {        case         1:            return TIMESTAMP;        case         2:            return PRIORITY;        case         3:            return BODY;        case         4:            return NANOS;        case         5:            return HOST;        case         6:            return FIELDS;        default:            return null;    }}
0
public static _Fields findByThriftIdOrThrow(int fieldId)
{    _Fields fields = findByThriftId(fieldId);    if (fields == null)        throw new IllegalArgumentException("Field " + fieldId + " doesn't exist!");    return fields;}
0
public static _Fields findByName(String name)
{    return byName.get(name);}
0
public short getThriftFieldId()
{    return _thriftId;}
0
public String getFieldName()
{    return _fieldName;}
0
public ThriftFlumeEvent deepCopy()
{    return new ThriftFlumeEvent(this);}
0
public void clear()
{    setTimestampIsSet(false);    this.timestamp = 0;    this.priority = null;    this.body = null;    setNanosIsSet(false);    this.nanos = 0;    this.host = null;    this.fields = null;}
0
public long getTimestamp()
{    return this.timestamp;}
0
public ThriftFlumeEvent setTimestamp(long timestamp)
{    this.timestamp = timestamp;    setTimestampIsSet(true);    return this;}
0
public void unsetTimestamp()
{    __isset_bitfield = EncodingUtils.clearBit(__isset_bitfield, __TIMESTAMP_ISSET_ID);}
0
public boolean isSetTimestamp()
{    return EncodingUtils.testBit(__isset_bitfield, __TIMESTAMP_ISSET_ID);}
0
public void setTimestampIsSet(boolean value)
{    __isset_bitfield = EncodingUtils.setBit(__isset_bitfield, __TIMESTAMP_ISSET_ID, value);}
0
public Priority getPriority()
{    return this.priority;}
0
public ThriftFlumeEvent setPriority(Priority priority)
{    this.priority = priority;    return this;}
0
public void unsetPriority()
{    this.priority = null;}
0
public boolean isSetPriority()
{    return this.priority != null;}
0
public void setPriorityIsSet(boolean value)
{    if (!value) {        this.priority = null;    }}
0
public byte[] getBody()
{    setBody(org.apache.thrift.TBaseHelper.rightSize(body));    return body == null ? null : body.array();}
0
public ByteBuffer bufferForBody()
{    return org.apache.thrift.TBaseHelper.copyBinary(body);}
0
public ThriftFlumeEvent setBody(byte[] body)
{    this.body = body == null ? (ByteBuffer) null : ByteBuffer.wrap(Arrays.copyOf(body, body.length));    return this;}
0
public ThriftFlumeEvent setBody(ByteBuffer body)
{    this.body = org.apache.thrift.TBaseHelper.copyBinary(body);    return this;}
0
public void unsetBody()
{    this.body = null;}
0
public boolean isSetBody()
{    return this.body != null;}
0
public void setBodyIsSet(boolean value)
{    if (!value) {        this.body = null;    }}
0
public long getNanos()
{    return this.nanos;}
0
public ThriftFlumeEvent setNanos(long nanos)
{    this.nanos = nanos;    setNanosIsSet(true);    return this;}
0
public void unsetNanos()
{    __isset_bitfield = EncodingUtils.clearBit(__isset_bitfield, __NANOS_ISSET_ID);}
0
public boolean isSetNanos()
{    return EncodingUtils.testBit(__isset_bitfield, __NANOS_ISSET_ID);}
0
public void setNanosIsSet(boolean value)
{    __isset_bitfield = EncodingUtils.setBit(__isset_bitfield, __NANOS_ISSET_ID, value);}
0
public String getHost()
{    return this.host;}
0
public ThriftFlumeEvent setHost(String host)
{    this.host = host;    return this;}
0
public void unsetHost()
{    this.host = null;}
0
public boolean isSetHost()
{    return this.host != null;}
0
public void setHostIsSet(boolean value)
{    if (!value) {        this.host = null;    }}
0
public int getFieldsSize()
{    return (this.fields == null) ? 0 : this.fields.size();}
0
public void putToFields(String key, ByteBuffer val)
{    if (this.fields == null) {        this.fields = new HashMap<String, ByteBuffer>();    }    this.fields.put(key, val);}
0
public Map<String, ByteBuffer> getFields()
{    return this.fields;}
0
public ThriftFlumeEvent setFields(Map<String, ByteBuffer> fields)
{    this.fields = fields;    return this;}
0
public void unsetFields()
{    this.fields = null;}
0
public boolean isSetFields()
{    return this.fields != null;}
0
public void setFieldsIsSet(boolean value)
{    if (!value) {        this.fields = null;    }}
0
public void setFieldValue(_Fields field, Object value)
{    switch(field) {        case TIMESTAMP:            if (value == null) {                unsetTimestamp();            } else {                setTimestamp((Long) value);            }            break;        case PRIORITY:            if (value == null) {                unsetPriority();            } else {                setPriority((Priority) value);            }            break;        case BODY:            if (value == null) {                unsetBody();            } else {                setBody((ByteBuffer) value);            }            break;        case NANOS:            if (value == null) {                unsetNanos();            } else {                setNanos((Long) value);            }            break;        case HOST:            if (value == null) {                unsetHost();            } else {                setHost((String) value);            }            break;        case FIELDS:            if (value == null) {                unsetFields();            } else {                setFields((Map<String, ByteBuffer>) value);            }            break;    }}
0
public Object getFieldValue(_Fields field)
{    switch(field) {        case TIMESTAMP:            return getTimestamp();        case PRIORITY:            return getPriority();        case BODY:            return getBody();        case NANOS:            return getNanos();        case HOST:            return getHost();        case FIELDS:            return getFields();    }    throw new IllegalStateException();}
0
public boolean isSet(_Fields field)
{    if (field == null) {        throw new IllegalArgumentException();    }    switch(field) {        case TIMESTAMP:            return isSetTimestamp();        case PRIORITY:            return isSetPriority();        case BODY:            return isSetBody();        case NANOS:            return isSetNanos();        case HOST:            return isSetHost();        case FIELDS:            return isSetFields();    }    throw new IllegalStateException();}
0
public boolean equals(Object that)
{    if (that == null)        return false;    if (that instanceof ThriftFlumeEvent)        return this.equals((ThriftFlumeEvent) that);    return false;}
0
public boolean equals(ThriftFlumeEvent that)
{    if (that == null)        return false;    boolean this_present_timestamp = true;    boolean that_present_timestamp = true;    if (this_present_timestamp || that_present_timestamp) {        if (!(this_present_timestamp && that_present_timestamp))            return false;        if (this.timestamp != that.timestamp)            return false;    }    boolean this_present_priority = true && this.isSetPriority();    boolean that_present_priority = true && that.isSetPriority();    if (this_present_priority || that_present_priority) {        if (!(this_present_priority && that_present_priority))            return false;        if (!this.priority.equals(that.priority))            return false;    }    boolean this_present_body = true && this.isSetBody();    boolean that_present_body = true && that.isSetBody();    if (this_present_body || that_present_body) {        if (!(this_present_body && that_present_body))            return false;        if (!this.body.equals(that.body))            return false;    }    boolean this_present_nanos = true;    boolean that_present_nanos = true;    if (this_present_nanos || that_present_nanos) {        if (!(this_present_nanos && that_present_nanos))            return false;        if (this.nanos != that.nanos)            return false;    }    boolean this_present_host = true && this.isSetHost();    boolean that_present_host = true && that.isSetHost();    if (this_present_host || that_present_host) {        if (!(this_present_host && that_present_host))            return false;        if (!this.host.equals(that.host))            return false;    }    boolean this_present_fields = true && this.isSetFields();    boolean that_present_fields = true && that.isSetFields();    if (this_present_fields || that_present_fields) {        if (!(this_present_fields && that_present_fields))            return false;        if (!this.fields.equals(that.fields))            return false;    }    return true;}
0
public int hashCode()
{    List<Object> list = new ArrayList<Object>();    boolean present_timestamp = true;    list.add(present_timestamp);    if (present_timestamp)        list.add(timestamp);    boolean present_priority = true && (isSetPriority());    list.add(present_priority);    if (present_priority)        list.add(priority.getValue());    boolean present_body = true && (isSetBody());    list.add(present_body);    if (present_body)        list.add(body);    boolean present_nanos = true;    list.add(present_nanos);    if (present_nanos)        list.add(nanos);    boolean present_host = true && (isSetHost());    list.add(present_host);    if (present_host)        list.add(host);    boolean present_fields = true && (isSetFields());    list.add(present_fields);    if (present_fields)        list.add(fields);    return list.hashCode();}
0
public int compareTo(ThriftFlumeEvent other)
{    if (!getClass().equals(other.getClass())) {        return getClass().getName().compareTo(other.getClass().getName());    }    int lastComparison = 0;    lastComparison = Boolean.valueOf(isSetTimestamp()).compareTo(other.isSetTimestamp());    if (lastComparison != 0) {        return lastComparison;    }    if (isSetTimestamp()) {        lastComparison = org.apache.thrift.TBaseHelper.compareTo(this.timestamp, other.timestamp);        if (lastComparison != 0) {            return lastComparison;        }    }    lastComparison = Boolean.valueOf(isSetPriority()).compareTo(other.isSetPriority());    if (lastComparison != 0) {        return lastComparison;    }    if (isSetPriority()) {        lastComparison = org.apache.thrift.TBaseHelper.compareTo(this.priority, other.priority);        if (lastComparison != 0) {            return lastComparison;        }    }    lastComparison = Boolean.valueOf(isSetBody()).compareTo(other.isSetBody());    if (lastComparison != 0) {        return lastComparison;    }    if (isSetBody()) {        lastComparison = org.apache.thrift.TBaseHelper.compareTo(this.body, other.body);        if (lastComparison != 0) {            return lastComparison;        }    }    lastComparison = Boolean.valueOf(isSetNanos()).compareTo(other.isSetNanos());    if (lastComparison != 0) {        return lastComparison;    }    if (isSetNanos()) {        lastComparison = org.apache.thrift.TBaseHelper.compareTo(this.nanos, other.nanos);        if (lastComparison != 0) {            return lastComparison;        }    }    lastComparison = Boolean.valueOf(isSetHost()).compareTo(other.isSetHost());    if (lastComparison != 0) {        return lastComparison;    }    if (isSetHost()) {        lastComparison = org.apache.thrift.TBaseHelper.compareTo(this.host, other.host);        if (lastComparison != 0) {            return lastComparison;        }    }    lastComparison = Boolean.valueOf(isSetFields()).compareTo(other.isSetFields());    if (lastComparison != 0) {        return lastComparison;    }    if (isSetFields()) {        lastComparison = org.apache.thrift.TBaseHelper.compareTo(this.fields, other.fields);        if (lastComparison != 0) {            return lastComparison;        }    }    return 0;}
0
public _Fields fieldForId(int fieldId)
{    return _Fields.findByThriftId(fieldId);}
0
public void read(org.apache.thrift.protocol.TProtocol iprot) throws org.apache.thrift.TException
{    schemes.get(iprot.getScheme()).getScheme().read(iprot, this);}
0
public void write(org.apache.thrift.protocol.TProtocol oprot) throws org.apache.thrift.TException
{    schemes.get(oprot.getScheme()).getScheme().write(oprot, this);}
0
public String toString()
{    StringBuilder sb = new StringBuilder("ThriftFlumeEvent(");    boolean first = true;    sb.append("timestamp:");    sb.append(this.timestamp);    first = false;    if (!first)        sb.append(", ");    sb.append("priority:");    if (this.priority == null) {        sb.append("null");    } else {        sb.append(this.priority);    }    first = false;    if (!first)        sb.append(", ");    sb.append("body:");    if (this.body == null) {        sb.append("null");    } else {        org.apache.thrift.TBaseHelper.toString(this.body, sb);    }    first = false;    if (!first)        sb.append(", ");    sb.append("nanos:");    sb.append(this.nanos);    first = false;    if (!first)        sb.append(", ");    sb.append("host:");    if (this.host == null) {        sb.append("null");    } else {        sb.append(this.host);    }    first = false;    if (!first)        sb.append(", ");    sb.append("fields:");    if (this.fields == null) {        sb.append("null");    } else {        sb.append(this.fields);    }    first = false;    sb.append(")");    return sb.toString();}
0
public void validate() throws org.apache.thrift.TException
{}
0
private void writeObject(java.io.ObjectOutputStream out) throws java.io.IOException
{    try {        write(new org.apache.thrift.protocol.TCompactProtocol(new org.apache.thrift.transport.TIOStreamTransport(out)));    } catch (org.apache.thrift.TException te) {        throw new java.io.IOException(te);    }}
0
private void readObject(java.io.ObjectInputStream in) throws java.io.IOException, ClassNotFoundException
{    try {                __isset_bitfield = 0;        read(new org.apache.thrift.protocol.TCompactProtocol(new org.apache.thrift.transport.TIOStreamTransport(in)));    } catch (org.apache.thrift.TException te) {        throw new java.io.IOException(te);    }}
0
public ThriftFlumeEventStandardScheme getScheme()
{    return new ThriftFlumeEventStandardScheme();}
0
public void read(org.apache.thrift.protocol.TProtocol iprot, ThriftFlumeEvent struct) throws org.apache.thrift.TException
{    org.apache.thrift.protocol.TField schemeField;    iprot.readStructBegin();    while (true) {        schemeField = iprot.readFieldBegin();        if (schemeField.type == org.apache.thrift.protocol.TType.STOP) {            break;        }        switch(schemeField.id) {            case             1:                if (schemeField.type == org.apache.thrift.protocol.TType.I64) {                    struct.timestamp = iprot.readI64();                    struct.setTimestampIsSet(true);                } else {                    org.apache.thrift.protocol.TProtocolUtil.skip(iprot, schemeField.type);                }                break;            case             2:                if (schemeField.type == org.apache.thrift.protocol.TType.I32) {                    struct.priority = com.cloudera.flume.handlers.thrift.Priority.findByValue(iprot.readI32());                    struct.setPriorityIsSet(true);                } else {                    org.apache.thrift.protocol.TProtocolUtil.skip(iprot, schemeField.type);                }                break;            case             3:                if (schemeField.type == org.apache.thrift.protocol.TType.STRING) {                    struct.body = iprot.readBinary();                    struct.setBodyIsSet(true);                } else {                    org.apache.thrift.protocol.TProtocolUtil.skip(iprot, schemeField.type);                }                break;            case             4:                if (schemeField.type == org.apache.thrift.protocol.TType.I64) {                    struct.nanos = iprot.readI64();                    struct.setNanosIsSet(true);                } else {                    org.apache.thrift.protocol.TProtocolUtil.skip(iprot, schemeField.type);                }                break;            case             5:                if (schemeField.type == org.apache.thrift.protocol.TType.STRING) {                    struct.host = iprot.readString();                    struct.setHostIsSet(true);                } else {                    org.apache.thrift.protocol.TProtocolUtil.skip(iprot, schemeField.type);                }                break;            case             6:                if (schemeField.type == org.apache.thrift.protocol.TType.MAP) {                    {                        org.apache.thrift.protocol.TMap _map0 = iprot.readMapBegin();                        struct.fields = new HashMap<String, ByteBuffer>(2 * _map0.size);                        String _key1;                        ByteBuffer _val2;                        for (int _i3 = 0; _i3 < _map0.size; ++_i3) {                            _key1 = iprot.readString();                            _val2 = iprot.readBinary();                            struct.fields.put(_key1, _val2);                        }                        iprot.readMapEnd();                    }                    struct.setFieldsIsSet(true);                } else {                    org.apache.thrift.protocol.TProtocolUtil.skip(iprot, schemeField.type);                }                break;            default:                org.apache.thrift.protocol.TProtocolUtil.skip(iprot, schemeField.type);        }        iprot.readFieldEnd();    }    iprot.readStructEnd();        struct.validate();}
0
public void write(org.apache.thrift.protocol.TProtocol oprot, ThriftFlumeEvent struct) throws org.apache.thrift.TException
{    struct.validate();    oprot.writeStructBegin(STRUCT_DESC);    oprot.writeFieldBegin(TIMESTAMP_FIELD_DESC);    oprot.writeI64(struct.timestamp);    oprot.writeFieldEnd();    if (struct.priority != null) {        oprot.writeFieldBegin(PRIORITY_FIELD_DESC);        oprot.writeI32(struct.priority.getValue());        oprot.writeFieldEnd();    }    if (struct.body != null) {        oprot.writeFieldBegin(BODY_FIELD_DESC);        oprot.writeBinary(struct.body);        oprot.writeFieldEnd();    }    oprot.writeFieldBegin(NANOS_FIELD_DESC);    oprot.writeI64(struct.nanos);    oprot.writeFieldEnd();    if (struct.host != null) {        oprot.writeFieldBegin(HOST_FIELD_DESC);        oprot.writeString(struct.host);        oprot.writeFieldEnd();    }    if (struct.fields != null) {        oprot.writeFieldBegin(FIELDS_FIELD_DESC);        {            oprot.writeMapBegin(new org.apache.thrift.protocol.TMap(org.apache.thrift.protocol.TType.STRING, org.apache.thrift.protocol.TType.STRING, struct.fields.size()));            for (Map.Entry<String, ByteBuffer> _iter4 : struct.fields.entrySet()) {                oprot.writeString(_iter4.getKey());                oprot.writeBinary(_iter4.getValue());            }            oprot.writeMapEnd();        }        oprot.writeFieldEnd();    }    oprot.writeFieldStop();    oprot.writeStructEnd();}
0
public ThriftFlumeEventTupleScheme getScheme()
{    return new ThriftFlumeEventTupleScheme();}
0
public void write(org.apache.thrift.protocol.TProtocol prot, ThriftFlumeEvent struct) throws org.apache.thrift.TException
{    TTupleProtocol oprot = (TTupleProtocol) prot;    BitSet optionals = new BitSet();    if (struct.isSetTimestamp()) {        optionals.set(0);    }    if (struct.isSetPriority()) {        optionals.set(1);    }    if (struct.isSetBody()) {        optionals.set(2);    }    if (struct.isSetNanos()) {        optionals.set(3);    }    if (struct.isSetHost()) {        optionals.set(4);    }    if (struct.isSetFields()) {        optionals.set(5);    }    oprot.writeBitSet(optionals, 6);    if (struct.isSetTimestamp()) {        oprot.writeI64(struct.timestamp);    }    if (struct.isSetPriority()) {        oprot.writeI32(struct.priority.getValue());    }    if (struct.isSetBody()) {        oprot.writeBinary(struct.body);    }    if (struct.isSetNanos()) {        oprot.writeI64(struct.nanos);    }    if (struct.isSetHost()) {        oprot.writeString(struct.host);    }    if (struct.isSetFields()) {        {            oprot.writeI32(struct.fields.size());            for (Map.Entry<String, ByteBuffer> _iter5 : struct.fields.entrySet()) {                oprot.writeString(_iter5.getKey());                oprot.writeBinary(_iter5.getValue());            }        }    }}
0
public void read(org.apache.thrift.protocol.TProtocol prot, ThriftFlumeEvent struct) throws org.apache.thrift.TException
{    TTupleProtocol iprot = (TTupleProtocol) prot;    BitSet incoming = iprot.readBitSet(6);    if (incoming.get(0)) {        struct.timestamp = iprot.readI64();        struct.setTimestampIsSet(true);    }    if (incoming.get(1)) {        struct.priority = com.cloudera.flume.handlers.thrift.Priority.findByValue(iprot.readI32());        struct.setPriorityIsSet(true);    }    if (incoming.get(2)) {        struct.body = iprot.readBinary();        struct.setBodyIsSet(true);    }    if (incoming.get(3)) {        struct.nanos = iprot.readI64();        struct.setNanosIsSet(true);    }    if (incoming.get(4)) {        struct.host = iprot.readString();        struct.setHostIsSet(true);    }    if (incoming.get(5)) {        {            org.apache.thrift.protocol.TMap _map6 = new org.apache.thrift.protocol.TMap(org.apache.thrift.protocol.TType.STRING, org.apache.thrift.protocol.TType.STRING, iprot.readI32());            struct.fields = new HashMap<String, ByteBuffer>(2 * _map6.size);            String _key7;            ByteBuffer _val8;            for (int _i9 = 0; _i9 < _map6.size; ++_i9) {                _key7 = iprot.readString();                _val8 = iprot.readBinary();                struct.fields.put(_key7, _val8);            }        }        struct.setFieldsIsSet(true);    }}
0
public Client getClient(org.apache.thrift.protocol.TProtocol prot)
{    return new Client(prot);}
0
public Client getClient(org.apache.thrift.protocol.TProtocol iprot, org.apache.thrift.protocol.TProtocol oprot)
{    return new Client(iprot, oprot);}
0
public void append(ThriftFlumeEvent evt) throws org.apache.thrift.TException
{    send_append(evt);}
0
public void send_append(ThriftFlumeEvent evt) throws org.apache.thrift.TException
{    append_args args = new append_args();    args.setEvt(evt);    sendBaseOneway("append", args);}
0
public void close() throws org.apache.thrift.TException
{    send_close();    recv_close();}
0
public void send_close() throws org.apache.thrift.TException
{    close_args args = new close_args();    sendBase("close", args);}
0
public void recv_close() throws org.apache.thrift.TException
{    close_result result = new close_result();    receiveBase(result, "close");    return;}
0
public AsyncClient getAsyncClient(org.apache.thrift.transport.TNonblockingTransport transport)
{    return new AsyncClient(protocolFactory, clientManager, transport);}
0
public void append(ThriftFlumeEvent evt, org.apache.thrift.async.AsyncMethodCallback resultHandler) throws org.apache.thrift.TException
{    checkReady();    append_call method_call = new append_call(evt, resultHandler, this, ___protocolFactory, ___transport);    this.___currentMethod = method_call;    ___manager.call(method_call);}
0
public void write_args(org.apache.thrift.protocol.TProtocol prot) throws org.apache.thrift.TException
{    prot.writeMessageBegin(new org.apache.thrift.protocol.TMessage("append", org.apache.thrift.protocol.TMessageType.ONEWAY, 0));    append_args args = new append_args();    args.setEvt(evt);    args.write(prot);    prot.writeMessageEnd();}
0
public void getResult() throws org.apache.thrift.TException
{    if (getState() != org.apache.thrift.async.TAsyncMethodCall.State.RESPONSE_READ) {        throw new IllegalStateException("Method call not finished!");    }    org.apache.thrift.transport.TMemoryInputTransport memoryTransport = new org.apache.thrift.transport.TMemoryInputTransport(getFrameBuffer().array());    org.apache.thrift.protocol.TProtocol prot = client.getProtocolFactory().getProtocol(memoryTransport);}
0
public void close(org.apache.thrift.async.AsyncMethodCallback resultHandler) throws org.apache.thrift.TException
{    checkReady();    close_call method_call = new close_call(resultHandler, this, ___protocolFactory, ___transport);    this.___currentMethod = method_call;    ___manager.call(method_call);}
0
public void write_args(org.apache.thrift.protocol.TProtocol prot) throws org.apache.thrift.TException
{    prot.writeMessageBegin(new org.apache.thrift.protocol.TMessage("close", org.apache.thrift.protocol.TMessageType.CALL, 0));    close_args args = new close_args();    args.write(prot);    prot.writeMessageEnd();}
0
public void getResult() throws org.apache.thrift.TException
{    if (getState() != org.apache.thrift.async.TAsyncMethodCall.State.RESPONSE_READ) {        throw new IllegalStateException("Method call not finished!");    }    org.apache.thrift.transport.TMemoryInputTransport memoryTransport = new org.apache.thrift.transport.TMemoryInputTransport(getFrameBuffer().array());    org.apache.thrift.protocol.TProtocol prot = client.getProtocolFactory().getProtocol(memoryTransport);    (new Client(prot)).recv_close();}
0
private static Map<String, org.apache.thrift.ProcessFunction<I, ? extends org.apache.thrift.TBase>> getProcessMap(Map<String, org.apache.thrift.ProcessFunction<I, ? extends org.apache.thrift.TBase>> processMap)
{    processMap.put("append", new append());    processMap.put("close", new close());    return processMap;}
0
public append_args getEmptyArgsInstance()
{    return new append_args();}
0
protected boolean isOneway()
{    return true;}
0
public org.apache.thrift.TBase getResult(I iface, append_args args) throws org.apache.thrift.TException
{    iface.append(args.evt);    return null;}
0
public close_args getEmptyArgsInstance()
{    return new close_args();}
0
protected boolean isOneway()
{    return false;}
0
public close_result getResult(I iface, close_args args) throws org.apache.thrift.TException
{    close_result result = new close_result();    iface.close();    return result;}
0
private static Map<String, org.apache.thrift.AsyncProcessFunction<I, ? extends org.apache.thrift.TBase, ?>> getProcessMap(Map<String, org.apache.thrift.AsyncProcessFunction<I, ? extends org.apache.thrift.TBase, ?>> processMap)
{    processMap.put("append", new append());    processMap.put("close", new close());    return processMap;}
0
public append_args getEmptyArgsInstance()
{    return new append_args();}
0
public AsyncMethodCallback<Void> getResultHandler(final AsyncFrameBuffer fb, final int seqid)
{    final org.apache.thrift.AsyncProcessFunction fcall = this;    return new AsyncMethodCallback<Void>() {        public void onComplete(Void o) {        }        public void onError(Exception e) {        }    };}
0
public void onComplete(Void o)
{}
0
public void onError(Exception e)
{}
0
protected boolean isOneway()
{    return true;}
0
public void start(I iface, append_args args, org.apache.thrift.async.AsyncMethodCallback<Void> resultHandler) throws TException
{    iface.append(args.evt, resultHandler);}
0
public close_args getEmptyArgsInstance()
{    return new close_args();}
0
public AsyncMethodCallback<Void> getResultHandler(final AsyncFrameBuffer fb, final int seqid)
{    final org.apache.thrift.AsyncProcessFunction fcall = this;    return new AsyncMethodCallback<Void>() {        public void onComplete(Void o) {            close_result result = new close_result();            try {                fcall.sendResponse(fb, result, org.apache.thrift.protocol.TMessageType.REPLY, seqid);                return;            } catch (Exception e) {                            }            fb.close();        }        public void onError(Exception e) {            byte msgType = org.apache.thrift.protocol.TMessageType.REPLY;            org.apache.thrift.TBase msg;            close_result result = new close_result();            {                msgType = org.apache.thrift.protocol.TMessageType.EXCEPTION;                msg = (org.apache.thrift.TBase) new org.apache.thrift.TApplicationException(org.apache.thrift.TApplicationException.INTERNAL_ERROR, e.getMessage());            }            try {                fcall.sendResponse(fb, msg, msgType, seqid);                return;            } catch (Exception ex) {                            }            fb.close();        }    };}
1
public void onComplete(Void o)
{    close_result result = new close_result();    try {        fcall.sendResponse(fb, result, org.apache.thrift.protocol.TMessageType.REPLY, seqid);        return;    } catch (Exception e) {            }    fb.close();}
1
public void onError(Exception e)
{    byte msgType = org.apache.thrift.protocol.TMessageType.REPLY;    org.apache.thrift.TBase msg;    close_result result = new close_result();    {        msgType = org.apache.thrift.protocol.TMessageType.EXCEPTION;        msg = (org.apache.thrift.TBase) new org.apache.thrift.TApplicationException(org.apache.thrift.TApplicationException.INTERNAL_ERROR, e.getMessage());    }    try {        fcall.sendResponse(fb, msg, msgType, seqid);        return;    } catch (Exception ex) {            }    fb.close();}
1
protected boolean isOneway()
{    return false;}
0
public void start(I iface, close_args args, org.apache.thrift.async.AsyncMethodCallback<Void> resultHandler) throws TException
{    iface.close(resultHandler);}
0
public static _Fields findByThriftId(int fieldId)
{    switch(fieldId) {        case         1:            return EVT;        default:            return null;    }}
0
public static _Fields findByThriftIdOrThrow(int fieldId)
{    _Fields fields = findByThriftId(fieldId);    if (fields == null)        throw new IllegalArgumentException("Field " + fieldId + " doesn't exist!");    return fields;}
0
public static _Fields findByName(String name)
{    return byName.get(name);}
0
public short getThriftFieldId()
{    return _thriftId;}
0
public String getFieldName()
{    return _fieldName;}
0
public append_args deepCopy()
{    return new append_args(this);}
0
public void clear()
{    this.evt = null;}
0
public ThriftFlumeEvent getEvt()
{    return this.evt;}
0
public append_args setEvt(ThriftFlumeEvent evt)
{    this.evt = evt;    return this;}
0
public void unsetEvt()
{    this.evt = null;}
0
public boolean isSetEvt()
{    return this.evt != null;}
0
public void setEvtIsSet(boolean value)
{    if (!value) {        this.evt = null;    }}
0
public void setFieldValue(_Fields field, Object value)
{    switch(field) {        case EVT:            if (value == null) {                unsetEvt();            } else {                setEvt((ThriftFlumeEvent) value);            }            break;    }}
0
public Object getFieldValue(_Fields field)
{    switch(field) {        case EVT:            return getEvt();    }    throw new IllegalStateException();}
0
public boolean isSet(_Fields field)
{    if (field == null) {        throw new IllegalArgumentException();    }    switch(field) {        case EVT:            return isSetEvt();    }    throw new IllegalStateException();}
0
public boolean equals(Object that)
{    if (that == null)        return false;    if (that instanceof append_args)        return this.equals((append_args) that);    return false;}
0
public boolean equals(append_args that)
{    if (that == null)        return false;    boolean this_present_evt = true && this.isSetEvt();    boolean that_present_evt = true && that.isSetEvt();    if (this_present_evt || that_present_evt) {        if (!(this_present_evt && that_present_evt))            return false;        if (!this.evt.equals(that.evt))            return false;    }    return true;}
0
public int hashCode()
{    List<Object> list = new ArrayList<Object>();    boolean present_evt = true && (isSetEvt());    list.add(present_evt);    if (present_evt)        list.add(evt);    return list.hashCode();}
0
public int compareTo(append_args other)
{    if (!getClass().equals(other.getClass())) {        return getClass().getName().compareTo(other.getClass().getName());    }    int lastComparison = 0;    lastComparison = Boolean.valueOf(isSetEvt()).compareTo(other.isSetEvt());    if (lastComparison != 0) {        return lastComparison;    }    if (isSetEvt()) {        lastComparison = org.apache.thrift.TBaseHelper.compareTo(this.evt, other.evt);        if (lastComparison != 0) {            return lastComparison;        }    }    return 0;}
0
public _Fields fieldForId(int fieldId)
{    return _Fields.findByThriftId(fieldId);}
0
public void read(org.apache.thrift.protocol.TProtocol iprot) throws org.apache.thrift.TException
{    schemes.get(iprot.getScheme()).getScheme().read(iprot, this);}
0
public void write(org.apache.thrift.protocol.TProtocol oprot) throws org.apache.thrift.TException
{    schemes.get(oprot.getScheme()).getScheme().write(oprot, this);}
0
public String toString()
{    StringBuilder sb = new StringBuilder("append_args(");    boolean first = true;    sb.append("evt:");    if (this.evt == null) {        sb.append("null");    } else {        sb.append(this.evt);    }    first = false;    sb.append(")");    return sb.toString();}
0
public void validate() throws org.apache.thrift.TException
{        if (evt != null) {        evt.validate();    }}
0
private void writeObject(java.io.ObjectOutputStream out) throws java.io.IOException
{    try {        write(new org.apache.thrift.protocol.TCompactProtocol(new org.apache.thrift.transport.TIOStreamTransport(out)));    } catch (org.apache.thrift.TException te) {        throw new java.io.IOException(te);    }}
0
private void readObject(java.io.ObjectInputStream in) throws java.io.IOException, ClassNotFoundException
{    try {        read(new org.apache.thrift.protocol.TCompactProtocol(new org.apache.thrift.transport.TIOStreamTransport(in)));    } catch (org.apache.thrift.TException te) {        throw new java.io.IOException(te);    }}
0
public append_argsStandardScheme getScheme()
{    return new append_argsStandardScheme();}
0
public void read(org.apache.thrift.protocol.TProtocol iprot, append_args struct) throws org.apache.thrift.TException
{    org.apache.thrift.protocol.TField schemeField;    iprot.readStructBegin();    while (true) {        schemeField = iprot.readFieldBegin();        if (schemeField.type == org.apache.thrift.protocol.TType.STOP) {            break;        }        switch(schemeField.id) {            case             1:                if (schemeField.type == org.apache.thrift.protocol.TType.STRUCT) {                    struct.evt = new ThriftFlumeEvent();                    struct.evt.read(iprot);                    struct.setEvtIsSet(true);                } else {                    org.apache.thrift.protocol.TProtocolUtil.skip(iprot, schemeField.type);                }                break;            default:                org.apache.thrift.protocol.TProtocolUtil.skip(iprot, schemeField.type);        }        iprot.readFieldEnd();    }    iprot.readStructEnd();        struct.validate();}
0
public void write(org.apache.thrift.protocol.TProtocol oprot, append_args struct) throws org.apache.thrift.TException
{    struct.validate();    oprot.writeStructBegin(STRUCT_DESC);    if (struct.evt != null) {        oprot.writeFieldBegin(EVT_FIELD_DESC);        struct.evt.write(oprot);        oprot.writeFieldEnd();    }    oprot.writeFieldStop();    oprot.writeStructEnd();}
0
public append_argsTupleScheme getScheme()
{    return new append_argsTupleScheme();}
0
public void write(org.apache.thrift.protocol.TProtocol prot, append_args struct) throws org.apache.thrift.TException
{    TTupleProtocol oprot = (TTupleProtocol) prot;    BitSet optionals = new BitSet();    if (struct.isSetEvt()) {        optionals.set(0);    }    oprot.writeBitSet(optionals, 1);    if (struct.isSetEvt()) {        struct.evt.write(oprot);    }}
0
public void read(org.apache.thrift.protocol.TProtocol prot, append_args struct) throws org.apache.thrift.TException
{    TTupleProtocol iprot = (TTupleProtocol) prot;    BitSet incoming = iprot.readBitSet(1);    if (incoming.get(0)) {        struct.evt = new ThriftFlumeEvent();        struct.evt.read(iprot);        struct.setEvtIsSet(true);    }}
0
public static _Fields findByThriftId(int fieldId)
{    switch(fieldId) {        default:            return null;    }}
0
public static _Fields findByThriftIdOrThrow(int fieldId)
{    _Fields fields = findByThriftId(fieldId);    if (fields == null)        throw new IllegalArgumentException("Field " + fieldId + " doesn't exist!");    return fields;}
0
public static _Fields findByName(String name)
{    return byName.get(name);}
0
public short getThriftFieldId()
{    return _thriftId;}
0
public String getFieldName()
{    return _fieldName;}
0
public close_args deepCopy()
{    return new close_args(this);}
0
public void clear()
{}
0
public void setFieldValue(_Fields field, Object value)
{    switch(field) {    }}
0
public Object getFieldValue(_Fields field)
{    switch(field) {    }    throw new IllegalStateException();}
0
public boolean isSet(_Fields field)
{    if (field == null) {        throw new IllegalArgumentException();    }    switch(field) {    }    throw new IllegalStateException();}
0
public boolean equals(Object that)
{    if (that == null)        return false;    if (that instanceof close_args)        return this.equals((close_args) that);    return false;}
0
public boolean equals(close_args that)
{    if (that == null)        return false;    return true;}
0
public int hashCode()
{    List<Object> list = new ArrayList<Object>();    return list.hashCode();}
0
public int compareTo(close_args other)
{    if (!getClass().equals(other.getClass())) {        return getClass().getName().compareTo(other.getClass().getName());    }    int lastComparison = 0;    return 0;}
0
public _Fields fieldForId(int fieldId)
{    return _Fields.findByThriftId(fieldId);}
0
public void read(org.apache.thrift.protocol.TProtocol iprot) throws org.apache.thrift.TException
{    schemes.get(iprot.getScheme()).getScheme().read(iprot, this);}
0
public void write(org.apache.thrift.protocol.TProtocol oprot) throws org.apache.thrift.TException
{    schemes.get(oprot.getScheme()).getScheme().write(oprot, this);}
0
public String toString()
{    StringBuilder sb = new StringBuilder("close_args(");    boolean first = true;    sb.append(")");    return sb.toString();}
0
public void validate() throws org.apache.thrift.TException
{}
0
private void writeObject(java.io.ObjectOutputStream out) throws java.io.IOException
{    try {        write(new org.apache.thrift.protocol.TCompactProtocol(new org.apache.thrift.transport.TIOStreamTransport(out)));    } catch (org.apache.thrift.TException te) {        throw new java.io.IOException(te);    }}
0
private void readObject(java.io.ObjectInputStream in) throws java.io.IOException, ClassNotFoundException
{    try {        read(new org.apache.thrift.protocol.TCompactProtocol(new org.apache.thrift.transport.TIOStreamTransport(in)));    } catch (org.apache.thrift.TException te) {        throw new java.io.IOException(te);    }}
0
public close_argsStandardScheme getScheme()
{    return new close_argsStandardScheme();}
0
public void read(org.apache.thrift.protocol.TProtocol iprot, close_args struct) throws org.apache.thrift.TException
{    org.apache.thrift.protocol.TField schemeField;    iprot.readStructBegin();    while (true) {        schemeField = iprot.readFieldBegin();        if (schemeField.type == org.apache.thrift.protocol.TType.STOP) {            break;        }        switch(schemeField.id) {            default:                org.apache.thrift.protocol.TProtocolUtil.skip(iprot, schemeField.type);        }        iprot.readFieldEnd();    }    iprot.readStructEnd();        struct.validate();}
0
public void write(org.apache.thrift.protocol.TProtocol oprot, close_args struct) throws org.apache.thrift.TException
{    struct.validate();    oprot.writeStructBegin(STRUCT_DESC);    oprot.writeFieldStop();    oprot.writeStructEnd();}
0
public close_argsTupleScheme getScheme()
{    return new close_argsTupleScheme();}
0
public void write(org.apache.thrift.protocol.TProtocol prot, close_args struct) throws org.apache.thrift.TException
{    TTupleProtocol oprot = (TTupleProtocol) prot;}
0
public void read(org.apache.thrift.protocol.TProtocol prot, close_args struct) throws org.apache.thrift.TException
{    TTupleProtocol iprot = (TTupleProtocol) prot;}
0
public static _Fields findByThriftId(int fieldId)
{    switch(fieldId) {        default:            return null;    }}
0
public static _Fields findByThriftIdOrThrow(int fieldId)
{    _Fields fields = findByThriftId(fieldId);    if (fields == null)        throw new IllegalArgumentException("Field " + fieldId + " doesn't exist!");    return fields;}
0
public static _Fields findByName(String name)
{    return byName.get(name);}
0
public short getThriftFieldId()
{    return _thriftId;}
0
public String getFieldName()
{    return _fieldName;}
0
public close_result deepCopy()
{    return new close_result(this);}
0
public void clear()
{}
0
public void setFieldValue(_Fields field, Object value)
{    switch(field) {    }}
0
public Object getFieldValue(_Fields field)
{    switch(field) {    }    throw new IllegalStateException();}
0
public boolean isSet(_Fields field)
{    if (field == null) {        throw new IllegalArgumentException();    }    switch(field) {    }    throw new IllegalStateException();}
0
public boolean equals(Object that)
{    if (that == null)        return false;    if (that instanceof close_result)        return this.equals((close_result) that);    return false;}
0
public boolean equals(close_result that)
{    if (that == null)        return false;    return true;}
0
public int hashCode()
{    List<Object> list = new ArrayList<Object>();    return list.hashCode();}
0
public int compareTo(close_result other)
{    if (!getClass().equals(other.getClass())) {        return getClass().getName().compareTo(other.getClass().getName());    }    int lastComparison = 0;    return 0;}
0
public _Fields fieldForId(int fieldId)
{    return _Fields.findByThriftId(fieldId);}
0
public void read(org.apache.thrift.protocol.TProtocol iprot) throws org.apache.thrift.TException
{    schemes.get(iprot.getScheme()).getScheme().read(iprot, this);}
0
public void write(org.apache.thrift.protocol.TProtocol oprot) throws org.apache.thrift.TException
{    schemes.get(oprot.getScheme()).getScheme().write(oprot, this);}
0
public String toString()
{    StringBuilder sb = new StringBuilder("close_result(");    boolean first = true;    sb.append(")");    return sb.toString();}
0
public void validate() throws org.apache.thrift.TException
{}
0
private void writeObject(java.io.ObjectOutputStream out) throws java.io.IOException
{    try {        write(new org.apache.thrift.protocol.TCompactProtocol(new org.apache.thrift.transport.TIOStreamTransport(out)));    } catch (org.apache.thrift.TException te) {        throw new java.io.IOException(te);    }}
0
private void readObject(java.io.ObjectInputStream in) throws java.io.IOException, ClassNotFoundException
{    try {        read(new org.apache.thrift.protocol.TCompactProtocol(new org.apache.thrift.transport.TIOStreamTransport(in)));    } catch (org.apache.thrift.TException te) {        throw new java.io.IOException(te);    }}
0
public close_resultStandardScheme getScheme()
{    return new close_resultStandardScheme();}
0
public void read(org.apache.thrift.protocol.TProtocol iprot, close_result struct) throws org.apache.thrift.TException
{    org.apache.thrift.protocol.TField schemeField;    iprot.readStructBegin();    while (true) {        schemeField = iprot.readFieldBegin();        if (schemeField.type == org.apache.thrift.protocol.TType.STOP) {            break;        }        switch(schemeField.id) {            default:                org.apache.thrift.protocol.TProtocolUtil.skip(iprot, schemeField.type);        }        iprot.readFieldEnd();    }    iprot.readStructEnd();        struct.validate();}
0
public void write(org.apache.thrift.protocol.TProtocol oprot, close_result struct) throws org.apache.thrift.TException
{    struct.validate();    oprot.writeStructBegin(STRUCT_DESC);    oprot.writeFieldStop();    oprot.writeStructEnd();}
0
public close_resultTupleScheme getScheme()
{    return new close_resultTupleScheme();}
0
public void write(org.apache.thrift.protocol.TProtocol prot, close_result struct) throws org.apache.thrift.TException
{    TTupleProtocol oprot = (TTupleProtocol) prot;}
0
public void read(org.apache.thrift.protocol.TProtocol prot, close_result struct) throws org.apache.thrift.TException
{    TTupleProtocol iprot = (TTupleProtocol) prot;}
0
public void append(ThriftFlumeEvent evt)
{    if (evt == null) {        return;    }    Map<String, String> headers = new HashMap<String, String>();        headers.put(HOST, evt.getHost());    headers.put(TIMESTAMP, Long.toString(evt.getTimestamp()));    headers.put(PRIORITY, evt.getPriority().toString());    headers.put(NANOS, Long.toString(evt.getNanos()));    for (Entry<String, ByteBuffer> entry : evt.getFields().entrySet()) {        headers.put(entry.getKey().toString(), UTF_8.decode(entry.getValue()).toString());    }    headers.put(OG_EVENT, "yes");    Event event = EventBuilder.withBody(evt.getBody(), headers);    counterGroup.incrementAndGet("rpc.events");    try {        getChannelProcessor().processEvent(event);    } catch (ChannelException ex) {                return;    }    counterGroup.incrementAndGet("rpc.successful");    return;}
1
public void close()
{}
0
public void run()
{    server.serve();}
0
public void configure(Context context)
{    port = Integer.parseInt(context.getString("port"));    host = context.getString("host");}
0
public void start()
{    try {        InetSocketAddress bindAddr = new InetSocketAddress(host, port);        serverTransport = new TServerSocket(bindAddr);        ThriftFlumeEventServer.Processor processor = new ThriftFlumeEventServer.Processor(new ThriftFlumeEventServerImpl());        server = new TThreadPoolServer(new TThreadPoolServer.Args(serverTransport).processor(processor));    } catch (TTransportException e) {        throw new FlumeException("Failed starting source", e);    }    ThriftHandler thriftHandler = new ThriftHandler(server);    thriftHandlerThread = new Thread(thriftHandler);    thriftHandlerThread.start();    super.start();}
0
public void stop()
{    server.stop();    serverTransport.close();    try {        thriftHandlerThread.join();    } catch (InterruptedException eI) {                return;    }    super.stop();}
1
public void append(ThriftFlumeEvent evt)
{    TTransport transport;    try {        transport = new TSocket(host, port);        TProtocol protocol = new TBinaryProtocol(transport);        Client client = new Client(protocol);        transport.open();        client.append(evt);        transport.close();    } catch (TTransportException e) {        e.printStackTrace();    } catch (TException e) {        e.printStackTrace();    }}
0
public void setUp() throws Exception
{    source = new ThriftLegacySource();    channel = new MemoryChannel();    Configurables.configure(channel, new Context());    List<Channel> channels = new ArrayList<Channel>();    channels.add(channel);    ChannelSelector rcs = new ReplicatingChannelSelector();    rcs.setChannels(channels);    source.setChannelProcessor(new ChannelProcessor(rcs));    try (ServerSocket socket = new ServerSocket(0)) {        selectedPort = socket.getLocalPort();    }}
0
private void bind() throws InterruptedException
{    Context context = new Context();    context.put("port", String.valueOf(selectedPort));    context.put("host", "0.0.0.0");    Configurables.configure(source, context);    source.start();    Assert.assertTrue("Reached start or error", LifecycleController.waitForOneOf(source, LifecycleState.START_OR_ERROR));    Assert.assertEquals("Server is started", LifecycleState.START, source.getLifecycleState());}
0
private void stop() throws InterruptedException
{    source.stop();    Assert.assertTrue("Reached stop or error", LifecycleController.waitForOneOf(source, LifecycleState.STOP_OR_ERROR));    Assert.assertEquals("Server is stopped", LifecycleState.STOP, source.getLifecycleState());}
0
public void testLifecycle() throws InterruptedException
{    bind();    stop();}
0
public void testRequest() throws InterruptedException, IOException
{    bind();    Map<String, ByteBuffer> flumeMap = new HashMap<>();    ThriftFlumeEvent thriftEvent = new ThriftFlumeEvent(1, Priority.INFO, ByteBuffer.wrap("foo".getBytes()), 0, "fooHost", flumeMap);    FlumeClient fClient = new FlumeClient("0.0.0.0", selectedPort);    fClient.append(thriftEvent);        Transaction transaction = channel.getTransaction();    transaction.begin();    Event event = channel.take();    Assert.assertNotNull(event);    Assert.assertEquals("Channel contained our event", "foo", new String(event.getBody()));    transaction.commit();    transaction.close();    stop();}
0
public void testHeaders() throws InterruptedException, IOException
{    bind();    Map<String, ByteBuffer> flumeHeaders = new HashMap<>();    flumeHeaders.put("hello", ByteBuffer.wrap("world".getBytes("UTF-8")));    ThriftFlumeEvent thriftEvent = new ThriftFlumeEvent(1, Priority.INFO, ByteBuffer.wrap("foo".getBytes()), 0, "fooHost", flumeHeaders);    FlumeClient fClient = new FlumeClient("0.0.0.0", selectedPort);    fClient.append(thriftEvent);        Transaction transaction = channel.getTransaction();    transaction.begin();    Event event = channel.take();    Assert.assertNotNull(event);    Assert.assertEquals("Event in channel has our header", "world", event.getHeaders().get("hello"));    transaction.commit();    transaction.close();    stop();}
0
public MaterializedConfiguration getConfiguration()
{    MaterializedConfiguration conf = new SimpleMaterializedConfiguration();    FlumeConfiguration fconfig = getFlumeConfiguration();    AgentConfiguration agentConf = fconfig.getConfigurationFor(getAgentName());    if (agentConf != null) {        Map<String, ChannelComponent> channelComponentMap = Maps.newHashMap();        Map<String, SourceRunner> sourceRunnerMap = Maps.newHashMap();        Map<String, SinkRunner> sinkRunnerMap = Maps.newHashMap();        try {            loadChannels(agentConf, channelComponentMap);            loadSources(agentConf, channelComponentMap, sourceRunnerMap);            loadSinks(agentConf, channelComponentMap, sinkRunnerMap);            Set<String> channelNames = new HashSet<String>(channelComponentMap.keySet());            for (String channelName : channelNames) {                ChannelComponent channelComponent = channelComponentMap.get(channelName);                if (channelComponent.components.isEmpty()) {                                        channelComponentMap.remove(channelName);                    Map<String, Channel> nameChannelMap = channelCache.get(channelComponent.channel.getClass());                    if (nameChannelMap != null) {                        nameChannelMap.remove(channelName);                    }                } else {                                        conf.addChannel(channelName, channelComponent.channel);                }            }            for (Map.Entry<String, SourceRunner> entry : sourceRunnerMap.entrySet()) {                conf.addSourceRunner(entry.getKey(), entry.getValue());            }            for (Map.Entry<String, SinkRunner> entry : sinkRunnerMap.entrySet()) {                conf.addSinkRunner(entry.getKey(), entry.getValue());            }        } catch (InstantiationException ex) {                    } finally {            channelComponentMap.clear();            sourceRunnerMap.clear();            sinkRunnerMap.clear();        }    } else {            }    return conf;}
1
public String getAgentName()
{    return agentName;}
0
private void loadChannels(AgentConfiguration agentConf, Map<String, ChannelComponent> channelComponentMap) throws InstantiationException
{        /*     * Some channels will be reused across re-configurations. To handle this,     * we store all the names of current channels, perform the reconfiguration,     * and then if a channel was not used, we delete our reference to it.     * This supports the scenario where you enable channel "ch0" then remove it     * and add it back. Without this, channels like memory channel would cause     * the first instances data to show up in the seconds.     */    ListMultimap<Class<? extends Channel>, String> channelsNotReused = ArrayListMultimap.create();        for (Map.Entry<Class<? extends Channel>, Map<String, Channel>> entry : channelCache.entrySet()) {        Class<? extends Channel> channelKlass = entry.getKey();        Set<String> channelNames = entry.getValue().keySet();        channelsNotReused.get(channelKlass).addAll(channelNames);    }    Set<String> channelNames = agentConf.getChannelSet();    Map<String, ComponentConfiguration> compMap = agentConf.getChannelConfigMap();    /*     * Components which have a ComponentConfiguration object     */    for (String chName : channelNames) {        ComponentConfiguration comp = compMap.get(chName);        if (comp != null) {            Channel channel = getOrCreateChannel(channelsNotReused, comp.getComponentName(), comp.getType());            try {                Configurables.configure(channel, comp);                channelComponentMap.put(comp.getComponentName(), new ChannelComponent(channel));                            } catch (Exception e) {                String msg = String.format("Channel %s has been removed due to an " + "error during configuration", chName);                            }        }    }    /*     * Components which DO NOT have a ComponentConfiguration object     * and use only Context     */    for (String chName : channelNames) {        Context context = agentConf.getChannelContext().get(chName);        if (context != null) {            Channel channel = getOrCreateChannel(channelsNotReused, chName, context.getString(BasicConfigurationConstants.CONFIG_TYPE));            try {                Configurables.configure(channel, context);                channelComponentMap.put(chName, new ChannelComponent(channel));                            } catch (Exception e) {                String msg = String.format("Channel %s has been removed due to an " + "error during configuration", chName);                            }        }    }    /*     * Any channel which was not re-used, will have it's reference removed     */    for (Class<? extends Channel> channelKlass : channelsNotReused.keySet()) {        Map<String, Channel> channelMap = channelCache.get(channelKlass);        if (channelMap != null) {            for (String channelName : channelsNotReused.get(channelKlass)) {                if (channelMap.remove(channelName) != null) {                                    }            }            if (channelMap.isEmpty()) {                channelCache.remove(channelKlass);            }        }    }}
1
private Channel getOrCreateChannel(ListMultimap<Class<? extends Channel>, String> channelsNotReused, String name, String type) throws FlumeException
{    Class<? extends Channel> channelClass = channelFactory.getClass(type);    /*     * Channel has requested a new instance on each re-configuration     */    if (channelClass.isAnnotationPresent(Disposable.class)) {        Channel channel = channelFactory.create(name, type);        channel.setName(name);        return channel;    }    Map<String, Channel> channelMap = channelCache.get(channelClass);    if (channelMap == null) {        channelMap = new HashMap<String, Channel>();        channelCache.put(channelClass, channelMap);    }    Channel channel = channelMap.get(name);    if (channel == null) {        channel = channelFactory.create(name, type);        channel.setName(name);        channelMap.put(name, channel);    }    channelsNotReused.get(channelClass).remove(name);    return channel;}
0
private void loadSources(AgentConfiguration agentConf, Map<String, ChannelComponent> channelComponentMap, Map<String, SourceRunner> sourceRunnerMap) throws InstantiationException
{    Set<String> sourceNames = agentConf.getSourceSet();    Map<String, ComponentConfiguration> compMap = agentConf.getSourceConfigMap();    /*     * Components which have a ComponentConfiguration object     */    for (String sourceName : sourceNames) {        ComponentConfiguration comp = compMap.get(sourceName);        if (comp != null) {            SourceConfiguration config = (SourceConfiguration) comp;            Source source = sourceFactory.create(comp.getComponentName(), comp.getType());            try {                Configurables.configure(source, config);                Set<String> channelNames = config.getChannels();                List<Channel> sourceChannels = getSourceChannels(channelComponentMap, source, channelNames);                if (sourceChannels.isEmpty()) {                    String msg = String.format("Source %s is not connected to a " + "channel", sourceName);                    throw new IllegalStateException(msg);                }                ChannelSelectorConfiguration selectorConfig = config.getSelectorConfiguration();                ChannelSelector selector = ChannelSelectorFactory.create(sourceChannels, selectorConfig);                ChannelProcessor channelProcessor = new ChannelProcessor(selector);                Configurables.configure(channelProcessor, config);                source.setChannelProcessor(channelProcessor);                sourceRunnerMap.put(comp.getComponentName(), SourceRunner.forSource(source));                for (Channel channel : sourceChannels) {                    ChannelComponent channelComponent = Preconditions.checkNotNull(channelComponentMap.get(channel.getName()), String.format("Channel %s", channel.getName()));                    channelComponent.components.add(sourceName);                }            } catch (Exception e) {                String msg = String.format("Source %s has been removed due to an " + "error during configuration", sourceName);                            }        }    }    /*     * Components which DO NOT have a ComponentConfiguration object     * and use only Context     */    Map<String, Context> sourceContexts = agentConf.getSourceContext();    for (String sourceName : sourceNames) {        Context context = sourceContexts.get(sourceName);        if (context != null) {            Source source = sourceFactory.create(sourceName, context.getString(BasicConfigurationConstants.CONFIG_TYPE));            try {                Configurables.configure(source, context);                String[] channelNames = context.getString(BasicConfigurationConstants.CONFIG_CHANNELS).split("\\s+");                List<Channel> sourceChannels = getSourceChannels(channelComponentMap, source, Arrays.asList(channelNames));                if (sourceChannels.isEmpty()) {                    String msg = String.format("Source %s is not connected to a " + "channel", sourceName);                    throw new IllegalStateException(msg);                }                Map<String, String> selectorConfig = context.getSubProperties(BasicConfigurationConstants.CONFIG_SOURCE_CHANNELSELECTOR_PREFIX);                ChannelSelector selector = ChannelSelectorFactory.create(sourceChannels, selectorConfig);                ChannelProcessor channelProcessor = new ChannelProcessor(selector);                Configurables.configure(channelProcessor, context);                source.setChannelProcessor(channelProcessor);                sourceRunnerMap.put(sourceName, SourceRunner.forSource(source));                for (Channel channel : sourceChannels) {                    ChannelComponent channelComponent = Preconditions.checkNotNull(channelComponentMap.get(channel.getName()), String.format("Channel %s", channel.getName()));                    channelComponent.components.add(sourceName);                }            } catch (Exception e) {                String msg = String.format("Source %s has been removed due to an " + "error during configuration", sourceName);                            }        }    }}
1
private List<Channel> getSourceChannels(Map<String, ChannelComponent> channelComponentMap, Source source, Collection<String> channelNames) throws InstantiationException
{    List<Channel> sourceChannels = new ArrayList<Channel>();    for (String chName : channelNames) {        ChannelComponent channelComponent = channelComponentMap.get(chName);        if (channelComponent != null) {            checkSourceChannelCompatibility(source, channelComponent.channel);            sourceChannels.add(channelComponent.channel);        }    }    return sourceChannels;}
0
private void checkSourceChannelCompatibility(Source source, Channel channel) throws InstantiationException
{    if (source instanceof BatchSizeSupported && channel instanceof TransactionCapacitySupported) {        long transCap = ((TransactionCapacitySupported) channel).getTransactionCapacity();        long batchSize = ((BatchSizeSupported) source).getBatchSize();        if (transCap < batchSize) {            String msg = String.format("Incompatible source and channel settings defined. " + "source's batch size is greater than the channels transaction capacity. " + "Source: %s, batch size = %d, channel %s, transaction capacity = %d", source.getName(), batchSize, channel.getName(), transCap);            throw new InstantiationException(msg);        }    }}
0
private void checkSinkChannelCompatibility(Sink sink, Channel channel) throws InstantiationException
{    if (sink instanceof BatchSizeSupported && channel instanceof TransactionCapacitySupported) {        long transCap = ((TransactionCapacitySupported) channel).getTransactionCapacity();        long batchSize = ((BatchSizeSupported) sink).getBatchSize();        if (transCap < batchSize) {            String msg = String.format("Incompatible sink and channel settings defined. " + "sink's batch size is greater than the channels transaction capacity. " + "Sink: %s, batch size = %d, channel %s, transaction capacity = %d", sink.getName(), batchSize, channel.getName(), transCap);            throw new InstantiationException(msg);        }    }}
0
private void loadSinks(AgentConfiguration agentConf, Map<String, ChannelComponent> channelComponentMap, Map<String, SinkRunner> sinkRunnerMap) throws InstantiationException
{    Set<String> sinkNames = agentConf.getSinkSet();    Map<String, ComponentConfiguration> compMap = agentConf.getSinkConfigMap();    Map<String, Sink> sinks = new HashMap<String, Sink>();    /*     * Components which have a ComponentConfiguration object     */    for (String sinkName : sinkNames) {        ComponentConfiguration comp = compMap.get(sinkName);        if (comp != null) {            SinkConfiguration config = (SinkConfiguration) comp;            Sink sink = sinkFactory.create(comp.getComponentName(), comp.getType());            try {                Configurables.configure(sink, config);                ChannelComponent channelComponent = channelComponentMap.get(config.getChannel());                if (channelComponent == null) {                    String msg = String.format("Sink %s is not connected to a " + "channel", sinkName);                    throw new IllegalStateException(msg);                }                checkSinkChannelCompatibility(sink, channelComponent.channel);                sink.setChannel(channelComponent.channel);                sinks.put(comp.getComponentName(), sink);                channelComponent.components.add(sinkName);            } catch (Exception e) {                String msg = String.format("Sink %s has been removed due to an " + "error during configuration", sinkName);                            }        }    }    /*     * Components which DO NOT have a ComponentConfiguration object     * and use only Context     */    Map<String, Context> sinkContexts = agentConf.getSinkContext();    for (String sinkName : sinkNames) {        Context context = sinkContexts.get(sinkName);        if (context != null) {            Sink sink = sinkFactory.create(sinkName, context.getString(BasicConfigurationConstants.CONFIG_TYPE));            try {                Configurables.configure(sink, context);                ChannelComponent channelComponent = channelComponentMap.get(context.getString(BasicConfigurationConstants.CONFIG_CHANNEL));                if (channelComponent == null) {                    String msg = String.format("Sink %s is not connected to a " + "channel", sinkName);                    throw new IllegalStateException(msg);                }                checkSinkChannelCompatibility(sink, channelComponent.channel);                sink.setChannel(channelComponent.channel);                sinks.put(sinkName, sink);                channelComponent.components.add(sinkName);            } catch (Exception e) {                String msg = String.format("Sink %s has been removed due to an " + "error during configuration", sinkName);                            }        }    }    loadSinkGroups(agentConf, sinks, sinkRunnerMap);}
1
private void loadSinkGroups(AgentConfiguration agentConf, Map<String, Sink> sinks, Map<String, SinkRunner> sinkRunnerMap) throws InstantiationException
{    Set<String> sinkGroupNames = agentConf.getSinkgroupSet();    Map<String, ComponentConfiguration> compMap = agentConf.getSinkGroupConfigMap();    Map<String, String> usedSinks = new HashMap<String, String>();    for (String groupName : sinkGroupNames) {        ComponentConfiguration comp = compMap.get(groupName);        if (comp != null) {            SinkGroupConfiguration groupConf = (SinkGroupConfiguration) comp;            List<Sink> groupSinks = new ArrayList<Sink>();            for (String sink : groupConf.getSinks()) {                Sink s = sinks.remove(sink);                if (s == null) {                    String sinkUser = usedSinks.get(sink);                    if (sinkUser != null) {                        throw new InstantiationException(String.format("Sink %s of group %s already " + "in use by group %s", sink, groupName, sinkUser));                    } else {                        throw new InstantiationException(String.format("Sink %s of group %s does " + "not exist or is not properly configured", sink, groupName));                    }                }                groupSinks.add(s);                usedSinks.put(sink, groupName);            }            try {                SinkGroup group = new SinkGroup(groupSinks);                Configurables.configure(group, groupConf);                sinkRunnerMap.put(comp.getComponentName(), new SinkRunner(group.getProcessor()));            } catch (Exception e) {                String msg = String.format("SinkGroup %s has been removed due to " + "an error during configuration", groupName);                            }        }    }        for (Entry<String, Sink> entry : sinks.entrySet()) {        if (!usedSinks.containsValue(entry.getKey())) {            try {                SinkProcessor pr = new DefaultSinkProcessor();                List<Sink> sinkMap = new ArrayList<Sink>();                sinkMap.add(entry.getValue());                pr.setSinks(sinkMap);                Configurables.configure(pr, new Context());                sinkRunnerMap.put(entry.getKey(), new SinkRunner(pr));            } catch (Exception e) {                String msg = String.format("SinkGroup %s has been removed due to " + "an error during configuration", entry.getKey());                            }        }    }}
1
protected Map<String, String> toMap(Properties properties)
{    Map<String, String> result = Maps.newHashMap();    Enumeration<?> propertyNames = properties.propertyNames();    while (propertyNames.hasMoreElements()) {        String name = (String) propertyNames.nextElement();        String value = properties.getProperty(name);        result.put(name, value);    }    return result;}
0
protected CuratorFramework createClient()
{    return CuratorFrameworkFactory.newClient(zkConnString, new ExponentialBackoffRetry(1000, 1));}
0
protected FlumeConfiguration configFromBytes(byte[] configData) throws IOException
{    Map<String, String> configMap;    if (configData == null || configData.length == 0) {        configMap = Collections.emptyMap();    } else {        String fileContent = new String(configData, Charsets.UTF_8);        Properties properties = new Properties();        properties.load(new StringReader(fileContent));        configMap = toMap(properties);    }    return new FlumeConfiguration(configMap);}
0
public void start()
{    lifecycleLock.lock();    try {        for (LifecycleAware component : components) {            supervisor.supervise(component, new SupervisorPolicy.AlwaysRestartPolicy(), LifecycleState.START);        }    } finally {        lifecycleLock.unlock();    }}
0
public void handleConfigurationEvent(MaterializedConfiguration conf)
{    try {        lifecycleLock.lockInterruptibly();        stopAllComponents();        startAllComponents(conf);    } catch (InterruptedException e) {                return;    } finally {                if (lifecycleLock.isHeldByCurrentThread()) {            lifecycleLock.unlock();        }    }}
1
public void stop()
{    lifecycleLock.lock();    stopAllComponents();    try {        supervisor.stop();        if (monitorServer != null) {            monitorServer.stop();        }    } finally {        lifecycleLock.unlock();    }}
0
private void stopAllComponents()
{    if (this.materializedConfiguration != null) {                for (Entry<String, SourceRunner> entry : this.materializedConfiguration.getSourceRunners().entrySet()) {            try {                                supervisor.unsupervise(entry.getValue());            } catch (Exception e) {                            }        }        for (Entry<String, SinkRunner> entry : this.materializedConfiguration.getSinkRunners().entrySet()) {            try {                                supervisor.unsupervise(entry.getValue());            } catch (Exception e) {                            }        }        for (Entry<String, Channel> entry : this.materializedConfiguration.getChannels().entrySet()) {            try {                                supervisor.unsupervise(entry.getValue());            } catch (Exception e) {                            }        }    }    if (monitorServer != null) {        monitorServer.stop();    }}
1
private void startAllComponents(MaterializedConfiguration materializedConfiguration)
{        this.materializedConfiguration = materializedConfiguration;    for (Entry<String, Channel> entry : materializedConfiguration.getChannels().entrySet()) {        try {                        supervisor.supervise(entry.getValue(), new SupervisorPolicy.AlwaysRestartPolicy(), LifecycleState.START);        } catch (Exception e) {                    }    }    /*     * Wait for all channels to start.     */    for (Channel ch : materializedConfiguration.getChannels().values()) {        while (ch.getLifecycleState() != LifecycleState.START && !supervisor.isComponentInErrorState(ch)) {            try {                                Thread.sleep(500);            } catch (InterruptedException e) {                                Throwables.propagate(e);            }        }    }    for (Entry<String, SinkRunner> entry : materializedConfiguration.getSinkRunners().entrySet()) {        try {                        supervisor.supervise(entry.getValue(), new SupervisorPolicy.AlwaysRestartPolicy(), LifecycleState.START);        } catch (Exception e) {                    }    }    for (Entry<String, SourceRunner> entry : materializedConfiguration.getSourceRunners().entrySet()) {        try {                        supervisor.supervise(entry.getValue(), new SupervisorPolicy.AlwaysRestartPolicy(), LifecycleState.START);        } catch (Exception e) {                    }    }    this.loadMonitoring();}
1
private void loadMonitoring()
{    Properties systemProps = System.getProperties();    Set<String> keys = systemProps.stringPropertyNames();    try {        if (keys.contains(CONF_MONITOR_CLASS)) {            String monitorType = systemProps.getProperty(CONF_MONITOR_CLASS);            Class<? extends MonitorService> klass;            try {                                klass = MonitoringType.valueOf(monitorType.toUpperCase(Locale.ENGLISH)).getMonitorClass();            } catch (Exception e) {                                klass = (Class<? extends MonitorService>) Class.forName(monitorType);            }            this.monitorServer = klass.newInstance();            Context context = new Context();            for (String key : keys) {                if (key.startsWith(CONF_MONITOR_PREFIX)) {                    context.put(key.substring(CONF_MONITOR_PREFIX.length()), systemProps.getProperty(key));                }            }            monitorServer.configure(context);            monitorServer.start();        }    } catch (Exception e) {            }}
1
public static void main(String[] args)
{    try {        SSLUtil.initGlobalSSLParameters();        Options options = new Options();        Option option = new Option("n", "name", true, "the name of this agent");        option.setRequired(true);        options.addOption(option);        option = new Option("f", "conf-file", true, "specify a config file (required if -z missing)");        option.setRequired(false);        options.addOption(option);        option = new Option(null, "no-reload-conf", false, "do not reload config file if changed");        options.addOption(option);                option = new Option("z", "zkConnString", true, "specify the ZooKeeper connection to use (required if -f missing)");        option.setRequired(false);        options.addOption(option);        option = new Option("p", "zkBasePath", true, "specify the base path in ZooKeeper for agent configs");        option.setRequired(false);        options.addOption(option);        option = new Option("h", "help", false, "display help text");        options.addOption(option);        CommandLineParser parser = new GnuParser();        CommandLine commandLine = parser.parse(options, args);        if (commandLine.hasOption('h')) {            new HelpFormatter().printHelp("flume-ng agent", options, true);            return;        }        String agentName = commandLine.getOptionValue('n');        boolean reload = !commandLine.hasOption("no-reload-conf");        boolean isZkConfigured = false;        if (commandLine.hasOption('z') || commandLine.hasOption("zkConnString")) {            isZkConfigured = true;        }        Application application;        if (isZkConfigured) {                        String zkConnectionStr = commandLine.getOptionValue('z');            String baseZkPath = commandLine.getOptionValue('p');            if (reload) {                EventBus eventBus = new EventBus(agentName + "-event-bus");                List<LifecycleAware> components = Lists.newArrayList();                PollingZooKeeperConfigurationProvider zookeeperConfigurationProvider = new PollingZooKeeperConfigurationProvider(agentName, zkConnectionStr, baseZkPath, eventBus);                components.add(zookeeperConfigurationProvider);                application = new Application(components);                eventBus.register(application);            } else {                StaticZooKeeperConfigurationProvider zookeeperConfigurationProvider = new StaticZooKeeperConfigurationProvider(agentName, zkConnectionStr, baseZkPath);                application = new Application();                application.handleConfigurationEvent(zookeeperConfigurationProvider.getConfiguration());            }        } else {            File configurationFile = new File(commandLine.getOptionValue('f'));            /*         * The following is to ensure that by default the agent will fail on         * startup if the file does not exist.         */            if (!configurationFile.exists()) {                                if (System.getProperty(Constants.SYSPROP_CALLED_FROM_SERVICE) == null) {                    String path = configurationFile.getPath();                    try {                        path = configurationFile.getCanonicalPath();                    } catch (IOException ex) {                                            }                    throw new ParseException("The specified configuration file does not exist: " + path);                }            }            List<LifecycleAware> components = Lists.newArrayList();            if (reload) {                EventBus eventBus = new EventBus(agentName + "-event-bus");                PollingPropertiesFileConfigurationProvider configurationProvider = new PollingPropertiesFileConfigurationProvider(agentName, configurationFile, eventBus, 30);                components.add(configurationProvider);                application = new Application(components);                eventBus.register(application);            } else {                PropertiesFileConfigurationProvider configurationProvider = new PropertiesFileConfigurationProvider(agentName, configurationFile);                application = new Application();                application.handleConfigurationEvent(configurationProvider.getConfiguration());            }        }        application.start();        final Application appReference = application;        Runtime.getRuntime().addShutdownHook(new Thread("agent-shutdown-hook") {            @Override            public void run() {                appReference.stop();            }        });    } catch (Exception e) {            }}
1
public void run()
{    appReference.stop();}
0
protected static String resolveEnvVars(String input)
{    Preconditions.checkNotNull(input);        Pattern p = Pattern.compile("\\$\\{(\\w+)\\}");    Matcher m = p.matcher(input);    StringBuffer sb = new StringBuffer();    while (m.find()) {        String envVarName = m.group(1);        String envVarValue = System.getenv(envVarName);        m.appendReplacement(sb, null == envVarValue ? "" : envVarValue);    }    m.appendTail(sb);    return sb.toString();}
0
public String getProperty(String key)
{    return resolveEnvVars(super.getProperty(key));}
0
public void start()
{        Preconditions.checkState(file != null, "The parameter file must not be null");    executorService = Executors.newSingleThreadScheduledExecutor(new ThreadFactoryBuilder().setNameFormat("conf-file-poller-%d").build());    FileWatcherRunnable fileWatcherRunnable = new FileWatcherRunnable(file, counterGroup);    executorService.scheduleWithFixedDelay(fileWatcherRunnable, 0, interval, TimeUnit.SECONDS);    lifecycleState = LifecycleState.START;    }
1
public void stop()
{        executorService.shutdown();    try {        if (!executorService.awaitTermination(500, TimeUnit.MILLISECONDS)) {                        executorService.shutdownNow();            while (!executorService.awaitTermination(500, TimeUnit.MILLISECONDS)) {                            }        }    } catch (InterruptedException e) {                Thread.currentThread().interrupt();    }    lifecycleState = LifecycleState.STOP;    }
1
public synchronized LifecycleState getLifecycleState()
{    return lifecycleState;}
0
public String toString()
{    return "{ file:" + file + " counterGroup:" + counterGroup + "  provider:" + getClass().getCanonicalName() + " agentName:" + getAgentName() + " }";}
0
public void run()
{        counterGroup.incrementAndGet("file.checks");    long lastModified = file.lastModified();    if (lastModified > lastChange) {                counterGroup.incrementAndGet("file.loads");        lastChange = lastModified;        try {            eventBus.post(getConfiguration());        } catch (Exception e) {                    } catch (NoClassDefFoundError e) {                    } catch (Throwable t) {                                }    }}
1
protected FlumeConfiguration getFlumeConfiguration()
{    return flumeConfiguration;}
0
public void start()
{        try {        client.start();        try {            agentNodeCache = new NodeCache(client, basePath + "/" + getAgentName());            agentNodeCache.start();            agentNodeCache.getListenable().addListener(new NodeCacheListener() {                @Override                public void nodeChanged() throws Exception {                    refreshConfiguration();                }            });        } catch (Exception e) {            client.close();            throw e;        }    } catch (Exception e) {        lifecycleState = LifecycleState.ERROR;        if (e instanceof RuntimeException) {            throw (RuntimeException) e;        } else {            throw new FlumeException(e);        }    }    lifecycleState = LifecycleState.START;}
1
public void nodeChanged() throws Exception
{    refreshConfiguration();}
0
private void refreshConfiguration() throws IOException
{        byte[] data = null;    ChildData childData = agentNodeCache.getCurrentData();    if (childData != null) {        data = childData.getData();    }    flumeConfiguration = configFromBytes(data);    eventBus.post(getConfiguration());}
1
public void stop()
{        if (agentNodeCache != null) {        try {            agentNodeCache.close();        } catch (IOException e) {                        lifecycleState = LifecycleState.ERROR;        }    }    try {        client.close();    } catch (Exception e) {                lifecycleState = LifecycleState.ERROR;    }    if (lifecycleState != LifecycleState.ERROR) {        lifecycleState = LifecycleState.STOP;    }}
1
public LifecycleState getLifecycleState()
{    return lifecycleState;}
0
public FlumeConfiguration getFlumeConfiguration()
{    BufferedReader reader = null;    try {        reader = new BufferedReader(new FileReader(file));        String resolverClassName = System.getProperty("propertiesImplementation", DEFAULT_PROPERTIES_IMPLEMENTATION);        Class<? extends Properties> propsclass = Class.forName(resolverClassName).asSubclass(Properties.class);        Properties properties = propsclass.newInstance();        properties.load(reader);        return new FlumeConfiguration(toMap(properties));    } catch (IOException ex) {            } catch (ClassNotFoundException e) {            } catch (InstantiationException e) {            } catch (IllegalAccessException e) {            } finally {        if (reader != null) {            try {                reader.close();            } catch (IOException ex) {                            }        }    }    return new FlumeConfiguration(new HashMap<String, String>());}
1
public String toString()
{    return "{ sourceRunners:" + sourceRunners + " sinkRunners:" + sinkRunners + " channels:" + channels + " }";}
0
public void addSourceRunner(String name, SourceRunner sourceRunner)
{    sourceRunners.put(name, sourceRunner);}
0
public void addSinkRunner(String name, SinkRunner sinkRunner)
{    sinkRunners.put(name, sinkRunner);}
0
public void addChannel(String name, Channel channel)
{    channels.put(name, channel);}
0
public Map<String, Channel> getChannels()
{    return ImmutableMap.copyOf(channels);}
0
public Map<String, SourceRunner> getSourceRunners()
{    return ImmutableMap.copyOf(sourceRunners);}
0
public Map<String, SinkRunner> getSinkRunners()
{    return ImmutableMap.copyOf(sinkRunners);}
0
protected FlumeConfiguration getFlumeConfiguration()
{    try {        CuratorFramework cf = createClient();        cf.start();        try {            byte[] data = cf.getData().forPath(basePath + "/" + getAgentName());            return configFromBytes(data);        } finally {            cf.close();        }    } catch (Exception e) {                throw new FlumeException(e);    }}
1
public void testDispoableChannel() throws Exception
{    String agentName = "agent1";    Map<String, String> properties = getPropertiesForChannel(agentName, DisposableChannel.class.getName());    MemoryConfigurationProvider provider = new MemoryConfigurationProvider(agentName, properties);    MaterializedConfiguration config1 = provider.getConfiguration();    Channel channel1 = config1.getChannels().values().iterator().next();    Assert.assertTrue(channel1 instanceof DisposableChannel);    MaterializedConfiguration config2 = provider.getConfiguration();    Channel channel2 = config2.getChannels().values().iterator().next();    Assert.assertTrue(channel2 instanceof DisposableChannel);    Assert.assertNotSame(channel1, channel2);}
0
public void testReusableChannel() throws Exception
{    String agentName = "agent1";    Map<String, String> properties = getPropertiesForChannel(agentName, RecyclableChannel.class.getName());    MemoryConfigurationProvider provider = new MemoryConfigurationProvider(agentName, properties);    MaterializedConfiguration config1 = provider.getConfiguration();    Channel channel1 = config1.getChannels().values().iterator().next();    Assert.assertTrue(channel1 instanceof RecyclableChannel);    MaterializedConfiguration config2 = provider.getConfiguration();    Channel channel2 = config2.getChannels().values().iterator().next();    Assert.assertTrue(channel2 instanceof RecyclableChannel);    Assert.assertSame(channel1, channel2);}
0
public void testUnspecifiedChannel() throws Exception
{    String agentName = "agent1";    Map<String, String> properties = getPropertiesForChannel(agentName, UnspecifiedChannel.class.getName());    MemoryConfigurationProvider provider = new MemoryConfigurationProvider(agentName, properties);    MaterializedConfiguration config1 = provider.getConfiguration();    Channel channel1 = config1.getChannels().values().iterator().next();    Assert.assertTrue(channel1 instanceof UnspecifiedChannel);    MaterializedConfiguration config2 = provider.getConfiguration();    Channel channel2 = config2.getChannels().values().iterator().next();    Assert.assertTrue(channel2 instanceof UnspecifiedChannel);    Assert.assertSame(channel1, channel2);}
0
public void testReusableChannelNotReusedLater() throws Exception
{    String agentName = "agent1";    Map<String, String> propertiesReusable = getPropertiesForChannel(agentName, RecyclableChannel.class.getName());    Map<String, String> propertiesDispoable = getPropertiesForChannel(agentName, DisposableChannel.class.getName());    MemoryConfigurationProvider provider = new MemoryConfigurationProvider(agentName, propertiesReusable);    MaterializedConfiguration config1 = provider.getConfiguration();    Channel channel1 = config1.getChannels().values().iterator().next();    Assert.assertTrue(channel1 instanceof RecyclableChannel);    provider.setProperties(propertiesDispoable);    MaterializedConfiguration config2 = provider.getConfiguration();    Channel channel2 = config2.getChannels().values().iterator().next();    Assert.assertTrue(channel2 instanceof DisposableChannel);    provider.setProperties(propertiesReusable);    MaterializedConfiguration config3 = provider.getConfiguration();    Channel channel3 = config3.getChannels().values().iterator().next();    Assert.assertTrue(channel3 instanceof RecyclableChannel);    Assert.assertNotSame(channel1, channel3);}
0
public void testSourceThrowsExceptionDuringConfiguration() throws Exception
{    String agentName = "agent1";    String sourceType = UnconfigurableSource.class.getName();    String channelType = "memory";    String sinkType = "null";    Map<String, String> properties = getProperties(agentName, sourceType, channelType, sinkType);    MemoryConfigurationProvider provider = new MemoryConfigurationProvider(agentName, properties);    MaterializedConfiguration config = provider.getConfiguration();    Assert.assertTrue(config.getSourceRunners().size() == 0);    Assert.assertTrue(config.getChannels().size() == 1);    Assert.assertTrue(config.getSinkRunners().size() == 1);}
0
public void testChannelThrowsExceptionDuringConfiguration() throws Exception
{    String agentName = "agent1";    String sourceType = "seq";    String channelType = UnconfigurableChannel.class.getName();    String sinkType = "null";    Map<String, String> properties = getProperties(agentName, sourceType, channelType, sinkType);    MemoryConfigurationProvider provider = new MemoryConfigurationProvider(agentName, properties);    MaterializedConfiguration config = provider.getConfiguration();    Assert.assertTrue(config.getSourceRunners().size() == 0);    Assert.assertTrue(config.getChannels().size() == 0);    Assert.assertTrue(config.getSinkRunners().size() == 0);}
0
public void testSinkThrowsExceptionDuringConfiguration() throws Exception
{    String agentName = "agent1";    String sourceType = "seq";    String channelType = "memory";    String sinkType = UnconfigurableSink.class.getName();    Map<String, String> properties = getProperties(agentName, sourceType, channelType, sinkType);    MemoryConfigurationProvider provider = new MemoryConfigurationProvider(agentName, properties);    MaterializedConfiguration config = provider.getConfiguration();    Assert.assertTrue(config.getSourceRunners().size() == 1);    Assert.assertTrue(config.getChannels().size() == 1);    Assert.assertTrue(config.getSinkRunners().size() == 0);}
0
public void testSourceAndSinkThrowExceptionDuringConfiguration() throws Exception
{    String agentName = "agent1";    String sourceType = UnconfigurableSource.class.getName();    String channelType = "memory";    String sinkType = UnconfigurableSink.class.getName();    Map<String, String> properties = getProperties(agentName, sourceType, channelType, sinkType);    MemoryConfigurationProvider provider = new MemoryConfigurationProvider(agentName, properties);    MaterializedConfiguration config = provider.getConfiguration();    Assert.assertTrue(config.getSourceRunners().size() == 0);    Assert.assertTrue(config.getChannels().size() == 0);    Assert.assertTrue(config.getSinkRunners().size() == 0);}
0
public void testSinkSourceMismatchDuringConfiguration() throws Exception
{    String agentName = "agent1";    String sourceType = "seq";    String channelType = "memory";    String sinkType = "avro";    Map<String, String> properties = getProperties(agentName, sourceType, channelType, sinkType);    properties.put(agentName + ".channels.channel1.capacity", "1000");    properties.put(agentName + ".channels.channel1.transactionCapacity", "1000");    properties.put(agentName + ".sources.source1.batchSize", "1000");    properties.put(agentName + ".sinks.sink1.batch-size", "1000");    properties.put(agentName + ".sinks.sink1.hostname", "10.10.10.10");    properties.put(agentName + ".sinks.sink1.port", "1010");    MemoryConfigurationProvider provider = new MemoryConfigurationProvider(agentName, properties);    MaterializedConfiguration config = provider.getConfiguration();    Assert.assertTrue(config.getSourceRunners().size() == 1);    Assert.assertTrue(config.getChannels().size() == 1);    Assert.assertTrue(config.getSinkRunners().size() == 1);    properties.put(agentName + ".sources.source1.batchSize", "1001");    properties.put(agentName + ".sinks.sink1.batch-size", "1000");    provider = new MemoryConfigurationProvider(agentName, properties);    config = provider.getConfiguration();    Assert.assertTrue(config.getSourceRunners().size() == 0);    Assert.assertTrue(config.getChannels().size() == 1);    Assert.assertTrue(config.getSinkRunners().size() == 1);    properties.put(agentName + ".sources.source1.batchSize", "1000");    properties.put(agentName + ".sinks.sink1.batch-size", "1001");    provider = new MemoryConfigurationProvider(agentName, properties);    config = provider.getConfiguration();    Assert.assertTrue(config.getSourceRunners().size() == 1);    Assert.assertTrue(config.getChannels().size() == 1);    Assert.assertTrue(config.getSinkRunners().size() == 0);    properties.put(agentName + ".sources.source1.batchSize", "1001");    properties.put(agentName + ".sinks.sink1.batch-size", "1001");    provider = new MemoryConfigurationProvider(agentName, properties);    config = provider.getConfiguration();    Assert.assertTrue(config.getSourceRunners().size() == 0);    Assert.assertTrue(config.getChannels().size() == 0);    Assert.assertTrue(config.getSinkRunners().size() == 0);}
0
private Map<String, String> getProperties(String agentName, String sourceType, String channelType, String sinkType)
{    Map<String, String> properties = Maps.newHashMap();    properties.put(agentName + ".sources", "source1");    properties.put(agentName + ".channels", "channel1");    properties.put(agentName + ".sinks", "sink1");    properties.put(agentName + ".sources.source1.type", sourceType);    properties.put(agentName + ".sources.source1.channels", "channel1");    properties.put(agentName + ".channels.channel1.type", channelType);    properties.put(agentName + ".channels.channel1.capacity", "100");    properties.put(agentName + ".sinks.sink1.type", sinkType);    properties.put(agentName + ".sinks.sink1.channel", "channel1");    return properties;}
0
private Map<String, String> getPropertiesForChannel(String agentName, String channelType)
{    return getProperties(agentName, "seq", channelType, "null");}
0
public void setProperties(Map<String, String> properties)
{    this.properties = properties;}
0
protected FlumeConfiguration getFlumeConfiguration()
{    return new FlumeConfiguration(properties);}
0
public void put(Event event) throws ChannelException
{    throw new UnsupportedOperationException();}
0
public Event take() throws ChannelException
{    throw new UnsupportedOperationException();}
0
public Transaction getTransaction()
{    throw new UnsupportedOperationException();}
0
public void put(Event event) throws ChannelException
{    throw new UnsupportedOperationException();}
0
public Event take() throws ChannelException
{    throw new UnsupportedOperationException();}
0
public Transaction getTransaction()
{    throw new UnsupportedOperationException();}
0
public void put(Event event) throws ChannelException
{    throw new UnsupportedOperationException();}
0
public Event take() throws ChannelException
{    throw new UnsupportedOperationException();}
0
public Transaction getTransaction()
{    throw new UnsupportedOperationException();}
0
public void configure(Context context)
{    throw new RuntimeException("expected");}
0
public void put(Event event) throws ChannelException
{    throw new UnsupportedOperationException();}
0
public Event take() throws ChannelException
{    throw new UnsupportedOperationException();}
0
public Transaction getTransaction()
{    throw new UnsupportedOperationException();}
0
public void configure(Context context)
{    throw new RuntimeException("expected");}
0
public void configure(Context context)
{    throw new RuntimeException("expected");}
0
public Status process() throws EventDeliveryException
{    throw new UnsupportedOperationException();}
0
public void setUp() throws Exception
{    zkServer = new TestingServer();    client = CuratorFrameworkFactory.newClient("localhost:" + zkServer.getPort(), new ExponentialBackoffRetry(1000, 3));    client.start();    EnsurePath ensurePath = new EnsurePath(AGENT_PATH);    ensurePath.ensure(client.getZookeeperClient());    doSetUp();}
0
public void tearDown() throws Exception
{    doTearDown();    zkServer.close();    client.close();}
0
protected void addData() throws Exception
{    Reader in = new InputStreamReader(getClass().getClassLoader().getResourceAsStream(FLUME_CONF_FILE), Charsets.UTF_8);    try {        String config = IOUtils.toString(in);        client.setData().forPath(AGENT_PATH, config.getBytes());    } finally {        in.close();    }}
0
protected void verifyProperties(AbstractConfigurationProvider cp)
{    FlumeConfiguration configuration = cp.getFlumeConfiguration();    Assert.assertNotNull(configuration);    /*     * Test the known errors in the file     */    List<String> expected = Lists.newArrayList();    expected.add("host5 CONFIG_ERROR");    expected.add("host5 INVALID_PROPERTY");    expected.add("host4 CONFIG_ERROR");    expected.add("host4 CONFIG_ERROR");    expected.add("host4 PROPERTY_VALUE_NULL");    expected.add("host4 PROPERTY_VALUE_NULL");    expected.add("host4 PROPERTY_VALUE_NULL");    expected.add("host4 AGENT_CONFIGURATION_INVALID");    expected.add("ch2 ATTRS_MISSING");    expected.add("host3 CONFIG_ERROR");    expected.add("host3 PROPERTY_VALUE_NULL");    expected.add("host3 AGENT_CONFIGURATION_INVALID");    expected.add("host2 PROPERTY_VALUE_NULL");    expected.add("host2 AGENT_CONFIGURATION_INVALID");    List<String> actual = Lists.newArrayList();    for (FlumeConfigurationError error : configuration.getConfigurationErrors()) {        actual.add(error.getComponentName() + " " + error.getErrorType().toString());    }    Collections.sort(expected);    Collections.sort(actual);    Assert.assertEquals(expected, actual);    FlumeConfiguration.AgentConfiguration agentConfiguration = configuration.getConfigurationFor("host1");    Assert.assertNotNull(agentConfiguration);    Set<String> sources = Sets.newHashSet("source1");    Set<String> sinks = Sets.newHashSet("sink1");    Set<String> channels = Sets.newHashSet("channel1");    Assert.assertEquals(sources, agentConfiguration.getSourceSet());    Assert.assertEquals(sinks, agentConfiguration.getSinkSet());    Assert.assertEquals(channels, agentConfiguration.getChannelSet());}
0
public void setup() throws Exception
{    baseDir = Files.createTempDir();}
0
public void tearDown() throws Exception
{    FileUtils.deleteDirectory(baseDir);}
0
private T mockLifeCycle(Class<T> klass)
{    T lifeCycleAware = mock(klass);    final AtomicReference<LifecycleState> state = new AtomicReference<LifecycleState>();    state.set(LifecycleState.IDLE);    when(lifeCycleAware.getLifecycleState()).then(new Answer<LifecycleState>() {        @Override        public LifecycleState answer(InvocationOnMock invocation) throws Throwable {            return state.get();        }    });    doAnswer(new Answer<Void>() {        @Override        public Void answer(InvocationOnMock invocation) throws Throwable {            state.set(LifecycleState.START);            return null;        }    }).when(lifeCycleAware).start();    doAnswer(new Answer<Void>() {        @Override        public Void answer(InvocationOnMock invocation) throws Throwable {            state.set(LifecycleState.STOP);            return null;        }    }).when(lifeCycleAware).stop();    return lifeCycleAware;}
0
public LifecycleState answer(InvocationOnMock invocation) throws Throwable
{    return state.get();}
0
public Void answer(InvocationOnMock invocation) throws Throwable
{    state.set(LifecycleState.START);    return null;}
0
public Void answer(InvocationOnMock invocation) throws Throwable
{    state.set(LifecycleState.STOP);    return null;}
0
public void testBasicConfiguration() throws Exception
{    EventBus eventBus = new EventBus("test-event-bus");    MaterializedConfiguration materializedConfiguration = new SimpleMaterializedConfiguration();    SourceRunner sourceRunner = mockLifeCycle(SourceRunner.class);    materializedConfiguration.addSourceRunner("test", sourceRunner);    SinkRunner sinkRunner = mockLifeCycle(SinkRunner.class);    materializedConfiguration.addSinkRunner("test", sinkRunner);    Channel channel = mockLifeCycle(Channel.class);    materializedConfiguration.addChannel("test", channel);    ConfigurationProvider configurationProvider = mock(ConfigurationProvider.class);    when(configurationProvider.getConfiguration()).thenReturn(materializedConfiguration);    Application application = new Application();    eventBus.register(application);    eventBus.post(materializedConfiguration);    application.start();    Thread.sleep(1000L);    verify(sourceRunner).start();    verify(sinkRunner).start();    verify(channel).start();    application.stop();    Thread.sleep(1000L);    verify(sourceRunner).stop();    verify(sinkRunner).stop();    verify(channel).stop();}
0
public void testFLUME1854() throws Exception
{    File configFile = new File(baseDir, "flume-conf.properties");    Files.copy(new File(getClass().getClassLoader().getResource("flume-conf.properties").getFile()), configFile);    Random random = new Random();    for (int i = 0; i < 3; i++) {        EventBus eventBus = new EventBus("test-event-bus");        PollingPropertiesFileConfigurationProvider configurationProvider = new PollingPropertiesFileConfigurationProvider("host1", configFile, eventBus, 1);        List<LifecycleAware> components = Lists.newArrayList();        components.add(configurationProvider);        Application application = new Application(components);        eventBus.register(application);        application.start();        Thread.sleep(random.nextInt(10000));        application.stop();    }}
0
public void testFLUME2786() throws Exception
{    final String agentName = "test";    final int interval = 1;    final long intervalMs = 1000L;    File configFile = new File(baseDir, "flume-conf.properties");    Files.copy(new File(getClass().getClassLoader().getResource("flume-conf.properties.2786").getFile()), configFile);    File mockConfigFile = spy(configFile);    when(mockConfigFile.lastModified()).then(new Answer<Long>() {        @Override        public Long answer(InvocationOnMock invocation) throws Throwable {            Thread.sleep(intervalMs);            return System.currentTimeMillis();        }    });    EventBus eventBus = new EventBus(agentName + "-event-bus");    PollingPropertiesFileConfigurationProvider configurationProvider = new PollingPropertiesFileConfigurationProvider(agentName, mockConfigFile, eventBus, interval);    PollingPropertiesFileConfigurationProvider mockConfigurationProvider = spy(configurationProvider);    doAnswer(new Answer<Void>() {        @Override        public Void answer(InvocationOnMock invocation) throws Throwable {            Thread.sleep(intervalMs);            invocation.callRealMethod();            return null;        }    }).when(mockConfigurationProvider).stop();    List<LifecycleAware> components = Lists.newArrayList();    components.add(mockConfigurationProvider);    Application application = new Application(components);    eventBus.register(application);    application.start();    Thread.sleep(1500L);    application.stop();}
0
public Long answer(InvocationOnMock invocation) throws Throwable
{    Thread.sleep(intervalMs);    return System.currentTimeMillis();}
0
public Void answer(InvocationOnMock invocation) throws Throwable
{    Thread.sleep(intervalMs);    invocation.callRealMethod();    return null;}
0
public void setUp() throws Exception
{    provider = new PropertiesFileConfigurationProvider("a1", TESTFILE);}
0
public void resolveEnvVar() throws Exception
{    environmentVariables.set("VARNAME", "varvalue");    String resolved = EnvVarResolverProperties.resolveEnvVars("padding ${VARNAME} padding");    Assert.assertEquals("padding varvalue padding", resolved);}
0
public void resolveEnvVars() throws Exception
{    environmentVariables.set("VARNAME1", "varvalue1");    environmentVariables.set("VARNAME2", "varvalue2");    String resolved = EnvVarResolverProperties.resolveEnvVars("padding ${VARNAME1} ${VARNAME2} padding");    Assert.assertEquals("padding varvalue1 varvalue2 padding", resolved);}
0
public void getProperty() throws Exception
{    String NC_PORT = "6667";    environmentVariables.set("NC_PORT", NC_PORT);    System.setProperty("propertiesImplementation", "org.apache.flume.node.EnvVarResolverProperties");    Assert.assertEquals(NC_PORT, provider.getFlumeConfiguration().getConfigurationFor("a1").getSourceContext().get("r1").getParameters().get("port"));}
0
public void setUp() throws Exception
{    baseDir = Files.createTempDir();    configFile = new File(baseDir, TESTFILE.getName());    Files.copy(TESTFILE, configFile);    eventBus = new EventBus("test");    provider = new PollingPropertiesFileConfigurationProvider("host1", configFile, eventBus, 1);    provider.start();    LifecycleController.waitForOneOf(provider, LifecycleState.START_OR_ERROR);}
0
public void tearDown() throws Exception
{    FileUtils.deleteDirectory(baseDir);    provider.stop();}
0
public void testPolling() throws Exception
{        Thread.sleep(2000L);    final List<MaterializedConfiguration> events = Lists.newArrayList();    Object eventHandler = new Object() {        @Subscribe        public synchronized void handleConfigurationEvent(MaterializedConfiguration event) {            events.add(event);        }    };    eventBus.register(eventHandler);    configFile.setLastModified(System.currentTimeMillis());        Thread.sleep(2000L);    Assert.assertEquals(String.valueOf(events), 1, events.size());    MaterializedConfiguration materializedConfiguration = events.remove(0);    Assert.assertEquals(1, materializedConfiguration.getSourceRunners().size());    Assert.assertEquals(1, materializedConfiguration.getSinkRunners().size());    Assert.assertEquals(1, materializedConfiguration.getChannels().size());}
0
public synchronized void handleConfigurationEvent(MaterializedConfiguration event)
{    events.add(event);}
0
public synchronized void notifyEvent(MaterializedConfiguration mConfig)
{    notified = true;    notifyAll();}
0
public synchronized void awaitEvent() throws InterruptedException
{    while (!notified) {        wait();    }}
0
public synchronized void reset()
{    notified = false;}
0
protected void doSetUp() throws Exception
{    eb = new EventBus("test");    es = new EventSync();    es.reset();    eb.register(es);    cp = new PollingZooKeeperConfigurationProvider(AGENT_NAME, "localhost:" + zkServer.getPort(), null, eb);    cp.start();    LifecycleController.waitForOneOf(cp, LifecycleState.START_OR_ERROR);}
0
protected void doTearDown() throws Exception
{}
0
public void testPolling() throws Exception
{    es.awaitEvent();    es.reset();    FlumeConfiguration fc = cp.getFlumeConfiguration();    Assert.assertTrue(fc.getConfigurationErrors().isEmpty());    AgentConfiguration ac = fc.getConfigurationFor(AGENT_NAME);    Assert.assertNull(ac);    addData();    es.awaitEvent();    es.reset();    verifyProperties(cp);}
0
public void setUp() throws Exception
{    provider = new PropertiesFileConfigurationProvider("test", TESTFILE);}
0
public void tearDown() throws Exception
{}
0
public void testPropertyRead() throws Exception
{    FlumeConfiguration configuration = provider.getFlumeConfiguration();    Assert.assertNotNull(configuration);    /*     * Test the known errors in the file     */    List<String> expected = Lists.newArrayList();    expected.add("host5 CONFIG_ERROR");    expected.add("host5 INVALID_PROPERTY");    expected.add("host4 CONFIG_ERROR");    expected.add("host4 CONFIG_ERROR");    expected.add("host4 PROPERTY_VALUE_NULL");    expected.add("host4 PROPERTY_VALUE_NULL");    expected.add("host4 PROPERTY_VALUE_NULL");    expected.add("host4 AGENT_CONFIGURATION_INVALID");    expected.add("ch2 ATTRS_MISSING");    expected.add("host3 CONFIG_ERROR");    expected.add("host3 PROPERTY_VALUE_NULL");    expected.add("host3 AGENT_CONFIGURATION_INVALID");    expected.add("host2 PROPERTY_VALUE_NULL");    expected.add("host2 AGENT_CONFIGURATION_INVALID");    List<String> actual = Lists.newArrayList();    for (FlumeConfigurationError error : configuration.getConfigurationErrors()) {        actual.add(error.getComponentName() + " " + error.getErrorType().toString());    }    Collections.sort(expected);    Collections.sort(actual);    Assert.assertEquals(expected, actual);    AgentConfiguration agentConfiguration = configuration.getConfigurationFor("host1");    Assert.assertNotNull(agentConfiguration);            Set<String> sources = Sets.newHashSet("source1");    Set<String> sinks = Sets.newHashSet("sink1");    Set<String> channels = Sets.newHashSet("channel1");    Assert.assertEquals(sources, agentConfiguration.getSourceSet());    Assert.assertEquals(sinks, agentConfiguration.getSinkSet());    Assert.assertEquals(channels, agentConfiguration.getChannelSet());}
1
protected void doSetUp() throws Exception
{    addData();    configurationProvider = new StaticZooKeeperConfigurationProvider(AGENT_NAME, "localhost:" + zkServer.getPort(), null);}
0
protected void doTearDown() throws Exception
{}
0
public void testPropertyRead() throws Exception
{    verifyProperties(configurationProvider);}
0
public static Collection<?> data()
{    Object[][] data = new Object[][] { { true }, { false } };    return Arrays.asList(data);}
0
private static int getFreePort()
{    try (ServerSocket socket = new ServerSocket(0)) {        return socket.getLocalPort();    } catch (IOException e) {        throw new AssertionError("Can not open socket", e);    }}
0
public void setUp()
{        channel = new MemoryChannel();    source = new NetcatSource();    Context context = new Context();    Configurables.configure(channel, context);    List<Channel> channels = Lists.newArrayList(channel);    ChannelSelector rcs = new ReplicatingChannelSelector();    rcs.setChannels(channels);    source.setChannelProcessor(new ChannelProcessor(rcs));}
1
public void testLifecycle() throws InterruptedException, LifecycleException, EventDeliveryException
{    final int port = getFreePort();    ExecutorService executor = Executors.newFixedThreadPool(3);    Context context = new Context();    context.put("bind", "0.0.0.0");    context.put("port", String.valueOf(port));    context.put("ack-every-event", String.valueOf(ackEveryEvent));    Configurables.configure(source, context);    source.start();    Runnable clientRequestRunnable = new Runnable() {        @Override        public void run() {            try {                SocketChannel clientChannel = SocketChannel.open(new InetSocketAddress(port));                Writer writer = Channels.newWriter(clientChannel, "utf-8");                BufferedReader reader = new BufferedReader(Channels.newReader(clientChannel, "utf-8"));                writer.write("Test message\n");                writer.flush();                if (ackEveryEvent) {                    String response = reader.readLine();                    Assert.assertEquals("Server should return OK", "OK", response);                } else {                    Assert.assertFalse("Server should not return anything", reader.ready());                }                clientChannel.close();            } catch (IOException e) {                            }        }    };    ChannelSelector selector = source.getChannelProcessor().getSelector();    Transaction tx = selector.getAllChannels().get(0).getTransaction();    tx.begin();    for (int i = 0; i < 100; i++) {                executor.submit(clientRequestRunnable);        Event event = channel.take();        Assert.assertNotNull(event);        Assert.assertArrayEquals("Test message".getBytes(), event.getBody());    }    tx.commit();    tx.close();    executor.shutdown();    while (!executor.isTerminated()) {        executor.awaitTermination(500, TimeUnit.MILLISECONDS);    }    source.stop();}
1
public void run()
{    try {        SocketChannel clientChannel = SocketChannel.open(new InetSocketAddress(port));        Writer writer = Channels.newWriter(clientChannel, "utf-8");        BufferedReader reader = new BufferedReader(Channels.newReader(clientChannel, "utf-8"));        writer.write("Test message\n");        writer.flush();        if (ackEveryEvent) {            String response = reader.readLine();            Assert.assertEquals("Server should return OK", "OK", response);        } else {            Assert.assertFalse("Server should not return anything", reader.ready());        }        clientChannel.close();    } catch (IOException e) {            }}
1
public int getBatchSize()
{    return batchSize;}
0
private synchronized void configureHosts(Properties properties) throws FlumeException
{    if (isActive) {                throw new FlumeException("This client was already configured, " + "cannot reconfigure.");    }    hosts = HostInfo.getHostInfoList(properties);    String tries = properties.getProperty(RpcClientConfigurationConstants.CONFIG_MAX_ATTEMPTS);    if (tries == null || tries.isEmpty()) {        maxTries = hosts.size();    } else {        try {            maxTries = Integer.parseInt(tries);        } catch (NumberFormatException e) {            maxTries = hosts.size();        }    }    batchSize = parseBatchSize(properties);    isActive = true;}
1
protected Integer getMaxTries()
{    return maxTries;}
0
private synchronized RpcClient getClient()
{    if (client == null || !this.client.isActive()) {        client = getNextClient();        return client;    } else {        return client;    }}
0
public void append(Event event) throws EventDeliveryException
{                    RpcClient localClient = null;    synchronized (this) {        if (!isActive) {                        throw new EventDeliveryException("Attempting to append to an already closed client.");        }    }        int tries = 0;    while (tries < maxTries) {        try {            tries++;            localClient = getClient();            localClient.append(event);            return;        } catch (EventDeliveryException e) {                                    localClient.close();            localClient = null;        } catch (Exception e2) {                        throw new EventDeliveryException("Failed to send event. Exception follows: ", e2);        }    }        throw new EventDeliveryException("Failed to send the event!");}
1
public void appendBatch(List<Event> events) throws EventDeliveryException
{    RpcClient localClient = null;    synchronized (this) {        if (!isActive) {                        throw new EventDeliveryException("Attempting to append to an already closed client!");        }    }    int tries = 0;    while (tries < maxTries) {        try {            tries++;            localClient = getClient();            localClient.appendBatch(events);            return;        } catch (EventDeliveryException e) {                                    localClient.close();            localClient = null;        } catch (Exception e1) {                        throw new EventDeliveryException("No clients currently active. " + "Exception follows: ", e1);        }    }        throw new EventDeliveryException("Failed to send the event!");}
1
public synchronized boolean isActive()
{    return isActive;}
0
public synchronized void close() throws FlumeException
{    if (client != null) {        client.close();        isActive = false;    }}
0
protected InetSocketAddress getLastConnectedServerAddress()
{    HostInfo hostInfo = hosts.get(lastCheckedhost);    return new InetSocketAddress(hostInfo.getHostName(), hostInfo.getPortNumber());}
0
private RpcClient getNextClient() throws FlumeException
{    lastCheckedhost = (lastCheckedhost == (hosts.size() - 1)) ? -1 : lastCheckedhost;    RpcClient localClient = null;    int limit = hosts.size();    Properties props = new Properties();    props.putAll(configurationProperties);    props.put(RpcClientConfigurationConstants.CONFIG_CLIENT_TYPE, RpcClientConfigurationConstants.DEFAULT_CLIENT_TYPE);        for (int count = lastCheckedhost + 1; count < limit; count++) {        HostInfo hostInfo = hosts.get(count);        try {            setDefaultProperties(hostInfo, props);            localClient = RpcClientFactory.getInstance(props);            lastCheckedhost = count;            return localClient;        } catch (FlumeException e) {                        continue;        }    }    for (int count = 0; count <= lastCheckedhost; count++) {        HostInfo hostInfo = hosts.get(count);        try {            setDefaultProperties(hostInfo, props);            localClient = RpcClientFactory.getInstance(props);            lastCheckedhost = count;            return localClient;        } catch (FlumeException e) {                        continue;        }    }    if (localClient == null) {        lastCheckedhost = -1;                throw new FlumeException("No active client.");    }        return localClient;}
1
private void setDefaultProperties(HostInfo hostInfo, Properties props)
{    props.put(RpcClientConfigurationConstants.CONFIG_CLIENT_TYPE, RpcClientFactory.ClientType.DEFAULT.name());    props.put(RpcClientConfigurationConstants.CONFIG_HOSTS, hostInfo.getReferenceName());}
0
public void configure(Properties properties) throws FlumeException
{    configurationProperties = new Properties();    configurationProperties.putAll(properties);    configureHosts(configurationProperties);}
0
public String getReferenceName()
{    return referenceName;}
0
public String getHostName()
{    return hostName;}
0
public int getPortNumber()
{    return portNumber;}
0
public String toString()
{    return referenceName + "{" + hostName + ":" + portNumber + "}";}
0
public static List<HostInfo> getHostInfoList(Properties properties)
{    List<HostInfo> hosts = new ArrayList<HostInfo>();    String hostNames = properties.getProperty(RpcClientConfigurationConstants.CONFIG_HOSTS);    String[] hostList;    if (hostNames != null && !hostNames.isEmpty()) {        hostList = hostNames.split("\\s+");        for (int i = 0; i < hostList.length; i++) {            String hostAndPortStr = properties.getProperty(RpcClientConfigurationConstants.CONFIG_HOSTS_PREFIX + hostList[i]);                        if (hostAndPortStr != null) {                String[] hostAndPort = hostAndPortStr.split(":");                if (hostAndPort.length != 2) {                                        throw new FlumeException("Invalid host address" + hostAndPortStr);                }                Integer port = null;                try {                    port = Integer.parseInt(hostAndPort[1]);                } catch (NumberFormatException e) {                                        throw new FlumeException("Invalid port number" + hostAndPortStr);                }                HostInfo info = new HostInfo(hostList[i], hostAndPort[0].trim(), port);                hosts.add(info);            }        }    }    return hosts;}
1
public void append(Event event) throws EventDeliveryException
{    throwIfClosed();    boolean eventSent = false;    Iterator<HostInfo> it = selector.createHostIterator();    while (it.hasNext()) {        HostInfo host = it.next();        try {            RpcClient client = getClient(host);            client.append(event);            eventSent = true;            break;        } catch (Exception ex) {            selector.informFailure(host);                    }    }    if (!eventSent) {        throw new EventDeliveryException("Unable to send event to any host");    }}
1
public void appendBatch(List<Event> events) throws EventDeliveryException
{    throwIfClosed();    boolean batchSent = false;    Iterator<HostInfo> it = selector.createHostIterator();    while (it.hasNext()) {        HostInfo host = it.next();        try {            RpcClient client = getClient(host);            client.appendBatch(events);            batchSent = true;            break;        } catch (Exception ex) {            selector.informFailure(host);                    }    }    if (!batchSent) {        throw new EventDeliveryException("Unable to send batch to any host");    }}
1
public boolean isActive()
{    return isOpen;}
0
private void throwIfClosed() throws EventDeliveryException
{    if (!isOpen) {        throw new EventDeliveryException("Rpc Client is closed");    }}
0
public void close() throws FlumeException
{    isOpen = false;    synchronized (this) {        Iterator<String> it = clientMap.keySet().iterator();        while (it.hasNext()) {            String name = it.next();            RpcClient client = clientMap.get(name);            if (client != null) {                try {                    client.close();                } catch (Exception ex) {                                    }            }            it.remove();        }    }}
1
protected void configure(Properties properties) throws FlumeException
{    clientMap = new HashMap<String, RpcClient>();    configurationProperties = new Properties();    configurationProperties.putAll(properties);    hosts = HostInfo.getHostInfoList(properties);    if (hosts.size() < 2) {        throw new FlumeException("At least two hosts are required to use the " + "load balancing RPC client.");    }    String lbTypeName = properties.getProperty(RpcClientConfigurationConstants.CONFIG_HOST_SELECTOR, RpcClientConfigurationConstants.HOST_SELECTOR_ROUND_ROBIN);    boolean backoff = Boolean.valueOf(properties.getProperty(RpcClientConfigurationConstants.CONFIG_BACKOFF, String.valueOf(false)));    String maxBackoffStr = properties.getProperty(RpcClientConfigurationConstants.CONFIG_MAX_BACKOFF);    long maxBackoff = 0;    if (maxBackoffStr != null) {        maxBackoff = Long.parseLong(maxBackoffStr);    }    if (lbTypeName.equalsIgnoreCase(RpcClientConfigurationConstants.HOST_SELECTOR_ROUND_ROBIN)) {        selector = new RoundRobinHostSelector(backoff, maxBackoff);    } else if (lbTypeName.equalsIgnoreCase(RpcClientConfigurationConstants.HOST_SELECTOR_RANDOM)) {        selector = new RandomOrderHostSelector(backoff, maxBackoff);    } else {        try {            @SuppressWarnings("unchecked")            Class<? extends HostSelector> klass = (Class<? extends HostSelector>) Class.forName(lbTypeName);            selector = klass.newInstance();        } catch (Exception ex) {            throw new FlumeException("Unable to instantiate host selector: " + lbTypeName, ex);        }    }    selector.setHosts(hosts);    batchSize = parseBatchSize(properties);    isOpen = true;}
0
private synchronized RpcClient getClient(HostInfo info) throws FlumeException, EventDeliveryException
{    throwIfClosed();    String name = info.getReferenceName();    RpcClient client = clientMap.get(name);    if (client == null) {        client = createClient(name);        clientMap.put(name, client);    } else if (!client.isActive()) {        try {            client.close();        } catch (Exception ex) {                    }        client = createClient(name);        clientMap.put(name, client);    }    return client;}
1
private RpcClient createClient(String referenceName) throws FlumeException
{    Properties props = getClientConfigurationProperties(referenceName);    return RpcClientFactory.getInstance(props);}
0
private Properties getClientConfigurationProperties(String referenceName)
{    Properties props = new Properties();    props.putAll(configurationProperties);    props.put(RpcClientConfigurationConstants.CONFIG_CLIENT_TYPE, RpcClientFactory.ClientType.DEFAULT);    props.put(RpcClientConfigurationConstants.CONFIG_HOSTS, referenceName);    return props;}
0
public synchronized Iterator<HostInfo> createHostIterator()
{    return selector.createIterator();}
0
public synchronized void setHosts(List<HostInfo> hosts)
{    selector.setObjects(hosts);}
0
public synchronized void informFailure(HostInfo failedHost)
{    selector.informFailure(failedHost);}
0
public synchronized Iterator<HostInfo> createHostIterator()
{    return selector.createIterator();}
0
public synchronized void setHosts(List<HostInfo> hosts)
{    selector.setObjects(hosts);}
0
public void informFailure(HostInfo failedHost)
{    selector.informFailure(failedHost);}
0
private void connect() throws FlumeException
{    connect(connectTimeout, TimeUnit.MILLISECONDS);}
0
private void connect(long timeout, TimeUnit tu) throws FlumeException
{    callTimeoutPool = Executors.newCachedThreadPool(new TransceiverThreadFactory("Flume Avro RPC Client Call Invoker"));    NioClientSocketChannelFactory socketChannelFactory = null;    try {        ExecutorService bossExecutor = Executors.newCachedThreadPool(new TransceiverThreadFactory("Avro " + NettyTransceiver.class.getSimpleName() + " Boss"));        ExecutorService workerExecutor = Executors.newCachedThreadPool(new TransceiverThreadFactory("Avro " + NettyTransceiver.class.getSimpleName() + " I/O Worker"));        if (enableDeflateCompression || enableSsl) {            if (maxIoWorkers >= 1) {                socketChannelFactory = new SSLCompressionChannelFactory(bossExecutor, workerExecutor, enableDeflateCompression, enableSsl, trustAllCerts, compressionLevel, truststore, truststorePassword, truststoreType, excludeProtocols, includeProtocols, excludeCipherSuites, includeCipherSuites, maxIoWorkers);            } else {                socketChannelFactory = new SSLCompressionChannelFactory(bossExecutor, workerExecutor, enableDeflateCompression, enableSsl, trustAllCerts, compressionLevel, truststore, truststorePassword, truststoreType, excludeProtocols, includeProtocols, excludeCipherSuites, includeCipherSuites);            }        } else {            if (maxIoWorkers >= 1) {                socketChannelFactory = new NioClientSocketChannelFactory(bossExecutor, workerExecutor, maxIoWorkers);            } else {                socketChannelFactory = new NioClientSocketChannelFactory(bossExecutor, workerExecutor);            }        }        transceiver = new NettyTransceiver(this.address, socketChannelFactory, tu.toMillis(timeout));        avroClient = SpecificRequestor.getClient(AvroSourceProtocol.Callback.class, transceiver);    } catch (Throwable t) {        if (callTimeoutPool != null) {            callTimeoutPool.shutdownNow();        }        if (socketChannelFactory != null) {            socketChannelFactory.releaseExternalResources();        }        if (t instanceof IOException) {            throw new FlumeException(this + ": RPC connection error", t);        } else if (t instanceof FlumeException) {            throw (FlumeException) t;        } else if (t instanceof Error) {            throw (Error) t;        } else {            throw new FlumeException(this + ": Unexpected exception", t);        }    }    setState(ConnState.READY);}
0
public void close() throws FlumeException
{    if (callTimeoutPool != null) {        callTimeoutPool.shutdown();        try {            if (!callTimeoutPool.awaitTermination(requestTimeout, TimeUnit.MILLISECONDS)) {                callTimeoutPool.shutdownNow();                if (!callTimeoutPool.awaitTermination(requestTimeout, TimeUnit.MILLISECONDS)) {                                    }            }        } catch (InterruptedException ex) {                                    callTimeoutPool.shutdownNow();                        Thread.currentThread().interrupt();        }        callTimeoutPool = null;    }    try {        transceiver.close();    } catch (IOException ex) {        throw new FlumeException(this + ": Error closing transceiver.", ex);    } finally {        setState(ConnState.DEAD);    }}
1
public String toString()
{    return "NettyAvroRpcClient { host: " + address.getHostName() + ", port: " + address.getPort() + " }";}
0
public void append(Event event) throws EventDeliveryException
{    try {        append(event, requestTimeout, TimeUnit.MILLISECONDS);    } catch (Throwable t) {                        setState(ConnState.DEAD);        if (t instanceof Error) {            throw (Error) t;        }        if (t instanceof TimeoutException) {            throw new EventDeliveryException(this + ": Failed to send event. " + "RPC request timed out after " + requestTimeout + "ms", t);        }        throw new EventDeliveryException(this + ": Failed to send event", t);    }}
0
private void append(Event event, long timeout, TimeUnit tu) throws EventDeliveryException
{    assertReady();    final CallFuture<Status> callFuture = new CallFuture<Status>();    final AvroFlumeEvent avroEvent = new AvroFlumeEvent();    avroEvent.setBody(ByteBuffer.wrap(event.getBody()));    avroEvent.setHeaders(toCharSeqMap(event.getHeaders()));    Future<Void> handshake;    try {                handshake = callTimeoutPool.submit(new Callable<Void>() {            @Override            public Void call() throws Exception {                avroClient.append(avroEvent, callFuture);                return null;            }        });    } catch (RejectedExecutionException ex) {        throw new EventDeliveryException(this + ": Executor error", ex);    }    try {        handshake.get(connectTimeout, TimeUnit.MILLISECONDS);    } catch (TimeoutException ex) {        throw new EventDeliveryException(this + ": Handshake timed out after " + connectTimeout + " ms", ex);    } catch (InterruptedException ex) {        throw new EventDeliveryException(this + ": Interrupted in handshake", ex);    } catch (ExecutionException ex) {        throw new EventDeliveryException(this + ": RPC request exception", ex);    } catch (CancellationException ex) {        throw new EventDeliveryException(this + ": RPC request cancelled", ex);    } finally {        if (!handshake.isDone()) {            handshake.cancel(true);        }    }    waitForStatusOK(callFuture, timeout, tu);}
0
public Void call() throws Exception
{    avroClient.append(avroEvent, callFuture);    return null;}
0
public void appendBatch(List<Event> events) throws EventDeliveryException
{    try {        appendBatch(events, requestTimeout, TimeUnit.MILLISECONDS);    } catch (Throwable t) {                        setState(ConnState.DEAD);        if (t instanceof Error) {            throw (Error) t;        }        if (t instanceof TimeoutException) {            throw new EventDeliveryException(this + ": Failed to send event. " + "RPC request timed out after " + requestTimeout + " ms", t);        }        throw new EventDeliveryException(this + ": Failed to send batch", t);    }}
0
private void appendBatch(List<Event> events, long timeout, TimeUnit tu) throws EventDeliveryException
{    assertReady();    Iterator<Event> iter = events.iterator();    final List<AvroFlumeEvent> avroEvents = new LinkedList<AvroFlumeEvent>();        while (iter.hasNext()) {        avroEvents.clear();        for (int i = 0; i < batchSize && iter.hasNext(); i++) {            Event event = iter.next();            AvroFlumeEvent avroEvent = new AvroFlumeEvent();            avroEvent.setBody(ByteBuffer.wrap(event.getBody()));            avroEvent.setHeaders(toCharSeqMap(event.getHeaders()));            avroEvents.add(avroEvent);        }        final CallFuture<Status> callFuture = new CallFuture<Status>();        Future<Void> handshake;        try {                        handshake = callTimeoutPool.submit(new Callable<Void>() {                @Override                public Void call() throws Exception {                    avroClient.appendBatch(avroEvents, callFuture);                    return null;                }            });        } catch (RejectedExecutionException ex) {            throw new EventDeliveryException(this + ": Executor error", ex);        }        try {            handshake.get(connectTimeout, TimeUnit.MILLISECONDS);        } catch (TimeoutException ex) {            throw new EventDeliveryException(this + ": Handshake timed out after " + connectTimeout + "ms", ex);        } catch (InterruptedException ex) {            throw new EventDeliveryException(this + ": Interrupted in handshake", ex);        } catch (ExecutionException ex) {            throw new EventDeliveryException(this + ": RPC request exception", ex);        } catch (CancellationException ex) {            throw new EventDeliveryException(this + ": RPC request cancelled", ex);        } finally {            if (!handshake.isDone()) {                handshake.cancel(true);            }        }        waitForStatusOK(callFuture, timeout, tu);    }}
0
public Void call() throws Exception
{    avroClient.appendBatch(avroEvents, callFuture);    return null;}
0
private void waitForStatusOK(CallFuture<Status> callFuture, long timeout, TimeUnit tu) throws EventDeliveryException
{    try {        Status status = callFuture.get(timeout, tu);        if (status != Status.OK) {            throw new EventDeliveryException(this + ": Avro RPC call returned " + "Status: " + status);        }    } catch (CancellationException ex) {        throw new EventDeliveryException(this + ": RPC future was cancelled", ex);    } catch (ExecutionException ex) {        throw new EventDeliveryException(this + ": Exception thrown from " + "remote handler", ex);    } catch (TimeoutException ex) {        throw new EventDeliveryException(this + ": RPC request timed out", ex);    } catch (InterruptedException ex) {        Thread.currentThread().interrupt();        throw new EventDeliveryException(this + ": RPC request interrupted", ex);    }}
0
private void setState(ConnState newState)
{    stateLock.lock();    try {        if (connState == ConnState.DEAD && connState != newState) {            throw new IllegalStateException("Cannot transition from CLOSED state.");        }        connState = newState;    } finally {        stateLock.unlock();    }}
0
private void assertReady() throws EventDeliveryException
{    stateLock.lock();    try {        ConnState curState = connState;        if (curState != ConnState.READY) {            throw new EventDeliveryException("RPC failed, client in an invalid " + "state: " + curState);        }    } finally {        stateLock.unlock();    }}
0
private static Map<CharSequence, CharSequence> toCharSeqMap(Map<String, String> stringMap)
{    Map<CharSequence, CharSequence> charSeqMap = new HashMap<CharSequence, CharSequence>();    for (Map.Entry<String, String> entry : stringMap.entrySet()) {        charSeqMap.put(entry.getKey(), entry.getValue());    }    return charSeqMap;}
0
public boolean isActive()
{    stateLock.lock();    try {        return (connState == ConnState.READY);    } finally {        stateLock.unlock();    }}
0
public Thread newThread(Runnable r)
{    Thread thread = new Thread(r);    thread.setDaemon(true);    thread.setName(prefix + " " + threadId.incrementAndGet());    return thread;}
0
public SocketChannel newChannel(ChannelPipeline pipeline)
{    TrustManager[] managers;    try {        if (enableCompression) {            ZlibEncoder encoder = new ZlibEncoder(compressionLevel);            pipeline.addFirst("deflater", encoder);            pipeline.addFirst("inflater", new ZlibDecoder());        }        if (enableSsl) {            if (trustAllCerts) {                                managers = new TrustManager[] { new PermissiveTrustManager() };            } else {                KeyStore keystore = null;                if (truststore != null) {                    InputStream truststoreStream = new FileInputStream(truststore);                    keystore = KeyStore.getInstance(truststoreType);                    keystore.load(truststoreStream, truststorePassword != null ? truststorePassword.toCharArray() : null);                }                TrustManagerFactory tmf = TrustManagerFactory.getInstance("SunX509");                                                tmf.init(keystore);                managers = tmf.getTrustManagers();            }            SSLContext sslContext = SSLContext.getInstance("TLS");            sslContext.init(null, managers, null);            SSLEngine sslEngine = sslContext.createSSLEngine();            sslEngine.setUseClientMode(true);            List<String> enabledProtocols = new ArrayList<String>();            for (String protocol : sslEngine.getEnabledProtocols()) {                if ((includeProtocols.isEmpty() || includeProtocols.contains(protocol)) && !excludeProtocols.contains(protocol)) {                    enabledProtocols.add(protocol);                }            }            sslEngine.setEnabledProtocols(enabledProtocols.toArray(new String[0]));            List<String> enabledCipherSuites = new ArrayList<String>();            for (String suite : sslEngine.getEnabledCipherSuites()) {                if ((includeCipherSuites.isEmpty() || includeCipherSuites.contains(suite)) && !excludeCipherSuites.contains(suite)) {                    enabledCipherSuites.add(suite);                }            }            sslEngine.setEnabledCipherSuites(enabledCipherSuites.toArray(new String[0]));                                                                        pipeline.addFirst("ssl", new SslHandler(sslEngine));        }        return super.newChannel(pipeline);    } catch (Exception ex) {                throw new RuntimeException("Cannot create SSL channel", ex);    }}
1
public void checkClientTrusted(X509Certificate[] certs, String s)
{}
0
public void checkServerTrusted(X509Certificate[] certs, String s)
{}
0
public X509Certificate[] getAcceptedIssuers()
{    return new X509Certificate[0];}
0
public static RpcClient getInstance(Properties properties) throws FlumeException
{    String type = null;    type = properties.getProperty(RpcClientConfigurationConstants.CONFIG_CLIENT_TYPE);    if (type == null || type.isEmpty()) {        type = ClientType.DEFAULT.getClientClassName();    }    Class<? extends AbstractRpcClient> clazz;    AbstractRpcClient client;    try {        String clientClassType = type;        ClientType clientType = null;        try {            clientType = ClientType.valueOf(type.toUpperCase(Locale.ENGLISH));        } catch (IllegalArgumentException e) {            clientType = ClientType.OTHER;        }        if (!clientType.equals(ClientType.OTHER)) {            clientClassType = clientType.getClientClassName();        }        clazz = (Class<? extends AbstractRpcClient>) Class.forName(clientClassType);    } catch (ClassNotFoundException e) {        throw new FlumeException("No such client!", e);    }    try {        client = clazz.newInstance();    } catch (InstantiationException e) {        throw new FlumeException("Cannot instantiate client. " + "Exception follows:", e);    } catch (IllegalAccessException e) {        throw new FlumeException("Cannot instantiate client. " + "Exception follows:", e);    }    client.configure(properties);    return client;}
0
public static RpcClient getInstance(File propertiesFile) throws FileNotFoundException, IOException
{    Reader reader = new FileReader(propertiesFile);    Properties props = new Properties();    props.load(reader);    return getInstance(props);}
0
public static RpcClient getInstance(String hostname, Integer port) throws FlumeException
{    return getDefaultInstance(hostname, port);}
0
public static RpcClient getDefaultInstance(String hostname, Integer port) throws FlumeException
{    return getDefaultInstance(hostname, port, 0);}
0
public static RpcClient getInstance(String hostname, Integer port, Integer batchSize) throws FlumeException
{    return getDefaultInstance(hostname, port, batchSize);}
0
public static RpcClient getDefaultInstance(String hostname, Integer port, Integer batchSize) throws FlumeException
{    if (hostname == null) {        throw new NullPointerException("hostname must not be null");    }    if (port == null) {        throw new NullPointerException("port must not be null");    }    if (batchSize == null) {        throw new NullPointerException("batchSize must not be null");    }    Properties props = new Properties();    props.setProperty(RpcClientConfigurationConstants.CONFIG_HOSTS, "h1");    props.setProperty(RpcClientConfigurationConstants.CONFIG_HOSTS_PREFIX + "h1", hostname + ":" + port.intValue());    props.setProperty(RpcClientConfigurationConstants.CONFIG_BATCH_SIZE, batchSize.toString());    NettyAvroRpcClient client = new NettyAvroRpcClient();    client.configure(props);    return client;}
0
public static RpcClient getThriftInstance(String hostname, Integer port, Integer batchSize)
{    if (hostname == null) {        throw new NullPointerException("hostname must not be null");    }    if (port == null) {        throw new NullPointerException("port must not be null");    }    if (batchSize == null) {        throw new NullPointerException("batchSize must not be null");    }    Properties props = new Properties();    props.setProperty(RpcClientConfigurationConstants.CONFIG_HOSTS, "h1");    props.setProperty(RpcClientConfigurationConstants.CONFIG_HOSTS_PREFIX + "h1", hostname + ":" + port.intValue());    props.setProperty(RpcClientConfigurationConstants.CONFIG_BATCH_SIZE, batchSize.toString());    ThriftRpcClient client = new ThriftRpcClient();    client.configure(props);    return client;}
0
public static RpcClient getThriftInstance(String hostname, Integer port)
{    return getThriftInstance(hostname, port, RpcClientConfigurationConstants.DEFAULT_BATCH_SIZE);}
0
public static RpcClient getThriftInstance(Properties props)
{    props.setProperty(RpcClientConfigurationConstants.CONFIG_CLIENT_TYPE, ClientType.THRIFT.clientClassName);    return getInstance(props);}
0
protected String getClientClassName()
{    return this.clientClassName;}
0
protected void configureSSL(Properties properties) throws FlumeException
{    enableSsl = Boolean.parseBoolean(properties.getProperty(RpcClientConfigurationConstants.CONFIG_SSL));    trustAllCerts = Boolean.parseBoolean(properties.getProperty(RpcClientConfigurationConstants.CONFIG_TRUST_ALL_CERTS));    truststore = properties.getProperty(RpcClientConfigurationConstants.CONFIG_TRUSTSTORE, SSLUtil.getGlobalTruststorePath());    truststorePassword = properties.getProperty(RpcClientConfigurationConstants.CONFIG_TRUSTSTORE_PASSWORD, SSLUtil.getGlobalTruststorePassword());    truststoreType = properties.getProperty(RpcClientConfigurationConstants.CONFIG_TRUSTSTORE_TYPE, SSLUtil.getGlobalTruststoreType("JKS"));    parseList(properties.getProperty(RpcClientConfigurationConstants.CONFIG_EXCLUDE_PROTOCOLS, SSLUtil.getGlobalExcludeProtocols()), excludeProtocols);    parseList(properties.getProperty(RpcClientConfigurationConstants.CONFIG_INCLUDE_PROTOCOLS, SSLUtil.getGlobalIncludeProtocols()), includeProtocols);    parseList(properties.getProperty(RpcClientConfigurationConstants.CONFIG_EXCLUDE_CIPHER_SUITES, SSLUtil.getGlobalExcludeCipherSuites()), excludeCipherSuites);    parseList(properties.getProperty(RpcClientConfigurationConstants.CONFIG_INCLUDE_CIPHER_SUITES, SSLUtil.getGlobalIncludeCipherSuites()), includeCipherSuites);}
0
private void parseList(String value, Set<String> set)
{    if (Objects.nonNull(value)) {        set.addAll(Arrays.asList(value.split(" ")));    }}
0
public Thread newThread(Runnable r)
{    Thread t = new Thread(r);    t.setName("Flume Thrift RPC thread - " + String.valueOf(threadCounter.incrementAndGet()));    return t;}
0
public void append(Event event) throws EventDeliveryException
{            ClientWrapper client = null;    boolean destroyedClient = false;    try {        if (!isActive()) {            throw new EventDeliveryException("Client was closed due to error. " + "Please create a new client");        }        client = connectionManager.checkout();        final ThriftFlumeEvent thriftEvent = new ThriftFlumeEvent(event.getHeaders(), ByteBuffer.wrap(event.getBody()));        doAppend(client, thriftEvent).get(requestTimeout, TimeUnit.MILLISECONDS);    } catch (Throwable e) {        if (e instanceof ExecutionException) {            Throwable cause = e.getCause();            if (cause instanceof EventDeliveryException) {                throw (EventDeliveryException) cause;            } else if (cause instanceof TimeoutException) {                throw new EventDeliveryException("Append call timeout", cause);            }        }        destroyedClient = true;                if (client != null) {            connectionManager.destroy(client);        }        if (e instanceof Error) {            throw (Error) e;        } else if (e instanceof RuntimeException) {            throw (RuntimeException) e;        }        throw new EventDeliveryException("Failed to send event. ", e);    } finally {        if (client != null && !destroyedClient) {            connectionManager.checkIn(client);        }    }}
0
public void appendBatch(List<Event> events) throws EventDeliveryException
{            ClientWrapper client = null;    boolean destroyedClient = false;    try {        if (!isActive()) {            throw new EventDeliveryException("Client was closed " + "due to error or is not yet configured.");        }        client = connectionManager.checkout();        final List<ThriftFlumeEvent> thriftFlumeEvents = new ArrayList<ThriftFlumeEvent>();        Iterator<Event> eventsIter = events.iterator();        while (eventsIter.hasNext()) {            thriftFlumeEvents.clear();            for (int i = 0; i < batchSize && eventsIter.hasNext(); i++) {                Event event = eventsIter.next();                thriftFlumeEvents.add(new ThriftFlumeEvent(event.getHeaders(), ByteBuffer.wrap(event.getBody())));            }            if (!thriftFlumeEvents.isEmpty()) {                doAppendBatch(client, thriftFlumeEvents).get(requestTimeout, TimeUnit.MILLISECONDS);            }        }    } catch (Throwable e) {        if (e instanceof ExecutionException) {            Throwable cause = e.getCause();            if (cause instanceof EventDeliveryException) {                throw (EventDeliveryException) cause;            } else if (cause instanceof TimeoutException) {                throw new EventDeliveryException("Append call timeout", cause);            }        }        destroyedClient = true;                if (client != null) {            connectionManager.destroy(client);        }        if (e instanceof Error) {            throw (Error) e;        } else if (e instanceof RuntimeException) {            throw (RuntimeException) e;        }        throw new EventDeliveryException("Failed to send event. ", e);    } finally {        if (client != null && !destroyedClient) {            connectionManager.checkIn(client);        }    }}
0
private Future<Void> doAppend(final ClientWrapper client, final ThriftFlumeEvent e) throws Exception
{    return callTimeoutPool.submit(new Callable<Void>() {        @Override        public Void call() throws Exception {            Status status = client.client.append(e);            if (status != Status.OK) {                throw new EventDeliveryException("Failed to deliver events. Server " + "returned status : " + status.name());            }            return null;        }    });}
0
public Void call() throws Exception
{    Status status = client.client.append(e);    if (status != Status.OK) {        throw new EventDeliveryException("Failed to deliver events. Server " + "returned status : " + status.name());    }    return null;}
0
private Future<Void> doAppendBatch(final ClientWrapper client, final List<ThriftFlumeEvent> e) throws Exception
{    return callTimeoutPool.submit(new Callable<Void>() {        @Override        public Void call() throws Exception {            Status status = client.client.appendBatch(e);            if (status != Status.OK) {                throw new EventDeliveryException("Failed to deliver events. Server " + "returned status : " + status.name());            }            return null;        }    });}
0
public Void call() throws Exception
{    Status status = client.client.appendBatch(e);    if (status != Status.OK) {        throw new EventDeliveryException("Failed to deliver events. Server " + "returned status : " + status.name());    }    return null;}
0
public boolean isActive()
{    stateLock.lock();    try {        return (connState == State.READY);    } finally {        stateLock.unlock();    }}
0
public void close() throws FlumeException
{    try {                stateLock.lock();        connState = State.DEAD;        connectionManager.closeAll();        callTimeoutPool.shutdown();        if (!callTimeoutPool.awaitTermination(5, TimeUnit.SECONDS)) {            callTimeoutPool.shutdownNow();        }    } catch (Throwable ex) {        if (ex instanceof Error) {            throw (Error) ex;        } else if (ex instanceof RuntimeException) {            throw (RuntimeException) ex;        }        throw new FlumeException("Failed to close RPC client. ", ex);    } finally {        stateLock.unlock();    }}
0
protected void configure(Properties properties) throws FlumeException
{    if (isActive()) {        throw new FlumeException("Attempting to re-configured an already " + "configured client!");    }    stateLock.lock();    try {        HostInfo host = HostInfo.getHostInfoList(properties).get(0);        hostname = host.getHostName();        port = host.getPortNumber();        protocol = properties.getProperty(CONFIG_PROTOCOL);        if (protocol == null) {                        protocol = COMPACT_PROTOCOL;        }                if (!(protocol.equalsIgnoreCase(BINARY_PROTOCOL) || protocol.equalsIgnoreCase(COMPACT_PROTOCOL))) {                        protocol = COMPACT_PROTOCOL;        }        batchSize = parseBatchSize(properties);        requestTimeout = Long.parseLong(properties.getProperty(RpcClientConfigurationConstants.CONFIG_REQUEST_TIMEOUT, String.valueOf(RpcClientConfigurationConstants.DEFAULT_REQUEST_TIMEOUT_MILLIS)));        if (requestTimeout < 1000) {                        requestTimeout = RpcClientConfigurationConstants.DEFAULT_REQUEST_TIMEOUT_MILLIS;        }        int connectionPoolSize = Integer.parseInt(properties.getProperty(RpcClientConfigurationConstants.CONFIG_CONNECTION_POOL_SIZE, String.valueOf(RpcClientConfigurationConstants.DEFAULT_CONNECTION_POOL_SIZE)));        if (connectionPoolSize < 1) {                        connectionPoolSize = RpcClientConfigurationConstants.DEFAULT_CONNECTION_POOL_SIZE;        }        configureSSL(properties);        connectionManager = new ConnectionPoolManager(connectionPoolSize);        connState = State.READY;    } catch (Throwable ex) {                connState = State.DEAD;        if (ex instanceof Error) {            throw (Error) ex;        } else if (ex instanceof RuntimeException) {            throw (RuntimeException) ex;        }        throw new FlumeException("Error while configuring RpcClient. ", ex);    } finally {        stateLock.unlock();    }}
1
protected TTransport getTransport(TSocket tsocket) throws Exception
{    return new TFastFramedTransport(tsocket);}
0
public boolean equals(Object o)
{    if (o == null) {        return false;    }        if (this == o) {        return true;    }    return false;}
0
public int hashCode()
{    return hashCode;}
0
public ClientWrapper checkout() throws Exception
{    ClientWrapper ret = null;    poolLock.lock();    try {        if (availableClients.isEmpty() && currentPoolSize < maxPoolSize) {            ret = new ClientWrapper();            currentPoolSize++;            checkedOutClients.add(ret);            return ret;        }        while (availableClients.isEmpty()) {            availableClientsCondition.await();        }        ret = availableClients.poll();        checkedOutClients.add(ret);    } finally {        poolLock.unlock();    }    return ret;}
0
public void checkIn(ClientWrapper client)
{    poolLock.lock();    try {        availableClients.add(client);        checkedOutClients.remove(client);        availableClientsCondition.signal();    } finally {        poolLock.unlock();    }}
0
public void destroy(ClientWrapper client)
{    poolLock.lock();    try {        checkedOutClients.remove(client);        currentPoolSize--;    } finally {        poolLock.unlock();    }    client.transport.close();}
0
public void closeAll()
{    poolLock.lock();    try {        for (ClientWrapper c : availableClients) {            c.transport.close();            currentPoolSize--;        }                for (ClientWrapper c : checkedOutClients) {            c.transport.close();            currentPoolSize--;        }    } finally {        poolLock.unlock();    }}
0
private static SSLContext createSSLContext(String truststore, String truststorePassword, String truststoreType) throws FlumeException
{    SSLContext ctx;    try {        ctx = SSLContext.getInstance("TLS");        TrustManagerFactory tmf;        tmf = TrustManagerFactory.getInstance(TrustManagerFactory.getDefaultAlgorithm());        KeyStore ts = null;        if (truststore != null && truststoreType != null) {            ts = KeyStore.getInstance(truststoreType);            ts.load(new FileInputStream(truststore), truststorePassword != null ? truststorePassword.toCharArray() : null);            tmf.init(ts);        }        tmf.init(ts);        ctx.init(null, tmf.getTrustManagers(), null);    } catch (Exception e) {        throw new FlumeException("Error creating the transport", e);    }    return ctx;}
0
private static TSocket createSSLSocket(SSLSocketFactory factory, String host, int port, int timeout, Set<String> excludeProtocols, Set<String> includeProtocols, Set<String> excludeCipherSuites, Set<String> includeCipherSuites) throws FlumeException
{    try {        SSLSocket socket = (SSLSocket) factory.createSocket(host, port);        socket.setSoTimeout(timeout);        List<String> enabledProtocols = new ArrayList<String>();        for (String protocol : socket.getEnabledProtocols()) {            if ((includeProtocols.isEmpty() || includeProtocols.contains(protocol)) && !excludeProtocols.contains(protocol)) {                enabledProtocols.add(protocol);            }        }        socket.setEnabledProtocols(enabledProtocols.toArray(new String[0]));        List<String> enabledCipherSuites = new ArrayList<String>();        for (String suite : socket.getEnabledCipherSuites()) {            if ((includeCipherSuites.isEmpty() || includeCipherSuites.contains(suite)) && !excludeCipherSuites.contains(suite)) {                enabledCipherSuites.add(suite);            }        }        socket.setEnabledCipherSuites(enabledCipherSuites.toArray(new String[0]));        return new TSocket(socket);    } catch (Exception e) {        throw new FlumeException("Could not connect to " + host + " on port " + port, e);    }}
0
public static Event withBody(byte[] body, Map<String, String> headers)
{    Event event = new SimpleEvent();    if (body == null) {        body = new byte[0];    }    event.setBody(body);    if (headers != null) {        event.setHeaders(new HashMap<String, String>(headers));    }    return event;}
0
public static Event withBody(byte[] body)
{    return withBody(body, null);}
0
public static Event withBody(String body, Charset charset, Map<String, String> headers)
{    return withBody(body.getBytes(charset), headers);}
0
public static Event withBody(String body, Charset charset)
{    return withBody(body, charset, null);}
0
public Map<String, String> getHeaders()
{    return headers;}
0
public void setHeaders(Map<String, String> headers)
{    this.headers = headers;}
0
public byte[] getBody()
{    if (body != null) {        try {            return body.getBytes(charset);        } catch (UnsupportedEncodingException ex) {            throw new FlumeException(String.format("%s encoding not supported", charset), ex);        }    } else {        return new byte[0];    }}
0
public void setBody(byte[] body)
{    if (body != null) {        this.body = new String(body);    } else {        this.body = "";    }}
0
public void setCharset(String charset)
{    this.charset = charset;}
0
public Map<String, String> getHeaders()
{    return headers;}
0
public void setHeaders(Map<String, String> headers)
{    this.headers = headers;}
0
public byte[] getBody()
{    return body;}
0
public void setBody(byte[] body)
{    if (body == null) {        body = new byte[0];    }    this.body = body;}
0
public String toString()
{    Integer bodyLen = null;    if (body != null)        bodyLen = body.length;    return "[Event headers = " + headers + ", body.length = " + bodyLen + " ]";}
0
public int getValue()
{    return value;}
0
public static Status findByValue(int value)
{    switch(value) {        case 0:            return OK;        case 1:            return FAILED;        case 2:            return ERROR;        case 3:            return UNKNOWN;        default:            return null;    }}
0
public static _Fields findByThriftId(int fieldId)
{    switch(fieldId) {        case         1:            return HEADERS;        case         2:            return BODY;        default:            return null;    }}
0
public static _Fields findByThriftIdOrThrow(int fieldId)
{    _Fields fields = findByThriftId(fieldId);    if (fields == null)        throw new IllegalArgumentException("Field " + fieldId + " doesn't exist!");    return fields;}
0
public static _Fields findByName(String name)
{    return byName.get(name);}
0
public short getThriftFieldId()
{    return _thriftId;}
0
public String getFieldName()
{    return _fieldName;}
0
public ThriftFlumeEvent deepCopy()
{    return new ThriftFlumeEvent(this);}
0
public void clear()
{    this.headers = null;    this.body = null;}
0
public int getHeadersSize()
{    return (this.headers == null) ? 0 : this.headers.size();}
0
public void putToHeaders(String key, String val)
{    if (this.headers == null) {        this.headers = new HashMap<String, String>();    }    this.headers.put(key, val);}
0
public Map<String, String> getHeaders()
{    return this.headers;}
0
public ThriftFlumeEvent setHeaders(Map<String, String> headers)
{    this.headers = headers;    return this;}
0
public void unsetHeaders()
{    this.headers = null;}
0
public boolean isSetHeaders()
{    return this.headers != null;}
0
public void setHeadersIsSet(boolean value)
{    if (!value) {        this.headers = null;    }}
0
public byte[] getBody()
{    setBody(org.apache.thrift.TBaseHelper.rightSize(body));    return body == null ? null : body.array();}
0
public ByteBuffer bufferForBody()
{    return org.apache.thrift.TBaseHelper.copyBinary(body);}
0
public ThriftFlumeEvent setBody(byte[] body)
{    this.body = body == null ? (ByteBuffer) null : ByteBuffer.wrap(Arrays.copyOf(body, body.length));    return this;}
0
public ThriftFlumeEvent setBody(ByteBuffer body)
{    this.body = org.apache.thrift.TBaseHelper.copyBinary(body);    return this;}
0
public void unsetBody()
{    this.body = null;}
0
public boolean isSetBody()
{    return this.body != null;}
0
public void setBodyIsSet(boolean value)
{    if (!value) {        this.body = null;    }}
0
public void setFieldValue(_Fields field, Object value)
{    switch(field) {        case HEADERS:            if (value == null) {                unsetHeaders();            } else {                setHeaders((Map<String, String>) value);            }            break;        case BODY:            if (value == null) {                unsetBody();            } else {                setBody((ByteBuffer) value);            }            break;    }}
0
public Object getFieldValue(_Fields field)
{    switch(field) {        case HEADERS:            return getHeaders();        case BODY:            return getBody();    }    throw new IllegalStateException();}
0
public boolean isSet(_Fields field)
{    if (field == null) {        throw new IllegalArgumentException();    }    switch(field) {        case HEADERS:            return isSetHeaders();        case BODY:            return isSetBody();    }    throw new IllegalStateException();}
0
public boolean equals(Object that)
{    if (that == null)        return false;    if (that instanceof ThriftFlumeEvent)        return this.equals((ThriftFlumeEvent) that);    return false;}
0
public boolean equals(ThriftFlumeEvent that)
{    if (that == null)        return false;    boolean this_present_headers = true && this.isSetHeaders();    boolean that_present_headers = true && that.isSetHeaders();    if (this_present_headers || that_present_headers) {        if (!(this_present_headers && that_present_headers))            return false;        if (!this.headers.equals(that.headers))            return false;    }    boolean this_present_body = true && this.isSetBody();    boolean that_present_body = true && that.isSetBody();    if (this_present_body || that_present_body) {        if (!(this_present_body && that_present_body))            return false;        if (!this.body.equals(that.body))            return false;    }    return true;}
0
public int hashCode()
{    List<Object> list = new ArrayList<Object>();    boolean present_headers = true && (isSetHeaders());    list.add(present_headers);    if (present_headers)        list.add(headers);    boolean present_body = true && (isSetBody());    list.add(present_body);    if (present_body)        list.add(body);    return list.hashCode();}
0
public int compareTo(ThriftFlumeEvent other)
{    if (!getClass().equals(other.getClass())) {        return getClass().getName().compareTo(other.getClass().getName());    }    int lastComparison = 0;    lastComparison = Boolean.valueOf(isSetHeaders()).compareTo(other.isSetHeaders());    if (lastComparison != 0) {        return lastComparison;    }    if (isSetHeaders()) {        lastComparison = org.apache.thrift.TBaseHelper.compareTo(this.headers, other.headers);        if (lastComparison != 0) {            return lastComparison;        }    }    lastComparison = Boolean.valueOf(isSetBody()).compareTo(other.isSetBody());    if (lastComparison != 0) {        return lastComparison;    }    if (isSetBody()) {        lastComparison = org.apache.thrift.TBaseHelper.compareTo(this.body, other.body);        if (lastComparison != 0) {            return lastComparison;        }    }    return 0;}
0
public _Fields fieldForId(int fieldId)
{    return _Fields.findByThriftId(fieldId);}
0
public void read(org.apache.thrift.protocol.TProtocol iprot) throws org.apache.thrift.TException
{    schemes.get(iprot.getScheme()).getScheme().read(iprot, this);}
0
public void write(org.apache.thrift.protocol.TProtocol oprot) throws org.apache.thrift.TException
{    schemes.get(oprot.getScheme()).getScheme().write(oprot, this);}
0
public String toString()
{    StringBuilder sb = new StringBuilder("ThriftFlumeEvent(");    boolean first = true;    sb.append("headers:");    if (this.headers == null) {        sb.append("null");    } else {        sb.append(this.headers);    }    first = false;    if (!first)        sb.append(", ");    sb.append("body:");    if (this.body == null) {        sb.append("null");    } else {        org.apache.thrift.TBaseHelper.toString(this.body, sb);    }    first = false;    sb.append(")");    return sb.toString();}
0
public void validate() throws org.apache.thrift.TException
{        if (headers == null) {        throw new org.apache.thrift.protocol.TProtocolException("Required field 'headers' was not present! Struct: " + toString());    }    if (body == null) {        throw new org.apache.thrift.protocol.TProtocolException("Required field 'body' was not present! Struct: " + toString());    }}
0
private void writeObject(java.io.ObjectOutputStream out) throws java.io.IOException
{    try {        write(new org.apache.thrift.protocol.TCompactProtocol(new org.apache.thrift.transport.TIOStreamTransport(out)));    } catch (org.apache.thrift.TException te) {        throw new java.io.IOException(te);    }}
0
private void readObject(java.io.ObjectInputStream in) throws java.io.IOException, ClassNotFoundException
{    try {        read(new org.apache.thrift.protocol.TCompactProtocol(new org.apache.thrift.transport.TIOStreamTransport(in)));    } catch (org.apache.thrift.TException te) {        throw new java.io.IOException(te);    }}
0
public ThriftFlumeEventStandardScheme getScheme()
{    return new ThriftFlumeEventStandardScheme();}
0
public void read(org.apache.thrift.protocol.TProtocol iprot, ThriftFlumeEvent struct) throws org.apache.thrift.TException
{    org.apache.thrift.protocol.TField schemeField;    iprot.readStructBegin();    while (true) {        schemeField = iprot.readFieldBegin();        if (schemeField.type == org.apache.thrift.protocol.TType.STOP) {            break;        }        switch(schemeField.id) {            case             1:                if (schemeField.type == org.apache.thrift.protocol.TType.MAP) {                    {                        org.apache.thrift.protocol.TMap _map0 = iprot.readMapBegin();                        struct.headers = new HashMap<String, String>(2 * _map0.size);                        String _key1;                        String _val2;                        for (int _i3 = 0; _i3 < _map0.size; ++_i3) {                            _key1 = iprot.readString();                            _val2 = iprot.readString();                            struct.headers.put(_key1, _val2);                        }                        iprot.readMapEnd();                    }                    struct.setHeadersIsSet(true);                } else {                    org.apache.thrift.protocol.TProtocolUtil.skip(iprot, schemeField.type);                }                break;            case             2:                if (schemeField.type == org.apache.thrift.protocol.TType.STRING) {                    struct.body = iprot.readBinary();                    struct.setBodyIsSet(true);                } else {                    org.apache.thrift.protocol.TProtocolUtil.skip(iprot, schemeField.type);                }                break;            default:                org.apache.thrift.protocol.TProtocolUtil.skip(iprot, schemeField.type);        }        iprot.readFieldEnd();    }    iprot.readStructEnd();        struct.validate();}
0
public void write(org.apache.thrift.protocol.TProtocol oprot, ThriftFlumeEvent struct) throws org.apache.thrift.TException
{    struct.validate();    oprot.writeStructBegin(STRUCT_DESC);    if (struct.headers != null) {        oprot.writeFieldBegin(HEADERS_FIELD_DESC);        {            oprot.writeMapBegin(new org.apache.thrift.protocol.TMap(org.apache.thrift.protocol.TType.STRING, org.apache.thrift.protocol.TType.STRING, struct.headers.size()));            for (Map.Entry<String, String> _iter4 : struct.headers.entrySet()) {                oprot.writeString(_iter4.getKey());                oprot.writeString(_iter4.getValue());            }            oprot.writeMapEnd();        }        oprot.writeFieldEnd();    }    if (struct.body != null) {        oprot.writeFieldBegin(BODY_FIELD_DESC);        oprot.writeBinary(struct.body);        oprot.writeFieldEnd();    }    oprot.writeFieldStop();    oprot.writeStructEnd();}
0
public ThriftFlumeEventTupleScheme getScheme()
{    return new ThriftFlumeEventTupleScheme();}
0
public void write(org.apache.thrift.protocol.TProtocol prot, ThriftFlumeEvent struct) throws org.apache.thrift.TException
{    TTupleProtocol oprot = (TTupleProtocol) prot;    {        oprot.writeI32(struct.headers.size());        for (Map.Entry<String, String> _iter5 : struct.headers.entrySet()) {            oprot.writeString(_iter5.getKey());            oprot.writeString(_iter5.getValue());        }    }    oprot.writeBinary(struct.body);}
0
public void read(org.apache.thrift.protocol.TProtocol prot, ThriftFlumeEvent struct) throws org.apache.thrift.TException
{    TTupleProtocol iprot = (TTupleProtocol) prot;    {        org.apache.thrift.protocol.TMap _map6 = new org.apache.thrift.protocol.TMap(org.apache.thrift.protocol.TType.STRING, org.apache.thrift.protocol.TType.STRING, iprot.readI32());        struct.headers = new HashMap<String, String>(2 * _map6.size);        String _key7;        String _val8;        for (int _i9 = 0; _i9 < _map6.size; ++_i9) {            _key7 = iprot.readString();            _val8 = iprot.readString();            struct.headers.put(_key7, _val8);        }    }    struct.setHeadersIsSet(true);    struct.body = iprot.readBinary();    struct.setBodyIsSet(true);}
0
public Client getClient(org.apache.thrift.protocol.TProtocol prot)
{    return new Client(prot);}
0
public Client getClient(org.apache.thrift.protocol.TProtocol iprot, org.apache.thrift.protocol.TProtocol oprot)
{    return new Client(iprot, oprot);}
0
public Status append(ThriftFlumeEvent event) throws org.apache.thrift.TException
{    send_append(event);    return recv_append();}
0
public void send_append(ThriftFlumeEvent event) throws org.apache.thrift.TException
{    append_args args = new append_args();    args.setEvent(event);    sendBase("append", args);}
0
public Status recv_append() throws org.apache.thrift.TException
{    append_result result = new append_result();    receiveBase(result, "append");    if (result.isSetSuccess()) {        return result.success;    }    throw new org.apache.thrift.TApplicationException(org.apache.thrift.TApplicationException.MISSING_RESULT, "append failed: unknown result");}
0
public Status appendBatch(List<ThriftFlumeEvent> events) throws org.apache.thrift.TException
{    send_appendBatch(events);    return recv_appendBatch();}
0
public void send_appendBatch(List<ThriftFlumeEvent> events) throws org.apache.thrift.TException
{    appendBatch_args args = new appendBatch_args();    args.setEvents(events);    sendBase("appendBatch", args);}
0
public Status recv_appendBatch() throws org.apache.thrift.TException
{    appendBatch_result result = new appendBatch_result();    receiveBase(result, "appendBatch");    if (result.isSetSuccess()) {        return result.success;    }    throw new org.apache.thrift.TApplicationException(org.apache.thrift.TApplicationException.MISSING_RESULT, "appendBatch failed: unknown result");}
0
public AsyncClient getAsyncClient(org.apache.thrift.transport.TNonblockingTransport transport)
{    return new AsyncClient(protocolFactory, clientManager, transport);}
0
public void append(ThriftFlumeEvent event, org.apache.thrift.async.AsyncMethodCallback resultHandler) throws org.apache.thrift.TException
{    checkReady();    append_call method_call = new append_call(event, resultHandler, this, ___protocolFactory, ___transport);    this.___currentMethod = method_call;    ___manager.call(method_call);}
0
public void write_args(org.apache.thrift.protocol.TProtocol prot) throws org.apache.thrift.TException
{    prot.writeMessageBegin(new org.apache.thrift.protocol.TMessage("append", org.apache.thrift.protocol.TMessageType.CALL, 0));    append_args args = new append_args();    args.setEvent(event);    args.write(prot);    prot.writeMessageEnd();}
0
public Status getResult() throws org.apache.thrift.TException
{    if (getState() != org.apache.thrift.async.TAsyncMethodCall.State.RESPONSE_READ) {        throw new IllegalStateException("Method call not finished!");    }    org.apache.thrift.transport.TMemoryInputTransport memoryTransport = new org.apache.thrift.transport.TMemoryInputTransport(getFrameBuffer().array());    org.apache.thrift.protocol.TProtocol prot = client.getProtocolFactory().getProtocol(memoryTransport);    return (new Client(prot)).recv_append();}
0
public void appendBatch(List<ThriftFlumeEvent> events, org.apache.thrift.async.AsyncMethodCallback resultHandler) throws org.apache.thrift.TException
{    checkReady();    appendBatch_call method_call = new appendBatch_call(events, resultHandler, this, ___protocolFactory, ___transport);    this.___currentMethod = method_call;    ___manager.call(method_call);}
0
public void write_args(org.apache.thrift.protocol.TProtocol prot) throws org.apache.thrift.TException
{    prot.writeMessageBegin(new org.apache.thrift.protocol.TMessage("appendBatch", org.apache.thrift.protocol.TMessageType.CALL, 0));    appendBatch_args args = new appendBatch_args();    args.setEvents(events);    args.write(prot);    prot.writeMessageEnd();}
0
public Status getResult() throws org.apache.thrift.TException
{    if (getState() != org.apache.thrift.async.TAsyncMethodCall.State.RESPONSE_READ) {        throw new IllegalStateException("Method call not finished!");    }    org.apache.thrift.transport.TMemoryInputTransport memoryTransport = new org.apache.thrift.transport.TMemoryInputTransport(getFrameBuffer().array());    org.apache.thrift.protocol.TProtocol prot = client.getProtocolFactory().getProtocol(memoryTransport);    return (new Client(prot)).recv_appendBatch();}
0
private static Map<String, org.apache.thrift.ProcessFunction<I, ? extends org.apache.thrift.TBase>> getProcessMap(Map<String, org.apache.thrift.ProcessFunction<I, ? extends org.apache.thrift.TBase>> processMap)
{    processMap.put("append", new append());    processMap.put("appendBatch", new appendBatch());    return processMap;}
0
public append_args getEmptyArgsInstance()
{    return new append_args();}
0
protected boolean isOneway()
{    return false;}
0
public append_result getResult(I iface, append_args args) throws org.apache.thrift.TException
{    append_result result = new append_result();    result.success = iface.append(args.event);    return result;}
0
public appendBatch_args getEmptyArgsInstance()
{    return new appendBatch_args();}
0
protected boolean isOneway()
{    return false;}
0
public appendBatch_result getResult(I iface, appendBatch_args args) throws org.apache.thrift.TException
{    appendBatch_result result = new appendBatch_result();    result.success = iface.appendBatch(args.events);    return result;}
0
private static Map<String, org.apache.thrift.AsyncProcessFunction<I, ? extends org.apache.thrift.TBase, ?>> getProcessMap(Map<String, org.apache.thrift.AsyncProcessFunction<I, ? extends org.apache.thrift.TBase, ?>> processMap)
{    processMap.put("append", new append());    processMap.put("appendBatch", new appendBatch());    return processMap;}
0
public append_args getEmptyArgsInstance()
{    return new append_args();}
0
public AsyncMethodCallback<Status> getResultHandler(final AsyncFrameBuffer fb, final int seqid)
{    final org.apache.thrift.AsyncProcessFunction fcall = this;    return new AsyncMethodCallback<Status>() {        public void onComplete(Status o) {            append_result result = new append_result();            result.success = o;            try {                fcall.sendResponse(fb, result, org.apache.thrift.protocol.TMessageType.REPLY, seqid);                return;            } catch (Exception e) {                            }            fb.close();        }        public void onError(Exception e) {            byte msgType = org.apache.thrift.protocol.TMessageType.REPLY;            org.apache.thrift.TBase msg;            append_result result = new append_result();            {                msgType = org.apache.thrift.protocol.TMessageType.EXCEPTION;                msg = (org.apache.thrift.TBase) new org.apache.thrift.TApplicationException(org.apache.thrift.TApplicationException.INTERNAL_ERROR, e.getMessage());            }            try {                fcall.sendResponse(fb, msg, msgType, seqid);                return;            } catch (Exception ex) {                            }            fb.close();        }    };}
1
public void onComplete(Status o)
{    append_result result = new append_result();    result.success = o;    try {        fcall.sendResponse(fb, result, org.apache.thrift.protocol.TMessageType.REPLY, seqid);        return;    } catch (Exception e) {            }    fb.close();}
1
public void onError(Exception e)
{    byte msgType = org.apache.thrift.protocol.TMessageType.REPLY;    org.apache.thrift.TBase msg;    append_result result = new append_result();    {        msgType = org.apache.thrift.protocol.TMessageType.EXCEPTION;        msg = (org.apache.thrift.TBase) new org.apache.thrift.TApplicationException(org.apache.thrift.TApplicationException.INTERNAL_ERROR, e.getMessage());    }    try {        fcall.sendResponse(fb, msg, msgType, seqid);        return;    } catch (Exception ex) {            }    fb.close();}
1
protected boolean isOneway()
{    return false;}
0
public void start(I iface, append_args args, org.apache.thrift.async.AsyncMethodCallback<Status> resultHandler) throws TException
{    iface.append(args.event, resultHandler);}
0
public appendBatch_args getEmptyArgsInstance()
{    return new appendBatch_args();}
0
public AsyncMethodCallback<Status> getResultHandler(final AsyncFrameBuffer fb, final int seqid)
{    final org.apache.thrift.AsyncProcessFunction fcall = this;    return new AsyncMethodCallback<Status>() {        public void onComplete(Status o) {            appendBatch_result result = new appendBatch_result();            result.success = o;            try {                fcall.sendResponse(fb, result, org.apache.thrift.protocol.TMessageType.REPLY, seqid);                return;            } catch (Exception e) {                            }            fb.close();        }        public void onError(Exception e) {            byte msgType = org.apache.thrift.protocol.TMessageType.REPLY;            org.apache.thrift.TBase msg;            appendBatch_result result = new appendBatch_result();            {                msgType = org.apache.thrift.protocol.TMessageType.EXCEPTION;                msg = (org.apache.thrift.TBase) new org.apache.thrift.TApplicationException(org.apache.thrift.TApplicationException.INTERNAL_ERROR, e.getMessage());            }            try {                fcall.sendResponse(fb, msg, msgType, seqid);                return;            } catch (Exception ex) {                            }            fb.close();        }    };}
1
public void onComplete(Status o)
{    appendBatch_result result = new appendBatch_result();    result.success = o;    try {        fcall.sendResponse(fb, result, org.apache.thrift.protocol.TMessageType.REPLY, seqid);        return;    } catch (Exception e) {            }    fb.close();}
1
public void onError(Exception e)
{    byte msgType = org.apache.thrift.protocol.TMessageType.REPLY;    org.apache.thrift.TBase msg;    appendBatch_result result = new appendBatch_result();    {        msgType = org.apache.thrift.protocol.TMessageType.EXCEPTION;        msg = (org.apache.thrift.TBase) new org.apache.thrift.TApplicationException(org.apache.thrift.TApplicationException.INTERNAL_ERROR, e.getMessage());    }    try {        fcall.sendResponse(fb, msg, msgType, seqid);        return;    } catch (Exception ex) {            }    fb.close();}
1
protected boolean isOneway()
{    return false;}
0
public void start(I iface, appendBatch_args args, org.apache.thrift.async.AsyncMethodCallback<Status> resultHandler) throws TException
{    iface.appendBatch(args.events, resultHandler);}
0
public static _Fields findByThriftId(int fieldId)
{    switch(fieldId) {        case         1:            return EVENT;        default:            return null;    }}
0
public static _Fields findByThriftIdOrThrow(int fieldId)
{    _Fields fields = findByThriftId(fieldId);    if (fields == null)        throw new IllegalArgumentException("Field " + fieldId + " doesn't exist!");    return fields;}
0
public static _Fields findByName(String name)
{    return byName.get(name);}
0
public short getThriftFieldId()
{    return _thriftId;}
0
public String getFieldName()
{    return _fieldName;}
0
public append_args deepCopy()
{    return new append_args(this);}
0
public void clear()
{    this.event = null;}
0
public ThriftFlumeEvent getEvent()
{    return this.event;}
0
public append_args setEvent(ThriftFlumeEvent event)
{    this.event = event;    return this;}
0
public void unsetEvent()
{    this.event = null;}
0
public boolean isSetEvent()
{    return this.event != null;}
0
public void setEventIsSet(boolean value)
{    if (!value) {        this.event = null;    }}
0
public void setFieldValue(_Fields field, Object value)
{    switch(field) {        case EVENT:            if (value == null) {                unsetEvent();            } else {                setEvent((ThriftFlumeEvent) value);            }            break;    }}
0
public Object getFieldValue(_Fields field)
{    switch(field) {        case EVENT:            return getEvent();    }    throw new IllegalStateException();}
0
public boolean isSet(_Fields field)
{    if (field == null) {        throw new IllegalArgumentException();    }    switch(field) {        case EVENT:            return isSetEvent();    }    throw new IllegalStateException();}
0
public boolean equals(Object that)
{    if (that == null)        return false;    if (that instanceof append_args)        return this.equals((append_args) that);    return false;}
0
public boolean equals(append_args that)
{    if (that == null)        return false;    boolean this_present_event = true && this.isSetEvent();    boolean that_present_event = true && that.isSetEvent();    if (this_present_event || that_present_event) {        if (!(this_present_event && that_present_event))            return false;        if (!this.event.equals(that.event))            return false;    }    return true;}
0
public int hashCode()
{    List<Object> list = new ArrayList<Object>();    boolean present_event = true && (isSetEvent());    list.add(present_event);    if (present_event)        list.add(event);    return list.hashCode();}
0
public int compareTo(append_args other)
{    if (!getClass().equals(other.getClass())) {        return getClass().getName().compareTo(other.getClass().getName());    }    int lastComparison = 0;    lastComparison = Boolean.valueOf(isSetEvent()).compareTo(other.isSetEvent());    if (lastComparison != 0) {        return lastComparison;    }    if (isSetEvent()) {        lastComparison = org.apache.thrift.TBaseHelper.compareTo(this.event, other.event);        if (lastComparison != 0) {            return lastComparison;        }    }    return 0;}
0
public _Fields fieldForId(int fieldId)
{    return _Fields.findByThriftId(fieldId);}
0
public void read(org.apache.thrift.protocol.TProtocol iprot) throws org.apache.thrift.TException
{    schemes.get(iprot.getScheme()).getScheme().read(iprot, this);}
0
public void write(org.apache.thrift.protocol.TProtocol oprot) throws org.apache.thrift.TException
{    schemes.get(oprot.getScheme()).getScheme().write(oprot, this);}
0
public String toString()
{    StringBuilder sb = new StringBuilder("append_args(");    boolean first = true;    sb.append("event:");    if (this.event == null) {        sb.append("null");    } else {        sb.append(this.event);    }    first = false;    sb.append(")");    return sb.toString();}
0
public void validate() throws org.apache.thrift.TException
{        if (event != null) {        event.validate();    }}
0
private void writeObject(java.io.ObjectOutputStream out) throws java.io.IOException
{    try {        write(new org.apache.thrift.protocol.TCompactProtocol(new org.apache.thrift.transport.TIOStreamTransport(out)));    } catch (org.apache.thrift.TException te) {        throw new java.io.IOException(te);    }}
0
private void readObject(java.io.ObjectInputStream in) throws java.io.IOException, ClassNotFoundException
{    try {        read(new org.apache.thrift.protocol.TCompactProtocol(new org.apache.thrift.transport.TIOStreamTransport(in)));    } catch (org.apache.thrift.TException te) {        throw new java.io.IOException(te);    }}
0
public append_argsStandardScheme getScheme()
{    return new append_argsStandardScheme();}
0
public void read(org.apache.thrift.protocol.TProtocol iprot, append_args struct) throws org.apache.thrift.TException
{    org.apache.thrift.protocol.TField schemeField;    iprot.readStructBegin();    while (true) {        schemeField = iprot.readFieldBegin();        if (schemeField.type == org.apache.thrift.protocol.TType.STOP) {            break;        }        switch(schemeField.id) {            case             1:                if (schemeField.type == org.apache.thrift.protocol.TType.STRUCT) {                    struct.event = new ThriftFlumeEvent();                    struct.event.read(iprot);                    struct.setEventIsSet(true);                } else {                    org.apache.thrift.protocol.TProtocolUtil.skip(iprot, schemeField.type);                }                break;            default:                org.apache.thrift.protocol.TProtocolUtil.skip(iprot, schemeField.type);        }        iprot.readFieldEnd();    }    iprot.readStructEnd();        struct.validate();}
0
public void write(org.apache.thrift.protocol.TProtocol oprot, append_args struct) throws org.apache.thrift.TException
{    struct.validate();    oprot.writeStructBegin(STRUCT_DESC);    if (struct.event != null) {        oprot.writeFieldBegin(EVENT_FIELD_DESC);        struct.event.write(oprot);        oprot.writeFieldEnd();    }    oprot.writeFieldStop();    oprot.writeStructEnd();}
0
public append_argsTupleScheme getScheme()
{    return new append_argsTupleScheme();}
0
public void write(org.apache.thrift.protocol.TProtocol prot, append_args struct) throws org.apache.thrift.TException
{    TTupleProtocol oprot = (TTupleProtocol) prot;    BitSet optionals = new BitSet();    if (struct.isSetEvent()) {        optionals.set(0);    }    oprot.writeBitSet(optionals, 1);    if (struct.isSetEvent()) {        struct.event.write(oprot);    }}
0
public void read(org.apache.thrift.protocol.TProtocol prot, append_args struct) throws org.apache.thrift.TException
{    TTupleProtocol iprot = (TTupleProtocol) prot;    BitSet incoming = iprot.readBitSet(1);    if (incoming.get(0)) {        struct.event = new ThriftFlumeEvent();        struct.event.read(iprot);        struct.setEventIsSet(true);    }}
0
public static _Fields findByThriftId(int fieldId)
{    switch(fieldId) {        case         0:            return SUCCESS;        default:            return null;    }}
0
public static _Fields findByThriftIdOrThrow(int fieldId)
{    _Fields fields = findByThriftId(fieldId);    if (fields == null)        throw new IllegalArgumentException("Field " + fieldId + " doesn't exist!");    return fields;}
0
public static _Fields findByName(String name)
{    return byName.get(name);}
0
public short getThriftFieldId()
{    return _thriftId;}
0
public String getFieldName()
{    return _fieldName;}
0
public append_result deepCopy()
{    return new append_result(this);}
0
public void clear()
{    this.success = null;}
0
public Status getSuccess()
{    return this.success;}
0
public append_result setSuccess(Status success)
{    this.success = success;    return this;}
0
public void unsetSuccess()
{    this.success = null;}
0
public boolean isSetSuccess()
{    return this.success != null;}
0
public void setSuccessIsSet(boolean value)
{    if (!value) {        this.success = null;    }}
0
public void setFieldValue(_Fields field, Object value)
{    switch(field) {        case SUCCESS:            if (value == null) {                unsetSuccess();            } else {                setSuccess((Status) value);            }            break;    }}
0
public Object getFieldValue(_Fields field)
{    switch(field) {        case SUCCESS:            return getSuccess();    }    throw new IllegalStateException();}
0
public boolean isSet(_Fields field)
{    if (field == null) {        throw new IllegalArgumentException();    }    switch(field) {        case SUCCESS:            return isSetSuccess();    }    throw new IllegalStateException();}
0
public boolean equals(Object that)
{    if (that == null)        return false;    if (that instanceof append_result)        return this.equals((append_result) that);    return false;}
0
public boolean equals(append_result that)
{    if (that == null)        return false;    boolean this_present_success = true && this.isSetSuccess();    boolean that_present_success = true && that.isSetSuccess();    if (this_present_success || that_present_success) {        if (!(this_present_success && that_present_success))            return false;        if (!this.success.equals(that.success))            return false;    }    return true;}
0
public int hashCode()
{    List<Object> list = new ArrayList<Object>();    boolean present_success = true && (isSetSuccess());    list.add(present_success);    if (present_success)        list.add(success.getValue());    return list.hashCode();}
0
public int compareTo(append_result other)
{    if (!getClass().equals(other.getClass())) {        return getClass().getName().compareTo(other.getClass().getName());    }    int lastComparison = 0;    lastComparison = Boolean.valueOf(isSetSuccess()).compareTo(other.isSetSuccess());    if (lastComparison != 0) {        return lastComparison;    }    if (isSetSuccess()) {        lastComparison = org.apache.thrift.TBaseHelper.compareTo(this.success, other.success);        if (lastComparison != 0) {            return lastComparison;        }    }    return 0;}
0
public _Fields fieldForId(int fieldId)
{    return _Fields.findByThriftId(fieldId);}
0
public void read(org.apache.thrift.protocol.TProtocol iprot) throws org.apache.thrift.TException
{    schemes.get(iprot.getScheme()).getScheme().read(iprot, this);}
0
public void write(org.apache.thrift.protocol.TProtocol oprot) throws org.apache.thrift.TException
{    schemes.get(oprot.getScheme()).getScheme().write(oprot, this);}
0
public String toString()
{    StringBuilder sb = new StringBuilder("append_result(");    boolean first = true;    sb.append("success:");    if (this.success == null) {        sb.append("null");    } else {        sb.append(this.success);    }    first = false;    sb.append(")");    return sb.toString();}
0
public void validate() throws org.apache.thrift.TException
{}
0
private void writeObject(java.io.ObjectOutputStream out) throws java.io.IOException
{    try {        write(new org.apache.thrift.protocol.TCompactProtocol(new org.apache.thrift.transport.TIOStreamTransport(out)));    } catch (org.apache.thrift.TException te) {        throw new java.io.IOException(te);    }}
0
private void readObject(java.io.ObjectInputStream in) throws java.io.IOException, ClassNotFoundException
{    try {        read(new org.apache.thrift.protocol.TCompactProtocol(new org.apache.thrift.transport.TIOStreamTransport(in)));    } catch (org.apache.thrift.TException te) {        throw new java.io.IOException(te);    }}
0
public append_resultStandardScheme getScheme()
{    return new append_resultStandardScheme();}
0
public void read(org.apache.thrift.protocol.TProtocol iprot, append_result struct) throws org.apache.thrift.TException
{    org.apache.thrift.protocol.TField schemeField;    iprot.readStructBegin();    while (true) {        schemeField = iprot.readFieldBegin();        if (schemeField.type == org.apache.thrift.protocol.TType.STOP) {            break;        }        switch(schemeField.id) {            case             0:                if (schemeField.type == org.apache.thrift.protocol.TType.I32) {                    struct.success = org.apache.flume.thrift.Status.findByValue(iprot.readI32());                    struct.setSuccessIsSet(true);                } else {                    org.apache.thrift.protocol.TProtocolUtil.skip(iprot, schemeField.type);                }                break;            default:                org.apache.thrift.protocol.TProtocolUtil.skip(iprot, schemeField.type);        }        iprot.readFieldEnd();    }    iprot.readStructEnd();        struct.validate();}
0
public void write(org.apache.thrift.protocol.TProtocol oprot, append_result struct) throws org.apache.thrift.TException
{    struct.validate();    oprot.writeStructBegin(STRUCT_DESC);    if (struct.success != null) {        oprot.writeFieldBegin(SUCCESS_FIELD_DESC);        oprot.writeI32(struct.success.getValue());        oprot.writeFieldEnd();    }    oprot.writeFieldStop();    oprot.writeStructEnd();}
0
public append_resultTupleScheme getScheme()
{    return new append_resultTupleScheme();}
0
public void write(org.apache.thrift.protocol.TProtocol prot, append_result struct) throws org.apache.thrift.TException
{    TTupleProtocol oprot = (TTupleProtocol) prot;    BitSet optionals = new BitSet();    if (struct.isSetSuccess()) {        optionals.set(0);    }    oprot.writeBitSet(optionals, 1);    if (struct.isSetSuccess()) {        oprot.writeI32(struct.success.getValue());    }}
0
public void read(org.apache.thrift.protocol.TProtocol prot, append_result struct) throws org.apache.thrift.TException
{    TTupleProtocol iprot = (TTupleProtocol) prot;    BitSet incoming = iprot.readBitSet(1);    if (incoming.get(0)) {        struct.success = org.apache.flume.thrift.Status.findByValue(iprot.readI32());        struct.setSuccessIsSet(true);    }}
0
public static _Fields findByThriftId(int fieldId)
{    switch(fieldId) {        case         1:            return EVENTS;        default:            return null;    }}
0
public static _Fields findByThriftIdOrThrow(int fieldId)
{    _Fields fields = findByThriftId(fieldId);    if (fields == null)        throw new IllegalArgumentException("Field " + fieldId + " doesn't exist!");    return fields;}
0
public static _Fields findByName(String name)
{    return byName.get(name);}
0
public short getThriftFieldId()
{    return _thriftId;}
0
public String getFieldName()
{    return _fieldName;}
0
public appendBatch_args deepCopy()
{    return new appendBatch_args(this);}
0
public void clear()
{    this.events = null;}
0
public int getEventsSize()
{    return (this.events == null) ? 0 : this.events.size();}
0
public java.util.Iterator<ThriftFlumeEvent> getEventsIterator()
{    return (this.events == null) ? null : this.events.iterator();}
0
public void addToEvents(ThriftFlumeEvent elem)
{    if (this.events == null) {        this.events = new ArrayList<ThriftFlumeEvent>();    }    this.events.add(elem);}
0
public List<ThriftFlumeEvent> getEvents()
{    return this.events;}
0
public appendBatch_args setEvents(List<ThriftFlumeEvent> events)
{    this.events = events;    return this;}
0
public void unsetEvents()
{    this.events = null;}
0
public boolean isSetEvents()
{    return this.events != null;}
0
public void setEventsIsSet(boolean value)
{    if (!value) {        this.events = null;    }}
0
public void setFieldValue(_Fields field, Object value)
{    switch(field) {        case EVENTS:            if (value == null) {                unsetEvents();            } else {                setEvents((List<ThriftFlumeEvent>) value);            }            break;    }}
0
public Object getFieldValue(_Fields field)
{    switch(field) {        case EVENTS:            return getEvents();    }    throw new IllegalStateException();}
0
public boolean isSet(_Fields field)
{    if (field == null) {        throw new IllegalArgumentException();    }    switch(field) {        case EVENTS:            return isSetEvents();    }    throw new IllegalStateException();}
0
public boolean equals(Object that)
{    if (that == null)        return false;    if (that instanceof appendBatch_args)        return this.equals((appendBatch_args) that);    return false;}
0
public boolean equals(appendBatch_args that)
{    if (that == null)        return false;    boolean this_present_events = true && this.isSetEvents();    boolean that_present_events = true && that.isSetEvents();    if (this_present_events || that_present_events) {        if (!(this_present_events && that_present_events))            return false;        if (!this.events.equals(that.events))            return false;    }    return true;}
0
public int hashCode()
{    List<Object> list = new ArrayList<Object>();    boolean present_events = true && (isSetEvents());    list.add(present_events);    if (present_events)        list.add(events);    return list.hashCode();}
0
public int compareTo(appendBatch_args other)
{    if (!getClass().equals(other.getClass())) {        return getClass().getName().compareTo(other.getClass().getName());    }    int lastComparison = 0;    lastComparison = Boolean.valueOf(isSetEvents()).compareTo(other.isSetEvents());    if (lastComparison != 0) {        return lastComparison;    }    if (isSetEvents()) {        lastComparison = org.apache.thrift.TBaseHelper.compareTo(this.events, other.events);        if (lastComparison != 0) {            return lastComparison;        }    }    return 0;}
0
public _Fields fieldForId(int fieldId)
{    return _Fields.findByThriftId(fieldId);}
0
public void read(org.apache.thrift.protocol.TProtocol iprot) throws org.apache.thrift.TException
{    schemes.get(iprot.getScheme()).getScheme().read(iprot, this);}
0
public void write(org.apache.thrift.protocol.TProtocol oprot) throws org.apache.thrift.TException
{    schemes.get(oprot.getScheme()).getScheme().write(oprot, this);}
0
public String toString()
{    StringBuilder sb = new StringBuilder("appendBatch_args(");    boolean first = true;    sb.append("events:");    if (this.events == null) {        sb.append("null");    } else {        sb.append(this.events);    }    first = false;    sb.append(")");    return sb.toString();}
0
public void validate() throws org.apache.thrift.TException
{}
0
private void writeObject(java.io.ObjectOutputStream out) throws java.io.IOException
{    try {        write(new org.apache.thrift.protocol.TCompactProtocol(new org.apache.thrift.transport.TIOStreamTransport(out)));    } catch (org.apache.thrift.TException te) {        throw new java.io.IOException(te);    }}
0
private void readObject(java.io.ObjectInputStream in) throws java.io.IOException, ClassNotFoundException
{    try {        read(new org.apache.thrift.protocol.TCompactProtocol(new org.apache.thrift.transport.TIOStreamTransport(in)));    } catch (org.apache.thrift.TException te) {        throw new java.io.IOException(te);    }}
0
public appendBatch_argsStandardScheme getScheme()
{    return new appendBatch_argsStandardScheme();}
0
public void read(org.apache.thrift.protocol.TProtocol iprot, appendBatch_args struct) throws org.apache.thrift.TException
{    org.apache.thrift.protocol.TField schemeField;    iprot.readStructBegin();    while (true) {        schemeField = iprot.readFieldBegin();        if (schemeField.type == org.apache.thrift.protocol.TType.STOP) {            break;        }        switch(schemeField.id) {            case             1:                if (schemeField.type == org.apache.thrift.protocol.TType.LIST) {                    {                        org.apache.thrift.protocol.TList _list10 = iprot.readListBegin();                        struct.events = new ArrayList<ThriftFlumeEvent>(_list10.size);                        ThriftFlumeEvent _elem11;                        for (int _i12 = 0; _i12 < _list10.size; ++_i12) {                            _elem11 = new ThriftFlumeEvent();                            _elem11.read(iprot);                            struct.events.add(_elem11);                        }                        iprot.readListEnd();                    }                    struct.setEventsIsSet(true);                } else {                    org.apache.thrift.protocol.TProtocolUtil.skip(iprot, schemeField.type);                }                break;            default:                org.apache.thrift.protocol.TProtocolUtil.skip(iprot, schemeField.type);        }        iprot.readFieldEnd();    }    iprot.readStructEnd();        struct.validate();}
0
public void write(org.apache.thrift.protocol.TProtocol oprot, appendBatch_args struct) throws org.apache.thrift.TException
{    struct.validate();    oprot.writeStructBegin(STRUCT_DESC);    if (struct.events != null) {        oprot.writeFieldBegin(EVENTS_FIELD_DESC);        {            oprot.writeListBegin(new org.apache.thrift.protocol.TList(org.apache.thrift.protocol.TType.STRUCT, struct.events.size()));            for (ThriftFlumeEvent _iter13 : struct.events) {                _iter13.write(oprot);            }            oprot.writeListEnd();        }        oprot.writeFieldEnd();    }    oprot.writeFieldStop();    oprot.writeStructEnd();}
0
public appendBatch_argsTupleScheme getScheme()
{    return new appendBatch_argsTupleScheme();}
0
public void write(org.apache.thrift.protocol.TProtocol prot, appendBatch_args struct) throws org.apache.thrift.TException
{    TTupleProtocol oprot = (TTupleProtocol) prot;    BitSet optionals = new BitSet();    if (struct.isSetEvents()) {        optionals.set(0);    }    oprot.writeBitSet(optionals, 1);    if (struct.isSetEvents()) {        {            oprot.writeI32(struct.events.size());            for (ThriftFlumeEvent _iter14 : struct.events) {                _iter14.write(oprot);            }        }    }}
0
public void read(org.apache.thrift.protocol.TProtocol prot, appendBatch_args struct) throws org.apache.thrift.TException
{    TTupleProtocol iprot = (TTupleProtocol) prot;    BitSet incoming = iprot.readBitSet(1);    if (incoming.get(0)) {        {            org.apache.thrift.protocol.TList _list15 = new org.apache.thrift.protocol.TList(org.apache.thrift.protocol.TType.STRUCT, iprot.readI32());            struct.events = new ArrayList<ThriftFlumeEvent>(_list15.size);            ThriftFlumeEvent _elem16;            for (int _i17 = 0; _i17 < _list15.size; ++_i17) {                _elem16 = new ThriftFlumeEvent();                _elem16.read(iprot);                struct.events.add(_elem16);            }        }        struct.setEventsIsSet(true);    }}
0
public static _Fields findByThriftId(int fieldId)
{    switch(fieldId) {        case         0:            return SUCCESS;        default:            return null;    }}
0
public static _Fields findByThriftIdOrThrow(int fieldId)
{    _Fields fields = findByThriftId(fieldId);    if (fields == null)        throw new IllegalArgumentException("Field " + fieldId + " doesn't exist!");    return fields;}
0
public static _Fields findByName(String name)
{    return byName.get(name);}
0
public short getThriftFieldId()
{    return _thriftId;}
0
public String getFieldName()
{    return _fieldName;}
0
public appendBatch_result deepCopy()
{    return new appendBatch_result(this);}
0
public void clear()
{    this.success = null;}
0
public Status getSuccess()
{    return this.success;}
0
public appendBatch_result setSuccess(Status success)
{    this.success = success;    return this;}
0
public void unsetSuccess()
{    this.success = null;}
0
public boolean isSetSuccess()
{    return this.success != null;}
0
public void setSuccessIsSet(boolean value)
{    if (!value) {        this.success = null;    }}
0
public void setFieldValue(_Fields field, Object value)
{    switch(field) {        case SUCCESS:            if (value == null) {                unsetSuccess();            } else {                setSuccess((Status) value);            }            break;    }}
0
public Object getFieldValue(_Fields field)
{    switch(field) {        case SUCCESS:            return getSuccess();    }    throw new IllegalStateException();}
0
public boolean isSet(_Fields field)
{    if (field == null) {        throw new IllegalArgumentException();    }    switch(field) {        case SUCCESS:            return isSetSuccess();    }    throw new IllegalStateException();}
0
public boolean equals(Object that)
{    if (that == null)        return false;    if (that instanceof appendBatch_result)        return this.equals((appendBatch_result) that);    return false;}
0
public boolean equals(appendBatch_result that)
{    if (that == null)        return false;    boolean this_present_success = true && this.isSetSuccess();    boolean that_present_success = true && that.isSetSuccess();    if (this_present_success || that_present_success) {        if (!(this_present_success && that_present_success))            return false;        if (!this.success.equals(that.success))            return false;    }    return true;}
0
public int hashCode()
{    List<Object> list = new ArrayList<Object>();    boolean present_success = true && (isSetSuccess());    list.add(present_success);    if (present_success)        list.add(success.getValue());    return list.hashCode();}
0
public int compareTo(appendBatch_result other)
{    if (!getClass().equals(other.getClass())) {        return getClass().getName().compareTo(other.getClass().getName());    }    int lastComparison = 0;    lastComparison = Boolean.valueOf(isSetSuccess()).compareTo(other.isSetSuccess());    if (lastComparison != 0) {        return lastComparison;    }    if (isSetSuccess()) {        lastComparison = org.apache.thrift.TBaseHelper.compareTo(this.success, other.success);        if (lastComparison != 0) {            return lastComparison;        }    }    return 0;}
0
public _Fields fieldForId(int fieldId)
{    return _Fields.findByThriftId(fieldId);}
0
public void read(org.apache.thrift.protocol.TProtocol iprot) throws org.apache.thrift.TException
{    schemes.get(iprot.getScheme()).getScheme().read(iprot, this);}
0
public void write(org.apache.thrift.protocol.TProtocol oprot) throws org.apache.thrift.TException
{    schemes.get(oprot.getScheme()).getScheme().write(oprot, this);}
0
public String toString()
{    StringBuilder sb = new StringBuilder("appendBatch_result(");    boolean first = true;    sb.append("success:");    if (this.success == null) {        sb.append("null");    } else {        sb.append(this.success);    }    first = false;    sb.append(")");    return sb.toString();}
0
public void validate() throws org.apache.thrift.TException
{}
0
private void writeObject(java.io.ObjectOutputStream out) throws java.io.IOException
{    try {        write(new org.apache.thrift.protocol.TCompactProtocol(new org.apache.thrift.transport.TIOStreamTransport(out)));    } catch (org.apache.thrift.TException te) {        throw new java.io.IOException(te);    }}
0
private void readObject(java.io.ObjectInputStream in) throws java.io.IOException, ClassNotFoundException
{    try {        read(new org.apache.thrift.protocol.TCompactProtocol(new org.apache.thrift.transport.TIOStreamTransport(in)));    } catch (org.apache.thrift.TException te) {        throw new java.io.IOException(te);    }}
0
public appendBatch_resultStandardScheme getScheme()
{    return new appendBatch_resultStandardScheme();}
0
public void read(org.apache.thrift.protocol.TProtocol iprot, appendBatch_result struct) throws org.apache.thrift.TException
{    org.apache.thrift.protocol.TField schemeField;    iprot.readStructBegin();    while (true) {        schemeField = iprot.readFieldBegin();        if (schemeField.type == org.apache.thrift.protocol.TType.STOP) {            break;        }        switch(schemeField.id) {            case             0:                if (schemeField.type == org.apache.thrift.protocol.TType.I32) {                    struct.success = org.apache.flume.thrift.Status.findByValue(iprot.readI32());                    struct.setSuccessIsSet(true);                } else {                    org.apache.thrift.protocol.TProtocolUtil.skip(iprot, schemeField.type);                }                break;            default:                org.apache.thrift.protocol.TProtocolUtil.skip(iprot, schemeField.type);        }        iprot.readFieldEnd();    }    iprot.readStructEnd();        struct.validate();}
0
public void write(org.apache.thrift.protocol.TProtocol oprot, appendBatch_result struct) throws org.apache.thrift.TException
{    struct.validate();    oprot.writeStructBegin(STRUCT_DESC);    if (struct.success != null) {        oprot.writeFieldBegin(SUCCESS_FIELD_DESC);        oprot.writeI32(struct.success.getValue());        oprot.writeFieldEnd();    }    oprot.writeFieldStop();    oprot.writeStructEnd();}
0
public appendBatch_resultTupleScheme getScheme()
{    return new appendBatch_resultTupleScheme();}
0
public void write(org.apache.thrift.protocol.TProtocol prot, appendBatch_result struct) throws org.apache.thrift.TException
{    TTupleProtocol oprot = (TTupleProtocol) prot;    BitSet optionals = new BitSet();    if (struct.isSetSuccess()) {        optionals.set(0);    }    oprot.writeBitSet(optionals, 1);    if (struct.isSetSuccess()) {        oprot.writeI32(struct.success.getValue());    }}
0
public void read(org.apache.thrift.protocol.TProtocol prot, appendBatch_result struct) throws org.apache.thrift.TException
{    TTupleProtocol iprot = (TTupleProtocol) prot;    BitSet incoming = iprot.readBitSet(1);    if (incoming.get(0)) {        struct.success = org.apache.flume.thrift.Status.findByValue(iprot.readI32());        struct.setSuccessIsSet(true);    }}
0
public void setObjects(List<T> objects)
{    for (T sink : objects) {        FailureState state = new FailureState();        stateMap.put(sink, state);    }}
0
public List<T> getObjects()
{    return new ArrayList<T>(stateMap.keySet());}
0
public void informFailure(T failedObject)
{        if (!shouldBackOff) {        return;    }    FailureState state = stateMap.get(failedObject);    long now = System.currentTimeMillis();    long delta = now - state.lastFail;    /*     * When do we increase the backoff period?     * We basically calculate the time difference between the last failure     * and the current one. If this failure happened within one hour of the     * last backoff period getting over, then we increase the timeout,     * since the object did not recover yet. Else we assume this is a fresh     * failure and reset the count.     */    long lastBackoffLength = Math.min(maxTimeout, 1000 * (1 << state.sequentialFails));    long allowableDiff = lastBackoffLength + CONSIDER_SEQUENTIAL_RANGE;    if (allowableDiff > delta) {        if (state.sequentialFails < EXP_BACKOFF_COUNTER_LIMIT) {            state.sequentialFails++;        }    } else {        state.sequentialFails = 1;    }    state.lastFail = now;                state.restoreTime = now + Math.min(maxTimeout, 1000 * (1 << state.sequentialFails));}
0
protected List<Integer> getIndexList()
{    long now = System.currentTimeMillis();    List<Integer> indexList = new ArrayList<Integer>();    int i = 0;    for (T obj : stateMap.keySet()) {        if (!isShouldBackOff() || stateMap.get(obj).restoreTime < now) {            indexList.add(i);        }        i++;    }    return indexList;}
0
public boolean isShouldBackOff()
{    return shouldBackOff;}
0
public void setMaxTimeOut(long timeout)
{    this.maxTimeout = timeout;}
0
public long getMaxTimeOut()
{    return this.maxTimeout;}
0
public synchronized Iterator<T> createIterator()
{    List<Integer> indexList = getIndexList();    int size = indexList.size();    int[] indexOrder = new int[size];    while (indexList.size() != 1) {        int pick = random.nextInt(indexList.size());        indexOrder[indexList.size() - 1] = indexList.remove(pick);    }    indexOrder[0] = indexList.get(0);    return new SpecificOrderIterator<T>(indexOrder, getObjects());}
0
public Iterator<T> createIterator()
{    List<Integer> activeIndices = getIndexList();    int size = activeIndices.size();        if (nextHead >= size) {        nextHead = 0;    }    int begin = nextHead++;    if (nextHead == activeIndices.size()) {        nextHead = 0;    }    int[] indexOrder = new int[size];    for (int i = 0; i < size; i++) {        indexOrder[i] = activeIndices.get((begin + i) % size);    }    return new SpecificOrderIterator<T>(indexOrder, getObjects());}
0
public boolean hasNext()
{    return index < order.length;}
0
public T next()
{    return items.get(order[index++]);}
0
public void remove()
{    throw new UnsupportedOperationException();}
0
public static void initGlobalSSLParameters()
{    initSysPropFromEnvVar(SYS_PROP_KEYSTORE_PATH, ENV_VAR_KEYSTORE_PATH, DESCR_KEYSTORE_PATH);    initSysPropFromEnvVar(SYS_PROP_KEYSTORE_PASSWORD, ENV_VAR_KEYSTORE_PASSWORD, DESCR_KEYSTORE_PASSWORD);    initSysPropFromEnvVar(SYS_PROP_KEYSTORE_TYPE, ENV_VAR_KEYSTORE_TYPE, DESCR_KEYSTORE_TYPE);    initSysPropFromEnvVar(SYS_PROP_TRUSTSTORE_PATH, ENV_VAR_TRUSTSTORE_PATH, DESCR_TRUSTSTORE_PATH);    initSysPropFromEnvVar(SYS_PROP_TRUSTSTORE_PASSWORD, ENV_VAR_TRUSTSTORE_PASSWORD, DESCR_TRUSTSTORE_PASSWORD);    initSysPropFromEnvVar(SYS_PROP_TRUSTSTORE_TYPE, ENV_VAR_TRUSTSTORE_TYPE, DESCR_TRUSTSTORE_TYPE);    initSysPropFromEnvVar(SYS_PROP_INCLUDE_PROTOCOLS, ENV_VAR_INCLUDE_PROTOCOLS, DESCR_INCLUDE_PROTOCOLS);    initSysPropFromEnvVar(SYS_PROP_EXCLUDE_PROTOCOLS, ENV_VAR_EXCLUDE_PROTOCOLS, DESCR_EXCLUDE_PROTOCOLS);    initSysPropFromEnvVar(SYS_PROP_INCLUDE_CIPHERSUITES, ENV_VAR_INCLUDE_CIPHERSUITES, DESCR_INCLUDE_CIPHERSUITES);    initSysPropFromEnvVar(SYS_PROP_EXCLUDE_CIPHERSUITES, ENV_VAR_EXCLUDE_CIPHERSUITES, DESCR_EXCLUDE_CIPHERSUITES);}
0
private static void initSysPropFromEnvVar(String sysPropName, String envVarName, String description)
{    if (System.getProperty(sysPropName) != null) {            } else {        String envVarValue = System.getenv(envVarName);        if (envVarValue != null) {            System.setProperty(sysPropName, envVarValue);                    } else {                    }    }}
1
public static String getGlobalKeystorePath()
{    return System.getProperty(SYS_PROP_KEYSTORE_PATH);}
0
public static String getGlobalKeystorePassword()
{    return System.getProperty(SYS_PROP_KEYSTORE_PASSWORD);}
0
public static String getGlobalKeystoreType(String defaultValue)
{    String sysPropValue = System.getProperty(SYS_PROP_KEYSTORE_TYPE);    return sysPropValue != null ? sysPropValue : defaultValue;}
0
public static String getGlobalTruststorePath()
{    return System.getProperty(SYS_PROP_TRUSTSTORE_PATH);}
0
public static String getGlobalTruststorePassword()
{    return System.getProperty(SYS_PROP_TRUSTSTORE_PASSWORD);}
0
public static String getGlobalTruststoreType(String defaultValue)
{    String sysPropValue = System.getProperty(SYS_PROP_TRUSTSTORE_TYPE);    return sysPropValue != null ? sysPropValue : defaultValue;}
0
public static String getGlobalExcludeProtocols()
{    return normalizeProperty(SYS_PROP_EXCLUDE_PROTOCOLS);}
0
public static String getGlobalIncludeProtocols()
{    return normalizeProperty(SYS_PROP_INCLUDE_PROTOCOLS);}
0
public static String getGlobalExcludeCipherSuites()
{    return normalizeProperty(SYS_PROP_EXCLUDE_CIPHERSUITES);}
0
public static String getGlobalIncludeCipherSuites()
{    return normalizeProperty(SYS_PROP_INCLUDE_CIPHERSUITES);}
0
private static String normalizeProperty(String name)
{    String property = System.getProperty(name);    return property == null ? null : property.replaceAll(",", " ");}
0
public static void handlerSimpleAppendTest(AvroSourceProtocol handler) throws FlumeException, EventDeliveryException
{    handlerSimpleAppendTest(handler, false, false, 0);}
0
public static void handlerSimpleAppendTest(AvroSourceProtocol handler, boolean enableServerCompression, boolean enableClientCompression, int compressionLevel) throws FlumeException, EventDeliveryException
{    NettyAvroRpcClient client = null;    Server server = startServer(handler, 0, enableServerCompression);    try {        Properties starterProp = new Properties();        if (enableClientCompression) {            starterProp.setProperty(RpcClientConfigurationConstants.CONFIG_COMPRESSION_TYPE, "deflate");            starterProp.setProperty(RpcClientConfigurationConstants.CONFIG_COMPRESSION_LEVEL, "" + compressionLevel);        } else {            starterProp.setProperty(RpcClientConfigurationConstants.CONFIG_COMPRESSION_TYPE, "none");        }        client = getStockLocalClient(server.getPort(), starterProp);        boolean isActive = client.isActive();        Assert.assertTrue("Client should be active", isActive);        client.append(EventBuilder.withBody("wheee!!!", Charset.forName("UTF8")));    } finally {        stopServer(server);        if (client != null)            client.close();    }}
0
public static void handlerBatchAppendTest(AvroSourceProtocol handler) throws FlumeException, EventDeliveryException
{    handlerBatchAppendTest(handler, false, false, 0);}
0
public static void handlerBatchAppendTest(AvroSourceProtocol handler, boolean enableServerCompression, boolean enableClientCompression, int compressionLevel) throws FlumeException, EventDeliveryException
{    NettyAvroRpcClient client = null;    Server server = startServer(handler, 0, enableServerCompression);    try {        Properties starterProp = new Properties();        if (enableClientCompression) {            starterProp.setProperty(RpcClientConfigurationConstants.CONFIG_COMPRESSION_TYPE, "deflate");            starterProp.setProperty(RpcClientConfigurationConstants.CONFIG_COMPRESSION_LEVEL, "" + compressionLevel);        } else {            starterProp.setProperty(RpcClientConfigurationConstants.CONFIG_COMPRESSION_TYPE, "none");        }        client = getStockLocalClient(server.getPort(), starterProp);        boolean isActive = client.isActive();        Assert.assertTrue("Client should be active", isActive);        int batchSize = client.getBatchSize();        List<Event> events = new ArrayList<Event>();        for (int i = 0; i < batchSize; i++) {            events.add(EventBuilder.withBody("evt: " + i, Charset.forName("UTF8")));        }        client.appendBatch(events);    } finally {        stopServer(server);        if (client != null)            client.close();    }}
0
public static NettyAvroRpcClient getStockLocalClient(int port)
{    Properties props = new Properties();    return getStockLocalClient(port, props);}
0
public static NettyAvroRpcClient getStockLocalClient(int port, Properties starterProp)
{    starterProp.setProperty(RpcClientConfigurationConstants.CONFIG_HOSTS, "h1");    starterProp.setProperty(RpcClientConfigurationConstants.CONFIG_HOSTS_PREFIX + "h1", "127.0.0.1" + ":" + port);    NettyAvroRpcClient client = new NettyAvroRpcClient();    client.configure(starterProp);    return client;}
0
public static Server startServer(AvroSourceProtocol handler, int port, boolean enableCompression)
{    Responder responder = new SpecificResponder(AvroSourceProtocol.class, handler);    Server server;    if (enableCompression) {        server = new NettyServer(responder, new InetSocketAddress(localhost, port), new NioServerSocketChannelFactory(Executors.newCachedThreadPool(), Executors.newCachedThreadPool()), new CompressionChannelPipelineFactory(), null);    } else {        server = new NettyServer(responder, new InetSocketAddress(localhost, port));    }    server.start();        try {        Thread.sleep(300L);    } catch (InterruptedException ex) {                Thread.currentThread().interrupt();    }    return server;}
1
public static Server startServer(AvroSourceProtocol handler)
{    return startServer(handler, 0, false);}
0
public static Server startServer(AvroSourceProtocol handler, int port)
{    return startServer(handler, port, false);}
0
public static void stopServer(Server server)
{    try {        server.close();        server.join();    } catch (InterruptedException ex) {                Thread.currentThread().interrupt();    }}
1
public int getAppendCount()
{    return appendCount;}
0
public int getAppendBatchCount()
{    return appendBatchCount;}
0
public boolean isFailed()
{    return failed;}
0
public void setFailed()
{    this.failed = true;}
0
public void setOK()
{    this.failed = false;}
0
public Status append(AvroFlumeEvent event) throws AvroRemoteException
{    if (failed) {                return Status.FAILED;    }        appendCount++;    return Status.OK;}
1
public Status appendBatch(List<AvroFlumeEvent> events) throws AvroRemoteException
{    if (failed) {                return Status.FAILED;    }        appendBatchCount++;    return Status.OK;}
1
public Status append(AvroFlumeEvent event) throws AvroRemoteException
{        return Status.OK;}
1
public Status appendBatch(List<AvroFlumeEvent> events) throws AvroRemoteException
{        return Status.OK;}
1
public Status append(AvroFlumeEvent event) throws AvroRemoteException
{        return Status.FAILED;}
1
public Status appendBatch(List<AvroFlumeEvent> events) throws AvroRemoteException
{        return Status.FAILED;}
1
public Status append(AvroFlumeEvent event) throws AvroRemoteException
{        return Status.UNKNOWN;}
1
public Status appendBatch(List<AvroFlumeEvent> events) throws AvroRemoteException
{        return Status.UNKNOWN;}
1
public Status append(AvroFlumeEvent event) throws AvroRemoteException
{        throw new AvroRemoteException("Handler smash!");}
1
public Status appendBatch(List<AvroFlumeEvent> events) throws AvroRemoteException
{        throw new AvroRemoteException("Handler smash!");}
1
public ChannelPipeline getPipeline() throws Exception
{    ChannelPipeline pipeline = Channels.pipeline();    ZlibEncoder encoder = new ZlibEncoder(6);    pipeline.addFirst("deflater", encoder);    pipeline.addFirst("inflater", new ZlibDecoder());    return pipeline;}
0
public void testFailover() throws FlumeException, EventDeliveryException, InterruptedException
{    FailoverRpcClient client = null;    Server server1 = RpcTestUtils.startServer(new OKAvroHandler());    Server server2 = RpcTestUtils.startServer(new OKAvroHandler());    Server server3 = RpcTestUtils.startServer(new OKAvroHandler());    Properties props = new Properties();    int s1Port = server1.getPort();    int s2Port = server2.getPort();    int s3Port = server3.getPort();    props.put("client.type", "default_failover");    props.put("hosts", "host1 host2 host3");    props.put("hosts.host1", "127.0.0.1:" + String.valueOf(s1Port));    props.put("hosts.host2", "127.0.0.1:" + String.valueOf(s2Port));    props.put("hosts.host3", "127.0.0.1:" + String.valueOf(s3Port));    client = (FailoverRpcClient) RpcClientFactory.getInstance(props);    List<Event> events = new ArrayList<Event>();    for (int i = 0; i < 50; i++) {        events.add(EventBuilder.withBody("evt: " + i, Charset.forName("UTF8")));    }    client.appendBatch(events);    Assert.assertEquals(client.getLastConnectedServerAddress(), new InetSocketAddress("127.0.0.1", server1.getPort()));    server1.close();        Thread.sleep(1000L);    events = new ArrayList<Event>();    for (int i = 0; i < 50; i++) {        events.add(EventBuilder.withBody("evt: " + i, Charset.forName("UTF8")));    }    client.appendBatch(events);    Assert.assertEquals(new InetSocketAddress("localhost", server2.getPort()), client.getLastConnectedServerAddress());    server2.close();        Thread.sleep(1000L);    client.append(EventBuilder.withBody("Had a sandwich?", Charset.forName("UTF8")));    Assert.assertEquals(new InetSocketAddress("localhost", server3.getPort()), client.getLastConnectedServerAddress());        Server server4 = RpcTestUtils.startServer(new OKAvroHandler(), s2Port);    server3.close();        Thread.sleep(1000L);    events = new ArrayList<Event>();    for (int i = 0; i < 50; i++) {        events.add(EventBuilder.withBody("evt: " + i, Charset.forName("UTF8")));    }    client.appendBatch(events);    Assert.assertEquals(new InetSocketAddress("localhost", s2Port), client.getLastConnectedServerAddress());    Server server5 = RpcTestUtils.startServer(new OKAvroHandler(), s1Port);        client.append(EventBuilder.withBody("Had a mango?", Charset.forName("UTF8")));    Assert.assertEquals(new InetSocketAddress("localhost", s2Port), client.getLastConnectedServerAddress());    server4.close();        Thread.sleep(1000L);    events = new ArrayList<Event>();    for (int i = 0; i < 50; i++) {        events.add(EventBuilder.withBody("evt: " + i, Charset.forName("UTF8")));    }    client.appendBatch(events);    Assert.assertEquals(new InetSocketAddress("localhost", s1Port), client.getLastConnectedServerAddress());    server5.close();        Thread.sleep(1000L);    Server server6 = RpcTestUtils.startServer(new OKAvroHandler(), s1Port);    client.append(EventBuilder.withBody("Had a whole watermelon?", Charset.forName("UTF8")));    Assert.assertEquals(new InetSocketAddress("localhost", s1Port), client.getLastConnectedServerAddress());    server6.close();        Thread.sleep(1000L);    Server server7 = RpcTestUtils.startServer(new OKAvroHandler(), s3Port);    events = new ArrayList<Event>();    for (int i = 0; i < 50; i++) {        events.add(EventBuilder.withBody("evt: " + i, Charset.forName("UTF8")));    }    client.appendBatch(events);    Assert.assertEquals(new InetSocketAddress("localhost", s3Port), client.getLastConnectedServerAddress());    server7.close();}
0
public void testFailedServers() throws FlumeException, EventDeliveryException
{    FailoverRpcClient client = null;    Server server1 = RpcTestUtils.startServer(new OKAvroHandler());    Server server2 = RpcTestUtils.startServer(new OKAvroHandler());    Server server3 = RpcTestUtils.startServer(new OKAvroHandler());    Properties props = new Properties();    props.put("client.type", "default_failover");    props.put("hosts", "host1 host2 host3");    props.put("hosts.host1", "localhost:" + String.valueOf(server1.getPort()));    props.put("hosts.host2", "localhost:" + String.valueOf(server2.getPort()));    props.put("hosts.host3", " localhost:" + String.valueOf(server3.getPort()));    client = (FailoverRpcClient) RpcClientFactory.getInstance(props);    List<Event> events = new ArrayList<Event>();    for (int i = 0; i < 50; i++) {        events.add(EventBuilder.withBody("evt: " + i, Charset.forName("UTF8")));    }    client.appendBatch(events);    server1.close();    server2.close();    server3.close();    events = new ArrayList<Event>();    for (int i = 0; i < 50; i++) {        events.add(EventBuilder.withBody("evt: " + i, Charset.forName("UTF8")));    }    client.appendBatch(events);}
0
public void testCreatingLbClientSingleHost()
{    Server server1 = null;    RpcClient c = null;    try {        server1 = RpcTestUtils.startServer(new OKAvroHandler());        Properties p = new Properties();        p.put("host1", "127.0.0.1:" + server1.getPort());        p.put("hosts", "host1");        p.put("client.type", "default_loadbalance");        RpcClientFactory.getInstance(p);    } finally {        if (server1 != null)            server1.close();        if (c != null)            c.close();    }}
0
public void testTwoHostFailover() throws Exception
{    Server s1 = null;    Server s2 = null;    RpcClient c = null;    try {        LoadBalancedAvroHandler h1 = new LoadBalancedAvroHandler();        LoadBalancedAvroHandler h2 = new LoadBalancedAvroHandler();        s1 = RpcTestUtils.startServer(h1);        s2 = RpcTestUtils.startServer(h2);        Properties p = new Properties();        p.put("hosts", "h1 h2");        p.put("client.type", "default_loadbalance");        p.put("hosts.h1", "127.0.0.1:" + s1.getPort());        p.put("hosts.h2", "127.0.0.1:" + s2.getPort());        c = RpcClientFactory.getInstance(p);        Assert.assertTrue(c instanceof LoadBalancingRpcClient);        for (int i = 0; i < 100; i++) {            if (i == 20) {                h2.setFailed();            } else if (i == 40) {                h2.setOK();            }            c.append(getEvent(i));        }        Assert.assertEquals(60, h1.getAppendCount());        Assert.assertEquals(40, h2.getAppendCount());    } finally {        if (s1 != null)            s1.close();        if (s2 != null)            s2.close();        if (c != null)            c.close();    }}
0
public void testTwoHostFailoverThrowAfterClose() throws Exception
{    Server s1 = null;    Server s2 = null;    RpcClient c = null;    try {        LoadBalancedAvroHandler h1 = new LoadBalancedAvroHandler();        LoadBalancedAvroHandler h2 = new LoadBalancedAvroHandler();        s1 = RpcTestUtils.startServer(h1);        s2 = RpcTestUtils.startServer(h2);        Properties p = new Properties();        p.put("hosts", "h1 h2");        p.put("client.type", "default_loadbalance");        p.put("hosts.h1", "127.0.0.1:" + s1.getPort());        p.put("hosts.h2", "127.0.0.1:" + s2.getPort());        c = RpcClientFactory.getInstance(p);        Assert.assertTrue(c instanceof LoadBalancingRpcClient);        for (int i = 0; i < 100; i++) {            if (i == 20) {                h2.setFailed();            } else if (i == 40) {                h2.setOK();            }            c.append(getEvent(i));        }        Assert.assertEquals(60, h1.getAppendCount());        Assert.assertEquals(40, h2.getAppendCount());        if (c != null)            c.close();        c.append(getEvent(3));        Assert.fail();    } finally {        if (s1 != null)            s1.close();        if (s2 != null)            s2.close();    }}
0
public void testTwoHostsOneDead() throws Exception
{        Server s1 = null;    RpcClient c1 = null;    RpcClient c2 = null;    try {        LoadBalancedAvroHandler h1 = new LoadBalancedAvroHandler();        s1 = RpcTestUtils.startServer(h1);                Properties p = new Properties();        p.put("hosts", "h1 h2");        p.put("client.type", "default_loadbalance");                p.put("hosts.h1", "127.0.0.1:" + 0);        p.put("hosts.h2", "127.0.0.1:" + s1.getPort());                c1 = RpcClientFactory.getInstance(p);        Assert.assertTrue(c1 instanceof LoadBalancingRpcClient);        for (int i = 0; i < 10; i++) {            c1.appendBatch(getBatchedEvent(i));        }        Assert.assertEquals(10, h1.getAppendBatchCount());                c2 = RpcClientFactory.getInstance(p);        Assert.assertTrue(c2 instanceof LoadBalancingRpcClient);        for (int i = 0; i < 10; i++) {            c2.append(getEvent(i));        }        Assert.assertEquals(10, h1.getAppendCount());    } finally {        if (s1 != null)            s1.close();        if (c1 != null)            c1.close();        if (c2 != null)            c2.close();    }}
1
public void testTwoHostFailoverBatch() throws Exception
{    Server s1 = null;    Server s2 = null;    RpcClient c = null;    try {        LoadBalancedAvroHandler h1 = new LoadBalancedAvroHandler();        LoadBalancedAvroHandler h2 = new LoadBalancedAvroHandler();        s1 = RpcTestUtils.startServer(h1);        s2 = RpcTestUtils.startServer(h2);        Properties p = new Properties();        p.put("hosts", "h1 h2");        p.put("client.type", "default_loadbalance");        p.put("hosts.h1", "127.0.0.1:" + s1.getPort());        p.put("hosts.h2", "127.0.0.1:" + s2.getPort());        c = RpcClientFactory.getInstance(p);        Assert.assertTrue(c instanceof LoadBalancingRpcClient);        for (int i = 0; i < 100; i++) {            if (i == 20) {                h2.setFailed();            } else if (i == 40) {                h2.setOK();            }            c.appendBatch(getBatchedEvent(i));        }        Assert.assertEquals(60, h1.getAppendBatchCount());        Assert.assertEquals(40, h2.getAppendBatchCount());    } finally {        if (s1 != null)            s1.close();        if (s2 != null)            s2.close();        if (c != null)            c.close();    }}
0
public void testLbDefaultClientTwoHosts() throws Exception
{    Server s1 = null;    Server s2 = null;    RpcClient c = null;    try {        LoadBalancedAvroHandler h1 = new LoadBalancedAvroHandler();        LoadBalancedAvroHandler h2 = new LoadBalancedAvroHandler();        s1 = RpcTestUtils.startServer(h1);        s2 = RpcTestUtils.startServer(h2);        Properties p = new Properties();        p.put("hosts", "h1 h2");        p.put("client.type", "default_loadbalance");        p.put("hosts.h1", "127.0.0.1:" + s1.getPort());        p.put("hosts.h2", "127.0.0.1:" + s2.getPort());        c = RpcClientFactory.getInstance(p);        Assert.assertTrue(c instanceof LoadBalancingRpcClient);        for (int i = 0; i < 100; i++) {            c.append(getEvent(i));        }        Assert.assertEquals(50, h1.getAppendCount());        Assert.assertEquals(50, h2.getAppendCount());    } finally {        if (s1 != null)            s1.close();        if (s2 != null)            s2.close();        if (c != null)            c.close();    }}
0
public void testLbDefaultClientTwoHostsBatch() throws Exception
{    Server s1 = null;    Server s2 = null;    RpcClient c = null;    try {        LoadBalancedAvroHandler h1 = new LoadBalancedAvroHandler();        LoadBalancedAvroHandler h2 = new LoadBalancedAvroHandler();        s1 = RpcTestUtils.startServer(h1);        s2 = RpcTestUtils.startServer(h2);        Properties p = new Properties();        p.put("hosts", "h1 h2");        p.put("client.type", "default_loadbalance");        p.put("hosts.h1", "127.0.0.1:" + s1.getPort());        p.put("hosts.h2", "127.0.0.1:" + s2.getPort());        c = RpcClientFactory.getInstance(p);        Assert.assertTrue(c instanceof LoadBalancingRpcClient);        for (int i = 0; i < 100; i++) {            c.appendBatch(getBatchedEvent(i));        }        Assert.assertEquals(50, h1.getAppendBatchCount());        Assert.assertEquals(50, h2.getAppendBatchCount());    } finally {        if (s1 != null)            s1.close();        if (s2 != null)            s2.close();        if (c != null)            c.close();    }}
0
public void testLbClientTenHostRandomDistribution() throws Exception
{    final int NUM_HOSTS = 10;    final int NUM_EVENTS = 1000;    Server[] s = new Server[NUM_HOSTS];    LoadBalancedAvroHandler[] h = new LoadBalancedAvroHandler[NUM_HOSTS];    RpcClient c = null;    try {        Properties p = new Properties();        StringBuilder hostList = new StringBuilder("");        for (int i = 0; i < NUM_HOSTS; i++) {            h[i] = new LoadBalancedAvroHandler();            s[i] = RpcTestUtils.startServer(h[i]);            String name = "h" + i;            p.put("hosts." + name, "127.0.0.1:" + s[i].getPort());            hostList.append(name).append(" ");        }        p.put("hosts", hostList.toString().trim());        p.put("client.type", "default_loadbalance");        p.put("host-selector", "random");        c = RpcClientFactory.getInstance(p);        Assert.assertTrue(c instanceof LoadBalancingRpcClient);        for (int i = 0; i < NUM_EVENTS; i++) {            c.append(getEvent(i));        }        Set<Integer> counts = new HashSet<Integer>();        int total = 0;        for (LoadBalancedAvroHandler handler : h) {            total += handler.getAppendCount();            counts.add(handler.getAppendCount());        }        Assert.assertTrue("Very unusual distribution", counts.size() > 2);        Assert.assertTrue("Missing events", total == NUM_EVENTS);    } finally {        for (int i = 0; i < NUM_HOSTS; i++) {            if (s[i] != null)                s[i].close();        }    }}
0
public void testLbClientTenHostRandomDistributionBatch() throws Exception
{    final int NUM_HOSTS = 10;    final int NUM_EVENTS = 1000;    Server[] s = new Server[NUM_HOSTS];    LoadBalancedAvroHandler[] h = new LoadBalancedAvroHandler[NUM_HOSTS];    RpcClient c = null;    try {        Properties p = new Properties();        StringBuilder hostList = new StringBuilder("");        for (int i = 0; i < NUM_HOSTS; i++) {            h[i] = new LoadBalancedAvroHandler();            s[i] = RpcTestUtils.startServer(h[i]);            String name = "h" + i;            p.put("hosts." + name, "127.0.0.1:" + s[i].getPort());            hostList.append(name).append(" ");        }        p.put("hosts", hostList.toString().trim());        p.put("client.type", "default_loadbalance");        p.put("host-selector", "random");        c = RpcClientFactory.getInstance(p);        Assert.assertTrue(c instanceof LoadBalancingRpcClient);        for (int i = 0; i < NUM_EVENTS; i++) {            c.appendBatch(getBatchedEvent(i));        }        Set<Integer> counts = new HashSet<Integer>();        int total = 0;        for (LoadBalancedAvroHandler handler : h) {            total += handler.getAppendBatchCount();            counts.add(handler.getAppendBatchCount());        }        Assert.assertTrue("Very unusual distribution", counts.size() > 2);        Assert.assertTrue("Missing events", total == NUM_EVENTS);    } finally {        for (int i = 0; i < NUM_HOSTS; i++) {            if (s[i] != null)                s[i].close();        }    }}
0
public void testLbClientTenHostRoundRobinDistribution() throws Exception
{    final int NUM_HOSTS = 10;    final int NUM_EVENTS = 1000;    Server[] s = new Server[NUM_HOSTS];    LoadBalancedAvroHandler[] h = new LoadBalancedAvroHandler[NUM_HOSTS];    RpcClient c = null;    try {        Properties p = new Properties();        StringBuilder hostList = new StringBuilder("");        for (int i = 0; i < NUM_HOSTS; i++) {            h[i] = new LoadBalancedAvroHandler();            s[i] = RpcTestUtils.startServer(h[i]);            String name = "h" + i;            p.put("hosts." + name, "127.0.0.1:" + s[i].getPort());            hostList.append(name).append(" ");        }        p.put("hosts", hostList.toString().trim());        p.put("client.type", "default_loadbalance");        p.put("host-selector", "round_robin");        c = RpcClientFactory.getInstance(p);        Assert.assertTrue(c instanceof LoadBalancingRpcClient);        for (int i = 0; i < NUM_EVENTS; i++) {            c.append(getEvent(i));        }        Set<Integer> counts = new HashSet<Integer>();        int total = 0;        for (LoadBalancedAvroHandler handler : h) {            total += handler.getAppendCount();            counts.add(handler.getAppendCount());        }        Assert.assertTrue("Very unusual distribution", counts.size() == 1);        Assert.assertTrue("Missing events", total == NUM_EVENTS);    } finally {        for (int i = 0; i < NUM_HOSTS; i++) {            if (s[i] != null)                s[i].close();        }    }}
0
public void testLbClientTenHostRoundRobinDistributionBatch() throws Exception
{    final int NUM_HOSTS = 10;    final int NUM_EVENTS = 1000;    Server[] s = new Server[NUM_HOSTS];    LoadBalancedAvroHandler[] h = new LoadBalancedAvroHandler[NUM_HOSTS];    RpcClient c = null;    try {        Properties p = new Properties();        StringBuilder hostList = new StringBuilder("");        for (int i = 0; i < NUM_HOSTS; i++) {            h[i] = new LoadBalancedAvroHandler();            s[i] = RpcTestUtils.startServer(h[i]);            String name = "h" + i;            p.put("hosts." + name, "127.0.0.1:" + s[i].getPort());            hostList.append(name).append(" ");        }        p.put("hosts", hostList.toString().trim());        p.put("client.type", "default_loadbalance");        p.put("host-selector", "round_robin");        c = RpcClientFactory.getInstance(p);        Assert.assertTrue(c instanceof LoadBalancingRpcClient);        for (int i = 0; i < NUM_EVENTS; i++) {            c.appendBatch(getBatchedEvent(i));        }        Set<Integer> counts = new HashSet<Integer>();        int total = 0;        for (LoadBalancedAvroHandler handler : h) {            total += handler.getAppendBatchCount();            counts.add(handler.getAppendBatchCount());        }        Assert.assertTrue("Very unusual distribution", counts.size() == 1);        Assert.assertTrue("Missing events", total == NUM_EVENTS);    } finally {        for (int i = 0; i < NUM_HOSTS; i++) {            if (s[i] != null)                s[i].close();        }    }}
0
public void testRandomBackoff() throws Exception
{    Properties p = new Properties();    List<LoadBalancedAvroHandler> hosts = new ArrayList<LoadBalancedAvroHandler>();    List<Server> servers = new ArrayList<Server>();    StringBuilder hostList = new StringBuilder("");    for (int i = 0; i < 3; i++) {        LoadBalancedAvroHandler s = new LoadBalancedAvroHandler();        hosts.add(s);        Server srv = RpcTestUtils.startServer(s);        servers.add(srv);        String name = "h" + i;        p.put("hosts." + name, "127.0.0.1:" + srv.getPort());        hostList.append(name).append(" ");    }    p.put("hosts", hostList.toString().trim());    p.put("client.type", "default_loadbalance");    p.put("host-selector", "random");    p.put("backoff", "true");    hosts.get(0).setFailed();    hosts.get(2).setFailed();    RpcClient c = RpcClientFactory.getInstance(p);    Assert.assertTrue(c instanceof LoadBalancingRpcClient);        for (int i = 0; i < 50; i++) {                c.append(EventBuilder.withBody(("test" + String.valueOf(i)).getBytes()));    }    Assert.assertEquals(50, hosts.get(1).getAppendCount());    Assert.assertEquals(0, hosts.get(0).getAppendCount());    Assert.assertEquals(0, hosts.get(2).getAppendCount());    hosts.get(0).setOK();        hosts.get(1).setFailed();    try {        c.append(EventBuilder.withBody("shouldfail".getBytes()));                Assert.fail("Expected EventDeliveryException");    } catch (EventDeliveryException e) {        }        Thread.sleep(2500);    for (int i = 0; i < 50; i++) {                c.append(EventBuilder.withBody(("test" + String.valueOf(i)).getBytes()));    }    Assert.assertEquals(50, hosts.get(0).getAppendCount());    Assert.assertEquals(50, hosts.get(1).getAppendCount());    Assert.assertEquals(0, hosts.get(2).getAppendCount());}
0
public void testRoundRobinBackoffInitialFailure() throws EventDeliveryException
{    Properties p = new Properties();    List<LoadBalancedAvroHandler> hosts = new ArrayList<LoadBalancedAvroHandler>();    List<Server> servers = new ArrayList<Server>();    StringBuilder hostList = new StringBuilder("");    for (int i = 0; i < 3; i++) {        LoadBalancedAvroHandler s = new LoadBalancedAvroHandler();        hosts.add(s);        Server srv = RpcTestUtils.startServer(s);        servers.add(srv);        String name = "h" + i;        p.put("hosts." + name, "127.0.0.1:" + srv.getPort());        hostList.append(name).append(" ");    }    p.put("hosts", hostList.toString().trim());    p.put("client.type", "default_loadbalance");    p.put("host-selector", "round_robin");    p.put("backoff", "true");    RpcClient c = RpcClientFactory.getInstance(p);    Assert.assertTrue(c instanceof LoadBalancingRpcClient);    for (int i = 0; i < 3; i++) {        c.append(EventBuilder.withBody("testing".getBytes()));    }    hosts.get(1).setFailed();    for (int i = 0; i < 3; i++) {        c.append(EventBuilder.withBody("testing".getBytes()));    }    hosts.get(1).setOK();        for (int i = 0; i < 3; i++) {        c.append(EventBuilder.withBody("testing".getBytes()));    }    Assert.assertEquals(1 + 2 + 1, hosts.get(0).getAppendCount());    Assert.assertEquals(1, hosts.get(1).getAppendCount());    Assert.assertEquals(1 + 1 + 2, hosts.get(2).getAppendCount());}
0
public void testRoundRobinBackoffIncreasingBackoffs() throws Exception
{    Properties p = new Properties();    List<LoadBalancedAvroHandler> hosts = new ArrayList<LoadBalancedAvroHandler>();    List<Server> servers = new ArrayList<Server>();    StringBuilder hostList = new StringBuilder("");    for (int i = 0; i < 3; i++) {        LoadBalancedAvroHandler s = new LoadBalancedAvroHandler();        hosts.add(s);        if (i == 1) {            s.setFailed();        }        Server srv = RpcTestUtils.startServer(s);        servers.add(srv);        String name = "h" + i;        p.put("hosts." + name, "127.0.0.1:" + srv.getPort());        hostList.append(name).append(" ");    }    p.put("hosts", hostList.toString().trim());    p.put("client.type", "default_loadbalance");    p.put("host-selector", "round_robin");    p.put("backoff", "true");    RpcClient c = RpcClientFactory.getInstance(p);    Assert.assertTrue(c instanceof LoadBalancingRpcClient);    for (int i = 0; i < 3; i++) {        c.append(EventBuilder.withBody("testing".getBytes()));    }    Assert.assertEquals(0, hosts.get(1).getAppendCount());    Thread.sleep(2100);        for (int i = 0; i < 3; i++) {        c.append(EventBuilder.withBody("testing".getBytes()));    }    Assert.assertEquals(0, hosts.get(1).getAppendCount());    hosts.get(1).setOK();    Thread.sleep(2100);        for (int i = 0; i < 3; i++) {        c.append(EventBuilder.withBody("testing".getBytes()));    }    Assert.assertEquals(0, hosts.get(1).getAppendCount());        Thread.sleep(2500);    int numEvents = 60;    for (int i = 0; i < numEvents; i++) {        c.append(EventBuilder.withBody("testing".getBytes()));    }    Assert.assertEquals(2 + 2 + 1 + (numEvents / 3), hosts.get(0).getAppendCount());    Assert.assertEquals((numEvents / 3), hosts.get(1).getAppendCount());    Assert.assertEquals(1 + 1 + 2 + (numEvents / 3), hosts.get(2).getAppendCount());}
0
public void testRoundRobinBackoffFailureRecovery() throws EventDeliveryException, InterruptedException
{    Properties p = new Properties();    List<LoadBalancedAvroHandler> hosts = new ArrayList<LoadBalancedAvroHandler>();    List<Server> servers = new ArrayList<Server>();    StringBuilder hostList = new StringBuilder("");    for (int i = 0; i < 3; i++) {        LoadBalancedAvroHandler s = new LoadBalancedAvroHandler();        hosts.add(s);        if (i == 1) {            s.setFailed();        }        Server srv = RpcTestUtils.startServer(s);        servers.add(srv);        String name = "h" + i;        p.put("hosts." + name, "127.0.0.1:" + srv.getPort());        hostList.append(name).append(" ");    }    p.put("hosts", hostList.toString().trim());    p.put("client.type", "default_loadbalance");    p.put("host-selector", "round_robin");    p.put("backoff", "true");    RpcClient c = RpcClientFactory.getInstance(p);    Assert.assertTrue(c instanceof LoadBalancingRpcClient);    for (int i = 0; i < 3; i++) {        c.append(EventBuilder.withBody("recovery test".getBytes()));    }    hosts.get(1).setOK();    Thread.sleep(3000);    int numEvents = 60;    for (int i = 0; i < numEvents; i++) {        c.append(EventBuilder.withBody("testing".getBytes()));    }    Assert.assertEquals(2 + (numEvents / 3), hosts.get(0).getAppendCount());    Assert.assertEquals(0 + (numEvents / 3), hosts.get(1).getAppendCount());    Assert.assertEquals(1 + (numEvents / 3), hosts.get(2).getAppendCount());}
0
private List<Event> getBatchedEvent(int index)
{    List<Event> result = new ArrayList<Event>();    result.add(getEvent(index));    return result;}
0
private Event getEvent(int index)
{    return EventBuilder.withBody(("event: " + index).getBytes());}
0
public void testOKServerSimple() throws FlumeException, EventDeliveryException
{    RpcTestUtils.handlerSimpleAppendTest(new OKAvroHandler());}
0
public void testOKServerSimpleCompressionLevel6() throws FlumeException, EventDeliveryException
{    RpcTestUtils.handlerSimpleAppendTest(new OKAvroHandler(), true, true, 6);}
0
public void testOKServerSimpleCompressionLevel0() throws FlumeException, EventDeliveryException
{    RpcTestUtils.handlerSimpleAppendTest(new OKAvroHandler(), true, true, 0);}
0
public void testOKServerSimpleCompressionClientOnly() throws FlumeException, EventDeliveryException
{    RpcTestUtils.handlerSimpleAppendTest(new OKAvroHandler(), false, true, 6);}
0
public void testOKServerSimpleCompressionServerOnly() throws FlumeException, EventDeliveryException
{    RpcTestUtils.handlerSimpleAppendTest(new OKAvroHandler(), true, false, 6);}
0
public void testOKServerBatch() throws FlumeException, EventDeliveryException
{    RpcTestUtils.handlerBatchAppendTest(new OKAvroHandler());}
0
public void testOKServerBatchCompressionLevel0() throws FlumeException, EventDeliveryException
{    RpcTestUtils.handlerBatchAppendTest(new OKAvroHandler(), true, true, 0);}
0
public void testOKServerBatchCompressionLevel6() throws FlumeException, EventDeliveryException
{    RpcTestUtils.handlerBatchAppendTest(new OKAvroHandler(), true, true, 6);}
0
public void testOKServerBatchCompressionServerOnly() throws FlumeException, EventDeliveryException
{    RpcTestUtils.handlerBatchAppendTest(new OKAvroHandler(), true, false, 6);}
0
public void testOKServerBatchCompressionClientOnly() throws FlumeException, EventDeliveryException
{    RpcTestUtils.handlerBatchAppendTest(new OKAvroHandler(), false, true, 6);}
0
public void testUnableToConnect() throws FlumeException
{    @SuppressWarnings("unused")    NettyAvroRpcClient client = new NettyAvroRpcClient();    Properties props = new Properties();    props.setProperty(RpcClientConfigurationConstants.CONFIG_HOSTS, "localhost");    props.setProperty(RpcClientConfigurationConstants.CONFIG_HOSTS_PREFIX + "localhost", localhost + ":" + 1);    client.configure(props);}
0
public void testBatchOverrun() throws FlumeException, EventDeliveryException
{    int batchSize = 10;    int moreThanBatchSize = batchSize + 1;    NettyAvroRpcClient client = null;    Server server = RpcTestUtils.startServer(new OKAvroHandler());    Properties props = new Properties();    props.setProperty(RpcClientConfigurationConstants.CONFIG_HOSTS, "localhost");    props.setProperty(RpcClientConfigurationConstants.CONFIG_HOSTS_PREFIX + "localhost", localhost + ":" + server.getPort());    props.setProperty(RpcClientConfigurationConstants.CONFIG_BATCH_SIZE, "" + batchSize);    try {        client = new NettyAvroRpcClient();        client.configure(props);                List<Event> events = new ArrayList<Event>();        for (int i = 0; i < moreThanBatchSize; i++) {            events.add(EventBuilder.withBody("evt: " + i, Charset.forName("UTF8")));        }        client.appendBatch(events);    } finally {        RpcTestUtils.stopServer(server);        if (client != null)            client.close();    }}
0
public void testServerDisconnect() throws FlumeException, EventDeliveryException, InterruptedException
{    NettyAvroRpcClient client = null;    Server server = RpcTestUtils.startServer(new OKAvroHandler());    try {        client = RpcTestUtils.getStockLocalClient(server.getPort());        server.close();                Thread.sleep(1000L);        try {            server.join();        } catch (InterruptedException ex) {                        Thread.currentThread().interrupt();        }        try {            client.append(EventBuilder.withBody("hello", Charset.forName("UTF8")));        } finally {            Assert.assertFalse("Client should not be active", client.isActive());        }    } finally {        RpcTestUtils.stopServer(server);        if (client != null)            client.close();    }}
1
public void testClientClosedRequest() throws FlumeException, EventDeliveryException
{    NettyAvroRpcClient client = null;    Server server = RpcTestUtils.startServer(new OKAvroHandler());    try {        client = RpcTestUtils.getStockLocalClient(server.getPort());        client.close();        Assert.assertFalse("Client should not be active", client.isActive());        System.out.println("Yaya! I am not active after client close!");        client.append(EventBuilder.withBody("hello", Charset.forName("UTF8")));    } finally {        RpcTestUtils.stopServer(server);        if (client != null)            client.close();    }}
0
public void testFailedServerSimple() throws FlumeException, EventDeliveryException
{    RpcTestUtils.handlerSimpleAppendTest(new FailedAvroHandler());    }
1
public void testUnknownServerSimple() throws FlumeException, EventDeliveryException
{    RpcTestUtils.handlerSimpleAppendTest(new UnknownAvroHandler());    }
1
public void testThrowingServerSimple() throws FlumeException, EventDeliveryException
{    RpcTestUtils.handlerSimpleAppendTest(new ThrowingAvroHandler());    }
1
public void testFailedServerBatch() throws FlumeException, EventDeliveryException
{    RpcTestUtils.handlerBatchAppendTest(new FailedAvroHandler());    }
1
public void testUnknownServerBatch() throws FlumeException, EventDeliveryException
{    RpcTestUtils.handlerBatchAppendTest(new UnknownAvroHandler());    }
1
public void testThrowingServerBatch() throws FlumeException, EventDeliveryException
{    RpcTestUtils.handlerBatchAppendTest(new ThrowingAvroHandler());    }
1
public void testAppendWithMaxIOWorkers() throws FlumeException, EventDeliveryException
{    NettyAvroRpcClient client = null;    Server server = RpcTestUtils.startServer(new OKAvroHandler());    Properties props = new Properties();    props.setProperty(RpcClientConfigurationConstants.CONFIG_HOSTS, "localhost");    props.setProperty(RpcClientConfigurationConstants.CONFIG_HOSTS_PREFIX + "localhost", localhost + ":" + server.getPort());    props.setProperty(RpcClientConfigurationConstants.MAX_IO_WORKERS, Integer.toString(2));    try {        client = new NettyAvroRpcClient();        client.configure(props);        for (int i = 0; i < 5; i++) {            client.append(EventBuilder.withBody("evt:" + i, Charset.forName("UTF8")));        }    } finally {        RpcTestUtils.stopServer(server);        if (client != null) {            client.close();        }    }}
0
public void testAppendWithMaxIOWorkersSimpleCompressionLevel0() throws FlumeException, EventDeliveryException
{    NettyAvroRpcClient client = null;    Server server = RpcTestUtils.startServer(new OKAvroHandler(), 0, true);    Properties props = new Properties();    props.setProperty(RpcClientConfigurationConstants.CONFIG_HOSTS, "localhost");    props.setProperty(RpcClientConfigurationConstants.CONFIG_HOSTS_PREFIX + "localhost", localhost + ":" + server.getPort());    props.setProperty(RpcClientConfigurationConstants.MAX_IO_WORKERS, Integer.toString(2));    props.setProperty(RpcClientConfigurationConstants.CONFIG_COMPRESSION_TYPE, "deflate");    props.setProperty(RpcClientConfigurationConstants.CONFIG_COMPRESSION_LEVEL, "" + 0);    try {        client = new NettyAvroRpcClient();        client.configure(props);        for (int i = 0; i < 5; i++) {            client.append(EventBuilder.withBody("evt:" + i, Charset.forName("UTF8")));        }    } finally {        RpcTestUtils.stopServer(server);        if (client != null) {            client.close();        }    }}
0
public void testTwoParamSimpleAppend() throws FlumeException, EventDeliveryException
{    RpcClient client = null;    Server server = RpcTestUtils.startServer(new OKAvroHandler());    try {        client = RpcClientFactory.getDefaultInstance(localhost, server.getPort());        client.append(EventBuilder.withBody("wheee!!!", Charset.forName("UTF8")));    } finally {        RpcTestUtils.stopServer(server);        if (client != null)            client.close();    }}
0
public void testTwoParamDeprecatedAppend() throws FlumeException, EventDeliveryException
{    RpcClient client = null;    Server server = RpcTestUtils.startServer(new OKAvroHandler());    try {        client = RpcClientFactory.getInstance(localhost, server.getPort());        client.append(EventBuilder.withBody("wheee!!!", Charset.forName("UTF8")));    } finally {        RpcTestUtils.stopServer(server);        if (client != null)            client.close();    }}
0
public void testThreeParamDeprecatedAppend() throws FlumeException, EventDeliveryException
{    RpcClient client = null;    Server server = RpcTestUtils.startServer(new OKAvroHandler());    try {        client = RpcClientFactory.getInstance(localhost, server.getPort(), 3);        Assert.assertEquals("Batch size was specified", 3, client.getBatchSize());        client.append(EventBuilder.withBody("wheee!!!", Charset.forName("UTF8")));    } finally {        RpcTestUtils.stopServer(server);        if (client != null)            client.close();    }}
0
public void testThreeParamBatchAppend() throws FlumeException, EventDeliveryException
{    int batchSize = 7;    RpcClient client = null;    Server server = RpcTestUtils.startServer(new OKAvroHandler());    try {        client = RpcClientFactory.getDefaultInstance(localhost, server.getPort(), batchSize);        List<Event> events = new ArrayList<Event>();        for (int i = 0; i < batchSize; i++) {            events.add(EventBuilder.withBody("evt: " + i, Charset.forName("UTF8")));        }        client.appendBatch(events);    } finally {        RpcTestUtils.stopServer(server);        if (client != null)            client.close();    }}
0
public void testPropertiesBatchAppend() throws FlumeException, EventDeliveryException
{    int batchSize = 7;    RpcClient client = null;    Server server = RpcTestUtils.startServer(new OKAvroHandler());    try {        Properties p = new Properties();        p.put("hosts", "host1");        p.put("hosts.host1", localhost + ":" + String.valueOf(server.getPort()));        p.put("batch-size", String.valueOf(batchSize));        client = RpcClientFactory.getInstance(p);        List<Event> events = new ArrayList<Event>();        for (int i = 0; i < batchSize; i++) {            events.add(EventBuilder.withBody("evt: " + i, Charset.forName("UTF8")));        }        client.appendBatch(events);    } finally {        RpcTestUtils.stopServer(server);        if (client != null)            client.close();    }}
0
public void testTwoParamBatchAppendOverflow() throws FlumeException, EventDeliveryException
{    RpcClient client = null;    Server server = RpcTestUtils.startServer(new OKAvroHandler());    try {        client = RpcClientFactory.getDefaultInstance(localhost, server.getPort());        int batchSize = client.getBatchSize();        int moreThanBatch = batchSize + 1;        List<Event> events = new ArrayList<Event>();        for (int i = 0; i < moreThanBatch; i++) {            events.add(EventBuilder.withBody("evt: " + i, Charset.forName("UTF8")));        }        client.appendBatch(events);    } finally {        RpcTestUtils.stopServer(server);        if (client != null)            client.close();    }}
0
public void setUp() throws Exception
{    props.setProperty("hosts", "h1");    try (ServerSocket socket = new ServerSocket(0)) {        port = socket.getLocalPort();    }    props.setProperty(RpcClientConfigurationConstants.CONFIG_CLIENT_TYPE, "thrift");    props.setProperty("hosts.h1", "0.0.0.0:" + String.valueOf(port));    props.setProperty(RpcClientConfigurationConstants.CONFIG_BATCH_SIZE, "10");    props.setProperty(RpcClientConfigurationConstants.CONFIG_REQUEST_TIMEOUT, "2000");    props.setProperty(ThriftRpcClient.CONFIG_PROTOCOL, ThriftRpcClient.COMPACT_PROTOCOL);}
0
public void tearDown() throws Exception
{    src.stop();}
0
private static void insertEvents(RpcClient client, int count) throws Exception
{    for (int i = 0; i < count; i++) {        Map<String, String> header = new HashMap<String, String>();        header.put(SEQ, String.valueOf(i));        client.append(EventBuilder.withBody(String.valueOf(i).getBytes(), header));    }}
0
private static void insertAsBatch(RpcClient client, int start, int limit) throws Exception
{    List<Event> events = new ArrayList<Event>();    for (int i = start; i <= limit; i++) {        Map<String, String> header = new HashMap<String, String>();        header.put(SEQ, String.valueOf(i));        events.add(EventBuilder.withBody(String.valueOf(i).getBytes(), header));    }    client.appendBatch(events);}
0
public void testOK() throws Exception
{    src = new ThriftTestingSource(ThriftTestingSource.HandlerType.OK.name(), port, ThriftRpcClient.COMPACT_PROTOCOL);    client = (ThriftRpcClient) RpcClientFactory.getInstance(props);        insertEvents(client, 10);        insertAsBatch(client, 10, 25);        insertAsBatch(client, 26, 37);    int count = 0;    Assert.assertEquals(38, src.flumeEvents.size());    for (Event e : src.flumeEvents) {        Assert.assertEquals(new String(e.getBody()), String.valueOf(count++));    }    Assert.assertEquals(10, src.individualCount);    Assert.assertEquals(4, src.batchCount);    Assert.assertEquals(2, src.incompleteBatches);}
0
public void testSlow() throws Exception
{    src = new ThriftTestingSource(ThriftTestingSource.HandlerType.SLOW.name(), port, ThriftRpcClient.COMPACT_PROTOCOL);    client = (ThriftRpcClient) RpcClientFactory.getInstance(props);        insertEvents(client, 2);        insertAsBatch(client, 2, 25);        insertAsBatch(client, 26, 37);    int count = 0;    Assert.assertEquals(38, src.flumeEvents.size());    for (Event e : src.flumeEvents) {        Assert.assertEquals(new String(e.getBody()), String.valueOf(count++));    }    Assert.assertEquals(2, src.individualCount);    Assert.assertEquals(5, src.batchCount);    Assert.assertEquals(2, src.incompleteBatches);}
0
public void testFail() throws Exception
{    src = new ThriftTestingSource(ThriftTestingSource.HandlerType.FAIL.name(), port, ThriftRpcClient.COMPACT_PROTOCOL);    client = (ThriftRpcClient) RpcClientFactory.getInstance(props);        insertEvents(client, 2);    Assert.fail("Expected EventDeliveryException to be thrown.");}
0
public void testError() throws Throwable
{    try {        src = new ThriftTestingSource(ThriftTestingSource.HandlerType.ERROR.name(), port, ThriftRpcClient.COMPACT_PROTOCOL);        client = (ThriftRpcClient) RpcClientFactory.getThriftInstance("0.0.0.0", port);                insertEvents(client, 2);    } catch (EventDeliveryException ex) {        Assert.assertEquals("Failed to send event. ", ex.getMessage());    }}
0
public void testTimeout() throws Throwable
{    try {        src = new ThriftTestingSource(ThriftTestingSource.HandlerType.TIMEOUT.name(), port, ThriftRpcClient.COMPACT_PROTOCOL);        client = (ThriftRpcClient) RpcClientFactory.getThriftInstance(props);                insertEvents(client, 2);    } catch (EventDeliveryException ex) {        throw ex.getCause();    }}
0
public void testMultipleThreads() throws Throwable
{    src = new ThriftTestingSource(ThriftTestingSource.HandlerType.OK.name(), port, ThriftRpcClient.COMPACT_PROTOCOL);    client = (ThriftRpcClient) RpcClientFactory.getThriftInstance("0.0.0.0", port, 10);    int threadCount = 100;    ExecutorService submissionSvc = Executors.newFixedThreadPool(threadCount);    ArrayList<Future<?>> futures = new ArrayList<Future<?>>(threadCount);    for (int i = 0; i < threadCount; i++) {        futures.add(submissionSvc.submit(new Runnable() {            @Override            public void run() {                try {                    insertAsBatch(client, 0, 9);                } catch (Exception e) {                                        e.printStackTrace();                                }            }        }));    }    for (int i = 0; i < threadCount; i++) {        futures.get(i).get();    }    ArrayList<String> events = new ArrayList<String>();    for (Event e : src.flumeEvents) {        events.add(new String(e.getBody()));    }    int count = 0;    Collections.sort(events);    for (int i = 0; i < events.size(); ) {        for (int j = 0; j < threadCount; j++) {            Assert.assertEquals(String.valueOf(count), events.get(i++));        }        count++;    }}
0
public void run()
{    try {        insertAsBatch(client, 0, 9);    } catch (Exception e) {                e.printStackTrace();        }}
0
public void setDelay(AtomicLong delay)
{    this.delay = delay;}
0
public Status append(ThriftFlumeEvent event) throws TException
{    flumeEvents.add(EventBuilder.withBody(event.getBody(), event.getHeaders()));    individualCount++;    return Status.OK;}
0
public Status appendBatch(List<ThriftFlumeEvent> events) throws TException
{    batchCount++;    if (events.size() < 10) {        incompleteBatches++;    }    for (ThriftFlumeEvent event : events) {        flumeEvents.add(EventBuilder.withBody(event.getBody(), event.getHeaders()));    }    return Status.OK;}
0
public Status append(ThriftFlumeEvent event) throws TException
{    return Status.FAILED;}
0
public Status appendBatch(List<ThriftFlumeEvent> events) throws TException
{    return Status.FAILED;}
0
public Status append(ThriftFlumeEvent event) throws TException
{    throw new FlumeException("Forced Error");}
0
public Status appendBatch(List<ThriftFlumeEvent> events) throws TException
{    throw new FlumeException("Forced Error");}
0
public Status append(ThriftFlumeEvent event) throws TException
{    try {        TimeUnit.MILLISECONDS.sleep(1550);    } catch (InterruptedException e) {        throw new FlumeException("Error", e);    }    return super.append(event);}
0
public Status appendBatch(List<ThriftFlumeEvent> events) throws TException
{    try {        TimeUnit.MILLISECONDS.sleep(1550);    } catch (InterruptedException e) {        throw new FlumeException("Error", e);    }    return super.appendBatch(events);}
0
public Status append(ThriftFlumeEvent event) throws TException
{    try {        TimeUnit.MILLISECONDS.sleep(5000);    } catch (InterruptedException e) {        throw new FlumeException("Error", e);    }    return super.append(event);}
0
public Status appendBatch(List<ThriftFlumeEvent> events) throws TException
{    try {        TimeUnit.MILLISECONDS.sleep(5000);    } catch (InterruptedException e) {        throw new FlumeException("Error", e);    }    return super.appendBatch(events);}
0
public Status append(ThriftFlumeEvent event) throws TException
{    try {        if (delay != null) {            TimeUnit.MILLISECONDS.sleep(delay.get());        }    } catch (InterruptedException e) {        throw new FlumeException("Error", e);    }    return super.append(event);}
0
public Status appendBatch(List<ThriftFlumeEvent> events) throws TException
{    try {        if (delay != null) {            TimeUnit.MILLISECONDS.sleep(delay.get());        }    } catch (InterruptedException e) {        throw new FlumeException("Error", e);    }    return super.appendBatch(events);}
0
private ThriftSourceProtocol.Iface getHandler(String handlerName)
{    ThriftSourceProtocol.Iface handler = null;    if (handlerName.equals(HandlerType.OK.name())) {        handler = new ThriftOKHandler();    } else if (handlerName.equals(HandlerType.FAIL.name())) {        handler = new ThriftFailHandler();    } else if (handlerName.equals(HandlerType.ERROR.name())) {        handler = new ThriftErrorHandler();    } else if (handlerName.equals(HandlerType.SLOW.name())) {        handler = new ThriftSlowHandler();    } else if (handlerName.equals(HandlerType.TIMEOUT.name())) {        handler = new ThriftTimeoutHandler();    } else if (handlerName.equals(HandlerType.ALTERNATE.name())) {        handler = new ThriftAlternateHandler();    }    return handler;}
0
public void run()
{    server.serve();}
0
public void run()
{    server.serve();}
0
public void stop()
{    server.stop();}
0
public void testBody()
{    Event e1 = EventBuilder.withBody("e1".getBytes());    Assert.assertNotNull(e1);    Assert.assertArrayEquals("body is correct", "e1".getBytes(), e1.getBody());    Event e2 = EventBuilder.withBody(Long.valueOf(2).toString().getBytes());    Assert.assertNotNull(e2);    Assert.assertArrayEquals("body is correct", Long.valueOf(2L).toString().getBytes(), e2.getBody());}
0
public void testHeaders()
{    Map<String, String> headers = new HashMap<String, String>();    headers.put("one", "1");    headers.put("two", "2");    Event e1 = EventBuilder.withBody("e1".getBytes(), headers);    Assert.assertNotNull(e1);    Assert.assertArrayEquals("e1 has the proper body", "e1".getBytes(), e1.getBody());    Assert.assertEquals("e1 has the proper headers", 2, e1.getHeaders().size());    Assert.assertEquals("e1 has a one key", "1", e1.getHeaders().get("one"));}
0
public void testJsonEventUnsupportedEncoding()
{    JSONEvent jsonEvent = new JSONEvent();    jsonEvent.setCharset("dummy");    jsonEvent.setBody("This is json event".getBytes());    jsonEvent.getBody();}
0
public static Collection<?> data()
{    return Arrays.asList(new Object[][] {     { null, null, null }, { "sysprop", null, "sysprop" }, { "sysprop,sysprop", null, "sysprop sysprop" }, { null, "envvar", "envvar" }, { null, "envvar,envvar", "envvar envvar" }, { "sysprop", "envvar", "sysprop" }, { "sysprop,sysprop", "envvar,envvar", "sysprop sysprop" } });}
0
public static Collection<?> data()
{    return Arrays.asList(new Object[][] {     { null, null, null }, { "sysprop", null, "sysprop" }, { null, "envvar", "envvar" }, { "sysprop", "envvar", "sysprop" } });}
0
public void setUp()
{    setSysProp(getSysPropName(), sysPropValue);    setEnvVar(getEnvVarName(), envVarValue);}
0
public void tearDown()
{    setSysProp(getSysPropName(), null);    setEnvVar(getEnvVarName(), null);}
0
private static void setSysProp(String name, String value)
{    if (value != null) {        System.setProperty(name, value);    } else {        System.clearProperty(name);    }}
0
private static void setEnvVar(String name, String value)
{    try {        injectEnvironmentVariable(name, value);    } catch (ReflectiveOperationException e) {        throw new AssertionError("Test setup  failed.", e);    }}
0
private static void injectEnvironmentVariable(String key, String value) throws ReflectiveOperationException
{    Class<?> processEnvironment = Class.forName("java.lang.ProcessEnvironment");    Field unmodifiableMapField = getAccessibleField(processEnvironment, "theUnmodifiableEnvironment");    Object unmodifiableMap = unmodifiableMapField.get(null);    injectIntoUnmodifiableMap(key, value, unmodifiableMap);    Field mapField = getAccessibleField(processEnvironment, "theEnvironment");    Map<String, String> map = (Map<String, String>) mapField.get(null);    if (value != null) {        map.put(key, value);    } else {        map.remove(key);    }}
0
private static Field getAccessibleField(Class<?> clazz, String fieldName) throws NoSuchFieldException
{    Field field = clazz.getDeclaredField(fieldName);    field.setAccessible(true);    return field;}
0
private static void injectIntoUnmodifiableMap(String key, String value, Object map) throws ReflectiveOperationException
{    Class unmodifiableMap = Class.forName("java.util.Collections$UnmodifiableMap");    Field field = getAccessibleField(unmodifiableMap, "m");    Object obj = field.get(map);    if (value != null) {        ((Map<String, String>) obj).put(key, value);    } else {        ((Map<String, String>) obj).remove(key);    }}
0
protected String getSysPropName()
{    return "flume.ssl.exclude.cipherSuites";}
0
protected String getEnvVarName()
{    return "FLUME_SSL_EXCLUDE_CIPHERSUITES";}
0
public void testIncludeProtocols()
{    SSLUtil.initGlobalSSLParameters();    String actualValue = SSLUtil.getGlobalExcludeCipherSuites();    Assert.assertEquals(expectedValue, actualValue);}
0
protected String getSysPropName()
{    return "flume.ssl.exclude.protocols";}
0
protected String getEnvVarName()
{    return "FLUME_SSL_EXCLUDE_PROTOCOLS";}
0
public void testExcludeProtocols()
{    SSLUtil.initGlobalSSLParameters();    String actualValue = SSLUtil.getGlobalExcludeProtocols();    Assert.assertEquals(expectedValue, actualValue);}
0
protected String getSysPropName()
{    return "flume.ssl.include.cipherSuites";}
0
protected String getEnvVarName()
{    return "FLUME_SSL_INCLUDE_CIPHERSUITES";}
0
public void testIncludeProtocols()
{    SSLUtil.initGlobalSSLParameters();    String actualValue = SSLUtil.getGlobalIncludeCipherSuites();    Assert.assertEquals(expectedValue, actualValue);}
0
protected String getSysPropName()
{    return "flume.ssl.include.protocols";}
0
protected String getEnvVarName()
{    return "FLUME_SSL_INCLUDE_PROTOCOLS";}
0
public void testIncludeProtocols()
{    SSLUtil.initGlobalSSLParameters();    String actualValue = SSLUtil.getGlobalIncludeProtocols();    Assert.assertEquals(expectedValue, actualValue);}
0
protected String getSysPropName()
{    return "javax.net.ssl.keyStorePassword";}
0
protected String getEnvVarName()
{    return "FLUME_SSL_KEYSTORE_PASSWORD";}
0
public void testKeystorePassword()
{    SSLUtil.initGlobalSSLParameters();    String keystorePassword = SSLUtil.getGlobalKeystorePassword();    Assert.assertEquals(expectedValue, keystorePassword);}
0
protected String getSysPropName()
{    return "javax.net.ssl.keyStore";}
0
protected String getEnvVarName()
{    return "FLUME_SSL_KEYSTORE_PATH";}
0
public void testKeystorePath()
{    SSLUtil.initGlobalSSLParameters();    String keystorePath = SSLUtil.getGlobalKeystorePath();    Assert.assertEquals(expectedValue, keystorePath);}
0
protected String getSysPropName()
{    return "javax.net.ssl.keyStoreType";}
0
protected String getEnvVarName()
{    return "FLUME_SSL_KEYSTORE_TYPE";}
0
public void testKeystoreType()
{    SSLUtil.initGlobalSSLParameters();    String keystoreType = SSLUtil.getGlobalKeystoreType(null);    Assert.assertEquals(expectedValue, keystoreType);}
0
public static Collection<?> data()
{    return Arrays.asList(new Object[][] {     { null, null, "default" }, { "sysprop", null, "sysprop" }, { null, "envvar", "envvar" }, { "sysprop", "envvar", "sysprop" } });}
0
protected String getSysPropName()
{    return "javax.net.ssl.keyStoreType";}
0
protected String getEnvVarName()
{    return "FLUME_SSL_KEYSTORE_TYPE";}
0
public void testKeystoreType()
{    SSLUtil.initGlobalSSLParameters();    String keystoreType = SSLUtil.getGlobalKeystoreType("default");    Assert.assertEquals(expectedValue, keystoreType);}
0
protected String getSysPropName()
{    return "javax.net.ssl.trustStorePassword";}
0
protected String getEnvVarName()
{    return "FLUME_SSL_TRUSTSTORE_PASSWORD";}
0
public void testTruststorePassword()
{    SSLUtil.initGlobalSSLParameters();    String truststorePassword = SSLUtil.getGlobalTruststorePassword();    Assert.assertEquals(expectedValue, truststorePassword);}
0
protected String getSysPropName()
{    return "javax.net.ssl.trustStore";}
0
protected String getEnvVarName()
{    return "FLUME_SSL_TRUSTSTORE_PATH";}
0
public void testTruststorePath()
{    SSLUtil.initGlobalSSLParameters();    String truststorePath = SSLUtil.getGlobalTruststorePath();    Assert.assertEquals(expectedValue, truststorePath);}
0
protected String getSysPropName()
{    return "javax.net.ssl.trustStoreType";}
0
protected String getEnvVarName()
{    return "FLUME_SSL_TRUSTSTORE_TYPE";}
0
public void testTruststoreType()
{    SSLUtil.initGlobalSSLParameters();    String truststoreType = SSLUtil.getGlobalTruststoreType(null);    Assert.assertEquals(expectedValue, truststoreType);}
0
public static Collection<?> data()
{    return Arrays.asList(new Object[][] {     { null, null, "default" }, { "sysprop", null, "sysprop" }, { null, "envvar", "envvar" }, { "sysprop", "envvar", "sysprop" } });}
0
protected String getSysPropName()
{    return "javax.net.ssl.trustStoreType";}
0
protected String getEnvVarName()
{    return "FLUME_SSL_TRUSTSTORE_TYPE";}
0
public void testTruststoreType()
{    SSLUtil.initGlobalSSLParameters();    String truststoreType = SSLUtil.getGlobalTruststoreType("default");    Assert.assertEquals(expectedValue, truststoreType);}
0
protected List<String> allowedFormats()
{    return Lists.newArrayList("avro", "parquet");}
0
public void configure(Context context)
{    this.context = context;    String principal = context.getString(AUTH_PRINCIPAL);    String keytab = context.getString(AUTH_KEYTAB);    String effectiveUser = context.getString(AUTH_PROXY_USER);    this.privilegedExecutor = FlumeAuthenticationUtil.getAuthenticator(principal, keytab).proxyAs(effectiveUser);        String datasetURI = context.getString(CONFIG_KITE_DATASET_URI);    if (datasetURI != null) {        this.datasetUri = URI.create(datasetURI);        this.datasetName = uriToName(datasetUri);    } else {        String repositoryURI = context.getString(CONFIG_KITE_REPO_URI);        Preconditions.checkNotNull(repositoryURI, "No dataset configured. Setting " + CONFIG_KITE_DATASET_URI + " is required.");        this.datasetName = context.getString(CONFIG_KITE_DATASET_NAME);        Preconditions.checkNotNull(datasetName, "No dataset configured. Setting " + CONFIG_KITE_DATASET_URI + " is required.");        String namespace = context.getString(CONFIG_KITE_DATASET_NAMESPACE, DEFAULT_NAMESPACE);        this.datasetUri = new URIBuilder(repositoryURI, namespace, datasetName).build();    }    this.setName(datasetUri.toString());    if (context.getBoolean(CONFIG_SYNCABLE_SYNC_ON_BATCH, DEFAULT_SYNCABLE_SYNC_ON_BATCH)) {        Preconditions.checkArgument(context.getBoolean(CONFIG_FLUSHABLE_COMMIT_ON_BATCH, DEFAULT_FLUSHABLE_COMMIT_ON_BATCH), "Configuration error: " + CONFIG_FLUSHABLE_COMMIT_ON_BATCH + " must be set to true when " + CONFIG_SYNCABLE_SYNC_ON_BATCH + " is set to true.");    }        this.failurePolicy = FAILURE_POLICY_FACTORY.newPolicy(context);        this.batchSize = context.getLong(CONFIG_KITE_BATCH_SIZE, DEFAULT_BATCH_SIZE);    this.rollIntervalSeconds = context.getInteger(CONFIG_KITE_ROLL_INTERVAL, DEFAULT_ROLL_INTERVAL);    this.counter = new SinkCounter(datasetName);}
0
public synchronized void start()
{    this.lastRolledMillis = System.currentTimeMillis();    counter.start();            super.start();}
1
 void roll()
{    this.lastRolledMillis = 0L;}
0
 DatasetWriter<GenericRecord> getWriter()
{    return writer;}
0
 void setWriter(DatasetWriter<GenericRecord> writer)
{    this.writer = writer;}
0
 void setParser(EntityParser<GenericRecord> parser)
{    this.parser = parser;}
0
 void setFailurePolicy(FailurePolicy failurePolicy)
{    this.failurePolicy = failurePolicy;}
0
public synchronized void stop()
{    counter.stop();    try {                        closeWriter();        commitTransaction();    } catch (EventDeliveryException ex) {        rollbackTransaction();                            }            super.stop();}
1
public Status process() throws EventDeliveryException
{    long processedEvents = 0;    try {        if (shouldRoll()) {            closeWriter();            commitTransaction();            createWriter();        }                Preconditions.checkNotNull(writer, "Can't process events with a null writer. This is likely a bug.");        Channel channel = getChannel();                enterTransaction(channel);        for (; processedEvents < batchSize; processedEvents += 1) {            Event event = channel.take();            if (event == null) {                                break;            }            write(event);        }                if (commitOnBatch) {                        if (syncOnBatch && writer instanceof Syncable) {                ((Syncable) writer).sync();            } else if (writer instanceof Flushable) {                ((Flushable) writer).flush();            }            boolean committed = commitTransaction();            Preconditions.checkState(committed, "Tried to commit a batch when there was no transaction");            committedBatch |= committed;        }    } catch (Throwable th) {                        rollbackTransaction();        if (commitOnBatch && committedBatch) {            try {                closeWriter();            } catch (EventDeliveryException ex) {                                            }        } else {            this.writer = null;        }                Throwables.propagateIfInstanceOf(th, Error.class);        Throwables.propagateIfInstanceOf(th, EventDeliveryException.class);        throw new EventDeliveryException(th);    }    if (processedEvents == 0) {        counter.incrementBatchEmptyCount();        return Status.BACKOFF;    } else if (processedEvents < batchSize) {        counter.incrementBatchUnderflowCount();    } else {        counter.incrementBatchCompleteCount();    }    counter.addToEventDrainSuccessCount(processedEvents);    return Status.READY;}
1
 void write(Event event) throws EventDeliveryException
{    try {        this.entity = parser.parse(event, reuseEntity ? entity : null);        this.bytesParsed += event.getBody().length;                                        writer.write(entity);    } catch (NonRecoverableEventException ex) {        failurePolicy.handle(event, ex);    } catch (DataFileWriter.AppendWriteException ex) {        failurePolicy.handle(event, ex);    } catch (RuntimeException ex) {        Throwables.propagateIfInstanceOf(ex, EventDeliveryException.class);        throw new EventDeliveryException(ex);    }}
0
 void createWriter() throws EventDeliveryException
{        committedBatch = false;    try {        View<GenericRecord> view;        view = privilegedExecutor.execute(new PrivilegedAction<Dataset<GenericRecord>>() {            @Override            public Dataset<GenericRecord> run() {                return Datasets.load(datasetUri);            }        });        DatasetDescriptor descriptor = view.getDataset().getDescriptor();        Format format = descriptor.getFormat();        Preconditions.checkArgument(allowedFormats().contains(format.getName()), "Unsupported format: " + format.getName());        Schema newSchema = descriptor.getSchema();        if (datasetSchema == null || !newSchema.equals(datasetSchema)) {            this.datasetSchema = descriptor.getSchema();                        parser = ENTITY_PARSER_FACTORY.newParser(datasetSchema, context);        }        this.reuseEntity = !(Formats.PARQUET.equals(format));                        this.commitOnBatch = context.getBoolean(CONFIG_FLUSHABLE_COMMIT_ON_BATCH, DEFAULT_FLUSHABLE_COMMIT_ON_BATCH) && (Formats.AVRO.equals(format));                        this.syncOnBatch = context.getBoolean(CONFIG_SYNCABLE_SYNC_ON_BATCH, DEFAULT_SYNCABLE_SYNC_ON_BATCH) && (Formats.AVRO.equals(format));        this.datasetName = view.getDataset().getName();        this.writer = view.newWriter();                this.lastRolledMillis = System.currentTimeMillis();        this.bytesParsed = 0L;    } catch (DatasetNotFoundException ex) {        throw new EventDeliveryException("Dataset " + datasetUri + " not found." + " The dataset must be created before Flume can write to it.", ex);    } catch (RuntimeException ex) {        throw new EventDeliveryException("Error trying to open a new" + " writer for dataset " + datasetUri, ex);    }}
0
public Dataset<GenericRecord> run()
{    return Datasets.load(datasetUri);}
0
private boolean shouldRoll()
{    long currentTimeMillis = System.currentTimeMillis();    long elapsedTimeSeconds = TimeUnit.MILLISECONDS.toSeconds(currentTimeMillis - lastRolledMillis);        return elapsedTimeSeconds >= rollIntervalSeconds || writer == null;}
1
 void closeWriter() throws EventDeliveryException
{    if (writer != null) {        try {            writer.close();            long elapsedTimeSeconds = TimeUnit.MILLISECONDS.toSeconds(System.currentTimeMillis() - lastRolledMillis);                    } catch (DatasetIOException ex) {            throw new EventDeliveryException("Check HDFS permissions/health. IO" + " error trying to close the  writer for dataset " + datasetUri, ex);        } catch (RuntimeException ex) {            throw new EventDeliveryException("Error trying to close the  writer for" + " dataset " + datasetUri, ex);        } finally {                                                                        this.writer = null;            failurePolicy.close();        }    }}
1
private void enterTransaction(Channel channel) throws EventDeliveryException
{        if (transaction == null) {        this.transaction = channel.getTransaction();        transaction.begin();        failurePolicy = FAILURE_POLICY_FACTORY.newPolicy(context);    }}
0
 boolean commitTransaction() throws EventDeliveryException
{    if (transaction != null) {        failurePolicy.sync();        transaction.commit();        transaction.close();        this.transaction = null;        return true;    } else {        return false;    }}
0
private void rollbackTransaction()
{    if (transaction != null) {        try {                                    transaction.rollback();        } catch (RuntimeException ex) {                                } finally {            transaction.close();            this.transaction = null;        }    }}
1
private static String uriToName(URI uri)
{    return Registration.lookupDatasetUri(URI.create(uri.getRawSchemeSpecificPart())).second().get("dataset");}
0
public long getBatchSize()
{    return batchSize;}
0
public Schema load(String literal)
{    Preconditions.checkNotNull(literal, "Schema literal cannot be null without a Schema URL");    return new Schema.Parser().parse(literal);}
0
public Schema load(String url) throws IOException
{    Schema.Parser parser = new Schema.Parser();    InputStream is = null;    try {        FileSystem fs = FileSystem.get(URI.create(url), conf);        if (url.toLowerCase(Locale.ENGLISH).startsWith("hdfs:/")) {            is = fs.open(new Path(url));        } else {            is = new URL(url).openStream();        }        return parser.parse(is);    } finally {        if (is != null) {            is.close();        }    }}
0
public DatumReader<GenericRecord> load(Schema schema)
{        return new GenericDatumReader<GenericRecord>(schema, datasetSchema);}
0
public GenericRecord parse(Event event, GenericRecord reuse) throws EventDeliveryException, NonRecoverableEventException
{    decoder = DecoderFactory.get().binaryDecoder(event.getBody(), decoder);    try {        DatumReader<GenericRecord> reader = readers.getUnchecked(schema(event));        return reader.read(reuse, decoder);    } catch (IOException ex) {        throw new NonRecoverableEventException("Cannot deserialize event", ex);    } catch (RuntimeException ex) {        throw new NonRecoverableEventException("Cannot deserialize event", ex);    }}
0
private static Schema schema(Event event) throws EventDeliveryException, NonRecoverableEventException
{    Map<String, String> headers = event.getHeaders();    String schemaURL = headers.get(AVRO_SCHEMA_URL_HEADER);    try {        if (schemaURL != null) {            return schemasFromURL.get(schemaURL);        } else {            String schemaLiteral = headers.get(AVRO_SCHEMA_LITERAL_HEADER);            if (schemaLiteral == null) {                throw new NonRecoverableEventException("No schema in event headers." + " Headers must include either " + AVRO_SCHEMA_URL_HEADER + " or " + AVRO_SCHEMA_LITERAL_HEADER);            }            return schemasFromLiteral.get(schemaLiteral);        }    } catch (ExecutionException ex) {        throw new EventDeliveryException("Cannot get schema", ex.getCause());    } catch (UncheckedExecutionException ex) {        throw new NonRecoverableEventException("Cannot parse schema", ex.getCause());    }}
0
public EntityParser<GenericRecord> build(Schema datasetSchema, Context config)
{    return new AvroParser(datasetSchema);}
0
public EntityParser<GenericRecord> newParser(Schema datasetSchema, Context config)
{    EntityParser<GenericRecord> parser;    String parserType = config.getString(CONFIG_ENTITY_PARSER, DEFAULT_ENTITY_PARSER);    if (parserType.equals(AVRO_ENTITY_PARSER)) {        parser = new AvroParser.Builder().build(datasetSchema, config);    } else {        Class<? extends EntityParser.Builder> builderClass;        Class c;        try {            c = Class.forName(parserType);        } catch (ClassNotFoundException ex) {            throw new IllegalArgumentException("EntityParser.Builder class " + parserType + " not found. Must set " + CONFIG_ENTITY_PARSER + " to a class that implements EntityParser.Builder or to a builtin" + " parser: " + Arrays.toString(AVAILABLE_PARSERS), ex);        }        if (c != null && EntityParser.Builder.class.isAssignableFrom(c)) {            builderClass = c;        } else {            throw new IllegalArgumentException("Class " + parserType + " does not" + " implement EntityParser.Builder. Must set " + CONFIG_ENTITY_PARSER + " to a class that extends" + " EntityParser.Builder or to a builtin parser: " + Arrays.toString(AVAILABLE_PARSERS));        }        EntityParser.Builder<GenericRecord> builder;        try {            builder = builderClass.newInstance();        } catch (InstantiationException ex) {            throw new IllegalArgumentException("Can't instantiate class " + parserType + ". Must set " + CONFIG_ENTITY_PARSER + " to a class" + " that extends EntityParser.Builder or to a builtin parser: " + Arrays.toString(AVAILABLE_PARSERS), ex);        } catch (IllegalAccessException ex) {            throw new IllegalArgumentException("Can't instantiate class " + parserType + ". Must set " + CONFIG_ENTITY_PARSER + " to a class" + " that extends EntityParser.Builder or to a builtin parser: " + Arrays.toString(AVAILABLE_PARSERS), ex);        }        parser = builder.build(datasetSchema, config);    }    return parser;}
0
public FailurePolicy newPolicy(Context config)
{    FailurePolicy policy;    String policyType = config.getString(CONFIG_FAILURE_POLICY, DEFAULT_FAILURE_POLICY);    if (policyType.equals(RETRY_FAILURE_POLICY)) {        policy = new RetryPolicy.Builder().build(config);    } else if (policyType.equals(SAVE_FAILURE_POLICY)) {        policy = new SavePolicy.Builder().build(config);    } else {        Class<? extends FailurePolicy.Builder> builderClass;        Class c;        try {            c = Class.forName(policyType);        } catch (ClassNotFoundException ex) {            throw new IllegalArgumentException("FailurePolicy.Builder class " + policyType + " not found. Must set " + CONFIG_FAILURE_POLICY + " to a class that implements FailurePolicy.Builder or to a builtin" + " policy: " + Arrays.toString(AVAILABLE_POLICIES), ex);        }        if (c != null && FailurePolicy.Builder.class.isAssignableFrom(c)) {            builderClass = c;        } else {            throw new IllegalArgumentException("Class " + policyType + " does not" + " implement FailurePolicy.Builder. Must set " + CONFIG_FAILURE_POLICY + " to a class that extends" + " FailurePolicy.Builder or to a builtin policy: " + Arrays.toString(AVAILABLE_POLICIES));        }        FailurePolicy.Builder builder;        try {            builder = builderClass.newInstance();        } catch (InstantiationException ex) {            throw new IllegalArgumentException("Can't instantiate class " + policyType + ". Must set " + CONFIG_FAILURE_POLICY + " to a class" + " that extends FailurePolicy.Builder or to a builtin policy: " + Arrays.toString(AVAILABLE_POLICIES), ex);        } catch (IllegalAccessException ex) {            throw new IllegalArgumentException("Can't instantiate class " + policyType + ". Must set " + CONFIG_FAILURE_POLICY + " to a class" + " that extends FailurePolicy.Builder or to a builtin policy: " + Arrays.toString(AVAILABLE_POLICIES), ex);        }        policy = builder.build(config);    }    return policy;}
0
public void handle(Event event, Throwable cause) throws EventDeliveryException
{            throw new EventDeliveryException(cause);}
1
public void sync() throws EventDeliveryException
{}
0
public void close() throws EventDeliveryException
{}
0
public FailurePolicy build(Context config)
{    return new RetryPolicy();}
0
public void handle(Event event, Throwable cause) throws EventDeliveryException
{    try {        if (writer == null) {            writer = dataset.newWriter();        }        final AvroFlumeEvent avroEvent = new AvroFlumeEvent();        avroEvent.setBody(ByteBuffer.wrap(event.getBody()));        avroEvent.setHeaders(toCharSeqMap(event.getHeaders()));        writer.write(avroEvent);        nEventsHandled++;    } catch (RuntimeException ex) {        throw new EventDeliveryException(ex);    }}
0
public void sync() throws EventDeliveryException
{    if (nEventsHandled > 0) {        if (Formats.PARQUET.equals(dataset.getDataset().getDescriptor().getFormat())) {                                    close();        } else {            if (writer instanceof Syncable) {                ((Syncable) writer).sync();            }        }    }}
0
public void close() throws EventDeliveryException
{    if (nEventsHandled > 0) {        try {            writer.close();        } catch (RuntimeException ex) {            throw new EventDeliveryException(ex);        } finally {            writer = null;            nEventsHandled = 0;        }    }}
0
private static Map<CharSequence, CharSequence> toCharSeqMap(Map<String, String> map)
{    return Maps.<CharSequence, CharSequence>newHashMap(map);}
0
public FailurePolicy build(Context config)
{    return new SavePolicy(config);}
0
public static void saveSchema() throws IOException
{    oldTestBuildDataProp = System.getProperty(TEST_BUILD_DATA_KEY);    System.setProperty(TEST_BUILD_DATA_KEY, DFS_DIR);    FileWriter schema = new FileWriter(SCHEMA_FILE);    schema.append(RECORD_SCHEMA.toString());    schema.close();}
0
public static void tearDownClass()
{    FileUtils.deleteQuietly(new File(DFS_DIR));    if (oldTestBuildDataProp != null) {        System.setProperty(TEST_BUILD_DATA_KEY, oldTestBuildDataProp);    }}
0
public void setup() throws EventDeliveryException
{    Datasets.delete(FILE_DATASET_URI);    Datasets.create(FILE_DATASET_URI, DESCRIPTOR);    this.config = new Context();    config.put("keep-alive", "0");    this.in = new MemoryChannel();    Configurables.configure(in, config);    config.put(DatasetSinkConstants.CONFIG_KITE_DATASET_URI, FILE_DATASET_URI);    GenericRecordBuilder builder = new GenericRecordBuilder(RECORD_SCHEMA);    expected = Lists.<GenericRecord>newArrayList(builder.set("id", "1").set("msg", "msg1").build(), builder.set("id", "2").set("msg", "msg2").build(), builder.set("id", "3").set("msg", "msg3").build());    putToChannel(in, Iterables.transform(expected, new Function<GenericRecord, Event>() {        private int i = 0;        @Override        public Event apply(@Nullable GenericRecord rec) {            this.i += 1;            boolean useURI = (i % 2) == 0;            return event(rec, RECORD_SCHEMA, SCHEMA_FILE, useURI);        }    }));}
0
public Event apply(@Nullable GenericRecord rec)
{    this.i += 1;    boolean useURI = (i % 2) == 0;    return event(rec, RECORD_SCHEMA, SCHEMA_FILE, useURI);}
0
public void teardown()
{    Datasets.delete(FILE_DATASET_URI);}
0
public void testOldConfig() throws EventDeliveryException
{    config.put(DatasetSinkConstants.CONFIG_KITE_DATASET_URI, null);    config.put(DatasetSinkConstants.CONFIG_KITE_REPO_URI, FILE_REPO_URI);    config.put(DatasetSinkConstants.CONFIG_KITE_DATASET_NAME, DATASET_NAME);    DatasetSink sink = sink(in, config);        sink.start();    sink.process();    sink.stop();    Assert.assertEquals(Sets.newHashSet(expected), read(Datasets.load(FILE_DATASET_URI)));    Assert.assertEquals("Should have committed", 0, remaining(in));}
0
public void testDatasetUriOverridesOldConfig() throws EventDeliveryException
{        config.put(DatasetSinkConstants.CONFIG_KITE_REPO_URI, "bad uri");    config.put(DatasetSinkConstants.CONFIG_KITE_DATASET_NAME, "");    DatasetSink sink = sink(in, config);        sink.start();    sink.process();    sink.stop();    Assert.assertEquals(Sets.newHashSet(expected), read(Datasets.load(FILE_DATASET_URI)));    Assert.assertEquals("Should have committed", 0, remaining(in));}
0
public void testFileStore() throws EventDeliveryException, NonRecoverableEventException, NonRecoverableEventException
{    DatasetSink sink = sink(in, config);        sink.start();    sink.process();    sink.stop();    Assert.assertEquals(Sets.newHashSet(expected), read(Datasets.load(FILE_DATASET_URI)));    Assert.assertEquals("Should have committed", 0, remaining(in));}
0
public void testParquetDataset() throws EventDeliveryException
{    Datasets.delete(FILE_DATASET_URI);    Dataset<GenericRecord> created = Datasets.create(FILE_DATASET_URI, new DatasetDescriptor.Builder(DESCRIPTOR).format("parquet").build());    DatasetSink sink = sink(in, config);        sink.start();    sink.process();        assertThrows("Transaction should still be open", IllegalStateException.class, new Callable() {        @Override        public Object call() throws EventDeliveryException {            in.getTransaction().begin();            return null;        }    });        Assert.assertEquals("Should not have committed", 0, read(created).size());    sink.stop();    Assert.assertEquals(Sets.newHashSet(expected), read(created));    Assert.assertEquals("Should have committed", 0, remaining(in));}
0
public Object call() throws EventDeliveryException
{    in.getTransaction().begin();    return null;}
0
public void testPartitionedData() throws EventDeliveryException
{    URI partitionedUri = URI.create("dataset:file:target/test_repo/partitioned");    try {        Datasets.create(partitionedUri, new DatasetDescriptor.Builder(DESCRIPTOR).partitionStrategy(new PartitionStrategy.Builder().identity("id",         10).build()).build());        config.put(DatasetSinkConstants.CONFIG_KITE_DATASET_URI, partitionedUri.toString());        DatasetSink sink = sink(in, config);                sink.start();        sink.process();        sink.stop();        Assert.assertEquals(Sets.newHashSet(expected), read(Datasets.load(partitionedUri)));        Assert.assertEquals("Should have committed", 0, remaining(in));    } finally {        if (Datasets.exists(partitionedUri)) {            Datasets.delete(partitionedUri);        }    }}
0
public void testStartBeforeDatasetCreated() throws EventDeliveryException
{        Datasets.delete(FILE_DATASET_URI);    DatasetSink sink = sink(in, config);        sink.start();        try {        sink.process();        Assert.fail("Should have thrown an exception: no such dataset");    } catch (EventDeliveryException e) {        }        Datasets.create(FILE_DATASET_URI, DESCRIPTOR);        sink.process();    sink.stop();    Assert.assertEquals(Sets.newHashSet(expected), read(Datasets.load(FILE_DATASET_URI)));    Assert.assertEquals("Should have committed", 0, remaining(in));}
0
public void testDatasetUpdate() throws EventDeliveryException
{        GenericRecordBuilder updatedBuilder = new GenericRecordBuilder(UPDATED_SCHEMA);    GenericData.Record updatedRecord = updatedBuilder.set("id", "0").set("priority", 1).set("msg", "Priority 1 message!").build();        Set<GenericRecord> expectedAsUpdated = Sets.newHashSet();    for (GenericRecord record : expected) {        expectedAsUpdated.add(updatedBuilder.clear("priority").set("id", record.get("id")).set("msg", record.get("msg")).build());    }    expectedAsUpdated.add(updatedRecord);    DatasetSink sink = sink(in, config);        sink.start();    sink.process();        DatasetDescriptor updated = new DatasetDescriptor.Builder(Datasets.load(FILE_DATASET_URI).getDataset().getDescriptor()).schema(UPDATED_SCHEMA).build();    Datasets.update(FILE_DATASET_URI, updated);        sink.roll();        putToChannel(in, event(updatedRecord, UPDATED_SCHEMA, null, false));        sink.process();    sink.stop();    Assert.assertEquals(expectedAsUpdated, read(Datasets.load(FILE_DATASET_URI)));    Assert.assertEquals("Should have committed", 0, remaining(in));}
0
public void testMiniClusterStore() throws EventDeliveryException, IOException
{        MiniDFSCluster cluster = new MiniDFSCluster.Builder(new Configuration()).build();    FileSystem dfs = cluster.getFileSystem();    Configuration conf = dfs.getConf();    URI hdfsUri = URI.create("dataset:" + conf.get("fs.defaultFS") + "/tmp/repo" + DATASET_NAME);    try {                Datasets.create(hdfsUri, DESCRIPTOR);                config.put(DatasetSinkConstants.CONFIG_KITE_DATASET_URI, hdfsUri.toString());        DatasetSink sink = sink(in, config);                sink.start();        sink.process();        sink.stop();        Assert.assertEquals(Sets.newHashSet(expected), read(Datasets.load(hdfsUri)));        Assert.assertEquals("Should have committed", 0, remaining(in));    } finally {        if (Datasets.exists(hdfsUri)) {            Datasets.delete(hdfsUri);        }        cluster.shutdown();    }}
0
public void testBatchSize() throws EventDeliveryException
{    DatasetSink sink = sink(in, config);        config.put("kite.batchSize", "2");    Configurables.configure(sink, config);    sink.start();        sink.process();        sink.roll();        sink.process();    Assert.assertEquals(Sets.newHashSet(expected.subList(0, 2)), read(Datasets.load(FILE_DATASET_URI)));    Assert.assertEquals("Should have committed", 0, remaining(in));        sink.roll();        sink.process();    Assert.assertEquals(Sets.newHashSet(expected), read(Datasets.load(FILE_DATASET_URI)));    sink.stop();}
0
public void testTimedFileRolling() throws EventDeliveryException, InterruptedException
{            config.put("kite.rollInterval", "1");    DatasetSink sink = sink(in, config);    Dataset<GenericRecord> records = Datasets.load(FILE_DATASET_URI);        sink.start();    sink.process();    Assert.assertEquals("Should have committed", 0, remaining(in));        Thread.sleep(1100);        sink.process();    Assert.assertEquals(Sets.newHashSet(expected), read(records));        sink.stop();}
0
public void testCompatibleSchemas() throws EventDeliveryException
{    DatasetSink sink = sink(in, config);        GenericRecordBuilder compatBuilder = new GenericRecordBuilder(COMPATIBLE_SCHEMA);    GenericData.Record compatibleRecord = compatBuilder.set("id", "0").build();        putToChannel(in, event(compatibleRecord, COMPATIBLE_SCHEMA, null, false));            GenericRecordBuilder builder = new GenericRecordBuilder(RECORD_SCHEMA);    GenericData.Record expectedRecord = builder.set("id", "0").build();    expected.add(expectedRecord);        sink.start();    sink.process();    sink.stop();    Assert.assertEquals(Sets.newHashSet(expected), read(Datasets.load(FILE_DATASET_URI)));    Assert.assertEquals("Should have committed", 0, remaining(in));}
0
public void testIncompatibleSchemas() throws EventDeliveryException
{    final DatasetSink sink = sink(in, config);    GenericRecordBuilder builder = new GenericRecordBuilder(INCOMPATIBLE_SCHEMA);    GenericData.Record rec = builder.set("username", "koala").build();    putToChannel(in, event(rec, INCOMPATIBLE_SCHEMA, null, false));        sink.start();    assertThrows("Should fail", EventDeliveryException.class, new Callable() {        @Override        public Object call() throws EventDeliveryException {            sink.process();            return null;        }    });    sink.stop();    Assert.assertEquals("Should have rolled back", expected.size() + 1, remaining(in));}
0
public Object call() throws EventDeliveryException
{    sink.process();    return null;}
0
public void testMissingSchema() throws EventDeliveryException
{    final DatasetSink sink = sink(in, config);    Event badEvent = new SimpleEvent();    badEvent.setHeaders(Maps.<String, String>newHashMap());    badEvent.setBody(serialize(expected.get(0), RECORD_SCHEMA));    putToChannel(in, badEvent);        sink.start();    assertThrows("Should fail", EventDeliveryException.class, new Callable() {        @Override        public Object call() throws EventDeliveryException {            sink.process();            return null;        }    });    sink.stop();    Assert.assertEquals("Should have rolled back", expected.size() + 1, remaining(in));}
0
public Object call() throws EventDeliveryException
{    sink.process();    return null;}
0
public void testFileStoreWithSavePolicy() throws EventDeliveryException
{    if (Datasets.exists(ERROR_DATASET_URI)) {        Datasets.delete(ERROR_DATASET_URI);    }    config.put(DatasetSinkConstants.CONFIG_FAILURE_POLICY, DatasetSinkConstants.SAVE_FAILURE_POLICY);    config.put(DatasetSinkConstants.CONFIG_KITE_ERROR_DATASET_URI, ERROR_DATASET_URI);    DatasetSink sink = sink(in, config);        sink.start();    sink.process();    sink.stop();    Assert.assertEquals(Sets.newHashSet(expected), read(Datasets.load(FILE_DATASET_URI)));    Assert.assertEquals("Should have committed", 0, remaining(in));}
0
public void testMissingSchemaWithSavePolicy() throws EventDeliveryException
{    if (Datasets.exists(ERROR_DATASET_URI)) {        Datasets.delete(ERROR_DATASET_URI);    }    config.put(DatasetSinkConstants.CONFIG_FAILURE_POLICY, DatasetSinkConstants.SAVE_FAILURE_POLICY);    config.put(DatasetSinkConstants.CONFIG_KITE_ERROR_DATASET_URI, ERROR_DATASET_URI);    final DatasetSink sink = sink(in, config);    Event badEvent = new SimpleEvent();    badEvent.setHeaders(Maps.<String, String>newHashMap());    badEvent.setBody(serialize(expected.get(0), RECORD_SCHEMA));    putToChannel(in, badEvent);        sink.start();    sink.process();    sink.stop();    Assert.assertEquals("Good records should have been written", Sets.newHashSet(expected), read(Datasets.load(FILE_DATASET_URI)));    Assert.assertEquals("Should not have rolled back", 0, remaining(in));    Assert.assertEquals("Should have saved the bad event", Sets.newHashSet(AvroFlumeEvent.newBuilder().setBody(ByteBuffer.wrap(badEvent.getBody())).setHeaders(toUtf8Map(badEvent.getHeaders())).build()), read(Datasets.load(ERROR_DATASET_URI, AvroFlumeEvent.class)));}
0
public void testSerializedWithIncompatibleSchemasWithSavePolicy() throws EventDeliveryException
{    if (Datasets.exists(ERROR_DATASET_URI)) {        Datasets.delete(ERROR_DATASET_URI);    }    config.put(DatasetSinkConstants.CONFIG_FAILURE_POLICY, DatasetSinkConstants.SAVE_FAILURE_POLICY);    config.put(DatasetSinkConstants.CONFIG_KITE_ERROR_DATASET_URI, ERROR_DATASET_URI);    final DatasetSink sink = sink(in, config);    GenericRecordBuilder builder = new GenericRecordBuilder(INCOMPATIBLE_SCHEMA);    GenericData.Record rec = builder.set("username", "koala").build();            Event badEvent = event(rec, INCOMPATIBLE_SCHEMA, SCHEMA_FILE, true);    putToChannel(in, badEvent);        sink.start();    sink.process();    sink.stop();    Assert.assertEquals("Good records should have been written", Sets.newHashSet(expected), read(Datasets.load(FILE_DATASET_URI)));    Assert.assertEquals("Should not have rolled back", 0, remaining(in));    Assert.assertEquals("Should have saved the bad event", Sets.newHashSet(AvroFlumeEvent.newBuilder().setBody(ByteBuffer.wrap(badEvent.getBody())).setHeaders(toUtf8Map(badEvent.getHeaders())).build()), read(Datasets.load(ERROR_DATASET_URI, AvroFlumeEvent.class)));}
0
public void testSerializedWithIncompatibleSchemas() throws EventDeliveryException
{    final DatasetSink sink = sink(in, config);    GenericRecordBuilder builder = new GenericRecordBuilder(INCOMPATIBLE_SCHEMA);    GenericData.Record rec = builder.set("username", "koala").build();            putToChannel(in, event(rec, INCOMPATIBLE_SCHEMA, SCHEMA_FILE, true));        sink.start();    assertThrows("Should fail", EventDeliveryException.class, new Callable() {        @Override        public Object call() throws EventDeliveryException {            sink.process();            return null;        }    });    sink.stop();    Assert.assertEquals("Should have rolled back", expected.size() + 1, remaining(in));}
0
public Object call() throws EventDeliveryException
{    sink.process();    return null;}
0
public void testCommitOnBatch() throws EventDeliveryException
{    DatasetSink sink = sink(in, config);        sink.start();    sink.process();        Assert.assertEquals("Should have committed", 0, remaining(in));        Assert.assertEquals(0, read(Datasets.load(FILE_DATASET_URI)).size());    sink.stop();    Assert.assertEquals(Sets.newHashSet(expected), read(Datasets.load(FILE_DATASET_URI)));}
0
public void testCommitOnBatchFalse() throws EventDeliveryException
{    config.put(DatasetSinkConstants.CONFIG_FLUSHABLE_COMMIT_ON_BATCH, Boolean.toString(false));    config.put(DatasetSinkConstants.CONFIG_SYNCABLE_SYNC_ON_BATCH, Boolean.toString(false));    DatasetSink sink = sink(in, config);        sink.start();    sink.process();        assertThrows("Transaction should still be open", IllegalStateException.class, new Callable() {        @Override        public Object call() throws EventDeliveryException {            in.getTransaction().begin();            return null;        }    });        Assert.assertEquals(0, read(Datasets.load(FILE_DATASET_URI)).size());    sink.stop();    Assert.assertEquals(Sets.newHashSet(expected), read(Datasets.load(FILE_DATASET_URI)));        Assert.assertEquals("Should have committed", 0, remaining(in));}
0
public Object call() throws EventDeliveryException
{    in.getTransaction().begin();    return null;}
0
public void testCommitOnBatchFalseSyncOnBatchTrue() throws EventDeliveryException
{    config.put(DatasetSinkConstants.CONFIG_FLUSHABLE_COMMIT_ON_BATCH, Boolean.toString(false));    config.put(DatasetSinkConstants.CONFIG_SYNCABLE_SYNC_ON_BATCH, Boolean.toString(true));    try {        sink(in, config);        Assert.fail("Should have thrown IllegalArgumentException");    } catch (IllegalArgumentException ex) {        }}
0
public void testCloseAndCreateWriter() throws EventDeliveryException
{    config.put(DatasetSinkConstants.CONFIG_FLUSHABLE_COMMIT_ON_BATCH, Boolean.toString(false));    config.put(DatasetSinkConstants.CONFIG_SYNCABLE_SYNC_ON_BATCH, Boolean.toString(false));    DatasetSink sink = sink(in, config);        sink.start();    sink.process();    sink.closeWriter();    sink.commitTransaction();    sink.createWriter();    Assert.assertNotNull("Writer should not be null", sink.getWriter());    Assert.assertEquals("Should have committed", 0, remaining(in));    sink.stop();    Assert.assertEquals(Sets.newHashSet(expected), read(Datasets.load(FILE_DATASET_URI)));}
0
public void testCloseWriter() throws EventDeliveryException
{    config.put(DatasetSinkConstants.CONFIG_FLUSHABLE_COMMIT_ON_BATCH, Boolean.toString(false));    config.put(DatasetSinkConstants.CONFIG_SYNCABLE_SYNC_ON_BATCH, Boolean.toString(false));    DatasetSink sink = sink(in, config);        sink.start();    sink.process();    sink.closeWriter();    sink.commitTransaction();    Assert.assertNull("Writer should be null", sink.getWriter());    Assert.assertEquals("Should have committed", 0, remaining(in));    sink.stop();    Assert.assertEquals(Sets.newHashSet(expected), read(Datasets.load(FILE_DATASET_URI)));}
0
public void testCreateWriter() throws EventDeliveryException
{    config.put(DatasetSinkConstants.CONFIG_FLUSHABLE_COMMIT_ON_BATCH, Boolean.toString(false));    config.put(DatasetSinkConstants.CONFIG_SYNCABLE_SYNC_ON_BATCH, Boolean.toString(false));    DatasetSink sink = sink(in, config);        sink.start();    sink.process();    sink.commitTransaction();    sink.createWriter();    Assert.assertNotNull("Writer should not be null", sink.getWriter());    Assert.assertEquals("Should have committed", 0, remaining(in));    sink.stop();    Assert.assertEquals(0, read(Datasets.load(FILE_DATASET_URI)).size());}
0
public void testAppendWriteExceptionInvokesPolicy() throws EventDeliveryException, NonRecoverableEventException
{    DatasetSink sink = sink(in, config);        sink.start();    sink.process();        Event mockEvent = mock(Event.class);    when(mockEvent.getBody()).thenReturn(new byte[] { 0x01 });        GenericRecord mockRecord = mock(GenericRecord.class);        EntityParser<GenericRecord> mockParser = mock(EntityParser.class);    when(mockParser.parse(eq(mockEvent), any(GenericRecord.class))).thenReturn(mockRecord);    sink.setParser(mockParser);        FailurePolicy mockFailurePolicy = mock(FailurePolicy.class);    sink.setFailurePolicy(mockFailurePolicy);        DatasetWriter<GenericRecord> mockWriter = mock(DatasetWriter.class);    doThrow(new DataFileWriter.AppendWriteException(new IOException())).when(mockWriter).write(mockRecord);    sink.setWriter(mockWriter);    sink.write(mockEvent);        verify(mockFailurePolicy).handle(eq(mockEvent), any(Throwable.class));    sink.stop();}
0
public void testRuntimeExceptionThrowsEventDeliveryException() throws EventDeliveryException, NonRecoverableEventException
{    DatasetSink sink = sink(in, config);        sink.start();    sink.process();        Event mockEvent = mock(Event.class);    when(mockEvent.getBody()).thenReturn(new byte[] { 0x01 });        GenericRecord mockRecord = mock(GenericRecord.class);        EntityParser<GenericRecord> mockParser = mock(EntityParser.class);    when(mockParser.parse(eq(mockEvent), any(GenericRecord.class))).thenReturn(mockRecord);    sink.setParser(mockParser);        FailurePolicy mockFailurePolicy = mock(FailurePolicy.class);    sink.setFailurePolicy(mockFailurePolicy);        DatasetWriter<GenericRecord> mockWriter = mock(DatasetWriter.class);    doThrow(new RuntimeException()).when(mockWriter).write(mockRecord);    sink.setWriter(mockWriter);    try {        sink.write(mockEvent);        Assert.fail("Should throw EventDeliveryException");    } catch (EventDeliveryException ex) {    }        verify(mockFailurePolicy, never()).handle(eq(mockEvent), any(Throwable.class));    sink.stop();}
0
public void testProcessHandlesNullWriter() throws EventDeliveryException, NonRecoverableEventException, NonRecoverableEventException
{    DatasetSink sink = sink(in, config);        sink.start();    sink.process();        sink.setWriter(null);        sink.process();    sink.stop();    Assert.assertEquals("Should have committed", 0, remaining(in));}
0
public static DatasetSink sink(Channel in, Context config)
{    DatasetSink sink = new DatasetSink();    sink.setChannel(in);    Configurables.configure(sink, config);    return sink;}
0
public static HashSet<T> read(View<T> view)
{    DatasetReader<T> reader = null;    try {        reader = view.newReader();        return Sets.newHashSet(reader.iterator());    } finally {        if (reader != null) {            reader.close();        }    }}
0
public static int remaining(Channel ch) throws EventDeliveryException
{    Transaction t = ch.getTransaction();    try {        t.begin();        int count = 0;        while (ch.take() != null) {            count += 1;        }        t.commit();        return count;    } catch (Throwable th) {        t.rollback();        Throwables.propagateIfInstanceOf(th, Error.class);        Throwables.propagateIfInstanceOf(th, EventDeliveryException.class);        throw new EventDeliveryException(th);    } finally {        t.close();    }}
0
public static void putToChannel(Channel in, Event... records) throws EventDeliveryException
{    putToChannel(in, Arrays.asList(records));}
0
public static void putToChannel(Channel in, Iterable<Event> records) throws EventDeliveryException
{    Transaction t = in.getTransaction();    try {        t.begin();        for (Event record : records) {            in.put(record);        }        t.commit();    } catch (Throwable th) {        t.rollback();        Throwables.propagateIfInstanceOf(th, Error.class);        Throwables.propagateIfInstanceOf(th, EventDeliveryException.class);        throw new EventDeliveryException(th);    } finally {        t.close();    }}
0
public static Event event(Object datum, Schema schema, File file, boolean useURI)
{    Map<String, String> headers = Maps.newHashMap();    if (useURI) {        headers.put(DatasetSinkConstants.AVRO_SCHEMA_URL_HEADER, file.getAbsoluteFile().toURI().toString());    } else {        headers.put(DatasetSinkConstants.AVRO_SCHEMA_LITERAL_HEADER, schema.toString());    }    Event e = new SimpleEvent();    e.setBody(serialize(datum, schema));    e.setHeaders(headers);    return e;}
0
public static byte[] serialize(Object datum, Schema schema)
{    ByteArrayOutputStream out = new ByteArrayOutputStream();    Encoder encoder = EncoderFactory.get().binaryEncoder(out, null);    ReflectDatumWriter writer = new ReflectDatumWriter(schema);    try {        writer.write(datum, encoder);        encoder.flush();    } catch (IOException ex) {        Throwables.propagate(ex);    }    return out.toByteArray();}
0
public static void assertThrows(String message, Class<? extends Exception> expected, Callable callable)
{    try {        callable.call();        Assert.fail("No exception was thrown (" + message + "), expected: " + expected.getName());    } catch (Exception actual) {        Assert.assertEquals(message, expected, actual.getClass());    }}
0
public static Map<CharSequence, CharSequence> toUtf8Map(Map<String, String> map)
{    Map<CharSequence, CharSequence> utf8Map = Maps.newHashMap();    for (Map.Entry<String, String> entry : map.entrySet()) {        utf8Map.put(new Utf8(entry.getKey()), new Utf8(entry.getValue()));    }    return utf8Map;}
0
public void configure(Context context)
{    configuredMinReplicas = context.getInteger("hdfs.minBlockReplicas");    if (configuredMinReplicas != null) {        Preconditions.checkArgument(configuredMinReplicas >= 0, "hdfs.minBlockReplicas must be greater than or equal to 0");    }    numberOfCloseRetries = context.getInteger("hdfs.closeTries", 1) - 1;    if (numberOfCloseRetries > 1) {        try {                        timeBetweenCloseRetries = context.getLong("hdfs.callTimeout", 30000L);        } catch (NumberFormatException e) {                    }        timeBetweenCloseRetries = Math.max(timeBetweenCloseRetries / numberOfCloseRetries, 1000);    }}
1
public boolean isUnderReplicated()
{    try {        int numBlocks = getNumCurrentReplicas();        if (numBlocks == -1) {            return false;        }        int desiredBlocks;        if (configuredMinReplicas != null) {            desiredBlocks = configuredMinReplicas;        } else {            desiredBlocks = getFsDesiredReplication();        }        return numBlocks < desiredBlocks;    } catch (IllegalAccessException e) {            } catch (InvocationTargetException e) {            } catch (IllegalArgumentException e) {            }    return false;}
1
protected void registerCurrentStream(FSDataOutputStream outputStream, FileSystem fs, Path destPath)
{    Preconditions.checkNotNull(outputStream, "outputStream must not be null");    Preconditions.checkNotNull(fs, "fs must not be null");    Preconditions.checkNotNull(destPath, "destPath must not be null");    this.outputStream = outputStream;    this.fs = fs;    this.destPath = destPath;    this.refGetNumCurrentReplicas = reflectGetNumCurrentReplicas(outputStream);    this.refGetDefaultReplication = reflectGetDefaultReplication(fs);    this.refHflushOrSync = reflectHflushOrSync(outputStream);}
0
protected void unregisterCurrentStream()
{    this.outputStream = null;    this.fs = null;    this.destPath = null;    this.refGetNumCurrentReplicas = null;    this.refGetDefaultReplication = null;}
0
public int getFsDesiredReplication()
{    short replication = 0;    if (fs != null && destPath != null) {        if (refGetDefaultReplication != null) {            try {                replication = (Short) refGetDefaultReplication.invoke(fs, destPath);            } catch (IllegalAccessException e) {                            } catch (InvocationTargetException e) {                            }        } else {                        replication = fs.getDefaultReplication();        }    }    return replication;}
1
public int getNumCurrentReplicas() throws IllegalArgumentException, IllegalAccessException, InvocationTargetException
{    if (refGetNumCurrentReplicas != null && outputStream != null) {        OutputStream dfsOutputStream = outputStream.getWrappedStream();        if (dfsOutputStream != null) {            Object repl = refGetNumCurrentReplicas.invoke(dfsOutputStream, NO_ARGS);            if (repl instanceof Integer) {                return ((Integer) repl).intValue();            }        }    }    return -1;}
0
private Method reflectHflushOrSync(FSDataOutputStream os)
{    Method m = null;    if (os != null) {        Class<?> fsDataOutputStreamClass = os.getClass();        try {            m = fsDataOutputStreamClass.getMethod("hflush");        } catch (NoSuchMethodException ex) {                        try {                m = fsDataOutputStreamClass.getMethod("sync");            } catch (Exception ex1) {                String msg = "Neither hflush not sync were found. That seems to be " + "a problem!";                                throw new FlumeException(msg, ex1);            }        }    }    return m;}
1
protected void hflushOrSync(FSDataOutputStream os) throws IOException
{    try {                        this.refHflushOrSync.invoke(os);    } catch (InvocationTargetException e) {        String msg = "Error while trying to hflushOrSync!";                Throwable cause = e.getCause();        if (cause != null && cause instanceof IOException) {            throw (IOException) cause;        }        throw new FlumeException(msg, e);    } catch (Exception e) {        String msg = "Error while trying to hflushOrSync!";                throw new FlumeException(msg, e);    }}
1
public void configure(Context context)
{    syncIntervalBytes = context.getInteger(SYNC_INTERVAL_BYTES, DEFAULT_SYNC_INTERVAL_BYTES);    compressionCodec = context.getString(COMPRESSION_CODEC, DEFAULT_COMPRESSION_CODEC);    staticSchemaURL = context.getString(STATIC_SCHEMA_URL, DEFAULT_STATIC_SCHEMA_URL);}
0
public void afterCreate() throws IOException
{}
0
public void afterReopen() throws IOException
{        throw new UnsupportedOperationException("Avro API doesn't support append");}
0
public void write(Event event) throws IOException
{    if (dataFileWriter == null) {        initialize(event);    }    dataFileWriter.appendEncoded(ByteBuffer.wrap(event.getBody()));}
0
private void initialize(Event event) throws IOException
{    Schema schema = null;    String schemaUrl = event.getHeaders().get(AVRO_SCHEMA_URL_HEADER);    String schemaString = event.getHeaders().get(AVRO_SCHEMA_LITERAL_HEADER);    if (schemaUrl != null) {                schema = schemaCache.get(schemaUrl);        if (schema == null) {            schema = loadFromUrl(schemaUrl);            schemaCache.put(schemaUrl, schema);        }    } else if (schemaString != null) {                schema = new Schema.Parser().parse(schemaString);    } else if (staticSchemaURL != null) {                schema = schemaCache.get(staticSchemaURL);        if (schema == null) {            schema = loadFromUrl(staticSchemaURL);            schemaCache.put(staticSchemaURL, schema);        }    } else {                throw new FlumeException("Could not find schema for event " + event);    }    writer = new GenericDatumWriter<Object>(schema);    dataFileWriter = new DataFileWriter<Object>(writer);    dataFileWriter.setSyncInterval(syncIntervalBytes);    try {        CodecFactory codecFactory = CodecFactory.fromString(compressionCodec);        dataFileWriter.setCodec(codecFactory);    } catch (AvroRuntimeException e) {            }    dataFileWriter.create(schema, out);}
1
private Schema loadFromUrl(String schemaUrl) throws IOException
{    Configuration conf = new Configuration();    Schema.Parser parser = new Schema.Parser();    if (schemaUrl.toLowerCase(Locale.ENGLISH).startsWith("hdfs://")) {        FileSystem fs = FileSystem.get(conf);        FSDataInputStream input = null;        try {            input = fs.open(new Path(schemaUrl));            return parser.parse(input);        } finally {            if (input != null) {                input.close();            }        }    } else {        InputStream is = null;        try {            is = new URL(schemaUrl).openStream();            return parser.parse(is);        } finally {            if (is != null) {                is.close();            }        }    }}
0
public void flush() throws IOException
{    dataFileWriter.flush();}
0
public void beforeClose() throws IOException
{}
0
public boolean supportsReopen()
{    return false;}
0
public EventSerializer build(Context context, OutputStream out)
{    AvroEventSerializer writer = new AvroEventSerializer(out);    writer.configure(context);    return writer;}
0
 void setFileSystem(FileSystem fs)
{    this.fileSystem = fs;    mockFsInjected = true;}
0
private void resetCounters()
{    eventCounter = 0;    processSize = 0;    batchCounter = 0;}
0
private Method getRefIsClosed()
{    try {        return fileSystem.getClass().getMethod("isFileClosed", Path.class);    } catch (Exception e) {                return null;    }}
1
private Boolean isFileClosed(FileSystem fs, Path tmpFilePath) throws Exception
{    return (Boolean) (isClosedMethod.invoke(fs, tmpFilePath));}
0
private void open() throws IOException, InterruptedException
{    if ((filePath == null) || (writer == null)) {        throw new IOException("Invalid file settings");    }    final Configuration config = new Configuration();        config.setBoolean("fs.automatic.close", false);        synchronized (staticLock) {        checkAndThrowInterruptedException();        try {            long counter = fileExtensionCounter.incrementAndGet();            String fullFileName = fileName + "." + counter;            if (fileSuffix != null && fileSuffix.length() > 0) {                fullFileName += fileSuffix;            } else if (codeC != null) {                fullFileName += codeC.getDefaultExtension();            }            bucketPath = filePath + "/" + inUsePrefix + fullFileName + inUseSuffix;            targetPath = filePath + "/" + fullFileName;                        callWithTimeout(new CallRunner<Void>() {                @Override                public Void call() throws Exception {                    if (codeC == null) {                                                if (!mockFsInjected) {                            fileSystem = new Path(bucketPath).getFileSystem(config);                        }                        writer.open(bucketPath);                    } else {                                                if (!mockFsInjected) {                            fileSystem = new Path(bucketPath).getFileSystem(config);                        }                        writer.open(bucketPath, codeC, compType);                    }                    return null;                }            });        } catch (Exception ex) {            sinkCounter.incrementConnectionFailedCount();            if (ex instanceof IOException) {                throw (IOException) ex;            } else {                throw Throwables.propagate(ex);            }        }    }    isClosedMethod = getRefIsClosed();    sinkCounter.incrementConnectionCreatedCount();    resetCounters();        if (rollInterval > 0) {        Callable<Void> action = new Callable<Void>() {            public Void call() throws Exception {                                try {                                        close(true);                } catch (Throwable t) {                                    }                return null;            }        };        timedRollFuture = timedRollerPool.schedule(action, rollInterval, TimeUnit.SECONDS);    }    isOpen = true;}
1
public Void call() throws Exception
{    if (codeC == null) {                if (!mockFsInjected) {            fileSystem = new Path(bucketPath).getFileSystem(config);        }        writer.open(bucketPath);    } else {                if (!mockFsInjected) {            fileSystem = new Path(bucketPath).getFileSystem(config);        }        writer.open(bucketPath, codeC, compType);    }    return null;}
0
public Void call() throws Exception
{        try {                close(true);    } catch (Throwable t) {            }    return null;}
1
public void close() throws InterruptedException
{    close(false);}
0
private CallRunner<Void> createCloseCallRunner()
{    return new CallRunner<Void>() {        @Override        public Void call() throws Exception {                        writer.close();            return null;        }    };}
0
public Void call() throws Exception
{        writer.close();    return null;}
0
public Void call() throws Exception
{    close(false);    return null;}
0
public void close(boolean immediate)
{    closeTries++;    boolean shouldRetry = closeTries < maxRetries && !immediate;    try {        callWithTimeout(createCloseCallRunner());        sinkCounter.incrementConnectionClosedCount();    } catch (InterruptedException | IOException e) {                if (timedRollerPool != null && !timedRollerPool.isTerminated()) {            if (shouldRetry) {                timedRollerPool.schedule(this, retryInterval, TimeUnit.SECONDS);            }        } else {                    }        if (!shouldRetry) {                        sinkCounter.incrementConnectionFailedCount();            recoverLease();        }    }}
1
public Void call() throws Exception
{    if (renameTries >= maxRetries) {                return null;    }    renameTries++;    try {        renameBucket(path, finalPath, fs);    } catch (Exception e) {                timedRollerPool.schedule(this, retryInterval, TimeUnit.SECONDS);        return null;    }    return null;}
1
private synchronized void recoverLease()
{    if (bucketPath != null && fileSystem instanceof DistributedFileSystem) {        try {                        ((DistributedFileSystem) fileSystem).recoverLease(new Path(bucketPath));        } catch (IOException ex) {                    }    }}
1
public void close(boolean callCloseCallback) throws InterruptedException
{    close(callCloseCallback, false);}
0
public void close(boolean callCloseCallback, boolean immediate) throws InterruptedException
{    if (callCloseCallback) {        if (closed.compareAndSet(false, true)) {                        runCloseAction();        } else {                    }    }    doClose(immediate);}
1
private synchronized void doClose(boolean immediate) throws InterruptedException
{    checkAndThrowInterruptedException();    try {        flush();    } catch (IOException e) {            }        if (isOpen) {        new CloseHandler().close(immediate);        isOpen = false;    } else {            }        if (timedRollFuture != null && !timedRollFuture.isDone()) {                timedRollFuture.cancel(false);        timedRollFuture = null;    }    if (idleFuture != null && !idleFuture.isDone()) {                idleFuture.cancel(false);        idleFuture = null;    }    if (bucketPath != null && fileSystem != null) {                try {            renameBucket(bucketPath, targetPath, fileSystem);        } catch (Exception e) {                        sinkCounter.incrementConnectionFailedCount();            final Callable<Void> scheduledRename = new ScheduledRenameCallable();            timedRollerPool.schedule(scheduledRename, retryInterval, TimeUnit.SECONDS);        }    }}
1
public synchronized void flush() throws IOException, InterruptedException
{    checkAndThrowInterruptedException();    if (!isBatchComplete()) {        doFlush();        if (idleTimeout > 0) {                        if (idleFuture == null || idleFuture.cancel(false)) {                Callable<Void> idleAction = new Callable<Void>() {                    public Void call() throws Exception {                                                if (isOpen) {                            close(true);                        }                        return null;                    }                };                idleFuture = timedRollerPool.schedule(idleAction, idleTimeout, TimeUnit.SECONDS);            }        }    }}
1
public Void call() throws Exception
{        if (isOpen) {        close(true);    }    return null;}
1
private void runCloseAction()
{    try {        if (onCloseCallback != null) {            onCloseCallback.run(onCloseCallbackPath);        }    } catch (Throwable t) {            }}
1
private void doFlush() throws IOException, InterruptedException
{    callWithTimeout(new CallRunner<Void>() {        @Override        public Void call() throws Exception {                        writer.sync();            return null;        }    });    batchCounter = 0;}
0
public Void call() throws Exception
{        writer.sync();    return null;}
0
public Void call() throws Exception
{        writer.append(event);    return null;}
0
private boolean shouldRotate()
{    boolean doRotate = false;    if (writer.isUnderReplicated()) {        this.isUnderReplicated = true;        doRotate = true;    } else {        this.isUnderReplicated = false;    }    if ((rollCount > 0) && (rollCount <= eventCounter)) {                doRotate = true;    }    if ((rollSize > 0) && (rollSize <= processSize)) {                doRotate = true;    }    return doRotate;}
1
private void renameBucket(String bucketPath, String targetPath, final FileSystem fs) throws IOException, InterruptedException
{    if (bucketPath.equals(targetPath)) {        return;    }    final Path srcPath = new Path(bucketPath);    final Path dstPath = new Path(targetPath);    callWithTimeout(new CallRunner<Void>() {        @Override        public Void call() throws Exception {            if (fs.exists(srcPath)) {                                                renameTries.incrementAndGet();                                fs.rename(srcPath, dstPath);            }            return null;        }    });}
1
public Void call() throws Exception
{    if (fs.exists(srcPath)) {                        renameTries.incrementAndGet();                fs.rename(srcPath, dstPath);    }    return null;}
1
public String toString()
{    return "[ " + this.getClass().getSimpleName() + " targetPath = " + targetPath + ", bucketPath = " + bucketPath + " ]";}
0
private boolean isBatchComplete()
{    return (batchCounter == 0);}
0
private static void checkAndThrowInterruptedException() throws InterruptedException
{    if (Thread.currentThread().interrupted()) {        throw new InterruptedException("Timed out before HDFS call was made. " + "Your hdfs.callTimeout might be set too low or HDFS calls are " + "taking too long.");    }}
0
private T callWithTimeout(final CallRunner<T> callRunner) throws IOException, InterruptedException
{    Future<T> future = callTimeoutPool.submit(new Callable<T>() {        @Override        public T call() throws Exception {            return proxyUser.execute(new PrivilegedExceptionAction<T>() {                @Override                public T run() throws Exception {                    return callRunner.call();                }            });        }    });    try {        if (callTimeout > 0) {            return future.get(callTimeout, TimeUnit.MILLISECONDS);        } else {            return future.get();        }    } catch (TimeoutException eT) {        future.cancel(true);        sinkCounter.incrementConnectionFailedCount();        throw new IOException("Callable timed out after " + callTimeout + " ms" + " on file: " + bucketPath, eT);    } catch (ExecutionException e1) {        sinkCounter.incrementConnectionFailedCount();        Throwable cause = e1.getCause();        if (cause instanceof IOException) {            throw (IOException) cause;        } else if (cause instanceof InterruptedException) {            throw (InterruptedException) cause;        } else if (cause instanceof RuntimeException) {            throw (RuntimeException) cause;        } else if (cause instanceof Error) {            throw (Error) cause;        } else {            throw new RuntimeException(e1);        }    } catch (CancellationException ce) {        throw new InterruptedException("Blocked callable interrupted by rotation event");    } catch (InterruptedException ex) {                throw ex;    }}
1
public T call() throws Exception
{    return proxyUser.execute(new PrivilegedExceptionAction<T>() {        @Override        public T run() throws Exception {            return callRunner.call();        }    });}
0
public T run() throws Exception
{    return callRunner.call();}
0
public void configure(Context context)
{    super.configure(context);    serializerType = context.getString("serializer", "TEXT");    useRawLocalFileSystem = context.getBoolean("hdfs.useRawLocalFileSystem", false);    serializerContext = new Context(context.getSubProperties(EventSerializer.CTX_PREFIX));    }
1
public void open(String filePath) throws IOException
{    DefaultCodec defCodec = new DefaultCodec();    CompressionType cType = CompressionType.BLOCK;    open(filePath, defCodec, cType);}
0
public void open(String filePath, CompressionCodec codec, CompressionType cType) throws IOException
{    Configuration conf = new Configuration();    Path dstPath = new Path(filePath);    FileSystem hdfs = dstPath.getFileSystem(conf);    if (useRawLocalFileSystem) {        if (hdfs instanceof LocalFileSystem) {            hdfs = ((LocalFileSystem) hdfs).getRaw();        } else {                    }    }    boolean appending = false;    if (conf.getBoolean("hdfs.append.support", false) == true && hdfs.isFile(dstPath)) {        fsOut = hdfs.append(dstPath);        appending = true;    } else {        fsOut = hdfs.create(dstPath);    }    if (compressor == null) {        compressor = CodecPool.getCompressor(codec, conf);    }    cmpOut = codec.createOutputStream(fsOut, compressor);    serializer = EventSerializerFactory.getInstance(serializerType, serializerContext, cmpOut);    if (appending && !serializer.supportsReopen()) {        cmpOut.close();        serializer = null;        throw new IOException("serializer (" + serializerType + ") does not support append");    }    registerCurrentStream(fsOut, hdfs, dstPath);    if (appending) {        serializer.afterReopen();    } else {        serializer.afterCreate();    }    isFinished = false;}
1
public void append(Event e) throws IOException
{    if (isFinished) {        cmpOut.resetState();        isFinished = false;    }    serializer.write(e);}
0
public void sync() throws IOException
{                        serializer.flush();    if (!isFinished) {        cmpOut.finish();        isFinished = true;    }    fsOut.flush();    hflushOrSync(this.fsOut);}
0
public void close() throws IOException
{    serializer.flush();    serializer.beforeClose();    if (!isFinished) {        cmpOut.finish();        isFinished = true;    }    fsOut.flush();    hflushOrSync(fsOut);    cmpOut.close();    if (compressor != null) {        CodecPool.returnCompressor(compressor);        compressor = null;    }    unregisterCurrentStream();}
0
public void configure(Context context)
{    super.configure(context);    serializerType = context.getString("serializer", "TEXT");    useRawLocalFileSystem = context.getBoolean("hdfs.useRawLocalFileSystem", false);    serializerContext = new Context(context.getSubProperties(EventSerializer.CTX_PREFIX));    }
1
protected FileSystem getDfs(Configuration conf, Path dstPath) throws IOException
{    return dstPath.getFileSystem(conf);}
0
protected void doOpen(Configuration conf, Path dstPath, FileSystem hdfs) throws IOException
{    if (useRawLocalFileSystem) {        if (hdfs instanceof LocalFileSystem) {            hdfs = ((LocalFileSystem) hdfs).getRaw();        } else {                    }    }    boolean appending = false;    if (conf.getBoolean("hdfs.append.support", false) == true && hdfs.isFile(dstPath)) {        outStream = hdfs.append(dstPath);        appending = true;    } else {        outStream = hdfs.create(dstPath);    }    serializer = EventSerializerFactory.getInstance(serializerType, serializerContext, outStream);    if (appending && !serializer.supportsReopen()) {        outStream.close();        serializer = null;        throw new IOException("serializer (" + serializerType + ") does not support append");    }        registerCurrentStream(outStream, hdfs, dstPath);    if (appending) {        serializer.afterReopen();    } else {        serializer.afterCreate();    }}
1
public void open(String filePath) throws IOException
{    Configuration conf = new Configuration();    Path dstPath = new Path(filePath);    FileSystem hdfs = getDfs(conf, dstPath);    doOpen(conf, dstPath, hdfs);}
0
public void open(String filePath, CompressionCodec codec, CompressionType cType) throws IOException
{    open(filePath);}
0
public void append(Event e) throws IOException
{    serializer.write(e);}
0
public void sync() throws IOException
{    serializer.flush();    outStream.flush();    hflushOrSync(outStream);}
0
public void close() throws IOException
{    serializer.flush();    serializer.beforeClose();    outStream.flush();    hflushOrSync(outStream);    outStream.close();    unregisterCurrentStream();}
0
protected boolean removeEldestEntry(Entry<String, BucketWriter> eldest)
{    if (size() > maxOpenFiles) {                try {            eldest.getValue().close();        } catch (InterruptedException e) {                        Thread.currentThread().interrupt();        }        return true;    } else {        return false;    }}
1
 Map<String, BucketWriter> getSfWriters()
{    return sfWriters;}
0
public void configure(Context context)
{    this.context = context;    filePath = Preconditions.checkNotNull(context.getString("hdfs.path"), "hdfs.path is required");    fileName = context.getString("hdfs.filePrefix", defaultFileName);    this.suffix = context.getString("hdfs.fileSuffix", defaultSuffix);    inUsePrefix = context.getString("hdfs.inUsePrefix", defaultInUsePrefix);    boolean emptyInUseSuffix = context.getBoolean("hdfs.emptyInUseSuffix", false);    if (emptyInUseSuffix) {        inUseSuffix = "";        String tmpInUseSuffix = context.getString(IN_USE_SUFFIX_PARAM_NAME);        if (tmpInUseSuffix != null) {                    }    } else {        inUseSuffix = context.getString(IN_USE_SUFFIX_PARAM_NAME, defaultInUseSuffix);    }    String tzName = context.getString("hdfs.timeZone");    timeZone = tzName == null ? null : TimeZone.getTimeZone(tzName);    rollInterval = context.getLong("hdfs.rollInterval", defaultRollInterval);    rollSize = context.getLong("hdfs.rollSize", defaultRollSize);    rollCount = context.getLong("hdfs.rollCount", defaultRollCount);    batchSize = context.getLong("hdfs.batchSize", defaultBatchSize);    idleTimeout = context.getInteger("hdfs.idleTimeout", 0);    String codecName = context.getString("hdfs.codeC");    fileType = context.getString("hdfs.fileType", defaultFileType);    maxOpenFiles = context.getInteger("hdfs.maxOpenFiles", defaultMaxOpenFiles);    callTimeout = context.getLong("hdfs.callTimeout", defaultCallTimeout);    threadsPoolSize = context.getInteger("hdfs.threadsPoolSize", defaultThreadPoolSize);    rollTimerPoolSize = context.getInteger("hdfs.rollTimerPoolSize", defaultRollTimerPoolSize);    String kerbConfPrincipal = context.getString("hdfs.kerberosPrincipal");    String kerbKeytab = context.getString("hdfs.kerberosKeytab");    String proxyUser = context.getString("hdfs.proxyUser");    tryCount = context.getInteger("hdfs.closeTries", defaultTryCount);    if (tryCount <= 0) {                tryCount = defaultTryCount;    }    retryInterval = context.getLong("hdfs.retryInterval", defaultRetryInterval);    if (retryInterval <= 0) {                tryCount = 1;    }    Preconditions.checkArgument(batchSize > 0, "batchSize must be greater than 0");    if (codecName == null) {        codeC = null;        compType = CompressionType.NONE;    } else {        codeC = getCodec(codecName);                compType = CompressionType.BLOCK;    }        if (fileType.equalsIgnoreCase(HDFSWriterFactory.DataStreamType) && codecName != null) {        throw new IllegalArgumentException("fileType: " + fileType + " which does NOT support compressed output. Please don't set codeC" + " or change the fileType if compressed output is desired.");    }    if (fileType.equalsIgnoreCase(HDFSWriterFactory.CompStreamType)) {        Preconditions.checkNotNull(codeC, "It's essential to set compress codec" + " when fileType is: " + fileType);    }        this.privExecutor = FlumeAuthenticationUtil.getAuthenticator(kerbConfPrincipal, kerbKeytab).proxyAs(proxyUser);    needRounding = context.getBoolean("hdfs.round", false);    if (needRounding) {        String unit = context.getString("hdfs.roundUnit", "second");        if (unit.equalsIgnoreCase("hour")) {            this.roundUnit = Calendar.HOUR_OF_DAY;        } else if (unit.equalsIgnoreCase("minute")) {            this.roundUnit = Calendar.MINUTE;        } else if (unit.equalsIgnoreCase("second")) {            this.roundUnit = Calendar.SECOND;        } else {                        needRounding = false;        }        this.roundValue = context.getInteger("hdfs.roundValue", 1);        if (roundUnit == Calendar.SECOND || roundUnit == Calendar.MINUTE) {            Preconditions.checkArgument(roundValue > 0 && roundValue <= 60, "Round value" + "must be > 0 and <= 60");        } else if (roundUnit == Calendar.HOUR_OF_DAY) {            Preconditions.checkArgument(roundValue > 0 && roundValue <= 24, "Round value" + "must be > 0 and <= 24");        }    }    this.useLocalTime = context.getBoolean("hdfs.useLocalTimeStamp", false);    if (useLocalTime) {        clock = new SystemClock();    }    if (sinkCounter == null) {        sinkCounter = new SinkCounter(getName());    }}
1
private static boolean codecMatches(Class<? extends CompressionCodec> cls, String codecName)
{    String simpleName = cls.getSimpleName();    if (cls.getName().equals(codecName) || simpleName.equalsIgnoreCase(codecName)) {        return true;    }    if (simpleName.endsWith("Codec")) {        String prefix = simpleName.substring(0, simpleName.length() - "Codec".length());        if (prefix.equalsIgnoreCase(codecName)) {            return true;        }    }    return false;}
0
 static CompressionCodec getCodec(String codecName)
{    Configuration conf = new Configuration();    List<Class<? extends CompressionCodec>> codecs = CompressionCodecFactory.getCodecClasses(conf);            CompressionCodec codec = null;    ArrayList<String> codecStrs = new ArrayList<String>();    codecStrs.add("None");    for (Class<? extends CompressionCodec> cls : codecs) {        codecStrs.add(cls.getSimpleName());        if (codecMatches(cls, codecName)) {            try {                codec = cls.newInstance();            } catch (InstantiationException e) {                            } catch (IllegalAccessException e) {                            }        }    }    if (codec == null) {        if (!codecName.equalsIgnoreCase("None")) {            throw new IllegalArgumentException("Unsupported compression codec " + codecName + ".  Please choose from: " + codecStrs);        }    } else if (codec instanceof org.apache.hadoop.conf.Configurable) {                                ((org.apache.hadoop.conf.Configurable) codec).setConf(conf);    }    return codec;}
1
public Status process() throws EventDeliveryException
{    Channel channel = getChannel();    Transaction transaction = channel.getTransaction();    transaction.begin();    try {        Set<BucketWriter> writers = new LinkedHashSet<>();        int txnEventCount = 0;        for (txnEventCount = 0; txnEventCount < batchSize; txnEventCount++) {            Event event = channel.take();            if (event == null) {                break;            }                        String realPath = BucketPath.escapeString(filePath, event.getHeaders(), timeZone, needRounding, roundUnit, roundValue, useLocalTime);            String realName = BucketPath.escapeString(fileName, event.getHeaders(), timeZone, needRounding, roundUnit, roundValue, useLocalTime);            String lookupPath = realPath + DIRECTORY_DELIMITER + realName;            BucketWriter bucketWriter;            HDFSWriter hdfsWriter = null;                                                WriterCallback closeCallback = new WriterCallback() {                @Override                public void run(String bucketPath) {                                        synchronized (sfWritersLock) {                        sfWriters.remove(bucketPath);                    }                }            };            synchronized (sfWritersLock) {                bucketWriter = sfWriters.get(lookupPath);                                if (bucketWriter == null) {                    hdfsWriter = writerFactory.getWriter(fileType);                    bucketWriter = initializeBucketWriter(realPath, realName, lookupPath, hdfsWriter, closeCallback);                    sfWriters.put(lookupPath, bucketWriter);                }            }                        try {                bucketWriter.append(event);            } catch (BucketClosedException ex) {                                hdfsWriter = writerFactory.getWriter(fileType);                bucketWriter = initializeBucketWriter(realPath, realName, lookupPath, hdfsWriter, closeCallback);                synchronized (sfWritersLock) {                    sfWriters.put(lookupPath, bucketWriter);                }                bucketWriter.append(event);            }                        if (!writers.contains(bucketWriter)) {                writers.add(bucketWriter);            }        }        if (txnEventCount == 0) {            sinkCounter.incrementBatchEmptyCount();        } else if (txnEventCount == batchSize) {            sinkCounter.incrementBatchCompleteCount();        } else {            sinkCounter.incrementBatchUnderflowCount();        }                for (BucketWriter bucketWriter : writers) {            bucketWriter.flush();        }        transaction.commit();        if (txnEventCount < 1) {            return Status.BACKOFF;        } else {            sinkCounter.addToEventDrainSuccessCount(txnEventCount);            return Status.READY;        }    } catch (IOException eIO) {        transaction.rollback();                sinkCounter.incrementEventWriteFail();        return Status.BACKOFF;    } catch (Throwable th) {        transaction.rollback();                sinkCounter.incrementEventWriteOrChannelFail(th);        if (th instanceof Error) {            throw (Error) th;        } else {            throw new EventDeliveryException(th);        }    } finally {        transaction.close();    }}
1
public void run(String bucketPath)
{        synchronized (sfWritersLock) {        sfWriters.remove(bucketPath);    }}
1
 BucketWriter initializeBucketWriter(String realPath, String realName, String lookupPath, HDFSWriter hdfsWriter, WriterCallback closeCallback)
{    HDFSWriter actualHdfsWriter = mockFs == null ? hdfsWriter : mockWriter;    BucketWriter bucketWriter = new BucketWriter(rollInterval, rollSize, rollCount, batchSize, context, realPath, realName, inUsePrefix, inUseSuffix, suffix, codeC, compType, actualHdfsWriter, timedRollerPool, privExecutor, sinkCounter, idleTimeout, closeCallback, lookupPath, callTimeout, callTimeoutPool, retryInterval, tryCount);    if (mockFs != null) {        bucketWriter.setFileSystem(mockFs);    }    return bucketWriter;}
0
public void stop()
{        synchronized (sfWritersLock) {        for (Entry<String, BucketWriter> entry : sfWriters.entrySet()) {                        try {                entry.getValue().close(false, true);            } catch (Exception ex) {                                if (ex instanceof InterruptedException) {                    Thread.currentThread().interrupt();                }            }        }    }        ExecutorService[] toShutdown = { callTimeoutPool, timedRollerPool };    for (ExecutorService execService : toShutdown) {        execService.shutdown();        try {            while (execService.isTerminated() == false) {                execService.awaitTermination(Math.max(defaultCallTimeout, callTimeout), TimeUnit.MILLISECONDS);            }        } catch (InterruptedException ex) {                    }    }    callTimeoutPool = null;    timedRollerPool = null;    synchronized (sfWritersLock) {        sfWriters.clear();        sfWriters = null;    }    sinkCounter.stop();    super.stop();}
1
public void start()
{    String timeoutName = "hdfs-" + getName() + "-call-runner-%d";    callTimeoutPool = Executors.newFixedThreadPool(threadsPoolSize, new ThreadFactoryBuilder().setNameFormat(timeoutName).build());    String rollerName = "hdfs-" + getName() + "-roll-timer-%d";    timedRollerPool = Executors.newScheduledThreadPool(rollTimerPoolSize, new ThreadFactoryBuilder().setNameFormat(rollerName).build());    this.sfWriters = new WriterLinkedHashMap(maxOpenFiles);    sinkCounter.start();    super.start();}
0
public String toString()
{    return "{ Sink type:" + getClass().getSimpleName() + ", name:" + getName() + " }";}
0
 void setBucketClock(Clock clock)
{    BucketPath.setClock(clock);}
0
 void setMockFs(FileSystem mockFs)
{    this.mockFs = mockFs;}
0
 void setMockWriter(HDFSWriter writer)
{    this.mockWriter = writer;}
0
 int getTryCount()
{    return tryCount;}
0
public long getBatchSize()
{    return batchSize;}
0
public void configure(Context context)
{    super.configure(context);        writeFormat = context.getString("hdfs.writeFormat", SequenceFileSerializerType.Writable.name());    useRawLocalFileSystem = context.getBoolean("hdfs.useRawLocalFileSystem", false);    serializerContext = new Context(context.getSubProperties(SequenceFileSerializerFactory.CTX_PREFIX));    serializer = SequenceFileSerializerFactory.getSerializer(writeFormat, serializerContext);    }
1
public void open(String filePath) throws IOException
{    open(filePath, null, CompressionType.NONE);}
0
public void open(String filePath, CompressionCodec codeC, CompressionType compType) throws IOException
{    Configuration conf = new Configuration();    Path dstPath = new Path(filePath);    FileSystem hdfs = dstPath.getFileSystem(conf);    open(dstPath, codeC, compType, conf, hdfs);}
0
protected void open(Path dstPath, CompressionCodec codeC, CompressionType compType, Configuration conf, FileSystem hdfs) throws IOException
{    if (useRawLocalFileSystem) {        if (hdfs instanceof LocalFileSystem) {            hdfs = ((LocalFileSystem) hdfs).getRaw();        } else {                    }    }    if (conf.getBoolean("hdfs.append.support", false) == true && hdfs.isFile(dstPath)) {        outStream = hdfs.append(dstPath);    } else {        outStream = hdfs.create(dstPath);    }    writer = SequenceFile.createWriter(conf, outStream, serializer.getKeyClass(), serializer.getValueClass(), compType, codeC);    registerCurrentStream(outStream, hdfs, dstPath);}
1
public void append(Event e) throws IOException
{    for (SequenceFileSerializer.Record record : serializer.serialize(e)) {        writer.append(record.getKey(), record.getValue());    }}
0
public void sync() throws IOException
{    writer.sync();    hflushOrSync(outStream);}
0
public void close() throws IOException
{    writer.close();    outStream.close();    unregisterCurrentStream();}
0
private Text makeText(Event e)
{    Text textObject = new Text();    textObject.set(e.getBody(), 0, e.getBody().length);    return textObject;}
0
public Class<LongWritable> getKeyClass()
{    return LongWritable.class;}
0
public Class<Text> getValueClass()
{    return Text.class;}
0
public Iterable<Record> serialize(Event e)
{    Object key = getKey(e);    Object value = getValue(e);    return Collections.singletonList(new Record(key, value));}
0
private Object getKey(Event e)
{        String timestamp = e.getHeaders().get("timestamp");    long eventStamp;    if (timestamp == null) {        eventStamp = System.currentTimeMillis();    } else {        eventStamp = Long.valueOf(timestamp);    }    return new LongWritable(eventStamp);}
0
private Object getValue(Event e)
{    return makeText(e);}
0
public SequenceFileSerializer build(Context context)
{    return new HDFSTextSerializer();}
0
private BytesWritable makeByteWritable(Event e)
{    BytesWritable bytesObject = new BytesWritable();    bytesObject.set(e.getBody(), 0, e.getBody().length);    return bytesObject;}
0
public Class<LongWritable> getKeyClass()
{    return LongWritable.class;}
0
public Class<BytesWritable> getValueClass()
{    return BytesWritable.class;}
0
public Iterable<Record> serialize(Event e)
{    Object key = getKey(e);    Object value = getValue(e);    return Collections.singletonList(new Record(key, value));}
0
private Object getKey(Event e)
{    String timestamp = e.getHeaders().get("timestamp");    long eventStamp;    if (timestamp == null) {        eventStamp = System.currentTimeMillis();    } else {        eventStamp = Long.valueOf(timestamp);    }    return new LongWritable(eventStamp);}
0
private Object getValue(Event e)
{    return makeByteWritable(e);}
0
public SequenceFileSerializer build(Context context)
{    return new HDFSWritableSerializer();}
0
public HDFSWriter getWriter(String fileType) throws IOException
{    if (fileType.equalsIgnoreCase(SequenceFileType)) {        return new HDFSSequenceFile();    } else if (fileType.equalsIgnoreCase(DataStreamType)) {        return new HDFSDataStream();    } else if (fileType.equalsIgnoreCase(CompStreamType)) {        return new HDFSCompressedDataStream();    } else {        throw new IOException("File type " + fileType + " not supported");    }}
0
public String getPrincipal()
{    return principal;}
0
public String getKeyTab()
{    return keyTab;}
0
public boolean equals(Object obj)
{    if (obj == null) {        return false;    }    if (getClass() != obj.getClass()) {        return false;    }    final KerberosUser other = (KerberosUser) obj;    if ((this.principal == null) ? (other.principal != null) : !this.principal.equals(other.principal)) {        return false;    }    if ((this.keyTab == null) ? (other.keyTab != null) : !this.keyTab.equals(other.keyTab)) {        return false;    }    return true;}
0
public int hashCode()
{    int hash = 7;    hash = 41 * hash + (this.principal != null ? this.principal.hashCode() : 0);    hash = 41 * hash + (this.keyTab != null ? this.keyTab.hashCode() : 0);    return hash;}
0
public String toString()
{    return "{ principal: " + principal + ", keytab: " + keyTab + " }";}
0
public Object getKey()
{    return key;}
0
public Object getValue()
{    return value;}
0
 static SequenceFileSerializer getSerializer(String formatType, Context context)
{    Preconditions.checkNotNull(formatType, "serialize type must not be null");        SequenceFileSerializerType type;    try {        type = SequenceFileSerializerType.valueOf(formatType);    } catch (IllegalArgumentException e) {                type = SequenceFileSerializerType.Other;    }    Class<? extends SequenceFileSerializer.Builder> builderClass = type.getBuilderClass();        if (builderClass == null) {        try {            Class c = Class.forName(formatType);            if (c != null && SequenceFileSerializer.Builder.class.isAssignableFrom(c)) {                builderClass = (Class<? extends SequenceFileSerializer.Builder>) c;            } else {                                return null;            }        } catch (ClassNotFoundException ex) {                        return null;        } catch (ClassCastException ex) {                        return null;        }    }        SequenceFileSerializer.Builder builder;    try {        builder = builderClass.newInstance();    } catch (InstantiationException ex) {                return null;    } catch (IllegalAccessException ex) {                return null;    }    return builder.build(context);}
1
public Class<? extends SequenceFileSerializer.Builder> getBuilderClass()
{    return builderClass;}
0
public void append(Event e) throws IOException
{    if (e.getHeaders().containsKey("fault")) {        throw new IOException("Injected fault");    } else if (e.getHeaders().containsKey("slow")) {        long waitTime = Long.parseLong(e.getHeaders().get("slow"));        try {            Thread.sleep(waitTime);        } catch (InterruptedException eT) {            throw new IOException("append interrupted", eT);        }    }    super.append(e);}
0
public void open(String filePath, CompressionCodec codeC, CompressionType compType) throws IOException
{    super.open(filePath, codeC, compType);    if (closed) {        opened = true;    }}
0
public void append(Event e) throws IOException
{    if (e.getHeaders().containsKey("fault")) {        throw new IOException("Injected fault");    } else if (e.getHeaders().containsKey("fault-once")) {        e.getHeaders().remove("fault-once");        throw new IOException("Injected fault");    } else if (e.getHeaders().containsKey("fault-until-reopen")) {                if (openCount == 1) {            throw new IOException("Injected fault-until-reopen");        }    } else if (e.getHeaders().containsKey("slow")) {        long waitTime = Long.parseLong(e.getHeaders().get("slow"));        try {            Thread.sleep(waitTime);        } catch (InterruptedException eT) {            throw new IOException("append interrupted", eT);        }    }    super.append(e);}
0
public void close() throws IOException
{    closed = true;    super.close();}
0
public HDFSWriter getWriter(String fileType) throws IOException
{    if (fileType == TestSequenceFileType) {        return new HDFSTestSeqWriter(openCount.incrementAndGet());    } else if (fileType == BadDataStreamType) {        return new HDFSBadDataStream();    } else {        throw new IOException("File type " + fileType + " not supported");    }}
0
protected FileSystem getDfs(Configuration conf, Path dstPath) throws IOException
{    return fs;}
0
public FSDataOutputStream append(Path arg0, int arg1, Progressable arg2) throws IOException
{    latestOutputStream = new MockFsDataOutputStream(fs.append(arg0, arg1, arg2), closeSucceed);    return latestOutputStream;}
0
public FSDataOutputStream create(Path arg0) throws IOException
{    latestOutputStream = new MockFsDataOutputStream(fs.create(arg0), closeSucceed);    return latestOutputStream;}
0
public FSDataOutputStream create(Path arg0, FsPermission arg1, boolean arg2, int arg3, short arg4, long arg5, Progressable arg6) throws IOException
{    throw new IOException("Not a real file system");}
0
public boolean delete(Path arg0) throws IOException
{    return fs.delete(arg0);}
0
public boolean delete(Path arg0, boolean arg1) throws IOException
{    return fs.delete(arg0, arg1);}
0
public FileStatus getFileStatus(Path arg0) throws IOException
{    return fs.getFileStatus(arg0);}
0
public URI getUri()
{    return fs.getUri();}
0
public Path getWorkingDirectory()
{    return fs.getWorkingDirectory();}
0
public FileStatus[] listStatus(Path arg0) throws IOException
{    return fs.listStatus(arg0);}
0
public boolean mkdirs(Path arg0, FsPermission arg1) throws IOException
{        return fs.mkdirs(arg0, arg1);}
0
public FSDataInputStream open(Path arg0, int arg1) throws IOException
{    return fs.open(arg0, arg1);}
0
public boolean rename(Path arg0, Path arg1) throws IOException
{    currentRenameAttempts++;        if (currentRenameAttempts >= numberOfRetriesRequired || numberOfRetriesRequired == 0) {                return fs.rename(arg0, arg1);    } else {        throw new IOException("MockIOException");    }}
1
public void setWorkingDirectory(Path arg0)
{    fs.setWorkingDirectory(arg0);}
0
public void close() throws IOException
{        if (closeSucceed) {                super.close();    } else {        throw new IOException("MockIOException");    }}
1
public int getFilesOpened()
{    return filesOpened;}
0
public int getFilesClosed()
{    return filesClosed;}
0
public int getBytesWritten()
{    return bytesWritten;}
0
public int getEventsWritten()
{    return eventsWritten;}
0
public String getOpenedFilePath()
{    return filePath;}
0
public void configure(Context context)
{}
0
public void open(String filePath) throws IOException
{    this.filePath = filePath;    filesOpened++;}
0
public void open(String filePath, CompressionCodec codec, CompressionType cType) throws IOException
{    this.filePath = filePath;    filesOpened++;}
0
public void append(Event e) throws IOException
{    eventsWritten++;    bytesWritten += e.getBody().length;}
0
public void sync() throws IOException
{}
0
public void close() throws IOException
{    filesClosed++;    int curr = currentCloseAttempts.incrementAndGet();        if (curr >= numberOfRetriesRequired || numberOfRetriesRequired == 0) {            } else {        throw new IOException("MockIOException");    }}
1
public boolean isUnderReplicated()
{    return false;}
0
public Class<LongWritable> getKeyClass()
{    return LongWritable.class;}
0
public Class<BytesWritable> getValueClass()
{    return BytesWritable.class;}
0
public Iterable<Record> serialize(Event e)
{    return Arrays.asList(new Record(new LongWritable(1234L), new BytesWritable(new byte[10])), new Record(new LongWritable(4567L), new BytesWritable(new byte[20])));}
0
public SequenceFileSerializer build(Context context)
{    return new MyCustomSerializer();}
0
public void setUp() throws Exception
{    file = File.createTempFile(getClass().getSimpleName(), "");}
0
public void tearDown() throws Exception
{    file.delete();}
0
public void testNoCompression() throws IOException
{    createAvroFile(file, null, false, false);    validateAvroFile(file);}
0
public void testNullCompression() throws IOException
{    createAvroFile(file, "null", false, false);    validateAvroFile(file);}
0
public void testDeflateCompression() throws IOException
{    createAvroFile(file, "deflate", false, false);    validateAvroFile(file);}
0
public void testSnappyCompression() throws IOException
{    createAvroFile(file, "snappy", false, false);    validateAvroFile(file);}
0
public void testSchemaUrl() throws IOException
{    createAvroFile(file, null, true, false);    validateAvroFile(file);}
0
public void testStaticSchemaUrl() throws IOException
{    createAvroFile(file, null, false, true);    validateAvroFile(file);}
0
public void testBothUrls() throws IOException
{    createAvroFile(file, null, true, true);    validateAvroFile(file);}
0
public void createAvroFile(File file, String codec, boolean useSchemaUrl, boolean useStaticSchemaUrl) throws IOException
{        OutputStream out = new FileOutputStream(file);    Context ctx = new Context();    if (codec != null) {        ctx.put("compressionCodec", codec);    }    Schema schema = Schema.createRecord("myrecord", null, null, false);    schema.setFields(Arrays.asList(new Schema.Field[] { new Schema.Field("message", Schema.create(Schema.Type.STRING), null, null) }));    GenericRecordBuilder recordBuilder = new GenericRecordBuilder(schema);    File schemaFile = null;    if (useSchemaUrl || useStaticSchemaUrl) {        schemaFile = File.createTempFile(getClass().getSimpleName(), ".avsc");        Files.write(schema.toString(), schemaFile, Charsets.UTF_8);    }    if (useStaticSchemaUrl) {        ctx.put(AvroEventSerializerConfigurationConstants.STATIC_SCHEMA_URL, schemaFile.toURI().toURL().toExternalForm());    }    EventSerializer.Builder builder = new AvroEventSerializer.Builder();    EventSerializer serializer = builder.build(ctx, out);    serializer.afterCreate();    for (int i = 0; i < 3; i++) {        GenericRecord record = recordBuilder.set("message", "Hello " + i).build();        Event event = EventBuilder.withBody(serializeAvro(record, schema));        if (schemaFile == null && !useSchemaUrl) {            event.getHeaders().put(AvroEventSerializer.AVRO_SCHEMA_LITERAL_HEADER, schema.toString());        } else if (useSchemaUrl) {            event.getHeaders().put(AvroEventSerializer.AVRO_SCHEMA_URL_HEADER, schemaFile.toURI().toURL().toExternalForm());        }        serializer.write(event);    }    serializer.flush();    serializer.beforeClose();    out.flush();    out.close();    if (schemaFile != null) {        schemaFile.delete();    }}
0
private byte[] serializeAvro(Object datum, Schema schema) throws IOException
{    ByteArrayOutputStream out = new ByteArrayOutputStream();    ReflectDatumWriter<Object> writer = new ReflectDatumWriter<Object>(schema);    BinaryEncoder encoder = EncoderFactory.get().binaryEncoder(out, null);    out.reset();    writer.write(datum, encoder);    encoder.flush();    return out.toByteArray();}
0
public void validateAvroFile(File file) throws IOException
{        DatumReader<GenericRecord> reader = new GenericDatumReader<GenericRecord>();    DataFileReader<GenericRecord> fileReader = new DataFileReader<GenericRecord>(file, reader);    GenericRecord record = new GenericData.Record(fileReader.getSchema());    int numEvents = 0;    while (fileReader.hasNext()) {        fileReader.next(record);        String bodyStr = record.get("message").toString();        System.out.println(bodyStr);        numEvents++;    }    fileReader.close();    Assert.assertEquals("Should have found a total of 3 events", 3, numEvents);}
0
public static void setup()
{    timedRollerPool = Executors.newSingleThreadScheduledExecutor();    proxy = FlumeAuthenticationUtil.getAuthenticator(null, null).proxyAs(null);}
0
public static void teardown() throws InterruptedException
{    timedRollerPool.shutdown();    timedRollerPool.awaitTermination(2, TimeUnit.SECONDS);    timedRollerPool.shutdownNow();}
0
public void testEventCountingRoller() throws IOException, InterruptedException
{    int maxEvents = 100;    MockHDFSWriter hdfsWriter = new MockHDFSWriter();    BucketWriter bucketWriter = new BucketWriterBuilder(hdfsWriter).setRollCount(maxEvents).build();    Event e = EventBuilder.withBody("foo", Charsets.UTF_8);    for (int i = 0; i < 1000; i++) {        bucketWriter.append(e);    }                Assert.assertEquals("events written", 1000, hdfsWriter.getEventsWritten());    Assert.assertEquals("bytes written", 3000, hdfsWriter.getBytesWritten());    Assert.assertEquals("files opened", 10, hdfsWriter.getFilesOpened());}
1
public void testSizeRoller() throws IOException, InterruptedException
{    int maxBytes = 300;    MockHDFSWriter hdfsWriter = new MockHDFSWriter();    BucketWriter bucketWriter = new BucketWriterBuilder(hdfsWriter).setRollSize(maxBytes).build();    Event e = EventBuilder.withBody("foo", Charsets.UTF_8);    for (int i = 0; i < 1000; i++) {        bucketWriter.append(e);    }                Assert.assertEquals("events written", 1000, hdfsWriter.getEventsWritten());    Assert.assertEquals("bytes written", 3000, hdfsWriter.getBytesWritten());    Assert.assertEquals("files opened", 10, hdfsWriter.getFilesOpened());}
1
public void testIntervalRoller() throws IOException, InterruptedException
{        final int ROLL_INTERVAL = 1;    final int NUM_EVENTS = 10;    final AtomicBoolean calledBack = new AtomicBoolean(false);    MockHDFSWriter hdfsWriter = new MockHDFSWriter();    BucketWriter bucketWriter = new BucketWriterBuilder(hdfsWriter).setRollInterval(ROLL_INTERVAL).setOnCloseCallback(new HDFSEventSink.WriterCallback() {        @Override        public void run(String filePath) {            calledBack.set(true);        }    }).build();    Event e = EventBuilder.withBody("foo", Charsets.UTF_8);    long startNanos = System.nanoTime();    for (int i = 0; i < NUM_EVENTS - 1; i++) {        bucketWriter.append(e);    }        Thread.sleep(2 * ROLL_INTERVAL * 1000L);    Assert.assertTrue(bucketWriter.closed.get());    Assert.assertTrue(calledBack.get());    bucketWriter = new BucketWriterBuilder(hdfsWriter).setRollInterval(ROLL_INTERVAL).build();        bucketWriter.append(e);    long elapsedMillis = TimeUnit.MILLISECONDS.convert(System.nanoTime() - startNanos, TimeUnit.NANOSECONDS);    long elapsedSeconds = elapsedMillis / 1000L;                        Assert.assertEquals("events written", NUM_EVENTS, hdfsWriter.getEventsWritten());    Assert.assertEquals("bytes written", e.getBody().length * NUM_EVENTS, hdfsWriter.getBytesWritten());    Assert.assertEquals("files opened", 2, hdfsWriter.getFilesOpened());        Assert.assertEquals("files closed", 1, hdfsWriter.getFilesClosed());        Thread.sleep(2 * ROLL_INTERVAL * 1000L);        Assert.assertEquals("files closed", 2, hdfsWriter.getFilesClosed());}
1
public void run(String filePath)
{    calledBack.set(true);}
0
public void testIntervalRollerBug() throws IOException, InterruptedException
{        final int ROLL_INTERVAL = 1;    final int NUM_EVENTS = 10;    HDFSWriter hdfsWriter = new HDFSWriter() {        private volatile boolean open = false;        public void configure(Context context) {        }        public void sync() throws IOException {            if (!open) {                throw new IOException("closed");            }        }        public void open(String filePath, CompressionCodec codec, CompressionType cType) throws IOException {            open = true;        }        public void open(String filePath) throws IOException {            open = true;        }        public void close() throws IOException {            open = false;        }        @Override        public boolean isUnderReplicated() {            return false;        }        public void append(Event e) throws IOException {                        open = true;        }    };    File tmpFile = File.createTempFile("flume", "test");    tmpFile.deleteOnExit();    String path = tmpFile.getParent();    String name = tmpFile.getName();    BucketWriter bucketWriter = new BucketWriterBuilder(hdfsWriter).setRollInterval(ROLL_INTERVAL).setFilePath(path).setFileName(name).build();    Event e = EventBuilder.withBody("foo", Charsets.UTF_8);    for (int i = 0; i < NUM_EVENTS - 1; i++) {        bucketWriter.append(e);    }        Thread.sleep(2 * ROLL_INTERVAL * 1000L);        bucketWriter.flush();}
0
public void configure(Context context)
{}
0
public void sync() throws IOException
{    if (!open) {        throw new IOException("closed");    }}
0
public void open(String filePath, CompressionCodec codec, CompressionType cType) throws IOException
{    open = true;}
0
public void open(String filePath) throws IOException
{    open = true;}
0
public void close() throws IOException
{    open = false;}
0
public boolean isUnderReplicated()
{    return false;}
0
public void append(Event e) throws IOException
{        open = true;}
0
public void testFileSuffixNotGiven() throws IOException, InterruptedException
{        final int ROLL_INTERVAL = 1000;    final String suffix = null;        final long testTime = System.currentTimeMillis();    Clock testClock = new Clock() {        public long currentTimeMillis() {            return testTime;        }    };    MockHDFSWriter hdfsWriter = new MockHDFSWriter();    BucketWriter bucketWriter = new BucketWriterBuilder(hdfsWriter).setRollInterval(ROLL_INTERVAL).setFileSuffix(suffix).setClock(testClock).build();    Event e = EventBuilder.withBody("foo", Charsets.UTF_8);    bucketWriter.append(e);    Assert.assertTrue("Incorrect suffix", hdfsWriter.getOpenedFilePath().endsWith(Long.toString(testTime + 1) + ".tmp"));}
0
public long currentTimeMillis()
{    return testTime;}
0
public void testFileSuffixGiven() throws IOException, InterruptedException
{        final int ROLL_INTERVAL = 1000;    final String suffix = ".avro";        final long testTime = System.currentTimeMillis();    Clock testClock = new Clock() {        public long currentTimeMillis() {            return testTime;        }    };    MockHDFSWriter hdfsWriter = new MockHDFSWriter();    BucketWriter bucketWriter = new BucketWriterBuilder(hdfsWriter).setRollInterval(ROLL_INTERVAL).setFileSuffix(suffix).setClock(testClock).build();    Event e = EventBuilder.withBody("foo", Charsets.UTF_8);    bucketWriter.append(e);    Assert.assertTrue("Incorrect suffix", hdfsWriter.getOpenedFilePath().endsWith(Long.toString(testTime + 1) + suffix + ".tmp"));}
0
public long currentTimeMillis()
{    return testTime;}
0
public void testFileSuffixCompressed() throws IOException, InterruptedException
{        final int ROLL_INTERVAL = 1000;    final String suffix = ".foo";    MockHDFSWriter hdfsWriter = new MockHDFSWriter();        final long testTime = System.currentTimeMillis();    Clock testClock = new Clock() {        public long currentTimeMillis() {            return testTime;        }    };    BucketWriter bucketWriter = new BucketWriterBuilder(hdfsWriter).setRollInterval(ROLL_INTERVAL).setFileSuffix(suffix).setCodeC(HDFSEventSink.getCodec("gzip")).setCompType(SequenceFile.CompressionType.BLOCK).setClock(testClock).build();    Event e = EventBuilder.withBody("foo", Charsets.UTF_8);    bucketWriter.append(e);    Assert.assertTrue("Incorrect suffix", hdfsWriter.getOpenedFilePath().endsWith(Long.toString(testTime + 1) + suffix + ".tmp"));}
0
public long currentTimeMillis()
{    return testTime;}
0
public void testInUsePrefix() throws IOException, InterruptedException
{        final int ROLL_INTERVAL = 1000;    final String PREFIX = "BRNO_IS_CITY_IN_CZECH_REPUBLIC";    MockHDFSWriter hdfsWriter = new MockHDFSWriter();    HDFSTextSerializer formatter = new HDFSTextSerializer();    BucketWriter bucketWriter = new BucketWriterBuilder(hdfsWriter).setRollInterval(ROLL_INTERVAL).setInUsePrefix(PREFIX).build();    Event e = EventBuilder.withBody("foo", Charsets.UTF_8);    bucketWriter.append(e);    Assert.assertTrue("Incorrect in use prefix", hdfsWriter.getOpenedFilePath().contains(PREFIX));}
0
public void testInUseSuffix() throws IOException, InterruptedException
{        final int ROLL_INTERVAL = 1000;    final String SUFFIX = "WELCOME_TO_THE_HELLMOUNTH";    MockHDFSWriter hdfsWriter = new MockHDFSWriter();    HDFSTextSerializer serializer = new HDFSTextSerializer();    BucketWriter bucketWriter = new BucketWriterBuilder(hdfsWriter).setRollInterval(ROLL_INTERVAL).setInUseSuffix(SUFFIX).build();    Event e = EventBuilder.withBody("foo", Charsets.UTF_8);    bucketWriter.append(e);    Assert.assertTrue("Incorrect in use suffix", hdfsWriter.getOpenedFilePath().contains(SUFFIX));}
0
public void testCallbackOnClose() throws IOException, InterruptedException
{        final int ROLL_INTERVAL = 1000;    final String SUFFIX = "WELCOME_TO_THE_EREBOR";    final AtomicBoolean callbackCalled = new AtomicBoolean(false);    BucketWriter bucketWriter = new BucketWriterBuilder().setRollInterval(ROLL_INTERVAL).setInUseSuffix(SUFFIX).setOnCloseCallback(new HDFSEventSink.WriterCallback() {        @Override        public void run(String filePath) {            callbackCalled.set(true);        }    }).setOnCloseCallbackPath("blah").build();    Event e = EventBuilder.withBody("foo", Charsets.UTF_8);    bucketWriter.append(e);    bucketWriter.close(true);    Assert.assertTrue(callbackCalled.get());}
0
public void run(String filePath)
{    callbackCalled.set(true);}
0
public void testSequenceFileRenameRetries() throws Exception
{    sequenceFileRenameRetryCoreTest(1, true);    sequenceFileRenameRetryCoreTest(5, true);    sequenceFileRenameRetryCoreTest(2, true);    sequenceFileRenameRetryCoreTest(1, false);    sequenceFileRenameRetryCoreTest(5, false);    sequenceFileRenameRetryCoreTest(2, false);}
0
public void testSequenceFileCloseRetries() throws Exception
{    sequenceFileCloseRetryCoreTest(5);    sequenceFileCloseRetryCoreTest(1);}
0
public void sequenceFileRenameRetryCoreTest(int numberOfRetriesRequired, boolean closeSucceed) throws Exception
{    String hdfsPath = "file:///tmp/flume-test." + Calendar.getInstance().getTimeInMillis() + "." + Thread.currentThread().getId();    Context context = new Context();    Configuration conf = new Configuration();    FileSystem fs = FileSystem.get(conf);    Path dirPath = new Path(hdfsPath);    fs.delete(dirPath, true);    fs.mkdirs(dirPath);    context.put("hdfs.path", hdfsPath);    context.put("hdfs.closeTries", String.valueOf(numberOfRetriesRequired));    context.put("hdfs.rollCount", "1");    context.put("hdfs.retryInterval", "1");    context.put("hdfs.callTimeout", Long.toString(1000));    MockFileSystem mockFs = new MockFileSystem(fs, numberOfRetriesRequired, closeSucceed);    MockDataStream writer = new MockDataStream(mockFs);    BucketWriter bucketWriter = new BucketWriterBuilder(writer).setRollCount(1).setBatchSize(1).setFilePath(hdfsPath).setFileName(hdfsPath).setInUsePrefix("singleBucket").setCompType(null).setRetryInterval(1).setMaxCloseTries(numberOfRetriesRequired).setWriter(writer).build();    bucketWriter.setFileSystem(mockFs);            Event event = EventBuilder.withBody("test", Charsets.UTF_8);    bucketWriter.append(event);        bucketWriter.append(event);    TimeUnit.SECONDS.sleep(numberOfRetriesRequired + 2);    Assert.assertTrue("Expected " + numberOfRetriesRequired + " " + "but got " + bucketWriter.renameTries.get(), bucketWriter.renameTries.get() == numberOfRetriesRequired);}
0
private void sequenceFileCloseRetryCoreTest(int numberOfRetriesRequired) throws Exception
{    String hdfsPath = "file:///tmp/flume-test." + Calendar.getInstance().getTimeInMillis() + "." + Thread.currentThread().getId();    Context context = new Context();    Configuration conf = new Configuration();    FileSystem fs = FileSystem.get(conf);    Path dirPath = new Path(hdfsPath);    fs.delete(dirPath, true);    fs.mkdirs(dirPath);    context.put("hdfs.path", hdfsPath);    context.put("hdfs.closeTries", String.valueOf(numberOfRetriesRequired));    context.put("hdfs.rollCount", "1");    context.put("hdfs.retryInterval", "1");    context.put("hdfs.callTimeout", Long.toString(1000));    MockHDFSWriter mockHDFSWriter = new MockHDFSWriter(Integer.MAX_VALUE);    ExecutorService executorService = Executors.newSingleThreadExecutor();    BucketWriter bucketWriter = new BucketWriter(0, 0, 1, 1, ctx, hdfsPath, hdfsPath, "singleBucket", ".tmp", null, null, null, mockHDFSWriter, timedRollerPool, proxy, new SinkCounter("test-bucket-writer-" + System.currentTimeMillis()), 0, null, null, 30000, executorService, 1, numberOfRetriesRequired);    Event event = EventBuilder.withBody("test", Charsets.UTF_8);    bucketWriter.append(event);    bucketWriter.close(false);    TimeUnit.SECONDS.sleep(numberOfRetriesRequired + 2);    Assert.assertEquals("ExcceutorService should be empty", executorService.shutdownNow().size(), 0);    Assert.assertEquals("Expected " + numberOfRetriesRequired + " " + "but got " + mockHDFSWriter.currentCloseAttempts, mockHDFSWriter.currentCloseAttempts.get(), numberOfRetriesRequired);}
0
public void testRotateBucketOnIOException() throws IOException, InterruptedException
{    MockHDFSWriter hdfsWriter = Mockito.spy(new MockHDFSWriter());    PrivilegedExecutor ugiProxy = FlumeAuthenticationUtil.getAuthenticator(null, null).proxyAs("alice");        final int ROLL_COUNT = 1;    BucketWriter bucketWriter = new BucketWriterBuilder(hdfsWriter).setProxyUser(ugiProxy).setRollCount(ROLL_COUNT).build();    Event e = EventBuilder.withBody("foo", Charsets.UTF_8);        bucketWriter.append(e);        IOException expectedIOException = new IOException("Test injected IOException");    Mockito.doThrow(expectedIOException).when(hdfsWriter).append(Mockito.any(Event.class));        try {        bucketWriter.append(e);        Assert.fail("Expected IOException wasn't thrown during append");    } catch (IOException ex) {        Assert.assertEquals(expectedIOException, ex);            }        try {        bucketWriter.append(e);        Assert.fail("BucketWriter should be already closed, BucketClosedException expected");    } catch (BucketClosedException ex) {            }    Assert.assertEquals("events written", 1, hdfsWriter.getEventsWritten());    Assert.assertEquals("2 files should be closed", 2, hdfsWriter.getFilesClosed());}
1
public BucketWriterBuilder setRollInterval(long rollInterval)
{    this.rollInterval = rollInterval;    return this;}
0
public BucketWriterBuilder setRollSize(long rollSize)
{    this.rollSize = rollSize;    return this;}
0
public BucketWriterBuilder setRollCount(long rollCount)
{    this.rollCount = rollCount;    return this;}
0
public BucketWriterBuilder setBatchSize(long batchSize)
{    this.batchSize = batchSize;    return this;}
0
public BucketWriterBuilder setContext(Context context)
{    this.context = context;    return this;}
0
public BucketWriterBuilder setFilePath(String filePath)
{    this.filePath = filePath;    return this;}
0
public BucketWriterBuilder setFileName(String fileName)
{    this.fileName = fileName;    return this;}
0
public BucketWriterBuilder setInUsePrefix(String inUsePrefix)
{    this.inUsePrefix = inUsePrefix;    return this;}
0
public BucketWriterBuilder setInUseSuffix(String inUseSuffix)
{    this.inUseSuffix = inUseSuffix;    return this;}
0
public BucketWriterBuilder setFileSuffix(String fileSuffix)
{    this.fileSuffix = fileSuffix;    return this;}
0
public BucketWriterBuilder setCodeC(CompressionCodec codeC)
{    this.codeC = codeC;    return this;}
0
public BucketWriterBuilder setCompType(CompressionType compType)
{    this.compType = compType;    return this;}
0
public BucketWriterBuilder setTimedRollerPool(ScheduledExecutorService timedRollerPool)
{    this.timedRollerPool = timedRollerPool;    return this;}
0
public BucketWriterBuilder setProxyUser(PrivilegedExecutor proxyUser)
{    this.proxyUser = proxyUser;    return this;}
0
public BucketWriterBuilder setSinkCounter(SinkCounter sinkCounter)
{    this.sinkCounter = sinkCounter;    return this;}
0
public BucketWriterBuilder setIdleTimeout(int idleTimeout)
{    this.idleTimeout = idleTimeout;    return this;}
0
public BucketWriterBuilder setOnCloseCallback(WriterCallback onCloseCallback)
{    this.onCloseCallback = onCloseCallback;    return this;}
0
public BucketWriterBuilder setOnCloseCallbackPath(String onCloseCallbackPath)
{    this.onCloseCallbackPath = onCloseCallbackPath;    return this;}
0
public BucketWriterBuilder setCallTimeout(long callTimeout)
{    this.callTimeout = callTimeout;    return this;}
0
public BucketWriterBuilder setCallTimeoutPool(ExecutorService callTimeoutPool)
{    this.callTimeoutPool = callTimeoutPool;    return this;}
0
public BucketWriterBuilder setRetryInterval(long retryInterval)
{    this.retryInterval = retryInterval;    return this;}
0
public BucketWriterBuilder setMaxCloseTries(int maxCloseTries)
{    this.maxCloseTries = maxCloseTries;    return this;}
0
public BucketWriterBuilder setWriter(HDFSWriter writer)
{    this.writer = writer;    return this;}
0
public BucketWriterBuilder setClock(Clock clock)
{    this.clock = clock;    return this;}
0
public BucketWriter build()
{    if (clock == null) {        clock = new SystemClock();    }    if (writer == null) {        writer = new MockHDFSWriter();    }    return new BucketWriter(rollInterval, rollSize, rollCount, batchSize, context, filePath, fileName, inUsePrefix, inUseSuffix, fileSuffix, codeC, compType, writer, timedRollerPool, proxyUser, sinkCounter, idleTimeout, onCloseCallback, onCloseCallbackPath, callTimeout, callTimeoutPool, retryInterval, maxCloseTries, clock);}
0
public void init() throws Exception
{    this.file = new File("target/test/data/foo.gz");    this.fileURI = file.getAbsoluteFile().toURI().toString();        Configuration conf = new Configuration();        conf.set("fs.file.impl", "org.apache.hadoop.fs.RawLocalFileSystem");    Path path = new Path(fileURI);        path.getFileSystem(conf);    this.factory = new CompressionCodecFactory(conf);}
1
public void testGzipDurability() throws Exception
{    Context context = new Context();    HDFSCompressedDataStream writer = new HDFSCompressedDataStream();    writer.configure(context);    writer.open(fileURI, factory.getCodec(new Path(fileURI)), SequenceFile.CompressionType.BLOCK);    String[] bodies = { "yarf!" };    writeBodies(writer, bodies);    byte[] buf = new byte[256];    GZIPInputStream cmpIn = new GZIPInputStream(new FileInputStream(file));    int len = cmpIn.read(buf);    String result = new String(buf, 0, len, Charsets.UTF_8);        result = result.trim();    Assert.assertEquals("input and output must match", bodies[0], result);}
0
public void testGzipDurabilityWithSerializer() throws Exception
{    Context context = new Context();    context.put("serializer", "AVRO_EVENT");    HDFSCompressedDataStream writer = new HDFSCompressedDataStream();    writer.configure(context);    writer.open(fileURI, factory.getCodec(new Path(fileURI)), SequenceFile.CompressionType.BLOCK);    String[] bodies = { "yarf!", "yarfing!" };    writeBodies(writer, bodies);    int found = 0;    int expected = bodies.length;    List<String> expectedBodies = Lists.newArrayList(bodies);    GZIPInputStream cmpIn = new GZIPInputStream(new FileInputStream(file));    DatumReader<GenericRecord> reader = new GenericDatumReader<GenericRecord>();    DataFileStream<GenericRecord> avroStream = new DataFileStream<GenericRecord>(cmpIn, reader);    GenericRecord record = new GenericData.Record(avroStream.getSchema());    while (avroStream.hasNext()) {        avroStream.next(record);        CharsetDecoder decoder = Charsets.UTF_8.newDecoder();        String bodyStr = decoder.decode((ByteBuffer) record.get("body")).toString();        expectedBodies.remove(bodyStr);        found++;    }    avroStream.close();    cmpIn.close();    Assert.assertTrue("Found = " + found + ", Expected = " + expected + ", Left = " + expectedBodies.size() + " " + expectedBodies, expectedBodies.size() == 0);}
0
private void writeBodies(HDFSCompressedDataStream writer, String... bodies) throws Exception
{    for (String body : bodies) {        Event evt = EventBuilder.withBody(body, Charsets.UTF_8);        writer.append(evt);    }    writer.sync();}
0
private void dirCleanup()
{    Configuration conf = new Configuration();    try {        FileSystem fs = FileSystem.get(conf);        Path dirPath = new Path(testPath);        if (fs.exists(dirPath)) {            fs.delete(dirPath, true);        }    } catch (IOException eIO) {            }}
1
public void setUp()
{        /*     * FIXME: Use a dynamic path to support concurrent test execution. Also,     * beware of the case where this path is used for something or when the     * Hadoop config points at file:/     * better way of testing HDFS related functionality.     */    testPath = "file:///tmp/flume-test." + Calendar.getInstance().getTimeInMillis() + "." + Thread.currentThread().getId();    sink = new HDFSEventSink();    sink.setName("HDFSEventSink-" + UUID.randomUUID().toString());    dirCleanup();}
1
public void tearDown()
{    if (System.getenv("hdfs_keepFiles") == null)        dirCleanup();}
0
public void testTextBatchAppend() throws Exception
{    doTestTextBatchAppend(false);}
0
public void testTextBatchAppendRawFS() throws Exception
{    doTestTextBatchAppend(true);}
0
public void doTestTextBatchAppend(boolean useRawLocalFileSystem) throws Exception
{        final long rollCount = 10;    final long batchSize = 2;    final String fileName = "FlumeData";    String newPath = testPath + "/singleTextBucket";    int totalEvents = 0;    int i = 1, j = 1;        Configuration conf = new Configuration();    FileSystem fs = FileSystem.get(conf);    Path dirPath = new Path(newPath);    fs.delete(dirPath, true);    fs.mkdirs(dirPath);    Context context = new Context();        context.put("hdfs.path", newPath);    context.put("hdfs.filePrefix", fileName);    context.put("hdfs.rollCount", String.valueOf(rollCount));    context.put("hdfs.rollInterval", "0");    context.put("hdfs.rollSize", "0");    context.put("hdfs.batchSize", String.valueOf(batchSize));    context.put("hdfs.writeFormat", "Text");    context.put("hdfs.useRawLocalFileSystem", Boolean.toString(useRawLocalFileSystem));    context.put("hdfs.fileType", "DataStream");    Configurables.configure(sink, context);    Channel channel = new MemoryChannel();    Configurables.configure(channel, context);    sink.setChannel(channel);    sink.start();    Calendar eventDate = Calendar.getInstance();    List<String> bodies = Lists.newArrayList();        for (i = 1; i <= (rollCount * 10) / batchSize; i++) {        Transaction txn = channel.getTransaction();        txn.begin();        for (j = 1; j <= batchSize; j++) {            Event event = new SimpleEvent();            eventDate.clear();                        eventDate.set(2011, i, i, i, 0);            String body = "Test." + i + "." + j;            event.setBody(body.getBytes());            bodies.add(body);            channel.put(event);            totalEvents++;        }        txn.commit();        txn.close();                sink.process();    }    sink.stop();        FileStatus[] dirStat = fs.listStatus(dirPath);    Path[] fList = FileUtil.stat2Paths(dirStat);        long expectedFiles = totalEvents / rollCount;    if (totalEvents % rollCount > 0)        expectedFiles++;    Assert.assertEquals("num files wrong, found: " + Lists.newArrayList(fList), expectedFiles, fList.length);        verifyOutputTextFiles(fs, conf, dirPath.toUri().getPath(), fileName, bodies);}
1
public void testLifecycle() throws InterruptedException, LifecycleException
{        Context context = new Context();    context.put("hdfs.path", testPath);    /*     * context.put("hdfs.rollInterval", String.class);     * context.get("hdfs.rollSize", String.class); context.get("hdfs.rollCount",     * String.class);     */    Configurables.configure(sink, context);    sink.setChannel(new MemoryChannel());    sink.start();    sink.stop();}
1
public void testEmptyChannelResultsInStatusBackoff() throws InterruptedException, LifecycleException, EventDeliveryException
{        Context context = new Context();    Channel channel = new MemoryChannel();    context.put("hdfs.path", testPath);    context.put("keep-alive", "0");    Configurables.configure(sink, context);    Configurables.configure(channel, context);    sink.setChannel(channel);    sink.start();    Assert.assertEquals(Status.BACKOFF, sink.process());    sink.stop();}
1
public void testKerbFileAccess() throws InterruptedException, LifecycleException, EventDeliveryException, IOException
{        final String fileName = "FlumeData";    final long rollCount = 5;    final long batchSize = 2;    String newPath = testPath + "/singleBucket";    String kerbConfPrincipal = "user1/localhost@EXAMPLE.COM";    String kerbKeytab = "/usr/lib/flume/nonexistkeytabfile";        Configuration conf = new Configuration();    conf.set(CommonConfigurationKeys.HADOOP_SECURITY_AUTHENTICATION, "kerberos");    UserGroupInformation.setConfiguration(conf);    Context context = new Context();    context.put("hdfs.path", newPath);    context.put("hdfs.filePrefix", fileName);    context.put("hdfs.rollCount", String.valueOf(rollCount));    context.put("hdfs.batchSize", String.valueOf(batchSize));    context.put("hdfs.kerberosPrincipal", kerbConfPrincipal);    context.put("hdfs.kerberosKeytab", kerbKeytab);    try {        Configurables.configure(sink, context);        Assert.fail("no exception thrown");    } catch (IllegalArgumentException expected) {        Assert.assertTrue(expected.getMessage().contains("Keytab is not a readable file"));    } finally {                conf.set(CommonConfigurationKeys.HADOOP_SECURITY_AUTHENTICATION, "simple");        UserGroupInformation.setConfiguration(conf);    }}
1
public void testTextAppend() throws InterruptedException, LifecycleException, EventDeliveryException, IOException
{        final long rollCount = 3;    final long batchSize = 2;    final String fileName = "FlumeData";    String newPath = testPath + "/singleTextBucket";    int totalEvents = 0;    int i = 1, j = 1;        Configuration conf = new Configuration();    FileSystem fs = FileSystem.get(conf);    Path dirPath = new Path(newPath);    fs.delete(dirPath, true);    fs.mkdirs(dirPath);    Context context = new Context();        context.put("hdfs.path", newPath);    context.put("hdfs.filePrefix", fileName);    context.put("hdfs.rollCount", String.valueOf(rollCount));    context.put("hdfs.batchSize", String.valueOf(batchSize));    context.put("hdfs.writeFormat", "Text");    context.put("hdfs.fileType", "DataStream");    Configurables.configure(sink, context);    Channel channel = new MemoryChannel();    Configurables.configure(channel, context);    sink.setChannel(channel);    sink.start();    Calendar eventDate = Calendar.getInstance();    List<String> bodies = Lists.newArrayList();        for (i = 1; i < 4; i++) {        Transaction txn = channel.getTransaction();        txn.begin();        for (j = 1; j <= batchSize; j++) {            Event event = new SimpleEvent();            eventDate.clear();                        eventDate.set(2011, i, i, i, 0);            event.getHeaders().put("timestamp", String.valueOf(eventDate.getTimeInMillis()));            event.getHeaders().put("hostname", "Host" + i);            String body = "Test." + i + "." + j;            event.setBody(body.getBytes());            bodies.add(body);            channel.put(event);            totalEvents++;        }        txn.commit();        txn.close();                sink.process();    }    sink.stop();        FileStatus[] dirStat = fs.listStatus(dirPath);    Path[] fList = FileUtil.stat2Paths(dirStat);        long expectedFiles = totalEvents / rollCount;    if (totalEvents % rollCount > 0)        expectedFiles++;    Assert.assertEquals("num files wrong, found: " + Lists.newArrayList(fList), expectedFiles, fList.length);    verifyOutputTextFiles(fs, conf, dirPath.toUri().getPath(), fileName, bodies);}
1
public void testAvroAppend() throws InterruptedException, LifecycleException, EventDeliveryException, IOException
{        final long rollCount = 3;    final long batchSize = 2;    final String fileName = "FlumeData";    String newPath = testPath + "/singleTextBucket";    int totalEvents = 0;    int i = 1, j = 1;        Configuration conf = new Configuration();    FileSystem fs = FileSystem.get(conf);    Path dirPath = new Path(newPath);    fs.delete(dirPath, true);    fs.mkdirs(dirPath);    Context context = new Context();        context.put("hdfs.path", newPath);    context.put("hdfs.filePrefix", fileName);    context.put("hdfs.rollCount", String.valueOf(rollCount));    context.put("hdfs.batchSize", String.valueOf(batchSize));    context.put("hdfs.writeFormat", "Text");    context.put("hdfs.fileType", "DataStream");    context.put("serializer", "AVRO_EVENT");    Configurables.configure(sink, context);    Channel channel = new MemoryChannel();    Configurables.configure(channel, context);    sink.setChannel(channel);    sink.start();    Calendar eventDate = Calendar.getInstance();    List<String> bodies = Lists.newArrayList();        for (i = 1; i < 4; i++) {        Transaction txn = channel.getTransaction();        txn.begin();        for (j = 1; j <= batchSize; j++) {            Event event = new SimpleEvent();            eventDate.clear();                        eventDate.set(2011, i, i, i, 0);            event.getHeaders().put("timestamp", String.valueOf(eventDate.getTimeInMillis()));            event.getHeaders().put("hostname", "Host" + i);            String body = "Test." + i + "." + j;            event.setBody(body.getBytes());            bodies.add(body);            channel.put(event);            totalEvents++;        }        txn.commit();        txn.close();                sink.process();    }    sink.stop();        FileStatus[] dirStat = fs.listStatus(dirPath);    Path[] fList = FileUtil.stat2Paths(dirStat);        long expectedFiles = totalEvents / rollCount;    if (totalEvents % rollCount > 0)        expectedFiles++;    Assert.assertEquals("num files wrong, found: " + Lists.newArrayList(fList), expectedFiles, fList.length);    verifyOutputAvroFiles(fs, conf, dirPath.toUri().getPath(), fileName, bodies);}
1
public void testSimpleAppend() throws InterruptedException, LifecycleException, EventDeliveryException, IOException
{        final String fileName = "FlumeData";    final long rollCount = 5;    final long batchSize = 2;    final int numBatches = 4;    String newPath = testPath + "/singleBucket";    int totalEvents = 0;    int i = 1, j = 1;        Configuration conf = new Configuration();    FileSystem fs = FileSystem.get(conf);    Path dirPath = new Path(newPath);    fs.delete(dirPath, true);    fs.mkdirs(dirPath);    Context context = new Context();    context.put("hdfs.path", newPath);    context.put("hdfs.filePrefix", fileName);    context.put("hdfs.rollCount", String.valueOf(rollCount));    context.put("hdfs.batchSize", String.valueOf(batchSize));    Configurables.configure(sink, context);    Channel channel = new MemoryChannel();    Configurables.configure(channel, context);    sink.setChannel(channel);    sink.start();    Calendar eventDate = Calendar.getInstance();    List<String> bodies = Lists.newArrayList();        for (i = 1; i < numBatches; i++) {        Transaction txn = channel.getTransaction();        txn.begin();        for (j = 1; j <= batchSize; j++) {            Event event = new SimpleEvent();            eventDate.clear();                        eventDate.set(2011, i, i, i, 0);            event.getHeaders().put("timestamp", String.valueOf(eventDate.getTimeInMillis()));            event.getHeaders().put("hostname", "Host" + i);            String body = "Test." + i + "." + j;            event.setBody(body.getBytes());            bodies.add(body);            channel.put(event);            totalEvents++;        }        txn.commit();        txn.close();                sink.process();    }    sink.stop();        FileStatus[] dirStat = fs.listStatus(dirPath);    Path[] fList = FileUtil.stat2Paths(dirStat);        long expectedFiles = totalEvents / rollCount;    if (totalEvents % rollCount > 0)        expectedFiles++;    Assert.assertEquals("num files wrong, found: " + Lists.newArrayList(fList), expectedFiles, fList.length);    verifyOutputSequenceFiles(fs, conf, dirPath.toUri().getPath(), fileName, bodies);}
1
public void testSimpleAppendLocalTime() throws InterruptedException, LifecycleException, EventDeliveryException, IOException
{    final long currentTime = System.currentTimeMillis();    Clock clk = new Clock() {        @Override        public long currentTimeMillis() {            return currentTime;        }    };        final String fileName = "FlumeData";    final long rollCount = 5;    final long batchSize = 2;    final int numBatches = 4;    String newPath = testPath + "/singleBucket/%s";    String expectedPath = testPath + "/singleBucket/" + String.valueOf(currentTime / 1000);    int totalEvents = 0;    int i = 1, j = 1;        Configuration conf = new Configuration();    FileSystem fs = FileSystem.get(conf);    Path dirPath = new Path(expectedPath);    fs.delete(dirPath, true);    fs.mkdirs(dirPath);    Context context = new Context();    context.put("hdfs.path", newPath);    context.put("hdfs.filePrefix", fileName);    context.put("hdfs.rollCount", String.valueOf(rollCount));    context.put("hdfs.batchSize", String.valueOf(batchSize));    context.put("hdfs.useLocalTimeStamp", String.valueOf(true));    Configurables.configure(sink, context);    Channel channel = new MemoryChannel();    Configurables.configure(channel, context);    sink.setChannel(channel);    sink.setBucketClock(clk);    sink.start();    Calendar eventDate = Calendar.getInstance();    List<String> bodies = Lists.newArrayList();        for (i = 1; i < numBatches; i++) {        Transaction txn = channel.getTransaction();        txn.begin();        for (j = 1; j <= batchSize; j++) {            Event event = new SimpleEvent();            eventDate.clear();                        eventDate.set(2011, i, i, i, 0);            event.getHeaders().put("timestamp", String.valueOf(eventDate.getTimeInMillis()));            event.getHeaders().put("hostname", "Host" + i);            String body = "Test." + i + "." + j;            event.setBody(body.getBytes());            bodies.add(body);            channel.put(event);            totalEvents++;        }        txn.commit();        txn.close();                sink.process();    }    sink.stop();        FileStatus[] dirStat = fs.listStatus(dirPath);    Path[] fList = FileUtil.stat2Paths(dirStat);        long expectedFiles = totalEvents / rollCount;    if (totalEvents % rollCount > 0)        expectedFiles++;    Assert.assertEquals("num files wrong, found: " + Lists.newArrayList(fList), expectedFiles, fList.length);    verifyOutputSequenceFiles(fs, conf, dirPath.toUri().getPath(), fileName, bodies);        sink.setBucketClock(new SystemClock());}
1
public long currentTimeMillis()
{    return currentTime;}
0
public void testAppend() throws InterruptedException, LifecycleException, EventDeliveryException, IOException
{        final long rollCount = 3;    final long batchSize = 2;    final String fileName = "FlumeData";        Configuration conf = new Configuration();    FileSystem fs = FileSystem.get(conf);    Path dirPath = new Path(testPath);    fs.delete(dirPath, true);    fs.mkdirs(dirPath);    Context context = new Context();    context.put("hdfs.path", testPath + "/%Y-%m-%d/%H");    context.put("hdfs.timeZone", "UTC");    context.put("hdfs.filePrefix", fileName);    context.put("hdfs.rollCount", String.valueOf(rollCount));    context.put("hdfs.batchSize", String.valueOf(batchSize));    Configurables.configure(sink, context);    Channel channel = new MemoryChannel();    Configurables.configure(channel, context);    sink.setChannel(channel);    sink.start();    Calendar eventDate = Calendar.getInstance();    List<String> bodies = Lists.newArrayList();        for (int i = 1; i < 4; i++) {        Transaction txn = channel.getTransaction();        txn.begin();        for (int j = 1; j <= batchSize; j++) {            Event event = new SimpleEvent();            eventDate.clear();                        eventDate.set(2011, i, i, i, 0);            event.getHeaders().put("timestamp", String.valueOf(eventDate.getTimeInMillis()));            event.getHeaders().put("hostname", "Host" + i);            String body = "Test." + i + "." + j;            event.setBody(body.getBytes());            bodies.add(body);            channel.put(event);        }        txn.commit();        txn.close();                sink.process();    }    sink.stop();    verifyOutputSequenceFiles(fs, conf, dirPath.toUri().getPath(), fileName, bodies);}
1
public void testBadSimpleAppend() throws InterruptedException, LifecycleException, EventDeliveryException, IOException
{        final String fileName = "FlumeData";    final long rollCount = 5;    final long batchSize = 2;    final int numBatches = 4;    String newPath = testPath + "/singleBucket";    int totalEvents = 0;    int i = 1, j = 1;    HDFSTestWriterFactory badWriterFactory = new HDFSTestWriterFactory();    sink = new HDFSEventSink(badWriterFactory);        Configuration conf = new Configuration();    FileSystem fs = FileSystem.get(conf);    Path dirPath = new Path(newPath);    fs.delete(dirPath, true);    fs.mkdirs(dirPath);    Context context = new Context();    context.put("hdfs.path", newPath);    context.put("hdfs.filePrefix", fileName);    context.put("hdfs.rollCount", String.valueOf(rollCount));    context.put("hdfs.batchSize", String.valueOf(batchSize));    context.put("hdfs.fileType", HDFSTestWriterFactory.TestSequenceFileType);    Configurables.configure(sink, context);    Channel channel = new MemoryChannel();    Configurables.configure(channel, context);    sink.setChannel(channel);    sink.start();    Calendar eventDate = Calendar.getInstance();    List<String> bodies = Lists.newArrayList();        for (i = 1; i < numBatches; i++) {        Transaction txn = channel.getTransaction();        txn.begin();        for (j = 1; j <= batchSize; j++) {            Event event = new SimpleEvent();            eventDate.clear();                        eventDate.set(2011, i, i, i, 0);            event.getHeaders().put("timestamp", String.valueOf(eventDate.getTimeInMillis()));            event.getHeaders().put("hostname", "Host" + i);            String body = "Test." + i + "." + j;            event.setBody(body.getBytes());            bodies.add(body);                        if ((totalEvents % 30) == 1) {                event.getHeaders().put("fault-once", "");            }            channel.put(event);            totalEvents++;        }        txn.commit();        txn.close();            }                sink.stop();    verifyOutputSequenceFiles(fs, conf, dirPath.toUri().getPath(), fileName, bodies);    SinkCounter sc = (SinkCounter) Whitebox.getInternalState(sink, "sinkCounter");    Assert.assertEquals(1, sc.getEventWriteFail());}
1
private List<String> getAllFiles(String input)
{    List<String> output = Lists.newArrayList();    File dir = new File(input);    if (dir.isFile()) {        output.add(dir.getAbsolutePath());    } else if (dir.isDirectory()) {        for (String file : dir.list()) {            File subDir = new File(dir, file);            output.addAll(getAllFiles(subDir.getAbsolutePath()));        }    }    return output;}
0
private void verifyOutputSequenceFiles(FileSystem fs, Configuration conf, String dir, String prefix, List<String> bodies) throws IOException
{    int found = 0;    int expected = bodies.size();    for (String outputFile : getAllFiles(dir)) {        String name = (new File(outputFile)).getName();        if (name.startsWith(prefix)) {            SequenceFile.Reader reader = new SequenceFile.Reader(fs, new Path(outputFile), conf);            LongWritable key = new LongWritable();            BytesWritable value = new BytesWritable();            while (reader.next(key, value)) {                String body = new String(value.getBytes(), 0, value.getLength());                if (bodies.contains(body)) {                                        bodies.remove(body);                    found++;                }            }            reader.close();        }    }    if (!bodies.isEmpty()) {        for (String body : bodies) {                    }    }    Assert.assertTrue("Found = " + found + ", Expected = " + expected + ", Left = " + bodies.size() + " " + bodies, bodies.size() == 0);}
1
private void verifyOutputTextFiles(FileSystem fs, Configuration conf, String dir, String prefix, List<String> bodies) throws IOException
{    int found = 0;    int expected = bodies.size();    for (String outputFile : getAllFiles(dir)) {        String name = (new File(outputFile)).getName();        if (name.startsWith(prefix)) {            FSDataInputStream input = fs.open(new Path(outputFile));            BufferedReader reader = new BufferedReader(new InputStreamReader(input));            String body = null;            while ((body = reader.readLine()) != null) {                bodies.remove(body);                found++;            }            reader.close();        }    }    Assert.assertTrue("Found = " + found + ", Expected = " + expected + ", Left = " + bodies.size() + " " + bodies, bodies.size() == 0);}
0
private void verifyOutputAvroFiles(FileSystem fs, Configuration conf, String dir, String prefix, List<String> bodies) throws IOException
{    int found = 0;    int expected = bodies.size();    for (String outputFile : getAllFiles(dir)) {        String name = (new File(outputFile)).getName();        if (name.startsWith(prefix)) {            FSDataInputStream input = fs.open(new Path(outputFile));            DatumReader<GenericRecord> reader = new GenericDatumReader<GenericRecord>();            DataFileStream<GenericRecord> avroStream = new DataFileStream<GenericRecord>(input, reader);            GenericRecord record = new GenericData.Record(avroStream.getSchema());            while (avroStream.hasNext()) {                avroStream.next(record);                ByteBuffer body = (ByteBuffer) record.get("body");                CharsetDecoder decoder = Charsets.UTF_8.newDecoder();                String bodyStr = decoder.decode(body).toString();                                bodies.remove(bodyStr);                found++;            }            avroStream.close();            input.close();        }    }    Assert.assertTrue("Found = " + found + ", Expected = " + expected + ", Left = " + bodies.size() + " " + bodies, bodies.size() == 0);}
1
public void testCloseReopen() throws InterruptedException, LifecycleException, EventDeliveryException, IOException
{        final int numBatches = 4;    final String fileName = "FlumeData";    final long rollCount = 5;    final long batchSize = 2;    String newPath = testPath + "/singleBucket";    int i = 1, j = 1;    HDFSTestWriterFactory badWriterFactory = new HDFSTestWriterFactory();    sink = new HDFSEventSink(badWriterFactory);        Configuration conf = new Configuration();    FileSystem fs = FileSystem.get(conf);    Path dirPath = new Path(newPath);    fs.delete(dirPath, true);    fs.mkdirs(dirPath);    Context context = new Context();    context.put("hdfs.path", newPath);    context.put("hdfs.filePrefix", fileName);    context.put("hdfs.rollCount", String.valueOf(rollCount));    context.put("hdfs.batchSize", String.valueOf(batchSize));    context.put("hdfs.fileType", HDFSTestWriterFactory.TestSequenceFileType);    Configurables.configure(sink, context);    MemoryChannel channel = new MemoryChannel();    Configurables.configure(channel, new Context());    sink.setChannel(channel);    sink.start();    Calendar eventDate = Calendar.getInstance();    List<String> bodies = Lists.newArrayList();        for (i = 1; i < numBatches; i++) {        channel.getTransaction().begin();        try {            for (j = 1; j <= batchSize; j++) {                Event event = new SimpleEvent();                eventDate.clear();                                eventDate.set(2011, i, i, i, 0);                event.getHeaders().put("timestamp", String.valueOf(eventDate.getTimeInMillis()));                event.getHeaders().put("hostname", "Host" + i);                String body = "Test." + i + "." + j;                event.setBody(body.getBytes());                bodies.add(body);                                event.getHeaders().put("fault-until-reopen", "");                channel.put(event);            }            channel.getTransaction().commit();        } finally {            channel.getTransaction().close();        }            }        sink.stop();    verifyOutputSequenceFiles(fs, conf, dirPath.toUri().getPath(), fileName, bodies);    SinkCounter sc = (SinkCounter) Whitebox.getInternalState(sink, "sinkCounter");    Assert.assertEquals(1, sc.getEventWriteFail());}
1
public void testCloseReopenOnRollTime() throws InterruptedException, LifecycleException, EventDeliveryException, IOException
{        final int numBatches = 4;    final String fileName = "FlumeData";    final long batchSize = 2;    String newPath = testPath + "/singleBucket";    int i = 1, j = 1;    HDFSTestWriterFactory badWriterFactory = new HDFSTestWriterFactory();    sink = new HDFSEventSink(badWriterFactory);        Configuration conf = new Configuration();    FileSystem fs = FileSystem.get(conf);    Path dirPath = new Path(newPath);    fs.delete(dirPath, true);    fs.mkdirs(dirPath);    Context context = new Context();    context.put("hdfs.path", newPath);    context.put("hdfs.filePrefix", fileName);    context.put("hdfs.rollCount", String.valueOf(0));    context.put("hdfs.rollSize", String.valueOf(0));    context.put("hdfs.rollInterval", String.valueOf(2));    context.put("hdfs.batchSize", String.valueOf(batchSize));    context.put("hdfs.fileType", HDFSTestWriterFactory.TestSequenceFileType);    Configurables.configure(sink, context);    MemoryChannel channel = new MemoryChannel();    Configurables.configure(channel, new Context());    sink.setChannel(channel);    sink.start();    Calendar eventDate = Calendar.getInstance();    List<String> bodies = Lists.newArrayList();        for (i = 1; i < numBatches; i++) {        channel.getTransaction().begin();        try {            for (j = 1; j <= batchSize; j++) {                Event event = new SimpleEvent();                eventDate.clear();                                eventDate.set(2011, i, i, i, 0);                event.getHeaders().put("timestamp", String.valueOf(eventDate.getTimeInMillis()));                event.getHeaders().put("hostname", "Host" + i);                String body = "Test." + i + "." + j;                event.setBody(body.getBytes());                bodies.add(body);                                event.getHeaders().put("count-check", "");                channel.put(event);            }            channel.getTransaction().commit();        } finally {            channel.getTransaction().close();        }                        if (i == 1) {            Thread.sleep(2001);        }    }        sink.stop();    Assert.assertTrue(badWriterFactory.openCount.get() >= 2);        verifyOutputSequenceFiles(fs, conf, dirPath.toUri().getPath(), fileName, bodies);}
1
public void testCloseRemovesFromSFWriters() throws InterruptedException, LifecycleException, EventDeliveryException, IOException
{        final String fileName = "FlumeData";    final long batchSize = 2;    String newPath = testPath + "/singleBucket";    int i = 1, j = 1;    HDFSTestWriterFactory badWriterFactory = new HDFSTestWriterFactory();    sink = new HDFSEventSink(badWriterFactory);        Configuration conf = new Configuration();    FileSystem fs = FileSystem.get(conf);    Path dirPath = new Path(newPath);    fs.delete(dirPath, true);    fs.mkdirs(dirPath);    Context context = new Context();    context.put("hdfs.path", newPath);    context.put("hdfs.filePrefix", fileName);    context.put("hdfs.rollCount", String.valueOf(0));    context.put("hdfs.rollSize", String.valueOf(0));    context.put("hdfs.rollInterval", String.valueOf(1));    context.put("hdfs.batchSize", String.valueOf(batchSize));    context.put("hdfs.fileType", HDFSTestWriterFactory.TestSequenceFileType);    String expectedLookupPath = newPath + "/FlumeData";    Configurables.configure(sink, context);    MemoryChannel channel = new MemoryChannel();    Configurables.configure(channel, new Context());    sink.setChannel(channel);    sink.start();    Calendar eventDate = Calendar.getInstance();    List<String> bodies = Lists.newArrayList();        channel.getTransaction().begin();    try {        for (j = 1; j <= 2 * batchSize; j++) {            Event event = new SimpleEvent();            eventDate.clear();                        eventDate.set(2011, i, i, i, 0);            event.getHeaders().put("timestamp", String.valueOf(eventDate.getTimeInMillis()));            event.getHeaders().put("hostname", "Host" + i);            String body = "Test." + i + "." + j;            event.setBody(body.getBytes());            bodies.add(body);                        event.getHeaders().put("count-check", "");            channel.put(event);        }        channel.getTransaction().commit();    } finally {        channel.getTransaction().close();    }        Assert.assertTrue(sink.getSfWriters().containsKey(expectedLookupPath));        Thread.sleep(2001);    Assert.assertFalse(sink.getSfWriters().containsKey(expectedLookupPath));                Assert.assertTrue(sink.getSfWriters().containsKey(expectedLookupPath));    sink.stop();        verifyOutputSequenceFiles(fs, conf, dirPath.toUri().getPath(), fileName, bodies);}
1
public void testSlowAppendFailure() throws InterruptedException, LifecycleException, EventDeliveryException, IOException
{        final String fileName = "FlumeData";    final long rollCount = 5;    final long batchSize = 2;    final int numBatches = 2;    String newPath = testPath + "/singleBucket";    int i = 1, j = 1;        Configuration conf = new Configuration();    FileSystem fs = FileSystem.get(conf);    Path dirPath = new Path(newPath);    fs.delete(dirPath, true);    fs.mkdirs(dirPath);        HDFSTestWriterFactory badWriterFactory = new HDFSTestWriterFactory();    sink = new HDFSEventSink(badWriterFactory);    Context context = new Context();    context.put("hdfs.path", newPath);    context.put("hdfs.filePrefix", fileName);    context.put("hdfs.rollCount", String.valueOf(rollCount));    context.put("hdfs.batchSize", String.valueOf(batchSize));    context.put("hdfs.fileType", HDFSTestWriterFactory.TestSequenceFileType);    context.put("hdfs.callTimeout", Long.toString(1000));    Configurables.configure(sink, context);    Channel channel = new MemoryChannel();    Configurables.configure(channel, context);    sink.setChannel(channel);    sink.start();    Calendar eventDate = Calendar.getInstance();        for (i = 0; i < numBatches; i++) {        Transaction txn = channel.getTransaction();        txn.begin();        for (j = 1; j <= batchSize; j++) {            Event event = new SimpleEvent();            eventDate.clear();                        eventDate.set(2011, i, i, i, 0);            event.getHeaders().put("timestamp", String.valueOf(eventDate.getTimeInMillis()));            event.getHeaders().put("hostname", "Host" + i);            event.getHeaders().put("slow", "1500");            event.setBody(("Test." + i + "." + j).getBytes());            channel.put(event);        }        txn.commit();        txn.close();                Status satus = sink.process();                Assert.assertEquals(satus, Status.BACKOFF);    }    sink.stop();    SinkCounter sc = (SinkCounter) Whitebox.getInternalState(sink, "sinkCounter");    Assert.assertEquals(2, sc.getEventWriteFail());}
1
private void slowAppendTestHelper(long appendTimeout) throws InterruptedException, IOException, LifecycleException, EventDeliveryException, IOException
{    final String fileName = "FlumeData";    final long rollCount = 5;    final long batchSize = 2;    final int numBatches = 2;    String newPath = testPath + "/singleBucket";    int totalEvents = 0;    int i = 1, j = 1;        Configuration conf = new Configuration();    FileSystem fs = FileSystem.get(conf);    Path dirPath = new Path(newPath);    fs.delete(dirPath, true);    fs.mkdirs(dirPath);        HDFSTestWriterFactory badWriterFactory = new HDFSTestWriterFactory();    sink = new HDFSEventSink(badWriterFactory);    Context context = new Context();    context.put("hdfs.path", newPath);    context.put("hdfs.filePrefix", fileName);    context.put("hdfs.rollCount", String.valueOf(rollCount));    context.put("hdfs.batchSize", String.valueOf(batchSize));    context.put("hdfs.fileType", HDFSTestWriterFactory.TestSequenceFileType);    context.put("hdfs.appendTimeout", String.valueOf(appendTimeout));    Configurables.configure(sink, context);    Channel channel = new MemoryChannel();    Configurables.configure(channel, context);    sink.setChannel(channel);    sink.start();    Calendar eventDate = Calendar.getInstance();    List<String> bodies = Lists.newArrayList();        for (i = 0; i < numBatches; i++) {        Transaction txn = channel.getTransaction();        txn.begin();        for (j = 1; j <= batchSize; j++) {            Event event = new SimpleEvent();            eventDate.clear();                        eventDate.set(2011, i, i, i, 0);            event.getHeaders().put("timestamp", String.valueOf(eventDate.getTimeInMillis()));            event.getHeaders().put("hostname", "Host" + i);            event.getHeaders().put("slow", "1500");            String body = "Test." + i + "." + j;            event.setBody(body.getBytes());            bodies.add(body);            channel.put(event);            totalEvents++;        }        txn.commit();        txn.close();                sink.process();    }    sink.stop();        FileStatus[] dirStat = fs.listStatus(dirPath);    Path[] fList = FileUtil.stat2Paths(dirStat);            long expectedFiles = totalEvents / rollCount;    if (totalEvents % rollCount > 0)        expectedFiles++;    Assert.assertEquals("num files wrong, found: " + Lists.newArrayList(fList), expectedFiles, fList.length);    verifyOutputSequenceFiles(fs, conf, dirPath.toUri().getPath(), fileName, bodies);}
0
public void testSlowAppendWithLongTimeout() throws InterruptedException, LifecycleException, EventDeliveryException, IOException
{        slowAppendTestHelper(3000);}
1
public void testSlowAppendWithoutTimeout() throws InterruptedException, LifecycleException, EventDeliveryException, IOException
{        slowAppendTestHelper(0);}
1
public void testCloseOnIdle() throws IOException, EventDeliveryException, InterruptedException
{    String hdfsPath = testPath + "/idleClose";    Configuration conf = new Configuration();    FileSystem fs = FileSystem.get(conf);    Path dirPath = new Path(hdfsPath);    fs.delete(dirPath, true);    fs.mkdirs(dirPath);    Context context = new Context();    context.put("hdfs.path", hdfsPath);    /*     * All three rolling methods are disabled so the only     * way a file can roll is through the idle timeout.     */    context.put("hdfs.rollCount", "0");    context.put("hdfs.rollSize", "0");    context.put("hdfs.rollInterval", "0");    context.put("hdfs.batchSize", "2");    context.put("hdfs.idleTimeout", "1");    Configurables.configure(sink, context);    Channel channel = new MemoryChannel();    Configurables.configure(channel, context);    sink.setChannel(channel);    sink.start();    Transaction txn = channel.getTransaction();    txn.begin();    for (int i = 0; i < 10; i++) {        Event event = new SimpleEvent();        event.setBody(("test event " + i).getBytes());        channel.put(event);    }    txn.commit();    txn.close();    sink.process();    sink.process();    Thread.sleep(1001);                        sink.process();    sink.process();        Thread.sleep(500);    sink.process();    sink.process();    sink.stop();    FileStatus[] dirStat = fs.listStatus(dirPath);    Path[] fList = FileUtil.stat2Paths(dirStat);    Assert.assertEquals("Incorrect content of the directory " + StringUtils.join(fList, ","), 2, fList.length);    Assert.assertTrue(!fList[0].getName().endsWith(".tmp") && !fList[1].getName().endsWith(".tmp"));    fs.close();}
0
public void testBlockCompressSequenceFileWriterSync() throws IOException, EventDeliveryException
{    String hdfsPath = testPath + "/sequenceFileWriterSync";    FileSystem fs = FileSystem.get(new Configuration());        fs.setVerifyChecksum(false);    fs.setWriteChecksum(false);        String[] codecs = { "BZip2Codec", "DeflateCodec" };    for (String codec : codecs) {        sequenceFileWriteAndVerifyEvents(fs, hdfsPath, codec, Collections.singletonList("single-event"));        sequenceFileWriteAndVerifyEvents(fs, hdfsPath, codec, Arrays.asList("multiple-events-1", "multiple-events-2", "multiple-events-3", "multiple-events-4", "multiple-events-5"));    }    fs.close();}
0
private void sequenceFileWriteAndVerifyEvents(FileSystem fs, String hdfsPath, String codec, Collection<String> eventBodies) throws IOException, EventDeliveryException
{    Path dirPath = new Path(hdfsPath);    fs.delete(dirPath, true);    fs.mkdirs(dirPath);    Context context = new Context();    context.put("hdfs.path", hdfsPath);        context.put("hdfs.rollCount", String.valueOf(eventBodies.size() + 1));    context.put("hdfs.rollSize", "0");    context.put("hdfs.rollInterval", "0");    context.put("hdfs.batchSize", "1");    context.put("hdfs.fileType", "SequenceFile");    context.put("hdfs.codeC", codec);    context.put("hdfs.writeFormat", "Writable");    Configurables.configure(sink, context);    Channel channel = new MemoryChannel();    Configurables.configure(channel, context);    sink.setChannel(channel);    sink.start();    for (String eventBody : eventBodies) {        Transaction txn = channel.getTransaction();        txn.begin();        Event event = new SimpleEvent();        event.setBody(eventBody.getBytes());        channel.put(event);        txn.commit();        txn.close();        sink.process();    }            FileStatus[] dirStat = fs.listStatus(dirPath);    Path[] paths = FileUtil.stat2Paths(dirStat);    Assert.assertEquals(1, paths.length);    SequenceFile.Reader reader = new SequenceFile.Reader(fs.getConf(), SequenceFile.Reader.stream(fs.open(paths[0])));    LongWritable key = new LongWritable();    BytesWritable value = new BytesWritable();    for (String eventBody : eventBodies) {        Assert.assertTrue(reader.next(key, value));        Assert.assertArrayEquals(eventBody.getBytes(), value.copyBytes());    }    Assert.assertFalse(reader.next(key, value));}
0
private Context getContextForRetryTests()
{    Context context = new Context();    context.put("hdfs.path", testPath + "/%{retryHeader}");    context.put("hdfs.filePrefix", "test");    context.put("hdfs.batchSize", String.valueOf(100));    context.put("hdfs.fileType", "DataStream");    context.put("hdfs.serializer", "text");    context.put("hdfs.closeTries", "3");    context.put("hdfs.rollCount", "1");    context.put("hdfs.retryInterval", "1");    return context;}
0
public void testBadConfigurationForRetryIntervalZero() throws Exception
{    Context context = getContextForRetryTests();    context.put("hdfs.retryInterval", "0");    Configurables.configure(sink, context);    Assert.assertEquals(1, sink.getTryCount());}
0
public void testBadConfigurationForRetryIntervalNegative() throws Exception
{    Context context = getContextForRetryTests();    context.put("hdfs.retryInterval", "-1");    Configurables.configure(sink, context);    Assert.assertEquals(1, sink.getTryCount());}
0
public void testBadConfigurationForRetryCountZero() throws Exception
{    Context context = getContextForRetryTests();    context.put("hdfs.closeTries", "0");    Configurables.configure(sink, context);    Assert.assertEquals(Integer.MAX_VALUE, sink.getTryCount());}
0
public void testBadConfigurationForRetryCountNegative() throws Exception
{    Context context = getContextForRetryTests();    context.put("hdfs.closeTries", "-4");    Configurables.configure(sink, context);    Assert.assertEquals(Integer.MAX_VALUE, sink.getTryCount());}
0
public void testRetryRename() throws InterruptedException, LifecycleException, EventDeliveryException, IOException
{    testRetryRename(true);    testRetryRename(false);}
0
private void testRetryRename(boolean closeSucceed) throws InterruptedException, LifecycleException, EventDeliveryException, IOException
{        String newPath = testPath + "/retryBucket";        Configuration conf = new Configuration();    FileSystem fs = FileSystem.get(conf);    Path dirPath = new Path(newPath);    fs.delete(dirPath, true);    fs.mkdirs(dirPath);    MockFileSystem mockFs = new MockFileSystem(fs, 6, closeSucceed);    Context context = getContextForRetryTests();    Configurables.configure(sink, context);    Channel channel = new MemoryChannel();    Configurables.configure(channel, context);    sink.setChannel(channel);    sink.setMockFs(mockFs);    HDFSWriter hdfsWriter = new MockDataStream(mockFs);    hdfsWriter.configure(context);    sink.setMockWriter(hdfsWriter);    sink.start();        for (int i = 0; i < 2; i++) {        Transaction txn = channel.getTransaction();        txn.begin();        Map<String, String> hdr = Maps.newHashMap();        hdr.put("retryHeader", "v1");        channel.put(EventBuilder.withBody("random".getBytes(), hdr));        txn.commit();        txn.close();                sink.process();    }        for (int i = 0; i < 2; i++) {        Transaction txn = channel.getTransaction();        txn.begin();        Map<String, String> hdr = Maps.newHashMap();        hdr.put("retryHeader", "v2");        channel.put(EventBuilder.withBody("random".getBytes(), hdr));        txn.commit();        txn.close();                sink.process();    }        TimeUnit.SECONDS.sleep(5);    Collection<BucketWriter> writers = sink.getSfWriters().values();    int totalRenameAttempts = 0;    for (BucketWriter writer : writers) {                totalRenameAttempts += writer.renameTries.get();    }            sink.stop();    Assert.assertEquals(6, totalRenameAttempts);}
1
public void testFlushedIfAppendFailedWithBucketClosedException() throws Exception
{    final Set<BucketWriter> bucketWriters = new HashSet<>();    sink = new HDFSEventSink() {        @Override        BucketWriter initializeBucketWriter(String realPath, String realName, String lookupPath, HDFSWriter hdfsWriter, WriterCallback closeCallback) {            BucketWriter bw = Mockito.spy(super.initializeBucketWriter(realPath, realName, lookupPath, hdfsWriter, closeCallback));            try {                                                Mockito.doCallRealMethod().doThrow(BucketClosedException.class).when(bw).append(Mockito.any(Event.class));            } catch (IOException | InterruptedException e) {                Assert.fail("This shouldn't happen, as append() is called during mocking.");            }            bucketWriters.add(bw);            return bw;        }    };    Context context = new Context(ImmutableMap.of("hdfs.path", testPath));    Configurables.configure(sink, context);    Channel channel = Mockito.spy(new MemoryChannel());    Configurables.configure(channel, new Context());    final Iterator<Event> events = Iterators.forArray(EventBuilder.withBody("test1".getBytes()), EventBuilder.withBody("test2".getBytes()));    Mockito.doAnswer(new Answer() {        @Override        public Object answer(InvocationOnMock invocation) throws Throwable {            return events.hasNext() ? events.next() : null;        }    }).when(channel).take();    sink.setChannel(channel);    sink.start();    sink.process();        Mockito.verify(channel, Mockito.times(3)).take();    FileSystem fs = FileSystem.get(new Configuration());    int fileCount = 0;    for (RemoteIterator<LocatedFileStatus> i = fs.listFiles(new Path(testPath), false); i.hasNext(); i.next()) {        fileCount++;    }    Assert.assertEquals(2, fileCount);    Assert.assertEquals(2, bucketWriters.size());        for (BucketWriter bw : bucketWriters) {        Mockito.verify(bw, Mockito.times(1)).flush();    }    sink.stop();}
0
 BucketWriter initializeBucketWriter(String realPath, String realName, String lookupPath, HDFSWriter hdfsWriter, WriterCallback closeCallback)
{    BucketWriter bw = Mockito.spy(super.initializeBucketWriter(realPath, realName, lookupPath, hdfsWriter, closeCallback));    try {                        Mockito.doCallRealMethod().doThrow(BucketClosedException.class).when(bw).append(Mockito.any(Event.class));    } catch (IOException | InterruptedException e) {        Assert.fail("This shouldn't happen, as append() is called during mocking.");    }    bucketWriters.add(bw);    return bw;}
0
public Object answer(InvocationOnMock invocation) throws Throwable
{    return events.hasNext() ? events.next() : null;}
0
public void testChannelException()
{        Context context = new Context();    context.put("hdfs.path", testPath);    context.put("keep-alive", "0");    Configurables.configure(sink, context);    Channel channel = Mockito.mock(Channel.class);    Mockito.when(channel.take()).thenThrow(new ChannelException("dummy"));    Mockito.when(channel.getTransaction()).thenReturn(Mockito.mock(BasicTransactionSemantics.class));    sink.setChannel(channel);    sink.start();    try {        sink.process();    } catch (EventDeliveryException e) {        }    sink.stop();    SinkCounter sc = (SinkCounter) Whitebox.getInternalState(sink, "sinkCounter");    Assert.assertEquals(1, sc.getChannelReadFail());}
1
public void testEmptyInUseSuffix()
{    String inUseSuffixConf = "aaaa";    Context context = new Context();    context.put("hdfs.path", testPath);    context.put("hdfs.inUseSuffix", inUseSuffixConf);        Configurables.configure(sink, context);    String inUseSuffix = (String) Whitebox.getInternalState(sink, "inUseSuffix");    Assert.assertEquals(inUseSuffixConf, inUseSuffix);    context.put("hdfs.emptyInUseSuffix", "true");    Configurables.configure(sink, context);    inUseSuffix = (String) Whitebox.getInternalState(sink, "inUseSuffix");    Assert.assertEquals("", inUseSuffix);    context.put("hdfs.emptyInUseSuffix", "false");    Configurables.configure(sink, context);    inUseSuffix = (String) Whitebox.getInternalState(sink, "inUseSuffix");    Assert.assertEquals(inUseSuffixConf, inUseSuffix);}
0
public static void main(String... args)
{    HDFSEventSink sink = new HDFSEventSink();    sink.setName("HDFSEventSink");    Context context = new Context(ImmutableMap.of("hdfs.path", "file:///tmp/flume-test/bucket-%t", "hdfs.filePrefix", "flumetest", "hdfs.rollInterval", "1", "hdfs.maxOpenFiles", "1", "hdfs.useLocalTimeStamp", "true"));    Configurables.configure(sink, context);    Channel channel = new MemoryChannel();    Configurables.configure(channel, new Context());    final SequenceGeneratorSource source = new SequenceGeneratorSource();    Configurables.configure(source, new Context());    ChannelSelector rcs = new ReplicatingChannelSelector();    rcs.setChannels(Collections.singletonList(channel));    source.setChannelProcessor(new ChannelProcessor(rcs));    sink.setChannel(channel);    channel.start();    source.start();    SinkProcessor sinkProcessor = new DefaultSinkProcessor();    sinkProcessor.setSinks(Collections.singletonList(sink));    SinkRunner sinkRunner = new SinkRunner();    sinkRunner.setSink(sinkProcessor);    sinkRunner.start();    ScheduledExecutorService executor = Executors.newScheduledThreadPool(3);    executor.execute(new Runnable() {        @Override        public void run() {            int i = 0;            while (true) {                try {                    source.process();                    System.out.println(i++);                    if (i == 250) {                        System.out.println("No deadlock found after 250 iterations, exiting");                        System.exit(0);                    }                    Thread.sleep((long) (Math.random() * 100 + 950));                } catch (Exception e) {                                }            }        }    });    executor.scheduleAtFixedRate(new Runnable() {        @Override        public void run() {            ThreadMXBean bean = ManagementFactory.getThreadMXBean();            long[] threadIds = bean.findDeadlockedThreads();            if (threadIds != null) {                System.out.println("Deadlocked threads found");                printThreadStackTraces(threadIds);                System.exit(1);            }        }    }, 0, 1, TimeUnit.SECONDS);}
0
public void run()
{    int i = 0;    while (true) {        try {            source.process();            System.out.println(i++);            if (i == 250) {                System.out.println("No deadlock found after 250 iterations, exiting");                System.exit(0);            }            Thread.sleep((long) (Math.random() * 100 + 950));        } catch (Exception e) {                }    }}
0
public void run()
{    ThreadMXBean bean = ManagementFactory.getThreadMXBean();    long[] threadIds = bean.findDeadlockedThreads();    if (threadIds != null) {        System.out.println("Deadlocked threads found");        printThreadStackTraces(threadIds);        System.exit(1);    }}
0
private static void printThreadStackTraces(long[] threadIds)
{    Set<Long> threadIdSet = new HashSet<>(Longs.asList(threadIds));    for (Thread th : Thread.getAllStackTraces().keySet()) {        if (threadIdSet.contains(th.getId())) {            System.out.println("Thread: " + th);            for (StackTraceElement e : th.getStackTrace()) {                System.out.println("\t" + e);            }            System.out.println("-----------------------------");        }    }}
0
public static void setupClass() throws IOException
{        File dfsDir = new File(DFS_DIR);    if (!dfsDir.isDirectory()) {        dfsDir.mkdirs();    }        oldTestBuildDataProp = System.getProperty(TEST_BUILD_DATA_KEY);    System.setProperty(TEST_BUILD_DATA_KEY, DFS_DIR);}
0
private static String getNameNodeURL(MiniDFSCluster cluster)
{    int nnPort = cluster.getNameNode().getNameNodeAddress().getPort();    return "hdfs://localhost:" + nnPort;}
0
public void simpleHDFSTest() throws EventDeliveryException, IOException
{    cluster = new MiniDFSCluster(new Configuration(), 1, true, null);    cluster.waitActive();    String outputDir = "/flume/simpleHDFSTest";    Path outputDirPath = new Path(outputDir);        FileSystem fs = cluster.getFileSystem();        if (fs.exists(outputDirPath)) {        fs.delete(outputDirPath, true);    }    String nnURL = getNameNodeURL(cluster);        Context chanCtx = new Context();    MemoryChannel channel = new MemoryChannel();    channel.setName("simpleHDFSTest-mem-chan");    channel.configure(chanCtx);    channel.start();    Context sinkCtx = new Context();    sinkCtx.put("hdfs.path", nnURL + outputDir);    sinkCtx.put("hdfs.fileType", HDFSWriterFactory.DataStreamType);    sinkCtx.put("hdfs.batchSize", Integer.toString(1));    HDFSEventSink sink = new HDFSEventSink();    sink.setName("simpleHDFSTest-hdfs-sink");    sink.configure(sinkCtx);    sink.setChannel(channel);    sink.start();        String EVENT_BODY = "yarg!";    channel.getTransaction().begin();    try {        channel.put(EventBuilder.withBody(EVENT_BODY, Charsets.UTF_8));        channel.getTransaction().commit();    } finally {        channel.getTransaction().close();    }        sink.process();        sink.stop();    channel.stop();        FileStatus[] statuses = fs.listStatus(outputDirPath);    Assert.assertNotNull("No files found written to HDFS", statuses);    Assert.assertEquals("Only one file expected", 1, statuses.length);    for (FileStatus status : statuses) {        Path filePath = status.getPath();                FSDataInputStream stream = fs.open(filePath);        BufferedReader reader = new BufferedReader(new InputStreamReader(stream));        String line = reader.readLine();                Assert.assertEquals(EVENT_BODY, line);    }    if (!KEEP_DATA) {        fs.delete(outputDirPath, true);    }    cluster.shutdown();    cluster = null;}
1
public void simpleHDFSGZipCompressedTest() throws EventDeliveryException, IOException
{    cluster = new MiniDFSCluster(new Configuration(), 1, true, null);    cluster.waitActive();    String outputDir = "/flume/simpleHDFSGZipCompressedTest";    Path outputDirPath = new Path(outputDir);        FileSystem fs = cluster.getFileSystem();        if (fs.exists(outputDirPath)) {        fs.delete(outputDirPath, true);    }    String nnURL = getNameNodeURL(cluster);        Context chanCtx = new Context();    MemoryChannel channel = new MemoryChannel();    channel.setName("simpleHDFSTest-mem-chan");    channel.configure(chanCtx);    channel.start();    Context sinkCtx = new Context();    sinkCtx.put("hdfs.path", nnURL + outputDir);    sinkCtx.put("hdfs.fileType", HDFSWriterFactory.CompStreamType);    sinkCtx.put("hdfs.batchSize", Integer.toString(1));    sinkCtx.put("hdfs.codeC", "gzip");    HDFSEventSink sink = new HDFSEventSink();    sink.setName("simpleHDFSTest-hdfs-sink");    sink.configure(sinkCtx);    sink.setChannel(channel);    sink.start();        String EVENT_BODY_1 = "yarg1";    String EVENT_BODY_2 = "yarg2";    channel.getTransaction().begin();    try {        channel.put(EventBuilder.withBody(EVENT_BODY_1, Charsets.UTF_8));        channel.put(EventBuilder.withBody(EVENT_BODY_2, Charsets.UTF_8));        channel.getTransaction().commit();    } finally {        channel.getTransaction().close();    }        sink.process();        sink.stop();    channel.stop();        FileStatus[] statuses = fs.listStatus(outputDirPath);    Assert.assertNotNull("No files found written to HDFS", statuses);    Assert.assertEquals("Only one file expected", 1, statuses.length);    for (FileStatus status : statuses) {        Path filePath = status.getPath();                FSDataInputStream stream = fs.open(filePath);        BufferedReader reader = new BufferedReader(new InputStreamReader(new GZIPInputStream(stream)));        String line = reader.readLine();                Assert.assertEquals(EVENT_BODY_1, line);                                                        }    if (!KEEP_DATA) {        fs.delete(outputDirPath, true);    }    cluster.shutdown();    cluster = null;}
1
public void underReplicationTest() throws EventDeliveryException, IOException
{    Configuration conf = new Configuration();    conf.set("dfs.replication", String.valueOf(3));    cluster = new MiniDFSCluster(conf, 3, true, null);    cluster.waitActive();    String outputDir = "/flume/underReplicationTest";    Path outputDirPath = new Path(outputDir);        FileSystem fs = cluster.getFileSystem();        if (fs.exists(outputDirPath)) {        fs.delete(outputDirPath, true);    }    String nnURL = getNameNodeURL(cluster);        Context chanCtx = new Context();    MemoryChannel channel = new MemoryChannel();    channel.setName("simpleHDFSTest-mem-chan");    channel.configure(chanCtx);    channel.start();    Context sinkCtx = new Context();    sinkCtx.put("hdfs.path", nnURL + outputDir);    sinkCtx.put("hdfs.fileType", HDFSWriterFactory.DataStreamType);    sinkCtx.put("hdfs.batchSize", Integer.toString(1));        sinkCtx.put("hdfs.retryInterval", "10");    HDFSEventSink sink = new HDFSEventSink();    sink.setName("simpleHDFSTest-hdfs-sink");    sink.configure(sinkCtx);    sink.setChannel(channel);    sink.start();        channel.getTransaction().begin();    try {        channel.put(EventBuilder.withBody("yarg 1", Charsets.UTF_8));        channel.put(EventBuilder.withBody("yarg 2", Charsets.UTF_8));        channel.put(EventBuilder.withBody("yarg 3", Charsets.UTF_8));        channel.put(EventBuilder.withBody("yarg 4", Charsets.UTF_8));        channel.put(EventBuilder.withBody("yarg 5", Charsets.UTF_8));        channel.put(EventBuilder.withBody("yarg 5", Charsets.UTF_8));        channel.getTransaction().commit();    } finally {        channel.getTransaction().close();    }                sink.process();        sink.process();            cluster.stopDataNode(0);                    sink.process();        sink.process();        sink.process();        sink.process();        sink.stop();    channel.stop();        FileStatus[] statuses = fs.listStatus(outputDirPath);    Assert.assertNotNull("No files found written to HDFS", statuses);    for (FileStatus status : statuses) {        Path filePath = status.getPath();                FSDataInputStream stream = fs.open(filePath);        BufferedReader reader = new BufferedReader(new InputStreamReader(stream));        String line = reader.readLine();                Assert.assertTrue(line.startsWith("yarg"));    }    Assert.assertTrue("4 or 5 files expected, found " + statuses.length, statuses.length == 4 || statuses.length == 5);    System.out.println("There are " + statuses.length + " files.");    if (!KEEP_DATA) {        fs.delete(outputDirPath, true);    }    cluster.shutdown();    cluster = null;}
1
public void maxUnderReplicationTest() throws EventDeliveryException, IOException
{    Configuration conf = new Configuration();    conf.set("dfs.replication", String.valueOf(3));    cluster = new MiniDFSCluster(conf, 3, true, null);    cluster.waitActive();    String outputDir = "/flume/underReplicationTest";    Path outputDirPath = new Path(outputDir);        FileSystem fs = cluster.getFileSystem();        if (fs.exists(outputDirPath)) {        fs.delete(outputDirPath, true);    }    String nnURL = getNameNodeURL(cluster);        Context chanCtx = new Context();    MemoryChannel channel = new MemoryChannel();    channel.setName("simpleHDFSTest-mem-chan");    channel.configure(chanCtx);    channel.start();    Context sinkCtx = new Context();    sinkCtx.put("hdfs.path", nnURL + outputDir);    sinkCtx.put("hdfs.fileType", HDFSWriterFactory.DataStreamType);    sinkCtx.put("hdfs.batchSize", Integer.toString(1));    HDFSEventSink sink = new HDFSEventSink();    sink.setName("simpleHDFSTest-hdfs-sink");    sink.configure(sinkCtx);    sink.setChannel(channel);    sink.start();        channel.getTransaction().begin();    try {        for (int i = 0; i < 50; i++) {            channel.put(EventBuilder.withBody("yarg " + i, Charsets.UTF_8));        }        channel.getTransaction().commit();    } finally {        channel.getTransaction().close();    }                sink.process();        sink.process();            cluster.stopDataNode(0);                    sink.process();    for (int i = 3; i < 50; i++) {                sink.process();    }        sink.stop();    channel.stop();        FileStatus[] statuses = fs.listStatus(outputDirPath);    Assert.assertNotNull("No files found written to HDFS", statuses);    for (FileStatus status : statuses) {        Path filePath = status.getPath();                FSDataInputStream stream = fs.open(filePath);        BufferedReader reader = new BufferedReader(new InputStreamReader(stream));        String line = reader.readLine();                Assert.assertTrue(line.startsWith("yarg"));    }    System.out.println("There are " + statuses.length + " files.");    Assert.assertEquals("31 files expected, found " + statuses.length, 31, statuses.length);    if (!KEEP_DATA) {        fs.delete(outputDirPath, true);    }    cluster.shutdown();    cluster = null;}
1
public void testLeaseRecoveredIfCloseThrowsIOException() throws Exception
{    testLeaseRecoveredIfCloseFails(new Callable<Void>() {        @Override        public Void call() throws Exception {            throw new IOException();        }    });}
0
public Void call() throws Exception
{    throw new IOException();}
0
public void testLeaseRecoveredIfCloseTimesOut() throws Exception
{    testLeaseRecoveredIfCloseFails(new Callable<Void>() {        @Override        public Void call() throws Exception {            TimeUnit.SECONDS.sleep(30);            return null;        }    });}
0
public Void call() throws Exception
{    TimeUnit.SECONDS.sleep(30);    return null;}
0
private void testLeaseRecoveredIfCloseFails(final Callable<?> doThisInClose) throws Exception
{    cluster = new MiniDFSCluster.Builder(new Configuration()).numDataNodes(1).format(true).build();    cluster.waitActive();    String outputDir = "/flume/leaseRecovery";    Path outputDirPath = new Path(outputDir);        FileSystem fs = cluster.getFileSystem();        if (fs.exists(outputDirPath)) {        fs.delete(outputDirPath, true);    }    String nnURL = getNameNodeURL(cluster);    Context ctx = new Context();    MemoryChannel channel = new MemoryChannel();    channel.configure(ctx);    channel.start();    ctx.put("hdfs.path", nnURL + outputDir);    ctx.put("hdfs.fileType", HDFSWriterFactory.DataStreamType);    ctx.put("hdfs.batchSize", Integer.toString(1));    ctx.put("hdfs.callTimeout", Integer.toString(1000));        ctx.put("hdfs.retryInterval", "10");    HDFSWriter hdfsWriter = new HDFSDataStream() {        @Override        public void close() throws IOException {            try {                doThisInClose.call();            } catch (Throwable e) {                Throwables.propagateIfPossible(e, IOException.class);                throw new RuntimeException(e);            }        }    };    hdfsWriter.configure(ctx);    HDFSEventSink sink = new HDFSEventSink();    sink.configure(ctx);    sink.setMockFs(fs);    sink.setMockWriter(hdfsWriter);    sink.setChannel(channel);    sink.start();    Transaction txn = channel.getTransaction();    txn.begin();    try {        channel.put(EventBuilder.withBody("test", Charsets.UTF_8));        txn.commit();    } finally {        txn.close();    }    sink.process();    sink.stop();    channel.stop();    FileStatus[] statuses = fs.listStatus(outputDirPath);    Assert.assertEquals(1, statuses.length);    String filePath = statuses[0].getPath().toUri().getPath();        long leaseRenewalTime = NameNodeAdapter.getLeaseRenewalTime(cluster.getNameNode(), filePath);        for (int i = 0; (i < 10) && (leaseRenewalTime != -1L); i++) {        TimeUnit.SECONDS.sleep(1);        leaseRenewalTime = NameNodeAdapter.getLeaseRenewalTime(cluster.getNameNode(), filePath);    }            Assert.assertEquals(-1L, leaseRenewalTime);    if (!KEEP_DATA) {        fs.delete(outputDirPath, true);    }    cluster.shutdown();    cluster = null;}
1
public void close() throws IOException
{    try {        doThisInClose.call();    } catch (Throwable e) {        Throwables.propagateIfPossible(e, IOException.class);        throw new RuntimeException(e);    }}
0
public static void teardownClass()
{        if (oldTestBuildDataProp != null) {        System.setProperty(TEST_BUILD_DATA_KEY, oldTestBuildDataProp);    }    if (!KEEP_DATA) {        FileUtils.deleteQuietly(new File(DFS_DIR));    }}
0
public void getTextFormatter()
{    SequenceFileSerializer formatter = SequenceFileSerializerFactory.getSerializer("Text", new Context());    assertTrue(formatter != null);    assertTrue(formatter.getClass().getName(), formatter instanceof HDFSTextSerializer);}
0
public void getWritableFormatter()
{    SequenceFileSerializer formatter = SequenceFileSerializerFactory.getSerializer("Writable", new Context());    assertTrue(formatter != null);    assertTrue(formatter.getClass().getName(), formatter instanceof HDFSWritableSerializer);}
0
public void getCustomFormatter()
{    SequenceFileSerializer formatter = SequenceFileSerializerFactory.getSerializer("org.apache.flume.sink.hdfs.MyCustomSerializer$Builder", new Context());    assertTrue(formatter != null);    assertTrue(formatter.getClass().getName(), formatter instanceof MyCustomSerializer);}
0
public void setup() throws Exception
{    baseDir = Files.createTempDir();    testFile = new File(baseDir.getAbsoluteFile(), "test");    context = new Context();    event = EventBuilder.withBody("test", Charsets.UTF_8);}
0
public void teardown() throws Exception
{    FileUtils.deleteQuietly(baseDir);}
0
public void testTestFile() throws Exception
{    String file = testFile.getCanonicalPath();    HDFSDataStream stream = new HDFSDataStream();    context.put("hdfs.useRawLocalFileSystem", "true");    stream.configure(context);    stream.open(file);    stream.append(event);    stream.sync();    Assert.assertTrue(testFile.length() > 0);}
0
public void testCompressedFile() throws Exception
{    String file = testFile.getCanonicalPath();    HDFSCompressedDataStream stream = new HDFSCompressedDataStream();    context.put("hdfs.useRawLocalFileSystem", "true");    stream.configure(context);    stream.open(file, new GzipCodec(), CompressionType.RECORD);    stream.append(event);    stream.sync();    Assert.assertTrue(testFile.length() > 0);}
0
public void testSequenceFile() throws Exception
{    String file = testFile.getCanonicalPath();    HDFSSequenceFile stream = new HDFSSequenceFile();    context.put("hdfs.useRawLocalFileSystem", "true");    stream.configure(context);    stream.open(file);    stream.append(event);    stream.sync();    Assert.assertTrue(testFile.length() > 0);}
0
public void write(TransactionBatch txnBatch, Event e) throws StreamingException, IOException, InterruptedException
{    txnBatch.write(e.getBody());}
0
public void write(TransactionBatch txnBatch, Collection<byte[]> events) throws StreamingException, IOException, InterruptedException
{    txnBatch.write(events);}
0
public RecordWriter createRecordWriter(HiveEndPoint endPoint) throws StreamingException, IOException, ClassNotFoundException
{    if (serdeSeparator == null) {        return new DelimitedInputWriter(fieldToColMapping, delimiter, endPoint);    }    return new DelimitedInputWriter(fieldToColMapping, delimiter, endPoint, null, serdeSeparator);}
0
public void configure(Context context)
{    delimiter = parseDelimiterSpec(context.getString(SERIALIZER_DELIMITER, defaultDelimiter));    String fieldNames = context.getString(SERIALIZER_FIELDNAMES);    if (fieldNames == null) {        throw new IllegalArgumentException("serializer.fieldnames is not specified " + "for serializer " + this.getClass().getName());    }    String serdeSeparatorStr = context.getString(SERIALIZER_SERDE_SEPARATOR);    this.serdeSeparator = parseSerdeSeparatorSpec(serdeSeparatorStr);        fieldToColMapping = fieldNames.trim().split(",", -1);}
0
private static String parseDelimiterSpec(String delimiter)
{    if (delimiter == null) {        return null;    }    if (delimiter.charAt(0) == '"' && delimiter.charAt(delimiter.length() - 1) == '"') {        return delimiter.substring(1, delimiter.length() - 1);    }    return delimiter;}
0
private static Character parseSerdeSeparatorSpec(String separatorStr)
{    if (separatorStr == null) {        return null;    }    if (separatorStr.length() == 1) {        return separatorStr.charAt(0);    }    if (separatorStr.length() == 3 && separatorStr.charAt(2) == '\'' && separatorStr.charAt(separatorStr.length() - 1) == '\'') {        return separatorStr.charAt(1);    }    throw new IllegalArgumentException("serializer.serdeSeparator spec is invalid " + "for " + ALIAS + " serializer ");}
0
public void write(TransactionBatch txnBatch, Event e) throws StreamingException, IOException, InterruptedException
{    txnBatch.write(e.getBody());}
0
public void write(TransactionBatch txnBatch, Collection<byte[]> events) throws StreamingException, IOException, InterruptedException
{    txnBatch.write(events);}
0
public RecordWriter createRecordWriter(HiveEndPoint endPoint) throws StreamingException, IOException, ClassNotFoundException
{    return new StrictJsonWriter(endPoint);}
0
public void configure(Context context)
{    return;}
0
 Map<HiveEndPoint, HiveWriter> getAllWriters()
{    return allWriters;}
0
public void configure(Context context)
{    metaStoreUri = context.getString(Config.HIVE_METASTORE);    if (metaStoreUri == null) {        throw new IllegalArgumentException(Config.HIVE_METASTORE + " config setting is not " + "specified for sink " + getName());    }    if (metaStoreUri.equalsIgnoreCase("null")) {                metaStoreUri = null;    }        proxyUser = null;    database = context.getString(Config.HIVE_DATABASE);    if (database == null) {        throw new IllegalArgumentException(Config.HIVE_DATABASE + " config setting is not " + "specified for sink " + getName());    }    table = context.getString(Config.HIVE_TABLE);    if (table == null) {        throw new IllegalArgumentException(Config.HIVE_TABLE + " config setting is not " + "specified for sink " + getName());    }    String partitions = context.getString(Config.HIVE_PARTITION);    if (partitions != null) {        partitionVals = Arrays.asList(partitions.split(","));    }    txnsPerBatchAsk = context.getInteger(Config.HIVE_TXNS_PER_BATCH_ASK, DEFAULT_TXNSPERBATCH);    if (txnsPerBatchAsk < 0) {                txnsPerBatchAsk = DEFAULT_TXNSPERBATCH;    }    batchSize = context.getInteger(Config.BATCH_SIZE, DEFAULT_BATCHSIZE);    if (batchSize < 0) {                batchSize = DEFAULT_BATCHSIZE;    }    idleTimeout = context.getInteger(Config.IDLE_TIMEOUT, DEFAULT_IDLETIMEOUT);    if (idleTimeout < 0) {                idleTimeout = DEFAULT_IDLETIMEOUT;    }    callTimeout = context.getInteger(Config.CALL_TIMEOUT, DEFAULT_CALLTIMEOUT);    if (callTimeout < 0) {                callTimeout = DEFAULT_CALLTIMEOUT;    }    heartBeatInterval = context.getInteger(Config.HEART_BEAT_INTERVAL, DEFAULT_HEARTBEATINTERVAL);    if (heartBeatInterval < 0) {                heartBeatInterval = DEFAULT_HEARTBEATINTERVAL;    }    maxOpenConnections = context.getInteger(Config.MAX_OPEN_CONNECTIONS, DEFAULT_MAXOPENCONNECTIONS);    autoCreatePartitions = context.getBoolean("autoCreatePartitions", true);        useLocalTime = context.getBoolean(Config.USE_LOCAL_TIME_STAMP, false);    String tzName = context.getString(Config.TIME_ZONE);    timeZone = (tzName == null) ? null : TimeZone.getTimeZone(tzName);    needRounding = context.getBoolean(Config.ROUND, false);    String unit = context.getString(Config.ROUND_UNIT, Config.MINUTE);    if (unit.equalsIgnoreCase(Config.HOUR)) {        this.roundUnit = Calendar.HOUR_OF_DAY;    } else if (unit.equalsIgnoreCase(Config.MINUTE)) {        this.roundUnit = Calendar.MINUTE;    } else if (unit.equalsIgnoreCase(Config.SECOND)) {        this.roundUnit = Calendar.SECOND;    } else {                needRounding = false;    }    this.roundValue = context.getInteger(Config.ROUND_VALUE, 1);    if (roundUnit == Calendar.SECOND || roundUnit == Calendar.MINUTE) {        Preconditions.checkArgument(roundValue > 0 && roundValue <= 60, "Round value must be > 0 and <= 60");    } else if (roundUnit == Calendar.HOUR_OF_DAY) {        Preconditions.checkArgument(roundValue > 0 && roundValue <= 24, "Round value must be > 0 and <= 24");    }        serializerType = context.getString(Config.SERIALIZER, "");    if (serializerType.isEmpty()) {        throw new IllegalArgumentException("serializer config setting is not " + "specified for sink " + getName());    }    serializer = createSerializer(serializerType);    serializer.configure(context);    Preconditions.checkArgument(batchSize > 0, "batchSize must be greater than 0");    if (sinkCounter == null) {        sinkCounter = new SinkCounter(getName());    }}
1
protected SinkCounter getCounter()
{    return sinkCounter;}
0
private HiveEventSerializer createSerializer(String serializerName)
{    if (serializerName.compareToIgnoreCase(HiveDelimitedTextSerializer.ALIAS) == 0 || serializerName.compareTo(HiveDelimitedTextSerializer.class.getName()) == 0) {        return new HiveDelimitedTextSerializer();    } else if (serializerName.compareToIgnoreCase(HiveJsonSerializer.ALIAS) == 0 || serializerName.compareTo(HiveJsonSerializer.class.getName()) == 0) {        return new HiveJsonSerializer();    }    try {        return (HiveEventSerializer) Class.forName(serializerName).newInstance();    } catch (Exception e) {        throw new IllegalArgumentException("Unable to instantiate serializer: " + serializerName + " on sink: " + getName(), e);    }}
0
public Status process() throws EventDeliveryException
{        Channel channel = getChannel();    Transaction transaction = channel.getTransaction();    transaction.begin();    boolean success = false;    try {                if (timeToSendHeartBeat.compareAndSet(true, false)) {            enableHeartBeatOnAllWriters();        }                int txnEventCount = drainOneBatch(channel);        transaction.commit();        success = true;                if (txnEventCount < 1) {            return Status.BACKOFF;        } else {            return Status.READY;        }    } catch (InterruptedException err) {                return Status.BACKOFF;    } catch (Exception e) {        sinkCounter.incrementEventWriteOrChannelFail(e);        throw new EventDeliveryException(e);    } finally {        if (!success) {            transaction.rollback();        }        transaction.close();    }}
1
private int drainOneBatch(Channel channel) throws HiveWriter.Failure, InterruptedException
{    int txnEventCount = 0;    try {        Map<HiveEndPoint, HiveWriter> activeWriters = Maps.newHashMap();        for (; txnEventCount < batchSize; ++txnEventCount) {                        Event event = channel.take();            if (event == null) {                break;            }                        HiveEndPoint endPoint = makeEndPoint(metaStoreUri, database, table, partitionVals, event.getHeaders(), timeZone, needRounding, roundUnit, roundValue, useLocalTime);                        HiveWriter writer = getOrCreateWriter(activeWriters, endPoint);                                    writer.write(event);        }                if (txnEventCount == 0) {            sinkCounter.incrementBatchEmptyCount();        } else if (txnEventCount == batchSize) {            sinkCounter.incrementBatchCompleteCount();        } else {            sinkCounter.incrementBatchUnderflowCount();        }        sinkCounter.addToEventDrainAttemptCount(txnEventCount);                for (HiveWriter writer : activeWriters.values()) {            writer.flush(true);        }        sinkCounter.addToEventDrainSuccessCount(txnEventCount);        return txnEventCount;    } catch (HiveWriter.Failure e) {                        abortAllWriters();        closeAllWriters();        throw e;    }}
1
private void enableHeartBeatOnAllWriters()
{    for (HiveWriter writer : allWriters.values()) {        writer.setHearbeatNeeded();    }}
0
private HiveWriter getOrCreateWriter(Map<HiveEndPoint, HiveWriter> activeWriters, HiveEndPoint endPoint) throws HiveWriter.ConnectException, InterruptedException
{    try {        HiveWriter writer = allWriters.get(endPoint);        if (writer == null) {                        writer = new HiveWriter(endPoint, txnsPerBatchAsk, autoCreatePartitions, callTimeout, callTimeoutPool, proxyUser, serializer, sinkCounter);            sinkCounter.incrementConnectionCreatedCount();            if (allWriters.size() > maxOpenConnections) {                int retired = closeIdleWriters();                if (retired == 0) {                    closeEldestWriter();                }            }            allWriters.put(endPoint, writer);            activeWriters.put(endPoint, writer);        } else {            if (activeWriters.get(endPoint) == null) {                activeWriters.put(endPoint, writer);            }        }        return writer;    } catch (HiveWriter.ConnectException e) {        sinkCounter.incrementConnectionFailedCount();        throw e;    }}
1
private HiveEndPoint makeEndPoint(String metaStoreUri, String database, String table, List<String> partVals, Map<String, String> headers, TimeZone timeZone, boolean needRounding, int roundUnit, Integer roundValue, boolean useLocalTime)
{    if (partVals == null) {        return new HiveEndPoint(metaStoreUri, database, table, null);    }    ArrayList<String> realPartVals = Lists.newArrayList();    for (String partVal : partVals) {        realPartVals.add(BucketPath.escapeString(partVal, headers, timeZone, needRounding, roundUnit, roundValue, useLocalTime));    }    return new HiveEndPoint(metaStoreUri, database, table, realPartVals);}
0
private void closeEldestWriter() throws InterruptedException
{    long oldestTimeStamp = System.currentTimeMillis();    HiveEndPoint eldest = null;    for (Entry<HiveEndPoint, HiveWriter> entry : allWriters.entrySet()) {        if (entry.getValue().getLastUsed() < oldestTimeStamp) {            eldest = entry.getKey();            oldestTimeStamp = entry.getValue().getLastUsed();        }    }    try {        sinkCounter.incrementConnectionCreatedCount();                allWriters.remove(eldest).close();    } catch (InterruptedException e) {                throw e;    }}
1
private int closeIdleWriters() throws InterruptedException
{    int count = 0;    long now = System.currentTimeMillis();    ArrayList<HiveEndPoint> retirees = Lists.newArrayList();        for (Entry<HiveEndPoint, HiveWriter> entry : allWriters.entrySet()) {        if (now - entry.getValue().getLastUsed() > idleTimeout) {            ++count;            retirees.add(entry.getKey());        }    }        for (HiveEndPoint ep : retirees) {        sinkCounter.incrementConnectionClosedCount();                allWriters.remove(ep).close();    }    return count;}
1
private void closeAllWriters() throws InterruptedException
{        for (Entry<HiveEndPoint, HiveWriter> entry : allWriters.entrySet()) {        entry.getValue().close();    }        allWriters.clear();}
0
private void abortAllWriters() throws InterruptedException
{    for (Entry<HiveEndPoint, HiveWriter> entry : allWriters.entrySet()) {        entry.getValue().abort();    }}
0
public void stop()
{        for (Entry<HiveEndPoint, HiveWriter> entry : allWriters.entrySet()) {        try {            HiveWriter w = entry.getValue();            w.close();        } catch (InterruptedException ex) {            Thread.currentThread().interrupt();        }    }        callTimeoutPool.shutdown();    try {        while (callTimeoutPool.isTerminated() == false) {            callTimeoutPool.awaitTermination(Math.max(DEFAULT_CALLTIMEOUT, callTimeout), TimeUnit.MILLISECONDS);        }    } catch (InterruptedException ex) {            }    callTimeoutPool = null;    allWriters.clear();    allWriters = null;    sinkCounter.stop();    super.stop();    }
1
public void start()
{    String timeoutName = "hive-" + getName() + "-call-runner-%d";        callTimeoutPool = Executors.newFixedThreadPool(1, new ThreadFactoryBuilder().setNameFormat(timeoutName).build());    this.allWriters = Maps.newHashMap();    sinkCounter.start();    super.start();    setupHeartBeatTimer();    }
1
private void setupHeartBeatTimer()
{    if (heartBeatInterval > 0) {        heartBeatTimer.schedule(new TimerTask() {            @Override            public void run() {                timeToSendHeartBeat.set(true);                setupHeartBeatTimer();            }        }, heartBeatInterval * 1000);    }}
0
public void run()
{    timeToSendHeartBeat.set(true);    setupHeartBeatTimer();}
0
public long getBatchSize()
{    return batchSize;}
0
public String toString()
{    return "{ Sink type:" + getClass().getSimpleName() + ", name:" + getName() + " }";}
0
public String toString()
{    return endPoint.toString();}
0
private void resetCounters()
{    eventCounter = 0;    processSize = 0;    batchCounter = 0;}
0
 void setHearbeatNeeded()
{    hearbeatNeeded = true;}
0
public int getRemainingTxns()
{    return txnBatch.remainingTransactions();}
0
public synchronized void write(final Event event) throws WriteException, InterruptedException
{    if (closed) {        throw new IllegalStateException("Writer closed. Cannot write to : " + endPoint);    }    batch.add(event);    if (batch.size() == writeBatchSz) {                writeEventBatchToSerializer();    }        processSize += event.getBody().length;    eventCounter++;}
0
private void writeEventBatchToSerializer() throws InterruptedException, WriteException
{    try {        timedCall(new CallRunner1<Void>() {            @Override            public Void call() throws InterruptedException, StreamingException {                try {                    for (Event event : batch) {                        try {                            serializer.write(txnBatch, event);                        } catch (SerializationError err) {                                                    }                    }                    return null;                } catch (IOException e) {                    throw new StreamingIOFailure(e.getMessage(), e);                }            }        });        batch.clear();    } catch (StreamingException e) {        throw new WriteException(endPoint, txnBatch.getCurrentTxnId(), e);    } catch (TimeoutException e) {        throw new WriteException(endPoint, txnBatch.getCurrentTxnId(), e);    }}
1
public Void call() throws InterruptedException, StreamingException
{    try {        for (Event event : batch) {            try {                serializer.write(txnBatch, event);            } catch (SerializationError err) {                            }        }        return null;    } catch (IOException e) {        throw new StreamingIOFailure(e.getMessage(), e);    }}
1
public void flush(boolean rollToNext) throws CommitException, TxnBatchException, TxnFailure, InterruptedException, WriteException
{    if (!batch.isEmpty()) {        writeEventBatchToSerializer();        batch.clear();    }        if (hearbeatNeeded) {        hearbeatNeeded = false;        heartBeat();    }    lastUsed = System.currentTimeMillis();    try {                commitTxn();        if (txnBatch.remainingTransactions() == 0) {            closeTxnBatch();            txnBatch = null;            if (rollToNext) {                txnBatch = nextTxnBatch(recordWriter);            }        }                if (rollToNext) {                                    txnBatch.beginNextTransaction();        }    } catch (StreamingException e) {        throw new TxnFailure(txnBatch, e);    }}
1
public void abort() throws InterruptedException
{    batch.clear();    abortTxn();}
0
public void heartBeat() throws InterruptedException
{        try {        timedCall(new CallRunner1<Void>() {            @Override            public Void call() throws StreamingException {                                txnBatch.heartbeat();                return null;            }        });    } catch (InterruptedException e) {        throw e;    } catch (Exception e) {                }}
1
public Void call() throws StreamingException
{        txnBatch.heartbeat();    return null;}
1
public void close() throws InterruptedException
{    batch.clear();    abortRemainingTxns();    closeTxnBatch();    closeConnection();    closed = true;}
0
private void abortRemainingTxns() throws InterruptedException
{    try {        if (!isClosed(txnBatch.getCurrentTransactionState())) {            abortCurrTxnHelper();        }                if (txnBatch.remainingTransactions() > 0) {            timedCall(new CallRunner1<Void>() {                @Override                public Void call() throws StreamingException, InterruptedException {                    txnBatch.beginNextTransaction();                    return null;                }            });            abortRemainingTxns();        }    } catch (StreamingException e) {                return;    } catch (TimeoutException e) {                return;    }}
1
public Void call() throws StreamingException, InterruptedException
{    txnBatch.beginNextTransaction();    return null;}
0
private void abortCurrTxnHelper() throws TimeoutException, InterruptedException
{    try {        timedCall(new CallRunner1<Void>() {            @Override            public Void call() throws StreamingException, InterruptedException {                txnBatch.abort();                                return null;            }        });    } catch (StreamingException e) {                }}
1
public Void call() throws StreamingException, InterruptedException
{    txnBatch.abort();        return null;}
1
private boolean isClosed(TransactionBatch.TxnState txnState)
{    if (txnState == TransactionBatch.TxnState.COMMITTED) {        return true;    }    if (txnState == TransactionBatch.TxnState.ABORTED) {        return true;    }    return false;}
0
public void closeConnection() throws InterruptedException
{        try {        timedCall(new CallRunner1<Void>() {            @Override            public Void call() {                                connection.close();                return null;            }        });        sinkCounter.incrementConnectionClosedCount();    } catch (Exception e) {                }}
1
public Void call()
{        connection.close();    return null;}
0
private void commitTxn() throws CommitException, InterruptedException
{    if (LOG.isInfoEnabled()) {            }    try {        timedCall(new CallRunner1<Void>() {            @Override            public Void call() throws StreamingException, InterruptedException {                                txnBatch.commit();                return null;            }        });    } catch (Exception e) {        throw new CommitException(endPoint, txnBatch.getCurrentTxnId(), e);    }}
1
public Void call() throws StreamingException, InterruptedException
{        txnBatch.commit();    return null;}
0
private void abortTxn() throws InterruptedException
{        try {        timedCall(new CallRunner1<Void>() {            @Override            public Void call() throws StreamingException, InterruptedException {                                txnBatch.abort();                return null;            }        });    } catch (InterruptedException e) {        throw e;    } catch (TimeoutException e) {            } catch (Exception e) {                }}
1
public Void call() throws StreamingException, InterruptedException
{        txnBatch.abort();    return null;}
0
private StreamingConnection newConnection(final String proxyUser) throws InterruptedException, ConnectException
{    try {        return timedCall(new CallRunner1<StreamingConnection>() {            @Override            public StreamingConnection call() throws InterruptedException, StreamingException {                                return endPoint.newConnection(autoCreatePartitions);            }        });    } catch (Exception e) {        throw new ConnectException(endPoint, e);    }}
0
public StreamingConnection call() throws InterruptedException, StreamingException
{        return endPoint.newConnection(autoCreatePartitions);}
0
private TransactionBatch nextTxnBatch(final RecordWriter recordWriter) throws InterruptedException, TxnBatchException
{        TransactionBatch batch = null;    try {        batch = timedCall(new CallRunner1<TransactionBatch>() {            @Override            public TransactionBatch call() throws InterruptedException, StreamingException {                                return connection.fetchTransactionBatch(txnsPerBatch, recordWriter);            }        });            } catch (Exception e) {        throw new TxnBatchException(endPoint, e);    }    return batch;}
1
public TransactionBatch call() throws InterruptedException, StreamingException
{        return connection.fetchTransactionBatch(txnsPerBatch, recordWriter);}
0
private void closeTxnBatch() throws InterruptedException
{    try {                timedCall(new CallRunner1<Void>() {            @Override            public Void call() throws InterruptedException, StreamingException {                                txnBatch.close();                return null;            }        });    } catch (InterruptedException e) {        throw e;    } catch (Exception e) {                }}
1
public Void call() throws InterruptedException, StreamingException
{        txnBatch.close();    return null;}
0
private T timedCall(final CallRunner1<T> callRunner) throws TimeoutException, InterruptedException, StreamingException
{    Future<T> future = callTimeoutPool.submit(new Callable<T>() {        @Override        public T call() throws StreamingException, InterruptedException, Failure {            return callRunner.call();        }    });    try {        if (callTimeout > 0) {            return future.get(callTimeout, TimeUnit.MILLISECONDS);        } else {            return future.get();        }    } catch (TimeoutException eT) {        future.cancel(true);        sinkCounter.incrementConnectionFailedCount();        throw eT;    } catch (ExecutionException e1) {        sinkCounter.incrementConnectionFailedCount();        Throwable cause = e1.getCause();        if (cause instanceof IOException) {            throw new StreamingException("I/O Failure", (IOException) cause);        } else if (cause instanceof StreamingException) {            throw (StreamingException) cause;        } else if (cause instanceof TimeoutException) {            throw new StreamingException("Operation Timed Out.", (TimeoutException) cause);        } else if (cause instanceof RuntimeException) {            throw (RuntimeException) cause;        } else if (cause instanceof InterruptedException) {            throw (InterruptedException) cause;        }        throw new StreamingException(e1.getMessage(), e1);    }}
0
public T call() throws StreamingException, InterruptedException, Failure
{    return callRunner.call();}
0
 long getLastUsed()
{    return lastUsed;}
0
public void setUp() throws Exception
{    TestUtil.dropDB(conf, dbName);    sink = new HiveSink();    sink.setName("HiveSink-" + UUID.randomUUID().toString());    String dbLocation = dbFolder.newFolder(dbName).getCanonicalPath() + ".db";        dbLocation = dbLocation.replaceAll("\\\\", "/");    TestUtil.createDbAndTable(driver, dbName, tblName, partitionVals, colNames, colTypes, partNames, dbLocation);}
0
public void tearDown() throws MetaException, HiveException
{    TestUtil.dropDB(conf, dbName);}
0
public void testSingleWriter(boolean partitioned, String dbName, String tblName, Channel pChannel) throws Exception
{    int totalRecords = 4;    int batchSize = 2;    int batchCount = totalRecords / batchSize;    Context context = new Context();    context.put("hive.metastore", metaStoreURI);    context.put("hive.database", dbName);    context.put("hive.table", tblName);    if (partitioned) {        context.put("hive.partition", PART1_VALUE + "," + PART2_VALUE);    }    context.put("autoCreatePartitions", "false");    context.put("batchSize", "" + batchSize);    context.put("serializer", HiveDelimitedTextSerializer.ALIAS);    context.put("serializer.fieldnames", COL1 + ",," + COL2 + ",");    context.put("heartBeatInterval", "0");    Channel channel = startSink(sink, context, pChannel);    List<String> bodies = Lists.newArrayList();        Transaction txn = channel.getTransaction();    txn.begin();    for (int j = 1; j <= totalRecords; j++) {        Event event = new SimpleEvent();        String body = j + ",blah,This is a log message,other stuff";        event.setBody(body.getBytes());        bodies.add(body);        channel.put(event);    }        txn.commit();    txn.close();    checkRecordCountInTable(0, dbName, tblName);    for (int i = 0; i < batchCount; i++) {        sink.process();    }    checkRecordCountInTable(totalRecords, dbName, tblName);    sink.stop();    checkRecordCountInTable(totalRecords, dbName, tblName);}
0
public void testSingleWriterSimplePartitionedTable() throws Exception
{    testSingleWriter(true, dbName, tblName, null);}
0
public void testSingleWriterSimpleUnPartitionedTable() throws Exception
{    TestUtil.dropDB(conf, dbName2);    String dbLocation = dbFolder.newFolder(dbName2).getCanonicalPath() + ".db";        dbLocation = dbLocation.replaceAll("\\\\", "/");    TestUtil.createDbAndTable(driver, dbName2, tblName2, null, colNames2, colTypes2, null, dbLocation);    try {        testSingleWriter(false, dbName2, tblName2, null);    } finally {        TestUtil.dropDB(conf, dbName2);    }}
0
public void testSingleWriterUseHeaders() throws Exception
{    String[] colNames = { COL1, COL2 };    String PART1_NAME = "country";    String PART2_NAME = "hour";    String[] partNames = { PART1_NAME, PART2_NAME };    List<String> partitionVals = null;    String PART1_VALUE = "%{" + PART1_NAME + "}";    String PART2_VALUE = "%y-%m-%d-%k";    partitionVals = new ArrayList<String>(2);    partitionVals.add(PART1_VALUE);    partitionVals.add(PART2_VALUE);    String tblName = "hourlydata";    TestUtil.dropDB(conf, dbName2);    String dbLocation = dbFolder.newFolder(dbName2).getCanonicalPath() + ".db";        dbLocation = dbLocation.replaceAll("\\\\", "/");    TestUtil.createDbAndTable(driver, dbName2, tblName, partitionVals, colNames, colTypes, partNames, dbLocation);    int totalRecords = 4;    int batchSize = 2;    int batchCount = totalRecords / batchSize;    Context context = new Context();    context.put("hive.metastore", metaStoreURI);    context.put("hive.database", dbName2);    context.put("hive.table", tblName);    context.put("hive.partition", PART1_VALUE + "," + PART2_VALUE);    context.put("autoCreatePartitions", "true");    context.put("useLocalTimeStamp", "false");    context.put("batchSize", "" + batchSize);    context.put("serializer", HiveDelimitedTextSerializer.ALIAS);    context.put("serializer.fieldnames", COL1 + ",," + COL2 + ",");    context.put("heartBeatInterval", "0");    Channel channel = startSink(sink, context);    Calendar eventDate = Calendar.getInstance();    List<String> bodies = Lists.newArrayList();        Transaction txn = channel.getTransaction();    txn.begin();    for (int j = 1; j <= totalRecords; j++) {        Event event = new SimpleEvent();        String body = j + ",blah,This is a log message,other stuff";        event.setBody(body.getBytes());        eventDate.clear();                eventDate.set(2014, 03, 03, j % batchCount, 1);        event.getHeaders().put("timestamp", String.valueOf(eventDate.getTimeInMillis()));        event.getHeaders().put(PART1_NAME, "Asia");        bodies.add(body);        channel.put(event);    }        txn.commit();    txn.close();    checkRecordCountInTable(0, dbName2, tblName);    for (int i = 0; i < batchCount; i++) {        sink.process();    }    checkRecordCountInTable(totalRecords, dbName2, tblName);    sink.stop();        SinkCounter counter = sink.getCounter();    Assert.assertEquals(2, counter.getConnectionCreatedCount());    Assert.assertEquals(2, counter.getConnectionClosedCount());    Assert.assertEquals(2, counter.getBatchCompleteCount());    Assert.assertEquals(0, counter.getBatchEmptyCount());    Assert.assertEquals(0, counter.getConnectionFailedCount());    Assert.assertEquals(4, counter.getEventDrainAttemptCount());    Assert.assertEquals(4, counter.getEventDrainSuccessCount());}
0
public void testHeartBeat() throws EventDeliveryException, IOException, CommandNeedRetryException
{    int batchSize = 2;    int batchCount = 3;    int totalRecords = batchCount * batchSize;    Context context = new Context();    context.put("hive.metastore", metaStoreURI);    context.put("hive.database", dbName);    context.put("hive.table", tblName);    context.put("hive.partition", PART1_VALUE + "," + PART2_VALUE);    context.put("autoCreatePartitions", "true");    context.put("batchSize", "" + batchSize);    context.put("serializer", HiveDelimitedTextSerializer.ALIAS);    context.put("serializer.fieldnames", COL1 + ",," + COL2 + ",");    context.put("hive.txnsPerBatchAsk", "20");        context.put("heartBeatInterval", "3");    Channel channel = startSink(sink, context);    List<String> bodies = Lists.newArrayList();        for (int i = 0; i < batchCount; i++) {        Transaction txn = channel.getTransaction();        txn.begin();        for (int j = 1; j <= batchSize; j++) {            Event event = new SimpleEvent();            String body = i * j + ",blah,This is a log message,other stuff";            event.setBody(body.getBytes());            bodies.add(body);            channel.put(event);        }                txn.commit();        txn.close();        sink.process();                sleep(3000);    }    sink.stop();    checkRecordCountInTable(totalRecords, dbName, tblName);}
0
public void testJsonSerializer() throws Exception
{    int batchSize = 2;    int batchCount = 2;    int totalRecords = batchCount * batchSize;    Context context = new Context();    context.put("hive.metastore", metaStoreURI);    context.put("hive.database", dbName);    context.put("hive.table", tblName);    context.put("hive.partition", PART1_VALUE + "," + PART2_VALUE);    context.put("autoCreatePartitions", "true");    context.put("batchSize", "" + batchSize);    context.put("serializer", HiveJsonSerializer.ALIAS);    context.put("serializer.fieldnames", COL1 + ",," + COL2 + ",");    context.put("heartBeatInterval", "0");    Channel channel = startSink(sink, context);    List<String> bodies = Lists.newArrayList();        for (int i = 0; i < batchCount; i++) {        Transaction txn = channel.getTransaction();        txn.begin();        for (int j = 1; j <= batchSize; j++) {            Event event = new SimpleEvent();            String body = "{\"id\" : 1, \"msg\" : \"using json serializer\"}";            event.setBody(body.getBytes());            bodies.add(body);            channel.put(event);        }                txn.commit();        txn.close();        sink.process();    }    checkRecordCountInTable(totalRecords, dbName, tblName);    sink.stop();    checkRecordCountInTable(totalRecords, dbName, tblName);}
0
public void testErrorCounter() throws Exception
{    Channel channel = Mockito.mock(Channel.class);    Mockito.when(channel.take()).thenThrow(new ChannelException("dummy"));    Transaction transaction = Mockito.mock(BasicTransactionSemantics.class);    Mockito.when(channel.getTransaction()).thenReturn(transaction);    try {        testSingleWriter(true, dbName, tblName, channel);    } catch (EventDeliveryException e) {        }    SinkCounter sinkCounter = (SinkCounter) Whitebox.getInternalState(sink, "sinkCounter");    Assert.assertEquals(1, sinkCounter.getChannelReadFail());}
0
private void sleep(int n)
{    try {        Thread.sleep(n);    } catch (InterruptedException e) {    }}
0
private static Channel startSink(HiveSink sink, Context context)
{    return startSink(sink, context, null);}
0
private static Channel startSink(HiveSink sink, Context context, Channel pChannel)
{    Configurables.configure(sink, context);    Channel channel = pChannel == null ? new MemoryChannel() : pChannel;    Configurables.configure(channel, context);    sink.setChannel(channel);    sink.start();    return channel;}
0
private void checkRecordCountInTable(int expectedCount, String db, String tbl) throws CommandNeedRetryException, IOException
{    int count = TestUtil.listRecordsInTable(driver, db, tbl).size();    Assert.assertEquals(expectedCount, count);}
0
public void setUp() throws Exception
{        TxnDbUtil.cleanDb();    TxnDbUtil.prepDb();        TestUtil.dropDB(conf, dbName);    String dbLocation = dbFolder.newFolder(dbName).getCanonicalPath() + ".db";        dbLocation = dbLocation.replaceAll("\\\\", "/");    TestUtil.createDbAndTable(driver, dbName, tblName, partVals, colNames, colTypes, partNames, dbLocation);        Context ctx = new Context();    ctx.put("serializer.fieldnames", COL1 + ",," + COL2 + ",");    serializer = new HiveDelimitedTextSerializer();    serializer.configure(ctx);}
0
public void testInstantiate() throws Exception
{    HiveEndPoint endPoint = new HiveEndPoint(metaStoreURI, dbName, tblName, partVals);    SinkCounter sinkCounter = new SinkCounter(this.getClass().getName());    HiveWriter writer = new HiveWriter(endPoint, 10, true, timeout, callTimeoutPool, "flumetest", serializer, sinkCounter);    writer.close();}
0
public void testWriteBasic() throws Exception
{    HiveEndPoint endPoint = new HiveEndPoint(metaStoreURI, dbName, tblName, partVals);    SinkCounter sinkCounter = new SinkCounter(this.getClass().getName());    HiveWriter writer = new HiveWriter(endPoint, 10, true, timeout, callTimeoutPool, "flumetest", serializer, sinkCounter);    writeEvents(writer, 3);    writer.flush(false);    writer.close();    checkRecordCountInTable(3);}
0
public void testWriteMultiFlush() throws Exception
{    HiveEndPoint endPoint = new HiveEndPoint(metaStoreURI, dbName, tblName, partVals);    SinkCounter sinkCounter = new SinkCounter(this.getClass().getName());    HiveWriter writer = new HiveWriter(endPoint, 10, true, timeout, callTimeoutPool, "flumetest", serializer, sinkCounter);    checkRecordCountInTable(0);    SimpleEvent event = new SimpleEvent();    String REC1 = "1,xyz,Hello world,abc";    event.setBody(REC1.getBytes());    writer.write(event);    checkRecordCountInTable(0);    writer.flush(true);    checkRecordCountInTable(1);    String REC2 = "2,xyz,Hello world,abc";    event.setBody(REC2.getBytes());    writer.write(event);    checkRecordCountInTable(1);    writer.flush(true);    checkRecordCountInTable(2);    String REC3 = "3,xyz,Hello world,abc";    event.setBody(REC3.getBytes());    writer.write(event);    writer.flush(true);    checkRecordCountInTable(3);    writer.close();    checkRecordCountInTable(3);}
0
public void testTxnBatchConsumption() throws Exception
{            HiveEndPoint endPoint = new HiveEndPoint(metaStoreURI, dbName, tblName, partVals);    SinkCounter sinkCounter = new SinkCounter(this.getClass().getName());    int txnPerBatch = 3;    HiveWriter writer = new HiveWriter(endPoint, txnPerBatch, true, timeout, callTimeoutPool, "flumetest", serializer, sinkCounter);    Assert.assertEquals(writer.getRemainingTxns(), 2);    writer.flush(true);    Assert.assertEquals(writer.getRemainingTxns(), 1);    writer.flush(true);    Assert.assertEquals(writer.getRemainingTxns(), 0);    writer.flush(true);        Assert.assertEquals(writer.getRemainingTxns(), 2);    writer.flush(true);    Assert.assertEquals(writer.getRemainingTxns(), 1);    writer.close();}
0
private void checkRecordCountInTable(int expectedCount) throws CommandNeedRetryException, IOException
{    int count = TestUtil.listRecordsInTable(driver, dbName, tblName).size();    Assert.assertEquals(expectedCount, count);}
0
public void testInOrderWrite() throws Exception
{    HiveEndPoint endPoint = new HiveEndPoint(metaStoreURI, dbName, tblName, partVals);    SinkCounter sinkCounter = new SinkCounter(this.getClass().getName());        int timeout = 5000;    HiveDelimitedTextSerializer serializer2 = new HiveDelimitedTextSerializer();    Context ctx = new Context();    ctx.put("serializer.fieldnames", COL1 + "," + COL2);    ctx.put("serializer.serdeSeparator", ",");    serializer2.configure(ctx);    HiveWriter writer = new HiveWriter(endPoint, 10, true, timeout, callTimeoutPool, "flumetest", serializer2, sinkCounter);    SimpleEvent event = new SimpleEvent();    event.setBody("1,Hello world 1".getBytes());    writer.write(event);    event.setBody("2,Hello world 2".getBytes());    writer.write(event);    event.setBody("3,Hello world 3".getBytes());    writer.write(event);    writer.flush(false);    writer.close();}
0
public void testSerdeSeparatorCharParsing() throws Exception
{    HiveEndPoint endPoint = new HiveEndPoint(metaStoreURI, dbName, tblName, partVals);    SinkCounter sinkCounter = new SinkCounter(this.getClass().getName());        int timeout = 10000;        HiveDelimitedTextSerializer serializer1 = new HiveDelimitedTextSerializer();    Context ctx = new Context();    ctx.put("serializer.fieldnames", COL1 + "," + COL2);    ctx.put("serializer.serdeSeparator", ",");    serializer1.configure(ctx);            HiveDelimitedTextSerializer serializer2 = new HiveDelimitedTextSerializer();    ctx = new Context();    ctx.put("serializer.fieldnames", COL1 + "," + COL2);    ctx.put("serializer.serdeSeparator", "'\t'");    serializer2.configure(ctx);            HiveDelimitedTextSerializer serializer3 = new HiveDelimitedTextSerializer();    ctx = new Context();    ctx.put("serializer.fieldnames", COL1 + "," + COL2);    ctx.put("serializer.serdeSeparator", "ab");    try {        serializer3.configure(ctx);        Assert.assertTrue("Bad serdeSeparator character was accepted", false);    } catch (Exception e) {        }}
0
public void testSecondWriterBeforeFirstCommits() throws Exception
{        HiveEndPoint endPoint1 = new HiveEndPoint(metaStoreURI, dbName, tblName, partVals);    ArrayList<String> partVals2 = new ArrayList<String>(2);    partVals2.add(PART1_VALUE);    partVals2.add("Nepal");    HiveEndPoint endPoint2 = new HiveEndPoint(metaStoreURI, dbName, tblName, partVals2);    SinkCounter sinkCounter1 = new SinkCounter(this.getClass().getName());    SinkCounter sinkCounter2 = new SinkCounter(this.getClass().getName());    HiveWriter writer1 = new HiveWriter(endPoint1, 10, true, timeout, callTimeoutPool, "flumetest", serializer, sinkCounter1);    writeEvents(writer1, 3);    HiveWriter writer2 = new HiveWriter(endPoint2, 10, true, timeout, callTimeoutPool, "flumetest", serializer, sinkCounter2);    writeEvents(writer2, 3);        writer2.flush(false);        writer1.flush(false);    writer1.close();    writer2.close();}
0
public void testSecondWriterAfterFirstCommits() throws Exception
{        HiveEndPoint endPoint1 = new HiveEndPoint(metaStoreURI, dbName, tblName, partVals);    ArrayList<String> partVals2 = new ArrayList<String>(2);    partVals2.add(PART1_VALUE);    partVals2.add("Nepal");    HiveEndPoint endPoint2 = new HiveEndPoint(metaStoreURI, dbName, tblName, partVals2);    SinkCounter sinkCounter1 = new SinkCounter(this.getClass().getName());    SinkCounter sinkCounter2 = new SinkCounter(this.getClass().getName());    HiveWriter writer1 = new HiveWriter(endPoint1, 10, true, timeout, callTimeoutPool, "flumetest", serializer, sinkCounter1);    writeEvents(writer1, 3);        writer1.flush(false);    HiveWriter writer2 = new HiveWriter(endPoint2, 10, true, timeout, callTimeoutPool, "flumetest", serializer, sinkCounter2);    writeEvents(writer2, 3);        writer2.flush(false);    writer1.close();    writer2.close();}
0
private void writeEvents(HiveWriter writer, int count) throws InterruptedException, HiveWriter.WriteException
{    SimpleEvent event = new SimpleEvent();    for (int i = 1; i <= count; i++) {        event.setBody((i + ",xyz,Hello world,abc").getBytes());        writer.write(event);    }}
0
public static void setConfValues(HiveConf conf)
{    conf.setVar(HiveConf.ConfVars.HIVE_TXN_MANAGER, txnMgr);    conf.setBoolVar(HiveConf.ConfVars.HIVE_SUPPORT_CONCURRENCY, true);    conf.set("fs.raw.impl", RawFileSystem.class.getName());}
0
public static void createDbAndTable(Driver driver, String databaseName, String tableName, List<String> partVals, String[] colNames, String[] colTypes, String[] partNames, String dbLocation) throws Exception
{    String dbUri = "raw://" + dbLocation;    String tableLoc = dbUri + Path.SEPARATOR + tableName;    runDDL(driver, "create database IF NOT EXISTS " + databaseName + " location '" + dbUri + "'");    runDDL(driver, "use " + databaseName);    String crtTbl = "create table " + tableName + " ( " + getTableColumnsStr(colNames, colTypes) + " )" + getPartitionStmtStr(partNames) + " clustered by ( " + colNames[0] + " )" + " into 10 buckets " + " stored as orc " + " location '" + tableLoc + "'" + " TBLPROPERTIES ('transactional'='true')";    runDDL(driver, crtTbl);    System.out.println("crtTbl = " + crtTbl);    if (partNames != null && partNames.length != 0) {        String addPart = "alter table " + tableName + " add partition ( " + getTablePartsStr2(partNames, partVals) + " )";        runDDL(driver, addPart);    }}
0
private static String getPartitionStmtStr(String[] partNames)
{    if (partNames == null || partNames.length == 0) {        return "";    }    return " partitioned by (" + getTablePartsStr(partNames) + " )";}
0
public static void dropDB(HiveConf conf, String databaseName) throws HiveException, MetaException
{    IMetaStoreClient client = new HiveMetaStoreClient(conf);    try {        for (String table : client.listTableNamesByFilter(databaseName, "", (short) -1)) {            client.dropTable(databaseName, table, true, true);        }        client.dropDatabase(databaseName);    } catch (TException e) {        client.close();    }}
0
private static String getTableColumnsStr(String[] colNames, String[] colTypes)
{    StringBuffer sb = new StringBuffer();    for (int i = 0; i < colNames.length; ++i) {        sb.append(colNames[i] + " " + colTypes[i]);        if (i < colNames.length - 1) {            sb.append(",");        }    }    return sb.toString();}
0
private static String getTablePartsStr(String[] partNames)
{    if (partNames == null || partNames.length == 0) {        return "";    }    StringBuffer sb = new StringBuffer();    for (int i = 0; i < partNames.length; ++i) {        sb.append(partNames[i] + " string");        if (i < partNames.length - 1) {            sb.append(",");        }    }    return sb.toString();}
0
private static String getTablePartsStr2(String[] partNames, List<String> partVals)
{    StringBuffer sb = new StringBuffer();    for (int i = 0; i < partVals.size(); ++i) {        sb.append(partNames[i] + " = '" + partVals.get(i) + "'");        if (i < partVals.size() - 1) {            sb.append(",");        }    }    return sb.toString();}
0
public static ArrayList<String> listRecordsInTable(Driver driver, String dbName, String tblName) throws CommandNeedRetryException, IOException
{    driver.run("select * from " + dbName + "." + tblName);    ArrayList<String> res = new ArrayList<String>();    driver.getResults(res);    return res;}
0
public static ArrayList<String> listRecordsInPartition(Driver driver, String dbName, String tblName, String continent, String country) throws CommandNeedRetryException, IOException
{    driver.run("select * from " + dbName + "." + tblName + " where continent='" + continent + "' and country='" + country + "'");    ArrayList<String> res = new ArrayList<String>();    driver.getResults(res);    return res;}
0
public URI getUri()
{    return NAME;}
0
 static String execCommand(File f, String... cmd) throws IOException
{    String[] args = new String[cmd.length + 1];    System.arraycopy(cmd, 0, args, 0, cmd.length);    args[cmd.length] = f.getCanonicalPath();    String output = Shell.execCommand(args);    return output;}
0
public FileStatus getFileStatus(Path path) throws IOException
{    File file = pathToFile(path);    if (!file.exists()) {        throw new FileNotFoundException("Can't find " + path);    }        short mod = 0;    if (file.canRead()) {        mod |= 0444;    }    if (file.canWrite()) {        mod |= 0200;    }    if (file.canExecute()) {        mod |= 0111;    }    ShimLoader.getHadoopShims();    return new FileStatus(file.length(), file.isDirectory(), 1, 1024, file.lastModified(), file.lastModified(), FsPermission.createImmutable(mod), "owen", "users", path);}
0
private static boolean runDDL(Driver driver, String sql) throws QueryFailedException
{        int retryCount = 1;    for (int attempt = 0; attempt <= retryCount; ++attempt) {        try {            driver.run(sql);            return true;        } catch (CommandNeedRetryException e) {            if (attempt == retryCount) {                throw new QueryFailedException(sql, e);            }            continue;        }    }        return false;}
0
public final void configure(final Context context)
{    String configuredEndpoint = context.getString("endpoint", "");        try {        endpointUrl = new URL(configuredEndpoint);    } catch (MalformedURLException e) {        throw new IllegalArgumentException("Endpoint URL invalid", e);    }    connectTimeout = context.getInteger("connectTimeout", DEFAULT_CONNECT_TIMEOUT);    if (connectTimeout <= 0) {        throw new IllegalArgumentException("Connect timeout must be a non-zero and positive");    }        requestTimeout = context.getInteger("requestTimeout", DEFAULT_REQUEST_TIMEOUT);    if (requestTimeout <= 0) {        throw new IllegalArgumentException("Request timeout must be a non-zero and positive");    }        acceptHeader = context.getString("acceptHeader", DEFAULT_ACCEPT_HEADER);        contentTypeHeader = context.getString("contentTypeHeader", DEFAULT_CONTENT_TYPE);        defaultBackoff = context.getBoolean("defaultBackoff", true);        defaultRollback = context.getBoolean("defaultRollback", true);        defaultIncrementMetrics = context.getBoolean("defaultIncrementMetrics", false);        parseConfigOverrides("backoff", context, backoffOverrides);    parseConfigOverrides("rollback", context, rollbackOverrides);    parseConfigOverrides("incrementMetrics", context, incrementMetricsOverrides);    if (this.sinkCounter == null) {        this.sinkCounter = new SinkCounter(this.getName());    }    connectionBuilder = new ConnectionBuilder();}
1
public final void start()
{        sinkCounter.start();}
1
public final void stop()
{        sinkCounter.stop();}
1
public final Status process() throws EventDeliveryException
{    Status status = null;    OutputStream outputStream = null;    Channel ch = getChannel();    Transaction txn = ch.getTransaction();    txn.begin();    try {        Event event = ch.take();        byte[] eventBody = null;        if (event != null) {            eventBody = event.getBody();        }        if (eventBody != null && eventBody.length > 0) {            sinkCounter.incrementEventDrainAttemptCount();                        try {                HttpURLConnection connection = connectionBuilder.getConnection();                outputStream = connection.getOutputStream();                outputStream.write(eventBody);                outputStream.flush();                outputStream.close();                int httpStatusCode = connection.getResponseCode();                                if (httpStatusCode < HttpURLConnection.HTTP_BAD_REQUEST) {                    connection.getInputStream().close();                } else {                                        connection.getErrorStream().close();                }                                if (httpStatusCode >= HTTP_STATUS_CONTINUE) {                    String httpStatusString = String.valueOf(httpStatusCode);                    boolean shouldRollback = findOverrideValue(httpStatusString, rollbackOverrides, defaultRollback);                    if (shouldRollback) {                        txn.rollback();                    } else {                        txn.commit();                    }                    boolean shouldBackoff = findOverrideValue(httpStatusString, backoffOverrides, defaultBackoff);                    if (shouldBackoff) {                        status = Status.BACKOFF;                    } else {                        status = Status.READY;                    }                    boolean shouldIncrementMetrics = findOverrideValue(httpStatusString, incrementMetricsOverrides, defaultIncrementMetrics);                    if (shouldIncrementMetrics) {                        sinkCounter.incrementEventDrainSuccessCount();                    }                    if (shouldRollback) {                        if (shouldBackoff) {                                                    } else {                                                    }                    }                } else {                    txn.rollback();                    status = Status.BACKOFF;                                    }            } catch (IOException e) {                txn.rollback();                status = Status.BACKOFF;                                sinkCounter.incrementEventWriteFail();            }        } else {            txn.commit();            status = Status.BACKOFF;                    }    } catch (Throwable t) {        txn.rollback();        status = Status.BACKOFF;                sinkCounter.incrementEventWriteOrChannelFail(t);                if (t instanceof Error) {            throw (Error) t;        }    } finally {        txn.close();        if (outputStream != null) {            try {                outputStream.close();            } catch (IOException e) {                        }        }    }    return status;}
1
private void parseConfigOverrides(final String propertyName, final Context context, final Map<String, Boolean> override)
{    Map<String, String> config = context.getSubProperties(propertyName + ".");    if (config != null) {        for (Map.Entry<String, String> value : config.entrySet()) {                        if (override.containsKey(value.getKey())) {                            } else {                override.put(value.getKey(), Boolean.valueOf(value.getValue()));            }        }    }}
1
private boolean findOverrideValue(final String statusCode, final HashMap<String, Boolean> overrides, final boolean defaultValue)
{    Boolean overrideValue = overrides.get(statusCode);    if (overrideValue == null) {        overrideValue = overrides.get(statusCode.substring(0, 1) + "XX");        if (overrideValue == null) {            overrideValue = defaultValue;        }    }    return overrideValue;}
0
 final void setConnectionBuilder(final ConnectionBuilder builder)
{    this.connectionBuilder = builder;}
0
 final void setSinkCounter(final SinkCounter newSinkCounter)
{    this.sinkCounter = newSinkCounter;}
0
public HttpURLConnection getConnection() throws IOException
{    HttpURLConnection connection = (HttpURLConnection) endpointUrl.openConnection();    connection.setRequestMethod("POST");    connection.setRequestProperty("Content-Type", contentTypeHeader);    connection.setRequestProperty("Accept", acceptHeader);    connection.setConnectTimeout(connectTimeout);    connection.setReadTimeout(requestTimeout);    connection.setDoOutput(true);    connection.setDoInput(true);    connection.connect();    return connection;}
0
public void ensureAllConfigurationOptionsRead()
{    whenDefaultStringConfig();    whenDefaultBooleanConfig();    when(configContext.getInteger(eq("connectTimeout"), Mockito.anyInt())).thenReturn(1000);    when(configContext.getInteger(eq("requestTimeout"), Mockito.anyInt())).thenReturn(1000);    new HttpSink().configure(configContext);    verify(configContext).getString("endpoint", "");    verify(configContext).getInteger(eq("connectTimeout"), Mockito.anyInt());    verify(configContext).getInteger(eq("requestTimeout"), Mockito.anyInt());    verify(configContext).getString(eq("acceptHeader"), Mockito.anyString());    verify(configContext).getString(eq("contentTypeHeader"), Mockito.anyString());    verify(configContext).getBoolean("defaultBackoff", true);    verify(configContext).getBoolean("defaultRollback", true);    verify(configContext).getBoolean("defaultIncrementMetrics", false);}
0
public void ensureExceptionIfEndpointUrlEmpty()
{    when(configContext.getString("endpoint", "")).thenReturn("");    new HttpSink().configure(configContext);}
0
public void ensureExceptionIfEndpointUrlInvalid()
{    when(configContext.getString("endpoint", "")).thenReturn("invalid url");    new HttpSink().configure(configContext);}
0
public void ensureExceptionIfConnectTimeoutNegative()
{    whenDefaultStringConfig();    when(configContext.getInteger("connectTimeout", 1000)).thenReturn(-1000);    when(configContext.getInteger(eq("requestTimeout"), Mockito.anyInt())).thenReturn(1000);    new HttpSink().configure(configContext);}
0
public void ensureDefaultConnectTimeoutCorrect()
{    whenDefaultStringConfig();    when(configContext.getInteger("connectTimeout", DEFAULT_CONNECT_TIMEOUT)).thenReturn(1000);    when(configContext.getInteger(eq("requestTimeout"), Mockito.anyInt())).thenReturn(1000);    new HttpSink().configure(configContext);    verify(configContext).getInteger("connectTimeout", DEFAULT_CONNECT_TIMEOUT);}
0
public void ensureExceptionIfRequestTimeoutNegative()
{    whenDefaultStringConfig();    when(configContext.getInteger("requestTimeout", 1000)).thenReturn(-1000);    when(configContext.getInteger(eq("connectTimeout"), Mockito.anyInt())).thenReturn(1000);    new HttpSink().configure(configContext);}
0
public void ensureDefaultRequestTimeoutCorrect()
{    whenDefaultStringConfig();    when(configContext.getInteger("requestTimeout", DEFAULT_REQUEST_TIMEOUT)).thenReturn(1000);    when(configContext.getInteger(eq("connectTimeout"), Mockito.anyInt())).thenReturn(1000);    new HttpSink().configure(configContext);    verify(configContext).getInteger("requestTimeout", DEFAULT_REQUEST_TIMEOUT);}
0
public void ensureDefaultAcceptHeaderCorrect()
{    whenDefaultTimeouts();    whenDefaultStringConfig();    new HttpSink().configure(configContext);    verify(configContext).getString("acceptHeader", DEFAULT_ACCEPT_HEADER);}
0
public void ensureDefaultContentTypeHeaderCorrect()
{    whenDefaultTimeouts();    whenDefaultStringConfig();    new HttpSink().configure(configContext);    verify(configContext).getString("contentTypeHeader", DEFAULT_CONTENT_TYPE_HEADER);}
0
public void ensureBackoffOnNullEvent() throws Exception
{    when(channel.take()).thenReturn(null);    executeWithMocks(true);}
0
public void ensureBackoffOnNullEventBody() throws Exception
{    when(channel.take()).thenReturn(event);    when(event.getBody()).thenReturn(null);    executeWithMocks(true);}
0
public void ensureBackoffOnEmptyEvent() throws Exception
{    when(channel.take()).thenReturn(event);    when(event.getBody()).thenReturn(new byte[] {});    executeWithMocks(true);}
0
public void ensureRollbackBackoffAndIncrementMetricsIfConfigured() throws Exception
{    when(channel.take()).thenReturn(event);    when(event.getBody()).thenReturn("something".getBytes());    Context context = new Context();    context.put("defaultRollback", "true");    context.put("defaultBackoff", "true");    context.put("defaultIncrementMetrics", "true");    executeWithMocks(false, Status.BACKOFF, true, true, context, HttpURLConnection.HTTP_OK);}
0
public void ensureCommitReadyAndNoIncrementMetricsIfConfigured() throws Exception
{    when(channel.take()).thenReturn(event);    when(event.getBody()).thenReturn("something".getBytes());    Context context = new Context();    context.put("defaultRollback", "false");    context.put("defaultBackoff", "false");    context.put("defaultIncrementMetrics", "false");    executeWithMocks(true, Status.READY, false, false, context, HttpURLConnection.HTTP_OK);}
0
public void ensureSingleStatusConfigurationCorrectlyUsed() throws Exception
{    when(channel.take()).thenReturn(event);    when(event.getBody()).thenReturn("something".getBytes());    Context context = new Context();    context.put("defaultRollback", "true");    context.put("defaultBackoff", "true");    context.put("defaultIncrementMetrics", "false");    context.put("rollback.200", "false");    context.put("backoff.200", "false");    context.put("incrementMetrics.200", "true");    executeWithMocks(true, Status.READY, true, true, context, HttpURLConnection.HTTP_OK);}
0
public void testErrorCounter() throws Exception
{    RuntimeException exception = new RuntimeException("dummy");    when(channel.take()).thenThrow(exception);    Context context = new Context();    context.put("defaultRollback", "false");    context.put("defaultBackoff", "false");    context.put("defaultIncrementMetrics", "false");    executeWithMocks(false, Status.BACKOFF, false, false, context, HttpURLConnection.HTTP_OK);    inOrder(sinkCounter).verify(sinkCounter).incrementEventWriteOrChannelFail(exception);}
0
public void ensureSingleErrorStatusConfigurationCorrectlyUsed() throws Exception
{    when(channel.take()).thenReturn(event);    when(event.getBody()).thenReturn("something".getBytes());    Context context = new Context();    context.put("defaultRollback", "true");    context.put("defaultBackoff", "true");    context.put("defaultIncrementMetrics", "false");    context.put("rollback.401", "false");    context.put("backoff.401", "false");    context.put("incrementMetrics.401", "false");    executeWithMocks(true, Status.READY, false, true, context, HttpURLConnection.HTTP_UNAUTHORIZED);}
0
public void ensureGroupConfigurationCorrectlyUsed() throws Exception
{    when(channel.take()).thenReturn(event);    when(event.getBody()).thenReturn("something".getBytes());    Context context = new Context();    context.put("defaultRollback", "true");    context.put("defaultBackoff", "true");    context.put("defaultIncrementMetrics", "false");    context.put("rollback.2XX", "false");    context.put("backoff.2XX", "false");    context.put("incrementMetrics.2XX", "true");    executeWithMocks(true, Status.READY, true, true, context, HttpURLConnection.HTTP_OK);    executeWithMocks(true, Status.READY, true, true, context, HttpURLConnection.HTTP_NO_CONTENT);}
0
public void ensureSingleStatusConfigurationOverridesGroupConfigurationCorrectly() throws Exception
{    when(channel.take()).thenReturn(event);    when(event.getBody()).thenReturn("something".getBytes());    Context context = new Context();    context.put("rollback.2XX", "false");    context.put("backoff.2XX", "false");    context.put("incrementMetrics.2XX", "true");    context.put("rollback.200", "true");    context.put("backoff.200", "true");    context.put("incrementMetrics.200", "false");    executeWithMocks(true, Status.READY, true, true, context, HttpURLConnection.HTTP_NO_CONTENT);    executeWithMocks(false, Status.BACKOFF, false, true, context, HttpURLConnection.HTTP_OK);}
0
private void executeWithMocks(boolean commit) throws Exception
{    Context context = new Context();    executeWithMocks(commit, Status.BACKOFF, false, false, context, HttpURLConnection.HTTP_OK);}
0
private void executeWithMocks(boolean expectedCommit, Status expectedStatus, boolean expectedIncrementSuccessMetrics, boolean expectedIncrementAttemptMetrics, Context context, int httpStatus) throws Exception
{    context.put("endpoint", "http://localhost:8080/endpoint");    HttpSink httpSink = new HttpSink();    httpSink.configure(context);    httpSink.setConnectionBuilder(httpSink.new ConnectionBuilder() {        @Override        public HttpURLConnection getConnection() throws IOException {            return httpURLConnection;        }    });    httpSink.setChannel(channel);    httpSink.setSinkCounter(sinkCounter);    when(channel.getTransaction()).thenReturn(transaction);    when(httpURLConnection.getOutputStream()).thenReturn(outputStream);    when(httpURLConnection.getInputStream()).thenReturn(inputStream);    when(httpURLConnection.getErrorStream()).thenReturn(inputStream);    when(httpURLConnection.getResponseCode()).thenReturn(httpStatus);    Status actualStatus = httpSink.process();    assert (actualStatus == expectedStatus);    inOrder(transaction).verify(transaction).begin();    if (expectedIncrementAttemptMetrics) {        inOrder(sinkCounter).verify(sinkCounter).incrementEventDrainAttemptCount();    }    if (expectedCommit) {        inOrder(transaction).verify(transaction).commit();    } else {        inOrder(transaction).verify(transaction).rollback();    }    if (expectedIncrementSuccessMetrics) {        inOrder(sinkCounter).verify(sinkCounter).incrementEventDrainSuccessCount();    }    inOrder(transaction).verify(transaction).close();}
0
public HttpURLConnection getConnection() throws IOException
{    return httpURLConnection;}
0
private void whenDefaultStringConfig()
{    when(configContext.getString("endpoint", "")).thenReturn("http://test.abc/");    when(configContext.getString("acceptHeader", "")).thenReturn("test/accept");    when(configContext.getString("contentTypeHeader", "")).thenReturn("test/content");}
0
private void whenDefaultBooleanConfig()
{    when(configContext.getBoolean("defaultBackoff", true)).thenReturn(true);    when(configContext.getBoolean("defaultRollback", true)).thenReturn(true);    when(configContext.getBoolean("defaultIncrementMetrics", false)).thenReturn(true);}
0
private void whenDefaultTimeouts()
{    when(configContext.getInteger(eq("requestTimeout"), Mockito.anyInt())).thenReturn(1000);    when(configContext.getInteger(eq("connectTimeout"), Mockito.anyInt())).thenReturn(1000);}
0
private static int findFreePort()
{    try (ServerSocket socket = new ServerSocket(0)) {        return socket.getLocalPort();    } catch (IOException e) {        throw new AssertionError("Can not find free port.", e);    }}
0
public void setupSink()
{    if (httpSink == null) {        Context httpSinkContext = new Context();        httpSinkContext.put("endpoint", "http://localhost:" + port + "/endpoint");        httpSinkContext.put("requestTimeout", "2000");        httpSinkContext.put("connectTimeout", "1500");        httpSinkContext.put("acceptHeader", "application/json");        httpSinkContext.put("contentTypeHeader", "application/json");        httpSinkContext.put("backoff.200", "false");        httpSinkContext.put("rollback.200", "false");        httpSinkContext.put("backoff.401", "false");        httpSinkContext.put("rollback.401", "false");        httpSinkContext.put("incrementMetrics.200", "true");        Context memoryChannelContext = new Context();        channel = new MemoryChannel();        channel.configure(memoryChannelContext);        channel.start();        httpSink = new HttpSink();        httpSink.configure(httpSinkContext);        httpSink.setChannel(channel);        httpSink.start();    }}
0
public void waitForShutdown() throws InterruptedException
{    httpSink.stop();    Thread.sleep(500);}
0
public void ensureSuccessfulMessageDelivery() throws Exception
{    service.stubFor(post(urlEqualTo("/endpoint")).withRequestBody(equalToJson(event("SUCCESS"))).willReturn(aResponse().withStatus(200)));    addEventToChannel(event("SUCCESS"));    service.verify(1, postRequestedFor(urlEqualTo("/endpoint")).withRequestBody(equalToJson(event("SUCCESS"))));}
0
public void ensureEventsResentOn503Failure() throws Exception
{    String errorScenario = "Error Scenario";    service.stubFor(post(urlEqualTo("/endpoint")).inScenario(errorScenario).whenScenarioStateIs(STARTED).withRequestBody(equalToJson(event("TRANSIENT_ERROR"))).willReturn(aResponse().withStatus(503)).willSetStateTo("Error Sent"));    service.stubFor(post(urlEqualTo("/endpoint")).inScenario(errorScenario).whenScenarioStateIs("Error Sent").withRequestBody(equalToJson(event("TRANSIENT_ERROR"))).willReturn(aResponse().withStatus(200)));    addEventToChannel(event("TRANSIENT_ERROR"), Status.BACKOFF);    addEventToChannel(event("TRANSIENT_ERROR"), Status.READY);    service.verify(2, postRequestedFor(urlEqualTo("/endpoint")).withRequestBody(equalToJson(event("TRANSIENT_ERROR"))));}
0
public void ensureEventsNotResentOn401Failure() throws Exception
{    String errorScenario = "Error skip scenario";    service.stubFor(post(urlEqualTo("/endpoint")).inScenario(errorScenario).whenScenarioStateIs(STARTED).withRequestBody(equalToJson(event("UNAUTHORIZED REQUEST"))).willReturn(aResponse().withStatus(401).withHeader("Content-Type", "text/plain").withBody("Not allowed!")).willSetStateTo("Error Sent"));    service.stubFor(post(urlEqualTo("/endpoint")).inScenario(errorScenario).whenScenarioStateIs("Error Sent").withRequestBody(equalToJson(event("NEXT EVENT"))).willReturn(aResponse().withStatus(200)));    addEventToChannel(event("UNAUTHORIZED REQUEST"), Status.READY);    addEventToChannel(event("NEXT EVENT"), Status.READY);    service.verify(1, postRequestedFor(urlEqualTo("/endpoint")).withRequestBody(equalToJson(event("UNAUTHORIZED REQUEST"))));    service.verify(1, postRequestedFor(urlEqualTo("/endpoint")).withRequestBody(equalToJson(event("NEXT EVENT"))));}
0
public void ensureEventsResentOnNetworkFailure() throws Exception
{    String errorScenario = "Error Scenario";    service.stubFor(post(urlEqualTo("/endpoint")).inScenario(errorScenario).whenScenarioStateIs(STARTED).withRequestBody(equalToJson(event("NETWORK_ERROR"))).willReturn(aResponse().withFault(Fault.RANDOM_DATA_THEN_CLOSE)).willSetStateTo("Error Sent"));    service.stubFor(post(urlEqualTo("/endpoint")).inScenario(errorScenario).whenScenarioStateIs("Error Sent").withRequestBody(equalToJson(event("NETWORK_ERROR"))).willReturn(aResponse().withStatus(200)));    addEventToChannel(event("NETWORK_ERROR"), Status.BACKOFF);    addEventToChannel(event("NETWORK_ERROR"), Status.READY);    service.verify(2, postRequestedFor(urlEqualTo("/endpoint")).withRequestBody(equalToJson(event("NETWORK_ERROR"))));}
0
public void ensureEventsResentOnConnectionTimeout() throws Exception
{    final CountDownLatch firstRequestReceived = new CountDownLatch(1);    service.addSocketAcceptDelay(new RequestDelaySpec(CONNECT_TIMEOUT));    service.addMockServiceRequestListener(new RequestListener() {        @Override        public void requestReceived(Request request, Response response) {            service.addSocketAcceptDelay(new RequestDelaySpec(0));            firstRequestReceived.countDown();        }    });    service.stubFor(post(urlEqualTo("/endpoint")).withRequestBody(equalToJson(event("SLOW_SOCKET"))).willReturn(aResponse().withStatus(200)));    addEventToChannel(event("SLOW_SOCKET"), Status.BACKOFF);        firstRequestReceived.await(2000, TimeUnit.MILLISECONDS);    addEventToChannel(event("SLOW_SOCKET"), Status.READY);    service.verify(2, postRequestedFor(urlEqualTo("/endpoint")).withRequestBody(equalToJson(event("SLOW_SOCKET"))));}
0
public void requestReceived(Request request, Response response)
{    service.addSocketAcceptDelay(new RequestDelaySpec(0));    firstRequestReceived.countDown();}
0
public void ensureEventsResentOnRequestTimeout() throws Exception
{    String errorScenario = "Error Scenario";    service.stubFor(post(urlEqualTo("/endpoint")).inScenario(errorScenario).whenScenarioStateIs(STARTED).withRequestBody(equalToJson(event("SLOW_RESPONSE"))).willReturn(aResponse().withFixedDelay(RESPONSE_TIMEOUT).withStatus(200)).willSetStateTo("Slow Response Sent"));    service.stubFor(post(urlEqualTo("/endpoint")).inScenario(errorScenario).whenScenarioStateIs("Slow Response Sent").withRequestBody(equalToJson(event("SLOW_RESPONSE"))).willReturn(aResponse().withStatus(200)));    addEventToChannel(event("SLOW_RESPONSE"), Status.BACKOFF);    addEventToChannel(event("SLOW_RESPONSE"), Status.READY);    service.verify(2, postRequestedFor(urlEqualTo("/endpoint")).withRequestBody(equalToJson(event("SLOW_RESPONSE"))));}
0
public void ensureHttpConnectionReusedForSuccessfulRequests() throws Exception
{        service.addSocketAcceptDelay(new RequestDelaySpec(1000));    service.stubFor(post(urlEqualTo("/endpoint")).withRequestBody(equalToJson(event("SUCCESS"))).willReturn(aResponse().withStatus(200)));    long startTime = System.currentTimeMillis();    addEventToChannel(event("SUCCESS"), Status.READY);    addEventToChannel(event("SUCCESS"), Status.READY);    addEventToChannel(event("SUCCESS"), Status.READY);    long endTime = System.currentTimeMillis();    assertTrue("Test should have completed faster", endTime - startTime < 2500);    service.verify(3, postRequestedFor(urlEqualTo("/endpoint")).withRequestBody(equalToJson(event("SUCCESS"))));}
0
private void addEventToChannel(String line) throws EventDeliveryException
{    addEventToChannel(line, Status.READY);}
0
private void addEventToChannel(String line, Status expectedStatus) throws EventDeliveryException
{    SimpleEvent event = new SimpleEvent();    event.setBody(line.getBytes());    Transaction channelTransaction = channel.getTransaction();    channelTransaction.begin();    channel.put(event);    channelTransaction.commit();    channelTransaction.close();    Sink.Status status = httpSink.process();    assertEquals(expectedStatus, status);}
0
private String event(String id)
{    return "{'id':'" + id + "'}";}
0
public void onRegistered()
{}
0
public void onDisconnected()
{    }
1
public void onError(String msg)
{    }
1
public void onError(int num, String msg)
{    }
1
public void onInvite(String chan, IRCUser u, String nickPass)
{}
0
public void onJoin(String chan, IRCUser u)
{}
0
public void onKick(String chan, IRCUser u, String nickPass, String msg)
{}
0
public void onMode(IRCUser u, String nickPass, String mode)
{}
0
public void onMode(String chan, IRCUser u, IRCModeParser mp)
{}
0
public void onNick(IRCUser u, String nickNew)
{}
0
public void onNotice(String target, IRCUser u, String msg)
{}
0
public void onPart(String chan, IRCUser u, String msg)
{}
0
public void onPrivmsg(String chan, IRCUser u, String msg)
{}
0
public void onQuit(IRCUser u, String msg)
{}
0
public void onReply(int num, String value, String msg)
{}
0
public void onTopic(String chan, IRCUser u, String topic)
{}
0
public void onPing(String p)
{}
0
public void unknown(String a, String b, String c, String d)
{}
0
public void configure(Context context)
{    hostname = context.getString("hostname");    String portStr = context.getString("port");    nick = context.getString("nick");    password = context.getString("password");    user = context.getString("user");    name = context.getString("name");    chan = context.getString("chan");    splitLines = context.getBoolean("splitlines", false);    splitChars = context.getString("splitchars");    if (portStr != null) {        port = Integer.parseInt(portStr);    } else {        port = DEFAULT_PORT;    }    if (splitChars == null) {        splitChars = DEFAULT_SPLIT_CHARS;    }    Preconditions.checkState(hostname != null, "No hostname specified");    Preconditions.checkState(nick != null, "No nick specified");    Preconditions.checkState(chan != null, "No chan specified");}
0
private void createConnection() throws IOException
{    if (connection == null) {                connection = new IRCConnection(hostname, new int[] { port }, password, nick, user, name);        connection.addIRCEventListener(new IRCConnectionListener());        connection.setEncoding("UTF-8");        connection.setPong(true);        connection.setDaemon(false);        connection.setColors(false);        connection.connect();        connection.send("join " + IRC_CHANNEL_PREFIX + chan);    }}
1
private void destroyConnection()
{    if (connection != null) {                connection.close();    }    connection = null;}
1
public void start()
{        try {        createConnection();    } catch (Exception e) {                /* Try to prevent leaking resources. */        destroyConnection();        /* FIXME: Mark ourselves as failed. */        return;    }    super.start();    }
1
public void stop()
{        destroyConnection();    super.stop();    }
1
private void sendLine(Event event)
{    String body = new String(event.getBody());    if (splitLines) {        String[] lines = body.split(splitChars);        for (String line : lines) {            connection.doPrivmsg(IRC_CHANNEL_PREFIX + this.chan, line);        }    } else {        connection.doPrivmsg(IRC_CHANNEL_PREFIX + this.chan, body);    }}
0
public Status process() throws EventDeliveryException
{    Status status = Status.READY;    Channel channel = getChannel();    Transaction transaction = channel.getTransaction();    try {        transaction.begin();        createConnection();        Event event = channel.take();        if (event == null) {            counterGroup.incrementAndGet("event.empty");            status = Status.BACKOFF;        } else {            sendLine(event);            counterGroup.incrementAndGet("event.irc");        }        transaction.commit();    } catch (ChannelException e) {        transaction.rollback();                status = Status.BACKOFF;    } catch (Exception e) {        transaction.rollback();                status = Status.BACKOFF;        destroyConnection();    } finally {        transaction.close();    }    return status;}
1
private static int findFreePort() throws IOException
{    ServerSocket socket = new ServerSocket(0);    int port = socket.getLocalPort();    socket.close();    return port;}
0
public void setUp() throws IOException
{    ircServerPort = findFreePort();    dumbIRCServer = new DumbIRCServer(ircServerPort);    dumbIRCServer.start();    eventFile = folder.newFile("eventFile.txt");}
0
public void tearDown() throws Exception
{    dumbIRCServer.shutdownServer();}
0
public void testIRCSinkMissingSplitLineProperty()
{    Sink ircSink = new IRCSink();    ircSink.setName("IRC Sink - " + UUID.randomUUID().toString());    Context context = new Context();    context.put("hostname", "localhost");    context.put("port", String.valueOf(ircServerPort));    context.put("nick", "flume");    context.put("password", "flume");    context.put("user", "flume");    context.put("name", "flume-dev");    context.put("chan", "flume");    context.put("splitchars", "false");    Configurables.configure(ircSink, context);    Channel memoryChannel = new MemoryChannel();    Configurables.configure(memoryChannel, context);    ircSink.setChannel(memoryChannel);    ircSink.start();    Transaction txn = memoryChannel.getTransaction();    txn.begin();    Event event = EventBuilder.withBody("Dummy Event".getBytes());    memoryChannel.put(event);    txn.commit();    txn.close();    try {        Sink.Status status = ircSink.process();        if (status == Sink.Status.BACKOFF) {            fail("Error occured");        }    } catch (EventDeliveryException eDelExcp) {        }}
0
public void run()
{    try {        ss = new ServerSocket(port);        while (true) {            try {                Socket socket = ss.accept();                process(socket);            } catch (Exception ex) {            /* noop */            }        }    } catch (IOException e) {        }}
0
public void shutdownServer() throws Exception
{    ss.close();}
0
private void process(Socket socket) throws IOException
{    FileOutputStream fileOutputStream = FileUtils.openOutputStream(eventFile);    List<String> input = IOUtils.readLines(socket.getInputStream());    for (String next : input) {        if (isPrivMessage(next)) {            fileOutputStream.write(next.getBytes());            fileOutputStream.write("\n".getBytes());        }    }    fileOutputStream.close();    socket.close();}
0
private boolean isPrivMessage(String input)
{    return input.startsWith("PRIVMSG");}
0
public IndexRequestBuilder createIndexRequest(Client client, String indexPrefix, String indexType, Event event) throws IOException
{    IndexRequestBuilder request = prepareIndex(client);    String realIndexPrefix = BucketPath.escapeString(indexPrefix, event.getHeaders());    String realIndexType = BucketPath.escapeString(indexType, event.getHeaders());    TimestampedEvent timestampedEvent = new TimestampedEvent(event);    long timestamp = timestampedEvent.getTimestamp();    String indexName = getIndexName(realIndexPrefix, timestamp);    prepareIndexRequest(request, indexName, realIndexType, timestampedEvent);    return request;}
0
 IndexRequestBuilder prepareIndex(Client client)
{    return client.prepareIndex();}
0
protected String getIndexName(String indexPrefix, long timestamp)
{    return new StringBuilder(indexPrefix).append('-').append(fastDateFormat.format(timestamp)).toString();}
0
public ElasticSearchClient getClient(String clientType, String[] hostNames, String clusterName, ElasticSearchEventSerializer serializer, ElasticSearchIndexRequestBuilderFactory indexBuilder) throws NoSuchClientTypeException
{    if (clientType.equalsIgnoreCase(TransportClient) && serializer != null) {        return new ElasticSearchTransportClient(hostNames, clusterName, serializer);    } else if (clientType.equalsIgnoreCase(TransportClient) && indexBuilder != null) {        return new ElasticSearchTransportClient(hostNames, clusterName, indexBuilder);    } else if (clientType.equalsIgnoreCase(RestClient) && serializer != null) {        return new ElasticSearchRestClient(hostNames, serializer);    }    throw new NoSuchClientTypeException();}
0
public ElasticSearchClient getLocalClient(String clientType, ElasticSearchEventSerializer serializer, ElasticSearchIndexRequestBuilderFactory indexBuilder) throws NoSuchClientTypeException
{    if (clientType.equalsIgnoreCase(TransportClient) && serializer != null) {        return new ElasticSearchTransportClient(serializer);    } else if (clientType.equalsIgnoreCase(TransportClient) && indexBuilder != null) {        return new ElasticSearchTransportClient(indexBuilder);    } else if (clientType.equalsIgnoreCase(RestClient)) {    }    throw new NoSuchClientTypeException();}
0
public void configure(Context context)
{}
0
public void close()
{}
0
public void addEvent(Event event, IndexNameBuilder indexNameBuilder, String indexType, long ttlMs) throws Exception
{    BytesReference content = serializer.getContentBuilder(event).bytes();    Map<String, Map<String, String>> parameters = new HashMap<String, Map<String, String>>();    Map<String, String> indexParameters = new HashMap<String, String>();    indexParameters.put(INDEX_PARAM, indexNameBuilder.getIndexName(event));    indexParameters.put(TYPE_PARAM, indexType);    if (ttlMs > 0) {        indexParameters.put(TTL_PARAM, Long.toString(ttlMs));    }    parameters.put(INDEX_OPERATION_NAME, indexParameters);    Gson gson = new Gson();    synchronized (bulkBuilder) {        bulkBuilder.append(gson.toJson(parameters));        bulkBuilder.append("\n");        bulkBuilder.append(content.toBytesArray().toUtf8());        bulkBuilder.append("\n");    }}
0
public void execute() throws Exception
{    int statusCode = 0, triesCount = 0;    HttpResponse response = null;    String entity;    synchronized (bulkBuilder) {        entity = bulkBuilder.toString();        bulkBuilder = new StringBuilder();    }    while (statusCode != HttpStatus.SC_OK && triesCount < serversList.size()) {        triesCount++;        String host = serversList.get();        String url = host + "/" + BULK_ENDPOINT;        HttpPost httpRequest = new HttpPost(url);        httpRequest.setEntity(new StringEntity(entity));        response = httpClient.execute(httpRequest);        statusCode = response.getStatusLine().getStatusCode();                if (response.getEntity() != null) {                    }    }    if (statusCode != HttpStatus.SC_OK) {        if (response.getEntity() != null) {            throw new EventDeliveryException(EntityUtils.toString(response.getEntity(), "UTF-8"));        } else {            throw new EventDeliveryException("Elasticsearch status code was: " + statusCode);        }    }}
1
 InetSocketTransportAddress[] getServerAddresses()
{    return serverAddresses;}
0
 void setBulkRequestBuilder(BulkRequestBuilder bulkRequestBuilder)
{    this.bulkRequestBuilder = bulkRequestBuilder;}
0
private void configureHostnames(String[] hostNames)
{        serverAddresses = new InetSocketTransportAddress[hostNames.length];    for (int i = 0; i < hostNames.length; i++) {        String[] hostPort = hostNames[i].trim().split(":");        String host = hostPort[0].trim();        int port = hostPort.length == 2 ? Integer.parseInt(hostPort[1].trim()) : DEFAULT_PORT;        serverAddresses[i] = new InetSocketTransportAddress(host, port);    }}
1
public void close()
{    if (client != null) {        client.close();    }    client = null;}
0
public void addEvent(Event event, IndexNameBuilder indexNameBuilder, String indexType, long ttlMs) throws Exception
{    if (bulkRequestBuilder == null) {        bulkRequestBuilder = client.prepareBulk();    }    IndexRequestBuilder indexRequestBuilder = null;    if (indexRequestBuilderFactory == null) {        indexRequestBuilder = client.prepareIndex(indexNameBuilder.getIndexName(event), indexType).setSource(serializer.getContentBuilder(event).bytes());    } else {        indexRequestBuilder = indexRequestBuilderFactory.createIndexRequest(client, indexNameBuilder.getIndexPrefix(event), indexType, event);    }    if (ttlMs > 0) {        indexRequestBuilder.setTTL(ttlMs);    }    bulkRequestBuilder.add(indexRequestBuilder);}
0
public void execute() throws Exception
{    try {        BulkResponse bulkResponse = bulkRequestBuilder.execute().actionGet();        if (bulkResponse.hasFailures()) {            throw new EventDeliveryException(bulkResponse.buildFailureMessage());        }    } finally {        bulkRequestBuilder = client.prepareBulk();    }}
0
private void openClient(String clusterName)
{        Settings settings = ImmutableSettings.settingsBuilder().put("cluster.name", clusterName).build();    TransportClient transportClient = new TransportClient(settings);    for (InetSocketTransportAddress host : serverAddresses) {        transportClient.addTransportAddress(host);    }    if (client != null) {        client.close();    }    client = transportClient;}
1
private void openLocalDiscoveryClient()
{        Node node = NodeBuilder.nodeBuilder().client(true).local(true).node();    if (client != null) {        client.close();    }    client = node.client();}
1
public void configure(Context context)
{}
0
public synchronized T get()
{    if (iterator.hasNext()) {        return iterator.next();    } else {        iterator = elements.iterator();        return iterator.next();    }}
0
public int size()
{    return elements.size();}
0
public static void appendField(XContentBuilder builder, String field, byte[] data) throws IOException
{    XContentType contentType = XContentFactory.xContentType(data);    if (contentType == null) {        addSimpleField(builder, field, data);    } else {        addComplexField(builder, field, contentType, data);    }}
0
public static void addSimpleField(XContentBuilder builder, String fieldName, byte[] data) throws IOException
{    builder.field(fieldName, new String(data, charset));}
0
public static void addComplexField(XContentBuilder builder, String fieldName, XContentType contentType, byte[] data) throws IOException
{    XContentParser parser = null;    try {                                                                parser = XContentFactory.xContent(contentType).createParser(data);        while (parser.nextToken() != null) {        }        ;                parser = XContentFactory.xContent(contentType).createParser(data);                builder.field(fieldName);                builder.copyCurrentStructure(parser);    } catch (JsonParseException ex) {                                addSimpleField(builder, fieldName, data);    } finally {        if (parser != null) {            parser.close();        }    }}
0
public void configure(Context context)
{}
0
public void configure(ComponentConfiguration conf)
{}
0
public XContentBuilder getContentBuilder(Event event) throws IOException
{    XContentBuilder builder = jsonBuilder().startObject();    appendBody(builder, event);    appendHeaders(builder, event);    return builder;}
0
private void appendBody(XContentBuilder builder, Event event) throws IOException
{    ContentBuilderUtil.appendField(builder, "body", event.getBody());}
0
private void appendHeaders(XContentBuilder builder, Event event) throws IOException
{    Map<String, String> headers = event.getHeaders();    for (String key : headers.keySet()) {        ContentBuilderUtil.appendField(builder, key, headers.get(key).getBytes(charset));    }}
0
public XContentBuilder getContentBuilder(Event event) throws IOException
{    XContentBuilder builder = jsonBuilder().startObject();    appendBody(builder, event);    appendHeaders(builder, event);    return builder;}
0
private void appendBody(XContentBuilder builder, Event event) throws IOException, UnsupportedEncodingException
{    byte[] body = event.getBody();    ContentBuilderUtil.appendField(builder, "@message", body);}
0
private void appendHeaders(XContentBuilder builder, Event event) throws IOException
{    Map<String, String> headers = Maps.newHashMap(event.getHeaders());    String timestamp = headers.get("timestamp");    if (!StringUtils.isBlank(timestamp) && StringUtils.isBlank(headers.get("@timestamp"))) {        long timestampMs = Long.parseLong(timestamp);        builder.field("@timestamp", new Date(timestampMs));    }    String source = headers.get("source");    if (!StringUtils.isBlank(source) && StringUtils.isBlank(headers.get("@source"))) {        ContentBuilderUtil.appendField(builder, "@source", source.getBytes(charset));    }    String type = headers.get("type");    if (!StringUtils.isBlank(type) && StringUtils.isBlank(headers.get("@type"))) {        ContentBuilderUtil.appendField(builder, "@type", type.getBytes(charset));    }    String host = headers.get("host");    if (!StringUtils.isBlank(host) && StringUtils.isBlank(headers.get("@source_host"))) {        ContentBuilderUtil.appendField(builder, "@source_host", host.getBytes(charset));    }    String srcPath = headers.get("src_path");    if (!StringUtils.isBlank(srcPath) && StringUtils.isBlank(headers.get("@source_path"))) {        ContentBuilderUtil.appendField(builder, "@source_path", srcPath.getBytes(charset));    }    builder.startObject("@fields");    for (String key : headers.keySet()) {        byte[] val = headers.get(key).getBytes(charset);        ContentBuilderUtil.appendField(builder, key, val);    }    builder.endObject();}
0
public void configure(Context context)
{}
0
public void configure(ComponentConfiguration conf)
{}
0
 String[] getServerAddresses()
{    return serverAddresses;}
0
 String getClusterName()
{    return clusterName;}
0
 String getIndexName()
{    return indexName;}
0
 String getIndexType()
{    return indexType;}
0
 long getTTLMs()
{    return ttlMs;}
0
 ElasticSearchEventSerializer getEventSerializer()
{    return eventSerializer;}
0
 IndexNameBuilder getIndexNameBuilder()
{    return indexNameBuilder;}
0
public long getBatchSize()
{    return batchSize;}
0
public Status process() throws EventDeliveryException
{        Status status = Status.READY;    Channel channel = getChannel();    Transaction txn = channel.getTransaction();    try {        txn.begin();        int count;        for (count = 0; count < batchSize; ++count) {            Event event = channel.take();            if (event == null) {                break;            }            String realIndexType = BucketPath.escapeString(indexType, event.getHeaders());            client.addEvent(event, indexNameBuilder, realIndexType, ttlMs);        }        if (count <= 0) {            sinkCounter.incrementBatchEmptyCount();            counterGroup.incrementAndGet("channel.underflow");            status = Status.BACKOFF;        } else {            if (count < batchSize) {                sinkCounter.incrementBatchUnderflowCount();                status = Status.BACKOFF;            } else {                sinkCounter.incrementBatchCompleteCount();            }            sinkCounter.addToEventDrainAttemptCount(count);            client.execute();        }        txn.commit();        sinkCounter.addToEventDrainSuccessCount(count);        counterGroup.incrementAndGet("transaction.success");    } catch (Throwable ex) {        try {            txn.rollback();            counterGroup.incrementAndGet("transaction.rollback");        } catch (Exception ex2) {                    }        if (ex instanceof Error || ex instanceof RuntimeException) {                        Throwables.propagate(ex);        } else {                        throw new EventDeliveryException("Failed to commit transaction. Transaction rolled back.", ex);        }    } finally {        txn.close();    }    return status;}
1
public void configure(Context context)
{    if (!isLocal) {        if (StringUtils.isNotBlank(context.getString(HOSTNAMES))) {            serverAddresses = StringUtils.deleteWhitespace(context.getString(HOSTNAMES)).split(",");        }        Preconditions.checkState(serverAddresses != null && serverAddresses.length > 0, "Missing Param:" + HOSTNAMES);    }    if (StringUtils.isNotBlank(context.getString(INDEX_NAME))) {        this.indexName = context.getString(INDEX_NAME);    }    if (StringUtils.isNotBlank(context.getString(INDEX_TYPE))) {        this.indexType = context.getString(INDEX_TYPE);    }    if (StringUtils.isNotBlank(context.getString(CLUSTER_NAME))) {        this.clusterName = context.getString(CLUSTER_NAME);    }    if (StringUtils.isNotBlank(context.getString(BATCH_SIZE))) {        this.batchSize = Integer.parseInt(context.getString(BATCH_SIZE));    }    if (StringUtils.isNotBlank(context.getString(TTL))) {        this.ttlMs = parseTTL(context.getString(TTL));        Preconditions.checkState(ttlMs > 0, TTL + " must be greater than 0 or not set.");    }    if (StringUtils.isNotBlank(context.getString(CLIENT_TYPE))) {        clientType = context.getString(CLIENT_TYPE);    }    elasticSearchClientContext = new Context();    elasticSearchClientContext.putAll(context.getSubProperties(CLIENT_PREFIX));    String serializerClazz = DEFAULT_SERIALIZER_CLASS;    if (StringUtils.isNotBlank(context.getString(SERIALIZER))) {        serializerClazz = context.getString(SERIALIZER);    }    Context serializerContext = new Context();    serializerContext.putAll(context.getSubProperties(SERIALIZER_PREFIX));    try {        @SuppressWarnings("unchecked")        Class<? extends Configurable> clazz = (Class<? extends Configurable>) Class.forName(serializerClazz);        Configurable serializer = clazz.newInstance();        if (serializer instanceof ElasticSearchIndexRequestBuilderFactory) {            indexRequestFactory = (ElasticSearchIndexRequestBuilderFactory) serializer;            indexRequestFactory.configure(serializerContext);        } else if (serializer instanceof ElasticSearchEventSerializer) {            eventSerializer = (ElasticSearchEventSerializer) serializer;            eventSerializer.configure(serializerContext);        } else {            throw new IllegalArgumentException(serializerClazz + " is not an ElasticSearchEventSerializer");        }    } catch (Exception e) {                Throwables.propagate(e);    }    if (sinkCounter == null) {        sinkCounter = new SinkCounter(getName());    }    String indexNameBuilderClass = DEFAULT_INDEX_NAME_BUILDER_CLASS;    if (StringUtils.isNotBlank(context.getString(INDEX_NAME_BUILDER))) {        indexNameBuilderClass = context.getString(INDEX_NAME_BUILDER);    }    Context indexnameBuilderContext = new Context();    serializerContext.putAll(context.getSubProperties(INDEX_NAME_BUILDER_PREFIX));    try {        @SuppressWarnings("unchecked")        Class<? extends IndexNameBuilder> clazz = (Class<? extends IndexNameBuilder>) Class.forName(indexNameBuilderClass);        indexNameBuilder = clazz.newInstance();        indexnameBuilderContext.put(INDEX_NAME, indexName);        indexNameBuilder.configure(indexnameBuilderContext);    } catch (Exception e) {                Throwables.propagate(e);    }    if (sinkCounter == null) {        sinkCounter = new SinkCounter(getName());    }    Preconditions.checkState(StringUtils.isNotBlank(indexName), "Missing Param:" + INDEX_NAME);    Preconditions.checkState(StringUtils.isNotBlank(indexType), "Missing Param:" + INDEX_TYPE);    Preconditions.checkState(StringUtils.isNotBlank(clusterName), "Missing Param:" + CLUSTER_NAME);    Preconditions.checkState(batchSize >= 1, BATCH_SIZE + " must be greater than 0");}
1
public void start()
{    ElasticSearchClientFactory clientFactory = new ElasticSearchClientFactory();        sinkCounter.start();    try {        if (isLocal) {            client = clientFactory.getLocalClient(clientType, eventSerializer, indexRequestFactory);        } else {            client = clientFactory.getClient(clientType, serverAddresses, clusterName, eventSerializer, indexRequestFactory);            client.configure(elasticSearchClientContext);        }        sinkCounter.incrementConnectionCreatedCount();    } catch (Exception ex) {        ex.printStackTrace();        sinkCounter.incrementConnectionFailedCount();        if (client != null) {            client.close();            sinkCounter.incrementConnectionClosedCount();        }    }    super.start();}
1
public void stop()
{        if (client != null) {        client.close();    }    sinkCounter.incrementConnectionClosedCount();    sinkCounter.stop();    super.stop();}
1
private long parseTTL(String ttl)
{    matcher = matcher.reset(ttl);    while (matcher.find()) {        if (matcher.group(2).equals("ms")) {            return Long.parseLong(matcher.group(1));        } else if (matcher.group(2).equals("s")) {            return TimeUnit.SECONDS.toMillis(Integer.parseInt(matcher.group(1)));        } else if (matcher.group(2).equals("m")) {            return TimeUnit.MINUTES.toMillis(Integer.parseInt(matcher.group(1)));        } else if (matcher.group(2).equals("h")) {            return TimeUnit.HOURS.toMillis(Integer.parseInt(matcher.group(1)));        } else if (matcher.group(2).equals("d")) {            return TimeUnit.DAYS.toMillis(Integer.parseInt(matcher.group(1)));        } else if (matcher.group(2).equals("w")) {            return TimeUnit.DAYS.toMillis(7 * Integer.parseInt(matcher.group(1)));        } else if (matcher.group(2).equals("")) {                        return TimeUnit.DAYS.toMillis(Integer.parseInt(matcher.group(1)));        } else {                        return 0;        }    }        return 0;}
1
public void configure(Context context)
{    serializer.configure(context);}
0
public void configure(ComponentConfiguration config)
{    serializer.configure(config);}
0
protected void prepareIndexRequest(IndexRequestBuilder indexRequest, String indexName, String indexType, Event event) throws IOException
{    BytesStream contentBuilder = serializer.getContentBuilder(event);    indexRequest.setIndex(indexName).setType(indexType).setSource(contentBuilder.bytes());}
0
public String getIndexName(Event event)
{    return BucketPath.escapeString(indexName, event.getHeaders());}
0
public String getIndexPrefix(Event event)
{    return BucketPath.escapeString(indexName, event.getHeaders());}
0
public void configure(Context context)
{    indexName = context.getString(ElasticSearchSinkConstants.INDEX_NAME);}
0
public void configure(ComponentConfiguration conf)
{}
0
 FastDateFormat getFastDateFormat()
{    return fastDateFormat;}
0
public String getIndexName(Event event)
{    TimestampedEvent timestampedEvent = new TimestampedEvent(event);    long timestamp = timestampedEvent.getTimestamp();    String realIndexPrefix = BucketPath.escapeString(indexPrefix, event.getHeaders());    return new StringBuilder(realIndexPrefix).append('-').append(fastDateFormat.format(timestamp)).toString();}
0
public String getIndexPrefix(Event event)
{    return BucketPath.escapeString(indexPrefix, event.getHeaders());}
0
public void configure(Context context)
{    String dateFormatString = context.getString(DATE_FORMAT);    String timeZoneString = context.getString(TIME_ZONE);    if (StringUtils.isBlank(dateFormatString)) {        dateFormatString = DEFAULT_DATE_FORMAT;    }    if (StringUtils.isBlank(timeZoneString)) {        timeZoneString = DEFAULT_TIME_ZONE;    }    fastDateFormat = FastDateFormat.getInstance(dateFormatString, TimeZone.getTimeZone(timeZoneString));    indexPrefix = context.getString(ElasticSearchSinkConstants.INDEX_NAME);}
0
public void configure(ComponentConfiguration conf)
{}
0
 long getTimestamp()
{    return timestamp;}
0
 void initDefaults()
{    parameters = Maps.newHashMap();    parameters.put(INDEX_NAME, DEFAULT_INDEX_NAME);    parameters.put(INDEX_TYPE, DEFAULT_INDEX_TYPE);    parameters.put(CLUSTER_NAME, DEFAULT_CLUSTER_NAME);    parameters.put(BATCH_SIZE, "1");    parameters.put(TTL, "5");    timestampedIndexName = DEFAULT_INDEX_NAME + '-' + ElasticSearchIndexRequestBuilderFactory.df.format(FIXED_TIME_MILLIS);}
0
 void createNodes() throws Exception
{    Settings settings = ImmutableSettings.settingsBuilder().put("number_of_shards", 1).put("number_of_replicas", 0).put("routing.hash.type", "simple").put("gateway.type", "none").put("path.data", "target/es-test").build();    node = NodeBuilder.nodeBuilder().settings(settings).local(true).node();    client = node.client();    client.admin().cluster().prepareHealth().setWaitForGreenStatus().execute().actionGet();}
0
 void shutdownNodes() throws Exception
{    ((InternalNode) node).injector().getInstance(Gateway.class).reset();    client.close();    node.close();}
0
public void setFixedJodaTime()
{    DateTimeUtils.setCurrentMillisFixed(FIXED_TIME_MILLIS);}
0
public void resetJodaTime()
{    DateTimeUtils.setCurrentMillisSystem();}
0
 Channel bindAndStartChannel(ElasticSearchSink fixture)
{        Channel channel = new MemoryChannel();    Configurables.configure(channel, new Context());        fixture.setChannel(channel);    fixture.start();    return channel;}
0
 void assertMatchAllQuery(int expectedHits, Event... events)
{    assertSearch(expectedHits, performSearch(QueryBuilders.matchAllQuery()), null, events);}
0
 void assertBodyQuery(int expectedHits, Event... events)
{        assertSearch(expectedHits, performSearch(QueryBuilders.fieldQuery("@message", "event")), null, events);}
0
 SearchResponse performSearch(QueryBuilder query)
{    return client.prepareSearch(timestampedIndexName).setTypes(DEFAULT_INDEX_TYPE).setQuery(query).execute().actionGet();}
0
 void assertSearch(int expectedHits, SearchResponse response, Map<String, Object> expectedBody, Event... events)
{    SearchHits hitResponse = response.getHits();    assertEquals(expectedHits, hitResponse.getTotalHits());    SearchHit[] hits = hitResponse.getHits();    Arrays.sort(hits, new Comparator<SearchHit>() {        @Override        public int compare(SearchHit o1, SearchHit o2) {            return o1.getSourceAsString().compareTo(o2.getSourceAsString());        }    });    for (int i = 0; i < events.length; i++) {        Event event = events[i];        SearchHit hit = hits[i];        Map<String, Object> source = hit.getSource();        if (expectedBody == null) {            assertEquals(new String(event.getBody()), source.get("@message"));        } else {            assertEquals(expectedBody, source.get("@message"));        }    }}
0
public int compare(SearchHit o1, SearchHit o2)
{    return o1.getSourceAsString().compareTo(o2.getSourceAsString());}
0
public void setUp()
{    fixture = new RoundRobinList<String>(Arrays.asList("test1", "test2"));}
0
public void shouldReturnNextElement()
{    assertEquals("test1", fixture.get());    assertEquals("test2", fixture.get());    assertEquals("test1", fixture.get());    assertEquals("test2", fixture.get());    assertEquals("test1", fixture.get());}
0
public void setUp()
{    initMocks(this);    factory = new ElasticSearchClientFactory();}
0
public void shouldReturnTransportClient() throws Exception
{    String[] hostNames = { "127.0.0.1" };    Object o = factory.getClient(ElasticSearchClientFactory.TransportClient, hostNames, "test", serializer, null);    assertThat(o, instanceOf(ElasticSearchTransportClient.class));}
0
public void shouldReturnRestClient() throws NoSuchClientTypeException
{    String[] hostNames = { "127.0.0.1" };    Object o = factory.getClient(ElasticSearchClientFactory.RestClient, hostNames, "test", serializer, null);    assertThat(o, instanceOf(ElasticSearchRestClient.class));}
0
public void shouldThrowNoSuchClientTypeException() throws NoSuchClientTypeException
{    String[] hostNames = { "127.0.0.1" };    factory.getClient("not_existing_client", hostNames, "test", null, null);}
0
public void setUp() throws IOException
{    initMocks(this);    BytesReference bytesReference = mock(BytesReference.class);    BytesStream bytesStream = mock(BytesStream.class);    when(nameBuilder.getIndexName(any(Event.class))).thenReturn(INDEX_NAME);    when(bytesReference.toBytesArray()).thenReturn(new BytesArray(MESSAGE_CONTENT));    when(bytesStream.bytes()).thenReturn(bytesReference);    when(serializer.getContentBuilder(any(Event.class))).thenReturn(bytesStream);    fixture = new ElasticSearchRestClient(HOSTS, serializer, httpClient);}
0
public void shouldAddNewEventWithoutTTL() throws Exception
{    ArgumentCaptor<HttpPost> argument = ArgumentCaptor.forClass(HttpPost.class);    when(httpStatus.getStatusCode()).thenReturn(HttpStatus.SC_OK);    when(httpResponse.getStatusLine()).thenReturn(httpStatus);    when(httpClient.execute(any(HttpUriRequest.class))).thenReturn(httpResponse);    fixture.addEvent(event, nameBuilder, "bar_type", -1);    fixture.execute();    verify(httpClient).execute(isA(HttpUriRequest.class));    verify(httpClient).execute(argument.capture());    assertEquals("http://host1/_bulk", argument.getValue().getURI().toString());    assertTrue(verifyJsonEvents("{\"index\":{\"_type\":\"bar_type\", \"_index\":\"foo_index\"}}\n", MESSAGE_CONTENT, EntityUtils.toString(argument.getValue().getEntity())));}
0
public void shouldAddNewEventWithTTL() throws Exception
{    ArgumentCaptor<HttpPost> argument = ArgumentCaptor.forClass(HttpPost.class);    when(httpStatus.getStatusCode()).thenReturn(HttpStatus.SC_OK);    when(httpResponse.getStatusLine()).thenReturn(httpStatus);    when(httpClient.execute(any(HttpUriRequest.class))).thenReturn(httpResponse);    fixture.addEvent(event, nameBuilder, "bar_type", 123);    fixture.execute();    verify(httpClient).execute(isA(HttpUriRequest.class));    verify(httpClient).execute(argument.capture());    assertEquals("http://host1/_bulk", argument.getValue().getURI().toString());    assertTrue(verifyJsonEvents("{\"index\":{\"_type\":\"bar_type\",\"_index\":\"foo_index\",\"_ttl\":\"123\"}}\n", MESSAGE_CONTENT, EntityUtils.toString(argument.getValue().getEntity())));}
0
private boolean verifyJsonEvents(String expectedIndex, String expectedBody, String actual)
{    Iterator<String> it = Splitter.on("\n").split(actual).iterator();    JsonParser parser = new JsonParser();    JsonObject[] arr = new JsonObject[2];    for (int i = 0; i < 2; i++) {        arr[i] = (JsonObject) parser.parse(it.next());    }    return arr[0].equals(parser.parse(expectedIndex)) && arr[1].equals(parser.parse(expectedBody));}
0
public void shouldThrowEventDeliveryException() throws Exception
{    ArgumentCaptor<HttpPost> argument = ArgumentCaptor.forClass(HttpPost.class);    when(httpStatus.getStatusCode()).thenReturn(HttpStatus.SC_INTERNAL_SERVER_ERROR);    when(httpResponse.getStatusLine()).thenReturn(httpStatus);    when(httpClient.execute(any(HttpUriRequest.class))).thenReturn(httpResponse);    fixture.addEvent(event, nameBuilder, "bar_type", 123);    fixture.execute();}
0
public void shouldRetryBulkOperation() throws Exception
{    ArgumentCaptor<HttpPost> argument = ArgumentCaptor.forClass(HttpPost.class);    when(httpStatus.getStatusCode()).thenReturn(HttpStatus.SC_INTERNAL_SERVER_ERROR, HttpStatus.SC_OK);    when(httpResponse.getStatusLine()).thenReturn(httpStatus);    when(httpClient.execute(any(HttpUriRequest.class))).thenReturn(httpResponse);    fixture.addEvent(event, nameBuilder, "bar_type", 123);    fixture.execute();    verify(httpClient, times(2)).execute(isA(HttpUriRequest.class));    verify(httpClient, times(2)).execute(argument.capture());    List<HttpPost> allValues = argument.getAllValues();    assertEquals("http://host1/_bulk", allValues.get(0).getURI().toString());    assertEquals("http://host2/_bulk", allValues.get(1).getURI().toString());}
0
public void setUp() throws IOException
{    initMocks(this);    BytesReference bytesReference = mock(BytesReference.class);    BytesStream bytesStream = mock(BytesStream.class);    when(nameBuilder.getIndexName(any(Event.class))).thenReturn("foo_index");    when(bytesReference.toBytes()).thenReturn("{\"body\":\"test\"}".getBytes());    when(bytesStream.bytes()).thenReturn(bytesReference);    when(serializer.getContentBuilder(any(Event.class))).thenReturn(bytesStream);    when(elasticSearchClient.prepareIndex(anyString(), anyString())).thenReturn(indexRequestBuilder);    when(indexRequestBuilder.setSource(bytesReference)).thenReturn(indexRequestBuilder);    fixture = new ElasticSearchTransportClient(elasticSearchClient, serializer);    fixture.setBulkRequestBuilder(bulkRequestBuilder);}
0
public void shouldAddNewEventWithoutTTL() throws Exception
{    fixture.addEvent(event, nameBuilder, "bar_type", -1);    verify(indexRequestBuilder).setSource(serializer.getContentBuilder(event).bytes());    verify(bulkRequestBuilder).add(indexRequestBuilder);}
0
public void shouldAddNewEventWithTTL() throws Exception
{    fixture.addEvent(event, nameBuilder, "bar_type", 10);    verify(indexRequestBuilder).setTTL(10);    verify(indexRequestBuilder).setSource(serializer.getContentBuilder(event).bytes());}
0
public void shouldExecuteBulkRequestBuilder() throws Exception
{    ListenableActionFuture<BulkResponse> action = (ListenableActionFuture<BulkResponse>) mock(ListenableActionFuture.class);    BulkResponse response = mock(BulkResponse.class);    when(bulkRequestBuilder.execute()).thenReturn(action);    when(action.actionGet()).thenReturn(response);    when(response.hasFailures()).thenReturn(false);    fixture.addEvent(event, nameBuilder, "bar_type", 10);    fixture.execute();    verify(bulkRequestBuilder).execute();}
0
public void shouldThrowExceptionOnExecuteFailed() throws Exception
{    ListenableActionFuture<BulkResponse> action = (ListenableActionFuture<BulkResponse>) mock(ListenableActionFuture.class);    BulkResponse response = mock(BulkResponse.class);    when(bulkRequestBuilder.execute()).thenReturn(action);    when(action.actionGet()).thenReturn(response);    when(response.hasFailures()).thenReturn(true);    fixture.addEvent(event, nameBuilder, "bar_type", 10);    fixture.execute();}
0
public void testRoundTrip() throws Exception
{    ElasticSearchDynamicSerializer fixture = new ElasticSearchDynamicSerializer();    Context context = new Context();    fixture.configure(context);    String message = "test body";    Map<String, String> headers = Maps.newHashMap();    headers.put("headerNameOne", "headerValueOne");    headers.put("headerNameTwo", "headerValueTwo");    headers.put("headerNameThree", "headerValueThree");    Event event = EventBuilder.withBody(message.getBytes(charset));    event.setHeaders(headers);    XContentBuilder expected = jsonBuilder().startObject();    expected.field("body", new String(message.getBytes(), charset));    for (String headerName : headers.keySet()) {        expected.field(headerName, new String(headers.get(headerName).getBytes(), charset));    }    expected.endObject();    XContentBuilder actual = fixture.getContentBuilder(event);    assertEquals(new String(expected.bytes().array()), new String(actual.bytes().array()));}
0
public void setupFactory() throws Exception
{    serializer = new FakeEventSerializer();    factory = new EventSerializerIndexRequestBuilderFactory(serializer) {        @Override        IndexRequestBuilder prepareIndex(Client client) {            return new IndexRequestBuilder(FAKE_CLIENT);        }    };}
0
 IndexRequestBuilder prepareIndex(Client client)
{    return new IndexRequestBuilder(FAKE_CLIENT);}
0
public void shouldUseUtcAsBasisForDateFormat()
{    assertEquals("Coordinated Universal Time", factory.fastDateFormat.getTimeZone().getDisplayName());}
0
public void indexNameShouldBePrefixDashFormattedTimestamp()
{    long millis = 987654321L;    assertEquals("prefix-" + factory.fastDateFormat.format(millis), factory.getIndexName("prefix", millis));}
0
public void shouldEnsureTimestampHeaderPresentInTimestampedEvent()
{    SimpleEvent base = new SimpleEvent();    TimestampedEvent timestampedEvent = new TimestampedEvent(base);    assertEquals(FIXED_TIME_MILLIS, timestampedEvent.getTimestamp());    assertEquals(String.valueOf(FIXED_TIME_MILLIS), timestampedEvent.getHeaders().get("timestamp"));}
0
public void shouldUseExistingTimestampHeaderInTimestampedEvent()
{    SimpleEvent base = new SimpleEvent();    Map<String, String> headersWithTimestamp = Maps.newHashMap();    headersWithTimestamp.put("timestamp", "-321");    base.setHeaders(headersWithTimestamp);    TimestampedEvent timestampedEvent = new TimestampedEvent(base);    assertEquals(-321L, timestampedEvent.getTimestamp());    assertEquals("-321", timestampedEvent.getHeaders().get("timestamp"));}
0
public void shouldUseExistingAtTimestampHeaderInTimestampedEvent()
{    SimpleEvent base = new SimpleEvent();    Map<String, String> headersWithTimestamp = Maps.newHashMap();    headersWithTimestamp.put("@timestamp", "-999");    base.setHeaders(headersWithTimestamp);    TimestampedEvent timestampedEvent = new TimestampedEvent(base);    assertEquals(-999L, timestampedEvent.getTimestamp());    assertEquals("-999", timestampedEvent.getHeaders().get("@timestamp"));    assertNull(timestampedEvent.getHeaders().get("timestamp"));}
0
public void shouldPreserveBodyAndNonTimestampHeadersInTimestampedEvent()
{    SimpleEvent base = new SimpleEvent();    base.setBody(new byte[] { 1, 2, 3, 4 });    Map<String, String> headersWithTimestamp = Maps.newHashMap();    headersWithTimestamp.put("foo", "bar");    base.setHeaders(headersWithTimestamp);    TimestampedEvent timestampedEvent = new TimestampedEvent(base);    assertEquals("bar", timestampedEvent.getHeaders().get("foo"));    assertArrayEquals(base.getBody(), timestampedEvent.getBody());}
0
public void shouldSetIndexNameTypeAndSerializedEventIntoIndexRequest() throws Exception
{    String indexPrefix = "qwerty";    String indexType = "uiop";    Event event = new SimpleEvent();    IndexRequestBuilder indexRequestBuilder = factory.createIndexRequest(FAKE_CLIENT, indexPrefix, indexType, event);    assertEquals(indexPrefix + '-' + ElasticSearchIndexRequestBuilderFactory.df.format(FIXED_TIME_MILLIS), indexRequestBuilder.request().index());    assertEquals(indexType, indexRequestBuilder.request().type());    assertArrayEquals(FakeEventSerializer.FAKE_BYTES, indexRequestBuilder.request().source().array());}
0
public void shouldSetIndexNameFromTimestampHeaderWhenPresent() throws Exception
{    String indexPrefix = "qwerty";    String indexType = "uiop";    Event event = new SimpleEvent();    event.getHeaders().put("timestamp", "1213141516");    IndexRequestBuilder indexRequestBuilder = factory.createIndexRequest(null, indexPrefix, indexType, event);    assertEquals(indexPrefix + '-' + ElasticSearchIndexRequestBuilderFactory.df.format(1213141516L), indexRequestBuilder.request().index());}
0
public void shouldSetIndexNameTypeFromHeaderWhenPresent() throws Exception
{    String indexPrefix = "%{index-name}";    String indexType = "%{index-type}";    String indexValue = "testing-index-name-from-headers";    String typeValue = "testing-index-type-from-headers";    Event event = new SimpleEvent();    event.getHeaders().put("index-name", indexValue);    event.getHeaders().put("index-type", typeValue);    IndexRequestBuilder indexRequestBuilder = factory.createIndexRequest(null, indexPrefix, indexType, event);    assertEquals(indexValue + '-' + ElasticSearchIndexRequestBuilderFactory.df.format(FIXED_TIME_MILLIS), indexRequestBuilder.request().index());    assertEquals(typeValue, indexRequestBuilder.request().type());}
0
public void shouldConfigureEventSerializer() throws Exception
{    assertFalse(serializer.configuredWithContext);    factory.configure(new Context());    assertTrue(serializer.configuredWithContext);    assertFalse(serializer.configuredWithComponentConfiguration);    factory.configure(new SinkConfiguration("name"));    assertTrue(serializer.configuredWithComponentConfiguration);}
0
public BytesStream getContentBuilder(Event event) throws IOException
{    FastByteArrayOutputStream fbaos = new FastByteArrayOutputStream(4);    fbaos.write(FAKE_BYTES);    return fbaos;}
0
public void configure(Context arg0)
{    configuredWithContext = true;}
0
public void configure(ComponentConfiguration arg0)
{    configuredWithComponentConfiguration = true;}
0
public void testRoundTrip() throws Exception
{    ElasticSearchLogStashEventSerializer fixture = new ElasticSearchLogStashEventSerializer();    Context context = new Context();    fixture.configure(context);    String message = "test body";    Map<String, String> headers = Maps.newHashMap();    long timestamp = System.currentTimeMillis();    headers.put("timestamp", String.valueOf(timestamp));    headers.put("source", "flume_tail_src");    headers.put("host", "test@localhost");    headers.put("src_path", "/tmp/test");    headers.put("headerNameOne", "headerValueOne");    headers.put("headerNameTwo", "headerValueTwo");    headers.put("type", "sometype");    Event event = EventBuilder.withBody(message.getBytes(charset));    event.setHeaders(headers);    XContentBuilder expected = jsonBuilder().startObject();    expected.field("@message", new String(message.getBytes(), charset));    expected.field("@timestamp", new Date(timestamp));    expected.field("@source", "flume_tail_src");    expected.field("@type", "sometype");    expected.field("@source_host", "test@localhost");    expected.field("@source_path", "/tmp/test");    expected.startObject("@fields");    expected.field("timestamp", String.valueOf(timestamp));    expected.field("src_path", "/tmp/test");    expected.field("host", "test@localhost");    expected.field("headerNameTwo", "headerValueTwo");    expected.field("source", "flume_tail_src");    expected.field("headerNameOne", "headerValueOne");    expected.field("type", "sometype");    expected.endObject();    expected.endObject();    XContentBuilder actual = fixture.getContentBuilder(event);    JsonParser parser = new JsonParser();    assertEquals(parser.parse(expected.string()), parser.parse(actual.string()));}
0
public void shouldHandleInvalidJSONDuringComplexParsing() throws Exception
{    ElasticSearchLogStashEventSerializer fixture = new ElasticSearchLogStashEventSerializer();    Context context = new Context();    fixture.configure(context);    String message = "{flume: somethingnotvalid}";    Map<String, String> headers = Maps.newHashMap();    long timestamp = System.currentTimeMillis();    headers.put("timestamp", String.valueOf(timestamp));    headers.put("source", "flume_tail_src");    headers.put("host", "test@localhost");    headers.put("src_path", "/tmp/test");    headers.put("headerNameOne", "headerValueOne");    headers.put("headerNameTwo", "headerValueTwo");    headers.put("type", "sometype");    Event event = EventBuilder.withBody(message.getBytes(charset));    event.setHeaders(headers);    XContentBuilder expected = jsonBuilder().startObject();    expected.field("@message", new String(message.getBytes(), charset));    expected.field("@timestamp", new Date(timestamp));    expected.field("@source", "flume_tail_src");    expected.field("@type", "sometype");    expected.field("@source_host", "test@localhost");    expected.field("@source_path", "/tmp/test");    expected.startObject("@fields");    expected.field("timestamp", String.valueOf(timestamp));    expected.field("src_path", "/tmp/test");    expected.field("host", "test@localhost");    expected.field("headerNameTwo", "headerValueTwo");    expected.field("source", "flume_tail_src");    expected.field("headerNameOne", "headerValueOne");    expected.field("type", "sometype");    expected.endObject();    expected.endObject();    XContentBuilder actual = fixture.getContentBuilder(event);    JsonParser parser = new JsonParser();    assertEquals(parser.parse(expected.string()), parser.parse(actual.string()));}
0
public void init() throws Exception
{    initDefaults();    createNodes();    fixture = new ElasticSearchSink(true);    fixture.setName("ElasticSearchSink-" + UUID.randomUUID().toString());}
0
public void tearDown() throws Exception
{    shutdownNodes();}
0
public void shouldIndexOneEvent() throws Exception
{    Configurables.configure(fixture, new Context(parameters));    Channel channel = bindAndStartChannel(fixture);    Transaction tx = channel.getTransaction();    tx.begin();    Event event = EventBuilder.withBody("event #1 or 1".getBytes());    channel.put(event);    tx.commit();    tx.close();    fixture.process();    fixture.stop();    client.admin().indices().refresh(Requests.refreshRequest(timestampedIndexName)).actionGet();    assertMatchAllQuery(1, event);    assertBodyQuery(1, event);}
0
public void shouldIndexInvalidComplexJsonBody() throws Exception
{    parameters.put(BATCH_SIZE, "3");    Configurables.configure(fixture, new Context(parameters));    Channel channel = bindAndStartChannel(fixture);    Transaction tx = channel.getTransaction();    tx.begin();    Event event1 = EventBuilder.withBody("TEST1 {test}".getBytes());    channel.put(event1);    Event event2 = EventBuilder.withBody("{test: TEST2 }".getBytes());    channel.put(event2);    Event event3 = EventBuilder.withBody("{\"test\":{ TEST3 {test} }}".getBytes());    channel.put(event3);    tx.commit();    tx.close();    fixture.process();    fixture.stop();    client.admin().indices().refresh(Requests.refreshRequest(timestampedIndexName)).actionGet();    assertMatchAllQuery(3);    assertSearch(1, performSearch(QueryBuilders.fieldQuery("@message", "TEST1")), null, event1);    assertSearch(1, performSearch(QueryBuilders.fieldQuery("@message", "TEST2")), null, event2);    assertSearch(1, performSearch(QueryBuilders.fieldQuery("@message", "TEST3")), null, event3);}
0
public void shouldIndexComplexJsonEvent() throws Exception
{    Configurables.configure(fixture, new Context(parameters));    Channel channel = bindAndStartChannel(fixture);    Transaction tx = channel.getTransaction();    tx.begin();    Event event = EventBuilder.withBody("{\"event\":\"json content\",\"num\":1}".getBytes());    channel.put(event);    tx.commit();    tx.close();    fixture.process();    fixture.stop();    client.admin().indices().refresh(Requests.refreshRequest(timestampedIndexName)).actionGet();    Map<String, Object> expectedBody = new HashMap<String, Object>();    expectedBody.put("event", "json content");    expectedBody.put("num", 1);    assertSearch(1, performSearch(QueryBuilders.matchAllQuery()), expectedBody, event);    assertSearch(1, performSearch(QueryBuilders.fieldQuery("@message.event", "json")), expectedBody, event);}
0
public void shouldIndexFiveEvents() throws Exception
{        parameters.put(BATCH_SIZE, "5");    Configurables.configure(fixture, new Context(parameters));    Channel channel = bindAndStartChannel(fixture);    int numberOfEvents = 5;    Event[] events = new Event[numberOfEvents];    Transaction tx = channel.getTransaction();    tx.begin();    for (int i = 0; i < numberOfEvents; i++) {        String body = "event #" + i + " of " + numberOfEvents;        Event event = EventBuilder.withBody(body.getBytes());        events[i] = event;        channel.put(event);    }    tx.commit();    tx.close();    fixture.process();    fixture.stop();    client.admin().indices().refresh(Requests.refreshRequest(timestampedIndexName)).actionGet();    assertMatchAllQuery(numberOfEvents, events);    assertBodyQuery(5, events);}
0
public void shouldIndexFiveEventsOverThreeBatches() throws Exception
{    parameters.put(BATCH_SIZE, "2");    Configurables.configure(fixture, new Context(parameters));    Channel channel = bindAndStartChannel(fixture);    int numberOfEvents = 5;    Event[] events = new Event[numberOfEvents];    Transaction tx = channel.getTransaction();    tx.begin();    for (int i = 0; i < numberOfEvents; i++) {        String body = "event #" + i + " of " + numberOfEvents;        Event event = EventBuilder.withBody(body.getBytes());        events[i] = event;        channel.put(event);    }    tx.commit();    tx.close();    int count = 0;    Status status = Status.READY;    while (status != Status.BACKOFF) {        count++;        status = fixture.process();    }    fixture.stop();    assertEquals(3, count);    client.admin().indices().refresh(Requests.refreshRequest(timestampedIndexName)).actionGet();    assertMatchAllQuery(numberOfEvents, events);    assertBodyQuery(5, events);}
0
public void shouldParseConfiguration()
{    parameters.put(HOSTNAMES, "10.5.5.27");    parameters.put(CLUSTER_NAME, "testing-cluster-name");    parameters.put(INDEX_NAME, "testing-index-name");    parameters.put(INDEX_TYPE, "testing-index-type");    parameters.put(TTL, "10");    fixture = new ElasticSearchSink();    fixture.configure(new Context(parameters));    String[] expected = { "10.5.5.27" };    assertEquals("testing-cluster-name", fixture.getClusterName());    assertEquals("testing-index-name", fixture.getIndexName());    assertEquals("testing-index-type", fixture.getIndexType());    assertEquals(TimeUnit.DAYS.toMillis(10), fixture.getTTLMs());    assertArrayEquals(expected, fixture.getServerAddresses());}
0
public void shouldParseConfigurationUsingDefaults()
{    parameters.put(HOSTNAMES, "10.5.5.27");    parameters.remove(INDEX_NAME);    parameters.remove(INDEX_TYPE);    parameters.remove(CLUSTER_NAME);    fixture = new ElasticSearchSink();    fixture.configure(new Context(parameters));    String[] expected = { "10.5.5.27" };    assertEquals(DEFAULT_INDEX_NAME, fixture.getIndexName());    assertEquals(DEFAULT_INDEX_TYPE, fixture.getIndexType());    assertEquals(DEFAULT_CLUSTER_NAME, fixture.getClusterName());    assertArrayEquals(expected, fixture.getServerAddresses());}
0
public void shouldParseMultipleHostUsingDefaultPorts()
{    parameters.put(HOSTNAMES, "10.5.5.27,10.5.5.28,10.5.5.29");    fixture = new ElasticSearchSink();    fixture.configure(new Context(parameters));    String[] expected = { "10.5.5.27", "10.5.5.28", "10.5.5.29" };    assertArrayEquals(expected, fixture.getServerAddresses());}
0
public void shouldParseMultipleHostWithWhitespacesUsingDefaultPorts()
{    parameters.put(HOSTNAMES, " 10.5.5.27 , 10.5.5.28 , 10.5.5.29 ");    fixture = new ElasticSearchSink();    fixture.configure(new Context(parameters));    String[] expected = { "10.5.5.27", "10.5.5.28", "10.5.5.29" };    assertArrayEquals(expected, fixture.getServerAddresses());}
0
public void shouldParseMultipleHostAndPorts()
{    parameters.put(HOSTNAMES, "10.5.5.27:9300,10.5.5.28:9301,10.5.5.29:9302");    fixture = new ElasticSearchSink();    fixture.configure(new Context(parameters));    String[] expected = { "10.5.5.27:9300", "10.5.5.28:9301", "10.5.5.29:9302" };    assertArrayEquals(expected, fixture.getServerAddresses());}
0
public void shouldParseMultipleHostAndPortsWithWhitespaces()
{    parameters.put(HOSTNAMES, " 10.5.5.27 : 9300 , 10.5.5.28 : 9301 , 10.5.5.29 : 9302 ");    fixture = new ElasticSearchSink();    fixture.configure(new Context(parameters));    String[] expected = { "10.5.5.27:9300", "10.5.5.28:9301", "10.5.5.29:9302" };    assertArrayEquals(expected, fixture.getServerAddresses());}
0
public void shouldAllowCustomElasticSearchIndexRequestBuilderFactory() throws Exception
{    parameters.put(SERIALIZER, CustomElasticSearchIndexRequestBuilderFactory.class.getName());    fixture.configure(new Context(parameters));    Channel channel = bindAndStartChannel(fixture);    Transaction tx = channel.getTransaction();    tx.begin();    String body = "{ foo: \"bar\" }";    Event event = EventBuilder.withBody(body.getBytes());    channel.put(event);    tx.commit();    tx.close();    fixture.process();    fixture.stop();    assertEquals(fixture.getIndexName() + "-05_17_36_789", CustomElasticSearchIndexRequestBuilderFactory.actualIndexName);    assertEquals(fixture.getIndexType(), CustomElasticSearchIndexRequestBuilderFactory.actualIndexType);    assertArrayEquals(event.getBody(), CustomElasticSearchIndexRequestBuilderFactory.actualEventBody);    assertTrue(CustomElasticSearchIndexRequestBuilderFactory.hasContext);}
0
public void shouldParseFullyQualifiedTTLs()
{    Map<String, Long> testTTLMap = new HashMap<String, Long>();    testTTLMap.put("1ms", Long.valueOf(1));    testTTLMap.put("1s", Long.valueOf(1000));    testTTLMap.put("1m", Long.valueOf(60000));    testTTLMap.put("1h", Long.valueOf(3600000));    testTTLMap.put("1d", Long.valueOf(86400000));    testTTLMap.put("1w", Long.valueOf(604800000));    testTTLMap.put("1", Long.valueOf(86400000));    parameters.put(HOSTNAMES, "10.5.5.27");    parameters.put(CLUSTER_NAME, "testing-cluster-name");    parameters.put(INDEX_NAME, "testing-index-name");    parameters.put(INDEX_TYPE, "testing-index-type");    for (String ttl : testTTLMap.keySet()) {        parameters.put(TTL, ttl);        fixture = new ElasticSearchSink();        fixture.configure(new Context(parameters));        String[] expected = { "10.5.5.27" };        assertEquals("testing-cluster-name", fixture.getClusterName());        assertEquals("testing-index-name", fixture.getIndexName());        assertEquals("testing-index-type", fixture.getIndexType());        assertEquals((long) testTTLMap.get(ttl), fixture.getTTLMs());        assertArrayEquals(expected, fixture.getServerAddresses());    }}
0
protected void prepareIndexRequest(IndexRequestBuilder indexRequest, String indexName, String indexType, Event event) throws IOException
{    actualIndexName = indexName;    actualIndexType = indexType;    actualEventBody = event.getBody();    indexRequest.setIndex(indexName).setType(indexType).setSource(event.getBody());}
0
public void configure(Context arg0)
{    hasContext = true;}
0
public void configure(ComponentConfiguration arg0)
{}
0
public void shouldFailToConfigureWithInvalidSerializerClass() throws Exception
{    parameters.put(SERIALIZER, "java.lang.String");    try {        Configurables.configure(fixture, new Context(parameters));    } catch (ClassCastException e) {        }    parameters.put(SERIALIZER, FakeConfigurable.class.getName());    try {        Configurables.configure(fixture, new Context(parameters));    } catch (IllegalArgumentException e) {        }}
0
public void shouldUseSpecifiedSerializer() throws Exception
{    Context context = new Context();    context.put(SERIALIZER, "org.apache.flume.sink.elasticsearch.FakeEventSerializer");    assertNull(fixture.getEventSerializer());    fixture.configure(context);    assertTrue(fixture.getEventSerializer() instanceof FakeEventSerializer);}
0
public void shouldUseSpecifiedIndexNameBuilder() throws Exception
{    Context context = new Context();    context.put(ElasticSearchSinkConstants.INDEX_NAME_BUILDER, "org.apache.flume.sink.elasticsearch.FakeIndexNameBuilder");    assertNull(fixture.getIndexNameBuilder());    fixture.configure(context);    assertTrue(fixture.getIndexNameBuilder() instanceof FakeIndexNameBuilder);}
0
public void configure(Context arg0)
{}
0
public BytesStream getContentBuilder(Event event) throws IOException
{    FastByteArrayOutputStream fbaos = new FastByteArrayOutputStream(4);    fbaos.write(FAKE_BYTES);    return fbaos;}
0
public void configure(Context arg0)
{    configuredWithContext = true;}
0
public void configure(ComponentConfiguration arg0)
{    configuredWithComponentConfiguration = true;}
0
public String getIndexName(Event event)
{    return INDEX_NAME;}
0
public String getIndexPrefix(Event event)
{    return INDEX_NAME;}
0
public void configure(Context context)
{}
0
public void configure(ComponentConfiguration conf)
{}
0
public void setUp()
{    sinkFactory = new DefaultSinkFactory();}
0
private void verifySinkCreation(String name, String type, Class<?> typeClass) throws FlumeException
{    Sink sink = sinkFactory.create(name, type);    Assert.assertNotNull(sink);    Assert.assertTrue(typeClass.isInstance(sink));}
0
public void testSinkCreation()
{    verifySinkCreation("elasticsearch-sink", "elasticsearch", ElasticSearchSink.class);}
0
public void setUp() throws Exception
{    Context context = new Context();    context.put(ElasticSearchSinkConstants.INDEX_NAME, "prefix");    indexNameBuilder = new TimeBasedIndexNameBuilder();    indexNameBuilder.configure(context);}
0
public void shouldUseUtcAsBasisForDateFormat()
{    assertEquals("Coordinated Universal Time", indexNameBuilder.getFastDateFormat().getTimeZone().getDisplayName());}
0
public void indexNameShouldBePrefixDashFormattedTimestamp()
{    long time = 987654321L;    Event event = new SimpleEvent();    Map<String, String> headers = new HashMap<String, String>();    headers.put("timestamp", Long.toString(time));    event.setHeaders(headers);    assertEquals("prefix-" + indexNameBuilder.getFastDateFormat().format(time), indexNameBuilder.getIndexName(event));}
0
public void setFixedJodaTime()
{    DateTimeUtils.setCurrentMillisFixed(FIXED_TIME_MILLIS);}
0
public void shouldEnsureTimestampHeaderPresentInTimestampedEvent()
{    SimpleEvent base = new SimpleEvent();    TimestampedEvent timestampedEvent = new TimestampedEvent(base);    assertEquals(FIXED_TIME_MILLIS, timestampedEvent.getTimestamp());    assertEquals(String.valueOf(FIXED_TIME_MILLIS), timestampedEvent.getHeaders().get("timestamp"));}
0
public void shouldUseExistingTimestampHeaderInTimestampedEvent()
{    SimpleEvent base = new SimpleEvent();    Map<String, String> headersWithTimestamp = Maps.newHashMap();    headersWithTimestamp.put("timestamp", "-321");    base.setHeaders(headersWithTimestamp);    TimestampedEvent timestampedEvent = new TimestampedEvent(base);    assertEquals(-321L, timestampedEvent.getTimestamp());    assertEquals("-321", timestampedEvent.getHeaders().get("timestamp"));}
0
public void shouldUseExistingAtTimestampHeaderInTimestampedEvent()
{    SimpleEvent base = new SimpleEvent();    Map<String, String> headersWithTimestamp = Maps.newHashMap();    headersWithTimestamp.put("@timestamp", "-999");    base.setHeaders(headersWithTimestamp);    TimestampedEvent timestampedEvent = new TimestampedEvent(base);    assertEquals(-999L, timestampedEvent.getTimestamp());    assertEquals("-999", timestampedEvent.getHeaders().get("@timestamp"));    assertNull(timestampedEvent.getHeaders().get("timestamp"));}
0
public void shouldPreserveBodyAndNonTimestampHeadersInTimestampedEvent()
{    SimpleEvent base = new SimpleEvent();    base.setBody(new byte[] { 1, 2, 3, 4 });    Map<String, String> headersWithTimestamp = Maps.newHashMap();    headersWithTimestamp.put("foo", "bar");    base.setHeaders(headersWithTimestamp);    TimestampedEvent timestampedEvent = new TimestampedEvent(base);    assertEquals("bar", timestampedEvent.getHeaders().get("foo"));    assertArrayEquals(base.getBody(), timestampedEvent.getBody());}
0
public Status process() throws EventDeliveryException
{    /*     * Reference to the boolean representing failure of the current transaction.     * Since each txn gets a new boolean, failure of one txn will not affect     * the next even if errbacks for the current txn get called while     * the next one is being processed.     *     */    if (!open) {        throw new EventDeliveryException("Sink was never opened. " + "Please fix the configuration.");    }    if (client == null) {        client = initHBaseClient();        if (client == null) {            throw new EventDeliveryException("Could not establish connection to HBase!");        }    }    AtomicBoolean txnFail = new AtomicBoolean(false);    AtomicInteger callbacksReceived = new AtomicInteger(0);    AtomicInteger callbacksExpected = new AtomicInteger(0);    final Lock lock = new ReentrantLock();    final Condition condition = lock.newCondition();    if (incrementBuffer != null) {        incrementBuffer.clear();    }    /*     * Callbacks can be reused per transaction, since they share the same     * locks and conditions.     */    Callback<Object, Object> putSuccessCallback = new SuccessCallback<Object, Object>(lock, callbacksReceived, condition);    Callback<Object, Exception> putFailureCallback = new FailureCallback<Object, Exception>(lock, callbacksReceived, txnFail, condition);    Callback<Long, Long> incrementSuccessCallback = new SuccessCallback<Long, Long>(lock, callbacksReceived, condition);    Callback<Long, Exception> incrementFailureCallback = new FailureCallback<Long, Exception>(lock, callbacksReceived, txnFail, condition);    Status status = Status.READY;    Channel channel = getChannel();    txn = channel.getTransaction();    txn.begin();    int i = 0;    try {        for (; i < batchSize; i++) {            Event event = channel.take();            if (event == null) {                status = Status.BACKOFF;                if (i == 0) {                    sinkCounter.incrementBatchEmptyCount();                } else {                    sinkCounter.incrementBatchUnderflowCount();                }                break;            } else {                serializer.setEvent(event);                List<PutRequest> actions = serializer.getActions();                List<AtomicIncrementRequest> increments = serializer.getIncrements();                callbacksExpected.addAndGet(actions.size());                if (!batchIncrements) {                    callbacksExpected.addAndGet(increments.size());                }                for (PutRequest action : actions) {                    action.setDurable(enableWal);                    client.put(action).addCallbacks(putSuccessCallback, putFailureCallback);                }                for (AtomicIncrementRequest increment : increments) {                    if (batchIncrements) {                        CellIdentifier identifier = new CellIdentifier(increment.key(), increment.qualifier());                        AtomicIncrementRequest request = incrementBuffer.get(identifier);                        if (request == null) {                            incrementBuffer.put(identifier, increment);                        } else {                            request.setAmount(request.getAmount() + increment.getAmount());                        }                    } else {                        client.atomicIncrement(increment).addCallbacks(incrementSuccessCallback, incrementFailureCallback);                    }                }            }        }        if (batchIncrements) {            Collection<AtomicIncrementRequest> increments = incrementBuffer.values();            for (AtomicIncrementRequest increment : increments) {                client.atomicIncrement(increment).addCallbacks(incrementSuccessCallback, incrementFailureCallback);            }            callbacksExpected.addAndGet(increments.size());        }        client.flush();    } catch (Throwable e) {        this.handleTransactionFailure(txn);        this.checkIfChannelExceptionAndThrow(e);    }    if (i == batchSize) {        sinkCounter.incrementBatchCompleteCount();    }    sinkCounter.addToEventDrainAttemptCount(i);    lock.lock();    long startTime = System.nanoTime();    long timeRemaining;    try {        while ((callbacksReceived.get() < callbacksExpected.get()) && !txnFail.get()) {            timeRemaining = timeout - (System.nanoTime() - startTime);            timeRemaining = (timeRemaining >= 0) ? timeRemaining : 0;            try {                if (!condition.await(timeRemaining, TimeUnit.NANOSECONDS)) {                    txnFail.set(true);                                    }            } catch (Exception ex) {                                this.handleTransactionFailure(txn);                Throwables.propagate(ex);            }        }    } finally {        lock.unlock();    }    if (isCoalesceTest) {        totalCallbacksReceived += callbacksReceived.get();    }    /*     * At this point, either the txn has failed     * or all callbacks received and txn is successful.     *     * This need not be in the monitor, since all callbacks for this txn     * have been received. So txnFail will not be modified any more(even if     * it is, it is set from true to true only - false happens only     * in the next process call).     *     */    if (txnFail.get()) {                if (lastTxnFailed) {            consecutiveHBaseFailures++;        }        lastTxnFailed = true;        this.handleTransactionFailure(txn);        throw new EventDeliveryException("Could not write events to Hbase. " + "Transaction failed, and rolled back.");    } else {        try {            lastTxnFailed = false;            consecutiveHBaseFailures = 0;            txn.commit();            txn.close();            sinkCounter.addToEventDrainSuccessCount(i);        } catch (Throwable e) {            this.handleTransactionFailure(txn);            this.checkIfChannelExceptionAndThrow(e);        }    }    return status;}
1
public void configure(Context context)
{    if (!HBaseVersionCheck.hasVersionLessThan2(logger)) {        throw new ConfigurationException("HBase major version number must be less than 2 for asynchbase sink. ");    }    tableName = context.getString(HBaseSinkConfigurationConstants.CONFIG_TABLE);    String cf = context.getString(HBaseSinkConfigurationConstants.CONFIG_COLUMN_FAMILY);    batchSize = context.getLong(HBaseSinkConfigurationConstants.CONFIG_BATCHSIZE, new Long(100));    serializerContext = new Context();        eventSerializerType = context.getString(HBaseSinkConfigurationConstants.CONFIG_SERIALIZER);    Preconditions.checkNotNull(tableName, "Table name cannot be empty, please specify in configuration file");    Preconditions.checkNotNull(cf, "Column family cannot be empty, please specify in configuration file");        if (eventSerializerType == null || eventSerializerType.isEmpty()) {        eventSerializerType = "org.apache.flume.sink.hbase.SimpleAsyncHbaseEventSerializer";            }    serializerContext.putAll(context.getSubProperties(HBaseSinkConfigurationConstants.CONFIG_SERIALIZER_PREFIX));    columnFamily = cf.getBytes(Charsets.UTF_8);    try {        @SuppressWarnings("unchecked")        Class<? extends AsyncHbaseEventSerializer> clazz = (Class<? extends AsyncHbaseEventSerializer>) Class.forName(eventSerializerType);        serializer = clazz.newInstance();        serializer.configure(serializerContext);        serializer.initialize(tableName.getBytes(Charsets.UTF_8), columnFamily);    } catch (Exception e) {                Throwables.propagate(e);    }    if (sinkCounter == null) {        sinkCounter = new SinkCounter(this.getName());    }    timeout = context.getLong(HBaseSinkConfigurationConstants.CONFIG_TIMEOUT, HBaseSinkConfigurationConstants.DEFAULT_TIMEOUT);    if (timeout <= 0) {                timeout = HBaseSinkConfigurationConstants.DEFAULT_TIMEOUT;    }        timeout = TimeUnit.MILLISECONDS.toNanos(timeout);    zkQuorum = context.getString(HBaseSinkConfigurationConstants.ZK_QUORUM, "").trim();    if (!zkQuorum.isEmpty()) {        zkBaseDir = context.getString(HBaseSinkConfigurationConstants.ZK_ZNODE_PARENT, HBaseSinkConfigurationConstants.DEFAULT_ZK_ZNODE_PARENT);    } else {        if (conf == null) {                        conf = HBaseConfiguration.create();        }        zkQuorum = ZKConfig.getZKQuorumServersString(conf);        zkBaseDir = conf.get(HConstants.ZOOKEEPER_ZNODE_PARENT, HConstants.DEFAULT_ZOOKEEPER_ZNODE_PARENT);    }    Preconditions.checkState(zkQuorum != null && !zkQuorum.isEmpty(), "The Zookeeper quorum cannot be null and should be specified.");    enableWal = context.getBoolean(HBaseSinkConfigurationConstants.CONFIG_ENABLE_WAL, HBaseSinkConfigurationConstants.DEFAULT_ENABLE_WAL);        if (!enableWal) {            }    batchIncrements = context.getBoolean(HBaseSinkConfigurationConstants.CONFIG_COALESCE_INCREMENTS, HBaseSinkConfigurationConstants.DEFAULT_COALESCE_INCREMENTS);    if (batchIncrements) {        incrementBuffer = Maps.newHashMap();            }    maxConsecutiveFails = context.getInteger(HBaseSinkConfigurationConstants.CONFIG_MAX_CONSECUTIVE_FAILS, HBaseSinkConfigurationConstants.DEFAULT_MAX_CONSECUTIVE_FAILS);    Map<String, String> asyncProperties = context.getSubProperties(HBaseSinkConfigurationConstants.ASYNC_PREFIX);    asyncClientConfig = new Config();    asyncClientConfig.overrideConfig(HBaseSinkConfigurationConstants.ASYNC_ZK_QUORUM_KEY, zkQuorum);    asyncClientConfig.overrideConfig(HBaseSinkConfigurationConstants.ASYNC_ZK_BASEPATH_KEY, zkBaseDir);    for (String property : asyncProperties.keySet()) {        asyncClientConfig.overrideConfig(property, asyncProperties.get(property));    }}
1
 int getTotalCallbacksReceived()
{    return totalCallbacksReceived;}
0
 boolean isConfNull()
{    return conf == null;}
0
public long getBatchSize()
{    return batchSize;}
0
public void start()
{    Preconditions.checkArgument(client == null, "Please call stop " + "before calling start on an old instance.");    sinkCounter.start();    sinkCounter.incrementConnectionCreatedCount();    client = initHBaseClient();    super.start();}
0
private HBaseClient initHBaseClient()
{        sinkCallbackPool = Executors.newCachedThreadPool(new ThreadFactoryBuilder().setNameFormat(this.getName() + " HBase Call Pool").build());        client = new HBaseClient(asyncClientConfig, new NioClientSocketChannelFactory(sinkCallbackPool, sinkCallbackPool));    final CountDownLatch latch = new CountDownLatch(1);    final AtomicBoolean fail = new AtomicBoolean(false);    client.ensureTableFamilyExists(tableName.getBytes(Charsets.UTF_8), columnFamily).addCallbacks(new Callback<Object, Object>() {        @Override        public Object call(Object arg) throws Exception {            latch.countDown();                        return null;        }    }, new Callback<Object, Object>() {        @Override        public Object call(Object arg) throws Exception {            fail.set(true);            latch.countDown();            return null;        }    });    try {                latch.await();            } catch (InterruptedException e) {        sinkCounter.incrementConnectionFailedCount();        throw new FlumeException("Interrupted while waiting for Hbase Callbacks", e);    }    if (fail.get()) {        sinkCounter.incrementConnectionFailedCount();        if (client != null) {            shutdownHBaseClient();        }        throw new FlumeException("Could not start sink. " + "Table or column family does not exist in Hbase.");    } else {        open = true;    }    client.setFlushInterval((short) 0);    return client;}
1
public Object call(Object arg) throws Exception
{    latch.countDown();        return null;}
1
public Object call(Object arg) throws Exception
{    fail.set(true);    latch.countDown();    return null;}
0
public void stop()
{    serializer.cleanUp();    if (client != null) {        shutdownHBaseClient();    }    sinkCounter.incrementConnectionClosedCount();    sinkCounter.stop();    try {        if (sinkCallbackPool != null) {            sinkCallbackPool.shutdown();            if (!sinkCallbackPool.awaitTermination(5, TimeUnit.SECONDS)) {                sinkCallbackPool.shutdownNow();            }        }    } catch (InterruptedException e) {                if (sinkCallbackPool != null) {            sinkCallbackPool.shutdownNow();        }    }    sinkCallbackPool = null;    client = null;    conf = null;    open = false;    super.stop();}
1
private void shutdownHBaseClient()
{        final CountDownLatch waiter = new CountDownLatch(1);    try {        client.shutdown().addCallback(new Callback<Object, Object>() {            @Override            public Object call(Object arg) throws Exception {                waiter.countDown();                return null;            }        }).addErrback(new Callback<Object, Object>() {            @Override            public Object call(Object arg) throws Exception {                                waiter.countDown();                return null;            }        });        if (!waiter.await(timeout, TimeUnit.NANOSECONDS)) {                    }    } catch (Exception ex) {            } finally {                client = null;    }}
1
public Object call(Object arg) throws Exception
{    waiter.countDown();    return null;}
0
public Object call(Object arg) throws Exception
{        waiter.countDown();    return null;}
1
private void handleTransactionFailure(Transaction txn) throws EventDeliveryException
{    if (maxConsecutiveFails > 0 && consecutiveHBaseFailures >= maxConsecutiveFails) {        if (client != null) {            shutdownHBaseClient();        }        consecutiveHBaseFailures = 0;    }    try {        txn.rollback();    } catch (Throwable e) {                if (e instanceof Error || e instanceof RuntimeException) {                        Throwables.propagate(e);        } else {                        throw new EventDeliveryException("Failed to commit transaction." + "Transaction rolled back.", e);        }    } finally {        txn.close();    }}
1
public R call(T arg) throws Exception
{    if (isTimeoutTesting) {        try {                        TimeUnit.NANOSECONDS.sleep(TimeUnit.SECONDS.toNanos(4));        } catch (InterruptedException e) {                }    }    doCall();    return null;}
0
private void doCall() throws Exception
{    callbacksReceived.incrementAndGet();    lock.lock();    try {        condition.signal();    } finally {        lock.unlock();    }}
0
public R call(T arg) throws Exception
{        if (isTimeoutTesting) {                try {            TimeUnit.NANOSECONDS.sleep(TimeUnit.SECONDS.toNanos(4));        } catch (InterruptedException e) {                }    }    doCall();    return null;}
1
private void doCall() throws Exception
{    callbacksReceived.incrementAndGet();    this.txnFail.set(true);    lock.lock();    try {        condition.signal();    } finally {        lock.unlock();    }}
0
private void checkIfChannelExceptionAndThrow(Throwable e) throws EventDeliveryException
{    if (e instanceof ChannelException) {        throw new EventDeliveryException("Error in processing transaction.", e);    } else if (e instanceof Error || e instanceof RuntimeException) {        Throwables.propagate(e);    }    throw new EventDeliveryException("Error in processing transaction.", e);}
0
public int hashCode()
{    return hashCode;}
0
public boolean equals(Object other)
{    CellIdentifier o = (CellIdentifier) other;    if (other == null) {        return false;    } else {        return (COMPARATOR.compare(row, o.row) == 0 && COMPARATOR.compare(column, o.column) == 0);    }}
0
public void start()
{    Preconditions.checkArgument(table == null, "Please call stop " + "before calling start on an old instance.");    try {        privilegedExecutor = FlumeAuthenticationUtil.getAuthenticator(kerberosPrincipal, kerberosKeytab);    } catch (Exception ex) {        sinkCounter.incrementConnectionFailedCount();        throw new FlumeException("Failed to login to HBase using " + "provided credentials.", ex);    }    try {        table = privilegedExecutor.execute(new PrivilegedExceptionAction<HTable>() {            @Override            public HTable run() throws Exception {                HTable table = new HTable(config, tableName);                table.setAutoFlush(false);                                return table;            }        });    } catch (Exception e) {        sinkCounter.incrementConnectionFailedCount();                throw new FlumeException("Could not load table, " + tableName + " from HBase", e);    }    try {        if (!privilegedExecutor.execute(new PrivilegedExceptionAction<Boolean>() {            @Override            public Boolean run() throws IOException {                return table.getTableDescriptor().hasFamily(columnFamily);            }        })) {            throw new IOException("Table " + tableName + " has no such column family " + Bytes.toString(columnFamily));        }    } catch (Exception e) {                        sinkCounter.incrementConnectionFailedCount();        throw new FlumeException("Error getting column family from HBase." + "Please verify that the table " + tableName + " and Column Family, " + Bytes.toString(columnFamily) + " exists in HBase, and the" + " current user has permissions to access that table.", e);    }    super.start();    sinkCounter.incrementConnectionCreatedCount();    sinkCounter.start();}
1
public HTable run() throws Exception
{    HTable table = new HTable(config, tableName);    table.setAutoFlush(false);        return table;}
0
public Boolean run() throws IOException
{    return table.getTableDescriptor().hasFamily(columnFamily);}
0
public void stop()
{    try {        if (table != null) {            table.close();        }        table = null;    } catch (IOException e) {        throw new FlumeException("Error closing table.", e);    }    sinkCounter.incrementConnectionClosedCount();    sinkCounter.stop();}
0
public void configure(Context context)
{    if (!HBaseVersionCheck.hasVersionLessThan2(logger)) {        throw new ConfigurationException("HBase major version number must be less than 2 for hbase-sink.");    }    tableName = context.getString(HBaseSinkConfigurationConstants.CONFIG_TABLE);    String cf = context.getString(HBaseSinkConfigurationConstants.CONFIG_COLUMN_FAMILY);    batchSize = context.getLong(HBaseSinkConfigurationConstants.CONFIG_BATCHSIZE, new Long(100));    serializerContext = new Context();        eventSerializerType = context.getString(HBaseSinkConfigurationConstants.CONFIG_SERIALIZER);    Preconditions.checkNotNull(tableName, "Table name cannot be empty, please specify in configuration file");    Preconditions.checkNotNull(cf, "Column family cannot be empty, please specify in configuration file");        if (eventSerializerType == null || eventSerializerType.isEmpty()) {        eventSerializerType = "org.apache.flume.sink.hbase.SimpleHbaseEventSerializer";            }    serializerContext.putAll(context.getSubProperties(HBaseSinkConfigurationConstants.CONFIG_SERIALIZER_PREFIX));    columnFamily = cf.getBytes(Charsets.UTF_8);    try {        Class<? extends HbaseEventSerializer> clazz = (Class<? extends HbaseEventSerializer>) Class.forName(eventSerializerType);        serializer = clazz.newInstance();        serializer.configure(serializerContext);    } catch (Exception e) {                Throwables.propagate(e);    }    kerberosKeytab = context.getString(HBaseSinkConfigurationConstants.CONFIG_KEYTAB);    kerberosPrincipal = context.getString(HBaseSinkConfigurationConstants.CONFIG_PRINCIPAL);    enableWal = context.getBoolean(HBaseSinkConfigurationConstants.CONFIG_ENABLE_WAL, HBaseSinkConfigurationConstants.DEFAULT_ENABLE_WAL);        if (!enableWal) {            }    batchIncrements = context.getBoolean(HBaseSinkConfigurationConstants.CONFIG_COALESCE_INCREMENTS, HBaseSinkConfigurationConstants.DEFAULT_COALESCE_INCREMENTS);    if (batchIncrements) {                refGetFamilyMap = reflectLookupGetFamilyMap();    }    String zkQuorum = context.getString(HBaseSinkConfigurationConstants.ZK_QUORUM);    Integer port = null;    /**     * HBase allows multiple nodes in the quorum, but all need to use the     * same client port. So get the nodes in host:port format,     * and ignore the ports for all nodes except the first one. If no port is     * specified, use default.     */    if (zkQuorum != null && !zkQuorum.isEmpty()) {        StringBuilder zkBuilder = new StringBuilder();                String[] zkHosts = zkQuorum.split(",");        int length = zkHosts.length;        for (int i = 0; i < length; i++) {            String[] zkHostAndPort = zkHosts[i].split(":");            zkBuilder.append(zkHostAndPort[0].trim());            if (i != length - 1) {                zkBuilder.append(",");            } else {                zkQuorum = zkBuilder.toString();            }            if (zkHostAndPort[1] == null) {                throw new FlumeException("Expected client port for the ZK node!");            }            if (port == null) {                port = Integer.parseInt(zkHostAndPort[1].trim());            } else if (!port.equals(Integer.parseInt(zkHostAndPort[1].trim()))) {                throw new FlumeException("All Zookeeper nodes in the quorum must " + "use the same client port.");            }        }        if (port == null) {            port = HConstants.DEFAULT_ZOOKEPER_CLIENT_PORT;        }        this.config.set(HConstants.ZOOKEEPER_QUORUM, zkQuorum);        this.config.setInt(HConstants.ZOOKEEPER_CLIENT_PORT, port);    }    String hbaseZnode = context.getString(HBaseSinkConfigurationConstants.ZK_ZNODE_PARENT);    if (hbaseZnode != null && !hbaseZnode.isEmpty()) {        this.config.set(HConstants.ZOOKEEPER_ZNODE_PARENT, hbaseZnode);    }    sinkCounter = new SinkCounter(this.getName());}
1
public Configuration getConfig()
{    return config;}
0
public Status process() throws EventDeliveryException
{    Status status = Status.READY;    Channel channel = getChannel();    Transaction txn = channel.getTransaction();    List<Row> actions = new LinkedList<Row>();    List<Increment> incs = new LinkedList<Increment>();    try {        txn.begin();        if (serializer instanceof BatchAware) {            ((BatchAware) serializer).onBatchStart();        }        long i = 0;        for (; i < batchSize; i++) {            Event event = channel.take();            if (event == null) {                if (i == 0) {                    status = Status.BACKOFF;                    sinkCounter.incrementBatchEmptyCount();                } else {                    sinkCounter.incrementBatchUnderflowCount();                }                break;            } else {                serializer.initialize(event, columnFamily);                actions.addAll(serializer.getActions());                incs.addAll(serializer.getIncrements());            }        }        if (i == batchSize) {            sinkCounter.incrementBatchCompleteCount();        }        sinkCounter.addToEventDrainAttemptCount(i);        putEventsAndCommit(actions, incs, txn);    } catch (Throwable e) {        try {            txn.rollback();        } catch (Exception e2) {                    }                if (e instanceof Error || e instanceof RuntimeException) {                        Throwables.propagate(e);        } else {                        throw new EventDeliveryException("Failed to commit transaction." + "Transaction rolled back.", e);        }    } finally {        txn.close();    }    return status;}
1
private void putEventsAndCommit(final List<Row> actions, final List<Increment> incs, Transaction txn) throws Exception
{    privilegedExecutor.execute(new PrivilegedExceptionAction<Void>() {        @Override        public Void run() throws Exception {            for (Row r : actions) {                if (r instanceof Put) {                    ((Put) r).setWriteToWAL(enableWal);                }                                if (r instanceof Increment) {                    ((Increment) r).setWriteToWAL(enableWal);                }            }            table.batch(actions);            return null;        }    });    privilegedExecutor.execute(new PrivilegedExceptionAction<Void>() {        @Override        public Void run() throws Exception {            List<Increment> processedIncrements;            if (batchIncrements) {                processedIncrements = coalesceIncrements(incs);            } else {                processedIncrements = incs;            }                        if (debugIncrCallback != null) {                debugIncrCallback.onAfterCoalesce(processedIncrements);            }            for (final Increment i : processedIncrements) {                i.setWriteToWAL(enableWal);                table.increment(i);            }            return null;        }    });    txn.commit();    sinkCounter.addToEventDrainSuccessCount(actions.size());}
0
public Void run() throws Exception
{    for (Row r : actions) {        if (r instanceof Put) {            ((Put) r).setWriteToWAL(enableWal);        }                if (r instanceof Increment) {            ((Increment) r).setWriteToWAL(enableWal);        }    }    table.batch(actions);    return null;}
0
public Void run() throws Exception
{    List<Increment> processedIncrements;    if (batchIncrements) {        processedIncrements = coalesceIncrements(incs);    } else {        processedIncrements = incs;    }        if (debugIncrCallback != null) {        debugIncrCallback.onAfterCoalesce(processedIncrements);    }    for (final Increment i : processedIncrements) {        i.setWriteToWAL(enableWal);        table.increment(i);    }    return null;}
0
private Map<byte[], NavigableMap<byte[], Long>> getFamilyMap(Increment inc)
{    Preconditions.checkNotNull(refGetFamilyMap, "Increment.getFamilymap() not found");    Preconditions.checkNotNull(inc, "Increment required");    Map<byte[], NavigableMap<byte[], Long>> familyMap = null;    try {        Object familyObj = refGetFamilyMap.invoke(inc);        familyMap = (Map<byte[], NavigableMap<byte[], Long>>) familyObj;    } catch (IllegalAccessException e) {                Throwables.propagate(e);    } catch (InvocationTargetException e) {                Throwables.propagate(e);    }    return familyMap;}
1
private List<Increment> coalesceIncrements(Iterable<Increment> incs)
{    Preconditions.checkNotNull(incs, "List of Increments must not be null");            Map<byte[], Map<byte[], NavigableMap<byte[], Long>>> counters = Maps.newTreeMap(Bytes.BYTES_COMPARATOR);    for (Increment inc : incs) {        byte[] row = inc.getRow();        Map<byte[], NavigableMap<byte[], Long>> families = getFamilyMap(inc);        for (Map.Entry<byte[], NavigableMap<byte[], Long>> familyEntry : families.entrySet()) {            byte[] family = familyEntry.getKey();            NavigableMap<byte[], Long> qualifiers = familyEntry.getValue();            for (Map.Entry<byte[], Long> qualifierEntry : qualifiers.entrySet()) {                byte[] qualifier = qualifierEntry.getKey();                Long count = qualifierEntry.getValue();                incrementCounter(counters, row, family, qualifier, count);            }        }    }        List<Increment> coalesced = Lists.newLinkedList();    for (Map.Entry<byte[], Map<byte[], NavigableMap<byte[], Long>>> rowEntry : counters.entrySet()) {        byte[] row = rowEntry.getKey();        Map<byte[], NavigableMap<byte[], Long>> families = rowEntry.getValue();        Increment inc = new Increment(row);        for (Map.Entry<byte[], NavigableMap<byte[], Long>> familyEntry : families.entrySet()) {            byte[] family = familyEntry.getKey();            NavigableMap<byte[], Long> qualifiers = familyEntry.getValue();            for (Map.Entry<byte[], Long> qualifierEntry : qualifiers.entrySet()) {                byte[] qualifier = qualifierEntry.getKey();                long count = qualifierEntry.getValue();                inc.addColumn(family, qualifier, count);            }        }        coalesced.add(inc);    }    return coalesced;}
0
private void incrementCounter(Map<byte[], Map<byte[], NavigableMap<byte[], Long>>> counters, byte[] row, byte[] family, byte[] qualifier, Long count)
{    Map<byte[], NavigableMap<byte[], Long>> families = counters.get(row);    if (families == null) {        families = Maps.newTreeMap(Bytes.BYTES_COMPARATOR);        counters.put(row, families);    }    NavigableMap<byte[], Long> qualifiers = families.get(family);    if (qualifiers == null) {        qualifiers = Maps.newTreeMap(Bytes.BYTES_COMPARATOR);        families.put(family, qualifiers);    }    Long existingValue = qualifiers.get(qualifier);    if (existingValue == null) {        qualifiers.put(qualifier, count);    } else {        qualifiers.put(qualifier, existingValue + count);    }}
0
 HbaseEventSerializer getSerializer()
{    return serializer;}
0
public long getBatchSize()
{    return batchSize;}
0
private static int getMajorVersion(String version) throws NumberFormatException
{    return Integer.parseInt(version.split("\\.")[0]);}
0
 static boolean hasVersionLessThan2(Logger logger)
{    String version = VersionInfo.getVersion();    try {        if (getMajorVersion(version) < 2) {            return true;        }    } catch (NumberFormatException ex) {            }        return false;}
1
public void configure(Context context)
{    String regex = context.getString(REGEX_CONFIG, REGEX_DEFAULT);    regexIgnoreCase = context.getBoolean(IGNORE_CASE_CONFIG, IGNORE_CASE_DEFAULT);    depositHeaders = context.getBoolean(DEPOSIT_HEADERS_CONFIG, DEPOSIT_HEADERS_DEFAULT);    inputPattern = Pattern.compile(regex, Pattern.DOTALL + (regexIgnoreCase ? Pattern.CASE_INSENSITIVE : 0));    charset = Charset.forName(context.getString(CHARSET_CONFIG, CHARSET_DEFAULT));    String colNameStr = context.getString(COL_NAME_CONFIG, COLUMN_NAME_DEFAULT);    String[] columnNames = colNameStr.split(",");    for (String s : columnNames) {        colNames.add(s.getBytes(charset));    }        rowKeyIndex = context.getInteger(ROW_KEY_INDEX_CONFIG, -1);        if (rowKeyIndex >= 0) {        if (rowKeyIndex >= columnNames.length) {            throw new IllegalArgumentException(ROW_KEY_INDEX_CONFIG + " must be " + "less than num columns " + columnNames.length);        }        if (!ROW_KEY_NAME.equalsIgnoreCase(columnNames[rowKeyIndex])) {            throw new IllegalArgumentException("Column at " + rowKeyIndex + " must be " + ROW_KEY_NAME + " and is " + columnNames[rowKeyIndex]);        }    }}
0
public void configure(ComponentConfiguration conf)
{}
0
public void initialize(Event event, byte[] columnFamily)
{    this.headers = event.getHeaders();    this.payload = event.getBody();    this.cf = columnFamily;}
0
protected byte[] getRowKey(Calendar cal)
{    /* NOTE: This key generation strategy has the following properties:     *      * 1) Within a single JVM, the same row key will never be duplicated.     * 2) Amongst any two JVM's operating at different time periods (according     *    to their respective clocks), the same row key will never be      *    duplicated.     * 3) Amongst any two JVM's operating concurrently (according to their     *    respective clocks), the odds of duplicating a row-key are non-zero     *    but infinitesimal. This would require simultaneous collision in (a)      *    the timestamp (b) the respective nonce and (c) the random string.     *    The string is necessary since (a) and (b) could collide if a fleet     *    of Flume agents are restarted in tandem.     *         *  Row-key uniqueness is important because conflicting row-keys will cause     *  data loss. */    String rowKey = String.format("%s-%s-%s", cal.getTimeInMillis(), randomKey, nonce.getAndIncrement());    return rowKey.getBytes(charset);}
0
protected byte[] getRowKey()
{    return getRowKey(Calendar.getInstance());}
0
public List<Row> getActions() throws FlumeException
{    List<Row> actions = Lists.newArrayList();    byte[] rowKey;    Matcher m = inputPattern.matcher(new String(payload, charset));    if (!m.matches()) {        return Lists.newArrayList();    }    if (m.groupCount() != colNames.size()) {        return Lists.newArrayList();    }    try {        if (rowKeyIndex < 0) {            rowKey = getRowKey();        } else {            rowKey = m.group(rowKeyIndex + 1).getBytes(Charsets.UTF_8);        }        Put put = new Put(rowKey);        for (int i = 0; i < colNames.size(); i++) {            if (i != rowKeyIndex) {                put.add(cf, colNames.get(i), m.group(i + 1).getBytes(Charsets.UTF_8));            }        }        if (depositHeaders) {            for (Map.Entry<String, String> entry : headers.entrySet()) {                put.add(cf, entry.getKey().getBytes(charset), entry.getValue().getBytes(charset));            }        }        actions.add(put);    } catch (Exception e) {        throw new FlumeException("Could not get row key!", e);    }    return actions;}
0
public List<Increment> getIncrements()
{    return Lists.newArrayList();}
0
public void close()
{}
0
public void initialize(byte[] table, byte[] cf)
{    this.table = table;    this.cf = cf;}
0
public List<PutRequest> getActions()
{    List<PutRequest> actions = new ArrayList<PutRequest>();    if (payloadColumn != null) {        byte[] rowKey;        try {            switch(keyType) {                case TS:                    rowKey = SimpleRowKeyGenerator.getTimestampKey(rowPrefix);                    break;                case TSNANO:                    rowKey = SimpleRowKeyGenerator.getNanoTimestampKey(rowPrefix);                    break;                case RANDOM:                    rowKey = SimpleRowKeyGenerator.getRandomKey(rowPrefix);                    break;                default:                    rowKey = SimpleRowKeyGenerator.getUUIDKey(rowPrefix);                    break;            }            PutRequest putRequest = new PutRequest(table, rowKey, cf, payloadColumn, payload);            actions.add(putRequest);        } catch (Exception e) {            throw new FlumeException("Could not get row key!", e);        }    }    return actions;}
0
public List<AtomicIncrementRequest> getIncrements()
{    List<AtomicIncrementRequest> actions = new ArrayList<AtomicIncrementRequest>();    if (incrementColumn != null) {        AtomicIncrementRequest inc = new AtomicIncrementRequest(table, incrementRow, cf, incrementColumn);        actions.add(inc);    }    return actions;}
0
public void cleanUp()
{}
0
public void configure(Context context)
{    String pCol = context.getString("payloadColumn", "pCol");    String iCol = context.getString("incrementColumn", "iCol");    rowPrefix = context.getString("rowPrefix", "default");    String suffix = context.getString("suffix", "uuid");    if (pCol != null && !pCol.isEmpty()) {        if (suffix.equals("timestamp")) {            keyType = KeyType.TS;        } else if (suffix.equals("random")) {            keyType = KeyType.RANDOM;        } else if (suffix.equals("nano")) {            keyType = KeyType.TSNANO;        } else {            keyType = KeyType.UUID;        }        payloadColumn = pCol.getBytes(Charsets.UTF_8);    }    if (iCol != null && !iCol.isEmpty()) {        incrementColumn = iCol.getBytes(Charsets.UTF_8);    }    incrementRow = context.getString("incrementRow", "incRow").getBytes(Charsets.UTF_8);}
0
public void setEvent(Event event)
{    this.payload = event.getBody();}
0
public void configure(ComponentConfiguration conf)
{}
0
public void configure(Context context)
{    rowPrefix = context.getString("rowPrefix", "default");    incrementRow = context.getString("incrementRow", "incRow").getBytes(Charsets.UTF_8);    String suffix = context.getString("suffix", "uuid");    String payloadColumn = context.getString("payloadColumn", "pCol");    String incColumn = context.getString("incrementColumn", "iCol");    if (payloadColumn != null && !payloadColumn.isEmpty()) {        if (suffix.equals("timestamp")) {            keyType = KeyType.TS;        } else if (suffix.equals("random")) {            keyType = KeyType.RANDOM;        } else if (suffix.equals("nano")) {            keyType = KeyType.TSNANO;        } else {            keyType = KeyType.UUID;        }        plCol = payloadColumn.getBytes(Charsets.UTF_8);    }    if (incColumn != null && !incColumn.isEmpty()) {        incCol = incColumn.getBytes(Charsets.UTF_8);    }}
0
public void configure(ComponentConfiguration conf)
{}
0
public void initialize(Event event, byte[] cf)
{    this.payload = event.getBody();    this.cf = cf;}
0
public List<Row> getActions() throws FlumeException
{    List<Row> actions = new LinkedList<Row>();    if (plCol != null) {        byte[] rowKey;        try {            if (keyType == KeyType.TS) {                rowKey = SimpleRowKeyGenerator.getTimestampKey(rowPrefix);            } else if (keyType == KeyType.RANDOM) {                rowKey = SimpleRowKeyGenerator.getRandomKey(rowPrefix);            } else if (keyType == KeyType.TSNANO) {                rowKey = SimpleRowKeyGenerator.getNanoTimestampKey(rowPrefix);            } else {                rowKey = SimpleRowKeyGenerator.getUUIDKey(rowPrefix);            }            Put put = new Put(rowKey);            put.add(cf, plCol, payload);            actions.add(put);        } catch (Exception e) {            throw new FlumeException("Could not get row key!", e);        }    }    return actions;}
0
public List<Increment> getIncrements()
{    List<Increment> incs = new LinkedList<Increment>();    if (incCol != null) {        Increment inc = new Increment(incrementRow);        inc.addColumn(cf, incCol, 1);        incs.add(inc);    }    return incs;}
0
public void close()
{}
0
public static byte[] getUUIDKey(String prefix) throws UnsupportedEncodingException
{    return (prefix + UUID.randomUUID().toString()).getBytes("UTF8");}
0
public static byte[] getRandomKey(String prefix) throws UnsupportedEncodingException
{    return (prefix + String.valueOf(new Random().nextLong())).getBytes("UTF8");}
0
public static byte[] getTimestampKey(String prefix) throws UnsupportedEncodingException
{    return (prefix + String.valueOf(System.currentTimeMillis())).getBytes("UTF8");}
0
public static byte[] getNanoTimestampKey(String prefix) throws UnsupportedEncodingException
{    return (prefix + String.valueOf(System.nanoTime())).getBytes("UTF8");}
0
public void initialize(byte[] table, byte[] cf)
{    this.table = table;    this.cf = cf;}
0
public void setEvent(Event event)
{    this.currentEvent = event;}
0
public List<PutRequest> getActions()
{    return Collections.emptyList();}
0
public List<AtomicIncrementRequest> getIncrements()
{    List<AtomicIncrementRequest> incrs = new ArrayList<AtomicIncrementRequest>();    AtomicIncrementRequest incr = new AtomicIncrementRequest(table, currentEvent.getBody(), cf, column, 1);    incrs.add(incr);    return incrs;}
0
public void cleanUp()
{}
0
public void configure(Context context)
{    column = context.getString("column", "col").getBytes();}
0
public void configure(ComponentConfiguration conf)
{}
0
public void configure(Context context)
{}
0
public void configure(ComponentConfiguration conf)
{}
0
public void close()
{}
0
public void initialize(Event event, byte[] columnFamily)
{    this.event = event;    this.family = columnFamily;}
0
public List<Row> getActions()
{    return Collections.emptyList();}
0
public List<Increment> getIncrements()
{    List<Increment> increments = Lists.newArrayList();    String body = new String(event.getBody(), Charsets.UTF_8);    String[] pieces = body.split(":");    String row = pieces[0];    String qualifier = pieces[1];    Increment inc = new Increment(row.getBytes(Charsets.UTF_8));    inc.addColumn(family, qualifier.getBytes(Charsets.UTF_8), 1L);    increments.add(inc);    return increments;}
0
public void onBatchStart()
{    numBatchesStarted++;}
0
public int getNumBatchesStarted()
{    return numBatchesStarted;}
0
public List<Row> getActions() throws FlumeException
{    if (throwException) {        throw new FlumeException("Exception for testing");    }    return super.getActions();}
0
public static void setUp() throws Exception
{    testUtility.startMiniCluster();    Map<String, String> ctxMap = new HashMap<String, String>();    ctxMap.put("table", tableName);    ctxMap.put("columnFamily", columnFamily);    ctxMap.put("serializer", "org.apache.flume.sink.hbase.SimpleAsyncHbaseEventSerializer");    ctxMap.put("serializer.payloadColumn", plCol);    ctxMap.put("serializer.incrementColumn", inColumn);    ctxMap.put("keep-alive", "0");    ctxMap.put("timeout", "10000");    ctx.putAll(ctxMap);    os = ManagementFactory.getOperatingSystemMXBean();}
0
public static void tearDown() throws Exception
{    testUtility.shutdownMiniCluster();}
0
public void tearDownTest() throws Exception
{    if (deleteTable) {        testUtility.deleteTable(tableName.getBytes());    }}
0
public void testOneEventWithDefaults() throws Exception
{    Map<String, String> ctxMap = new HashMap<String, String>();    ctxMap.put("table", tableName);    ctxMap.put("columnFamily", columnFamily);    ctxMap.put("serializer", "org.apache.flume.sink.hbase.SimpleAsyncHbaseEventSerializer");    ctxMap.put("keep-alive", "0");    ctxMap.put("timeout", "10000");    Context tmpctx = new Context();    tmpctx.putAll(ctxMap);    testUtility.createTable(tableName.getBytes(), columnFamily.getBytes());    deleteTable = true;    AsyncHBaseSink sink = new AsyncHBaseSink(testUtility.getConfiguration());    Configurables.configure(sink, tmpctx);    Channel channel = new MemoryChannel();    Configurables.configure(channel, tmpctx);    sink.setChannel(channel);    sink.start();    Transaction tx = channel.getTransaction();    tx.begin();    Event e = EventBuilder.withBody(Bytes.toBytes(valBase));    channel.put(e);    tx.commit();    tx.close();    Assert.assertFalse(sink.isConfNull());    sink.process();    sink.stop();    HTable table = new HTable(testUtility.getConfiguration(), tableName);    byte[][] results = getResults(table, 1);    byte[] out = results[0];    Assert.assertArrayEquals(e.getBody(), out);    out = results[1];    Assert.assertArrayEquals(Longs.toByteArray(1), out);}
0
public void testOneEvent() throws Exception
{    testUtility.createTable(tableName.getBytes(), columnFamily.getBytes());    deleteTable = true;    AsyncHBaseSink sink = new AsyncHBaseSink(testUtility.getConfiguration());    Configurables.configure(sink, ctx);    Channel channel = new MemoryChannel();    Configurables.configure(channel, ctx);    sink.setChannel(channel);    sink.start();    Transaction tx = channel.getTransaction();    tx.begin();    Event e = EventBuilder.withBody(Bytes.toBytes(valBase));    channel.put(e);    tx.commit();    tx.close();    Assert.assertFalse(sink.isConfNull());    sink.process();    sink.stop();    HTable table = new HTable(testUtility.getConfiguration(), tableName);    byte[][] results = getResults(table, 1);    byte[] out = results[0];    Assert.assertArrayEquals(e.getBody(), out);    out = results[1];    Assert.assertArrayEquals(Longs.toByteArray(1), out);}
0
public void testThreeEvents() throws Exception
{    testUtility.createTable(tableName.getBytes(), columnFamily.getBytes());    deleteTable = true;    AsyncHBaseSink sink = new AsyncHBaseSink(testUtility.getConfiguration());    Configurables.configure(sink, ctx);    Channel channel = new MemoryChannel();    Configurables.configure(channel, ctx);    sink.setChannel(channel);    sink.start();    Transaction tx = channel.getTransaction();    tx.begin();    for (int i = 0; i < 3; i++) {        Event e = EventBuilder.withBody(Bytes.toBytes(valBase + "-" + i));        channel.put(e);    }    tx.commit();    tx.close();    Assert.assertFalse(sink.isConfNull());    sink.process();    sink.stop();    HTable table = new HTable(testUtility.getConfiguration(), tableName);    byte[][] results = getResults(table, 3);    byte[] out;    int found = 0;    for (int i = 0; i < 3; i++) {        for (int j = 0; j < 3; j++) {            if (Arrays.equals(results[j], Bytes.toBytes(valBase + "-" + i))) {                found++;                break;            }        }    }    Assert.assertEquals(3, found);    out = results[3];    Assert.assertArrayEquals(Longs.toByteArray(3), out);}
0
public void testTimeOut() throws Exception
{    testUtility.createTable(tableName.getBytes(), columnFamily.getBytes());    deleteTable = true;    AsyncHBaseSink sink = new AsyncHBaseSink(testUtility.getConfiguration(), true, false);    Configurables.configure(sink, ctx);    Channel channel = new MemoryChannel();    Configurables.configure(channel, ctx);    sink.setChannel(channel);    channel.start();    sink.start();    Transaction tx = channel.getTransaction();    tx.begin();    for (int i = 0; i < 3; i++) {        Event e = EventBuilder.withBody(Bytes.toBytes(valBase + "-" + i));        channel.put(e);    }    tx.commit();    tx.close();    Assert.assertFalse(sink.isConfNull());    sink.process();    Assert.fail();}
0
public void testMultipleBatches() throws Exception
{    testUtility.createTable(tableName.getBytes(), columnFamily.getBytes());    deleteTable = true;    ctx.put("batchSize", "2");    AsyncHBaseSink sink = new AsyncHBaseSink(testUtility.getConfiguration());    Configurables.configure(sink, ctx);        ctx.put("batchSize", "100");    Channel channel = new MemoryChannel();    Configurables.configure(channel, ctx);    sink.setChannel(channel);    sink.start();    Transaction tx = channel.getTransaction();    tx.begin();    for (int i = 0; i < 3; i++) {        Event e = EventBuilder.withBody(Bytes.toBytes(valBase + "-" + i));        channel.put(e);    }    tx.commit();    tx.close();    int count = 0;    Status status = Status.READY;    while (status != Status.BACKOFF) {        count++;        status = sink.process();    }    Assert.assertFalse(sink.isConfNull());    sink.stop();    Assert.assertEquals(2, count);    HTable table = new HTable(testUtility.getConfiguration(), tableName);    byte[][] results = getResults(table, 3);    byte[] out;    int found = 0;    for (int i = 0; i < 3; i++) {        for (int j = 0; j < 3; j++) {            if (Arrays.equals(results[j], Bytes.toBytes(valBase + "-" + i))) {                found++;                break;            }        }    }    Assert.assertEquals(3, found);    out = results[3];    Assert.assertArrayEquals(Longs.toByteArray(3), out);}
0
public void testMultipleBatchesBatchIncrementsWithCoalescing() throws Exception
{    doTestMultipleBatchesBatchIncrements(true);}
0
public void testMultipleBatchesBatchIncrementsNoCoalescing() throws Exception
{    doTestMultipleBatchesBatchIncrements(false);}
0
public void doTestMultipleBatchesBatchIncrements(boolean coalesce) throws Exception
{    testUtility.createTable(tableName.getBytes(), columnFamily.getBytes());    deleteTable = true;    AsyncHBaseSink sink = new AsyncHBaseSink(testUtility.getConfiguration(), false, true);    if (coalesce) {        ctx.put(HBaseSinkConfigurationConstants.CONFIG_COALESCE_INCREMENTS, "true");    }    ctx.put("batchSize", "2");    ctx.put("serializer", IncrementAsyncHBaseSerializer.class.getName());    ctx.put("serializer.column", "test");    Configurables.configure(sink, ctx);        ctx.put("batchSize", "100");        ctx.put("serializer", SimpleAsyncHbaseEventSerializer.class.getName());        ctx.put(HBaseSinkConfigurationConstants.CONFIG_COALESCE_INCREMENTS, "false");    Channel channel = new MemoryChannel();    Configurables.configure(channel, ctx);    sink.setChannel(channel);    sink.start();    Transaction tx = channel.getTransaction();    tx.begin();    for (int i = 0; i < 4; i++) {        for (int j = 0; j < 3; j++) {            Event e = EventBuilder.withBody(Bytes.toBytes(valBase + "-" + i));            channel.put(e);        }    }    tx.commit();    tx.close();    int count = 0;    Status status = Status.READY;    while (status != Status.BACKOFF) {        count++;        status = sink.process();    }    Assert.assertFalse(sink.isConfNull());    sink.stop();    Assert.assertEquals(7, count);    HTable table = new HTable(testUtility.getConfiguration(), tableName);    Scan scan = new Scan();    scan.addColumn(columnFamily.getBytes(), "test".getBytes());    scan.setStartRow(Bytes.toBytes(valBase));    ResultScanner rs = table.getScanner(scan);    int i = 0;    try {        for (Result r = rs.next(); r != null; r = rs.next()) {            byte[] out = r.getValue(columnFamily.getBytes(), "test".getBytes());            Assert.assertArrayEquals(Longs.toByteArray(3), out);            Assert.assertTrue(new String(r.getRow()).startsWith(valBase));            i++;        }    } finally {        rs.close();    }    Assert.assertEquals(4, i);    if (coalesce) {        Assert.assertEquals(8, sink.getTotalCallbacksReceived());    } else {        Assert.assertEquals(12, sink.getTotalCallbacksReceived());    }}
0
public void testWithoutConfigurationObject() throws Exception
{    testUtility.createTable(tableName.getBytes(), columnFamily.getBytes());    deleteTable = true;    ctx.put("batchSize", "2");    ctx.put(HBaseSinkConfigurationConstants.ZK_QUORUM, ZKConfig.getZKQuorumServersString(testUtility.getConfiguration()));    ctx.put(HBaseSinkConfigurationConstants.ZK_ZNODE_PARENT, testUtility.getConfiguration().get(HConstants.ZOOKEEPER_ZNODE_PARENT));    AsyncHBaseSink sink = new AsyncHBaseSink();    Configurables.configure(sink, ctx);        ctx.put(HBaseSinkConfigurationConstants.ZK_QUORUM, null);    ctx.put(HBaseSinkConfigurationConstants.ZK_ZNODE_PARENT, null);    ctx.put("batchSize", "100");    Channel channel = new MemoryChannel();    Configurables.configure(channel, ctx);    sink.setChannel(channel);    sink.start();    Transaction tx = channel.getTransaction();    tx.begin();    for (int i = 0; i < 3; i++) {        Event e = EventBuilder.withBody(Bytes.toBytes(valBase + "-" + i));        channel.put(e);    }    tx.commit();    tx.close();    int count = 0;    Status status = Status.READY;    while (status != Status.BACKOFF) {        count++;        status = sink.process();    }    /*     * Make sure that the configuration was picked up from the context itself     * and not from a configuration object which was created by the sink.     */    Assert.assertTrue(sink.isConfNull());    sink.stop();    Assert.assertEquals(2, count);    HTable table = new HTable(testUtility.getConfiguration(), tableName);    byte[][] results = getResults(table, 3);    byte[] out;    int found = 0;    for (int i = 0; i < 3; i++) {        for (int j = 0; j < 3; j++) {            if (Arrays.equals(results[j], Bytes.toBytes(valBase + "-" + i))) {                found++;                break;            }        }    }    Assert.assertEquals(3, found);    out = results[3];    Assert.assertArrayEquals(Longs.toByteArray(3), out);}
0
public void testMissingTable() throws Exception
{    deleteTable = false;    ctx.put("batchSize", "2");    AsyncHBaseSink sink = new AsyncHBaseSink(testUtility.getConfiguration());    Configurables.configure(sink, ctx);        ctx.put("batchSize", "100");    Channel channel = new MemoryChannel();    Configurables.configure(channel, ctx);    sink.setChannel(channel);    sink.start();    Transaction tx = channel.getTransaction();    tx.begin();    for (int i = 0; i < 3; i++) {        Event e = EventBuilder.withBody(Bytes.toBytes(valBase + "-" + i));        channel.put(e);    }    tx.commit();    tx.close();    sink.process();    Assert.assertFalse(sink.isConfNull());    HTable table = new HTable(testUtility.getConfiguration(), tableName);    byte[][] results = getResults(table, 2);    byte[] out;    int found = 0;    for (int i = 0; i < 2; i++) {        for (int j = 0; j < 2; j++) {            if (Arrays.equals(results[j], Bytes.toBytes(valBase + "-" + i))) {                found++;                break;            }        }    }    Assert.assertEquals(2, found);    out = results[2];    Assert.assertArrayEquals(Longs.toByteArray(2), out);    sink.process();    sink.stop();}
0
private long getOpenFileDescriptorCount()
{    if (os instanceof UnixOperatingSystemMXBean) {        return ((UnixOperatingSystemMXBean) os).getOpenFileDescriptorCount();    } else {        return -1;    }}
0
public void testFDLeakOnShutdown() throws Exception
{    if (getOpenFileDescriptorCount() < 0) {        return;    }    testUtility.createTable(tableName.getBytes(), columnFamily.getBytes());    deleteTable = true;    AsyncHBaseSink sink = new AsyncHBaseSink(testUtility.getConfiguration(), true, false);    ctx.put("maxConsecutiveFails", "1");    Configurables.configure(sink, ctx);    Channel channel = new MemoryChannel();    Configurables.configure(channel, ctx);    sink.setChannel(channel);    channel.start();    sink.start();    Transaction tx = channel.getTransaction();    tx.begin();    for (int i = 0; i < 3; i++) {        Event e = EventBuilder.withBody(Bytes.toBytes(valBase + "-" + i));        channel.put(e);    }    tx.commit();    tx.close();    Assert.assertFalse(sink.isConfNull());    long initialFDCount = getOpenFileDescriptorCount();        for (int i = 0; i < 10; i++) {        try {            sink.process();        } catch (EventDeliveryException ex) {        }    }    long increaseInFD = getOpenFileDescriptorCount() - initialFDCount;    Assert.assertTrue("File Descriptor leak detected. FDs have increased by " + increaseInFD + " from an initial FD count of " + initialFDCount, increaseInFD < 50);}
0
public void testHBaseFailure() throws Exception
{    ctx.put("batchSize", "2");    testUtility.createTable(tableName.getBytes(), columnFamily.getBytes());    deleteTable = false;    AsyncHBaseSink sink = new AsyncHBaseSink(testUtility.getConfiguration());    Configurables.configure(sink, ctx);        ctx.put("batchSize", "100");    Channel channel = new MemoryChannel();    Configurables.configure(channel, ctx);    sink.setChannel(channel);    sink.start();    Transaction tx = channel.getTransaction();    tx.begin();    for (int i = 0; i < 3; i++) {        Event e = EventBuilder.withBody(Bytes.toBytes(valBase + "-" + i));        channel.put(e);    }    tx.commit();    tx.close();    sink.process();    Assert.assertFalse(sink.isConfNull());    HTable table = new HTable(testUtility.getConfiguration(), tableName);    byte[][] results = getResults(table, 2);    byte[] out;    int found = 0;    for (int i = 0; i < 2; i++) {        for (int j = 0; j < 2; j++) {            if (Arrays.equals(results[j], Bytes.toBytes(valBase + "-" + i))) {                found++;                break;            }        }    }    Assert.assertEquals(2, found);    out = results[2];    Assert.assertArrayEquals(Longs.toByteArray(2), out);    testUtility.shutdownMiniCluster();    sink.process();    sink.stop();}
0
private byte[][] getResults(HTable table, int numEvents) throws IOException
{    byte[][] results = new byte[numEvents + 1][];    Scan scan = new Scan();    scan.addColumn(columnFamily.getBytes(), plCol.getBytes());    scan.setStartRow(Bytes.toBytes("default"));    ResultScanner rs = table.getScanner(scan);    byte[] out = null;    int i = 0;    try {        for (Result r = rs.next(); r != null; r = rs.next()) {            out = r.getValue(columnFamily.getBytes(), plCol.getBytes());            if (i >= results.length - 1) {                rs.close();                throw new FlumeException("More results than expected in the table." + "Expected = " + numEvents + ". Found = " + i);            }            results[i++] = out;            System.out.println(out);        }    } finally {        rs.close();    }    Assert.assertEquals(i, results.length - 1);    scan = new Scan();    scan.addColumn(columnFamily.getBytes(), inColumn.getBytes());    scan.setStartRow(Bytes.toBytes("incRow"));    rs = table.getScanner(scan);    out = null;    try {        for (Result r = rs.next(); r != null; r = rs.next()) {            out = r.getValue(columnFamily.getBytes(), inColumn.getBytes());            results[i++] = out;            System.out.println(out);        }    } finally {        rs.close();    }    return results;}
0
public void setUp() throws Exception
{    Map<String, String> ctxMap = new HashMap<>();    ctxMap.put("table", tableName);    ctxMap.put("columnFamily", columnFamily);    ctx = new Context();    ctx.putAll(ctxMap);}
0
public void testAsyncConfigBackwardCompatibility() throws Exception
{        String oldZkQuorumTestValue = "old_zookeeper_quorum_test_value";    String oldZkZnodeParentValue = "old_zookeeper_znode_parent_test_value";    ctx.put(HBaseSinkConfigurationConstants.ZK_QUORUM, oldZkQuorumTestValue);    ctx.put(HBaseSinkConfigurationConstants.ZK_ZNODE_PARENT, oldZkZnodeParentValue);    AsyncHBaseSink sink = new AsyncHBaseSink();    Configurables.configure(sink, ctx);    Assert.assertEquals(oldZkQuorumTestValue, sink.asyncClientConfig.getString(HBaseSinkConfigurationConstants.ASYNC_ZK_QUORUM_KEY));    Assert.assertEquals(oldZkZnodeParentValue, sink.asyncClientConfig.getString(HBaseSinkConfigurationConstants.ASYNC_ZK_BASEPATH_KEY));}
0
public void testAsyncConfigNewStyleOverwriteOldOne() throws Exception
{        String oldZkQuorumTestValue = "old_zookeeper_quorum_test_value";    String oldZkZnodeParentValue = "old_zookeeper_znode_parent_test_value";    ctx.put(HBaseSinkConfigurationConstants.ZK_QUORUM, oldZkQuorumTestValue);    ctx.put(HBaseSinkConfigurationConstants.ZK_ZNODE_PARENT, oldZkZnodeParentValue);    String newZkQuorumTestValue = "new_zookeeper_quorum_test_value";    String newZkZnodeParentValue = "new_zookeeper_znode_parent_test_value";    ctx.put(HBaseSinkConfigurationConstants.ASYNC_PREFIX + HBaseSinkConfigurationConstants.ASYNC_ZK_QUORUM_KEY, newZkQuorumTestValue);    ctx.put(HBaseSinkConfigurationConstants.ASYNC_PREFIX + HBaseSinkConfigurationConstants.ASYNC_ZK_BASEPATH_KEY, newZkZnodeParentValue);    AsyncHBaseSink sink = new AsyncHBaseSink();    Configurables.configure(sink, ctx);    Assert.assertEquals(newZkQuorumTestValue, sink.asyncClientConfig.getString(HBaseSinkConfigurationConstants.ASYNC_ZK_QUORUM_KEY));    Assert.assertEquals(newZkZnodeParentValue, sink.asyncClientConfig.getString(HBaseSinkConfigurationConstants.ASYNC_ZK_BASEPATH_KEY));}
0
public void testAsyncConfigAnyKeyCanBePassed() throws Exception
{    String valueOfANewProp = "vale of the new property";    String keyOfANewProp = "some.key.to.be.passed";    ctx.put(HBaseSinkConfigurationConstants.ASYNC_PREFIX + keyOfANewProp, valueOfANewProp);    AsyncHBaseSink sink = new AsyncHBaseSink();    Configurables.configure(sink, ctx);    Assert.assertEquals(valueOfANewProp, sink.asyncClientConfig.getString(keyOfANewProp));}
0
public static void setUpOnce() throws Exception
{    testUtility.startMiniCluster();}
0
public static void tearDownOnce() throws Exception
{    testUtility.shutdownMiniCluster();}
0
public void setUp() throws IOException
{    conf = new Configuration(testUtility.getConfiguration());    ctx = new Context();    testUtility.createTable(tableName.getBytes(), columnFamily.getBytes());}
0
public void tearDown() throws IOException
{    testUtility.deleteTable(tableName.getBytes());}
0
private void initContextForSimpleHbaseEventSerializer()
{    ctx = new Context();    ctx.put("table", tableName);    ctx.put("columnFamily", columnFamily);    ctx.put("serializer", SimpleHbaseEventSerializer.class.getName());    ctx.put("serializer.payloadColumn", plCol);    ctx.put("serializer.incrementColumn", inColumn);}
0
private void initContextForIncrementHBaseSerializer()
{    ctx = new Context();    ctx.put("table", tableName);    ctx.put("columnFamily", columnFamily);    ctx.put("serializer", IncrementHBaseSerializer.class.getName());}
0
public void testOneEventWithDefaults() throws Exception
{        ctx = new Context();    ctx.put("table", tableName);    ctx.put("columnFamily", columnFamily);    ctx.put("serializer", SimpleHbaseEventSerializer.class.getName());    HBaseSink sink = new HBaseSink(conf);    Configurables.configure(sink, ctx);    Channel channel = new MemoryChannel();    Configurables.configure(channel, new Context());    sink.setChannel(channel);    sink.start();    Transaction tx = channel.getTransaction();    tx.begin();    Event e = EventBuilder.withBody(Bytes.toBytes(valBase));    channel.put(e);    tx.commit();    tx.close();    sink.process();    sink.stop();    HTable table = new HTable(conf, tableName);    byte[][] results = getResults(table, 1);    byte[] out = results[0];    Assert.assertArrayEquals(e.getBody(), out);    out = results[1];    Assert.assertArrayEquals(Longs.toByteArray(1), out);}
0
public void testOneEvent() throws Exception
{    initContextForSimpleHbaseEventSerializer();    HBaseSink sink = new HBaseSink(conf);    Configurables.configure(sink, ctx);    Channel channel = new MemoryChannel();    Configurables.configure(channel, new Context());    sink.setChannel(channel);    sink.start();    Transaction tx = channel.getTransaction();    tx.begin();    Event e = EventBuilder.withBody(Bytes.toBytes(valBase));    channel.put(e);    tx.commit();    tx.close();    sink.process();    sink.stop();    HTable table = new HTable(conf, tableName);    byte[][] results = getResults(table, 1);    byte[] out = results[0];    Assert.assertArrayEquals(e.getBody(), out);    out = results[1];    Assert.assertArrayEquals(Longs.toByteArray(1), out);}
0
public void testThreeEvents() throws Exception
{    initContextForSimpleHbaseEventSerializer();    ctx.put("batchSize", "3");    HBaseSink sink = new HBaseSink(conf);    Configurables.configure(sink, ctx);    Channel channel = new MemoryChannel();    Configurables.configure(channel, new Context());    sink.setChannel(channel);    sink.start();    Transaction tx = channel.getTransaction();    tx.begin();    for (int i = 0; i < 3; i++) {        Event e = EventBuilder.withBody(Bytes.toBytes(valBase + "-" + i));        channel.put(e);    }    tx.commit();    tx.close();    sink.process();    sink.stop();    HTable table = new HTable(conf, tableName);    byte[][] results = getResults(table, 3);    byte[] out;    int found = 0;    for (int i = 0; i < 3; i++) {        for (int j = 0; j < 3; j++) {            if (Arrays.equals(results[j], Bytes.toBytes(valBase + "-" + i))) {                found++;                break;            }        }    }    Assert.assertEquals(3, found);    out = results[3];    Assert.assertArrayEquals(Longs.toByteArray(3), out);}
0
public void testMultipleBatches() throws Exception
{    initContextForSimpleHbaseEventSerializer();    ctx.put("batchSize", "2");    HBaseSink sink = new HBaseSink(conf);    Configurables.configure(sink, ctx);        ctx.put("batchSize", "100");    Channel channel = new MemoryChannel();    Configurables.configure(channel, new Context());    sink.setChannel(channel);    sink.start();    Transaction tx = channel.getTransaction();    tx.begin();    for (int i = 0; i < 3; i++) {        Event e = EventBuilder.withBody(Bytes.toBytes(valBase + "-" + i));        channel.put(e);    }    tx.commit();    tx.close();    int count = 0;    while (sink.process() != Status.BACKOFF) {        count++;    }    sink.stop();    Assert.assertEquals(2, count);    HTable table = new HTable(conf, tableName);    byte[][] results = getResults(table, 3);    byte[] out;    int found = 0;    for (int i = 0; i < 3; i++) {        for (int j = 0; j < 3; j++) {            if (Arrays.equals(results[j], Bytes.toBytes(valBase + "-" + i))) {                found++;                break;            }        }    }    Assert.assertEquals(3, found);    out = results[3];    Assert.assertArrayEquals(Longs.toByteArray(3), out);}
0
public void testMissingTable() throws Exception
{        initContextForSimpleHbaseEventSerializer();            testUtility.deleteTable(tableName.getBytes());    ctx.put("batchSize", "2");    HBaseSink sink = new HBaseSink(conf);    Configurables.configure(sink, ctx);        ctx.put("batchSize", "100");    Channel channel = new MemoryChannel();    Configurables.configure(channel, new Context());    sink.setChannel(channel);        Transaction tx = channel.getTransaction();    tx.begin();    for (int i = 0; i < 3; i++) {        Event e = EventBuilder.withBody(Bytes.toBytes(valBase + "-" + i));        channel.put(e);    }    tx.commit();    tx.close();        try {                        sink.start();                        sink.process();                sink.stop();    } finally {                testUtility.createTable(tableName.getBytes(), columnFamily.getBytes());    }        Assert.fail();    HTable table = new HTable(conf, tableName);    byte[][] results = getResults(table, 2);    byte[] out;    int found = 0;    for (int i = 0; i < 2; i++) {        for (int j = 0; j < 2; j++) {            if (Arrays.equals(results[j], Bytes.toBytes(valBase + "-" + i))) {                found++;                break;            }        }    }    Assert.assertEquals(2, found);    out = results[2];    Assert.assertArrayEquals(Longs.toByteArray(2), out);    sink.process();}
1
public void testHBaseFailure() throws Exception
{    initContextForSimpleHbaseEventSerializer();    ctx.put("batchSize", "2");    HBaseSink sink = new HBaseSink(conf);    Configurables.configure(sink, ctx);        ctx.put("batchSize", "100");    Channel channel = new MemoryChannel();    Configurables.configure(channel, new Context());    sink.setChannel(channel);    sink.start();    Transaction tx = channel.getTransaction();    tx.begin();    for (int i = 0; i < 3; i++) {        Event e = EventBuilder.withBody(Bytes.toBytes(valBase + "-" + i));        channel.put(e);    }    tx.commit();    tx.close();    sink.process();    HTable table = new HTable(conf, tableName);    byte[][] results = getResults(table, 2);    byte[] out;    int found = 0;    for (int i = 0; i < 2; i++) {        for (int j = 0; j < 2; j++) {            if (Arrays.equals(results[j], Bytes.toBytes(valBase + "-" + i))) {                found++;                break;            }        }    }    Assert.assertEquals(2, found);    out = results[2];    Assert.assertArrayEquals(Longs.toByteArray(2), out);    testUtility.shutdownMiniCluster();    sink.process();    sink.stop();}
0
private byte[][] getResults(HTable table, int numEvents) throws IOException
{    byte[][] results = new byte[numEvents + 1][];    Scan scan = new Scan();    scan.addColumn(columnFamily.getBytes(), plCol.getBytes());    scan.setStartRow(Bytes.toBytes("default"));    ResultScanner rs = table.getScanner(scan);    byte[] out = null;    int i = 0;    try {        for (Result r = rs.next(); r != null; r = rs.next()) {            out = r.getValue(columnFamily.getBytes(), plCol.getBytes());            if (i >= results.length - 1) {                rs.close();                throw new FlumeException("More results than expected in the table." + "Expected = " + numEvents + ". Found = " + i);            }            results[i++] = out;            System.out.println(out);        }    } finally {        rs.close();    }    Assert.assertEquals(i, results.length - 1);    scan = new Scan();    scan.addColumn(columnFamily.getBytes(), inColumn.getBytes());    scan.setStartRow(Bytes.toBytes("incRow"));    rs = table.getScanner(scan);    out = null;    try {        for (Result r = rs.next(); r != null; r = rs.next()) {            out = r.getValue(columnFamily.getBytes(), inColumn.getBytes());            results[i++] = out;            System.out.println(out);        }    } finally {        rs.close();    }    return results;}
0
public void testTransactionStateOnChannelException() throws Exception
{    initContextForSimpleHbaseEventSerializer();    ctx.put("batchSize", "1");    HBaseSink sink = new HBaseSink(conf);    Configurables.configure(sink, ctx);        Channel channel = spy(new MemoryChannel());    Configurables.configure(channel, new Context());    sink.setChannel(channel);    sink.start();    Transaction tx = channel.getTransaction();    tx.begin();    Event e = EventBuilder.withBody(Bytes.toBytes(valBase + "-" + 0));    channel.put(e);    tx.commit();    tx.close();    doThrow(new ChannelException("Mock Exception")).when(channel).take();    try {        sink.process();        Assert.fail("take() method should throw exception");    } catch (ChannelException ex) {        Assert.assertEquals("Mock Exception", ex.getMessage());    }    doReturn(e).when(channel).take();    sink.process();    sink.stop();    HTable table = new HTable(conf, tableName);    byte[][] results = getResults(table, 1);    byte[] out = results[0];    Assert.assertArrayEquals(e.getBody(), out);    out = results[1];    Assert.assertArrayEquals(Longs.toByteArray(1), out);}
0
public void testTransactionStateOnSerializationException() throws Exception
{    initContextForSimpleHbaseEventSerializer();    ctx.put("batchSize", "1");    ctx.put(HBaseSinkConfigurationConstants.CONFIG_SERIALIZER, "org.apache.flume.sink.hbase.MockSimpleHbaseEventSerializer");    HBaseSink sink = new HBaseSink(conf);    Configurables.configure(sink, ctx);        ctx.put("batchSize", "100");    Channel channel = new MemoryChannel();    Configurables.configure(channel, new Context());    sink.setChannel(channel);    sink.start();    Transaction tx = channel.getTransaction();    tx.begin();    Event e = EventBuilder.withBody(Bytes.toBytes(valBase + "-" + 0));    channel.put(e);    tx.commit();    tx.close();    try {        MockSimpleHbaseEventSerializer.throwException = true;        sink.process();        Assert.fail("FlumeException expected from serilazer");    } catch (FlumeException ex) {        Assert.assertEquals("Exception for testing", ex.getMessage());    }    MockSimpleHbaseEventSerializer.throwException = false;    sink.process();    sink.stop();    HTable table = new HTable(conf, tableName);    byte[][] results = getResults(table, 1);    byte[] out = results[0];    Assert.assertArrayEquals(e.getBody(), out);    out = results[1];    Assert.assertArrayEquals(Longs.toByteArray(1), out);}
0
public void testWithoutConfigurationObject() throws Exception
{    initContextForSimpleHbaseEventSerializer();    Context tmpContext = new Context(ctx.getParameters());    tmpContext.put("batchSize", "2");    tmpContext.put(HBaseSinkConfigurationConstants.ZK_QUORUM, ZKConfig.getZKQuorumServersString(conf));    System.out.print(ctx.getString(HBaseSinkConfigurationConstants.ZK_QUORUM));    tmpContext.put(HBaseSinkConfigurationConstants.ZK_ZNODE_PARENT, conf.get(HConstants.ZOOKEEPER_ZNODE_PARENT, HConstants.DEFAULT_ZOOKEEPER_ZNODE_PARENT));    HBaseSink sink = new HBaseSink();    Configurables.configure(sink, tmpContext);    Channel channel = new MemoryChannel();    Configurables.configure(channel, ctx);    sink.setChannel(channel);    sink.start();    Transaction tx = channel.getTransaction();    tx.begin();    for (int i = 0; i < 3; i++) {        Event e = EventBuilder.withBody(Bytes.toBytes(valBase + "-" + i));        channel.put(e);    }    tx.commit();    tx.close();    Status status = Status.READY;    while (status != Status.BACKOFF) {        status = sink.process();    }    sink.stop();    HTable table = new HTable(conf, tableName);    byte[][] results = getResults(table, 3);    byte[] out;    int found = 0;    for (int i = 0; i < 3; i++) {        for (int j = 0; j < 3; j++) {            if (Arrays.equals(results[j], Bytes.toBytes(valBase + "-" + i))) {                found++;                break;            }        }    }    Assert.assertEquals(3, found);    out = results[3];    Assert.assertArrayEquals(Longs.toByteArray(3), out);}
0
public void testZKQuorum() throws Exception
{    initContextForSimpleHbaseEventSerializer();    Context tmpContext = new Context(ctx.getParameters());    String zkQuorum = "zk1.flume.apache.org:3342, zk2.flume.apache.org:3342, " + "zk3.flume.apache.org:3342";    tmpContext.put("batchSize", "2");    tmpContext.put(HBaseSinkConfigurationConstants.ZK_QUORUM, zkQuorum);    tmpContext.put(HBaseSinkConfigurationConstants.ZK_ZNODE_PARENT, conf.get(HConstants.ZOOKEEPER_ZNODE_PARENT, HConstants.DEFAULT_ZOOKEEPER_ZNODE_PARENT));    HBaseSink sink = new HBaseSink();    Configurables.configure(sink, tmpContext);    Assert.assertEquals("zk1.flume.apache.org,zk2.flume.apache.org," + "zk3.flume.apache.org", sink.getConfig().get(HConstants.ZOOKEEPER_QUORUM));    Assert.assertEquals(String.valueOf(3342), sink.getConfig().get(HConstants.ZOOKEEPER_CLIENT_PORT));}
0
public void testZKQuorumIncorrectPorts() throws Exception
{    initContextForSimpleHbaseEventSerializer();    Context tmpContext = new Context(ctx.getParameters());    String zkQuorum = "zk1.flume.apache.org:3345, zk2.flume.apache.org:3342, " + "zk3.flume.apache.org:3342";    tmpContext.put("batchSize", "2");    tmpContext.put(HBaseSinkConfigurationConstants.ZK_QUORUM, zkQuorum);    tmpContext.put(HBaseSinkConfigurationConstants.ZK_ZNODE_PARENT, conf.get(HConstants.ZOOKEEPER_ZNODE_PARENT, HConstants.DEFAULT_ZOOKEEPER_ZNODE_PARENT));    HBaseSink sink = new HBaseSink();    Configurables.configure(sink, tmpContext);    Assert.fail();}
0
public void testCoalesce() throws EventDeliveryException
{    initContextForIncrementHBaseSerializer();    ctx.put("batchSize", "100");    ctx.put(HBaseSinkConfigurationConstants.CONFIG_COALESCE_INCREMENTS, String.valueOf(true));    final Map<String, Long> expectedCounts = Maps.newHashMap();    expectedCounts.put("r1:c1", 10L);    expectedCounts.put("r1:c2", 20L);    expectedCounts.put("r2:c1", 7L);    expectedCounts.put("r2:c3", 63L);    HBaseSink.DebugIncrementsCallback cb = new CoalesceValidator(expectedCounts);    HBaseSink sink = new HBaseSink(testUtility.getConfiguration(), cb);    Configurables.configure(sink, ctx);    Channel channel = createAndConfigureMemoryChannel(sink);    List<Event> events = Lists.newLinkedList();    generateEvents(events, expectedCounts);    putEvents(channel, events);    sink.start();        sink.process();    sink.stop();}
0
public void negativeTestCoalesce() throws EventDeliveryException
{    initContextForIncrementHBaseSerializer();    ctx.put("batchSize", "10");    final Map<String, Long> expectedCounts = Maps.newHashMap();    expectedCounts.put("r1:c1", 10L);    HBaseSink.DebugIncrementsCallback cb = new CoalesceValidator(expectedCounts);    HBaseSink sink = new HBaseSink(testUtility.getConfiguration(), cb);    Configurables.configure(sink, ctx);    Channel channel = createAndConfigureMemoryChannel(sink);    List<Event> events = Lists.newLinkedList();    generateEvents(events, expectedCounts);    putEvents(channel, events);    sink.start();        sink.process();    sink.stop();}
0
public void testBatchAware() throws EventDeliveryException
{        initContextForIncrementHBaseSerializer();    HBaseSink sink = new HBaseSink(testUtility.getConfiguration());    Configurables.configure(sink, ctx);    Channel channel = createAndConfigureMemoryChannel(sink);    sink.start();    int batchCount = 3;    for (int i = 0; i < batchCount; i++) {        sink.process();    }    sink.stop();    Assert.assertEquals(batchCount, ((IncrementHBaseSerializer) sink.getSerializer()).getNumBatchesStarted());}
1
public void onAfterCoalesce(Iterable<Increment> increments)
{    for (Increment inc : increments) {        byte[] row = inc.getRow();        Map<byte[], NavigableMap<byte[], Long>> families = null;        try {            families = (Map<byte[], NavigableMap<byte[], Long>>) refGetFamilyMap.invoke(inc);        } catch (Exception e) {            Throwables.propagate(e);        }        for (byte[] family : families.keySet()) {            NavigableMap<byte[], Long> qualifiers = families.get(family);            for (Map.Entry<byte[], Long> entry : qualifiers.entrySet()) {                byte[] qualifier = entry.getKey();                Long count = entry.getValue();                StringBuilder b = new StringBuilder(20);                b.append(new String(row, Charsets.UTF_8));                b.append(':');                b.append(new String(qualifier, Charsets.UTF_8));                String key = b.toString();                Assert.assertEquals("Expected counts don't match observed for " + key, expectedCounts.get(key), count);            }        }    }}
0
private void generateEvents(List<Event> events, Map<String, Long> counts)
{    for (String key : counts.keySet()) {        long count = counts.get(key);        for (long i = 0; i < count; i++) {            events.add(EventBuilder.withBody(key, Charsets.UTF_8));        }    }}
0
private Channel createAndConfigureMemoryChannel(HBaseSink sink)
{    Channel channel = new MemoryChannel();    Context channelCtx = new Context();    channelCtx.put("capacity", String.valueOf(1000L));    channelCtx.put("transactionCapacity", String.valueOf(1000L));    Configurables.configure(channel, channelCtx);    sink.setChannel(channel);    channel.start();    return channel;}
0
private void putEvents(Channel channel, Iterable<Event> events)
{    Transaction tx = channel.getTransaction();    tx.begin();    for (Event event : events) {        channel.put(event);    }    tx.commit();    tx.close();}
0
public void setUp()
{    sinkFactory = new DefaultSinkFactory();}
0
private void verifySinkCreation(String name, String type, Class<?> typeClass) throws FlumeException
{    Sink sink = sinkFactory.create(name, type);    Assert.assertNotNull(sink);    Assert.assertTrue(typeClass.isInstance(sink));}
0
public void testSinkCreation()
{    verifySinkCreation("hbase-sink", "hbase", HBaseSink.class);    verifySinkCreation("asynchbase-sink", "asynchbase", AsyncHBaseSink.class);}
0
public void testDefaultBehavior() throws Exception
{    RegexHbaseEventSerializer s = new RegexHbaseEventSerializer();    Context context = new Context();    s.configure(context);    String logMsg = "The sky is falling!";    Event e = EventBuilder.withBody(Bytes.toBytes(logMsg));    s.initialize(e, "CF".getBytes());    List<Row> actions = s.getActions();    assertTrue(actions.size() == 1);    assertTrue(actions.get(0) instanceof Put);    Put put = (Put) actions.get(0);    assertTrue(put.getFamilyMap().containsKey(s.cf));    List<KeyValue> kvPairs = put.getFamilyMap().get(s.cf);    assertTrue(kvPairs.size() == 1);    Map<String, String> resultMap = Maps.newHashMap();    for (KeyValue kv : kvPairs) {        resultMap.put(new String(kv.getQualifier()), new String(kv.getValue()));    }    assertTrue(resultMap.containsKey(RegexHbaseEventSerializer.COLUMN_NAME_DEFAULT));    assertEquals("The sky is falling!", resultMap.get(RegexHbaseEventSerializer.COLUMN_NAME_DEFAULT));}
0
public void testRowIndexKey() throws Exception
{    RegexHbaseEventSerializer s = new RegexHbaseEventSerializer();    Context context = new Context();    context.put(RegexHbaseEventSerializer.REGEX_CONFIG, "^([^\t]+)\t([^\t]+)\t" + "([^\t]+)$");    context.put(RegexHbaseEventSerializer.COL_NAME_CONFIG, "col1,col2,ROW_KEY");    context.put("rowKeyIndex", "2");    s.configure(context);    String body = "val1\tval2\trow1";    Event e = EventBuilder.withBody(Bytes.toBytes(body));    s.initialize(e, "CF".getBytes());    List<Row> actions = s.getActions();    Put put = (Put) actions.get(0);    List<KeyValue> kvPairs = put.getFamilyMap().get(s.cf);    assertTrue(kvPairs.size() == 2);    Map<String, String> resultMap = Maps.newHashMap();    for (KeyValue kv : kvPairs) {        resultMap.put(new String(kv.getQualifier()), new String(kv.getValue()));    }    assertEquals("val1", resultMap.get("col1"));    assertEquals("val2", resultMap.get("col2"));    assertEquals("row1", Bytes.toString(put.getRow()));}
0
public void testApacheRegex() throws Exception
{    RegexHbaseEventSerializer s = new RegexHbaseEventSerializer();    Context context = new Context();    context.put(RegexHbaseEventSerializer.REGEX_CONFIG, "([^ ]*) ([^ ]*) ([^ ]*) (-|\\[[^\\]]*\\]) \"([^ ]+) ([^ ]+)" + " ([^\"]+)\" (-|[0-9]*) (-|[0-9]*)(?: ([^ \"]*|\"[^\"]*\")" + " ([^ \"]*|\"[^\"]*\"))?");    context.put(RegexHbaseEventSerializer.COL_NAME_CONFIG, "host,identity,user,time,method,request,protocol,status,size," + "referer,agent");    s.configure(context);    String logMsg = "33.22.11.00 - - [20/May/2011:07:01:19 +0000] " + "\"GET /wp-admin/css/install.css HTTP/1.0\" 200 813 " + "\"http://www.cloudera.com/wp-admin/install.php\" \"Mozilla/5.0 (comp" + "atible; Yahoo! Slurp; http://help.yahoo.com/help/us/ysearch/slurp)\"";    Event e = EventBuilder.withBody(Bytes.toBytes(logMsg));    s.initialize(e, "CF".getBytes());    List<Row> actions = s.getActions();    assertEquals(1, s.getActions().size());    assertTrue(actions.get(0) instanceof Put);    Put put = (Put) actions.get(0);    assertTrue(put.getFamilyMap().containsKey(s.cf));    List<KeyValue> kvPairs = put.getFamilyMap().get(s.cf);    assertTrue(kvPairs.size() == 11);    Map<String, String> resultMap = Maps.newHashMap();    for (KeyValue kv : kvPairs) {        resultMap.put(new String(kv.getQualifier()), new String(kv.getValue()));    }    assertEquals("33.22.11.00", resultMap.get("host"));    assertEquals("-", resultMap.get("identity"));    assertEquals("-", resultMap.get("user"));    assertEquals("[20/May/2011:07:01:19 +0000]", resultMap.get("time"));    assertEquals("GET", resultMap.get("method"));    assertEquals("/wp-admin/css/install.css", resultMap.get("request"));    assertEquals("HTTP/1.0", resultMap.get("protocol"));    assertEquals("200", resultMap.get("status"));    assertEquals("813", resultMap.get("size"));    assertEquals("\"http://www.cloudera.com/wp-admin/install.php\"", resultMap.get("referer"));    assertEquals("\"Mozilla/5.0 (compatible; Yahoo! Slurp; " + "http://help.yahoo.com/help/us/ysearch/slurp)\"", resultMap.get("agent"));    List<Increment> increments = s.getIncrements();    assertEquals(0, increments.size());}
0
public void testRowKeyGeneration()
{    Context context = new Context();    RegexHbaseEventSerializer s1 = new RegexHbaseEventSerializer();    s1.configure(context);    RegexHbaseEventSerializer s2 = new RegexHbaseEventSerializer();    s2.configure(context);        RegexHbaseEventSerializer.nonce.set(0);    String randomString = RegexHbaseEventSerializer.randomKey;    Event e1 = EventBuilder.withBody(Bytes.toBytes("body"));    Event e2 = EventBuilder.withBody(Bytes.toBytes("body"));    Event e3 = EventBuilder.withBody(Bytes.toBytes("body"));    Calendar cal = mock(Calendar.class);    when(cal.getTimeInMillis()).thenReturn(1L);    s1.initialize(e1, "CF".getBytes());    String rk1 = new String(s1.getRowKey(cal));    assertEquals("1-" + randomString + "-0", rk1);    when(cal.getTimeInMillis()).thenReturn(10L);    s1.initialize(e2, "CF".getBytes());    String rk2 = new String(s1.getRowKey(cal));    assertEquals("10-" + randomString + "-1", rk2);    when(cal.getTimeInMillis()).thenReturn(100L);    s2.initialize(e3, "CF".getBytes());    String rk3 = new String(s2.getRowKey(cal));    assertEquals("100-" + randomString + "-2", rk3);}
0
public void testDepositHeaders() throws Exception
{    Charset charset = Charset.forName("KOI8-R");    RegexHbaseEventSerializer s = new RegexHbaseEventSerializer();    Context context = new Context();    context.put(RegexHbaseEventSerializer.DEPOSIT_HEADERS_CONFIG, "true");    context.put(RegexHbaseEventSerializer.CHARSET_CONFIG, charset.toString());    s.configure(context);    String body = "body";    Map<String, String> headers = Maps.newHashMap();    headers.put("header1", "value1");    headers.put("2", "2");    Event e = EventBuilder.withBody(Bytes.toBytes(body), headers);    s.initialize(e, "CF".getBytes());    List<Row> actions = s.getActions();    assertEquals(1, s.getActions().size());    assertTrue(actions.get(0) instanceof Put);    Put put = (Put) actions.get(0);    assertTrue(put.getFamilyMap().containsKey(s.cf));    List<KeyValue> kvPairs = put.getFamilyMap().get(s.cf);    assertTrue(kvPairs.size() == 3);    Map<String, byte[]> resultMap = Maps.newHashMap();    for (KeyValue kv : kvPairs) {        resultMap.put(new String(kv.getQualifier(), charset), kv.getValue());    }    assertEquals(body, new String(resultMap.get(RegexHbaseEventSerializer.COLUMN_NAME_DEFAULT), charset));    assertEquals("value1", new String(resultMap.get("header1"), charset));    assertArrayEquals("2".getBytes(charset), resultMap.get("2"));    assertEquals("2".length(), resultMap.get("2").length);    List<Increment> increments = s.getIncrements();    assertEquals(0, increments.size());}
0
public void start()
{    Preconditions.checkArgument(table == null, "Please call stop " + "before calling start on an old instance.");    try {        privilegedExecutor = FlumeAuthenticationUtil.getAuthenticator(kerberosPrincipal, kerberosKeytab);    } catch (Exception ex) {        sinkCounter.incrementConnectionFailedCount();        throw new FlumeException("Failed to login to HBase using " + "provided credentials.", ex);    }    try {        conn = privilegedExecutor.execute((PrivilegedExceptionAction<Connection>) () -> {            conn = ConnectionFactory.createConnection(config);            return conn;        });                        table = conn.getBufferedMutator(TableName.valueOf(tableName));    } catch (Exception e) {        sinkCounter.incrementConnectionFailedCount();                throw new FlumeException("Could not load table, " + tableName + " from HBase", e);    }    try {        if (!privilegedExecutor.execute((PrivilegedExceptionAction<Boolean>) () -> {            Table t = null;            try {                t = conn.getTable(TableName.valueOf(tableName));                return t.getTableDescriptor().hasFamily(columnFamily);            } finally {                if (t != null) {                    t.close();                }            }        })) {            throw new IOException("Table " + tableName + " has no such column family " + Bytes.toString(columnFamily));        }    } catch (Exception e) {                        sinkCounter.incrementConnectionFailedCount();        throw new FlumeException("Error getting column family from HBase." + "Please verify that the table " + tableName + " and Column Family, " + Bytes.toString(columnFamily) + " exists in HBase, and the" + " current user has permissions to access that table.", e);    }    super.start();    sinkCounter.incrementConnectionCreatedCount();    sinkCounter.start();}
1
public void stop()
{    try {        if (table != null) {            table.close();        }        table = null;    } catch (IOException e) {        throw new FlumeException("Error closing table.", e);    }    try {        if (conn != null) {            conn.close();        }        conn = null;    } catch (IOException e) {        throw new FlumeException("Error closing connection.", e);    }    sinkCounter.incrementConnectionClosedCount();    sinkCounter.stop();}
0
public void configure(Context context)
{    if (!this.hasVersionAtLeast2()) {        throw new ConfigurationException("HBase major version number must be at least 2 for hbase2sink");    }    tableName = context.getString(HBase2SinkConfigurationConstants.CONFIG_TABLE);    String cf = context.getString(HBase2SinkConfigurationConstants.CONFIG_COLUMN_FAMILY);    batchSize = context.getLong(HBase2SinkConfigurationConstants.CONFIG_BATCHSIZE, 100L);    Context serializerContext = new Context();        String eventSerializerType = context.getString(HBase2SinkConfigurationConstants.CONFIG_SERIALIZER);    Preconditions.checkNotNull(tableName, "Table name cannot be empty, please specify in configuration file");    Preconditions.checkNotNull(cf, "Column family cannot be empty, please specify in configuration file");        if (eventSerializerType == null || eventSerializerType.isEmpty()) {        eventSerializerType = "org.apache.flume.sink.hbase2.SimpleHBase2EventSerializer";            }    serializerContext.putAll(context.getSubProperties(HBase2SinkConfigurationConstants.CONFIG_SERIALIZER_PREFIX));    columnFamily = cf.getBytes(Charsets.UTF_8);    try {        Class<? extends HBase2EventSerializer> clazz = (Class<? extends HBase2EventSerializer>) Class.forName(eventSerializerType);        serializer = clazz.newInstance();        serializer.configure(serializerContext);    } catch (Exception e) {                Throwables.propagate(e);    }    kerberosKeytab = context.getString(HBase2SinkConfigurationConstants.CONFIG_KEYTAB);    kerberosPrincipal = context.getString(HBase2SinkConfigurationConstants.CONFIG_PRINCIPAL);    enableWal = context.getBoolean(HBase2SinkConfigurationConstants.CONFIG_ENABLE_WAL, HBase2SinkConfigurationConstants.DEFAULT_ENABLE_WAL);        if (!enableWal) {            }    batchIncrements = context.getBoolean(HBase2SinkConfigurationConstants.CONFIG_COALESCE_INCREMENTS, HBase2SinkConfigurationConstants.DEFAULT_COALESCE_INCREMENTS);    if (batchIncrements) {            }    String zkQuorum = context.getString(HBase2SinkConfigurationConstants.ZK_QUORUM);    Integer port = null;    /*     * HBase allows multiple nodes in the quorum, but all need to use the     * same client port. So get the nodes in host:port format,     * and ignore the ports for all nodes except the first one. If no port is     * specified, use default.     */    if (zkQuorum != null && !zkQuorum.isEmpty()) {        StringBuilder zkBuilder = new StringBuilder();                String[] zkHosts = zkQuorum.split(",");        int length = zkHosts.length;        for (int i = 0; i < length; i++) {            String[] zkHostAndPort = zkHosts[i].split(":");            zkBuilder.append(zkHostAndPort[0].trim());            if (i != length - 1) {                zkBuilder.append(",");            } else {                zkQuorum = zkBuilder.toString();            }            if (zkHostAndPort[1] == null) {                throw new FlumeException("Expected client port for the ZK node!");            }            if (port == null) {                port = Integer.parseInt(zkHostAndPort[1].trim());            } else if (!port.equals(Integer.parseInt(zkHostAndPort[1].trim()))) {                throw new FlumeException("All Zookeeper nodes in the quorum must " + "use the same client port.");            }        }        if (port == null) {            port = HConstants.DEFAULT_ZOOKEPER_CLIENT_PORT;        }        this.config.set(HConstants.ZOOKEEPER_QUORUM, zkQuorum);        this.config.setInt(HConstants.ZOOKEEPER_CLIENT_PORT, port);    }    String hbaseZnode = context.getString(HBase2SinkConfigurationConstants.ZK_ZNODE_PARENT);    if (hbaseZnode != null && !hbaseZnode.isEmpty()) {        this.config.set(HConstants.ZOOKEEPER_ZNODE_PARENT, hbaseZnode);    }    sinkCounter = new SinkCounter(this.getName());}
1
public Configuration getConfig()
{    return config;}
0
public Status process() throws EventDeliveryException
{    Status status = Status.READY;    Channel channel = getChannel();    Transaction txn = channel.getTransaction();    List<Row> actions = new LinkedList<>();    List<Increment> incs = new LinkedList<>();    try {        txn.begin();        if (serializer instanceof BatchAware) {            ((BatchAware) serializer).onBatchStart();        }        long i = 0;        for (; i < batchSize; i++) {            Event event = channel.take();            if (event == null) {                if (i == 0) {                    status = Status.BACKOFF;                    sinkCounter.incrementBatchEmptyCount();                } else {                    sinkCounter.incrementBatchUnderflowCount();                }                break;            } else {                serializer.initialize(event, columnFamily);                actions.addAll(serializer.getActions());                incs.addAll(serializer.getIncrements());            }        }        if (i == batchSize) {            sinkCounter.incrementBatchCompleteCount();        }        sinkCounter.addToEventDrainAttemptCount(i);        putEventsAndCommit(actions, incs, txn);    } catch (Throwable e) {        try {            txn.rollback();        } catch (Exception e2) {                    }                sinkCounter.incrementEventWriteOrChannelFail(e);        if (e instanceof Error || e instanceof RuntimeException) {                        Throwables.propagate(e);        } else {                        throw new EventDeliveryException("Failed to commit transaction." + "Transaction rolled back.", e);        }    } finally {        txn.close();    }    return status;}
1
private void putEventsAndCommit(final List<Row> actions, final List<Increment> incs, Transaction txn) throws Exception
{    privilegedExecutor.execute((PrivilegedExceptionAction<Void>) () -> {        final List<Mutation> mutations = new ArrayList<>(actions.size());        for (Row r : actions) {            if (r instanceof Put) {                ((Put) r).setDurability(enableWal ? Durability.USE_DEFAULT : Durability.SKIP_WAL);            }                        if (r instanceof Increment) {                ((Increment) r).setDurability(enableWal ? Durability.USE_DEFAULT : Durability.SKIP_WAL);            }            if (r instanceof Mutation) {                mutations.add((Mutation) r);            } else {                            }        }        table.mutate(mutations);        table.flush();        return null;    });    privilegedExecutor.execute((PrivilegedExceptionAction<Void>) () -> {        List<Increment> processedIncrements;        if (batchIncrements) {            processedIncrements = coalesceIncrements(incs);        } else {            processedIncrements = incs;        }                if (debugIncrCallback != null) {            debugIncrCallback.onAfterCoalesce(processedIncrements);        }        for (final Increment i : processedIncrements) {            i.setDurability(enableWal ? Durability.USE_DEFAULT : Durability.SKIP_WAL);            table.mutate(i);        }        table.flush();        return null;    });    txn.commit();    sinkCounter.addToEventDrainSuccessCount(actions.size());}
1
private Map<byte[], NavigableMap<byte[], Long>> getFamilyMap(Increment inc)
{    Preconditions.checkNotNull(inc, "Increment required");    return inc.getFamilyMapOfLongs();}
0
private List<Increment> coalesceIncrements(Iterable<Increment> incs)
{    Preconditions.checkNotNull(incs, "List of Increments must not be null");            Map<byte[], Map<byte[], NavigableMap<byte[], Long>>> counters = Maps.newTreeMap(Bytes.BYTES_COMPARATOR);    for (Increment inc : incs) {        byte[] row = inc.getRow();        Map<byte[], NavigableMap<byte[], Long>> families = getFamilyMap(inc);        for (Map.Entry<byte[], NavigableMap<byte[], Long>> familyEntry : families.entrySet()) {            byte[] family = familyEntry.getKey();            NavigableMap<byte[], Long> qualifiers = familyEntry.getValue();            for (Map.Entry<byte[], Long> qualifierEntry : qualifiers.entrySet()) {                byte[] qualifier = qualifierEntry.getKey();                Long count = qualifierEntry.getValue();                incrementCounter(counters, row, family, qualifier, count);            }        }    }        List<Increment> coalesced = Lists.newLinkedList();    for (Map.Entry<byte[], Map<byte[], NavigableMap<byte[], Long>>> rowEntry : counters.entrySet()) {        byte[] row = rowEntry.getKey();        Map<byte[], NavigableMap<byte[], Long>> families = rowEntry.getValue();        Increment inc = new Increment(row);        for (Map.Entry<byte[], NavigableMap<byte[], Long>> familyEntry : families.entrySet()) {            byte[] family = familyEntry.getKey();            NavigableMap<byte[], Long> qualifiers = familyEntry.getValue();            for (Map.Entry<byte[], Long> qualifierEntry : qualifiers.entrySet()) {                byte[] qualifier = qualifierEntry.getKey();                long count = qualifierEntry.getValue();                inc.addColumn(family, qualifier, count);            }        }        coalesced.add(inc);    }    return coalesced;}
0
private void incrementCounter(Map<byte[], Map<byte[], NavigableMap<byte[], Long>>> counters, byte[] row, byte[] family, byte[] qualifier, Long count)
{    Map<byte[], NavigableMap<byte[], Long>> families = counters.computeIfAbsent(row, k -> Maps.newTreeMap(Bytes.BYTES_COMPARATOR));    NavigableMap<byte[], Long> qualifiers = families.computeIfAbsent(family, k -> Maps.newTreeMap(Bytes.BYTES_COMPARATOR));    qualifiers.merge(qualifier, count, (a, b) -> a + b);}
0
 String getHBbaseVersionString()
{    return VersionInfo.getVersion();}
0
private int getMajorVersion(String version) throws NumberFormatException
{    return Integer.parseInt(version.split("\\.")[0]);}
0
private boolean hasVersionAtLeast2()
{    String version = getHBbaseVersionString();    try {        if (this.getMajorVersion(version) >= 2) {            return true;        }    } catch (NumberFormatException ex) {            }        return false;}
1
 HBase2EventSerializer getSerializer()
{    return serializer;}
0
public long getBatchSize()
{    return batchSize;}
0
public void configure(Context context)
{    String regex = context.getString(REGEX_CONFIG, REGEX_DEFAULT);    boolean regexIgnoreCase = context.getBoolean(IGNORE_CASE_CONFIG, IGNORE_CASE_DEFAULT);    depositHeaders = context.getBoolean(DEPOSIT_HEADERS_CONFIG, DEPOSIT_HEADERS_DEFAULT);    inputPattern = Pattern.compile(regex, Pattern.DOTALL + (regexIgnoreCase ? Pattern.CASE_INSENSITIVE : 0));    charset = Charset.forName(context.getString(CHARSET_CONFIG, CHARSET_DEFAULT));    String colNameStr = context.getString(COL_NAME_CONFIG, COLUMN_NAME_DEFAULT);    String[] columnNames = colNameStr.split(",");    for (String s : columnNames) {        colNames.add(s.getBytes(charset));    }        rowKeyIndex = context.getInteger(ROW_KEY_INDEX_CONFIG, -1);        if (rowKeyIndex >= 0) {        if (rowKeyIndex >= columnNames.length) {            throw new IllegalArgumentException(ROW_KEY_INDEX_CONFIG + " must be " + "less than num columns " + columnNames.length);        }        if (!ROW_KEY_NAME.equalsIgnoreCase(columnNames[rowKeyIndex])) {            throw new IllegalArgumentException("Column at " + rowKeyIndex + " must be " + ROW_KEY_NAME + " and is " + columnNames[rowKeyIndex]);        }    }}
0
public void configure(ComponentConfiguration conf)
{}
0
public void initialize(Event event, byte[] columnFamily)
{    this.headers = event.getHeaders();    this.payload = event.getBody();    this.cf = columnFamily;}
0
protected byte[] getRowKey(Calendar cal)
{    /* NOTE: This key generation strategy has the following properties:     *      * 1) Within a single JVM, the same row key will never be duplicated.     * 2) Amongst any two JVM's operating at different time periods (according     *    to their respective clocks), the same row key will never be      *    duplicated.     * 3) Amongst any two JVM's operating concurrently (according to their     *    respective clocks), the odds of duplicating a row-key are non-zero     *    but infinitesimal. This would require simultaneous collision in (a)      *    the timestamp (b) the respective nonce and (c) the random string.     *    The string is necessary since (a) and (b) could collide if a fleet     *    of Flume agents are restarted in tandem.     *         *  Row-key uniqueness is important because conflicting row-keys will cause     *  data loss. */    String rowKey = String.format("%s-%s-%s", cal.getTimeInMillis(), randomKey, nonce.getAndIncrement());    return rowKey.getBytes(charset);}
0
protected byte[] getRowKey()
{    return getRowKey(Calendar.getInstance());}
0
public List<Row> getActions() throws FlumeException
{    List<Row> actions = Lists.newArrayList();    byte[] rowKey;    Matcher m = inputPattern.matcher(new String(payload, charset));    if (!m.matches()) {        return Lists.newArrayList();    }    if (m.groupCount() != colNames.size()) {        return Lists.newArrayList();    }    try {        if (rowKeyIndex < 0) {            rowKey = getRowKey();        } else {            rowKey = m.group(rowKeyIndex + 1).getBytes(Charsets.UTF_8);        }        Put put = new Put(rowKey);        for (int i = 0; i < colNames.size(); i++) {            if (i != rowKeyIndex) {                put.addColumn(cf, colNames.get(i), m.group(i + 1).getBytes(Charsets.UTF_8));            }        }        if (depositHeaders) {            for (Map.Entry<String, String> entry : headers.entrySet()) {                put.addColumn(cf, entry.getKey().getBytes(charset), entry.getValue().getBytes(charset));            }        }        actions.add(put);    } catch (Exception e) {        throw new FlumeException("Could not get row key!", e);    }    return actions;}
0
public List<Increment> getIncrements()
{    return Lists.newArrayList();}
0
public void close()
{}
0
public void configure(Context context)
{    rowPrefix = context.getString("rowPrefix", "default");    incrementRow = context.getString("incrementRow", "incRow").getBytes(Charsets.UTF_8);    String suffix = context.getString("suffix", "uuid");    String payloadColumn = context.getString("payloadColumn", "pCol");    String incColumn = context.getString("incrementColumn", "iCol");    if (payloadColumn != null && !payloadColumn.isEmpty()) {        switch(suffix) {            case "timestamp":                keyType = KeyType.TS;                break;            case "random":                keyType = KeyType.RANDOM;                break;            case "nano":                keyType = KeyType.TSNANO;                break;            default:                keyType = KeyType.UUID;                break;        }        plCol = payloadColumn.getBytes(Charsets.UTF_8);    }    if (incColumn != null && !incColumn.isEmpty()) {        incCol = incColumn.getBytes(Charsets.UTF_8);    }}
0
public void configure(ComponentConfiguration conf)
{}
0
public void initialize(Event event, byte[] cf)
{    this.payload = event.getBody();    this.cf = cf;}
0
public List<Row> getActions() throws FlumeException
{    List<Row> actions = new LinkedList<>();    if (plCol != null) {        byte[] rowKey;        try {            if (keyType == KeyType.TS) {                rowKey = SimpleRowKeyGenerator.getTimestampKey(rowPrefix);            } else if (keyType == KeyType.RANDOM) {                rowKey = SimpleRowKeyGenerator.getRandomKey(rowPrefix);            } else if (keyType == KeyType.TSNANO) {                rowKey = SimpleRowKeyGenerator.getNanoTimestampKey(rowPrefix);            } else {                rowKey = SimpleRowKeyGenerator.getUUIDKey(rowPrefix);            }            Put put = new Put(rowKey);            put.addColumn(cf, plCol, payload);            actions.add(put);        } catch (Exception e) {            throw new FlumeException("Could not get row key!", e);        }    }    return actions;}
0
public List<Increment> getIncrements()
{    List<Increment> incs = new LinkedList<>();    if (incCol != null) {        Increment inc = new Increment(incrementRow);        inc.addColumn(cf, incCol, 1);        incs.add(inc);    }    return incs;}
0
public void close()
{}
0
public static byte[] getUUIDKey(String prefix) throws UnsupportedEncodingException
{    return (prefix + UUID.randomUUID().toString()).getBytes("UTF8");}
0
public static byte[] getRandomKey(String prefix) throws UnsupportedEncodingException
{    return (prefix + String.valueOf(new Random().nextLong())).getBytes("UTF8");}
0
public static byte[] getTimestampKey(String prefix) throws UnsupportedEncodingException
{    return (prefix + String.valueOf(System.currentTimeMillis())).getBytes("UTF8");}
0
public static byte[] getNanoTimestampKey(String prefix) throws UnsupportedEncodingException
{    return (prefix + String.valueOf(System.nanoTime())).getBytes("UTF8");}
0
public void configure(Context context)
{}
0
public void configure(ComponentConfiguration conf)
{}
0
public void close()
{}
0
public void initialize(Event event, byte[] columnFamily)
{    this.event = event;    this.family = columnFamily;}
0
public List<Row> getActions()
{    return Collections.emptyList();}
0
public List<Increment> getIncrements()
{    List<Increment> increments = Lists.newArrayList();    String body = new String(event.getBody(), Charsets.UTF_8);    String[] pieces = body.split(":");    String row = pieces[0];    String qualifier = pieces[1];    Increment inc = new Increment(row.getBytes(Charsets.UTF_8));    inc.addColumn(family, qualifier.getBytes(Charsets.UTF_8), 1L);    increments.add(inc);    return increments;}
0
public void onBatchStart()
{    numBatchesStarted++;}
0
public int getNumBatchesStarted()
{    return numBatchesStarted;}
0
public List<Row> getActions() throws FlumeException
{    if (throwException) {        throw new FlumeException("Exception for testing");    }    return super.getActions();}
0
public static void setUpOnce() throws Exception
{    String hbaseVer = org.apache.hadoop.hbase.util.VersionInfo.getVersion();    System.out.println("HBASE VERSION:" + hbaseVer);    Configuration conf = HBaseConfiguration.create();    conf.setBoolean("hbase.localcluster.assign.random.ports", true);    testUtility = new HBaseTestingUtility(conf);    testUtility.startMiniCluster();}
0
public static void tearDownOnce() throws Exception
{    testUtility.shutdownMiniCluster();}
0
public void setUp() throws IOException
{    conf = new Configuration(testUtility.getConfiguration());    testUtility.createTable(TableName.valueOf(tableName), columnFamily.getBytes());}
0
public void tearDown() throws IOException
{    testUtility.deleteTable(TableName.valueOf(tableName));}
0
private Context getContextForSimpleHBase2EventSerializer()
{    Context ctx = new Context();    ctx.put("table", tableName);    ctx.put("columnFamily", columnFamily);    ctx.put("serializer", SimpleHBase2EventSerializer.class.getName());    ctx.put("serializer.payloadColumn", plCol);    ctx.put("serializer.incrementColumn", inColumn);    return ctx;}
0
private Context getContextForIncrementHBaseSerializer()
{    Context ctx = new Context();    ctx.put("table", tableName);    ctx.put("columnFamily", columnFamily);    ctx.put("serializer", IncrementHBase2Serializer.class.getName());    return ctx;}
0
private Context getContextWithoutIncrementHBaseSerializer()
{        Context ctx = new Context();    ctx.put("table", tableName);    ctx.put("columnFamily", columnFamily);    ctx.put("serializer", SimpleHBase2EventSerializer.class.getName());    return ctx;}
0
public void testOneEventWithDefaults() throws Exception
{    Context ctx = getContextWithoutIncrementHBaseSerializer();    HBase2Sink sink = new HBase2Sink(conf);    Configurables.configure(sink, ctx);    Channel channel = new MemoryChannel();    Configurables.configure(channel, new Context());    sink.setChannel(channel);    sink.start();    Transaction tx = channel.getTransaction();    tx.begin();    Event e = EventBuilder.withBody(Bytes.toBytes(valBase));    channel.put(e);    tx.commit();    tx.close();    sink.process();    sink.stop();    try (Connection connection = ConnectionFactory.createConnection(conf);        Table table = connection.getTable(TableName.valueOf(tableName))) {        byte[][] results = getResults(table, 1);        byte[] out = results[0];        Assert.assertArrayEquals(e.getBody(), out);        out = results[1];        Assert.assertArrayEquals(Longs.toByteArray(1), out);    }}
0
public void testOneEvent() throws Exception
{    Context ctx = getContextForSimpleHBase2EventSerializer();    HBase2Sink sink = new HBase2Sink(conf);    Configurables.configure(sink, ctx);    Channel channel = new MemoryChannel();    Configurables.configure(channel, new Context());    sink.setChannel(channel);    sink.start();    Transaction tx = channel.getTransaction();    tx.begin();    Event e = EventBuilder.withBody(Bytes.toBytes(valBase));    channel.put(e);    tx.commit();    tx.close();    sink.process();    sink.stop();    try (Connection connection = ConnectionFactory.createConnection(conf);        Table table = connection.getTable(TableName.valueOf(tableName))) {        byte[][] results = getResults(table, 1);        byte[] out = results[0];        Assert.assertArrayEquals(e.getBody(), out);        out = results[1];        Assert.assertArrayEquals(Longs.toByteArray(1), out);    }}
0
public void testThreeEvents() throws Exception
{    Context ctx = getContextForSimpleHBase2EventSerializer();    ctx.put("batchSize", "3");    HBase2Sink sink = new HBase2Sink(conf);    Configurables.configure(sink, ctx);    Channel channel = new MemoryChannel();    Configurables.configure(channel, new Context());    sink.setChannel(channel);    sink.start();    Transaction tx = channel.getTransaction();    tx.begin();    for (int i = 0; i < 3; i++) {        Event e = EventBuilder.withBody(Bytes.toBytes(valBase + "-" + i));        channel.put(e);    }    tx.commit();    tx.close();    sink.process();    sink.stop();    try (Connection connection = ConnectionFactory.createConnection(conf);        Table table = connection.getTable(TableName.valueOf(tableName))) {        byte[][] results = getResults(table, 3);        byte[] out;        int found = 0;        for (int i = 0; i < 3; i++) {            for (int j = 0; j < 3; j++) {                if (Arrays.equals(results[j], Bytes.toBytes(valBase + "-" + i))) {                    found++;                    break;                }            }        }        Assert.assertEquals(3, found);        out = results[3];        Assert.assertArrayEquals(Longs.toByteArray(3), out);    }}
0
public void testMultipleBatches() throws Exception
{    Context ctx = getContextForSimpleHBase2EventSerializer();    ctx.put("batchSize", "2");    HBase2Sink sink = new HBase2Sink(conf);    Configurables.configure(sink, ctx);        ctx.put("batchSize", "100");    Channel channel = new MemoryChannel();    Configurables.configure(channel, new Context());    sink.setChannel(channel);    sink.start();    Transaction tx = channel.getTransaction();    tx.begin();    for (int i = 0; i < 3; i++) {        Event e = EventBuilder.withBody(Bytes.toBytes(valBase + "-" + i));        channel.put(e);    }    tx.commit();    tx.close();    int count = 0;    while (sink.process() != Status.BACKOFF) {        count++;    }    sink.stop();    Assert.assertEquals(2, count);    try (Connection connection = ConnectionFactory.createConnection(conf)) {        Table table = connection.getTable(TableName.valueOf(tableName));        byte[][] results = getResults(table, 3);        byte[] out;        int found = 0;        for (int i = 0; i < 3; i++) {            for (int j = 0; j < 3; j++) {                if (Arrays.equals(results[j], Bytes.toBytes(valBase + "-" + i))) {                    found++;                    break;                }            }        }        Assert.assertEquals(3, found);        out = results[3];        Assert.assertArrayEquals(Longs.toByteArray(3), out);    }}
0
public void testMissingTable() throws Exception
{        Context ctx = getContextForSimpleHBase2EventSerializer();            testUtility.deleteTable(TableName.valueOf(tableName));    ctx.put("batchSize", "2");    HBase2Sink sink = new HBase2Sink(conf);    Configurables.configure(sink, ctx);        ctx.put("batchSize", "100");    Channel channel = new MemoryChannel();    Configurables.configure(channel, new Context());    sink.setChannel(channel);        Transaction tx = channel.getTransaction();    tx.begin();    for (int i = 0; i < 3; i++) {        Event e = EventBuilder.withBody(Bytes.toBytes(valBase + "-" + i));        channel.put(e);    }    tx.commit();    tx.close();        try {                        sink.start();                        sink.process();                sink.stop();    } finally {                testUtility.createTable(TableName.valueOf(tableName), columnFamily.getBytes());    }        Assert.fail();}
1
public void testHBaseFailure() throws Exception
{    Context ctx = getContextForSimpleHBase2EventSerializer();    ctx.put("batchSize", "2");    HBase2Sink sink = new HBase2Sink(conf);    Configurables.configure(sink, ctx);        ctx.put("batchSize", "100");    Channel channel = new MemoryChannel();    Configurables.configure(channel, new Context());    sink.setChannel(channel);    sink.start();    Transaction tx = channel.getTransaction();    tx.begin();    for (int i = 0; i < 3; i++) {        Event e = EventBuilder.withBody(Bytes.toBytes(valBase + "-" + i));        channel.put(e);    }    tx.commit();    tx.close();    sink.process();    try (Connection connection = ConnectionFactory.createConnection(conf);        Table table = connection.getTable(TableName.valueOf(tableName))) {        byte[][] results = getResults(table, 2);        byte[] out;        int found = 0;        for (int i = 0; i < 2; i++) {            for (int j = 0; j < 2; j++) {                if (Arrays.equals(results[j], Bytes.toBytes(valBase + "-" + i))) {                    found++;                    break;                }            }        }        Assert.assertEquals(2, found);        out = results[2];        Assert.assertArrayEquals(Longs.toByteArray(2), out);    }    testUtility.shutdownMiniCluster();    sink.process();    sink.stop();}
0
private byte[][] getResults(Table table, int numEvents) throws IOException
{    byte[][] results = new byte[numEvents + 1][];    Scan scan = new Scan();    scan.addColumn(columnFamily.getBytes(), plCol.getBytes());    scan.withStartRow(Bytes.toBytes("default"));    ResultScanner rs = table.getScanner(scan);    byte[] out;    int i = 0;    try {        for (Result r = rs.next(); r != null; r = rs.next()) {            out = r.getValue(columnFamily.getBytes(), plCol.getBytes());            if (i >= results.length - 1) {                rs.close();                throw new FlumeException("More results than expected in the table." + "Expected = " + numEvents + ". Found = " + i);            }            results[i++] = out;            System.out.println(out);        }    } finally {        rs.close();    }    Assert.assertEquals(i, results.length - 1);    scan = new Scan();    scan.addColumn(columnFamily.getBytes(), inColumn.getBytes());    scan.withStartRow(Bytes.toBytes("incRow"));    rs = table.getScanner(scan);    try {        for (Result r = rs.next(); r != null; r = rs.next()) {            out = r.getValue(columnFamily.getBytes(), inColumn.getBytes());            results[i++] = out;            System.out.println(out);        }    } finally {        rs.close();    }    return results;}
0
public void testTransactionStateOnChannelException() throws Exception
{    Context ctx = getContextForSimpleHBase2EventSerializer();    ctx.put("batchSize", "1");    HBase2Sink sink = new HBase2Sink(conf);    Configurables.configure(sink, ctx);        Channel channel = spy(new MemoryChannel());    Configurables.configure(channel, new Context());    sink.setChannel(channel);    sink.start();    Transaction tx = channel.getTransaction();    tx.begin();    Event e = EventBuilder.withBody(Bytes.toBytes(valBase + "-" + 0));    channel.put(e);    tx.commit();    tx.close();    doThrow(new ChannelException("Mock Exception")).when(channel).take();    try {        sink.process();        Assert.fail("take() method should throw exception");    } catch (ChannelException ex) {        Assert.assertEquals("Mock Exception", ex.getMessage());        SinkCounter sinkCounter = (SinkCounter) Whitebox.getInternalState(sink, "sinkCounter");        Assert.assertEquals(1, sinkCounter.getChannelReadFail());    }    doReturn(e).when(channel).take();    sink.process();    sink.stop();    try (Connection connection = ConnectionFactory.createConnection(conf);        Table table = connection.getTable(TableName.valueOf(tableName))) {        byte[][] results = getResults(table, 1);        byte[] out = results[0];        Assert.assertArrayEquals(e.getBody(), out);        out = results[1];        Assert.assertArrayEquals(Longs.toByteArray(1), out);    }}
0
public void testTransactionStateOnSerializationException() throws Exception
{    Context ctx = getContextForSimpleHBase2EventSerializer();    ctx.put("batchSize", "1");    ctx.put(HBase2SinkConfigurationConstants.CONFIG_SERIALIZER, "org.apache.flume.sink.hbase2.MockSimpleHBase2EventSerializer");    HBase2Sink sink = new HBase2Sink(conf);    Configurables.configure(sink, ctx);        ctx.put("batchSize", "100");    Channel channel = new MemoryChannel();    Configurables.configure(channel, new Context());    sink.setChannel(channel);    sink.start();    Transaction tx = channel.getTransaction();    tx.begin();    Event e = EventBuilder.withBody(Bytes.toBytes(valBase + "-" + 0));    channel.put(e);    tx.commit();    tx.close();    try {        MockSimpleHBase2EventSerializer.throwException = true;        sink.process();        Assert.fail("FlumeException expected from serializer");    } catch (FlumeException ex) {        Assert.assertEquals("Exception for testing", ex.getMessage());        SinkCounter sinkCounter = (SinkCounter) Whitebox.getInternalState(sink, "sinkCounter");        Assert.assertEquals(1, sinkCounter.getEventWriteFail());    }    MockSimpleHBase2EventSerializer.throwException = false;    sink.process();    sink.stop();    try (Connection connection = ConnectionFactory.createConnection(conf);        Table table = connection.getTable(TableName.valueOf(tableName))) {        byte[][] results = getResults(table, 1);        byte[] out = results[0];        Assert.assertArrayEquals(e.getBody(), out);        out = results[1];        Assert.assertArrayEquals(Longs.toByteArray(1), out);    }}
0
public void testWithoutConfigurationObject() throws Exception
{    Context ctx = getContextForSimpleHBase2EventSerializer();    Context tmpContext = new Context(ctx.getParameters());    tmpContext.put("batchSize", "2");    tmpContext.put(HBase2SinkConfigurationConstants.ZK_QUORUM, ZKConfig.getZKQuorumServersString(conf));    System.out.print(ctx.getString(HBase2SinkConfigurationConstants.ZK_QUORUM));    tmpContext.put(HBase2SinkConfigurationConstants.ZK_ZNODE_PARENT, conf.get(HConstants.ZOOKEEPER_ZNODE_PARENT, HConstants.DEFAULT_ZOOKEEPER_ZNODE_PARENT));    HBase2Sink sink = new HBase2Sink();    Configurables.configure(sink, tmpContext);    Channel channel = new MemoryChannel();    Configurables.configure(channel, ctx);    sink.setChannel(channel);    sink.start();    Transaction tx = channel.getTransaction();    tx.begin();    for (int i = 0; i < 3; i++) {        Event e = EventBuilder.withBody(Bytes.toBytes(valBase + "-" + i));        channel.put(e);    }    tx.commit();    tx.close();    Status status = Status.READY;    while (status != Status.BACKOFF) {        status = sink.process();    }    sink.stop();    try (Connection connection = ConnectionFactory.createConnection(conf);        Table table = connection.getTable(TableName.valueOf(tableName))) {        byte[][] results = getResults(table, 3);        byte[] out;        int found = 0;        for (int i = 0; i < 3; i++) {            for (int j = 0; j < 3; j++) {                if (Arrays.equals(results[j], Bytes.toBytes(valBase + "-" + i))) {                    found++;                    break;                }            }        }        Assert.assertEquals(3, found);        out = results[3];        Assert.assertArrayEquals(Longs.toByteArray(3), out);    }}
0
public void testZKQuorum() throws Exception
{    Context ctx = getContextForSimpleHBase2EventSerializer();    Context tmpContext = new Context(ctx.getParameters());    String zkQuorum = "zk1.flume.apache.org:3342, zk2.flume.apache.org:3342, " + "zk3.flume.apache.org:3342";    tmpContext.put("batchSize", "2");    tmpContext.put(HBase2SinkConfigurationConstants.ZK_QUORUM, zkQuorum);    tmpContext.put(HBase2SinkConfigurationConstants.ZK_ZNODE_PARENT, conf.get(HConstants.ZOOKEEPER_ZNODE_PARENT, HConstants.DEFAULT_ZOOKEEPER_ZNODE_PARENT));    HBase2Sink sink = new HBase2Sink();    Configurables.configure(sink, tmpContext);    Assert.assertEquals("zk1.flume.apache.org,zk2.flume.apache.org," + "zk3.flume.apache.org", sink.getConfig().get(HConstants.ZOOKEEPER_QUORUM));    Assert.assertEquals(String.valueOf(3342), sink.getConfig().get(HConstants.ZOOKEEPER_CLIENT_PORT));}
0
public void testZKQuorumIncorrectPorts() throws Exception
{    Context ctx = getContextForSimpleHBase2EventSerializer();    Context tmpContext = new Context(ctx.getParameters());    String zkQuorum = "zk1.flume.apache.org:3345, zk2.flume.apache.org:3342, " + "zk3.flume.apache.org:3342";    tmpContext.put("batchSize", "2");    tmpContext.put(HBase2SinkConfigurationConstants.ZK_QUORUM, zkQuorum);    tmpContext.put(HBase2SinkConfigurationConstants.ZK_ZNODE_PARENT, conf.get(HConstants.ZOOKEEPER_ZNODE_PARENT, HConstants.DEFAULT_ZOOKEEPER_ZNODE_PARENT));    HBase2Sink sink = new HBase2Sink();    Configurables.configure(sink, tmpContext);    Assert.fail();}
0
public void testCoalesce() throws EventDeliveryException
{    Context ctx = getContextForIncrementHBaseSerializer();    ctx.put("batchSize", "100");    ctx.put(HBase2SinkConfigurationConstants.CONFIG_COALESCE_INCREMENTS, String.valueOf(true));    final Map<String, Long> expectedCounts = Maps.newHashMap();    expectedCounts.put("r1:c1", 10L);    expectedCounts.put("r1:c2", 20L);    expectedCounts.put("r2:c1", 7L);    expectedCounts.put("r2:c3", 63L);    HBase2Sink.DebugIncrementsCallback cb = new CoalesceValidator(expectedCounts);    HBase2Sink sink = new HBase2Sink(testUtility.getConfiguration(), cb);    Configurables.configure(sink, ctx);    Channel channel = createAndConfigureMemoryChannel(sink);    List<Event> events = Lists.newLinkedList();    generateEvents(events, expectedCounts);    putEvents(channel, events);    sink.start();        sink.process();    sink.stop();}
0
public void negativeTestCoalesce() throws EventDeliveryException
{    Context ctx = getContextForIncrementHBaseSerializer();    ctx.put("batchSize", "10");    final Map<String, Long> expectedCounts = Maps.newHashMap();    expectedCounts.put("r1:c1", 10L);    HBase2Sink.DebugIncrementsCallback cb = new CoalesceValidator(expectedCounts);    HBase2Sink sink = new HBase2Sink(testUtility.getConfiguration(), cb);    Configurables.configure(sink, ctx);    Channel channel = createAndConfigureMemoryChannel(sink);    List<Event> events = Lists.newLinkedList();    generateEvents(events, expectedCounts);    putEvents(channel, events);    sink.start();        sink.process();    sink.stop();}
0
public void testBatchAware() throws EventDeliveryException
{        Context ctx = getContextForIncrementHBaseSerializer();    HBase2Sink sink = new HBase2Sink(testUtility.getConfiguration());    Configurables.configure(sink, ctx);    Channel channel = createAndConfigureMemoryChannel(sink);    sink.start();    int batchCount = 3;    for (int i = 0; i < batchCount; i++) {        sink.process();    }    sink.stop();    Assert.assertEquals(batchCount, ((IncrementHBase2Serializer) sink.getSerializer()).getNumBatchesStarted());}
1
public void testHBaseVersionCheck() throws Exception
{    Context ctx = getContextWithoutIncrementHBaseSerializer();    HBase2Sink sink = mock(HBase2Sink.class);    doCallRealMethod().when(sink).configure(any());    when(sink.getHBbaseVersionString()).thenReturn("1.0.0");    Configurables.configure(sink, ctx);}
0
public void testHBaseVersionCheckNotANumber() throws Exception
{    Context ctx = getContextWithoutIncrementHBaseSerializer();    HBase2Sink sink = mock(HBase2Sink.class);    doCallRealMethod().when(sink).configure(any());    when(sink.getHBbaseVersionString()).thenReturn("Dummy text");    Configurables.configure(sink, ctx);}
0
public void onAfterCoalesce(Iterable<Increment> increments)
{    for (Increment inc : increments) {        byte[] row = inc.getRow();        Map<byte[], NavigableMap<byte[], Long>> families = null;        try {            families = inc.getFamilyMapOfLongs();        } catch (Exception e) {            Throwables.propagate(e);        }        assert families != null;        for (byte[] family : families.keySet()) {            NavigableMap<byte[], Long> qualifiers = families.get(family);            for (Map.Entry<byte[], Long> entry : qualifiers.entrySet()) {                byte[] qualifier = entry.getKey();                Long count = entry.getValue();                String key = new String(row, Charsets.UTF_8) + ':' + new String(qualifier, Charsets.UTF_8);                Assert.assertEquals("Expected counts don't match observed for " + key, expectedCounts.get(key), count);            }        }    }}
0
private void generateEvents(List<Event> events, Map<String, Long> counts)
{    for (String key : counts.keySet()) {        long count = counts.get(key);        for (long i = 0; i < count; i++) {            events.add(EventBuilder.withBody(key, Charsets.UTF_8));        }    }}
0
private Channel createAndConfigureMemoryChannel(HBase2Sink sink)
{    Channel channel = new MemoryChannel();    Context channelCtx = new Context();    channelCtx.put("capacity", String.valueOf(1000L));    channelCtx.put("transactionCapacity", String.valueOf(1000L));    Configurables.configure(channel, channelCtx);    sink.setChannel(channel);    channel.start();    return channel;}
0
private void putEvents(Channel channel, Iterable<Event> events)
{    Transaction tx = channel.getTransaction();    tx.begin();    for (Event event : events) {        channel.put(event);    }    tx.commit();    tx.close();}
0
public void setUp()
{    sinkFactory = new DefaultSinkFactory();}
0
private void verifySinkCreation(Class<?> typeClass) throws FlumeException
{    Sink sink = sinkFactory.create("hbase2-sink", "hbase2");    Assert.assertNotNull(sink);    Assert.assertTrue(typeClass.isInstance(sink));}
0
public void testSinkCreation()
{    verifySinkCreation(HBase2Sink.class);}
0
public void testDefaultBehavior() throws Exception
{    RegexHBase2EventSerializer s = new RegexHBase2EventSerializer();    Context context = new Context();    s.configure(context);    String logMsg = "The sky is falling!";    Event e = EventBuilder.withBody(Bytes.toBytes(logMsg));    s.initialize(e, "CF".getBytes());    List<Row> actions = s.getActions();    assertTrue(actions.size() == 1);    assertTrue(actions.get(0) instanceof Put);    Put put = (Put) actions.get(0);    assertTrue(put.getFamilyCellMap().containsKey(s.cf));    List<Cell> cells = put.getFamilyCellMap().get(s.cf);    assertTrue(cells.size() == 1);    Map<String, String> resultMap = Maps.newHashMap();    for (Cell cell : cells) {        resultMap.put(new String(CellUtil.cloneQualifier(cell)), new String(CellUtil.cloneValue(cell)));    }    assertTrue(resultMap.containsKey(RegexHBase2EventSerializer.COLUMN_NAME_DEFAULT));    assertEquals("The sky is falling!", resultMap.get(RegexHBase2EventSerializer.COLUMN_NAME_DEFAULT));}
0
public void testRowIndexKey() throws Exception
{    RegexHBase2EventSerializer s = new RegexHBase2EventSerializer();    Context context = new Context();    context.put(RegexHBase2EventSerializer.REGEX_CONFIG, "^([^\t]+)\t([^\t]+)\t" + "([^\t]+)$");    context.put(RegexHBase2EventSerializer.COL_NAME_CONFIG, "col1,col2,ROW_KEY");    context.put("rowKeyIndex", "2");    s.configure(context);    String body = "val1\tval2\trow1";    Event e = EventBuilder.withBody(Bytes.toBytes(body));    s.initialize(e, "CF".getBytes());    List<Row> actions = s.getActions();    Put put = (Put) actions.get(0);    List<Cell> cells = put.getFamilyCellMap().get(s.cf);    assertTrue(cells.size() == 2);    Map<String, String> resultMap = Maps.newHashMap();    for (Cell cell : cells) {        resultMap.put(new String(CellUtil.cloneQualifier(cell)), new String(CellUtil.cloneValue(cell)));    }    assertEquals("val1", resultMap.get("col1"));    assertEquals("val2", resultMap.get("col2"));    assertEquals("row1", Bytes.toString(put.getRow()));}
0
public void testApacheRegex() throws Exception
{    RegexHBase2EventSerializer s = new RegexHBase2EventSerializer();    Context context = new Context();    context.put(RegexHBase2EventSerializer.REGEX_CONFIG, "([^ ]*) ([^ ]*) ([^ ]*) (-|\\[[^\\]]*\\]) \"([^ ]+) ([^ ]+)" + " ([^\"]+)\" (-|[0-9]*) (-|[0-9]*)(?: ([^ \"]*|\"[^\"]*\")" + " ([^ \"]*|\"[^\"]*\"))?");    context.put(RegexHBase2EventSerializer.COL_NAME_CONFIG, "host,identity,user,time,method,request,protocol,status,size," + "referer,agent");    s.configure(context);    String logMsg = "33.22.11.00 - - [20/May/2011:07:01:19 +0000] " + "\"GET /wp-admin/css/install.css HTTP/1.0\" 200 813 " + "\"http://www.cloudera.com/wp-admin/install.php\" \"Mozilla/5.0 (comp" + "atible; Yahoo! Slurp; http://help.yahoo.com/help/us/ysearch/slurp)\"";    Event e = EventBuilder.withBody(Bytes.toBytes(logMsg));    s.initialize(e, "CF".getBytes());    List<Row> actions = s.getActions();    assertEquals(1, s.getActions().size());    assertTrue(actions.get(0) instanceof Put);    Put put = (Put) actions.get(0);    assertTrue(put.getFamilyCellMap().containsKey(s.cf));    List<Cell> cells = put.getFamilyCellMap().get(s.cf);    assertTrue(cells.size() == 11);    Map<String, String> resultMap = Maps.newHashMap();    for (Cell cell : cells) {        resultMap.put(new String(CellUtil.cloneQualifier(cell)), new String(CellUtil.cloneValue(cell)));    }    assertEquals("33.22.11.00", resultMap.get("host"));    assertEquals("-", resultMap.get("identity"));    assertEquals("-", resultMap.get("user"));    assertEquals("[20/May/2011:07:01:19 +0000]", resultMap.get("time"));    assertEquals("GET", resultMap.get("method"));    assertEquals("/wp-admin/css/install.css", resultMap.get("request"));    assertEquals("HTTP/1.0", resultMap.get("protocol"));    assertEquals("200", resultMap.get("status"));    assertEquals("813", resultMap.get("size"));    assertEquals("\"http://www.cloudera.com/wp-admin/install.php\"", resultMap.get("referer"));    assertEquals("\"Mozilla/5.0 (compatible; Yahoo! Slurp; " + "http://help.yahoo.com/help/us/ysearch/slurp)\"", resultMap.get("agent"));    List<Increment> increments = s.getIncrements();    assertEquals(0, increments.size());}
0
public void testRowKeyGeneration()
{    Context context = new Context();    RegexHBase2EventSerializer s1 = new RegexHBase2EventSerializer();    s1.configure(context);    RegexHBase2EventSerializer s2 = new RegexHBase2EventSerializer();    s2.configure(context);        RegexHBase2EventSerializer.nonce.set(0);    String randomString = RegexHBase2EventSerializer.randomKey;    Event e1 = EventBuilder.withBody(Bytes.toBytes("body"));    Event e2 = EventBuilder.withBody(Bytes.toBytes("body"));    Event e3 = EventBuilder.withBody(Bytes.toBytes("body"));    Calendar cal = mock(Calendar.class);    when(cal.getTimeInMillis()).thenReturn(1L);    s1.initialize(e1, "CF".getBytes());    String rk1 = new String(s1.getRowKey(cal));    assertEquals("1-" + randomString + "-0", rk1);    when(cal.getTimeInMillis()).thenReturn(10L);    s1.initialize(e2, "CF".getBytes());    String rk2 = new String(s1.getRowKey(cal));    assertEquals("10-" + randomString + "-1", rk2);    when(cal.getTimeInMillis()).thenReturn(100L);    s2.initialize(e3, "CF".getBytes());    String rk3 = new String(s2.getRowKey(cal));    assertEquals("100-" + randomString + "-2", rk3);}
0
public void testDepositHeaders() throws Exception
{    Charset charset = Charset.forName("KOI8-R");    RegexHBase2EventSerializer s = new RegexHBase2EventSerializer();    Context context = new Context();    context.put(RegexHBase2EventSerializer.DEPOSIT_HEADERS_CONFIG, "true");    context.put(RegexHBase2EventSerializer.CHARSET_CONFIG, charset.toString());    s.configure(context);    String body = "body";    Map<String, String> headers = Maps.newHashMap();    headers.put("header1", "value1");    headers.put("2", "2");    Event e = EventBuilder.withBody(Bytes.toBytes(body), headers);    s.initialize(e, "CF".getBytes());    List<Row> actions = s.getActions();    assertEquals(1, s.getActions().size());    assertTrue(actions.get(0) instanceof Put);    Put put = (Put) actions.get(0);    assertTrue(put.getFamilyCellMap().containsKey(s.cf));    List<Cell> cells = put.getFamilyCellMap().get(s.cf);    assertTrue(cells.size() == 3);    Map<String, byte[]> resultMap = Maps.newHashMap();    for (Cell cell : cells) {        resultMap.put(new String(CellUtil.cloneQualifier(cell), charset), CellUtil.cloneValue(cell));    }    assertEquals(body, new String(resultMap.get(RegexHBase2EventSerializer.COLUMN_NAME_DEFAULT), charset));    assertEquals("value1", new String(resultMap.get("header1"), charset));    assertArrayEquals("2".getBytes(charset), resultMap.get("2"));    assertEquals("2".length(), resultMap.get("2").length);    List<Increment> increments = s.getIncrements();    assertEquals(0, increments.size());}
0
public String getTopic()
{    return topic;}
0
public long getBatchSize()
{    return batchSize;}
0
public Status process() throws EventDeliveryException
{    Status result = Status.READY;    Channel channel = getChannel();    Transaction transaction = null;    Event event = null;    String eventTopic = null;    String eventKey = null;    try {        long processedEvents = 0;        transaction = channel.getTransaction();        transaction.begin();        kafkaFutures.clear();        long batchStartTime = System.nanoTime();        for (; processedEvents < batchSize; processedEvents += 1) {            event = channel.take();            if (event == null) {                                if (processedEvents == 0) {                    result = Status.BACKOFF;                    counter.incrementBatchEmptyCount();                } else {                    counter.incrementBatchUnderflowCount();                }                break;            }            counter.incrementEventDrainAttemptCount();            byte[] eventBody = event.getBody();            Map<String, String> headers = event.getHeaders();            if (allowTopicOverride) {                eventTopic = headers.get(topicHeader);                if (eventTopic == null) {                    eventTopic = BucketPath.escapeString(topic, event.getHeaders());                                    }            } else {                eventTopic = topic;            }            eventKey = headers.get(KEY_HEADER);            if (logger.isTraceEnabled()) {                if (LogPrivacyUtil.allowLogRawData()) {                    logger.trace("{Event} " + eventTopic + " : " + eventKey + " : " + new String(eventBody, "UTF-8"));                } else {                    logger.trace("{Event} " + eventTopic + " : " + eventKey);                }            }                                    long startTime = System.currentTimeMillis();            Integer partitionId = null;            try {                ProducerRecord<String, byte[]> record;                if (staticPartitionId != null) {                    partitionId = staticPartitionId;                }                                if (partitionHeader != null) {                    String headerVal = event.getHeaders().get(partitionHeader);                    if (headerVal != null) {                        partitionId = Integer.parseInt(headerVal);                    }                }                if (partitionId != null) {                    record = new ProducerRecord<String, byte[]>(eventTopic, partitionId, eventKey, serializeEvent(event, useAvroEventFormat));                } else {                    record = new ProducerRecord<String, byte[]>(eventTopic, eventKey, serializeEvent(event, useAvroEventFormat));                }                kafkaFutures.add(producer.send(record, new SinkCallback(startTime)));            } catch (NumberFormatException ex) {                throw new EventDeliveryException("Non integer partition id specified", ex);            } catch (Exception ex) {                                throw new EventDeliveryException("Could not send event", ex);            }        }                producer.flush();                if (processedEvents > 0) {            for (Future<RecordMetadata> future : kafkaFutures) {                future.get();            }            long endTime = System.nanoTime();            counter.addToKafkaEventSendTimer((endTime - batchStartTime) / (1000 * 1000));            counter.addToEventDrainSuccessCount(Long.valueOf(kafkaFutures.size()));        }        transaction.commit();    } catch (Exception ex) {        String errorMsg = "Failed to publish events";                counter.incrementEventWriteOrChannelFail(ex);        result = Status.BACKOFF;        if (transaction != null) {            try {                kafkaFutures.clear();                transaction.rollback();                counter.incrementRollbackCount();            } catch (Exception e) {                                throw Throwables.propagate(e);            }        }        throw new EventDeliveryException(errorMsg, ex);    } finally {        if (transaction != null) {            transaction.close();        }    }    return result;}
1
public synchronized void start()
{        producer = new KafkaProducer<String, byte[]>(kafkaProps);    counter.start();    super.start();}
0
public synchronized void stop()
{    producer.close();    counter.stop();        super.stop();}
1
public void configure(Context context)
{    translateOldProps(context);    String topicStr = context.getString(TOPIC_CONFIG);    if (topicStr == null || topicStr.isEmpty()) {        topicStr = DEFAULT_TOPIC;            } else {            }    topic = topicStr;    batchSize = context.getInteger(BATCH_SIZE, DEFAULT_BATCH_SIZE);    if (logger.isDebugEnabled()) {            }    useAvroEventFormat = context.getBoolean(KafkaSinkConstants.AVRO_EVENT, KafkaSinkConstants.DEFAULT_AVRO_EVENT);    partitionHeader = context.getString(KafkaSinkConstants.PARTITION_HEADER_NAME);    staticPartitionId = context.getInteger(KafkaSinkConstants.STATIC_PARTITION_CONF);    allowTopicOverride = context.getBoolean(KafkaSinkConstants.ALLOW_TOPIC_OVERRIDE_HEADER, KafkaSinkConstants.DEFAULT_ALLOW_TOPIC_OVERRIDE_HEADER);    topicHeader = context.getString(KafkaSinkConstants.TOPIC_OVERRIDE_HEADER, KafkaSinkConstants.DEFAULT_TOPIC_OVERRIDE_HEADER);    if (logger.isDebugEnabled()) {            }    kafkaFutures = new LinkedList<Future<RecordMetadata>>();    String bootStrapServers = context.getString(BOOTSTRAP_SERVERS_CONFIG);    if (bootStrapServers == null || bootStrapServers.isEmpty()) {        throw new ConfigurationException("Bootstrap Servers must be specified");    }    setProducerProps(context, bootStrapServers);    if (logger.isDebugEnabled() && LogPrivacyUtil.allowLogPrintConfig()) {            }    if (counter == null) {        counter = new KafkaSinkCounter(getName());    }}
1
private void translateOldProps(Context ctx)
{    if (!(ctx.containsKey(TOPIC_CONFIG))) {        ctx.put(TOPIC_CONFIG, ctx.getString("topic"));            }        if (!(ctx.containsKey(BOOTSTRAP_SERVERS_CONFIG))) {        String brokerList = ctx.getString(BROKER_LIST_FLUME_KEY);        if (brokerList == null || brokerList.isEmpty()) {            throw new ConfigurationException("Bootstrap Servers must be specified");        } else {            ctx.put(BOOTSTRAP_SERVERS_CONFIG, brokerList);                    }    }        if (!(ctx.containsKey(BATCH_SIZE))) {        String oldBatchSize = ctx.getString(OLD_BATCH_SIZE);        if (oldBatchSize != null && !oldBatchSize.isEmpty()) {            ctx.put(BATCH_SIZE, oldBatchSize);                    }    }        if (!(ctx.containsKey(KAFKA_PRODUCER_PREFIX + ProducerConfig.ACKS_CONFIG))) {        String requiredKey = ctx.getString(KafkaSinkConstants.REQUIRED_ACKS_FLUME_KEY);        if (!(requiredKey == null) && !(requiredKey.isEmpty())) {            ctx.put(KAFKA_PRODUCER_PREFIX + ProducerConfig.ACKS_CONFIG, requiredKey);                    }    }    if (ctx.containsKey(KEY_SERIALIZER_KEY)) {            }    if (ctx.containsKey(MESSAGE_SERIALIZER_KEY)) {            }}
1
private void setProducerProps(Context context, String bootStrapServers)
{    kafkaProps.clear();    kafkaProps.put(ProducerConfig.ACKS_CONFIG, DEFAULT_ACKS);        kafkaProps.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, DEFAULT_KEY_SERIALIZER);    kafkaProps.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, DEFAULT_VALUE_SERIAIZER);    kafkaProps.putAll(context.getSubProperties(KAFKA_PRODUCER_PREFIX));    kafkaProps.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, bootStrapServers);    KafkaSSLUtil.addGlobalSSLParameters(kafkaProps);}
0
protected Properties getKafkaProps()
{    return kafkaProps;}
0
private byte[] serializeEvent(Event event, boolean useAvroEventFormat) throws IOException
{    byte[] bytes;    if (useAvroEventFormat) {        if (!tempOutStream.isPresent()) {            tempOutStream = Optional.of(new ByteArrayOutputStream());        }        if (!writer.isPresent()) {            writer = Optional.of(new SpecificDatumWriter<AvroFlumeEvent>(AvroFlumeEvent.class));        }        tempOutStream.get().reset();        AvroFlumeEvent e = new AvroFlumeEvent(toCharSeqMap(event.getHeaders()), ByteBuffer.wrap(event.getBody()));        encoder = EncoderFactory.get().directBinaryEncoder(tempOutStream.get(), encoder);        writer.get().write(e, encoder);        encoder.flush();        bytes = tempOutStream.get().toByteArray();    } else {        bytes = event.getBody();    }    return bytes;}
0
private static Map<CharSequence, CharSequence> toCharSeqMap(Map<String, String> stringMap)
{    Map<CharSequence, CharSequence> charSeqMap = new HashMap<CharSequence, CharSequence>();    for (Map.Entry<String, String> entry : stringMap.entrySet()) {        charSeqMap.put(entry.getKey(), entry.getValue());    }    return charSeqMap;}
0
public void onCompletion(RecordMetadata metadata, Exception exception)
{    if (exception != null) {            }    if (logger.isDebugEnabled()) {        long eventElapsedTime = System.currentTimeMillis() - startTime;        if (metadata != null) {                    }            }}
1
public static void setup()
{    testUtil.prepare();    List<String> topics = new ArrayList<String>(3);    topics.add(DEFAULT_TOPIC);    topics.add(TestConstants.STATIC_TOPIC);    topics.add(TestConstants.CUSTOM_TOPIC);    topics.add(TestConstants.HEADER_1_VALUE + "-topic");    testUtil.initTopicList(topics);}
0
public static void tearDown()
{    testUtil.tearDown();}
0
public void testKafkaProperties()
{    KafkaSink kafkaSink = new KafkaSink();    Context context = new Context();    context.put(KAFKA_PREFIX + TOPIC_CONFIG, "");    context.put(KAFKA_PRODUCER_PREFIX + ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, "override.default.serializer");    context.put("kafka.producer.fake.property", "kafka.property.value");    context.put("kafka.bootstrap.servers", "localhost:9092,localhost:9092");    context.put("brokerList", "real-broker-list");    Configurables.configure(kafkaSink, context);    Properties kafkaProps = kafkaSink.getKafkaProps();        assertEquals(kafkaProps.getProperty(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG), DEFAULT_KEY_SERIALIZER);        assertEquals(kafkaProps.getProperty(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG), "override.default.serializer");        assertEquals(kafkaProps.getProperty("fake.property"), "kafka.property.value");        assertEquals(kafkaProps.getProperty("bootstrap.servers"), "localhost:9092,localhost:9092");}
0
public void testOldProperties()
{    KafkaSink kafkaSink = new KafkaSink();    Context context = new Context();    context.put("topic", "test-topic");    context.put(OLD_BATCH_SIZE, "300");    context.put(BROKER_LIST_FLUME_KEY, "localhost:9092,localhost:9092");    context.put(REQUIRED_ACKS_FLUME_KEY, "all");    Configurables.configure(kafkaSink, context);    Properties kafkaProps = kafkaSink.getKafkaProps();    assertEquals(kafkaSink.getTopic(), "test-topic");    assertEquals(kafkaSink.getBatchSize(), 300);    assertEquals(kafkaProps.getProperty(CommonClientConfigs.BOOTSTRAP_SERVERS_CONFIG), "localhost:9092,localhost:9092");    assertEquals(kafkaProps.getProperty(ProducerConfig.ACKS_CONFIG), "all");}
0
public void testDefaultTopic()
{    Sink kafkaSink = new KafkaSink();    Context context = prepareDefaultContext();    Configurables.configure(kafkaSink, context);    Channel memoryChannel = new MemoryChannel();    Configurables.configure(memoryChannel, context);    kafkaSink.setChannel(memoryChannel);    kafkaSink.start();    String msg = "default-topic-test";    Transaction tx = memoryChannel.getTransaction();    tx.begin();    Event event = EventBuilder.withBody(msg.getBytes());    memoryChannel.put(event);    tx.commit();    tx.close();    try {        Sink.Status status = kafkaSink.process();        if (status == Sink.Status.BACKOFF) {            fail("Error Occurred");        }    } catch (EventDeliveryException ex) {        }    checkMessageArrived(msg, DEFAULT_TOPIC);}
0
private void checkMessageArrived(String msg, String topic)
{    ConsumerRecords recs = pollConsumerRecords(topic);    assertNotNull(recs);    assertTrue(recs.count() > 0);    ConsumerRecord consumerRecord = (ConsumerRecord) recs.iterator().next();    assertEquals(msg, consumerRecord.value());}
0
public void testStaticTopic()
{    Context context = prepareDefaultContext();        context.put(TOPIC_CONFIG, TestConstants.STATIC_TOPIC);    String msg = "static-topic-test";    try {        Sink.Status status = prepareAndSend(context, msg);        if (status == Sink.Status.BACKOFF) {            fail("Error Occurred");        }    } catch (EventDeliveryException ex) {        }    checkMessageArrived(msg, TestConstants.STATIC_TOPIC);}
0
public void testTopicAndKeyFromHeader()
{    Sink kafkaSink = new KafkaSink();    Context context = prepareDefaultContext();    Configurables.configure(kafkaSink, context);    Channel memoryChannel = new MemoryChannel();    Configurables.configure(memoryChannel, context);    kafkaSink.setChannel(memoryChannel);    kafkaSink.start();    String msg = "test-topic-and-key-from-header";    Map<String, String> headers = new HashMap<String, String>();    headers.put("topic", TestConstants.CUSTOM_TOPIC);    headers.put("key", TestConstants.CUSTOM_KEY);    Transaction tx = memoryChannel.getTransaction();    tx.begin();    Event event = EventBuilder.withBody(msg.getBytes(), headers);    memoryChannel.put(event);    tx.commit();    tx.close();    try {        Sink.Status status = kafkaSink.process();        if (status == Sink.Status.BACKOFF) {            fail("Error Occurred");        }    } catch (EventDeliveryException ex) {        }    checkMessageArrived(msg, TestConstants.CUSTOM_TOPIC);}
0
public void testTopicFromConfHeader()
{    String customTopicHeader = "customTopicHeader";    Sink kafkaSink = new KafkaSink();    Context context = prepareDefaultContext();    context.put(KafkaSinkConstants.TOPIC_OVERRIDE_HEADER, customTopicHeader);    Configurables.configure(kafkaSink, context);    Channel memoryChannel = new MemoryChannel();    Configurables.configure(memoryChannel, context);    kafkaSink.setChannel(memoryChannel);    kafkaSink.start();    String msg = "test-topic-from-config-header";    Map<String, String> headers = new HashMap<String, String>();    headers.put(customTopicHeader, TestConstants.CUSTOM_TOPIC);    Transaction tx = memoryChannel.getTransaction();    tx.begin();    Event event = EventBuilder.withBody(msg.getBytes(), headers);    memoryChannel.put(event);    tx.commit();    tx.close();    try {        Sink.Status status = kafkaSink.process();        if (status == Sink.Status.BACKOFF) {            fail("Error Occurred");        }    } catch (EventDeliveryException ex) {        }    checkMessageArrived(msg, TestConstants.CUSTOM_TOPIC);}
0
public void testTopicNotFromConfHeader()
{    Sink kafkaSink = new KafkaSink();    Context context = prepareDefaultContext();    context.put(KafkaSinkConstants.ALLOW_TOPIC_OVERRIDE_HEADER, "false");    context.put(KafkaSinkConstants.TOPIC_OVERRIDE_HEADER, "foo");    Configurables.configure(kafkaSink, context);    Channel memoryChannel = new MemoryChannel();    Configurables.configure(memoryChannel, context);    kafkaSink.setChannel(memoryChannel);    kafkaSink.start();    String msg = "test-topic-from-config-header";    Map<String, String> headers = new HashMap<String, String>();    headers.put(KafkaSinkConstants.DEFAULT_TOPIC_OVERRIDE_HEADER, TestConstants.CUSTOM_TOPIC);    headers.put("foo", TestConstants.CUSTOM_TOPIC);    Transaction tx = memoryChannel.getTransaction();    tx.begin();    Event event = EventBuilder.withBody(msg.getBytes(), headers);    memoryChannel.put(event);    tx.commit();    tx.close();    try {        Sink.Status status = kafkaSink.process();        if (status == Sink.Status.BACKOFF) {            fail("Error Occurred");        }    } catch (EventDeliveryException ex) {        }    checkMessageArrived(msg, DEFAULT_TOPIC);}
0
public void testReplaceSubStringOfTopicWithHeaders()
{    String topic = TestConstants.HEADER_1_VALUE + "-topic";    Sink kafkaSink = new KafkaSink();    Context context = prepareDefaultContext();    context.put(TOPIC_CONFIG, TestConstants.HEADER_TOPIC);    Configurables.configure(kafkaSink, context);    Channel memoryChannel = new MemoryChannel();    Configurables.configure(memoryChannel, context);    kafkaSink.setChannel(memoryChannel);    kafkaSink.start();    String msg = "test-replace-substring-of-topic-with-headers";    Map<String, String> headers = new HashMap<>();    headers.put(TestConstants.HEADER_1_KEY, TestConstants.HEADER_1_VALUE);    Transaction tx = memoryChannel.getTransaction();    tx.begin();    Event event = EventBuilder.withBody(msg.getBytes(), headers);    memoryChannel.put(event);    tx.commit();    tx.close();    try {        Sink.Status status = kafkaSink.process();        if (status == Sink.Status.BACKOFF) {            fail("Error Occurred");        }    } catch (EventDeliveryException ex) {        }    checkMessageArrived(msg, topic);}
0
public void testAvroEvent() throws IOException
{    Sink kafkaSink = new KafkaSink();    Context context = prepareDefaultContext();    context.put(AVRO_EVENT, "true");    Configurables.configure(kafkaSink, context);    Channel memoryChannel = new MemoryChannel();    Configurables.configure(memoryChannel, context);    kafkaSink.setChannel(memoryChannel);    kafkaSink.start();    String msg = "test-avro-event";    Map<String, String> headers = new HashMap<String, String>();    headers.put("topic", TestConstants.CUSTOM_TOPIC);    headers.put("key", TestConstants.CUSTOM_KEY);    headers.put(TestConstants.HEADER_1_KEY, TestConstants.HEADER_1_VALUE);    Transaction tx = memoryChannel.getTransaction();    tx.begin();    Event event = EventBuilder.withBody(msg.getBytes(), headers);    memoryChannel.put(event);    tx.commit();    tx.close();    try {        Sink.Status status = kafkaSink.process();        if (status == Sink.Status.BACKOFF) {            fail("Error Occurred");        }    } catch (EventDeliveryException ex) {        }    String topic = TestConstants.CUSTOM_TOPIC;    ConsumerRecords<String, String> recs = pollConsumerRecords(topic);    assertNotNull(recs);    assertTrue(recs.count() > 0);    ConsumerRecord<String, String> consumerRecord = recs.iterator().next();    ByteArrayInputStream in = new ByteArrayInputStream(consumerRecord.value().getBytes());    BinaryDecoder decoder = DecoderFactory.get().directBinaryDecoder(in, null);    SpecificDatumReader<AvroFlumeEvent> reader = new SpecificDatumReader<>(AvroFlumeEvent.class);    AvroFlumeEvent avroevent = reader.read(null, decoder);    String eventBody = new String(avroevent.getBody().array(), Charsets.UTF_8);    Map<CharSequence, CharSequence> eventHeaders = avroevent.getHeaders();    assertEquals(msg, eventBody);    assertEquals(TestConstants.CUSTOM_KEY, consumerRecord.key());    assertEquals(TestConstants.HEADER_1_VALUE, eventHeaders.get(new Utf8(TestConstants.HEADER_1_KEY)).toString());    assertEquals(TestConstants.CUSTOM_KEY, eventHeaders.get(new Utf8("key")).toString());}
0
private ConsumerRecords<String, String> pollConsumerRecords(String topic)
{    return pollConsumerRecords(topic, 20);}
0
private ConsumerRecords<String, String> pollConsumerRecords(String topic, int maxIter)
{    ConsumerRecords<String, String> recs = null;    for (int i = 0; i < maxIter; i++) {        recs = testUtil.getNextMessageFromConsumer(topic);        if (recs.count() > 0)            break;        try {            Thread.sleep(1000L);        } catch (InterruptedException e) {                }    }    return recs;}
0
public void testEmptyChannel() throws EventDeliveryException
{    Sink kafkaSink = new KafkaSink();    Context context = prepareDefaultContext();    Configurables.configure(kafkaSink, context);    Channel memoryChannel = new MemoryChannel();    Configurables.configure(memoryChannel, context);    kafkaSink.setChannel(memoryChannel);    kafkaSink.start();    Sink.Status status = kafkaSink.process();    if (status != Sink.Status.BACKOFF) {        fail("Error Occurred");    }    ConsumerRecords recs = pollConsumerRecords(DEFAULT_TOPIC, 2);    assertNotNull(recs);    assertEquals(recs.count(), 0);}
0
public void testPartitionHeaderSet() throws Exception
{    doPartitionHeader(PartitionTestScenario.PARTITION_ID_HEADER_ONLY);}
0
public void testPartitionHeaderNotSet() throws Exception
{    doPartitionHeader(PartitionTestScenario.NO_PARTITION_HEADERS);}
0
public void testStaticPartitionAndHeaderSet() throws Exception
{    doPartitionHeader(PartitionTestScenario.STATIC_HEADER_AND_PARTITION_ID);}
0
public void testStaticPartitionHeaderNotSet() throws Exception
{    doPartitionHeader(PartitionTestScenario.STATIC_HEADER_ONLY);}
0
public void testPartitionHeaderMissing() throws Exception
{    doPartitionErrors(PartitionOption.NOTSET);}
0
public void testPartitionHeaderOutOfRange() throws Exception
{    Sink kafkaSink = new KafkaSink();    try {        doPartitionErrors(PartitionOption.VALIDBUTOUTOFRANGE, kafkaSink);        fail();    } catch (EventDeliveryException e) {        }    SinkCounter sinkCounter = (SinkCounter) Whitebox.getInternalState(kafkaSink, "counter");    assertEquals(1, sinkCounter.getEventWriteFail());}
0
public void testPartitionHeaderInvalid() throws Exception
{    doPartitionErrors(PartitionOption.NOTANUMBER);}
0
public void testDefaultSettingsOnReConfigure()
{    String sampleProducerProp = "compression.type";    String sampleProducerVal = "snappy";    Context context = prepareDefaultContext();    context.put(KafkaSinkConstants.KAFKA_PRODUCER_PREFIX + sampleProducerProp, sampleProducerVal);    KafkaSink kafkaSink = new KafkaSink();    Configurables.configure(kafkaSink, context);    Assert.assertEquals(sampleProducerVal, kafkaSink.getKafkaProps().getProperty(sampleProducerProp));    context = prepareDefaultContext();    Configurables.configure(kafkaSink, context);    Assert.assertNull(kafkaSink.getKafkaProps().getProperty(sampleProducerProp));}
0
private void doPartitionErrors(PartitionOption option) throws Exception
{    doPartitionErrors(option, new KafkaSink());}
0
private void doPartitionErrors(PartitionOption option, Sink kafkaSink) throws Exception
{    Context context = prepareDefaultContext();    context.put(KafkaSinkConstants.PARTITION_HEADER_NAME, "partition-header");    Configurables.configure(kafkaSink, context);    Channel memoryChannel = new MemoryChannel();    Configurables.configure(memoryChannel, context);    kafkaSink.setChannel(memoryChannel);    kafkaSink.start();    String topic = findUnusedTopic();    createTopic(topic, 5);    Transaction tx = memoryChannel.getTransaction();    tx.begin();    Map<String, String> headers = new HashMap<String, String>();    headers.put("topic", topic);    switch(option) {        case VALIDBUTOUTOFRANGE:            headers.put("partition-header", "9");            break;        case NOTSET:            headers.put("wrong-header", "2");            break;        case NOTANUMBER:            headers.put("partition-header", "not-a-number");            break;        default:            break;    }    Event event = EventBuilder.withBody(String.valueOf(9).getBytes(), headers);    memoryChannel.put(event);    tx.commit();    tx.close();    Sink.Status status = kafkaSink.process();    assertEquals(Sink.Status.READY, status);    deleteTopic(topic);}
0
private void doPartitionHeader(PartitionTestScenario scenario) throws Exception
{    final int numPtns = 5;    final int numMsgs = numPtns * 10;    final Integer staticPtn = 3;    String topic = findUnusedTopic();    createTopic(topic, numPtns);    Context context = prepareDefaultContext();    context.put(BATCH_SIZE, "100");    if (scenario == PartitionTestScenario.PARTITION_ID_HEADER_ONLY || scenario == PartitionTestScenario.STATIC_HEADER_AND_PARTITION_ID) {        context.put(KafkaSinkConstants.PARTITION_HEADER_NAME, KafkaPartitionTestUtil.PARTITION_HEADER);    }    if (scenario == PartitionTestScenario.STATIC_HEADER_AND_PARTITION_ID || scenario == PartitionTestScenario.STATIC_HEADER_ONLY) {        context.put(KafkaSinkConstants.STATIC_PARTITION_CONF, staticPtn.toString());    }    Sink kafkaSink = new KafkaSink();    Configurables.configure(kafkaSink, context);    Channel memoryChannel = new MemoryChannel();    Configurables.configure(memoryChannel, context);    kafkaSink.setChannel(memoryChannel);    kafkaSink.start();        Map<Integer, List<Event>> partitionMap = new HashMap<Integer, List<Event>>(numPtns);    for (int i = 0; i < numPtns; i++) {        partitionMap.put(i, new ArrayList<Event>());    }    Transaction tx = memoryChannel.getTransaction();    tx.begin();    List<Event> orderedEvents = KafkaPartitionTestUtil.generateSkewedMessageList(scenario, numMsgs, partitionMap, numPtns, staticPtn);    for (Event event : orderedEvents) {        event.getHeaders().put("topic", topic);        memoryChannel.put(event);    }    tx.commit();    tx.close();    Sink.Status status = kafkaSink.process();    assertEquals(Sink.Status.READY, status);    Properties props = new Properties();    props.put("bootstrap.servers", testUtil.getKafkaServerUrl());    props.put("group.id", "group_1");    props.put("enable.auto.commit", "true");    props.put("auto.commit.interval.ms", "1000");    props.put("session.timeout.ms", "30000");    props.put("key.deserializer", "org.apache.kafka.common.serialization.StringDeserializer");    props.put("value.deserializer", "org.apache.kafka.common.serialization.ByteArrayDeserializer");    props.put("auto.offset.reset", "earliest");    Map<Integer, List<byte[]>> resultsMap = KafkaPartitionTestUtil.retrieveRecordsFromPartitions(topic, numPtns, props);    KafkaPartitionTestUtil.checkResultsAgainstSkew(scenario, partitionMap, resultsMap, staticPtn, numMsgs);    memoryChannel.stop();    kafkaSink.stop();    deleteTopic(topic);}
0
private Context prepareDefaultContext()
{        Context context = new Context();    context.put(BOOTSTRAP_SERVERS_CONFIG, testUtil.getKafkaServerUrl());    context.put(BATCH_SIZE, "1");    return context;}
0
private Sink.Status prepareAndSend(Context context, String msg) throws EventDeliveryException
{    Sink kafkaSink = new KafkaSink();    Configurables.configure(kafkaSink, context);    Channel memoryChannel = new MemoryChannel();    Configurables.configure(memoryChannel, context);    kafkaSink.setChannel(memoryChannel);    kafkaSink.start();    Transaction tx = memoryChannel.getTransaction();    tx.begin();    Event event = EventBuilder.withBody(msg.getBytes());    memoryChannel.put(event);    tx.commit();    tx.close();    return kafkaSink.process();}
0
private void createTopic(String topicName, int numPartitions)
{    testUtil.createTopics(Collections.singletonList(topicName), numPartitions);}
0
private void deleteTopic(String topicName)
{    testUtil.deleteTopic(topicName);}
0
private String findUnusedTopic()
{    String newTopic = null;    boolean topicFound = false;    while (!topicFound) {        newTopic = RandomStringUtils.randomAlphabetic(8);        if (!usedTopics.contains(newTopic)) {            usedTopics.add(newTopic);            topicFound = true;        }    }    return newTopic;}
0
public void start() throws Exception
{    kafka.startup();}
0
public void stop()
{    kafka.shutdown();}
0
public static TestUtil getInstance()
{    return instance;}
0
private void init()
{    try {        Properties settings = new Properties();        InputStream in = Class.class.getResourceAsStream("/testutil.properties");        if (in != null) {            settings.load(in);        }        externalServers = "true".equalsIgnoreCase(settings.getProperty("external-servers"));        if (externalServers) {            kafkaServerUrl = settings.getProperty("kafka-server-url");            zkServerUrl = settings.getProperty("zk-server-url");        } else {            String hostname = InetAddress.getLocalHost().getHostName();            zkLocalPort = getNextPort();            kafkaLocalPort = getNextPort();            kafkaServerUrl = hostname + ":" + kafkaLocalPort;            zkServerUrl = hostname + ":" + zkLocalPort;        }        clientProps = createClientProperties();    } catch (Exception e) {                throw new RuntimeException("Unexpected error", e);    }}
1
private boolean startEmbeddedKafkaServer()
{    Properties kafkaProperties = new Properties();    Properties zkProperties = new Properties();        try {                zkProperties.load(Class.class.getResourceAsStream("/zookeeper.properties"));                        zkProperties.setProperty("clientPort", Integer.toString(zkLocalPort));        new ZooKeeperLocal(zkProperties);                kafkaProperties.load(Class.class.getResourceAsStream("/kafka-server.properties"));                kafkaProperties.setProperty("zookeeper.connect", getZkUrl());                kafkaProperties.setProperty("port", Integer.toString(kafkaLocalPort));        kafkaServer = new KafkaLocal(kafkaProperties);        kafkaServer.start();                return true;    } catch (Exception e) {                return false;    }}
1
private AdminClient getAdminClient()
{    if (adminClient == null) {        Properties adminClientProps = createAdminClientProperties();        adminClient = AdminClient.create(adminClientProps);    }    return adminClient;}
0
private Properties createClientProperties()
{    final Properties props = createAdminClientProperties();    props.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class.getName());    props.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class.getName());    props.put("auto.commit.interval.ms", "1000");    props.put("auto.offset.reset", "earliest");    props.put("consumer.timeout.ms", "10000");    props.put("max.poll.interval.ms", "10000");        return props;}
0
private Properties createAdminClientProperties()
{    final Properties props = new Properties();    props.put(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, getKafkaServerUrl());    props.put(ConsumerConfig.GROUP_ID_CONFIG, "group_1");    return props;}
0
public void initTopicList(List<String> topics)
{    consumer = new KafkaConsumer<>(clientProps);    consumer.subscribe(topics);}
0
public void createTopics(List<String> topicNames, int numPartitions)
{    List<NewTopic> newTopics = new ArrayList<>();    for (String topicName : topicNames) {        NewTopic newTopic = new NewTopic(topicName, numPartitions, (short) 1);        newTopics.add(newTopic);    }    getAdminClient().createTopics(newTopics);        DescribeTopicsResult dtr = getAdminClient().describeTopics(topicNames);    try {        dtr.all().get(10, TimeUnit.SECONDS);    } catch (Exception e) {        throw new RuntimeException("Error getting topic info", e);    }}
0
public void deleteTopic(String topicName)
{    getAdminClient().deleteTopics(Collections.singletonList(topicName));}
0
public ConsumerRecords<String, String> getNextMessageFromConsumer(String topic)
{    return consumer.poll(Duration.ofMillis(1000L));}
0
public void prepare()
{    if (externalServers) {        return;    }    boolean startStatus = startEmbeddedKafkaServer();    if (!startStatus) {        throw new RuntimeException("Error starting the server!");    }    try {                Thread.sleep(3 * 1000);        } catch (InterruptedException e) {        }    }
1
public void tearDown()
{        if (consumer != null) {        consumer.close();    }    if (adminClient != null) {        adminClient.close();        adminClient = null;    }    try {                Thread.sleep(3 * 1000);        } catch (InterruptedException e) {        }    if (kafkaServer != null) {                kafkaServer.stop();    }    }
1
private synchronized int getNextPort() throws IOException
{    try (ServerSocket socket = new ServerSocket(0)) {        return socket.getLocalPort();    }}
0
public String getZkUrl()
{    return zkServerUrl;}
0
public String getKafkaServerUrl()
{    return kafkaServerUrl;}
0
public void run()
{    try {        zooKeeperServer.runFromConfig(configuration);    } catch (IOException e) {            }}
1
public Event readEvent() throws IOException
{    ensureOpen();    ByteArrayOutputStream blob = null;    byte[] buf = new byte[Math.min(maxBlobLength, DEFAULT_BUFFER_SIZE)];    int blobLength = 0;    int n = 0;    while ((n = in.read(buf, 0, Math.min(buf.length, maxBlobLength - blobLength))) != -1) {        if (blob == null) {            blob = new ByteArrayOutputStream(n);        }        blob.write(buf, 0, n);        blobLength += n;        if (blobLength >= maxBlobLength) {                        break;        }    }    if (blob == null) {        return null;    } else {        return EventBuilder.withBody(blob.toByteArray());    }}
1
public List<Event> readEvents(int numEvents) throws IOException
{    ensureOpen();    List<Event> events = Lists.newLinkedList();    for (int i = 0; i < numEvents; i++) {        Event event = readEvent();        if (event != null) {            events.add(event);        } else {            break;        }    }    return events;}
0
public void mark() throws IOException
{    ensureOpen();    in.mark();}
0
public void reset() throws IOException
{    ensureOpen();    in.reset();}
0
public void close() throws IOException
{    if (isOpen) {        reset();        in.close();        isOpen = false;    }}
0
private void ensureOpen()
{    if (!isOpen) {        throw new IllegalStateException("Serializer has been closed");    }}
0
public BlobDeserializer build(Context context, ResettableInputStream in)
{    return new BlobDeserializer(context, in);}
0
public void configure(Context context)
{    this.maxBlobLength = context.getInteger(MAX_BLOB_LENGTH_KEY, MAX_BLOB_LENGTH_DEFAULT);    if (this.maxBlobLength <= 0) {        throw new ConfigurationException("Configuration parameter " + MAX_BLOB_LENGTH_KEY + " must be greater than zero: " + maxBlobLength);    }}
0
public List<Event> getEvents(HttpServletRequest request) throws Exception
{    Map<String, String> headers = getHeaders(request);    InputStream in = request.getInputStream();    try {        ByteArrayOutputStream blob = null;        byte[] buf = new byte[Math.min(maxBlobLength, DEFAULT_BUFFER_SIZE)];        int blobLength = 0;        int n = 0;        while ((n = in.read(buf, 0, Math.min(buf.length, maxBlobLength - blobLength))) != -1) {            if (blob == null) {                blob = new ByteArrayOutputStream(n);            }            blob.write(buf, 0, n);            blobLength += n;            if (blobLength >= maxBlobLength) {                                break;            }        }        byte[] array = blob != null ? blob.toByteArray() : new byte[0];        Event event = EventBuilder.withBody(array, headers);        if (LOGGER.isDebugEnabled() && LogPrivacyUtil.allowLogRawData()) {                    }        return Collections.singletonList(event);    } finally {        in.close();    }}
1
private Map<String, String> getHeaders(HttpServletRequest request)
{    if (LOGGER.isDebugEnabled() && LogPrivacyUtil.allowLogRawData()) {        Map requestHeaders = new HashMap();        Enumeration iter = request.getHeaderNames();        while (iter.hasMoreElements()) {            String name = (String) iter.nextElement();            requestHeaders.put(name, request.getHeader(name));        }            }    Map<String, String> headers = new HashMap();    if (request.getContentType() != null) {        headers.put(Metadata.CONTENT_TYPE, request.getContentType());    }    Enumeration iter = request.getParameterNames();    while (iter.hasMoreElements()) {        String name = (String) iter.nextElement();        headers.put(name, request.getParameter(name));    }    return headers;}
1
 void setMorphlineContext(MorphlineContext morphlineContext)
{    this.morphlineContext = morphlineContext;}
0
 void setFinalChild(Command finalChild)
{    this.finalChild = finalChild;}
0
public void configure(Context context)
{    String morphlineFile = context.getString(MORPHLINE_FILE_PARAM);    String morphlineId = context.getString(MORPHLINE_ID_PARAM);    if (morphlineFile == null || morphlineFile.trim().length() == 0) {        throw new MorphlineCompilationException("Missing parameter: " + MORPHLINE_FILE_PARAM, null);    }    morphlineFileAndId = morphlineFile + "@" + morphlineId;    if (morphlineContext == null) {        FaultTolerance faultTolerance = new FaultTolerance(context.getBoolean(FaultTolerance.IS_PRODUCTION_MODE, false), context.getBoolean(FaultTolerance.IS_IGNORING_RECOVERABLE_EXCEPTIONS, false), context.getString(FaultTolerance.RECOVERABLE_EXCEPTION_CLASSES));        morphlineContext = new MorphlineContext.Builder().setExceptionHandler(faultTolerance).setMetricRegistry(SharedMetricRegistries.getOrCreate(morphlineFileAndId)).build();    }    Config override = ConfigFactory.parseMap(context.getSubProperties(MORPHLINE_VARIABLE_PARAM + "."));    morphline = new Compiler().compile(new File(morphlineFile), morphlineId, morphlineContext, finalChild, override);    this.mappingTimer = morphlineContext.getMetricRegistry().timer(MetricRegistry.name("morphline.app", Metrics.ELAPSED_TIME));    this.numRecords = morphlineContext.getMetricRegistry().meter(MetricRegistry.name("morphline.app", Metrics.NUM_RECORDS));    this.numFailedRecords = morphlineContext.getMetricRegistry().meter(MetricRegistry.name("morphline.app", "numFailedRecords"));    this.numExceptionRecords = morphlineContext.getMetricRegistry().meter(MetricRegistry.name("morphline.app", "numExceptionRecords"));}
0
public void process(Event event)
{    numRecords.mark();    Timer.Context timerContext = mappingTimer.time();    try {        Record record = new Record();        for (Entry<String, String> entry : event.getHeaders().entrySet()) {            record.put(entry.getKey(), entry.getValue());        }        byte[] bytes = event.getBody();        if (bytes != null && bytes.length > 0) {            record.put(Fields.ATTACHMENT_BODY, bytes);        }        try {            Notifications.notifyStartSession(morphline);            if (!morphline.process(record)) {                numFailedRecords.mark();                            }        } catch (RuntimeException t) {            numExceptionRecords.mark();            morphlineContext.getExceptionHandler().handleException(t, record);        }    } finally {        timerContext.stop();    }}
1
public void beginTransaction()
{    Notifications.notifyBeginTransaction(morphline);}
0
public void commitTransaction()
{    Notifications.notifyCommitTransaction(morphline);}
0
public void rollbackTransaction()
{    Notifications.notifyRollbackTransaction(morphline);}
0
public void stop()
{    Notifications.notifyShutdown(morphline);}
0
public void initialize()
{}
0
public void close()
{    LocalMorphlineInterceptor interceptor;    while ((interceptor = pool.poll()) != null) {        interceptor.close();    }}
0
public List<Event> intercept(List<Event> events)
{    LocalMorphlineInterceptor interceptor = borrowFromPool();    List<Event> results = interceptor.intercept(events);    returnToPool(interceptor);    return results;}
0
public Event intercept(Event event)
{    LocalMorphlineInterceptor interceptor = borrowFromPool();    Event result = interceptor.intercept(event);    returnToPool(interceptor);    return result;}
0
private void returnToPool(LocalMorphlineInterceptor interceptor)
{    pool.add(interceptor);}
0
private LocalMorphlineInterceptor borrowFromPool()
{    LocalMorphlineInterceptor interceptor = pool.poll();    if (interceptor == null) {        interceptor = new LocalMorphlineInterceptor(context);    }    return interceptor;}
0
public MorphlineInterceptor build()
{    return new MorphlineInterceptor(context);}
0
public void configure(Context context)
{    this.context = context;}
0
public void initialize()
{}
0
public void close()
{    morphline.stop();}
0
public List<Event> intercept(List<Event> events)
{    List results = new ArrayList(events.size());    for (Event event : events) {        event = intercept(event);        if (event != null) {            results.add(event);        }    }    return results;}
0
public Event intercept(Event event)
{    collector.reset();    morphline.process(event);    List<Record> results = collector.getRecords();    if (results.size() == 0) {        return null;    }    if (results.size() > 1) {        throw new FlumeException(getClass().getName() + " must not generate more than one output record per input event");    }    Event result = toEvent(results.get(0));    return result;}
0
private Event toEvent(Record record)
{    Map<String, String> headers = new HashMap();    Map<String, Collection<Object>> recordMap = record.getFields().asMap();    byte[] body = null;    for (Map.Entry<String, Collection<Object>> entry : recordMap.entrySet()) {        if (entry.getValue().size() > 1) {            throw new FlumeException(getClass().getName() + " must not generate more than one output value per record field");        }                assert entry.getValue().size() != 0;        Object firstValue = entry.getValue().iterator().next();        if (Fields.ATTACHMENT_BODY.equals(entry.getKey())) {            if (firstValue instanceof byte[]) {                body = (byte[]) firstValue;            } else if (firstValue instanceof InputStream) {                try {                    body = ByteStreams.toByteArray((InputStream) firstValue);                } catch (IOException e) {                    throw new FlumeException(e);                }            } else {                throw new FlumeException(getClass().getName() + " must non generate attachments that are not a byte[] or InputStream");            }        } else {            headers.put(entry.getKey(), firstValue.toString());        }    }    return EventBuilder.withBody(body, headers);}
0
public List<Record> getRecords()
{    return results;}
0
public void reset()
{    results.clear();}
0
public Command getParent()
{    return null;}
0
public void notify(Record notification)
{}
0
public boolean process(Record record)
{    Preconditions.checkNotNull(record);    results.add(record);    return true;}
0
public void configure(Context context)
{    this.context = context;    maxBatchSize = context.getInteger(BATCH_SIZE, maxBatchSize);    maxBatchDurationMillis = context.getLong(BATCH_DURATION_MILLIS, maxBatchDurationMillis);    handlerClass = context.getString(HANDLER_CLASS, MorphlineHandlerImpl.class.getName());    if (sinkCounter == null) {        sinkCounter = new SinkCounter(getName());    }}
0
private int getMaxBatchSize()
{    return maxBatchSize;}
0
private long getMaxBatchDurationMillis()
{    return maxBatchDurationMillis;}
0
public synchronized void start()
{        sinkCounter.start();    if (handler == null) {        MorphlineHandler tmpHandler;        try {            tmpHandler = (MorphlineHandler) Class.forName(handlerClass).newInstance();        } catch (Exception e) {            throw new ConfigurationException(e);        }        tmpHandler.configure(context);        handler = tmpHandler;    }    super.start();    }
1
public synchronized void stop()
{        try {        if (handler != null) {            handler.stop();        }        sinkCounter.stop();            } finally {        super.stop();    }}
1
public Status process() throws EventDeliveryException
{    int batchSize = getMaxBatchSize();    long batchEndTime = System.currentTimeMillis() + getMaxBatchDurationMillis();    Channel myChannel = getChannel();    Transaction txn = myChannel.getTransaction();    txn.begin();    boolean isMorphlineTransactionCommitted = true;    try {        int numEventsTaken = 0;        handler.beginTransaction();        isMorphlineTransactionCommitted = false;                for (int i = 0; i < batchSize; i++) {            Event event = myChannel.take();            if (event == null) {                break;            }            sinkCounter.incrementEventDrainAttemptCount();            numEventsTaken++;            if (LOGGER.isTraceEnabled() && LogPrivacyUtil.allowLogRawData()) {                LOGGER.trace("Flume event arrived {}", event);            }                        handler.process(event);            if (System.currentTimeMillis() >= batchEndTime) {                break;            }        }                if (numEventsTaken == 0) {            sinkCounter.incrementBatchEmptyCount();        }        if (numEventsTaken < batchSize) {            sinkCounter.incrementBatchUnderflowCount();        } else {            sinkCounter.incrementBatchCompleteCount();        }        handler.commitTransaction();        isMorphlineTransactionCommitted = true;        txn.commit();        sinkCounter.addToEventDrainSuccessCount(numEventsTaken);        return numEventsTaken == 0 ? Status.BACKOFF : Status.READY;    } catch (Throwable t) {                        sinkCounter.incrementEventWriteOrChannelFail(t);        try {            if (!isMorphlineTransactionCommitted) {                handler.rollbackTransaction();            }        } catch (Throwable t2) {                    } finally {            try {                txn.rollback();            } catch (Throwable t4) {                            }        }        if (t instanceof Error) {                        throw (Error) t;        } else if (t instanceof ChannelException) {            return Status.BACKOFF;        } else {                        throw new EventDeliveryException("Failed to send events", t);        }    } finally {        txn.close();    }}
1
public long getBatchSize()
{    return getMaxBatchSize();}
0
public String toString()
{    int i = getClass().getName().lastIndexOf('.') + 1;    String shortClassName = getClass().getName().substring(i);    return getName() + " (" + shortClassName + ")";}
0
public void configure(Context context)
{    if (context.getString(FaultTolerance.RECOVERABLE_EXCEPTION_CLASSES) == null) {        context.put(FaultTolerance.RECOVERABLE_EXCEPTION_CLASSES, "org.apache.solr.client.solrj.SolrServerException");    }    super.configure(context);}
0
public void initialize()
{}
0
protected String getPrefix()
{    return prefix;}
0
protected String generateUUID()
{    return getPrefix() + UUID.randomUUID().toString();}
0
protected boolean isMatch(Event event)
{    return true;}
0
public Event intercept(Event event)
{    Map<String, String> headers = event.getHeaders();    if (preserveExisting && headers.containsKey(headerName)) {        } else if (isMatch(event)) {        headers.put(headerName, generateUUID());    }    return event;}
0
public List<Event> intercept(List<Event> events)
{    List results = new ArrayList(events.size());    for (Event event : events) {        event = intercept(event);        if (event != null) {            results.add(event);        }    }    return results;}
0
public void close()
{}
0
public UUIDInterceptor build()
{    return new UUIDInterceptor(context);}
0
public void configure(Context context)
{    this.context = context;}
0
public void load(Event event) throws EventDeliveryException
{    getChannelProcessor().processEvent(event);    sink.process();}
0
public void load(List<Event> events) throws EventDeliveryException
{    getChannelProcessor().processEventBatch(events);    sink.process();}
0
public boolean isFinished()
{    throw new UnsupportedOperationException("Not supported yet.");}
0
public boolean isReady()
{    throw new UnsupportedOperationException("Not supported yet.");}
0
public void setReadListener(ReadListener readListener)
{    throw new UnsupportedOperationException("Not supported yet.");}
0
public int read() throws IOException
{    return in.read();}
0
public String getAuthType()
{    throw new UnsupportedOperationException("Not supported yet.");}
0
public Cookie[] getCookies()
{    throw new UnsupportedOperationException("Not supported yet.");}
0
public long getDateHeader(String name)
{    throw new UnsupportedOperationException("Not supported yet.");}
0
public String getHeader(String name)
{    throw new UnsupportedOperationException("Not supported yet.");}
0
public Enumeration getHeaders(String name)
{    throw new UnsupportedOperationException("Not supported yet.");}
0
public Enumeration getHeaderNames()
{    return Collections.enumeration(Collections.EMPTY_LIST);}
0
public int getIntHeader(String name)
{    throw new UnsupportedOperationException("Not supported yet.");}
0
public String getMethod()
{    throw new UnsupportedOperationException("Not supported yet.");}
0
public String getPathInfo()
{    throw new UnsupportedOperationException("Not supported yet.");}
0
public String getPathTranslated()
{    throw new UnsupportedOperationException("Not supported yet.");}
0
public String getContextPath()
{    throw new UnsupportedOperationException("Not supported yet.");}
0
public String getQueryString()
{    throw new UnsupportedOperationException("Not supported yet.");}
0
public String getRemoteUser()
{    throw new UnsupportedOperationException("Not supported yet.");}
0
public boolean isUserInRole(String role)
{    throw new UnsupportedOperationException("Not supported yet.");}
0
public Principal getUserPrincipal()
{    throw new UnsupportedOperationException("Not supported yet.");}
0
public String getRequestedSessionId()
{    throw new UnsupportedOperationException("Not supported yet.");}
0
public String getRequestURI()
{    throw new UnsupportedOperationException("Not supported yet.");}
0
public StringBuffer getRequestURL()
{    throw new UnsupportedOperationException("Not supported yet.");}
0
public String getServletPath()
{    throw new UnsupportedOperationException("Not supported yet.");}
0
public HttpSession getSession(boolean create)
{    throw new UnsupportedOperationException("Not supported yet.");}
0
public HttpSession getSession()
{    throw new UnsupportedOperationException("Not supported yet.");}
0
public String changeSessionId()
{    return null;}
0
public boolean isRequestedSessionIdValid()
{    throw new UnsupportedOperationException("Not supported yet.");}
0
public boolean isRequestedSessionIdFromCookie()
{    throw new UnsupportedOperationException("Not supported yet.");}
0
public boolean isRequestedSessionIdFromURL()
{    throw new UnsupportedOperationException("Not supported yet.");}
0
public boolean isRequestedSessionIdFromUrl()
{    throw new UnsupportedOperationException("Not supported yet.");}
0
public boolean authenticate(HttpServletResponse response) throws IOException, ServletException
{    throw new UnsupportedOperationException("Not supported yet.");}
0
public void login(String username, String password) throws ServletException
{    throw new UnsupportedOperationException("Not supported yet.");}
0
public void logout() throws ServletException
{    throw new UnsupportedOperationException("Not supported yet.");}
0
public Collection<Part> getParts() throws IOException, ServletException
{    throw new UnsupportedOperationException("Not supported yet.");}
0
public Part getPart(String name) throws IOException, ServletException
{    throw new UnsupportedOperationException("Not supported yet.");}
0
public T upgrade(Class<T> handlerClass) throws IOException, ServletException
{    throw new UnsupportedOperationException("Not supported yet.");}
0
public Object getAttribute(String name)
{    throw new UnsupportedOperationException("Not supported yet.");}
0
public Enumeration<String> getAttributeNames()
{    throw new UnsupportedOperationException("Not supported yet.");}
0
public String getCharacterEncoding()
{    return charset;}
0
public void setCharacterEncoding(String env) throws UnsupportedEncodingException
{    this.charset = env;}
0
public int getContentLength()
{    throw new UnsupportedOperationException("Not supported yet.");}
0
public long getContentLengthLong()
{    return 0;}
0
public String getContentType()
{    return null;}
0
public ServletInputStream getInputStream() throws IOException
{    return stream;}
0
public String getParameter(String name)
{    throw new UnsupportedOperationException("Not supported yet.");}
0
public Enumeration getParameterNames()
{    return Collections.enumeration(Collections.EMPTY_LIST);}
0
public String[] getParameterValues(String name)
{    throw new UnsupportedOperationException("Not supported yet.");}
0
public Map getParameterMap()
{    throw new UnsupportedOperationException("Not supported yet.");}
0
public String getProtocol()
{    throw new UnsupportedOperationException("Not supported yet.");}
0
public String getScheme()
{    throw new UnsupportedOperationException("Not supported yet.");}
0
public String getServerName()
{    throw new UnsupportedOperationException("Not supported yet.");}
0
public int getServerPort()
{    throw new UnsupportedOperationException("Not supported yet.");}
0
public BufferedReader getReader() throws IOException
{    throw new UnsupportedOperationException("Not supported yet.");}
0
public String getRemoteAddr()
{    throw new UnsupportedOperationException("Not supported yet.");}
0
public String getRemoteHost()
{    throw new UnsupportedOperationException("Not supported yet.");}
0
public void setAttribute(String name, Object o)
{    throw new UnsupportedOperationException("Not supported yet.");}
0
public void removeAttribute(String name)
{    throw new UnsupportedOperationException("Not supported yet.");}
0
public Locale getLocale()
{    throw new UnsupportedOperationException("Not supported yet.");}
0
public Enumeration<Locale> getLocales()
{    throw new UnsupportedOperationException("Not supported yet.");}
0
public boolean isSecure()
{    throw new UnsupportedOperationException("Not supported yet.");}
0
public RequestDispatcher getRequestDispatcher(String path)
{    throw new UnsupportedOperationException("Not supported yet.");}
0
public String getRealPath(String path)
{    throw new UnsupportedOperationException("Not supported yet.");}
0
public int getRemotePort()
{    throw new UnsupportedOperationException("Not supported yet.");}
0
public String getLocalName()
{    throw new UnsupportedOperationException("Not supported yet.");}
0
public String getLocalAddr()
{    throw new UnsupportedOperationException("Not supported yet.");}
0
public int getLocalPort()
{    throw new UnsupportedOperationException("Not supported yet.");}
0
public ServletContext getServletContext()
{    throw new UnsupportedOperationException("Not supported yet.");}
0
public AsyncContext startAsync() throws IllegalStateException
{    throw new UnsupportedOperationException("Not supported yet.");}
0
public AsyncContext startAsync(ServletRequest servletRequest, ServletResponse servletResponse) throws IllegalStateException
{    throw new UnsupportedOperationException("Not supported yet.");}
0
public boolean isAsyncStarted()
{    throw new UnsupportedOperationException("Not supported yet.");}
0
public boolean isAsyncSupported()
{    throw new UnsupportedOperationException("Not supported yet.");}
0
public AsyncContext getAsyncContext()
{    throw new UnsupportedOperationException("Not supported yet.");}
0
public DispatcherType getDispatcherType()
{    throw new UnsupportedOperationException("Not supported yet.");}
0
public int readChar() throws IOException
{    throw new UnsupportedOperationException("This test class doesn't return " + "strings!");}
0
public void mark() throws IOException
{    markPos = curPos;}
0
public void reset() throws IOException
{    curPos = markPos;}
0
public void seek(long position) throws IOException
{    throw new UnsupportedOperationException("Unimplemented in test class");}
0
public long tell() throws IOException
{    throw new UnsupportedOperationException("Unimplemented in test class");}
0
public int read() throws IOException
{    if (curPos >= str.length()) {        return -1;    }    return str.charAt(curPos++);}
0
public int read(byte[] b, int off, int len) throws IOException
{    if (curPos >= str.length()) {        return -1;    }    int n = 0;    while (len > 0 && curPos < str.length()) {        b[off++] = (byte) str.charAt(curPos++);        n++;        len--;    }    return n;}
0
public void close() throws IOException
{}
0
public void setup()
{    StringBuilder sb = new StringBuilder();    sb.append("line 1\n");    sb.append("line 2\n");    mini = sb.toString();}
0
public void testSimple() throws IOException
{    ResettableInputStream in = new ResettableTestStringInputStream(mini);    EventDeserializer des = new BlobDeserializer(new Context(), in);    validateMiniParse(des);}
0
public void testSimpleViaBuilder() throws IOException
{    ResettableInputStream in = new ResettableTestStringInputStream(mini);    EventDeserializer.Builder builder = new BlobDeserializer.Builder();    EventDeserializer des = builder.build(new Context(), in);    validateMiniParse(des);}
0
public void testSimpleViaFactory() throws IOException
{    ResettableInputStream in = new ResettableTestStringInputStream(mini);    EventDeserializer des;    des = EventDeserializerFactory.getInstance(BlobDeserializer.Builder.class.getName(), new Context(), in);    validateMiniParse(des);}
0
public void testBatch() throws IOException
{    ResettableInputStream in = new ResettableTestStringInputStream(mini);    EventDeserializer des = new BlobDeserializer(new Context(), in);    List<Event> events;        events = des.readEvents(10);    assertEquals(1, events.size());    assertEventBodyEquals(mini, events.get(0));    des.mark();    des.close();}
0
public void testMaxLineLength() throws IOException
{    String longLine = "abcdefghijklmnopqrstuvwxyz\n";    Context ctx = new Context();    ctx.put(BlobDeserializer.MAX_BLOB_LENGTH_KEY, "10");    ResettableInputStream in = new ResettableTestStringInputStream(longLine);    EventDeserializer des = new BlobDeserializer(ctx, in);    assertEventBodyEquals("abcdefghij", des.readEvent());    assertEventBodyEquals("klmnopqrst", des.readEvent());    assertEventBodyEquals("uvwxyz\n", des.readEvent());    assertNull(des.readEvent());}
0
private void assertEventBodyEquals(String expected, Event event)
{    String bodyStr = new String(event.getBody(), Charsets.UTF_8);    assertEquals(expected, bodyStr);}
0
private void validateMiniParse(EventDeserializer des) throws IOException
{    Event evt;    des.mark();    evt = des.readEvent();    assertEquals(new String(evt.getBody()), mini);        des.reset();    evt = des.readEvent();    assertEquals("data should be repeated, " + "because we reset() the stream", new String(evt.getBody()), mini);    evt = des.readEvent();    assertNull("Event should be null because there are no lines " + "left to read", evt);    des.mark();    des.close();}
0
public void setUp()
{    handler = new BlobHandler();}
0
public void testSingleEvent() throws Exception
{    byte[] json = "foo".getBytes("UTF-8");    HttpServletRequest req = new FlumeHttpServletRequestWrapper(json);    List<Event> deserialized = handler.getEvents(req);    assertEquals(1, deserialized.size());    Event e = deserialized.get(0);    assertEquals(0, e.getHeaders().size());    assertEquals("foo", new String(e.getBody(), "UTF-8"));}
0
public void testEmptyEvent() throws Exception
{    byte[] json = "".getBytes("UTF-8");    HttpServletRequest req = new FlumeHttpServletRequestWrapper(json);    List<Event> deserialized = handler.getEvents(req);    assertEquals(1, deserialized.size());    Event e = deserialized.get(0);    assertEquals(0, e.getHeaders().size());    assertEquals("", new String(e.getBody(), "UTF-8"));}
0
public void testEnvironment() throws UnknownHostException
{    super.testEnvironment();}
0
public void testNoOperation() throws Exception
{    Context context = new Context();    context.put(MorphlineHandlerImpl.MORPHLINE_FILE_PARAM, RESOURCES_DIR + "/test-morphlines/noOperation.conf");    Event input = EventBuilder.withBody("foo", Charsets.UTF_8);    input.getHeaders().put("name", "nadja");    MorphlineInterceptor interceptor = build(context);    Event actual = interceptor.intercept(input);    interceptor.close();    Event expected = EventBuilder.withBody("foo".getBytes("UTF-8"), ImmutableMap.of("name", "nadja"));    assertEqualsEvent(expected, actual);    List<Event> actualList = build(context).intercept(Collections.singletonList(input));    List<Event> expectedList = Collections.singletonList(expected);    assertEqualsEventList(expectedList, actualList);}
0
public void testReadClob() throws Exception
{    Context context = new Context();    context.put(MorphlineHandlerImpl.MORPHLINE_FILE_PARAM, RESOURCES_DIR + "/test-morphlines/readClob.conf");    Event input = EventBuilder.withBody("foo", Charsets.UTF_8);    input.getHeaders().put("name", "nadja");    Event actual = build(context).intercept(input);    Event expected = EventBuilder.withBody(null, ImmutableMap.of("name", "nadja", Fields.MESSAGE, "foo"));    assertEqualsEvent(expected, actual);    List<Event> actualList = build(context).intercept(Collections.singletonList(input));    List<Event> expectedList = Collections.singletonList(expected);    assertEqualsEventList(expectedList, actualList);}
0
public void testGrokIfNotMatchDropEventRetain() throws Exception
{    Context context = new Context();    context.put(MorphlineHandlerImpl.MORPHLINE_FILE_PARAM, RESOURCES_DIR + "/test-morphlines/grokIfNotMatchDropRecord.conf");    String msg = "<164>Feb  4 10:46:14 syslog sshd[607]: Server listening on 0.0.0.0 port 22.";    Event input = EventBuilder.withBody(null, ImmutableMap.of(Fields.MESSAGE, msg));    Event actual = build(context).intercept(input);    Map<String, String> expected = new HashMap();    expected.put(Fields.MESSAGE, msg);    expected.put("syslog_pri", "164");    expected.put("syslog_timestamp", "Feb  4 10:46:14");    expected.put("syslog_hostname", "syslog");    expected.put("syslog_program", "sshd");    expected.put("syslog_pid", "607");    expected.put("syslog_message", "Server listening on 0.0.0.0 port 22.");    Event expectedEvent = EventBuilder.withBody(null, expected);    assertEqualsEvent(expectedEvent, actual);}
0
public void testGrokIfNotMatchDropEventDrop() throws Exception
{    Context context = new Context();    context.put(MorphlineHandlerImpl.MORPHLINE_FILE_PARAM, RESOURCES_DIR + "/test-morphlines/grokIfNotMatchDropRecord.conf");    String msg = "<XXXXXXXXXXXXX164>Feb  4 10:46:14 syslog sshd[607]: Server listening on 0.0.0.0" + " port 22.";    Event input = EventBuilder.withBody(null, ImmutableMap.of(Fields.MESSAGE, msg));    Event actual = build(context).intercept(input);    assertNull(actual);}
0
public void testIfDetectMimeTypeRouteToSouthPole() throws Exception
{    Context context = new Context();    context.put(MorphlineHandlerImpl.MORPHLINE_FILE_PARAM, RESOURCES_DIR + "/test-morphlines/ifDetectMimeType.conf");    context.put(MorphlineHandlerImpl.MORPHLINE_VARIABLE_PARAM + ".MY.MIME_TYPE", "avro/binary");    Event input = EventBuilder.withBody(Files.toByteArray(new File(RESOURCES_DIR + "/test-documents/sample-statuses-20120906-141433.avro")));    Event actual = build(context).intercept(input);    Map<String, String> expected = new HashMap();    expected.put(Fields.ATTACHMENT_MIME_TYPE, "avro/binary");    expected.put("flume.selector.header", "goToSouthPole");    Event expectedEvent = EventBuilder.withBody(input.getBody(), expected);    assertEqualsEvent(expectedEvent, actual);}
0
public void testIfDetectMimeTypeRouteToNorthPole() throws Exception
{    Context context = new Context();    context.put(MorphlineHandlerImpl.MORPHLINE_FILE_PARAM, RESOURCES_DIR + "/test-morphlines/ifDetectMimeType.conf");    context.put(MorphlineHandlerImpl.MORPHLINE_VARIABLE_PARAM + ".MY.MIME_TYPE", "avro/binary");    Event input = EventBuilder.withBody(Files.toByteArray(new File(RESOURCES_DIR + "/test-documents/testPDF.pdf")));    Event actual = build(context).intercept(input);    Map<String, String> expected = new HashMap();    expected.put(Fields.ATTACHMENT_MIME_TYPE, "application/pdf");    expected.put("flume.selector.header", "goToNorthPole");    Event expectedEvent = EventBuilder.withBody(input.getBody(), expected);    assertEqualsEvent(expectedEvent, actual);}
0
private MorphlineInterceptor build(Context context)
{    MorphlineInterceptor.Builder builder = new MorphlineInterceptor.Builder();    builder.configure(context);    return builder.build();}
0
private void assertEqualsEvent(Event x, Event y)
{    assertEquals(x.getHeaders(), y.getHeaders());    assertArrayEquals(x.getBody(), y.getBody());}
0
private void assertEqualsEventList(List<Event> x, List<Event> y)
{    assertEquals(x.size(), y.size());    for (int i = 0; i < x.size(); i++) {        assertEqualsEvent(x.get(i), y.get(i));    }}
0
public static void beforeClass() throws Exception
{    initCore(RESOURCES_DIR + "/solr/collection1/conf/solrconfig.xml", RESOURCES_DIR + "/solr/collection1/conf/schema.xml", RESOURCES_DIR + "/solr");}
0
public void setUp() throws Exception
{    super.setUp();    String path = RESOURCES_DIR + "/test-documents";    expectedRecords = new HashMap();    expectedRecords.put(path + "/sample-statuses-20120906-141433.avro", 2);    expectedRecords.put(path + "/sample-statuses-20120906-141433", 2);    expectedRecords.put(path + "/sample-statuses-20120906-141433.gz", 2);    expectedRecords.put(path + "/sample-statuses-20120906-141433.bz2", 2);    expectedRecords.put(path + "/cars.csv", 5);    expectedRecords.put(path + "/cars.csv.gz", 5);    expectedRecords.put(path + "/cars.tar.gz", 4);    expectedRecords.put(path + "/cars.tsv", 5);    expectedRecords.put(path + "/cars.ssv", 5);    final Map<String, String> context = new HashMap();    if (EXTERNAL_SOLR_SERVER_URL != null) {        throw new UnsupportedOperationException();                } else {        if (TEST_WITH_EMBEDDED_SOLR_SERVER) {            solrServer = new TestEmbeddedSolrServer(h.getCoreContainer(), "");        } else {            throw new RuntimeException("Not yet implemented");                }    }    Map<String, String> channelContext = new HashMap();    channelContext.put("capacity", "1000000");        channelContext.put("keep-alive", "0");    Channel channel = new MemoryChannel();    channel.setName(channel.getClass().getName() + SEQ_NUM.getAndIncrement());    Configurables.configure(channel, new Context(channelContext));    class MySolrSink extends MorphlineSolrSink {        public MySolrSink(MorphlineHandlerImpl indexer) {            super(indexer);        }    }    int batchSize = SEQ_NUM2.incrementAndGet() % 2 == 0 ? 100 : 1;    DocumentLoader testServer = new SolrServerDocumentLoader(solrServer, batchSize);    MorphlineContext solrMorphlineContext = new SolrMorphlineContext.Builder().setDocumentLoader(testServer).setExceptionHandler(new FaultTolerance(false, false, SolrServerException.class.getName())).setMetricRegistry(new MetricRegistry()).build();    MorphlineHandlerImpl impl = new MorphlineHandlerImpl();    impl.setMorphlineContext(solrMorphlineContext);    class MySolrLocator extends     SolrLocator {        public MySolrLocator(MorphlineContext indexer) {            super(indexer);        }    }    SolrLocator locator = new MySolrLocator(solrMorphlineContext);    locator.setSolrHomeDir(testSolrHome + "/collection1");    String str1 = "SOLR_LOCATOR : " + locator.toString();            File morphlineFile = new File("target/test-classes/test-morphlines/solrCellDocumentTypes.conf");    String str2 = Files.toString(morphlineFile, Charsets.UTF_8);    tmpFile = File.createTempFile("morphline", ".conf");    tmpFile.deleteOnExit();    Files.write(str1 + "\n" + str2, tmpFile, Charsets.UTF_8);    context.put("morphlineFile", tmpFile.getPath());    impl.configure(new Context(context));    sink = new MySolrSink(impl);    sink.setName(sink.getClass().getName() + SEQ_NUM.getAndIncrement());    sink.configure(new Context(context));    sink.setChannel(channel);    sink.start();    source = new EmbeddedSource(sink);    ChannelSelector rcs = new ReplicatingChannelSelector();    rcs.setChannels(Collections.singletonList(channel));    ChannelProcessor chp = new ChannelProcessor(rcs);    Context chpContext = new Context();    chpContext.put("interceptors", "uuidinterceptor");    chpContext.put("interceptors.uuidinterceptor.type", UUIDInterceptor.Builder.class.getName());    chp.configure(chpContext);    source.setChannelProcessor(chp);    deleteAllDocuments();}
0
private void deleteAllDocuments() throws SolrServerException, IOException
{    SolrServer s = solrServer;        s.deleteByQuery("*:*");    s.commit();}
0
public void tearDown() throws Exception
{    try {        if (source != null) {            source.stop();            source = null;        }        if (sink != null) {            sink.stop();            sink = null;        }        if (tmpFile != null) {            tmpFile.delete();        }    } finally {        solrServer = null;        expectedRecords = null;        super.tearDown();    }}
0
public void testDocumentTypes() throws Exception
{    String path = RESOURCES_DIR + "/test-documents";    String[] files = new String[] { path + "/testBMPfp.txt", path + "/boilerplate.html", path + "/NullHeader.docx", path + "/testWORD_various.doc", path + "/testPDF.pdf", path + "/testJPEG_EXIF.jpg", path + "/testXML.xml",     path + "/sample-statuses-20120906-141433.avro", path + "/sample-statuses-20120906-141433", path + "/sample-statuses-20120906-141433.gz", path + "/sample-statuses-20120906-141433.bz2" };    testDocumentTypesInternal(files);}
0
public void testDocumentTypes2() throws Exception
{    String path = RESOURCES_DIR + "/test-documents";    String[] files = new String[] { path + "/testPPT_various.ppt", path + "/testPPT_various.pptx", path + "/testEXCEL.xlsx", path + "/testEXCEL.xls", path + "/testPages.pages", path + "/testNumbers.numbers", path + "/testKeynote.key", path + "/testRTFVarious.rtf", path + "/complex.mbox", path + "/test-outlook.msg", path + "/testEMLX.emlx",     path + "/rsstest.rss", path + "/testMP3i18n.mp3", path + "/testAIFF.aif", path + "/testFLAC.flac",     path + "/testMP4.m4a", path + "/testWAV.wav", path + "/testFLV.flv", path + "/testBMP.bmp", path + "/testPNG.png", path + "/testPSD.psd", path + "/testSVG.svg", path + "/testTIFF.tif",     path + "/testTrueType.ttf", path + "/testVISIO.vsd"                 };    testDocumentTypesInternal(files);}
0
public void testErrorCounters() throws Exception
{    Channel channel = Mockito.mock(Channel.class);    Mockito.when(channel.take()).thenThrow(new ChannelException("dummy"));    Transaction transaction = Mockito.mock(BasicTransactionSemantics.class);    Mockito.when(channel.getTransaction()).thenReturn(transaction);    sink.setChannel(channel);    sink.process();    SinkCounter sinkCounter = (SinkCounter) Whitebox.getInternalState(sink, "sinkCounter");    assertEquals(1, sinkCounter.getChannelReadFail());}
0
public void testAvroRoundTrip() throws Exception
{    String file = RESOURCES_DIR + "/test-documents" + "/sample-statuses-20120906-141433.avro";    testDocumentTypesInternal(file);    QueryResponse rsp = query("*:*");    Iterator<SolrDocument> iter = rsp.getResults().iterator();    ListMultimap<String, String> expectedFieldValues;    expectedFieldValues = ImmutableListMultimap.of("id", "1234567890", "text", "sample tweet one", "user_screen_name", "fake_user1");    assertEquals(expectedFieldValues, next(iter));    expectedFieldValues = ImmutableListMultimap.of("id", "2345678901", "text", "sample tweet two", "user_screen_name", "fake_user2");    assertEquals(expectedFieldValues, next(iter));    assertFalse(iter.hasNext());}
0
private ListMultimap<String, Object> next(Iterator<SolrDocument> iter)
{    SolrDocument doc = iter.next();    Record record = toRecord(doc);        record.removeAll("_version_");    return record.getFields();}
0
private Record toRecord(SolrDocument doc)
{    Record record = new Record();    for (String key : doc.keySet()) {        record.getFields().replaceValues(key, doc.getFieldValues(key));    }    return record;}
0
private void testDocumentTypesInternal(String... files) throws Exception
{    int numDocs = 0;    long startTime = System.currentTimeMillis();    assertEquals(numDocs, queryResultSetSize("*:*"));        for (int i = 0; i < 1; i++) {        for (String file : files) {            File f = new File(file);            byte[] body = Files.toByteArray(f);            Event event = EventBuilder.withBody(body);            event.getHeaders().put(Fields.ATTACHMENT_NAME, f.getName());            load(event);            Integer count = expectedRecords.get(file);            if (count != null) {                numDocs += count;            } else {                numDocs++;            }            assertEquals(numDocs, queryResultSetSize("*:*"));        }        LOGGER.trace("iter: {}", i);    }    LOGGER.trace("all done with put at {}", System.currentTimeMillis() - startTime);    assertEquals(numDocs, queryResultSetSize("*:*"));    LOGGER.trace("sink: ", sink);}
0
public void benchmarkDocumentTypes() throws Exception
{    int iters = 200;        assertEquals(0, queryResultSetSize("*:*"));    String path = RESOURCES_DIR + "/test-documents";    String[] files = new String[] {     path + "/sample-statuses-20120906-141433-medium.avro" };    List<Event> events = new ArrayList();    for (String file : files) {        File f = new File(file);        byte[] body = Files.toByteArray(f);        Event event = EventBuilder.withBody(body);                events.add(event);    }    long startTime = System.currentTimeMillis();    for (int i = 0; i < iters; i++) {        if (i % 10000 == 0) {                    }        for (Event event : events) {            event = EventBuilder.withBody(event.getBody(), new HashMap(event.getHeaders()));            event.getHeaders().put("id", UUID.randomUUID().toString());            load(event);        }    }    float secs = (System.currentTimeMillis() - startTime) / 1000.0f;    long numDocs = queryResultSetSize("*:*");                }
1
private void load(Event event) throws EventDeliveryException
{    source.load(event);}
0
private void commit() throws SolrServerException, IOException
{    solrServer.commit(false, true, true);}
0
private int queryResultSetSize(String query) throws SolrServerException, IOException
{    commit();    QueryResponse rsp = query(query);        int size = rsp.getResults().size();    return size;}
1
private QueryResponse query(String query) throws SolrServerException, IOException
{    commit();    QueryResponse rsp = solrServer.query(new SolrQuery(query).setRows(Integer.MAX_VALUE));        return rsp;}
1
public void testBasic() throws Exception
{    Context context = new Context();    context.put(UUIDInterceptor.HEADER_NAME, ID);    context.put(UUIDInterceptor.PRESERVE_EXISTING_NAME, "true");    Event event = new SimpleEvent();    assertTrue(build(context).intercept(event).getHeaders().get(ID).length() > 0);}
0
public void testPreserveExisting() throws Exception
{    Context context = new Context();    context.put(UUIDInterceptor.HEADER_NAME, ID);    context.put(UUIDInterceptor.PRESERVE_EXISTING_NAME, "true");    Event event = new SimpleEvent();    event.getHeaders().put(ID, "foo");    assertEquals("foo", build(context).intercept(event).getHeaders().get(ID));}
0
public void testPrefix() throws Exception
{    Context context = new Context();    context.put(UUIDInterceptor.HEADER_NAME, ID);    context.put(UUIDInterceptor.PREFIX_NAME, "bar#");    Event event = new SimpleEvent();    assertTrue(build(context).intercept(event).getHeaders().get(ID).startsWith("bar#"));}
0
private UUIDInterceptor build(Context context)
{    UUIDInterceptor.Builder builder = new UUIDInterceptor.Builder();    builder.configure(context);    return builder.build();}
0
public JMSMessageConverter build(Context context)
{    return new DefaultJMSMessageConverter(context.getString(JMSSourceConfiguration.CONVERTER_CHARSET, JMSSourceConfiguration.CONVERTER_CHARSET_DEFAULT).trim());}
0
public List<Event> convert(Message message) throws JMSException
{    Event event = new SimpleEvent();    Map<String, String> headers = event.getHeaders();    @SuppressWarnings("rawtypes")    Enumeration propertyNames = message.getPropertyNames();    while (propertyNames.hasMoreElements()) {        String name = propertyNames.nextElement().toString();        String value = message.getStringProperty(name);        headers.put(name, value);    }    if (message instanceof BytesMessage) {        BytesMessage bytesMessage = (BytesMessage) message;        long length = bytesMessage.getBodyLength();        if (length > 0L) {            if (length > Integer.MAX_VALUE) {                throw new JMSException("Unable to process message " + "of size " + length);            }            byte[] body = new byte[(int) length];            int count = bytesMessage.readBytes(body);            if (count != length) {                throw new JMSException("Unable to read full message. " + "Read " + count + " of total " + length);            }            event.setBody(body);        }    } else if (message instanceof TextMessage) {        TextMessage textMessage = (TextMessage) message;        String text = textMessage.getText();        if (text != null) {            event.setBody(text.getBytes(charset));        }    } else if (message instanceof ObjectMessage) {        ObjectMessage objectMessage = (ObjectMessage) message;        Object object = objectMessage.getObject();        if (object != null) {            ByteArrayOutputStream bos = new ByteArrayOutputStream();            ObjectOutput out = null;            try {                out = new ObjectOutputStream(bos);                out.writeObject(object);                event.setBody(bos.toByteArray());            } catch (IOException e) {                throw new FlumeException("Error serializing object", e);            } finally {                try {                    if (out != null) {                        out.close();                    }                } catch (IOException e) {                    throw new FlumeException("Error closing ObjectOutputStream", e);                }                try {                    if (bos != null) {                        bos.close();                    }                } catch (IOException e) {                    throw new FlumeException("Error closing ByteArrayOutputStream", e);                }            }        }    }    List<Event> events = new ArrayList<Event>(1);    events.add(event);    return events;}
0
public InitialContext create(Properties properties) throws NamingException
{    return new InitialContext(properties);}
0
 List<Event> take() throws JMSException
{    List<Event> result = new ArrayList<Event>(batchSize);    Message message;    message = receive();    if (message != null) {        result.addAll(messageConverter.convert(message));        int max = batchSize - 1;        for (int i = 0; i < max; i++) {            message = receiveNoWait();            if (message == null) {                break;            }            result.addAll(messageConverter.convert(message));        }    }    if (logger.isDebugEnabled()) {            }    return result;}
1
private Message receive() throws JMSException
{    try {        return messageConsumer.receive(pollTimeout);    } catch (RuntimeException runtimeException) {        JMSException jmsException = new JMSException("JMS provider has thrown runtime exception: " + runtimeException.getMessage());        jmsException.setLinkedException(runtimeException);        throw jmsException;    }}
0
private Message receiveNoWait() throws JMSException
{    try {        return messageConsumer.receiveNoWait();    } catch (RuntimeException runtimeException) {        JMSException jmsException = new JMSException("JMS provider has thrown runtime exception: " + runtimeException.getMessage());        jmsException.setLinkedException(runtimeException);        throw jmsException;    }}
0
 void commit()
{    try {        session.commit();    } catch (JMSException jmsException) {            } catch (RuntimeException runtimeException) {            }}
1
 void rollback()
{    try {        session.rollback();    } catch (JMSException jmsException) {            } catch (RuntimeException runtimeException) {            }}
1
 void close()
{    try {        if (session != null) {            session.close();        }    } catch (JMSException e) {            }    try {        if (connection != null) {            connection.close();        }    } catch (JMSException e) {            }}
1
protected void doConfigure(Context context) throws FlumeException
{    sourceCounter = new SourceCounter(getName());    initialContextFactoryName = context.getString(JMSSourceConfiguration.INITIAL_CONTEXT_FACTORY, "").trim();    providerUrl = context.getString(JMSSourceConfiguration.PROVIDER_URL, "").trim();    destinationName = context.getString(JMSSourceConfiguration.DESTINATION_NAME, "").trim();    String destinationTypeName = context.getString(JMSSourceConfiguration.DESTINATION_TYPE, "").trim().toUpperCase(Locale.ENGLISH);    String destinationLocatorName = context.getString(JMSSourceConfiguration.DESTINATION_LOCATOR, JMSSourceConfiguration.DESTINATION_LOCATOR_DEFAULT).trim().toUpperCase(Locale.ENGLISH);    messageSelector = context.getString(JMSSourceConfiguration.MESSAGE_SELECTOR, "").trim();    batchSize = context.getInteger(JMSSourceConfiguration.BATCH_SIZE, JMSSourceConfiguration.BATCH_SIZE_DEFAULT);    errorThreshold = context.getInteger(JMSSourceConfiguration.ERROR_THRESHOLD, JMSSourceConfiguration.ERROR_THRESHOLD_DEFAULT);    userName = Optional.fromNullable(context.getString(JMSSourceConfiguration.USERNAME));    pollTimeout = context.getLong(JMSSourceConfiguration.POLL_TIMEOUT, JMSSourceConfiguration.POLL_TIMEOUT_DEFAULT);    clientId = Optional.fromNullable(context.getString(JMSSourceConfiguration.CLIENT_ID));    createDurableSubscription = context.getBoolean(JMSSourceConfiguration.CREATE_DURABLE_SUBSCRIPTION, JMSSourceConfiguration.DEFAULT_CREATE_DURABLE_SUBSCRIPTION);    durableSubscriptionName = context.getString(JMSSourceConfiguration.DURABLE_SUBSCRIPTION_NAME, JMSSourceConfiguration.DEFAULT_DURABLE_SUBSCRIPTION_NAME);    String passwordFile = context.getString(JMSSourceConfiguration.PASSWORD_FILE, "").trim();    if (passwordFile.isEmpty()) {        password = Optional.absent();    } else {        try {            password = Optional.of(Files.toString(new File(passwordFile), Charsets.UTF_8).trim());        } catch (IOException e) {            throw new FlumeException(String.format("Could not read password file %s", passwordFile), e);        }    }    String converterClassName = context.getString(JMSSourceConfiguration.CONVERTER_TYPE, JMSSourceConfiguration.CONVERTER_TYPE_DEFAULT).trim();    if (JMSSourceConfiguration.CONVERTER_TYPE_DEFAULT.equalsIgnoreCase(converterClassName)) {        converterClassName = DefaultJMSMessageConverter.Builder.class.getName();    }    Context converterContext = new Context(context.getSubProperties(JMSSourceConfiguration.CONVERTER + "."));    try {        @SuppressWarnings("rawtypes")        Class clazz = Class.forName(converterClassName);        boolean isBuilder = JMSMessageConverter.Builder.class.isAssignableFrom(clazz);        if (isBuilder) {            JMSMessageConverter.Builder builder = (JMSMessageConverter.Builder) clazz.newInstance();            converter = builder.build(converterContext);        } else {            Preconditions.checkState(JMSMessageConverter.class.isAssignableFrom(clazz), String.format("Class %s is not a subclass of JMSMessageConverter", clazz.getName()));            converter = (JMSMessageConverter) clazz.newInstance();            boolean configured = Configurables.configure(converter, converterContext);            if (logger.isDebugEnabled()) {                            }        }    } catch (Exception e) {        throw new FlumeException(String.format("Unable to create instance of converter %s", converterClassName), e);    }    String connectionFactoryName = context.getString(JMSSourceConfiguration.CONNECTION_FACTORY, JMSSourceConfiguration.CONNECTION_FACTORY_DEFAULT).trim();    assertNotEmpty(initialContextFactoryName, String.format("Initial Context Factory is empty. This is specified by %s", JMSSourceConfiguration.INITIAL_CONTEXT_FACTORY));    assertNotEmpty(providerUrl, String.format("Provider URL is empty. This is specified by %s", JMSSourceConfiguration.PROVIDER_URL));    assertNotEmpty(destinationName, String.format("Destination Name is empty. This is specified by %s", JMSSourceConfiguration.DESTINATION_NAME));    assertNotEmpty(destinationTypeName, String.format("Destination Type is empty. This is specified by %s", JMSSourceConfiguration.DESTINATION_TYPE));    try {        destinationType = JMSDestinationType.valueOf(destinationTypeName);    } catch (IllegalArgumentException e) {        throw new FlumeException(String.format("Destination type '%s' is " + "invalid.", destinationTypeName), e);    }    if (createDurableSubscription) {        if (JMSDestinationType.TOPIC != destinationType) {            throw new FlumeException(String.format("Only Destination type '%s' supports durable subscriptions.", JMSDestinationType.TOPIC.toString()));        }        if (!clientId.isPresent()) {            throw new FlumeException(String.format("You have to specify '%s' when using durable subscriptions.", JMSSourceConfiguration.CLIENT_ID));        }        if (StringUtils.isEmpty(durableSubscriptionName)) {            throw new FlumeException(String.format("If '%s' is set to true, '%s' has to be specified.", JMSSourceConfiguration.CREATE_DURABLE_SUBSCRIPTION, JMSSourceConfiguration.DURABLE_SUBSCRIPTION_NAME));        }    } else if (!StringUtils.isEmpty(durableSubscriptionName)) {            }    try {        destinationLocator = JMSDestinationLocator.valueOf(destinationLocatorName);    } catch (IllegalArgumentException e) {        throw new FlumeException(String.format("Destination locator '%s' is " + "invalid.", destinationLocatorName), e);    }    Preconditions.checkArgument(batchSize > 0, "Batch size must be greater than 0");    try {        Properties contextProperties = new Properties();        contextProperties.setProperty(javax.naming.Context.INITIAL_CONTEXT_FACTORY, initialContextFactoryName);        contextProperties.setProperty(javax.naming.Context.PROVIDER_URL, providerUrl);                if (this.userName.isPresent()) {            contextProperties.setProperty(javax.naming.Context.SECURITY_PRINCIPAL, this.userName.get());        }        if (this.password.isPresent()) {            contextProperties.setProperty(javax.naming.Context.SECURITY_CREDENTIALS, this.password.get());        }        initialContext = initialContextFactory.create(contextProperties);    } catch (NamingException e) {        throw new FlumeException(String.format("Could not create initial context %s provider %s", initialContextFactoryName, providerUrl), e);    }    try {        connectionFactory = (ConnectionFactory) initialContext.lookup(connectionFactoryName);    } catch (NamingException e) {        throw new FlumeException("Could not lookup ConnectionFactory", e);    }}
1
private void assertNotEmpty(String arg, String msg)
{    Preconditions.checkArgument(!arg.isEmpty(), msg);}
0
protected synchronized Status doProcess() throws EventDeliveryException
{    boolean error = true;    try {        if (consumer == null) {            consumer = createConsumer();        }        List<Event> events = consumer.take();        int size = events.size();        if (size == 0) {            error = false;            return Status.BACKOFF;        }        sourceCounter.incrementAppendBatchReceivedCount();        sourceCounter.addToEventReceivedCount(size);        getChannelProcessor().processEventBatch(events);        error = false;        sourceCounter.addToEventAcceptedCount(size);        sourceCounter.incrementAppendBatchAcceptedCount();        return Status.READY;    } catch (ChannelException channelException) {                sourceCounter.incrementChannelWriteFail();    } catch (JMSException jmsException) {                if (++jmsExceptionCounter > errorThreshold) {            if (consumer != null) {                                sourceCounter.incrementEventReadFail();                consumer.rollback();                consumer.close();                consumer = null;            }        }    } catch (Throwable throwable) {                sourceCounter.incrementEventReadFail();        if (throwable instanceof Error) {            throw (Error) throwable;        }    } finally {        if (error) {            if (consumer != null) {                consumer.rollback();            }        } else {            if (consumer != null) {                consumer.commit();                jmsExceptionCounter = 0;            }        }    }    return Status.BACKOFF;}
1
protected synchronized void doStart()
{    try {        consumer = createConsumer();        jmsExceptionCounter = 0;        sourceCounter.start();    } catch (JMSException e) {        throw new FlumeException("Unable to create consumer", e);    }}
0
protected synchronized void doStop()
{    if (consumer != null) {        consumer.close();        consumer = null;    }    sourceCounter.stop();}
0
 JMSMessageConsumer createConsumer() throws JMSException
{        JMSMessageConsumer consumer = new JMSMessageConsumer(initialContext, connectionFactory, destinationName, destinationLocator, destinationType, messageSelector, batchSize, pollTimeout, converter, userName, password, clientId, createDurableSubscription, durableSubscriptionName);    jmsExceptionCounter = 0;    return consumer;}
1
public long getBatchSize()
{    return batchSize;}
0
public void setup() throws Exception
{    beforeSetup();    connectionFactory = mock(ConnectionFactory.class);    connection = mock(Connection.class);    session = mock(Session.class);    queue = mock(Queue.class);    topic = mock(Topic.class);    messageConsumer = mock(MessageConsumer.class);    message = mock(TextMessage.class);    when(message.getPropertyNames()).thenReturn(new Enumeration<Object>() {        @Override        public boolean hasMoreElements() {            return false;        }        @Override        public Object nextElement() {            throw new UnsupportedOperationException();        }    });    when(message.getText()).thenReturn(TEXT);    when(connectionFactory.createConnection(USERNAME, PASSWORD)).thenReturn(connection);    when(connection.createSession(true, Session.SESSION_TRANSACTED)).thenReturn(session);    when(session.createQueue(destinationName)).thenReturn(queue);    when(session.createConsumer(any(Destination.class), anyString())).thenReturn(messageConsumer);    when(messageConsumer.receiveNoWait()).thenReturn(message);    when(messageConsumer.receive(anyLong())).thenReturn(message);    destinationName = DESTINATION_NAME;    destinationType = JMSDestinationType.QUEUE;    destinationLocator = JMSDestinationLocator.CDI;    messageSelector = SELECTOR;    batchSize = 10;    pollTimeout = 500L;    context = new Context();    converter = new DefaultJMSMessageConverter.Builder().build(context);    event = converter.convert(message).iterator().next();    userName = Optional.of(USERNAME);    password = Optional.of(PASSWORD);    afterSetup();}
0
public boolean hasMoreElements()
{    return false;}
0
public Object nextElement()
{    throw new UnsupportedOperationException();}
0
 void beforeSetup() throws Exception
{}
0
 void afterSetup() throws Exception
{}
0
 void beforeTearDown() throws Exception
{}
0
 void afterTearDown() throws Exception
{}
0
 void assertBodyIsExpected(List<Event> events)
{    for (Event event : events) {        assertEquals(TEXT, new String(event.getBody(), Charsets.UTF_8));    }}
0
 JMSMessageConsumer create()
{    return new JMSMessageConsumer(WONT_USE, connectionFactory, destinationName, destinationLocator, destinationType, messageSelector, batchSize, pollTimeout, converter, userName, password, Optional.<String>absent(), false, "");}
0
public void tearDown() throws Exception
{    beforeTearDown();    if (consumer != null) {        consumer.close();    }    afterTearDown();}
0
public void setUp() throws Exception
{    headers = Maps.newHashMap();    context = new Context();    converter = new DefaultJMSMessageConverter.Builder().build(context);}
0
 void createTextMessage() throws Exception
{    TextMessage message = mock(TextMessage.class);    when(message.getText()).thenReturn(TEXT);    this.message = message;}
0
 void createNullTextMessage() throws Exception
{    TextMessage message = mock(TextMessage.class);    when(message.getText()).thenReturn(null);    this.message = message;}
0
 void createBytesMessage() throws Exception
{    BytesMessage message = mock(BytesMessage.class);    when(message.getBodyLength()).thenReturn((long) BYTES.length);    when(message.readBytes(any(byte[].class))).then(new Answer<Integer>() {        @Override        public Integer answer(InvocationOnMock invocation) throws Throwable {            byte[] buffer = (byte[]) invocation.getArguments()[0];            if (buffer != null) {                assertEquals(buffer.length, BYTES.length);                System.arraycopy(BYTES, 0, buffer, 0, BYTES.length);            }            return BYTES.length;        }    });    this.message = message;}
0
public Integer answer(InvocationOnMock invocation) throws Throwable
{    byte[] buffer = (byte[]) invocation.getArguments()[0];    if (buffer != null) {        assertEquals(buffer.length, BYTES.length);        System.arraycopy(BYTES, 0, buffer, 0, BYTES.length);    }    return BYTES.length;}
0
 void createObjectMessage() throws Exception
{    ObjectMessage message = mock(ObjectMessage.class);    when(message.getObject()).thenReturn(TEXT);    this.message = message;}
0
 void createHeaders() throws Exception
{    final Iterator<String> keys = headers.keySet().iterator();    when(message.getPropertyNames()).thenReturn(new Enumeration<Object>() {        @Override        public boolean hasMoreElements() {            return keys.hasNext();        }        @Override        public Object nextElement() {            return keys.next();        }    });    when(message.getStringProperty(anyString())).then(new Answer<String>() {        @Override        public String answer(InvocationOnMock invocation) throws Throwable {            return headers.get(invocation.getArguments()[0]);        }    });}
0
public boolean hasMoreElements()
{    return keys.hasNext();}
0
public Object nextElement()
{    return keys.next();}
0
public String answer(InvocationOnMock invocation) throws Throwable
{    return headers.get(invocation.getArguments()[0]);}
0
public void testTextMessage() throws Exception
{    createTextMessage();    headers.put("key1", "value1");    headers.put("key2", "value2");    createHeaders();    Event event = converter.convert(message).iterator().next();    assertEquals(headers, event.getHeaders());    assertEquals(TEXT, new String(event.getBody(), Charsets.UTF_8));}
0
public void testNullTextMessage() throws Exception
{    createNullTextMessage();    headers.put("key1", "value1");    headers.put("key2", "value2");    createHeaders();    Event event = converter.convert(message).iterator().next();    assertEquals(headers, event.getHeaders());            assertEquals(event.getBody().length, 0);}
0
public void testBytesMessage() throws Exception
{    createBytesMessage();    headers.put("key1", "value1");    headers.put("key2", "value2");    createHeaders();    Event event = converter.convert(message).iterator().next();    assertEquals(headers, event.getHeaders());    assertArrayEquals(BYTES, event.getBody());}
0
public void testBytesMessageTooLarge() throws Exception
{    createBytesMessage();    when(((BytesMessage) message).getBodyLength()).thenReturn(Long.MAX_VALUE);    createHeaders();    converter.convert(message);}
0
public void testBytesMessagePartialReturn() throws Exception
{    createBytesMessage();    when(((BytesMessage) message).readBytes(any(byte[].class))).thenReturn(BYTES.length + 1);    createHeaders();    converter.convert(message);}
0
public void testObjectMessage() throws Exception
{    createObjectMessage();    headers.put("key1", "value1");    headers.put("key2", "value2");    createHeaders();    Event event = converter.convert(message).iterator().next();    assertEquals(headers, event.getHeaders());    ByteArrayOutputStream bos = new ByteArrayOutputStream();    ObjectOutput out = new ObjectOutputStream(bos);    out.writeObject(TEXT);    assertArrayEquals(bos.toByteArray(), event.getBody());}
0
public void testNoHeaders() throws Exception
{    createTextMessage();    createHeaders();    Event event = converter.convert(message).iterator().next();    assertEquals(Collections.EMPTY_MAP, event.getHeaders());    assertEquals(TEXT, new String(event.getBody(), Charsets.UTF_8));}
0
public static Collection<Object[]> parameters()
{    return Arrays.asList(new Object[][] { { TestMode.WITH_AUTHENTICATION }, { TestMode.WITHOUT_AUTHENTICATION } });}
0
public void setup() throws Exception
{    baseDir = Files.createTempDir();    tmpDir = new File(baseDir, "tmp");    dataDir = new File(baseDir, "data");    Assert.assertTrue(tmpDir.mkdir());    broker = new BrokerService();    broker.addConnector(BROKER_BIND_URL);    broker.setTmpDataDirectory(tmpDir);    broker.setDataDirectoryFile(dataDir);    context = new Context();    context.put(JMSSourceConfiguration.INITIAL_CONTEXT_FACTORY, INITIAL_CONTEXT_FACTORY);    context.put(JMSSourceConfiguration.PROVIDER_URL, BROKER_BIND_URL);    context.put(JMSSourceConfiguration.DESTINATION_NAME, DESTINATION_NAME);    if (jmsUserName != null) {        File passwordFile = new File(baseDir, "password");        Files.write(jmsPassword.getBytes(Charsets.UTF_8), passwordFile);        AuthenticationUser jmsUser = new AuthenticationUser(jmsUserName, jmsPassword, "");        List<AuthenticationUser> users = Collections.singletonList(jmsUser);        SimpleAuthenticationPlugin authentication = new SimpleAuthenticationPlugin(users);        broker.setPlugins(new BrokerPlugin[] { authentication });        context.put(JMSSourceConfiguration.USERNAME, jmsUserName);        context.put(JMSSourceConfiguration.PASSWORD_FILE, passwordFile.getAbsolutePath());    }    broker.start();    events = Lists.newArrayList();    source = new JMSSource();    source.setName("JMSSource-" + UUID.randomUUID());    ChannelProcessor channelProcessor = mock(ChannelProcessor.class);    doAnswer(new Answer<Void>() {        @Override        public Void answer(InvocationOnMock invocation) throws Throwable {            events.addAll((List<Event>) invocation.getArguments()[0]);            return null;        }    }).when(channelProcessor).processEventBatch(any(List.class));    source.setChannelProcessor(channelProcessor);}
0
public Void answer(InvocationOnMock invocation) throws Throwable
{    events.addAll((List<Event>) invocation.getArguments()[0]);    return null;}
0
public void tearDown() throws Exception
{    if (source != null) {        source.stop();    }    if (broker != null) {        broker.stop();    }    FileUtils.deleteDirectory(baseDir);}
0
private void putQueue(List<String> events) throws Exception
{    ConnectionFactory factory = new ActiveMQConnectionFactory(jmsUserName, jmsPassword, BROKER_BIND_URL);    Connection connection = factory.createConnection();    connection.start();    Session session = connection.createSession(true, Session.AUTO_ACKNOWLEDGE);    Destination destination = session.createQueue(DESTINATION_NAME);    MessageProducer producer = session.createProducer(destination);    for (String event : events) {        TextMessage message = session.createTextMessage();        message.setText(event);        producer.send(message);    }    session.commit();    session.close();    connection.close();}
0
private void putTopic(List<String> events) throws Exception
{    ConnectionFactory factory = new ActiveMQConnectionFactory(jmsUserName, jmsPassword, BROKER_BIND_URL);    Connection connection = factory.createConnection();    connection.start();    Session session = connection.createSession(true, Session.AUTO_ACKNOWLEDGE);    Destination destination = session.createTopic(DESTINATION_NAME);    MessageProducer producer = session.createProducer(destination);    for (String event : events) {        TextMessage message = session.createTextMessage();        message.setText(event);        producer.send(message);    }    session.commit();    session.close();    connection.close();}
0
public void testQueueLocatedWithJndi() throws Exception
{    context.put(JMSSourceConfiguration.DESTINATION_NAME, JNDI_PREFIX + DESTINATION_NAME);    context.put(JMSSourceConfiguration.DESTINATION_LOCATOR, JMSDestinationLocator.JNDI.name());    testQueue();}
0
public void testQueue() throws Exception
{    context.put(JMSSourceConfiguration.DESTINATION_TYPE, JMSSourceConfiguration.DESTINATION_TYPE_QUEUE);    source.configure(context);    source.start();    Thread.sleep(500L);    List<String> expected = Lists.newArrayList();    for (int i = 0; i < 10; i++) {        expected.add(String.valueOf(i));    }    putQueue(expected);    Thread.sleep(500L);    Assert.assertEquals(Status.READY, source.process());    Assert.assertEquals(Status.BACKOFF, source.process());    Assert.assertEquals(expected.size(), events.size());    List<String> actual = Lists.newArrayList();    for (Event event : events) {        actual.add(new String(event.getBody(), Charsets.UTF_8));    }    Collections.sort(expected);    Collections.sort(actual);    Assert.assertEquals(expected, actual);}
0
public void testTopic() throws Exception
{    context.put(JMSSourceConfiguration.DESTINATION_TYPE, JMSSourceConfiguration.DESTINATION_TYPE_TOPIC);    source.configure(context);    source.start();    Thread.sleep(500L);    List<String> expected = Lists.newArrayList();    for (int i = 0; i < 10; i++) {        expected.add(String.valueOf(i));    }    putTopic(expected);    Thread.sleep(500L);    Assert.assertEquals(Status.READY, source.process());    Assert.assertEquals(Status.BACKOFF, source.process());    Assert.assertEquals(expected.size(), events.size());    List<String> actual = Lists.newArrayList();    for (Event event : events) {        actual.add(new String(event.getBody(), Charsets.UTF_8));    }    Collections.sort(expected);    Collections.sort(actual);    Assert.assertEquals(expected, actual);}
0
public void testDurableSubscription() throws Exception
{    context.put(JMSSourceConfiguration.DESTINATION_TYPE, JMSSourceConfiguration.DESTINATION_TYPE_TOPIC);    context.put(JMSSourceConfiguration.CLIENT_ID, "FLUME");    context.put(JMSSourceConfiguration.DURABLE_SUBSCRIPTION_NAME, "SOURCE1");    context.put(JMSSourceConfiguration.CREATE_DURABLE_SUBSCRIPTION, "true");    context.put(JMSSourceConfiguration.BATCH_SIZE, "10");    source.configure(context);    source.start();    Thread.sleep(5000L);    List<String> expected = Lists.newArrayList();    List<String> input = Lists.newArrayList();    for (int i = 0; i < 10; i++) {        input.add("before " + String.valueOf(i));    }    expected.addAll(input);    putTopic(input);    Thread.sleep(500L);    Assert.assertEquals(Status.READY, source.process());    Assert.assertEquals(Status.BACKOFF, source.process());    source.stop();    Thread.sleep(500L);    input = Lists.newArrayList();    for (int i = 0; i < 10; i++) {        input.add("during " + String.valueOf(i));    }    expected.addAll(input);    putTopic(input);    source.start();    Thread.sleep(500L);    input = Lists.newArrayList();    for (int i = 0; i < 10; i++) {        input.add("after " + String.valueOf(i));    }    expected.addAll(input);    putTopic(input);    Assert.assertEquals(Status.READY, source.process());    Assert.assertEquals(Status.READY, source.process());    Assert.assertEquals(Status.BACKOFF, source.process());    Assert.assertEquals(expected.size(), events.size());    List<String> actual = Lists.newArrayList();    for (Event event : events) {        actual.add(new String(event.getBody(), Charsets.UTF_8));    }    Collections.sort(expected);    Collections.sort(actual);    Assert.assertEquals(expected, actual);}
0
public void testCreateConnectionFails() throws Exception
{    when(connectionFactory.createConnection(USERNAME, PASSWORD)).thenThrow(new JMSException(""));    create();}
0
public void testCreateSessionFails() throws Exception
{    when(connection.createSession(true, Session.SESSION_TRANSACTED)).thenThrow(new JMSException(""));    try {        create();        fail("Expected exception: org.apache.flume.FlumeException");    } catch (FlumeException e) {        verify(connection).close();    }}
0
public void testCreateQueueFails() throws Exception
{    when(session.createQueue(destinationName)).thenThrow(new JMSException(""));    try {        create();        fail("Expected exception: org.apache.flume.FlumeException");    } catch (FlumeException e) {        verify(session).close();        verify(connection).close();    }}
0
public void testCreateTopicFails() throws Exception
{    destinationType = JMSDestinationType.TOPIC;    when(session.createTopic(destinationName)).thenThrow(new JMSException(""));    try {        create();        fail("Expected exception: org.apache.flume.FlumeException");    } catch (FlumeException e) {        verify(session).close();        verify(connection).close();    }}
0
public void testCreateConsumerFails() throws Exception
{    when(session.createConsumer(any(Destination.class), anyString())).thenThrow(new JMSException(""));    try {        create();        fail("Expected exception: org.apache.flume.FlumeException");    } catch (FlumeException e) {        verify(session).close();        verify(connection).close();    }}
0
public void testInvalidBatchSizeZero() throws Exception
{    batchSize = 0;    create();}
0
public void testInvalidPollTime() throws Exception
{    pollTimeout = -1L;    create();}
0
public void testInvalidBatchSizeNegative() throws Exception
{    batchSize = -1;    create();}
0
public void testQueue() throws Exception
{    destinationType = JMSDestinationType.QUEUE;    when(session.createQueue(destinationName)).thenReturn(queue);    consumer = create();    List<Event> events = consumer.take();    assertEquals(batchSize, events.size());    assertBodyIsExpected(events);    verify(session, never()).createTopic(anyString());}
0
public void testTopic() throws Exception
{    destinationType = JMSDestinationType.TOPIC;    when(session.createTopic(destinationName)).thenReturn(topic);    consumer = create();    List<Event> events = consumer.take();    assertEquals(batchSize, events.size());    assertBodyIsExpected(events);    verify(session, never()).createQueue(anyString());}
0
public void testUserPass() throws Exception
{    consumer = create();    List<Event> events = consumer.take();    assertEquals(batchSize, events.size());    assertBodyIsExpected(events);}
0
public void testNoUserPass() throws Exception
{    userName = Optional.absent();    when(connectionFactory.createConnection(USERNAME, PASSWORD)).thenThrow(new AssertionError());    when(connectionFactory.createConnection()).thenReturn(connection);    consumer = create();    List<Event> events = consumer.take();    assertEquals(batchSize, events.size());    assertBodyIsExpected(events);}
0
public void testNoEvents() throws Exception
{    when(messageConsumer.receive(anyLong())).thenReturn(null);    consumer = create();    List<Event> events = consumer.take();    assertEquals(0, events.size());    verify(messageConsumer, times(1)).receive(anyLong());    verifyNoMoreInteractions(messageConsumer);}
0
public void testSingleEvent() throws Exception
{    when(messageConsumer.receiveNoWait()).thenReturn(null);    consumer = create();    List<Event> events = consumer.take();    assertEquals(1, events.size());    assertBodyIsExpected(events);}
0
public void testPartialBatch() throws Exception
{    when(messageConsumer.receiveNoWait()).thenReturn(message, (Message) null);    consumer = create();    List<Event> events = consumer.take();    assertEquals(2, events.size());    assertBodyIsExpected(events);}
0
public void testCommit() throws Exception
{    consumer = create();    consumer.commit();    verify(session, times(1)).commit();}
0
public void testRollback() throws Exception
{    consumer = create();    consumer.rollback();    verify(session, times(1)).rollback();}
0
public void testClose() throws Exception
{    doThrow(new JMSException("")).when(session).close();    consumer = create();    consumer.close();    verify(session, times(1)).close();    verify(connection, times(1)).close();}
0
public void testCreateDurableSubscription() throws Exception
{    String name = "SUBSCRIPTION_NAME";    String clientID = "CLIENT_ID";    TopicSubscriber mockTopicSubscriber = mock(TopicSubscriber.class);    when(session.createDurableSubscriber(any(Topic.class), anyString(), anyString(), anyBoolean())).thenReturn(mockTopicSubscriber);    when(session.createTopic(destinationName)).thenReturn(topic);    new JMSMessageConsumer(WONT_USE, connectionFactory, destinationName, destinationLocator, JMSDestinationType.TOPIC, messageSelector, batchSize, pollTimeout, converter, userName, password, Optional.of(clientID), true, name);    verify(connection, times(1)).setClientID(clientID);    verify(session, times(1)).createDurableSubscriber(topic, name, messageSelector, true);}
0
public void testTakeFailsDueToJMSExceptionFromReceive() throws JMSException
{    when(messageConsumer.receive(anyLong())).thenThrow(new JMSException(""));    consumer = create();    consumer.take();}
0
public void testTakeFailsDueToRuntimeExceptionFromReceive() throws JMSException
{    when(messageConsumer.receive(anyLong())).thenThrow(new RuntimeException());    consumer = create();    consumer.take();}
0
public void testTakeFailsDueToJMSExceptionFromReceiveNoWait() throws JMSException
{    when(messageConsumer.receiveNoWait()).thenThrow(new JMSException(""));    consumer = create();    consumer.take();}
0
public void testTakeFailsDueToRuntimeExceptionFromReceiveNoWait() throws JMSException
{    when(messageConsumer.receiveNoWait()).thenThrow(new RuntimeException());    consumer = create();    consumer.take();}
0
public void testCommitFailsDueToJMSException() throws JMSException
{    doThrow(new JMSException("")).when(session).commit();    consumer = create();    consumer.commit();}
0
public void testCommitFailsDueToRuntimeException() throws JMSException
{    doThrow(new RuntimeException()).when(session).commit();    consumer = create();    consumer.commit();}
0
public void testRollbackFailsDueToJMSException() throws JMSException
{    doThrow(new JMSException("")).when(session).rollback();    consumer = create();    consumer.rollback();}
0
public void testRollbackFailsDueToRuntimeException() throws JMSException
{    doThrow(new RuntimeException()).when(session).rollback();    consumer = create();    consumer.rollback();}
0
 void afterSetup() throws Exception
{    baseDir = Files.createTempDir();    passwordFile = new File(baseDir, "password");    Assert.assertTrue(passwordFile.createNewFile());    initialContext = mock(InitialContext.class);    channelProcessor = mock(ChannelProcessor.class);    events = Lists.newArrayList();    doAnswer(new Answer<Void>() {        @Override        public Void answer(InvocationOnMock invocation) throws Throwable {            events.addAll((List<Event>) invocation.getArguments()[0]);            return null;        }    }).when(channelProcessor).processEventBatch(any(List.class));    consumer = spy(create());    when(initialContext.lookup(anyString())).thenReturn(connectionFactory);    contextFactory = mock(InitialContextFactory.class);    when(contextFactory.create(any(Properties.class))).thenReturn(initialContext);    source = spy(new JMSSource(contextFactory));    doReturn(consumer).when(source).createConsumer();    source.setName("JMSSource-" + UUID.randomUUID());    source.setChannelProcessor(channelProcessor);    context = new Context();    context.put(JMSSourceConfiguration.BATCH_SIZE, String.valueOf(batchSize));    context.put(JMSSourceConfiguration.DESTINATION_NAME, "INBOUND");    context.put(JMSSourceConfiguration.DESTINATION_TYPE, JMSSourceConfiguration.DESTINATION_TYPE_QUEUE);    context.put(JMSSourceConfiguration.PROVIDER_URL, "dummy:1414");    context.put(JMSSourceConfiguration.INITIAL_CONTEXT_FACTORY, "ldap://dummy:389");}
0
public Void answer(InvocationOnMock invocation) throws Throwable
{    events.addAll((List<Event>) invocation.getArguments()[0]);    return null;}
0
 void afterTearDown() throws Exception
{    FileUtils.deleteDirectory(baseDir);}
0
public void testStop() throws Exception
{    source.configure(context);    source.start();    source.stop();    verify(consumer).close();}
0
public void testConfigureWithoutInitialContextFactory() throws Exception
{    context.put(JMSSourceConfiguration.INITIAL_CONTEXT_FACTORY, "");    source.configure(context);}
0
public void testConfigureWithoutProviderURL() throws Exception
{    context.put(JMSSourceConfiguration.PROVIDER_URL, "");    source.configure(context);}
0
public void testConfigureWithoutDestinationName() throws Exception
{    context.put(JMSSourceConfiguration.DESTINATION_NAME, "");    source.configure(context);}
0
public void testConfigureWithBadDestinationType() throws Exception
{    context.put(JMSSourceConfiguration.DESTINATION_TYPE, "DUMMY");    source.configure(context);}
0
public void testConfigureWithEmptyDestinationType() throws Exception
{    context.put(JMSSourceConfiguration.DESTINATION_TYPE, "");    source.configure(context);}
0
public void testStartConsumerCreateThrowsException() throws Exception
{    doThrow(new RuntimeException("Expected")).when(source).createConsumer();    source.configure(context);    source.start();    try {        source.process();        Assert.fail();    } catch (FlumeException expected) {    }}
0
public void testConfigureWithContextLookupThrowsException() throws Exception
{    when(initialContext.lookup(anyString())).thenThrow(new NamingException());    source.configure(context);}
0
public void testConfigureWithContextCreateThrowsException() throws Exception
{    when(contextFactory.create(any(Properties.class))).thenThrow(new NamingException());    source.configure(context);}
0
public void testConfigureWithInvalidBatchSize() throws Exception
{    context.put(JMSSourceConfiguration.BATCH_SIZE, "0");    source.configure(context);}
0
public void testConfigureWithInvalidPasswordFile() throws Exception
{    context.put(JMSSourceConfiguration.PASSWORD_FILE, "/dev/does/not/exist/nor/will/ever/exist");    source.configure(context);}
0
public void testConfigureWithUserNameButNoPasswordFile() throws Exception
{    context.put(JMSSourceConfiguration.USERNAME, "dummy");    source.configure(context);    source.start();    Assert.assertEquals(Status.READY, source.process());    Assert.assertEquals(batchSize, events.size());    assertBodyIsExpected(events);}
0
public void testConfigureWithUserNameAndPasswordFile() throws Exception
{    context.put(JMSSourceConfiguration.USERNAME, "dummy");    context.put(JMSSourceConfiguration.PASSWORD_FILE, passwordFile.getAbsolutePath());    source.configure(context);    source.start();    Assert.assertEquals(Status.READY, source.process());    Assert.assertEquals(batchSize, events.size());    assertBodyIsExpected(events);}
0
public void testConfigureWithInvalidConverterClass() throws Exception
{    context.put(JMSSourceConfiguration.CONVERTER_TYPE, "not a valid classname");    source.configure(context);}
0
public void testProcessNoStart() throws Exception
{    try {        source.process();        Assert.fail();    } catch (EventDeliveryException expected) {    }}
0
public void testNonDefaultConverter() throws Exception
{        context.put(JMSSourceConfiguration.CONVERTER_TYPE, DefaultJMSMessageConverter.Builder.class.getName());    source.configure(context);    source.start();    Assert.assertEquals(Status.READY, source.process());    Assert.assertEquals(batchSize, events.size());    assertBodyIsExpected(events);    verify(consumer).commit();}
0
public List<Event> convert(Message message) throws JMSException
{    throw new UnsupportedOperationException();}
0
public List<Event> convert(Message message) throws JMSException
{    throw new UnsupportedOperationException();}
0
public void configure(Context context)
{}
0
public void testNonBuilderConfigurableConverter() throws Exception
{        context.put(JMSSourceConfiguration.CONVERTER_TYPE, NonBuilderConfigurableConverter.class.getName());    source.configure(context);    source.start();    Assert.assertEquals(Status.READY, source.process());    Assert.assertEquals(batchSize, events.size());    assertBodyIsExpected(events);    verify(consumer).commit();}
0
public void testNonBuilderNonConfigurableConverter() throws Exception
{        context.put(JMSSourceConfiguration.CONVERTER_TYPE, NonBuilderNonConfigurableConverter.class.getName());    source.configure(context);    source.start();    Assert.assertEquals(Status.READY, source.process());    Assert.assertEquals(batchSize, events.size());    assertBodyIsExpected(events);    verify(consumer).commit();}
0
public void testProcessFullBatch() throws Exception
{    source.configure(context);    source.start();    Assert.assertEquals(Status.READY, source.process());    Assert.assertEquals(batchSize, events.size());    assertBodyIsExpected(events);    verify(consumer).commit();}
0
public void testProcessNoEvents() throws Exception
{    when(messageConsumer.receive(anyLong())).thenReturn(null);    source.configure(context);    source.start();    Assert.assertEquals(Status.BACKOFF, source.process());    Assert.assertEquals(0, events.size());    verify(consumer).commit();}
0
public void testProcessPartialBatch() throws Exception
{    when(messageConsumer.receiveNoWait()).thenReturn(message, (Message) null);    source.configure(context);    source.start();    Assert.assertEquals(Status.READY, source.process());    Assert.assertEquals(2, events.size());    assertBodyIsExpected(events);    verify(consumer).commit();}
0
public void testProcessChannelProcessorThrowsChannelException() throws Exception
{    doThrow(new ChannelException("dummy")).when(channelProcessor).processEventBatch(any(List.class));    source.configure(context);    source.start();    Assert.assertEquals(Status.BACKOFF, source.process());    verify(consumer).rollback();}
0
public void testProcessChannelProcessorThrowsError() throws Exception
{    doThrow(new Error()).when(channelProcessor).processEventBatch(any(List.class));    source.configure(context);    source.start();    try {        source.process();        Assert.fail();    } catch (Error ignores) {    }    verify(consumer).rollback();}
0
public void testProcessReconnect() throws Exception
{    source.configure(context);    source.start();    when(consumer.take()).thenThrow(new JMSException("dummy"));    int attempts = JMSSourceConfiguration.ERROR_THRESHOLD_DEFAULT;    for (int i = 0; i < attempts; i++) {        Assert.assertEquals(Status.BACKOFF, source.process());    }    Assert.assertEquals(Status.BACKOFF, source.process());    SourceCounter sc = (SourceCounter) Whitebox.getInternalState(source, "sourceCounter");    Assert.assertEquals(1, sc.getEventReadFail());    verify(consumer, times(attempts + 1)).rollback();    verify(consumer, times(1)).close();}
0
public void testErrorCounterEventReadFail() throws Exception
{    source.configure(context);    source.start();    when(consumer.take()).thenThrow(new RuntimeException("dummy"));    source.process();    SourceCounter sc = (SourceCounter) Whitebox.getInternalState(source, "sourceCounter");    Assert.assertEquals(1, sc.getEventReadFail());}
0
public void testErrorCounterChannelWriteFail() throws Exception
{    source.configure(context);    source.start();    when(source.getChannelProcessor()).thenThrow(new ChannelException("dummy"));    source.process();    SourceCounter sc = (SourceCounter) Whitebox.getInternalState(source, "sourceCounter");    Assert.assertEquals(1, sc.getChannelWriteFail());}
0
public void setUp()
{    sourceFactory = new DefaultSourceFactory();}
0
private void verifySourceCreation(String name, String type, Class<?> typeClass) throws FlumeException
{    Source src = sourceFactory.create(name, type);    Assert.assertNotNull(src);    Assert.assertTrue(typeClass.isInstance(src));}
0
public void testJMSSourceCreation()
{    verifySourceCreation("jms-src", "jms", JMSSource.class);}
0
public long getBatchSize()
{    return batchUpperLimit;}
0
public T get()
{    return null;}
0
public void subscribe(KafkaConsumer<?, ?> consumer, SourceRebalanceListener listener)
{    consumer.subscribe(topicList, listener);}
0
public List<String> get()
{    return topicList;}
0
public void subscribe(KafkaConsumer<?, ?> consumer, SourceRebalanceListener listener)
{    consumer.subscribe(pattern, listener);}
0
public Pattern get()
{    return pattern;}
0
protected Status doProcess() throws EventDeliveryException
{    final String batchUUID = UUID.randomUUID().toString();    String kafkaKey;    Event event;    byte[] eventBody;    try {                final long nanoBatchStartTime = System.nanoTime();        final long batchStartTime = System.currentTimeMillis();        final long maxBatchEndTime = System.currentTimeMillis() + maxBatchDurationMillis;        while (eventList.size() < batchUpperLimit && System.currentTimeMillis() < maxBatchEndTime) {            if (it == null || !it.hasNext()) {                                                long durMs = Math.max(0L, maxBatchEndTime - System.currentTimeMillis());                Duration duration = Duration.ofMillis(durMs);                ConsumerRecords<String, byte[]> records = consumer.poll(duration);                it = records.iterator();                                if (rebalanceFlag.compareAndSet(true, false)) {                    break;                }                                if (!it.hasNext()) {                    counter.incrementKafkaEmptyCount();                                                            break;                }            }                        ConsumerRecord<String, byte[]> message = it.next();            kafkaKey = message.key();            if (useAvroEventFormat) {                                                ByteArrayInputStream in = new ByteArrayInputStream(message.value());                decoder = DecoderFactory.get().directBinaryDecoder(in, decoder);                if (!reader.isPresent()) {                    reader = Optional.of(new SpecificDatumReader<AvroFlumeEvent>(AvroFlumeEvent.class));                }                                                AvroFlumeEvent avroevent = reader.get().read(null, decoder);                eventBody = avroevent.getBody().array();                headers = toStringMap(avroevent.getHeaders());            } else {                eventBody = message.value();                headers.clear();                headers = new HashMap<String, String>(4);            }                        if (!headers.containsKey(KafkaSourceConstants.TIMESTAMP_HEADER)) {                headers.put(KafkaSourceConstants.TIMESTAMP_HEADER, String.valueOf(System.currentTimeMillis()));            }                        if (setTopicHeader && !headers.containsKey(topicHeader)) {                headers.put(topicHeader, message.topic());            }            if (!headers.containsKey(KafkaSourceConstants.PARTITION_HEADER)) {                headers.put(KafkaSourceConstants.PARTITION_HEADER, String.valueOf(message.partition()));            }            if (!headers.containsKey(OFFSET_HEADER)) {                headers.put(OFFSET_HEADER, String.valueOf(message.offset()));            }            if (kafkaKey != null) {                headers.put(KafkaSourceConstants.KEY_HEADER, kafkaKey);            }            if (log.isTraceEnabled()) {                if (LogPrivacyUtil.allowLogRawData()) {                    log.trace("Topic: {} Partition: {} Message: {}", new String[] { message.topic(), String.valueOf(message.partition()), new String(eventBody) });                } else {                    log.trace("Topic: {} Partition: {} Message arrived.", message.topic(), String.valueOf(message.partition()));                }            }            event = EventBuilder.withBody(eventBody, headers);            eventList.add(event);            if (log.isDebugEnabled()) {                                            }                        tpAndOffsetMetadata.put(new TopicPartition(message.topic(), message.partition()), new OffsetAndMetadata(message.offset() + 1, batchUUID));        }        if (eventList.size() > 0) {            counter.addToKafkaEventGetTimer((System.nanoTime() - nanoBatchStartTime) / (1000 * 1000));            counter.addToEventReceivedCount((long) eventList.size());            getChannelProcessor().processEventBatch(eventList);            counter.addToEventAcceptedCount(eventList.size());            if (log.isDebugEnabled()) {                            }            eventList.clear();            if (!tpAndOffsetMetadata.isEmpty()) {                long commitStartTime = System.nanoTime();                consumer.commitSync(tpAndOffsetMetadata);                long commitEndTime = System.nanoTime();                counter.addToKafkaCommitTimer((commitEndTime - commitStartTime) / (1000 * 1000));                tpAndOffsetMetadata.clear();            }            return Status.READY;        }        return Status.BACKOFF;    } catch (Exception e) {                counter.incrementEventReadOrChannelFail(e);        return Status.BACKOFF;    }}
1
protected void doConfigure(Context context) throws FlumeException
{    this.context = context;    headers = new HashMap<String, String>(4);    tpAndOffsetMetadata = new HashMap<TopicPartition, OffsetAndMetadata>();    rebalanceFlag = new AtomicBoolean(false);    kafkaProps = new Properties();            translateOldProperties(context);    String topicProperty = context.getString(KafkaSourceConstants.TOPICS_REGEX);    if (topicProperty != null && !topicProperty.isEmpty()) {                subscriber = new PatternSubscriber(topicProperty);    } else if ((topicProperty = context.getString(KafkaSourceConstants.TOPICS)) != null && !topicProperty.isEmpty()) {                subscriber = new TopicListSubscriber(topicProperty);    } else if (subscriber == null) {        throw new ConfigurationException("At least one Kafka topic must be specified.");    }    batchUpperLimit = context.getInteger(KafkaSourceConstants.BATCH_SIZE, KafkaSourceConstants.DEFAULT_BATCH_SIZE);    maxBatchDurationMillis = context.getInteger(KafkaSourceConstants.BATCH_DURATION_MS, KafkaSourceConstants.DEFAULT_BATCH_DURATION);    useAvroEventFormat = context.getBoolean(KafkaSourceConstants.AVRO_EVENT, KafkaSourceConstants.DEFAULT_AVRO_EVENT);    if (log.isDebugEnabled()) {            }    zookeeperConnect = context.getString(ZOOKEEPER_CONNECT_FLUME_KEY);    migrateZookeeperOffsets = context.getBoolean(MIGRATE_ZOOKEEPER_OFFSETS, DEFAULT_MIGRATE_ZOOKEEPER_OFFSETS);    bootstrapServers = context.getString(KafkaSourceConstants.BOOTSTRAP_SERVERS);    if (bootstrapServers == null || bootstrapServers.isEmpty()) {        if (zookeeperConnect == null || zookeeperConnect.isEmpty()) {            throw new ConfigurationException("Bootstrap Servers must be specified");        } else {                                                String securityProtocolStr = context.getSubProperties(KafkaSourceConstants.KAFKA_CONSUMER_PREFIX).get(CommonClientConfigs.SECURITY_PROTOCOL_CONFIG);            if (securityProtocolStr == null || securityProtocolStr.isEmpty()) {                securityProtocolStr = CommonClientConfigs.DEFAULT_SECURITY_PROTOCOL;            }            bootstrapServers = lookupBootstrap(zookeeperConnect, SecurityProtocol.valueOf(securityProtocolStr));        }    }    String groupIdProperty = context.getString(KAFKA_CONSUMER_PREFIX + ConsumerConfig.GROUP_ID_CONFIG);    if (groupIdProperty != null && !groupIdProperty.isEmpty()) {                groupId = groupIdProperty;    }    if (groupId == null || groupId.isEmpty()) {        groupId = DEFAULT_GROUP_ID;            }    setTopicHeader = context.getBoolean(KafkaSourceConstants.SET_TOPIC_HEADER, KafkaSourceConstants.DEFAULT_SET_TOPIC_HEADER);    topicHeader = context.getString(KafkaSourceConstants.TOPIC_HEADER, KafkaSourceConstants.DEFAULT_TOPIC_HEADER);    setConsumerProps(context);    if (log.isDebugEnabled() && LogPrivacyUtil.allowLogPrintConfig()) {            }    if (counter == null) {        counter = new KafkaSourceCounter(getName());    }}
1
private void translateOldProperties(Context ctx)
{        String topic = context.getString(KafkaSourceConstants.TOPIC);    if (topic != null && !topic.isEmpty()) {        subscriber = new TopicListSubscriber(topic);            }        groupId = ctx.getString(KafkaSourceConstants.OLD_GROUP_ID);    if (groupId != null && !groupId.isEmpty()) {            }}
1
private void setConsumerProps(Context ctx)
{    kafkaProps.clear();    kafkaProps.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, KafkaSourceConstants.DEFAULT_KEY_DESERIALIZER);    kafkaProps.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, KafkaSourceConstants.DEFAULT_VALUE_DESERIALIZER);        kafkaProps.putAll(ctx.getSubProperties(KafkaSourceConstants.KAFKA_CONSUMER_PREFIX));        kafkaProps.put(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, bootstrapServers);    if (groupId != null) {        kafkaProps.put(ConsumerConfig.GROUP_ID_CONFIG, groupId);    }    kafkaProps.put(ConsumerConfig.ENABLE_AUTO_COMMIT_CONFIG, KafkaSourceConstants.DEFAULT_AUTO_COMMIT);    KafkaSSLUtil.addGlobalSSLParameters(kafkaProps);}
0
private String lookupBootstrap(String zookeeperConnect, SecurityProtocol securityProtocol)
{    try (KafkaZkClient zkClient = KafkaZkClient.apply(zookeeperConnect, JaasUtils.isZkSecurityEnabled(), ZK_SESSION_TIMEOUT, ZK_CONNECTION_TIMEOUT, 10, Time.SYSTEM, "kafka.server", "SessionExpireListener")) {        List<Broker> brokerList = JavaConverters.seqAsJavaListConverter(zkClient.getAllBrokersInCluster()).asJava();        List<BrokerEndPoint> endPoints = brokerList.stream().map(broker -> broker.brokerEndPoint(ListenerName.forSecurityProtocol(securityProtocol))).collect(Collectors.toList());        List<String> connections = new ArrayList<>();        for (BrokerEndPoint endPoint : endPoints) {            connections.add(endPoint.connectionString());        }        return StringUtils.join(connections, ',');    }}
0
 String getBootstrapServers()
{    return bootstrapServers;}
0
 Properties getConsumerProps()
{    return kafkaProps;}
0
private static Map<String, String> toStringMap(Map<CharSequence, CharSequence> charSeqMap)
{    Map<String, String> stringMap = new HashMap<String, String>();    for (Map.Entry<CharSequence, CharSequence> entry : charSeqMap.entrySet()) {        stringMap.put(entry.getKey().toString(), entry.getValue().toString());    }    return stringMap;}
0
 Subscriber<T> getSubscriber()
{    return subscriber;}
0
protected void doStart() throws FlumeException
{            if (migrateZookeeperOffsets && zookeeperConnect != null && !zookeeperConnect.isEmpty()) {                if (subscriber instanceof TopicListSubscriber && ((TopicListSubscriber) subscriber).get().size() == 1) {            String topicStr = ((TopicListSubscriber) subscriber).get().get(0);            migrateOffsets(topicStr);        } else {                    }    }        consumer = new KafkaConsumer<String, byte[]>(kafkaProps);        subscriber.subscribe(consumer, new SourceRebalanceListener(rebalanceFlag));        counter.start();}
1
protected void doStop() throws FlumeException
{    if (consumer != null) {        consumer.wakeup();        consumer.close();    }    if (counter != null) {        counter.stop();    }    }
1
private void migrateOffsets(String topicStr)
{    try (KafkaZkClient zkClient = KafkaZkClient.apply(zookeeperConnect, JaasUtils.isZkSecurityEnabled(), ZK_SESSION_TIMEOUT, ZK_CONNECTION_TIMEOUT, 10, Time.SYSTEM, "kafka.server", "SessionExpireListener");        KafkaConsumer<String, byte[]> consumer = new KafkaConsumer<>(kafkaProps)) {        Map<TopicPartition, OffsetAndMetadata> kafkaOffsets = getKafkaOffsets(consumer, topicStr);        if (kafkaOffsets == null) {                        return;        }        if (!kafkaOffsets.isEmpty()) {                                    return;        }                Map<TopicPartition, OffsetAndMetadata> zookeeperOffsets = getZookeeperOffsets(zkClient, consumer, topicStr);        if (zookeeperOffsets.isEmpty()) {                        return;        }                        consumer.commitSync(zookeeperOffsets);                Map<TopicPartition, OffsetAndMetadata> newKafkaOffsets = getKafkaOffsets(consumer, topicStr);                if (newKafkaOffsets == null || !newKafkaOffsets.keySet().containsAll(zookeeperOffsets.keySet())) {            throw new FlumeException("Offsets could not be committed");        }    }}
1
private Map<TopicPartition, OffsetAndMetadata> getKafkaOffsets(KafkaConsumer<String, byte[]> client, String topicStr)
{    Map<TopicPartition, OffsetAndMetadata> offsets = null;    List<PartitionInfo> partitions = client.partitionsFor(topicStr);    if (partitions != null) {        offsets = new HashMap<>();        for (PartitionInfo partition : partitions) {            TopicPartition key = new TopicPartition(topicStr, partition.partition());            OffsetAndMetadata offsetAndMetadata = client.committed(key);            if (offsetAndMetadata != null) {                offsets.put(key, offsetAndMetadata);            }        }    }    return offsets;}
0
private Map<TopicPartition, OffsetAndMetadata> getZookeeperOffsets(KafkaZkClient zkClient, KafkaConsumer<String, byte[]> consumer, String topicStr)
{    Map<TopicPartition, OffsetAndMetadata> offsets = new HashMap<>();    List<PartitionInfo> partitions = consumer.partitionsFor(topicStr);    for (PartitionInfo partition : partitions) {        TopicPartition topicPartition = new TopicPartition(topicStr, partition.partition());        Option<Object> optionOffset = zkClient.getConsumerOffset(groupId, topicPartition);        if (optionOffset.nonEmpty()) {            Long offset = (Long) optionOffset.get();            OffsetAndMetadata offsetAndMetadata = new OffsetAndMetadata(offset);            offsets.put(topicPartition, offsetAndMetadata);        }    }    return offsets;}
0
public void onPartitionsRevoked(Collection<TopicPartition> partitions)
{    for (TopicPartition partition : partitions) {                rebalanceFlag.set(true);    }}
1
public void onPartitionsAssigned(Collection<TopicPartition> partitions)
{    for (TopicPartition partition : partitions) {            }}
1
private static int findFreePort()
{    try (ServerSocket socket = new ServerSocket(0)) {        return socket.getLocalPort();    } catch (IOException e) {        throw new AssertionError("Can not find free port.", e);    }}
0
public void stop() throws IOException
{    producer.close();    kafkaServer.shutdown();    zookeeper.stopZookeeper();}
0
public String getZkConnectString()
{    return zookeeper.getConnectString();}
0
public String getBootstrapServers()
{    return HOST + ":" + serverPort;}
0
private void initProducer()
{    Properties props = new Properties();    props.put("bootstrap.servers", HOST + ":" + serverPort);    props.put("acks", "1");    producer = new KafkaProducer<String, byte[]>(props, new StringSerializer(), new ByteArraySerializer());}
0
public void produce(String topic, String k, String v)
{    produce(topic, k, v.getBytes());}
0
public void produce(String topic, String k, byte[] v)
{    ProducerRecord<String, byte[]> rec = new ProducerRecord<String, byte[]>(topic, k, v);    try {        producer.send(rec).get();    } catch (InterruptedException e) {        e.printStackTrace();    } catch (ExecutionException e) {        e.printStackTrace();    }}
0
public void produce(String topic, int partition, String k, String v)
{    produce(topic, partition, k, v.getBytes());}
0
public void produce(String topic, int partition, String k, byte[] v)
{    ProducerRecord<String, byte[]> rec = new ProducerRecord<String, byte[]>(topic, partition, k, v);    try {        producer.send(rec).get();    } catch (InterruptedException e) {        e.printStackTrace();    } catch (ExecutionException e) {        e.printStackTrace();    }}
0
public void createTopic(String topicName, int numPartitions)
{    AdminClient adminClient = getAdminClient();    NewTopic newTopic = new NewTopic(topicName, numPartitions, (short) 1);    adminClient.createTopics(Collections.singletonList(newTopic));        DescribeTopicsResult dtr = adminClient.describeTopics(Collections.singletonList(topicName));    try {        dtr.all().get(10, TimeUnit.SECONDS);    } catch (Exception e) {        throw new RuntimeException("Error getting topic info", e);    }}
0
private AdminClient getAdminClient()
{    if (adminClient == null) {        final Properties props = new Properties();        props.put(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, HOST + ":" + serverPort);        props.put(ConsumerConfig.GROUP_ID_CONFIG, "group_1");        adminClient = AdminClient.create(props);    }    return adminClient;}
0
public void deleteTopics(List<String> topic)
{    getAdminClient().deleteTopics(topic);}
0
public void stopZookeeper() throws IOException
{    zookeeper.shutdown();    factory.shutdown();    FileUtils.deleteDirectory(dir);}
0
public String getConnectString()
{    return KafkaSourceEmbeddedKafka.HOST + ":" + zkPort;}
0
public static void startKafkaServer()
{    kafkaServer = new KafkaSourceEmbeddedKafka(null);    startupCheck();}
0
public void setup() throws Exception
{    kafkaSource = new KafkaSource();    try {        topic0 = findUnusedTopic();        kafkaServer.createTopic(topic0, 1);        usedTopics.add(topic0);        topic1 = findUnusedTopic();        kafkaServer.createTopic(topic1, 3);        usedTopics.add(topic1);    } catch (TopicExistsException e) {                e.printStackTrace();    }    context = prepareDefaultContext("flume-group");    kafkaSource.setChannelProcessor(createGoodChannel());}
0
private static void startupCheck()
{    String startupTopic = "startupCheck";    KafkaConsumer<String, String> startupConsumer;    kafkaServer.createTopic(startupTopic, 1);    final Properties props = new Properties();    props.put(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, kafkaServer.getBootstrapServers());    props.put(ConsumerConfig.GROUP_ID_CONFIG, "group_1");    props.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class.getName());    props.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class.getName());    KafkaConsumer<String, String> consumer = new KafkaConsumer<>(props);    consumer.subscribe(Collections.singletonList(startupTopic));        boolean success = false;    for (int i = 0; i < 20; i++) {        kafkaServer.produce(startupTopic, "", "record");        ConsumerRecords recs = consumer.poll(Duration.ofMillis(1000L));        if (!recs.isEmpty()) {            success = true;            break;        }    }    if (!success) {        fail("Kafka server startup failed");    }        consumer.close();    kafkaServer.deleteTopics(Collections.singletonList(startupTopic));}
1
private Context prepareDefaultContext(String groupId)
{    Context context = new Context();    context.put(BOOTSTRAP_SERVERS, kafkaServer.getBootstrapServers());    context.put(KAFKA_CONSUMER_PREFIX + "group.id", groupId);    return context;}
0
public void tearDown() throws Exception
{    try {        kafkaSource.stop();    } catch (Exception e) {            }    topic0 = null;    topic1 = null;    kafkaServer.deleteTopics(usedTopics);    usedTopics.clear();}
1
public static void stopKafkaServer() throws Exception
{    kafkaServer.stop();}
0
private void startKafkaSource() throws EventDeliveryException, InterruptedException
{    kafkaSource.start();    /* Timing magic: We call the process method, that executes a consumer.poll()      A thread.sleep(10000L) does not work even though it takes longer */    for (int i = 0; i < 3; i++) {        kafkaSource.process();        Thread.sleep(1000);    }}
0
public void testOffsets() throws InterruptedException, EventDeliveryException
{    long batchDuration = 2000;    context.put(TOPICS, topic1);    context.put(BATCH_DURATION_MS, String.valueOf(batchDuration));    context.put(BATCH_SIZE, "3");    kafkaSource.configure(context);    startKafkaSource();    Thread.sleep(500L);    Status status = kafkaSource.process();    assertEquals(Status.BACKOFF, status);    assertEquals(0, events.size());    kafkaServer.produce(topic1, "", "record1");    kafkaServer.produce(topic1, "", "record2");    Thread.sleep(500L);    status = kafkaSource.process();    assertEquals(Status.READY, status);    assertEquals(2, events.size());    events.clear();    kafkaServer.produce(topic1, "", "record3");    kafkaServer.produce(topic1, "", "record4");    kafkaServer.produce(topic1, "", "record5");    Thread.sleep(500L);    assertEquals(Status.READY, kafkaSource.process());    assertEquals(3, events.size());    assertEquals("record3", new String(events.get(0).getBody(), Charsets.UTF_8));    assertEquals("record4", new String(events.get(1).getBody(), Charsets.UTF_8));    assertEquals("record5", new String(events.get(2).getBody(), Charsets.UTF_8));    events.clear();    kafkaServer.produce(topic1, "", "record6");    kafkaServer.produce(topic1, "", "record7");    kafkaServer.produce(topic1, "", "record8");    kafkaServer.produce(topic1, "", "record9");    kafkaServer.produce(topic1, "", "record10");    Thread.sleep(500L);    assertEquals(Status.READY, kafkaSource.process());    assertEquals(3, events.size());    assertEquals("record6", new String(events.get(0).getBody(), Charsets.UTF_8));    assertEquals("record7", new String(events.get(1).getBody(), Charsets.UTF_8));    assertEquals("record8", new String(events.get(2).getBody(), Charsets.UTF_8));    events.clear();    kafkaServer.produce(topic1, "", "record11");        assertEquals(Status.READY, kafkaSource.process());    assertEquals(3, events.size());    assertEquals("record9", new String(events.get(0).getBody(), Charsets.UTF_8));    assertEquals("record10", new String(events.get(1).getBody(), Charsets.UTF_8));    assertEquals("record11", new String(events.get(2).getBody(), Charsets.UTF_8));    events.clear();    kafkaServer.produce(topic1, "", "record12");    kafkaServer.produce(topic1, "", "record13");        kafkaSource.stop();        kafkaSource = new KafkaSource();    kafkaSource.setChannelProcessor(createGoodChannel());    kafkaSource.configure(context);    startKafkaSource();    kafkaServer.produce(topic1, "", "record14");    Thread.sleep(1000L);    assertEquals(Status.READY, kafkaSource.process());    assertEquals(3, events.size());    assertEquals("record12", new String(events.get(0).getBody(), Charsets.UTF_8));    assertEquals("record13", new String(events.get(1).getBody(), Charsets.UTF_8));    assertEquals("record14", new String(events.get(2).getBody(), Charsets.UTF_8));    events.clear();}
0
public void testProcessItNotEmpty() throws EventDeliveryException, SecurityException, NoSuchFieldException, IllegalArgumentException, IllegalAccessException, InterruptedException
{    context.put(TOPICS, topic0);    context.put(BATCH_SIZE, "1");    kafkaSource.configure(context);    startKafkaSource();    Thread.sleep(500L);    kafkaServer.produce(topic0, "", "hello, world");    Thread.sleep(500L);    Assert.assertEquals(Status.READY, kafkaSource.process());    Assert.assertEquals(Status.BACKOFF, kafkaSource.process());    Assert.assertEquals(1, events.size());    Assert.assertEquals("hello, world", new String(events.get(0).getBody(), Charsets.UTF_8));}
0
public void testProcessItNotEmptyBatch() throws EventDeliveryException, SecurityException, NoSuchFieldException, IllegalArgumentException, IllegalAccessException, InterruptedException
{    context.put(TOPICS, topic0);    context.put(BATCH_SIZE, "2");    kafkaSource.configure(context);    startKafkaSource();    Thread.sleep(500L);    kafkaServer.produce(topic0, "", "hello, world");    kafkaServer.produce(topic0, "", "foo, bar");    Thread.sleep(500L);    Status status = kafkaSource.process();    assertEquals(Status.READY, status);    Assert.assertEquals("hello, world", new String(events.get(0).getBody(), Charsets.UTF_8));    Assert.assertEquals("foo, bar", new String(events.get(1).getBody(), Charsets.UTF_8));}
0
public void testProcessItEmpty() throws EventDeliveryException, SecurityException, NoSuchFieldException, IllegalArgumentException, IllegalAccessException, InterruptedException
{    context.put(TOPICS, topic0);    kafkaSource.configure(context);    startKafkaSource();    Thread.sleep(500L);    Status status = kafkaSource.process();    assertEquals(Status.BACKOFF, status);}
0
public void testNonExistingTopic() throws EventDeliveryException, SecurityException, NoSuchFieldException, IllegalArgumentException, IllegalAccessException, InterruptedException
{    context.put(TOPICS, "faketopic");    kafkaSource.configure(context);    startKafkaSource();    Thread.sleep(500L);    assertEquals(LifecycleState.START, kafkaSource.getLifecycleState());    Status status = kafkaSource.process();    assertEquals(Status.BACKOFF, status);}
0
public void testNonExistingKafkaServer() throws EventDeliveryException, SecurityException, NoSuchFieldException, IllegalArgumentException, IllegalAccessException, InterruptedException
{    context.put(TOPICS, topic0);    context.put(BOOTSTRAP_SERVERS, "blabla:666");    kafkaSource.configure(context);    startKafkaSource();    Thread.sleep(500L);    Status status = kafkaSource.process();    assertEquals(Status.BACKOFF, status);}
0
public void testBatchTime() throws InterruptedException, EventDeliveryException
{    context.put(TOPICS, topic0);    context.put(BATCH_DURATION_MS, "250");    kafkaSource.configure(context);    startKafkaSource();        kafkaSource.process();    Thread.sleep(500L);    for (int i = 1; i < 5000; i++) {        kafkaServer.produce(topic0, "", "hello, world " + i);    }    Thread.sleep(500L);    long error = 50;    long startTime = System.currentTimeMillis();    Status status = kafkaSource.process();    long endTime = System.currentTimeMillis();    assertEquals(Status.READY, status);    assertTrue(endTime - startTime < (context.getLong(BATCH_DURATION_MS) + error));}
0
public void testCommit() throws InterruptedException, EventDeliveryException
{    context.put(TOPICS, topic0);    context.put(BATCH_SIZE, "1");    kafkaSource.configure(context);    startKafkaSource();    Thread.sleep(500L);    kafkaServer.produce(topic0, "", "hello, world");    Thread.sleep(500L);    Assert.assertEquals(Status.READY, kafkaSource.process());    kafkaSource.stop();    Thread.sleep(500L);    startKafkaSource();    Thread.sleep(500L);    Assert.assertEquals(Status.BACKOFF, kafkaSource.process());}
0
public void testNonCommit() throws EventDeliveryException, InterruptedException
{    context.put(TOPICS, topic0);    context.put(BATCH_SIZE, "1");    context.put(BATCH_DURATION_MS, "30000");    kafkaSource.configure(context);    startKafkaSource();    Thread.sleep(500L);    kafkaServer.produce(topic0, "", "hello, world");    Thread.sleep(500L);    kafkaSource.setChannelProcessor(createBadChannel());        Assert.assertEquals(Status.BACKOFF, kafkaSource.process());        kafkaSource.setChannelProcessor(createGoodChannel());        kafkaSource.process();    Assert.assertEquals("hello, world", new String(events.get(0).getBody(), Charsets.UTF_8));}
1
public void testTwoBatches() throws InterruptedException, EventDeliveryException
{    context.put(TOPICS, topic0);    context.put(BATCH_SIZE, "1");    context.put(BATCH_DURATION_MS, "30000");    kafkaSource.configure(context);    startKafkaSource();    Thread.sleep(500L);    kafkaServer.produce(topic0, "", "event 1");    Thread.sleep(500L);    kafkaSource.process();    Assert.assertEquals("event 1", new String(events.get(0).getBody(), Charsets.UTF_8));    events.clear();    kafkaServer.produce(topic0, "", "event 2");    Thread.sleep(500L);    kafkaSource.process();    Assert.assertEquals("event 2", new String(events.get(0).getBody(), Charsets.UTF_8));}
0
public void testTwoBatchesWithAutocommit() throws InterruptedException, EventDeliveryException
{    context.put(TOPICS, topic0);    context.put(BATCH_SIZE, "1");    context.put(BATCH_DURATION_MS, "30000");    context.put(KAFKA_CONSUMER_PREFIX + "enable.auto.commit", "true");    kafkaSource.configure(context);    startKafkaSource();    Thread.sleep(500L);    kafkaServer.produce(topic0, "", "event 1");    Thread.sleep(500L);    kafkaSource.process();    Assert.assertEquals("event 1", new String(events.get(0).getBody(), Charsets.UTF_8));    events.clear();    kafkaServer.produce(topic0, "", "event 2");    Thread.sleep(500L);    kafkaSource.process();    Assert.assertEquals("event 2", new String(events.get(0).getBody(), Charsets.UTF_8));}
0
public void testNullKey() throws EventDeliveryException, SecurityException, NoSuchFieldException, IllegalArgumentException, IllegalAccessException, InterruptedException
{    context.put(TOPICS, topic0);    context.put(BATCH_SIZE, "1");    kafkaSource.configure(context);    startKafkaSource();    Thread.sleep(500L);    kafkaServer.produce(topic0, null, "hello, world");    Thread.sleep(500L);    Assert.assertEquals(Status.READY, kafkaSource.process());    Assert.assertEquals(Status.BACKOFF, kafkaSource.process());    Assert.assertEquals(1, events.size());    Assert.assertEquals("hello, world", new String(events.get(0).getBody(), Charsets.UTF_8));}
0
public void testErrorCounters() throws InterruptedException, EventDeliveryException
{    context.put(TOPICS, topic0);    context.put(BATCH_SIZE, "1");    kafkaSource.configure(context);    ChannelProcessor cp = Mockito.mock(ChannelProcessor.class);    doThrow(new ChannelException("dummy")).doThrow(new RuntimeException("dummy")).when(cp).processEventBatch(any(List.class));    kafkaSource.setChannelProcessor(cp);    startKafkaSource();    Thread.sleep(500L);    kafkaServer.produce(topic0, "", "hello, world");    Thread.sleep(500L);    kafkaSource.doProcess();    kafkaSource.doProcess();    SourceCounter sc = (SourceCounter) Whitebox.getInternalState(kafkaSource, "counter");    Assert.assertEquals(1, sc.getChannelWriteFail());    Assert.assertEquals(1, sc.getEventReadFail());    kafkaSource.stop();}
0
public void testSourceProperties()
{    Context context = new Context();    context.put(TOPICS, "test1, test2");    context.put(TOPICS_REGEX, "^stream[0-9]$");    context.put(BOOTSTRAP_SERVERS, "bootstrap-servers-list");    KafkaSource source = new KafkaSource();    source.doConfigure(context);            KafkaSource.Subscriber<Pattern> subscriber = source.getSubscriber();    Pattern pattern = subscriber.get();    Assert.assertTrue(pattern.matcher("stream1").find());}
0
public void testKafkaProperties()
{    Context context = new Context();    context.put(TOPICS, "test1, test2");    context.put(KAFKA_CONSUMER_PREFIX + ConsumerConfig.GROUP_ID_CONFIG, "override.default.group.id");    context.put(KAFKA_CONSUMER_PREFIX + "fake.property", "kafka.property.value");    context.put(BOOTSTRAP_SERVERS, "real-bootstrap-servers-list");    context.put(KAFKA_CONSUMER_PREFIX + "bootstrap.servers", "bad-bootstrap-servers-list");    KafkaSource source = new KafkaSource();    source.doConfigure(context);    Properties kafkaProps = source.getConsumerProps();        assertEquals(String.valueOf(DEFAULT_AUTO_COMMIT), kafkaProps.getProperty(ConsumerConfig.ENABLE_AUTO_COMMIT_CONFIG));        assertEquals("override.default.group.id", kafkaProps.getProperty(ConsumerConfig.GROUP_ID_CONFIG));        assertEquals("kafka.property.value", kafkaProps.getProperty("fake.property"));        assertEquals("real-bootstrap-servers-list", kafkaProps.getProperty("bootstrap.servers"));}
0
public void testOldProperties()
{    Context context = new Context();    context.put(TOPIC, "old.topic");    context.put(OLD_GROUP_ID, "old.groupId");    context.put(BOOTSTRAP_SERVERS, "real-bootstrap-servers-list");    KafkaSource source = new KafkaSource();    source.doConfigure(context);    Properties kafkaProps = source.getConsumerProps();    KafkaSource.Subscriber<List<String>> subscriber = source.getSubscriber();        assertEquals("old.topic", subscriber.get().get(0));        assertEquals("old.groupId", kafkaProps.getProperty(ConsumerConfig.GROUP_ID_CONFIG));    source = new KafkaSource();    context.put(KAFKA_CONSUMER_PREFIX + ConsumerConfig.GROUP_ID_CONFIG, "override.old.group.id");    source.doConfigure(context);    kafkaProps = source.getConsumerProps();        assertEquals("override.old.group.id", kafkaProps.getProperty(ConsumerConfig.GROUP_ID_CONFIG));    context.clear();    context.put(BOOTSTRAP_SERVERS, "real-bootstrap-servers-list");    context.put(TOPIC, "old.topic");    source = new KafkaSource();    source.doConfigure(context);    kafkaProps = source.getConsumerProps();        assertEquals(KafkaSourceConstants.DEFAULT_GROUP_ID, kafkaProps.getProperty(ConsumerConfig.GROUP_ID_CONFIG));}
0
public void testPatternBasedSubscription()
{    Context context = new Context();    context.put(TOPICS_REGEX, "^topic[0-9]$");    context.put(OLD_GROUP_ID, "old.groupId");    context.put(BOOTSTRAP_SERVERS, "real-bootstrap-servers-list");    KafkaSource source = new KafkaSource();    source.doConfigure(context);    KafkaSource.Subscriber<Pattern> subscriber = source.getSubscriber();    for (int i = 0; i < 10; i++) {        Assert.assertTrue(subscriber.get().matcher("topic" + i).find());    }    Assert.assertFalse(subscriber.get().matcher("topic").find());}
0
public void testAvroEvent() throws InterruptedException, EventDeliveryException, IOException
{    SpecificDatumWriter<AvroFlumeEvent> writer;    ByteArrayOutputStream tempOutStream;    BinaryEncoder encoder;    byte[] bytes;    context.put(TOPICS, topic0);    context.put(BATCH_SIZE, "1");    context.put(AVRO_EVENT, "true");    kafkaSource.configure(context);    startKafkaSource();    Thread.sleep(500L);    tempOutStream = new ByteArrayOutputStream();    writer = new SpecificDatumWriter<AvroFlumeEvent>(AvroFlumeEvent.class);    Map<CharSequence, CharSequence> headers = new HashMap<CharSequence, CharSequence>();    headers.put("header1", "value1");    headers.put("header2", "value2");    AvroFlumeEvent e = new AvroFlumeEvent(headers, ByteBuffer.wrap("hello, world".getBytes()));    encoder = EncoderFactory.get().directBinaryEncoder(tempOutStream, null);    writer.write(e, encoder);    encoder.flush();    bytes = tempOutStream.toByteArray();    kafkaServer.produce(topic0, "", bytes);    String currentTimestamp = Long.toString(System.currentTimeMillis());    headers.put(TIMESTAMP_HEADER, currentTimestamp);    headers.put(PARTITION_HEADER, "1");    headers.put(DEFAULT_TOPIC_HEADER, "topic0");    e = new AvroFlumeEvent(headers, ByteBuffer.wrap("hello, world2".getBytes()));    tempOutStream.reset();    encoder = EncoderFactory.get().directBinaryEncoder(tempOutStream, null);    writer.write(e, encoder);    encoder.flush();    bytes = tempOutStream.toByteArray();    kafkaServer.produce(topic0, "", bytes);    Thread.sleep(500L);    Assert.assertEquals(Status.READY, kafkaSource.process());    Assert.assertEquals(Status.READY, kafkaSource.process());    Assert.assertEquals(Status.BACKOFF, kafkaSource.process());    Assert.assertEquals(2, events.size());    Event event = events.get(0);    Assert.assertEquals("hello, world", new String(event.getBody(), Charsets.UTF_8));    Assert.assertEquals("value1", e.getHeaders().get("header1"));    Assert.assertEquals("value2", e.getHeaders().get("header2"));    event = events.get(1);    Assert.assertEquals("hello, world2", new String(event.getBody(), Charsets.UTF_8));    Assert.assertEquals("value1", e.getHeaders().get("header1"));    Assert.assertEquals("value2", e.getHeaders().get("header2"));    Assert.assertEquals(currentTimestamp, e.getHeaders().get(TIMESTAMP_HEADER));    Assert.assertEquals(e.getHeaders().get(PARTITION_HEADER), "1");    Assert.assertEquals(e.getHeaders().get(DEFAULT_TOPIC_HEADER), "topic0");}
0
public void testBootstrapLookup()
{    Context context = new Context();    context.put(ZOOKEEPER_CONNECT_FLUME_KEY, kafkaServer.getZkConnectString());    context.put(TOPIC, "old.topic");    context.put(OLD_GROUP_ID, "old.groupId");    KafkaSource source = new KafkaSource();    source.doConfigure(context);    String bootstrapServers = source.getBootstrapServers();    Assert.assertEquals(kafkaServer.getBootstrapServers(), bootstrapServers);}
0
public void testMigrateOffsetsNone() throws Exception
{    doTestMigrateZookeeperOffsets(false, false, "testMigrateOffsets-none");}
0
public void testMigrateOffsetsZookeeper() throws Exception
{    doTestMigrateZookeeperOffsets(true, false, "testMigrateOffsets-zookeeper");}
0
public void testMigrateOffsetsKafka() throws Exception
{    doTestMigrateZookeeperOffsets(false, true, "testMigrateOffsets-kafka");}
0
public void testMigrateOffsetsBoth() throws Exception
{    doTestMigrateZookeeperOffsets(true, true, "testMigrateOffsets-both");}
0
public void testDefaultSettingsOnReConfigure() throws Exception
{    String sampleConsumerProp = "auto.offset.reset";    String sampleConsumerVal = "earliest";    String group = "group";    Context context = prepareDefaultContext(group);    context.put(KafkaSourceConstants.KAFKA_CONSUMER_PREFIX + sampleConsumerProp, sampleConsumerVal);    context.put(TOPIC, "random-topic");    kafkaSource.configure(context);    Assert.assertEquals(sampleConsumerVal, kafkaSource.getConsumerProps().getProperty(sampleConsumerProp));    context = prepareDefaultContext(group);    context.put(TOPIC, "random-topic");    kafkaSource.configure(context);    Assert.assertNull(kafkaSource.getConsumerProps().getProperty(sampleConsumerProp));}
0
public void testTopicHeaderSet() throws InterruptedException, EventDeliveryException
{    context.put(TOPICS, topic0);    kafkaSource.configure(context);    startKafkaSource();    Thread.sleep(500L);    kafkaServer.produce(topic0, "", "hello, world");    Thread.sleep(500L);    Status status = kafkaSource.process();    assertEquals(Status.READY, status);    Assert.assertEquals("hello, world", new String(events.get(0).getBody(), Charsets.UTF_8));    Assert.assertEquals(topic0, events.get(0).getHeaders().get("topic"));    kafkaSource.stop();    events.clear();}
0
public void testTopicCustomHeaderSet() throws InterruptedException, EventDeliveryException
{    context.put(TOPICS, topic0);    context.put(KafkaSourceConstants.TOPIC_HEADER, "customTopicHeader");    kafkaSource.configure(context);    startKafkaSource();    Thread.sleep(500L);    kafkaServer.produce(topic0, "", "hello, world2");    Thread.sleep(500L);    Status status = kafkaSource.process();    assertEquals(Status.READY, status);    Assert.assertEquals("hello, world2", new String(events.get(0).getBody(), Charsets.UTF_8));    Assert.assertEquals(topic0, events.get(0).getHeaders().get("customTopicHeader"));    kafkaSource.stop();    events.clear();}
0
public void testTopicCustomHeaderNotSet() throws InterruptedException, EventDeliveryException
{    context.put(TOPICS, topic0);    context.put(KafkaSourceConstants.SET_TOPIC_HEADER, "false");    kafkaSource.configure(context);    startKafkaSource();    Thread.sleep(500L);    kafkaServer.produce(topic0, "", "hello, world3");    Thread.sleep(500L);    Status status = kafkaSource.process();    assertEquals(Status.READY, status);    Assert.assertEquals("hello, world3", new String(events.get(0).getBody(), Charsets.UTF_8));    Assert.assertNull(events.get(0).getHeaders().get("customTopicHeader"));    kafkaSource.stop();}
0
private void doTestMigrateZookeeperOffsets(boolean hasZookeeperOffsets, boolean hasKafkaOffsets, String group) throws Exception
{        String topic = findUnusedTopic();    kafkaServer.createTopic(topic, 1);    Context context = prepareDefaultContext(group);    context.put(ZOOKEEPER_CONNECT_FLUME_KEY, kafkaServer.getZkConnectString());    context.put(TOPIC, topic);    KafkaSource source = new KafkaSource();    source.doConfigure(context);        Long fifthOffset = 0L;    Long tenthOffset = 0L;    Properties props = createProducerProps(kafkaServer.getBootstrapServers());    KafkaProducer<String, byte[]> producer = new KafkaProducer<>(props);    for (int i = 1; i <= 50; i++) {        ProducerRecord<String, byte[]> data = new ProducerRecord<>(topic, null, String.valueOf(i).getBytes());        RecordMetadata recordMetadata = producer.send(data).get();        if (i == 5) {            fifthOffset = recordMetadata.offset();        }        if (i == 10) {            tenthOffset = recordMetadata.offset();        }    }        if (hasZookeeperOffsets) {        KafkaZkClient zkClient = KafkaZkClient.apply(kafkaServer.getZkConnectString(), JaasUtils.isZkSecurityEnabled(), 30000, 30000, 10, Time.SYSTEM, "kafka.server", "SessionExpireListener");        zkClient.getConsumerOffset(group, new TopicPartition(topic, 0));        Long offset = tenthOffset + 1;        zkClient.setOrCreateConsumerOffset(group, new TopicPartition(topic, 0), offset);        zkClient.close();    }        if (hasKafkaOffsets) {        Map<TopicPartition, OffsetAndMetadata> offsets = new HashMap<>();        offsets.put(new TopicPartition(topic, 0), new OffsetAndMetadata(fifthOffset + 1));        KafkaConsumer<String, byte[]> consumer = new KafkaConsumer<>(source.getConsumerProps());        consumer.commitSync(offsets);        consumer.close();    }        source.setChannelProcessor(createGoodChannel());    source.start();    for (int i = 0; i < 3; i++) {        source.process();        Thread.sleep(1000);    }    Thread.sleep(500L);    source.process();    List<Integer> finals = new ArrayList<Integer>(40);    for (Event event : events) {        finals.add(Integer.parseInt(new String(event.getBody())));    }    source.stop();    if (!hasKafkaOffsets && !hasZookeeperOffsets) {                org.junit.Assert.assertTrue("Source should read no messages", finals.isEmpty());    } else if (hasKafkaOffsets && hasZookeeperOffsets) {                org.junit.Assert.assertFalse("Source should not read the 5th message", finals.contains(5));        org.junit.Assert.assertTrue("Source should read the 6th message", finals.contains(6));    } else if (hasKafkaOffsets) {                org.junit.Assert.assertFalse("Source should not read the 5th message", finals.contains(5));        org.junit.Assert.assertTrue("Source should read the 6th message", finals.contains(6));    } else {                org.junit.Assert.assertFalse("Source should not read the 10th message", finals.contains(10));        org.junit.Assert.assertTrue("Source should read the 11th message", finals.contains(11));    }}
0
public void testMigrateZookeeperOffsetsWhenTopicNotExists() throws Exception
{    String topic = findUnusedTopic();    Context context = prepareDefaultContext("testMigrateOffsets-nonExistingTopic");    context.put(ZOOKEEPER_CONNECT_FLUME_KEY, kafkaServer.getZkConnectString());    context.put(TOPIC, topic);    KafkaSource source = new KafkaSource();    source.doConfigure(context);    source.setChannelProcessor(createGoodChannel());    source.start();    assertEquals(LifecycleState.START, source.getLifecycleState());    Status status = source.process();    assertEquals(Status.BACKOFF, status);    source.stop();}
0
 ChannelProcessor createGoodChannel()
{    ChannelProcessor channelProcessor = mock(ChannelProcessor.class);    events = Lists.newArrayList();    doAnswer(new Answer<Void>() {        @Override        public Void answer(InvocationOnMock invocation) throws Throwable {            events.addAll((List<Event>) invocation.getArguments()[0]);            return null;        }    }).when(channelProcessor).processEventBatch(any(List.class));    return channelProcessor;}
0
public Void answer(InvocationOnMock invocation) throws Throwable
{    events.addAll((List<Event>) invocation.getArguments()[0]);    return null;}
0
 ChannelProcessor createBadChannel()
{    ChannelProcessor channelProcessor = mock(ChannelProcessor.class);    doAnswer(new Answer<Void>() {        @Override        public Void answer(InvocationOnMock invocation) throws Throwable {            throw new ChannelException("channel intentional broken");        }    }).when(channelProcessor).processEventBatch(any(List.class));    return channelProcessor;}
0
public Void answer(InvocationOnMock invocation) throws Throwable
{    throw new ChannelException("channel intentional broken");}
0
public String findUnusedTopic()
{    String newTopic = null;    boolean topicFound = false;    while (!topicFound) {        newTopic = RandomStringUtils.randomAlphabetic(8);        if (!usedTopics.contains(newTopic)) {            usedTopics.add(newTopic);            topicFound = true;        }    }    return newTopic;}
0
private Properties createProducerProps(String bootStrapServers)
{    Properties props = new Properties();    props.put(ProducerConfig.ACKS_CONFIG, "-1");    props.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, "org.apache.kafka.common.serialization.StringSerializer");    props.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, "org.apache.kafka.common.serialization.ByteArraySerializer");    props.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, bootStrapServers);    return props;}
0
public static _Fields findByThriftId(int fieldId)
{    switch(fieldId) {        case         1:            return CATEGORY;        case         2:            return MESSAGE;        default:            return null;    }}
0
public static _Fields findByThriftIdOrThrow(int fieldId)
{    _Fields fields = findByThriftId(fieldId);    if (fields == null)        throw new IllegalArgumentException("Field " + fieldId + " doesn't exist!");    return fields;}
0
public static _Fields findByName(String name)
{    return byName.get(name);}
0
public short getThriftFieldId()
{    return _thriftId;}
0
public String getFieldName()
{    return _fieldName;}
0
public LogEntry deepCopy()
{    return new LogEntry(this);}
0
public void clear()
{    this.category = null;    this.message = null;}
0
public String getCategory()
{    return this.category;}
0
public LogEntry setCategory(String category)
{    this.category = category;    return this;}
0
public void unsetCategory()
{    this.category = null;}
0
public boolean isSetCategory()
{    return this.category != null;}
0
public void setCategoryIsSet(boolean value)
{    if (!value) {        this.category = null;    }}
0
public String getMessage()
{    return this.message;}
0
public LogEntry setMessage(String message)
{    this.message = message;    return this;}
0
public void unsetMessage()
{    this.message = null;}
0
public boolean isSetMessage()
{    return this.message != null;}
0
public void setMessageIsSet(boolean value)
{    if (!value) {        this.message = null;    }}
0
public void setFieldValue(_Fields field, Object value)
{    switch(field) {        case CATEGORY:            if (value == null) {                unsetCategory();            } else {                setCategory((String) value);            }            break;        case MESSAGE:            if (value == null) {                unsetMessage();            } else {                setMessage((String) value);            }            break;    }}
0
public Object getFieldValue(_Fields field)
{    switch(field) {        case CATEGORY:            return getCategory();        case MESSAGE:            return getMessage();    }    throw new IllegalStateException();}
0
public boolean isSet(_Fields field)
{    if (field == null) {        throw new IllegalArgumentException();    }    switch(field) {        case CATEGORY:            return isSetCategory();        case MESSAGE:            return isSetMessage();    }    throw new IllegalStateException();}
0
public boolean equals(Object that)
{    if (that == null)        return false;    if (that instanceof LogEntry)        return this.equals((LogEntry) that);    return false;}
0
public boolean equals(LogEntry that)
{    if (that == null)        return false;    boolean this_present_category = true && this.isSetCategory();    boolean that_present_category = true && that.isSetCategory();    if (this_present_category || that_present_category) {        if (!(this_present_category && that_present_category))            return false;        if (!this.category.equals(that.category))            return false;    }    boolean this_present_message = true && this.isSetMessage();    boolean that_present_message = true && that.isSetMessage();    if (this_present_message || that_present_message) {        if (!(this_present_message && that_present_message))            return false;        if (!this.message.equals(that.message))            return false;    }    return true;}
0
public int hashCode()
{    List<Object> list = new ArrayList<Object>();    boolean present_category = true && (isSetCategory());    list.add(present_category);    if (present_category)        list.add(category);    boolean present_message = true && (isSetMessage());    list.add(present_message);    if (present_message)        list.add(message);    return list.hashCode();}
0
public int compareTo(LogEntry other)
{    if (!getClass().equals(other.getClass())) {        return getClass().getName().compareTo(other.getClass().getName());    }    int lastComparison = 0;    lastComparison = Boolean.valueOf(isSetCategory()).compareTo(other.isSetCategory());    if (lastComparison != 0) {        return lastComparison;    }    if (isSetCategory()) {        lastComparison = org.apache.thrift.TBaseHelper.compareTo(this.category, other.category);        if (lastComparison != 0) {            return lastComparison;        }    }    lastComparison = Boolean.valueOf(isSetMessage()).compareTo(other.isSetMessage());    if (lastComparison != 0) {        return lastComparison;    }    if (isSetMessage()) {        lastComparison = org.apache.thrift.TBaseHelper.compareTo(this.message, other.message);        if (lastComparison != 0) {            return lastComparison;        }    }    return 0;}
0
public _Fields fieldForId(int fieldId)
{    return _Fields.findByThriftId(fieldId);}
0
public void read(org.apache.thrift.protocol.TProtocol iprot) throws org.apache.thrift.TException
{    schemes.get(iprot.getScheme()).getScheme().read(iprot, this);}
0
public void write(org.apache.thrift.protocol.TProtocol oprot) throws org.apache.thrift.TException
{    schemes.get(oprot.getScheme()).getScheme().write(oprot, this);}
0
public String toString()
{    StringBuilder sb = new StringBuilder("LogEntry(");    boolean first = true;    sb.append("category:");    if (this.category == null) {        sb.append("null");    } else {        sb.append(this.category);    }    first = false;    if (!first)        sb.append(", ");    sb.append("message:");    if (this.message == null) {        sb.append("null");    } else {        sb.append(this.message);    }    first = false;    sb.append(")");    return sb.toString();}
0
public void validate() throws org.apache.thrift.TException
{}
0
private void writeObject(java.io.ObjectOutputStream out) throws java.io.IOException
{    try {        write(new org.apache.thrift.protocol.TCompactProtocol(new org.apache.thrift.transport.TIOStreamTransport(out)));    } catch (org.apache.thrift.TException te) {        throw new java.io.IOException(te);    }}
0
private void readObject(java.io.ObjectInputStream in) throws java.io.IOException, ClassNotFoundException
{    try {        read(new org.apache.thrift.protocol.TCompactProtocol(new org.apache.thrift.transport.TIOStreamTransport(in)));    } catch (org.apache.thrift.TException te) {        throw new java.io.IOException(te);    }}
0
public LogEntryStandardScheme getScheme()
{    return new LogEntryStandardScheme();}
0
public void read(org.apache.thrift.protocol.TProtocol iprot, LogEntry struct) throws org.apache.thrift.TException
{    org.apache.thrift.protocol.TField schemeField;    iprot.readStructBegin();    while (true) {        schemeField = iprot.readFieldBegin();        if (schemeField.type == org.apache.thrift.protocol.TType.STOP) {            break;        }        switch(schemeField.id) {            case             1:                if (schemeField.type == org.apache.thrift.protocol.TType.STRING) {                    struct.category = iprot.readString();                    struct.setCategoryIsSet(true);                } else {                    org.apache.thrift.protocol.TProtocolUtil.skip(iprot, schemeField.type);                }                break;            case             2:                if (schemeField.type == org.apache.thrift.protocol.TType.STRING) {                    struct.message = iprot.readString();                    struct.setMessageIsSet(true);                } else {                    org.apache.thrift.protocol.TProtocolUtil.skip(iprot, schemeField.type);                }                break;            default:                org.apache.thrift.protocol.TProtocolUtil.skip(iprot, schemeField.type);        }        iprot.readFieldEnd();    }    iprot.readStructEnd();        struct.validate();}
0
public void write(org.apache.thrift.protocol.TProtocol oprot, LogEntry struct) throws org.apache.thrift.TException
{    struct.validate();    oprot.writeStructBegin(STRUCT_DESC);    if (struct.category != null) {        oprot.writeFieldBegin(CATEGORY_FIELD_DESC);        oprot.writeString(struct.category);        oprot.writeFieldEnd();    }    if (struct.message != null) {        oprot.writeFieldBegin(MESSAGE_FIELD_DESC);        oprot.writeString(struct.message);        oprot.writeFieldEnd();    }    oprot.writeFieldStop();    oprot.writeStructEnd();}
0
public LogEntryTupleScheme getScheme()
{    return new LogEntryTupleScheme();}
0
public void write(org.apache.thrift.protocol.TProtocol prot, LogEntry struct) throws org.apache.thrift.TException
{    TTupleProtocol oprot = (TTupleProtocol) prot;    BitSet optionals = new BitSet();    if (struct.isSetCategory()) {        optionals.set(0);    }    if (struct.isSetMessage()) {        optionals.set(1);    }    oprot.writeBitSet(optionals, 2);    if (struct.isSetCategory()) {        oprot.writeString(struct.category);    }    if (struct.isSetMessage()) {        oprot.writeString(struct.message);    }}
0
public void read(org.apache.thrift.protocol.TProtocol prot, LogEntry struct) throws org.apache.thrift.TException
{    TTupleProtocol iprot = (TTupleProtocol) prot;    BitSet incoming = iprot.readBitSet(2);    if (incoming.get(0)) {        struct.category = iprot.readString();        struct.setCategoryIsSet(true);    }    if (incoming.get(1)) {        struct.message = iprot.readString();        struct.setMessageIsSet(true);    }}
0
public int getValue()
{    return value;}
0
public static ResultCode findByValue(int value)
{    switch(value) {        case 0:            return OK;        case 1:            return TRY_LATER;        default:            return null;    }}
0
public Client getClient(org.apache.thrift.protocol.TProtocol prot)
{    return new Client(prot);}
0
public Client getClient(org.apache.thrift.protocol.TProtocol iprot, org.apache.thrift.protocol.TProtocol oprot)
{    return new Client(iprot, oprot);}
0
public ResultCode Log(List<LogEntry> messages) throws org.apache.thrift.TException
{    send_Log(messages);    return recv_Log();}
0
public void send_Log(List<LogEntry> messages) throws org.apache.thrift.TException
{    Log_args args = new Log_args();    args.setMessages(messages);    sendBase("Log", args);}
0
public ResultCode recv_Log() throws org.apache.thrift.TException
{    Log_result result = new Log_result();    receiveBase(result, "Log");    if (result.isSetSuccess()) {        return result.success;    }    throw new org.apache.thrift.TApplicationException(org.apache.thrift.TApplicationException.MISSING_RESULT, "Log failed: unknown result");}
0
public AsyncClient getAsyncClient(org.apache.thrift.transport.TNonblockingTransport transport)
{    return new AsyncClient(protocolFactory, clientManager, transport);}
0
public void Log(List<LogEntry> messages, org.apache.thrift.async.AsyncMethodCallback resultHandler) throws org.apache.thrift.TException
{    checkReady();    Log_call method_call = new Log_call(messages, resultHandler, this, ___protocolFactory, ___transport);    this.___currentMethod = method_call;    ___manager.call(method_call);}
0
public void write_args(org.apache.thrift.protocol.TProtocol prot) throws org.apache.thrift.TException
{    prot.writeMessageBegin(new org.apache.thrift.protocol.TMessage("Log", org.apache.thrift.protocol.TMessageType.CALL, 0));    Log_args args = new Log_args();    args.setMessages(messages);    args.write(prot);    prot.writeMessageEnd();}
0
public ResultCode getResult() throws org.apache.thrift.TException
{    if (getState() != org.apache.thrift.async.TAsyncMethodCall.State.RESPONSE_READ) {        throw new IllegalStateException("Method call not finished!");    }    org.apache.thrift.transport.TMemoryInputTransport memoryTransport = new org.apache.thrift.transport.TMemoryInputTransport(getFrameBuffer().array());    org.apache.thrift.protocol.TProtocol prot = client.getProtocolFactory().getProtocol(memoryTransport);    return (new Client(prot)).recv_Log();}
0
private static Map<String, org.apache.thrift.ProcessFunction<I, ? extends org.apache.thrift.TBase>> getProcessMap(Map<String, org.apache.thrift.ProcessFunction<I, ? extends org.apache.thrift.TBase>> processMap)
{    processMap.put("Log", new Log());    return processMap;}
0
public Log_args getEmptyArgsInstance()
{    return new Log_args();}
0
protected boolean isOneway()
{    return false;}
0
public Log_result getResult(I iface, Log_args args) throws org.apache.thrift.TException
{    Log_result result = new Log_result();    result.success = iface.Log(args.messages);    return result;}
0
private static Map<String, org.apache.thrift.AsyncProcessFunction<I, ? extends org.apache.thrift.TBase, ?>> getProcessMap(Map<String, org.apache.thrift.AsyncProcessFunction<I, ? extends org.apache.thrift.TBase, ?>> processMap)
{    processMap.put("Log", new Log());    return processMap;}
0
public Log_args getEmptyArgsInstance()
{    return new Log_args();}
0
public AsyncMethodCallback<ResultCode> getResultHandler(final AsyncFrameBuffer fb, final int seqid)
{    final org.apache.thrift.AsyncProcessFunction fcall = this;    return new AsyncMethodCallback<ResultCode>() {        public void onComplete(ResultCode o) {            Log_result result = new Log_result();            result.success = o;            try {                fcall.sendResponse(fb, result, org.apache.thrift.protocol.TMessageType.REPLY, seqid);                return;            } catch (Exception e) {                            }            fb.close();        }        public void onError(Exception e) {            byte msgType = org.apache.thrift.protocol.TMessageType.REPLY;            org.apache.thrift.TBase msg;            Log_result result = new Log_result();            {                msgType = org.apache.thrift.protocol.TMessageType.EXCEPTION;                msg = (org.apache.thrift.TBase) new org.apache.thrift.TApplicationException(org.apache.thrift.TApplicationException.INTERNAL_ERROR, e.getMessage());            }            try {                fcall.sendResponse(fb, msg, msgType, seqid);                return;            } catch (Exception ex) {                            }            fb.close();        }    };}
1
public void onComplete(ResultCode o)
{    Log_result result = new Log_result();    result.success = o;    try {        fcall.sendResponse(fb, result, org.apache.thrift.protocol.TMessageType.REPLY, seqid);        return;    } catch (Exception e) {            }    fb.close();}
1
public void onError(Exception e)
{    byte msgType = org.apache.thrift.protocol.TMessageType.REPLY;    org.apache.thrift.TBase msg;    Log_result result = new Log_result();    {        msgType = org.apache.thrift.protocol.TMessageType.EXCEPTION;        msg = (org.apache.thrift.TBase) new org.apache.thrift.TApplicationException(org.apache.thrift.TApplicationException.INTERNAL_ERROR, e.getMessage());    }    try {        fcall.sendResponse(fb, msg, msgType, seqid);        return;    } catch (Exception ex) {            }    fb.close();}
1
protected boolean isOneway()
{    return false;}
0
public void start(I iface, Log_args args, org.apache.thrift.async.AsyncMethodCallback<ResultCode> resultHandler) throws TException
{    iface.Log(args.messages, resultHandler);}
0
public static _Fields findByThriftId(int fieldId)
{    switch(fieldId) {        case         1:            return MESSAGES;        default:            return null;    }}
0
public static _Fields findByThriftIdOrThrow(int fieldId)
{    _Fields fields = findByThriftId(fieldId);    if (fields == null)        throw new IllegalArgumentException("Field " + fieldId + " doesn't exist!");    return fields;}
0
public static _Fields findByName(String name)
{    return byName.get(name);}
0
public short getThriftFieldId()
{    return _thriftId;}
0
public String getFieldName()
{    return _fieldName;}
0
public Log_args deepCopy()
{    return new Log_args(this);}
0
public void clear()
{    this.messages = null;}
0
public int getMessagesSize()
{    return (this.messages == null) ? 0 : this.messages.size();}
0
public java.util.Iterator<LogEntry> getMessagesIterator()
{    return (this.messages == null) ? null : this.messages.iterator();}
0
public void addToMessages(LogEntry elem)
{    if (this.messages == null) {        this.messages = new ArrayList<LogEntry>();    }    this.messages.add(elem);}
0
public List<LogEntry> getMessages()
{    return this.messages;}
0
public Log_args setMessages(List<LogEntry> messages)
{    this.messages = messages;    return this;}
0
public void unsetMessages()
{    this.messages = null;}
0
public boolean isSetMessages()
{    return this.messages != null;}
0
public void setMessagesIsSet(boolean value)
{    if (!value) {        this.messages = null;    }}
0
public void setFieldValue(_Fields field, Object value)
{    switch(field) {        case MESSAGES:            if (value == null) {                unsetMessages();            } else {                setMessages((List<LogEntry>) value);            }            break;    }}
0
public Object getFieldValue(_Fields field)
{    switch(field) {        case MESSAGES:            return getMessages();    }    throw new IllegalStateException();}
0
public boolean isSet(_Fields field)
{    if (field == null) {        throw new IllegalArgumentException();    }    switch(field) {        case MESSAGES:            return isSetMessages();    }    throw new IllegalStateException();}
0
public boolean equals(Object that)
{    if (that == null)        return false;    if (that instanceof Log_args)        return this.equals((Log_args) that);    return false;}
0
public boolean equals(Log_args that)
{    if (that == null)        return false;    boolean this_present_messages = true && this.isSetMessages();    boolean that_present_messages = true && that.isSetMessages();    if (this_present_messages || that_present_messages) {        if (!(this_present_messages && that_present_messages))            return false;        if (!this.messages.equals(that.messages))            return false;    }    return true;}
0
public int hashCode()
{    List<Object> list = new ArrayList<Object>();    boolean present_messages = true && (isSetMessages());    list.add(present_messages);    if (present_messages)        list.add(messages);    return list.hashCode();}
0
public int compareTo(Log_args other)
{    if (!getClass().equals(other.getClass())) {        return getClass().getName().compareTo(other.getClass().getName());    }    int lastComparison = 0;    lastComparison = Boolean.valueOf(isSetMessages()).compareTo(other.isSetMessages());    if (lastComparison != 0) {        return lastComparison;    }    if (isSetMessages()) {        lastComparison = org.apache.thrift.TBaseHelper.compareTo(this.messages, other.messages);        if (lastComparison != 0) {            return lastComparison;        }    }    return 0;}
0
public _Fields fieldForId(int fieldId)
{    return _Fields.findByThriftId(fieldId);}
0
public void read(org.apache.thrift.protocol.TProtocol iprot) throws org.apache.thrift.TException
{    schemes.get(iprot.getScheme()).getScheme().read(iprot, this);}
0
public void write(org.apache.thrift.protocol.TProtocol oprot) throws org.apache.thrift.TException
{    schemes.get(oprot.getScheme()).getScheme().write(oprot, this);}
0
public String toString()
{    StringBuilder sb = new StringBuilder("Log_args(");    boolean first = true;    sb.append("messages:");    if (this.messages == null) {        sb.append("null");    } else {        sb.append(this.messages);    }    first = false;    sb.append(")");    return sb.toString();}
0
public void validate() throws org.apache.thrift.TException
{}
0
private void writeObject(java.io.ObjectOutputStream out) throws java.io.IOException
{    try {        write(new org.apache.thrift.protocol.TCompactProtocol(new org.apache.thrift.transport.TIOStreamTransport(out)));    } catch (org.apache.thrift.TException te) {        throw new java.io.IOException(te);    }}
0
private void readObject(java.io.ObjectInputStream in) throws java.io.IOException, ClassNotFoundException
{    try {        read(new org.apache.thrift.protocol.TCompactProtocol(new org.apache.thrift.transport.TIOStreamTransport(in)));    } catch (org.apache.thrift.TException te) {        throw new java.io.IOException(te);    }}
0
public Log_argsStandardScheme getScheme()
{    return new Log_argsStandardScheme();}
0
public void read(org.apache.thrift.protocol.TProtocol iprot, Log_args struct) throws org.apache.thrift.TException
{    org.apache.thrift.protocol.TField schemeField;    iprot.readStructBegin();    while (true) {        schemeField = iprot.readFieldBegin();        if (schemeField.type == org.apache.thrift.protocol.TType.STOP) {            break;        }        switch(schemeField.id) {            case             1:                if (schemeField.type == org.apache.thrift.protocol.TType.LIST) {                    {                        org.apache.thrift.protocol.TList _list0 = iprot.readListBegin();                        struct.messages = new ArrayList<LogEntry>(_list0.size);                        LogEntry _elem1;                        for (int _i2 = 0; _i2 < _list0.size; ++_i2) {                            _elem1 = new LogEntry();                            _elem1.read(iprot);                            struct.messages.add(_elem1);                        }                        iprot.readListEnd();                    }                    struct.setMessagesIsSet(true);                } else {                    org.apache.thrift.protocol.TProtocolUtil.skip(iprot, schemeField.type);                }                break;            default:                org.apache.thrift.protocol.TProtocolUtil.skip(iprot, schemeField.type);        }        iprot.readFieldEnd();    }    iprot.readStructEnd();        struct.validate();}
0
public void write(org.apache.thrift.protocol.TProtocol oprot, Log_args struct) throws org.apache.thrift.TException
{    struct.validate();    oprot.writeStructBegin(STRUCT_DESC);    if (struct.messages != null) {        oprot.writeFieldBegin(MESSAGES_FIELD_DESC);        {            oprot.writeListBegin(new org.apache.thrift.protocol.TList(org.apache.thrift.protocol.TType.STRUCT, struct.messages.size()));            for (LogEntry _iter3 : struct.messages) {                _iter3.write(oprot);            }            oprot.writeListEnd();        }        oprot.writeFieldEnd();    }    oprot.writeFieldStop();    oprot.writeStructEnd();}
0
public Log_argsTupleScheme getScheme()
{    return new Log_argsTupleScheme();}
0
public void write(org.apache.thrift.protocol.TProtocol prot, Log_args struct) throws org.apache.thrift.TException
{    TTupleProtocol oprot = (TTupleProtocol) prot;    BitSet optionals = new BitSet();    if (struct.isSetMessages()) {        optionals.set(0);    }    oprot.writeBitSet(optionals, 1);    if (struct.isSetMessages()) {        {            oprot.writeI32(struct.messages.size());            for (LogEntry _iter4 : struct.messages) {                _iter4.write(oprot);            }        }    }}
0
public void read(org.apache.thrift.protocol.TProtocol prot, Log_args struct) throws org.apache.thrift.TException
{    TTupleProtocol iprot = (TTupleProtocol) prot;    BitSet incoming = iprot.readBitSet(1);    if (incoming.get(0)) {        {            org.apache.thrift.protocol.TList _list5 = new org.apache.thrift.protocol.TList(org.apache.thrift.protocol.TType.STRUCT, iprot.readI32());            struct.messages = new ArrayList<LogEntry>(_list5.size);            LogEntry _elem6;            for (int _i7 = 0; _i7 < _list5.size; ++_i7) {                _elem6 = new LogEntry();                _elem6.read(iprot);                struct.messages.add(_elem6);            }        }        struct.setMessagesIsSet(true);    }}
0
public static _Fields findByThriftId(int fieldId)
{    switch(fieldId) {        case         0:            return SUCCESS;        default:            return null;    }}
0
public static _Fields findByThriftIdOrThrow(int fieldId)
{    _Fields fields = findByThriftId(fieldId);    if (fields == null)        throw new IllegalArgumentException("Field " + fieldId + " doesn't exist!");    return fields;}
0
public static _Fields findByName(String name)
{    return byName.get(name);}
0
public short getThriftFieldId()
{    return _thriftId;}
0
public String getFieldName()
{    return _fieldName;}
0
public Log_result deepCopy()
{    return new Log_result(this);}
0
public void clear()
{    this.success = null;}
0
public ResultCode getSuccess()
{    return this.success;}
0
public Log_result setSuccess(ResultCode success)
{    this.success = success;    return this;}
0
public void unsetSuccess()
{    this.success = null;}
0
public boolean isSetSuccess()
{    return this.success != null;}
0
public void setSuccessIsSet(boolean value)
{    if (!value) {        this.success = null;    }}
0
public void setFieldValue(_Fields field, Object value)
{    switch(field) {        case SUCCESS:            if (value == null) {                unsetSuccess();            } else {                setSuccess((ResultCode) value);            }            break;    }}
0
public Object getFieldValue(_Fields field)
{    switch(field) {        case SUCCESS:            return getSuccess();    }    throw new IllegalStateException();}
0
public boolean isSet(_Fields field)
{    if (field == null) {        throw new IllegalArgumentException();    }    switch(field) {        case SUCCESS:            return isSetSuccess();    }    throw new IllegalStateException();}
0
public boolean equals(Object that)
{    if (that == null)        return false;    if (that instanceof Log_result)        return this.equals((Log_result) that);    return false;}
0
public boolean equals(Log_result that)
{    if (that == null)        return false;    boolean this_present_success = true && this.isSetSuccess();    boolean that_present_success = true && that.isSetSuccess();    if (this_present_success || that_present_success) {        if (!(this_present_success && that_present_success))            return false;        if (!this.success.equals(that.success))            return false;    }    return true;}
0
public int hashCode()
{    List<Object> list = new ArrayList<Object>();    boolean present_success = true && (isSetSuccess());    list.add(present_success);    if (present_success)        list.add(success.getValue());    return list.hashCode();}
0
public int compareTo(Log_result other)
{    if (!getClass().equals(other.getClass())) {        return getClass().getName().compareTo(other.getClass().getName());    }    int lastComparison = 0;    lastComparison = Boolean.valueOf(isSetSuccess()).compareTo(other.isSetSuccess());    if (lastComparison != 0) {        return lastComparison;    }    if (isSetSuccess()) {        lastComparison = org.apache.thrift.TBaseHelper.compareTo(this.success, other.success);        if (lastComparison != 0) {            return lastComparison;        }    }    return 0;}
0
public _Fields fieldForId(int fieldId)
{    return _Fields.findByThriftId(fieldId);}
0
public void read(org.apache.thrift.protocol.TProtocol iprot) throws org.apache.thrift.TException
{    schemes.get(iprot.getScheme()).getScheme().read(iprot, this);}
0
public void write(org.apache.thrift.protocol.TProtocol oprot) throws org.apache.thrift.TException
{    schemes.get(oprot.getScheme()).getScheme().write(oprot, this);}
0
public String toString()
{    StringBuilder sb = new StringBuilder("Log_result(");    boolean first = true;    sb.append("success:");    if (this.success == null) {        sb.append("null");    } else {        sb.append(this.success);    }    first = false;    sb.append(")");    return sb.toString();}
0
public void validate() throws org.apache.thrift.TException
{}
0
private void writeObject(java.io.ObjectOutputStream out) throws java.io.IOException
{    try {        write(new org.apache.thrift.protocol.TCompactProtocol(new org.apache.thrift.transport.TIOStreamTransport(out)));    } catch (org.apache.thrift.TException te) {        throw new java.io.IOException(te);    }}
0
private void readObject(java.io.ObjectInputStream in) throws java.io.IOException, ClassNotFoundException
{    try {        read(new org.apache.thrift.protocol.TCompactProtocol(new org.apache.thrift.transport.TIOStreamTransport(in)));    } catch (org.apache.thrift.TException te) {        throw new java.io.IOException(te);    }}
0
public Log_resultStandardScheme getScheme()
{    return new Log_resultStandardScheme();}
0
public void read(org.apache.thrift.protocol.TProtocol iprot, Log_result struct) throws org.apache.thrift.TException
{    org.apache.thrift.protocol.TField schemeField;    iprot.readStructBegin();    while (true) {        schemeField = iprot.readFieldBegin();        if (schemeField.type == org.apache.thrift.protocol.TType.STOP) {            break;        }        switch(schemeField.id) {            case             0:                if (schemeField.type == org.apache.thrift.protocol.TType.I32) {                    struct.success = org.apache.flume.source.scribe.ResultCode.findByValue(iprot.readI32());                    struct.setSuccessIsSet(true);                } else {                    org.apache.thrift.protocol.TProtocolUtil.skip(iprot, schemeField.type);                }                break;            default:                org.apache.thrift.protocol.TProtocolUtil.skip(iprot, schemeField.type);        }        iprot.readFieldEnd();    }    iprot.readStructEnd();        struct.validate();}
0
public void write(org.apache.thrift.protocol.TProtocol oprot, Log_result struct) throws org.apache.thrift.TException
{    struct.validate();    oprot.writeStructBegin(STRUCT_DESC);    if (struct.success != null) {        oprot.writeFieldBegin(SUCCESS_FIELD_DESC);        oprot.writeI32(struct.success.getValue());        oprot.writeFieldEnd();    }    oprot.writeFieldStop();    oprot.writeStructEnd();}
0
public Log_resultTupleScheme getScheme()
{    return new Log_resultTupleScheme();}
0
public void write(org.apache.thrift.protocol.TProtocol prot, Log_result struct) throws org.apache.thrift.TException
{    TTupleProtocol oprot = (TTupleProtocol) prot;    BitSet optionals = new BitSet();    if (struct.isSetSuccess()) {        optionals.set(0);    }    oprot.writeBitSet(optionals, 1);    if (struct.isSetSuccess()) {        oprot.writeI32(struct.success.getValue());    }}
0
public void read(org.apache.thrift.protocol.TProtocol prot, Log_result struct) throws org.apache.thrift.TException
{    TTupleProtocol iprot = (TTupleProtocol) prot;    BitSet incoming = iprot.readBitSet(1);    if (incoming.get(0)) {        struct.success = org.apache.flume.source.scribe.ResultCode.findByValue(iprot.readI32());        struct.setSuccessIsSet(true);    }}
0
public void configure(Context context)
{    port = context.getInteger("port", DEFAULT_PORT);    maxReadBufferBytes = context.getInteger("maxReadBufferBytes", DEFAULT_MAX_READ_BUFFER_BYTES);    if (maxReadBufferBytes <= 0) {        maxReadBufferBytes = DEFAULT_MAX_READ_BUFFER_BYTES;    }    workers = context.getInteger("workerThreads", DEFAULT_WORKERS);    if (workers <= 0) {        workers = DEFAULT_WORKERS;    }    if (sourceCounter == null) {        sourceCounter = new SourceCounter(getName());    }}
0
public void run()
{    try {        Scribe.Processor processor = new Scribe.Processor(new Receiver());        TNonblockingServerTransport transport = new TNonblockingServerSocket(port);        THsHaServer.Args args = new THsHaServer.Args(transport);        args.minWorkerThreads(workers);        args.maxWorkerThreads(workers);        args.processor(processor);        args.transportFactory(new TFramedTransport.Factory(maxReadBufferBytes));        args.protocolFactory(new TBinaryProtocol.Factory(false, false));        args.maxReadBufferBytes = maxReadBufferBytes;        server = new THsHaServer(args);                server.serve();    } catch (Exception e) {            }}
1
public void start()
{    Startup startupThread = new Startup();    startupThread.start();    try {        Thread.sleep(3000);    } catch (InterruptedException e) {    }    if (!server.isServing()) {        throw new IllegalStateException("Failed initialization of ScribeSource");    }    sourceCounter.start();    super.start();}
0
public void stop()
{        if (server != null) {        server.stop();    }    sourceCounter.stop();    super.stop();    }
1
public ResultCode Log(List<LogEntry> list) throws TException
{    if (list != null) {        sourceCounter.addToEventReceivedCount(list.size());        try {            List<Event> events = new ArrayList<Event>(list.size());            for (LogEntry entry : list) {                Map<String, String> headers = new HashMap<String, String>(1, 1);                String category = entry.getCategory();                if (category != null) {                    headers.put(SCRIBE_CATEGORY, category);                }                Event event = EventBuilder.withBody(entry.getMessage().getBytes(), headers);                events.add(event);            }            if (events.size() > 0) {                getChannelProcessor().processEventBatch(events);            }            sourceCounter.addToEventAcceptedCount(list.size());            return ResultCode.OK;        } catch (Exception e) {                        sourceCounter.incrementEventReadOrChannelFail(e);        }    }    return ResultCode.TRY_LATER;}
1
private static int findFreePort() throws IOException
{    ServerSocket socket = new ServerSocket(0);    int port = socket.getLocalPort();    socket.close();    return port;}
0
public static void setUpClass() throws Exception
{    port = findFreePort();    Context context = new Context();    context.put("port", String.valueOf(port));    scribeSource = new ScribeSource();    scribeSource.setName("Scribe Source");    Configurables.configure(scribeSource, context);    memoryChannel = new MemoryChannel();    Configurables.configure(memoryChannel, context);    List<Channel> channels = new ArrayList<Channel>(1);    channels.add(memoryChannel);    ChannelSelector rcs = new ReplicatingChannelSelector();    rcs.setChannels(channels);    memoryChannel.start();    scribeSource.setChannelProcessor(new ChannelProcessor(rcs));    scribeSource.start();}
0
private void sendSingle() throws org.apache.thrift.TException
{    TTransport transport = new TFramedTransport(new TSocket("localhost", port));    TProtocol protocol = new TBinaryProtocol(transport);    Scribe.Client client = new Scribe.Client(protocol);    transport.open();    LogEntry logEntry = new LogEntry("INFO", "Sending info msg to scribe source");    List<LogEntry> logEntries = new ArrayList<LogEntry>(1);    logEntries.add(logEntry);    client.Log(logEntries);}
0
public void testScribeMessage() throws Exception
{    sendSingle();        Transaction tx = memoryChannel.getTransaction();    tx.begin();    Event e = memoryChannel.take();    Assert.assertNotNull(e);    Assert.assertEquals("Sending info msg to scribe source", new String(e.getBody()));    tx.commit();    tx.close();}
0
public void testScribeMultipleMessages() throws Exception
{    TTransport transport = new TFramedTransport(new TSocket("localhost", port));    TProtocol protocol = new TBinaryProtocol(transport);    Scribe.Client client = new Scribe.Client(protocol);    transport.open();    List<LogEntry> logEntries = new ArrayList<LogEntry>(10);    for (int i = 0; i < 10; i++) {        LogEntry logEntry = new LogEntry("INFO", String.format("Sending info msg# %d to scribe source", i));        logEntries.add(logEntry);    }    client.Log(logEntries);        Transaction tx = memoryChannel.getTransaction();    tx.begin();    for (int i = 0; i < 10; i++) {        Event e = memoryChannel.take();        Assert.assertNotNull(e);        Assert.assertEquals(String.format("Sending info msg# %d to scribe source", i), new String(e.getBody()));    }    tx.commit();    tx.close();}
0
public void testErrorCounter() throws Exception
{    ChannelProcessor cp = mock(ChannelProcessor.class);    doThrow(new ChannelException("dummy")).when(cp).processEventBatch(anyListOf(Event.class));    ChannelProcessor origCp = scribeSource.getChannelProcessor();    scribeSource.setChannelProcessor(cp);    sendSingle();    scribeSource.setChannelProcessor(origCp);    SourceCounter sc = (SourceCounter) Whitebox.getInternalState(scribeSource, "sourceCounter");    org.junit.Assert.assertEquals(1, sc.getChannelWriteFail());}
0
public static void cleanup()
{    memoryChannel.stop();    scribeSource.stop();}
0
public void loadPositionFile(String filePath)
{    Long inode, pos;    String path;    FileReader fr = null;    JsonReader jr = null;    try {        fr = new FileReader(filePath);        jr = new JsonReader(fr);        jr.beginArray();        while (jr.hasNext()) {            inode = null;            pos = null;            path = null;            jr.beginObject();            while (jr.hasNext()) {                switch(jr.nextName()) {                    case "inode":                        inode = jr.nextLong();                        break;                    case "pos":                        pos = jr.nextLong();                        break;                    case "file":                        path = jr.nextString();                        break;                }            }            jr.endObject();            for (Object v : Arrays.asList(inode, pos, path)) {                Preconditions.checkNotNull(v, "Detected missing value in position file. " + "inode: " + inode + ", pos: " + pos + ", path: " + path);            }            TailFile tf = tailFiles.get(inode);            if (tf != null && tf.updatePos(path, inode, pos)) {                tailFiles.put(inode, tf);            } else {                            }        }        jr.endArray();    } catch (FileNotFoundException e) {            } catch (IOException e) {            } finally {        try {            if (fr != null)                fr.close();            if (jr != null)                jr.close();        } catch (IOException e) {                    }    }}
1
public Map<Long, TailFile> getTailFiles()
{    return tailFiles;}
0
public void setCurrentFile(TailFile currentFile)
{    this.currentFile = currentFile;}
0
public Event readEvent() throws IOException
{    List<Event> events = readEvents(1);    if (events.isEmpty()) {        return null;    }    return events.get(0);}
0
public List<Event> readEvents(int numEvents) throws IOException
{    return readEvents(numEvents, false);}
0
public List<Event> readEvents(TailFile tf, int numEvents) throws IOException
{    setCurrentFile(tf);    return readEvents(numEvents, true);}
0
public List<Event> readEvents(int numEvents, boolean backoffWithoutNL) throws IOException
{    if (!committed) {        if (currentFile == null) {            throw new IllegalStateException("current file does not exist. " + currentFile.getPath());        }                long lastPos = currentFile.getPos();        currentFile.updateFilePos(lastPos);    }    List<Event> events = currentFile.readEvents(numEvents, backoffWithoutNL, addByteOffset);    if (events.isEmpty()) {        return events;    }    Map<String, String> headers = currentFile.getHeaders();    if (annotateFileName || (headers != null && !headers.isEmpty())) {        for (Event event : events) {            if (headers != null && !headers.isEmpty()) {                event.getHeaders().putAll(headers);            }            if (annotateFileName) {                event.getHeaders().put(fileNameHeader, currentFile.getPath());            }        }    }    committed = false;    return events;}
1
public void close() throws IOException
{    for (TailFile tf : tailFiles.values()) {        if (tf.getRaf() != null)            tf.getRaf().close();    }}
0
public void commit() throws IOException
{    if (!committed && currentFile != null) {        long pos = currentFile.getLineReadPos();        currentFile.setPos(pos);        currentFile.setLastUpdated(updateTime);        committed = true;    }}
0
public List<Long> updateTailFiles(boolean skipToEnd) throws IOException
{    updateTime = System.currentTimeMillis();    List<Long> updatedInodes = Lists.newArrayList();    for (TaildirMatcher taildir : taildirCache) {        Map<String, String> headers = headerTable.row(taildir.getFileGroup());        for (File f : taildir.getMatchingFiles()) {            long inode;            try {                inode = getInode(f);            } catch (NoSuchFileException e) {                                continue;            }            TailFile tf = tailFiles.get(inode);            if (tf == null || !tf.getPath().equals(f.getAbsolutePath())) {                long startPos = skipToEnd ? f.length() : 0;                tf = openFile(f, headers, inode, startPos);            } else {                boolean updated = tf.getLastUpdated() < f.lastModified() || tf.getPos() != f.length();                if (updated) {                    if (tf.getRaf() == null) {                        tf = openFile(f, headers, inode, tf.getPos());                    }                    if (f.length() < tf.getPos()) {                                                tf.updatePos(tf.getPath(), inode, 0);                    }                }                tf.setNeedTail(updated);            }            tailFiles.put(inode, tf);            updatedInodes.add(inode);        }    }    return updatedInodes;}
1
public List<Long> updateTailFiles() throws IOException
{    return updateTailFiles(false);}
0
private long getInode(File file) throws IOException
{    long inode = (long) Files.getAttribute(file.toPath(), "unix:ino");    return inode;}
0
private TailFile openFile(File file, Map<String, String> headers, long inode, long pos)
{    try {                return new TailFile(file, headers, inode, pos);    } catch (IOException e) {        throw new FlumeException("Failed opening file: " + file, e);    }}
1
public Builder filePaths(Map<String, String> filePaths)
{    this.filePaths = filePaths;    return this;}
0
public Builder headerTable(Table<String, String, String> headerTable)
{    this.headerTable = headerTable;    return this;}
0
public Builder positionFilePath(String positionFilePath)
{    this.positionFilePath = positionFilePath;    return this;}
0
public Builder skipToEnd(boolean skipToEnd)
{    this.skipToEnd = skipToEnd;    return this;}
0
public Builder addByteOffset(boolean addByteOffset)
{    this.addByteOffset = addByteOffset;    return this;}
0
public Builder cachePatternMatching(boolean cachePatternMatching)
{    this.cachePatternMatching = cachePatternMatching;    return this;}
0
public Builder annotateFileName(boolean annotateFileName)
{    this.annotateFileName = annotateFileName;    return this;}
0
public Builder fileNameHeader(String fileNameHeader)
{    this.fileNameHeader = fileNameHeader;    return this;}
0
public ReliableTaildirEventReader build() throws IOException
{    return new ReliableTaildirEventReader(filePaths, headerTable, positionFilePath, skipToEnd, addByteOffset, cachePatternMatching, annotateFileName, fileNameHeader);}
0
public boolean accept(Path entry) throws IOException
{    return matcher.matches(entry.getFileName()) && !Files.isDirectory(entry);}
0
 List<File> getMatchingFiles()
{    long now = TimeUnit.SECONDS.toMillis(TimeUnit.MILLISECONDS.toSeconds(System.currentTimeMillis()));    long currentParentDirMTime = parentDir.lastModified();    List<File> result;        if (!cachePatternMatching || lastSeenParentDirMTime < currentParentDirMTime || !(currentParentDirMTime < lastCheckedTime)) {        lastMatchedFiles = sortByLastModifiedTime(getMatchingFilesNoCache());        lastSeenParentDirMTime = currentParentDirMTime;        lastCheckedTime = now;    }    return lastMatchedFiles;}
0
private List<File> getMatchingFilesNoCache()
{    List<File> result = Lists.newArrayList();    try (DirectoryStream<Path> stream = Files.newDirectoryStream(parentDir.toPath(), fileFilter)) {        for (Path entry : stream) {            result.add(entry.toFile());        }    } catch (IOException e) {            }    return result;}
1
private static List<File> sortByLastModifiedTime(List<File> files)
{    final HashMap<File, Long> lastModificationTimes = new HashMap<File, Long>(files.size());    for (File f : files) {        lastModificationTimes.put(f, f.lastModified());    }    Collections.sort(files, new Comparator<File>() {        @Override        public int compare(File o1, File o2) {            return lastModificationTimes.get(o1).compareTo(lastModificationTimes.get(o2));        }    });    return files;}
0
public int compare(File o1, File o2)
{    return lastModificationTimes.get(o1).compareTo(lastModificationTimes.get(o2));}
0
public String toString()
{    return "{" + "filegroup='" + fileGroup + '\'' + ", filePattern='" + filePattern + '\'' + ", cached=" + cachePatternMatching + '}';}
0
public boolean equals(Object o)
{    if (this == o)        return true;    if (o == null || getClass() != o.getClass())        return false;    TaildirMatcher that = (TaildirMatcher) o;    return fileGroup.equals(that.fileGroup);}
0
public int hashCode()
{    return fileGroup.hashCode();}
0
public String getFileGroup()
{    return fileGroup;}
0
public synchronized void start()
{        try {        reader = new ReliableTaildirEventReader.Builder().filePaths(filePaths).headerTable(headerTable).positionFilePath(positionFilePath).skipToEnd(skipToEnd).addByteOffset(byteOffsetHeader).cachePatternMatching(cachePatternMatching).annotateFileName(fileHeader).fileNameHeader(fileHeaderKey).build();    } catch (IOException e) {        throw new FlumeException("Error instantiating ReliableTaildirEventReader", e);    }    idleFileChecker = Executors.newSingleThreadScheduledExecutor(new ThreadFactoryBuilder().setNameFormat("idleFileChecker").build());    idleFileChecker.scheduleWithFixedDelay(new idleFileCheckerRunnable(), idleTimeout, checkIdleInterval, TimeUnit.MILLISECONDS);    positionWriter = Executors.newSingleThreadScheduledExecutor(new ThreadFactoryBuilder().setNameFormat("positionWriter").build());    positionWriter.scheduleWithFixedDelay(new PositionWriterRunnable(), writePosInitDelay, writePosInterval, TimeUnit.MILLISECONDS);    super.start();        sourceCounter.start();}
1
public synchronized void stop()
{    try {        super.stop();        ExecutorService[] services = { idleFileChecker, positionWriter };        for (ExecutorService service : services) {            service.shutdown();            if (!service.awaitTermination(1, TimeUnit.SECONDS)) {                service.shutdownNow();            }        }                writePosition();        reader.close();    } catch (InterruptedException e) {            } catch (IOException e) {            }    sourceCounter.stop();    }
1
public String toString()
{    return String.format("Taildir source: { positionFile: %s, skipToEnd: %s, " + "byteOffsetHeader: %s, idleTimeout: %s, writePosInterval: %s }", positionFilePath, skipToEnd, byteOffsetHeader, idleTimeout, writePosInterval);}
0
public synchronized void configure(Context context)
{    String fileGroups = context.getString(FILE_GROUPS);    Preconditions.checkState(fileGroups != null, "Missing param: " + FILE_GROUPS);    filePaths = selectByKeys(context.getSubProperties(FILE_GROUPS_PREFIX), fileGroups.split("\\s+"));    Preconditions.checkState(!filePaths.isEmpty(), "Mapping for tailing files is empty or invalid: '" + FILE_GROUPS_PREFIX + "'");    String homePath = System.getProperty("user.home").replace('\\', '/');    positionFilePath = context.getString(POSITION_FILE, homePath + DEFAULT_POSITION_FILE);    Path positionFile = Paths.get(positionFilePath);    try {        Files.createDirectories(positionFile.getParent());    } catch (IOException e) {        throw new FlumeException("Error creating positionFile parent directories", e);    }    headerTable = getTable(context, HEADERS_PREFIX);    batchSize = context.getInteger(BATCH_SIZE, DEFAULT_BATCH_SIZE);    skipToEnd = context.getBoolean(SKIP_TO_END, DEFAULT_SKIP_TO_END);    byteOffsetHeader = context.getBoolean(BYTE_OFFSET_HEADER, DEFAULT_BYTE_OFFSET_HEADER);    idleTimeout = context.getInteger(IDLE_TIMEOUT, DEFAULT_IDLE_TIMEOUT);    writePosInterval = context.getInteger(WRITE_POS_INTERVAL, DEFAULT_WRITE_POS_INTERVAL);    cachePatternMatching = context.getBoolean(CACHE_PATTERN_MATCHING, DEFAULT_CACHE_PATTERN_MATCHING);    backoffSleepIncrement = context.getLong(PollableSourceConstants.BACKOFF_SLEEP_INCREMENT, PollableSourceConstants.DEFAULT_BACKOFF_SLEEP_INCREMENT);    maxBackOffSleepInterval = context.getLong(PollableSourceConstants.MAX_BACKOFF_SLEEP, PollableSourceConstants.DEFAULT_MAX_BACKOFF_SLEEP);    fileHeader = context.getBoolean(FILENAME_HEADER, DEFAULT_FILE_HEADER);    fileHeaderKey = context.getString(FILENAME_HEADER_KEY, DEFAULT_FILENAME_HEADER_KEY);    maxBatchCount = context.getLong(MAX_BATCH_COUNT, DEFAULT_MAX_BATCH_COUNT);    if (maxBatchCount <= 0) {        maxBatchCount = DEFAULT_MAX_BATCH_COUNT;            }    if (sourceCounter == null) {        sourceCounter = new SourceCounter(getName());    }}
1
public long getBatchSize()
{    return batchSize;}
0
private Map<String, String> selectByKeys(Map<String, String> map, String[] keys)
{    Map<String, String> result = Maps.newHashMap();    for (String key : keys) {        if (map.containsKey(key)) {            result.put(key, map.get(key));        }    }    return result;}
0
private Table<String, String, String> getTable(Context context, String prefix)
{    Table<String, String, String> table = HashBasedTable.create();    for (Entry<String, String> e : context.getSubProperties(prefix).entrySet()) {        String[] parts = e.getKey().split("\\.", 2);        table.put(parts[0], parts[1], e.getValue());    }    return table;}
0
protected SourceCounter getSourceCounter()
{    return sourceCounter;}
0
public Status process()
{    Status status = Status.BACKOFF;    try {        existingInodes.clear();        existingInodes.addAll(reader.updateTailFiles());        for (long inode : existingInodes) {            TailFile tf = reader.getTailFiles().get(inode);            if (tf.needTail()) {                boolean hasMoreLines = tailFileProcess(tf, true);                if (hasMoreLines) {                    status = Status.READY;                }            }        }        closeTailFiles();    } catch (Throwable t) {                sourceCounter.incrementEventReadFail();        status = Status.BACKOFF;    }    return status;}
1
public long getBackOffSleepIncrement()
{    return backoffSleepIncrement;}
0
public long getMaxBackOffSleepInterval()
{    return maxBackOffSleepInterval;}
0
private boolean tailFileProcess(TailFile tf, boolean backoffWithoutNL) throws IOException, InterruptedException
{    long batchCount = 0;    while (true) {        reader.setCurrentFile(tf);        List<Event> events = reader.readEvents(batchSize, backoffWithoutNL);        if (events.isEmpty()) {            return false;        }        sourceCounter.addToEventReceivedCount(events.size());        sourceCounter.incrementAppendBatchReceivedCount();        try {            getChannelProcessor().processEventBatch(events);            reader.commit();        } catch (ChannelException ex) {                        sourceCounter.incrementChannelWriteFail();            TimeUnit.MILLISECONDS.sleep(retryInterval);            retryInterval = retryInterval << 1;            retryInterval = Math.min(retryInterval, maxRetryInterval);            continue;        }        retryInterval = 1000;        sourceCounter.addToEventAcceptedCount(events.size());        sourceCounter.incrementAppendBatchAcceptedCount();        if (events.size() < batchSize) {                        return false;        }        if (++batchCount >= maxBatchCount) {                        return true;        }    }}
1
private void closeTailFiles() throws IOException, InterruptedException
{    for (long inode : idleInodes) {        TailFile tf = reader.getTailFiles().get(inode);        if (tf.getRaf() != null) {                        tailFileProcess(tf, false);            tf.close();                    }    }    idleInodes.clear();}
1
public void run()
{    try {        long now = System.currentTimeMillis();        for (TailFile tf : reader.getTailFiles().values()) {            if (tf.getLastUpdated() + idleTimeout < now && tf.getRaf() != null) {                idleInodes.add(tf.getInode());            }        }    } catch (Throwable t) {                sourceCounter.incrementGenericProcessingFail();    }}
1
public void run()
{    writePosition();}
0
private void writePosition()
{    File file = new File(positionFilePath);    FileWriter writer = null;    try {        writer = new FileWriter(file);        if (!existingInodes.isEmpty()) {            String json = toPosInfoJson();            writer.write(json);        }    } catch (Throwable t) {                sourceCounter.incrementGenericProcessingFail();    } finally {        try {            if (writer != null)                writer.close();        } catch (IOException e) {                        sourceCounter.incrementGenericProcessingFail();        }    }}
1
private String toPosInfoJson()
{    @SuppressWarnings("rawtypes")    List<Map> posInfos = Lists.newArrayList();    for (Long inode : existingInodes) {        TailFile tf = reader.getTailFiles().get(inode);        posInfos.add(ImmutableMap.of("inode", inode, "pos", tf.getPos(), "file", tf.getPath()));    }    return new Gson().toJson(posInfos);}
0
public RandomAccessFile getRaf()
{    return raf;}
0
public String getPath()
{    return path;}
0
public long getInode()
{    return inode;}
0
public long getPos()
{    return pos;}
0
public long getLastUpdated()
{    return lastUpdated;}
0
public boolean needTail()
{    return needTail;}
0
public Map<String, String> getHeaders()
{    return headers;}
0
public long getLineReadPos()
{    return lineReadPos;}
0
public void setPos(long pos)
{    this.pos = pos;}
0
public void setLastUpdated(long lastUpdated)
{    this.lastUpdated = lastUpdated;}
0
public void setNeedTail(boolean needTail)
{    this.needTail = needTail;}
0
public void setLineReadPos(long lineReadPos)
{    this.lineReadPos = lineReadPos;}
0
public boolean updatePos(String path, long inode, long pos) throws IOException
{    if (this.inode == inode && this.path.equals(path)) {        setPos(pos);        updateFilePos(pos);                return true;    }    return false;}
1
public void updateFilePos(long pos) throws IOException
{    raf.seek(pos);    lineReadPos = pos;    bufferPos = NEED_READING;    oldBuffer = new byte[0];}
0
public List<Event> readEvents(int numEvents, boolean backoffWithoutNL, boolean addByteOffset) throws IOException
{    List<Event> events = Lists.newLinkedList();    for (int i = 0; i < numEvents; i++) {        Event event = readEvent(backoffWithoutNL, addByteOffset);        if (event == null) {            break;        }        events.add(event);    }    return events;}
0
private Event readEvent(boolean backoffWithoutNL, boolean addByteOffset) throws IOException
{    Long posTmp = getLineReadPos();    LineResult line = readLine();    if (line == null) {        return null;    }    if (backoffWithoutNL && !line.lineSepInclude) {                updateFilePos(posTmp);        return null;    }    Event event = EventBuilder.withBody(line.line);    if (addByteOffset == true) {        event.getHeaders().put(BYTE_OFFSET_HEADER_KEY, posTmp.toString());    }    return event;}
1
private void readFile() throws IOException
{    if ((raf.length() - raf.getFilePointer()) < BUFFER_SIZE) {        buffer = new byte[(int) (raf.length() - raf.getFilePointer())];    } else {        buffer = new byte[BUFFER_SIZE];    }    raf.read(buffer, 0, buffer.length);    bufferPos = 0;}
0
private byte[] concatByteArrays(byte[] a, int startIdxA, int lenA, byte[] b, int startIdxB, int lenB)
{    byte[] c = new byte[lenA + lenB];    System.arraycopy(a, startIdxA, c, 0, lenA);    System.arraycopy(b, startIdxB, c, lenA, lenB);    return c;}
0
public LineResult readLine() throws IOException
{    LineResult lineResult = null;    while (true) {        if (bufferPos == NEED_READING) {            if (raf.getFilePointer() < raf.length()) {                readFile();            } else {                if (oldBuffer.length > 0) {                    lineResult = new LineResult(false, oldBuffer);                    oldBuffer = new byte[0];                    setLineReadPos(lineReadPos + lineResult.line.length);                }                break;            }        }        for (int i = bufferPos; i < buffer.length; i++) {            if (buffer[i] == BYTE_NL) {                int oldLen = oldBuffer.length;                                int lineLen = i - bufferPos;                                if (i > 0 && buffer[i - 1] == BYTE_CR) {                    lineLen -= 1;                } else if (oldBuffer.length > 0 && oldBuffer[oldBuffer.length - 1] == BYTE_CR) {                    oldLen -= 1;                }                lineResult = new LineResult(true, concatByteArrays(oldBuffer, 0, oldLen, buffer, bufferPos, lineLen));                setLineReadPos(lineReadPos + (oldBuffer.length + (i - bufferPos + 1)));                oldBuffer = new byte[0];                if (i + 1 < buffer.length) {                    bufferPos = i + 1;                } else {                    bufferPos = NEED_READING;                }                break;            }        }        if (lineResult != null) {            break;        }                oldBuffer = concatByteArrays(oldBuffer, 0, oldBuffer.length, buffer, bufferPos, buffer.length - bufferPos);        bufferPos = NEED_READING;    }    return lineResult;}
0
public void close()
{    try {        raf.close();        raf = null;        long now = System.currentTimeMillis();        setLastUpdated(now);    } catch (IOException e) {            }}
1
public static String bodyAsString(Event event)
{    return new String(event.getBody());}
0
 static List<String> bodiesAsStrings(List<Event> events)
{    List<String> bodies = Lists.newArrayListWithCapacity(events.size());    for (Event event : events) {        bodies.add(new String(event.getBody()));    }    return bodies;}
0
 static List<String> headersAsStrings(List<Event> events, String headerKey)
{    List<String> headers = Lists.newArrayListWithCapacity(events.size());    for (Event event : events) {        headers.add(new String(event.getHeaders().get(headerKey)));    }    return headers;}
0
private ReliableTaildirEventReader getReader(Map<String, String> filePaths, Table<String, String, String> headerTable, boolean addByteOffset, boolean cachedPatternMatching)
{    ReliableTaildirEventReader reader;    try {        reader = new ReliableTaildirEventReader.Builder().filePaths(filePaths).headerTable(headerTable).positionFilePath(posFilePath).skipToEnd(false).addByteOffset(addByteOffset).cachePatternMatching(cachedPatternMatching).build();        reader.updateTailFiles();    } catch (IOException ioe) {        throw Throwables.propagate(ioe);    }    return reader;}
0
private ReliableTaildirEventReader getReader(boolean addByteOffset, boolean cachedPatternMatching)
{    Map<String, String> filePaths = ImmutableMap.of("testFiles", tmpDir.getAbsolutePath() + "/file.*");    Table<String, String, String> headerTable = HashBasedTable.create();    return getReader(filePaths, headerTable, addByteOffset, cachedPatternMatching);}
0
private ReliableTaildirEventReader getReader()
{    return getReader(false, false);}
0
public void setUp()
{    tmpDir = Files.createTempDir();    posFilePath = tmpDir.getAbsolutePath() + "/taildir_position_test.json";}
0
public void tearDown()
{    for (File f : tmpDir.listFiles()) {        if (f.isDirectory()) {            for (File sdf : f.listFiles()) {                sdf.delete();            }        }        f.delete();    }    tmpDir.delete();}
0
public void testBasicReadFiles() throws IOException
{    File f1 = new File(tmpDir, "file1");    File f2 = new File(tmpDir, "file2");    File f3 = new File(tmpDir, "file3");    Files.write("file1line1\nfile1line2\n", f1, Charsets.UTF_8);    Files.write("file2line1\nfile2line2\n", f2, Charsets.UTF_8);    Files.write("file3line1\nfile3line2\n", f3, Charsets.UTF_8);    ReliableTaildirEventReader reader = getReader();    List<String> out = Lists.newArrayList();    for (TailFile tf : reader.getTailFiles().values()) {        List<String> bodies = bodiesAsStrings(reader.readEvents(tf, 2));        out.addAll(bodies);        reader.commit();    }    assertEquals(6, out.size());        assertTrue(out.contains("file1line1"));    assertTrue(out.contains("file1line2"));    assertTrue(out.contains("file2line1"));    assertTrue(out.contains("file2line2"));    assertTrue(out.contains("file3line1"));    assertTrue(out.contains("file3line2"));    Files.append("file3line3\nfile3line4\n", f3, Charsets.UTF_8);    reader.updateTailFiles();    for (TailFile tf : reader.getTailFiles().values()) {        List<String> bodies = bodiesAsStrings(reader.readEvents(tf, 2));        out.addAll(bodies);        reader.commit();    }    assertEquals(8, out.size());    assertTrue(out.contains("file3line3"));    assertTrue(out.contains("file3line4"));}
0
public void testDeleteFiles() throws IOException
{    File f1 = new File(tmpDir, "file1");    Files.write("file1line1\nfile1line2\n", f1, Charsets.UTF_8);            ReliableTaildirEventReader reader = getReader(false, true);    File dir = f1.getParentFile();    long lastModified = dir.lastModified();    f1.delete();        dir.setLastModified(lastModified - 1000);    reader.updateTailFiles();}
0
public void testInitiallyEmptyDirAndBehaviorAfterReadingAll() throws IOException
{    ReliableTaildirEventReader reader = getReader();    List<Long> fileInodes = reader.updateTailFiles();    assertEquals(0, fileInodes.size());    File f1 = new File(tmpDir, "file1");    Files.write("file1line1\nfile1line2\n", f1, Charsets.UTF_8);    reader.updateTailFiles();    List<String> out = null;    for (TailFile tf : reader.getTailFiles().values()) {        out = bodiesAsStrings(reader.readEvents(tf, 2));        reader.commit();    }    assertEquals(2, out.size());        assertTrue(out.contains("file1line1"));    assertTrue(out.contains("file1line2"));    reader.updateTailFiles();    List<String> empty = null;    for (TailFile tf : reader.getTailFiles().values()) {        empty = bodiesAsStrings(reader.readEvents(tf, 15));        reader.commit();    }    assertEquals(0, empty.size());}
0
public void testBasicCommitFailure() throws IOException
{    File f1 = new File(tmpDir, "file1");    Files.write("file1line1\nfile1line2\nfile1line3\nfile1line4\n" + "file1line5\nfile1line6\nfile1line7\nfile1line8\n" + "file1line9\nfile1line10\nfile1line11\nfile1line12\n", f1, Charsets.UTF_8);    ReliableTaildirEventReader reader = getReader();    List<String> out1 = null;    for (TailFile tf : reader.getTailFiles().values()) {        out1 = bodiesAsStrings(reader.readEvents(tf, 4));    }    assertTrue(out1.contains("file1line1"));    assertTrue(out1.contains("file1line2"));    assertTrue(out1.contains("file1line3"));    assertTrue(out1.contains("file1line4"));    List<String> out2 = bodiesAsStrings(reader.readEvents(4));    assertTrue(out2.contains("file1line1"));    assertTrue(out2.contains("file1line2"));    assertTrue(out2.contains("file1line3"));    assertTrue(out2.contains("file1line4"));    reader.commit();    List<String> out3 = bodiesAsStrings(reader.readEvents(4));    assertTrue(out3.contains("file1line5"));    assertTrue(out3.contains("file1line6"));    assertTrue(out3.contains("file1line7"));    assertTrue(out3.contains("file1line8"));    reader.commit();    List<String> out4 = bodiesAsStrings(reader.readEvents(4));    assertEquals(4, out4.size());    assertTrue(out4.contains("file1line9"));    assertTrue(out4.contains("file1line10"));    assertTrue(out4.contains("file1line11"));    assertTrue(out4.contains("file1line12"));}
0
public void testBasicCommitFailureAndBatchSizeChanges() throws IOException
{    File f1 = new File(tmpDir, "file1");    Files.write("file1line1\nfile1line2\nfile1line3\nfile1line4\n" + "file1line5\nfile1line6\nfile1line7\nfile1line8\n", f1, Charsets.UTF_8);    ReliableTaildirEventReader reader = getReader();    List<String> out1 = null;    for (TailFile tf : reader.getTailFiles().values()) {        out1 = bodiesAsStrings(reader.readEvents(tf, 5));    }    assertTrue(out1.contains("file1line1"));    assertTrue(out1.contains("file1line2"));    assertTrue(out1.contains("file1line3"));    assertTrue(out1.contains("file1line4"));    assertTrue(out1.contains("file1line5"));    List<String> out2 = bodiesAsStrings(reader.readEvents(2));    assertTrue(out2.contains("file1line1"));    assertTrue(out2.contains("file1line2"));    reader.commit();    List<String> out3 = bodiesAsStrings(reader.readEvents(2));    assertTrue(out3.contains("file1line3"));    assertTrue(out3.contains("file1line4"));    reader.commit();    List<String> out4 = bodiesAsStrings(reader.readEvents(15));    assertTrue(out4.contains("file1line5"));    assertTrue(out4.contains("file1line6"));    assertTrue(out4.contains("file1line7"));    assertTrue(out4.contains("file1line8"));}
0
public void testIncludeEmptyFile() throws IOException
{    File f1 = new File(tmpDir, "file1");    File f2 = new File(tmpDir, "file2");    Files.write("file1line1\nfile1line2\n", f1, Charsets.UTF_8);    Files.touch(f2);    ReliableTaildirEventReader reader = getReader();        List<String> out = Lists.newArrayList();    for (TailFile tf : reader.getTailFiles().values()) {        out.addAll(bodiesAsStrings(reader.readEvents(tf, 5)));        reader.commit();    }    assertEquals(2, out.size());    assertTrue(out.contains("file1line1"));    assertTrue(out.contains("file1line2"));    assertNull(reader.readEvent());}
0
public void testBackoffWithoutNewLine() throws IOException
{    File f1 = new File(tmpDir, "file1");    Files.write("file1line1\nfile1", f1, Charsets.UTF_8);    ReliableTaildirEventReader reader = getReader();    List<String> out = Lists.newArrayList();        for (TailFile tf : reader.getTailFiles().values()) {        out.addAll(bodiesAsStrings(reader.readEvents(tf, 5)));        reader.commit();    }    assertEquals(1, out.size());    assertTrue(out.contains("file1line1"));    Files.append("line2\nfile1line3\nfile1line4", f1, Charsets.UTF_8);    for (TailFile tf : reader.getTailFiles().values()) {        out.addAll(bodiesAsStrings(reader.readEvents(tf, 5)));        reader.commit();    }    assertEquals(3, out.size());    assertTrue(out.contains("file1line2"));    assertTrue(out.contains("file1line3"));        out.addAll(bodiesAsStrings(reader.readEvents(5, false)));    reader.commit();    assertEquals(4, out.size());    assertTrue(out.contains("file1line4"));}
0
public void testBatchedReadsAcrossFileBoundary() throws IOException
{    File f1 = new File(tmpDir, "file1");    Files.write("file1line1\nfile1line2\nfile1line3\nfile1line4\n" + "file1line5\nfile1line6\nfile1line7\nfile1line8\n", f1, Charsets.UTF_8);    ReliableTaildirEventReader reader = getReader();    List<String> out1 = Lists.newArrayList();    for (TailFile tf : reader.getTailFiles().values()) {        out1.addAll(bodiesAsStrings(reader.readEvents(tf, 5)));        reader.commit();    }    File f2 = new File(tmpDir, "file2");    Files.write("file2line1\nfile2line2\nfile2line3\nfile2line4\n" + "file2line5\nfile2line6\nfile2line7\nfile2line8\n", f2, Charsets.UTF_8);    List<String> out2 = bodiesAsStrings(reader.readEvents(5));    reader.commit();    reader.updateTailFiles();    List<String> out3 = Lists.newArrayList();    for (TailFile tf : reader.getTailFiles().values()) {        out3.addAll(bodiesAsStrings(reader.readEvents(tf, 5)));        reader.commit();    }        assertEquals(5, out1.size());    assertTrue(out1.contains("file1line1"));    assertTrue(out1.contains("file1line2"));    assertTrue(out1.contains("file1line3"));    assertTrue(out1.contains("file1line4"));    assertTrue(out1.contains("file1line5"));        assertEquals(3, out2.size());    assertTrue(out2.contains("file1line6"));    assertTrue(out2.contains("file1line7"));    assertTrue(out2.contains("file1line8"));        assertEquals(5, out3.size());    assertTrue(out3.contains("file2line1"));    assertTrue(out3.contains("file2line2"));    assertTrue(out3.contains("file2line3"));    assertTrue(out3.contains("file2line4"));    assertTrue(out3.contains("file2line5"));}
0
public void testLargeNumberOfFiles() throws IOException
{    int fileNum = 1000;    Set<String> expected = Sets.newHashSet();    for (int i = 0; i < fileNum; i++) {        String data = "data" + i;        File f = new File(tmpDir, "file" + i);        Files.write(data + "\n", f, Charsets.UTF_8);        expected.add(data);    }    ReliableTaildirEventReader reader = getReader();    for (TailFile tf : reader.getTailFiles().values()) {        List<Event> events = reader.readEvents(tf, 10);        for (Event e : events) {            expected.remove(new String(e.getBody()));        }        reader.commit();    }    assertEquals(0, expected.size());}
0
public void testLoadPositionFile() throws IOException
{    File f1 = new File(tmpDir, "file1");    File f2 = new File(tmpDir, "file2");    File f3 = new File(tmpDir, "file3");    Files.write("file1line1\nfile1line2\nfile1line3\n", f1, Charsets.UTF_8);    Files.write("file2line1\nfile2line2\n", f2, Charsets.UTF_8);    Files.write("file3line1\n", f3, Charsets.UTF_8);    ReliableTaildirEventReader reader = getReader();    Map<Long, TailFile> tailFiles = reader.getTailFiles();    long pos = f2.length();    int i = 1;    File posFile = new File(posFilePath);    for (TailFile tf : tailFiles.values()) {        Files.append(i == 1 ? "[" : "", posFile, Charsets.UTF_8);        Files.append(String.format("{\"inode\":%s,\"pos\":%s,\"file\":\"%s\"}", tf.getInode(), pos, tf.getPath()), posFile, Charsets.UTF_8);        Files.append(i == 3 ? "]" : ",", posFile, Charsets.UTF_8);        i++;    }    reader.loadPositionFile(posFilePath);    for (TailFile tf : tailFiles.values()) {        if (tf.getPath().equals(tmpDir + "file3")) {                        assertEquals(0, tf.getPos());        } else {            assertEquals(pos, tf.getPos());        }    }}
0
public void testSkipToEndPosition() throws IOException
{    ReliableTaildirEventReader reader = getReader();    File f1 = new File(tmpDir, "file1");    Files.write("file1line1\nfile1line2\n", f1, Charsets.UTF_8);    reader.updateTailFiles();    for (TailFile tf : reader.getTailFiles().values()) {        if (tf.getPath().equals(tmpDir + "file1")) {            assertEquals(0, tf.getPos());        }    }    File f2 = new File(tmpDir, "file2");    Files.write("file2line1\nfile2line2\n", f2, Charsets.UTF_8);        reader.updateTailFiles(true);    for (TailFile tf : reader.getTailFiles().values()) {        if (tf.getPath().equals(tmpDir + "file2")) {            assertEquals(f2.length(), tf.getPos());        }    }}
0
public void testByteOffsetHeader() throws IOException
{    File f1 = new File(tmpDir, "file1");    String line1 = "file1line1\n";    String line2 = "file1line2\n";    String line3 = "file1line3\n";    Files.write(line1 + line2 + line3, f1, Charsets.UTF_8);    ReliableTaildirEventReader reader = getReader(true, false);    List<String> headers = null;    for (TailFile tf : reader.getTailFiles().values()) {        headers = headersAsStrings(reader.readEvents(tf, 5), BYTE_OFFSET_HEADER_KEY);        reader.commit();    }    assertEquals(3, headers.size());        assertTrue(headers.contains(String.valueOf(0)));    assertTrue(headers.contains(String.valueOf(line1.length())));    assertTrue(headers.contains(String.valueOf((line1 + line2).length())));}
0
public void testNewLineBoundaries() throws IOException
{    File f1 = new File(tmpDir, "file1");    Files.write("file1line1\nfile1line2\rfile1line2\nfile1line3\r\nfile1line4\n", f1, Charsets.UTF_8);    ReliableTaildirEventReader reader = getReader();    List<String> out = Lists.newArrayList();    for (TailFile tf : reader.getTailFiles().values()) {        out.addAll(bodiesAsStrings(reader.readEvents(tf, 5)));        reader.commit();    }    assertEquals(4, out.size());        assertTrue(out.contains("file1line1"));        assertTrue(out.contains("file1line2\rfile1line2"));        assertTrue(out.contains("file1line3"));    assertTrue(out.contains("file1line4"));}
0
public void testUpdateWhenLastUpdatedSameAsModificationTime() throws IOException
{    File file = new File(tmpDir, "file");    Files.write("line1\n", file, Charsets.UTF_8);    ReliableTaildirEventReader reader = getReader();    for (TailFile tf : reader.getTailFiles().values()) {        reader.readEvents(tf, 1);        reader.commit();    }    Files.append("line2\n", file, Charsets.UTF_8);    for (TailFile tf : reader.getTailFiles().values()) {        tf.setLastUpdated(file.lastModified());    }    reader.updateTailFiles();    for (TailFile tf : reader.getTailFiles().values()) {        assertEquals(true, tf.needTail());    }}
0
private void append(String fileName) throws IOException
{    File f;    if (!files.containsKey(fileName)) {        f = new File(tmpDir, fileName);        files.put(fileName, f);    } else {        f = files.get(fileName);    }    Files.append(fileName + "line\n", f, Charsets.UTF_8);}
0
private static List<String> filesToNames(List<File> origList)
{    Function<File, String> file2nameFn = new Function<File, String>() {        @Override        public String apply(File input) {            return input.getName();        }    };    return Lists.transform(origList, file2nameFn);}
0
public String apply(File input)
{    return input.getName();}
0
public void setUp() throws Exception
{    files = Maps.newHashMap();    tmpDir = Files.createTempDir();}
0
public void tearDown() throws Exception
{    for (File f : tmpDir.listFiles()) {        if (f.isDirectory()) {            for (File sdf : f.listFiles()) {                sdf.delete();            }        }        f.delete();    }    tmpDir.delete();    files = null;}
0
public void getMatchingFiles() throws Exception
{    append("file0");    append("file1");    TaildirMatcher tm = new TaildirMatcher("f1", tmpDir.getAbsolutePath() + File.separator + "file.*", isCachingNeeded);    List<String> files = filesToNames(tm.getMatchingFiles());    assertEquals(msgAlreadyExistingFile, 2, files.size());    assertTrue(msgAlreadyExistingFile, files.contains("file1"));    append("file1");    files = filesToNames(tm.getMatchingFiles());    assertEquals(msgAfterNewFileCreated, 2, files.size());    assertTrue(msgAfterNewFileCreated, files.contains("file0"));    assertTrue(msgAfterNewFileCreated, files.contains("file1"));    append("file2");    append("file3");    files = filesToNames(tm.getMatchingFiles());    assertEquals(msgAfterAppend, 4, files.size());    assertTrue(msgAfterAppend, files.contains("file0"));    assertTrue(msgAfterAppend, files.contains("file1"));    assertTrue(msgAfterAppend, files.contains("file2"));    assertTrue(msgAfterAppend, files.contains("file3"));    this.files.get("file0").delete();    files = filesToNames(tm.getMatchingFiles());    assertEquals(msgAfterDelete, 3, files.size());    assertFalse(msgAfterDelete, files.contains("file0"));    assertTrue(msgNoChange, files.contains("file1"));    assertTrue(msgNoChange, files.contains("file2"));    assertTrue(msgNoChange, files.contains("file3"));}
0
public void getMatchingFilesNoCache() throws Exception
{    append("file0");    append("file1");    TaildirMatcher tm = new TaildirMatcher("f1", tmpDir.getAbsolutePath() + File.separator + "file.*", false);    List<String> files = filesToNames(tm.getMatchingFiles());    assertEquals(msgAlreadyExistingFile, 2, files.size());    assertTrue(msgAlreadyExistingFile, files.contains("file1"));    append("file1");    files = filesToNames(tm.getMatchingFiles());    assertEquals(msgAfterAppend, 2, files.size());    assertTrue(msgAfterAppend, files.contains("file0"));    assertTrue(msgAfterAppend, files.contains("file1"));    append("file2");    append("file3");    files = filesToNames(tm.getMatchingFiles());    assertEquals(msgAfterNewFileCreated, 4, files.size());    assertTrue(msgAfterNewFileCreated, files.contains("file0"));    assertTrue(msgAfterNewFileCreated, files.contains("file1"));    assertTrue(msgAfterNewFileCreated, files.contains("file2"));    assertTrue(msgAfterNewFileCreated, files.contains("file3"));    this.files.get("file0").delete();    files = filesToNames(tm.getMatchingFiles());    assertEquals(msgAfterDelete, 3, files.size());    assertFalse(msgAfterDelete, files.contains("file0"));    assertTrue(msgNoChange, files.contains("file1"));    assertTrue(msgNoChange, files.contains("file2"));    assertTrue(msgNoChange, files.contains("file3"));}
0
public void testEmtpyDirMatching() throws Exception
{    TaildirMatcher tm = new TaildirMatcher("empty", tmpDir.getAbsolutePath() + File.separator + ".*", isCachingNeeded);    List<File> files = tm.getMatchingFiles();    assertNotNull(msgEmptyDir, files);    assertTrue(msgEmptyDir, files.isEmpty());}
0
public void testNoMatching() throws Exception
{    TaildirMatcher tm = new TaildirMatcher("nomatch", tmpDir.getAbsolutePath() + File.separator + "abracadabra_nonexisting", isCachingNeeded);    List<File> files = tm.getMatchingFiles();    assertNotNull(msgNoMatch, files);    assertTrue(msgNoMatch, files.isEmpty());}
0
public void testNonExistingDir()
{    TaildirMatcher tm = new TaildirMatcher("exception", "/abracadabra/doesntexist/.*", isCachingNeeded);}
0
public void testDirectoriesAreNotListed() throws Exception
{    new File(tmpDir, "outerFile").createNewFile();    new File(tmpDir, "recursiveDir").mkdir();    new File(tmpDir + File.separator + "recursiveDir", "innerFile").createNewFile();    TaildirMatcher tm = new TaildirMatcher("f1", tmpDir.getAbsolutePath() + File.separator + ".*", isCachingNeeded);    List<String> files = filesToNames(tm.getMatchingFiles());    assertEquals(msgSubDirs, 1, files.size());    assertTrue(msgSubDirs, files.contains("outerFile"));}
0
public void testRegexFileNameFiltering() throws IOException
{    append("a.log");    append("a.log.1");    append("b.log");    append("c.log.yyyy.MM-01");    append("c.log.yyyy.MM-02");        TaildirMatcher tm1 = new TaildirMatcher("ab", tmpDir.getAbsolutePath() + File.separator + "[ab].log", isCachingNeeded);        TaildirMatcher tm2 = new TaildirMatcher("c", tmpDir.getAbsolutePath() + File.separator + "c.log.*", isCachingNeeded);    List<String> files1 = filesToNames(tm1.getMatchingFiles());    List<String> files2 = filesToNames(tm2.getMatchingFiles());    assertEquals(2, files1.size());    assertEquals(2, files2.size());        assertTrue("Regex pattern for ab should have matched a.log file", files1.contains("a.log"));    assertFalse("Regex pattern for ab should NOT have matched a.log.1 file", files1.contains("a.log.1"));    assertTrue("Regex pattern for ab should have matched b.log file", files1.contains("b.log"));    assertTrue("Regex pattern for c should have matched c.log.yyyy-MM-01 file", files2.contains("c.log.yyyy.MM-01"));    assertTrue("Regex pattern for c should have matched c.log.yyyy-MM-02 file", files2.contains("c.log.yyyy.MM-02"));}
0
public void setUp()
{    source = new TaildirSource();    channel = new MemoryChannel();    Configurables.configure(channel, new Context());    List<Channel> channels = new ArrayList<Channel>();    channels.add(channel);    ChannelSelector rcs = new ReplicatingChannelSelector();    rcs.setChannels(channels);    source.setChannelProcessor(new ChannelProcessor(rcs));    tmpDir = Files.createTempDir();    posFilePath = tmpDir.getAbsolutePath() + "/taildir_position_test.json";}
0
public void tearDown()
{    for (File f : tmpDir.listFiles()) {        f.delete();    }    tmpDir.delete();}
0
public void testRegexFileNameFilteringEndToEnd() throws IOException
{    File f1 = new File(tmpDir, "a.log");    File f2 = new File(tmpDir, "a.log.1");    File f3 = new File(tmpDir, "b.log");    File f4 = new File(tmpDir, "c.log.yyyy-MM-01");    File f5 = new File(tmpDir, "c.log.yyyy-MM-02");    Files.write("a.log\n", f1, Charsets.UTF_8);    Files.write("a.log.1\n", f2, Charsets.UTF_8);    Files.write("b.log\n", f3, Charsets.UTF_8);    Files.write("c.log.yyyy-MM-01\n", f4, Charsets.UTF_8);    Files.write("c.log.yyyy-MM-02\n", f5, Charsets.UTF_8);    Context context = new Context();    context.put(POSITION_FILE, posFilePath);    context.put(FILE_GROUPS, "ab c");        context.put(FILE_GROUPS_PREFIX + "ab", tmpDir.getAbsolutePath() + "/[ab].log");        context.put(FILE_GROUPS_PREFIX + "c", tmpDir.getAbsolutePath() + "/c.log.*");    Configurables.configure(source, context);    source.start();    source.process();    Transaction txn = channel.getTransaction();    txn.begin();    List<String> out = Lists.newArrayList();    for (int i = 0; i < 5; i++) {        Event e = channel.take();        if (e != null) {            out.add(TestTaildirEventReader.bodyAsString(e));        }    }    txn.commit();    txn.close();    assertEquals(4, out.size());        assertTrue(out.contains("a.log"));    assertFalse(out.contains("a.log.1"));    assertTrue(out.contains("b.log"));    assertTrue(out.contains("c.log.yyyy-MM-01"));    assertTrue(out.contains("c.log.yyyy-MM-02"));}
0
public void testHeaderMapping() throws IOException
{    File f1 = new File(tmpDir, "file1");    File f2 = new File(tmpDir, "file2");    File f3 = new File(tmpDir, "file3");    Files.write("file1line1\nfile1line2\n", f1, Charsets.UTF_8);    Files.write("file2line1\nfile2line2\n", f2, Charsets.UTF_8);    Files.write("file3line1\nfile3line2\n", f3, Charsets.UTF_8);    Context context = new Context();    context.put(POSITION_FILE, posFilePath);    context.put(FILE_GROUPS, "f1 f2 f3");    context.put(FILE_GROUPS_PREFIX + "f1", tmpDir.getAbsolutePath() + "/file1$");    context.put(FILE_GROUPS_PREFIX + "f2", tmpDir.getAbsolutePath() + "/file2$");    context.put(FILE_GROUPS_PREFIX + "f3", tmpDir.getAbsolutePath() + "/file3$");    context.put(HEADERS_PREFIX + "f1.headerKeyTest", "value1");    context.put(HEADERS_PREFIX + "f2.headerKeyTest", "value2");    context.put(HEADERS_PREFIX + "f2.headerKeyTest2", "value2-2");    Configurables.configure(source, context);    source.start();    source.process();    Transaction txn = channel.getTransaction();    txn.begin();    for (int i = 0; i < 6; i++) {        Event e = channel.take();        String body = new String(e.getBody(), Charsets.UTF_8);        String headerValue = e.getHeaders().get("headerKeyTest");        String headerValue2 = e.getHeaders().get("headerKeyTest2");        if (body.startsWith("file1")) {            assertEquals("value1", headerValue);            assertNull(headerValue2);        } else if (body.startsWith("file2")) {            assertEquals("value2", headerValue);            assertEquals("value2-2", headerValue2);        } else if (body.startsWith("file3")) {                        assertNull(headerValue);            assertNull(headerValue2);        }    }    txn.commit();    txn.close();}
0
public void testLifecycle() throws IOException, InterruptedException
{    File f1 = new File(tmpDir, "file1");    Files.write("file1line1\nfile1line2\n", f1, Charsets.UTF_8);    Context context = new Context();    context.put(POSITION_FILE, posFilePath);    context.put(FILE_GROUPS, "f1");    context.put(FILE_GROUPS_PREFIX + "f1", tmpDir.getAbsolutePath() + "/file1$");    Configurables.configure(source, context);    for (int i = 0; i < 3; i++) {        source.start();        source.process();        assertTrue("Reached start or error", LifecycleController.waitForOneOf(source, LifecycleState.START_OR_ERROR));        assertEquals("Server is started", LifecycleState.START, source.getLifecycleState());        source.stop();        assertTrue("Reached stop or error", LifecycleController.waitForOneOf(source, LifecycleState.STOP_OR_ERROR));        assertEquals("Server is stopped", LifecycleState.STOP, source.getLifecycleState());    }}
0
private ArrayList<String> prepareFileConsumeOrder() throws IOException
{    System.out.println(tmpDir.toString());        File f1 = new File(tmpDir, "file1");    String line1 = "file1line1\n";    String line2 = "file1line2\n";    String line3 = "file1line3\n";    Files.write(line1 + line2 + line3, f1, Charsets.UTF_8);    try {                Thread.sleep(1000);    } catch (InterruptedException e) {    }        String line1b = "file2line1\n";    String line2b = "file2line2\n";    String line3b = "file2line3\n";    File f2 = new File(tmpDir, "file2");    Files.write(line1b + line2b + line3b, f2, Charsets.UTF_8);    try {                Thread.sleep(1000);    } catch (InterruptedException e) {    }        String line1c = "file3line1\n";    String line2c = "file3line2\n";    String line3c = "file3line3\n";    File f3 = new File(tmpDir, "file3");    Files.write(line1c + line2c + line3c, f3, Charsets.UTF_8);    try {                Thread.sleep(1000);    } catch (InterruptedException e) {    }        String line1d = "file4line1\n";    String line2d = "file4line2\n";    String line3d = "file4line3\n";    File f4 = new File(tmpDir, "file4");    Files.write(line1d + line2d + line3d, f4, Charsets.UTF_8);    try {                Thread.sleep(1000);    } catch (InterruptedException e) {    }        f3.setLastModified(System.currentTimeMillis());        Context context = new Context();    context.put(POSITION_FILE, posFilePath);    context.put(FILE_GROUPS, "g1");    context.put(FILE_GROUPS_PREFIX + "g1", tmpDir.getAbsolutePath() + "/.*");    Configurables.configure(source, context);        ArrayList<String> expected =     Lists.newArrayList(    line1,     line2,     line3,     line1b,     line2b,     line3b,     line1d,     line2d,     line3d,     line1c,     line2c,     line3c);    for (int i = 0; i != expected.size(); ++i) {        expected.set(i, expected.get(i).trim());    }    return expected;}
0
public void testFileConsumeOrder() throws IOException
{    ArrayList<String> consumedOrder = Lists.newArrayList();    ArrayList<String> expected = prepareFileConsumeOrder();    source.start();    source.process();    Transaction txn = channel.getTransaction();    txn.begin();    for (int i = 0; i < 12; i++) {        Event e = channel.take();        String body = new String(e.getBody(), Charsets.UTF_8);        consumedOrder.add(body);    }    txn.commit();    txn.close();    System.out.println(consumedOrder);    assertArrayEquals("Files not consumed in expected order", expected.toArray(), consumedOrder.toArray());}
0
private File configureSource() throws IOException
{    File f1 = new File(tmpDir, "file1");    Files.write("f1\n", f1, Charsets.UTF_8);    Context context = new Context();    context.put(POSITION_FILE, posFilePath);    context.put(FILE_GROUPS, "fg");    context.put(FILE_GROUPS_PREFIX + "fg", tmpDir.getAbsolutePath() + "/file.*");    context.put(FILENAME_HEADER, "true");    context.put(FILENAME_HEADER_KEY, "path");    Configurables.configure(source, context);    return f1;}
0
public void testPutFilenameHeader() throws IOException
{    File f1 = configureSource();    source.start();    source.process();    Transaction txn = channel.getTransaction();    txn.begin();    Event e = channel.take();    txn.commit();    txn.close();    assertNotNull(e.getHeaders().get("path"));    assertEquals(f1.getAbsolutePath(), e.getHeaders().get("path"));}
0
public void testErrorCounterEventReadFail() throws Exception
{    configureSource();    source.start();    ReliableTaildirEventReader reader = Mockito.mock(ReliableTaildirEventReader.class);    Whitebox.setInternalState(source, "reader", reader);    when(reader.updateTailFiles()).thenReturn(Collections.singletonList(123L));    when(reader.getTailFiles()).thenThrow(new RuntimeException("hello"));    source.process();    assertEquals(1, source.getSourceCounter().getEventReadFail());    source.stop();}
0
public void testErrorCounterFileHandlingFail() throws Exception
{    configureSource();    Whitebox.setInternalState(source, "idleTimeout", 0);    Whitebox.setInternalState(source, "checkIdleInterval", 60);    source.start();    ReliableTaildirEventReader reader = Mockito.mock(ReliableTaildirEventReader.class);    when(reader.getTailFiles()).thenThrow(new RuntimeException("hello"));    Whitebox.setInternalState(source, "reader", reader);    TimeUnit.MILLISECONDS.sleep(200);    assertTrue(0 < source.getSourceCounter().getGenericProcessingFail());    source.stop();}
0
public void testErrorCounterChannelWriteFail() throws Exception
{    prepareFileConsumeOrder();    ChannelProcessor cp = Mockito.mock(ChannelProcessor.class);    source.setChannelProcessor(cp);    doThrow(new ChannelException("dummy")).doNothing().when(cp).processEventBatch(anyListOf(Event.class));    source.start();    source.process();    assertEquals(1, source.getSourceCounter().getChannelWriteFail());    source.stop();}
0
public void testMaxBatchCount() throws IOException
{    File f1 = new File(tmpDir, "file1");    File f2 = new File(tmpDir, "file2");    Files.write("file1line1\nfile1line2\n" + "file1line3\nfile1line4\n", f1, Charsets.UTF_8);    Files.write("file2line1\nfile2line2\n" + "file2line3\nfile2line4\n", f2, Charsets.UTF_8);    Context context = new Context();    context.put(POSITION_FILE, posFilePath);    context.put(FILE_GROUPS, "fg");    context.put(FILE_GROUPS_PREFIX + "fg", tmpDir.getAbsolutePath() + "/file.*");    context.put(BATCH_SIZE, String.valueOf(1));    context.put(MAX_BATCH_COUNT, String.valueOf(2));    Configurables.configure(source, context);    source.start();        source.process();    source.process();    List<Event> eventList = new ArrayList<Event>();    for (int i = 0; i < 8; i++) {        Transaction txn = channel.getTransaction();        txn.begin();        Event e = channel.take();        txn.commit();        txn.close();        if (e == null) {            break;        }        eventList.add(e);    }    assertEquals("1", context.getString(BATCH_SIZE));    assertEquals("2", context.getString(MAX_BATCH_COUNT));    assertEquals(8, eventList.size());        String firstFile = new String(eventList.get(0).getBody()).substring(0, 5);    String secondFile = firstFile.equals("file1") ? "file2" : "file1";    assertEquals(firstFile + "line1", new String(eventList.get(0).getBody()));    assertEquals(firstFile + "line2", new String(eventList.get(1).getBody()));    assertEquals(secondFile + "line1", new String(eventList.get(2).getBody()));    assertEquals(secondFile + "line2", new String(eventList.get(3).getBody()));    assertEquals(firstFile + "line3", new String(eventList.get(4).getBody()));    assertEquals(firstFile + "line4", new String(eventList.get(5).getBody()));    assertEquals(secondFile + "line3", new String(eventList.get(6).getBody()));    assertEquals(secondFile + "line4", new String(eventList.get(7).getBody()));}
0
public void testStatus() throws IOException
{    File f1 = new File(tmpDir, "file1");    File f2 = new File(tmpDir, "file2");    Files.write("file1line1\nfile1line2\n" + "file1line3\nfile1line4\nfile1line5\n", f1, Charsets.UTF_8);    Files.write("file2line1\nfile2line2\n" + "file2line3\n", f2, Charsets.UTF_8);    Context context = new Context();    context.put(POSITION_FILE, posFilePath);    context.put(FILE_GROUPS, "fg");    context.put(FILE_GROUPS_PREFIX + "fg", tmpDir.getAbsolutePath() + "/file.*");    context.put(BATCH_SIZE, String.valueOf(1));    context.put(MAX_BATCH_COUNT, String.valueOf(2));    Configurables.configure(source, context);    source.start();    Status status;    status = source.process();    assertEquals(Status.READY, status);    status = source.process();    assertEquals(Status.READY, status);    status = source.process();    assertEquals(Status.BACKOFF, status);    status = source.process();    assertEquals(Status.BACKOFF, status);}
0
public void configure(Context context)
{    String consumerKey = context.getString("consumerKey");    String consumerSecret = context.getString("consumerSecret");    String accessToken = context.getString("accessToken");    String accessTokenSecret = context.getString("accessTokenSecret");    twitterStream = new TwitterStreamFactory().getInstance();    twitterStream.setOAuthConsumer(consumerKey, consumerSecret);    twitterStream.setOAuthAccessToken(new AccessToken(accessToken, accessTokenSecret));    twitterStream.addListener(this);    avroSchema = createAvroSchema();    dataFileWriter = new DataFileWriter<GenericRecord>(new GenericDatumWriter<GenericRecord>(avroSchema));    maxBatchSize = context.getInteger("maxBatchSize", maxBatchSize);    maxBatchDurationMillis = context.getInteger("maxBatchDurationMillis", maxBatchDurationMillis);}
0
public synchronized void start()
{        docCount = 0;    startTime = System.currentTimeMillis();    exceptionCount = 0;    totalTextIndexed = 0;    skippedDocs = 0;    batchEndTime = System.currentTimeMillis() + maxBatchDurationMillis;    twitterStream.sample();                            super.start();}
1
public synchronized void stop()
{        twitterStream.shutdown();    super.stop();    }
1
public void onStatus(Status status)
{    Record doc = extractRecord("", avroSchema, status);    if (doc == null) {                return;    }    docs.add(doc);    if (docs.size() >= maxBatchSize || System.currentTimeMillis() >= batchEndTime) {        batchEndTime = System.currentTimeMillis() + maxBatchDurationMillis;        byte[] bytes;        try {            bytes = serializeToAvro(avroSchema, docs);        } catch (IOException e) {                                    return;        }        Event event = EventBuilder.withBody(bytes);                getChannelProcessor().processEvent(event);        docs.clear();    }    docCount++;    if ((docCount % REPORT_INTERVAL) == 0) {            }    if ((docCount % STATS_INTERVAL) == 0) {        logStats();    }}
1
private Schema createAvroSchema()
{    Schema avroSchema = Schema.createRecord("Doc", "adoc", null, false);    List<Field> fields = new ArrayList<Field>();    fields.add(new Field("id", Schema.create(Type.STRING), null, null));    fields.add(new Field("user_friends_count", createOptional(Schema.create(Type.INT)), null, null));    fields.add(new Field("user_location", createOptional(Schema.create(Type.STRING)), null, null));    fields.add(new Field("user_description", createOptional(Schema.create(Type.STRING)), null, null));    fields.add(new Field("user_statuses_count", createOptional(Schema.create(Type.INT)), null, null));    fields.add(new Field("user_followers_count", createOptional(Schema.create(Type.INT)), null, null));    fields.add(new Field("user_name", createOptional(Schema.create(Type.STRING)), null, null));    fields.add(new Field("user_screen_name", createOptional(Schema.create(Type.STRING)), null, null));    fields.add(new Field("created_at", createOptional(Schema.create(Type.STRING)), null, null));    fields.add(new Field("text", createOptional(Schema.create(Type.STRING)), null, null));    fields.add(new Field("retweet_count", createOptional(Schema.create(Type.LONG)), null, null));    fields.add(new Field("retweeted", createOptional(Schema.create(Type.BOOLEAN)), null, null));    fields.add(new Field("in_reply_to_user_id", createOptional(Schema.create(Type.LONG)), null, null));    fields.add(new Field("source", createOptional(Schema.create(Type.STRING)), null, null));    fields.add(new Field("in_reply_to_status_id", createOptional(Schema.create(Type.LONG)), null, null));    fields.add(new Field("media_url_https", createOptional(Schema.create(Type.STRING)), null, null));    fields.add(new Field("expanded_url", createOptional(Schema.create(Type.STRING)), null, null));    avroSchema.setFields(fields);    return avroSchema;}
0
private Record extractRecord(String idPrefix, Schema avroSchema, Status status)
{    User user = status.getUser();    Record doc = new Record(avroSchema);    doc.put("id", idPrefix + status.getId());    doc.put("created_at", formatterTo.format(status.getCreatedAt()));    doc.put("retweet_count", status.getRetweetCount());    doc.put("retweeted", status.isRetweet());    doc.put("in_reply_to_user_id", status.getInReplyToUserId());    doc.put("in_reply_to_status_id", status.getInReplyToStatusId());    addString(doc, "source", status.getSource());    addString(doc, "text", status.getText());    MediaEntity[] mediaEntities = status.getMediaEntities();    if (mediaEntities.length > 0) {        addString(doc, "media_url_https", mediaEntities[0].getMediaURLHttps());        addString(doc, "expanded_url", mediaEntities[0].getExpandedURL());    }    doc.put("user_friends_count", user.getFriendsCount());    doc.put("user_statuses_count", user.getStatusesCount());    doc.put("user_followers_count", user.getFollowersCount());    addString(doc, "user_location", user.getLocation());    addString(doc, "user_description", user.getDescription());    addString(doc, "user_screen_name", user.getScreenName());    addString(doc, "user_name", user.getName());    return doc;}
0
private byte[] serializeToAvro(Schema avroSchema, List<Record> docList) throws IOException
{    serializationBuffer.reset();    dataFileWriter.create(avroSchema, serializationBuffer);    for (Record doc2 : docList) {        dataFileWriter.append(doc2);    }    dataFileWriter.close();    return serializationBuffer.toByteArray();}
0
private Schema createOptional(Schema schema)
{    return Schema.createUnion(Arrays.asList(new Schema[] { schema, Schema.create(Type.NULL) }));}
0
private void addString(Record doc, String avroField, String val)
{    if (val == null) {        return;    }    doc.put(avroField, val);    totalTextIndexed += val.length();}
0
private void logStats()
{    double mbIndexed = totalTextIndexed / (1024 * 1024.0);    long seconds = (System.currentTimeMillis() - startTime) / 1000;    seconds = Math.max(seconds, 1);                        }
1
public void onDeletionNotice(StatusDeletionNotice statusDeletionNotice)
{}
0
public void onScrubGeo(long userId, long upToStatusId)
{}
0
public void onStallWarning(StallWarning warning)
{}
0
public void onTrackLimitationNotice(int numberOfLimitedStatuses)
{}
0
public void onException(Exception e)
{    }
1
public long getBatchSize()
{    return maxBatchSize;}
0
public static void setUp()
{    try {        Assume.assumeNotNull(InetAddress.getByName("stream.twitter.com"));    } catch (UnknownHostException e) {                Assume.assumeTrue(false);    }}
0
public void testBasic() throws Exception
{    String consumerKey = System.getProperty("twitter.consumerKey");    Assume.assumeNotNull(consumerKey);    String consumerSecret = System.getProperty("twitter.consumerSecret");    Assume.assumeNotNull(consumerSecret);    String accessToken = System.getProperty("twitter.accessToken");    Assume.assumeNotNull(accessToken);    String accessTokenSecret = System.getProperty("twitter.accessTokenSecret");    Assume.assumeNotNull(accessTokenSecret);    Context context = new Context();    context.put("consumerKey", consumerKey);    context.put("consumerSecret", consumerSecret);    context.put("accessToken", accessToken);    context.put("accessTokenSecret", accessTokenSecret);    context.put("maxBatchDurationMillis", "1000");    TwitterSource source = new TwitterSource();    source.configure(context);    Map<String, String> channelContext = new HashMap();    channelContext.put("capacity", "1000000");        channelContext.put("keep-alive", "0");    Channel channel = new MemoryChannel();    Configurables.configure(channel, new Context(channelContext));    Sink sink = new LoggerSink();    sink.setChannel(channel);    sink.start();    DefaultSinkProcessor proc = new DefaultSinkProcessor();    proc.setSinks(Collections.singletonList(sink));    SinkRunner sinkRunner = new SinkRunner(proc);    sinkRunner.start();    ChannelSelector rcs = new ReplicatingChannelSelector();    rcs.setChannels(Collections.singletonList(channel));    ChannelProcessor chp = new ChannelProcessor(rcs);    source.setChannelProcessor(chp);    source.start();    Thread.sleep(5000);    source.stop();    sinkRunner.stop();    sink.stop();}
0
public void testCarrotDateFormatBug() throws Exception
{    SimpleDateFormat formatterFrom = new SimpleDateFormat("EEE MMM dd HH:mm:ss Z yyyy");    formatterFrom.parse("Fri Oct 26 22:53:55 +0000 2012");}
0
public void setup() throws Exception
{    File agentDir = StagedInstall.getInstance().getStageDir();        File testDir = new File(agentDir, TestConfig.class.getName());    if (testDir.exists()) {        FileUtils.deleteDirectory(testDir);    }    assertTrue(testDir.mkdirs());    agentProps = new Properties();    agentEnv = new HashMap<>();    agentOptions = new HashMap<>();    agentOptions.put("-C", getAdditionalClassPath());        agentProps.put("agent.sources.seq-01.type", "seq");    agentProps.put("agent.sources.seq-01.totalEvents", "100");    agentProps.put("agent.sources.seq-01.channels", "mem-01 mem-02 mem-03");    agentProps.put("agent.channels.mem-01.type", "MEMORY");    agentProps.put("agent.channels.mem-01.capacity", String.valueOf(100000));    agentProps.put("agent.channels.mem-02.type", "MEMORY");    agentProps.put("agent.channels.mem-02.capacity", String.valueOf(100000));    agentProps.put("agent.channels.mem-03.type", "MEMORY");    agentProps.put("agent.channels.mem-04.capacity", String.valueOf(100000));    sinkOutputDir1 = new File(testDir, "out1");    assertTrue("Unable to create sink output dir: " + sinkOutputDir1.getPath(), sinkOutputDir1.mkdir());    sinkOutputDir2 = new File(testDir, "out2");    assertTrue("Unable to create sink output dir: " + sinkOutputDir2.getPath(), sinkOutputDir2.mkdir());    sinkOutputDir3 = new File(testDir, "out3");    assertTrue("Unable to create sink output dir: " + sinkOutputDir3.getPath(), sinkOutputDir3.mkdir());    environmentVariables.set("HADOOP_CREDSTORE_PASSWORD", "envSecret");    agentEnv.put("dirname_env", sinkOutputDir1.getAbsolutePath());    agentEnv.put("HADOOP_CREDSTORE_PASSWORD", "envSecret");    hadoopCredStore = new File(testDir, "credstore.jceks");    String providerPath = "jceks://file/" + hadoopCredStore.getAbsolutePath();    ToolRunner.run(new Configuration(), new CredentialShell(), ("create dirname_hadoop -value " + sinkOutputDir3.getAbsolutePath() + " -provider " + providerPath).split(" "));    agentProps.put("agent.sinks.roll-01.channel", "mem-01");    agentProps.put("agent.sinks.roll-01.type", "FILE_ROLL");    agentProps.put("agent.sinks.roll-01.sink.directory", "${filter-01[\"dirname_env\"]}");    agentProps.put("agent.sinks.roll-01.sink.rollInterval", "0");    agentProps.put("agent.sinks.roll-02.channel", "mem-02");    agentProps.put("agent.sinks.roll-02.type", "FILE_ROLL");    agentProps.put("agent.sinks.roll-02.sink.directory", sinkOutputDir2.getParentFile().getAbsolutePath() + "/${filter-02['out2']}");    agentProps.put("agent.sinks.roll-02.sink.rollInterval", "0");    agentProps.put("agent.sinks.roll-03.channel", "mem-03");    agentProps.put("agent.sinks.roll-03.type", "FILE_ROLL");    agentProps.put("agent.sinks.roll-03.sink.directory", "${filter-03[dirname_hadoop]}");    agentProps.put("agent.sinks.roll-03.sink.rollInterval", "0");    agentProps.put("agent.configfilters.filter-01.type", "env");    agentProps.put("agent.configfilters.filter-02.type", "external");    agentProps.put("agent.configfilters.filter-02.command", "echo");    agentProps.put("agent.configfilters.filter-03.type", "hadoop");    agentProps.put("agent.configfilters.filter-03.credential.provider.path", providerPath);    agentProps.put("agent.sources", "seq-01");    agentProps.put("agent.channels", "mem-01 mem-02 mem-03");    agentProps.put("agent.sinks", "roll-01 roll-02 roll-03");    agentProps.put("agent.configfilters", "filter-01 filter-02 filter-03");}
1
private String getAdditionalClassPath() throws Exception
{    URL resource = this.getClass().getClassLoader().getResource("classpath.txt");    Path path = Paths.get(Objects.requireNonNull(resource).getPath());    return Files.readAllLines(path).stream().findFirst().orElse("");}
0
public void teardown() throws Exception
{    StagedInstall.getInstance().stopAgent();}
0
private void validateSeenEvents(File outDir, int outFiles, int events) throws IOException
{    File[] sinkOutputDirChildren = outDir.listFiles();    assertEquals("Unexpected number of files in output dir", outFiles, sinkOutputDirChildren.length);    Set<String> seenEvents = new HashSet<>();    for (File outFile : sinkOutputDirChildren) {        Scanner scanner = new Scanner(outFile);        while (scanner.hasNext()) {            seenEvents.add(scanner.nextLine());        }    }    for (int event = 0; event < events; event++) {        assertTrue("Missing event: {" + event + "}", seenEvents.contains(String.valueOf(event)));    }}
0
public void testConfigReplacement() throws Exception
{        StagedInstall.getInstance().startAgent("agent", agentProps, agentEnv, agentOptions);        TimeUnit.SECONDS.sleep(10);        validateSeenEvents(sinkOutputDir1, 1, 100);    validateSeenEvents(sinkOutputDir2, 1, 100);    validateSeenEvents(sinkOutputDir3, 1, 100);        }
1
public void testConfigReload() throws Exception
{        agentProps.put("agent.channels.mem-01.transactionCapacity", "10");    agentProps.put("agent.sinks.roll-01.sink.batchSize", "20");    StagedInstall.getInstance().startAgent("agent", agentProps, agentEnv, agentOptions);        TimeUnit.SECONDS.sleep(10);        validateSeenEvents(sinkOutputDir1, 0, 0);        validateSeenEvents(sinkOutputDir2, 1, 100);    validateSeenEvents(sinkOutputDir3, 1, 100);            agentProps.put("agent.channels.mem-01.transactionCapacity", "20");    StagedInstall.getInstance().reconfigure(agentProps);        TimeUnit.SECONDS.sleep(40);        validateSeenEvents(sinkOutputDir1, 1, 100);    }
1
public void setUp() throws Exception
{    /* Create 3 temp dirs, each used as value within agentProps */    final File sinkOutputDir = Files.createTempDir();    tempResources.add(sinkOutputDir);    final String sinkOutputDirPath = sinkOutputDir.getCanonicalPath();        final File channelCheckpointDir = Files.createTempDir();    tempResources.add(channelCheckpointDir);    final String channelCheckpointDirPath = channelCheckpointDir.getCanonicalPath();        final File channelDataDir = Files.createTempDir();    tempResources.add(channelDataDir);    final String channelDataDirPath = channelDataDir.getCanonicalPath();        /* Build props to pass to flume agent */    Properties agentProps = new Properties();        agentProps.put("a1.channels", "c1");    agentProps.put("a1.sources", "r1");    agentProps.put("a1.sinks", "k1");        agentProps.put("a1.channels.c1.type", "FILE");    agentProps.put("a1.channels.c1.checkpointDir", channelCheckpointDirPath);    agentProps.put("a1.channels.c1.dataDirs", channelDataDirPath);        agentProps.put("a1.sources.r1.channels", "c1");    agentProps.put("a1.sources.r1.type", "EXEC");    agentProps.put("a1.sources.r1.command", "seq 1 100");        agentProps.put("a1.sinks.k1.channel", "c1");    agentProps.put("a1.sinks.k1.type", "FILE_ROLL");    agentProps.put("a1.sinks.k1.sink.directory", sinkOutputDirPath);    agentProps.put("a1.sinks.k1.sink.rollInterval", "0");    this.agentProps = agentProps;    this.sinkOutputDir = sinkOutputDir;}
1
public void tearDown() throws Exception
{    StagedInstall.getInstance().stopAgent();    for (File tempResource : tempResources) {        tempResource.delete();    }    agentProps = null;}
0
public void testInOut() throws Exception
{        StagedInstall.getInstance().startAgent("a1", agentProps);        TimeUnit.SECONDS.sleep(10);        /* Create expected output */    StringBuffer sb = new StringBuffer();    for (int i = 1; i <= 100; i++) {        sb.append(i).append("\n");    }    String expectedOutput = sb.toString();        /* Create actual output file */    File[] sinkOutputDirChildren = sinkOutputDir.listFiles();        Assert.assertEquals("Expected FILE_ROLL sink's dir to have only 1 child," + " but found " + sinkOutputDirChildren.length + " children.", 1, sinkOutputDirChildren.length);    File actualOutput = sinkOutputDirChildren[0];    if (!Files.toString(actualOutput, Charsets.UTF_8).equals(expectedOutput)) {                throw new AssertionError("FILE_ROLL sink's actual output doesn't " + "match expected output.");    }    }
1
public void setUp() throws Exception
{    port = StagedInstall.getInstance().startAgent("rpccagent", CONFIG_FILE_PRCCLIENT_TEST);}
0
public void tearDown() throws Exception
{    StagedInstall.getInstance().stopAgent();}
0
public void testRpcClient() throws Exception
{    StagedInstall.waitUntilPortOpens("localhost", port, 20000);    RpcClient client = RpcClientFactory.getDefaultInstance("localhost", port);    String[] text = { "foo", "bar", "xyz", "abc" };    for (String str : text) {        client.append(EventBuilder.withBody(str.getBytes()));    }}
0
public void testFailure() throws Exception
{    try {        int port = StagedInstall.getInstance().startAgent("rpccagent", CONFIG_FILE_PRCCLIENT_TEST);        StagedInstall.waitUntilPortOpens("localhost", port, 20000);        RpcClient client = RpcClientFactory.getDefaultInstance("localhost", port);        String[] text = { "foo", "bar", "xyz", "abc" };        for (String str : text) {            client.append(EventBuilder.withBody(str.getBytes()));        }                StagedInstall.getInstance().stopAgent();                try {            client.append(EventBuilder.withBody("test".getBytes()));            Assert.fail("EventDeliveryException expected but not raised");        } catch (EventDeliveryException ex) {            System.out.println("Attempting to close client");            client.close();        }    } finally {        if (StagedInstall.getInstance().isRunning()) {            StagedInstall.getInstance().stopAgent();        }    }}
0
public void setup() throws Exception
{    File agentDir = StagedInstall.getInstance().getStageDir();        File testDir = new File(agentDir, TestSpooldirSource.class.getName());    assertTrue(testDir.mkdirs());    File spoolParentDir = new File(testDir, "spools");    assertTrue("Unable to create sink output dir: " + spoolParentDir.getPath(), spoolParentDir.mkdir());    final int NUM_SOURCES = 100;    agentProps = new Properties();    List<String> spooldirSrcNames = Lists.newArrayList();    String channelName = "mem-01";        for (int i = 0; i < NUM_SOURCES; i++) {        String srcName = String.format("spooldir-%03d", i);        File spoolDir = new File(spoolParentDir, srcName);        assertTrue(spoolDir.mkdir());        spooldirSrcNames.add(srcName);        spoolDirs.add(spoolDir);        agentProps.put(String.format("agent.sources.%s.type", srcName), "SPOOLDIR");        agentProps.put(String.format("agent.sources.%s.spoolDir", srcName), spoolDir.getPath());        agentProps.put(String.format("agent.sources.%s.channels", srcName), channelName);    }        agentProps.put("agent.channels.mem-01.type", "MEMORY");    agentProps.put("agent.channels.mem-01.capacity", String.valueOf(100000));    sinkOutputDir = new File(testDir, "out");    assertTrue("Unable to create sink output dir: " + sinkOutputDir.getPath(), sinkOutputDir.mkdir());    agentProps.put("agent.sinks.roll-01.channel", channelName);    agentProps.put("agent.sinks.roll-01.type", "FILE_ROLL");    agentProps.put("agent.sinks.roll-01.sink.directory", sinkOutputDir.getPath());    agentProps.put("agent.sinks.roll-01.sink.rollInterval", "0");    agentProps.put("agent.sources", Joiner.on(" ").join(spooldirSrcNames));    agentProps.put("agent.channels", channelName);    agentProps.put("agent.sinks", "roll-01");}
1
public void teardown() throws Exception
{    StagedInstall.getInstance().stopAgent();}
0
private String getTestString(int dirNum, int fileNum)
{    return String.format("Test dir %03d, test file %03d.\n", dirNum, fileNum);}
0
private void createInputTestFiles(List<File> spoolDirs, int numFiles, int startNum) throws IOException
{    int numSpoolDirs = spoolDirs.size();    for (int dirNum = 0; dirNum < numSpoolDirs; dirNum++) {        File spoolDir = spoolDirs.get(dirNum);        for (int fileNum = startNum; fileNum < numFiles; fileNum++) {                        File tmp = new File(spoolDir.getParent(), UUID.randomUUID().toString());            Files.append(getTestString(dirNum, fileNum), tmp, Charsets.UTF_8);            File dst = new File(spoolDir, String.format("test-file-%03d", fileNum));                        assertTrue(String.format("Failed to rename %s to %s", tmp, dst), tmp.renameTo(dst));        }    }}
0
private void validateSeenEvents(File outDir, int outFiles, int dirs, int events) throws IOException
{    File[] sinkOutputDirChildren = outDir.listFiles();    assertEquals("Unexpected number of files in output dir", outFiles, sinkOutputDirChildren.length);    Set<String> seenEvents = Sets.newHashSet();    for (File outFile : sinkOutputDirChildren) {        List<String> lines = Files.readLines(outFile, Charsets.UTF_8);        for (String line : lines) {            seenEvents.add(line);        }    }    for (int dirNum = 0; dirNum < dirs; dirNum++) {        for (int fileNum = 0; fileNum < events; fileNum++) {            String event = getTestString(dirNum, fileNum).trim();            assertTrue("Missing event: {" + event + "}", seenEvents.contains(event));        }    }}
0
public void testManySpooldirs() throws Exception
{        StagedInstall.getInstance().startAgent("agent", agentProps);    final int NUM_FILES_PER_DIR = 10;    createInputTestFiles(spoolDirs, NUM_FILES_PER_DIR, 0);        TimeUnit.SECONDS.sleep(10);        validateSeenEvents(sinkOutputDir, 1, spoolDirs.size(), NUM_FILES_PER_DIR);        }
1
public static Collection syslogSourceTypes()
{    List<Object[]> sourceTypes = new ArrayList<Object[]>();    for (SyslogAgent.SyslogSourceType sourceType : SyslogAgent.SyslogSourceType.values()) {        sourceTypes.add(new Object[] { sourceType });    }    return sourceTypes;}
0
public void setUp() throws Exception
{    agent = new SyslogAgent();    agent.configure(sourceType);}
0
public void tearDown() throws Exception
{    if (agent != null) {        agent.stop();        agent = null;    }}
0
public void testKeepFields() throws Exception
{        agent.start("all");    agent.runKeepFieldsTest();    }
1
public void testRemoveFields() throws Exception
{        agent.start("none");    agent.runKeepFieldsTest();    }
1
public void testKeepTimestampAndHostname() throws Exception
{        agent.start("timestamp hostname");    agent.runKeepFieldsTest();    }
1
public static synchronized StagedInstall getInstance() throws Exception
{    if (INSTANCE == null) {        INSTANCE = new StagedInstall();    }    return INSTANCE;}
0
public static int findFreePort() throws IOException
{    try (ServerSocket socket = new ServerSocket(0)) {        return socket.getLocalPort();    }}
0
public synchronized boolean isRunning()
{    return process != null;}
0
public synchronized void stopAgent() throws Exception
{    if (process == null) {        throw new Exception("Process not found");    }        process.destroy();    process.waitFor();    process = null;    consumer.interrupt();    consumer = null;    configFilePath = null;    Runtime.getRuntime().removeShutdownHook(shutdownHook);    shutdownHook = null;        Thread.sleep(3000);}
1
public synchronized int startAgent(String name, String configResource) throws Exception
{    if (process != null) {        throw new Exception("A process is already running");    }    int port = findFreePort();    Properties props = new Properties();    props.load(ClassLoader.getSystemResourceAsStream(configResource));    props.put("rpccagent.sources.src1.port", String.valueOf(port));    startAgent(name, props);    return port;}
0
public synchronized void startAgent(String name, Properties properties) throws Exception
{    startAgent(name, properties, new HashMap<>(), new HashMap<>());}
0
public synchronized void startAgent(String name, Properties properties, Map<String, String> environmentVariables, Map<String, String> commandOptions) throws Exception
{    Preconditions.checkArgument(!name.isEmpty(), "agent name must not be empty");    Preconditions.checkNotNull(properties, "properties object must not be null");    agentName = name;    if (process != null) {        throw new Exception("A process is already running");    }        File configFile = createConfigurationFile(agentName, properties);    configFilePath = configFile.getCanonicalPath();    String configFileName = configFile.getName();    String logFileName = "flume-" + agentName + "-" + configFileName.substring(0, configFileName.indexOf('.')) + ".log";        ImmutableList.Builder<String> builder = new ImmutableList.Builder<String>();    builder.add(launchScriptPath);    builder.add("agent");    builder.add("--conf", confDirPath);    builder.add("--conf-file", configFilePath);    builder.add("--name", agentName);    builder.add("-D" + ENV_FLUME_LOG_DIR + "=" + logDirPath);    builder.add("-D" + ENV_FLUME_ROOT_LOGGER + "=" + ENV_FLUME_ROOT_LOGGER_VALUE);    builder.add("-D" + ENV_FLUME_LOG_FILE + "=" + logFileName);    commandOptions.forEach((key, value) -> builder.add(key, value));    List<String> cmdArgs = builder.build();        ProcessBuilder pb = new ProcessBuilder(cmdArgs);    Map<String, String> env = pb.environment();    env.putAll(environmentVariables);        pb.directory(baseDir);    pb.redirectErrorStream(true);    process = pb.start();    consumer = new ProcessInputStreamConsumer(process.getInputStream());    consumer.start();    shutdownHook = new ProcessShutdownHook();    Runtime.getRuntime().addShutdownHook(shutdownHook);        Thread.sleep(3000);}
1
public synchronized void reconfigure(Properties properties) throws Exception
{    File configFile = createConfigurationFile(agentName, properties);    Files.copy(configFile, new File(configFilePath));    configFile.delete();    }
1
public synchronized File getStageDir()
{    return stageDir;}
0
private File createConfigurationFile(String agentName, Properties properties) throws Exception
{    Preconditions.checkNotNull(properties, "properties object must not be null");    File file = File.createTempFile("agent", "config.properties", stageDir);    OutputStream os = null;    try {        os = new FileOutputStream(file);        properties.store(os, "Config file for agent: " + agentName);    } catch (Exception ex) {                throw ex;    } finally {        if (os != null) {            try {                os.close();            } catch (Exception ex) {                            }        }    }    return file;}
1
private void giveExecutePermissions(File file) throws Exception
{    String[] args = { "chmod", "+x", file.getCanonicalPath() };    Runtime.getRuntime().exec(args);    }
1
private void untarTarFile(File tarFile, File destDir) throws Exception
{    TarArchiveInputStream tarInputStream = null;    try {        tarInputStream = new TarArchiveInputStream(new FileInputStream(tarFile));        TarArchiveEntry entry = null;        while ((entry = tarInputStream.getNextTarEntry()) != null) {            String name = entry.getName();                        File destFile = new File(destDir, entry.getName());            if (entry.isDirectory()) {                destFile.mkdirs();                continue;            }            File destParent = destFile.getParentFile();            destParent.mkdirs();            OutputStream entryOutputStream = null;            try {                entryOutputStream = new FileOutputStream(destFile);                byte[] buffer = new byte[2048];                int length = 0;                while ((length = tarInputStream.read(buffer, 0, 2048)) != -1) {                    entryOutputStream.write(buffer, 0, length);                }            } catch (Exception ex) {                                throw ex;            } finally {                if (entryOutputStream != null) {                    try {                        entryOutputStream.close();                    } catch (Exception ex) {                                            }                }            }        }    } catch (Exception ex) {                throw ex;    } finally {        if (tarInputStream != null) {            try {                tarInputStream.close();            } catch (Exception ex) {                            }        }    }}
1
private File gunzipDistTarball(File tarballFile, File destDir) throws Exception
{    File tarFile = null;    InputStream tarballInputStream = null;    OutputStream tarFileOutputStream = null;    try {        tarballInputStream = new GZIPInputStream(new FileInputStream(tarballFile));        File temp2File = File.createTempFile("flume", "-bin", destDir);        String temp2FilePath = temp2File.getCanonicalPath();        temp2File.delete();        tarFile = new File(temp2FilePath + ".tar");                tarFileOutputStream = new FileOutputStream(tarFile);        int length = 0;        byte[] buffer = new byte[10240];        while ((length = tarballInputStream.read(buffer, 0, 10240)) != -1) {            tarFileOutputStream.write(buffer, 0, length);        }    } catch (Exception ex) {                throw ex;    } finally {        if (tarballInputStream != null) {            try {                tarballInputStream.close();            } catch (Exception ex) {                            }        }        if (tarFileOutputStream != null) {            try {                tarFileOutputStream.close();            } catch (Exception ex) {                            }        }    }    return tarFile;}
1
private File getStagingDirectory() throws Exception
{    File targetDir = new File("target");    if (!targetDir.exists() || !targetDir.isDirectory()) {                targetDir = new File(System.getProperty("java.io.tmpdir"));    }    File testDir = new File(targetDir, "test");    testDir.mkdirs();    File tempFile = File.createTempFile("flume", "_stage", testDir);    String absFileName = tempFile.getCanonicalPath();    tempFile.delete();    File stageDir = new File(absFileName + "_dir");    if (stageDir.exists()) {        throw new Exception("Stage directory exists: " + stageDir.getCanonicalPath());    }    stageDir.mkdirs();        return stageDir;}
1
private String getRelativeTarballPath() throws Exception
{    String tarballPath = null;    File dir = new File("..");    while (dir != null && dir.isDirectory()) {        File testFile = new File(dir, "flume-ng-dist/target");        if (testFile.exists() && testFile.isDirectory()) {                        File[] candidateFiles = testFile.listFiles(new FileFilter() {                @Override                public boolean accept(File pathname) {                    String name = pathname.getName();                    if (name != null && name.startsWith("apache-flume-") && name.endsWith("-bin.tar.gz")) {                        return true;                    }                    return false;                }            });                        if (candidateFiles != null && candidateFiles.length > 0) {                if (candidateFiles.length == 1) {                                        File file = candidateFiles[0];                    if (file.isFile() && file.canRead()) {                        tarballPath = file.getCanonicalPath();                                                break;                    } else {                                            }                } else {                    StringBuilder sb = new StringBuilder("Multiple candate tarballs");                    sb.append(" found in directory ");                    sb.append(testFile.getCanonicalPath()).append(": ");                    boolean first = true;                    for (File file : candidateFiles) {                        if (first) {                            first = false;                            sb.append(" ");                        } else {                            sb.append(", ");                        }                        sb.append(file.getCanonicalPath());                    }                    sb.append(". All these files will be ignored.");                                    }            }        }        dir = dir.getParentFile();    }    return tarballPath;}
1
public boolean accept(File pathname)
{    String name = pathname.getName();    if (name != null && name.startsWith("apache-flume-") && name.endsWith("-bin.tar.gz")) {        return true;    }    return false;}
0
public static void waitUntilPortOpens(String host, int port, long timeout) throws IOException, InterruptedException
{    long startTime = System.currentTimeMillis();    Socket socket;    boolean connected = false;        while (System.currentTimeMillis() - startTime < timeout) {        try {            socket = new Socket(host, port);            socket.close();            connected = true;            break;        } catch (IOException e) {            Thread.sleep(2000);        }    }    if (!connected) {        throw new IOException("Port not opened within specified timeout.");    }}
0
public void run()
{    synchronized (StagedInstall.this) {        if (StagedInstall.this.process != null) {            process.destroy();        }    }}
0
public void run()
{    try {        byte[] buffer = new byte[1024];        int length = 0;        while ((length = is.read(buffer, 0, 1024)) != -1) {                    }    } catch (Exception ex) {            }}
1
public String toString()
{    return syslogSourceType;}
0
public void setRandomPort() throws IOException
{    ServerSocket s = new ServerSocket(0);    port = s.getLocalPort();    s.close();}
0
public void configure(SyslogSourceType sourceType) throws IOException
{    /* Create 3 temp dirs, each used as value within agentProps */    sinkOutputDir = Files.createTempDir();    tempResources.add(sinkOutputDir);    final String sinkOutputDirPath = sinkOutputDir.getCanonicalPath();        /* Build props to pass to flume agent */    agentProps = new Properties();        agentProps.put("a1.channels", "c1");    agentProps.put("a1.sources", "r1");    agentProps.put("a1.sinks", "k1");        agentProps.put("a1.channels.c1.type", "memory");    agentProps.put("a1.channels.c1.capacity", "1000");    agentProps.put("a1.channels.c1.transactionCapacity", "100");        agentProps.put("a1.sources.r1.channels", "c1");    agentProps.put("a1.sources.r1.type", sourceType.toString());    agentProps.put("a1.sources.r1.host", hostname);    if (sourceType.equals(SyslogSourceType.MULTIPORTTCP)) {        agentProps.put("a1.sources.r1.ports", Integer.toString(port));    } else {        agentProps.put("a1.sources.r1.port", Integer.toString(port));    }        agentProps.put("a1.sinks.k1.channel", "c1");    agentProps.put("a1.sinks.k1.sink.directory", sinkOutputDirPath);    agentProps.put("a1.sinks.k1.type", "FILE_ROLL");    agentProps.put("a1.sinks.k1.sink.rollInterval", "0");}
1
public void start(String keepFields) throws Exception
{    this.keepFields = keepFields;        agentProps.put("a1.sources.r1.keepFields", keepFields);        sinkOutputDir.mkdir();    /* Start flume agent */    StagedInstall.getInstance().startAgent("a1", agentProps);            int numberOfAttempts = 0;    while (client == null) {        try {            client = new BufferedOutputStream(new Socket(hostname, port).getOutputStream());        } catch (IOException e) {            if (++numberOfAttempts >= DEFAULT_ATTEMPTS) {                throw new AssertionError("Could not connect to source after " + DEFAULT_ATTEMPTS + " attempts with " + DEFAULT_TIMEOUT + " ms timeout.");            }            TimeUnit.MILLISECONDS.sleep(DEFAULT_TIMEOUT);        }    }}
1
public boolean isRunning() throws Exception
{    return StagedInstall.getInstance().isRunning();}
0
public void stop() throws Exception
{    if (client != null) {        client.close();    }    client = null;    StagedInstall.getInstance().stopAgent();    for (File tempResource : tempResources) {                FileUtils.deleteDirectory(tempResource);    }}
0
public void runKeepFieldsTest() throws Exception
{    /* Create expected output and log message */    String logMessage = "<34>1 Oct 11 22:14:15 mymachine su: Test\n";    String expectedOutput = "su: Test\n";    if (keepFields.equals("true") || keepFields.equals("all")) {        expectedOutput = logMessage;    } else if (!keepFields.equals("false") && !keepFields.equals("none")) {        if (keepFields.indexOf("hostname") != -1) {            expectedOutput = "mymachine " + expectedOutput;        }        if (keepFields.indexOf("timestamp") != -1) {            expectedOutput = "Oct 11 22:14:15 " + expectedOutput;        }        if (keepFields.indexOf("version") != -1) {            expectedOutput = "1 " + expectedOutput;        }        if (keepFields.indexOf("priority") != -1) {            expectedOutput = "<34>" + expectedOutput;        }    }        /* Send test message to agent */    sendMessage(logMessage);    /* Wait for output file */    int numberOfListDirAttempts = 0;    while (sinkOutputDir.listFiles().length == 0) {        if (++numberOfListDirAttempts >= DEFAULT_ATTEMPTS) {            throw new AssertionError("FILE_ROLL sink hasn't written any files after " + DEFAULT_ATTEMPTS + " attempts with " + DEFAULT_TIMEOUT + " ms timeout.");        }        TimeUnit.MILLISECONDS.sleep(DEFAULT_TIMEOUT);    }        File[] sinkOutputDirChildren = sinkOutputDir.listFiles();    Assert.assertEquals("Expected FILE_ROLL sink's dir to have only 1 child," + " but found " + sinkOutputDirChildren.length + " children.", 1, sinkOutputDirChildren.length);    /* Wait for output file stats to be as expected. */    File outputDirChild = sinkOutputDirChildren[0];    int numberOfStatsAttempts = 0;    while (outputDirChild.length() != expectedOutput.length()) {        if (++numberOfStatsAttempts >= DEFAULT_ATTEMPTS) {            throw new AssertionError("Expected output and FILE_ROLL sink's" + " lengths did not match after " + DEFAULT_ATTEMPTS + " attempts with " + DEFAULT_TIMEOUT + " ms timeout.");        }        TimeUnit.MILLISECONDS.sleep(DEFAULT_TIMEOUT);    }    File actualOutput = sinkOutputDirChildren[0];    if (!Files.toString(actualOutput, Charsets.UTF_8).equals(expectedOutput)) {                        throw new AssertionError("FILE_ROLL sink's actual output doesn't " + "match expected output.");    }}
1
private void sendMessage(String message) throws IOException
{    client.write(message.getBytes());    client.flush();}
0
public static void addGlobalSSLParameters(Properties kafkaProps)
{    if (isSSLEnabled(kafkaProps)) {        addGlobalSSLParameter(kafkaProps, SslConfigs.SSL_KEYSTORE_LOCATION_CONFIG, SSLUtil.getGlobalKeystorePath());        addGlobalSSLParameter(kafkaProps, SslConfigs.SSL_KEYSTORE_PASSWORD_CONFIG, SSLUtil.getGlobalKeystorePassword());        addGlobalSSLParameter(kafkaProps, SslConfigs.SSL_KEYSTORE_TYPE_CONFIG, SSLUtil.getGlobalKeystoreType(null));        addGlobalSSLParameter(kafkaProps, SslConfigs.SSL_TRUSTSTORE_LOCATION_CONFIG, SSLUtil.getGlobalTruststorePath());        addGlobalSSLParameter(kafkaProps, SslConfigs.SSL_TRUSTSTORE_PASSWORD_CONFIG, SSLUtil.getGlobalTruststorePassword());        addGlobalSSLParameter(kafkaProps, SslConfigs.SSL_TRUSTSTORE_TYPE_CONFIG, SSLUtil.getGlobalTruststoreType(null));    }}
0
private static void addGlobalSSLParameter(Properties kafkaProps, String propName, String globalValue)
{    if (!kafkaProps.containsKey(propName) && globalValue != null) {        kafkaProps.put(propName, globalValue);    }}
0
private static boolean isSSLEnabled(Properties kafkaProps)
{    String securityProtocol = kafkaProps.getProperty(CommonClientConfigs.SECURITY_PROTOCOL_CONFIG);    return securityProtocol != null && (securityProtocol.equals(SecurityProtocol.SSL.name) || securityProtocol.equals(SecurityProtocol.SASL_SSL.name));}
0
public void initSystemProperties()
{    System.setProperty("javax.net.ssl.keyStore", "global-keystore-path");    System.setProperty("javax.net.ssl.keyStorePassword", "global-keystore-password");    System.setProperty("javax.net.ssl.keyStoreType", "global-keystore-type");    System.setProperty("javax.net.ssl.trustStore", "global-truststore-path");    System.setProperty("javax.net.ssl.trustStorePassword", "global-truststore-password");    System.setProperty("javax.net.ssl.trustStoreType", "global-truststore-type");}
0
public void clearSystemProperties()
{    System.clearProperty("javax.net.ssl.keyStore");    System.clearProperty("javax.net.ssl.keyStorePassword");    System.clearProperty("javax.net.ssl.keyStoreType");    System.clearProperty("javax.net.ssl.trustStore");    System.clearProperty("javax.net.ssl.trustStorePassword");    System.clearProperty("javax.net.ssl.trustStoreType");}
0
public void testSecurityProtocol_PLAINTEXT()
{    Properties kafkaProps = new Properties();    kafkaProps.put(CommonClientConfigs.SECURITY_PROTOCOL_CONFIG, SecurityProtocol.PLAINTEXT.name);    KafkaSSLUtil.addGlobalSSLParameters(kafkaProps);    assertNoSSLParameters(kafkaProps);}
0
public void testSecurityProtocol_SASL_PLAINTEXT()
{    Properties kafkaProps = new Properties();    kafkaProps.put(CommonClientConfigs.SECURITY_PROTOCOL_CONFIG, SecurityProtocol.SASL_PLAINTEXT.name);    KafkaSSLUtil.addGlobalSSLParameters(kafkaProps);    assertNoSSLParameters(kafkaProps);}
0
public void testSecurityProtocol_SSL()
{    Properties kafkaProps = new Properties();    kafkaProps.put(CommonClientConfigs.SECURITY_PROTOCOL_CONFIG, SecurityProtocol.SSL.name);    KafkaSSLUtil.addGlobalSSLParameters(kafkaProps);    assertGlobalSSLParameters(kafkaProps);}
0
public void testSecurityProtocol_SASL_SSL()
{    Properties kafkaProps = new Properties();    kafkaProps.put(CommonClientConfigs.SECURITY_PROTOCOL_CONFIG, SecurityProtocol.SASL_SSL.name);    KafkaSSLUtil.addGlobalSSLParameters(kafkaProps);    assertGlobalSSLParameters(kafkaProps);}
0
public void testComponentParametersNotOverridden()
{    Properties kafkaProps = new Properties();    kafkaProps.put(CommonClientConfigs.SECURITY_PROTOCOL_CONFIG, SecurityProtocol.SSL.name);    kafkaProps.put(SslConfigs.SSL_KEYSTORE_LOCATION_CONFIG, "component-keystore-path");    kafkaProps.put(SslConfigs.SSL_KEYSTORE_PASSWORD_CONFIG, "component-keystore-password");    kafkaProps.put(SslConfigs.SSL_KEYSTORE_TYPE_CONFIG, "component-keystore-type");    kafkaProps.put(SslConfigs.SSL_TRUSTSTORE_LOCATION_CONFIG, "component-truststore-path");    kafkaProps.put(SslConfigs.SSL_TRUSTSTORE_PASSWORD_CONFIG, "component-truststore-password");    kafkaProps.put(SslConfigs.SSL_TRUSTSTORE_TYPE_CONFIG, "component-truststore-type");    KafkaSSLUtil.addGlobalSSLParameters(kafkaProps);    assertComponentSSLParameters(kafkaProps);}
0
public void testEmptyGlobalParametersNotAdded()
{    Properties kafkaProps = new Properties();    kafkaProps.put(CommonClientConfigs.SECURITY_PROTOCOL_CONFIG, SecurityProtocol.SSL.name);    clearSystemProperties();    KafkaSSLUtil.addGlobalSSLParameters(kafkaProps);    assertNoSSLParameters(kafkaProps);}
0
private void assertNoSSLParameters(Properties kafkaProps)
{    assertFalse(kafkaProps.containsKey(SslConfigs.SSL_KEYSTORE_LOCATION_CONFIG));    assertFalse(kafkaProps.containsKey(SslConfigs.SSL_KEYSTORE_PASSWORD_CONFIG));    assertFalse(kafkaProps.containsKey(SslConfigs.SSL_KEYSTORE_TYPE_CONFIG));    assertFalse(kafkaProps.containsKey(SslConfigs.SSL_TRUSTSTORE_LOCATION_CONFIG));    assertFalse(kafkaProps.containsKey(SslConfigs.SSL_TRUSTSTORE_PASSWORD_CONFIG));    assertFalse(kafkaProps.containsKey(SslConfigs.SSL_TRUSTSTORE_TYPE_CONFIG));}
0
private void assertGlobalSSLParameters(Properties kafkaProps)
{    assertEquals("global-keystore-path", kafkaProps.getProperty(SslConfigs.SSL_KEYSTORE_LOCATION_CONFIG));    assertEquals("global-keystore-password", kafkaProps.getProperty(SslConfigs.SSL_KEYSTORE_PASSWORD_CONFIG));    assertEquals("global-keystore-type", kafkaProps.getProperty(SslConfigs.SSL_KEYSTORE_TYPE_CONFIG));    assertEquals("global-truststore-path", kafkaProps.getProperty(SslConfigs.SSL_TRUSTSTORE_LOCATION_CONFIG));    assertEquals("global-truststore-password", kafkaProps.getProperty(SslConfigs.SSL_TRUSTSTORE_PASSWORD_CONFIG));    assertEquals("global-truststore-type", kafkaProps.getProperty(SslConfigs.SSL_TRUSTSTORE_TYPE_CONFIG));}
0
private void assertComponentSSLParameters(Properties kafkaProps)
{    assertEquals("component-keystore-path", kafkaProps.getProperty(SslConfigs.SSL_KEYSTORE_LOCATION_CONFIG));    assertEquals("component-keystore-password", kafkaProps.getProperty(SslConfigs.SSL_KEYSTORE_PASSWORD_CONFIG));    assertEquals("component-keystore-type", kafkaProps.getProperty(SslConfigs.SSL_KEYSTORE_TYPE_CONFIG));    assertEquals("component-truststore-path", kafkaProps.getProperty(SslConfigs.SSL_TRUSTSTORE_LOCATION_CONFIG));    assertEquals("component-truststore-password", kafkaProps.getProperty(SslConfigs.SSL_TRUSTSTORE_PASSWORD_CONFIG));    assertEquals("component-truststore-type", kafkaProps.getProperty(SslConfigs.SSL_TRUSTSTORE_TYPE_CONFIG));}
0
public static void checkResultsAgainstSkew(PartitionTestScenario scenario, Map<Integer, List<Event>> partitionMap, Map<Integer, List<byte[]>> resultsMap, int staticPtn, int numMsgs)
{    int numPtns = partitionMap.size();    if (scenario == PartitionTestScenario.NO_PARTITION_HEADERS && numMsgs % numPtns != 0) {        throw new IllegalArgumentException("This method is not designed to work with scenarios" + " where there is expected to be a non-even distribution of messages");    }    for (int ptn = 0; ptn < numPtns; ptn++) {        List<Event> expectedResults = partitionMap.get(ptn);        List<byte[]> actualResults = resultsMap.get(ptn);        if (scenario == PartitionTestScenario.PARTITION_ID_HEADER_ONLY || scenario == PartitionTestScenario.STATIC_HEADER_AND_PARTITION_ID) {                        Assert.assertEquals(expectedResults.size(), actualResults.size());                        for (int idx = 0; idx < expectedResults.size(); idx++) {                Assert.assertArrayEquals(expectedResults.get(idx).getBody(), actualResults.get(idx));            }        } else if (scenario == PartitionTestScenario.STATIC_HEADER_ONLY) {                        if (ptn == staticPtn) {                Assert.assertEquals(numMsgs, actualResults.size());            } else {                Assert.assertEquals(0, actualResults.size());            }        } else if (scenario == PartitionTestScenario.NO_PARTITION_HEADERS) {                        Assert.assertEquals(numMsgs / numPtns, actualResults.size());        }    }}
0
public static List<Event> generateSkewedMessageList(PartitionTestScenario scenario, int numMsgs, Map<Integer, List<Event>> partitionMap, int numPtns, int staticPtn)
{    List<Event> msgs = new ArrayList<Event>(numMsgs);        if (numMsgs < 0) {        throw new IllegalArgumentException("Number of messages must be greater than zero");    }    if (staticPtn >= numPtns) {        throw new IllegalArgumentException("The static partition must be less than the " + "number of partitions");    }    if (numPtns < 5) {        throw new IllegalArgumentException("This method is designed to work with at least 5 " + "partitions");    }    if (partitionMap.size() != numPtns) {        throw new IllegalArgumentException("partitionMap has not been correctly initialised");    }    for (int i = 0; i < numMsgs; i++) {        Map<String, String> headers = new HashMap<String, String>();        Integer partition = null;        if (scenario == PartitionTestScenario.NO_PARTITION_HEADERS) {                } else if (scenario == PartitionTestScenario.STATIC_HEADER_ONLY) {            partition = staticPtn;        } else {                        if (i % 5 == 0) {                partition = 4;                headers.put(PARTITION_HEADER, String.valueOf(partition));            } else if (i % 3 == 0) {                partition = 3;                headers.put(PARTITION_HEADER, String.valueOf(partition));            } else if (scenario == PartitionTestScenario.STATIC_HEADER_AND_PARTITION_ID) {                                                partition = staticPtn;            } else if (scenario == PartitionTestScenario.PARTITION_ID_HEADER_ONLY) {                partition = 2;                headers.put(PARTITION_HEADER, String.valueOf(partition));            }                }                Event event = EventBuilder.withBody(String.valueOf(i).getBytes(), headers);        if (scenario != PartitionTestScenario.NO_PARTITION_HEADERS) {                        partitionMap.get(partition).add(event);        }                msgs.add(event);    }    return msgs;}
0
public static Map<Integer, List<byte[]>> retrieveRecordsFromPartitions(String topic, int numPtns, Properties consumerProperties)
{    Map<Integer, List<byte[]>> resultsMap = new HashMap<Integer, List<byte[]>>();    for (int i = 0; i < numPtns; i++) {        List<byte[]> partitionResults = new ArrayList<byte[]>();        resultsMap.put(i, partitionResults);        KafkaConsumer<String, byte[]> consumer = new KafkaConsumer<String, byte[]>(consumerProperties);        TopicPartition partition = new TopicPartition(topic, i);        consumer.assign(Arrays.asList(partition));        ConsumerRecords<String, byte[]> records = consumer.poll(1000);        for (ConsumerRecord<String, byte[]> record : records) {            partitionResults.add(record.value());        }        consumer.close();    }    return resultsMap;}
0
public boolean validateEvent(Event event)
{    return true;}
0
public void run(String[] args) throws IOException, ParseException
{    boolean shouldContinue = parseCommandLineOpts(args);    if (!shouldContinue) {                System.exit(1);    }    for (File dataDir : dataDirs) {        File[] dataFiles = dataDir.listFiles(new FilenameFilter() {            @Override            public boolean accept(File dir, String name) {                if (!name.endsWith(Serialization.METADATA_FILENAME) && !name.endsWith(Serialization.METADATA_TMP_FILENAME) && !name.endsWith(Serialization.OLD_METADATA_FILENAME) && !name.equals(Log.FILE_LOCK)) {                    return true;                }                return false;            }        });        if (dataFiles != null && dataFiles.length > 0) {            for (File dataFile : dataFiles) {                                LogFile.SequentialReader reader = new LogFileV3.SequentialReader(dataFile, null, true);                LogFile.OperationRecordUpdater updater = new LogFile.OperationRecordUpdater(dataFile);                boolean fileDone = false;                boolean fileBackedup = false;                while (!fileDone) {                    long eventPosition = 0;                    try {                                                                        eventPosition = reader.getPosition();                                                                                                LogRecord record = reader.next();                        totalChannelEvents++;                        if (record != null) {                            TransactionEventRecord recordEvent = record.getEvent();                            Event event = EventUtils.getEventFromTransactionEvent(recordEvent);                            if (event != null) {                                totalPutEvents++;                                try {                                    if (!eventValidator.validateEvent(event)) {                                        if (!fileBackedup) {                                            Serialization.copyFile(dataFile, new File(dataFile.getParent(), dataFile.getName() + ".bak"));                                            fileBackedup = true;                                        }                                        invalidEvents++;                                        updater.markRecordAsNoop(eventPosition);                                    } else {                                        validEvents++;                                    }                                } catch (Exception e) {                                                                                                                                                System.err.println("Encountered Exception while validating event, " + "marking as invalid");                                    updater.markRecordAsNoop(eventPosition);                                    eventsWithException++;                                }                            }                        } else {                            fileDone = true;                        }                    } catch (CorruptEventException e) {                        corruptEvents++;                        totalChannelEvents++;                                                if (!fileBackedup) {                            Serialization.copyFile(dataFile, new File(dataFile.getParent(), dataFile.getName() + ".bak"));                            fileBackedup = true;                        }                        updater.markRecordAsNoop(eventPosition);                    }                }                updater.close();                reader.close();            }        }    }    printSummary();}
1
public boolean accept(File dir, String name)
{    if (!name.endsWith(Serialization.METADATA_FILENAME) && !name.endsWith(Serialization.METADATA_TMP_FILENAME) && !name.endsWith(Serialization.OLD_METADATA_FILENAME) && !name.equals(Log.FILE_LOCK)) {        return true;    }    return false;}
0
private boolean parseCommandLineOpts(String[] args) throws ParseException
{    Options options = new Options();    options.addOption("l", "dataDirs", true, "Comma-separated list of data " + "directories which the tool must verify. This option is mandatory").addOption("h", "help", false, "Display help").addOption("e", "eventValidator", true, "Fully Qualified Name of Event Validator Implementation");    Option property = OptionBuilder.withArgName("property=value").hasArgs(2).withValueSeparator().withDescription("custom properties").create("D");    options.addOption(property);    CommandLineParser parser = new GnuParser();    CommandLine commandLine = parser.parse(options, args);    if (commandLine.hasOption("help")) {        new HelpFormatter().printHelp("bin/flume-ng tool fcintegritytool ", options, true);        return false;    }    if (!commandLine.hasOption("dataDirs")) {        new HelpFormatter().printHelp("bin/flume-ng tool fcintegritytool ", "", options, "dataDirs is required.", true);        return false;    } else {        String[] dataDirStr = commandLine.getOptionValue("dataDirs").split(",");        for (String dataDir : dataDirStr) {            File f = new File(dataDir);            if (!f.exists()) {                throw new FlumeException("Data directory, " + dataDir + " does not exist.");            }            dataDirs.add(f);        }    }    if (commandLine.hasOption("eventValidator")) {        try {            Class<? extends EventValidator.Builder> eventValidatorClassName = (Class<? extends EventValidator.Builder>) Class.forName(commandLine.getOptionValue("eventValidator"));            EventValidator.Builder eventValidatorBuilder = eventValidatorClassName.newInstance();                        Properties systemProperties = commandLine.getOptionProperties("D");            Context context = new Context();            Set<String> keys = systemProperties.stringPropertyNames();            for (String key : keys) {                context.put(key, systemProperties.getProperty(key));            }            eventValidatorBuilder.configure(context);            eventValidator = eventValidatorBuilder.build();        } catch (Exception e) {            System.err.println(String.format("Could find class %s in lib folder", commandLine.getOptionValue("eventValidator")));            e.printStackTrace();            return false;        }    }    return true;}
0
private void printSummary()
{    System.out.println("---------- Summary --------------------");    System.out.println("Number of Events in the Channel = " + totalChannelEvents++);    System.out.println("Number of Put Events Processed = " + totalPutEvents);    System.out.println("Number of Valid Put Events = " + validEvents);    System.out.println("Number of Invalid Put Events = " + invalidEvents);    System.out.println("Number of Put Events that threw Exception during validation = " + eventsWithException);    System.out.println("Number of Corrupt Events = " + corruptEvents);    System.out.println("---------------------------------------");}
0
public static void main(String[] args) throws Exception
{    new FlumeToolsMain().run(args);}
0
public void run(String[] args) throws Exception
{    String error = "Expected name of tool and arguments for" + " tool to be passed in on the command line. Please pass one of the " + "following as arguments to this command: \n";    StringBuilder builder = new StringBuilder(error);    for (FlumeToolType type : FlumeToolType.values()) {        builder.append(type.name()).append("\n");    }    if (args == null || args.length == 0) {        System.out.println(builder.toString());        System.exit(1);    }    String toolName = args[0];    FlumeTool tool = null;    for (FlumeToolType type : FlumeToolType.values()) {        if (toolName.equalsIgnoreCase(type.name())) {            tool = type.getClassInstance().newInstance();            break;        }    }    Preconditions.checkNotNull(tool, "Cannot find tool matching " + toolName + ". Please select one of: \n " + FlumeToolType.getNames());    if (args.length == 1) {        tool.run(new String[0]);    } else {        tool.run(Arrays.asList(args).subList(1, args.length).toArray(new String[0]));    }}
0
public Class<? extends FlumeTool> getClassInstance()
{    return this.klass;}
0
public static String getNames()
{    StringBuilder builder = new StringBuilder();    for (FlumeToolType type : values()) {        builder.append(type.name().toLowerCase(Locale.ENGLISH) + "\n");    }    return builder.toString();}
0
public static void setUpClass() throws Exception
{    createDataFiles();}
0
public void setUp() throws Exception
{    checkpointDir = new File(baseDir, "checkpoint");    dataDir = new File(baseDir, "dataDir");    Assert.assertTrue(checkpointDir.mkdirs() || checkpointDir.isDirectory());    Assert.assertTrue(dataDir.mkdirs() || dataDir.isDirectory());    File[] dataFiles = origDataDir.listFiles(new FilenameFilter() {        @Override        public boolean accept(File dir, String name) {            if (name.contains("lock")) {                return false;            }            return true;        }    });    for (File dataFile : dataFiles) {        Serialization.copyFile(dataFile, new File(dataDir, dataFile.getName()));    }}
0
public boolean accept(File dir, String name)
{    if (name.contains("lock")) {        return false;    }    return true;}
0
public void tearDown() throws Exception
{    FileUtils.deleteDirectory(checkpointDir);    FileUtils.deleteDirectory(dataDir);}
0
public static void tearDownClass() throws Exception
{    FileUtils.deleteDirectory(origCheckpointDir);    FileUtils.deleteDirectory(origDataDir);}
0
public void testFixCorruptRecordsWithCheckpoint() throws Exception
{    doTestFixCorruptEvents(true);}
0
public void testFixCorruptRecords() throws Exception
{    doTestFixCorruptEvents(false);}
0
public void testFixInvalidRecords() throws Exception
{    doTestFixInvalidEvents(false, DummyEventVerifier.Builder.class.getName());}
0
public void testFixInvalidRecordsWithCheckpoint() throws Exception
{    doTestFixInvalidEvents(true, DummyEventVerifier.Builder.class.getName());}
0
public void doTestFixInvalidEvents(boolean withCheckpoint, String eventHandler) throws Exception
{    FileChannelIntegrityTool tool = new FileChannelIntegrityTool();    tool.run(new String[] { "-l", dataDir.toString(), "-e", eventHandler, "-DvalidatorValue=0" });    FileChannel channel = new FileChannel();    channel.setName("channel");    if (withCheckpoint) {        File[] cpFiles = origCheckpointDir.listFiles(new FilenameFilter() {            @Override            public boolean accept(File dir, String name) {                if (name.contains("lock") || name.contains("queueset")) {                    return false;                }                return true;            }        });        for (File cpFile : cpFiles) {            Serialization.copyFile(cpFile, new File(checkpointDir, cpFile.getName()));        }    } else {        FileUtils.deleteDirectory(checkpointDir);        Assert.assertTrue(checkpointDir.mkdirs());    }    ctx.put(FileChannelConfiguration.CHECKPOINT_DIR, checkpointDir.toString());    ctx.put(FileChannelConfiguration.DATA_DIRS, dataDir.toString());    channel.configure(ctx);    channel.start();    Transaction tx = channel.getTransaction();    tx.begin();    int i = 0;    while (channel.take() != null) {        i++;    }    tx.commit();    tx.close();    channel.stop();    Assert.assertTrue(invalidEvent != 0);    Assert.assertEquals(25 - invalidEvent, i);}
0
public boolean accept(File dir, String name)
{    if (name.contains("lock") || name.contains("queueset")) {        return false;    }    return true;}
0
public void doTestFixCorruptEvents(boolean withCheckpoint) throws Exception
{    Set<String> corruptFiles = new HashSet<String>();    File[] files = dataDir.listFiles(new FilenameFilter() {        @Override        public boolean accept(File dir, String name) {            if (name.contains("lock") || name.contains("meta")) {                return false;            }            return true;        }    });    Random random = new Random();    int corrupted = 0;    for (File dataFile : files) {        LogFile.SequentialReader reader = new LogFileV3.SequentialReader(dataFile, null, true);        RandomAccessFile handle = new RandomAccessFile(dataFile, "rw");        long eventPosition1 = reader.getPosition();        LogRecord rec = reader.next();                if (rec == null || rec.getEvent().getClass().getName().equals("org.apache.flume.channel.file.Commit")) {            handle.close();            reader.close();            continue;        }        long eventPosition2 = reader.getPosition();        rec = reader.next();        handle.seek(eventPosition1 + 100);        handle.writeInt(random.nextInt());        corrupted++;        corruptFiles.add(dataFile.getName());        if (rec == null || rec.getEvent().getClass().getName().equals("org.apache.flume.channel.file.Commit")) {            handle.close();            reader.close();            continue;        }        handle.seek(eventPosition2 + 100);        handle.writeInt(random.nextInt());        corrupted++;        handle.close();        reader.close();    }    FileChannelIntegrityTool tool = new FileChannelIntegrityTool();    tool.run(new String[] { "-l", dataDir.toString() });    FileChannel channel = new FileChannel();    channel.setName("channel");    if (withCheckpoint) {        File[] cpFiles = origCheckpointDir.listFiles(new FilenameFilter() {            @Override            public boolean accept(File dir, String name) {                if (name.contains("lock") || name.contains("queueset")) {                    return false;                }                return true;            }        });        for (File cpFile : cpFiles) {            Serialization.copyFile(cpFile, new File(checkpointDir, cpFile.getName()));        }    } else {        FileUtils.deleteDirectory(checkpointDir);        Assert.assertTrue(checkpointDir.mkdirs());    }    ctx.put(FileChannelConfiguration.CHECKPOINT_DIR, checkpointDir.toString());    ctx.put(FileChannelConfiguration.DATA_DIRS, dataDir.toString());    channel.configure(ctx);    channel.start();    Transaction tx = channel.getTransaction();    tx.begin();    int i = 0;    while (channel.take() != null) {        i++;    }    tx.commit();    tx.close();    channel.stop();    Assert.assertEquals(25 - corrupted, i);    files = dataDir.listFiles(new FilenameFilter() {        @Override        public boolean accept(File dir, String name) {            if (name.contains(".bak")) {                return true;            }            return false;        }    });    Assert.assertEquals(corruptFiles.size(), files.length);    for (File file : files) {        String name = file.getName();        name = name.replaceAll(".bak", "");        Assert.assertTrue(corruptFiles.remove(name));    }    Assert.assertTrue(corruptFiles.isEmpty());}
0
public boolean accept(File dir, String name)
{    if (name.contains("lock") || name.contains("meta")) {        return false;    }    return true;}
0
public boolean accept(File dir, String name)
{    if (name.contains("lock") || name.contains("queueset")) {        return false;    }    return true;}
0
public boolean accept(File dir, String name)
{    if (name.contains(".bak")) {        return true;    }    return false;}
0
private static void createDataFiles() throws Exception
{    final byte[] eventData = new byte[2000];    for (int i = 0; i < 2000; i++) {        eventData[i] = 1;    }    WriteOrderOracle.setSeed(System.currentTimeMillis());    event = EventBuilder.withBody(eventData);    baseDir = Files.createTempDir();    if (baseDir.exists()) {        FileUtils.deleteDirectory(baseDir);    }    baseDir = Files.createTempDir();    origCheckpointDir = new File(baseDir, "chkpt");    Assert.assertTrue(origCheckpointDir.mkdirs() || origCheckpointDir.isDirectory());    origDataDir = new File(baseDir, "data");    Assert.assertTrue(origDataDir.mkdirs() || origDataDir.isDirectory());    FileChannel channel = new FileChannel();    channel.setName("channel");    ctx = new Context();    ctx.put(FileChannelConfiguration.CAPACITY, "1000");    ctx.put(FileChannelConfiguration.CHECKPOINT_DIR, origCheckpointDir.toString());    ctx.put(FileChannelConfiguration.DATA_DIRS, origDataDir.toString());    ctx.put(FileChannelConfiguration.MAX_FILE_SIZE, "10000");    ctx.put(FileChannelConfiguration.TRANSACTION_CAPACITY, "100");    channel.configure(ctx);    channel.start();    for (int j = 0; j < 5; j++) {        Transaction tx = channel.getTransaction();        tx.begin();        for (int i = 0; i < 5; i++) {            if (i % 3 == 0) {                event.getBody()[0] = 0;                invalidEvent++;            } else {                event.getBody()[0] = 1;            }            channel.put(event);        }        tx.commit();        tx.close();    }    Log log = field("log").ofType(Log.class).in(channel).get();    Assert.assertTrue("writeCheckpoint returned false", method("writeCheckpoint").withReturnType(Boolean.class).withParameterTypes(Boolean.class).in(log).invoke(true));    channel.stop();}
0
public boolean validateEvent(Event event)
{    return event.getBody()[0] != value;}
0
public EventValidator build()
{    return new DummyEventVerifier(binaryValidator);}
0
public void configure(Context context)
{    binaryValidator = context.getInteger("validatorValue");}
0
