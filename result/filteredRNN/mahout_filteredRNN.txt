public T value()
{    if (obj == null) {        obj = deserialize(buf);    }    return obj;}
0
private byte[] serialize(Writable w)
{    ByteArrayOutputStream bos = new ByteArrayOutputStream();    try {        ObjectOutputStream oos = new ObjectOutputStream(bos);        w.write(oos);        oos.close();    } catch (IOException e) {        return null;    }    return bos.toByteArray();}
0
private T deserialize(byte[] buf)
{    T ret = null;    try (ByteArrayInputStream bis = new ByteArrayInputStream(buf)) {        ObjectInputStream ois = new ObjectInputStream(bis);        if (isMatrix) {            MatrixWritable w = new MatrixWritable();            w.readFields(ois);            ret = (T) w.get();        } else {            VectorWritable w = new VectorWritable();            w.readFields(ois);            ret = (T) w.get();        }    } catch (IOException e) {        e.printStackTrace();    }    return ret;}
0
public void close() throws IOException
{}
0
private void cow()
{    if (cow != null) {        return;    }    if (chks[0].isSparse()) {        cow = new SparseMatrix(chks[0].len(), chks.length);    } else {        cow = new DenseMatrix(chks[0].len(), chks.length);    }    for (int c = 0; c < chks.length; c++) {        for (int r = 0; r < chks[0].len(); r++) {            cow.setQuick(r, c, chks[c].atd(r));        }    }}
0
public void setQuick(int row, int col, double val)
{    cow();    cow.setQuick(row, col, val);}
0
public Matrix like(int nrow, int ncol)
{    if (chks[0].isSparse()) {        return new SparseMatrix(nrow, ncol);    } else {        return new DenseMatrix(nrow, ncol);    }}
0
public Matrix like()
{    if (chks[0].isSparse()) {        return new SparseMatrix(rowSize(), columnSize());    } else {        return new DenseMatrix(rowSize(), columnSize());    }}
0
public double getQuick(int row, int col)
{    if (cow != null) {        return cow.getQuick(row, col);    } else {        return chks[col].atd(row);    }}
0
public Matrix assignRow(int row, Vector v)
{    cow();    cow.assignRow(row, v);    return cow;}
0
public Matrix assignColumn(int col, Vector v)
{    cow();    cow.assignColumn(col, v);    return cow;}
0
public MatrixFlavor getFlavor()
{    if (cow != null) {        return cow.getFlavor();    } else if (chks[0].isSparse()) {        return MatrixFlavor.SPARSELIKE;    } else {        return MatrixFlavor.DENSELIKE;    }}
0
public static boolean isSeqfile(String filename)
{    try {        Configuration conf = new Configuration();        Path path = new Path(filename);        FileSystem fs = FileSystem.get(URI.create(filename), conf);        FSDataInputStream fin = fs.open(path);        byte[] seq = new byte[3];        fin.read(seq);        fin.close();        return seq[0] == 'S' && seq[1] == 'E' && seq[2] == 'Q';    } catch (IOException e) {        return false;    }}
0
public static H2ODrm drmFromFile(String filename, int parMin)
{    try {        if (isSeqfile(filename)) {            return drmFromSeqfile(filename, parMin);        } else {            return new H2ODrm(FrameUtils.parseFrame(null, new File(filename)));        }    } catch (IOException e) {        return null;    }}
0
public static H2ODrm drmFromSeqfile(String filename, int parMin)
{    long rows = 0;    int cols = 0;    Frame frame = null;    Vec labels = null;    SequenceFile.Reader reader = null;    try {        Configuration conf = new Configuration();        Path path = new Path(filename);        FileSystem fs = FileSystem.get(URI.create(filename), conf);        Vec.Writer[] writers;        Vec.Writer labelwriter = null;        boolean isIntKey = false, isLongKey = false, isStringKey = false;        reader = new SequenceFile.Reader(fs, path, conf);        if (reader.getValueClass() != VectorWritable.class) {            System.out.println("ValueClass in file " + filename + "must be VectorWritable, but found " + reader.getValueClassName());            return null;        }        Writable key = (Writable) ReflectionUtils.newInstance(reader.getKeyClass(), conf);        VectorWritable value = (VectorWritable) ReflectionUtils.newInstance(reader.getValueClass(), conf);        long start = reader.getPosition();        if (reader.getKeyClass() == Text.class) {            isStringKey = true;        } else if (reader.getKeyClass() == LongWritable.class) {            isLongKey = true;        } else {            isIntKey = true;        }        while (reader.next(key, value)) {            if (cols == 0) {                Vector v = value.get();                cols = Math.max(v.size(), cols);            }            if (isLongKey) {                rows = Math.max(((LongWritable) (key)).get() + 1, rows);            }            if (isIntKey) {                rows = Math.max(((IntWritable) (key)).get() + 1, rows);            }            if (isStringKey) {                rows++;            }        }        reader.seek(start);        frame = H2OHelper.emptyFrame(rows, cols, parMin, -1);        writers = new Vec.Writer[cols];        for (int i = 0; i < writers.length; i++) {            writers[i] = frame.vecs()[i].open();        }        if (reader.getKeyClass() == Text.class) {            labels = H2OHelper.makeEmptyStrVec(frame.anyVec());            labelwriter = labels.open();        }        long r = 0;        while (reader.next(key, value)) {            Vector v = value.get();            if (isLongKey) {                r = ((LongWritable) (key)).get();            }            if (isIntKey) {                r = ((IntWritable) (key)).get();            }            for (int c = 0; c < v.size(); c++) {                writers[c].set(r, v.getQuick(c));            }            if (labels != null) {                labelwriter.set(r, (key).toString());            }            if (isStringKey) {                r++;            }        }        Futures fus = new Futures();        for (Vec.Writer w : writers) {            w.close(fus);        }        if (labelwriter != null) {            labelwriter.close(fus);        }        fus.blockForPending();    } catch (java.io.IOException e) {        return null;    } finally {        IOUtils.closeStream(reader);    }    return new H2ODrm(frame, labels);}
0
public static void drmToFile(String filename, H2ODrm drm) throws java.io.IOException
{    Frame frame = drm.frame;    Vec labels = drm.keys;    Configuration conf = new Configuration();    Path path = new Path(filename);    FileSystem fs = FileSystem.get(URI.create(filename), conf);    SequenceFile.Writer writer;    boolean isSparse = H2OHelper.isSparse(frame);    ValueString vstr = new ValueString();    if (labels != null) {        writer = SequenceFile.createWriter(fs, conf, path, Text.class, VectorWritable.class);    } else {        writer = SequenceFile.createWriter(fs, conf, path, IntWritable.class, VectorWritable.class);    }    for (long r = 0; r < frame.anyVec().length(); r++) {        Vector v;        if (isSparse) {            v = new SequentialAccessSparseVector(frame.numCols());        } else {            v = new DenseVector(frame.numCols());        }        for (int c = 0; c < frame.numCols(); c++) {            v.setQuick(c, frame.vecs()[c].at(r));        }        if (labels != null) {            writer.append(new Text(labels.atStr(vstr, r).toString()), new VectorWritable(v));        } else {            writer.append(new IntWritable((int) r), new VectorWritable(v));        }    }    writer.close();}
0
public static boolean isSparse(Frame frame)
{    long rows = frame.numRows();    long cols = frame.numCols();    /**     * MRTask to aggregate precalculated per-chunk sparse lengths     */    class MRTaskNZ extends MRTask<MRTaskNZ> {        long sparselen;        @Override        public void map(Chunk[] chks) {            for (Chunk chk : chks) {                sparselen += chk.sparseLen();            }        }        @Override        public void reduce(MRTaskNZ other) {            sparselen += other.sparselen;        }    }    long sparselen = new MRTaskNZ().doAll(frame).sparselen;    return (((rows * cols) / (sparselen + 1)) > 32);}
0
public void map(Chunk[] chks)
{    for (Chunk chk : chks) {        sparselen += chk.sparseLen();    }}
0
public void reduce(MRTaskNZ other)
{    sparselen += other.sparselen;}
0
public static Matrix matrixFromDrm(H2ODrm drm)
{    Frame frame = drm.frame;    Vec labels = drm.keys;    Matrix m;    if (isSparse(frame)) {        m = new SparseMatrix((int) frame.numRows(), frame.numCols());    } else {        m = new DenseMatrix((int) frame.numRows(), frame.numCols());    }    int c = 0;        for (Vec v : frame.vecs()) {        for (int r = 0; r < frame.numRows(); r++) {            double d;            if (!v.isNA(r) && ((d = v.at(r)) != 0.0)) {                m.setQuick(r, c, d);            }        }        c++;    }        if (labels != null) {        Map<String, Integer> map = new HashMap<>();        ValueString vstr = new ValueString();        for (long i = 0; i < labels.length(); i++) {            map.put(labels.atStr(vstr, i).toString(), (int) i);        }        m.setRowLabelBindings(map);    }    return m;}
0
public static Vector colMeans(Frame frame)
{    double[] means = new double[frame.numCols()];    for (int i = 0; i < frame.numCols(); i++) {        means[i] = frame.vecs()[i].mean();    }    return new DenseVector(means);}
0
public static Vector colSums(Frame frame)
{    /**     * MRTask to calculate sums of elements in all columns.     */    class MRTaskSum extends MRTask<MRTaskSum> {        public double[] sums;        @Override        public void map(Chunk[] chks) {            sums = new double[chks.length];            for (int c = 0; c < chks.length; c++) {                for (int r = 0; r < chks[c].len(); r++) {                    sums[c] += chks[c].atd(r);                }            }        }        @Override        public void reduce(MRTaskSum other) {            ArrayUtils.add(sums, other.sums);        }    }    return new DenseVector(new MRTaskSum().doAll(frame).sums);}
0
public void map(Chunk[] chks)
{    sums = new double[chks.length];    for (int c = 0; c < chks.length; c++) {        for (int r = 0; r < chks[c].len(); r++) {            sums[c] += chks[c].atd(r);        }    }}
0
public void reduce(MRTaskSum other)
{    ArrayUtils.add(sums, other.sums);}
0
public static double sumSqr(Frame frame)
{    /**     * MRTask to calculate sums of squares of all elements.     */    class MRTaskSumSqr extends MRTask<MRTaskSumSqr> {        public double sumSqr;        @Override        public void map(Chunk[] chks) {            for (Chunk chk : chks) {                for (int r = 0; r < chk.len(); r++) {                    sumSqr += (chk.atd(r) * chk.atd(r));                }            }        }        @Override        public void reduce(MRTaskSumSqr other) {            sumSqr += other.sumSqr;        }    }    return new MRTaskSumSqr().doAll(frame).sumSqr;}
0
public void map(Chunk[] chks)
{    for (Chunk chk : chks) {        for (int r = 0; r < chk.len(); r++) {            sumSqr += (chk.atd(r) * chk.atd(r));        }    }}
0
public void reduce(MRTaskSumSqr other)
{    sumSqr += other.sumSqr;}
0
public static Vector nonZeroCnt(Frame frame)
{    /**     * MRTask to count all non-zero elements.     */    class MRTaskNonZero extends MRTask<MRTaskNonZero> {        public double[] sums;        @Override        public void map(Chunk[] chks) {            sums = new double[chks.length];            for (int c = 0; c < chks.length; c++) {                for (int r = 0; r < chks[c].len(); r++) {                    if ((long) chks[c].atd(r) != 0) {                        sums[c]++;                    }                }            }        }        @Override        public void reduce(MRTaskNonZero other) {            ArrayUtils.add(sums, other.sums);        }    }    return new DenseVector(new MRTaskNonZero().doAll(frame).sums);}
0
public void map(Chunk[] chks)
{    sums = new double[chks.length];    for (int c = 0; c < chks.length; c++) {        for (int r = 0; r < chks[c].len(); r++) {            if ((long) chks[c].atd(r) != 0) {                sums[c]++;            }        }    }}
0
public void reduce(MRTaskNonZero other)
{    ArrayUtils.add(sums, other.sums);}
0
private static Map<Integer, String> reverseMap(Map<String, Integer> map)
{    if (map == null) {        return null;    }    Map<Integer, String> rmap = new HashMap<>();    for (Map.Entry<String, Integer> entry : map.entrySet()) {        rmap.put(entry.getValue(), entry.getKey());    }    return rmap;}
0
private static int chunkSize(long nrow, int minHint, int exactHint)
{    int chunkSz;    int partsHint = Math.max(minHint, exactHint);    if (partsHint < 1) {        /* XXX: calculate based on cloud size and # of cpu */        partsHint = 4;    }    chunkSz = (int) (((nrow - 1) / partsHint) + 1);    if (exactHint > 0) {        return chunkSz;    }    if (chunkSz > 1e6) {        chunkSz = (int) 1e6;    }    if (minHint > 0) {        return chunkSz;    }    if (chunkSz < 1e3) {        chunkSz = (int) 1e3;    }    return chunkSz;}
0
public static H2ODrm drmFromMatrix(Matrix m, int minHint, int exactHint)
{        Frame frame = emptyFrame(m.rowSize(), m.columnSize(), minHint, exactHint);    Vec labels = null;    Vec.Writer[] writers = new Vec.Writer[m.columnSize()];    Futures closer = new Futures();        for (int i = 0; i < writers.length; i++) {        writers[i] = frame.vecs()[i].open();    }    for (int r = 0; r < m.rowSize(); r++) {        for (int c = 0; c < m.columnSize(); c++) {            writers[c].set(r, m.getQuick(r, c));        }    }    for (int c = 0; c < m.columnSize(); c++) {        writers[c].close(closer);    }        Map<String, Integer> map = m.getRowLabelBindings();    if (map != null) {                labels = makeEmptyStrVec(frame.anyVec());        Vec.Writer writer = labels.open();        Map<Integer, String> rmap = reverseMap(map);        for (int r = 0; r < m.rowSize(); r++) {            writer.set(r, rmap.get(r));        }        writer.close(closer);    }    closer.blockForPending();    return new H2ODrm(frame, labels);}
0
public static Frame emptyFrame(long nrow, int ncol, int minHint, int exactHint)
{    Vec.VectorGroup vg = new Vec.VectorGroup();    return emptyFrame(nrow, ncol, minHint, exactHint, vg);}
0
public static Frame emptyFrame(long nrow, int ncol, int minHint, int exactHint, Vec.VectorGroup vg)
{    int chunkSz = chunkSize(nrow, minHint, exactHint);        int nchunks = (int) ((nrow - 1) / chunkSz) + 1;    long[] espc = new long[nchunks + 1];    for (int i = 0; i < nchunks; i++) {        espc[i] = i * chunkSz;    }    espc[nchunks] = nrow;        Vec vtemplate = new Vec(vg.addVec(), espc);        Vec[] vecs = vtemplate.makeCons(ncol, 0, null, null);    return new Frame(vecs);}
0
public static Vec makeEmptyStrVec(final Vec template)
{    final int nChunks = template.nChunks();    Key<Vec> key = template.group().addVec();    final Vec emptystr = new Vec(key, template._espc, null, Vec.T_NUM);    new MRTask() {        @Override        protected void setupLocal() {            for (int i = 0; i < nChunks; i++) {                Key k = emptystr.chunkKey(i);                int chklen = vecChunkLen(template, i);                int[] stridx = new int[chklen];                byte[] b = new byte[1];                b[0] = 0;                for (int j = 0; j < chklen; j++) stridx[j] = -1;                if (k.home())                    DKV.put(k, new CStrChunk(1, b, chklen, stridx), _fs);            }            if (emptystr._key.home())                DKV.put(emptystr._key, emptystr, _fs);        }    }.doAllNodes();    return emptystr;}
0
protected void setupLocal()
{    for (int i = 0; i < nChunks; i++) {        Key k = emptystr.chunkKey(i);        int chklen = vecChunkLen(template, i);        int[] stridx = new int[chklen];        byte[] b = new byte[1];        b[0] = 0;        for (int j = 0; j < chklen; j++) stridx[j] = -1;        if (k.home())            DKV.put(k, new CStrChunk(1, b, chklen, stridx), _fs);    }    if (emptystr._key.home())        DKV.put(emptystr._key, emptystr, _fs);}
0
public static int vecChunkLen(Vec template, int chunk)
{    return (int) (template._espc[chunk + 1] - template._espc[chunk]);}
0
public static H2ODrm emptyDrm(long nrow, int ncol, int minHint, int exactHint)
{    return new H2ODrm(emptyFrame(nrow, ncol, minHint, exactHint));}
0
public static Matrix allreduceBlock(H2ODrm drmA, Object bmfn, Object rfn)
{    class MRTaskMR extends MRTask<MRTaskMR> {        H2OBCast<Matrix> bmf_out;        Serializable bmf;        Serializable rf;        public MRTaskMR(Object _bmf, Object _rf) {            bmf = (Serializable) _bmf;            rf = (Serializable) _rf;        }        @Override        public void map(Chunk[] chks) {            Function1 f = (Function1) bmf;            bmf_out = new H2OBCast((Matrix) f.apply(new scala.Tuple2(null, new H2OBlockMatrix(chks))));        }        @Override        public void reduce(MRTaskMR that) {            Function2 f = (Function2) rf;            bmf_out = new H2OBCast((Matrix) f.apply(this.bmf_out.value(), that.bmf_out.value()));        }    }    return new MRTaskMR(bmfn, rfn).doAll(drmA.frame).bmf_out.value();}
0
public void map(Chunk[] chks)
{    Function1 f = (Function1) bmf;    bmf_out = new H2OBCast((Matrix) f.apply(new scala.Tuple2(null, new H2OBlockMatrix(chks))));}
0
public void reduce(MRTaskMR that)
{    Function2 f = (Function2) rf;    bmf_out = new H2OBCast((Matrix) f.apply(this.bmf_out.value(), that.bmf_out.value()));}
0
public static H2ODrm exec(H2ODrm drmA, H2ODrm drmB)
{    Frame A = drmA.frame;    Vec keys = drmA.keys;    final Frame B = drmB.frame;    int ABt_cols = (int) B.numRows();                        Frame ABt = new MRTask() {        public void map(Chunk[] chks, NewChunk[] ncs) {            int chunkSize = chks[0].len();            Vec[] B_vecs = B.vecs();            for (int c = 0; c < ncs.length; c++) {                for (int r = 0; r < chunkSize; r++) {                    double v = 0;                    for (int i = 0; i < chks.length; i++) {                        v += (chks[i].atd(r) * B_vecs[i].at(c));                    }                    ncs[c].addNum(v);                }            }        }    }.doAll(ABt_cols, A).outputFrame(null, null);        return new H2ODrm(ABt, keys);}
0
public void map(Chunk[] chks, NewChunk[] ncs)
{    int chunkSize = chks[0].len();    Vec[] B_vecs = B.vecs();    for (int c = 0; c < ncs.length; c++) {        for (int r = 0; r < chunkSize; r++) {            double v = 0;            for (int i = 0; i < chks.length; i++) {                v += (chks[i].atd(r) * B_vecs[i].at(c));            }            ncs[c].addNum(v);        }    }}
0
public static H2ODrm exec(H2ODrm drmA, H2ODrm drmB, final String op)
{    final Frame A = drmA.frame;    final Frame B = drmB.frame;    Vec keys = drmA.keys;    int AewB_cols = A.numCols();                        Frame AewB = new MRTask() {        private double opfn(String op, double a, double b) {            if (a == 0.0 && b == 0.0) {                return 0.0;            }            if (op.equals("+")) {                return a + b;            } else if (op.equals("-")) {                return a - b;            } else if (op.equals("*")) {                return a * b;            } else if (op.equals("/")) {                return a / b;            }            return 0.0;        }        @Override        public void map(Chunk[] chks, NewChunk[] ncs) {            int chunkSize = chks[0].len();            Vec[] B_vecs = B.vecs();            long start = chks[0].start();            for (int c = 0; c < chks.length; c++) {                for (int r = 0; r < chunkSize; r++) {                    ncs[c].addNum(opfn(op, chks[c].atd(r), B_vecs[c].at(start + r)));                }            }        }    }.doAll(AewB_cols, A).outputFrame(null, null);        return new H2ODrm(AewB, keys);}
0
private double opfn(String op, double a, double b)
{    if (a == 0.0 && b == 0.0) {        return 0.0;    }    if (op.equals("+")) {        return a + b;    } else if (op.equals("-")) {        return a - b;    } else if (op.equals("*")) {        return a * b;    } else if (op.equals("/")) {        return a / b;    }    return 0.0;}
0
public void map(Chunk[] chks, NewChunk[] ncs)
{    int chunkSize = chks[0].len();    Vec[] B_vecs = B.vecs();    long start = chks[0].start();    for (int c = 0; c < chks.length; c++) {        for (int r = 0; r < chunkSize; r++) {            ncs[c].addNum(opfn(op, chks[c].atd(r), B_vecs[c].at(start + r)));        }    }}
0
public static H2ODrm exec(H2ODrm drmA, final double s, final String op)
{    Frame A = drmA.frame;    Vec keys = drmA.keys;    int AewScalar_cols = A.numCols();            Frame AewScalar = new MRTask() {        private double opfn(String op, double a, double b) {            if (a == 0.0 && b == 0.0) {                return 0.0;            }            if (op.equals("+")) {                return a + b;            } else if (op.equals("-")) {                return a - b;            } else if (op.equals("*")) {                return a * b;            } else if (op.equals("/")) {                return a / b;            }            return 0.0;        }        public void map(Chunk[] chks, NewChunk[] ncs) {            int chunkSize = chks[0].len();            long start = chks[0].start();            for (int c = 0; c < chks.length; c++) {                for (int r = 0; r < chunkSize; r++) {                    ncs[c].addNum(opfn(op, chks[c].atd(r), s));                }            }        }    }.doAll(AewScalar_cols, A).outputFrame(null, null);        return new H2ODrm(AewScalar, keys);}
0
private double opfn(String op, double a, double b)
{    if (a == 0.0 && b == 0.0) {        return 0.0;    }    if (op.equals("+")) {        return a + b;    } else if (op.equals("-")) {        return a - b;    } else if (op.equals("*")) {        return a * b;    } else if (op.equals("/")) {        return a / b;    }    return 0.0;}
0
public void map(Chunk[] chks, NewChunk[] ncs)
{    int chunkSize = chks[0].len();    long start = chks[0].start();    for (int c = 0; c < chks.length; c++) {        for (int r = 0; r < chunkSize; r++) {            ncs[c].addNum(opfn(op, chks[c].atd(r), s));        }    }}
0
public static H2ODrm exec(H2ODrm drmA, Object f, final boolean evalZeros)
{    Frame A = drmA.frame;    Vec keys = drmA.keys;    final int ncol = A.numCols();    /**     * MRTask to execute fn on all elements.     */    class MRTaskAewUnary extends MRTask<MRTaskAewUnary> {        Serializable fn;        MRTaskAewUnary(Object _fn) {            fn = (Serializable) _fn;        }        public void map(Chunk[] chks, NewChunk[] ncs) {            for (int c = 0; c < chks.length; c++) {                Chunk chk = chks[c];                Function1 f = (Function1) fn;                int ChunkLen = chk.len();                if (!evalZeros && chk.isSparse()) {                    /* sparse and skip zeros */                    int prev_offset = -1;                    for (int r = chk.nextNZ(-1); r < ChunkLen; r = chk.nextNZ(prev_offset)) {                        if (r - prev_offset > 1)                            ncs[c].addZeros(r - prev_offset - 1);                        ncs[c].addNum((double) f.apply(chk.atd(r)));                        prev_offset = r;                    }                    if (ChunkLen - prev_offset > 1)                        ncs[c].addZeros(chk._len - prev_offset - 1);                } else {                    /* dense or non-skip zeros */                    for (int r = 0; r < ChunkLen; r++) {                        ncs[c].addNum((double) f.apply(chk.atd(r)));                    }                }            }        }    }    Frame fmap = new MRTaskAewUnary(f).doAll(ncol, A).outputFrame(null, null);    return new H2ODrm(fmap, keys);}
0
public void map(Chunk[] chks, NewChunk[] ncs)
{    for (int c = 0; c < chks.length; c++) {        Chunk chk = chks[c];        Function1 f = (Function1) fn;        int ChunkLen = chk.len();        if (!evalZeros && chk.isSparse()) {            /* sparse and skip zeros */            int prev_offset = -1;            for (int r = chk.nextNZ(-1); r < ChunkLen; r = chk.nextNZ(prev_offset)) {                if (r - prev_offset > 1)                    ncs[c].addZeros(r - prev_offset - 1);                ncs[c].addNum((double) f.apply(chk.atd(r)));                prev_offset = r;            }            if (ChunkLen - prev_offset > 1)                ncs[c].addZeros(chk._len - prev_offset - 1);        } else {            /* dense or non-skip zeros */            for (int r = 0; r < ChunkLen; r++) {                ncs[c].addNum((double) f.apply(chk.atd(r)));            }        }    }}
0
public static H2ODrm exec(H2ODrm drmA)
{    final Frame A = drmA.frame;            Frame At = H2OHelper.emptyFrame(A.numCols(), (int) A.numRows(), -1, -1);            new MRTask() {        public void map(Chunk[] chks) {            int chunkSize = chks[0].len();            long start = chks[0].start();            Vec[] A_vecs = A.vecs();            for (int c = 0; c < chks.length; c++) {                for (int r = 0; r < chunkSize; r++) {                    chks[c].set(r, A_vecs[(int) (start + r)].at(c));                }            }        }    }.doAll(At);        return new H2ODrm(At);}
0
public void map(Chunk[] chks)
{    int chunkSize = chks[0].len();    long start = chks[0].start();    Vec[] A_vecs = A.vecs();    for (int c = 0; c < chks.length; c++) {        for (int r = 0; r < chunkSize; r++) {            chks[c].set(r, A_vecs[(int) (start + r)].at(c));        }    }}
0
public static H2ODrm exec(H2ODrm drmA)
{    final Frame A = drmA.frame;        Frame AtA = H2OHelper.emptyFrame(A.numCols(), A.numCols(), -1, -1);                    new MRTask() {        public void map(Chunk[] chks) {            int chunkSize = chks[0].len();            long start = chks[0].start();            Vec[] A_vecs = A.vecs();            long A_rows = A.numRows();            for (int c = 0; c < chks.length; c++) {                for (int r = 0; r < chunkSize; r++) {                    double v = 0;                    for (long i = 0; i < A_rows; i++) {                        v += (A_vecs[(int) (start + r)].at(i) * A_vecs[c].at(i));                    }                    chks[c].set(r, v);                }            }        }    }.doAll(AtA);        return new H2ODrm(AtA);}
0
public void map(Chunk[] chks)
{    int chunkSize = chks[0].len();    long start = chks[0].start();    Vec[] A_vecs = A.vecs();    long A_rows = A.numRows();    for (int c = 0; c < chks.length; c++) {        for (int r = 0; r < chunkSize; r++) {            double v = 0;            for (long i = 0; i < A_rows; i++) {                v += (A_vecs[(int) (start + r)].at(i) * A_vecs[c].at(i));            }            chks[c].set(r, v);        }    }}
0
public static H2ODrm exec(H2ODrm drmA, H2ODrm drmB)
{    final Frame A = drmA.frame;    final Frame B = drmB.frame;        Frame AtB = H2OHelper.emptyFrame(A.numCols(), B.numCols(), -1, -1);                    new MRTask() {        public void map(Chunk[] chks) {            int chunkSize = chks[0].len();            long start = chks[0].start();            long A_rows = A.numRows();            Vec[] A_vecs = A.vecs();            Vec[] B_vecs = B.vecs();            for (int c = 0; c < chks.length; c++) {                for (int r = 0; r < chunkSize; r++) {                    double v = 0;                    for (long i = 0; i < A_rows; i++) {                        v += (A_vecs[(int) (start + r)].at(i) * B_vecs[c].at(i));                    }                    chks[c].set(r, v);                }            }        }    }.doAll(AtB);        return new H2ODrm(AtB);}
0
public void map(Chunk[] chks)
{    int chunkSize = chks[0].len();    long start = chks[0].start();    long A_rows = A.numRows();    Vec[] A_vecs = A.vecs();    Vec[] B_vecs = B.vecs();    for (int c = 0; c < chks.length; c++) {        for (int r = 0; r < chunkSize; r++) {            double v = 0;            for (long i = 0; i < A_rows; i++) {                v += (A_vecs[(int) (start + r)].at(i) * B_vecs[c].at(i));            }            chks[c].set(r, v);        }    }}
0
public static H2ODrm exec(H2ODrm drmA, Vector x)
{    Frame A = drmA.frame;    final H2OBCast<Vector> bx = new H2OBCast<>(x);        class MRTaskAtx extends MRTask<MRTaskAtx> {        double[] atx;        public void map(Chunk[] chks) {            int chunkSize = chks[0].len();            Vector x = bx.value();            long start = chks[0].start();            atx = new double[chks.length];            for (int r = 0; r < chunkSize; r++) {                double d = x.getQuick((int) start + r);                for (int c = 0; c < chks.length; c++) {                    atx[c] += (chks[c].atd(r) * d);                }            }        }        public void reduce(MRTaskAtx other) {            ArrayUtils.add(atx, other.atx);        }    }                Vector v = new DenseVector(new MRTaskAtx().doAll(A).atx);    Matrix m = new DenseMatrix(A.numCols(), 1);    m.assignColumn(0, v);    return H2OHelper.drmFromMatrix(m, -1, -1);}
0
public void map(Chunk[] chks)
{    int chunkSize = chks[0].len();    Vector x = bx.value();    long start = chks[0].start();    atx = new double[chks.length];    for (int r = 0; r < chunkSize; r++) {        double d = x.getQuick((int) start + r);        for (int c = 0; c < chks.length; c++) {            atx[c] += (chks[c].atd(r) * d);        }    }}
0
public void reduce(MRTaskAtx other)
{    ArrayUtils.add(atx, other.atx);}
0
public static H2ODrm exec(H2ODrm drmA, Vector x)
{    Frame A = drmA.frame;    Vec keys = drmA.keys;    final H2OBCast<Vector> bx = new H2OBCast<>(x);                    Frame Ax = new MRTask() {        public void map(Chunk[] chks, NewChunk nc) {            int chunkSize = chks[0].len();            Vector x = bx.value();            for (int r = 0; r < chunkSize; r++) {                double v = 0;                for (int c = 0; c < chks.length; c++) {                    v += (chks[c].atd(r) * x.getQuick(c));                }                nc.addNum(v);            }        }    }.doAll(1, A).outputFrame(null, null);        return new H2ODrm(Ax, keys);}
0
public void map(Chunk[] chks, NewChunk nc)
{    int chunkSize = chks[0].len();    Vector x = bx.value();    for (int r = 0; r < chunkSize; r++) {        double v = 0;        for (int c = 0; c < chks.length; c++) {            v += (chks[c].atd(r) * x.getQuick(c));        }        nc.addNum(v);    }}
0
public static H2ODrm exec(H2ODrm drmA, H2ODrm drmB)
{    Frame fra = drmA.frame;    Vec keysa = drmA.keys;    Frame frb = drmB.frame;    Vec keysb = drmB.keys;        if (fra.anyVec().group() == frb.anyVec().group()) {                return zip(fra, keysa, frb, keysb);    } else {                return join(fra, keysa, frb, keysb);    }}
0
private static H2ODrm zip(final Frame fra, final Vec keysa, final Frame frb, final Vec keysb)
{        Vec[] vecs = new Vec[fra.vecs().length + frb.vecs().length];    int d = 0;        for (Vec vfra : fra.vecs()) {        vecs[d++] = vfra;    }        for (Vec vfrb : frb.vecs()) {        vecs[d++] = vfrb;    }        Frame fr = new Frame(vecs);    /* Finally, inherit A's string labels into the result */    return new H2ODrm(fr, keysa);}
0
private static H2ODrm join(final Frame fra, final Vec keysa, final Frame frb, final Vec keysb)
{        Vec[] bvecs = new Vec[frb.vecs().length];    for (int i = 0; i < bvecs.length; i++) {                bvecs[i] = fra.anyVec().makeZero();    }            new MRTask() {        public void map(Chunk[] chks) {            int chunkSize = chks[0].len();            long start = chks[0].start();            Vec[] vecs = frb.vecs();            for (int r = 0; r < chunkSize; r++) {                for (int c = 0; c < chks.length; c++) {                                        chks[c].set(r, vecs[c].at(start + r));                }            }        }    }.doAll(bvecs);        return zip(fra, keysa, new Frame(bvecs), null);}
0
public void map(Chunk[] chks)
{    int chunkSize = chks[0].len();    long start = chks[0].start();    Vec[] vecs = frb.vecs();    for (int r = 0; r < chunkSize; r++) {        for (int c = 0; c < chks.length; c++) {                        chks[c].set(r, vecs[c].at(start + r));        }    }}
0
public static H2ODrm exec(H2ODrm drmA, double scalar, boolean leftbind)
{    Frame fra = drmA.frame;    Vec newcol = fra.anyVec().makeCon(scalar);    Vec[] vecs = new Vec[fra.vecs().length + 1];    int d = 0;    if (leftbind)        vecs[d++] = newcol;    for (Vec vfra : fra.vecs()) vecs[d++] = vfra;    if (!leftbind)        vecs[d++] = newcol;    return new H2ODrm(new Frame(vecs), drmA.keys);}
0
public static H2ODrm exec(H2ODrm drmA, int ncol, Object bmf, final boolean isRstr, final ClassTag<K> k, final ClassTag<R> r)
{    Frame A = drmA.frame;    Vec keys = drmA.keys;    /**     * MRTask to execute bmf on partitions. Partitions are     * made accessible to bmf in the form of H2OBlockMatrix.     */    class MRTaskBMF extends MRTask<MRTaskBMF> {        Serializable bmf;        Vec labels;        MRTaskBMF(Object _bmf, Vec _labels) {                                                                                                bmf = (Serializable) _bmf;            labels = _labels;        }        /**         * Create H2OBlockMatrix from the partition         */        private Matrix blockify(Chunk[] chks) {            return new H2OBlockMatrix(chks);        }        /**         * Ingest the output of bmf into the output partition         */        private void deblockify(Matrix out, NewChunk[] ncs) {                        for (int c = 0; c < out.columnSize(); c++) {                for (int r = 0; r < out.rowSize(); r++) {                    ncs[c].addNum(out.getQuick(r, c));                }            }        }                                                                                                                public void map(Chunk[] chks, NewChunk[] ncs) {            long start = chks[0].start();            NewChunk nclabel = isRstr ? ncs[ncs.length - 1] : null;            deblockify(MapBlockHelper.exec(bmf, blockify(chks), start, labels, nclabel, k, r), ncs);                }    }    int ncolRes = ncol + (isRstr ? 1 : 0);    Frame fmap = new MRTaskBMF(bmf, keys).doAll(ncolRes, A).outputFrame(null, null);    Vec vmap = null;    if (isRstr) {                                vmap = fmap.vecs()[ncol];        fmap = new Frame(Arrays.copyOfRange(fmap.vecs(), 0, ncol));    }    return new H2ODrm(fmap, vmap);}
0
private Matrix blockify(Chunk[] chks)
{    return new H2OBlockMatrix(chks);}
0
private void deblockify(Matrix out, NewChunk[] ncs)
{        for (int c = 0; c < out.columnSize(); c++) {        for (int r = 0; r < out.rowSize(); r++) {            ncs[c].addNum(out.getQuick(r, c));        }    }}
0
public void map(Chunk[] chks, NewChunk[] ncs)
{    long start = chks[0].start();    NewChunk nclabel = isRstr ? ncs[ncs.length - 1] : null;    deblockify(MapBlockHelper.exec(bmf, blockify(chks), start, labels, nclabel, k, r), ncs);}
0
public static H2ODrm exec(H2ODrm drmA, int min, int exact)
{    final Frame frin = drmA.frame;    final Vec vin = drmA.keys;        Frame frout = H2OHelper.emptyFrame(frin.numRows(), frin.numCols(), min, exact);    Vec vout = null;    if (vin != null) {                                                                vout = new MRTask() {            public void map(Chunk[] chks, NewChunk nc) {                int chunkSize = chks[0].len();                Vec[] vins = frin.vecs();                long start = chks[0].start();                ValueString vstr = new ValueString();                for (int r = 0; r < chunkSize; r++) {                    for (int c = 0; c < chks.length; c++) {                        chks[c].set(r, vins[c].at(start + r));                    }                    nc.addStr(vin.atStr(vstr, start + r));                }            }        }.doAll(1, frout).outputFrame(null, null).anyVec();    } else {                        new MRTask() {            public void map(Chunk[] chks) {                int chunkSize = chks[0].len();                Vec[] vins = frin.vecs();                long start = chks[0].start();                for (int r = 0; r < chunkSize; r++) {                    for (int c = 0; c < chks.length; c++) {                        chks[c].set(r, vins[c].at(start + r));                    }                }            }        }.doAll(frout);    }    return new H2ODrm(frout, vout);}
0
public void map(Chunk[] chks, NewChunk nc)
{    int chunkSize = chks[0].len();    Vec[] vins = frin.vecs();    long start = chks[0].start();    ValueString vstr = new ValueString();    for (int r = 0; r < chunkSize; r++) {        for (int c = 0; c < chks.length; c++) {            chks[c].set(r, vins[c].at(start + r));        }        nc.addStr(vin.atStr(vstr, start + r));    }}
0
public void map(Chunk[] chks)
{    int chunkSize = chks[0].len();    Vec[] vins = frin.vecs();    long start = chks[0].start();    for (int r = 0; r < chunkSize; r++) {        for (int c = 0; c < chks.length; c++) {            chks[c].set(r, vins[c].at(start + r));        }    }}
0
public static H2ODrm exec(H2ODrm drmA, H2ODrm drmB)
{    final Frame fra = drmA.frame;    final Vec keysa = drmA.keys;    final Frame frb = drmB.frame;    final Vec keysb = drmB.keys;                Frame frbind = H2OHelper.emptyFrame(fra.numRows() + frb.numRows(), fra.numCols(), -1, -1, fra.anyVec().group());    Vec keys = null;    MRTask task = new MRTask() {        public void map(Chunk[] chks, NewChunk nc) {            Vec[] A_vecs = fra.vecs();            Vec[] B_vecs = frb.vecs();            long A_rows = fra.numRows();            long B_rows = frb.numRows();            long start = chks[0].start();            int chunkSize = chks[0].len();            ValueString vstr = new ValueString();            for (int r = 0; r < chunkSize; r++) {                for (int c = 0; c < chks.length; c++) {                    if (r + start < A_rows) {                        chks[c].set(r, A_vecs[c].at(r + start));                        if (keysa != null) {                            nc.addStr(keysa.atStr(vstr, r + start));                        }                    } else {                        chks[c].set(r, B_vecs[c].at(r + start - A_rows));                        if (keysb != null) {                            nc.addStr(keysb.atStr(vstr, r + start - A_rows));                        }                    }                }            }        }    };    if (keysa == null) {        keys = task.doAll(1, frbind).outputFrame(null, null).anyVec();    } else {        task.doAll(frbind);    }    return new H2ODrm(frbind, keys);}
0
public void map(Chunk[] chks, NewChunk nc)
{    Vec[] A_vecs = fra.vecs();    Vec[] B_vecs = frb.vecs();    long A_rows = fra.numRows();    long B_rows = frb.numRows();    long start = chks[0].start();    int chunkSize = chks[0].len();    ValueString vstr = new ValueString();    for (int r = 0; r < chunkSize; r++) {        for (int c = 0; c < chks.length; c++) {            if (r + start < A_rows) {                chks[c].set(r, A_vecs[c].at(r + start));                if (keysa != null) {                    nc.addStr(keysa.atStr(vstr, r + start));                }            } else {                chks[c].set(r, B_vecs[c].at(r + start - A_rows));                if (keysb != null) {                    nc.addStr(keysb.atStr(vstr, r + start - A_rows));                }            }        }    }}
0
public static H2ODrm exec(H2ODrm drmA, final Range R)
{    Frame A = drmA.frame;    Vec keys = drmA.keys;            Frame Arr = new MRTask() {        public void map(Chunk[] chks, NewChunk[] ncs) {            int chunkSize = chks[0].len();            long chunkStart = chks[0].start();                        if (chunkStart > R.end() || (chunkStart + chunkSize) < R.start()) {                return;            }                        for (int r = 0; r < chunkSize; r++) {                if (!R.contains(chunkStart + r)) {                    continue;                }                for (int c = 0; c < chks.length; c++) {                    ncs[c].addNum(chks[c].atd(r));                }            }        }    }.doAll(A.numCols(), A).outputFrame(null, null);    Vec Vrr = (keys == null) ? null : new MRTask() {                        public void map(Chunk chk, NewChunk nc) {            int chunkSize = chk.len();            long chunkStart = chk.start();            ValueString vstr = new ValueString();            if (chunkStart > R.end() || (chunkStart + chunkSize) < R.start()) {                return;            }            for (int r = 0; r < chunkSize; r++) {                if (!R.contains(chunkStart + r)) {                    continue;                }                nc.addStr(chk.atStr(vstr, r));            }        }    }.doAll(1, keys).outputFrame(null, null).anyVec();    return new H2ODrm(Arr, Vrr);}
0
public void map(Chunk[] chks, NewChunk[] ncs)
{    int chunkSize = chks[0].len();    long chunkStart = chks[0].start();        if (chunkStart > R.end() || (chunkStart + chunkSize) < R.start()) {        return;    }        for (int r = 0; r < chunkSize; r++) {        if (!R.contains(chunkStart + r)) {            continue;        }        for (int c = 0; c < chks.length; c++) {            ncs[c].addNum(chks[c].atd(r));        }    }}
0
public void map(Chunk chk, NewChunk nc)
{    int chunkSize = chk.len();    long chunkStart = chk.start();    ValueString vstr = new ValueString();    if (chunkStart > R.end() || (chunkStart + chunkSize) < R.start()) {        return;    }    for (int r = 0; r < chunkSize; r++) {        if (!R.contains(chunkStart + r)) {            continue;        }        nc.addStr(chk.atStr(vstr, r));    }}
0
public static H2ODrm exec(H2ODrm drmA, Matrix B)
{    Frame A = drmA.frame;    Vec keys = drmA.keys;    Frame AinCoreB = null;    if (B instanceof DiagonalMatrix) {        AinCoreB = execDiagonal(A, B.viewDiagonal());    } else {        AinCoreB = execCommon(A, B);    }    return new H2ODrm(AinCoreB, keys);}
0
private static Frame execDiagonal(final Frame A, Vector d)
{    final H2OBCast<Vector> bd = new H2OBCast<>(d);    return new MRTask() {        public void map(Chunk[] chks, NewChunk[] ncs) {            Vector D = bd.value();            int chunkSize = chks[0].len();            for (int c = 0; c < ncs.length; c++) {                for (int r = 0; r < chunkSize; r++) {                    double v = (chks[c].atd(r) * D.getQuick(c));                    ncs[c].addNum(v);                }            }        }    }.doAll(d.size(), A).outputFrame(null, null);}
0
public void map(Chunk[] chks, NewChunk[] ncs)
{    Vector D = bd.value();    int chunkSize = chks[0].len();    for (int c = 0; c < ncs.length; c++) {        for (int r = 0; r < chunkSize; r++) {            double v = (chks[c].atd(r) * D.getQuick(c));            ncs[c].addNum(v);        }    }}
0
private static Frame execCommon(final Frame A, Matrix b)
{    final H2OBCast<Matrix> bb = new H2OBCast<>(b);    return new MRTask() {        public void map(Chunk[] chks, NewChunk[] ncs) {            Matrix B = bb.value();            int chunkSize = chks[0].len();            for (int c = 0; c < ncs.length; c++) {                for (int r = 0; r < chunkSize; r++) {                    double v = 0;                    for (int i = 0; i < chks.length; i++) {                        v += (chks[i].atd(r) * B.getQuick(i, c));                    }                    ncs[c].addNum(v);                }            }        }    }.doAll(b.columnSize(), A).outputFrame(null, null);}
0
public void map(Chunk[] chks, NewChunk[] ncs)
{    Matrix B = bb.value();    int chunkSize = chks[0].len();    for (int c = 0; c < ncs.length; c++) {        for (int r = 0; r < chunkSize; r++) {            double v = 0;            for (int i = 0; i < chks.length; i++) {                v += (chks[i].atd(r) * B.getQuick(i, c));            }            ncs[c].addNum(v);        }    }}
0
protected int randIndex()
{    return BenchmarkRunner.randIndex();}
0
protected boolean randBool()
{    return BenchmarkRunner.randBool();}
0
protected boolean depends(Vector v)
{    return randIndex() < v.getNumNondefaultElements();}
0
protected int randIndex()
{    return BenchmarkRunner.randIndex();}
0
protected boolean randBool()
{    return BenchmarkRunner.randBool();}
0
protected boolean depends(Vector v)
{    return randIndex() < v.getNumNondefaultElements();}
0
private static int randIndex()
{    return R.nextInt(BUCKET_SIZE);}
0
private static boolean randBool()
{    return R.nextBoolean();}
0
public TimingStatistics benchmark(BenchmarkFn function)
{    TimingStatistics stats = new TimingStatistics();    boolean result = false;    while (true) {        int i = R.nextInt(BUCKET_SIZE);        TimingStatistics.Call call = stats.newCall(leadTimeUsec);        result = result ^ function.apply(i);        if (call.end(maxTimeUsec)) {            break;        }    }    return stats;}
0
public TimingStatistics benchmarkD(BenchmarkFnD function)
{    TimingStatistics stats = new TimingStatistics();    double result = 0;    while (true) {        int i = R.nextInt(BUCKET_SIZE);        TimingStatistics.Call call = stats.newCall(leadTimeUsec);        result += function.apply(i);        if (call.end(maxTimeUsec)) {            break;        }    }        System.err.println("Result = " + result);    return stats;}
0
public void benchmark()
{    mark.printStats(mark.getRunner().benchmark(new BenchmarkFn() {        @Override        public Boolean apply(Integer i) {            mark.vectors[0][mark.vIndex(i)] = mark.vectors[0][mark.vIndex(i)].clone();            return depends(mark.vectors[0][mark.vIndex(i)]);        }    }), CLONE, DENSE_VECTOR);    mark.printStats(mark.getRunner().benchmark(new BenchmarkFn() {        @Override        public Boolean apply(Integer i) {            mark.vectors[1][mark.vIndex(i)] = mark.vectors[1][mark.vIndex(i)].clone();            return depends(mark.vectors[1][mark.vIndex(i)]);        }    }), CLONE, RAND_SPARSE_VECTOR);    mark.printStats(mark.getRunner().benchmark(new BenchmarkFn() {        @Override        public Boolean apply(Integer i) {            mark.vectors[2][mark.vIndex(i)] = mark.vectors[2][mark.vIndex(i)].clone();            return depends(mark.vectors[2][mark.vIndex(i)]);        }    }), CLONE, SEQ_SPARSE_VECTOR);}
0
public Boolean apply(Integer i)
{    mark.vectors[0][mark.vIndex(i)] = mark.vectors[0][mark.vIndex(i)].clone();    return depends(mark.vectors[0][mark.vIndex(i)]);}
0
public Boolean apply(Integer i)
{    mark.vectors[1][mark.vIndex(i)] = mark.vectors[1][mark.vIndex(i)].clone();    return depends(mark.vectors[1][mark.vIndex(i)]);}
0
public Boolean apply(Integer i)
{    mark.vectors[2][mark.vIndex(i)] = mark.vectors[2][mark.vIndex(i)].clone();    return depends(mark.vectors[2][mark.vIndex(i)]);}
0
public void benchmark(DistanceMeasure measure) throws IOException
{    SparseMatrix clusterDistances = new SparseMatrix(mark.numClusters, mark.numClusters);    for (int i = 0; i < mark.numClusters; i++) {        for (int j = 0; j < mark.numClusters; j++) {            double distance = Double.POSITIVE_INFINITY;            if (i != j) {                distance = measure.distance(mark.clusters[i], mark.clusters[j]);            }            clusterDistances.setQuick(i, j, distance);        }    }    long distanceCalculations = 0;    TimingStatistics stats = new TimingStatistics();    for (int l = 0; l < mark.loop; l++) {        TimingStatistics.Call call = stats.newCall(mark.leadTimeUsec);        for (int i = 0; i < mark.numVectors; i++) {            Vector vector = mark.vectors[1][mark.vIndex(i)];            double minDistance = Double.MAX_VALUE;            for (int k = 0; k < mark.numClusters; k++) {                double distance = measure.distance(vector, mark.clusters[k]);                distanceCalculations++;                if (distance < minDistance) {                    minDistance = distance;                }            }        }        if (call.end(mark.maxTimeUsec)) {            break;        }    }    mark.printStats(stats, measure.getClass().getName(), "Closest C w/o Elkan's trick", "distanceCalculations = " + distanceCalculations);    distanceCalculations = 0;    stats = new TimingStatistics();    Random rand = RandomUtils.getRandom();    for (int l = 0; l < mark.loop; l++) {        TimingStatistics.Call call = stats.newCall(mark.leadTimeUsec);        for (int i = 0; i < mark.numVectors; i++) {            Vector vector = mark.vectors[1][mark.vIndex(i)];            int closestCentroid = rand.nextInt(mark.numClusters);            double dist = measure.distance(vector, mark.clusters[closestCentroid]);            distanceCalculations++;            for (int k = 0; k < mark.numClusters; k++) {                if (closestCentroid != k) {                    double centroidDist = clusterDistances.getQuick(k, closestCentroid);                    if (centroidDist < 2 * dist) {                        dist = measure.distance(vector, mark.clusters[k]);                        closestCentroid = k;                        distanceCalculations++;                    }                }            }        }        if (call.end(mark.maxTimeUsec)) {            break;        }    }    mark.printStats(stats, measure.getClass().getName(), "Closest C w/ Elkan's trick", "distanceCalculations = " + distanceCalculations);}
0
public void benchmark(final DistanceMeasure measure)
{    mark.printStats(mark.getRunner().benchmarkD(new BenchmarkFnD() {        @Override        public Double apply(Integer i) {            return measure.distance(mark.vectors[0][mark.vIndex(i)], mark.vectors[0][mark.vIndex(randIndex())]);        }    }), measure.getClass().getName(), DENSE_VECTOR);    mark.printStats(mark.getRunner().benchmarkD(new BenchmarkFnD() {        @Override        public Double apply(Integer i) {            return measure.distance(mark.vectors[1][mark.vIndex(i)], mark.vectors[1][mark.vIndex(randIndex())]);        }    }), measure.getClass().getName(), RAND_SPARSE_VECTOR);    mark.printStats(mark.getRunner().benchmarkD(new BenchmarkFnD() {        @Override        public Double apply(Integer i) {            return measure.distance(mark.vectors[2][mark.vIndex(i)], mark.vectors[2][mark.vIndex(randIndex())]);        }    }), measure.getClass().getName(), SEQ_SPARSE_VECTOR);    mark.printStats(mark.getRunner().benchmarkD(new BenchmarkFnD() {        @Override        public Double apply(Integer i) {            return measure.distance(mark.vectors[0][mark.vIndex(i)], mark.vectors[1][mark.vIndex(randIndex())]);        }    }), measure.getClass().getName(), DENSE_FN_RAND);    mark.printStats(mark.getRunner().benchmarkD(new BenchmarkFnD() {        @Override        public Double apply(Integer i) {            return measure.distance(mark.vectors[0][mark.vIndex(i)], mark.vectors[2][mark.vIndex(randIndex())]);        }    }), measure.getClass().getName(), DENSE_FN_SEQ);    mark.printStats(mark.getRunner().benchmarkD(new BenchmarkFnD() {        @Override        public Double apply(Integer i) {            return measure.distance(mark.vectors[1][mark.vIndex(i)], mark.vectors[0][mark.vIndex(randIndex())]);        }    }), measure.getClass().getName(), RAND_FN_DENSE);    mark.printStats(mark.getRunner().benchmarkD(new BenchmarkFnD() {        @Override        public Double apply(Integer i) {            return measure.distance(mark.vectors[1][mark.vIndex(i)], mark.vectors[2][mark.vIndex(randIndex())]);        }    }), measure.getClass().getName(), RAND_FN_SEQ);    mark.printStats(mark.getRunner().benchmarkD(new BenchmarkFnD() {        @Override        public Double apply(Integer i) {            return measure.distance(mark.vectors[2][mark.vIndex(i)], mark.vectors[0][mark.vIndex(randIndex())]);        }    }), measure.getClass().getName(), SEQ_FN_DENSE);    mark.printStats(mark.getRunner().benchmarkD(new BenchmarkFnD() {        @Override        public Double apply(Integer i) {            return measure.distance(mark.vectors[2][mark.vIndex(i)], mark.vectors[1][mark.vIndex(randIndex())]);        }    }), measure.getClass().getName(), SEQ_FN_RAND);}
0
public Double apply(Integer i)
{    return measure.distance(mark.vectors[0][mark.vIndex(i)], mark.vectors[0][mark.vIndex(randIndex())]);}
0
public Double apply(Integer i)
{    return measure.distance(mark.vectors[1][mark.vIndex(i)], mark.vectors[1][mark.vIndex(randIndex())]);}
0
public Double apply(Integer i)
{    return measure.distance(mark.vectors[2][mark.vIndex(i)], mark.vectors[2][mark.vIndex(randIndex())]);}
0
public Double apply(Integer i)
{    return measure.distance(mark.vectors[0][mark.vIndex(i)], mark.vectors[1][mark.vIndex(randIndex())]);}
0
public Double apply(Integer i)
{    return measure.distance(mark.vectors[0][mark.vIndex(i)], mark.vectors[2][mark.vIndex(randIndex())]);}
0
public Double apply(Integer i)
{    return measure.distance(mark.vectors[1][mark.vIndex(i)], mark.vectors[0][mark.vIndex(randIndex())]);}
0
public Double apply(Integer i)
{    return measure.distance(mark.vectors[1][mark.vIndex(i)], mark.vectors[2][mark.vIndex(randIndex())]);}
0
public Double apply(Integer i)
{    return measure.distance(mark.vectors[2][mark.vIndex(i)], mark.vectors[0][mark.vIndex(randIndex())]);}
0
public Double apply(Integer i)
{    return measure.distance(mark.vectors[2][mark.vIndex(i)], mark.vectors[1][mark.vIndex(randIndex())]);}
0
public void benchmark()
{    benchmarkDot();    benchmarkNorm1();    benchmarkNorm2();    benchmarkLogNormalize();}
0
private void benchmarkLogNormalize()
{    mark.printStats(mark.getRunner().benchmark(new BenchmarkFn() {        @Override        public Boolean apply(Integer i) {            return depends(mark.vectors[0][mark.vIndex(i)].logNormalize());        }    }), LOG_NORMALIZE, DENSE_VECTOR);    mark.printStats(mark.getRunner().benchmark(new BenchmarkFn() {        @Override        public Boolean apply(Integer i) {            return depends(mark.vectors[1][mark.vIndex(i)].logNormalize());        }    }), LOG_NORMALIZE, RAND_SPARSE_VECTOR);    mark.printStats(mark.getRunner().benchmark(new BenchmarkFn() {        @Override        public Boolean apply(Integer i) {            return depends(mark.vectors[2][mark.vIndex(i)].logNormalize());        }    }), LOG_NORMALIZE, SEQ_SPARSE_VECTOR);}
0
public Boolean apply(Integer i)
{    return depends(mark.vectors[0][mark.vIndex(i)].logNormalize());}
0
public Boolean apply(Integer i)
{    return depends(mark.vectors[1][mark.vIndex(i)].logNormalize());}
0
public Boolean apply(Integer i)
{    return depends(mark.vectors[2][mark.vIndex(i)].logNormalize());}
0
private void benchmarkNorm1()
{    mark.printStats(mark.getRunner().benchmarkD(new BenchmarkFnD() {        @Override        public Double apply(Integer i) {            return mark.vectors[0][mark.vIndex(i)].norm(1);        }    }), NORM1, DENSE_VECTOR);    mark.printStats(mark.getRunner().benchmarkD(new BenchmarkFnD() {        @Override        public Double apply(Integer i) {            return mark.vectors[1][mark.vIndex(i)].norm(1);        }    }), NORM1, RAND_SPARSE_VECTOR);    mark.printStats(mark.getRunner().benchmarkD(new BenchmarkFnD() {        @Override        public Double apply(Integer i) {            return mark.vectors[2][mark.vIndex(i)].norm(1);        }    }), NORM1, SEQ_SPARSE_VECTOR);}
0
public Double apply(Integer i)
{    return mark.vectors[0][mark.vIndex(i)].norm(1);}
0
public Double apply(Integer i)
{    return mark.vectors[1][mark.vIndex(i)].norm(1);}
0
public Double apply(Integer i)
{    return mark.vectors[2][mark.vIndex(i)].norm(1);}
0
private void benchmarkNorm2()
{    mark.printStats(mark.getRunner().benchmarkD(new BenchmarkFnD() {        @Override        public Double apply(Integer i) {            return mark.vectors[0][mark.vIndex(i)].norm(2);        }    }), NORM2, DENSE_VECTOR);    mark.printStats(mark.getRunner().benchmarkD(new BenchmarkFnD() {        @Override        public Double apply(Integer i) {            return mark.vectors[1][mark.vIndex(i)].norm(2);        }    }), NORM2, RAND_SPARSE_VECTOR);    mark.printStats(mark.getRunner().benchmarkD(new BenchmarkFnD() {        @Override        public Double apply(Integer i) {            return mark.vectors[2][mark.vIndex(i)].norm(2);        }    }), NORM2, SEQ_SPARSE_VECTOR);}
0
public Double apply(Integer i)
{    return mark.vectors[0][mark.vIndex(i)].norm(2);}
0
public Double apply(Integer i)
{    return mark.vectors[1][mark.vIndex(i)].norm(2);}
0
public Double apply(Integer i)
{    return mark.vectors[2][mark.vIndex(i)].norm(2);}
0
private void benchmarkDot()
{    mark.printStats(mark.getRunner().benchmarkD(new BenchmarkFnD() {        @Override        public Double apply(Integer i) {            return mark.vectors[0][mark.vIndex(i)].dot(mark.vectors[0][mark.vIndex(randIndex())]);        }    }), DOT_PRODUCT, DENSE_VECTOR);    mark.printStats(mark.getRunner().benchmarkD(new BenchmarkFnD() {        @Override        public Double apply(Integer i) {            return mark.vectors[1][mark.vIndex(i)].dot(mark.vectors[1][mark.vIndex(randIndex())]);        }    }), DOT_PRODUCT, RAND_SPARSE_VECTOR);    mark.printStats(mark.getRunner().benchmarkD(new BenchmarkFnD() {        @Override        public Double apply(Integer i) {            return mark.vectors[2][mark.vIndex(i)].dot(mark.vectors[2][mark.vIndex(randIndex())]);        }    }), DOT_PRODUCT, SEQ_SPARSE_VECTOR);    mark.printStats(mark.getRunner().benchmarkD(new BenchmarkFnD() {        @Override        public Double apply(Integer i) {            return mark.vectors[0][mark.vIndex(i)].dot(mark.vectors[1][mark.vIndex(randIndex())]);        }    }), DOT_PRODUCT, DENSE_FN_RAND);    mark.printStats(mark.getRunner().benchmarkD(new BenchmarkFnD() {        @Override        public Double apply(Integer i) {            return mark.vectors[0][mark.vIndex(i)].dot(mark.vectors[2][mark.vIndex(randIndex())]);        }    }), DOT_PRODUCT, DENSE_FN_SEQ);    mark.printStats(mark.getRunner().benchmarkD(new BenchmarkFnD() {        @Override        public Double apply(Integer i) {            return mark.vectors[1][mark.vIndex(i)].dot(mark.vectors[0][mark.vIndex(randIndex())]);        }    }), DOT_PRODUCT, RAND_FN_DENSE);    mark.printStats(mark.getRunner().benchmarkD(new BenchmarkFnD() {        @Override        public Double apply(Integer i) {            return mark.vectors[1][mark.vIndex(i)].dot(mark.vectors[2][mark.vIndex(randIndex())]);        }    }), DOT_PRODUCT, RAND_FN_SEQ);    mark.printStats(mark.getRunner().benchmarkD(new BenchmarkFnD() {        @Override        public Double apply(Integer i) {            return mark.vectors[2][mark.vIndex(i)].dot(mark.vectors[0][mark.vIndex(randIndex())]);        }    }), DOT_PRODUCT, SEQ_FN_DENSE);    mark.printStats(mark.getRunner().benchmarkD(new BenchmarkFnD() {        @Override        public Double apply(Integer i) {            return mark.vectors[2][mark.vIndex(i)].dot(mark.vectors[1][mark.vIndex(randIndex())]);        }    }), DOT_PRODUCT, SEQ_FN_RAND);}
0
public Double apply(Integer i)
{    return mark.vectors[0][mark.vIndex(i)].dot(mark.vectors[0][mark.vIndex(randIndex())]);}
0
public Double apply(Integer i)
{    return mark.vectors[1][mark.vIndex(i)].dot(mark.vectors[1][mark.vIndex(randIndex())]);}
0
public Double apply(Integer i)
{    return mark.vectors[2][mark.vIndex(i)].dot(mark.vectors[2][mark.vIndex(randIndex())]);}
0
public Double apply(Integer i)
{    return mark.vectors[0][mark.vIndex(i)].dot(mark.vectors[1][mark.vIndex(randIndex())]);}
0
public Double apply(Integer i)
{    return mark.vectors[0][mark.vIndex(i)].dot(mark.vectors[2][mark.vIndex(randIndex())]);}
0
public Double apply(Integer i)
{    return mark.vectors[1][mark.vIndex(i)].dot(mark.vectors[0][mark.vIndex(randIndex())]);}
0
public Double apply(Integer i)
{    return mark.vectors[1][mark.vIndex(i)].dot(mark.vectors[2][mark.vIndex(randIndex())]);}
0
public Double apply(Integer i)
{    return mark.vectors[2][mark.vIndex(i)].dot(mark.vectors[0][mark.vIndex(randIndex())]);}
0
public Double apply(Integer i)
{    return mark.vectors[2][mark.vIndex(i)].dot(mark.vectors[1][mark.vIndex(randIndex())]);}
0
public static void main(String[] args)
{    VectorBenchmarks mark = new VectorBenchmarks(1000000, 100, 1000, 10, 1);    mark.createData();    new DotBenchmark(mark).benchmarkNorm2();    System.out.println(mark);}
0
public void benchmark()
{    mark.printStats(mark.getRunner().benchmark(new BenchmarkFn() {        @Override        public Boolean apply(Integer i) {            Vector v = mark.vectors[0][mark.vIndex(i)].minus(mark.vectors[0][mark.vIndex(randIndex())]);            return depends(v);        }    }), MINUS, DENSE_VECTOR);    mark.printStats(mark.getRunner().benchmark(new BenchmarkFn() {        @Override        public Boolean apply(Integer i) {            Vector v = mark.vectors[1][mark.vIndex(i)].minus(mark.vectors[1][mark.vIndex(randIndex())]);            return depends(v);        }    }), MINUS, RAND_SPARSE_VECTOR);    mark.printStats(mark.getRunner().benchmark(new BenchmarkFn() {        @Override        public Boolean apply(Integer i) {            Vector v = mark.vectors[2][mark.vIndex(i)].minus(mark.vectors[2][mark.vIndex(randIndex())]);            return depends(v);        }    }), MINUS, SEQ_SPARSE_VECTOR);    mark.printStats(mark.getRunner().benchmark(new BenchmarkFn() {        @Override        public Boolean apply(Integer i) {            Vector v = mark.vectors[0][mark.vIndex(i)].minus(mark.vectors[1][mark.vIndex(randIndex())]);            return depends(v);        }    }), MINUS, DENSE_FN_RAND);    mark.printStats(mark.getRunner().benchmark(new BenchmarkFn() {        @Override        public Boolean apply(Integer i) {            Vector v = mark.vectors[0][mark.vIndex(i)].minus(mark.vectors[2][mark.vIndex(randIndex())]);            return depends(v);        }    }), MINUS, DENSE_FN_SEQ);    mark.printStats(mark.getRunner().benchmark(new BenchmarkFn() {        @Override        public Boolean apply(Integer i) {            Vector v = mark.vectors[1][mark.vIndex(i)].minus(mark.vectors[0][mark.vIndex(randIndex())]);            return depends(v);        }    }), MINUS, RAND_FN_DENSE);    mark.printStats(mark.getRunner().benchmark(new BenchmarkFn() {        @Override        public Boolean apply(Integer i) {            Vector v = mark.vectors[1][mark.vIndex(i)].minus(mark.vectors[2][mark.vIndex(randIndex())]);            return depends(v);        }    }), MINUS, RAND_FN_SEQ);    mark.printStats(mark.getRunner().benchmark(new BenchmarkFn() {        @Override        public Boolean apply(Integer i) {            Vector v = mark.vectors[2][mark.vIndex(i)].minus(mark.vectors[0][mark.vIndex(randIndex())]);            return depends(v);        }    }), MINUS, SEQ_FN_DENSE);    mark.printStats(mark.getRunner().benchmark(new BenchmarkFn() {        @Override        public Boolean apply(Integer i) {            Vector v = mark.vectors[2][mark.vIndex(i)].minus(mark.vectors[1][mark.vIndex(randIndex())]);            return depends(v);        }    }), MINUS, SEQ_FN_RAND);}
0
public Boolean apply(Integer i)
{    Vector v = mark.vectors[0][mark.vIndex(i)].minus(mark.vectors[0][mark.vIndex(randIndex())]);    return depends(v);}
0
public Boolean apply(Integer i)
{    Vector v = mark.vectors[1][mark.vIndex(i)].minus(mark.vectors[1][mark.vIndex(randIndex())]);    return depends(v);}
0
public Boolean apply(Integer i)
{    Vector v = mark.vectors[2][mark.vIndex(i)].minus(mark.vectors[2][mark.vIndex(randIndex())]);    return depends(v);}
0
public Boolean apply(Integer i)
{    Vector v = mark.vectors[0][mark.vIndex(i)].minus(mark.vectors[1][mark.vIndex(randIndex())]);    return depends(v);}
0
public Boolean apply(Integer i)
{    Vector v = mark.vectors[0][mark.vIndex(i)].minus(mark.vectors[2][mark.vIndex(randIndex())]);    return depends(v);}
0
public Boolean apply(Integer i)
{    Vector v = mark.vectors[1][mark.vIndex(i)].minus(mark.vectors[0][mark.vIndex(randIndex())]);    return depends(v);}
0
public Boolean apply(Integer i)
{    Vector v = mark.vectors[1][mark.vIndex(i)].minus(mark.vectors[2][mark.vIndex(randIndex())]);    return depends(v);}
0
public Boolean apply(Integer i)
{    Vector v = mark.vectors[2][mark.vIndex(i)].minus(mark.vectors[0][mark.vIndex(randIndex())]);    return depends(v);}
0
public Boolean apply(Integer i)
{    Vector v = mark.vectors[2][mark.vIndex(i)].minus(mark.vectors[1][mark.vIndex(randIndex())]);    return depends(v);}
0
public void benchmark()
{    mark.printStats(mark.getRunner().benchmark(new BenchmarkFn() {        @Override        public Boolean apply(Integer i) {            Vector v = mark.vectors[0][mark.vIndex(i)].plus(mark.vectors[0][mark.vIndex(randIndex())]);            return depends(v);        }    }), PLUS, DENSE_VECTOR);    mark.printStats(mark.getRunner().benchmark(new BenchmarkFn() {        @Override        public Boolean apply(Integer i) {            Vector v = mark.vectors[1][mark.vIndex(i)].plus(mark.vectors[1][mark.vIndex(randIndex())]);            return depends(v);        }    }), PLUS, RAND_SPARSE_VECTOR);    mark.printStats(mark.getRunner().benchmark(new BenchmarkFn() {        @Override        public Boolean apply(Integer i) {            Vector v = mark.vectors[2][mark.vIndex(i)].plus(mark.vectors[2][mark.vIndex(randIndex())]);            return depends(v);        }    }), PLUS, SEQ_SPARSE_VECTOR);    mark.printStats(mark.getRunner().benchmark(new BenchmarkFn() {        @Override        public Boolean apply(Integer i) {            Vector v = mark.vectors[0][mark.vIndex(i)].plus(mark.vectors[1][mark.vIndex(randIndex())]);            return depends(v);        }    }), PLUS, DENSE_FN_RAND);    mark.printStats(mark.getRunner().benchmark(new BenchmarkFn() {        @Override        public Boolean apply(Integer i) {            Vector v = mark.vectors[0][mark.vIndex(i)].plus(mark.vectors[2][mark.vIndex(randIndex())]);            return depends(v);        }    }), PLUS, DENSE_FN_SEQ);    mark.printStats(mark.getRunner().benchmark(new BenchmarkFn() {        @Override        public Boolean apply(Integer i) {            Vector v = mark.vectors[1][mark.vIndex(i)].plus(mark.vectors[0][mark.vIndex(randIndex())]);            return depends(v);        }    }), PLUS, RAND_FN_DENSE);    mark.printStats(mark.getRunner().benchmark(new BenchmarkFn() {        @Override        public Boolean apply(Integer i) {            Vector v = mark.vectors[1][mark.vIndex(i)].plus(mark.vectors[2][mark.vIndex(randIndex())]);            return depends(v);        }    }), PLUS, RAND_FN_SEQ);    mark.printStats(mark.getRunner().benchmark(new BenchmarkFn() {        @Override        public Boolean apply(Integer i) {            Vector v = mark.vectors[2][mark.vIndex(i)].plus(mark.vectors[0][mark.vIndex(randIndex())]);            return depends(v);        }    }), PLUS, SEQ_FN_DENSE);    mark.printStats(mark.getRunner().benchmark(new BenchmarkFn() {        @Override        public Boolean apply(Integer i) {            Vector v = mark.vectors[2][mark.vIndex(i)].plus(mark.vectors[1][mark.vIndex(randIndex())]);            return depends(v);        }    }), PLUS, SEQ_FN_RAND);}
0
public Boolean apply(Integer i)
{    Vector v = mark.vectors[0][mark.vIndex(i)].plus(mark.vectors[0][mark.vIndex(randIndex())]);    return depends(v);}
0
public Boolean apply(Integer i)
{    Vector v = mark.vectors[1][mark.vIndex(i)].plus(mark.vectors[1][mark.vIndex(randIndex())]);    return depends(v);}
0
public Boolean apply(Integer i)
{    Vector v = mark.vectors[2][mark.vIndex(i)].plus(mark.vectors[2][mark.vIndex(randIndex())]);    return depends(v);}
0
public Boolean apply(Integer i)
{    Vector v = mark.vectors[0][mark.vIndex(i)].plus(mark.vectors[1][mark.vIndex(randIndex())]);    return depends(v);}
0
public Boolean apply(Integer i)
{    Vector v = mark.vectors[0][mark.vIndex(i)].plus(mark.vectors[2][mark.vIndex(randIndex())]);    return depends(v);}
0
public Boolean apply(Integer i)
{    Vector v = mark.vectors[1][mark.vIndex(i)].plus(mark.vectors[0][mark.vIndex(randIndex())]);    return depends(v);}
0
public Boolean apply(Integer i)
{    Vector v = mark.vectors[1][mark.vIndex(i)].plus(mark.vectors[2][mark.vIndex(randIndex())]);    return depends(v);}
0
public Boolean apply(Integer i)
{    Vector v = mark.vectors[2][mark.vIndex(i)].plus(mark.vectors[0][mark.vIndex(randIndex())]);    return depends(v);}
0
public Boolean apply(Integer i)
{    Vector v = mark.vectors[2][mark.vIndex(i)].plus(mark.vectors[1][mark.vIndex(randIndex())]);    return depends(v);}
0
public void benchmark() throws IOException
{    serializeBenchmark();    deserializeBenchmark();}
0
public void serializeBenchmark() throws IOException
{    Configuration conf = new Configuration();    FileSystem fs = FileSystem.get(conf);    Writable one = new IntWritable(0);    VectorWritable vec = new VectorWritable();    TimingStatistics stats = new TimingStatistics();    try (SequenceFile.Writer writer = new SequenceFile.Writer(fs, conf, new Path("/tmp/dense-vector"), IntWritable.class, VectorWritable.class)) {        for (int i = 0; i < mark.loop; i++) {            TimingStatistics.Call call = stats.newCall(mark.leadTimeUsec);            vec.set(mark.vectors[0][mark.vIndex(i)]);            writer.append(one, vec);            if (call.end(mark.maxTimeUsec)) {                break;            }        }    }    mark.printStats(stats, SERIALIZE, DENSE_VECTOR);    stats = new TimingStatistics();    try (SequenceFile.Writer writer = new SequenceFile.Writer(fs, conf, new Path("/tmp/randsparse-vector"), IntWritable.class, VectorWritable.class)) {        for (int i = 0; i < mark.loop; i++) {            TimingStatistics.Call call = stats.newCall(mark.leadTimeUsec);            vec.set(mark.vectors[1][mark.vIndex(i)]);            writer.append(one, vec);            if (call.end(mark.maxTimeUsec)) {                break;            }        }    }    mark.printStats(stats, SERIALIZE, RAND_SPARSE_VECTOR);    stats = new TimingStatistics();    try (SequenceFile.Writer writer = new SequenceFile.Writer(fs, conf, new Path("/tmp/seqsparse-vector"), IntWritable.class, VectorWritable.class)) {        for (int i = 0; i < mark.loop; i++) {            TimingStatistics.Call call = stats.newCall(mark.leadTimeUsec);            vec.set(mark.vectors[2][mark.vIndex(i)]);            writer.append(one, vec);            if (call.end(mark.maxTimeUsec)) {                break;            }        }    }    mark.printStats(stats, SERIALIZE, SEQ_SPARSE_VECTOR);}
0
public void deserializeBenchmark() throws IOException
{    doDeserializeBenchmark(DENSE_VECTOR, "/tmp/dense-vector");    doDeserializeBenchmark(RAND_SPARSE_VECTOR, "/tmp/randsparse-vector");    doDeserializeBenchmark(SEQ_SPARSE_VECTOR, "/tmp/seqsparse-vector");}
0
private void doDeserializeBenchmark(String name, String pathString) throws IOException
{    TimingStatistics stats = new TimingStatistics();    TimingStatistics.Call call = stats.newCall(mark.leadTimeUsec);    SequenceFileValueIterator<Writable> iterator = new SequenceFileValueIterator<>(new Path(pathString), true, new Configuration());    while (iterator.hasNext()) {        iterator.next();        call.end();        call = stats.newCall(mark.leadTimeUsec);    }    iterator.close();    mark.printStats(stats, DESERIALIZE, name);}
0
public void benchmark()
{    mark.printStats(mark.getRunner().benchmark(new BenchmarkFn() {        @Override        public Boolean apply(Integer i) {            Vector v = mark.vectors[0][mark.vIndex(i)].times(mark.vectors[0][mark.vIndex(randIndex())]);            return depends(v);        }    }), TIMES, DENSE_VECTOR);    mark.printStats(mark.getRunner().benchmark(new BenchmarkFn() {        @Override        public Boolean apply(Integer i) {            Vector v = mark.vectors[1][mark.vIndex(i)].times(mark.vectors[1][mark.vIndex(randIndex())]);            return depends(v);        }    }), TIMES, RAND_SPARSE_VECTOR);    mark.printStats(mark.getRunner().benchmark(new BenchmarkFn() {        @Override        public Boolean apply(Integer i) {            Vector v = mark.vectors[2][mark.vIndex(i)].times(mark.vectors[2][mark.vIndex(randIndex())]);            return depends(v);        }    }), TIMES, SEQ_SPARSE_VECTOR);    mark.printStats(mark.getRunner().benchmark(new BenchmarkFn() {        @Override        public Boolean apply(Integer i) {            Vector v = mark.vectors[0][mark.vIndex(i)].times(mark.vectors[1][mark.vIndex(randIndex())]);            return depends(v);        }    }), TIMES, DENSE_FN_RAND);    mark.printStats(mark.getRunner().benchmark(new BenchmarkFn() {        @Override        public Boolean apply(Integer i) {            Vector v = mark.vectors[0][mark.vIndex(i)].times(mark.vectors[2][mark.vIndex(randIndex())]);            return depends(v);        }    }), TIMES, DENSE_FN_SEQ);    mark.printStats(mark.getRunner().benchmark(new BenchmarkFn() {        @Override        public Boolean apply(Integer i) {            Vector v = mark.vectors[1][mark.vIndex(i)].times(mark.vectors[0][mark.vIndex(randIndex())]);            return depends(v);        }    }), TIMES, RAND_FN_DENSE);    mark.printStats(mark.getRunner().benchmark(new BenchmarkFn() {        @Override        public Boolean apply(Integer i) {            Vector v = mark.vectors[1][mark.vIndex(i)].times(mark.vectors[2][mark.vIndex(randIndex())]);            return depends(v);        }    }), TIMES, RAND_FN_SEQ);    mark.printStats(mark.getRunner().benchmark(new BenchmarkFn() {        @Override        public Boolean apply(Integer i) {            Vector v = mark.vectors[2][mark.vIndex(i)].times(mark.vectors[0][mark.vIndex(randIndex())]);            return depends(v);        }    }), TIMES, SEQ_FN_DENSE);    mark.printStats(mark.getRunner().benchmark(new BenchmarkFn() {        @Override        public Boolean apply(Integer i) {            Vector v = mark.vectors[2][mark.vIndex(i)].times(mark.vectors[1][mark.vIndex(randIndex())]);            return depends(v);        }    }), TIMES, SEQ_FN_RAND);}
0
public Boolean apply(Integer i)
{    Vector v = mark.vectors[0][mark.vIndex(i)].times(mark.vectors[0][mark.vIndex(randIndex())]);    return depends(v);}
0
public Boolean apply(Integer i)
{    Vector v = mark.vectors[1][mark.vIndex(i)].times(mark.vectors[1][mark.vIndex(randIndex())]);    return depends(v);}
0
public Boolean apply(Integer i)
{    Vector v = mark.vectors[2][mark.vIndex(i)].times(mark.vectors[2][mark.vIndex(randIndex())]);    return depends(v);}
0
public Boolean apply(Integer i)
{    Vector v = mark.vectors[0][mark.vIndex(i)].times(mark.vectors[1][mark.vIndex(randIndex())]);    return depends(v);}
0
public Boolean apply(Integer i)
{    Vector v = mark.vectors[0][mark.vIndex(i)].times(mark.vectors[2][mark.vIndex(randIndex())]);    return depends(v);}
0
public Boolean apply(Integer i)
{    Vector v = mark.vectors[1][mark.vIndex(i)].times(mark.vectors[0][mark.vIndex(randIndex())]);    return depends(v);}
0
public Boolean apply(Integer i)
{    Vector v = mark.vectors[1][mark.vIndex(i)].times(mark.vectors[2][mark.vIndex(randIndex())]);    return depends(v);}
0
public Boolean apply(Integer i)
{    Vector v = mark.vectors[2][mark.vIndex(i)].times(mark.vectors[0][mark.vIndex(randIndex())]);    return depends(v);}
0
public Boolean apply(Integer i)
{    Vector v = mark.vectors[2][mark.vIndex(i)].times(mark.vectors[1][mark.vIndex(randIndex())]);    return depends(v);}
0
private void setUpVectors(int cardinality, int numNonZeros, int numVectors)
{    for (int i = 0; i < numVectors; i++) {                Vector v = new SequentialAccessSparseVector(cardinality, numNonZeros);        BitSet featureSpace = new BitSet(cardinality);        int[] indexes = new int[numNonZeros];        double[] values = new double[numNonZeros];        int j = 0;        while (j < numNonZeros) {            double value = r.nextGaussian();            int index = r.nextInt(cardinality);            if (!featureSpace.get(index) && value != 0) {                featureSpace.set(index);                indexes[j] = index;                values[j++] = value;                v.set(index, value);            }        }        randomVectorIndices.add(indexes);        randomVectorValues.add(values);        randomVectors.add(v);    }}
0
 void printStats(TimingStatistics stats, String benchmarkName, String implName, String content)
{    printStats(stats, benchmarkName, implName, content, 1);}
0
 void printStats(TimingStatistics stats, String benchmarkName, String implName)
{    printStats(stats, benchmarkName, implName, "", 1);}
0
private void printStats(TimingStatistics stats, String benchmarkName, String implName, String content, int multiplier)
{    float speed = multiplier * stats.getNCalls() * (numNonZeros * 1000.0f * 12 / stats.getSumTime());    float opsPerSec = stats.getNCalls() * 1000000000.0f / stats.getSumTime();        if (!implType.containsKey(implName)) {        implType.put(implName, implType.size());    }    int implId = implType.get(implName);    if (!statsMap.containsKey(benchmarkName)) {        statsMap.put(benchmarkName, new ArrayList<String[]>());    }    List<String[]> implStats = statsMap.get(benchmarkName);    while (implStats.size() < implId + 1) {        implStats.add(EMPTY);    }    implStats.set(implId, TAB_NEWLINE_PATTERN.split(stats + "\tSpeed  = " + DF.format(opsPerSec) + " /sec\tRate   = " + DF.format(speed) + " MB/s"));}
1
public void createData()
{    for (int i = 0; i < Math.max(numVectors, numClusters); ++i) {        vectors[0][vIndex(i)] = new DenseVector(randomVectors.get(vIndex(i)));        vectors[1][vIndex(i)] = new RandomAccessSparseVector(randomVectors.get(vIndex(i)));        vectors[2][vIndex(i)] = new SequentialAccessSparseVector(randomVectors.get(vIndex(i)));        if (numClusters > 0) {            clusters[cIndex(i)] = new RandomAccessSparseVector(randomVectors.get(vIndex(i)));        }    }}
0
public void createBenchmark()
{    printStats(runner.benchmark(new BenchmarkFn() {        @Override        public Boolean apply(Integer i) {            vectors[0][vIndex(i)] = new DenseVector(randomVectors.get(vIndex(i)));            return depends(vectors[0][vIndex(i)]);        }    }), CREATE_COPY, DENSE_VECTOR);    printStats(runner.benchmark(new BenchmarkFn() {        @Override        public Boolean apply(Integer i) {            vectors[1][vIndex(i)] = new RandomAccessSparseVector(randomVectors.get(vIndex(i)));            return depends(vectors[1][vIndex(i)]);        }    }), CREATE_COPY, RAND_SPARSE_VECTOR);    printStats(runner.benchmark(new BenchmarkFn() {        @Override        public Boolean apply(Integer i) {            vectors[2][vIndex(i)] = new SequentialAccessSparseVector(randomVectors.get(vIndex(i)));            return depends(vectors[2][vIndex(i)]);        }    }), CREATE_COPY, SEQ_SPARSE_VECTOR);    if (numClusters > 0) {        printStats(runner.benchmark(new BenchmarkFn() {            @Override            public Boolean apply(Integer i) {                clusters[cIndex(i)] = new RandomAccessSparseVector(randomVectors.get(vIndex(i)));                return depends(clusters[cIndex(i)]);            }        }), CREATE_COPY, CLUSTERS);    }}
0
public Boolean apply(Integer i)
{    vectors[0][vIndex(i)] = new DenseVector(randomVectors.get(vIndex(i)));    return depends(vectors[0][vIndex(i)]);}
0
public Boolean apply(Integer i)
{    vectors[1][vIndex(i)] = new RandomAccessSparseVector(randomVectors.get(vIndex(i)));    return depends(vectors[1][vIndex(i)]);}
0
public Boolean apply(Integer i)
{    vectors[2][vIndex(i)] = new SequentialAccessSparseVector(randomVectors.get(vIndex(i)));    return depends(vectors[2][vIndex(i)]);}
0
public Boolean apply(Integer i)
{    clusters[cIndex(i)] = new RandomAccessSparseVector(randomVectors.get(vIndex(i)));    return depends(clusters[cIndex(i)]);}
0
private boolean buildVectorIncrementally(TimingStatistics stats, int randomIndex, Vector v, boolean useSetQuick)
{    int[] indexes = randomVectorIndices.get(randomIndex);    double[] values = randomVectorValues.get(randomIndex);    List<Integer> randomOrder = new ArrayList<>();    for (int i = 0; i < indexes.length; i++) {        randomOrder.add(i);    }    Collections.shuffle(randomOrder);    int[] permutation = new int[randomOrder.size()];    for (int i = 0; i < randomOrder.size(); i++) {        permutation[i] = randomOrder.get(i);    }    TimingStatistics.Call call = stats.newCall(leadTimeUsec);    if (useSetQuick) {        for (int i : permutation) {            v.setQuick(indexes[i], values[i]);        }    } else {        for (int i : permutation) {            v.set(indexes[i], values[i]);        }    }    return call.end(maxTimeUsec);}
0
public void incrementalCreateBenchmark()
{    TimingStatistics stats = new TimingStatistics();    for (int i = 0; i < loop; i++) {        vectors[0][vIndex(i)] = new DenseVector(cardinality);        if (buildVectorIncrementally(stats, vIndex(i), vectors[0][vIndex(i)], false)) {            break;        }    }    printStats(stats, CREATE_INCREMENTALLY, DENSE_VECTOR);    stats = new TimingStatistics();    for (int i = 0; i < loop; i++) {        vectors[1][vIndex(i)] = new RandomAccessSparseVector(cardinality);        if (buildVectorIncrementally(stats, vIndex(i), vectors[1][vIndex(i)], false)) {            break;        }    }    printStats(stats, CREATE_INCREMENTALLY, RAND_SPARSE_VECTOR);    stats = new TimingStatistics();    for (int i = 0; i < loop; i++) {        vectors[2][vIndex(i)] = new SequentialAccessSparseVector(cardinality);        if (buildVectorIncrementally(stats, vIndex(i), vectors[2][vIndex(i)], false)) {            break;        }    }    printStats(stats, CREATE_INCREMENTALLY, SEQ_SPARSE_VECTOR);    if (numClusters > 0) {        stats = new TimingStatistics();        for (int i = 0; i < loop; i++) {            clusters[cIndex(i)] = new RandomAccessSparseVector(cardinality);            if (buildVectorIncrementally(stats, vIndex(i), clusters[cIndex(i)], false)) {                break;            }        }        printStats(stats, CREATE_INCREMENTALLY, CLUSTERS);    }}
0
public int vIndex(int i)
{    return i % numVectors;}
0
public int cIndex(int i)
{    return i % numClusters;}
0
public static void main(String[] args) throws IOException
{    DefaultOptionBuilder obuilder = new DefaultOptionBuilder();    ArgumentBuilder abuilder = new ArgumentBuilder();    GroupBuilder gbuilder = new GroupBuilder();    Option vectorSizeOpt = obuilder.withLongName("vectorSize").withRequired(false).withArgument(abuilder.withName("vs").withDefault(1000000).create()).withDescription("Cardinality of the vector. Default: 1000000").withShortName("vs").create();    Option numNonZeroOpt = obuilder.withLongName("numNonZero").withRequired(false).withArgument(abuilder.withName("nz").withDefault(1000).create()).withDescription("Size of the vector. Default: 1000").withShortName("nz").create();    Option numVectorsOpt = obuilder.withLongName("numVectors").withRequired(false).withArgument(abuilder.withName("nv").withDefault(25).create()).withDescription("Number of Vectors to create. Default: 25").withShortName("nv").create();    Option numClustersOpt = obuilder.withLongName("numClusters").withRequired(false).withArgument(abuilder.withName("nc").withDefault(0).create()).withDescription("Number of clusters to create. Set to non zero to run cluster benchmark. Default: 0").withShortName("nc").create();    Option numOpsOpt = obuilder.withLongName("numOps").withRequired(false).withArgument(abuilder.withName("numOps").withDefault(10).create()).withDescription("Number of operations to do per timer. " + "E.g In distance measure, the distance is calculated numOps times" + " and the total time is measured. Default: 10").withShortName("no").create();    Option helpOpt = DefaultOptionCreator.helpOption();    Group group = gbuilder.withName("Options").withOption(vectorSizeOpt).withOption(numNonZeroOpt).withOption(numVectorsOpt).withOption(numOpsOpt).withOption(numClustersOpt).withOption(helpOpt).create();    try {        Parser parser = new Parser();        parser.setGroup(group);        CommandLine cmdLine = parser.parse(args);        if (cmdLine.hasOption(helpOpt)) {            CommandLineUtil.printHelpWithGenericOptions(group);            return;        }        int cardinality = 1000000;        if (cmdLine.hasOption(vectorSizeOpt)) {            cardinality = Integer.parseInt((String) cmdLine.getValue(vectorSizeOpt));        }        int numClusters = 0;        if (cmdLine.hasOption(numClustersOpt)) {            numClusters = Integer.parseInt((String) cmdLine.getValue(numClustersOpt));        }        int numNonZero = 1000;        if (cmdLine.hasOption(numNonZeroOpt)) {            numNonZero = Integer.parseInt((String) cmdLine.getValue(numNonZeroOpt));        }        int numVectors = 25;        if (cmdLine.hasOption(numVectorsOpt)) {            numVectors = Integer.parseInt((String) cmdLine.getValue(numVectorsOpt));        }        int numOps = 10;        if (cmdLine.hasOption(numOpsOpt)) {            numOps = Integer.parseInt((String) cmdLine.getValue(numOpsOpt));        }        VectorBenchmarks mark = new VectorBenchmarks(cardinality, numNonZero, numVectors, numClusters, numOps);        runBenchmark(mark);                    } catch (OptionException e) {        CommandLineUtil.printHelp(group);    }}
1
private static void runBenchmark(VectorBenchmarks mark) throws IOException
{        mark.createData();    mark.createBenchmark();    if (mark.cardinality < 200000) {                mark.incrementalCreateBenchmark();    }    new CloneBenchmark(mark).benchmark();    new DotBenchmark(mark).benchmark();    new PlusBenchmark(mark).benchmark();    new MinusBenchmark(mark).benchmark();    new TimesBenchmark(mark).benchmark();    new SerializationBenchmark(mark).benchmark();    DistanceBenchmark distanceBenchmark = new DistanceBenchmark(mark);    distanceBenchmark.benchmark(new CosineDistanceMeasure());    distanceBenchmark.benchmark(new SquaredEuclideanDistanceMeasure());    distanceBenchmark.benchmark(new EuclideanDistanceMeasure());    distanceBenchmark.benchmark(new ManhattanDistanceMeasure());    distanceBenchmark.benchmark(new TanimotoDistanceMeasure());    distanceBenchmark.benchmark(new ChebyshevDistanceMeasure());    distanceBenchmark.benchmark(new MinkowskiDistanceMeasure());    if (mark.numClusters > 0) {        ClosestCentroidBenchmark centroidBenchmark = new ClosestCentroidBenchmark(mark);        centroidBenchmark.benchmark(new CosineDistanceMeasure());        centroidBenchmark.benchmark(new SquaredEuclideanDistanceMeasure());        centroidBenchmark.benchmark(new EuclideanDistanceMeasure());        centroidBenchmark.benchmark(new ManhattanDistanceMeasure());        centroidBenchmark.benchmark(new TanimotoDistanceMeasure());        centroidBenchmark.benchmark(new ChebyshevDistanceMeasure());        centroidBenchmark.benchmark(new MinkowskiDistanceMeasure());    }}
0
private String asCsvString()
{    List<String> keys = new ArrayList<>(statsMap.keySet());    Collections.sort(keys);    Map<Integer, String> implMap = new HashMap<>();    for (Entry<String, Integer> e : implType.entrySet()) {        implMap.put(e.getValue(), e.getKey());    }    StringBuilder sb = new StringBuilder(1000);    for (String benchmarkName : keys) {        int i = 0;        for (String[] stats : statsMap.get(benchmarkName)) {            if (stats.length < 8) {                continue;            }            sb.append(benchmarkName).append(',');            sb.append(implMap.get(i++)).append(',');            sb.append(stats[7].trim().split("=|/")[1].trim());            sb.append('\n');        }    }    sb.append('\n');    return sb.toString();}
0
public String toString()
{    int pad = 24;    StringBuilder sb = new StringBuilder(1000);    sb.append(StringUtils.rightPad("BenchMarks", pad));    for (int i = 0; i < implType.size(); i++) {        for (Entry<String, Integer> e : implType.entrySet()) {            if (e.getValue() == i) {                sb.append(StringUtils.rightPad(e.getKey(), pad).substring(0, pad));                break;            }        }    }    sb.append('\n');    List<String> keys = new ArrayList<>(statsMap.keySet());    Collections.sort(keys);    for (String benchmarkName : keys) {        List<String[]> implTokenizedStats = statsMap.get(benchmarkName);        int maxStats = 0;        for (String[] stat : implTokenizedStats) {            maxStats = Math.max(maxStats, stat.length);        }        for (int i = 0; i < maxStats; i++) {            boolean printedName = false;            for (String[] stats : implTokenizedStats) {                if (i == 0 && !printedName) {                    sb.append(StringUtils.rightPad(benchmarkName, pad));                    printedName = true;                } else if (!printedName) {                    printedName = true;                    sb.append(StringUtils.rightPad("", pad));                }                if (stats.length > i) {                    sb.append(StringUtils.rightPad(stats[i], pad));                } else {                    sb.append(StringUtils.rightPad("", pad));                }            }            sb.append('\n');        }        sb.append('\n');    }    return sb.toString();}
0
public BenchmarkRunner getRunner()
{    return runner;}
0
public LongPrimitiveIterator getUserIDs()
{    SliceQuery<Long, Long, ?> query = buildNoValueSliceQuery(USER_IDS_CF);    query.setKey(ID_ROW_KEY);    FastIDSet userIDs = new FastIDSet();    for (HColumn<Long, ?> userIDColumn : query.execute().get().getColumns()) {        userIDs.add(userIDColumn.getName());    }    return userIDs.iterator();}
0
public PreferenceArray getPreferencesFromUser(long userID) throws TasteException
{    return userCache.get(userID);}
0
public FastIDSet getItemIDsFromUser(long userID) throws TasteException
{    return itemIDsFromUserCache.get(userID);}
0
public LongPrimitiveIterator getItemIDs()
{    SliceQuery<Long, Long, ?> query = buildNoValueSliceQuery(ITEM_IDS_CF);    query.setKey(ID_ROW_KEY);    FastIDSet itemIDs = new FastIDSet();    for (HColumn<Long, ?> itemIDColumn : query.execute().get().getColumns()) {        itemIDs.add(itemIDColumn.getName());    }    return itemIDs.iterator();}
0
public PreferenceArray getPreferencesForItem(long itemID) throws TasteException
{    return itemCache.get(itemID);}
0
public Float getPreferenceValue(long userID, long itemID)
{    ColumnQuery<Long, Long, Float> query = HFactory.createColumnQuery(keyspace, LongSerializer.get(), LongSerializer.get(), FloatSerializer.get());    query.setColumnFamily(USERS_CF);    query.setKey(userID);    query.setName(itemID);    HColumn<Long, Float> column = query.execute().get();    return column == null ? null : column.getValue();}
0
public Long getPreferenceTime(long userID, long itemID)
{    ColumnQuery<Long, Long, ?> query = HFactory.createColumnQuery(keyspace, LongSerializer.get(), LongSerializer.get(), BytesArraySerializer.get());    query.setColumnFamily(USERS_CF);    query.setKey(userID);    query.setName(itemID);    HColumn<Long, ?> result = query.execute().get();    return result == null ? null : result.getClock();}
0
public int getNumItems()
{    Integer itemCount = itemCountCache.get();    if (itemCount == null) {        CountQuery<Long, Long> countQuery = HFactory.createCountQuery(keyspace, LongSerializer.get(), LongSerializer.get());        countQuery.setKey(ID_ROW_KEY);        countQuery.setColumnFamily(ITEM_IDS_CF);        countQuery.setRange(null, null, Integer.MAX_VALUE);        itemCount = countQuery.execute().get();        itemCountCache.set(itemCount);    }    return itemCount;}
0
public int getNumUsers()
{    Integer userCount = userCountCache.get();    if (userCount == null) {        CountQuery<Long, Long> countQuery = HFactory.createCountQuery(keyspace, LongSerializer.get(), LongSerializer.get());        countQuery.setKey(ID_ROW_KEY);        countQuery.setColumnFamily(USER_IDS_CF);        countQuery.setRange(null, null, Integer.MAX_VALUE);        userCount = countQuery.execute().get();        userCountCache.set(userCount);    }    return userCount;}
0
public int getNumUsersWithPreferenceFor(long itemID) throws TasteException
{    /*    CountQuery<Long,Long> query = HFactory.createCountQuery(keyspace, LongSerializer.get(), LongSerializer.get());    query.setColumnFamily(ITEMS_CF);    query.setKey(itemID);    query.setRange(null, null, Integer.MAX_VALUE);    return query.execute().get();     */    return userIDsFromItemCache.get(itemID).size();}
0
public int getNumUsersWithPreferenceFor(long itemID1, long itemID2) throws TasteException
{    FastIDSet userIDs1 = userIDsFromItemCache.get(itemID1);    FastIDSet userIDs2 = userIDsFromItemCache.get(itemID2);    return userIDs1.size() < userIDs2.size() ? userIDs2.intersectionSize(userIDs1) : userIDs1.intersectionSize(userIDs2);}
0
public void setPreference(long userID, long itemID, float value)
{    if (Float.isNaN(value)) {        value = 1.0f;    }    long now = System.currentTimeMillis();    Mutator<Long> mutator = HFactory.createMutator(keyspace, LongSerializer.get());    HColumn<Long, Float> itemForUsers = new HColumnImpl<>(LongSerializer.get(), FloatSerializer.get());    itemForUsers.setName(itemID);    itemForUsers.setClock(now);    itemForUsers.setValue(value);    mutator.addInsertion(userID, USERS_CF, itemForUsers);    HColumn<Long, Float> userForItems = new HColumnImpl<>(LongSerializer.get(), FloatSerializer.get());    userForItems.setName(userID);    userForItems.setClock(now);    userForItems.setValue(value);    mutator.addInsertion(itemID, ITEMS_CF, userForItems);    HColumn<Long, byte[]> userIDs = new HColumnImpl<>(LongSerializer.get(), BytesArraySerializer.get());    userIDs.setName(userID);    userIDs.setClock(now);    userIDs.setValue(EMPTY);    mutator.addInsertion(ID_ROW_KEY, USER_IDS_CF, userIDs);    HColumn<Long, byte[]> itemIDs = new HColumnImpl<>(LongSerializer.get(), BytesArraySerializer.get());    itemIDs.setName(itemID);    itemIDs.setClock(now);    itemIDs.setValue(EMPTY);    mutator.addInsertion(ID_ROW_KEY, ITEM_IDS_CF, itemIDs);    mutator.execute();}
0
public void removePreference(long userID, long itemID)
{    Mutator<Long> mutator = HFactory.createMutator(keyspace, LongSerializer.get());    mutator.addDeletion(userID, USERS_CF, itemID, LongSerializer.get());    mutator.addDeletion(itemID, ITEMS_CF, userID, LongSerializer.get());    mutator.execute();}
0
public boolean hasPreferenceValues()
{    return true;}
0
public float getMaxPreference()
{    return Float.NaN;}
0
public float getMinPreference()
{    return Float.NaN;}
0
public void refresh(Collection<Refreshable> alreadyRefreshed)
{    userCache.clear();    itemCache.clear();    userIDsFromItemCache.clear();    itemIDsFromUserCache.clear();    userCountCache.set(null);    itemCountCache.set(null);}
0
public String toString()
{    return "CassandraDataModel[" + keyspace + ']';}
0
public void close()
{    HFactory.shutdownCluster(cluster);}
0
private SliceQuery<Long, Long, byte[]> buildNoValueSliceQuery(String cf)
{    SliceQuery<Long, Long, byte[]> query = HFactory.createSliceQuery(keyspace, LongSerializer.get(), LongSerializer.get(), BytesArraySerializer.get());    query.setColumnFamily(cf);    query.setRange(null, null, false, Integer.MAX_VALUE);    return query;}
0
private SliceQuery<Long, Long, Float> buildValueSliceQuery(String cf)
{    SliceQuery<Long, Long, Float> query = HFactory.createSliceQuery(keyspace, LongSerializer.get(), LongSerializer.get(), FloatSerializer.get());    query.setColumnFamily(cf);    query.setRange(null, null, false, Integer.MAX_VALUE);    return query;}
0
public HConsistencyLevel get(OperationType op)
{    return HConsistencyLevel.ONE;}
0
public HConsistencyLevel get(OperationType op, String cfName)
{    return HConsistencyLevel.ONE;}
0
public PreferenceArray get(Long userID) throws TasteException
{    SliceQuery<Long, Long, Float> query = buildValueSliceQuery(USERS_CF);    query.setKey(userID);    ColumnSlice<Long, Float> result = query.execute().get();    if (result == null) {        throw new NoSuchUserException(userID);    }    List<HColumn<Long, Float>> itemIDColumns = result.getColumns();    if (itemIDColumns.isEmpty()) {        throw new NoSuchUserException(userID);    }    int size = itemIDColumns.size();    PreferenceArray prefs = new GenericUserPreferenceArray(size);    prefs.setUserID(0, userID);    for (int i = 0; i < size; i++) {        HColumn<Long, Float> itemIDColumn = itemIDColumns.get(i);        prefs.setItemID(i, itemIDColumn.getName());        prefs.setValue(i, itemIDColumn.getValue());    }    return prefs;}
0
public PreferenceArray get(Long itemID) throws TasteException
{    SliceQuery<Long, Long, Float> query = buildValueSliceQuery(ITEMS_CF);    query.setKey(itemID);    ColumnSlice<Long, Float> result = query.execute().get();    if (result == null) {        throw new NoSuchItemException(itemID);    }    List<HColumn<Long, Float>> userIDColumns = result.getColumns();    if (userIDColumns.isEmpty()) {        throw new NoSuchItemException(itemID);    }    int size = userIDColumns.size();    PreferenceArray prefs = new GenericItemPreferenceArray(size);    prefs.setItemID(0, itemID);    for (int i = 0; i < size; i++) {        HColumn<Long, Float> userIDColumn = userIDColumns.get(i);        prefs.setUserID(i, userIDColumn.getName());        prefs.setValue(i, userIDColumn.getValue());    }    return prefs;}
0
public FastIDSet get(Long itemID) throws TasteException
{    SliceQuery<Long, Long, byte[]> query = buildNoValueSliceQuery(ITEMS_CF);    query.setKey(itemID);    ColumnSlice<Long, byte[]> result = query.execute().get();    if (result == null) {        throw new NoSuchItemException(itemID);    }    List<HColumn<Long, byte[]>> columns = result.getColumns();    FastIDSet userIDs = new FastIDSet(columns.size());    for (HColumn<Long, ?> userIDColumn : columns) {        userIDs.add(userIDColumn.getName());    }    return userIDs;}
0
public FastIDSet get(Long userID) throws TasteException
{    SliceQuery<Long, Long, byte[]> query = buildNoValueSliceQuery(USERS_CF);    query.setKey(userID);    FastIDSet itemIDs = new FastIDSet();    ColumnSlice<Long, byte[]> result = query.execute().get();    if (result == null) {        throw new NoSuchUserException(userID);    }    List<HColumn<Long, byte[]>> columns = result.getColumns();    if (columns.isEmpty()) {        throw new NoSuchUserException(userID);    }    for (HColumn<Long, ?> itemIDColumn : columns) {        itemIDs.add(itemIDColumn.getName());    }    return itemIDs;}
0
public String getTableName()
{    return tableName;}
0
private void bootstrap(Configuration conf) throws IOException
{    HTableDescriptor tDesc = new HTableDescriptor(Bytes.toBytes(tableName));    tDesc.addFamily(new HColumnDescriptor(USERS_CF));    tDesc.addFamily(new HColumnDescriptor(ITEMS_CF));    try (HBaseAdmin admin = new HBaseAdmin(conf)) {        admin.createTable(tDesc);            }}
1
private static byte[] userToBytes(long userID)
{    ByteBuffer bb = ByteBuffer.allocate(9);        bb.put((byte) 0x75);    bb.putLong(userID);    return bb.array();}
0
private static byte[] itemToBytes(long itemID)
{    ByteBuffer bb = ByteBuffer.allocate(9);        bb.put((byte) 0x69);    bb.putLong(itemID);    return bb.array();}
0
private static long bytesToUserOrItemID(byte[] ba)
{    ByteBuffer bb = ByteBuffer.wrap(ba);    return bb.getLong(1);}
0
public LongPrimitiveIterator getUserIDs()
{    return userIDs.iterator();}
0
public PreferenceArray getPreferencesFromUser(long userID) throws TasteException
{    Result result;    try {        HTableInterface table = pool.getTable(tableName);        Get get = new Get(userToBytes(userID));        get.addFamily(ITEMS_CF);        result = table.get(get);        table.close();    } catch (IOException e) {        throw new TasteException("Failed to retrieve user preferences from HBase", e);    }    if (result.isEmpty()) {        throw new NoSuchUserException(userID);    }    SortedMap<byte[], byte[]> families = result.getFamilyMap(ITEMS_CF);    PreferenceArray prefs = new GenericUserPreferenceArray(families.size());    prefs.setUserID(0, userID);    int i = 0;    for (Map.Entry<byte[], byte[]> entry : families.entrySet()) {        prefs.setItemID(i, Bytes.toLong(entry.getKey()));        prefs.setValue(i, Bytes.toFloat(entry.getValue()));        i++;    }    return prefs;}
0
public FastIDSet getItemIDsFromUser(long userID) throws TasteException
{    Result result;    try {        HTableInterface table = pool.getTable(tableName);        Get get = new Get(userToBytes(userID));        get.addFamily(ITEMS_CF);        result = table.get(get);        table.close();    } catch (IOException e) {        throw new TasteException("Failed to retrieve item IDs from HBase", e);    }    if (result.isEmpty()) {        throw new NoSuchUserException(userID);    }    SortedMap<byte[], byte[]> families = result.getFamilyMap(ITEMS_CF);    FastIDSet ids = new FastIDSet(families.size());    for (byte[] family : families.keySet()) {        ids.add(Bytes.toLong(family));    }    return ids;}
0
public LongPrimitiveIterator getItemIDs()
{    return itemIDs.iterator();}
0
public PreferenceArray getPreferencesForItem(long itemID) throws TasteException
{    Result result;    try {        HTableInterface table = pool.getTable(tableName);        Get get = new Get(itemToBytes(itemID));        get.addFamily(USERS_CF);        result = table.get(get);        table.close();    } catch (IOException e) {        throw new TasteException("Failed to retrieve item preferences from HBase", e);    }    if (result.isEmpty()) {        throw new NoSuchItemException(itemID);    }    SortedMap<byte[], byte[]> families = result.getFamilyMap(USERS_CF);    PreferenceArray prefs = new GenericItemPreferenceArray(families.size());    prefs.setItemID(0, itemID);    int i = 0;    for (Map.Entry<byte[], byte[]> entry : families.entrySet()) {        prefs.setUserID(i, Bytes.toLong(entry.getKey()));        prefs.setValue(i, Bytes.toFloat(entry.getValue()));        i++;    }    return prefs;}
0
public Float getPreferenceValue(long userID, long itemID) throws TasteException
{    Result result;    try {        HTableInterface table = pool.getTable(tableName);        Get get = new Get(userToBytes(userID));        get.addColumn(ITEMS_CF, Bytes.toBytes(itemID));        result = table.get(get);        table.close();    } catch (IOException e) {        throw new TasteException("Failed to retrieve user preferences from HBase", e);    }    if (result.isEmpty()) {        throw new NoSuchUserException(userID);    }    if (result.containsColumn(ITEMS_CF, Bytes.toBytes(itemID))) {        return Bytes.toFloat(result.getValue(ITEMS_CF, Bytes.toBytes(itemID)));    } else {        return null;    }}
0
public Long getPreferenceTime(long userID, long itemID) throws TasteException
{    Result result;    try {        HTableInterface table = pool.getTable(tableName);        Get get = new Get(userToBytes(userID));        get.addColumn(ITEMS_CF, Bytes.toBytes(itemID));        result = table.get(get);        table.close();    } catch (IOException e) {        throw new TasteException("Failed to retrieve user preferences from HBase", e);    }    if (result.isEmpty()) {        throw new NoSuchUserException(userID);    }    if (result.containsColumn(ITEMS_CF, Bytes.toBytes(itemID))) {        KeyValue kv = result.getColumnLatest(ITEMS_CF, Bytes.toBytes(itemID));        return kv.getTimestamp();    } else {        return null;    }}
0
public int getNumItems()
{    return itemIDs.size();}
0
public int getNumUsers()
{    return userIDs.size();}
0
public int getNumUsersWithPreferenceFor(long itemID) throws TasteException
{    PreferenceArray prefs = getPreferencesForItem(itemID);    return prefs.length();}
0
public int getNumUsersWithPreferenceFor(long itemID1, long itemID2) throws TasteException
{    Result[] results;    try {        HTableInterface table = pool.getTable(tableName);        List<Get> gets = new ArrayList<>(2);        gets.add(new Get(itemToBytes(itemID1)));        gets.add(new Get(itemToBytes(itemID2)));        gets.get(0).addFamily(USERS_CF);        gets.get(1).addFamily(USERS_CF);        results = table.get(gets);        table.close();    } catch (IOException e) {        throw new TasteException("Failed to retrieve item preferences from HBase", e);    }    if (results[0].isEmpty()) {        throw new NoSuchItemException(itemID1);    }    if (results[1].isEmpty()) {        throw new NoSuchItemException(itemID2);    }        Result result = results[0];    SortedMap<byte[], byte[]> families = result.getFamilyMap(USERS_CF);    FastIDSet idSet1 = new FastIDSet(families.size());    for (byte[] id : families.keySet()) {        idSet1.add(Bytes.toLong(id));    }        result = results[1];    families = result.getFamilyMap(USERS_CF);    FastIDSet idSet2 = new FastIDSet(families.size());    for (byte[] id : families.keySet()) {        idSet2.add(Bytes.toLong(id));    }    return idSet1.intersectionSize(idSet2);}
0
public void setPreference(long userID, long itemID, float value) throws TasteException
{    try {        HTableInterface table = pool.getTable(tableName);        List<Put> puts = new ArrayList<>(2);        puts.add(new Put(userToBytes(userID)));        puts.add(new Put(itemToBytes(itemID)));        puts.get(0).add(ITEMS_CF, Bytes.toBytes(itemID), Bytes.toBytes(value));        puts.get(1).add(USERS_CF, Bytes.toBytes(userID), Bytes.toBytes(value));        table.put(puts);        table.close();    } catch (IOException e) {        throw new TasteException("Failed to store preference in HBase", e);    }}
0
public void removePreference(long userID, long itemID) throws TasteException
{    try {        HTableInterface table = pool.getTable(tableName);        List<Delete> deletes = new ArrayList<>(2);        deletes.add(new Delete(userToBytes(userID)));        deletes.add(new Delete(itemToBytes(itemID)));        deletes.get(0).deleteColumns(ITEMS_CF, Bytes.toBytes(itemID));        deletes.get(1).deleteColumns(USERS_CF, Bytes.toBytes(userID));        table.delete(deletes);        table.close();    } catch (IOException e) {        throw new TasteException("Failed to remove preference from HBase", e);    }}
0
public boolean hasPreferenceValues()
{    return true;}
0
public float getMaxPreference()
{    throw new UnsupportedOperationException();}
0
public float getMinPreference()
{    throw new UnsupportedOperationException();}
0
public void close() throws IOException
{    pool.close();}
0
public void refresh(Collection<Refreshable> alreadyRefreshed)
{    if (alreadyRefreshed == null || !alreadyRefreshed.contains(this)) {        try {                        long t1 = System.currentTimeMillis();            refreshItemIDs();            refreshUserIDs();            long t2 = System.currentTimeMillis();                    } catch (IOException e) {            throw new IllegalStateException("Could not reload DataModel", e);        }    }}
1
private synchronized void refreshItemIDs() throws IOException
{        HTableInterface table = pool.getTable(tableName);    Scan scan = new Scan(new byte[] { 0x69 }, new byte[] { 0x70 });    scan.setFilter(new FilterList(FilterList.Operator.MUST_PASS_ALL, new KeyOnlyFilter(), new FirstKeyOnlyFilter()));    ResultScanner scanner = table.getScanner(scan);    Collection<Long> ids = new LinkedList<>();    for (Result result : scanner) {        ids.add(bytesToUserOrItemID(result.getRow()));    }    table.close();        FastIDSet itemIDs = new FastIDSet(ids.size());    for (long l : ids) {        itemIDs.add(l);    }        this.itemIDs = itemIDs;}
0
private synchronized void refreshUserIDs() throws IOException
{        HTableInterface table = pool.getTable(tableName);    Scan scan = new Scan(new byte[] { 0x75 }, new byte[] { 0x76 });    scan.setFilter(new FilterList(FilterList.Operator.MUST_PASS_ALL, new KeyOnlyFilter(), new FirstKeyOnlyFilter()));    ResultScanner scanner = table.getScanner(scan);    Collection<Long> ids = new LinkedList<>();    for (Result result : scanner) {        ids.add(bytesToUserOrItemID(result.getRow()));    }    table.close();        FastIDSet userIDs = new FastIDSet(ids.size());    for (long l : ids) {        userIDs.add(l);    }        this.userIDs = userIDs;}
0
protected Preference buildPreference(ResultSet rs) throws SQLException
{    return new BooleanPreference(getLongColumn(rs, 1), getLongColumn(rs, 2));}
0
 String getSetPreferenceSQL()
{    return setPreferenceSQL;}
0
public void setPreference(long userID, long itemID, float value) throws TasteException
{    Preconditions.checkArgument(!Float.isNaN(value), "NaN value");        Connection conn = null;    PreparedStatement stmt = null;    try {        conn = getDataSource().getConnection();        stmt = conn.prepareStatement(setPreferenceSQL);        setLongParameter(stmt, 1, userID);        setLongParameter(stmt, 2, itemID);                stmt.executeUpdate();    } catch (SQLException sqle) {                throw new TasteException(sqle);    } finally {        IOUtils.quietClose(null, stmt, conn);    }}
1
public boolean hasPreferenceValues()
{    return false;}
0
public float getMaxPreference()
{    return 1.0f;}
0
public float getMinPreference()
{    return 1.0f;}
0
public DataSource getDataSource()
{    return dataSource;}
0
public String getPreferenceTable()
{    return preferenceTable;}
0
public String getUserIDColumn()
{    return userIDColumn;}
0
public String getItemIDColumn()
{    return itemIDColumn;}
0
public String getPreferenceColumn()
{    return preferenceColumn;}
0
 String getSetPreferenceSQL()
{    return setPreferenceSQL;}
0
public LongPrimitiveIterator getUserIDs() throws TasteException
{        try {        return new ResultSetIDIterator(getUsersSQL);    } catch (SQLException sqle) {        throw new TasteException(sqle);    }}
1
public PreferenceArray getPreferencesFromUser(long userID) throws TasteException
{        Connection conn = null;    PreparedStatement stmt = null;    ResultSet rs = null;    try {        conn = dataSource.getConnection();        stmt = conn.prepareStatement(getUserSQL, ResultSet.TYPE_FORWARD_ONLY, ResultSet.CONCUR_READ_ONLY);        stmt.setFetchDirection(ResultSet.FETCH_FORWARD);        stmt.setFetchSize(getFetchSize());        setLongParameter(stmt, 1, userID);                rs = stmt.executeQuery();        List<Preference> prefs = new ArrayList<>();        while (rs.next()) {            prefs.add(buildPreference(rs));        }        if (prefs.isEmpty()) {            throw new NoSuchUserException(userID);        }        return new GenericUserPreferenceArray(prefs);    } catch (SQLException sqle) {                throw new TasteException(sqle);    } finally {        IOUtils.quietClose(rs, stmt, conn);    }}
1
public FastByIDMap<PreferenceArray> exportWithPrefs() throws TasteException
{        Connection conn = null;    Statement stmt = null;    ResultSet rs = null;    FastByIDMap<PreferenceArray> result = new FastByIDMap<>();    try {        conn = dataSource.getConnection();        stmt = conn.createStatement(ResultSet.TYPE_FORWARD_ONLY, ResultSet.CONCUR_READ_ONLY);        stmt.setFetchDirection(ResultSet.FETCH_FORWARD);        stmt.setFetchSize(getFetchSize());                rs = stmt.executeQuery(getAllUsersSQL);        Long currentUserID = null;        List<Preference> currentPrefs = new ArrayList<>();        while (rs.next()) {            long nextUserID = getLongColumn(rs, 1);            if (currentUserID != null && !currentUserID.equals(nextUserID) && !currentPrefs.isEmpty()) {                result.put(currentUserID, new GenericUserPreferenceArray(currentPrefs));                currentPrefs.clear();            }            currentPrefs.add(buildPreference(rs));            currentUserID = nextUserID;        }        if (!currentPrefs.isEmpty()) {            result.put(currentUserID, new GenericUserPreferenceArray(currentPrefs));        }        return result;    } catch (SQLException sqle) {                throw new TasteException(sqle);    } finally {        IOUtils.quietClose(rs, stmt, conn);    }}
1
public FastByIDMap<FastIDSet> exportWithIDsOnly() throws TasteException
{        Connection conn = null;    Statement stmt = null;    ResultSet rs = null;    FastByIDMap<FastIDSet> result = new FastByIDMap<>();    try {        conn = dataSource.getConnection();        stmt = conn.createStatement(ResultSet.TYPE_FORWARD_ONLY, ResultSet.CONCUR_READ_ONLY);        stmt.setFetchDirection(ResultSet.FETCH_FORWARD);        stmt.setFetchSize(getFetchSize());                rs = stmt.executeQuery(getAllUsersSQL);        boolean currentUserIDSet = false;                long currentUserID = 0L;        FastIDSet currentItemIDs = new FastIDSet(2);        while (rs.next()) {            long nextUserID = getLongColumn(rs, 1);            if (currentUserIDSet && currentUserID != nextUserID && !currentItemIDs.isEmpty()) {                result.put(currentUserID, currentItemIDs);                currentItemIDs = new FastIDSet(2);            }            currentItemIDs.add(getLongColumn(rs, 2));            currentUserID = nextUserID;            currentUserIDSet = true;        }        if (!currentItemIDs.isEmpty()) {            result.put(currentUserID, currentItemIDs);        }        return result;    } catch (SQLException sqle) {                throw new TasteException(sqle);    } finally {        IOUtils.quietClose(rs, stmt, conn);    }}
1
public FastIDSet getItemIDsFromUser(long userID) throws TasteException
{        Connection conn = null;    PreparedStatement stmt = null;    ResultSet rs = null;    try {        conn = dataSource.getConnection();        stmt = conn.prepareStatement(getUserSQL, ResultSet.TYPE_FORWARD_ONLY, ResultSet.CONCUR_READ_ONLY);        stmt.setFetchDirection(ResultSet.FETCH_FORWARD);        stmt.setFetchSize(getFetchSize());        setLongParameter(stmt, 1, userID);                rs = stmt.executeQuery();        FastIDSet result = new FastIDSet();        while (rs.next()) {            result.add(getLongColumn(rs, 2));        }        if (result.isEmpty()) {            throw new NoSuchUserException(userID);        }        return result;    } catch (SQLException sqle) {                throw new TasteException(sqle);    } finally {        IOUtils.quietClose(rs, stmt, conn);    }}
1
public Float getPreferenceValue(long userID, long itemID) throws TasteException
{        Connection conn = null;    PreparedStatement stmt = null;    ResultSet rs = null;    try {        conn = dataSource.getConnection();        stmt = conn.prepareStatement(getPreferenceSQL, ResultSet.TYPE_FORWARD_ONLY, ResultSet.CONCUR_READ_ONLY);        stmt.setFetchDirection(ResultSet.FETCH_FORWARD);        stmt.setFetchSize(1);        setLongParameter(stmt, 1, userID);        setLongParameter(stmt, 2, itemID);                rs = stmt.executeQuery();        if (rs.next()) {            return rs.getFloat(1);        } else {            return null;        }    } catch (SQLException sqle) {                throw new TasteException(sqle);    } finally {        IOUtils.quietClose(rs, stmt, conn);    }}
1
public Long getPreferenceTime(long userID, long itemID) throws TasteException
{    if (getPreferenceTimeSQL == null) {        return null;    }        Connection conn = null;    PreparedStatement stmt = null;    ResultSet rs = null;    try {        conn = dataSource.getConnection();        stmt = conn.prepareStatement(getPreferenceTimeSQL, ResultSet.TYPE_FORWARD_ONLY, ResultSet.CONCUR_READ_ONLY);        stmt.setFetchDirection(ResultSet.FETCH_FORWARD);        stmt.setFetchSize(1);        setLongParameter(stmt, 1, userID);        setLongParameter(stmt, 2, itemID);                rs = stmt.executeQuery();        if (rs.next()) {            return rs.getLong(1);        } else {            return null;        }    } catch (SQLException sqle) {                throw new TasteException(sqle);    } finally {        IOUtils.quietClose(rs, stmt, conn);    }}
1
public LongPrimitiveIterator getItemIDs() throws TasteException
{        try {        return new ResultSetIDIterator(getItemsSQL);    } catch (SQLException sqle) {        throw new TasteException(sqle);    }}
1
public PreferenceArray getPreferencesForItem(long itemID) throws TasteException
{    List<Preference> list = doGetPreferencesForItem(itemID);    if (list.isEmpty()) {        throw new NoSuchItemException(itemID);    }    return new GenericItemPreferenceArray(list);}
0
protected List<Preference> doGetPreferencesForItem(long itemID) throws TasteException
{        Connection conn = null;    PreparedStatement stmt = null;    ResultSet rs = null;    try {        conn = dataSource.getConnection();        stmt = conn.prepareStatement(getPrefsForItemSQL, ResultSet.TYPE_FORWARD_ONLY, ResultSet.CONCUR_READ_ONLY);        stmt.setFetchDirection(ResultSet.FETCH_FORWARD);        stmt.setFetchSize(getFetchSize());        setLongParameter(stmt, 1, itemID);                rs = stmt.executeQuery();        List<Preference> prefs = new ArrayList<>();        while (rs.next()) {            prefs.add(buildPreference(rs));        }        return prefs;    } catch (SQLException sqle) {                throw new TasteException(sqle);    } finally {        IOUtils.quietClose(rs, stmt, conn);    }}
1
public int getNumItems() throws TasteException
{    if (cachedNumItems < 0) {        cachedNumItems = getNumThings("items", getNumItemsSQL);    }    return cachedNumItems;}
0
public int getNumUsers() throws TasteException
{    if (cachedNumUsers < 0) {        cachedNumUsers = getNumThings("users", getNumUsersSQL);    }    return cachedNumUsers;}
0
public int getNumUsersWithPreferenceFor(long itemID) throws TasteException
{    return itemPrefCounts.get(itemID);}
0
public int getNumUsersWithPreferenceFor(long itemID1, long itemID2) throws TasteException
{    return getNumThings("user preferring items", getNumPreferenceForItemsSQL, itemID1, itemID2);}
0
private int getNumThings(String name, String sql, long... args) throws TasteException
{        Connection conn = null;    PreparedStatement stmt = null;    ResultSet rs = null;    try {        conn = dataSource.getConnection();        stmt = conn.prepareStatement(sql, ResultSet.TYPE_FORWARD_ONLY, ResultSet.CONCUR_READ_ONLY);        stmt.setFetchDirection(ResultSet.FETCH_FORWARD);        stmt.setFetchSize(getFetchSize());        if (args != null) {            for (int i = 1; i <= args.length; i++) {                setLongParameter(stmt, i, args[i - 1]);            }        }                rs = stmt.executeQuery();        rs.next();        return rs.getInt(1);    } catch (SQLException sqle) {                throw new TasteException(sqle);    } finally {        IOUtils.quietClose(rs, stmt, conn);    }}
1
public void setPreference(long userID, long itemID, float value) throws TasteException
{    Preconditions.checkArgument(!Float.isNaN(value), "NaN value");        Connection conn = null;    PreparedStatement stmt = null;    try {        conn = dataSource.getConnection();        stmt = conn.prepareStatement(setPreferenceSQL);        setLongParameter(stmt, 1, userID);        setLongParameter(stmt, 2, itemID);        stmt.setDouble(3, value);        stmt.setDouble(4, value);                stmt.executeUpdate();    } catch (SQLException sqle) {                throw new TasteException(sqle);    } finally {        IOUtils.quietClose(null, stmt, conn);    }}
1
public void removePreference(long userID, long itemID) throws TasteException
{        Connection conn = null;    PreparedStatement stmt = null;    try {        conn = dataSource.getConnection();        stmt = conn.prepareStatement(removePreferenceSQL);        setLongParameter(stmt, 1, userID);        setLongParameter(stmt, 2, itemID);                stmt.executeUpdate();    } catch (SQLException sqle) {                throw new TasteException(sqle);    } finally {        IOUtils.quietClose(null, stmt, conn);    }}
1
public void refresh(Collection<Refreshable> alreadyRefreshed)
{    cachedNumUsers = -1;    cachedNumItems = -1;    minPreference = Float.NaN;    maxPreference = Float.NaN;    itemPrefCounts.clear();}
0
public boolean hasPreferenceValues()
{    return true;}
0
public float getMaxPreference()
{    if (Float.isNaN(maxPreference)) {        Connection conn = null;        PreparedStatement stmt = null;        ResultSet rs = null;        try {            conn = dataSource.getConnection();            stmt = conn.prepareStatement(getMaxPreferenceSQL);                        rs = stmt.executeQuery();            rs.next();            maxPreference = rs.getFloat(1);        } catch (SQLException sqle) {                            } finally {            IOUtils.quietClose(rs, stmt, conn);        }    }    return maxPreference;}
1
public float getMinPreference()
{    if (Float.isNaN(minPreference)) {        Connection conn = null;        PreparedStatement stmt = null;        ResultSet rs = null;        try {            conn = dataSource.getConnection();            stmt = conn.prepareStatement(getMinPreferenceSQL);                        rs = stmt.executeQuery();            rs.next();            minPreference = rs.getFloat(1);        } catch (SQLException sqle) {                            } finally {            IOUtils.quietClose(rs, stmt, conn);        }    }    return minPreference;}
1
protected Preference buildPreference(ResultSet rs) throws SQLException
{    return new GenericPreference(getLongColumn(rs, 1), getLongColumn(rs, 2), rs.getFloat(3));}
0
protected long getLongColumn(ResultSet rs, int position) throws SQLException
{    return rs.getLong(position);}
0
protected void setLongParameter(PreparedStatement stmt, int position, long value) throws SQLException
{    stmt.setLong(position, value);}
0
protected Long parseElement(ResultSet resultSet) throws SQLException
{    return getLongColumn(resultSet, 1);}
0
public long nextLong()
{    return next();}
0
public long peek()
{        throw new UnsupportedOperationException();}
0
public Integer get(Long key) throws TasteException
{    return getNumThings("user preferring item", getNumPreferenceForItemSQL, key);}
0
public Connection getConnection() throws SQLException
{    return delegate.getConnection();}
0
public Connection getConnection(String username, String password) throws SQLException
{    return delegate.getConnection(username, password);}
0
public PrintWriter getLogWriter() throws SQLException
{    return delegate.getLogWriter();}
0
public void setLogWriter(PrintWriter printWriter) throws SQLException
{    delegate.setLogWriter(printWriter);}
0
public void setLoginTimeout(int timeout) throws SQLException
{    delegate.setLoginTimeout(timeout);}
0
public int getLoginTimeout() throws SQLException
{    return delegate.getLoginTimeout();}
0
public T unwrap(Class<T> iface) throws SQLException
{    return delegate.unwrap(iface);}
0
public boolean isWrapperFor(Class<?> iface) throws SQLException
{    return delegate.isWrapperFor(iface);}
0
public Logger getParentLogger() throws SQLFeatureNotSupportedException
{    throw new SQLFeatureNotSupportedException();}
0
public Connection createConnection() throws SQLException
{    Connection connection = underlyingDataSource.getConnection();    connection.setTransactionIsolation(Connection.TRANSACTION_READ_UNCOMMITTED);    connection.setHoldability(ResultSet.CLOSE_CURSORS_AT_COMMIT);    return connection;}
0
private static Properties getPropertiesFromFile(File file) throws TasteException
{    try {        return getPropertiesFromStream(new FileInputStream(file));    } catch (FileNotFoundException fnfe) {        throw new TasteException(fnfe);    }}
0
private static Properties getPropertiesFromStream(InputStream is) throws TasteException
{    try {        try {            Properties props = new Properties();            props.load(is);            return props;        } finally {            Closeables.close(is, true);        }    } catch (IOException ioe) {        throw new TasteException(ioe);    }}
0
protected int getFetchSize()
{        return Integer.MIN_VALUE;}
0
protected int getFetchSize()
{        return Integer.MIN_VALUE;}
0
public void setPreference(long userID, long itemID, float value) throws TasteException
{    Preconditions.checkArgument(!Float.isNaN(value), "NaN value");        String setPreferenceSQL = getSetPreferenceSQL();    Connection conn = null;    PreparedStatement stmt = null;    try {        conn = getDataSource().getConnection();        stmt = conn.prepareStatement(setPreferenceSQL);        setLongParameter(stmt, 1, userID);        setLongParameter(stmt, 2, itemID);                stmt.executeUpdate();    } catch (SQLException sqle) {        if (!POSTGRESQL_DUPLICATE_KEY_STATE.equals(sqle.getSQLState())) {                        throw new TasteException(sqle);        }    } finally {        IOUtils.quietClose(null, stmt, conn);    }}
1
public void setPreference(long userID, long itemID, float value) throws TasteException
{    Preconditions.checkArgument(!Float.isNaN(value), "NaN value");        String setPreferenceSQL = getSetPreferenceSQL();    Connection conn = null;    PreparedStatement stmt1 = null;    PreparedStatement stmt2 = null;    try {        conn = getDataSource().getConnection();        stmt1 = conn.prepareStatement(setPreferenceSQL);        setLongParameter(stmt1, 1, userID);        setLongParameter(stmt1, 2, itemID);        stmt1.setDouble(3, value);                try {            stmt1.executeUpdate();        } catch (SQLException sqle) {            if (!POSTGRESQL_DUPLICATE_KEY_STATE.equals(sqle.getSQLState())) {                throw sqle;            }        }                stmt2 = conn.prepareStatement(getUpdatePreferenceSQL());        stmt2.setDouble(1, value);        setLongParameter(stmt2, 2, userID);        setLongParameter(stmt2, 3, itemID);                stmt2.executeUpdate();    } catch (SQLException sqle) {                throw new TasteException(sqle);    } finally {        IOUtils.quietClose(null, stmt1, null);        IOUtils.quietClose(null, stmt2, null);        IOUtils.quietClose(null, null, conn);    }}
1
public Void call()
{    reload();        return null;}
0
public void refresh(Collection<Refreshable> alreadyRefreshed)
{    refreshHelper.refresh(alreadyRefreshed);}
0
private void reload()
{    try {                        DataModel newDelegateInMemory = delegate.hasPreferenceValues() ? new GenericDataModel(delegate.exportWithPrefs()) : new GenericBooleanPrefDataModel(delegate.exportWithIDsOnly());                        delegateInMemory = newDelegateInMemory;    } catch (TasteException te) {                }}
1
public JDBCDataModel getDelegate()
{    return delegate;}
0
public DataModel getDelegateInMemory()
{    return delegateInMemory;}
0
public LongPrimitiveIterator getUserIDs() throws TasteException
{    return delegateInMemory.getUserIDs();}
0
public PreferenceArray getPreferencesFromUser(long id) throws TasteException
{    return delegateInMemory.getPreferencesFromUser(id);}
0
public FastIDSet getItemIDsFromUser(long id) throws TasteException
{    return delegateInMemory.getItemIDsFromUser(id);}
0
public Float getPreferenceValue(long userID, long itemID) throws TasteException
{    return delegateInMemory.getPreferenceValue(userID, itemID);}
0
public Long getPreferenceTime(long userID, long itemID) throws TasteException
{    return delegateInMemory.getPreferenceTime(userID, itemID);}
0
public LongPrimitiveIterator getItemIDs() throws TasteException
{    return delegateInMemory.getItemIDs();}
0
public PreferenceArray getPreferencesForItem(long itemID) throws TasteException
{    return delegateInMemory.getPreferencesForItem(itemID);}
0
public int getNumItems() throws TasteException
{    return delegateInMemory.getNumItems();}
0
public int getNumUsers() throws TasteException
{    return delegateInMemory.getNumUsers();}
0
public int getNumUsersWithPreferenceFor(long itemID) throws TasteException
{    return delegateInMemory.getNumUsersWithPreferenceFor(itemID);}
0
public int getNumUsersWithPreferenceFor(long itemID1, long itemID2) throws TasteException
{    return delegateInMemory.getNumUsersWithPreferenceFor(itemID1, itemID2);}
0
public void setPreference(long userID, long itemID, float value) throws TasteException
{    delegateInMemory.setPreference(userID, itemID, value);}
0
public void removePreference(long userID, long itemID) throws TasteException
{    delegateInMemory.removePreference(userID, itemID);}
0
public boolean hasPreferenceValues()
{    return delegateInMemory.hasPreferenceValues();}
0
public float getMaxPreference()
{    return delegateInMemory.getMaxPreference();}
0
public float getMinPreference()
{    return delegateInMemory.getMinPreference();}
0
protected String getVerifyPreferenceSQL()
{    return verifyPreferenceSQL;}
0
public void setPreference(long userID, long itemID, float value) throws TasteException
{    Preconditions.checkArgument(!Float.isNaN(value), "NaN value");        String setPreferenceSQL = getSetPreferenceSQL();    Connection conn = null;    PreparedStatement stmt1 = null;    PreparedStatement stmt2 = null;    ResultSet rs = null;    try {        conn = getDataSource().getConnection();        stmt1 = conn.prepareStatement(verifyPreferenceSQL, ResultSet.TYPE_FORWARD_ONLY, ResultSet.CONCUR_READ_ONLY);        setLongParameter(stmt1, 1, userID);        setLongParameter(stmt1, 2, itemID);        rs = stmt1.executeQuery();                if (!rs.first()) {            stmt2 = conn.prepareStatement(setPreferenceSQL);            setLongParameter(stmt2, 1, userID);            setLongParameter(stmt2, 2, itemID);            stmt2.setDouble(3, value);                        stmt2.executeUpdate();        }    } catch (SQLException sqle) {                throw new TasteException(sqle);    } finally {        IOUtils.quietClose(rs);        IOUtils.quietClose(stmt1);        IOUtils.quietClose(stmt2);        IOUtils.quietClose(conn);    }}
1
protected String getUpdatePreferenceSQL()
{    return updatePreferenceSQL;}
0
protected String getVerifyPreferenceSQL()
{    return verifyPreferenceSQL;}
0
public void setPreference(long userID, long itemID, float value) throws TasteException
{    Preconditions.checkArgument(!Float.isNaN(value), "NaN value");        String setPreferenceSQL = getSetPreferenceSQL();    Connection conn = null;    PreparedStatement stmt1 = null;    PreparedStatement stmt2 = null;    PreparedStatement stmt3 = null;    ResultSet rs = null;    try {        conn = getDataSource().getConnection();        stmt1 = conn.prepareStatement(verifyPreferenceSQL, ResultSet.TYPE_FORWARD_ONLY, ResultSet.CONCUR_READ_ONLY);        setLongParameter(stmt1, 1, userID);        setLongParameter(stmt1, 2, itemID);        rs = stmt1.executeQuery();                if (rs.first()) {                        stmt2 = conn.prepareStatement(updatePreferenceSQL);            stmt2.setDouble(1, value);            setLongParameter(stmt2, 2, userID);            setLongParameter(stmt2, 3, itemID);                        stmt2.executeUpdate();        } else {                        stmt3 = conn.prepareStatement(setPreferenceSQL);            setLongParameter(stmt3, 1, userID);            setLongParameter(stmt3, 2, itemID);            stmt3.setDouble(3, value);                        stmt3.executeUpdate();        }    } catch (SQLException sqle) {                throw new TasteException(sqle);    } finally {        IOUtils.quietClose(rs);        IOUtils.quietClose(stmt1);        IOUtils.quietClose(stmt2);        IOUtils.quietClose(stmt3);        IOUtils.quietClose(conn);    }}
1
public void refreshData(String userID, Iterable<List<String>> items, boolean add) throws NoSuchUserException, NoSuchItemException
{    checkData(userID, items, add);    long id = Long.parseLong(fromIdToLong(userID, true));    for (List<String> item : items) {        item.set(0, fromIdToLong(item.get(0), false));    }    if (reloadLock.tryLock()) {        try {            if (add) {                delegate = addUserItem(id, items);            } else {                delegate = removeUserItem(id, items);            }        } finally {            reloadLock.unlock();        }    }}
0
public void refresh(Collection<Refreshable> alreadyRefreshed)
{    BasicDBObject query = new BasicDBObject();    query.put("deleted_at", new BasicDBObject("$gt", mongoTimestamp));    DBCursor cursor = collection.find(query);    Date ts = new Date(0);    while (cursor.hasNext()) {        Map<String, Object> user = (Map<String, Object>) cursor.next().toMap();        String userID = getID(user.get(mongoUserID), true);        Collection<List<String>> items = new ArrayList<>();        List<String> item = new ArrayList<>();        item.add(getID(user.get(mongoItemID), false));        item.add(Float.toString(getPreference(user.get(mongoPreference))));        items.add(item);        try {            refreshData(userID, items, false);        } catch (NoSuchUserException e) {                    } catch (NoSuchItemException e) {                    }        if (ts.compareTo(getDate(user.get("created_at"))) < 0) {            ts = getDate(user.get("created_at"));        }    }    query = new BasicDBObject();    query.put("created_at", new BasicDBObject("$gt", mongoTimestamp));    cursor = collection.find(query);    while (cursor.hasNext()) {        Map<String, Object> user = (Map<String, Object>) cursor.next().toMap();        if (!user.containsKey("deleted_at")) {            String userID = getID(user.get(mongoUserID), true);            Collection<List<String>> items = new ArrayList<>();            List<String> item = new ArrayList<>();            item.add(getID(user.get(mongoItemID), false));            item.add(Float.toString(getPreference(user.get(mongoPreference))));            items.add(item);            try {                refreshData(userID, items, true);            } catch (NoSuchUserException e) {                            } catch (NoSuchItemException e) {                            }            if (ts.compareTo(getDate(user.get("created_at"))) < 0) {                ts = getDate(user.get("created_at"));            }        }    }    if (mongoTimestamp.compareTo(ts) < 0) {        mongoTimestamp = ts;    }}
1
public String fromIdToLong(String id, boolean isUser)
{    DBObject objectIdLong = collectionMap.findOne(new BasicDBObject("element_id", id));    if (objectIdLong != null) {        Map<String, Object> idLong = (Map<String, Object>) objectIdLong.toMap();        Object value = idLong.get("long_value");        return value == null ? null : value.toString();    } else {        objectIdLong = new BasicDBObject();        String longValue = Long.toString(idCounter++);        objectIdLong.put("element_id", id);        objectIdLong.put("long_value", longValue);        collectionMap.insert(objectIdLong);                return longValue;    }}
1
public String fromLongToId(long id)
{    DBObject objectIdLong = collectionMap.findOne(new BasicDBObject("long_value", Long.toString(id)));    Map<String, Object> idLong = (Map<String, Object>) objectIdLong.toMap();    Object value = idLong.get("element_id");    return value == null ? null : value.toString();}
0
public boolean isIDInModel(String ID)
{    DBObject objectIdLong = collectionMap.findOne(new BasicDBObject("element_id", ID));    return objectIdLong != null;}
0
public Date mongoUpdateDate()
{    return mongoTimestamp;}
0
private void buildModel() throws UnknownHostException
{    userIsObject = false;    itemIsObject = false;    idCounter = 0;    preferenceIsString = true;    Mongo mongoDDBB = new Mongo(mongoHost, mongoPort);    DB db = mongoDDBB.getDB(mongoDB);    mongoTimestamp = new Date(0);    FastByIDMap<Collection<Preference>> userIDPrefMap = new FastByIDMap<>();    if (!mongoAuth || db.authenticate(mongoUsername, mongoPassword.toCharArray())) {        collection = db.getCollection(mongoCollection);        collectionMap = db.getCollection(mongoMapCollection);        DBObject indexObj = new BasicDBObject();        indexObj.put("element_id", 1);        collectionMap.ensureIndex(indexObj);        indexObj = new BasicDBObject();        indexObj.put("long_value", 1);        collectionMap.ensureIndex(indexObj);        collectionMap.remove(new BasicDBObject());        DBCursor cursor = collection.find();        while (cursor.hasNext()) {            Map<String, Object> user = (Map<String, Object>) cursor.next().toMap();            if (!user.containsKey("deleted_at")) {                long userID = Long.parseLong(fromIdToLong(getID(user.get(mongoUserID), true), true));                long itemID = Long.parseLong(fromIdToLong(getID(user.get(mongoItemID), false), false));                float ratingValue = getPreference(user.get(mongoPreference));                Collection<Preference> userPrefs = userIDPrefMap.get(userID);                if (userPrefs == null) {                    userPrefs = new ArrayList<>(2);                    userIDPrefMap.put(userID, userPrefs);                }                userPrefs.add(new GenericPreference(userID, itemID, ratingValue));                if (user.containsKey("created_at") && mongoTimestamp.compareTo(getDate(user.get("created_at"))) < 0) {                    mongoTimestamp = getDate(user.get("created_at"));                }            }        }    }    delegate = new GenericDataModel(GenericDataModel.toDataMap(userIDPrefMap, true));}
0
private void removeMongoUserItem(String userID, String itemID)
{    String userId = fromLongToId(Long.parseLong(userID));    String itemId = fromLongToId(Long.parseLong(itemID));    if (isUserItemInDB(userId, itemId)) {        mongoTimestamp = new Date();        BasicDBObject query = new BasicDBObject();        query.put(mongoUserID, userIsObject ? new ObjectId(userId) : userId);        query.put(mongoItemID, itemIsObject ? new ObjectId(itemId) : itemId);        if (mongoFinalRemove) {                    } else {            BasicDBObject update = new BasicDBObject();            update.put("$set", new BasicDBObject("deleted_at", mongoTimestamp));                    }            }}
1
private void addMongoUserItem(String userID, String itemID, String preferenceValue)
{    String userId = fromLongToId(Long.parseLong(userID));    String itemId = fromLongToId(Long.parseLong(itemID));    if (!isUserItemInDB(userId, itemId)) {        mongoTimestamp = new Date();        BasicDBObject user = new BasicDBObject();        Object userIdObject = userIsObject ? new ObjectId(userId) : userId;        Object itemIdObject = itemIsObject ? new ObjectId(itemId) : itemId;        user.put(mongoUserID, userIdObject);        user.put(mongoItemID, itemIdObject);        user.put(mongoPreference, preferenceIsString ? preferenceValue : Double.parseDouble(preferenceValue));        user.put("created_at", mongoTimestamp);        collection.insert(user);            }}
1
private boolean isUserItemInDB(String userID, String itemID)
{    BasicDBObject query = new BasicDBObject();    Object userId = userIsObject ? new ObjectId(userID) : userID;    Object itemId = itemIsObject ? new ObjectId(itemID) : itemID;    query.put(mongoUserID, userId);    query.put(mongoItemID, itemId);    return collection.findOne(query) != null;}
0
private DataModel removeUserItem(long userID, Iterable<List<String>> items)
{    FastByIDMap<PreferenceArray> rawData = ((GenericDataModel) delegate).getRawUserData();    for (List<String> item : items) {        PreferenceArray prefs = rawData.get(userID);        long itemID = Long.parseLong(item.get(0));        if (prefs != null) {            boolean exists = false;            int length = prefs.length();            for (int i = 0; i < length; i++) {                if (prefs.getItemID(i) == itemID) {                    exists = true;                    break;                }            }            if (exists) {                rawData.remove(userID);                if (length > 1) {                    PreferenceArray newPrefs = new GenericUserPreferenceArray(length - 1);                    for (int i = 0, j = 0; i < length; i++, j++) {                        if (prefs.getItemID(i) == itemID) {                            j--;                        } else {                            newPrefs.set(j, prefs.get(i));                        }                    }                    rawData.put(userID, newPrefs);                }                                if (mongoManage) {                    removeMongoUserItem(Long.toString(userID), Long.toString(itemID));                }            }        }    }    return new GenericDataModel(rawData);}
1
private DataModel addUserItem(long userID, Iterable<List<String>> items)
{    FastByIDMap<PreferenceArray> rawData = ((GenericDataModel) delegate).getRawUserData();    PreferenceArray prefs = rawData.get(userID);    for (List<String> item : items) {        long itemID = Long.parseLong(item.get(0));        float preferenceValue = Float.parseFloat(item.get(1));        boolean exists = false;        if (prefs != null) {            for (int i = 0; i < prefs.length(); i++) {                if (prefs.getItemID(i) == itemID) {                    exists = true;                    prefs.setValue(i, preferenceValue);                    break;                }            }        }        if (!exists) {            if (prefs == null) {                prefs = new GenericUserPreferenceArray(1);            } else {                PreferenceArray newPrefs = new GenericUserPreferenceArray(prefs.length() + 1);                for (int i = 0, j = 1; i < prefs.length(); i++, j++) {                    newPrefs.set(j, prefs.get(i));                }                prefs = newPrefs;            }            prefs.setUserID(0, userID);            prefs.setItemID(0, itemID);            prefs.setValue(0, preferenceValue);                        rawData.put(userID, prefs);            if (mongoManage) {                addMongoUserItem(Long.toString(userID), Long.toString(itemID), Float.toString(preferenceValue));            }        }    }    return new GenericDataModel(rawData);}
1
private Date getDate(Object date)
{    if (date.getClass().getName().contains("Date")) {        return (Date) date;    }    if (date.getClass().getName().contains("String")) {        try {            synchronized (dateFormat) {                return dateFormat.parse(date.toString());            }        } catch (ParseException ioe) {                    }    }    return new Date(0);}
1
private float getPreference(Object value)
{    if (value != null) {        if (value.getClass().getName().contains("String")) {            preferenceIsString = true;            return Float.parseFloat(value.toString());        } else {            preferenceIsString = false;            return Double.valueOf(value.toString()).floatValue();        }    } else {        return 0.5f;    }}
0
private String getID(Object id, boolean isUser)
{    if (id.getClass().getName().contains("ObjectId")) {        if (isUser) {            userIsObject = true;        } else {            itemIsObject = true;        }        return ((ObjectId) id).toStringMongod();    } else {        return id.toString();    }}
0
private void checkData(String userID, Iterable<List<String>> items, boolean add) throws NoSuchUserException, NoSuchItemException
{    Preconditions.checkNotNull(userID);    Preconditions.checkNotNull(items);    Preconditions.checkArgument(!userID.isEmpty(), "userID is empty");    for (List<String> item : items) {        Preconditions.checkNotNull(item.get(0));        Preconditions.checkArgument(!item.get(0).isEmpty(), "item is empty");    }    if (userIsObject && !ID_PATTERN.matcher(userID).matches()) {        throw new IllegalArgumentException();    }    for (List<String> item : items) {        if (itemIsObject && !ID_PATTERN.matcher(item.get(0)).matches()) {            throw new IllegalArgumentException();        }    }    if (!add && !isIDInModel(userID)) {        throw new NoSuchUserException();    }    for (List<String> item : items) {        if (!add && !isIDInModel(item.get(0))) {            throw new NoSuchItemException();        }    }}
0
public void cleanupMappingCollection()
{    collectionMap.drop();}
0
public LongPrimitiveIterator getUserIDs() throws TasteException
{    return delegate.getUserIDs();}
0
public PreferenceArray getPreferencesFromUser(long id) throws TasteException
{    return delegate.getPreferencesFromUser(id);}
0
public FastIDSet getItemIDsFromUser(long userID) throws TasteException
{    return delegate.getItemIDsFromUser(userID);}
0
public LongPrimitiveIterator getItemIDs() throws TasteException
{    return delegate.getItemIDs();}
0
public PreferenceArray getPreferencesForItem(long itemID) throws TasteException
{    return delegate.getPreferencesForItem(itemID);}
0
public Float getPreferenceValue(long userID, long itemID) throws TasteException
{    return delegate.getPreferenceValue(userID, itemID);}
0
public Long getPreferenceTime(long userID, long itemID) throws TasteException
{    return delegate.getPreferenceTime(userID, itemID);}
0
public int getNumItems() throws TasteException
{    return delegate.getNumItems();}
0
public int getNumUsers() throws TasteException
{    return delegate.getNumUsers();}
0
public int getNumUsersWithPreferenceFor(long itemID) throws TasteException
{    return delegate.getNumUsersWithPreferenceFor(itemID);}
0
public int getNumUsersWithPreferenceFor(long itemID1, long itemID2) throws TasteException
{    return delegate.getNumUsersWithPreferenceFor(itemID1, itemID2);}
0
public void setPreference(long userID, long itemID, float value)
{    throw new UnsupportedOperationException();}
0
public void removePreference(long userID, long itemID)
{    throw new UnsupportedOperationException();}
0
public boolean hasPreferenceValues()
{    return delegate.hasPreferenceValues();}
0
public float getMaxPreference()
{    return delegate.getMaxPreference();}
0
public float getMinPreference()
{    return delegate.getMinPreference();}
0
public String toString()
{    return "MongoDBDataModel";}
0
public double itemSimilarity(long itemID1, long itemID2) throws TasteException
{    return delegate.itemSimilarity(itemID1, itemID2);}
0
public double[] itemSimilarities(long itemID1, long[] itemID2s) throws TasteException
{    return delegate.itemSimilarities(itemID1, itemID2s);}
0
public long[] allSimilarItemIDs(long itemID) throws TasteException
{    return delegate.allSimilarItemIDs(itemID);}
0
public void refresh(Collection<Refreshable> alreadyRefreshed)
{        reload();}
1
protected void reload()
{    if (reloadLock.tryLock()) {        try {            delegate = new GenericItemSimilarity(new JDBCSimilaritiesIterable(dataSource, getAllItemSimilaritiesSQL));        } finally {            reloadLock.unlock();        }    }}
0
public Iterator<GenericItemSimilarity.ItemItemSimilarity> iterator()
{    try {        return new JDBCSimilaritiesIterator(dataSource, getAllItemSimilaritiesSQL);    } catch (SQLException sqle) {        throw new IllegalStateException(sqle);    }}
0
protected GenericItemSimilarity.ItemItemSimilarity parseElement(ResultSet resultSet) throws SQLException
{    return new GenericItemSimilarity.ItemItemSimilarity(resultSet.getLong(1), resultSet.getLong(2), resultSet.getDouble(3));}
0
protected String getSimilarityTable()
{    return similarityTable;}
0
protected String getItemAIDColumn()
{    return itemAIDColumn;}
0
protected String getItemBIDColumn()
{    return itemBIDColumn;}
0
protected String getSimilarityColumn()
{    return similarityColumn;}
0
public double itemSimilarity(long itemID1, long itemID2) throws TasteException
{    if (itemID1 == itemID2) {        return 1.0;    }    Connection conn = null;    PreparedStatement stmt = null;    try {        conn = dataSource.getConnection();        stmt = conn.prepareStatement(getItemItemSimilaritySQL, ResultSet.TYPE_FORWARD_ONLY, ResultSet.CONCUR_READ_ONLY);        stmt.setFetchDirection(ResultSet.FETCH_FORWARD);        stmt.setFetchSize(getFetchSize());        return doItemSimilarity(stmt, itemID1, itemID2);    } catch (SQLException sqle) {                throw new TasteException(sqle);    } finally {        IOUtils.quietClose(null, stmt, conn);    }}
1
public double[] itemSimilarities(long itemID1, long[] itemID2s) throws TasteException
{    double[] result = new double[itemID2s.length];    Connection conn = null;    PreparedStatement stmt = null;    try {        conn = dataSource.getConnection();        stmt = conn.prepareStatement(getItemItemSimilaritySQL, ResultSet.TYPE_FORWARD_ONLY, ResultSet.CONCUR_READ_ONLY);        stmt.setFetchDirection(ResultSet.FETCH_FORWARD);        stmt.setFetchSize(getFetchSize());        for (int i = 0; i < itemID2s.length; i++) {            result[i] = doItemSimilarity(stmt, itemID1, itemID2s[i]);        }    } catch (SQLException sqle) {                throw new TasteException(sqle);    } finally {        IOUtils.quietClose(null, stmt, conn);    }    return result;}
1
public long[] allSimilarItemIDs(long itemID) throws TasteException
{    FastIDSet allSimilarItemIDs = new FastIDSet();    Connection conn = null;    PreparedStatement stmt = null;    ResultSet rs = null;    try {        conn = dataSource.getConnection();        stmt = conn.prepareStatement(getAllSimilarItemIDsSQL, ResultSet.TYPE_FORWARD_ONLY, ResultSet.CONCUR_READ_ONLY);        stmt.setFetchDirection(ResultSet.FETCH_FORWARD);        stmt.setFetchSize(getFetchSize());        stmt.setLong(1, itemID);        stmt.setLong(2, itemID);        rs = stmt.executeQuery();        while (rs.next()) {            allSimilarItemIDs.add(rs.getLong(1));            allSimilarItemIDs.add(rs.getLong(2));        }    } catch (SQLException sqle) {                throw new TasteException(sqle);    } finally {        IOUtils.quietClose(rs, stmt, conn);    }    allSimilarItemIDs.remove(itemID);    return allSimilarItemIDs.toArray();}
1
public void refresh(Collection<Refreshable> alreadyRefreshed)
{}
0
private double doItemSimilarity(PreparedStatement stmt, long itemID1, long itemID2) throws SQLException
{        if (itemID1 > itemID2) {        long temp = itemID1;        itemID1 = itemID2;        itemID2 = temp;    }    stmt.setLong(1, itemID1);    stmt.setLong(2, itemID2);        ResultSet rs = null;    try {        rs = stmt.executeQuery();                return rs.next() ? rs.getDouble(1) : Double.NaN;    } finally {        IOUtils.quietClose(rs);    }}
1
protected int getFetchSize()
{        return Integer.MIN_VALUE;}
0
protected int getFetchSize()
{        return Integer.MIN_VALUE;}
0
public void init(ServletConfig config) throws ServletException
{    super.init(config);    String recommenderClassName = config.getInitParameter("recommender-class");    if (recommenderClassName == null) {        throw new ServletException("Servlet init-param \"recommender-class\" is not defined");    }    RecommenderSingleton.initializeIfNeeded(recommenderClassName);    recommender = RecommenderSingleton.getInstance().getRecommender();}
0
public void doGet(HttpServletRequest request, HttpServletResponse response) throws ServletException
{    String userIDString = request.getParameter("userID");    if (userIDString == null) {        throw new ServletException("userID was not specified");    }    long userID = Long.parseLong(userIDString);    String howManyString = request.getParameter("howMany");    int howMany = howManyString == null ? DEFAULT_HOW_MANY : Integer.parseInt(howManyString);    boolean debug = Boolean.parseBoolean(request.getParameter("debug"));    String format = request.getParameter("format");    if (format == null) {        format = "text";    }    try {        List<RecommendedItem> items = recommender.recommend(userID, howMany);        if ("text".equals(format)) {            writePlainText(response, userID, debug, items);        } else if ("xml".equals(format)) {            writeXML(response, items);        } else if ("json".equals(format)) {            writeJSON(response, items);        } else {            throw new ServletException("Bad format parameter: " + format);        }    } catch (TasteException | IOException te) {        throw new ServletException(te);    }}
0
private static void writeXML(HttpServletResponse response, Iterable<RecommendedItem> items) throws IOException
{    response.setContentType("application/xml");    response.setCharacterEncoding("UTF-8");    response.setHeader("Cache-Control", "no-cache");    PrintWriter writer = response.getWriter();    writer.print("<?xml version=\"1.0\" encoding=\"UTF-8\"?><recommendedItems>");    for (RecommendedItem recommendedItem : items) {        writer.print("<item><value>");        writer.print(recommendedItem.getValue());        writer.print("</value><id>");        writer.print(recommendedItem.getItemID());        writer.print("</id></item>");    }    writer.println("</recommendedItems>");}
0
private static void writeJSON(HttpServletResponse response, Iterable<RecommendedItem> items) throws IOException
{    response.setContentType("application/json");    response.setCharacterEncoding("UTF-8");    response.setHeader("Cache-Control", "no-cache");    PrintWriter writer = response.getWriter();    writer.print("{\"recommendedItems\":{\"item\":[");    boolean first = true;    for (RecommendedItem recommendedItem : items) {        if (first) {            first = false;        } else {            writer.print(',');        }        writer.print("{\"value\":\"");        writer.print(recommendedItem.getValue());        writer.print("\",\"id\":\"");        writer.print(recommendedItem.getItemID());        writer.print("\"}");    }    writer.println("]}}");}
0
private void writePlainText(HttpServletResponse response, long userID, boolean debug, Iterable<RecommendedItem> items) throws IOException, TasteException
{    response.setContentType("text/plain");    response.setCharacterEncoding("UTF-8");    response.setHeader("Cache-Control", "no-cache");    PrintWriter writer = response.getWriter();    if (debug) {        writeDebugRecommendations(userID, items, writer);    } else {        writeRecommendations(items, writer);    }}
0
private static void writeRecommendations(Iterable<RecommendedItem> items, PrintWriter writer)
{    for (RecommendedItem recommendedItem : items) {        writer.print(recommendedItem.getValue());        writer.print('\t');        writer.println(recommendedItem.getItemID());    }}
0
private void writeDebugRecommendations(long userID, Iterable<RecommendedItem> items, PrintWriter writer) throws TasteException
{    DataModel dataModel = recommender.getDataModel();    writer.print("User:");    writer.println(userID);    writer.print("Recommender: ");    writer.println(recommender);    writer.println();    writer.print("Top ");    writer.print(NUM_TOP_PREFERENCES);    writer.println(" Preferences:");    PreferenceArray rawPrefs = dataModel.getPreferencesFromUser(userID);    int length = rawPrefs.length();    PreferenceArray sortedPrefs = rawPrefs.clone();    sortedPrefs.sortByValueReversed();        int max = Math.min(NUM_TOP_PREFERENCES, length);    for (int i = 0; i < max; i++) {        Preference pref = sortedPrefs.get(i);        writer.print(pref.getValue());        writer.print('\t');        writer.println(pref.getItemID());    }    writer.println();    writer.println("Recommendations:");    for (RecommendedItem recommendedItem : items) {        writer.print(recommendedItem.getValue());        writer.print('\t');        writer.println(recommendedItem.getItemID());    }}
0
public void doPost(HttpServletRequest request, HttpServletResponse response) throws ServletException
{    doGet(request, response);}
0
public String toString()
{    return "RecommenderServlet[recommender:" + recommender + ']';}
0
public static synchronized RecommenderSingleton getInstance()
{    if (instance == null) {        throw new IllegalStateException("Not initialized");    }    return instance;}
0
public static synchronized void initializeIfNeeded(String recommenderClassName)
{    if (instance == null) {        instance = new RecommenderSingleton(recommenderClassName);    }}
0
public Recommender getRecommender()
{    return recommender;}
0
public List<RecommendedItem> recommend(long userID, int howMany) throws TasteException
{    return delegate.recommend(userID, howMany);}
0
public List<RecommendedItem> recommend(long userID, int howMany, IDRescorer rescorer) throws TasteException
{    return delegate.recommend(userID, howMany, rescorer);}
0
public float estimatePreference(long userID, long itemID) throws TasteException
{    return delegate.estimatePreference(userID, itemID);}
0
public void setPreference(long userID, long itemID, float value) throws TasteException
{    delegate.setPreference(userID, itemID, value);}
0
public void removePreference(long userID, long itemID) throws TasteException
{    delegate.removePreference(userID, itemID);}
0
public DataModel getDataModel()
{    return delegate.getDataModel();}
0
public void refresh(Collection<Refreshable> alreadyRefreshed)
{    delegate.refresh(alreadyRefreshed);}
0
public static File readResourceToTempFile(String resourceName) throws IOException
{    String absoluteResource = resourceName.startsWith("/") ? resourceName : '/' + resourceName;        InputSupplier<? extends InputStream> inSupplier;    try {        URL resourceURL = Resources.getResource(RecommenderWrapper.class, absoluteResource);        inSupplier = Resources.newInputStreamSupplier(resourceURL);    } catch (IllegalArgumentException iae) {        File resourceFile = new File(resourceName);                inSupplier = Files.newInputStreamSupplier(resourceFile);    }    File tempFile = File.createTempFile("taste", null);    tempFile.deleteOnExit();    Files.copy(inSupplier, tempFile);    return tempFile;}
1
public static void main(String[] args) throws Exception
{    ToolRunner.run(new ConfusionMatrixDumper(), args);}
0
public int run(String[] args) throws IOException
{    addInputOption();        addOption("output", "o", "Output path", null);    addOption(DefaultOptionCreator.overwriteOption().create());    addFlag("html", null, "Create complete HTML page");    addFlag("text", null, "Dump simple text");    Map<String, List<String>> parsedArgs = parseArguments(args);    if (parsedArgs == null) {        return -1;    }    Path inputPath = getInputPath();    String outputFile = hasOption("output") ? getOption("output") : null;    boolean text = parsedArgs.containsKey("--text");    boolean wrapHtml = parsedArgs.containsKey("--html");    PrintStream out = getPrintStream(outputFile);    if (text) {        exportText(inputPath, out);    } else {        exportTable(inputPath, out, wrapHtml);    }    out.flush();    if (out != System.out) {        out.close();    }    return 0;}
0
private static void exportText(Path inputPath, PrintStream out) throws IOException
{    MatrixWritable mw = new MatrixWritable();    Text key = new Text();    readSeqFile(inputPath, key, mw);    Matrix m = mw.get();    ConfusionMatrix cm = new ConfusionMatrix(m);    out.println(String.format("%-40s", "Label") + TAB_SEPARATOR + String.format("%-10s", "Total") + TAB_SEPARATOR + String.format("%-10s", "Correct") + TAB_SEPARATOR + String.format("%-6s", "%") + TAB_SEPARATOR);    out.println(String.format("%-70s", "-").replace(' ', '-'));    List<String> labels = stripDefault(cm);    for (String label : labels) {        int correct = cm.getCorrect(label);        double accuracy = cm.getAccuracy(label);        int count = getCount(cm, label);        out.println(String.format("%-40s", label) + TAB_SEPARATOR + String.format("%-10s", count) + TAB_SEPARATOR + String.format("%-10s", correct) + TAB_SEPARATOR + String.format("%-6s", (int) Math.round(accuracy)) + TAB_SEPARATOR);    }    out.println(String.format("%-70s", "-").replace(' ', '-'));    out.println(cm.toString());}
0
private static void exportTable(Path inputPath, PrintStream out, boolean wrapHtml) throws IOException
{    MatrixWritable mw = new MatrixWritable();    Text key = new Text();    readSeqFile(inputPath, key, mw);    String fileName = inputPath.getName();    fileName = fileName.substring(fileName.lastIndexOf('/') + 1, fileName.length());    Matrix m = mw.get();    ConfusionMatrix cm = new ConfusionMatrix(m);    if (wrapHtml) {        printHeader(out, fileName);    }    out.println("<p/>");    printSummaryTable(cm, out);    out.println("<p/>");    printGrayTable(cm, out);    out.println("<p/>");    printCountsTable(cm, out);    out.println("<p/>");    printTextInBox(cm, out);    out.println("<p/>");    if (wrapHtml) {        printFooter(out);    }}
0
private static List<String> stripDefault(ConfusionMatrix cm)
{    List<String> stripped = Lists.newArrayList(cm.getLabels().iterator());    String defaultLabel = cm.getDefaultLabel();    int unclassified = cm.getTotal(defaultLabel);    if (unclassified > 0) {        return stripped;    }    stripped.remove(defaultLabel);    return stripped;}
0
private static void readSeqFile(Path path, Text key, MatrixWritable m) throws IOException
{    Configuration conf = new Configuration();    FileSystem fs = FileSystem.get(conf);    SequenceFile.Reader reader = new SequenceFile.Reader(fs, path, conf);    reader.next(key, m);}
0
private static PrintStream getPrintStream(String outputFilename) throws IOException
{    if (outputFilename != null) {        File outputFile = new File(outputFilename);        if (outputFile.exists()) {            outputFile.delete();        }        outputFile.createNewFile();        OutputStream os = new FileOutputStream(outputFile);        return new PrintStream(os, false, Charsets.UTF_8.displayName());    } else {        return System.out;    }}
0
private static int getLabelTotal(ConfusionMatrix cm, String rowLabel)
{    Iterator<String> iter = cm.getLabels().iterator();    int count = 0;    while (iter.hasNext()) {        count += cm.getCount(rowLabel, iter.next());    }    return count;}
0
private static void printTextInBox(ConfusionMatrix cm, PrintStream out)
{    out.println("<div style='width:90%;overflow:scroll;'>");    out.println("<pre>");    out.println(cm.toString());    out.println("</pre>");    out.println("</div>");}
0
public static void printSummaryTable(ConfusionMatrix cm, PrintStream out)
{    format("<table class='%s'>\n", out, CSS_TABLE);    format("<tr class='%s'>", out, CSS_LABEL);    out.println("<td>Label</td><td>Total</td><td>Correct</td><td>%</td>");    out.println("</tr>");    List<String> labels = stripDefault(cm);    for (String label : labels) {        printSummaryRow(cm, out, label);    }    out.println("</table>");}
0
private static void printSummaryRow(ConfusionMatrix cm, PrintStream out, String label)
{    format("<tr class='%s'>", out, CSS_CELL);    int correct = cm.getCorrect(label);    double accuracy = cm.getAccuracy(label);    int count = getCount(cm, label);    format("<td class='%s'>%s</td><td>%d</td><td>%d</td><td>%d</td>", out, CSS_CELL, label, count, correct, (int) Math.round(accuracy));    out.println("</tr>");}
0
private static int getCount(ConfusionMatrix cm, String label)
{    int count = 0;    for (String s : cm.getLabels()) {        count += cm.getCount(label, s);    }    return count;}
0
public static void printGrayTable(ConfusionMatrix cm, PrintStream out)
{    format("<table class='%s'>\n", out, CSS_TABLE);    printCountsHeader(cm, out, true);    printGrayRows(cm, out);    out.println("</table>");}
0
private static void printGrayRows(ConfusionMatrix cm, PrintStream out)
{    List<String> labels = stripDefault(cm);    for (String label : labels) {        printGrayRow(cm, out, labels, label);    }}
0
private static void printGrayRow(ConfusionMatrix cm, PrintStream out, Iterable<String> labels, String rowLabel)
{    format("<tr class='%s'>", out, CSS_LABEL);    format("<td>%s</td>", out, rowLabel);    int total = getLabelTotal(cm, rowLabel);    for (String columnLabel : labels) {        printGrayCell(cm, out, total, rowLabel, columnLabel);    }    out.println("</tr>");}
0
private static void printGrayCell(ConfusionMatrix cm, PrintStream out, int total, String rowLabel, String columnLabel)
{    int count = cm.getCount(rowLabel, columnLabel);    if (count == 0) {        out.format("<td class='%s'/>", CSS_EMPTY);    } else {                int rating = (int) ((count / (double) total) * 4);        String css = CSS_GRAY_CELLS[rating];        format("<td class='%s' title='%s'>%s</td>", out, css, columnLabel, count);    }}
0
public static void printCountsTable(ConfusionMatrix cm, PrintStream out)
{    format("<table class='%s'>\n", out, CSS_TABLE);    printCountsHeader(cm, out, false);    printCountsRows(cm, out);    out.println("</table>");}
0
private static void printCountsRows(ConfusionMatrix cm, PrintStream out)
{    List<String> labels = stripDefault(cm);    for (String label : labels) {        printCountsRow(cm, out, labels, label);    }}
0
private static void printCountsRow(ConfusionMatrix cm, PrintStream out, Iterable<String> labels, String rowLabel)
{    out.println("<tr>");    format("<td class='%s'>%s</td>", out, CSS_LABEL, rowLabel);    for (String columnLabel : labels) {        printCountsCell(cm, out, rowLabel, columnLabel);    }    out.println("</tr>");}
0
private static void printCountsCell(ConfusionMatrix cm, PrintStream out, String rowLabel, String columnLabel)
{    int count = cm.getCount(rowLabel, columnLabel);    String s = count == 0 ? "" : Integer.toString(count);    format("<td class='%s' title='%s'>%s</td>", out, CSS_CELL, columnLabel, s);}
0
private static void printCountsHeader(ConfusionMatrix cm, PrintStream out, boolean vertical)
{    List<String> labels = stripDefault(cm);    int longest = getLongestHeader(labels);    if (vertical) {                out.format("<tr class='%s' style='height:%dem'><th>&nbsp;</th>%n", CSS_TALL_HEADER, longest / 2);        for (String label : labels) {            out.format("<th><div class='%s'>%s</div></th>", CSS_VERTICAL, label);        }        out.println("</tr>");    } else {                out.format("<tr class='%s'><td class='%s'></td>%n", CSS_TABLE, CSS_LABEL);        for (String label : labels) {            out.format("<td>%s</td>", label);        }        out.format("</tr>");    }}
0
private static int getLongestHeader(Iterable<String> labels)
{    int max = 0;    for (String label : labels) {        max = Math.max(label.length(), max);    }    return max;}
0
private static void format(String format, PrintStream out, Object... args)
{    String format2 = String.format(format, args);    out.println(format2);}
0
public static void printHeader(PrintStream out, CharSequence title)
{    out.println(HEADER.replace("TITLE", title));}
0
public static void printFooter(PrintStream out)
{    out.println(FOOTER);}
0
private static List<Cluster> loadClusters(Configuration conf, Path clustersIn)
{    List<Cluster> clusters = new ArrayList<>();    for (ClusterWritable clusterWritable : new SequenceFileDirValueIterable<ClusterWritable>(clustersIn, PathType.LIST, PathFilters.logsCRCFilter(), conf)) {        Cluster cluster = clusterWritable.getValue();        clusters.add(cluster);    }    return clusters;}
0
private void computeStd(int cI)
{    List<VectorWritable> repPts = representativePoints.get(cI);    GaussianAccumulator accumulator = new OnlineGaussianAccumulator();    for (VectorWritable vw : repPts) {        accumulator.observe(vw.get(), 1.0);    }    accumulator.compute();    double d = accumulator.getAverageStd();    stDevs.put(cI, d);}
0
private double density(Vector uIJ, int cI, int cJ, double avgStd)
{    List<VectorWritable> repI = representativePoints.get(cI);    List<VectorWritable> repJ = representativePoints.get(cJ);    double sum = 0.0;        for (VectorWritable vwI : repI) {        if (uIJ != null && measure.distance(uIJ, vwI.get()) <= avgStd) {            sum++;        }    }    for (VectorWritable vwJ : repJ) {        if (uIJ != null && measure.distance(uIJ, vwJ.get()) <= avgStd) {            sum++;        }    }    int nI = repI.size();    int nJ = repJ.size();    return sum / (nI + nJ);}
0
public double getCDbw()
{    return intraClusterDensity() * separation();}
0
public double intraClusterDensity()
{    double avgDensity = 0;    int count = 0;    for (Element elem : intraClusterDensities().nonZeroes()) {        double value = elem.get();        if (!Double.isNaN(value)) {            avgDensity += value;            count++;        }    }    return avgDensity / count;}
0
public Map<Integer, Map<Integer, Double>> interClusterDensities()
{    if (interClusterDensities != null) {        return interClusterDensities;    }    interClusterDensities = new TreeMap<>();        for (int i = 0; i < clusters.size(); i++) {        int cI = clusters.get(i).getId();        Map<Integer, Double> map = new TreeMap<>();        interClusterDensities.put(cI, map);        for (int j = i + 1; j < clusters.size(); j++) {            int cJ = clusters.get(j).getId();                        double minDistance = minimumDistance(cI, cJ);                        Vector uIJ = midpointVector(cI, cJ);            double stdSum = stDevs.get(cI) + stDevs.get(cJ);            double density = density(uIJ, cI, cJ, stdSum / 2);            double interDensity = minDistance * density / stdSum;            map.put(cJ, interDensity);            if (log.isDebugEnabled()) {                                                            }        }    }    return interClusterDensities;}
1
public double separation()
{    double minDistanceSum = 0;    Map<Integer, Map<Integer, Double>> distances = minimumDistances();    for (Map<Integer, Double> map : distances.values()) {        for (Double dist : map.values()) {            if (!Double.isInfinite(dist)) {                                minDistanceSum += dist * 2;            }        }    }    return minDistanceSum / (1.0 + interClusterDensity());}
0
public double interClusterDensity()
{    if (interClusterDensity != null) {        return interClusterDensity;    }    double sum = 0.0;    int count = 0;    Map<Integer, Map<Integer, Double>> distances = interClusterDensities();    for (Map<Integer, Double> row : distances.values()) {        for (Double density : row.values()) {            if (!Double.isNaN(density)) {                sum += density;                count++;            }        }    }        interClusterDensity = sum / count;    return interClusterDensity;}
1
public Vector intraClusterDensities()
{    Vector densities = new RandomAccessSparseVector(Integer.MAX_VALUE);        double stdev = 0.0;    for (Integer cI : representativePoints.keySet()) {        stdev += stDevs.get(cI);    }    int c = representativePoints.size();    stdev /= c;    for (Cluster cluster : clusters) {        Integer cI = cluster.getId();        List<VectorWritable> repPtsI = representativePoints.get(cI);        int r = repPtsI.size();        double sumJ = 0.0;                for (VectorWritable pt : repPtsI) {                        Vector repJ = pt.get();            double densityIJ = measure.distance(cluster.getCenter(), repJ) <= stdev ? 1.0 : 0.0;                        sumJ += densityIJ / stdev;        }        densities.set(cI, sumJ / r);    }    return densities;}
0
private Map<Integer, Map<Integer, Double>> minimumDistances()
{    if (minimumDistances != null) {        return minimumDistances;    }    minimumDistances = new TreeMap<>();    closestRepPointIndices = new TreeMap<>();    for (int i = 0; i < clusters.size(); i++) {        Integer cI = clusters.get(i).getId();        Map<Integer, Double> map = new TreeMap<>();        Map<Integer, int[]> treeMap = new TreeMap<>();        closestRepPointIndices.put(cI, treeMap);        minimumDistances.put(cI, map);        List<VectorWritable> closRepI = representativePoints.get(cI);        for (int j = i + 1; j < clusters.size(); j++) {                        Integer cJ = clusters.get(j).getId();            List<VectorWritable> closRepJ = representativePoints.get(cJ);            double minDistance = Double.MAX_VALUE;            int[] midPointIndices = null;            for (int xI = 0; xI < closRepI.size(); xI++) {                VectorWritable aRepI = closRepI.get(xI);                for (int xJ = 0; xJ < closRepJ.size(); xJ++) {                    VectorWritable aRepJ = closRepJ.get(xJ);                    double distance = measure.distance(aRepI.get(), aRepJ.get());                    if (distance < minDistance) {                        minDistance = distance;                        midPointIndices = new int[] { xI, xJ };                    }                }            }            map.put(cJ, minDistance);            treeMap.put(cJ, midPointIndices);        }    }    return minimumDistances;}
0
private double minimumDistance(int cI, int cJ)
{    Map<Integer, Double> distances = minimumDistances().get(cI);    if (distances != null) {        return distances.get(cJ);    } else {        return minimumDistances().get(cJ).get(cI);    }}
0
private Vector midpointVector(int cI, int cJ)
{    Map<Integer, Double> distances = minimumDistances().get(cI);    if (distances != null) {        int[] ks = closestRepPointIndices.get(cI).get(cJ);        if (ks == null) {            return null;        }        return representativePoints.get(cI).get(ks[0]).get().plus(representativePoints.get(cJ).get(ks[1]).get()).divide(2);    } else {        int[] ks = closestRepPointIndices.get(cJ).get(cI);        if (ks == null) {            return null;        }        return representativePoints.get(cJ).get(ks[1]).get().plus(representativePoints.get(cI).get(ks[0]).get()).divide(2);    }}
0
public static void main(String[] args) throws IOException, InterruptedException, ClassNotFoundException
{    DefaultOptionBuilder obuilder = new DefaultOptionBuilder();    ArgumentBuilder abuilder = new ArgumentBuilder();    GroupBuilder gbuilder = new GroupBuilder();    Option inputOpt = DefaultOptionCreator.inputOption().withRequired(false).create();    Option outputOpt = DefaultOptionCreator.outputOption().withRequired(false).create();    Option vectorOpt = obuilder.withLongName("vector").withRequired(false).withArgument(abuilder.withName("v").withMinimum(1).withMaximum(1).create()).withDescription("The vector implementation to use.").withShortName("v").create();    Option helpOpt = DefaultOptionCreator.helpOption();    Group group = gbuilder.withName("Options").withOption(inputOpt).withOption(outputOpt).withOption(vectorOpt).withOption(helpOpt).create();    try {        Parser parser = new Parser();        parser.setGroup(group);        CommandLine cmdLine = parser.parse(args);        if (cmdLine.hasOption(helpOpt)) {            CommandLineUtil.printHelp(group);            return;        }        Path input = new Path(cmdLine.getValue(inputOpt, "testdata").toString());        Path output = new Path(cmdLine.getValue(outputOpt, "output").toString());        String vectorClassName = cmdLine.getValue(vectorOpt, "org.apache.mahout.math.RandomAccessSparseVector").toString();        runJob(input, output, vectorClassName);    } catch (OptionException e) {                CommandLineUtil.printHelp(group);    }}
1
public static void runJob(Path input, Path output, String vectorClassName) throws IOException, InterruptedException, ClassNotFoundException
{    Configuration conf = new Configuration();    conf.set("vector.implementation.class.name", vectorClassName);    Job job = new Job(conf, "Input Driver running over input: " + input);    job.setOutputKeyClass(Text.class);    job.setOutputValueClass(VectorWritable.class);    job.setOutputFormatClass(SequenceFileOutputFormat.class);    job.setMapperClass(InputMapper.class);    job.setNumReduceTasks(0);    job.setJarByClass(InputDriver.class);    FileInputFormat.addInputPath(job, input);    FileOutputFormat.setOutputPath(job, output);    boolean succeeded = job.waitForCompletion(true);    if (!succeeded) {        throw new IllegalStateException("Job failed!");    }}
0
protected void map(LongWritable key, Text values, Context context) throws IOException, InterruptedException
{    String[] numbers = SPACE.split(values.toString());        Collection<Double> doubles = new ArrayList<>();    for (String value : numbers) {        if (!value.isEmpty()) {            doubles.add(Double.valueOf(value));        }    }        if (!doubles.isEmpty()) {        try {            Vector result = (Vector) constructor.newInstance(doubles.size());            int index = 0;            for (Double d : doubles) {                result.set(index++, d);            }            VectorWritable vectorWritable = new VectorWritable(result);            context.write(new Text(String.valueOf(index)), vectorWritable);        } catch (InstantiationException | IllegalAccessException | InvocationTargetException e) {            throw new IllegalStateException(e);        }    }}
0
protected void setup(Context context) throws IOException, InterruptedException
{    super.setup(context);    Configuration conf = context.getConfiguration();    String vectorImplClassName = conf.get("vector.implementation.class.name");    try {        Class<? extends Vector> outputClass = conf.getClassByName(vectorImplClassName).asSubclass(Vector.class);        constructor = outputClass.getConstructor(int.class);    } catch (NoSuchMethodException | ClassNotFoundException e) {        throw new IllegalStateException(e);    }}
0
private static List<Cluster> loadClusters(Configuration conf, Path clustersIn)
{    List<Cluster> clusters = new ArrayList<>();    for (ClusterWritable clusterWritable : new SequenceFileDirValueIterable<ClusterWritable>(clustersIn, PathType.LIST, PathFilters.logsCRCFilter(), conf)) {        Cluster cluster = clusterWritable.getValue();        clusters.add(cluster);    }    return clusters;}
0
public double interClusterDensity()
{    double max = Double.NEGATIVE_INFINITY;    double min = Double.POSITIVE_INFINITY;    double sum = 0;    int count = 0;    Map<Integer, Vector> distances = interClusterDistances();    for (Vector row : distances.values()) {        for (Element element : row.nonZeroes()) {            double d = element.get();            min = Math.min(d, min);            max = Math.max(d, max);            sum += d;            count++;        }    }    double density = (sum / count - min) / (max - min);        return density;}
1
public Map<Integer, Vector> interClusterDistances()
{    Map<Integer, Vector> distances = new TreeMap<>();    for (int i = 0; i < clusters.size(); i++) {        Cluster clusterI = clusters.get(i);        RandomAccessSparseVector row = new RandomAccessSparseVector(Integer.MAX_VALUE);        distances.put(clusterI.getId(), row);        for (int j = i + 1; j < clusters.size(); j++) {            Cluster clusterJ = clusters.get(j);            double d = measure.distance(clusterI.getCenter(), clusterJ.getCenter());            row.set(clusterJ.getId(), d);        }    }    return distances;}
0
public double intraClusterDensity()
{    double avgDensity = 0;    int count = 0;    for (Element elem : intraClusterDensities().nonZeroes()) {        double value = elem.get();        if (!Double.isNaN(value)) {            avgDensity += value;            count++;        }    }    avgDensity = clusters.isEmpty() ? 0 : avgDensity / count;        return avgDensity;}
1
public Vector intraClusterDensities()
{    Vector densities = new RandomAccessSparseVector(Integer.MAX_VALUE);    for (Cluster cluster : clusters) {        int count = 0;        double max = Double.NEGATIVE_INFINITY;        double min = Double.POSITIVE_INFINITY;        double sum = 0;        List<VectorWritable> repPoints = representativePoints.get(cluster.getId());        for (int i = 0; i < repPoints.size(); i++) {            for (int j = i + 1; j < repPoints.size(); j++) {                Vector v1 = repPoints.get(i).get();                Vector v2 = repPoints.get(j).get();                double d = measure.distance(v1, v2);                min = Math.min(d, min);                max = Math.max(d, max);                sum += d;                count++;            }        }        double density = (sum / count - min) / (max - min);        densities.set(cluster.getId(), density);            }    return densities;}
1
public static void main(String[] args) throws Exception
{    ToolRunner.run(new Configuration(), new RepresentativePointsDriver(), args);}
0
public int run(String[] args) throws ClassNotFoundException, IOException, InterruptedException
{    addInputOption();    addOutputOption();    addOption("clusteredPoints", "cp", "The path to the clustered points", true);    addOption(DefaultOptionCreator.distanceMeasureOption().create());    addOption(DefaultOptionCreator.maxIterationsOption().create());    addOption(DefaultOptionCreator.methodOption().create());    if (parseArguments(args) == null) {        return -1;    }    Path input = getInputPath();    Path output = getOutputPath();    String distanceMeasureClass = getOption(DefaultOptionCreator.DISTANCE_MEASURE_OPTION);    int maxIterations = Integer.parseInt(getOption(DefaultOptionCreator.MAX_ITERATIONS_OPTION));    boolean runSequential = getOption(DefaultOptionCreator.METHOD_OPTION).equalsIgnoreCase(DefaultOptionCreator.SEQUENTIAL_METHOD);    DistanceMeasure measure = ClassUtils.instantiateAs(distanceMeasureClass, DistanceMeasure.class);    Path clusteredPoints = new Path(getOption("clusteredPoints"));    run(getConf(), input, clusteredPoints, output, measure, maxIterations, runSequential);    return 0;}
0
public static void printRepresentativePoints(Path output, int numIterations)
{    for (int i = 0; i <= numIterations; i++) {        Path out = new Path(output, "representativePoints-" + i);        System.out.println("Representative Points for iteration " + i);        Configuration conf = new Configuration();        for (Pair<IntWritable, VectorWritable> record : new SequenceFileDirIterable<IntWritable, VectorWritable>(out, PathType.LIST, PathFilters.logsCRCFilter(), null, true, conf)) {            System.out.println("\tC-" + record.getFirst().get() + ": " + AbstractCluster.formatVector(record.getSecond().get(), null));        }    }}
0
public static void run(Configuration conf, Path clustersIn, Path clusteredPointsIn, Path output, DistanceMeasure measure, int numIterations, boolean runSequential) throws IOException, InterruptedException, ClassNotFoundException
{    Path stateIn = new Path(output, "representativePoints-0");    writeInitialState(stateIn, clustersIn);    for (int iteration = 0; iteration < numIterations; iteration++) {                        Path stateOut = new Path(output, "representativePoints-" + (iteration + 1));        runIteration(conf, clusteredPointsIn, stateIn, stateOut, measure, runSequential);                stateIn = stateOut;    }    conf.set(STATE_IN_KEY, stateIn.toString());    conf.set(DISTANCE_MEASURE_KEY, measure.getClass().getName());}
1
private static void writeInitialState(Path output, Path clustersIn) throws IOException
{    Configuration conf = new Configuration();    FileSystem fs = FileSystem.get(output.toUri(), conf);    for (FileStatus dir : fs.globStatus(clustersIn)) {        Path inPath = dir.getPath();        for (FileStatus part : fs.listStatus(inPath, PathFilters.logsCRCFilter())) {            Path inPart = part.getPath();            Path path = new Path(output, inPart.getName());            try (SequenceFile.Writer writer = new SequenceFile.Writer(fs, conf, path, IntWritable.class, VectorWritable.class)) {                for (ClusterWritable clusterWritable : new SequenceFileValueIterable<ClusterWritable>(inPart, true, conf)) {                    Cluster cluster = clusterWritable.getValue();                    if (log.isDebugEnabled()) {                                            }                    writer.append(new IntWritable(cluster.getId()), new VectorWritable(cluster.getCenter()));                }            }        }    }}
1
private static void runIteration(Configuration conf, Path clusteredPointsIn, Path stateIn, Path stateOut, DistanceMeasure measure, boolean runSequential) throws IOException, InterruptedException, ClassNotFoundException
{    if (runSequential) {        runIterationSeq(conf, clusteredPointsIn, stateIn, stateOut, measure);    } else {        runIterationMR(conf, clusteredPointsIn, stateIn, stateOut, measure);    }}
0
private static void runIterationSeq(Configuration conf, Path clusteredPointsIn, Path stateIn, Path stateOut, DistanceMeasure measure) throws IOException
{    Map<Integer, List<VectorWritable>> repPoints = RepresentativePointsMapper.getRepresentativePoints(conf, stateIn);    Map<Integer, WeightedVectorWritable> mostDistantPoints = new HashMap<>();    FileSystem fs = FileSystem.get(clusteredPointsIn.toUri(), conf);    for (Pair<IntWritable, WeightedVectorWritable> record : new SequenceFileDirIterable<IntWritable, WeightedVectorWritable>(clusteredPointsIn, PathType.LIST, PathFilters.logsCRCFilter(), null, true, conf)) {        RepresentativePointsMapper.mapPoint(record.getFirst(), record.getSecond(), measure, repPoints, mostDistantPoints);    }    int part = 0;    try (SequenceFile.Writer writer = new SequenceFile.Writer(fs, conf, new Path(stateOut, "part-m-" + part++), IntWritable.class, VectorWritable.class)) {        for (Entry<Integer, List<VectorWritable>> entry : repPoints.entrySet()) {            for (VectorWritable vw : entry.getValue()) {                writer.append(new IntWritable(entry.getKey()), vw);            }        }    }    try (SequenceFile.Writer writer = new SequenceFile.Writer(fs, conf, new Path(stateOut, "part-m-" + part++), IntWritable.class, VectorWritable.class)) {        for (Map.Entry<Integer, WeightedVectorWritable> entry : mostDistantPoints.entrySet()) {            writer.append(new IntWritable(entry.getKey()), new VectorWritable(entry.getValue().getVector()));        }    }}
0
private static void runIterationMR(Configuration conf, Path input, Path stateIn, Path stateOut, DistanceMeasure measure) throws IOException, InterruptedException, ClassNotFoundException
{    conf.set(STATE_IN_KEY, stateIn.toString());    conf.set(DISTANCE_MEASURE_KEY, measure.getClass().getName());    Job job = new Job(conf, "Representative Points Driver running over input: " + input);    job.setJarByClass(RepresentativePointsDriver.class);    job.setOutputKeyClass(IntWritable.class);    job.setOutputValueClass(VectorWritable.class);    job.setMapOutputKeyClass(IntWritable.class);    job.setMapOutputValueClass(WeightedVectorWritable.class);    FileInputFormat.setInputPaths(job, input);    FileOutputFormat.setOutputPath(job, stateOut);    job.setMapperClass(RepresentativePointsMapper.class);    job.setReducerClass(RepresentativePointsReducer.class);    job.setInputFormatClass(SequenceFileInputFormat.class);    job.setOutputFormatClass(SequenceFileOutputFormat.class);    boolean succeeded = job.waitForCompletion(true);    if (!succeeded) {        throw new IllegalStateException("Job failed!");    }}
0
protected void cleanup(Context context) throws IOException, InterruptedException
{    for (Map.Entry<Integer, WeightedVectorWritable> entry : mostDistantPoints.entrySet()) {        context.write(new IntWritable(entry.getKey()), entry.getValue());    }    super.cleanup(context);}
0
protected void map(IntWritable clusterId, WeightedVectorWritable point, Context context) throws IOException, InterruptedException
{    mapPoint(clusterId, point, measure, representativePoints, mostDistantPoints);}
0
public static void mapPoint(IntWritable clusterId, WeightedVectorWritable point, DistanceMeasure measure, Map<Integer, List<VectorWritable>> representativePoints, Map<Integer, WeightedVectorWritable> mostDistantPoints)
{    int key = clusterId.get();    WeightedVectorWritable currentMDP = mostDistantPoints.get(key);    List<VectorWritable> repPoints = representativePoints.get(key);    double totalDistance = 0.0;    if (repPoints != null) {        for (VectorWritable refPoint : repPoints) {            totalDistance += measure.distance(refPoint.get(), point.getVector());        }    }    if (currentMDP == null || currentMDP.getWeight() < totalDistance) {        mostDistantPoints.put(key, new WeightedVectorWritable(totalDistance, point.getVector().clone()));    }}
0
protected void setup(Context context) throws IOException, InterruptedException
{    super.setup(context);    Configuration conf = context.getConfiguration();    measure = ClassUtils.instantiateAs(conf.get(RepresentativePointsDriver.DISTANCE_MEASURE_KEY), DistanceMeasure.class);    representativePoints = getRepresentativePoints(conf);}
0
public void configure(Map<Integer, List<VectorWritable>> referencePoints, DistanceMeasure measure)
{    this.representativePoints = referencePoints;    this.measure = measure;}
0
public static Map<Integer, List<VectorWritable>> getRepresentativePoints(Configuration conf)
{    String statePath = conf.get(RepresentativePointsDriver.STATE_IN_KEY);    return getRepresentativePoints(conf, new Path(statePath));}
0
public static Map<Integer, List<VectorWritable>> getRepresentativePoints(Configuration conf, Path statePath)
{    Map<Integer, List<VectorWritable>> representativePoints = new HashMap<>();    for (Pair<IntWritable, VectorWritable> record : new SequenceFileDirIterable<IntWritable, VectorWritable>(statePath, PathType.LIST, PathFilters.logsCRCFilter(), conf)) {        int keyValue = record.getFirst().get();        List<VectorWritable> repPoints = representativePoints.get(keyValue);        if (repPoints == null) {            repPoints = new ArrayList<>();            representativePoints.put(keyValue, repPoints);        }        repPoints.add(record.getSecond());    }    return representativePoints;}
0
protected void cleanup(Context context) throws IOException, InterruptedException
{    for (Map.Entry<Integer, List<VectorWritable>> entry : representativePoints.entrySet()) {        IntWritable iw = new IntWritable(entry.getKey());        for (VectorWritable vw : entry.getValue()) {            context.write(iw, vw);        }    }    super.cleanup(context);}
0
protected void reduce(IntWritable key, Iterable<WeightedVectorWritable> values, Context context) throws IOException, InterruptedException
{        WeightedVectorWritable mdp = null;    for (WeightedVectorWritable dpw : values) {        if (mdp == null || mdp.getWeight() < dpw.getWeight()) {            mdp = new WeightedVectorWritable(dpw.getWeight(), dpw.getVector());        }    }    context.write(new IntWritable(key.get()), new VectorWritable(mdp.getVector()));}
0
protected void setup(Context context) throws IOException, InterruptedException
{    super.setup(context);    Configuration conf = context.getConfiguration();    representativePoints = RepresentativePointsMapper.getRepresentativePoints(conf);}
0
public void configure(Map<Integer, List<VectorWritable>> representativePoints)
{    this.representativePoints = representativePoints;}
0
private static void ensureQueueSize(Collection<Queue<Pair<String, Double>>> queues, int k)
{    for (int i = queues.size(); i <= k; ++i) {        queues.add(new PriorityQueue<Pair<String, Double>>());    }}
0
public static void main(String[] args) throws Exception
{    DefaultOptionBuilder obuilder = new DefaultOptionBuilder();    ArgumentBuilder abuilder = new ArgumentBuilder();    GroupBuilder gbuilder = new GroupBuilder();    Option inputOpt = DefaultOptionCreator.inputOption().create();    Option dictOpt = obuilder.withLongName("dict").withRequired(true).withArgument(abuilder.withName("dict").withMinimum(1).withMaximum(1).create()).withDescription("Dictionary to read in, in the same format as one created by " + "org.apache.mahout.utils.vectors.lucene.Driver").withShortName("d").create();    Option outOpt = DefaultOptionCreator.outputOption().create();    Option wordOpt = obuilder.withLongName("words").withRequired(false).withArgument(abuilder.withName("words").withMinimum(0).withMaximum(1).withDefault("20").create()).withDescription("Number of words to print").withShortName("w").create();    Option dictTypeOpt = obuilder.withLongName("dictionaryType").withRequired(false).withArgument(abuilder.withName("dictionaryType").withMinimum(1).withMaximum(1).create()).withDescription("The dictionary file type (text|sequencefile)").withShortName("dt").create();    Option helpOpt = obuilder.withLongName("help").withDescription("Print out help").withShortName("h").create();    Group group = gbuilder.withName("Options").withOption(dictOpt).withOption(outOpt).withOption(wordOpt).withOption(inputOpt).withOption(dictTypeOpt).create();    try {        Parser parser = new Parser();        parser.setGroup(group);        CommandLine cmdLine = parser.parse(args);        if (cmdLine.hasOption(helpOpt)) {            CommandLineUtil.printHelp(group);            return;        }        String input = cmdLine.getValue(inputOpt).toString();        String dictFile = cmdLine.getValue(dictOpt).toString();        int numWords = 20;        if (cmdLine.hasOption(wordOpt)) {            numWords = Integer.parseInt(cmdLine.getValue(wordOpt).toString());        }        Configuration config = new Configuration();        String dictionaryType = "text";        if (cmdLine.hasOption(dictTypeOpt)) {            dictionaryType = cmdLine.getValue(dictTypeOpt).toString();        }        List<String> wordList;        if ("text".equals(dictionaryType)) {            wordList = Arrays.asList(VectorHelper.loadTermDictionary(new File(dictFile)));        } else if ("sequencefile".equals(dictionaryType)) {            wordList = Arrays.asList(VectorHelper.loadTermDictionary(config, dictFile));        } else {            throw new IllegalArgumentException("Invalid dictionary format");        }        List<Queue<Pair<String, Double>>> topWords = topWordsForTopics(input, config, wordList, numWords);        File output = null;        if (cmdLine.hasOption(outOpt)) {            output = new File(cmdLine.getValue(outOpt).toString());            if (!output.exists() && !output.mkdirs()) {                throw new IOException("Could not create directory: " + output);            }        }        printTopWords(topWords, output);    } catch (OptionException e) {        CommandLineUtil.printHelp(group);        throw e;    }}
0
private static void maybeEnqueue(Queue<Pair<String, Double>> q, String word, double score, int numWordsToPrint)
{    if (q.size() >= numWordsToPrint && score > q.peek().getSecond()) {        q.poll();    }    if (q.size() < numWordsToPrint) {        q.add(new Pair<>(word, score));    }}
0
private static void printTopWords(List<Queue<Pair<String, Double>>> topWords, File outputDir) throws IOException
{    for (int i = 0; i < topWords.size(); ++i) {        Collection<Pair<String, Double>> topK = topWords.get(i);        Writer out = null;        boolean printingToSystemOut = false;        try {            if (outputDir != null) {                out = new OutputStreamWriter(new FileOutputStream(new File(outputDir, "topic_" + i)), Charsets.UTF_8);            } else {                out = new OutputStreamWriter(System.out, Charsets.UTF_8);                printingToSystemOut = true;                out.write("Topic " + i);                out.write('\n');                out.write("===========");                out.write('\n');            }            List<Pair<String, Double>> topKasList = new ArrayList<>(topK.size());            for (Pair<String, Double> wordWithScore : topK) {                topKasList.add(wordWithScore);            }            Collections.sort(topKasList, new Comparator<Pair<String, Double>>() {                @Override                public int compare(Pair<String, Double> pair1, Pair<String, Double> pair2) {                    return pair2.getSecond().compareTo(pair1.getSecond());                }            });            for (Pair<String, Double> wordWithScore : topKasList) {                out.write(wordWithScore.getFirst() + " [p(" + wordWithScore.getFirst() + "|topic_" + i + ") = " + wordWithScore.getSecond());                out.write('\n');            }        } finally {            if (!printingToSystemOut) {                Closeables.close(out, false);            } else {                out.flush();            }        }    }}
0
public int compare(Pair<String, Double> pair1, Pair<String, Double> pair2)
{    return pair2.getSecond().compareTo(pair1.getSecond());}
0
private static List<Queue<Pair<String, Double>>> topWordsForTopics(String dir, Configuration job, List<String> wordList, int numWordsToPrint)
{    List<Queue<Pair<String, Double>>> queues = new ArrayList<>();    Map<Integer, Double> expSums = new HashMap<>();    for (Pair<IntPairWritable, DoubleWritable> record : new SequenceFileDirIterable<IntPairWritable, DoubleWritable>(new Path(dir, "part-*"), PathType.GLOB, null, null, true, job)) {        IntPairWritable key = record.getFirst();        int topic = key.getFirst();        int word = key.getSecond();        ensureQueueSize(queues, topic);        if (word >= 0 && topic >= 0) {            double score = record.getSecond().get();            if (expSums.get(topic) == null) {                expSums.put(topic, 0.0);            }            expSums.put(topic, expSums.get(topic) + Math.exp(score));            String realWord = wordList.get(word);            maybeEnqueue(queues.get(topic), realWord, score, numWordsToPrint);        }    }    for (int i = 0; i < queues.size(); i++) {        Queue<Pair<String, Double>> queue = queues.get(i);        Queue<Pair<String, Double>> newQueue = new PriorityQueue<>(queue.size());        double norm = expSums.get(i);        for (Pair<String, Double> pair : queue) {            newQueue.add(new Pair<>(pair.getFirst(), Math.exp(pair.getSecond()) / norm));        }        queues.set(i, newQueue);    }    return queues;}
0
protected TokenStreamComponents createComponents(String fieldName)
{    Tokenizer tokenizer = new StandardTokenizer();    TokenStream result = new StandardFilter(tokenizer);    result = new LowerCaseFilter(result);    result = new ASCIIFoldingFilter(result);    result = new AlphaNumericMaxLengthFilter(result);    result = new StopFilter(result, STOP_SET);    result = new PorterStemFilter(result);    return new TokenStreamComponents(tokenizer, result);}
0
public final boolean incrementToken() throws IOException
{        while (input.incrementToken()) {        int length = termAtt.length();        if (length >= 2 && length <= 28) {            char[] buf = termAtt.buffer();            int at = 0;            for (int c = 0; c < length; c++) {                char ch = buf[c];                if (ch != '\'') {                    output[at++] = ch;                }            }            String term = new String(output, 0, at);            MATCHER.reset(term);            if (MATCHER.matches() && !term.startsWith("a0")) {                termAtt.setEmpty();                termAtt.append(term);                return true;            }        }    }    return false;}
0
public RecordReader<IntWritable, BytesWritable> createRecordReader(InputSplit inputSplit, TaskAttemptContext taskAttemptContext) throws IOException
{    return new CombineFileRecordReader<>((CombineFileSplit) inputSplit, taskAttemptContext, WholeFileRecordReader.class);}
0
protected void process(FileStatus fst, Path current) throws IOException
{    FileSystem fs = getFs();    ChunkedWriter writer = getWriter();    if (fst.isDir()) {        String dirPath = getPrefix() + Path.SEPARATOR + current.getName() + Path.SEPARATOR + fst.getPath().getName();        fs.listStatus(fst.getPath(), new PrefixAdditionFilter(getConf(), dirPath, getOptions(), writer, getCharset(), fs));    } else {        try (InputStream in = fs.open(fst.getPath())) {            StringBuilder file = new StringBuilder();            for (String aFit : new FileLineIterable(in, getCharset(), false)) {                file.append(aFit).append('\n');            }            String name = current.getName().equals(fst.getPath().getName()) ? current.getName() : current.getName() + Path.SEPARATOR + fst.getPath().getName();            writer.write(getPrefix() + Path.SEPARATOR + name, file.toString());        }    }}
0
public static void main(String[] args) throws Exception
{    ToolRunner.run(new SequenceFilesFromDirectory(), args);}
0
public int run(String[] args) throws Exception
{    addOptions();    addOption(DefaultOptionCreator.methodOption().create());    addOption(DefaultOptionCreator.overwriteOption().create());    if (parseArguments(args) == null) {        return -1;    }    Map<String, String> options = parseOptions();    Path output = getOutputPath();    if (hasOption(DefaultOptionCreator.OVERWRITE_OPTION)) {        HadoopUtil.delete(getConf(), output);    }    if (getOption(DefaultOptionCreator.METHOD_OPTION, DefaultOptionCreator.MAPREDUCE_METHOD).equals(DefaultOptionCreator.SEQUENTIAL_METHOD)) {        runSequential(getConf(), getInputPath(), output, options);    } else {        runMapReduce(getInputPath(), output);    }    return 0;}
0
private int runSequential(Configuration conf, Path input, Path output, Map<String, String> options) throws IOException, InterruptedException, NoSuchMethodException
{        Charset charset = Charset.forName(getOption(CHARSET_OPTION[0]));    String keyPrefix = getOption(KEY_PREFIX_OPTION[0]);    FileSystem fs = FileSystem.get(input.toUri(), conf);    try (ChunkedWriter writer = new ChunkedWriter(conf, Integer.parseInt(options.get(CHUNK_SIZE_OPTION[0])), output)) {        SequenceFilesFromDirectoryFilter pathFilter;        String fileFilterClassName = options.get(FILE_FILTER_CLASS_OPTION[0]);        if (PrefixAdditionFilter.class.getName().equals(fileFilterClassName)) {            pathFilter = new PrefixAdditionFilter(conf, keyPrefix, options, writer, charset, fs);        } else {            pathFilter = ClassUtils.instantiateAs(fileFilterClassName, SequenceFilesFromDirectoryFilter.class, new Class[] { Configuration.class, String.class, Map.class, ChunkedWriter.class, Charset.class, FileSystem.class }, new Object[] { conf, keyPrefix, options, writer, charset, fs });        }        fs.listStatus(input, pathFilter);    }    return 0;}
0
private int runMapReduce(Path input, Path output) throws IOException, ClassNotFoundException, InterruptedException
{    int chunkSizeInMB = 64;    if (hasOption(CHUNK_SIZE_OPTION[0])) {        chunkSizeInMB = Integer.parseInt(getOption(CHUNK_SIZE_OPTION[0]));    }    String keyPrefix = null;    if (hasOption(KEY_PREFIX_OPTION[0])) {        keyPrefix = getOption(KEY_PREFIX_OPTION[0]);    }    String fileFilterClassName = null;    if (hasOption(FILE_FILTER_CLASS_OPTION[0])) {        fileFilterClassName = getOption(FILE_FILTER_CLASS_OPTION[0]);    }    PathFilter pathFilter = null;        if (!StringUtils.isBlank(fileFilterClassName) && !PrefixAdditionFilter.class.getName().equals(fileFilterClassName)) {        try {            pathFilter = (PathFilter) Class.forName(fileFilterClassName).newInstance();        } catch (InstantiationException | IllegalAccessException e) {            throw new IllegalStateException(e);        }    }        Job job = prepareJob(input, output, MultipleTextFileInputFormat.class, SequenceFilesFromDirectoryMapper.class, Text.class, Text.class, SequenceFileOutputFormat.class, "SequenceFilesFromDirectory");    Configuration jobConfig = job.getConfiguration();    jobConfig.set(KEY_PREFIX_OPTION[0], keyPrefix);    jobConfig.set(FILE_FILTER_CLASS_OPTION[0], fileFilterClassName);    FileSystem fs = FileSystem.get(jobConfig);    FileStatus fsFileStatus = fs.getFileStatus(input);    String inputDirList;    if (pathFilter != null) {        inputDirList = HadoopUtil.buildDirList(fs, fsFileStatus, pathFilter);    } else {        inputDirList = HadoopUtil.buildDirList(fs, fsFileStatus);    }    jobConfig.set(BASE_INPUT_PATH, input.toString());    long chunkSizeInBytes = chunkSizeInMB * 1024 * 1024;        jobConfig.set("mapreduce.job.max.split.locations", String.valueOf(MAX_JOB_SPLIT_LOCATIONS));    FileInputFormat.setInputPaths(job, inputDirList);        FileInputFormat.setMaxInputSplitSize(job, chunkSizeInBytes);    FileOutputFormat.setCompressOutput(job, true);    boolean succeeded = job.waitForCompletion(true);    if (!succeeded) {        return -1;    }    return 0;}
0
protected void addOptions()
{    addInputOption();    addOutputOption();    addOption(DefaultOptionCreator.overwriteOption().create());    addOption(DefaultOptionCreator.methodOption().create());    addOption(CHUNK_SIZE_OPTION[0], CHUNK_SIZE_OPTION[1], "The chunkSize in MegaBytes. Defaults to 64", "64");    addOption(FILE_FILTER_CLASS_OPTION[0], FILE_FILTER_CLASS_OPTION[1], "The name of the class to use for file parsing. Default: " + PREFIX_ADDITION_FILTER, PREFIX_ADDITION_FILTER);    addOption(KEY_PREFIX_OPTION[0], KEY_PREFIX_OPTION[1], "The prefix to be prepended to the key", "");    addOption(CHARSET_OPTION[0], CHARSET_OPTION[1], "The name of the character encoding of the input files. Default to UTF-8", "UTF-8");}
0
protected Map<String, String> parseOptions()
{    Map<String, String> options = new HashMap<>();    options.put(CHUNK_SIZE_OPTION[0], getOption(CHUNK_SIZE_OPTION[0]));    options.put(FILE_FILTER_CLASS_OPTION[0], getOption(FILE_FILTER_CLASS_OPTION[0]));    options.put(CHARSET_OPTION[0], getOption(CHARSET_OPTION[0]));    return options;}
0
protected final String getPrefix()
{    return prefix;}
0
protected final ChunkedWriter getWriter()
{    return writer;}
0
protected final Charset getCharset()
{    return charset;}
0
protected final FileSystem getFs()
{    return fs;}
0
protected final Map<String, String> getOptions()
{    return options;}
0
protected final Configuration getConf()
{    return conf;}
0
public final boolean accept(Path current)
{        try {        for (FileStatus fst : fs.listStatus(current)) {                        process(fst, current);        }    } catch (IOException ioe) {        throw new IllegalStateException(ioe);    }    return false;}
1
protected void setup(Context context) throws IOException, InterruptedException
{    super.setup(context);    this.keyPrefix = context.getConfiguration().get(KEY_PREFIX_OPTION[0], "");}
0
public void map(IntWritable key, BytesWritable value, Context context) throws IOException, InterruptedException
{    Configuration configuration = context.getConfiguration();    Path filePath = ((CombineFileSplit) context.getInputSplit()).getPath(key.get());    String relativeFilePath = HadoopUtil.calcRelativeFilePath(configuration, filePath);    String filename = this.keyPrefix.length() > 0 ? this.keyPrefix + Path.SEPARATOR + relativeFilePath : Path.SEPARATOR + relativeFilePath;    fileValue.set(value.getBytes(), 0, value.getBytes().length);    context.write(new Text(filename), fileValue);}
0
public void createSequenceFiles(MailOptions options) throws IOException
{    try (ChunkedWriter writer = new ChunkedWriter(getConf(), options.getChunkSize(), new Path(options.getOutputDir()))) {        MailProcessor processor = new MailProcessor(options, options.getPrefix(), writer);        if (options.getInput().isDirectory()) {            PrefixAdditionDirectoryWalker walker = new PrefixAdditionDirectoryWalker(processor, writer);            walker.walk(options.getInput());                    } else {            long start = System.currentTimeMillis();            long cnt = processor.parseMboxLineByLine(options.getInput());            long finish = System.currentTimeMillis();                    }    }}
1
public void walk(File startDirectory) throws IOException
{    super.walk(startDirectory, null);}
0
public long getMessageCount()
{    return messageCounts.getFirst();}
0
protected void handleDirectoryStart(File current, int depth, Collection<Object> results) throws IOException
{    if (depth > 0) {                MailProcessor processor = processors.getFirst();        MailProcessor subDirProcessor = new MailProcessor(processor.getOptions(), processor.getPrefix() + File.separator + current.getName(), writer);        processors.push(subDirProcessor);        messageCounts.push(0L);    }}
1
protected File[] filterDirectoryContents(File directory, int depth, File[] files) throws IOException
{    Arrays.sort(files, FILE_COMPARATOR);    return files;}
0
protected void handleFile(File current, int depth, Collection<Object> results) throws IOException
{    MailProcessor processor = processors.getFirst();    long currentDirMessageCount = messageCounts.pop();    try {        currentDirMessageCount += processor.parseMboxLineByLine(current);    } catch (IOException e) {        throw new IllegalStateException("Error processing " + current, e);    }    messageCounts.push(currentDirMessageCount);}
0
protected void handleDirectoryEnd(File current, int depth, Collection<Object> results) throws IOException
{    if (depth > 0) {        final long currentDirMessageCount = messageCounts.pop();                processors.pop();                long parentDirMessageCount = messageCounts.pop();        parentDirMessageCount += currentDirMessageCount;        messageCounts.push(parentDirMessageCount);    }}
1
public static void main(String[] args) throws Exception
{    ToolRunner.run(new Configuration(), new SequenceFilesFromMailArchives(), args);}
0
public int run(String[] args) throws Exception
{    addInputOption();    addOutputOption();    addOption(DefaultOptionCreator.methodOption().create());    addOption(CHUNK_SIZE_OPTION[0], CHUNK_SIZE_OPTION[1], "The chunkSize in MegaBytes. Defaults to 64", "64");    addOption(KEY_PREFIX_OPTION[0], KEY_PREFIX_OPTION[1], "The prefix to be prepended to the key", "");    addOption(CHARSET_OPTION[0], CHARSET_OPTION[1], "The name of the character encoding of the input files. Default to UTF-8", "UTF-8");    addFlag(SUBJECT_OPTION[0], SUBJECT_OPTION[1], "Include the Mail subject as part of the text.  Default is false");    addFlag(TO_OPTION[0], TO_OPTION[1], "Include the to field in the text.  Default is false");    addFlag(FROM_OPTION[0], FROM_OPTION[1], "Include the from field in the text.  Default is false");    addFlag(REFERENCES_OPTION[0], REFERENCES_OPTION[1], "Include the references field in the text.  Default is false");    addFlag(BODY_OPTION[0], BODY_OPTION[1], "Include the body in the output.  Default is false");    addFlag(STRIP_QUOTED_OPTION[0], STRIP_QUOTED_OPTION[1], "Strip (remove) quoted email text in the body.  Default is false");    addOption(QUOTED_REGEX_OPTION[0], QUOTED_REGEX_OPTION[1], "Specify the regex that identifies quoted text.  " + "Default is to look for > or | at the beginning of the line.");    addOption(SEPARATOR_OPTION[0], SEPARATOR_OPTION[1], "The separator to use between metadata items (to, from, etc.).  Default is \\n", "\n");    addOption(BODY_SEPARATOR_OPTION[0], BODY_SEPARATOR_OPTION[1], "The separator to use between lines in the body.  Default is \\n.  " + "Useful to change if you wish to have the message be on one line", "\n");    addOption(DefaultOptionCreator.helpOption());    Map<String, List<String>> parsedArgs = parseArguments(args);    if (parsedArgs == null) {        return -1;    }    File input = getInputFile();    String outputDir = getOutputPath().toString();    int chunkSize = 64;    if (hasOption(CHUNK_SIZE_OPTION[0])) {        chunkSize = Integer.parseInt(getOption(CHUNK_SIZE_OPTION[0]));    }    String prefix = "";    if (hasOption(KEY_PREFIX_OPTION[0])) {        prefix = getOption(KEY_PREFIX_OPTION[0]);    }    Charset charset = Charset.forName(getOption(CHARSET_OPTION[0]));    MailOptions options = new MailOptions();    options.setInput(input);    options.setOutputDir(outputDir);    options.setPrefix(prefix);    options.setChunkSize(chunkSize);    options.setCharset(charset);    List<Pattern> patterns = new ArrayList<>(5);                Map<String, Integer> patternOrder = new HashMap<>();    int order = 0;    if (hasOption(FROM_OPTION[0])) {        patterns.add(MailProcessor.FROM_PREFIX);        patternOrder.put(MailOptions.FROM, order++);    }    if (hasOption(TO_OPTION[0])) {        patterns.add(MailProcessor.TO_PREFIX);        patternOrder.put(MailOptions.TO, order++);    }    if (hasOption(REFERENCES_OPTION[0])) {        patterns.add(MailProcessor.REFS_PREFIX);        patternOrder.put(MailOptions.REFS, order++);    }    if (hasOption(SUBJECT_OPTION[0])) {        patterns.add(MailProcessor.SUBJECT_PREFIX);        patternOrder.put(MailOptions.SUBJECT, order += 1);    }    options.setStripQuotedText(hasOption(STRIP_QUOTED_OPTION[0]));    options.setPatternsToMatch(patterns.toArray(new Pattern[patterns.size()]));    options.setPatternOrder(patternOrder);    options.setIncludeBody(hasOption(BODY_OPTION[0]));    if (hasOption(SEPARATOR_OPTION[0])) {        options.setSeparator(getOption(SEPARATOR_OPTION[0]));    } else {        options.setSeparator("\n");    }    if (hasOption(BODY_SEPARATOR_OPTION[0])) {        options.setBodySeparator(getOption(BODY_SEPARATOR_OPTION[0]));    }    if (hasOption(QUOTED_REGEX_OPTION[0])) {        options.setQuotedTextPattern(Pattern.compile(getOption(QUOTED_REGEX_OPTION[0])));    }    if (getOption(DefaultOptionCreator.METHOD_OPTION, DefaultOptionCreator.MAPREDUCE_METHOD).equals(DefaultOptionCreator.SEQUENTIAL_METHOD)) {        runSequential(options);    } else {        runMapReduce(getInputPath(), getOutputPath());    }    return 0;}
0
private int runSequential(MailOptions options) throws IOException, InterruptedException, NoSuchMethodException
{    long start = System.currentTimeMillis();    createSequenceFiles(options);    long finish = System.currentTimeMillis();        return 0;}
1
private int runMapReduce(Path input, Path output) throws IOException, InterruptedException, ClassNotFoundException
{    Job job = prepareJob(input, output, MultipleTextFileInputFormat.class, SequenceFilesFromMailArchivesMapper.class, Text.class, Text.class, SequenceFileOutputFormat.class, "SequentialFilesFromMailArchives");    Configuration jobConfig = job.getConfiguration();    if (hasOption(KEY_PREFIX_OPTION[0])) {        jobConfig.set(KEY_PREFIX_OPTION[1], getOption(KEY_PREFIX_OPTION[0]));    }    int chunkSize = 0;    if (hasOption(CHUNK_SIZE_OPTION[0])) {        chunkSize = Integer.parseInt(getOption(CHUNK_SIZE_OPTION[0]));        jobConfig.set(CHUNK_SIZE_OPTION[0], String.valueOf(chunkSize));    }    Charset charset;    if (hasOption(CHARSET_OPTION[0])) {        charset = Charset.forName(getOption(CHARSET_OPTION[0]));        jobConfig.set(CHARSET_OPTION[0], charset.displayName());    }    if (hasOption(FROM_OPTION[0])) {        jobConfig.set(FROM_OPTION[1], "true");    }    if (hasOption(TO_OPTION[0])) {        jobConfig.set(TO_OPTION[1], "true");    }    if (hasOption(REFERENCES_OPTION[0])) {        jobConfig.set(REFERENCES_OPTION[1], "true");    }    if (hasOption(SUBJECT_OPTION[0])) {        jobConfig.set(SUBJECT_OPTION[1], "true");    }    if (hasOption(QUOTED_REGEX_OPTION[0])) {        jobConfig.set(QUOTED_REGEX_OPTION[1], Pattern.compile(getOption(QUOTED_REGEX_OPTION[0])).toString());    }    if (hasOption(SEPARATOR_OPTION[0])) {        jobConfig.set(SEPARATOR_OPTION[1], getOption(SEPARATOR_OPTION[0]));    } else {        jobConfig.set(SEPARATOR_OPTION[1], "\n");    }    if (hasOption(BODY_OPTION[0])) {        jobConfig.set(BODY_OPTION[1], "true");    } else {        jobConfig.set(BODY_OPTION[1], "false");    }    if (hasOption(BODY_SEPARATOR_OPTION[0])) {        jobConfig.set(BODY_SEPARATOR_OPTION[1], getOption(BODY_SEPARATOR_OPTION[0]));    } else {        jobConfig.set(BODY_SEPARATOR_OPTION[1], "\n");    }    FileSystem fs = FileSystem.get(jobConfig);    FileStatus fsFileStatus = fs.getFileStatus(inputPath);    jobConfig.set(BASE_INPUT_PATH, inputPath.toString());    String inputDirList = HadoopUtil.buildDirList(fs, fsFileStatus);    FileInputFormat.setInputPaths(job, inputDirList);    long chunkSizeInBytes = chunkSize * 1024 * 1024;        FileInputFormat.setMaxInputSplitSize(job, chunkSizeInBytes);        jobConfig.set("mapreduce.job.max.split.locations", String.valueOf(MAX_JOB_SPLIT_LOCATIONS));    boolean succeeded = job.waitForCompletion(true);    if (!succeeded) {        return -1;    }    return 0;}
0
public void setup(Context context) throws IOException, InterruptedException
{    Configuration configuration = context.getConfiguration();        this.options = new MailOptions();    options.setPrefix(configuration.get(KEY_PREFIX_OPTION[1], ""));    if (!configuration.get(CHUNK_SIZE_OPTION[0], "").equals("")) {        options.setChunkSize(configuration.getInt(CHUNK_SIZE_OPTION[0], 64));    }    if (!configuration.get(CHARSET_OPTION[0], "").equals("")) {        Charset charset = Charset.forName(configuration.get(CHARSET_OPTION[0], "UTF-8"));        options.setCharset(charset);    } else {        Charset charset = Charset.forName("UTF-8");        options.setCharset(charset);    }    List<Pattern> patterns = Lists.newArrayListWithCapacity(5);                        Map<String, Integer> patternOrder = Maps.newHashMap();    int order = 0;    if (!configuration.get(FROM_OPTION[1], "").equals("")) {        patterns.add(MailProcessor.FROM_PREFIX);        patternOrder.put(MailOptions.FROM, order++);    }    if (!configuration.get(TO_OPTION[1], "").equals("")) {        patterns.add(MailProcessor.TO_PREFIX);        patternOrder.put(MailOptions.TO, order++);    }    if (!configuration.get(REFERENCES_OPTION[1], "").equals("")) {        patterns.add(MailProcessor.REFS_PREFIX);        patternOrder.put(MailOptions.REFS, order++);    }    if (!configuration.get(SUBJECT_OPTION[1], "").equals("")) {        patterns.add(MailProcessor.SUBJECT_PREFIX);        patternOrder.put(MailOptions.SUBJECT, order += 1);    }    options.setStripQuotedText(configuration.getBoolean(STRIP_QUOTED_OPTION[1], false));    options.setPatternsToMatch(patterns.toArray(new Pattern[patterns.size()]));    options.setPatternOrder(patternOrder);    options.setIncludeBody(configuration.getBoolean(BODY_OPTION[1], false));    options.setSeparator("\n");    if (!configuration.get(SEPARATOR_OPTION[1], "").equals("")) {        options.setSeparator(configuration.get(SEPARATOR_OPTION[1], ""));    }    if (!configuration.get(BODY_SEPARATOR_OPTION[1], "").equals("")) {        options.setBodySeparator(configuration.get(BODY_SEPARATOR_OPTION[1], ""));    }    if (!configuration.get(QUOTED_REGEX_OPTION[1], "").equals("")) {        options.setQuotedTextPattern(Pattern.compile(configuration.get(QUOTED_REGEX_OPTION[1], "")));    }}
0
public long parseMailboxLineByLine(String filename, InputStream mailBoxInputStream, Context context) throws IOException, InterruptedException
{    long messageCount = 0;    try {        StringBuilder contents = new StringBuilder();        StringBuilder body = new StringBuilder();        Matcher messageIdMatcher = MESSAGE_ID_PREFIX.matcher("");        Matcher messageBoundaryMatcher = MESSAGE_START.matcher("");        String[] patternResults = new String[options.getPatternsToMatch().length];        Matcher[] matches = new Matcher[options.getPatternsToMatch().length];        for (int i = 0; i < matches.length; i++) {            matches[i] = options.getPatternsToMatch()[i].matcher("");        }        String messageId = null;        boolean inBody = false;        Pattern quotedTextPattern = options.getQuotedTextPattern();        for (String nextLine : new FileLineIterable(mailBoxInputStream, options.getCharset(), false, filename)) {            if (!options.isStripQuotedText() || !quotedTextPattern.matcher(nextLine).find()) {                for (int i = 0; i < matches.length; i++) {                    Matcher matcher = matches[i];                    matcher.reset(nextLine);                    if (matcher.matches()) {                        patternResults[i] = matcher.group(1);                    }                }                                if (messageId != null) {                                        messageBoundaryMatcher.reset(nextLine);                    if (messageBoundaryMatcher.matches()) {                                                String key = generateKey(filename, options.getPrefix(), messageId);                                                                        writeContent(options.getSeparator(), contents, body, patternResults);                        this.outKey.set(key);                        this.outValue.set(contents.toString());                        context.write(this.outKey, this.outValue);                                                contents.setLength(0);                        body.setLength(0);                        messageId = null;                        inBody = false;                    } else {                        if (inBody && options.isIncludeBody()) {                            if (!nextLine.isEmpty()) {                                body.append(nextLine).append(options.getBodySeparator());                            }                        } else {                                                                                    inBody = nextLine.isEmpty();                        }                    }                } else {                    if (nextLine.length() > 14) {                        messageIdMatcher.reset(nextLine);                        if (messageIdMatcher.matches()) {                            messageId = messageIdMatcher.group(1);                            ++messageCount;                        }                    }                }            }        }                if (messageId != null) {            String key = generateKey(filename, options.getPrefix(), messageId);            writeContent(options.getSeparator(), contents, body, patternResults);            this.outKey.set(key);            this.outValue.set(contents.toString());            context.write(this.outKey, this.outValue);                        contents.setLength(0);        }    } catch (FileNotFoundException ignored) {    }    return messageCount;}
0
protected static String generateKey(String mboxFilename, String prefix, String messageId)
{    return Joiner.on(Path.SEPARATOR).join(Lists.newArrayList(prefix, mboxFilename, messageId).iterator());}
0
private static void writeContent(String separator, StringBuilder contents, CharSequence body, String[] matches)
{    String matchesString = Joiner.on(separator).useForNull("").join(Arrays.asList(matches).iterator());    contents.append(matchesString).append(separator).append(body);}
0
public void map(IntWritable key, BytesWritable value, Context context) throws IOException, InterruptedException
{    Configuration configuration = context.getConfiguration();    Path filePath = ((CombineFileSplit) context.getInputSplit()).getPath(key.get());    String relativeFilePath = HadoopUtil.calcRelativeFilePath(configuration, filePath);    ByteArrayInputStream is = new ByteArrayInputStream(value.getBytes());    parseMailboxLineByLine(relativeFilePath, is, context);}
0
public int run(String[] strings) throws Exception
{    Configuration originalConf = getConf();    Job job = prepareJob(new Path(originalConf.get("mapred.input.dir")), new Path(originalConf.get("mapred.output.dir")), SequenceFileInputFormat.class, SplitMap.class, Text.class, Text.class, Reducer.class, Text.class, Text.class, SequenceFileOutputFormat.class);    job.setNumReduceTasks(0);    boolean succeeded = job.waitForCompletion(true);    return succeeded ? 0 : -1;}
0
protected void map(Text key, Text text, Context context) throws IOException, InterruptedException
{    Text outText = new Text();    int loc = 0;    while (loc >= 0 && loc < text.getLength()) {        int nextLoc = text.find("\n\n", loc + 1);        if (nextLoc > 0) {            outText.set(text.getBytes(), loc, nextLoc - loc);            context.write(key, outText);        }        loc = nextLoc;    }}
0
public static void main(String[] args) throws Exception
{    ToolRunner.run(new TextParagraphSplittingJob(), args);}
0
public IntWritable getCurrentKey()
{    return index;}
0
public BytesWritable getCurrentValue()
{    return value;}
0
public float getProgress() throws IOException
{    return processed ? 1.0f : 0.0f;}
0
public void initialize(InputSplit inputSplit, TaskAttemptContext taskAttemptContext) throws IOException, InterruptedException
{    if (!StringUtils.isBlank(fileFilterClassName) && !PrefixAdditionFilter.class.getName().equals(fileFilterClassName)) {        try {            pathFilter = (PathFilter) Class.forName(fileFilterClassName).newInstance();        } catch (ClassNotFoundException | InstantiationException | IllegalAccessException e) {            throw new IllegalStateException(e);        }    }}
0
public boolean nextKeyValue() throws IOException
{    if (!processed) {        byte[] contents = new byte[(int) fileSplit.getLength()];        Path file = fileSplit.getPath();        FileSystem fs = file.getFileSystem(this.configuration);        if (!fs.isFile(file)) {            return false;        }        FileStatus[] fileStatuses;        if (pathFilter != null) {            fileStatuses = fs.listStatus(file, pathFilter);        } else {            fileStatuses = fs.listStatus(file);        }        if (fileStatuses.length == 1) {            try (FSDataInputStream in = fs.open(fileStatuses[0].getPath())) {                IOUtils.readFully(in, contents, 0, contents.length);                value.setCapacity(contents.length);                value.set(contents, 0, contents.length);            }            processed = true;            return true;        }    }    return false;}
0
public void close() throws IOException
{}
0
protected TokenStreamComponents createComponents(String fieldName)
{    Tokenizer tokenizer = new WikipediaTokenizer();    TokenStream result = new StandardFilter(tokenizer);    result = new LowerCaseFilter(result);    result = new StopFilter(result, getStopwordSet());    return new TokenStreamComponents(tokenizer, result);}
0
public static void main(String[] args) throws IOException, InterruptedException
{    DefaultOptionBuilder obuilder = new DefaultOptionBuilder();    ArgumentBuilder abuilder = new ArgumentBuilder();    GroupBuilder gbuilder = new GroupBuilder();    Option dirInputPathOpt = DefaultOptionCreator.inputOption().create();    Option dirOutputPathOpt = DefaultOptionCreator.outputOption().create();    Option categoriesOpt = obuilder.withLongName("categories").withRequired(true).withArgument(abuilder.withName("categories").withMinimum(1).withMaximum(1).create()).withDescription("Location of the categories file.  One entry per line. " + "Will be used to make a string match in Wikipedia Category field").withShortName("c").create();    Option exactMatchOpt = obuilder.withLongName("exactMatch").withDescription("If set, then the category name must exactly match the " + "entry in the categories file. Default is false").withShortName("e").create();    Option analyzerOpt = obuilder.withLongName("analyzer").withRequired(false).withArgument(abuilder.withName("analyzer").withMinimum(1).withMaximum(1).create()).withDescription("The analyzer to use, must have a no argument constructor").withShortName("a").create();    Option helpOpt = DefaultOptionCreator.helpOption();    Group group = gbuilder.withName("Options").withOption(categoriesOpt).withOption(dirInputPathOpt).withOption(dirOutputPathOpt).withOption(exactMatchOpt).withOption(analyzerOpt).withOption(helpOpt).create();    Parser parser = new Parser();    parser.setGroup(group);    try {        CommandLine cmdLine = parser.parse(args);        if (cmdLine.hasOption(helpOpt)) {            CommandLineUtil.printHelp(group);            return;        }        String inputPath = (String) cmdLine.getValue(dirInputPathOpt);        String outputPath = (String) cmdLine.getValue(dirOutputPathOpt);        String catFile = (String) cmdLine.getValue(categoriesOpt);        Class<? extends Analyzer> analyzerClass = WikipediaAnalyzer.class;        if (cmdLine.hasOption(analyzerOpt)) {            String className = cmdLine.getValue(analyzerOpt).toString();            analyzerClass = Class.forName(className).asSubclass(Analyzer.class);                                    ClassUtils.instantiateAs(analyzerClass, Analyzer.class);        }        runJob(inputPath, outputPath, catFile, cmdLine.hasOption(exactMatchOpt), analyzerClass);    } catch (OptionException e) {                CommandLineUtil.printHelp(group);    } catch (ClassNotFoundException e) {                CommandLineUtil.printHelp(group);    }}
1
public static void runJob(String input, String output, String catFile, boolean exactMatchOnly, Class<? extends Analyzer> analyzerClass) throws IOException, InterruptedException, ClassNotFoundException
{    Configuration conf = new Configuration();    conf.set("key.value.separator.in.input.line", " ");    conf.set("xmlinput.start", "<page>");    conf.set("xmlinput.end", "</page>");    conf.setBoolean("exact.match.only", exactMatchOnly);    conf.set("analyzer.class", analyzerClass.getName());    conf.set("io.serializations", "org.apache.hadoop.io.serializer.JavaSerialization," + "org.apache.hadoop.io.serializer.WritableSerialization");            Set<String> categories = new HashSet<>();    for (String line : new FileLineIterable(new File(catFile))) {        categories.add(line.trim().toLowerCase(Locale.ENGLISH));    }    Stringifier<Set<String>> setStringifier = new DefaultStringifier<>(conf, GenericsUtil.getClass(categories));    String categoriesStr = setStringifier.toString(categories);    conf.set("wikipedia.categories", categoriesStr);    Job job = new Job(conf);        job.setJarByClass(WikipediaDatasetCreatorDriver.class);    job.setOutputKeyClass(Text.class);    job.setOutputValueClass(Text.class);    job.setMapperClass(WikipediaDatasetCreatorMapper.class);        job.setInputFormatClass(XmlInputFormat.class);    job.setReducerClass(WikipediaDatasetCreatorReducer.class);    job.setOutputFormatClass(TextOutputFormat.class);    FileInputFormat.setInputPaths(job, new Path(input));    Path outPath = new Path(output);    FileOutputFormat.setOutputPath(job, outPath);    HadoopUtil.delete(conf, outPath);    boolean succeeded = job.waitForCompletion(true);    if (!succeeded) {        throw new IllegalStateException("Job failed!");    }}
1
protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException
{    String document = value.toString();    document = StringEscapeUtils.unescapeHtml4(CLOSE_TEXT_TAG_PATTERN.matcher(OPEN_TEXT_TAG_PATTERN.matcher(document).replaceFirst("")).replaceAll(""));    String catMatch = findMatchingCategory(document);    if (!"Unknown".equals(catMatch)) {        StringBuilder contents = new StringBuilder(1000);        TokenStream stream = analyzer.tokenStream(catMatch, new StringReader(document));        CharTermAttribute termAtt = stream.addAttribute(CharTermAttribute.class);        stream.reset();        while (stream.incrementToken()) {            contents.append(termAtt.buffer(), 0, termAtt.length()).append(' ');        }        context.write(new Text(SPACE_NON_ALPHA_PATTERN.matcher(catMatch).replaceAll("_")), new Text(contents.toString()));        stream.end();        Closeables.close(stream, true);    }}
0
protected void setup(Context context) throws IOException, InterruptedException
{    super.setup(context);    Configuration conf = context.getConfiguration();    if (inputCategories == null) {        Set<String> newCategories = new HashSet<>();        DefaultStringifier<Set<String>> setStringifier = new DefaultStringifier<>(conf, GenericsUtil.getClass(newCategories));        String categoriesStr = conf.get("wikipedia.categories", setStringifier.toString(newCategories));        Set<String> inputCategoriesSet = setStringifier.fromString(categoriesStr);        inputCategories = new ArrayList<>(inputCategoriesSet);        inputCategoryPatterns = new ArrayList<>(inputCategories.size());        for (String inputCategory : inputCategories) {            inputCategoryPatterns.add(Pattern.compile(".*\\b" + inputCategory + "\\b.*"));        }    }    exactMatchOnly = conf.getBoolean("exact.match.only", false);    if (analyzer == null) {        String analyzerStr = conf.get("analyzer.class", WikipediaAnalyzer.class.getName());        analyzer = ClassUtils.instantiateAs(analyzerStr, Analyzer.class);    }    }
1
private String findMatchingCategory(String document)
{    int startIndex = 0;    int categoryIndex;    while ((categoryIndex = document.indexOf("[[Category:", startIndex)) != -1) {        categoryIndex += 11;        int endIndex = document.indexOf("]]", categoryIndex);        if (endIndex >= document.length() || endIndex < 0) {            break;        }        String category = document.substring(categoryIndex, endIndex).toLowerCase(Locale.ENGLISH).trim();                if (exactMatchOnly && inputCategories.contains(category)) {            return category;        }        if (!exactMatchOnly) {            for (int i = 0; i < inputCategories.size(); i++) {                String inputCategory = inputCategories.get(i);                Pattern inputCategoryPattern = inputCategoryPatterns.get(i);                if (inputCategoryPattern.matcher(category).matches()) {                                        return inputCategory;                }            }        }        startIndex = endIndex;    }    return "Unknown";}
0
protected void reduce(Text key, Iterable<Text> values, Context context) throws IOException, InterruptedException
{        for (Text value : values) {        context.write(key, value);    }}
0
protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException
{    String content = value.toString();    if (content.contains(REDIRECT)) {        return;    }    String document;    String title;    try {        document = getDocument(content);        title = getTitle(content);    } catch (RuntimeException e) {                return;    }    String catMatch = findMatchingCategory(document);    if (!all) {        if ("Unknown".equals(catMatch)) {            return;        }    }    document = StringEscapeUtils.unescapeHtml4(document);    if (removeLabels) {        document = removeCategoriesFromText(document);                if (document == null) {            return;        }    }        String category = "/" + catMatch.toLowerCase(Locale.ENGLISH) + "/" + SPACE_NON_ALPHA_PATTERN.matcher(title).replaceAll("_");    context.write(new Text(category), new Text(document));}
0
protected void setup(Context context) throws IOException, InterruptedException
{    super.setup(context);    Configuration conf = context.getConfiguration();    Set<String> newCategories = new HashSet<>();    DefaultStringifier<Set<String>> setStringifier = new DefaultStringifier<>(conf, GenericsUtil.getClass(newCategories));    String categoriesStr = conf.get("wikipedia.categories");    inputCategories = setStringifier.fromString(categoriesStr);    exactMatchOnly = conf.getBoolean("exact.match.only", false);    all = conf.getBoolean("all.files", false);    removeLabels = conf.getBoolean("remove.labels", false);    }
1
private static String getDocument(String xml)
{    int start = xml.indexOf(START_DOC) + START_DOC.length();    int end = xml.indexOf(END_DOC, start);    return xml.substring(start, end);}
0
private static String getTitle(CharSequence xml)
{    Matcher m = TITLE.matcher(xml);    return m.find() ? m.group(1) : "";}
0
private String findMatchingCategory(String document)
{    int startIndex = 0;    int categoryIndex;    while ((categoryIndex = document.indexOf("[[Category:", startIndex)) != -1) {        categoryIndex += 11;        int endIndex = document.indexOf("]]", categoryIndex);        if (endIndex >= document.length() || endIndex < 0) {            break;        }        String category = document.substring(categoryIndex, endIndex).toLowerCase(Locale.ENGLISH).trim();        if (exactMatchOnly && inputCategories.contains(category)) {            return category.toLowerCase(Locale.ENGLISH);        }        if (!exactMatchOnly) {            for (String inputCategory : inputCategories) {                if (category.contains(inputCategory)) {                                        return inputCategory.toLowerCase(Locale.ENGLISH);                }            }        }        startIndex = endIndex;    }    return "Unknown";}
0
private String removeCategoriesFromText(String document)
{    int startIndex = 0;    int categoryIndex;    try {        while ((categoryIndex = document.indexOf("[[Category:", startIndex)) != -1) {            int endIndex = document.indexOf("]]", categoryIndex);            if (endIndex >= document.length() || endIndex < 0) {                break;            }            document = document.replace(document.substring(categoryIndex, endIndex + 2), "");            if (categoryIndex < document.length()) {                startIndex = categoryIndex;            } else {                break;            }        }    } catch (StringIndexOutOfBoundsException e) {        return null;    }    return document;}
0
public static void main(String[] args) throws IOException
{    DefaultOptionBuilder obuilder = new DefaultOptionBuilder();    ArgumentBuilder abuilder = new ArgumentBuilder();    GroupBuilder gbuilder = new GroupBuilder();    Option dumpFileOpt = obuilder.withLongName("dumpFile").withRequired(true).withArgument(abuilder.withName("dumpFile").withMinimum(1).withMaximum(1).create()).withDescription("The path to the wikipedia dump file (.bz2 or uncompressed)").withShortName("d").create();    Option outputDirOpt = obuilder.withLongName("outputDir").withRequired(true).withArgument(abuilder.withName("outputDir").withMinimum(1).withMaximum(1).create()).withDescription("The output directory to place the splits in:\n" + "local files:\n\t/var/data/wikipedia-xml-chunks or\n\tfile:///var/data/wikipedia-xml-chunks\n" + "Hadoop DFS:\n\thdfs://wikipedia-xml-chunks\n" + "AWS S3 (blocks):\n\ts3://bucket-name/wikipedia-xml-chunks\n" + "AWS S3 (native files):\n\ts3n://bucket-name/wikipedia-xml-chunks\n").withShortName("o").create();    Option s3IdOpt = obuilder.withLongName("s3ID").withRequired(false).withArgument(abuilder.withName("s3Id").withMinimum(1).withMaximum(1).create()).withDescription("Amazon S3 ID key").withShortName("i").create();    Option s3SecretOpt = obuilder.withLongName("s3Secret").withRequired(false).withArgument(abuilder.withName("s3Secret").withMinimum(1).withMaximum(1).create()).withDescription("Amazon S3 secret key").withShortName("s").create();    Option chunkSizeOpt = obuilder.withLongName("chunkSize").withRequired(true).withArgument(abuilder.withName("chunkSize").withMinimum(1).withMaximum(1).create()).withDescription("The Size of the chunk, in megabytes").withShortName("c").create();    Option numChunksOpt = obuilder.withLongName("numChunks").withRequired(false).withArgument(abuilder.withName("numChunks").withMinimum(1).withMaximum(1).create()).withDescription("The maximum number of chunks to create.  If specified, program will only create a subset of the chunks").withShortName("n").create();    Group group = gbuilder.withName("Options").withOption(dumpFileOpt).withOption(outputDirOpt).withOption(chunkSizeOpt).withOption(numChunksOpt).withOption(s3IdOpt).withOption(s3SecretOpt).create();    Parser parser = new Parser();    parser.setGroup(group);    CommandLine cmdLine;    try {        cmdLine = parser.parse(args);    } catch (OptionException e) {                CommandLineUtil.printHelp(group);        return;    }    Configuration conf = new Configuration();    String dumpFilePath = (String) cmdLine.getValue(dumpFileOpt);    String outputDirPath = (String) cmdLine.getValue(outputDirOpt);    if (cmdLine.hasOption(s3IdOpt)) {        String id = (String) cmdLine.getValue(s3IdOpt);        conf.set("fs.s3n.awsAccessKeyId", id);        conf.set("fs.s3.awsAccessKeyId", id);    }    if (cmdLine.hasOption(s3SecretOpt)) {        String secret = (String) cmdLine.getValue(s3SecretOpt);        conf.set("fs.s3n.awsSecretAccessKey", secret);        conf.set("fs.s3.awsSecretAccessKey", secret);    }        conf.set("fs.file.impl", "org.apache.hadoop.fs.RawLocalFileSystem");    FileSystem fs = FileSystem.get(URI.create(outputDirPath), conf);    int chunkSize = 1024 * 1024 * Integer.parseInt((String) cmdLine.getValue(chunkSizeOpt));    int numChunks = Integer.MAX_VALUE;    if (cmdLine.hasOption(numChunksOpt)) {        numChunks = Integer.parseInt((String) cmdLine.getValue(numChunksOpt));    }    String header = "<mediawiki xmlns=\"http://www.mediawiki.org/xml/export-0.3/\" " + "xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" " + "xsi:schemaLocation=\"http://www.mediawiki.org/xml/export-0.3/ " + "http://www.mediawiki.org/xml/export-0.3.xsd\" " + "version=\"0.3\" " + "xml:lang=\"en\">\n" + "  <siteinfo>\n" + "<sitename>Wikipedia</sitename>\n" + "    <base>http://en.wikipedia.org/wiki/Main_Page</base>\n" + "    <generator>MediaWiki 1.13alpha</generator>\n" + "    <case>first-letter</case>\n" + "    <namespaces>\n" + "      <namespace key=\"-2\">Media</namespace>\n" + "      <namespace key=\"-1\">Special</namespace>\n" + "      <namespace key=\"0\" />\n" + "      <namespace key=\"1\">Talk</namespace>\n" + "      <namespace key=\"2\">User</namespace>\n" + "      <namespace key=\"3\">User talk</namespace>\n" + "      <namespace key=\"4\">Wikipedia</namespace>\n" + "      <namespace key=\"5\">Wikipedia talk</namespace>\n" + "      <namespace key=\"6\">Image</namespace>\n" + "      <namespace key=\"7\">Image talk</namespace>\n" + "      <namespace key=\"8\">MediaWiki</namespace>\n" + "      <namespace key=\"9\">MediaWiki talk</namespace>\n" + "      <namespace key=\"10\">Template</namespace>\n" + "      <namespace key=\"11\">Template talk</namespace>\n" + "      <namespace key=\"12\">Help</namespace>\n" + "      <namespace key=\"13\">Help talk</namespace>\n" + "      <namespace key=\"14\">Category</namespace>\n" + "      <namespace key=\"15\">Category talk</namespace>\n" + "      <namespace key=\"100\">Portal</namespace>\n" + "      <namespace key=\"101\">Portal talk</namespace>\n" + "    </namespaces>\n" + "  </siteinfo>\n";    StringBuilder content = new StringBuilder();    content.append(header);    NumberFormat decimalFormatter = new DecimalFormat("0000");    File dumpFile = new File(dumpFilePath);        if (!dumpFile.exists()) {                return;    }    FileLineIterator it;    if (dumpFilePath.endsWith(".bz2")) {                CompressionCodec codec = new BZip2Codec();        it = new FileLineIterator(codec.createInputStream(new FileInputStream(dumpFile)));    } else {                it = new FileLineIterator(dumpFile);    }    int fileNumber = 0;    while (it.hasNext()) {        String thisLine = it.next();        if (thisLine.trim().startsWith("<page>")) {            boolean end = false;            while (!thisLine.trim().startsWith("</page>")) {                content.append(thisLine).append('\n');                if (it.hasNext()) {                    thisLine = it.next();                } else {                    end = true;                    break;                }            }            content.append(thisLine).append('\n');            if (content.length() > chunkSize || end) {                content.append("</mediawiki>");                fileNumber++;                String filename = outputDirPath + "/chunk-" + decimalFormatter.format(fileNumber) + ".xml";                try (BufferedWriter chunkWriter = new BufferedWriter(new OutputStreamWriter(fs.create(new Path(filename)), "UTF-8"))) {                    chunkWriter.write(content.toString(), 0, content.length());                }                if (fileNumber >= numChunks) {                    break;                }                content = new StringBuilder();                content.append(header);            }        }    }}
1
public RecordReader<LongWritable, Text> createRecordReader(InputSplit split, TaskAttemptContext context)
{    try {        return new XmlRecordReader((FileSplit) split, context.getConfiguration());    } catch (IOException ioe) {                return null;    }}
1
private boolean next(LongWritable key, Text value) throws IOException
{    if (fsin.getPos() < end && readUntilMatch(startTag, false)) {        try {            buffer.write(startTag);            if (readUntilMatch(endTag, true)) {                key.set(fsin.getPos());                value.set(buffer.getData(), 0, buffer.getLength());                return true;            }        } finally {            buffer.reset();        }    }    return false;}
0
public void close() throws IOException
{    Closeables.close(fsin, true);}
0
public float getProgress() throws IOException
{    return (fsin.getPos() - start) / (float) (end - start);}
0
private boolean readUntilMatch(byte[] match, boolean withinBlock) throws IOException
{    int i = 0;    while (true) {        int b = fsin.read();                if (b == -1) {            return false;        }                if (withinBlock) {            buffer.write(b);        }                if (b == match[i]) {            i++;            if (i >= match.length) {                return true;            }        } else {            i = 0;        }                if (!withinBlock && i == 0 && fsin.getPos() >= end) {            return false;        }    }}
0
public LongWritable getCurrentKey() throws IOException, InterruptedException
{    return currentKey;}
0
public Text getCurrentValue() throws IOException, InterruptedException
{    return currentValue;}
0
public void initialize(InputSplit split, TaskAttemptContext context) throws IOException, InterruptedException
{}
0
public boolean nextKeyValue() throws IOException, InterruptedException
{    currentKey = new LongWritable();    currentValue = new Text();    return next(currentKey, currentValue);}
0
public static void main(String[] args) throws IOException
{    DefaultOptionBuilder obuilder = new DefaultOptionBuilder();    ArgumentBuilder abuilder = new ArgumentBuilder();    GroupBuilder gbuilder = new GroupBuilder();    Option dirInputPathOpt = DefaultOptionCreator.inputOption().create();    Option dirOutputPathOpt = DefaultOptionCreator.outputOption().create();    Option categoriesOpt = obuilder.withLongName("categories").withArgument(abuilder.withName("categories").withMinimum(1).withMaximum(1).create()).withDescription("Location of the categories file.  One entry per line. " + "Will be used to make a string match in Wikipedia Category field").withShortName("c").create();    Option exactMatchOpt = obuilder.withLongName("exactMatch").withDescription("If set, then the category name must exactly match the " + "entry in the categories file. Default is false").withShortName("e").create();    Option allOpt = obuilder.withLongName("all").withDescription("If set, Select all files. Default is false").withShortName("all").create();    Option removeLabelOpt = obuilder.withLongName("removeLabels").withDescription("If set, remove [[Category:labels]] from document text after extracting label." + "Default is false").withShortName("rl").create();    Option helpOpt = DefaultOptionCreator.helpOption();    Group group = gbuilder.withName("Options").withOption(categoriesOpt).withOption(dirInputPathOpt).withOption(dirOutputPathOpt).withOption(exactMatchOpt).withOption(allOpt).withOption(helpOpt).withOption(removeLabelOpt).create();    Parser parser = new Parser();    parser.setGroup(group);    parser.setHelpOption(helpOpt);    try {        CommandLine cmdLine = parser.parse(args);        if (cmdLine.hasOption(helpOpt)) {            CommandLineUtil.printHelp(group);            return;        }        String inputPath = (String) cmdLine.getValue(dirInputPathOpt);        String outputPath = (String) cmdLine.getValue(dirOutputPathOpt);        String catFile = "";        if (cmdLine.hasOption(categoriesOpt)) {            catFile = (String) cmdLine.getValue(categoriesOpt);        }        boolean all = false;        if (cmdLine.hasOption(allOpt)) {            all = true;        }        boolean removeLabels = false;        if (cmdLine.hasOption(removeLabelOpt)) {            removeLabels = true;        }        runJob(inputPath, outputPath, catFile, cmdLine.hasOption(exactMatchOpt), all, removeLabels);    } catch (OptionException | InterruptedException | ClassNotFoundException e) {                CommandLineUtil.printHelp(group);    }}
1
public static void runJob(String input, String output, String catFile, boolean exactMatchOnly, boolean all, boolean removeLabels) throws IOException, InterruptedException, ClassNotFoundException
{    Configuration conf = new Configuration();    conf.set("xmlinput.start", "<page>");    conf.set("xmlinput.end", "</page>");    conf.setBoolean("exact.match.only", exactMatchOnly);    conf.setBoolean("all.files", all);    conf.setBoolean("remove.labels", removeLabels);    conf.set("io.serializations", "org.apache.hadoop.io.serializer.JavaSerialization," + "org.apache.hadoop.io.serializer.WritableSerialization");    Set<String> categories = new HashSet<>();    if (!catFile.isEmpty()) {        for (String line : new FileLineIterable(new File(catFile))) {            categories.add(line.trim().toLowerCase(Locale.ENGLISH));        }    }    Stringifier<Set<String>> setStringifier = new DefaultStringifier<>(conf, GenericsUtil.getClass(categories));    String categoriesStr = setStringifier.toString(categories);    conf.set("wikipedia.categories", categoriesStr);    Job job = new Job(conf);        job.setOutputKeyClass(Text.class);    job.setOutputValueClass(Text.class);    FileInputFormat.setInputPaths(job, new Path(input));    Path outPath = new Path(output);    FileOutputFormat.setOutputPath(job, outPath);    job.setMapperClass(WikipediaMapper.class);    job.setInputFormatClass(XmlInputFormat.class);    job.setReducerClass(Reducer.class);    job.setOutputFormatClass(SequenceFileOutputFormat.class);    job.setJarByClass(WikipediaToSequenceFile.class);    /*     * conf.set("mapred.compress.map.output", "true"); conf.set("mapred.map.output.compression.type",     * "BLOCK"); conf.set("mapred.output.compress", "true"); conf.set("mapred.output.compression.type",     * "BLOCK"); conf.set("mapred.output.compression.codec", "org.apache.hadoop.io.compress.GzipCodec");     */    HadoopUtil.delete(conf, outPath);    boolean succeeded = job.waitForCompletion(true);    if (!succeeded) {        throw new IllegalStateException("Job failed!");    }}
1
 static int scale(double value, double base)
{    double scale = value / base;        int i = 0;    while (i < BUMPS.length - 1 && BUMPS[i + 1] <= scale) {        i++;    }    return BUMPS[i];}
0
 static long base(double value)
{    return Math.max(1, (long) Math.pow(10, (int) Math.floor(Math.log10(value))));}
0
public long increment()
{    long delta;    if (counter >= 10) {        long base = base(counter / 4.0);        int scale = scale(counter / 4.0, base);        delta = base * scale;    } else {        delta = 1;    }    counter += delta;    return counter;}
0
protected Writer getWriter()
{    return writer;}
0
protected Map<Integer, List<WeightedPropertyVectorWritable>> getClusterIdToPoints()
{    return clusterIdToPoints;}
0
public static String getTopFeatures(Vector vector, String[] dictionary, int numTerms)
{    StringBuilder sb = new StringBuilder(100);    for (Pair<String, Double> item : getTopPairs(vector, dictionary, numTerms)) {        String term = item.getFirst();        sb.append("\n\t\t");        sb.append(StringUtils.rightPad(term, 40));        sb.append("=>");        sb.append(StringUtils.leftPad(item.getSecond().toString(), 20));    }    return sb.toString();}
0
public static String getTopTerms(Vector vector, String[] dictionary, int numTerms)
{    StringBuilder sb = new StringBuilder(100);    for (Pair<String, Double> item : getTopPairs(vector, dictionary, numTerms)) {        String term = item.getFirst();        sb.append(term).append('_');    }    sb.deleteCharAt(sb.length() - 1);    return sb.toString();}
0
public long write(Iterable<ClusterWritable> iterable) throws IOException
{    return write(iterable, Long.MAX_VALUE);}
0
public void close() throws IOException
{    writer.close();}
0
public long write(Iterable<ClusterWritable> iterable, long maxDocs) throws IOException
{    long result = 0;    Iterator<ClusterWritable> iterator = iterable.iterator();    while (result < maxDocs && iterator.hasNext()) {        write(iterator.next());        result++;    }    return result;}
0
private static Collection<Pair<String, Double>> getTopPairs(Vector vector, String[] dictionary, int numTerms)
{    List<TermIndexWeight> vectorTerms = Lists.newArrayList();    for (Vector.Element elt : vector.nonZeroes()) {        vectorTerms.add(new TermIndexWeight(elt.index(), elt.get()));    }        Collections.sort(vectorTerms, new Comparator<TermIndexWeight>() {        @Override        public int compare(TermIndexWeight one, TermIndexWeight two) {            return Double.compare(two.weight, one.weight);        }    });    Collection<Pair<String, Double>> topTerms = Lists.newLinkedList();    for (int i = 0; i < vectorTerms.size() && i < numTerms; i++) {        int index = vectorTerms.get(i).index;        String dictTerm = dictionary[index];        if (dictTerm == null) {                        continue;        }        topTerms.add(new Pair<>(dictTerm, vectorTerms.get(i).weight));    }    return topTerms;}
1
public int compare(TermIndexWeight one, TermIndexWeight two)
{    return Double.compare(two.weight, one.weight);}
0
public static void main(String[] args) throws Exception
{    new ClusterDumper().run(args);}
0
public int run(String[] args) throws Exception
{    addInputOption();    addOutputOption();    addOption(OUTPUT_FORMAT_OPT, "of", "The optional output format for the results.  Options: TEXT, CSV, JSON or GRAPH_ML", "TEXT");    addOption(SUBSTRING_OPTION, "b", "The number of chars of the asFormatString() to print");    addOption(NUM_WORDS_OPTION, "n", "The number of top terms to print");    addOption(POINTS_DIR_OPTION, "p", "The directory containing points sequence files mapping input vectors to their cluster.  " + "If specified, then the program will output the points associated with a cluster");    addOption(SAMPLE_POINTS, "sp", "Specifies the maximum number of points to include _per_ cluster.  The default " + "is to include all points");    addOption(DICTIONARY_OPTION, "d", "The dictionary file");    addOption(DICTIONARY_TYPE_OPTION, "dt", "The dictionary file type (text|sequencefile)", "text");    addOption(buildOption(EVALUATE_CLUSTERS, "e", "Run ClusterEvaluator and CDbwEvaluator over the input.  " + "The output will be appended to the rest of the output at the end.", false, false, null));    addOption(DefaultOptionCreator.distanceMeasureOption().create());        if (parseArguments(args, false, true) == null) {        return -1;    }    seqFileDir = getInputPath();    if (hasOption(POINTS_DIR_OPTION)) {        pointsDir = new Path(getOption(POINTS_DIR_OPTION));    }    outputFile = getOutputFile();    if (hasOption(SUBSTRING_OPTION)) {        int sub = Integer.parseInt(getOption(SUBSTRING_OPTION));        if (sub >= 0) {            subString = sub;        }    }    termDictionary = getOption(DICTIONARY_OPTION);    dictionaryFormat = getOption(DICTIONARY_TYPE_OPTION);    if (hasOption(NUM_WORDS_OPTION)) {        numTopFeatures = Integer.parseInt(getOption(NUM_WORDS_OPTION));    }    if (hasOption(OUTPUT_FORMAT_OPT)) {        outputFormat = OUTPUT_FORMAT.valueOf(getOption(OUTPUT_FORMAT_OPT));    }    if (hasOption(SAMPLE_POINTS)) {        maxPointsPerCluster = Long.parseLong(getOption(SAMPLE_POINTS));    } else {        maxPointsPerCluster = Long.MAX_VALUE;    }    runEvaluation = hasOption(EVALUATE_CLUSTERS);    String distanceMeasureClass = getOption(DefaultOptionCreator.DISTANCE_MEASURE_OPTION);    measure = ClassUtils.instantiateAs(distanceMeasureClass, DistanceMeasure.class);    init();    printClusters(null);    return 0;}
0
public void printClusters(String[] dictionary) throws Exception
{    Configuration conf = new Configuration();    if (this.termDictionary != null) {        if ("text".equals(dictionaryFormat)) {            dictionary = VectorHelper.loadTermDictionary(new File(this.termDictionary));        } else if ("sequencefile".equals(dictionaryFormat)) {            dictionary = VectorHelper.loadTermDictionary(conf, this.termDictionary);        } else {            throw new IllegalArgumentException("Invalid dictionary format");        }    }    Writer writer;    boolean shouldClose;    if (this.outputFile == null) {        shouldClose = false;        writer = new OutputStreamWriter(System.out, Charsets.UTF_8);    } else {        shouldClose = true;        if (outputFile.getName().startsWith("s3n://")) {            Path p = outputPath;            FileSystem fs = FileSystem.get(p.toUri(), conf);            writer = new OutputStreamWriter(fs.create(p), Charsets.UTF_8);        } else {            Files.createParentDirs(outputFile);            writer = Files.newWriter(this.outputFile, Charsets.UTF_8);        }    }    ClusterWriter clusterWriter = createClusterWriter(writer, dictionary);    try {        long numWritten = clusterWriter.write(new SequenceFileDirValueIterable<ClusterWritable>(new Path(seqFileDir, "part-*"), PathType.GLOB, conf));        writer.flush();        if (runEvaluation) {            HadoopUtil.delete(conf, new Path("tmp/representative"));            int numIters = 5;            RepresentativePointsDriver.main(new String[] { "--input", seqFileDir.toString(), "--output", "tmp/representative", "--clusteredPoints", pointsDir.toString(), "--distanceMeasure", measure.getClass().getName(), "--maxIter", String.valueOf(numIters) });            conf.set(RepresentativePointsDriver.DISTANCE_MEASURE_KEY, measure.getClass().getName());            conf.set(RepresentativePointsDriver.STATE_IN_KEY, "tmp/representative/representativePoints-" + numIters);            ClusterEvaluator ce = new ClusterEvaluator(conf, seqFileDir);            writer.append("\n");            writer.append("Inter-Cluster Density: ").append(String.valueOf(ce.interClusterDensity())).append("\n");            writer.append("Intra-Cluster Density: ").append(String.valueOf(ce.intraClusterDensity())).append("\n");            CDbwEvaluator cdbw = new CDbwEvaluator(conf, seqFileDir);            writer.append("CDbw Inter-Cluster Density: ").append(String.valueOf(cdbw.interClusterDensity())).append("\n");            writer.append("CDbw Intra-Cluster Density: ").append(String.valueOf(cdbw.intraClusterDensity())).append("\n");            writer.append("CDbw Separation: ").append(String.valueOf(cdbw.separation())).append("\n");            writer.flush();        }            } finally {        if (shouldClose) {            Closeables.close(clusterWriter, false);        } else {            if (clusterWriter instanceof GraphMLClusterWriter) {                clusterWriter.close();            }        }    }}
1
 ClusterWriter createClusterWriter(Writer writer, String[] dictionary) throws IOException
{    ClusterWriter result;    switch(outputFormat) {        case TEXT:            result = new ClusterDumperWriter(writer, clusterIdToPoints, measure, numTopFeatures, dictionary, subString);            break;        case CSV:            result = new CSVClusterWriter(writer, clusterIdToPoints, measure);            break;        case GRAPH_ML:            result = new GraphMLClusterWriter(writer, clusterIdToPoints, measure, numTopFeatures, dictionary, subString);            break;        case JSON:            result = new JsonClusterWriter(writer, clusterIdToPoints, measure, numTopFeatures, dictionary);            break;        default:            throw new IllegalStateException("Unknown outputformat: " + outputFormat);    }    return result;}
0
public void setOutputFormat(OUTPUT_FORMAT of)
{    outputFormat = of;}
0
private void init()
{    if (this.pointsDir != null) {        Configuration conf = new Configuration();                clusterIdToPoints = readPoints(this.pointsDir, maxPointsPerCluster, conf);    } else {        clusterIdToPoints = Collections.emptyMap();    }}
0
public int getSubString()
{    return subString;}
0
public void setSubString(int subString)
{    this.subString = subString;}
0
public Map<Integer, List<WeightedPropertyVectorWritable>> getClusterIdToPoints()
{    return clusterIdToPoints;}
0
public String getTermDictionary()
{    return termDictionary;}
0
public void setTermDictionary(String termDictionary, String dictionaryType)
{    this.termDictionary = termDictionary;    this.dictionaryFormat = dictionaryType;}
0
public void setNumTopFeatures(int num)
{    this.numTopFeatures = num;}
0
public int getNumTopFeatures()
{    return this.numTopFeatures;}
0
public long getMaxPointsPerCluster()
{    return maxPointsPerCluster;}
0
public void setMaxPointsPerCluster(long maxPointsPerCluster)
{    this.maxPointsPerCluster = maxPointsPerCluster;}
0
public static Map<Integer, List<WeightedPropertyVectorWritable>> readPoints(Path pointsPathDir, long maxPointsPerCluster, Configuration conf)
{    Map<Integer, List<WeightedPropertyVectorWritable>> result = new TreeMap<>();    for (Pair<IntWritable, WeightedPropertyVectorWritable> record : new SequenceFileDirIterable<IntWritable, WeightedPropertyVectorWritable>(pointsPathDir, PathType.LIST, PathFilters.logsCRCFilter(), conf)) {                                int keyValue = record.getFirst().get();        List<WeightedPropertyVectorWritable> pointList = result.get(keyValue);        if (pointList == null) {            pointList = new ArrayList<>();            result.put(keyValue, pointList);        }        if (pointList.size() < maxPointsPerCluster) {            pointList.add(record.getSecond());        }    }    return result;}
0
public void write(ClusterWritable clusterWritable) throws IOException
{    Cluster cluster = clusterWritable.getValue();    String fmtStr = cluster.asFormatString(dictionary);    Writer writer = getWriter();    if (subString > 0 && fmtStr.length() > subString) {        writer.write(':');        writer.write(fmtStr, 0, Math.min(subString, fmtStr.length()));    } else {        writer.write(fmtStr);    }    writer.write('\n');    if (dictionary != null) {        String topTerms = getTopFeatures(clusterWritable.getValue().getCenter(), dictionary, numTopFeatures);        writer.write("\tTop Terms: ");        writer.write(topTerms);        writer.write('\n');    }    Map<Integer, List<WeightedPropertyVectorWritable>> clusterIdToPoints = getClusterIdToPoints();    List<WeightedPropertyVectorWritable> points = clusterIdToPoints.get(clusterWritable.getValue().getId());    if (points != null) {        writer.write("\tWeight : [props - optional]:  Point:\n\t");        for (Iterator<WeightedPropertyVectorWritable> iterator = points.iterator(); iterator.hasNext(); ) {            WeightedPropertyVectorWritable point = iterator.next();            writer.write(String.valueOf(point.getWeight()));            Map<Text, Text> map = point.getProperties();                        writer.write(" : [");            if (map != null) {                for (Map.Entry<Text, Text> entry : map.entrySet()) {                    writer.write(entry.getKey().toString());                    writer.write("=");                    writer.write(entry.getValue().toString());                }            }            writer.write("]");            writer.write(": ");            writer.write(AbstractCluster.formatVector(point.getVector(), dictionary));            if (iterator.hasNext()) {                writer.write("\n\t");            }        }        writer.write('\n');    }}
0
public void write(ClusterWritable clusterWritable) throws IOException
{    StringBuilder line = new StringBuilder();    Cluster cluster = clusterWritable.getValue();    line.append(cluster.getId());    List<WeightedPropertyVectorWritable> points = getClusterIdToPoints().get(cluster.getId());    if (points != null) {        for (WeightedPropertyVectorWritable point : points) {            Vector theVec = point.getVector();            line.append(',');            if (theVec instanceof NamedVector) {                line.append(((NamedVector) theVec).getName());            } else {                String vecStr = theVec.asFormatString();                                vecStr = VEC_PATTERN.matcher(vecStr).replaceAll("_");                line.append(vecStr);            }        }        getWriter().append(line).append("\n");    }}
0
private void init(Writer writer) throws IOException
{    writer.append("<?xml version=\"1.0\" encoding=\"UTF-8\"?>");    writer.append("<graphml xmlns=\"http://graphml.graphdrawing.org/xmlns\"\n" + "xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\"\n" + "xsi:schemaLocation=\"http://graphml.graphdrawing.org/xmlns\n" + "http://graphml.graphdrawing.org/xmlns/1.0/graphml.xsd\">");        writer.append("<key attr.name=\"r\" attr.type=\"int\" for=\"node\" id=\"r\"/>\n" + "<key attr.name=\"g\" attr.type=\"int\" for=\"node\" id=\"g\"/>\n" + "<key attr.name=\"b\" attr.type=\"int\" for=\"node\" id=\"b\"/>" + "<key attr.name=\"size\" attr.type=\"int\" for=\"node\" id=\"size\"/>" + "<key attr.name=\"weight\" attr.type=\"float\" for=\"edge\" id=\"weight\"/>" + "<key attr.name=\"x\" attr.type=\"float\" for=\"node\" id=\"x\"/>" + "<key attr.name=\"y\" attr.type=\"float\" for=\"node\" id=\"y\"/>");    writer.append("<graph edgedefault=\"undirected\">");    lastClusterColor = new Color();    posStep = (int) (0.1 * clusterIdToPoints.size()) + 100;    random = RandomUtils.getRandom();}
0
public void write(ClusterWritable clusterWritable) throws IOException
{    StringBuilder line = new StringBuilder();    Cluster cluster = clusterWritable.getValue();    Color rgb = getColor(cluster.getId());    String topTerms = "";    if (dictionary != null) {        topTerms = getTopTerms(cluster.getCenter(), dictionary, numTopFeatures);    }    String clusterLabel = String.valueOf(cluster.getId()) + '_' + topTerms;            float x = lastX + 1000;    float y = lastY;    if (x > (1000 + posStep)) {        y = lastY + 1000;        x = 0;    }    line.append(createNode(clusterLabel, rgb, x, y));    List<WeightedPropertyVectorWritable> points = clusterIdToPoints.get(cluster.getId());    if (points != null) {        for (WeightedVectorWritable point : points) {            Vector theVec = point.getVector();            double distance = 1;            if (measure != null) {                                distance = measure.distance(cluster.getCenter().getLengthSquared(), cluster.getCenter(), theVec) * 500;            }            String vecStr;                        int angle = random.nextInt(360);            double angleRads = Math.toRadians(angle);            float targetX = x + (float) (distance * Math.cos(angleRads));            float targetY = y + (float) (distance * Math.sin(angleRads));            if (theVec instanceof NamedVector) {                vecStr = ((NamedVector) theVec).getName();            } else {                vecStr = theVec.asFormatString();                                vecStr = VEC_PATTERN.matcher(vecStr).replaceAll("_");            }            if (subString > 0 && vecStr.length() > subString) {                vecStr = vecStr.substring(0, subString);            }            line.append(createNode(vecStr, rgb, targetX, targetY));            line.append(createEdge(clusterLabel, vecStr, distance));        }    }    lastClusterColor = rgb;    lastX = x;    lastY = y;    getWriter().append(line).append("\n");}
0
private Color getColor(int clusterId)
{    Color result = colors.get(clusterId);    if (result == null) {        result = new Color();                int incR = 0;        int incG = 0;        int incB = 0;        if (lastClusterColor.r + 20 < 256 && lastClusterColor.g + 20 < 256 && lastClusterColor.b + 20 < 256) {            incR = 20;            incG = 0;            incB = 0;        } else if (lastClusterColor.r + 20 >= 256 && lastClusterColor.g + 20 < 256 && lastClusterColor.b + 20 < 256) {            incG = 20;            incB = 0;        } else if (lastClusterColor.r + 20 >= 256 && lastClusterColor.g + 20 >= 256 && lastClusterColor.b + 20 < 256) {            incB = 20;        } else {            incR += 3;            incG += 3;            incR += 3;        }        result.r = (lastClusterColor.r + incR) % 256;        result.g = (lastClusterColor.g + incG) % 256;        result.b = (lastClusterColor.b + incB) % 256;        colors.put(clusterId, result);    }    return result;}
0
private static String createEdge(String left, String right, double distance)
{    left = StringUtils.escapeXML(left);    right = StringUtils.escapeXML(right);    return "<edge id=\"" + left + '_' + right + "\" source=\"" + left + "\" target=\"" + right + "\">" + "<data key=\"weight\">" + distance + "</data></edge>";}
0
private static String createNode(String s, Color rgb, float x, float y)
{    return "<node id=\"" + StringUtils.escapeXML(s) + "\"><data key=\"r\">" + rgb.r + "</data>" + "<data key=\"g\">" + rgb.g + "</data>" + "<data key=\"b\">" + rgb.b + "</data>" + "<data key=\"x\">" + x + "</data>" + "<data key=\"y\">" + y + "</data>" + "</node>";}
0
public void close() throws IOException
{    getWriter().append("</graph>").append("</graphml>");    super.close();}
0
public void write(ClusterWritable clusterWritable) throws IOException
{    Map<String, Object> res = new HashMap<>();        if (dictionary != null) {        List<Object> topTerms = getTopFeaturesList(clusterWritable.getValue().getCenter(), dictionary, numTopFeatures);        res.put("top_terms", topTerms);    } else {        res.put("top_terms", new ArrayList<>());    }        Cluster cluster = clusterWritable.getValue();    res.put("cluster_id", cluster.getId());    if (dictionary != null) {        Map<String, Object> fmtStr = cluster.asJson(dictionary);        res.put("cluster", fmtStr);                List<Object> points = getPoints(cluster, dictionary);        res.put("points", points);    } else {        res.put("cluster", new HashMap<>());        res.put("points", new ArrayList<>());    }        Writer writer = getWriter();    writer.write(jxn.writeValueAsString(res) + "\n");}
0
public List<Object> getTopFeaturesList(Vector vector, String[] dictionary, int numTerms)
{    List<TermIndexWeight> vectorTerms = new ArrayList<>();    for (Vector.Element elt : vector.nonZeroes()) {        vectorTerms.add(new TermIndexWeight(elt.index(), elt.get()));    }        Collections.sort(vectorTerms, new Comparator<TermIndexWeight>() {        @Override        public int compare(TermIndexWeight one, TermIndexWeight two) {            return Double.compare(two.weight, one.weight);        }    });    List<Object> topTerms = new ArrayList<>();    for (int i = 0; i < vectorTerms.size() && i < numTerms; i++) {        int index = vectorTerms.get(i).index;        String dictTerm = dictionary[index];        if (dictTerm == null) {                        continue;        }        Map<String, Object> term_entry = new HashMap<>();        term_entry.put(dictTerm, vectorTerms.get(i).weight);        topTerms.add(term_entry);    }    return topTerms;}
1
public int compare(TermIndexWeight one, TermIndexWeight two)
{    return Double.compare(two.weight, one.weight);}
0
public List<Object> getPoints(Cluster cluster, String[] dictionary)
{    List<Object> vectorObjs = new ArrayList<>();    List<WeightedPropertyVectorWritable> points = getClusterIdToPoints().get(cluster.getId());    if (points != null) {        for (WeightedPropertyVectorWritable point : points) {            Map<String, Object> entry = new HashMap<>();            Vector theVec = point.getVector();            if (theVec instanceof NamedVector) {                entry.put("vector_name", ((NamedVector) theVec).getName());            } else {                String vecStr = theVec.asFormatString();                                vecStr = VEC_PATTERN.matcher(vecStr).replaceAll("_");                entry.put("vector_name", vecStr);            }            entry.put("weight", String.valueOf(point.getWeight()));            try {                entry.put("point", AbstractCluster.formatVectorAsJson(point.getVector(), dictionary));            } catch (IOException e) {                            }            vectorObjs.add(entry);        }    }    return vectorObjs;}
1
public File getInput()
{    return input;}
0
public void setInput(File input)
{    this.input = input;}
0
public String getOutputDir()
{    return outputDir;}
0
public void setOutputDir(String outputDir)
{    this.outputDir = outputDir;}
0
public String getPrefix()
{    return prefix;}
0
public void setPrefix(String prefix)
{    this.prefix = prefix;}
0
public int getChunkSize()
{    return chunkSize;}
0
public void setChunkSize(int chunkSize)
{    this.chunkSize = chunkSize;}
0
public Charset getCharset()
{    return charset;}
0
public void setCharset(Charset charset)
{    this.charset = charset;}
0
public String getSeparator()
{    return separator;}
0
public void setSeparator(String separator)
{    this.separator = separator;}
0
public String getBodySeparator()
{    return bodySeparator;}
0
public void setBodySeparator(String bodySeparator)
{    this.bodySeparator = bodySeparator;}
0
public boolean isIncludeBody()
{    return includeBody;}
0
public void setIncludeBody(boolean includeBody)
{    this.includeBody = includeBody;}
0
public Pattern[] getPatternsToMatch()
{    return patternsToMatch;}
0
public void setPatternsToMatch(Pattern[] patternsToMatch)
{    this.patternsToMatch = patternsToMatch;}
0
public Map<String, Integer> getPatternOrder()
{    return patternOrder;}
0
public void setPatternOrder(Map<String, Integer> patternOrder)
{    this.patternOrder = patternOrder;}
0
public boolean isStripQuotedText()
{    return stripQuotedText;}
0
public void setStripQuotedText(boolean stripQuotedText)
{    this.stripQuotedText = stripQuotedText;}
0
public Pattern getQuotedTextPattern()
{    return quotedTextPattern;}
0
public void setQuotedTextPattern(Pattern quotedTextPattern)
{    this.quotedTextPattern = quotedTextPattern;}
0
public long parseMboxLineByLine(File mboxFile) throws IOException
{    long messageCount = 0;    try {        StringBuilder contents = new StringBuilder();                StringBuilder body = new StringBuilder();        Matcher messageIdMatcher = MESSAGE_ID_PREFIX.matcher("");        Matcher messageBoundaryMatcher = MESSAGE_START.matcher("");        String[] patternResults = new String[options.getPatternsToMatch().length];        Matcher[] matchers = new Matcher[options.getPatternsToMatch().length];        for (int i = 0; i < matchers.length; i++) {            matchers[i] = options.getPatternsToMatch()[i].matcher("");        }        String messageId = null;        boolean inBody = false;        Pattern quotedTextPattern = options.getQuotedTextPattern();        for (String nextLine : new FileLineIterable(mboxFile, options.getCharset(), false)) {            if (options.isStripQuotedText() && quotedTextPattern.matcher(nextLine).find()) {                continue;            }            for (int i = 0; i < matchers.length; i++) {                Matcher matcher = matchers[i];                matcher.reset(nextLine);                if (matcher.matches()) {                    patternResults[i] = matcher.group(1);                }            }                        if (messageId != null) {                                messageBoundaryMatcher.reset(nextLine);                if (messageBoundaryMatcher.matches()) {                                        String key = generateKey(mboxFile, prefix, messageId);                                        writeContent(options.getSeparator(), contents, body, patternResults);                    writer.write(key, contents.toString());                                        contents.setLength(0);                    body.setLength(0);                    messageId = null;                    inBody = false;                } else {                    if (inBody && options.isIncludeBody()) {                        if (!nextLine.isEmpty()) {                            body.append(nextLine).append(options.getBodySeparator());                        }                    } else {                                                                        inBody = nextLine.isEmpty();                    }                }            } else {                if (nextLine.length() > 14) {                    messageIdMatcher.reset(nextLine);                    if (messageIdMatcher.matches()) {                        messageId = messageIdMatcher.group(1);                        ++messageCount;                    }                }            }        }                if (messageId != null) {            String key = generateKey(mboxFile, prefix, messageId);            writeContent(options.getSeparator(), contents, body, patternResults);            writer.write(key, contents.toString());                        contents.setLength(0);        }    } catch (FileNotFoundException e) {                    }        return messageCount;}
1
protected static String generateKey(File mboxFile, String prefix, String messageId)
{    return prefix + File.separator + mboxFile.getName() + File.separator + messageId;}
0
public String getPrefix()
{    return prefix;}
0
public MailOptions getOptions()
{    return options;}
0
private static void writeContent(String separator, StringBuilder contents, CharSequence body, String[] matches)
{    for (String match : matches) {        if (match != null) {            contents.append(match).append(separator);        } else {            contents.append(separator);        }    }    contents.append('\n').append(body);}
0
public void write(String key, String value) throws IOException
{    writer.write(key, value);}
0
public void close() throws IOException
{    writer.close();}
0
private Path getPath(int chunkID)
{    return new Path(output, "chunk-" + chunkID);}
0
public void write(String key, String value) throws IOException
{    if (currentChunkSize > maxChunkSizeInBytes) {        Closeables.close(writer, false);        currentChunkID++;        writer = new SequenceFile.Writer(fs, conf, getPath(currentChunkID), Text.class, Text.class);        currentChunkSize = 0;    }    Text keyT = new Text(key);    Text valueT = new Text(value);        currentChunkSize += keyT.getBytes().length + valueT.getBytes().length;    writer.append(keyT, valueT);}
0
public void close() throws IOException
{    Closeables.close(writer, false);}
0
public void write(String key, String value) throws IOException
{    writer.write(key + ' ' + value);}
0
public void close() throws IOException
{    writer.close();}
0
public static void main(String[] args) throws Exception
{    ToolRunner.run(new MatrixDumper(), args);}
0
public int run(String[] args) throws Exception
{    addInputOption();        addOption("output", "o", "Output path", null);    Map<String, List<String>> parsedArgs = parseArguments(args);    if (parsedArgs == null) {        return -1;    }    String outputFile = hasOption("output") ? getOption("output") : null;    exportCSV(getInputPath(), outputFile, false);    return 0;}
0
private static void exportCSV(Path inputPath, String outputFile, boolean doLabels) throws IOException
{    SequenceFileValueIterator<MatrixWritable> it = new SequenceFileValueIterator<>(inputPath, true, new Configuration());    Matrix m = it.next().get();    it.close();    PrintStream ps = getPrintStream(outputFile);    String[] columnLabels = getLabels(m.numCols(), m.getColumnLabelBindings(), "col");    String[] rowLabels = getLabels(m.numRows(), m.getRowLabelBindings(), "row");    if (doLabels) {        ps.print("rowid,");        ps.print(columnLabels[0]);        for (int c = 1; c < m.numCols(); c++) {            ps.print(',' + columnLabels[c]);        }        ps.println();    }    for (int r = 0; r < m.numRows(); r++) {        if (doLabels) {            ps.print(rowLabels[0] + ',');        }        ps.print(Double.toString(m.getQuick(r, 0)));        for (int c = 1; c < m.numCols(); c++) {            ps.print(",");            ps.print(Double.toString(m.getQuick(r, c)));        }        ps.println();    }    if (ps != System.out) {        ps.close();    }}
0
private static PrintStream getPrintStream(String outputPath) throws IOException
{    if (outputPath == null) {        return System.out;    }    File outputFile = new File(outputPath);    if (outputFile.exists()) {        outputFile.delete();    }    outputFile.createNewFile();    OutputStream os = new FileOutputStream(outputFile);    return new PrintStream(os, false, Charsets.UTF_8.displayName());}
0
private static String[] getLabels(int length, Map<String, Integer> labels, String start)
{    if (labels != null) {        return sortLabels(labels);    }    String[] sorted = new String[length];    for (int i = 1; i <= length; i++) {        sorted[i] = start + i;    }    return sorted;}
0
private static String[] sortLabels(Map<String, Integer> labels)
{    String[] sorted = new String[labels.size()];    for (Map.Entry<String, Integer> entry : labels.entrySet()) {        sorted[entry.getValue()] = entry.getKey();    }    return sorted;}
0
public boolean incrementToken() throws IOException
{    while (input.incrementToken()) {        ByteBuffer bytes = encoder.encode(CharBuffer.wrap(termAtt.buffer(), 0, termAtt.length()));        key.set(bytes.array(), 1.0f);        boolean member = filter.membershipTest(key);        if ((keepMembers && member) || (!keepMembers && !member)) {            return true;        }    }    return false;}
0
public String transformMatch(String match)
{    StringBuilder result = new StringBuilder();    try (TokenStream ts = analyzer.tokenStream(fieldName, new StringReader(match))) {        ts.addAttribute(CharTermAttribute.class);        ts.reset();        TokenStreamIterator iter = new TokenStreamIterator(ts);        while (iter.hasNext()) {            result.append(iter.next()).append(' ');        }        ts.end();    } catch (IOException e) {        throw new IllegalStateException(e);    }    return result.toString();}
0
public Analyzer getAnalyzer()
{    return analyzer;}
0
public void setAnalyzer(Analyzer analyzer)
{    this.analyzer = analyzer;}
0
public String transformMatch(String match)
{    String result = match;    for (RegexTransformer transformer : chain) {        result = transformer.transformMatch(result);    }    return result;}
0
public List<RegexTransformer> getChain()
{    return chain;}
0
public void setChain(List<RegexTransformer> chain)
{    this.chain = chain;}
0
public String format(String toFormat)
{    return '\t' + WHITESPACE.matcher(toFormat).replaceAll("|");}
0
public String format(String toFormat)
{    return toFormat;}
0
public String transformMatch(String match)
{    return match;}
0
public int run(String[] args) throws Exception
{    addInputOption();    addOutputOption();    addOption(DefaultOptionCreator.overwriteOption().create());    addOption("regex", "regex", "The regular expression to use", true);    addOption("groupsToKeep", "g", "The number of the capturing groups to keep", false);    addOption("transformerClass", "t", "The optional class specifying the Regex Transformer", false);    addOption("formatterClass", "t", "The optional class specifying the Regex Formatter", false);    addOption(DefaultOptionCreator.analyzerOption().create());    if (parseArguments(args) == null) {        return -1;    }    Configuration conf = getConf();            conf.set(RegexMapper.REGEX, getOption("regex"));    String gtk = getOption("groupsToKeep");    if (gtk != null) {        conf.set(RegexMapper.GROUP_MATCHERS, gtk);    }    String trans = getOption("transformerClass");    if (trans != null) {        if ("url".equalsIgnoreCase(trans)) {            trans = URLDecodeTransformer.class.getName();        }        conf.set(RegexMapper.TRANSFORMER_CLASS, trans);    }    String formatter = getOption("formatterClass");    if (formatter != null) {        if ("fpg".equalsIgnoreCase(formatter)) {            formatter = FPGFormatter.class.getName();        }        conf.set(RegexMapper.FORMATTER_CLASS, formatter);    }    Path input = getInputPath();    Path output = getOutputPath();    if (hasOption(DefaultOptionCreator.OVERWRITE_OPTION)) {        HadoopUtil.delete(getConf(), output);    }    Class<? extends Analyzer> analyzerClass = getAnalyzerClassFromOption();    if (analyzerClass != null) {        conf.set(RegexMapper.ANALYZER_NAME, analyzerClass.getName());    }    Job job = prepareJob(input, output, TextInputFormat.class, RegexMapper.class, LongWritable.class, Text.class, TextOutputFormat.class);    boolean succeeded = job.waitForCompletion(true);    return succeeded ? 0 : -1;}
0
public static void main(String[] args) throws Exception
{    ToolRunner.run(new RegexConverterDriver(), args);}
0
protected void setup(Context context) throws IOException, InterruptedException
{    groupsToKeep = new ArrayList<>();    Configuration config = context.getConfiguration();    String regexStr = config.get(REGEX);    regex = Pattern.compile(regexStr);    String[] groups = config.getStrings(GROUP_MATCHERS);    if (groups != null) {        for (String group : groups) {            groupsToKeep.add(Integer.parseInt(group));        }    }    transformer = ClassUtils.instantiateAs(config.get(TRANSFORMER_CLASS, IdentityTransformer.class.getName()), RegexTransformer.class);    String analyzerName = config.get(ANALYZER_NAME);    if (analyzerName != null && transformer instanceof AnalyzerTransformer) {        Analyzer analyzer = ClassUtils.instantiateAs(analyzerName, Analyzer.class);        ((AnalyzerTransformer) transformer).setAnalyzer(analyzer);    }    formatter = ClassUtils.instantiateAs(config.get(FORMATTER_CLASS, IdentityFormatter.class.getName()), RegexFormatter.class);}
0
protected void map(LongWritable key, Text text, Context context) throws IOException, InterruptedException
{    String result = RegexUtils.extract(text.toString(), regex, groupsToKeep, " ", transformer);    if (!result.isEmpty()) {        String format = formatter.format(result);        context.write(key, new Text(format));    }}
0
public static String extract(CharSequence line, Pattern pattern, Collection<Integer> groupsToKeep, String separator, RegexTransformer transformer)
{    StringBuilder bldr = new StringBuilder();    extract(line, bldr, pattern, groupsToKeep, separator, transformer);    return bldr.toString();}
0
public static void extract(CharSequence line, StringBuilder outputBuffer, Pattern pattern, Collection<Integer> groupsToKeep, String separator, RegexTransformer transformer)
{    if (transformer == null) {        transformer = IDENTITY_TRANSFORMER;    }    Matcher matcher = pattern.matcher(line);    String match;    if (groupsToKeep.isEmpty()) {        while (matcher.find()) {            match = matcher.group();            if (match != null) {                outputBuffer.append(transformer.transformMatch(match)).append(separator);            }        }    } else {        while (matcher.find()) {            for (Integer groupNum : groupsToKeep) {                match = matcher.group(groupNum);                if (match != null) {                    outputBuffer.append(transformer.transformMatch(match)).append(separator);                }            }        }    }        if (outputBuffer.length() > 0) {        outputBuffer.setLength(outputBuffer.length() - separator.length());    }}
0
public String transformMatch(String match)
{    try {        return URLDecoder.decode(match, enc);    } catch (UnsupportedEncodingException e) {        throw new IllegalStateException(e);    }}
0
public int run(String[] args) throws Exception
{    addInputOption();    addOutputOption();    addOption("substring", "b", "The number of chars to print out per value", false);    addOption(buildOption("count", "c", "Report the count only", false, false, null));    addOption("numItems", "n", "Output at most <n> key value pairs", false);    addOption(buildOption("facets", "fa", "Output the counts per key.  Note, if there are a lot of unique keys, " + "this can take up a fair amount of memory", false, false, null));    addOption(buildOption("quiet", "q", "Print only file contents.", false, false, null));    if (parseArguments(args, false, true) == null) {        return -1;    }    Path[] pathArr;    Configuration conf = new Configuration();    Path input = getInputPath();    FileSystem fs = input.getFileSystem(conf);    if (fs.getFileStatus(input).isDir()) {        pathArr = FileUtil.stat2Paths(fs.listStatus(input, PathFilters.logsCRCFilter()));    } else {        pathArr = new Path[1];        pathArr[0] = input;    }    Writer writer;    boolean shouldClose;    if (hasOption("output")) {        shouldClose = true;        writer = Files.newWriter(new File(getOption("output")), Charsets.UTF_8);    } else {        shouldClose = false;        writer = new OutputStreamWriter(System.out, Charsets.UTF_8);    }    try {        for (Path path : pathArr) {            if (!hasOption("quiet")) {                writer.append("Input Path: ").append(String.valueOf(path)).append('\n');            }            int sub = Integer.MAX_VALUE;            if (hasOption("substring")) {                sub = Integer.parseInt(getOption("substring"));            }            boolean countOnly = hasOption("count");            SequenceFileIterator<?, ?> iterator = new SequenceFileIterator<>(path, true, conf);            if (!hasOption("quiet")) {                writer.append("Key class: ").append(iterator.getKeyClass().toString());                writer.append(" Value Class: ").append(iterator.getValueClass().toString()).append('\n');            }            OpenObjectIntHashMap<String> facets = null;            if (hasOption("facets")) {                facets = new OpenObjectIntHashMap<>();            }            long count = 0;            if (countOnly) {                while (iterator.hasNext()) {                    Pair<?, ?> record = iterator.next();                    String key = record.getFirst().toString();                    if (facets != null) {                                                facets.adjustOrPutValue(key, 1, 1);                    }                    count++;                }                writer.append("Count: ").append(String.valueOf(count)).append('\n');            } else {                long numItems = Long.MAX_VALUE;                if (hasOption("numItems")) {                    numItems = Long.parseLong(getOption("numItems"));                    if (!hasOption("quiet")) {                        writer.append("Max Items to dump: ").append(String.valueOf(numItems)).append("\n");                    }                }                while (iterator.hasNext() && count < numItems) {                    Pair<?, ?> record = iterator.next();                    String key = record.getFirst().toString();                    writer.append("Key: ").append(key);                    String str = record.getSecond().toString();                    writer.append(": Value: ").append(str.length() > sub ? str.substring(0, sub) : str);                    writer.write('\n');                    if (facets != null) {                                                facets.adjustOrPutValue(key, 1, 1);                    }                    count++;                }                if (!hasOption("quiet")) {                    writer.append("Count: ").append(String.valueOf(count)).append('\n');                }            }            if (facets != null) {                List<String> keyList = new ArrayList<>(facets.size());                IntArrayList valueList = new IntArrayList(facets.size());                facets.pairsSortedByKey(keyList, valueList);                writer.append("-----Facets---\n");                writer.append("Key\t\tCount\n");                int i = 0;                for (String key : keyList) {                    writer.append(key).append("\t\t").append(String.valueOf(valueList.get(i++))).append('\n');                }            }        }        writer.flush();    } finally {        if (shouldClose) {            Closeables.close(writer, false);        }    }    return 0;}
0
public static void main(String[] args) throws Exception
{    new SequenceFileDumper().run(args);}
0
public int run(String[] args) throws Exception
{    if (parseArgs(args)) {        splitDirectory();    }    return 0;}
0
public static void main(String[] args) throws Exception
{    ToolRunner.run(new Configuration(), new SplitInput(), args);}
0
private boolean parseArgs(String[] args) throws Exception
{    addInputOption();    addOption("trainingOutput", "tr", "The training data output directory", false);    addOption("testOutput", "te", "The test data output directory", false);    addOption("testSplitSize", "ss", "The number of documents held back as test data for each category", false);    addOption("testSplitPct", "sp", "The % of documents held back as test data for each category", false);    addOption("splitLocation", "sl", "Location for start of test data expressed as a percentage of the input file " + "size (0=start, 50=middle, 100=end", false);    addOption("randomSelectionSize", "rs", "The number of items to be randomly selected as test data ", false);    addOption("randomSelectionPct", "rp", "Percentage of items to be randomly selected as test data when using " + "mapreduce mode", false);    addOption("charset", "c", "The name of the character encoding of the input files (not needed if using " + "SequenceFiles)", false);    addOption(buildOption("sequenceFiles", "seq", "Set if the input files are sequence files.  Default is false", false, false, "false"));    addOption(DefaultOptionCreator.methodOption().create());    addOption(DefaultOptionCreator.overwriteOption().create());        addOption("keepPct", "k", "The percentage of total data to keep in map-reduce mode, the rest will be ignored.  " + "Default is 100%", false);    addOption("mapRedOutputDir", "mro", "Output directory for map reduce jobs", false);    if (parseArguments(args) == null) {        return false;    }    try {        inputDirectory = getInputPath();        useMapRed = getOption(DefaultOptionCreator.METHOD_OPTION).equalsIgnoreCase(DefaultOptionCreator.MAPREDUCE_METHOD);        if (useMapRed) {            if (!hasOption("randomSelectionPct")) {                throw new OptionException(getCLIOption("randomSelectionPct"), "must set randomSelectionPct when mapRed option is used");            }            if (!hasOption("mapRedOutputDir")) {                throw new OptionException(getCLIOption("mapRedOutputDir"), "mapRedOutputDir must be set when mapRed option is used");            }            mapRedOutputDirectory = new Path(getOption("mapRedOutputDir"));            if (hasOption("keepPct")) {                keepPct = Integer.parseInt(getOption("keepPct"));            }            if (hasOption(DefaultOptionCreator.OVERWRITE_OPTION)) {                HadoopUtil.delete(getConf(), mapRedOutputDirectory);            }        } else {            if (!hasOption("trainingOutput") || !hasOption("testOutput")) {                throw new OptionException(getCLIOption("trainingOutput"), "trainingOutput and testOutput must be set if mapRed option is not used");            }            if (!hasOption("testSplitSize") && !hasOption("testSplitPct") && !hasOption("randomSelectionPct") && !hasOption("randomSelectionSize")) {                throw new OptionException(getCLIOption("testSplitSize"), "must set one of test split size/percentage or randomSelectionSize/percentage");            }            trainingOutputDirectory = new Path(getOption("trainingOutput"));            testOutputDirectory = new Path(getOption("testOutput"));            FileSystem fs = trainingOutputDirectory.getFileSystem(getConf());            if (hasOption(DefaultOptionCreator.OVERWRITE_OPTION)) {                HadoopUtil.delete(fs.getConf(), trainingOutputDirectory);                HadoopUtil.delete(fs.getConf(), testOutputDirectory);            }            fs.mkdirs(trainingOutputDirectory);            fs.mkdirs(testOutputDirectory);        }        if (hasOption("charset")) {            charset = Charset.forName(getOption("charset"));        }        if (hasOption("testSplitSize") && hasOption("testSplitPct")) {            throw new OptionException(getCLIOption("testSplitPct"), "must have either split size or split percentage " + "option, not BOTH");        }        if (hasOption("testSplitSize")) {            setTestSplitSize(Integer.parseInt(getOption("testSplitSize")));        }        if (hasOption("testSplitPct")) {            setTestSplitPct(Integer.parseInt(getOption("testSplitPct")));        }        if (hasOption("splitLocation")) {            setSplitLocation(Integer.parseInt(getOption("splitLocation")));        }        if (hasOption("randomSelectionSize")) {            setTestRandomSelectionSize(Integer.parseInt(getOption("randomSelectionSize")));        }        if (hasOption("randomSelectionPct")) {            setTestRandomSelectionPct(Integer.parseInt(getOption("randomSelectionPct")));        }        useSequence = hasOption("sequenceFiles");    } catch (OptionException e) {                CommandLineUtil.printHelp(getGroup());        return false;    }    validate();    return true;}
1
public void splitDirectory() throws IOException, ClassNotFoundException, InterruptedException
{    this.splitDirectory(inputDirectory);}
0
public void splitDirectory(Path inputDir) throws IOException, ClassNotFoundException, InterruptedException
{    Configuration conf = getConf();    splitDirectory(conf, inputDir);}
0
public void splitDirectory(Configuration conf, Path inputDir) throws IOException, ClassNotFoundException, InterruptedException
{    FileSystem fs = inputDir.getFileSystem(conf);    if (fs.getFileStatus(inputDir) == null) {        throw new IOException(inputDir + " does not exist");    }    if (!fs.getFileStatus(inputDir).isDir()) {        throw new IOException(inputDir + " is not a directory");    }    if (useMapRed) {        SplitInputJob.run(conf, inputDir, mapRedOutputDirectory, keepPct, testRandomSelectionPct);    } else {                FileStatus[] fileStats = fs.listStatus(inputDir, PathFilters.logsCRCFilter());        for (FileStatus inputFile : fileStats) {            if (!inputFile.isDir()) {                splitFile(inputFile.getPath());            }        }    }}
0
public void splitFile(Path inputFile) throws IOException
{    Configuration conf = getConf();    FileSystem fs = inputFile.getFileSystem(conf);    if (fs.getFileStatus(inputFile) == null) {        throw new IOException(inputFile + " does not exist");    }    if (fs.getFileStatus(inputFile).isDir()) {        throw new IOException(inputFile + " is a directory");    }    validate();    Path testOutputFile = new Path(testOutputDirectory, inputFile.getName());    Path trainingOutputFile = new Path(trainingOutputDirectory, inputFile.getName());    int lineCount = countLines(fs, inputFile, charset);        int testSplitStart = 0;        int testSplitSize = this.testSplitSize;    BitSet randomSel = null;    if (testRandomSelectionPct > 0 || testRandomSelectionSize > 0) {        testSplitSize = this.testRandomSelectionSize;        if (testRandomSelectionPct > 0) {            testSplitSize = Math.round(lineCount * testRandomSelectionPct / 100.0f);        }                long[] ridx = new long[testSplitSize];        RandomSampler.sample(testSplitSize, lineCount - 1, testSplitSize, 0, ridx, 0, RandomUtils.getRandom());        randomSel = new BitSet(lineCount);        for (long idx : ridx) {            randomSel.set((int) idx + 1);        }    } else {        if (testSplitPct > 0) {                        testSplitSize = Math.round(lineCount * testSplitPct / 100.0f);                    } else {                    }        if (splitLocation > 0) {                        testSplitStart = Math.round(lineCount * splitLocation / 100.0f);            if (lineCount - testSplitStart < testSplitSize) {                                testSplitStart = lineCount - testSplitSize;            }                    }        if (testSplitStart < 0) {            throw new IllegalArgumentException("test split size for " + inputFile + " is too large, it would produce an " + "empty training set from the initial set of " + lineCount + " examples");        } else if (lineCount - testSplitSize < testSplitSize) {                    }    }    int trainCount = 0;    int testCount = 0;    if (!useSequence) {        try (BufferedReader reader = new BufferedReader(new InputStreamReader(fs.open(inputFile), charset));            Writer trainingWriter = new OutputStreamWriter(fs.create(trainingOutputFile), charset);            Writer testWriter = new OutputStreamWriter(fs.create(testOutputFile), charset)) {            String line;            int pos = 0;            while ((line = reader.readLine()) != null) {                pos++;                Writer writer;                if (testRandomSelectionPct > 0) {                                        writer = randomSel.get(pos) ? testWriter : trainingWriter;                } else {                                        writer = pos > testSplitStart ? testWriter : trainingWriter;                }                if (writer == testWriter) {                    if (testCount >= testSplitSize) {                        writer = trainingWriter;                    } else {                        testCount++;                    }                }                if (writer == trainingWriter) {                    trainCount++;                }                writer.write(line);                writer.write('\n');            }        }    } else {        try (SequenceFileIterator<Writable, Writable> iterator = new SequenceFileIterator<>(inputFile, false, fs.getConf());            SequenceFile.Writer trainingWriter = SequenceFile.createWriter(fs, fs.getConf(), trainingOutputFile, iterator.getKeyClass(), iterator.getValueClass());            SequenceFile.Writer testWriter = SequenceFile.createWriter(fs, fs.getConf(), testOutputFile, iterator.getKeyClass(), iterator.getValueClass())) {            int pos = 0;            while (iterator.hasNext()) {                pos++;                SequenceFile.Writer writer;                if (testRandomSelectionPct > 0) {                                        writer = randomSel.get(pos) ? testWriter : trainingWriter;                } else {                                        writer = pos > testSplitStart ? testWriter : trainingWriter;                }                if (writer == testWriter) {                    if (testCount >= testSplitSize) {                        writer = trainingWriter;                    } else {                        testCount++;                    }                }                if (writer == trainingWriter) {                    trainCount++;                }                Pair<Writable, Writable> pair = iterator.next();                writer.append(pair.getFirst(), pair.getSecond());            }        }    }            if (callback != null) {        callback.splitComplete(inputFile, lineCount, trainCount, testCount, testSplitStart);    }}
1
public int getTestSplitSize()
{    return testSplitSize;}
0
public void setTestSplitSize(int testSplitSize)
{    this.testSplitSize = testSplitSize;}
0
public int getTestSplitPct()
{    return testSplitPct;}
0
public void setTestSplitPct(int testSplitPct)
{    this.testSplitPct = testSplitPct;}
0
public void setKeepPct(int keepPct)
{    this.keepPct = keepPct;}
0
public void setUseMapRed(boolean useMapRed)
{    this.useMapRed = useMapRed;}
0
public void setMapRedOutputDirectory(Path mapRedOutputDirectory)
{    this.mapRedOutputDirectory = mapRedOutputDirectory;}
0
public int getSplitLocation()
{    return splitLocation;}
0
public void setSplitLocation(int splitLocation)
{    this.splitLocation = splitLocation;}
0
public Charset getCharset()
{    return charset;}
0
public void setCharset(Charset charset)
{    this.charset = charset;}
0
public Path getInputDirectory()
{    return inputDirectory;}
0
public void setInputDirectory(Path inputDir)
{    this.inputDirectory = inputDir;}
0
public Path getTrainingOutputDirectory()
{    return trainingOutputDirectory;}
0
public void setTrainingOutputDirectory(Path trainingOutputDir)
{    this.trainingOutputDirectory = trainingOutputDir;}
0
public Path getTestOutputDirectory()
{    return testOutputDirectory;}
0
public void setTestOutputDirectory(Path testOutputDir)
{    this.testOutputDirectory = testOutputDir;}
0
public SplitCallback getCallback()
{    return callback;}
0
public void setCallback(SplitCallback callback)
{    this.callback = callback;}
0
public int getTestRandomSelectionSize()
{    return testRandomSelectionSize;}
0
public void setTestRandomSelectionSize(int testRandomSelectionSize)
{    this.testRandomSelectionSize = testRandomSelectionSize;}
0
public int getTestRandomSelectionPct()
{    return testRandomSelectionPct;}
0
public void setTestRandomSelectionPct(int randomSelectionPct)
{    this.testRandomSelectionPct = randomSelectionPct;}
0
public void validate() throws IOException
{    Preconditions.checkArgument(testSplitSize >= 1 || testSplitSize == -1, "Invalid testSplitSize: " + testSplitSize + ". Must be: testSplitSize >= 1 or testSplitSize = -1");    Preconditions.checkArgument(splitLocation >= 0 && splitLocation <= 100 || splitLocation == -1, "Invalid splitLocation percentage: " + splitLocation + ". Must be: 0 <= splitLocation <= 100 or splitLocation = -1");    Preconditions.checkArgument(testSplitPct >= 0 && testSplitPct <= 100 || testSplitPct == -1, "Invalid testSplitPct percentage: " + testSplitPct + ". Must be: 0 <= testSplitPct <= 100 or testSplitPct = -1");    Preconditions.checkArgument(testRandomSelectionPct >= 0 && testRandomSelectionPct <= 100 || testRandomSelectionPct == -1, "Invalid testRandomSelectionPct percentage: " + testRandomSelectionPct + ". Must be: 0 <= testRandomSelectionPct <= 100 or testRandomSelectionPct = -1");    Preconditions.checkArgument(trainingOutputDirectory != null || useMapRed, "No training output directory was specified");    Preconditions.checkArgument(testOutputDirectory != null || useMapRed, "No test output directory was specified");        int count = 0;    if (testSplitSize > 0) {        count++;    }    if (testSplitPct > 0) {        count++;    }    if (testRandomSelectionSize > 0) {        count++;    }    if (testRandomSelectionPct > 0) {        count++;    }    Preconditions.checkArgument(count == 1, "Exactly one of testSplitSize, testSplitPct, testRandomSelectionSize, " + "testRandomSelectionPct should be set");    if (!useMapRed) {        Configuration conf = getConf();        FileSystem fs = trainingOutputDirectory.getFileSystem(conf);        FileStatus trainingOutputDirStatus = fs.getFileStatus(trainingOutputDirectory);        Preconditions.checkArgument(trainingOutputDirStatus != null && trainingOutputDirStatus.isDir(), "%s is not a directory", trainingOutputDirectory);        FileStatus testOutputDirStatus = fs.getFileStatus(testOutputDirectory);        Preconditions.checkArgument(testOutputDirStatus != null && testOutputDirStatus.isDir(), "%s is not a directory", testOutputDirectory);    }}
0
public static int countLines(FileSystem fs, Path inputFile, Charset charset) throws IOException
{    int lineCount = 0;    try (BufferedReader reader = new BufferedReader(new InputStreamReader(fs.open(inputFile), charset))) {        while (reader.readLine() != null) {            lineCount++;        }    }    return lineCount;}
0
public static void run(Configuration initialConf, Path inputPath, Path outputPath, int keepPct, float randomSelectionPercent) throws IOException, ClassNotFoundException, InterruptedException
{    int downsamplingFactor = (int) (100.0 / keepPct);    initialConf.setInt(DOWNSAMPLING_FACTOR, downsamplingFactor);    initialConf.setFloat(RANDOM_SELECTION_PCT, randomSelectionPercent);        FileSystem fs = FileSystem.get(initialConf);    SequenceFileDirIterator<? extends WritableComparable, Writable> iterator = new SequenceFileDirIterator<>(inputPath, PathType.LIST, PathFilters.partFilter(), null, false, fs.getConf());    Class<? extends WritableComparable> keyClass;    Class<? extends Writable> valueClass;    if (iterator.hasNext()) {        Pair<? extends WritableComparable, Writable> pair = iterator.next();        keyClass = pair.getFirst().getClass();        valueClass = pair.getSecond().getClass();    } else {        throw new IllegalStateException("Couldn't determine class of the input values");    }    Job job = new Job(new Configuration(initialConf));    MultipleOutputs.addNamedOutput(job, TRAINING_TAG, SequenceFileOutputFormat.class, keyClass, valueClass);    MultipleOutputs.addNamedOutput(job, TEST_TAG, SequenceFileOutputFormat.class, keyClass, valueClass);    job.setJarByClass(SplitInputJob.class);    FileInputFormat.addInputPath(job, inputPath);    FileOutputFormat.setOutputPath(job, outputPath);    job.setNumReduceTasks(1);    job.setInputFormatClass(SequenceFileInputFormat.class);    job.setOutputFormatClass(SequenceFileOutputFormat.class);    job.setMapperClass(SplitInputMapper.class);    job.setReducerClass(SplitInputReducer.class);    job.setSortComparatorClass(SplitInputComparator.class);    job.setOutputKeyClass(keyClass);    job.setOutputValueClass(valueClass);    job.submit();    boolean succeeded = job.waitForCompletion(true);    if (!succeeded) {        throw new IllegalStateException("Job failed!");    }}
0
public void setup(Context ctx)
{    downsamplingFactor = ctx.getConfiguration().getInt(DOWNSAMPLING_FACTOR, 1);}
0
public void run(Context context) throws IOException, InterruptedException
{    setup(context);    int i = 0;    while (context.nextKeyValue()) {        if (i % downsamplingFactor == 0) {            map(context.getCurrentKey(), context.getCurrentValue(), context);        }        i++;    }    cleanup(context);}
0
protected void setup(Context ctx) throws IOException
{    randomSelectionPercent = ctx.getConfiguration().getFloat(RANDOM_SELECTION_PCT, 0);    multipleOutputs = new MultipleOutputs(ctx);}
0
protected void reduce(WritableComparable<?> key, Iterable<Writable> values, Context context) throws IOException, InterruptedException
{    for (Writable value : values) {        if (rnd.nextInt(100) < randomSelectionPercent) {            multipleOutputs.write(TEST_TAG, key, value);        } else {            multipleOutputs.write(TRAINING_TAG, key, value);        }    }}
0
protected void cleanup(Context context) throws IOException
{    try {        multipleOutputs.close();    } catch (InterruptedException e) {        throw new IOException(e);    }}
0
public int compare(byte[] b1, int s1, int l1, byte[] b2, int s2, int l2)
{    if (rnd.nextBoolean()) {        return 1;    } else {        return -1;    }}
0
protected Vector computeNext()
{    String line;    try {        while ((line = reader.readLine()) != null) {            line = line.trim();            if (!line.isEmpty() && !line.startsWith(ARFFModel.ARFF_COMMENT)) {                break;            }        }    } catch (IOException ioe) {        throw new IllegalStateException(ioe);    }    if (line == null) {        try {            Closeables.close(reader, true);        } catch (IOException e) {            throw new IllegalStateException(e);        }        return endOfData();    }    Vector result;    Matcher contents = DATA_PATTERN.matcher(line);    if (contents.find()) {        line = contents.group(1);        String[] splits = splitCSV(line);        result = new RandomAccessSparseVector(model.getLabelSize());        for (String split : splits) {            int idIndex = split.indexOf(' ');            int idx = Integer.parseInt(split.substring(0, idIndex).trim());            String data = split.substring(idIndex).trim();            if (!"?".equals(data)) {                result.setQuick(idx, model.getValue(data, idx));            }        }    } else {        result = new DenseVector(model.getLabelSize());        String[] splits = splitCSV(line);        for (int i = 0; i < splits.length; i++) {            String split = splits[i];            split = split.trim();            if (WORDS_WITHOUT_SPARSE.matcher(split).matches() && !"?".equals(split)) {                result.setQuick(i, model.getValue(split, i));            }        }    }    return result;}
0
public static String[] splitCSV(String line)
{    StringBuilder sb = new StringBuilder(128);    List<String> tokens = new ArrayList<>();    char escapeChar = '\0';    for (int i = 0; i < line.length(); i++) {        char c = line.charAt(i);        if (c == '\\') {            i++;            sb.append(line.charAt(i));        } else if (c == '"' || c == '\'') {                        if (c == escapeChar) {                escapeChar = '\0';            } else if (escapeChar == '\0') {                escapeChar = c;            }            sb.append(c);        } else if (c == ',') {            if (escapeChar == '\0') {                tokens.add(sb.toString().trim());                                sb.setLength(0);            } else {                sb.append(c);            }        } else {            sb.append(c);        }    }    if (sb.length() > 0) {        tokens.add(sb.toString().trim());    }    return tokens.toArray(new String[tokens.size()]);}
0
public String getIndicator()
{    return indicator;}
0
public String getLabel(String line)
{    int idx = line.lastIndexOf(indicator);    return removeQuotes(line.substring(ARFFModel.ATTRIBUTE.length(), idx));}
0
public static String removeQuotes(String str)
{    String cleaned = str;    if (cleaned != null) {        cleaned = cleaned.trim();        boolean isQuoted = cleaned.length() > 1 && (cleaned.startsWith("\"") && cleaned.endsWith("\"") || cleaned.startsWith("'") && cleaned.endsWith("'"));        if (isQuoted) {            cleaned = cleaned.substring(1, cleaned.length() - 1);        }    }    return cleaned;}
0
public Iterator<Vector> iterator()
{    return new ARFFIterator(buff, model);}
0
public ARFFModel getModel()
{    return model;}
0
public static void main(String[] args) throws IOException
{    DefaultOptionBuilder obuilder = new DefaultOptionBuilder();    ArgumentBuilder abuilder = new ArgumentBuilder();    GroupBuilder gbuilder = new GroupBuilder();    Option inputOpt = obuilder.withLongName("input").withRequired(true).withArgument(abuilder.withName("input").withMinimum(1).withMaximum(1).create()).withDescription("The file or directory containing the ARFF files.  If it is a directory, all .arff files will be converted").withShortName("d").create();    Option outputOpt = obuilder.withLongName("output").withRequired(true).withArgument(abuilder.withName("output").withMinimum(1).withMaximum(1).create()).withDescription("The output directory.  Files will have the same name as the input, but with the extension .mvc").withShortName("o").create();    Option maxOpt = obuilder.withLongName("max").withRequired(false).withArgument(abuilder.withName("max").withMinimum(1).withMaximum(1).create()).withDescription("The maximum number of vectors to output.  If not specified, then it will loop over all docs").withShortName("m").create();    Option dictOutOpt = obuilder.withLongName("dictOut").withRequired(true).withArgument(abuilder.withName("dictOut").withMinimum(1).withMaximum(1).create()).withDescription("The file to output the label bindings").withShortName("t").create();    Option jsonDictonaryOpt = obuilder.withLongName("json-dictonary").withRequired(false).withDescription("Write dictonary in JSON format").withShortName("j").create();    Option delimiterOpt = obuilder.withLongName("delimiter").withRequired(false).withArgument(abuilder.withName("delimiter").withMinimum(1).withMaximum(1).create()).withDescription("The delimiter for outputing the dictionary").withShortName("l").create();    Option helpOpt = obuilder.withLongName("help").withDescription("Print out help").withShortName("h").create();    Group group = gbuilder.withName("Options").withOption(inputOpt).withOption(outputOpt).withOption(maxOpt).withOption(helpOpt).withOption(dictOutOpt).withOption(jsonDictonaryOpt).withOption(delimiterOpt).create();    try {        Parser parser = new Parser();        parser.setGroup(group);        CommandLine cmdLine = parser.parse(args);        if (cmdLine.hasOption(helpOpt)) {            CommandLineUtil.printHelp(group);            return;        }        if (cmdLine.hasOption(inputOpt)) {                        File input = new File(cmdLine.getValue(inputOpt).toString());            long maxDocs = Long.MAX_VALUE;            if (cmdLine.hasOption(maxOpt)) {                maxDocs = Long.parseLong(cmdLine.getValue(maxOpt).toString());            }            if (maxDocs < 0) {                throw new IllegalArgumentException("maxDocs must be >= 0");            }            String outDir = cmdLine.getValue(outputOpt).toString();                        String delimiter = cmdLine.hasOption(delimiterOpt) ? cmdLine.getValue(delimiterOpt).toString() : "\t";            File dictOut = new File(cmdLine.getValue(dictOutOpt).toString());            boolean jsonDictonary = cmdLine.hasOption(jsonDictonaryOpt);            ARFFModel model = new MapBackedARFFModel();            if (input.exists() && input.isDirectory()) {                File[] files = input.listFiles(new FilenameFilter() {                    @Override                    public boolean accept(File file, String name) {                        return name.endsWith(".arff");                    }                });                for (File file : files) {                    writeFile(outDir, file, maxDocs, model, dictOut, delimiter, jsonDictonary);                }            } else {                writeFile(outDir, input, maxDocs, model, dictOut, delimiter, jsonDictonary);            }        }    } catch (OptionException e) {                CommandLineUtil.printHelp(group);    }}
1
public boolean accept(File file, String name)
{    return name.endsWith(".arff");}
0
protected static void writeLabelBindings(File dictOut, ARFFModel arffModel, String delimiter, boolean jsonDictonary) throws IOException
{    try (Writer writer = Files.newWriterSupplier(dictOut, Charsets.UTF_8, true).getOutput()) {        if (jsonDictonary) {            writeLabelBindingsJSON(writer, arffModel);        } else {            writeLabelBindings(writer, arffModel, delimiter);        }    }}
0
protected static void writeLabelBindingsJSON(Writer writer, ARFFModel arffModel) throws IOException
{        List<Entry<String, Integer>> attributes = new ArrayList<>();    attributes.addAll(arffModel.getLabelBindings().entrySet());    Collections.sort(attributes, new Comparator<Map.Entry<String, Integer>>() {        @Override        public int compare(Entry<String, Integer> t, Entry<String, Integer> t1) {            return t.getValue().compareTo(t1.getValue());        }    });        List<Map<String, Object>> jsonObjects = new LinkedList<>();    for (int i = 0; i < attributes.size(); i++) {        Entry<String, Integer> modelRepresentation = attributes.get(i);        Map<String, Object> jsonRepresentation = new HashMap<>();        jsonObjects.add(jsonRepresentation);                jsonRepresentation.put("label", i < (attributes.size() - 1) ? String.valueOf(false) : String.valueOf(true));        String attribute = modelRepresentation.getKey();        jsonRepresentation.put("attribute", attribute);        Map<String, Integer> nominalValues = arffModel.getNominalMap().get(attribute);        if (nominalValues != null) {            String[] values = nominalValues.keySet().toArray(new String[1]);            jsonRepresentation.put("values", values);            jsonRepresentation.put("type", "categorical");        } else {            jsonRepresentation.put("type", "numerical");        }    }    writer.write(OBJECT_MAPPER.writeValueAsString(jsonObjects));}
0
public int compare(Entry<String, Integer> t, Entry<String, Integer> t1)
{    return t.getValue().compareTo(t1.getValue());}
0
protected static void writeLabelBindings(Writer writer, ARFFModel arffModel, String delimiter) throws IOException
{    Map<String, Integer> labels = arffModel.getLabelBindings();    writer.write("Label bindings for Relation " + arffModel.getRelation() + '\n');    for (Map.Entry<String, Integer> entry : labels.entrySet()) {        writer.write(entry.getKey());        writer.write(delimiter);        writer.write(String.valueOf(entry.getValue()));        writer.write('\n');    }    writer.write('\n');    writer.write("Values for nominal attributes\n");        Map<String, Map<String, Integer>> nominalMap = arffModel.getNominalMap();        writer.write(String.valueOf(nominalMap.size()) + "\n");    for (Entry<String, Map<String, Integer>> entry : nominalMap.entrySet()) {                writer.write(entry.getKey() + "\n");        Set<Entry<String, Integer>> attributeValues = entry.getValue().entrySet();                writer.write(attributeValues.size() + "\n");        for (Map.Entry<String, Integer> value : attributeValues) {                        writer.write(String.format("%s%s%s\n", value.getKey(), delimiter, value.getValue().toString()));        }    }}
0
protected static void writeFile(String outDir, File file, long maxDocs, ARFFModel arffModel, File dictOut, String delimiter, boolean jsonDictonary) throws IOException
{        ARFFModel model = new MapBackedARFFModel(arffModel.getWords(), arffModel.getWordCount() + 1, arffModel.getNominalMap());    Iterable<Vector> iteratable = new ARFFVectorIterable(file, model);    String outFile = outDir + '/' + file.getName() + ".mvc";    try (VectorWriter vectorWriter = getSeqFileWriter(outFile)) {        long numDocs = vectorWriter.write(iteratable, maxDocs);        writeLabelBindings(dictOut, model, delimiter, jsonDictonary);            }}
1
private static VectorWriter getSeqFileWriter(String outFile) throws IOException
{    Path path = new Path(outFile);    Configuration conf = new Configuration();    FileSystem fs = FileSystem.get(conf);    SequenceFile.Writer seqWriter = SequenceFile.createWriter(fs, conf, path, LongWritable.class, VectorWritable.class);    return new SequenceFileVectorWriter(seqWriter);}
0
public String getRelation()
{    return relation;}
0
public void setRelation(String relation)
{    this.relation = relation;}
0
public double getValue(String data, int idx)
{    ARFFType type = typeMap.get(idx);    if (type == null) {        throw new IllegalArgumentException("Attribute type cannot be NULL, attribute index was: " + idx);    }    data = QUOTE_PATTERN.matcher(data).replaceAll("");    data = data.trim();    double result;    switch(type) {        case NUMERIC:        case INTEGER:        case REAL:            result = processNumeric(data);            break;        case DATE:            result = processDate(data, idx);            break;        case STRING:                        result = processString(data);            break;        case NOMINAL:            String label = idxLabel.get(idx);            result = processNominal(label, data);            break;        default:            throw new IllegalStateException("Unknown type: " + type);    }    return result;}
0
protected double processNominal(String label, String data)
{    double result;    Map<String, Integer> classes = nominalMap.get(label);    if (classes != null) {        Integer ord = classes.get(ARFFType.removeQuotes(data));        if (ord != null) {            result = ord;        } else {            throw new IllegalStateException("Invalid nominal: " + data + " for label: " + label);        }    } else {        throw new IllegalArgumentException("Invalid nominal label: " + label + " Data: " + data);    }    return result;}
0
protected double processString(String data)
{    data = QUOTE_PATTERN.matcher(data).replaceAll("");        Long theLong = words.get(data);    if (theLong == null) {        theLong = wordCount++;        words.put(data, theLong);    }    return theLong;}
0
protected static double processNumeric(String data)
{    if (isNumeric(data)) {        return Double.parseDouble(data);    }    return Double.NaN;}
0
public static boolean isNumeric(String str)
{    NumberFormat formatter = NumberFormat.getInstance();    ParsePosition parsePosition = new ParsePosition(0);    formatter.parse(str, parsePosition);    return str.length() == parsePosition.getIndex();}
0
protected double processDate(String data, int idx)
{    DateFormat format = dateMap.get(idx);    if (format == null) {        format = new SimpleDateFormat("yyyy-MM-dd'T'HH:mm:ss", Locale.ENGLISH);    }    double result;    try {        Date date = format.parse(data);                result = date.getTime();    } catch (ParseException e) {        throw new IllegalArgumentException(e);    }    return result;}
0
public Map<String, Integer> getLabelBindings()
{    return Collections.unmodifiableMap(labelBindings);}
0
public Map<Integer, ARFFType> getTypeMap()
{    return Collections.unmodifiableMap(typeMap);}
0
public Map<Integer, DateFormat> getDateMap()
{    return Collections.unmodifiableMap(dateMap);}
0
public Map<String, Map<String, Integer>> getNominalMap()
{    return nominalMap;}
0
public Map<String, Long> getWords()
{    return words;}
0
public Integer getNominalValue(String label, String nominal)
{    return nominalMap.get(label).get(nominal);}
0
public void addNominal(String label, String nominal, int idx)
{    Map<String, Integer> noms = nominalMap.get(label);    if (noms == null) {        noms = new HashMap<>();        nominalMap.put(label, noms);    }    noms.put(nominal, idx);}
0
public DateFormat getDateFormat(Integer idx)
{    return dateMap.get(idx);}
0
public void addDateFormat(Integer idx, DateFormat format)
{    dateMap.put(idx, format);}
0
public Integer getLabelIndex(String label)
{    return labelBindings.get(label);}
0
public void addLabel(String label, Integer idx)
{    labelBindings.put(label, idx);    idxLabel.put(idx, label);}
0
public ARFFType getARFFType(Integer idx)
{    return typeMap.get(idx);}
0
public void addType(Integer idx, ARFFType type)
{    typeMap.put(idx, type);}
0
public long getWordCount()
{    return wordCount;}
0
public int getLabelSize()
{    return labelBindings.size();}
0
protected Vector computeNext()
{    String[] line;    try {        line = parser.getLine();    } catch (IOException e) {        throw new IllegalStateException(e);    }    if (line == null) {        return endOfData();    }    Vector result = new DenseVector(line.length);    for (int i = 0; i < line.length; i++) {        result.setQuick(i, Double.parseDouble(line[i]));    }    return result;}
0
public void write(TermInfo ti) throws IOException
{    Iterator<TermEntry> entIter = ti.getAllEntries();    try {        writer.write(String.valueOf(ti.totalTerms(field)));        writer.write('\n');        writer.write("#term" + delimiter + "doc freq" + delimiter + "idx");        writer.write('\n');        while (entIter.hasNext()) {            TermEntry entry = entIter.next();            writer.write(entry.getTerm());            writer.write(delimiter);            writer.write(String.valueOf(entry.getDocFreq()));            writer.write(delimiter);            writer.write(String.valueOf(entry.getTermIdx()));            writer.write('\n');        }    } finally {        Closeables.close(writer, false);    }}
0
public void close()
{}
0
public long write(Iterable<Vector> iterable, long maxDocs) throws IOException
{    for (Vector point : iterable) {        if (recNum >= maxDocs) {            break;        }        if (point != null) {            writer.append(new LongWritable(recNum++), new VectorWritable(point));        }    }    return recNum;}
0
public void write(Vector vector) throws IOException
{    writer.append(new LongWritable(recNum++), new VectorWritable(vector));}
0
public long write(Iterable<Vector> iterable) throws IOException
{    return write(iterable, Long.MAX_VALUE);}
0
public void close() throws IOException
{    Closeables.close(writer, false);}
0
public SequenceFile.Writer getWriter()
{    return writer;}
0
protected Writer getWriter()
{    return writer;}
0
public long write(Iterable<Vector> iterable) throws IOException
{    return write(iterable, Long.MAX_VALUE);}
0
public long write(Iterable<Vector> iterable, long maxDocs) throws IOException
{    long result = 0;    for (Vector vector : iterable) {        if (result >= maxDocs) {            break;        }        write(vector);        result++;    }    return result;}
0
public void write(Vector vector) throws IOException
{    writer.write(vector.asFormatString());    writer.write('\n');}
0
public void close() throws IOException
{    Closeables.close(writer, false);}
0
protected Vector computeNext()
{    try {        int doc;        Terms termFreqVector;        String name;        do {            doc = this.nextDocId;            nextDocId++;            if (doc >= indexReader.maxDoc()) {                return endOfData();            }            termFreqVector = indexReader.getTermVector(doc, field);            name = getVectorName(doc);            if (termFreqVector == null) {                numErrorDocs++;                if (numErrorDocs >= maxErrorDocs) {                                        throw new IllegalStateException("There are too many documents that do not have a term vector for " + field);                }                if (numErrorDocs >= nextLogRecord) {                    if (skippedErrorMessages == 0) {                                            } else {                                            }                    nextLogRecord = bump.increment();                    skippedErrorMessages = 0;                } else {                    skippedErrorMessages++;                }            }        } while (termFreqVector == null);                TermsEnum te = termFreqVector.iterator();        BytesRef term;        TFDFMapper mapper = new TFDFMapper(indexReader.numDocs(), weight, this.terminfo);        mapper.setExpectations(field, termFreqVector.size());        while ((term = te.next()) != null) {            mapper.map(term, (int) te.totalTermFreq());        }        Vector result = mapper.getVector();        if (result == null) {                        return null;        }        if (normPower == LuceneIterable.NO_NORMALIZING) {            result = new NamedVector(result, name);        } else {            result = new NamedVector(result.normalize(normPower), name);        }        return result;    } catch (IOException ioe) {        throw new IllegalStateException(ioe);    }}
1
public int totalTerms(String field)
{    return termEntries.size();}
0
public TermEntry getTermEntry(String field, String term)
{    if (!this.field.equals(field)) {        return null;    }    return termEntries.get(term);}
0
public Iterator<TermEntry> getAllEntries()
{    return termEntries.values().iterator();}
0
public void getLabels() throws IOException
{    try (Writer writer = (this.output == null) ? new OutputStreamWriter(System.out, Charsets.UTF_8) : Files.newWriter(new File(this.output), Charsets.UTF_8)) {        for (Map.Entry<Integer, List<WeightedPropertyVectorWritable>> integerListEntry : clusterIdToPoints.entrySet()) {            List<WeightedPropertyVectorWritable> wpvws = integerListEntry.getValue();            List<TermInfoClusterInOut> termInfos = getClusterLabels(integerListEntry.getKey(), wpvws);            if (termInfos != null) {                writer.write('\n');                writer.write("Top labels for Cluster ");                writer.write(String.valueOf(integerListEntry.getKey()));                writer.write(" containing ");                writer.write(String.valueOf(wpvws.size()));                writer.write(" vectors");                writer.write('\n');                writer.write("Term \t\t LLR \t\t In-ClusterDF \t\t Out-ClusterDF ");                writer.write('\n');                for (TermInfoClusterInOut termInfo : termInfos) {                    writer.write(termInfo.getTerm());                    writer.write("\t\t");                    writer.write(String.valueOf(termInfo.getLogLikelihoodRatio()));                    writer.write("\t\t");                    writer.write(String.valueOf(termInfo.getInClusterDF()));                    writer.write("\t\t");                    writer.write(String.valueOf(termInfo.getOutClusterDF()));                    writer.write('\n');                }            }        }    }}
0
protected List<TermInfoClusterInOut> getClusterLabels(Integer integer, Collection<WeightedPropertyVectorWritable> wpvws) throws IOException
{    if (wpvws.size() < minNumIds) {                return null;    }        Directory dir = FSDirectory.open(Paths.get(this.indexDir));    IndexReader reader = DirectoryReader.open(dir);        Collection<String> idSet = new HashSet<>();    for (WeightedPropertyVectorWritable wpvw : wpvws) {        Vector vector = wpvw.getVector();        if (vector instanceof NamedVector) {            idSet.add(((NamedVector) vector).getName());        }    }    int numDocs = reader.numDocs();    FixedBitSet clusterDocBitset = getClusterDocBitset(reader, idSet, this.idField);        /**     * This code is as that of CachedTermInfo, with one major change, which is to get the document frequency.     *     * Since we have deleted the documents out of the cluster, the document frequency for a term should only     * include the in-cluster documents. The document frequency obtained from TermEnum reflects the frequency     * in the entire index. To get the in-cluster frequency, we need to query the index to get the term     * frequencies in each document. The number of results of this call will be the in-cluster document     * frequency.     */    Terms t = MultiFields.getTerms(reader, contentField);    TermsEnum te = t.iterator();    Map<String, TermEntry> termEntryMap = new LinkedHashMap<>();        Bits liveDocs = MultiFields.getLiveDocs(reader);    int count = 0;    BytesRef term;    while ((term = te.next()) != null) {        FixedBitSet termBitset = new FixedBitSet(reader.maxDoc());        PostingsEnum docsEnum = MultiFields.getTermDocsEnum(reader, contentField, term);        int docID;        while ((docID = docsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {                        if (liveDocs != null && !liveDocs.get(docID)) {                                termBitset.set(docsEnum.docID());            }        }                        termBitset.and(clusterDocBitset);        int inclusterDF = (int) termBitset.cardinality();        TermEntry entry = new TermEntry(term.utf8ToString(), count++, inclusterDF);        termEntryMap.put(entry.getTerm(), entry);    }    List<TermInfoClusterInOut> clusteredTermInfo = new LinkedList<>();    int clusterSize = wpvws.size();    for (TermEntry termEntry : termEntryMap.values()) {        int corpusDF = reader.docFreq(new Term(this.contentField, termEntry.getTerm()));        int outDF = corpusDF - termEntry.getDocFreq();        int inDF = termEntry.getDocFreq();        double logLikelihoodRatio = scoreDocumentFrequencies(inDF, outDF, clusterSize, numDocs);        TermInfoClusterInOut termInfoCluster = new TermInfoClusterInOut(termEntry.getTerm(), inDF, outDF, logLikelihoodRatio);        clusteredTermInfo.add(termInfoCluster);    }    Collections.sort(clusteredTermInfo);        Closeables.close(reader, true);    termEntryMap.clear();    return clusteredTermInfo.subList(0, Math.min(clusteredTermInfo.size(), maxLabels));}
1
private static FixedBitSet getClusterDocBitset(IndexReader reader, Collection<String> idSet, String idField) throws IOException
{    int numDocs = reader.numDocs();    FixedBitSet bitset = new FixedBitSet(numDocs);    Set<String> idFieldSelector = null;    if (idField != null) {        idFieldSelector = new TreeSet<>();        idFieldSelector.add(idField);    }    for (int i = 0; i < numDocs; i++) {        String id;                if (idField == null) {            id = Integer.toString(i);        } else {            id = reader.document(i, idFieldSelector).get(idField);        }        if (idSet.contains(id)) {            bitset.set(i);        }    }        return bitset;}
1
private static double scoreDocumentFrequencies(long inDF, long outDF, long clusterSize, long corpusSize)
{    long k12 = clusterSize - inDF;    long k22 = corpusSize - clusterSize - outDF;    return LogLikelihood.logLikelihoodRatio(inDF, k12, outDF, k22);}
0
public String getIdField()
{    return idField;}
0
public void setIdField(String idField)
{    this.idField = idField;}
0
public String getOutput()
{    return output;}
0
public void setOutput(String output)
{    this.output = output;}
0
public static void main(String[] args)
{    DefaultOptionBuilder obuilder = new DefaultOptionBuilder();    ArgumentBuilder abuilder = new ArgumentBuilder();    GroupBuilder gbuilder = new GroupBuilder();    Option indexOpt = obuilder.withLongName("dir").withRequired(true).withArgument(abuilder.withName("dir").withMinimum(1).withMaximum(1).create()).withDescription("The Lucene index directory").withShortName("d").create();    Option outputOpt = obuilder.withLongName("output").withRequired(false).withArgument(abuilder.withName("output").withMinimum(1).withMaximum(1).create()).withDescription("The output file. If not specified, the result is printed on console.").withShortName("o").create();    Option fieldOpt = obuilder.withLongName("field").withRequired(true).withArgument(abuilder.withName("field").withMinimum(1).withMaximum(1).create()).withDescription("The content field in the index").withShortName("f").create();    Option idFieldOpt = obuilder.withLongName("idField").withRequired(false).withArgument(abuilder.withName("idField").withMinimum(1).withMaximum(1).create()).withDescription("The field for the document ID in the index.  If null, then the Lucene internal doc " + "id is used which is prone to error if the underlying index changes").withShortName("i").create();    Option seqOpt = obuilder.withLongName("seqFileDir").withRequired(true).withArgument(abuilder.withName("seqFileDir").withMinimum(1).withMaximum(1).create()).withDescription("The directory containing Sequence Files for the Clusters").withShortName("s").create();    Option pointsOpt = obuilder.withLongName("pointsDir").withRequired(true).withArgument(abuilder.withName("pointsDir").withMinimum(1).withMaximum(1).create()).withDescription("The directory containing points sequence files mapping input vectors to their cluster.  ").withShortName("p").create();    Option minClusterSizeOpt = obuilder.withLongName("minClusterSize").withRequired(false).withArgument(abuilder.withName("minClusterSize").withMinimum(1).withMaximum(1).create()).withDescription("The minimum number of points required in a cluster to print the labels for").withShortName("m").create();    Option maxLabelsOpt = obuilder.withLongName("maxLabels").withRequired(false).withArgument(abuilder.withName("maxLabels").withMinimum(1).withMaximum(1).create()).withDescription("The maximum number of labels to print per cluster").withShortName("x").create();    Option helpOpt = DefaultOptionCreator.helpOption();    Group group = gbuilder.withName("Options").withOption(indexOpt).withOption(idFieldOpt).withOption(outputOpt).withOption(fieldOpt).withOption(seqOpt).withOption(pointsOpt).withOption(helpOpt).withOption(maxLabelsOpt).withOption(minClusterSizeOpt).create();    try {        Parser parser = new Parser();        parser.setGroup(group);        CommandLine cmdLine = parser.parse(args);        if (cmdLine.hasOption(helpOpt)) {            CommandLineUtil.printHelp(group);            return;        }        Path seqFileDir = new Path(cmdLine.getValue(seqOpt).toString());        Path pointsDir = new Path(cmdLine.getValue(pointsOpt).toString());        String indexDir = cmdLine.getValue(indexOpt).toString();        String contentField = cmdLine.getValue(fieldOpt).toString();        String idField = null;        if (cmdLine.hasOption(idFieldOpt)) {            idField = cmdLine.getValue(idFieldOpt).toString();        }        String output = null;        if (cmdLine.hasOption(outputOpt)) {            output = cmdLine.getValue(outputOpt).toString();        }        int maxLabels = DEFAULT_MAX_LABELS;        if (cmdLine.hasOption(maxLabelsOpt)) {            maxLabels = Integer.parseInt(cmdLine.getValue(maxLabelsOpt).toString());        }        int minSize = DEFAULT_MIN_IDS;        if (cmdLine.hasOption(minClusterSizeOpt)) {            minSize = Integer.parseInt(cmdLine.getValue(minClusterSizeOpt).toString());        }        ClusterLabels clusterLabel = new ClusterLabels(seqFileDir, pointsDir, indexDir, contentField, minSize, maxLabels);        if (idField != null) {            clusterLabel.setIdField(idField);        }        if (output != null) {            clusterLabel.setOutput(output);        }        clusterLabel.getLabels();    } catch (OptionException e) {                CommandLineUtil.printHelp(group);    } catch (IOException e) {            }}
1
public void dumpVectors() throws IOException
{    File file = new File(luceneDir);    Preconditions.checkArgument(file.isDirectory(), "Lucene directory: " + file.getAbsolutePath() + " does not exist or is not a directory");    Preconditions.checkArgument(maxDocs >= 0, "maxDocs must be >= 0");    Preconditions.checkArgument(minDf >= 1, "minDf must be >= 1");    Preconditions.checkArgument(maxDFPercent <= 99, "maxDFPercent must be <= 99");    Directory dir = FSDirectory.open(Paths.get(file.getAbsolutePath()));    IndexReader reader = DirectoryReader.open(dir);    Weight weight;    if ("tf".equalsIgnoreCase(weightType)) {        weight = new TF();    } else if ("tfidf".equalsIgnoreCase(weightType)) {        weight = new TFIDF();    } else {        throw new IllegalArgumentException("Weight type " + weightType + " is not supported");    }    TermInfo termInfo = new CachedTermInfo(reader, field, minDf, maxDFPercent);    LuceneIterable iterable;    if (norm == LuceneIterable.NO_NORMALIZING) {        iterable = new LuceneIterable(reader, idField, field, termInfo, weight, LuceneIterable.NO_NORMALIZING, maxPercentErrorDocs);    } else {        iterable = new LuceneIterable(reader, idField, field, termInfo, weight, norm, maxPercentErrorDocs);    }        try (VectorWriter vectorWriter = getSeqFileWriter(outFile)) {        long numDocs = vectorWriter.write(iterable, maxDocs);            }    File dictOutFile = new File(dictOut);        Writer writer = Files.newWriter(dictOutFile, Charsets.UTF_8);    try (DelimitedTermInfoWriter tiWriter = new DelimitedTermInfoWriter(writer, delimiter, field)) {        tiWriter.write(termInfo);    }    if (!"".equals(seqDictOut)) {                Path path = new Path(seqDictOut);        Configuration conf = new Configuration();        FileSystem fs = FileSystem.get(conf);        try (SequenceFile.Writer seqWriter = SequenceFile.createWriter(fs, conf, path, Text.class, IntWritable.class)) {            Text term = new Text();            IntWritable termIndex = new IntWritable();            Iterator<TermEntry> termEntries = termInfo.getAllEntries();            while (termEntries.hasNext()) {                TermEntry termEntry = termEntries.next();                term.set(termEntry.getTerm());                termIndex.set(termEntry.getTermIdx());                seqWriter.append(term, termIndex);            }        }    }}
1
public static void main(String[] args) throws IOException
{    DefaultOptionBuilder obuilder = new DefaultOptionBuilder();    ArgumentBuilder abuilder = new ArgumentBuilder();    GroupBuilder gbuilder = new GroupBuilder();    Option inputOpt = obuilder.withLongName("dir").withRequired(true).withArgument(abuilder.withName("dir").withMinimum(1).withMaximum(1).create()).withDescription("The Lucene directory").withShortName("d").create();    Option outputOpt = obuilder.withLongName("output").withRequired(true).withArgument(abuilder.withName("output").withMinimum(1).withMaximum(1).create()).withDescription("The output file").withShortName("o").create();    Option fieldOpt = obuilder.withLongName("field").withRequired(true).withArgument(abuilder.withName("field").withMinimum(1).withMaximum(1).create()).withDescription("The field in the index").withShortName("f").create();    Option idFieldOpt = obuilder.withLongName("idField").withRequired(false).withArgument(abuilder.withName("idField").withMinimum(1).withMaximum(1).create()).withDescription("The field in the index containing the index.  If null, then the Lucene internal doc " + "id is used which is prone to error if the underlying index changes").create();    Option dictOutOpt = obuilder.withLongName("dictOut").withRequired(true).withArgument(abuilder.withName("dictOut").withMinimum(1).withMaximum(1).create()).withDescription("The output of the dictionary").withShortName("t").create();    Option seqDictOutOpt = obuilder.withLongName("seqDictOut").withRequired(false).withArgument(abuilder.withName("seqDictOut").withMinimum(1).withMaximum(1).create()).withDescription("The output of the dictionary as sequence file").withShortName("st").create();    Option weightOpt = obuilder.withLongName("weight").withRequired(false).withArgument(abuilder.withName("weight").withMinimum(1).withMaximum(1).create()).withDescription("The kind of weight to use. Currently TF or TFIDF").withShortName("w").create();    Option delimiterOpt = obuilder.withLongName("delimiter").withRequired(false).withArgument(abuilder.withName("delimiter").withMinimum(1).withMaximum(1).create()).withDescription("The delimiter for outputting the dictionary").withShortName("l").create();    Option powerOpt = obuilder.withLongName("norm").withRequired(false).withArgument(abuilder.withName("norm").withMinimum(1).withMaximum(1).create()).withDescription("The norm to use, expressed as either a double or \"INF\" if you want to use the Infinite norm.  " + "Must be greater or equal to 0.  The default is not to normalize").withShortName("n").create();    Option maxOpt = obuilder.withLongName("max").withRequired(false).withArgument(abuilder.withName("max").withMinimum(1).withMaximum(1).create()).withDescription("The maximum number of vectors to output.  If not specified, then it will loop over all docs").withShortName("m").create();    Option minDFOpt = obuilder.withLongName("minDF").withRequired(false).withArgument(abuilder.withName("minDF").withMinimum(1).withMaximum(1).create()).withDescription("The minimum document frequency.  Default is 1").withShortName("md").create();    Option maxDFPercentOpt = obuilder.withLongName("maxDFPercent").withRequired(false).withArgument(abuilder.withName("maxDFPercent").withMinimum(1).withMaximum(1).create()).withDescription("The max percentage of docs for the DF.  Can be used to remove really high frequency terms." + "  Expressed as an integer between 0 and 100. Default is 99.").withShortName("x").create();    Option maxPercentErrorDocsOpt = obuilder.withLongName("maxPercentErrorDocs").withRequired(false).withArgument(abuilder.withName("maxPercentErrorDocs").withMinimum(1).withMaximum(1).create()).withDescription("The max percentage of docs that can have a null term vector. These are noise document and can occur if the " + "analyzer used strips out all terms in the target field. This percentage is expressed as a value " + "between 0 and 1. The default is 0.").withShortName("err").create();    Option helpOpt = obuilder.withLongName("help").withDescription("Print out help").withShortName("h").create();    Group group = gbuilder.withName("Options").withOption(inputOpt).withOption(idFieldOpt).withOption(outputOpt).withOption(delimiterOpt).withOption(helpOpt).withOption(fieldOpt).withOption(maxOpt).withOption(dictOutOpt).withOption(seqDictOutOpt).withOption(powerOpt).withOption(maxDFPercentOpt).withOption(weightOpt).withOption(minDFOpt).withOption(maxPercentErrorDocsOpt).create();    try {        Parser parser = new Parser();        parser.setGroup(group);        CommandLine cmdLine = parser.parse(args);        if (cmdLine.hasOption(helpOpt)) {            CommandLineUtil.printHelp(group);            return;        }        if (cmdLine.hasOption(inputOpt)) {                        Driver luceneDriver = new Driver();            luceneDriver.setLuceneDir(cmdLine.getValue(inputOpt).toString());            if (cmdLine.hasOption(maxOpt)) {                luceneDriver.setMaxDocs(Long.parseLong(cmdLine.getValue(maxOpt).toString()));            }            if (cmdLine.hasOption(weightOpt)) {                luceneDriver.setWeightType(cmdLine.getValue(weightOpt).toString());            }            luceneDriver.setField(cmdLine.getValue(fieldOpt).toString());            if (cmdLine.hasOption(minDFOpt)) {                luceneDriver.setMinDf(Integer.parseInt(cmdLine.getValue(minDFOpt).toString()));            }            if (cmdLine.hasOption(maxDFPercentOpt)) {                luceneDriver.setMaxDFPercent(Integer.parseInt(cmdLine.getValue(maxDFPercentOpt).toString()));            }            if (cmdLine.hasOption(powerOpt)) {                String power = cmdLine.getValue(powerOpt).toString();                if ("INF".equals(power)) {                    luceneDriver.setNorm(Double.POSITIVE_INFINITY);                } else {                    luceneDriver.setNorm(Double.parseDouble(power));                }            }            if (cmdLine.hasOption(idFieldOpt)) {                luceneDriver.setIdField(cmdLine.getValue(idFieldOpt).toString());            }            if (cmdLine.hasOption(maxPercentErrorDocsOpt)) {                luceneDriver.setMaxPercentErrorDocs(Double.parseDouble(cmdLine.getValue(maxPercentErrorDocsOpt).toString()));            }            luceneDriver.setOutFile(cmdLine.getValue(outputOpt).toString());            luceneDriver.setDelimiter(cmdLine.hasOption(delimiterOpt) ? cmdLine.getValue(delimiterOpt).toString() : "\t");            luceneDriver.setDictOut(cmdLine.getValue(dictOutOpt).toString());            if (cmdLine.hasOption(seqDictOutOpt)) {                luceneDriver.setSeqDictOut(cmdLine.getValue(seqDictOutOpt).toString());            }            luceneDriver.dumpVectors();        }    } catch (OptionException e) {                CommandLineUtil.printHelp(group);    }}
1
private static VectorWriter getSeqFileWriter(String outFile) throws IOException
{    Path path = new Path(outFile);    Configuration conf = new Configuration();    FileSystem fs = FileSystem.get(conf);        SequenceFile.Writer seqWriter = SequenceFile.createWriter(fs, conf, path, LongWritable.class, VectorWritable.class);    return new SequenceFileVectorWriter(seqWriter);}
0
public void setLuceneDir(String luceneDir)
{    this.luceneDir = luceneDir;}
0
public void setMaxDocs(long maxDocs)
{    this.maxDocs = maxDocs;}
0
public void setWeightType(String weightType)
{    this.weightType = weightType;}
0
public void setField(String field)
{    this.field = field;}
0
public void setMinDf(int minDf)
{    this.minDf = minDf;}
0
public void setMaxDFPercent(int maxDFPercent)
{    this.maxDFPercent = maxDFPercent;}
0
public void setNorm(double norm)
{    this.norm = norm;}
0
public void setIdField(String idField)
{    this.idField = idField;}
0
public void setOutFile(String outFile)
{    this.outFile = outFile;}
0
public void setDelimiter(String delimiter)
{    this.delimiter = delimiter;}
0
public void setDictOut(String dictOut)
{    this.dictOut = dictOut;}
0
public void setSeqDictOut(String seqDictOut)
{    this.seqDictOut = seqDictOut;}
0
public void setMaxPercentErrorDocs(double maxPercentErrorDocs)
{    this.maxPercentErrorDocs = maxPercentErrorDocs;}
0
public Iterator<Vector> iterator()
{    return new LuceneIterator(indexReader, idField, field, terminfo, weight, normPower, maxPercentErrorDocs);}
0
protected String getVectorName(int documentIndex) throws IOException
{    String name;    if (idField != null) {        name = indexReader.document(documentIndex, idFieldSelector).get(idField);    } else {        name = String.valueOf(documentIndex);    }    return name;}
0
public int hashCode()
{    return term.hashCode() ^ inClusterDF ^ outClusterDF ^ RandomUtils.hashDouble(logLikelihoodRatio);}
0
public boolean equals(Object o)
{    if (!(o instanceof TermInfoClusterInOut)) {        return false;    }    TermInfoClusterInOut other = (TermInfoClusterInOut) o;    return term.equals(other.getTerm()) && inClusterDF == other.getInClusterDF() && outClusterDF == other.getOutClusterDF() && logLikelihoodRatio == other.getLogLikelihoodRatio();}
0
public int compareTo(TermInfoClusterInOut that)
{    int res = Double.compare(that.logLikelihoodRatio, logLikelihoodRatio);    if (res == 0) {        res = term.compareTo(that.term);    }    return res;}
0
public int getInClusterDiff()
{    return this.inClusterDF - this.outClusterDF;}
0
 String getTerm()
{    return term;}
0
 int getInClusterDF()
{    return inClusterDF;}
0
 int getOutClusterDF()
{    return outClusterDF;}
0
 double getLogLikelihoodRatio()
{    return logLikelihoodRatio;}
0
public void setExpectations(String field, long numTerms)
{    this.field = field;    vector = new RandomAccessSparseVector(termInfo.totalTerms(field));    this.numTerms = numTerms;}
0
public void map(BytesRef term, int frequency)
{    TermEntry entry = termInfo.getTermEntry(field, term.utf8ToString());    if (entry != null) {        vector.setQuick(entry.getTermIdx(), weight.calculate(frequency, entry.getDocFreq(), (int) numTerms, numDocs));    }}
0
public Vector getVector()
{    return this.vector;}
0
public int run(String[] args) throws Exception
{    addInputOption();    addOutputOption();    Map<String, List<String>> parsedArgs = parseArguments(args);    if (parsedArgs == null) {        return -1;    }    Configuration conf = getConf();    FileSystem fs = FileSystem.get(conf);    Path outputPath = getOutputPath();    Path indexPath = new Path(outputPath, "docIndex");    Path matrixPath = new Path(outputPath, "matrix");    try (SequenceFile.Writer indexWriter = SequenceFile.createWriter(fs, conf, indexPath, IntWritable.class, Text.class);        SequenceFile.Writer matrixWriter = SequenceFile.createWriter(fs, conf, matrixPath, IntWritable.class, VectorWritable.class)) {        IntWritable docId = new IntWritable();        int i = 0;        int numCols = 0;        for (Pair<Text, VectorWritable> record : new SequenceFileDirIterable<Text, VectorWritable>(getInputPath(), PathType.LIST, PathFilters.logsCRCFilter(), null, true, conf)) {            VectorWritable value = record.getSecond();            docId.set(i);            indexWriter.append(docId, record.getFirst());            matrixWriter.append(docId, value);            i++;            numCols = value.get().size();        }                return 0;    }}
1
public static void main(String[] args) throws Exception
{    ToolRunner.run(new RowIdJob(), args);}
0
public String getTerm()
{    return term;}
0
public int getTermIdx()
{    return termIdx;}
0
public int getDocFreq()
{    return docFreq;}
0
public int run(String[] args) throws Exception
{    /**     *     Option seqOpt = obuilder.withLongName("seqFile").withRequired(false).withArgument(     *     abuilder.withName("seqFile").withMinimum(1).withMaximum(1).create()).withDescription(     *     "The Sequence File containing the Vectors").withShortName("s").create();     *     Option dirOpt = obuilder.withLongName("seqDirectory").withRequired(false).withArgument(     *     abuilder.withName("seqDirectory").withMinimum(1).withMaximum(1).create())     *     .withDescription("The directory containing Sequence File of Vectors")     *     .withShortName("d").create();     */    addInputOption();    addOutputOption();    addOption("useKey", "u", "If the Key is a vector than dump that instead");    addOption("printKey", "p", "Print out the key as well, delimited by tab (or the value if useKey is true");    addOption("dictionary", "d", "The dictionary file.", false);    addOption("dictionaryType", "dt", "The dictionary file type (text|seqfile)", false);    addOption("csv", "c", "Output the Vector as CSV.  Otherwise it substitutes in the terms for vector cell entries");    addOption("namesAsComments", "n", "If using CSV output, optionally add a comment line for each NamedVector " + "(if the vector is one) printing out the name");    addOption("nameOnly", "N", "Use the name as the value for each NamedVector (skip other vectors)");    addOption("sortVectors", "sort", "Sort output key/value pairs of the vector entries in abs magnitude " + "descending order");    addOption("quiet", "q", "Print only file contents");    addOption("sizeOnly", "sz", "Dump only the size of the vector");    addOption("numItems", "ni", "Output at most <n> vecors", false);    addOption("vectorSize", "vs", "Truncate vectors to <vs> length when dumping (most useful when in" + " conjunction with -sort", false);    addOption(buildOption("filter", "fi", "Only dump out those vectors whose name matches the filter." + "  Multiple items may be specified by repeating the argument.", true, 1, Integer.MAX_VALUE, false, null));    if (parseArguments(args, false, true) == null) {        return -1;    }    Path[] pathArr;    Configuration conf = new Configuration();    FileSystem fs = FileSystem.get(conf);    Path input = getInputPath();    FileStatus fileStatus = fs.getFileStatus(input);    if (fileStatus.isDir()) {        pathArr = FileUtil.stat2Paths(fs.listStatus(input, PathFilters.logsCRCFilter()));    } else {        FileStatus[] inputPaths = fs.globStatus(input);        pathArr = new Path[inputPaths.length];        int i = 0;        for (FileStatus fstatus : inputPaths) {            pathArr[i++] = fstatus.getPath();        }    }    String dictionaryType = getOption("dictionaryType", "text");    boolean sortVectors = hasOption("sortVectors");    boolean quiet = hasOption("quiet");    if (!quiet) {            }    String[] dictionary = null;    if (hasOption("dictionary")) {        String dictFile = getOption("dictionary");        switch(dictionaryType) {            case "text":                dictionary = VectorHelper.loadTermDictionary(new File(dictFile));                break;            case "sequencefile":                dictionary = VectorHelper.loadTermDictionary(conf, dictFile);                break;            default:                                throw new IOException("Invalid dictionary type: " + dictionaryType);        }    }    Set<String> filters;    if (hasOption("filter")) {        filters = Sets.newHashSet(getOptions("filter"));    } else {        filters = null;    }    boolean useCSV = hasOption("csv");    boolean sizeOnly = hasOption("sizeOnly");    boolean nameOnly = hasOption("nameOnly");    boolean namesAsComments = hasOption("namesAsComments");    boolean transposeKeyValue = hasOption("vectorAsKey");    Writer writer;    boolean shouldClose;    File output = getOutputFile();    if (output != null) {        shouldClose = true;                Files.createParentDirs(output);        writer = Files.newWriter(output, Charsets.UTF_8);    } else {        shouldClose = false;        writer = new OutputStreamWriter(System.out, Charsets.UTF_8);    }    try {        boolean printKey = hasOption("printKey");        if (useCSV && dictionary != null) {            writer.write("#");            for (int j = 0; j < dictionary.length; j++) {                writer.write(dictionary[j]);                if (j < dictionary.length - 1) {                    writer.write(',');                }            }            writer.write('\n');        }        Long numItems = null;        if (hasOption("numItems")) {            numItems = Long.parseLong(getOption("numItems"));            if (quiet) {                writer.append("#Max Items to dump: ").append(String.valueOf(numItems)).append('\n');            }        }        int maxIndexesPerVector = hasOption("vectorSize") ? Integer.parseInt(getOption("vectorSize")) : Integer.MAX_VALUE;        long itemCount = 0;        int fileCount = 0;        for (Path path : pathArr) {            if (numItems != null && numItems <= itemCount) {                break;            }            if (quiet) {                            }            SequenceFileIterable<Writable, Writable> iterable = new SequenceFileIterable<>(path, true, conf);            Iterator<Pair<Writable, Writable>> iterator = iterable.iterator();            long i = 0;            while (iterator.hasNext() && (numItems == null || itemCount < numItems)) {                Pair<Writable, Writable> record = iterator.next();                Writable keyWritable = record.getFirst();                Writable valueWritable = record.getSecond();                if (printKey) {                    Writable notTheVectorWritable = transposeKeyValue ? valueWritable : keyWritable;                    writer.write(notTheVectorWritable.toString());                    writer.write('\t');                }                Vector vector;                try {                    vector = ((VectorWritable) (transposeKeyValue ? keyWritable : valueWritable)).get();                } catch (ClassCastException e) {                    if ((transposeKeyValue ? keyWritable : valueWritable) instanceof WeightedPropertyVectorWritable) {                        vector = ((WeightedPropertyVectorWritable) (transposeKeyValue ? keyWritable : valueWritable)).getVector();                    } else {                        throw e;                    }                }                if (filters == null || !(vector instanceof NamedVector) || filters.contains(((NamedVector) vector).getName())) {                    if (sizeOnly) {                        if (vector instanceof NamedVector) {                            writer.write(((NamedVector) vector).getName());                            writer.write(":");                        } else {                            writer.write(String.valueOf(i++));                            writer.write(":");                        }                        writer.write(String.valueOf(vector.size()));                        writer.write('\n');                    } else if (nameOnly) {                        if (vector instanceof NamedVector) {                            writer.write(((NamedVector) vector).getName());                            writer.write('\n');                        }                    } else {                        String fmtStr;                        if (useCSV) {                            fmtStr = VectorHelper.vectorToCSVString(vector, namesAsComments);                        } else {                            fmtStr = VectorHelper.vectorToJson(vector, dictionary, maxIndexesPerVector, sortVectors);                        }                        writer.write(fmtStr);                        writer.write('\n');                    }                    itemCount++;                }            }        }        writer.flush();    } finally {        if (shouldClose) {            Closeables.close(writer, false);        }    }    return 0;}
1
public static void main(String[] args) throws Exception
{    ToolRunner.run(new Configuration(), new VectorDumper(), args);}
0
public static String vectorToCSVString(Vector vector, boolean namesAsComments) throws IOException
{    Appendable bldr = new StringBuilder(2048);    vectorToCSVString(vector, namesAsComments, bldr);    return bldr.toString();}
0
public static String buildJson(Iterable<Pair<String, Double>> iterable)
{    return buildJson(iterable, new StringBuilder(2048));}
0
public static String buildJson(Iterable<Pair<String, Double>> iterable, StringBuilder bldr)
{    bldr.append('{');    for (Pair<String, Double> p : iterable) {        bldr.append(p.getFirst());        bldr.append(':');        bldr.append(p.getSecond());        bldr.append(',');    }    if (bldr.length() > 1) {        bldr.setCharAt(bldr.length() - 1, '}');    }    return bldr.toString();}
0
public static List<Pair<Integer, Double>> topEntries(Vector vector, int maxEntries)
{        int sizeOfNonZeroElementsInVector = vector.getNumNonZeroElements();        if (sizeOfNonZeroElementsInVector < maxEntries) {        maxEntries = sizeOfNonZeroElementsInVector;    }    PriorityQueue<Pair<Integer, Double>> queue = new TDoublePQ<>(-1, maxEntries);    for (Element e : vector.nonZeroes()) {        queue.insertWithOverflow(Pair.of(e.index(), e.get()));    }    List<Pair<Integer, Double>> entries = new ArrayList<>();    Pair<Integer, Double> pair;    while ((pair = queue.pop()) != null) {        if (pair.getFirst() > -1) {            entries.add(pair);        }    }    Collections.sort(entries, new Comparator<Pair<Integer, Double>>() {        @Override        public int compare(Pair<Integer, Double> a, Pair<Integer, Double> b) {            return b.getSecond().compareTo(a.getSecond());        }    });    return entries;}
0
public int compare(Pair<Integer, Double> a, Pair<Integer, Double> b)
{    return b.getSecond().compareTo(a.getSecond());}
0
public static List<Pair<Integer, Double>> firstEntries(Vector vector, int maxEntries)
{    List<Pair<Integer, Double>> entries = new ArrayList<>();    Iterator<Vector.Element> it = vector.nonZeroes().iterator();    int i = 0;    while (it.hasNext() && i++ < maxEntries) {        Vector.Element e = it.next();        entries.add(Pair.of(e.index(), e.get()));    }    return entries;}
0
public static List<Pair<String, Double>> toWeightedTerms(Collection<Pair<Integer, Double>> entries, final String[] dictionary)
{    if (dictionary != null) {        return new ArrayList<>(Collections2.transform(entries, new Function<Pair<Integer, Double>, Pair<String, Double>>() {            @Override            public Pair<String, Double> apply(Pair<Integer, Double> p) {                return Pair.of(dictionary[p.getFirst()], p.getSecond());            }        }));    } else {        return new ArrayList<>(Collections2.transform(entries, new Function<Pair<Integer, Double>, Pair<String, Double>>() {            @Override            public Pair<String, Double> apply(Pair<Integer, Double> p) {                return Pair.of(Integer.toString(p.getFirst()), p.getSecond());            }        }));    }}
0
public Pair<String, Double> apply(Pair<Integer, Double> p)
{    return Pair.of(dictionary[p.getFirst()], p.getSecond());}
0
public Pair<String, Double> apply(Pair<Integer, Double> p)
{    return Pair.of(Integer.toString(p.getFirst()), p.getSecond());}
0
public static String vectorToJson(Vector vector, String[] dictionary, int maxEntries, boolean sort)
{    return buildJson(toWeightedTerms(sort ? topEntries(vector, maxEntries) : firstEntries(vector, maxEntries), dictionary));}
0
public static void vectorToCSVString(Vector vector, boolean namesAsComments, Appendable bldr) throws IOException
{    if (namesAsComments && vector instanceof NamedVector) {        bldr.append('#').append(((NamedVector) vector).getName()).append('\n');    }    Iterator<Vector.Element> iter = vector.all().iterator();    boolean first = true;    while (iter.hasNext()) {        if (first) {            first = false;        } else {            bldr.append(',');        }        Vector.Element elt = iter.next();        bldr.append(String.valueOf(elt.get()));    }    bldr.append('\n');}
0
public static String[] loadTermDictionary(File dictFile) throws IOException
{    try (InputStream in = new FileInputStream(dictFile)) {        return loadTermDictionary(in);    }}
0
public static String[] loadTermDictionary(Configuration conf, String filePattern)
{    OpenObjectIntHashMap<String> dict = new OpenObjectIntHashMap<>();    int maxIndexValue = 0;    for (Pair<Text, IntWritable> record : new SequenceFileDirIterable<Text, IntWritable>(new Path(filePattern), PathType.GLOB, null, null, true, conf)) {        dict.put(record.getFirst().toString(), record.getSecond().get());        if (record.getSecond().get() > maxIndexValue) {            maxIndexValue = record.getSecond().get();        }    }        int maxDictionarySize = maxIndexValue + 1 > dict.size() ? maxIndexValue + 1 : dict.size();    String[] dictionary = new String[maxDictionarySize];    for (String feature : dict.keys()) {        dictionary[dict.get(feature)] = feature;    }    return dictionary;}
0
private static String[] loadTermDictionary(InputStream is) throws IOException
{    FileLineIterator it = new FileLineIterator(is);    int numEntries = Integer.parseInt(it.next());    String[] result = new String[numEntries];    while (it.hasNext()) {        String line = it.next();        if (line.startsWith("#")) {            continue;        }        String[] tokens = TAB_PATTERN.split(line);        if (tokens.length < 3) {            continue;        }                int index = Integer.parseInt(tokens[2]);        result[index] = tokens[0];    }    return result;}
0
protected boolean lessThan(Pair<T, Double> a, Pair<T, Double> b)
{    return a.getSecond().compareTo(b.getSecond()) < 0;}
0
protected Pair<T, Double> getSentinelObject()
{    return Pair.of(sentinel, Double.NEGATIVE_INFINITY);}
0
public void testMemoryLoad() throws Exception
{    DataSource dataSource = EasyMock.createMock(DataSource.class);    Connection connection = EasyMock.createMock(Connection.class);    PreparedStatement statement = EasyMock.createMock(PreparedStatement.class);    ResultSet resultSet = EasyMock.createMock(ResultSet.class);    EasyMock.expect(dataSource.getConnection()).andReturn(connection);    EasyMock.expect(connection.prepareStatement(MySQLJDBCInMemoryItemSimilarity.DEFAULT_GET_ALL_ITEMSIMILARITIES_SQL, ResultSet.TYPE_FORWARD_ONLY, ResultSet.CONCUR_READ_ONLY)).andReturn(statement);    statement.setFetchDirection(ResultSet.FETCH_FORWARD);    EasyMock.expect(statement.executeQuery()).andReturn(resultSet);    EasyMock.expect(resultSet.next()).andReturn(true);    EasyMock.expect(resultSet.getLong(1)).andReturn(1L);    EasyMock.expect(resultSet.getLong(2)).andReturn(2L);    EasyMock.expect(resultSet.getDouble(3)).andReturn(0.5);    EasyMock.expect(resultSet.next()).andReturn(true);    EasyMock.expect(resultSet.getLong(1)).andReturn(1L);    EasyMock.expect(resultSet.getLong(2)).andReturn(3L);    EasyMock.expect(resultSet.getDouble(3)).andReturn(0.4);    EasyMock.expect(resultSet.next()).andReturn(true);    EasyMock.expect(resultSet.getLong(1)).andReturn(3L);    EasyMock.expect(resultSet.getLong(2)).andReturn(4L);    EasyMock.expect(resultSet.getDouble(3)).andReturn(0.1);    EasyMock.expect(resultSet.next()).andReturn(false);    resultSet.close();    statement.close();    connection.close();    EasyMock.replay(dataSource, connection, statement, resultSet);    ItemSimilarity similarity = new MySQLJDBCInMemoryItemSimilarity(dataSource);    assertEquals(0.5, similarity.itemSimilarity(1L, 2L), EPSILON);    assertEquals(0.4, similarity.itemSimilarity(1L, 3L), EPSILON);    assertEquals(0.1, similarity.itemSimilarity(3L, 4L), EPSILON);    assertTrue(Double.isNaN(similarity.itemSimilarity(1L, 4L)));    EasyMock.verify(dataSource, connection, statement, resultSet);}
0
public void setUp() throws Exception
{    super.setUp();    conf = getConfiguration();    fs = FileSystem.get(conf);    testdata = getTestTempDirPath("testdata");    output = getTestTempDirPath("output");        referenceData = TestKmeansClustering.getPointsWritable(REFERENCE);        generateSamples();}
0
private void initData(double dC, double dP, DistanceMeasure measure)
{    clusters = new ArrayList<>();    clusters.add(new Canopy(new DenseVector(new double[] { -dC, -dC }), 1, measure));    clusters.add(new Canopy(new DenseVector(new double[] { -dC, dC }), 3, measure));    clusters.add(new Canopy(new DenseVector(new double[] { dC, dC }), 5, measure));    clusters.add(new Canopy(new DenseVector(new double[] { dC, -dC }), 7, measure));    representativePoints = new HashMap<>();    for (Cluster cluster : clusters) {        List<VectorWritable> points = new ArrayList<>();        representativePoints.put(cluster.getId(), points);        points.add(new VectorWritable(cluster.getCenter().clone()));        points.add(new VectorWritable(cluster.getCenter().plus(new DenseVector(new double[] { dP, dP }))));        points.add(new VectorWritable(cluster.getCenter().plus(new DenseVector(new double[] { dP, -dP }))));        points.add(new VectorWritable(cluster.getCenter().plus(new DenseVector(new double[] { -dP, -dP }))));        points.add(new VectorWritable(cluster.getCenter().plus(new DenseVector(new double[] { -dP, dP }))));    }}
0
private void generateSamples(int num, double mx, double my, double sd)
{        for (int i = 0; i < num; i++) {        sampleData.add(new VectorWritable(new DenseVector(new double[] { UncommonDistributions.rNorm(mx, sd), UncommonDistributions.rNorm(my, sd) })));    }}
1
private void generateSamples()
{    generateSamples(500, 1, 1, 3);    generateSamples(300, 1, 0, 0.5);    generateSamples(300, 0, 2, 0.1);}
0
public void testCDbw0() throws IOException
{    ClusteringTestUtils.writePointsToFile(referenceData, getTestTempFilePath("testdata/file1"), fs, conf);    DistanceMeasure measure = new EuclideanDistanceMeasure();    initData(1, 0.25, measure);    CDbwEvaluator evaluator = new CDbwEvaluator(representativePoints, clusters, measure);    System.out.println("CDbw = " + evaluator.getCDbw());    System.out.println("Intra-cluster density = " + evaluator.intraClusterDensity());    System.out.println("Inter-cluster density = " + evaluator.interClusterDensity());    System.out.println("Separation = " + evaluator.separation());}
0
public void testCDbw1() throws IOException
{    ClusteringTestUtils.writePointsToFile(referenceData, getTestTempFilePath("testdata/file1"), fs, conf);    DistanceMeasure measure = new EuclideanDistanceMeasure();    initData(1, 0.5, measure);    CDbwEvaluator evaluator = new CDbwEvaluator(representativePoints, clusters, measure);    System.out.println("CDbw = " + evaluator.getCDbw());    System.out.println("Intra-cluster density = " + evaluator.intraClusterDensity());    System.out.println("Inter-cluster density = " + evaluator.interClusterDensity());    System.out.println("Separation = " + evaluator.separation());}
0
public void testCDbw2() throws IOException
{    ClusteringTestUtils.writePointsToFile(referenceData, getTestTempFilePath("testdata/file1"), fs, conf);    DistanceMeasure measure = new EuclideanDistanceMeasure();    initData(1, 0.75, measure);    CDbwEvaluator evaluator = new CDbwEvaluator(representativePoints, clusters, measure);    System.out.println("CDbw = " + evaluator.getCDbw());    System.out.println("Intra-cluster density = " + evaluator.intraClusterDensity());    System.out.println("Inter-cluster density = " + evaluator.interClusterDensity());    System.out.println("Separation = " + evaluator.separation());}
0
public void testEmptyCluster() throws IOException
{    ClusteringTestUtils.writePointsToFile(referenceData, getTestTempFilePath("testdata/file1"), fs, conf);    DistanceMeasure measure = new EuclideanDistanceMeasure();    initData(1, 0.25, measure);    Canopy cluster = new Canopy(new DenseVector(new double[] { 10, 10 }), 19, measure);    clusters.add(cluster);    List<VectorWritable> points = new ArrayList<>();    representativePoints.put(cluster.getId(), points);    CDbwEvaluator evaluator = new CDbwEvaluator(representativePoints, clusters, measure);    System.out.println("CDbw = " + evaluator.getCDbw());    System.out.println("Intra-cluster density = " + evaluator.intraClusterDensity());    System.out.println("Inter-cluster density = " + evaluator.interClusterDensity());    System.out.println("Separation = " + evaluator.separation());}
0
public void testSingleValueCluster() throws IOException
{    ClusteringTestUtils.writePointsToFile(referenceData, getTestTempFilePath("testdata/file1"), fs, conf);    DistanceMeasure measure = new EuclideanDistanceMeasure();    initData(1, 0.25, measure);    Canopy cluster = new Canopy(new DenseVector(new double[] { 0, 0 }), 19, measure);    clusters.add(cluster);    List<VectorWritable> points = new ArrayList<>();    points.add(new VectorWritable(cluster.getCenter().plus(new DenseVector(new double[] { 1, 1 }))));    representativePoints.put(cluster.getId(), points);    CDbwEvaluator evaluator = new CDbwEvaluator(representativePoints, clusters, measure);    System.out.println("CDbw = " + evaluator.getCDbw());    System.out.println("Intra-cluster density = " + evaluator.intraClusterDensity());    System.out.println("Inter-cluster density = " + evaluator.interClusterDensity());    System.out.println("Separation = " + evaluator.separation());}
0
public void testAllSameValueCluster() throws IOException
{    ClusteringTestUtils.writePointsToFile(referenceData, getTestTempFilePath("testdata/file1"), fs, conf);    DistanceMeasure measure = new EuclideanDistanceMeasure();    initData(1, 0.25, measure);    Canopy cluster = new Canopy(new DenseVector(new double[] { 0, 0 }), 19, measure);    clusters.add(cluster);    List<VectorWritable> points = new ArrayList<>();    points.add(new VectorWritable(cluster.getCenter()));    points.add(new VectorWritable(cluster.getCenter()));    points.add(new VectorWritable(cluster.getCenter()));    representativePoints.put(cluster.getId(), points);    CDbwEvaluator evaluator = new CDbwEvaluator(representativePoints, clusters, measure);    System.out.println("CDbw = " + evaluator.getCDbw());    System.out.println("Intra-cluster density = " + evaluator.intraClusterDensity());    System.out.println("Inter-cluster density = " + evaluator.interClusterDensity());    System.out.println("Separation = " + evaluator.separation());}
0
public void testAlmostSameValueCluster() throws IOException
{    ClusteringTestUtils.writePointsToFile(referenceData, getTestTempFilePath("testdata/file1"), fs, conf);    DistanceMeasure measure = new EuclideanDistanceMeasure();    initData(1, 0.25, measure);    Canopy cluster = new Canopy(new DenseVector(new double[] { 0, 0 }), 19, measure);    clusters.add(cluster);    List<VectorWritable> points = new ArrayList<>();    Vector delta = new DenseVector(new double[] { 0, Double.MIN_NORMAL });    points.add(new VectorWritable(delta.clone()));    points.add(new VectorWritable(delta.clone()));    points.add(new VectorWritable(delta.clone()));    points.add(new VectorWritable(delta.clone()));    points.add(new VectorWritable(delta.clone()));    representativePoints.put(cluster.getId(), points);    CDbwEvaluator evaluator = new CDbwEvaluator(representativePoints, clusters, measure);    System.out.println("CDbw = " + evaluator.getCDbw());    System.out.println("Intra-cluster density = " + evaluator.intraClusterDensity());    System.out.println("Inter-cluster density = " + evaluator.interClusterDensity());    System.out.println("Separation = " + evaluator.separation());}
0
public void testCanopy() throws Exception
{    ClusteringTestUtils.writePointsToFile(sampleData, getTestTempFilePath("testdata/file1"), fs, conf);    DistanceMeasure measure = new EuclideanDistanceMeasure();    CanopyDriver.run(getConfiguration(), testdata, output, measure, 3.1, 2.1, true, 0.0, true);    int numIterations = 10;    Path clustersIn = new Path(output, "clusters-0-final");    RepresentativePointsDriver.run(conf, clustersIn, new Path(output, "clusteredPoints"), output, measure, numIterations, true);    CDbwEvaluator evaluator = new CDbwEvaluator(conf, clustersIn);            System.out.println("Canopy CDbw = " + evaluator.getCDbw());    System.out.println("Intra-cluster density = " + evaluator.intraClusterDensity());    System.out.println("Inter-cluster density = " + evaluator.interClusterDensity());    System.out.println("Separation = " + evaluator.separation());}
0
public void testKmeans() throws Exception
{    ClusteringTestUtils.writePointsToFile(sampleData, getTestTempFilePath("testdata/file1"), fs, conf);    DistanceMeasure measure = new EuclideanDistanceMeasure();        CanopyDriver.run(getConfiguration(), testdata, output, measure, 3.1, 2.1, false, 0.0, true);        Path kmeansOutput = new Path(output, "kmeans");    KMeansDriver.run(testdata, new Path(output, "clusters-0-final"), kmeansOutput, 0.001, 10, true, 0.0, true);    int numIterations = 10;    Path clustersIn = new Path(kmeansOutput, "clusters-10-final");    RepresentativePointsDriver.run(conf, clustersIn, new Path(kmeansOutput, "clusteredPoints"), kmeansOutput, measure, numIterations, true);    CDbwEvaluator evaluator = new CDbwEvaluator(conf, clustersIn);    RepresentativePointsDriver.printRepresentativePoints(kmeansOutput, numIterations);        System.out.println("K-Means CDbw = " + evaluator.getCDbw());    System.out.println("Intra-cluster density = " + evaluator.intraClusterDensity());    System.out.println("Inter-cluster density = " + evaluator.interClusterDensity());    System.out.println("Separation = " + evaluator.separation());}
0
public void testFuzzyKmeans() throws Exception
{    ClusteringTestUtils.writePointsToFile(sampleData, getTestTempFilePath("testdata/file1"), fs, conf);    DistanceMeasure measure = new EuclideanDistanceMeasure();        CanopyDriver.run(getConfiguration(), testdata, output, measure, 3.1, 2.1, false, 0.0, true);    Path fuzzyKMeansOutput = new Path(output, "fuzzyk");        FuzzyKMeansDriver.run(testdata, new Path(output, "clusters-0-final"), fuzzyKMeansOutput, 0.001, 10, 2, true, true, 0, true);    int numIterations = 10;    Path clustersIn = new Path(fuzzyKMeansOutput, "clusters-4");    RepresentativePointsDriver.run(conf, clustersIn, new Path(fuzzyKMeansOutput, "clusteredPoints"), fuzzyKMeansOutput, measure, numIterations, true);    CDbwEvaluator evaluator = new CDbwEvaluator(conf, clustersIn);    RepresentativePointsDriver.printRepresentativePoints(fuzzyKMeansOutput, numIterations);        System.out.println("Fuzzy K-Means CDbw = " + evaluator.getCDbw());    System.out.println("Intra-cluster density = " + evaluator.intraClusterDensity());    System.out.println("Inter-cluster density = " + evaluator.interClusterDensity());    System.out.println("Separation = " + evaluator.separation());}
0
public void setUp() throws Exception
{    super.setUp();    Configuration conf = getConfiguration();    FileSystem fs = FileSystem.get(conf);        getSampleData(DOCS);    ClusteringTestUtils.writePointsToFile(sampleData, true, getTestTempFilePath("testdata/file1"), fs, conf);}
0
private void getSampleData(String[] docs2) throws IOException
{    sampleData = new ArrayList<>();    RAMDirectory directory = new RAMDirectory();    try (IndexWriter writer = new IndexWriter(directory, new IndexWriterConfig(new StandardAnalyzer()))) {        for (int i = 0; i < docs2.length; i++) {            Document doc = new Document();            Field id = new StringField("id", "doc_" + i, Field.Store.YES);            doc.add(id);                        FieldType fieldType = new FieldType();            fieldType.setStored(false);            fieldType.setIndexOptions(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS);            fieldType.setTokenized(true);            fieldType.setStoreTermVectors(true);            fieldType.setStoreTermVectorPositions(true);            fieldType.setStoreTermVectorOffsets(true);            fieldType.freeze();            Field text = new Field("content", docs2[i], fieldType);            doc.add(text);            writer.addDocument(doc);        }    }    IndexReader reader = DirectoryReader.open(directory);    Weight weight = new TFIDF();    TermInfo termInfo = new CachedTermInfo(reader, "content", 1, 100);    int numTerms = 0;    for (Iterator<TermEntry> it = termInfo.getAllEntries(); it.hasNext(); ) {        it.next();        numTerms++;    }    termDictionary = new String[numTerms];    int i = 0;    for (Iterator<TermEntry> it = termInfo.getAllEntries(); it.hasNext(); ) {        String term = it.next().getTerm();        termDictionary[i] = term;        System.out.println(i + " " + term);        i++;    }    Iterable<Vector> iterable = new LuceneIterable(reader, "id", "content", termInfo, weight);    i = 0;    for (Vector vector : iterable) {        assertNotNull(vector);        NamedVector namedVector;        if (vector instanceof NamedVector) {                        namedVector = new NamedVector(((NamedVector) vector).getDelegate(), "P(" + i + ')');        } else {            namedVector = new NamedVector(vector, "P(" + i + ')');        }        System.out.println(AbstractCluster.formatVector(namedVector, termDictionary));        sampleData.add(new VectorWritable(namedVector));        i++;    }}
0
private static Path finalClusterPath(Configuration conf, Path output, int maxIterations) throws IOException
{    FileSystem fs = FileSystem.get(conf);    for (int i = maxIterations; i >= 0; i--) {        Path clusters = new Path(output, "clusters-" + i + "-final");        if (fs.exists(clusters)) {            return clusters;        }    }    return null;}
0
public void testKmeans() throws Exception
{    DistanceMeasure measure = new EuclideanDistanceMeasure();    Path input = getTestTempFilePath("input");    Path output = getTestTempDirPath("output");    Path initialPoints = new Path(output, Cluster.CLUSTERS_DIR + '0' + Cluster.FINAL_ITERATION_SUFFIX);    Configuration conf = getConfiguration();    FileSystem fs = FileSystem.get(conf);        ClusteringTestUtils.writePointsToFile(sampleData, input, fs, conf);        RandomSeedGenerator.buildRandom(conf, input, initialPoints, 8, measure, 1L);        Path kMeansOutput = new Path(output, "kmeans");    KMeansDriver.run(conf, getTestTempDirPath("testdata"), initialPoints, kMeansOutput, 0.001, 10, true, 0.0, false);        ClusterDumper clusterDumper = new ClusterDumper(finalClusterPath(conf, output, 10), new Path(kMeansOutput, "clusteredPoints"));    clusterDumper.printClusters(termDictionary);}
0
public void testJsonClusterDumper() throws Exception
{    DistanceMeasure measure = new EuclideanDistanceMeasure();    Path input = getTestTempFilePath("input");    Path output = getTestTempDirPath("output");    Path initialPoints = new Path(output, Cluster.CLUSTERS_DIR + '0' + Cluster.FINAL_ITERATION_SUFFIX);    Configuration conf = getConfiguration();    FileSystem fs = FileSystem.get(conf);        ClusteringTestUtils.writePointsToFile(sampleData, input, fs, conf);        RandomSeedGenerator.buildRandom(conf, input, initialPoints, 8, measure, 1L);        Path kmeansOutput = new Path(output, "kmeans");    KMeansDriver.run(conf, getTestTempDirPath("testdata"), initialPoints, kmeansOutput, 0.001, 10, true, 0.0, false);        ClusterDumper clusterDumper = new ClusterDumper(finalClusterPath(conf, output, 10), new Path(kmeansOutput, "clusteredPoints"));    clusterDumper.setOutputFormat(ClusterDumper.OUTPUT_FORMAT.JSON);    clusterDumper.printClusters(termDictionary);}
0
public void testFuzzyKmeans() throws Exception
{    DistanceMeasure measure = new EuclideanDistanceMeasure();    Path input = getTestTempFilePath("input");    Path output = getTestTempDirPath("output");    Path initialPoints = new Path(output, Cluster.CLUSTERS_DIR + '0' + Cluster.FINAL_ITERATION_SUFFIX);    Configuration conf = getConfiguration();    FileSystem fs = FileSystem.get(conf);        ClusteringTestUtils.writePointsToFile(sampleData, input, fs, conf);        RandomSeedGenerator.buildRandom(conf, input, initialPoints, 8, measure, 1L);        Path kMeansOutput = new Path(output, "kmeans");    FuzzyKMeansDriver.run(conf, getTestTempDirPath("testdata"), initialPoints, kMeansOutput, 0.001, 10, 1.1f, true, true, 0, true);        ClusterDumper clusterDumper = new ClusterDumper(finalClusterPath(conf, output, 10), new Path(kMeansOutput, "clusteredPoints"));    clusterDumper.printClusters(termDictionary);}
0
public void setUp() throws Exception
{    super.setUp();    conf = getConfiguration();    fs = FileSystem.get(conf);    testdata = getTestTempDirPath("testdata");    output = getTestTempDirPath("output");        referenceData = TestKmeansClustering.getPointsWritable(REFERENCE);        generateSamples();}
0
private void generateSamples(int num, double mx, double my, double sd)
{        for (int i = 0; i < num; i++) {        sampleData.add(new VectorWritable(new DenseVector(new double[] { UncommonDistributions.rNorm(mx, sd), UncommonDistributions.rNorm(my, sd) })));    }}
1
private void generateSamples()
{    generateSamples(500, 1, 1, 3);    generateSamples(300, 1, 0, 0.5);    generateSamples(300, 0, 2, 0.1);}
0
private void printRepPoints(int numIterations)
{    RepresentativePointsDriver.printRepresentativePoints(output, numIterations);}
0
private void initData(double dC, double dP, DistanceMeasure measure)
{    clusters = Lists.newArrayList();    clusters.add(new Canopy(new DenseVector(new double[] { -dC, -dC }), 1, measure));    clusters.add(new Canopy(new DenseVector(new double[] { -dC, dC }), 3, measure));    clusters.add(new Canopy(new DenseVector(new double[] { dC, dC }), 5, measure));    clusters.add(new Canopy(new DenseVector(new double[] { dC, -dC }), 7, measure));    representativePoints = Maps.newHashMap();    for (Cluster cluster : clusters) {        List<VectorWritable> points = Lists.newArrayList();        representativePoints.put(cluster.getId(), points);        points.add(new VectorWritable(cluster.getCenter().clone()));        points.add(new VectorWritable(cluster.getCenter().plus(new DenseVector(new double[] { dP, dP }))));        points.add(new VectorWritable(cluster.getCenter().plus(new DenseVector(new double[] { dP, -dP }))));        points.add(new VectorWritable(cluster.getCenter().plus(new DenseVector(new double[] { -dP, -dP }))));        points.add(new VectorWritable(cluster.getCenter().plus(new DenseVector(new double[] { -dP, dP }))));    }}
0
public void testRepresentativePoints() throws Exception
{    ClusteringTestUtils.writePointsToFile(referenceData, new Path(testdata, "file1"), fs, conf);    DistanceMeasure measure = new EuclideanDistanceMeasure();    Configuration conf = getConfiguration();        CanopyDriver.run(conf, testdata, output, measure, 3.1, 1.1, true, 0.0, true);    int numIterations = 2;    Path clustersIn = new Path(output, "clusters-0-final");    RepresentativePointsDriver.run(conf, clustersIn, new Path(output, "clusteredPoints"), output, measure, numIterations, false);    printRepPoints(numIterations);    ClusterEvaluator evaluatorMR = new ClusterEvaluator(conf, clustersIn);        HadoopUtil.delete(conf, output);    CanopyDriver.run(conf, testdata, output, measure, 3.1, 1.1, true, 0.0, true);    RepresentativePointsDriver.run(conf, clustersIn, new Path(output, "clusteredPoints"), output, measure, numIterations, true);    printRepPoints(numIterations);    ClusterEvaluator evaluatorSeq = new ClusterEvaluator(conf, clustersIn);        assertEquals("InterCluster Density", evaluatorMR.interClusterDensity(), evaluatorSeq.interClusterDensity(), EPSILON);    assertEquals("IntraCluster Density", evaluatorMR.intraClusterDensity(), evaluatorSeq.intraClusterDensity(), EPSILON);}
0
public void testCluster0() throws IOException
{    ClusteringTestUtils.writePointsToFile(referenceData, new Path(testdata, "file1"), fs, conf);    DistanceMeasure measure = new EuclideanDistanceMeasure();    initData(1, 0.25, measure);    ClusterEvaluator evaluator = new ClusterEvaluator(representativePoints, clusters, measure);    assertEquals("inter cluster density", 0.33333333333333315, evaluator.interClusterDensity(), EPSILON);    assertEquals("intra cluster density", 0.3656854249492381, evaluator.intraClusterDensity(), EPSILON);}
0
public void testCluster1() throws IOException
{    ClusteringTestUtils.writePointsToFile(referenceData, new Path(testdata, "file1"), fs, conf);    DistanceMeasure measure = new EuclideanDistanceMeasure();    initData(1, 0.5, measure);    ClusterEvaluator evaluator = new ClusterEvaluator(representativePoints, clusters, measure);    assertEquals("inter cluster density", 0.33333333333333315, evaluator.interClusterDensity(), EPSILON);    assertEquals("intra cluster density", 0.3656854249492381, evaluator.intraClusterDensity(), EPSILON);}
0
public void testCluster2() throws IOException
{    ClusteringTestUtils.writePointsToFile(referenceData, new Path(testdata, "file1"), fs, conf);    DistanceMeasure measure = new EuclideanDistanceMeasure();    initData(1, 0.75, measure);    ClusterEvaluator evaluator = new ClusterEvaluator(representativePoints, clusters, measure);    assertEquals("inter cluster density", 0.33333333333333315, evaluator.interClusterDensity(), EPSILON);    assertEquals("intra cluster density", 0.3656854249492381, evaluator.intraClusterDensity(), EPSILON);}
0
public void testEmptyCluster() throws IOException
{    ClusteringTestUtils.writePointsToFile(referenceData, new Path(testdata, "file1"), fs, conf);    DistanceMeasure measure = new EuclideanDistanceMeasure();    initData(1, 0.25, measure);    Canopy cluster = new Canopy(new DenseVector(new double[] { 10, 10 }), 19, measure);    clusters.add(cluster);    List<VectorWritable> points = Lists.newArrayList();    representativePoints.put(cluster.getId(), points);    ClusterEvaluator evaluator = new ClusterEvaluator(representativePoints, clusters, measure);    assertEquals("inter cluster density", 0.371534146934532, evaluator.interClusterDensity(), EPSILON);    assertEquals("intra cluster density", 0.3656854249492381, evaluator.intraClusterDensity(), EPSILON);}
0
public void testSingleValueCluster() throws IOException
{    ClusteringTestUtils.writePointsToFile(referenceData, new Path(testdata, "file1"), fs, conf);    DistanceMeasure measure = new EuclideanDistanceMeasure();    initData(1, 0.25, measure);    Canopy cluster = new Canopy(new DenseVector(new double[] { 0, 0 }), 19, measure);    clusters.add(cluster);    List<VectorWritable> points = Lists.newArrayList();    points.add(new VectorWritable(cluster.getCenter().plus(new DenseVector(new double[] { 1, 1 }))));    representativePoints.put(cluster.getId(), points);    ClusterEvaluator evaluator = new ClusterEvaluator(representativePoints, clusters, measure);    assertEquals("inter cluster density", 0.3656854249492381, evaluator.interClusterDensity(), EPSILON);    assertEquals("intra cluster density", 0.3656854249492381, evaluator.intraClusterDensity(), EPSILON);}
0
public void testAllSameValueCluster() throws IOException
{    ClusteringTestUtils.writePointsToFile(referenceData, new Path(testdata, "file1"), fs, conf);    DistanceMeasure measure = new EuclideanDistanceMeasure();    initData(1, 0.25, measure);    Canopy cluster = new Canopy(new DenseVector(new double[] { 0, 0 }), 19, measure);    clusters.add(cluster);    List<VectorWritable> points = Lists.newArrayList();    points.add(new VectorWritable(cluster.getCenter()));    points.add(new VectorWritable(cluster.getCenter()));    points.add(new VectorWritable(cluster.getCenter()));    representativePoints.put(cluster.getId(), points);    ClusterEvaluator evaluator = new ClusterEvaluator(representativePoints, clusters, measure);    assertEquals("inter cluster density", 0.3656854249492381, evaluator.interClusterDensity(), EPSILON);    assertEquals("intra cluster density", 0.3656854249492381, evaluator.intraClusterDensity(), EPSILON);}
0
public void testCanopy() throws Exception
{    ClusteringTestUtils.writePointsToFile(sampleData, new Path(testdata, "file1"), fs, conf);    DistanceMeasure measure = new EuclideanDistanceMeasure();    Configuration conf = getConfiguration();    CanopyDriver.run(conf, testdata, output, measure, 3.1, 1.1, true, 0.0, true);    int numIterations = 10;    Path clustersIn = new Path(output, "clusters-0-final");    RepresentativePointsDriver.run(conf, clustersIn, new Path(output, "clusteredPoints"), output, measure, numIterations, true);        ClusterEvaluator evaluator = new ClusterEvaluator(conf, clustersIn);        System.out.println("Intra-cluster density = " + evaluator.intraClusterDensity());    System.out.println("Inter-cluster density = " + evaluator.interClusterDensity());}
0
public void testKmeans() throws Exception
{    ClusteringTestUtils.writePointsToFile(sampleData, new Path(testdata, "file1"), fs, conf);    DistanceMeasure measure = new EuclideanDistanceMeasure();        Configuration conf = getConfiguration();    CanopyDriver.run(conf, testdata, output, measure, 3.1, 1.1, false, 0.0, true);        Path kmeansOutput = new Path(output, "kmeans");    KMeansDriver.run(testdata, new Path(output, "clusters-0-final"), kmeansOutput, 0.001, 10, true, 0.0, true);    int numIterations = 10;    Path clustersIn = new Path(kmeansOutput, "clusters-2");    RepresentativePointsDriver.run(conf, clustersIn, new Path(kmeansOutput, "clusteredPoints"), kmeansOutput, measure, numIterations, true);    RepresentativePointsDriver.printRepresentativePoints(kmeansOutput, numIterations);    ClusterEvaluator evaluator = new ClusterEvaluator(conf, clustersIn);        System.out.println("Intra-cluster density = " + evaluator.intraClusterDensity());    System.out.println("Inter-cluster density = " + evaluator.interClusterDensity());}
0
public void testFuzzyKmeans() throws Exception
{    ClusteringTestUtils.writePointsToFile(sampleData, new Path(testdata, "file1"), fs, conf);    DistanceMeasure measure = new EuclideanDistanceMeasure();        Configuration conf = getConfiguration();    CanopyDriver.run(conf, testdata, output, measure, 3.1, 1.1, false, 0.0, true);    Path fuzzyKMeansOutput = new Path(output, "fuzzyk");        FuzzyKMeansDriver.run(testdata, new Path(output, "clusters-0-final"), fuzzyKMeansOutput, 0.001, 10, 2, true, true, 0, true);    int numIterations = 10;    Path clustersIn = new Path(fuzzyKMeansOutput, "clusters-4");    RepresentativePointsDriver.run(conf, clustersIn, new Path(fuzzyKMeansOutput, "clusteredPoints"), fuzzyKMeansOutput, measure, numIterations, true);    RepresentativePointsDriver.printRepresentativePoints(fuzzyKMeansOutput, numIterations);    ClusterEvaluator evaluator = new ClusterEvaluator(conf, clustersIn);        System.out.println("Intra-cluster density = " + evaluator.intraClusterDensity());    System.out.println("Inter-cluster density = " + evaluator.interClusterDensity());}
0
public String getField1()
{    return field1;}
0
public String getField2()
{    return field2;}
0
public Document asLuceneDocument()
{    Document document = super.asLuceneDocument();    document.add(new TextField(FIELD1, this.field1, Field.Store.YES));    document.add(new TextField(FIELD2, this.field2, Field.Store.YES));    return document;}
0
public Document asLuceneDocument()
{    Document document = new Document();    document.add(new StringField(ID_FIELD, getId(), Field.Store.YES));    document.add(new TextField(FIELD, getField(), Field.Store.YES));    document.add(new IntField(NUMERIC_FIELD, numericField, Field.Store.YES));    return document;}
0
public int getNumericField()
{    return numericField;}
0
public String getId()
{    return id;}
0
public String getField()
{    return field;}
0
public Document asLuceneDocument()
{    Document document = new Document();    Field idField = new StringField(ID_FIELD, getId(), Field.Store.YES);    Field field = new TextField(FIELD, getField(), Field.Store.YES);    document.add(idField);    document.add(field);    return document;}
0
public Document asLuceneDocument()
{    Document document = super.asLuceneDocument();    document.add(new StringField(UNSTORED_FIELD, "", Field.Store.NO));    return document;}
0
public void testAnalysis() throws Exception
{    Analyzer analyzer = new MailArchivesClusteringAnalyzer();    String text = "A test message\n" + "atokenthatistoolongtobeusefulforclustertextanalysis\n" + "Mahout is a scalable, machine-learning LIBRARY\n" + "we've added some additional stopwords such as html, mailto, regards\t" + "apache_hadoop provides the foundation for scalability\n" + "www.nabble.com general-help@incubator.apache.org\n" + "public void int protected package";    Reader reader = new StringReader(text);            String[] expectedTokens = { "test", "mahout", "scalabl", "machin", "learn", "librari", "weve", "ad", "stopword", "apache_hadoop", "provid", "foundat", "scalabl" };    TokenStream tokenStream = analyzer.tokenStream("test", reader);    assertNotNull(tokenStream);    tokenStream.reset();    CharTermAttribute termAtt = tokenStream.addAttribute(CharTermAttribute.class);    int e = 0;    while (tokenStream.incrementToken() && e < expectedTokens.length) {        assertEquals(expectedTokens[e++], termAtt.toString());    }    assertEquals(e, expectedTokens.length);    tokenStream.end();    tokenStream.close();}
0
public void setUp() throws Exception
{    super.setUp();    inputDir = getTestTempDir("mail-archives-in");        File subDir = new File(inputDir, "subdir");    subDir.mkdir();    File gzFile = new File(subDir, "mail-messages.gz");    try (GZIPOutputStream gzOut = new GZIPOutputStream(new FileOutputStream(gzFile))) {        gzOut.write(testMailMessages.getBytes("UTF-8"));        gzOut.finish();    }    File subDir2 = new File(subDir, "subsubdir");    subDir2.mkdir();    File gzFile2 = new File(subDir2, "mail-messages-2.gz");    try (GZIPOutputStream gzOut = new GZIPOutputStream(new FileOutputStream(gzFile2))) {        gzOut.write(testMailMessages.getBytes("UTF-8"));        gzOut.finish();    }}
0
public void testSequential() throws Exception
{    File outputDir = this.getTestTempDir("mail-archives-out");    String[] args = { "--input", inputDir.getAbsolutePath(), "--output", outputDir.getAbsolutePath(), "--charset", "UTF-8", "--keyPrefix", "TEST", "--method", "sequential", "--body", "--subject", "--separator", "" };        SequenceFilesFromMailArchives.main(args);        File expectedChunkFile = new File(outputDir, "chunk-0");    String expectedChunkPath = expectedChunkFile.getAbsolutePath();    Assert.assertTrue("Expected chunk file " + expectedChunkPath + " not found!", expectedChunkFile.isFile());    Configuration conf = getConfiguration();    SequenceFileIterator<Text, Text> iterator = new SequenceFileIterator<>(new Path(expectedChunkPath), true, conf);    Assert.assertTrue("First key/value pair not found!", iterator.hasNext());    Pair<Text, Text> record = iterator.next();    File parentFile = new File(new File(new File("TEST"), "subdir"), "mail-messages.gz");    Assert.assertEquals(new File(parentFile, testVars[0][0]).toString(), record.getFirst().toString());    Assert.assertEquals(testVars[0][1] + testVars[0][2], record.getSecond().toString());    Assert.assertTrue("Second key/value pair not found!", iterator.hasNext());    record = iterator.next();    Assert.assertEquals(new File(parentFile, testVars[1][0]).toString(), record.getFirst().toString());    Assert.assertEquals(testVars[1][1] + testVars[1][2], record.getSecond().toString());    record = iterator.next();    File parentFileSubSubDir = new File(new File(new File(new File("TEST"), "subdir"), "subsubdir"), "mail-messages-2.gz");    Assert.assertEquals(new File(parentFileSubSubDir, testVars[0][0]).toString(), record.getFirst().toString());    Assert.assertEquals(testVars[0][1] + testVars[0][2], record.getSecond().toString());    Assert.assertTrue("Second key/value pair not found!", iterator.hasNext());    record = iterator.next();    Assert.assertEquals(new File(parentFileSubSubDir, testVars[1][0]).toString(), record.getFirst().toString());    Assert.assertEquals(testVars[1][1] + testVars[1][2], record.getSecond().toString());    Assert.assertFalse("Only two key/value pairs expected!", iterator.hasNext());}
0
public void testMapReduce() throws Exception
{    Path tmpDir = getTestTempDirPath();    Path mrOutputDir = new Path(tmpDir, "mail-archives-out-mr");    Configuration configuration = getConfiguration();    FileSystem fs = FileSystem.get(configuration);    File expectedInputFile = new File(inputDir.toString());    String[] args = { "-Dhadoop.tmp.dir=" + configuration.get("hadoop.tmp.dir"), "--input", expectedInputFile.getAbsolutePath(), "--output", mrOutputDir.toString(), "--charset", "UTF-8", "--keyPrefix", "TEST", "--method", "mapreduce", "--body", "--subject", "--separator", "" };        SequenceFilesFromMailArchives.main(args);        FileStatus[] fileStatuses = fs.listStatus(mrOutputDir.suffix("/part-m-00000"));        assertEquals(1, fileStatuses.length);    assertEquals("part-m-00000", fileStatuses[0].getPath().getName());    SequenceFileIterator<Text, Text> iterator = new SequenceFileIterator<>(mrOutputDir.suffix("/part-m-00000"), true, configuration);    Assert.assertTrue("First key/value pair not found!", iterator.hasNext());    Pair<Text, Text> record = iterator.next();    File parentFileSubSubDir = new File(new File(new File(new File("TEST"), "subdir"), "subsubdir"), "mail-messages-2.gz");    String expected = record.getFirst().toString();    if (SystemUtils.IS_OS_WINDOWS) {        expected = expected.replace("/", "\\");    }    Assert.assertEquals(new File(parentFileSubSubDir, testVars[0][0]).toString(), expected);    Assert.assertEquals(testVars[0][1] + testVars[0][2], record.getSecond().toString());    Assert.assertTrue("Second key/value pair not found!", iterator.hasNext());    record = iterator.next();    expected = record.getFirst().toString();    if (SystemUtils.IS_OS_WINDOWS) {        expected = expected.replace("/", "\\");    }    Assert.assertEquals(new File(parentFileSubSubDir, testVars[1][0]).toString(), expected);    Assert.assertEquals(testVars[1][1] + testVars[1][2], record.getSecond().toString());        File parentFile = new File(new File(new File("TEST"), "subdir"), "mail-messages.gz");    record = iterator.next();    expected = record.getFirst().toString();    if (SystemUtils.IS_OS_WINDOWS) {        expected = expected.replace("/", "\\");    }    Assert.assertEquals(new File(parentFile, testVars[0][0]).toString(), expected);    Assert.assertEquals(testVars[0][1] + testVars[0][2], record.getSecond().toString());    Assert.assertTrue("Second key/value pair not found!", iterator.hasNext());    record = iterator.next();    expected = record.getFirst().toString();    if (SystemUtils.IS_OS_WINDOWS) {        expected = expected.replace("/", "\\");    }    Assert.assertEquals(new File(parentFile, testVars[1][0]).toString(), expected);    Assert.assertEquals(testVars[1][1] + testVars[1][2], record.getSecond().toString());    Assert.assertFalse("Only four key/value pairs expected!", iterator.hasNext());}
0
public boolean accept(Path path)
{    return path.getName().startsWith("t") || path.getName().startsWith("r") || path.getName().startsWith("f");}
0
public void testSequenceFileFromDirectoryBasic() throws Exception
{        Configuration configuration = getConfiguration();    FileSystem fs = FileSystem.get(configuration);        Path tmpDir = this.getTestTempDirPath();    Path inputDir = new Path(tmpDir, "inputDir");    fs.mkdirs(inputDir);    Path outputDir = new Path(tmpDir, "outputDir");    Path outputDirRecursive = new Path(tmpDir, "outputDirRecursive");    Path inputDirRecursive = new Path(tmpDir, "inputDirRecur");    fs.mkdirs(inputDirRecursive);        createFilesFromArrays(configuration, inputDir, DATA1);    SequenceFilesFromDirectory.main(new String[] { "--input", inputDir.toString(), "--output", outputDir.toString(), "--chunkSize", "64", "--charset", Charsets.UTF_8.name(), "--keyPrefix", "UID", "--method", "sequential" });        checkChunkFiles(configuration, outputDir, DATA1, "UID");    createRecursiveDirFilesFromArrays(configuration, inputDirRecursive, DATA2);    FileStatus fstInputPath = fs.getFileStatus(inputDirRecursive);    String dirs = HadoopUtil.buildDirList(fs, fstInputPath);    System.out.println("\n\n ----- recursive dirs: " + dirs);    SequenceFilesFromDirectory.main(new String[] { "--input", inputDirRecursive.toString(), "--output", outputDirRecursive.toString(), "--chunkSize", "64", "--charset", Charsets.UTF_8.name(), "--keyPrefix", "UID", "--method", "sequential" });    checkRecursiveChunkFiles(configuration, outputDirRecursive, DATA2, "UID");}
0
public void testSequenceFileFromDirectoryMapReduce() throws Exception
{    Configuration conf = getConfiguration();    FileSystem fs = FileSystem.get(conf);        Path tmpDir = this.getTestTempDirPath();    Path inputDir = new Path(tmpDir, "inputDir");    fs.mkdirs(inputDir);    Path inputDirRecur = new Path(tmpDir, "inputDirRecur");    fs.mkdirs(inputDirRecur);    Path mrOutputDir = new Path(tmpDir, "mrOutputDir");    Path mrOutputDirRecur = new Path(tmpDir, "mrOutputDirRecur");    createFilesFromArrays(conf, inputDir, DATA1);    SequenceFilesFromDirectory.main(new String[] { "-Dhadoop.tmp.dir=" + conf.get("hadoop.tmp.dir"), "--input", inputDir.toString(), "--output", mrOutputDir.toString(), "--chunkSize", "64", "--charset", Charsets.UTF_8.name(), "--method", "mapreduce", "--keyPrefix", "UID", "--fileFilterClass", "org.apache.mahout.text.TestPathFilter" });    checkMRResultFiles(conf, mrOutputDir, DATA1, "UID");    createRecursiveDirFilesFromArrays(conf, inputDirRecur, DATA2);    FileStatus fst_input_path = fs.getFileStatus(inputDirRecur);    String dirs = HadoopUtil.buildDirList(fs, fst_input_path);        SequenceFilesFromDirectory.main(new String[] { "-Dhadoop.tmp.dir=" + conf.get("hadoop.tmp.dir"), "--input", inputDirRecur.toString(), "--output", mrOutputDirRecur.toString(), "--chunkSize", "64", "--charset", Charsets.UTF_8.name(), "--method", "mapreduce", "--keyPrefix", "UID", "--fileFilterClass", "org.apache.mahout.text.TestPathFilter" });    checkMRResultFilesRecursive(conf, mrOutputDirRecur, DATA2, "UID");}
1
private static void createFilesFromArrays(Configuration conf, Path inputDir, String[][] data) throws IOException
{    FileSystem fs = FileSystem.get(conf);    for (String[] aData : data) {        try (OutputStreamWriter writer = new OutputStreamWriter(fs.create(new Path(inputDir, aData[0])), Charsets.UTF_8)) {            writer.write(aData[1]);        }    }}
0
private static void createRecursiveDirFilesFromArrays(Configuration configuration, Path inputDir, String[][] data) throws IOException
{    FileSystem fs = FileSystem.get(configuration);        Path curPath;    String currentRecursiveDir = inputDir.toString();    for (String[] aData : data) {        currentRecursiveDir += "/" + aData[0];        File subDir = new File(currentRecursiveDir);        subDir.mkdir();        curPath = new Path(subDir.toString(), "file.txt");                try (OutputStreamWriter writer = new OutputStreamWriter(fs.create(curPath), Charsets.UTF_8)) {            writer.write(aData[1]);        }    }}
1
private static void checkChunkFiles(Configuration configuration, Path outputDir, String[][] data, String prefix) throws IOException
{    FileSystem fs = FileSystem.get(configuration);        FileStatus[] fileStatuses = fs.listStatus(outputDir, PathFilters.logsCRCFilter());        assertEquals(1, fileStatuses.length);    assertEquals("chunk-0", fileStatuses[0].getPath().getName());    Map<String, String> fileToData = new HashMap<>();    for (String[] aData : data) {        fileToData.put(prefix + Path.SEPARATOR + aData[0], aData[1]);    }        try (SequenceFileIterator<Text, Text> iterator = new SequenceFileIterator<>(fileStatuses[0].getPath(), true, configuration)) {        while (iterator.hasNext()) {            Pair<Text, Text> record = iterator.next();            String retrievedData = fileToData.get(record.getFirst().toString().trim());            assertNotNull(retrievedData);            assertEquals(retrievedData, record.getSecond().toString().trim());        }    }}
0
private static void checkRecursiveChunkFiles(Configuration configuration, Path outputDir, String[][] data, String prefix) throws IOException
{    FileSystem fs = FileSystem.get(configuration);    System.out.println(" ----------- check_Recursive_ChunkFiles ------------");        FileStatus[] fileStatuses = fs.listStatus(outputDir, PathFilters.logsCRCFilter());        assertEquals(1, fileStatuses.length);    assertEquals("chunk-0", fileStatuses[0].getPath().getName());    Map<String, String> fileToData = new HashMap<>();    String currentPath = prefix;    for (String[] aData : data) {        currentPath += Path.SEPARATOR + aData[0];        fileToData.put(currentPath + Path.SEPARATOR + "file.txt", aData[1]);    }        try (SequenceFileIterator<Text, Text> iterator = new SequenceFileIterator<>(fileStatuses[0].getPath(), true, configuration)) {        while (iterator.hasNext()) {            Pair<Text, Text> record = iterator.next();            String retrievedData = fileToData.get(record.getFirst().toString().trim());            System.out.printf("%s >> %s\n", record.getFirst().toString().trim(), record.getSecond().toString().trim());            assertNotNull(retrievedData);            assertEquals(retrievedData, record.getSecond().toString().trim());            System.out.printf(">>> k: %s, v: %s\n", record.getFirst().toString(), record.getSecond().toString());        }    }}
0
private static void checkMRResultFiles(Configuration conf, Path outputDir, String[][] data, String prefix) throws IOException
{    FileSystem fs = FileSystem.get(conf);        FileStatus[] fileStatuses = fs.listStatus(outputDir.suffix("/part-m-00000"), PathFilters.logsCRCFilter());        assertEquals(1, fileStatuses.length);    assertEquals("part-m-00000", fileStatuses[0].getPath().getName());    Map<String, String> fileToData = new HashMap<>();    for (String[] aData : data) {        System.out.printf("map.put: %s %s\n", prefix + Path.SEPARATOR + aData[0], aData[1]);        fileToData.put(prefix + Path.SEPARATOR + aData[0], aData[1]);    }        try (SequenceFileIterator<Text, Text> iterator = new SequenceFileIterator<>(fileStatuses[0].getPath(), true, conf)) {        while (iterator.hasNext()) {            Pair<Text, Text> record = iterator.next();            String retrievedData = fileToData.get(record.getFirst().toString().trim());            System.out.printf("MR> %s >> %s\n", record.getFirst().toString().trim(), record.getSecond().toString().trim());            assertNotNull(retrievedData);            assertEquals(retrievedData, record.getSecond().toString().trim());        }    }}
0
private static void checkMRResultFilesRecursive(Configuration configuration, Path outputDir, String[][] data, String prefix) throws IOException
{    FileSystem fs = FileSystem.get(configuration);        FileStatus[] fileStatuses = fs.listStatus(outputDir.suffix("/part-m-00000"), PathFilters.logsCRCFilter());        assertEquals(1, fileStatuses.length);    assertEquals("part-m-00000", fileStatuses[0].getPath().getName());    Map<String, String> fileToData = new HashMap<>();    String currentPath = prefix;    for (String[] aData : data) {        currentPath += Path.SEPARATOR + aData[0];        fileToData.put(currentPath + Path.SEPARATOR + "file.txt", aData[1]);    }        try (SequenceFileIterator<Text, Text> iterator = new SequenceFileIterator<>(fileStatuses[0].getPath(), true, configuration)) {        while (iterator.hasNext()) {            Pair<Text, Text> record = iterator.next();            System.out.printf("MR-Recur > Trying to check: %s\n", record.getFirst().toString().trim());            String retrievedData = fileToData.get(record.getFirst().toString().trim());            assertNotNull(retrievedData);            assertEquals(retrievedData, record.getSecond().toString().trim());        }    }}
0
public void testIncrement() throws Exception
{    Iterator<Integer> ref = Lists.newArrayList(1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 12, 14, 16, 18, 20, 25, 30, 35, 40, 50, 60, 70, 80, 100, 120, 140, 160, 180, 200, 250, 300, 350, 400, 500, 600, 700, 800, 1000, 1200, 1400, 1600, 1800, 2000, 2500, 3000, 3500, 4000, 5000, 6000, 7000).iterator();    Bump125 b = new Bump125();    for (int i = 0; i < 50; i++) {        long x = b.increment();        assertEquals(ref.next().longValue(), x);    }}
0
public void testLabel() throws Exception
{    StringWriter writer = new StringWriter();    MailOptions options = new MailOptions();    options.setSeparator(":::");    options.setCharset(Charsets.UTF_8);    options.setPatternsToMatch(new Pattern[] { MailProcessor.FROM_PREFIX, MailProcessor.SUBJECT_PREFIX, MailProcessor.TO_PREFIX });    options.setInput(new File(System.getProperty("user.dir")));    MailProcessor proc = new MailProcessor(options, "", writer);    URL url = MailProcessorTest.class.getClassLoader().getResource("test.mbox");    File file = new File(url.toURI());    long count = proc.parseMboxLineByLine(file);    assertEquals(7, count);}
0
public void testStripQuoted() throws Exception
{    StringWriter writer = new StringWriter();    MailOptions options = new MailOptions();    options.setSeparator(":::");    options.setCharset(Charsets.UTF_8);    options.setPatternsToMatch(new Pattern[] { MailProcessor.SUBJECT_PREFIX });    options.setInput(new File(System.getProperty("user.dir")));    options.setIncludeBody(true);    MailProcessor proc = new MailProcessor(options, "", writer);    URL url = MailProcessorTest.class.getClassLoader().getResource("test.mbox");    File file = new File(url.toURI());    long count = proc.parseMboxLineByLine(file);    assertEquals(7, count);    assertTrue(writer.getBuffer().toString().contains("> Cocoon Cron Block Configurable Clustering"));    writer = new StringWriter();    proc = new MailProcessor(options, "", writer);    options.setStripQuotedText(true);    count = proc.parseMboxLineByLine(file);    assertEquals(7, count);    assertFalse(writer.getBuffer().toString().contains("> Cocoon Cron Block Configurable Clustering"));}
0
public void testFilter() throws IOException
{    Filter filter = getFilter(filterTokens);    Key k = new Key();    for (String s : filterTokens) {        setKey(k, s);        assertTrue("Key for string " + s + " should be filter member", filter.membershipTest(k));    }    for (String s : notFilterTokens) {        setKey(k, s);        assertFalse("Key for string " + s + " should not be filter member", filter.membershipTest(k));    }}
0
public void testAnalyzer() throws IOException
{    Reader reader = new StringReader(input);    Analyzer analyzer = new WhitespaceAnalyzer();    TokenStream ts = analyzer.tokenStream(null, reader);    ts.reset();    validateTokens(allTokens, ts);    ts.end();    ts.close();}
0
public void testNonKeepdAnalyzer() throws IOException
{    Reader reader = new StringReader(input);    Analyzer analyzer = new WhitespaceAnalyzer();    TokenStream ts = analyzer.tokenStream(null, reader);    ts.reset();    TokenStream f = new BloomTokenFilter(getFilter(filterTokens), false, /* toss matching tokens */    ts);    validateTokens(expectedNonKeepTokens, f);    ts.end();    ts.close();}
0
public void testKeepAnalyzer() throws IOException
{    Reader reader = new StringReader(input);    Analyzer analyzer = new WhitespaceAnalyzer();    TokenStream ts = analyzer.tokenStream(null, reader);    ts.reset();    TokenStream f = new BloomTokenFilter(getFilter(filterTokens), true, /* keep matching tokens */    ts);    validateTokens(expectedKeepTokens, f);    ts.end();    ts.close();}
0
public void testShingleFilteredAnalyzer() throws IOException
{    Reader reader = new StringReader(input);    Analyzer analyzer = new WhitespaceAnalyzer();    TokenStream ts = analyzer.tokenStream(null, reader);    ts.reset();    ShingleFilter sf = new ShingleFilter(ts, 3);    TokenStream f = new BloomTokenFilter(getFilter(shingleKeepTokens), true, sf);    validateTokens(expectedShingleTokens, f);    ts.end();    ts.close();}
0
private static void setKey(Key k, String s) throws IOException
{    ByteBuffer buffer = encoder.encode(CharBuffer.wrap(s.toCharArray()));    k.set(buffer.array(), 1.0);}
0
private static void validateTokens(String[] expected, TokenStream ts) throws IOException
{    int pos = 0;    while (ts.incrementToken()) {        assertTrue("Analyzer produced too many tokens", pos <= expected.length);        CharTermAttribute termAttr = ts.getAttribute(CharTermAttribute.class);        assertEquals("Unexpected term", expected[pos++], termAttr.toString());    }    assertEquals("Analyzer produced too few terms", expected.length, pos);}
0
private static Filter getFilter(String[] tokens) throws IOException
{    Filter filter = new BloomFilter(100, 50, Hash.JENKINS_HASH);    Key k = new Key();    for (String s : tokens) {        setKey(k, s);        filter.add(k);    }    return filter;}
0
public void testRegex() throws Exception
{    RegexMapper mapper = new RegexMapper();    Configuration conf = getConfiguration();    conf.set(RegexMapper.REGEX, "(?<=(\\?|&)q=).*?(?=&|$)");    conf.set(RegexMapper.TRANSFORMER_CLASS, URLDecodeTransformer.class.getName());    DummyRecordWriter<LongWritable, Text> mapWriter = new DummyRecordWriter<>();    Mapper<LongWritable, Text, LongWritable, Text>.Context mapContext = DummyRecordWriter.build(mapper, conf, mapWriter);    mapper.setup(mapContext);    for (int i = 0; i < RegexUtilsTest.TEST_STRS.length; i++) {        String testStr = RegexUtilsTest.TEST_STRS[i];        LongWritable key = new LongWritable(i);        mapper.map(key, new Text(testStr), mapContext);        List<Text> value = mapWriter.getValue(key);        if (!RegexUtilsTest.GOLD[i].isEmpty()) {            assertEquals(1, value.size());            assertEquals(RegexUtilsTest.GOLD[i], value.get(0).toString());        }    }}
0
public void testGroups() throws Exception
{    RegexMapper mapper = new RegexMapper();    Configuration conf = getConfiguration();    conf.set(RegexMapper.REGEX, "(\\d+)\\.(\\d+)\\.(\\d+)");    conf.set(RegexMapper.TRANSFORMER_CLASS, URLDecodeTransformer.class.getName());    conf.setStrings(RegexMapper.GROUP_MATCHERS, "1", "3");    DummyRecordWriter<LongWritable, Text> mapWriter = new DummyRecordWriter<>();    Mapper<LongWritable, Text, LongWritable, Text>.Context mapContext = DummyRecordWriter.build(mapper, conf, mapWriter);    mapper.setup(mapContext);    for (int i = 0; i < RegexUtilsTest.TEST_STRS.length; i++) {        String testStr = RegexUtilsTest.TEST_STRS[i];        LongWritable key = new LongWritable(i);        mapper.map(key, new Text(testStr), mapContext);        List<Text> value = mapWriter.getValue(key);        assertEquals(1, value.size());        assertEquals("127 0", value.get(0).toString());    }}
0
public void testFPGFormatter() throws Exception
{    RegexMapper mapper = new RegexMapper();    Configuration conf = getConfiguration();    conf.set(RegexMapper.REGEX, "(?<=(\\?|&)q=).*?(?=&|$)");    conf.set(RegexMapper.TRANSFORMER_CLASS, URLDecodeTransformer.class.getName());    conf.set(RegexMapper.FORMATTER_CLASS, FPGFormatter.class.getName());    DummyRecordWriter<LongWritable, Text> mapWriter = new DummyRecordWriter<>();    Mapper<LongWritable, Text, LongWritable, Text>.Context mapContext = DummyRecordWriter.build(mapper, conf, mapWriter);    mapper.setup(mapContext);    RegexFormatter formatter = new FPGFormatter();    for (int i = 0; i < RegexUtilsTest.TEST_STRS.length; i++) {        String testStr = RegexUtilsTest.TEST_STRS[i];        LongWritable key = new LongWritable(i);        mapper.map(key, new Text(testStr), mapContext);        List<Text> value = mapWriter.getValue(key);        if (!RegexUtilsTest.GOLD[i].isEmpty()) {            assertEquals(1, value.size());            assertEquals(formatter.format(RegexUtilsTest.GOLD[i]), value.get(0).toString());        }    }}
0
public void testExtract() throws Exception
{    Pattern pattern = Pattern.compile("(?<=(\\?|&)q=).*?(?=&|$)");    String line = "127.0.0.1 -  -  [24/05/2010:01:19:22 +0000] \"GET /solr/select?q=import statement&start=1 HTTP/1.1\" 200 37571";    String res = RegexUtils.extract(line, pattern, Collections.<Integer>emptyList(), " ", RegexUtils.IDENTITY_TRANSFORMER);    assertEquals(res, "import statement", res);    for (int i = 0; i < TEST_STRS.length; i++) {        String testStr = TEST_STRS[i];        res = RegexUtils.extract(testStr, pattern, Collections.<Integer>emptyList(), " ", new URLDecodeTransformer());        assertEquals(GOLD[i], res);    }    pattern = Pattern.compile("((?<=(\\?|&)q=)(.*?)(?=(&|$))|(?<=((\\?|&)start=))(\\d+))");    res = RegexUtils.extract(line, pattern, Collections.<Integer>emptyList(), " ", RegexUtils.IDENTITY_TRANSFORMER);    assertEquals(res, "import statement 1", res);    pattern = Pattern.compile("(start=1) HTTP");    Collection<Integer> groupsToKeep = new ArrayList<>();    groupsToKeep.add(1);    res = RegexUtils.extract(line, pattern, groupsToKeep, " ", RegexUtils.IDENTITY_TRANSFORMER);    assertEquals(res, "start=1", res);}
0
public void setUp() throws Exception
{    Configuration conf = getConfiguration();    fs = FileSystem.get(conf);    super.setUp();    countMap = new OpenObjectIntHashMap<>();    charset = Charsets.UTF_8;    tempSequenceDirectory = getTestTempFilePath("tmpsequence");    tempInputFile = getTestTempFilePath("bayesinputfile");    tempTrainingDirectory = getTestTempDirPath("bayestrain");    tempTestDirectory = getTestTempDirPath("bayestest");    tempMapRedOutputDirectory = new Path(getTestTempDirPath(), "mapRedOutput");    tempInputDirectory = getTestTempDirPath("bayesinputdir");    si = new SplitInput();    si.setTrainingOutputDirectory(tempTrainingDirectory);    si.setTestOutputDirectory(tempTestDirectory);    si.setInputDirectory(tempInputDirectory);}
0
private void writeMultipleInputFiles() throws IOException
{    Writer writer = null;    String currentLabel = null;    try {        for (String[] entry : ClassifierData.DATA) {            if (!entry[0].equals(currentLabel)) {                currentLabel = entry[0];                Closeables.close(writer, false);                writer = new BufferedWriter(new OutputStreamWriter(fs.create(new Path(tempInputDirectory, currentLabel)), Charsets.UTF_8));            }            countMap.adjustOrPutValue(currentLabel, 1, 1);            writer.write(currentLabel + '\t' + entry[1] + '\n');        }    } finally {        Closeables.close(writer, false);    }}
0
private void writeSingleInputFile() throws IOException
{    Writer writer = new BufferedWriter(new OutputStreamWriter(fs.create(tempInputFile), Charsets.UTF_8));    try {        for (String[] entry : ClassifierData.DATA) {            writer.write(entry[0] + '\t' + entry[1] + '\n');        }    } finally {        Closeables.close(writer, true);    }}
0
public void testSplitDirectory() throws Exception
{    writeMultipleInputFiles();    final int testSplitSize = 1;    si.setTestSplitSize(testSplitSize);    si.setCallback(new SplitInput.SplitCallback() {        @Override        public void splitComplete(Path inputFile, int lineCount, int trainCount, int testCount, int testSplitStart) {            int trainingLines = countMap.get(inputFile.getName()) - testSplitSize;            assertSplit(fs, inputFile, charset, testSplitSize, trainingLines, tempTrainingDirectory, tempTestDirectory);        }    });    si.splitDirectory(tempInputDirectory);}
0
public void splitComplete(Path inputFile, int lineCount, int trainCount, int testCount, int testSplitStart)
{    int trainingLines = countMap.get(inputFile.getName()) - testSplitSize;    assertSplit(fs, inputFile, charset, testSplitSize, trainingLines, tempTrainingDirectory, tempTestDirectory);}
0
public void testSplitFile() throws Exception
{    writeSingleInputFile();    si.setTestSplitSize(2);    si.setCallback(new TestCallback(2, 10));    si.splitFile(tempInputFile);}
0
public void testSplitFileLocation() throws Exception
{    writeSingleInputFile();    si.setTestSplitSize(2);    si.setSplitLocation(50);    si.setCallback(new TestCallback(2, 10));    si.splitFile(tempInputFile);}
0
public void testSplitFilePct() throws Exception
{    writeSingleInputFile();    si.setTestSplitPct(25);    si.setCallback(new TestCallback(3, 9));    si.splitFile(tempInputFile);}
0
public void testSplitFilePctLocation() throws Exception
{    writeSingleInputFile();    si.setTestSplitPct(25);    si.setSplitLocation(50);    si.setCallback(new TestCallback(3, 9));    si.splitFile(tempInputFile);}
0
public void testSplitFileRandomSelectionSize() throws Exception
{    writeSingleInputFile();    si.setTestRandomSelectionSize(5);    si.setCallback(new TestCallback(5, 7));    si.splitFile(tempInputFile);}
0
public void testSplitFileRandomSelectionPct() throws Exception
{    writeSingleInputFile();    si.setTestRandomSelectionPct(25);    si.setCallback(new TestCallback(3, 9));    si.splitFile(tempInputFile);}
0
private void writeVectorSequenceFile(Path path, int testPoints) throws IOException
{    Path tempSequenceFile = new Path(path, "part-00000");    Configuration conf = getConfiguration();    IntWritable key = new IntWritable();    VectorWritable value = new VectorWritable();    try (SequenceFile.Writer writer = SequenceFile.createWriter(fs, conf, tempSequenceFile, IntWritable.class, VectorWritable.class)) {        for (int i = 0; i < testPoints; i++) {            key.set(i);            Vector v = new SequentialAccessSparseVector(4);            v.assign(i);            value.set(v);            writer.append(key, value);        }    }}
0
private void writeTextSequenceFile(Path path, int testPoints) throws IOException
{    Path tempSequenceFile = new Path(path, "part-00000");    Configuration conf = getConfiguration();    Text key = new Text();    Text value = new Text();    try (SequenceFile.Writer writer = SequenceFile.createWriter(fs, conf, tempSequenceFile, Text.class, Text.class)) {        for (int i = 0; i < testPoints; i++) {            key.set(Integer.toString(i));            value.set("Line " + i);            writer.append(key, value);        }    }}
0
private void displaySequenceFile(Path sequenceFilePath) throws IOException
{    for (Pair<?, ?> record : new SequenceFileIterable<>(sequenceFilePath, true, getConfiguration())) {        System.out.println(record.getFirst() + "\t" + record.getSecond());    }}
0
private int getNumberRecords(Path sequenceFilePath) throws IOException
{    int numberRecords = 0;    for (Object value : new SequenceFileValueIterable<>(sequenceFilePath, true, getConfiguration())) {        numberRecords++;    }    return numberRecords;}
0
public void testSplitInputMapReduceText() throws Exception
{    writeTextSequenceFile(tempSequenceDirectory, 1000);    testSplitInputMapReduce(1000);}
0
public void testSplitInputMapReduceTextCli() throws Exception
{    writeTextSequenceFile(tempSequenceDirectory, 1000);    testSplitInputMapReduceCli(1000);}
0
public void testSplitInputMapReduceVector() throws Exception
{    writeVectorSequenceFile(tempSequenceDirectory, 1000);    testSplitInputMapReduce(1000);}
0
public void testSplitInputMapReduceVectorCli() throws Exception
{    writeVectorSequenceFile(tempSequenceDirectory, 1000);    testSplitInputMapReduceCli(1000);}
0
private void testSplitInputMapReduceCli(int numPoints) throws Exception
{    int randomSelectionPct = 25;    int keepPct = 10;    String[] args = { "--method", "mapreduce", "--input", tempSequenceDirectory.toString(), "--mapRedOutputDir", tempMapRedOutputDirectory.toString(), "--randomSelectionPct", Integer.toString(randomSelectionPct), "--keepPct", Integer.toString(keepPct), "-ow" };    ToolRunner.run(getConfiguration(), new SplitInput(), args);    validateSplitInputMapReduce(numPoints, randomSelectionPct, keepPct);}
0
private void testSplitInputMapReduce(int numPoints) throws Exception
{    int randomSelectionPct = 25;    si.setTestRandomSelectionPct(randomSelectionPct);    int keepPct = 10;    si.setKeepPct(keepPct);    si.setMapRedOutputDirectory(tempMapRedOutputDirectory);    si.setUseMapRed(true);    si.splitDirectory(getConfiguration(), tempSequenceDirectory);    validateSplitInputMapReduce(numPoints, randomSelectionPct, keepPct);}
0
private void validateSplitInputMapReduce(int numPoints, int randomSelectionPct, int keepPct) throws IOException
{    Path testPath = new Path(tempMapRedOutputDirectory, "test-r-00000");    Path trainingPath = new Path(tempMapRedOutputDirectory, "training-r-00000");    int numberTestRecords = getNumberRecords(testPath);    int numberTrainingRecords = getNumberRecords(trainingPath);    System.out.printf("Test data: %d records\n", numberTestRecords);    displaySequenceFile(testPath);    System.out.printf("Training data: %d records\n", numberTrainingRecords);    displaySequenceFile(trainingPath);    assertEquals((randomSelectionPct / 100.0) * (keepPct / 100.0) * numPoints, numberTestRecords, 2);    assertEquals((1 - randomSelectionPct / 100.0) * (keepPct / 100.0) * numPoints, numberTrainingRecords, 2);}
0
public void testValidate() throws Exception
{    SplitInput st = new SplitInput();    assertValidateException(st);    st.setTestSplitSize(100);    assertValidateException(st);    st.setTestOutputDirectory(tempTestDirectory);    assertValidateException(st);    st.setTrainingOutputDirectory(tempTrainingDirectory);    st.validate();    st.setTestSplitPct(50);    assertValidateException(st);    st = new SplitInput();    st.setTestRandomSelectionPct(50);    st.setTestOutputDirectory(tempTestDirectory);    st.setTrainingOutputDirectory(tempTrainingDirectory);    st.validate();    st.setTestSplitPct(50);    assertValidateException(st);    st = new SplitInput();    st.setTestRandomSelectionPct(50);    st.setTestOutputDirectory(tempTestDirectory);    st.setTrainingOutputDirectory(tempTrainingDirectory);    st.validate();    st.setTestSplitSize(100);    assertValidateException(st);}
0
public void splitComplete(Path inputFile, int lineCount, int trainCount, int testCount, int testSplitStart)
{    assertSplit(fs, tempInputFile, charset, testSplitSize, trainingLines, tempTrainingDirectory, tempTestDirectory);}
0
private static void assertValidateException(SplitInput st) throws IOException
{    try {        st.validate();        fail("Expected IllegalArgumentException");    } catch (IllegalArgumentException iae) {        }}
0
private static void assertSplit(FileSystem fs, Path tempInputFile, Charset charset, int testSplitSize, int trainingLines, Path tempTrainingDirectory, Path tempTestDirectory)
{    try {        Path testFile = new Path(tempTestDirectory, tempInputFile.getName());                assertEquals("test line count", testSplitSize, SplitInput.countLines(fs, testFile, charset));        Path trainingFile = new Path(tempTrainingDirectory, tempInputFile.getName());                assertEquals("training line count", trainingLines, SplitInput.countLines(fs, trainingFile, charset));    } catch (IOException ioe) {        fail(ioe.toString());    }}
0
public void removeQuotes()
{    assertNull(ARFFType.removeQuotes(null));    assertEquals("", ARFFType.removeQuotes("\"\""));    assertEquals("", ARFFType.removeQuotes("''"));    assertEquals("", ARFFType.removeQuotes(""));    assertEquals("", ARFFType.removeQuotes("  "));    assertEquals("single", ARFFType.removeQuotes("'single'"));    assertEquals("double", ARFFType.removeQuotes("\"double\""));    assertEquals("trim", ARFFType.removeQuotes(" trim "));}
0
public void testValues() throws Exception
{    ARFFVectorIterable iterable = readModelFromResource("sample.arff");    assertEquals("Mahout", iterable.getModel().getRelation());    Map<String, Integer> bindings = iterable.getModel().getLabelBindings();    assertNotNull(bindings);    assertEquals(5, bindings.size());    Iterator<Vector> iter = iterable.iterator();    assertTrue(iter.hasNext());    Vector next = iter.next();    assertNotNull(next);    assertTrue("Wrong instanceof", next instanceof DenseVector);    assertEquals(1.0, next.get(0), EPSILON);    assertEquals(2.0, next.get(1), EPSILON);    assertTrue(iter.hasNext());    next = iter.next();    assertNotNull(next);    assertTrue("Wrong instanceof", next instanceof DenseVector);    assertEquals(2.0, next.get(0), EPSILON);    assertEquals(3.0, next.get(1), EPSILON);    assertTrue(iter.hasNext());    next = iter.next();    assertNotNull(next);    assertTrue("Wrong instanceof", next instanceof RandomAccessSparseVector);    assertEquals(5.0, next.get(0), EPSILON);    assertEquals(23.0, next.get(1), EPSILON);    assertFalse(iter.hasNext());}
0
public void testDense() throws Exception
{    Iterable<Vector> iterable = readModelFromResource("sample-dense.arff");    Vector firstVector = iterable.iterator().next();    assertEquals(1.0, firstVector.get(0), 0);    assertEquals(65.0, firstVector.get(1), 0);    assertEquals(1.0, firstVector.get(3), 0);    assertEquals(1.0, firstVector.get(4), 0);    int count = 0;    for (Vector vector : iterable) {        assertTrue("Vector is not dense", vector instanceof DenseVector);        count++;    }    assertEquals(5, count);}
0
public void testSparse() throws Exception
{    Iterable<Vector> iterable = readModelFromResource("sample-sparse.arff");    Vector firstVector = iterable.iterator().next();    assertEquals(23.1, firstVector.get(1), 0);    assertEquals(3.23, firstVector.get(2), 0);    assertEquals(1.2, firstVector.get(3), 0);    int count = 0;    for (Vector vector : iterable) {        assertTrue("Vector is not dense", vector instanceof RandomAccessSparseVector);        count++;    }    assertEquals(9, count);}
0
public void testNonNumeric() throws Exception
{    MapBackedARFFModel model = new MapBackedARFFModel();    ARFFVectorIterable iterable = getVectors("non-numeric-1.arff", model);    int count = 0;    for (Vector vector : iterable) {        assertTrue("Vector is not dense", vector instanceof RandomAccessSparseVector);        count++;    }    iterable = getVectors("non-numeric-1.arff", model);    Iterator<Vector> iter = iterable.iterator();    Vector firstVector = iter.next();    assertEquals(1.0, firstVector.get(2), 0);    assertEquals(10, count);    Map<String, Map<String, Integer>> nominalMap = iterable.getModel().getNominalMap();    assertNotNull(nominalMap);    assertEquals(1, nominalMap.size());    Map<String, Integer> noms = nominalMap.get("bar");    assertNotNull("nominals for bar are null", noms);    assertEquals(5, noms.size());    Map<Integer, ARFFType> integerARFFTypeMap = model.getTypeMap();    assertNotNull("Type map null", integerARFFTypeMap);    assertEquals(5, integerARFFTypeMap.size());    Map<String, Long> words = model.getWords();    assertNotNull("words null", words);    assertEquals(10, words.size());    Map<Integer, DateFormat> integerDateFormatMap = model.getDateMap();    assertNotNull("date format null", integerDateFormatMap);    assertEquals(1, integerDateFormatMap.size());}
0
public void testDate() throws Exception
{    ARFFVectorIterable iterable = readModelFromResource("date.arff");    Iterator<Vector> iter = iterable.iterator();    Vector firstVector = iter.next();    DateFormat format = new SimpleDateFormat("yyyy-MM-dd'T'HH:mm:ss", Locale.ENGLISH);    Date date = format.parse("2001-07-04T12:08:56");    long result = date.getTime();    assertEquals(result, firstVector.get(1), 0);    format = new SimpleDateFormat("yyyy.MM.dd G 'at' HH:mm:ss z", Locale.ENGLISH);    date = format.parse("2001.07.04 AD at 12:08:56 PDT");    result = date.getTime();    assertEquals(result, firstVector.get(2), 0);    format = new SimpleDateFormat("EEE, MMM d, ''yy", Locale.ENGLISH);    date = format.parse("Wed, Jul 4, '01,4 0:08 PM, PDT");    result = date.getTime();    assertEquals(result, firstVector.get(3), 0);    format = new SimpleDateFormat("K:mm a, z", Locale.ENGLISH);    date = format.parse("0:08 PM, PDT");    result = date.getTime();    assertEquals(result, firstVector.get(4), 0);    format = new SimpleDateFormat("yyyyy.MMMMM.dd GGG hh:mm aaa", Locale.ENGLISH);    date = format.parse("02001.July.04 AD 12:08 PM");    result = date.getTime();    assertEquals(result, firstVector.get(5), 0);    format = new SimpleDateFormat("EEE, d MMM yyyy HH:mm:ss Z", Locale.ENGLISH);    date = format.parse("Wed, 4 Jul 2001 12:08:56 -0700");    result = date.getTime();    assertEquals(result, firstVector.get(6), 0);}
0
public void testMultipleNoms() throws Exception
{    MapBackedARFFModel model = new MapBackedARFFModel();    ARFFVectorIterable iterable = getVectors("non-numeric-1.arff", model);    int count = 0;    for (Vector vector : iterable) {        assertTrue("Vector is not dense", vector instanceof RandomAccessSparseVector);        count++;    }    assertEquals(10, count);    Map<String, Map<String, Integer>> nominalMap = iterable.getModel().getNominalMap();    assertNotNull(nominalMap);    assertEquals(1, nominalMap.size());    Map<String, Integer> noms = nominalMap.get("bar");    assertNotNull("nominals for bar are null", noms);    assertEquals(5, noms.size());    Map<Integer, ARFFType> integerARFFTypeMap = model.getTypeMap();    assertNotNull("Type map null", integerARFFTypeMap);    assertEquals(5, integerARFFTypeMap.size());    Map<String, Long> words = model.getWords();    assertNotNull("words null", words);    assertEquals(10, words.size());    Map<Integer, DateFormat> integerDateFormatMap = model.getDateMap();    assertNotNull("date format null", integerDateFormatMap);    assertEquals(1, integerDateFormatMap.size());    iterable = getVectors("non-numeric-2.arff", model);    count = 0;    for (Vector vector : iterable) {        assertTrue("Vector is not dense", vector instanceof RandomAccessSparseVector);        count++;    }    nominalMap = model.getNominalMap();    assertNotNull(nominalMap);    assertEquals(2, nominalMap.size());    noms = nominalMap.get("test");    assertNotNull("nominals for bar are null", noms);    assertEquals(2, noms.size());}
0
public void testNumerics() throws Exception
{    String arff = "@RELATION numerics\n" + "@ATTRIBUTE theNumeric NUMERIC\n" + "@ATTRIBUTE theInteger INTEGER\n" + "@ATTRIBUTE theReal REAL\n" + "@DATA\n" + "1.0,2,3.0";    ARFFModel model = new MapBackedARFFModel();    ARFFVectorIterable iterable = new ARFFVectorIterable(arff, model);    model = iterable.getModel();    assertNotNull(model);    assertEquals(3, model.getLabelSize());    assertEquals(ARFFType.NUMERIC, model.getARFFType(0));    assertEquals(ARFFType.INTEGER, model.getARFFType(1));    assertEquals(ARFFType.REAL, model.getARFFType(2));    Iterator<Vector> it = iterable.iterator();    Vector vector = it.next();    assertEquals(1.0, vector.get(0), EPSILON);    assertEquals(2.0, vector.get(1), EPSILON);    assertEquals(3.0, vector.get(2), EPSILON);}
0
public void testQuotes() throws Exception
{        ARFFModel model = new MapBackedARFFModel();    ARFFVectorIterable iterable = getVectors("quoted-id.arff", model);    model = iterable.getModel();    assertNotNull(model);    assertEquals("quotes", model.getRelation());        assertEquals(4, model.getLabelSize());    assertEquals(ARFFType.NUMERIC, model.getARFFType(0));    assertEquals(ARFFType.INTEGER, model.getARFFType(1));    assertEquals(ARFFType.REAL, model.getARFFType(2));    assertEquals(ARFFType.NOMINAL, model.getARFFType(3));    Map<String, Integer> labelBindings = model.getLabelBindings();    assertTrue(labelBindings.keySet().contains("thenumeric"));    assertTrue(labelBindings.keySet().contains("theinteger"));    assertTrue(labelBindings.keySet().contains("thereal"));    assertTrue(labelBindings.keySet().contains("thenominal"));        Map<String, Integer> nominalMap = model.getNominalMap().get("thenominal");    assertNotNull(nominalMap);    assertEquals(3, nominalMap.size());    assertTrue(nominalMap.keySet().contains("double-quote"));    assertTrue(nominalMap.keySet().contains("single-quote"));    assertTrue(nominalMap.keySet().contains("no-quote"));        Iterator<Vector> it = iterable.iterator();    Vector vector = it.next();    assertEquals(nominalMap.get("no-quote"), vector.get(3), EPSILON);    assertEquals(nominalMap.get("single-quote"), it.next().get(3), EPSILON);    assertEquals(nominalMap.get("double-quote"), it.next().get(3), EPSILON);}
0
 static ARFFVectorIterable getVectors(String resourceName, ARFFModel model) throws IOException
{    String sample = Resources.toString(Resources.getResource(resourceName), Charsets.UTF_8);    return new ARFFVectorIterable(sample, model);}
0
private static ARFFVectorIterable readModelFromResource(String resourceName) throws IOException
{    ARFFModel model = new MapBackedARFFModel();    return getVectors(resourceName, model);}
0
public void dictionary() throws IOException
{    ARFFModel model = new MapBackedARFFModel();    ARFFVectorIterableTest.getVectors("sample-dense.arff", model);    StringWriter writer = new StringWriter();    Driver.writeLabelBindings(writer, model, ",");    String expected1 = Resources.toString(Resources.getResource("expected-arff-dictionary.csv"), Charsets.UTF_8);    String expected2 = Resources.toString(Resources.getResource("expected-arff-dictionary-2.csv"), Charsets.UTF_8);    assertTrue(expected1.equals(writer.toString()) || expected2.equals(writer.toString()));}
0
public void dictionaryJSON() throws IOException
{    ARFFModel model = new MapBackedARFFModel();    ARFFVectorIterableTest.getVectors("sample-dense.arff", model);    StringWriter writer = new StringWriter();    Driver.writeLabelBindingsJSON(writer, model);    String expected1 = Resources.toString(Resources.getResource("expected-arff-schema.json"), Charsets.UTF_8);    String expected2 = Resources.toString(Resources.getResource("expected-arff-schema-2.json"), Charsets.UTF_8);    assertTrue(expected1.equals(writer.toString()) || expected2.equals(writer.toString()));}
0
public void processNominal()
{    String windy = "windy";    String breezy = "breezy";    ARFFModel model = new MapBackedARFFModel();    model.addNominal(windy, breezy, 77);    model.addNominal(windy, "strong", 23);    model.addNominal(windy, "nuking", 55);    Map<String, Map<String, Integer>> nominalMap = model.getNominalMap();    assertEquals(1, nominalMap.size());    Map<String, Integer> windyValues = nominalMap.get(windy);    assertEquals(77, windyValues.get(breezy).intValue());}
0
public void processBadNumeric()
{    ARFFModel model = new MapBackedARFFModel();    model.addLabel("b1shkt70694difsmmmdv0ikmoh", 77);    model.addType(77, ARFFType.REAL);    assertTrue(Double.isNaN(model.getValue("b1shkt70694difsmmmdv0ikmoh", 77)));}
0
public void processGoodNumeric()
{    ARFFModel model = new MapBackedARFFModel();    model.addLabel("1234", 77);    model.addType(77, ARFFType.INTEGER);    assertTrue(1234 == model.getValue("1234", 77));    model.addLabel("131.34", 78);    model.addType(78, ARFFType.REAL);    assertTrue(131.34 == model.getValue("131.34", 78));}
0
public void testCount() throws Exception
{    StringWriter sWriter = new StringWriter();    try (TextualVectorWriter writer = new TextualVectorWriter(sWriter) {        @Override        public void write(Vector vector) throws IOException {            String vecStr = VectorHelper.vectorToCSVString(vector, false);            getWriter().write(vecStr);        }    }) {        Iterable<Vector> iter = new RandomVectorIterable(50);        writer.write(iter);    }    Iterator<Vector> csvIter = new CSVVectorIterator(new StringReader(sWriter.getBuffer().toString()));    int count = 0;    while (csvIter.hasNext()) {        csvIter.next();        count++;    }    assertEquals(50, count);}
0
public void write(Vector vector) throws IOException
{    String vecStr = VectorHelper.vectorToCSVString(vector, false);    getWriter().write(vecStr);}
0
public void testSFVW() throws Exception
{    Path path = getTestTempFilePath("sfvw");    Configuration conf = getConfiguration();    FileSystem fs = FileSystem.get(conf);    SequenceFile.Writer seqWriter = new SequenceFile.Writer(fs, conf, path, LongWritable.class, VectorWritable.class);    try (SequenceFileVectorWriter writer = new SequenceFileVectorWriter(seqWriter)) {        writer.write(new RandomVectorIterable(50));    }    long count = HadoopUtil.countRecords(path, conf);    assertEquals(50, count);}
0
public void testTextOutputSize() throws Exception
{    StringWriter strWriter = new StringWriter();    try (VectorWriter writer = new TextualVectorWriter(strWriter)) {        Collection<Vector> vectors = new ArrayList<>();        vectors.add(new DenseVector(new double[] { 0.3, 1.5, 4.5 }));        vectors.add(new DenseVector(new double[] { 1.3, 1.5, 3.5 }));        writer.write(vectors);    }    String buffer = strWriter.toString();    assertNotNull(buffer);    assertFalse(buffer.isEmpty());}
0
public void before() throws IOException
{    directory = new RAMDirectory();    FieldType fieldType = new FieldType();    fieldType.setStored(false);    fieldType.setIndexOptions(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS);    fieldType.setTokenized(true);    fieldType.setStoreTermVectors(false);    fieldType.setStoreTermVectorPositions(false);    fieldType.setStoreTermVectorOffsets(false);    fieldType.freeze();    directory = createTestIndex(fieldType, directory, 0);}
0
public void test() throws Exception
{    IndexReader reader = DirectoryReader.open(directory);    CachedTermInfo cti = new CachedTermInfo(reader, "content", 0, 100);    assertEquals(3, cti.totalTerms("content"));    assertNotNull(cti.getTermEntry("content", "a"));    assertNull(cti.getTermEntry("content", "e"));        cti = new CachedTermInfo(reader, "content", 3, 100);    assertEquals(2, cti.totalTerms("content"));    assertNotNull(cti.getTermEntry("content", "a"));    assertNull(cti.getTermEntry("content", "c"));        cti = new CachedTermInfo(reader, "content", 0, 85);    assertEquals(2, cti.totalTerms("content"));    assertNotNull(cti.getTermEntry("content", "b"));    assertNotNull(cti.getTermEntry("content", "c"));    assertNull(cti.getTermEntry("content", "a"));}
0
 static RAMDirectory createTestIndex(FieldType fieldType, RAMDirectory directory, int startingId) throws IOException
{    IndexWriter writer = new IndexWriter(directory, new IndexWriterConfig(new WhitespaceAnalyzer()));    try {        for (int i = 0; i < DOCS.length; i++) {            Document doc = new Document();            Field id = new StringField("id", "doc_" + (i + startingId), Field.Store.YES);            doc.add(id);            Field text = new Field("content", DOCS[i], fieldType);            doc.add(text);            Field text2 = new Field("content2", DOCS2[i], fieldType);            doc.add(text2);            writer.addDocument(doc);        }    } finally {        Closeables.close(writer, false);    }    return directory;}
0
public void setUp() throws Exception
{    super.setUp();    indexDir = getTestTempDir("intermediate");    indexDir.delete();    outputDir = getTestTempDir("output");    outputDir.delete();    conf = getConfiguration();}
0
private Document asDocument(String line)
{    Document doc = new Document();    doc.add(new TextFieldWithTermVectors("text", line));    return doc;}
0
public void sequenceFileDictionary() throws IOException
{    Directory index = new SimpleFSDirectory(Paths.get(indexDir.getAbsolutePath()));    Analyzer analyzer = new StandardAnalyzer();    IndexWriterConfig config = new IndexWriterConfig(analyzer);    config.setCommitOnClose(true);    final IndexWriter writer = new IndexWriter(index, config);    try {        writer.addDocument(asDocument("One Ring to rule them all"));        writer.addDocument(asDocument("One Ring to find them,"));        writer.addDocument(asDocument("One Ring to bring them all"));        writer.addDocument(asDocument("and in the darkness bind them"));    } finally {        writer.close();    }    File seqDict = new File(outputDir, "dict.seq");    Driver.main(new String[] { "--dir", indexDir.getAbsolutePath(), "--output", new File(outputDir, "out").getAbsolutePath(), "--field", "text", "--dictOut", new File(outputDir, "dict.txt").getAbsolutePath(), "--seqDictOut", seqDict.getAbsolutePath() });    SequenceFile.Reader reader = null;    Set<String> indexTerms = Sets.newHashSet();    try {        reader = new SequenceFile.Reader(FileSystem.getLocal(conf), new Path(seqDict.getAbsolutePath()), conf);        Text term = new Text();        IntWritable termIndex = new IntWritable();        while (reader.next(term, termIndex)) {            indexTerms.add(term.toString());        }    } finally {        Closeables.close(reader, true);    }    Set<String> expectedIndexTerms = Sets.newHashSet("all", "bind", "bring", "darkness", "find", "one", "ring", "rule");        assertEquals(expectedIndexTerms.size(), Sets.union(expectedIndexTerms, indexTerms).size());}
0
public void before() throws IOException
{    TYPE_NO_TERM_VECTORS.setIndexOptions(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS);    TYPE_NO_TERM_VECTORS.setTokenized(true);    TYPE_NO_TERM_VECTORS.setStoreTermVectors(false);    TYPE_NO_TERM_VECTORS.setStoreTermVectorPositions(false);    TYPE_NO_TERM_VECTORS.setStoreTermVectorOffsets(false);    TYPE_NO_TERM_VECTORS.freeze();    TYPE_TERM_VECTORS.setIndexOptions(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS);    TYPE_TERM_VECTORS.setTokenized(true);    TYPE_TERM_VECTORS.setStored(true);    TYPE_TERM_VECTORS.setStoreTermVectors(true);    TYPE_TERM_VECTORS.setStoreTermVectorPositions(true);    TYPE_TERM_VECTORS.setStoreTermVectorOffsets(true);    TYPE_TERM_VECTORS.freeze();    directory = createTestIndex(TYPE_TERM_VECTORS);}
0
public void testIterable() throws Exception
{    IndexReader reader = DirectoryReader.open(directory);    Weight weight = new TFIDF();    TermInfo termInfo = new CachedTermInfo(reader, "content", 1, 100);    LuceneIterable iterable = new LuceneIterable(reader, "id", "content", termInfo, weight);        for (Vector vector : iterable) {        assertNotNull(vector);        assertTrue("vector is not an instanceof " + NamedVector.class, vector instanceof NamedVector);        assertTrue("vector Size: " + vector.size() + " is not greater than: " + 0, vector.size() > 0);        assertTrue(((NamedVector) vector).getName().startsWith("doc_"));    }    iterable = new LuceneIterable(reader, "id", "content", termInfo, weight, 3);        for (Vector vector : iterable) {        assertNotNull(vector);        assertTrue("vector is not an instanceof " + NamedVector.class, vector instanceof NamedVector);        assertTrue("vector Size: " + vector.size() + " is not greater than: " + 0, vector.size() > 0);        assertTrue(((NamedVector) vector).getName().startsWith("doc_"));    }}
0
public void testIterableNoTermVectors() throws IOException
{    RAMDirectory directory = createTestIndex(TYPE_NO_TERM_VECTORS);    IndexReader reader = DirectoryReader.open(directory);    Weight weight = new TFIDF();    TermInfo termInfo = new CachedTermInfo(reader, "content", 1, 100);    LuceneIterable iterable = new LuceneIterable(reader, "id", "content", termInfo, weight);    Iterator<Vector> iterator = iterable.iterator();    Iterators.advance(iterator, 1);}
0
public void testIterableSomeNoiseTermVectors() throws IOException
{        RAMDirectory directory = createTestIndex(TYPE_TERM_VECTORS, new RAMDirectory(), 0);        createTestIndex(TYPE_NO_TERM_VECTORS, directory, 5);    IndexReader reader = DirectoryReader.open(directory);    Weight weight = new TFIDF();    TermInfo termInfo = new CachedTermInfo(reader, "content", 1, 100);    boolean exceptionThrown;        LuceneIterable iterable = new LuceneIterable(reader, "id", "content", termInfo, weight);    try {        Iterables.skip(iterable, Iterables.size(iterable));        exceptionThrown = false;    } catch (IllegalStateException ise) {        exceptionThrown = true;    }    assertTrue(exceptionThrown);        iterable = new LuceneIterable(reader, "id", "content", termInfo, weight, -1, 1.0);    try {        Iterables.skip(iterable, Iterables.size(iterable));        exceptionThrown = false;    } catch (IllegalStateException ise) {        exceptionThrown = true;    }    assertFalse(exceptionThrown);        iterable = new LuceneIterable(reader, "id", "content", termInfo, weight, -1, 0.5);    Iterator<Vector> iterator = iterable.iterator();    Iterators.advance(iterator, 5);    try {        Iterators.advance(iterator, Iterators.size(iterator));        exceptionThrown = false;    } catch (IllegalStateException ise) {        exceptionThrown = true;    }    assertTrue(exceptionThrown);}
0
 static RAMDirectory createTestIndex(FieldType fieldType) throws IOException
{    return createTestIndex(fieldType, new RAMDirectory(), 0);}
0
 static RAMDirectory createTestIndex(FieldType fieldType, RAMDirectory directory, int startingId) throws IOException
{    try (IndexWriter writer = new IndexWriter(directory, new IndexWriterConfig(new StandardAnalyzer()))) {        for (int i = 0; i < DOCS.length; i++) {            Document doc = new Document();            Field id = new StringField("id", "doc_" + (i + startingId), Field.Store.YES);            doc.add(id);                        Field text = new Field("content", DOCS[i], fieldType);            doc.add(text);            Field text2 = new Field("content2", DOCS[i], fieldType);            doc.add(text2);            writer.addDocument(doc);        }    }    return directory;}
0
public Iterator<Vector> iterator()
{    return Iterators.transform(new CountingIterator(numItems), new Function<Integer, Vector>() {        private final Random random = RandomUtils.getRandom();        @Override        public Vector apply(Integer dummy) {            Vector result = type == VectorType.SPARSE ? new RandomAccessSparseVector(numItems) : new DenseVector(numItems);            result.assign(new DoubleFunction() {                @Override                public double apply(double ignored) {                    return random.nextDouble();                }            });            return result;        }    });}
0
public Vector apply(Integer dummy)
{    Vector result = type == VectorType.SPARSE ? new RandomAccessSparseVector(numItems) : new DenseVector(numItems);    result.assign(new DoubleFunction() {        @Override        public double apply(double ignored) {            return random.nextDouble();        }    });    return result;}
0
public double apply(double ignored)
{    return random.nextDouble();}
0
public void setUp() throws Exception
{    super.setUp();    conf = getConfiguration();    inputPathOne = getTestTempFilePath("documents/docs-one.file");    FileSystem fs = FileSystem.get(inputPathOne.toUri(), conf);    try (SequenceFile.Writer writer = new SequenceFile.Writer(fs, conf, inputPathOne, Text.class, IntWritable.class)) {        Random rd = RandomUtils.getRandom();        for (int i = 0; i < NUM_DOCS; i++) {                        writer.append(new Text("Document::ID::" + i), new IntWritable(NUM_DOCS + rd.nextInt(NUM_DOCS)));        }    }    inputPathTwo = getTestTempFilePath("documents/docs-two.file");    fs = FileSystem.get(inputPathTwo.toUri(), conf);    try (SequenceFile.Writer writer = new SequenceFile.Writer(fs, conf, inputPathTwo, Text.class, IntWritable.class)) {        Random rd = RandomUtils.getRandom();        for (int i = 0; i < NUM_DOCS; i++) {                        writer.append(new Text("Document::ID::" + i), new IntWritable(rd.nextInt(NUM_DOCS)));        }    }}
0
public void testJsonFormatting() throws Exception
{    Vector v = new SequentialAccessSparseVector(10);    v.set(2, 3.1);    v.set(4, 1.0);    v.set(6, 8.1);    v.set(7, -100);    v.set(9, 12.2);    String UNUSED = "UNUSED";    String[] dictionary = { UNUSED, UNUSED, "two", UNUSED, "four", UNUSED, "six", "seven", UNUSED, "nine" };    assertEquals("sorted json form incorrect: ", "{nine:12.2,six:8.1,two:3.1}", VectorHelper.vectorToJson(v, dictionary, 3, true));    assertEquals("unsorted form incorrect: ", "{two:3.1,four:1.0}", VectorHelper.vectorToJson(v, dictionary, 2, false));    assertEquals("sorted json form incorrect: ", "{nine:12.2,six:8.1,two:3.1,four:1.0}", VectorHelper.vectorToJson(v, dictionary, 4, true));    assertEquals("sorted json form incorrect: ", "{nine:12.2,six:8.1,two:3.1,four:1.0,seven:-100.0}", VectorHelper.vectorToJson(v, dictionary, 5, true));    assertEquals("sorted json form incorrect: ", "{nine:12.2,six:8.1}", VectorHelper.vectorToJson(v, dictionary, 2, true));    assertEquals("unsorted form incorrect: ", "{two:3.1,four:1.0}", VectorHelper.vectorToJson(v, dictionary, 2, false));}
0
public void testTopEntries() throws Exception
{    Vector v = new SequentialAccessSparseVector(10);    v.set(2, 3.1);    v.set(4, 1.0);    v.set(6, 8.1);    v.set(7, -100);    v.set(9, 12.2);    v.set(1, 0.0);    v.set(3, 0.0);    v.set(8, 2.7);        assertEquals(6, VectorHelper.topEntries(v, 6).size());        assertTrue(VectorHelper.topEntries(v, 9).size() < 9);        assertTrue(VectorHelper.topEntries(v, 5).size() < v.getNumNonZeroElements());}
0
public void testTopEntriesWhenAllZeros() throws Exception
{    Vector v = new SequentialAccessSparseVector(10);    v.set(2, 0.0);    v.set(4, 0.0);    v.set(6, 0.0);    v.set(7, 0);    v.set(9, 0.0);    v.set(1, 0.0);    v.set(3, 0.0);    v.set(8, 0.0);    assertEquals(0, VectorHelper.topEntries(v, 6).size());}
0
public void testLoadTermDictionary() throws Exception
{        VectorHelper.loadTermDictionary(conf, inputPathOne.toString());        VectorHelper.loadTermDictionary(conf, inputPathTwo.toString());}
0
 static Vector readFirstRow(Path dir, Configuration conf) throws IOException
{    Iterator<VectorWritable> iterator = new SequenceFileDirValueIterator<>(dir, PathType.LIST, PathFilters.partFilter(), null, true, conf);    return iterator.hasNext() ? iterator.next().get() : null;}
0
public static OpenIntObjectHashMap<Vector> readMatrixByRowsFromDistributedCache(int numEntities, Configuration conf) throws IOException
{    IntWritable rowIndex = new IntWritable();    VectorWritable row = new VectorWritable();    OpenIntObjectHashMap<Vector> featureMatrix = numEntities > 0 ? new OpenIntObjectHashMap<Vector>(numEntities) : new OpenIntObjectHashMap<Vector>();    Path[] cachedFiles = HadoopUtil.getCachedFiles(conf);    LocalFileSystem localFs = FileSystem.getLocal(conf);    for (Path cachedFile : cachedFiles) {        try (SequenceFile.Reader reader = new SequenceFile.Reader(localFs.getConf(), SequenceFile.Reader.file(cachedFile))) {            while (reader.next(rowIndex, row)) {                featureMatrix.put(rowIndex.get(), row.get());            }        }    }    Preconditions.checkState(!featureMatrix.isEmpty(), "Feature matrix is empty");    return featureMatrix;}
0
public static OpenIntObjectHashMap<Vector> readMatrixByRows(Path dir, Configuration conf)
{    OpenIntObjectHashMap<Vector> matrix = new OpenIntObjectHashMap<>();    for (Pair<IntWritable, VectorWritable> pair : new SequenceFileDirIterable<IntWritable, VectorWritable>(dir, PathType.LIST, PathFilters.partFilter(), conf)) {        int rowIndex = pair.getFirst().get();        Vector row = pair.getSecond().get();        matrix.put(rowIndex, row);    }    return matrix;}
0
public static Vector solveExplicit(VectorWritable ratingsWritable, OpenIntObjectHashMap<Vector> uOrM, double lambda, int numFeatures)
{    Vector ratings = ratingsWritable.get();    List<Vector> featureVectors = new ArrayList<>(ratings.getNumNondefaultElements());    for (Vector.Element e : ratings.nonZeroes()) {        int index = e.index();        featureVectors.add(uOrM.get(index));    }    return AlternatingLeastSquaresSolver.solve(featureVectors, ratings, lambda, numFeatures);}
0
public static void main(String[] args) throws Exception
{    ToolRunner.run(new DatasetSplitter(), args);}
0
public int run(String[] args) throws Exception
{    addInputOption();    addOutputOption();    addOption("trainingPercentage", "t", "percentage of the data to use as training set (default: " + DEFAULT_TRAINING_PERCENTAGE + ')', String.valueOf(DEFAULT_TRAINING_PERCENTAGE));    addOption("probePercentage", "p", "percentage of the data to use as probe set (default: " + DEFAULT_PROBE_PERCENTAGE + ')', String.valueOf(DEFAULT_PROBE_PERCENTAGE));    Map<String, List<String>> parsedArgs = parseArguments(args);    if (parsedArgs == null) {        return -1;    }    double trainingPercentage = Double.parseDouble(getOption("trainingPercentage"));    double probePercentage = Double.parseDouble(getOption("probePercentage"));    String tempDir = getOption("tempDir");    Path markedPrefs = new Path(tempDir, "markedPreferences");    Path trainingSetPath = new Path(getOutputPath(), "trainingSet");    Path probeSetPath = new Path(getOutputPath(), "probeSet");    Job markPreferences = prepareJob(getInputPath(), markedPrefs, TextInputFormat.class, MarkPreferencesMapper.class, Text.class, Text.class, SequenceFileOutputFormat.class);    markPreferences.getConfiguration().set(TRAINING_PERCENTAGE, String.valueOf(trainingPercentage));    markPreferences.getConfiguration().set(PROBE_PERCENTAGE, String.valueOf(probePercentage));    boolean succeeded = markPreferences.waitForCompletion(true);    if (!succeeded) {        return -1;    }    Job createTrainingSet = prepareJob(markedPrefs, trainingSetPath, SequenceFileInputFormat.class, WritePrefsMapper.class, NullWritable.class, Text.class, TextOutputFormat.class);    createTrainingSet.getConfiguration().set(PART_TO_USE, INTO_TRAINING_SET.toString());    succeeded = createTrainingSet.waitForCompletion(true);    if (!succeeded) {        return -1;    }    Job createProbeSet = prepareJob(markedPrefs, probeSetPath, SequenceFileInputFormat.class, WritePrefsMapper.class, NullWritable.class, Text.class, TextOutputFormat.class);    createProbeSet.getConfiguration().set(PART_TO_USE, INTO_PROBE_SET.toString());    succeeded = createProbeSet.waitForCompletion(true);    if (!succeeded) {        return -1;    }    return 0;}
0
protected void setup(Context ctx) throws IOException, InterruptedException
{    random = RandomUtils.getRandom();    trainingBound = Double.parseDouble(ctx.getConfiguration().get(TRAINING_PERCENTAGE));    probeBound = trainingBound + Double.parseDouble(ctx.getConfiguration().get(PROBE_PERCENTAGE));}
0
protected void map(LongWritable key, Text text, Context ctx) throws IOException, InterruptedException
{    double randomValue = random.nextDouble();    if (randomValue <= trainingBound) {        ctx.write(INTO_TRAINING_SET, text);    } else if (randomValue <= probeBound) {        ctx.write(INTO_PROBE_SET, text);    }}
0
protected void setup(Context ctx) throws IOException, InterruptedException
{    partToUse = ctx.getConfiguration().get(PART_TO_USE);}
0
protected void map(Text key, Text text, Context ctx) throws IOException, InterruptedException
{    if (partToUse.equals(key.toString())) {        ctx.write(NullWritable.get(), text);    }}
0
public static void main(String[] args) throws Exception
{    ToolRunner.run(new FactorizationEvaluator(), args);}
0
public int run(String[] args) throws Exception
{    addInputOption();    addOption("userFeatures", null, "path to the user feature matrix", true);    addOption("itemFeatures", null, "path to the item feature matrix", true);    addOption("usesLongIDs", null, "input contains long IDs that need to be translated");    addOutputOption();    Map<String, List<String>> parsedArgs = parseArguments(args);    if (parsedArgs == null) {        return -1;    }    Path errors = getTempPath("errors");    Job predictRatings = prepareJob(getInputPath(), errors, TextInputFormat.class, PredictRatingsMapper.class, DoubleWritable.class, NullWritable.class, SequenceFileOutputFormat.class);    Configuration conf = predictRatings.getConfiguration();    conf.set(USER_FEATURES_PATH, getOption("userFeatures"));    conf.set(ITEM_FEATURES_PATH, getOption("itemFeatures"));    boolean usesLongIDs = Boolean.parseBoolean(getOption("usesLongIDs"));    if (usesLongIDs) {        conf.set(ParallelALSFactorizationJob.USES_LONG_IDS, String.valueOf(true));    }    boolean succeeded = predictRatings.waitForCompletion(true);    if (!succeeded) {        return -1;    }    FileSystem fs = FileSystem.get(getOutputPath().toUri(), getConf());    FSDataOutputStream outputStream = fs.create(getOutputPath("rmse.txt"));    try (BufferedWriter writer = new BufferedWriter(new OutputStreamWriter(outputStream, Charsets.UTF_8))) {        double rmse = computeRmse(errors);        writer.write(String.valueOf(rmse));    }    return 0;}
0
private double computeRmse(Path errors)
{    RunningAverage average = new FullRunningAverage();    for (Pair<DoubleWritable, NullWritable> entry : new SequenceFileDirIterable<DoubleWritable, NullWritable>(errors, PathType.LIST, PathFilters.logsCRCFilter(), getConf())) {        DoubleWritable error = entry.getFirst();        average.addDatum(error.get() * error.get());    }    return Math.sqrt(average.getAverage());}
0
protected void setup(Context ctx) throws IOException, InterruptedException
{    Configuration conf = ctx.getConfiguration();    Path pathToU = new Path(conf.get(USER_FEATURES_PATH));    Path pathToM = new Path(conf.get(ITEM_FEATURES_PATH));    U = ALS.readMatrixByRows(pathToU, conf);    M = ALS.readMatrixByRows(pathToM, conf);    usesLongIDs = conf.getBoolean(ParallelALSFactorizationJob.USES_LONG_IDS, false);}
0
protected void map(LongWritable key, Text value, Context ctx) throws IOException, InterruptedException
{    String[] tokens = TasteHadoopUtils.splitPrefTokens(value.toString());    int userID = TasteHadoopUtils.readID(tokens[TasteHadoopUtils.USER_ID_POS], usesLongIDs);    int itemID = TasteHadoopUtils.readID(tokens[TasteHadoopUtils.ITEM_ID_POS], usesLongIDs);    double rating = Double.parseDouble(tokens[2]);    if (U.containsKey(userID) && M.containsKey(itemID)) {        double estimate = U.get(userID).dot(M.get(itemID));        error.set(rating - estimate);        ctx.write(error, NullWritable.get());    }}
0
public void run(Context ctx) throws IOException, InterruptedException
{    Class<Mapper<K1, V1, K2, V2>> mapperClass = MultithreadedSharingMapper.getMapperClass((JobContext) ctx);    Preconditions.checkNotNull(mapperClass, "Could not find Multithreaded Mapper class.");    Configuration conf = ctx.getConfiguration();        Mapper<K1, V1, K2, V2> mapper1 = ReflectionUtils.newInstance(mapperClass, conf);    SharingMapper<K1, V1, K2, V2, ?> mapper = null;    if (mapper1 instanceof SharingMapper) {        mapper = (SharingMapper<K1, V1, K2, V2, ?>) mapper1;    }    Preconditions.checkNotNull(mapper, "Could not instantiate SharingMapper. Class was: %s", mapper1.getClass().getName());        mapper.setupSharedInstance(ctx);        super.run(ctx);}
0
public static void main(String[] args) throws Exception
{    ToolRunner.run(new ParallelALSFactorizationJob(), args);}
0
public int run(String[] args) throws Exception
{    addInputOption();    addOutputOption();    addOption("lambda", null, "regularization parameter", true);    addOption("implicitFeedback", null, "data consists of implicit feedback?", String.valueOf(false));    addOption("alpha", null, "confidence parameter (only used on implicit feedback)", String.valueOf(40));    addOption("numFeatures", null, "dimension of the feature space", true);    addOption("numIterations", null, "number of iterations", true);    addOption("numThreadsPerSolver", null, "threads per solver mapper", String.valueOf(1));    addOption("usesLongIDs", null, "input contains long IDs that need to be translated");    Map<String, List<String>> parsedArgs = parseArguments(args);    if (parsedArgs == null) {        return -1;    }    numFeatures = Integer.parseInt(getOption("numFeatures"));    numIterations = Integer.parseInt(getOption("numIterations"));    lambda = Double.parseDouble(getOption("lambda"));    alpha = Double.parseDouble(getOption("alpha"));    implicitFeedback = Boolean.parseBoolean(getOption("implicitFeedback"));    numThreadsPerSolver = Integer.parseInt(getOption("numThreadsPerSolver"));    boolean usesLongIDs = Boolean.parseBoolean(getOption("usesLongIDs", String.valueOf(false)));    if (usesLongIDs) {        Job mapUsers = prepareJob(getInputPath(), getOutputPath("userIDIndex"), TextInputFormat.class, MapLongIDsMapper.class, VarIntWritable.class, VarLongWritable.class, IDMapReducer.class, VarIntWritable.class, VarLongWritable.class, SequenceFileOutputFormat.class);        mapUsers.getConfiguration().set(TOKEN_POS, String.valueOf(TasteHadoopUtils.USER_ID_POS));        mapUsers.waitForCompletion(true);        Job mapItems = prepareJob(getInputPath(), getOutputPath("itemIDIndex"), TextInputFormat.class, MapLongIDsMapper.class, VarIntWritable.class, VarLongWritable.class, IDMapReducer.class, VarIntWritable.class, VarLongWritable.class, SequenceFileOutputFormat.class);        mapItems.getConfiguration().set(TOKEN_POS, String.valueOf(TasteHadoopUtils.ITEM_ID_POS));        mapItems.waitForCompletion(true);    }    /* create A' */    Job itemRatings = prepareJob(getInputPath(), pathToItemRatings(), TextInputFormat.class, ItemRatingVectorsMapper.class, IntWritable.class, VectorWritable.class, VectorSumReducer.class, IntWritable.class, VectorWritable.class, SequenceFileOutputFormat.class);    itemRatings.setCombinerClass(VectorSumCombiner.class);    itemRatings.getConfiguration().set(USES_LONG_IDS, String.valueOf(usesLongIDs));    boolean succeeded = itemRatings.waitForCompletion(true);    if (!succeeded) {        return -1;    }    /* create A */    Job userRatings = prepareJob(pathToItemRatings(), pathToUserRatings(), TransposeMapper.class, IntWritable.class, VectorWritable.class, MergeUserVectorsReducer.class, IntWritable.class, VectorWritable.class);    userRatings.setCombinerClass(MergeVectorsCombiner.class);    succeeded = userRatings.waitForCompletion(true);    if (!succeeded) {        return -1;    }        Job averageItemRatings = prepareJob(pathToItemRatings(), getTempPath("averageRatings"), AverageRatingMapper.class, IntWritable.class, VectorWritable.class, MergeVectorsReducer.class, IntWritable.class, VectorWritable.class);    averageItemRatings.setCombinerClass(MergeVectorsCombiner.class);    succeeded = averageItemRatings.waitForCompletion(true);    if (!succeeded) {        return -1;    }    Vector averageRatings = ALS.readFirstRow(getTempPath("averageRatings"), getConf());    int numItems = averageRatings.getNumNondefaultElements();    int numUsers = (int) userRatings.getCounters().findCounter(Stats.NUM_USERS).getValue();        /* create an initial M */    initializeM(averageRatings);    for (int currentIteration = 0; currentIteration < numIterations; currentIteration++) {        /* broadcast M, read A row-wise, recompute U row-wise */                runSolver(pathToUserRatings(), pathToU(currentIteration), pathToM(currentIteration - 1), currentIteration, "U", numItems);        /* broadcast U, read A' row-wise, recompute M row-wise */                runSolver(pathToItemRatings(), pathToM(currentIteration), pathToU(currentIteration), currentIteration, "M", numUsers);    }    return 0;}
1
private void initializeM(Vector averageRatings) throws IOException
{    Random random = RandomUtils.getRandom();    FileSystem fs = FileSystem.get(pathToM(-1).toUri(), getConf());    try (SequenceFile.Writer writer = new SequenceFile.Writer(fs, getConf(), new Path(pathToM(-1), "part-m-00000"), IntWritable.class, VectorWritable.class)) {        IntWritable index = new IntWritable();        VectorWritable featureVector = new VectorWritable();        for (Vector.Element e : averageRatings.nonZeroes()) {            Vector row = new DenseVector(numFeatures);            row.setQuick(0, e.get());            for (int m = 1; m < numFeatures; m++) {                row.setQuick(m, random.nextDouble());            }            index.set(e.index());            featureVector.set(row);            writer.append(index, featureVector);        }    }}
0
protected void reduce(WritableComparable<?> key, Iterable<VectorWritable> values, Context ctx) throws IOException, InterruptedException
{    Vector sum = Vectors.sum(values.iterator());    result.set(new SequentialAccessSparseVector(sum));    ctx.write(key, result);}
0
public void reduce(WritableComparable<?> key, Iterable<VectorWritable> vectors, Context ctx) throws IOException, InterruptedException
{    Vector merged = VectorWritable.merge(vectors.iterator()).get();    result.set(new SequentialAccessSparseVector(merged));    ctx.write(key, result);    ctx.getCounter(Stats.NUM_USERS).increment(1);}
0
protected void setup(Context ctx) throws IOException, InterruptedException
{    usesLongIDs = ctx.getConfiguration().getBoolean(USES_LONG_IDS, false);}
0
protected void map(LongWritable offset, Text line, Context ctx) throws IOException, InterruptedException
{    String[] tokens = TasteHadoopUtils.splitPrefTokens(line.toString());    int userID = TasteHadoopUtils.readID(tokens[TasteHadoopUtils.USER_ID_POS], usesLongIDs);    int itemID = TasteHadoopUtils.readID(tokens[TasteHadoopUtils.ITEM_ID_POS], usesLongIDs);    float rating = Float.parseFloat(tokens[2]);    ratings.setQuick(userID, rating);    itemIDWritable.set(itemID);    ratingsWritable.set(ratings);    ctx.write(itemIDWritable, ratingsWritable);        ratings.setQuick(userID, 0.0d);}
0
private void runSolver(Path ratings, Path output, Path pathToUorM, int currentIteration, String matrixName, int numEntities) throws ClassNotFoundException, IOException, InterruptedException
{        SharingMapper.reset();    Class<? extends Mapper<IntWritable, VectorWritable, IntWritable, VectorWritable>> solverMapperClassInternal;    String name;    if (implicitFeedback) {        solverMapperClassInternal = SolveImplicitFeedbackMapper.class;        name = "Recompute " + matrixName + ", iteration (" + currentIteration + '/' + numIterations + "), " + '(' + numThreadsPerSolver + " threads, " + numFeatures + " features, implicit feedback)";    } else {        solverMapperClassInternal = SolveExplicitFeedbackMapper.class;        name = "Recompute " + matrixName + ", iteration (" + currentIteration + '/' + numIterations + "), " + '(' + numThreadsPerSolver + " threads, " + numFeatures + " features, explicit feedback)";    }    Job solverForUorI = prepareJob(ratings, output, SequenceFileInputFormat.class, MultithreadedSharingMapper.class, IntWritable.class, VectorWritable.class, SequenceFileOutputFormat.class, name);    Configuration solverConf = solverForUorI.getConfiguration();    solverConf.set(LAMBDA, String.valueOf(lambda));    solverConf.set(ALPHA, String.valueOf(alpha));    solverConf.setInt(NUM_FEATURES, numFeatures);    solverConf.set(NUM_ENTITIES, String.valueOf(numEntities));    FileSystem fs = FileSystem.get(pathToUorM.toUri(), solverConf);    FileStatus[] parts = fs.listStatus(pathToUorM, PathFilters.partFilter());    for (FileStatus part : parts) {        if (log.isDebugEnabled()) {                    }        DistributedCache.addCacheFile(part.getPath().toUri(), solverConf);    }    MultithreadedMapper.setMapperClass(solverForUorI, solverMapperClassInternal);    MultithreadedMapper.setNumberOfThreads(solverForUorI, numThreadsPerSolver);    boolean succeeded = solverForUorI.waitForCompletion(true);    if (!succeeded) {        throw new IllegalStateException("Job failed!");    }}
1
protected void map(IntWritable r, VectorWritable v, Context ctx) throws IOException, InterruptedException
{    RunningAverage avg = new FullRunningAverage();    for (Vector.Element e : v.get().nonZeroes()) {        avg.addDatum(e.get());    }    featureVector.setQuick(r.get(), avg.getAverage());    featureVectorWritable.set(featureVector);    ctx.write(firstIndex, featureVectorWritable);        featureVector.setQuick(r.get(), 0.0d);}
0
protected void setup(Context ctx) throws IOException, InterruptedException
{    tokenPos = ctx.getConfiguration().getInt(TOKEN_POS, -1);    Preconditions.checkState(tokenPos >= 0);}
0
protected void map(LongWritable key, Text line, Context ctx) throws IOException, InterruptedException
{    String[] tokens = TasteHadoopUtils.splitPrefTokens(line.toString());    long id = Long.parseLong(tokens[tokenPos]);    index.set(TasteHadoopUtils.idToIndex(id));    idWritable.set(id);    ctx.write(index, idWritable);}
0
protected void reduce(VarIntWritable index, Iterable<VarLongWritable> ids, Context ctx) throws IOException, InterruptedException
{    ctx.write(index, ids.iterator().next());}
0
private Path pathToM(int iteration)
{    return iteration == numIterations - 1 ? getOutputPath("M") : getTempPath("M-" + iteration);}
0
private Path pathToU(int iteration)
{    return iteration == numIterations - 1 ? getOutputPath("U") : getTempPath("U-" + iteration);}
0
private Path pathToItemRatings()
{    return getTempPath("itemRatings");}
0
private Path pathToUserRatings()
{    return getOutputPath("userRatings");}
0
 Pair<OpenIntObjectHashMap<Vector>, OpenIntObjectHashMap<Vector>> createSharedInstance(Context ctx)
{    Configuration conf = ctx.getConfiguration();    Path pathToU = new Path(conf.get(RecommenderJob.USER_FEATURES_PATH));    Path pathToM = new Path(conf.get(RecommenderJob.ITEM_FEATURES_PATH));    OpenIntObjectHashMap<Vector> U = ALS.readMatrixByRows(pathToU, conf);    OpenIntObjectHashMap<Vector> M = ALS.readMatrixByRows(pathToM, conf);    return new Pair<>(U, M);}
0
protected void setup(Context ctx) throws IOException, InterruptedException
{    Configuration conf = ctx.getConfiguration();    recommendationsPerUser = conf.getInt(RecommenderJob.NUM_RECOMMENDATIONS, RecommenderJob.DEFAULT_NUM_RECOMMENDATIONS);    maxRating = Float.parseFloat(conf.get(RecommenderJob.MAX_RATING));    usesLongIDs = conf.getBoolean(ParallelALSFactorizationJob.USES_LONG_IDS, false);    if (usesLongIDs) {        userIDIndex = TasteHadoopUtils.readIDIndexMap(conf.get(RecommenderJob.USER_INDEX_PATH), conf);        itemIDIndex = TasteHadoopUtils.readIDIndexMap(conf.get(RecommenderJob.ITEM_INDEX_PATH), conf);    }}
0
protected void map(IntWritable userIndexWritable, VectorWritable ratingsWritable, Context ctx) throws IOException, InterruptedException
{    Pair<OpenIntObjectHashMap<Vector>, OpenIntObjectHashMap<Vector>> uAndM = getSharedInstance();    OpenIntObjectHashMap<Vector> U = uAndM.getFirst();    OpenIntObjectHashMap<Vector> M = uAndM.getSecond();    Vector ratings = ratingsWritable.get();    int userIndex = userIndexWritable.get();    final OpenIntHashSet alreadyRatedItems = new OpenIntHashSet(ratings.getNumNondefaultElements());    for (Vector.Element e : ratings.nonZeroes()) {        alreadyRatedItems.add(e.index());    }    final TopItemsQueue topItemsQueue = new TopItemsQueue(recommendationsPerUser);    final Vector userFeatures = U.get(userIndex);    M.forEachPair(new IntObjectProcedure<Vector>() {        @Override        public boolean apply(int itemID, Vector itemFeatures) {            if (!alreadyRatedItems.contains(itemID)) {                double predictedRating = userFeatures.dot(itemFeatures);                MutableRecommendedItem top = topItemsQueue.top();                if (predictedRating > top.getValue()) {                    top.set(itemID, (float) predictedRating);                    topItemsQueue.updateTop();                }            }            return true;        }    });    List<RecommendedItem> recommendedItems = topItemsQueue.getTopItems();    if (!recommendedItems.isEmpty()) {                for (RecommendedItem topItem : recommendedItems) {            ((MutableRecommendedItem) topItem).capToMaxValue(maxRating);        }        if (usesLongIDs) {            long userID = userIDIndex.get(userIndex);            userIDWritable.set(userID);            for (RecommendedItem topItem : recommendedItems) {                                long itemID = itemIDIndex.get((int) topItem.getItemID());                ((MutableRecommendedItem) topItem).setItemID(itemID);            }        } else {            userIDWritable.set(userIndex);        }        recommendations.set(recommendedItems);        ctx.write(userIDWritable, recommendations);    }}
0
public boolean apply(int itemID, Vector itemFeatures)
{    if (!alreadyRatedItems.contains(itemID)) {        double predictedRating = userFeatures.dot(itemFeatures);        MutableRecommendedItem top = topItemsQueue.top();        if (predictedRating > top.getValue()) {            top.set(itemID, (float) predictedRating);            topItemsQueue.updateTop();        }    }    return true;}
0
public static void main(String[] args) throws Exception
{    ToolRunner.run(new RecommenderJob(), args);}
0
public int run(String[] args) throws Exception
{    addInputOption();    addOption("userFeatures", null, "path to the user feature matrix", true);    addOption("itemFeatures", null, "path to the item feature matrix", true);    addOption("numRecommendations", null, "number of recommendations per user", String.valueOf(DEFAULT_NUM_RECOMMENDATIONS));    addOption("maxRating", null, "maximum rating available", true);    addOption("numThreads", null, "threads per mapper", String.valueOf(1));    addOption("usesLongIDs", null, "input contains long IDs that need to be translated");    addOption("userIDIndex", null, "index for user long IDs (necessary if usesLongIDs is true)");    addOption("itemIDIndex", null, "index for user long IDs (necessary if usesLongIDs is true)");    addOutputOption();    Map<String, List<String>> parsedArgs = parseArguments(args);    if (parsedArgs == null) {        return -1;    }    Job prediction = prepareJob(getInputPath(), getOutputPath(), SequenceFileInputFormat.class, MultithreadedSharingMapper.class, IntWritable.class, RecommendedItemsWritable.class, TextOutputFormat.class);    Configuration conf = prediction.getConfiguration();    int numThreads = Integer.parseInt(getOption("numThreads"));    conf.setInt(NUM_RECOMMENDATIONS, Integer.parseInt(getOption("numRecommendations")));    conf.set(USER_FEATURES_PATH, getOption("userFeatures"));    conf.set(ITEM_FEATURES_PATH, getOption("itemFeatures"));    conf.set(MAX_RATING, getOption("maxRating"));    boolean usesLongIDs = Boolean.parseBoolean(getOption("usesLongIDs"));    if (usesLongIDs) {        conf.set(ParallelALSFactorizationJob.USES_LONG_IDS, String.valueOf(true));        conf.set(USER_INDEX_PATH, getOption("userIDIndex"));        conf.set(ITEM_INDEX_PATH, getOption("itemIDIndex"));    }    MultithreadedMapper.setMapperClass(prediction, PredictionMapper.class);    MultithreadedMapper.setNumberOfThreads(prediction, numThreads);    boolean succeeded = prediction.waitForCompletion(true);    if (!succeeded) {        return -1;    }    return 0;}
0
 final void setupSharedInstance(Context context) throws IOException
{    if (SHARED_INSTANCE == null) {        SHARED_INSTANCE = createSharedInstance(context);    }}
0
 final S getSharedInstance()
{    return (S) SHARED_INSTANCE;}
0
 static void reset()
{    SHARED_INSTANCE = null;}
0
 OpenIntObjectHashMap<Vector> createSharedInstance(Context ctx) throws IOException
{    Configuration conf = ctx.getConfiguration();    int numEntities = Integer.parseInt(conf.get(ParallelALSFactorizationJob.NUM_ENTITIES));    return ALS.readMatrixByRowsFromDistributedCache(numEntities, conf);}
0
protected void setup(Mapper.Context ctx) throws IOException, InterruptedException
{    lambda = Double.parseDouble(ctx.getConfiguration().get(ParallelALSFactorizationJob.LAMBDA));    numFeatures = ctx.getConfiguration().getInt(ParallelALSFactorizationJob.NUM_FEATURES, -1);    Preconditions.checkArgument(numFeatures > 0, "numFeatures must be greater then 0!");}
0
protected void map(IntWritable userOrItemID, VectorWritable ratingsWritable, Context ctx) throws IOException, InterruptedException
{    OpenIntObjectHashMap<Vector> uOrM = getSharedInstance();    uiOrmj.set(ALS.solveExplicit(ratingsWritable, uOrM, lambda, numFeatures));    ctx.write(userOrItemID, uiOrmj);}
0
 ImplicitFeedbackAlternatingLeastSquaresSolver createSharedInstance(Context ctx) throws IOException
{    Configuration conf = ctx.getConfiguration();    double lambda = Double.parseDouble(conf.get(ParallelALSFactorizationJob.LAMBDA));    double alpha = Double.parseDouble(conf.get(ParallelALSFactorizationJob.ALPHA));    int numFeatures = conf.getInt(ParallelALSFactorizationJob.NUM_FEATURES, -1);    int numEntities = Integer.parseInt(conf.get(ParallelALSFactorizationJob.NUM_ENTITIES));    Preconditions.checkArgument(numFeatures > 0, "numFeatures must be greater then 0!");    return new ImplicitFeedbackAlternatingLeastSquaresSolver(numFeatures, lambda, alpha, ALS.readMatrixByRowsFromDistributedCache(numEntities, conf), 1);}
0
protected void map(IntWritable userOrItemID, VectorWritable ratingsWritable, Context ctx) throws IOException, InterruptedException
{    ImplicitFeedbackAlternatingLeastSquaresSolver solver = getSharedInstance();    uiOrmj.set(solver.solve(ratingsWritable.get()));    ctx.write(userOrItemID, uiOrmj);}
0
 long getAID()
{    return aID;}
0
 long getBID()
{    return bID;}
0
public void write(DataOutput out) throws IOException
{    Varint.writeSignedVarLong(aID, out);    Varint.writeSignedVarLong(bID, out);}
0
public void readFields(DataInput in) throws IOException
{    aID = Varint.readSignedVarLong(in);    bID = Varint.readSignedVarLong(in);}
0
public int compareTo(EntityEntityWritable that)
{    int aCompare = compare(aID, that.getAID());    return aCompare == 0 ? compare(bID, that.getBID()) : aCompare;}
0
private static int compare(long a, long b)
{    return a < b ? -1 : a > b ? 1 : 0;}
0
public int hashCode()
{    return Longs.hashCode(aID) + 31 * Longs.hashCode(bID);}
0
public boolean equals(Object o)
{    if (o instanceof EntityEntityWritable) {        EntityEntityWritable that = (EntityEntityWritable) o;        return aID == that.getAID() && bID == that.getBID();    }    return false;}
0
public String toString()
{    return aID + "\t" + bID;}
0
public EntityEntityWritable clone()
{    return new EntityEntityWritable(aID, bID);}
0
public long getID()
{    return get();}
0
public float getPrefValue()
{    return prefValue;}
0
public void write(DataOutput out) throws IOException
{    super.write(out);    out.writeFloat(prefValue);}
0
public void readFields(DataInput in) throws IOException
{    super.readFields(in);    prefValue = in.readFloat();}
0
public int hashCode()
{    return super.hashCode() ^ RandomUtils.hashFloat(prefValue);}
0
public boolean equals(Object o)
{    if (!(o instanceof EntityPrefWritable)) {        return false;    }    EntityPrefWritable other = (EntityPrefWritable) o;    return get() == other.get() && prefValue == other.getPrefValue();}
0
public String toString()
{    return get() + "\t" + prefValue;}
0
public EntityPrefWritable clone()
{    return new EntityPrefWritable(get(), prefValue);}
0
protected void setup(Context context) throws IOException
{    Configuration conf = context.getConfiguration();    recommendationsPerUser = conf.getInt(NUM_RECOMMENDATIONS, DEFAULT_NUM_RECOMMENDATIONS);    booleanData = conf.getBoolean(RecommenderJob.BOOLEAN_DATA, false);    indexItemIDMap = TasteHadoopUtils.readIDIndexMap(conf.get(ITEMID_INDEX_PATH), conf);    idReader = new IDReader(conf);    idReader.readIDs();    itemsToRecommendFor = idReader.getItemIds();}
0
protected void reduce(VarLongWritable userID, Iterable<PrefAndSimilarityColumnWritable> values, Context context) throws IOException, InterruptedException
{    if (booleanData) {        reduceBooleanData(userID, values, context);    } else {        reduceNonBooleanData(userID, values, context);    }}
0
private void reduceBooleanData(VarLongWritable userID, Iterable<PrefAndSimilarityColumnWritable> values, Context context) throws IOException, InterruptedException
{    /* having boolean data, each estimated preference can only be 1,     * however we can't use this to rank the recommended items,     * so we use the sum of similarities for that. */    Iterator<PrefAndSimilarityColumnWritable> columns = values.iterator();    Vector predictions = columns.next().getSimilarityColumn();    while (columns.hasNext()) {        predictions.assign(columns.next().getSimilarityColumn(), Functions.PLUS);    }    writeRecommendedItems(userID, predictions, context);}
0
private void reduceNonBooleanData(VarLongWritable userID, Iterable<PrefAndSimilarityColumnWritable> values, Context context) throws IOException, InterruptedException
{    /* each entry here is the sum in the numerator of the prediction formula */    Vector numerators = null;    /* each entry here is the sum in the denominator of the prediction formula */    Vector denominators = null;    /* each entry here is the number of similar items used in the prediction formula */    Vector numberOfSimilarItemsUsed = new RandomAccessSparseVector(Integer.MAX_VALUE, 100);    for (PrefAndSimilarityColumnWritable prefAndSimilarityColumn : values) {        Vector simColumn = prefAndSimilarityColumn.getSimilarityColumn();        float prefValue = prefAndSimilarityColumn.getPrefValue();        /* count the number of items used for each prediction */        for (Element e : simColumn.nonZeroes()) {            int itemIDIndex = e.index();            numberOfSimilarItemsUsed.setQuick(itemIDIndex, numberOfSimilarItemsUsed.getQuick(itemIDIndex) + 1);        }        if (denominators == null) {            denominators = simColumn.clone();        } else {            denominators.assign(simColumn, Functions.PLUS_ABS);        }        if (numerators == null) {            numerators = simColumn.clone();            if (prefValue != BOOLEAN_PREF_VALUE) {                numerators.assign(Functions.MULT, prefValue);            }        } else {            if (prefValue != BOOLEAN_PREF_VALUE) {                simColumn.assign(Functions.MULT, prefValue);            }            numerators.assign(simColumn, Functions.PLUS);        }    }    if (numerators == null) {        return;    }    Vector recommendationVector = new RandomAccessSparseVector(Integer.MAX_VALUE, 100);    for (Element element : numerators.nonZeroes()) {        int itemIDIndex = element.index();        /* preference estimations must be based on at least 2 datapoints */        if (numberOfSimilarItemsUsed.getQuick(itemIDIndex) > 1) {            /* compute normalized prediction */            double prediction = element.get() / denominators.getQuick(itemIDIndex);            recommendationVector.setQuick(itemIDIndex, prediction);        }    }    writeRecommendedItems(userID, recommendationVector, context);}
0
private void writeRecommendedItems(VarLongWritable userID, Vector recommendationVector, Context context) throws IOException, InterruptedException
{    TopItemsQueue topKItems = new TopItemsQueue(recommendationsPerUser);    FastIDSet itemsForUser = null;    if (idReader != null && idReader.isUserItemFilterSpecified()) {        itemsForUser = idReader.getItemsToRecommendForUser(userID.get());    }    for (Element element : recommendationVector.nonZeroes()) {        int index = element.index();        long itemID;        if (indexItemIDMap != null && !indexItemIDMap.isEmpty()) {            itemID = indexItemIDMap.get(index);        } else {                        itemID = index;        }        if (shouldIncludeItemIntoRecommendations(itemID, itemsToRecommendFor, itemsForUser)) {            float value = (float) element.get();            if (!Float.isNaN(value)) {                MutableRecommendedItem topItem = topKItems.top();                if (value > topItem.getValue()) {                    topItem.set(itemID, value);                    topKItems.updateTop();                }            }        }    }    List<RecommendedItem> topItems = topKItems.getTopItems();    if (!topItems.isEmpty()) {        recommendedItems.set(topItems);        context.write(userID, recommendedItems);    }}
0
private boolean shouldIncludeItemIntoRecommendations(long itemID, FastIDSet allItemsToRecommendFor, FastIDSet itemsForUser)
{    if (allItemsToRecommendFor == null && itemsForUser == null) {        return true;    } else if (itemsForUser != null) {        return itemsForUser.contains(itemID);    } else {        return allItemsToRecommendFor.contains(itemID);    }}
0
public void readIDs() throws IOException, IllegalStateException
{    if (isUserItemFileSpecified()) {        readUserItemFilterIfNeeded();    }    if (isUsersFileSpecified() || isUserItemFilterSpecified()) {        readUserIds();    }    if (isItemsFileSpecified() || isUserItemFilterSpecified()) {        readItemIds();    }}
0
public FastIDSet getItemsToRecommendForUser(Long userId)
{    if (isUserItemFilterSpecified() && userItemFilter.containsKey(userId)) {        return userItemFilter.get(userId);    } else {        return emptySet;    }}
0
private void readUserIds() throws IOException, IllegalStateException
{    if (isUsersFileSpecified() && !isUserItemFileSpecified()) {        userIds = readIDList(usersFile);    } else if (isUserItemFileSpecified() && !isUsersFileSpecified()) {        readUserItemFilterIfNeeded();        userIds = extractAllUserIdsFromUserItemFilter(userItemFilter);    } else if (!isUsersFileSpecified()) {        throw new IllegalStateException("Neither usersFile nor userItemFile options are specified");    } else {        throw new IllegalStateException("usersFile and userItemFile options cannot be used simultaneously");    }}
0
private void readItemIds() throws IOException, IllegalStateException
{    if (isItemsFileSpecified() && !isUserItemFileSpecified()) {        itemIds = readIDList(itemsFile);    } else if (isUserItemFileSpecified() && !isItemsFileSpecified()) {        readUserItemFilterIfNeeded();        itemIds = extractAllItemIdsFromUserItemFilter(userItemFilter);    } else if (!isItemsFileSpecified()) {        throw new IllegalStateException("Neither itemsFile nor userItemFile options are specified");    } else {        throw new IllegalStateException("itemsFile and userItemFile options cannot be specified simultaneously");    }}
0
private void readUserItemFilterIfNeeded() throws IOException
{    if (!isUserItemFilterSpecified() && isUserItemFileSpecified()) {        userItemFilter = readUserItemFilter(userItemFile);    }}
0
private Map<Long, FastIDSet> readUserItemFilter(String pathString) throws IOException
{    Map<Long, FastIDSet> result = new HashMap<>();    try (InputStream in = openFile(pathString)) {        for (String line : new FileLineIterable(in)) {            try {                String[] tokens = SEPARATOR.split(line);                Long userId = Long.parseLong(tokens[0]);                Long itemId = Long.parseLong(tokens[1]);                addUserAndItemIdToUserItemFilter(result, userId, itemId);            } catch (NumberFormatException nfe) {                            }        }    }    return result;}
1
 void addUserAndItemIdToUserItemFilter(Map<Long, FastIDSet> filter, Long userId, Long itemId)
{    FastIDSet itemIds;    if (filter.containsKey(userId)) {        itemIds = filter.get(userId);    } else {        itemIds = new FastIDSet();        filter.put(userId, itemIds);    }    itemIds.add(itemId);}
0
 static FastIDSet extractAllUserIdsFromUserItemFilter(Map<Long, FastIDSet> filter)
{    FastIDSet result = new FastIDSet();    for (Long userId : filter.keySet()) {        result.add(userId);    }    return result;}
0
private FastIDSet extractAllItemIdsFromUserItemFilter(Map<Long, FastIDSet> filter)
{    FastIDSet result = new FastIDSet();    for (FastIDSet itemIds : filter.values()) {        result.addAll(itemIds);    }    return result;}
0
private FastIDSet readIDList(String pathString) throws IOException
{    FastIDSet result = null;    if (pathString != null) {        result = new FastIDSet();        try (InputStream in = openFile(pathString)) {            for (String line : new FileLineIterable(in)) {                try {                    result.add(Long.parseLong(line));                } catch (NumberFormatException nfe) {                                    }            }        }    }    return result;}
1
private InputStream openFile(String pathString) throws IOException
{    return HadoopUtil.openStream(new Path(pathString), conf);}
0
public boolean isUsersFileSpecified()
{    return usersFile != null;}
0
public boolean isItemsFileSpecified()
{    return itemsFile != null;}
0
public boolean isUserItemFileSpecified()
{    return userItemFile != null;}
0
public boolean isUserItemFilterSpecified()
{    return userItemFilter != null;}
0
public FastIDSet getUserIds()
{    return userIds;}
0
public FastIDSet getItemIds()
{    return itemIds;}
0
protected void reduce(VarLongWritable itemID, Iterable<VarLongWritable> values, Context ctx) throws IOException, InterruptedException
{    int itemIDIndex = TasteHadoopUtils.idToIndex(itemID.get());    Vector vector = new RandomAccessSparseVector(Integer.MAX_VALUE, 1);    /* artificial NaN summand to exclude this item from the recommendations for all users specified in userIDs */    vector.set(itemIDIndex, Double.NaN);    List<Long> userIDs = new ArrayList<>();    List<Float> prefValues = new ArrayList<>();    for (VarLongWritable userID : values) {        userIDs.add(userID.get());        prefValues.add(1.0f);    }    itemIDIndexWritable.set(itemIDIndex);    vectorAndPrefs.set(vector, userIDs, prefValues);    ctx.write(itemIDIndexWritable, vectorAndPrefs);}
0
protected void map(LongWritable key, Text line, Context ctx) throws IOException, InterruptedException
{    String[] tokens = SEPARATOR.split(line.toString());    long userID = Long.parseLong(tokens[0]);    long itemID = Long.parseLong(tokens[1]);    itemIDWritable.set(itemID);    userIDWritable.set(userID);    ctx.write(itemIDWritable, userIDWritable);}
0
protected void setup(Context context)
{    Configuration jobConf = context.getConfiguration();    transpose = jobConf.getBoolean(ToEntityPrefsMapper.TRANSPOSE_USER_ITEM, false);}
0
protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException
{    String[] tokens = TasteHadoopUtils.splitPrefTokens(value.toString());    long itemID = Long.parseLong(tokens[transpose ? 0 : 1]);    int index = TasteHadoopUtils.idToIndex(itemID);    indexWritable.set(index);    itemIDWritable.set(itemID);    context.write(indexWritable, itemIDWritable);}
0
protected void reduce(VarIntWritable index, Iterable<VarLongWritable> possibleItemIDs, Context context) throws IOException, InterruptedException
{    long minimumItemID = Long.MAX_VALUE;    for (VarLongWritable varLongWritable : possibleItemIDs) {        long itemID = varLongWritable.get();        if (itemID < minimumItemID) {            minimumItemID = itemID;        }    }    if (minimumItemID != Long.MAX_VALUE) {        minimumItemIDWritable.set(minimumItemID);        context.write(index, minimumItemIDWritable);    }}
0
protected void map(VarIntWritable key, VectorAndPrefsWritable vectorAndPrefsWritable, Context context) throws IOException, InterruptedException
{    Vector similarityMatrixColumn = vectorAndPrefsWritable.getVector();    List<Long> userIDs = vectorAndPrefsWritable.getUserIDs();    List<Float> prefValues = vectorAndPrefsWritable.getValues();    for (int i = 0; i < userIDs.size(); i++) {        long userID = userIDs.get(i);        float prefValue = prefValues.get(i);        if (!Float.isNaN(prefValue)) {            prefAndSimilarityColumn.set(prefValue, similarityMatrixColumn);            userIDWritable.set(userID);            context.write(userIDWritable, prefAndSimilarityColumn);        }    }}
0
public void set(float prefValue, Vector similarityColumn)
{    this.prefValue = prefValue;    this.similarityColumn = similarityColumn;}
0
public float getPrefValue()
{    return prefValue;}
0
public Vector getSimilarityColumn()
{    return similarityColumn;}
0
public void readFields(DataInput in) throws IOException
{    prefValue = in.readFloat();    VectorWritable vw = new VectorWritable();    vw.readFields(in);    similarityColumn = vw.get();}
0
public void write(DataOutput out) throws IOException
{    out.writeFloat(prefValue);    VectorWritable vw = new VectorWritable(similarityColumn);    vw.setWritesLaxPrecision(true);    vw.write(out);}
0
public boolean equals(Object obj)
{    if (obj instanceof PrefAndSimilarityColumnWritable) {        PrefAndSimilarityColumnWritable other = (PrefAndSimilarityColumnWritable) obj;        return prefValue == other.prefValue && similarityColumn.equals(other.similarityColumn);    }    return false;}
0
public int hashCode()
{    return RandomUtils.hashFloat(prefValue) + 31 * similarityColumn.hashCode();}
0
public int run(String[] args) throws Exception
{    addInputOption();    addOutputOption();    addOption("numRecommendations", "n", "Number of recommendations per user", String.valueOf(AggregateAndRecommendReducer.DEFAULT_NUM_RECOMMENDATIONS));    addOption("usersFile", null, "File of users to recommend for", null);    addOption("itemsFile", null, "File of items to recommend for", null);    addOption("filterFile", "f", "File containing comma-separated userID,itemID pairs. Used to exclude the item from " + "the recommendations for that user (optional)", null);    addOption("userItemFile", "uif", "File containing comma-separated userID,itemID pairs (optional). " + "Used to include only these items into recommendations. " + "Cannot be used together with usersFile or itemsFile", null);    addOption("booleanData", "b", "Treat input as without pref values", Boolean.FALSE.toString());    addOption("maxPrefsPerUser", "mxp", "Maximum number of preferences considered per user in final recommendation phase", String.valueOf(UserVectorSplitterMapper.DEFAULT_MAX_PREFS_PER_USER_CONSIDERED));    addOption("minPrefsPerUser", "mp", "ignore users with less preferences than this in the similarity computation " + "(default: " + DEFAULT_MIN_PREFS_PER_USER + ')', String.valueOf(DEFAULT_MIN_PREFS_PER_USER));    addOption("maxSimilaritiesPerItem", "m", "Maximum number of similarities considered per item ", String.valueOf(DEFAULT_MAX_SIMILARITIES_PER_ITEM));    addOption("maxPrefsInItemSimilarity", "mpiis", "max number of preferences to consider per user or item in the " + "item similarity computation phase, users or items with more preferences will be sampled down (default: " + DEFAULT_MAX_PREFS + ')', String.valueOf(DEFAULT_MAX_PREFS));    addOption("similarityClassname", "s", "Name of distributed similarity measures class to instantiate, " + "alternatively use one of the predefined similarities (" + VectorSimilarityMeasures.list() + ')', true);    addOption("threshold", "tr", "discard item pairs with a similarity value below this", false);    addOption("outputPathForSimilarityMatrix", "opfsm", "write the item similarity matrix to this path (optional)", false);    addOption("randomSeed", null, "use this seed for sampling", false);    addFlag("sequencefileOutput", null, "write the output into a SequenceFile instead of a text file");    Map<String, List<String>> parsedArgs = parseArguments(args);    if (parsedArgs == null) {        return -1;    }    Path outputPath = getOutputPath();    int numRecommendations = Integer.parseInt(getOption("numRecommendations"));    String usersFile = getOption("usersFile");    String itemsFile = getOption("itemsFile");    String filterFile = getOption("filterFile");    String userItemFile = getOption("userItemFile");    boolean booleanData = Boolean.valueOf(getOption("booleanData"));    int maxPrefsPerUser = Integer.parseInt(getOption("maxPrefsPerUser"));    int minPrefsPerUser = Integer.parseInt(getOption("minPrefsPerUser"));    int maxPrefsInItemSimilarity = Integer.parseInt(getOption("maxPrefsInItemSimilarity"));    int maxSimilaritiesPerItem = Integer.parseInt(getOption("maxSimilaritiesPerItem"));    String similarityClassname = getOption("similarityClassname");    double threshold = hasOption("threshold") ? Double.parseDouble(getOption("threshold")) : RowSimilarityJob.NO_THRESHOLD;    long randomSeed = hasOption("randomSeed") ? Long.parseLong(getOption("randomSeed")) : RowSimilarityJob.NO_FIXED_RANDOM_SEED;    Path prepPath = getTempPath(DEFAULT_PREPARE_PATH);    Path similarityMatrixPath = getTempPath("similarityMatrix");    Path explicitFilterPath = getTempPath("explicitFilterPath");    Path partialMultiplyPath = getTempPath("partialMultiply");    AtomicInteger currentPhase = new AtomicInteger();    int numberOfUsers = -1;    if (shouldRunNextPhase(parsedArgs, currentPhase)) {        ToolRunner.run(getConf(), new PreparePreferenceMatrixJob(), new String[] { "--input", getInputPath().toString(), "--output", prepPath.toString(), "--minPrefsPerUser", String.valueOf(minPrefsPerUser), "--booleanData", String.valueOf(booleanData), "--tempDir", getTempPath().toString() });        numberOfUsers = HadoopUtil.readInt(new Path(prepPath, PreparePreferenceMatrixJob.NUM_USERS), getConf());    }    if (shouldRunNextPhase(parsedArgs, currentPhase)) {        /* special behavior if phase 1 is skipped */        if (numberOfUsers == -1) {            numberOfUsers = (int) HadoopUtil.countRecords(new Path(prepPath, PreparePreferenceMatrixJob.USER_VECTORS), PathType.LIST, null, getConf());        }                ToolRunner.run(getConf(), new RowSimilarityJob(), new String[] { "--input", new Path(prepPath, PreparePreferenceMatrixJob.RATING_MATRIX).toString(), "--output", similarityMatrixPath.toString(), "--numberOfColumns", String.valueOf(numberOfUsers), "--similarityClassname", similarityClassname, "--maxObservationsPerRow", String.valueOf(maxPrefsInItemSimilarity), "--maxObservationsPerColumn", String.valueOf(maxPrefsInItemSimilarity), "--maxSimilaritiesPerRow", String.valueOf(maxSimilaritiesPerItem), "--excludeSelfSimilarity", String.valueOf(Boolean.TRUE), "--threshold", String.valueOf(threshold), "--randomSeed", String.valueOf(randomSeed), "--tempDir", getTempPath().toString() });                if (hasOption("outputPathForSimilarityMatrix")) {            Path outputPathForSimilarityMatrix = new Path(getOption("outputPathForSimilarityMatrix"));            Job outputSimilarityMatrix = prepareJob(similarityMatrixPath, outputPathForSimilarityMatrix, SequenceFileInputFormat.class, ItemSimilarityJob.MostSimilarItemPairsMapper.class, EntityEntityWritable.class, DoubleWritable.class, ItemSimilarityJob.MostSimilarItemPairsReducer.class, EntityEntityWritable.class, DoubleWritable.class, TextOutputFormat.class);            Configuration mostSimilarItemsConf = outputSimilarityMatrix.getConfiguration();            mostSimilarItemsConf.set(ItemSimilarityJob.ITEM_ID_INDEX_PATH_STR, new Path(prepPath, PreparePreferenceMatrixJob.ITEMID_INDEX).toString());            mostSimilarItemsConf.setInt(ItemSimilarityJob.MAX_SIMILARITIES_PER_ITEM, maxSimilaritiesPerItem);            outputSimilarityMatrix.waitForCompletion(true);        }    }        if (shouldRunNextPhase(parsedArgs, currentPhase)) {        Job partialMultiply = Job.getInstance(getConf(), "partialMultiply");        Configuration partialMultiplyConf = partialMultiply.getConfiguration();        MultipleInputs.addInputPath(partialMultiply, similarityMatrixPath, SequenceFileInputFormat.class, SimilarityMatrixRowWrapperMapper.class);        MultipleInputs.addInputPath(partialMultiply, new Path(prepPath, PreparePreferenceMatrixJob.USER_VECTORS), SequenceFileInputFormat.class, UserVectorSplitterMapper.class);        partialMultiply.setJarByClass(ToVectorAndPrefReducer.class);        partialMultiply.setMapOutputKeyClass(VarIntWritable.class);        partialMultiply.setMapOutputValueClass(VectorOrPrefWritable.class);        partialMultiply.setReducerClass(ToVectorAndPrefReducer.class);        partialMultiply.setOutputFormatClass(SequenceFileOutputFormat.class);        partialMultiply.setOutputKeyClass(VarIntWritable.class);        partialMultiply.setOutputValueClass(VectorAndPrefsWritable.class);        partialMultiplyConf.setBoolean("mapred.compress.map.output", true);        partialMultiplyConf.set("mapred.output.dir", partialMultiplyPath.toString());        if (usersFile != null) {            partialMultiplyConf.set(UserVectorSplitterMapper.USERS_FILE, usersFile);        }        if (userItemFile != null) {            partialMultiplyConf.set(IDReader.USER_ITEM_FILE, userItemFile);        }        partialMultiplyConf.setInt(UserVectorSplitterMapper.MAX_PREFS_PER_USER_CONSIDERED, maxPrefsPerUser);        boolean succeeded = partialMultiply.waitForCompletion(true);        if (!succeeded) {            return -1;        }    }    if (shouldRunNextPhase(parsedArgs, currentPhase)) {        /* convert the user/item pairs to filter if a filterfile has been specified */        if (filterFile != null) {            Job itemFiltering = prepareJob(new Path(filterFile), explicitFilterPath, TextInputFormat.class, ItemFilterMapper.class, VarLongWritable.class, VarLongWritable.class, ItemFilterAsVectorAndPrefsReducer.class, VarIntWritable.class, VectorAndPrefsWritable.class, SequenceFileOutputFormat.class);            boolean succeeded = itemFiltering.waitForCompletion(true);            if (!succeeded) {                return -1;            }        }        String aggregateAndRecommendInput = partialMultiplyPath.toString();        if (filterFile != null) {            aggregateAndRecommendInput += "," + explicitFilterPath;        }        Class<? extends OutputFormat> outputFormat = parsedArgs.containsKey("--sequencefileOutput") ? SequenceFileOutputFormat.class : TextOutputFormat.class;                Job aggregateAndRecommend = prepareJob(new Path(aggregateAndRecommendInput), outputPath, SequenceFileInputFormat.class, PartialMultiplyMapper.class, VarLongWritable.class, PrefAndSimilarityColumnWritable.class, AggregateAndRecommendReducer.class, VarLongWritable.class, RecommendedItemsWritable.class, outputFormat);        Configuration aggregateAndRecommendConf = aggregateAndRecommend.getConfiguration();        if (itemsFile != null) {            aggregateAndRecommendConf.set(AggregateAndRecommendReducer.ITEMS_FILE, itemsFile);        }        if (userItemFile != null) {            aggregateAndRecommendConf.set(IDReader.USER_ITEM_FILE, userItemFile);        }        if (filterFile != null) {            setS3SafeCombinedInputPath(aggregateAndRecommend, getTempPath(), partialMultiplyPath, explicitFilterPath);        }        setIOSort(aggregateAndRecommend);        aggregateAndRecommendConf.set(AggregateAndRecommendReducer.ITEMID_INDEX_PATH, new Path(prepPath, PreparePreferenceMatrixJob.ITEMID_INDEX).toString());        aggregateAndRecommendConf.setInt(AggregateAndRecommendReducer.NUM_RECOMMENDATIONS, numRecommendations);        aggregateAndRecommendConf.setBoolean(BOOLEAN_DATA, booleanData);        boolean succeeded = aggregateAndRecommend.waitForCompletion(true);        if (!succeeded) {            return -1;        }    }    return 0;}
0
private static void setIOSort(JobContext job)
{    Configuration conf = job.getConfiguration();    conf.setInt("io.sort.factor", 100);        String javaOpts = conf.get("mapred.map.child.java.opts");    if (javaOpts == null) {                javaOpts = conf.get("mapred.child.java.opts");    }    int assumedHeapSize = 512;    if (javaOpts != null) {        Matcher m = Pattern.compile("-Xmx([0-9]+)([mMgG])").matcher(javaOpts);        if (m.find()) {            assumedHeapSize = Integer.parseInt(m.group(1));            String megabyteOrGigabyte = m.group(2);            if ("g".equalsIgnoreCase(megabyteOrGigabyte)) {                assumedHeapSize *= 1024;            }        }    }        conf.setInt("io.sort.mb", Math.min(assumedHeapSize / 2, 1024));            conf.setInt("mapred.task.timeout", 60 * 60 * 1000);}
0
public static void main(String[] args) throws Exception
{    ToolRunner.run(new Configuration(), new RecommenderJob(), args);}
0
protected void map(IntWritable key, VectorWritable value, Context context) throws IOException, InterruptedException
{    Vector similarityMatrixRow = value.get();    /* remove self similarity */    similarityMatrixRow.set(key.get(), Double.NaN);    index.set(key.get());    vectorOrPref.set(similarityMatrixRow);    context.write(index, vectorOrPref);}
0
protected void setup(Context ctx) throws IOException, InterruptedException
{    super.setup(ctx);    minPreferences = ctx.getConfiguration().getInt(MIN_PREFERENCES_PER_USER, 1);}
0
protected void reduce(VarLongWritable userID, Iterable<VarLongWritable> itemPrefs, Context context) throws IOException, InterruptedException
{    Vector userVector = new RandomAccessSparseVector(Integer.MAX_VALUE, 100);    for (VarLongWritable itemPref : itemPrefs) {        int index = TasteHadoopUtils.idToIndex(itemPref.get());        float value = itemPref instanceof EntityPrefWritable ? ((EntityPrefWritable) itemPref).getPrefValue() : 1.0f;        userVector.set(index, value);    }    if (userVector.getNumNondefaultElements() >= minPreferences) {        userVectorWritable.set(userVector);        userVectorWritable.setWritesLaxPrecision(true);        context.getCounter(Counters.USERS).increment(1);        context.write(userID, userVectorWritable);    }}
0
protected void reduce(VarIntWritable key, Iterable<VectorOrPrefWritable> values, Context context) throws IOException, InterruptedException
{    List<Long> userIDs = new ArrayList<>();    List<Float> prefValues = new ArrayList<>();    Vector similarityMatrixColumn = null;    for (VectorOrPrefWritable value : values) {        if (value.getVector() == null) {                        userIDs.add(value.getUserID());            prefValues.add(value.getValue());        } else {                        if (similarityMatrixColumn != null) {                throw new IllegalStateException("Found two similarity-matrix columns for item index " + key.get());            }            similarityMatrixColumn = value.getVector();        }    }    if (similarityMatrixColumn == null) {        return;    }    vectorAndPrefs.set(similarityMatrixColumn, userIDs, prefValues);    context.write(key, vectorAndPrefs);}
0
protected void setup(Context context) throws IOException
{    Configuration jobConf = context.getConfiguration();    maxPrefsPerUserConsidered = jobConf.getInt(MAX_PREFS_PER_USER_CONSIDERED, DEFAULT_MAX_PREFS_PER_USER_CONSIDERED);    IDReader idReader = new IDReader(jobConf);    idReader.readIDs();    usersToRecommendFor = idReader.getUserIds();}
0
protected void map(VarLongWritable key, VectorWritable value, Context context) throws IOException, InterruptedException
{    long userID = key.get();        if (usersToRecommendFor != null && !usersToRecommendFor.contains(userID)) {        return;    }    Vector userVector = maybePruneUserVector(value.get());    for (Element e : userVector.nonZeroes()) {        itemIndexWritable.set(e.index());        vectorOrPref.set(userID, (float) e.get());        context.write(itemIndexWritable, vectorOrPref);    }}
1
private Vector maybePruneUserVector(Vector userVector)
{    if (userVector.getNumNondefaultElements() <= maxPrefsPerUserConsidered) {        return userVector;    }    float smallestLargeValue = findSmallestLargeValue(userVector);        for (Element e : userVector.nonZeroes()) {        float absValue = Math.abs((float) e.get());        if (absValue < smallestLargeValue) {            e.set(Float.NaN);        }    }    return userVector;}
0
private float findSmallestLargeValue(Vector userVector)
{    PriorityQueue<Float> topPrefValues = new PriorityQueue<Float>(maxPrefsPerUserConsidered) {        @Override        protected boolean lessThan(Float f1, Float f2) {            return f1 < f2;        }    };    for (Element e : userVector.nonZeroes()) {        float absValue = Math.abs((float) e.get());        topPrefValues.insertWithOverflow(absValue);    }    return topPrefValues.top();}
0
protected boolean lessThan(Float f1, Float f2)
{    return f1 < f2;}
0
public void set(Vector vector, List<Long> userIDs, List<Float> values)
{    this.vector = vector;    this.userIDs = userIDs;    this.values = values;}
0
public Vector getVector()
{    return vector;}
0
public List<Long> getUserIDs()
{    return userIDs;}
0
public List<Float> getValues()
{    return values;}
0
public void write(DataOutput out) throws IOException
{    VectorWritable vw = new VectorWritable(vector);    vw.setWritesLaxPrecision(true);    vw.write(out);    Varint.writeUnsignedVarInt(userIDs.size(), out);    for (int i = 0; i < userIDs.size(); i++) {        Varint.writeSignedVarLong(userIDs.get(i), out);        out.writeFloat(values.get(i));    }}
0
public void readFields(DataInput in) throws IOException
{    VectorWritable writable = new VectorWritable();    writable.readFields(in);    vector = writable.get();    int size = Varint.readUnsignedVarInt(in);    userIDs = new ArrayList<>(size);    values = new ArrayList<>(size);    for (int i = 0; i < size; i++) {        userIDs.add(Varint.readSignedVarLong(in));        values.add(in.readFloat());    }}
0
public String toString()
{    return vector + "\t" + userIDs + '\t' + values;}
0
public Vector getVector()
{    return vector;}
0
public long getUserID()
{    return userID;}
0
public float getValue()
{    return value;}
0
 void set(Vector vector)
{    this.vector = vector;    this.userID = Long.MIN_VALUE;    this.value = Float.NaN;}
0
public void set(long userID, float value)
{    this.vector = null;    this.userID = userID;    this.value = value;}
0
public void write(DataOutput out) throws IOException
{    if (vector == null) {        out.writeBoolean(false);        Varint.writeSignedVarLong(userID, out);        out.writeFloat(value);    } else {        out.writeBoolean(true);        VectorWritable vw = new VectorWritable(vector);        vw.setWritesLaxPrecision(true);        vw.write(out);    }}
0
public void readFields(DataInput in) throws IOException
{    boolean hasVector = in.readBoolean();    if (hasVector) {        VectorWritable writable = new VectorWritable();        writable.readFields(in);        set(writable.get());    } else {        long theUserID = Varint.readSignedVarLong(in);        float theValue = in.readFloat();        set(theUserID, theValue);    }}
0
public String toString()
{    return vector == null ? userID + ":" + value : vector.toString();}
0
public long getItemID()
{    return itemID;}
0
public float getValue()
{    return value;}
0
public void setItemID(long itemID)
{    this.itemID = itemID;}
0
public void set(long itemID, float value)
{    this.itemID = itemID;    this.value = value;}
0
public void capToMaxValue(float maxValue)
{    if (value > maxValue) {        value = maxValue;    }}
0
public String toString()
{    return "MutableRecommendedItem[item:" + itemID + ", value:" + value + ']';}
0
public int hashCode()
{    return (int) itemID ^ RandomUtils.hashFloat(value);}
0
public boolean equals(Object o)
{    if (!(o instanceof MutableRecommendedItem)) {        return false;    }    RecommendedItem other = (RecommendedItem) o;    return itemID == other.getItemID() && value == other.getValue();}
0
public static void main(String[] args) throws Exception
{    ToolRunner.run(new PreparePreferenceMatrixJob(), args);}
0
public int run(String[] args) throws Exception
{    addInputOption();    addOutputOption();    addOption("minPrefsPerUser", "mp", "ignore users with less preferences than this " + "(default: " + DEFAULT_MIN_PREFS_PER_USER + ')', String.valueOf(DEFAULT_MIN_PREFS_PER_USER));    addOption("booleanData", "b", "Treat input as without pref values", Boolean.FALSE.toString());    addOption("ratingShift", "rs", "shift ratings by this value", "0.0");    Map<String, List<String>> parsedArgs = parseArguments(args);    if (parsedArgs == null) {        return -1;    }    int minPrefsPerUser = Integer.parseInt(getOption("minPrefsPerUser"));    boolean booleanData = Boolean.valueOf(getOption("booleanData"));    float ratingShift = Float.parseFloat(getOption("ratingShift"));        Job itemIDIndex = prepareJob(getInputPath(), getOutputPath(ITEMID_INDEX), TextInputFormat.class, ItemIDIndexMapper.class, VarIntWritable.class, VarLongWritable.class, ItemIDIndexReducer.class, VarIntWritable.class, VarLongWritable.class, SequenceFileOutputFormat.class);    itemIDIndex.setCombinerClass(ItemIDIndexReducer.class);    boolean succeeded = itemIDIndex.waitForCompletion(true);    if (!succeeded) {        return -1;    }        Job toUserVectors = prepareJob(getInputPath(), getOutputPath(USER_VECTORS), TextInputFormat.class, ToItemPrefsMapper.class, VarLongWritable.class, booleanData ? VarLongWritable.class : EntityPrefWritable.class, ToUserVectorsReducer.class, VarLongWritable.class, VectorWritable.class, SequenceFileOutputFormat.class);    toUserVectors.getConfiguration().setBoolean(RecommenderJob.BOOLEAN_DATA, booleanData);    toUserVectors.getConfiguration().setInt(ToUserVectorsReducer.MIN_PREFERENCES_PER_USER, minPrefsPerUser);    toUserVectors.getConfiguration().set(ToEntityPrefsMapper.RATING_SHIFT, String.valueOf(ratingShift));    succeeded = toUserVectors.waitForCompletion(true);    if (!succeeded) {        return -1;    }        int numberOfUsers = (int) toUserVectors.getCounters().findCounter(ToUserVectorsReducer.Counters.USERS).getValue();    HadoopUtil.writeInt(numberOfUsers, getOutputPath(NUM_USERS), getConf());        Job toItemVectors = prepareJob(getOutputPath(USER_VECTORS), getOutputPath(RATING_MATRIX), ToItemVectorsMapper.class, IntWritable.class, VectorWritable.class, ToItemVectorsReducer.class, IntWritable.class, VectorWritable.class);    toItemVectors.setCombinerClass(ToItemVectorsReducer.class);    succeeded = toItemVectors.waitForCompletion(true);    if (!succeeded) {        return -1;    }    return 0;}
0
protected void map(VarLongWritable rowIndex, VectorWritable vectorWritable, Context ctx) throws IOException, InterruptedException
{    Vector userRatings = vectorWritable.get();    int column = TasteHadoopUtils.idToIndex(rowIndex.get());    itemVectorWritable.setWritesLaxPrecision(true);    Vector itemVector = new RandomAccessSparseVector(Integer.MAX_VALUE, 1);    for (Vector.Element elem : userRatings.nonZeroes()) {        itemID.set(elem.index());        itemVector.setQuick(column, elem.get());        itemVectorWritable.set(itemVector);        ctx.write(itemID, itemVectorWritable);                itemVector.setQuick(elem.index(), 0.0);    }}
0
protected void reduce(IntWritable row, Iterable<VectorWritable> vectors, Context ctx) throws IOException, InterruptedException
{    merged.setWritesLaxPrecision(true);    merged.set(VectorWritable.mergeToVector(vectors.iterator()));    ctx.write(row, merged);}
0
public List<RecommendedItem> getRecommendedItems()
{    return recommended;}
0
public void set(List<RecommendedItem> recommended)
{    this.recommended = recommended;}
0
public void write(DataOutput out) throws IOException
{    out.writeInt(recommended.size());    for (RecommendedItem item : recommended) {        Varint.writeSignedVarLong(item.getItemID(), out);        out.writeFloat(item.getValue());    }}
0
public void readFields(DataInput in) throws IOException
{    int size = in.readInt();    recommended = new ArrayList<>(size);    for (int i = 0; i < size; i++) {        long itemID = Varint.readSignedVarLong(in);        float value = in.readFloat();        RecommendedItem recommendedItem = new GenericRecommendedItem(itemID, value);        recommended.add(recommendedItem);    }}
0
public String toString()
{    StringBuilder result = new StringBuilder(200);    result.append('[');    boolean first = true;    for (RecommendedItem item : recommended) {        if (first) {            first = false;        } else {            result.append(',');        }        result.append(String.valueOf(item.getItemID()));        result.append(':');        result.append(String.valueOf(item.getValue()));    }    result.append(']');    return result.toString();}
0
public static void main(String[] args) throws Exception
{    ToolRunner.run(new ItemSimilarityJob(), args);}
0
public int run(String[] args) throws Exception
{    addInputOption();    addOutputOption();    addOption("similarityClassname", "s", "Name of distributed similarity measures class to instantiate, " + "alternatively use one of the predefined similarities (" + VectorSimilarityMeasures.list() + ')');    addOption("maxSimilaritiesPerItem", "m", "try to cap the number of similar items per item to this number " + "(default: " + DEFAULT_MAX_SIMILAR_ITEMS_PER_ITEM + ')', String.valueOf(DEFAULT_MAX_SIMILAR_ITEMS_PER_ITEM));    addOption("maxPrefs", "mppu", "max number of preferences to consider per user or item, " + "users or items with more preferences will be sampled down (default: " + DEFAULT_MAX_PREFS + ')', String.valueOf(DEFAULT_MAX_PREFS));    addOption("minPrefsPerUser", "mp", "ignore users with less preferences than this " + "(default: " + DEFAULT_MIN_PREFS_PER_USER + ')', String.valueOf(DEFAULT_MIN_PREFS_PER_USER));    addOption("booleanData", "b", "Treat input as without pref values", String.valueOf(Boolean.FALSE));    addOption("threshold", "tr", "discard item pairs with a similarity value below this", false);    addOption("randomSeed", null, "use this seed for sampling", false);    Map<String, List<String>> parsedArgs = parseArguments(args);    if (parsedArgs == null) {        return -1;    }    String similarityClassName = getOption("similarityClassname");    int maxSimilarItemsPerItem = Integer.parseInt(getOption("maxSimilaritiesPerItem"));    int maxPrefs = Integer.parseInt(getOption("maxPrefs"));    int minPrefsPerUser = Integer.parseInt(getOption("minPrefsPerUser"));    boolean booleanData = Boolean.valueOf(getOption("booleanData"));    double threshold = hasOption("threshold") ? Double.parseDouble(getOption("threshold")) : RowSimilarityJob.NO_THRESHOLD;    long randomSeed = hasOption("randomSeed") ? Long.parseLong(getOption("randomSeed")) : RowSimilarityJob.NO_FIXED_RANDOM_SEED;    Path similarityMatrixPath = getTempPath("similarityMatrix");    Path prepPath = getTempPath("prepareRatingMatrix");    AtomicInteger currentPhase = new AtomicInteger();    if (shouldRunNextPhase(parsedArgs, currentPhase)) {        ToolRunner.run(getConf(), new PreparePreferenceMatrixJob(), new String[] { "--input", getInputPath().toString(), "--output", prepPath.toString(), "--minPrefsPerUser", String.valueOf(minPrefsPerUser), "--booleanData", String.valueOf(booleanData), "--tempDir", getTempPath().toString() });    }    if (shouldRunNextPhase(parsedArgs, currentPhase)) {        int numberOfUsers = HadoopUtil.readInt(new Path(prepPath, PreparePreferenceMatrixJob.NUM_USERS), getConf());        ToolRunner.run(getConf(), new RowSimilarityJob(), new String[] { "--input", new Path(prepPath, PreparePreferenceMatrixJob.RATING_MATRIX).toString(), "--output", similarityMatrixPath.toString(), "--numberOfColumns", String.valueOf(numberOfUsers), "--similarityClassname", similarityClassName, "--maxObservationsPerRow", String.valueOf(maxPrefs), "--maxObservationsPerColumn", String.valueOf(maxPrefs), "--maxSimilaritiesPerRow", String.valueOf(maxSimilarItemsPerItem), "--excludeSelfSimilarity", String.valueOf(Boolean.TRUE), "--threshold", String.valueOf(threshold), "--randomSeed", String.valueOf(randomSeed), "--tempDir", getTempPath().toString() });    }    if (shouldRunNextPhase(parsedArgs, currentPhase)) {        Job mostSimilarItems = prepareJob(similarityMatrixPath, getOutputPath(), SequenceFileInputFormat.class, MostSimilarItemPairsMapper.class, EntityEntityWritable.class, DoubleWritable.class, MostSimilarItemPairsReducer.class, EntityEntityWritable.class, DoubleWritable.class, TextOutputFormat.class);        Configuration mostSimilarItemsConf = mostSimilarItems.getConfiguration();        mostSimilarItemsConf.set(ITEM_ID_INDEX_PATH_STR, new Path(prepPath, PreparePreferenceMatrixJob.ITEMID_INDEX).toString());        mostSimilarItemsConf.setInt(MAX_SIMILARITIES_PER_ITEM, maxSimilarItemsPerItem);        boolean succeeded = mostSimilarItems.waitForCompletion(true);        if (!succeeded) {            return -1;        }    }    return 0;}
0
protected void setup(Context ctx)
{    Configuration conf = ctx.getConfiguration();    maxSimilarItemsPerItem = conf.getInt(MAX_SIMILARITIES_PER_ITEM, -1);    indexItemIDMap = TasteHadoopUtils.readIDIndexMap(conf.get(ITEM_ID_INDEX_PATH_STR), conf);    Preconditions.checkArgument(maxSimilarItemsPerItem > 0, "maxSimilarItemsPerItem must be greater then 0!");}
0
protected void map(IntWritable itemIDIndexWritable, VectorWritable similarityVector, Context ctx) throws IOException, InterruptedException
{    int itemIDIndex = itemIDIndexWritable.get();    TopSimilarItemsQueue topKMostSimilarItems = new TopSimilarItemsQueue(maxSimilarItemsPerItem);    for (Vector.Element element : similarityVector.get().nonZeroes()) {        SimilarItem top = topKMostSimilarItems.top();        double candidateSimilarity = element.get();        if (candidateSimilarity > top.getSimilarity()) {            top.set(indexItemIDMap.get(element.index()), candidateSimilarity);            topKMostSimilarItems.updateTop();        }    }    long itemID = indexItemIDMap.get(itemIDIndex);    for (SimilarItem similarItem : topKMostSimilarItems.getTopItems()) {        long otherItemID = similarItem.getItemID();        if (itemID < otherItemID) {            ctx.write(new EntityEntityWritable(itemID, otherItemID), new DoubleWritable(similarItem.getSimilarity()));        } else {            ctx.write(new EntityEntityWritable(otherItemID, itemID), new DoubleWritable(similarItem.getSimilarity()));        }    }}
0
protected void reduce(EntityEntityWritable pair, Iterable<DoubleWritable> values, Context ctx) throws IOException, InterruptedException
{    ctx.write(pair, values.iterator().next());}
0
public List<SimilarItem> getTopItems()
{    List<SimilarItem> items = new ArrayList<>(maxSize);    while (size() > 0) {        SimilarItem topItem = pop();                if (topItem.getItemID() != SENTINEL_ID) {            items.add(topItem);        }    }    Collections.reverse(items);    return items;}
0
protected boolean lessThan(SimilarItem one, SimilarItem two)
{    return one.getSimilarity() < two.getSimilarity();}
0
protected SimilarItem getSentinelObject()
{    return new SimilarItem(SENTINEL_ID, Double.MIN_VALUE);}
0
public static String[] splitPrefTokens(CharSequence line)
{    return PREFERENCE_TOKEN_DELIMITER.split(line);}
0
public static int idToIndex(long id)
{    return 0x7FFFFFFF & Longs.hashCode(id) % 0x7FFFFFFE;}
0
public static int readID(String token, boolean usesLongIDs)
{    return usesLongIDs ? idToIndex(Long.parseLong(token)) : Integer.parseInt(token);}
0
public static OpenIntLongHashMap readIDIndexMap(String idIndexPathStr, Configuration conf)
{    OpenIntLongHashMap indexIDMap = new OpenIntLongHashMap();    Path itemIDIndexPath = new Path(idIndexPathStr);    for (Pair<VarIntWritable, VarLongWritable> record : new SequenceFileDirIterable<VarIntWritable, VarLongWritable>(itemIDIndexPath, PathType.LIST, PathFilters.partFilter(), null, true, conf)) {        indexIDMap.put(record.getFirst().get(), record.getSecond().get());    }    return indexIDMap;}
0
protected void setup(Context context)
{    Configuration jobConf = context.getConfiguration();    booleanData = jobConf.getBoolean(RecommenderJob.BOOLEAN_DATA, false);    transpose = jobConf.getBoolean(TRANSPOSE_USER_ITEM, false);    ratingShift = Float.parseFloat(jobConf.get(RATING_SHIFT, "0.0"));}
0
public void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException
{    String[] tokens = DELIMITER.split(value.toString());    long userID = Long.parseLong(tokens[0]);    long itemID = Long.parseLong(tokens[1]);    if (itemKey ^ transpose) {                                long temp = userID;        userID = itemID;        itemID = temp;    }    if (booleanData) {        context.write(new VarLongWritable(userID), new VarLongWritable(itemID));    } else {        float prefValue = tokens.length > 2 ? Float.parseFloat(tokens[2]) + ratingShift : 1.0f;        context.write(new VarLongWritable(userID), new EntityPrefWritable(itemID, prefValue));    }}
0
public List<RecommendedItem> getTopItems()
{    List<RecommendedItem> recommendedItems = new ArrayList<>(maxSize);    while (size() > 0) {        MutableRecommendedItem topItem = pop();                if (topItem.getItemID() != SENTINEL_ID) {            recommendedItems.add(topItem);        }    }    Collections.reverse(recommendedItems);    return recommendedItems;}
0
protected boolean lessThan(MutableRecommendedItem one, MutableRecommendedItem two)
{    return one.getValue() < two.getValue();}
0
protected MutableRecommendedItem getSentinelObject()
{    return new MutableRecommendedItem(SENTINEL_ID, Float.MIN_VALUE);}
0
public Long next()
{    return nextLong();}
0
 boolean get(int index)
{        return (bits[index >>> 6] & 1L << (index & 0x3F)) != 0L;}
0
 void set(int index)
{        bits[index >>> 6] |= 1L << (index & 0x3F);}
0
 void clear(int index)
{        bits[index >>> 6] &= ~(1L << (index & 0x3F));}
0
 void clear()
{    int length = bits.length;    for (int i = 0; i < length; i++) {        bits[i] = 0L;    }}
0
public BitSet clone()
{    return new BitSet(bits.clone());}
0
public int hashCode()
{    return Arrays.hashCode(bits);}
0
public boolean equals(Object o)
{    if (!(o instanceof BitSet)) {        return false;    }    BitSet other = (BitSet) o;    return Arrays.equals(bits, other.bits);}
0
public String toString()
{    StringBuilder result = new StringBuilder(64 * bits.length);    for (long l : bits) {        for (int j = 0; j < 64; j++) {            result.append((l & 1L << j) == 0 ? '0' : '1');        }        result.append(' ');    }    return result.toString();}
0
public V get(K key) throws TasteException
{    V value;    synchronized (cache) {        value = cache.get(key);    }    if (value == null) {        return getAndCacheValue(key);    }    return value == NULL ? null : value;}
0
public void remove(K key)
{    synchronized (cache) {        cache.remove(key);    }}
0
public void removeKeysMatching(MatchPredicate<K> predicate)
{    synchronized (cache) {        Iterator<K> it = cache.keySet().iterator();        while (it.hasNext()) {            K key = it.next();            if (predicate.matches(key)) {                it.remove();            }        }    }}
0
public void removeValueMatching(MatchPredicate<V> predicate)
{    synchronized (cache) {        Iterator<V> it = cache.values().iterator();        while (it.hasNext()) {            V value = it.next();            if (predicate.matches(value)) {                it.remove();            }        }    }}
0
public void clear()
{    synchronized (cache) {        cache.clear();    }}
0
private V getAndCacheValue(K key) throws TasteException
{    V value = retriever.get(key);    if (value == null) {        value = (V) NULL;    }    synchronized (cache) {        cache.put(key, value);    }    return value;}
0
public String toString()
{    return "Cache[retriever:" + retriever + ']';}
0
private int find(long key)
{        int theHashCode = (int) key & 0x7FFFFFFF;    long[] keys = this.keys;    int hashSize = keys.length;    int jump = 1 + theHashCode % (hashSize - 2);    int index = theHashCode % hashSize;    long currentKey = keys[index];    while (currentKey != NULL && key != currentKey) {        index -= index < jump ? jump - hashSize : jump;        currentKey = keys[index];    }    return index;}
0
private int findForAdd(long key)
{        int theHashCode = (int) key & 0x7FFFFFFF;    long[] keys = this.keys;    int hashSize = keys.length;    int jump = 1 + theHashCode % (hashSize - 2);    int index = theHashCode % hashSize;    long currentKey = keys[index];    while (currentKey != NULL && currentKey != REMOVED && key != currentKey) {        index -= index < jump ? jump - hashSize : jump;        currentKey = keys[index];    }    if (currentKey != REMOVED) {        return index;    }        int addIndex = index;    while (currentKey != NULL && key != currentKey) {        index -= index < jump ? jump - hashSize : jump;        currentKey = keys[index];    }    return key == currentKey ? index : addIndex;}
0
public V get(long key)
{    if (key == NULL) {        return null;    }    int index = find(key);    if (countingAccesses) {        recentlyAccessed.set(index);    }    return values[index];}
0
public int size()
{    return numEntries;}
0
public boolean isEmpty()
{    return numEntries == 0;}
0
public boolean containsKey(long key)
{    return key != NULL && key != REMOVED && keys[find(key)] != NULL;}
0
public boolean containsValue(Object value)
{    if (value == null) {        return false;    }    for (V theValue : values) {        if (theValue != null && value.equals(theValue)) {            return true;        }    }    return false;}
0
public V put(long key, V value)
{    Preconditions.checkArgument(key != NULL && key != REMOVED);    Preconditions.checkNotNull(value);        if (numSlotsUsed * loadFactor >= keys.length) {                if (numEntries * loadFactor >= numSlotsUsed) {            growAndRehash();        } else {                        rehash();        }    }        int index = findForAdd(key);    long keyIndex = keys[index];    if (keyIndex == key) {        V oldValue = values[index];        values[index] = value;        return oldValue;    }        if (countingAccesses && numEntries >= maxSize) {                clearStaleEntry(index);    }    keys[index] = key;    values[index] = value;    numEntries++;    if (keyIndex == NULL) {        numSlotsUsed++;    }    return null;}
0
private void clearStaleEntry(int index)
{    while (true) {        long currentKey;        do {            if (index == 0) {                index = keys.length - 1;            } else {                index--;            }            currentKey = keys[index];        } while (currentKey == NULL || currentKey == REMOVED);        if (recentlyAccessed.get(index)) {            recentlyAccessed.clear(index);        } else {            break;        }    }        keys[index] = REMOVED;    numEntries--;    values[index] = null;}
0
public V remove(long key)
{    if (key == NULL || key == REMOVED) {        return null;    }    int index = find(key);    if (keys[index] == NULL) {        return null;    } else {        keys[index] = REMOVED;        numEntries--;        V oldValue = values[index];        values[index] = null;                return oldValue;    }}
0
public void clear()
{    numEntries = 0;    numSlotsUsed = 0;    Arrays.fill(keys, NULL);    Arrays.fill(values, null);    if (countingAccesses) {        recentlyAccessed.clear();    }}
0
public LongPrimitiveIterator keySetIterator()
{    return new KeyIterator();}
0
public Set<Map.Entry<Long, V>> entrySet()
{    return new EntrySet();}
0
public Collection<V> values()
{    return new ValueCollection();}
0
public void rehash()
{    rehash(RandomUtils.nextTwinPrime((int) (loadFactor * numEntries)));}
0
private void growAndRehash()
{    if (keys.length * loadFactor >= RandomUtils.MAX_INT_SMALLER_TWIN_PRIME) {        throw new IllegalStateException("Can't grow any more");    }    rehash(RandomUtils.nextTwinPrime((int) (loadFactor * keys.length)));}
0
private void rehash(int newHashSize)
{    long[] oldKeys = keys;    V[] oldValues = values;    numEntries = 0;    numSlotsUsed = 0;    if (countingAccesses) {        recentlyAccessed = new BitSet(newHashSize);    }    keys = new long[newHashSize];    Arrays.fill(keys, NULL);    values = (V[]) new Object[newHashSize];    int length = oldKeys.length;    for (int i = 0; i < length; i++) {        long key = oldKeys[i];        if (key != NULL && key != REMOVED) {            put(key, oldValues[i]);        }    }}
0
 void iteratorRemove(int lastNext)
{    if (lastNext >= values.length) {        throw new NoSuchElementException();    }    if (lastNext < 0) {        throw new IllegalStateException();    }    values[lastNext] = null;    keys[lastNext] = REMOVED;    numEntries--;}
0
public FastByIDMap<V> clone()
{    FastByIDMap<V> clone;    try {        clone = (FastByIDMap<V>) super.clone();    } catch (CloneNotSupportedException cnse) {        throw new AssertionError();    }    clone.keys = keys.clone();    clone.values = values.clone();    clone.recentlyAccessed = countingAccesses ? new BitSet(keys.length) : null;    return clone;}
0
public String toString()
{    if (isEmpty()) {        return "{}";    }    StringBuilder result = new StringBuilder();    result.append('{');    for (int i = 0; i < keys.length; i++) {        long key = keys[i];        if (key != NULL && key != REMOVED) {            result.append(key).append('=').append(values[i]).append(',');        }    }    result.setCharAt(result.length() - 1, '}');    return result.toString();}
0
public int hashCode()
{    int hash = 0;    long[] keys = this.keys;    int max = keys.length;    for (int i = 0; i < max; i++) {        long key = keys[i];        if (key != NULL && key != REMOVED) {            hash = 31 * hash + ((int) (key >> 32) ^ (int) key);            hash = 31 * hash + values[i].hashCode();        }    }    return hash;}
0
public boolean equals(Object other)
{    if (!(other instanceof FastByIDMap)) {        return false;    }    FastByIDMap<V> otherMap = (FastByIDMap<V>) other;    long[] otherKeys = otherMap.keys;    V[] otherValues = otherMap.values;    int length = keys.length;    int otherLength = otherKeys.length;    int max = Math.min(length, otherLength);    int i = 0;    while (i < max) {        long key = keys[i];        long otherKey = otherKeys[i];        if (key == NULL || key == REMOVED) {            if (otherKey != NULL && otherKey != REMOVED) {                return false;            }        } else {            if (key != otherKey || !values[i].equals(otherValues[i])) {                return false;            }        }        i++;    }    while (i < length) {        long key = keys[i];        if (key != NULL && key != REMOVED) {            return false;        }        i++;    }    while (i < otherLength) {        long key = otherKeys[i];        if (key != NULL && key != REMOVED) {            return false;        }        i++;    }    return true;}
0
public boolean hasNext()
{    goToNext();    return position < keys.length;}
0
public long nextLong()
{    goToNext();    lastNext = position;    if (position >= keys.length) {        throw new NoSuchElementException();    }    return keys[position++];}
0
public long peek()
{    goToNext();    if (position >= keys.length) {        throw new NoSuchElementException();    }    return keys[position];}
0
private void goToNext()
{    int length = values.length;    while (position < length && values[position] == null) {        position++;    }}
0
public void remove()
{    iteratorRemove(lastNext);}
0
public void skip(int n)
{    position += n;}
0
public int size()
{    return FastByIDMap.this.size();}
0
public boolean isEmpty()
{    return FastByIDMap.this.isEmpty();}
0
public boolean contains(Object o)
{    return containsKey((Long) o);}
0
public Iterator<Map.Entry<Long, V>> iterator()
{    return new EntryIterator();}
0
public boolean add(Map.Entry<Long, V> t)
{    throw new UnsupportedOperationException();}
0
public boolean remove(Object o)
{    throw new UnsupportedOperationException();}
0
public boolean addAll(Collection<? extends Map.Entry<Long, V>> ts)
{    throw new UnsupportedOperationException();}
0
public boolean retainAll(Collection<?> objects)
{    throw new UnsupportedOperationException();}
0
public boolean removeAll(Collection<?> objects)
{    throw new UnsupportedOperationException();}
0
public void clear()
{    FastByIDMap.this.clear();}
0
public Long getKey()
{    return keys[index];}
0
public V getValue()
{    return values[index];}
0
public V setValue(V value)
{    Preconditions.checkArgument(value != null);    V oldValue = values[index];    values[index] = value;    return oldValue;}
0
public boolean hasNext()
{    goToNext();    return position < keys.length;}
0
public Map.Entry<Long, V> next()
{    goToNext();    lastNext = position;    if (position >= keys.length) {        throw new NoSuchElementException();    }    return new MapEntry(position++);}
0
private void goToNext()
{    int length = values.length;    while (position < length && values[position] == null) {        position++;    }}
0
public void remove()
{    iteratorRemove(lastNext);}
0
public int size()
{    return FastByIDMap.this.size();}
0
public boolean isEmpty()
{    return FastByIDMap.this.isEmpty();}
0
public boolean contains(Object o)
{    return containsValue(o);}
0
public Iterator<V> iterator()
{    return new ValueIterator();}
0
public boolean add(V v)
{    throw new UnsupportedOperationException();}
0
public boolean remove(Object o)
{    throw new UnsupportedOperationException();}
0
public boolean addAll(Collection<? extends V> vs)
{    throw new UnsupportedOperationException();}
0
public boolean removeAll(Collection<?> objects)
{    throw new UnsupportedOperationException();}
0
public boolean retainAll(Collection<?> objects)
{    throw new UnsupportedOperationException();}
0
public void clear()
{    FastByIDMap.this.clear();}
0
public boolean hasNext()
{    goToNext();    return position < values.length;}
0
public V next()
{    goToNext();    lastNext = position;    if (position >= values.length) {        throw new NoSuchElementException();    }    return values[position++];}
0
private void goToNext()
{    int length = values.length;    while (position < length && values[position] == null) {        position++;    }}
0
public void remove()
{    iteratorRemove(lastNext);}
0
private int find(long key)
{        int theHashCode = (int) key & 0x7FFFFFFF;    long[] keys = this.keys;    int hashSize = keys.length;    int jump = 1 + theHashCode % (hashSize - 2);    int index = theHashCode % hashSize;    long currentKey = keys[index];    while (currentKey != NULL && key != currentKey) {                index -= index < jump ? jump - hashSize : jump;        currentKey = keys[index];    }    return index;}
0
private int findForAdd(long key)
{        int theHashCode = (int) key & 0x7FFFFFFF;    long[] keys = this.keys;    int hashSize = keys.length;    int jump = 1 + theHashCode % (hashSize - 2);    int index = theHashCode % hashSize;    long currentKey = keys[index];    while (currentKey != NULL && currentKey != REMOVED && key != currentKey) {        index -= index < jump ? jump - hashSize : jump;        currentKey = keys[index];    }    if (currentKey != REMOVED) {        return index;    }        int addIndex = index;    while (currentKey != NULL && key != currentKey) {        index -= index < jump ? jump - hashSize : jump;        currentKey = keys[index];    }    return key == currentKey ? index : addIndex;}
0
public int size()
{    return numEntries;}
0
public boolean isEmpty()
{    return numEntries == 0;}
0
public boolean contains(long key)
{    return key != NULL && key != REMOVED && keys[find(key)] != NULL;}
0
public boolean add(long key)
{    Preconditions.checkArgument(key != NULL && key != REMOVED);        if (numSlotsUsed * loadFactor >= keys.length) {                if (numEntries * loadFactor >= numSlotsUsed) {            growAndRehash();        } else {                        rehash();        }    }        int index = findForAdd(key);    long keyIndex = keys[index];    if (keyIndex != key) {        keys[index] = key;        numEntries++;        if (keyIndex == NULL) {            numSlotsUsed++;        }        return true;    }    return false;}
0
public LongPrimitiveIterator iterator()
{    return new KeyIterator();}
0
public long[] toArray()
{    long[] result = new long[numEntries];    for (int i = 0, position = 0; i < result.length; i++) {        while (keys[position] == NULL || keys[position] == REMOVED) {            position++;        }        result[i] = keys[position++];    }    return result;}
0
public boolean remove(long key)
{    if (key == NULL || key == REMOVED) {        return false;    }    int index = find(key);    if (keys[index] == NULL) {        return false;    } else {        keys[index] = REMOVED;        numEntries--;        return true;    }}
0
public boolean addAll(long[] c)
{    boolean changed = false;    for (long k : c) {        if (add(k)) {            changed = true;        }    }    return changed;}
0
public boolean addAll(FastIDSet c)
{    boolean changed = false;    for (long k : c.keys) {        if (k != NULL && k != REMOVED && add(k)) {            changed = true;        }    }    return changed;}
0
public boolean removeAll(long[] c)
{    boolean changed = false;    for (long o : c) {        if (remove(o)) {            changed = true;        }    }    return changed;}
0
public boolean removeAll(FastIDSet c)
{    boolean changed = false;    for (long k : c.keys) {        if (k != NULL && k != REMOVED && remove(k)) {            changed = true;        }    }    return changed;}
0
public boolean retainAll(FastIDSet c)
{    boolean changed = false;    for (int i = 0; i < keys.length; i++) {        long k = keys[i];        if (k != NULL && k != REMOVED && !c.contains(k)) {            keys[i] = REMOVED;            numEntries--;            changed = true;        }    }    return changed;}
0
public void clear()
{    numEntries = 0;    numSlotsUsed = 0;    Arrays.fill(keys, NULL);}
0
private void growAndRehash()
{    if (keys.length * loadFactor >= RandomUtils.MAX_INT_SMALLER_TWIN_PRIME) {        throw new IllegalStateException("Can't grow any more");    }    rehash(RandomUtils.nextTwinPrime((int) (loadFactor * keys.length)));}
0
public void rehash()
{    rehash(RandomUtils.nextTwinPrime((int) (loadFactor * numEntries)));}
0
private void rehash(int newHashSize)
{    long[] oldKeys = keys;    numEntries = 0;    numSlotsUsed = 0;    keys = new long[newHashSize];    Arrays.fill(keys, NULL);    for (long key : oldKeys) {        if (key != NULL && key != REMOVED) {            add(key);        }    }}
0
public int intersectionSize(FastIDSet other)
{    int count = 0;    for (long key : other.keys) {        if (key != NULL && key != REMOVED && keys[find(key)] != NULL) {            count++;        }    }    return count;}
0
public FastIDSet clone()
{    FastIDSet clone;    try {        clone = (FastIDSet) super.clone();    } catch (CloneNotSupportedException cnse) {        throw new AssertionError();    }    clone.keys = keys.clone();    return clone;}
0
public int hashCode()
{    int hash = 0;    long[] keys = this.keys;    for (long key : keys) {        if (key != NULL && key != REMOVED) {            hash = 31 * hash + ((int) (key >> 32) ^ (int) key);        }    }    return hash;}
0
public boolean equals(Object other)
{    if (!(other instanceof FastIDSet)) {        return false;    }    FastIDSet otherMap = (FastIDSet) other;    long[] otherKeys = otherMap.keys;    int length = keys.length;    int otherLength = otherKeys.length;    int max = Math.min(length, otherLength);    int i = 0;    while (i < max) {        long key = keys[i];        long otherKey = otherKeys[i];        if (key == NULL || key == REMOVED) {            if (otherKey != NULL && otherKey != REMOVED) {                return false;            }        } else {            if (key != otherKey) {                return false;            }        }        i++;    }    while (i < length) {        long key = keys[i];        if (key != NULL && key != REMOVED) {            return false;        }        i++;    }    while (i < otherLength) {        long key = otherKeys[i];        if (key != NULL && key != REMOVED) {            return false;        }        i++;    }    return true;}
0
public String toString()
{    if (isEmpty()) {        return "[]";    }    StringBuilder result = new StringBuilder();    result.append('[');    for (long key : keys) {        if (key != NULL && key != REMOVED) {            result.append(key).append(',');        }    }    result.setCharAt(result.length() - 1, ']');    return result.toString();}
0
public boolean hasNext()
{    goToNext();    return position < keys.length;}
0
public long nextLong()
{    goToNext();    lastNext = position;    if (position >= keys.length) {        throw new NoSuchElementException();    }    return keys[position++];}
0
public long peek()
{    goToNext();    if (position >= keys.length) {        throw new NoSuchElementException();    }    return keys[position];}
0
private void goToNext()
{    int length = keys.length;    while (position < length && (keys[position] == NULL || keys[position] == REMOVED)) {        position++;    }}
0
public void remove()
{    if (lastNext >= keys.length) {        throw new NoSuchElementException();    }    if (lastNext < 0) {        throw new IllegalStateException();    }    keys[lastNext] = REMOVED;    numEntries--;}
0
public Iterator<Long> iterator()
{    return new KeyIterator();}
0
public void skip(int n)
{    position += n;}
0
private int find(Object key)
{        int theHashCode = key.hashCode() & 0x7FFFFFFF;    K[] keys = this.keys;    int hashSize = keys.length;    int jump = 1 + theHashCode % (hashSize - 2);    int index = theHashCode % hashSize;    K currentKey = keys[index];    while (currentKey != null && !key.equals(currentKey)) {        index -= index < jump ? jump - hashSize : jump;        currentKey = keys[index];    }    return index;}
0
private int findForAdd(Object key)
{        int theHashCode = key.hashCode() & 0x7FFFFFFF;    K[] keys = this.keys;    int hashSize = keys.length;    int jump = 1 + theHashCode % (hashSize - 2);    int index = theHashCode % hashSize;    K currentKey = keys[index];    while (currentKey != null && currentKey != REMOVED && key != currentKey) {        index -= index < jump ? jump - hashSize : jump;        currentKey = keys[index];    }    if (currentKey != REMOVED) {        return index;    }        int addIndex = index;    while (currentKey != null && key != currentKey) {        index -= index < jump ? jump - hashSize : jump;        currentKey = keys[index];    }    return key == currentKey ? index : addIndex;}
0
public V get(Object key)
{    if (key == null) {        return null;    }    int index = find(key);    if (countingAccesses) {        recentlyAccessed.set(index);    }    return values[index];}
0
public int size()
{    return numEntries;}
0
public boolean isEmpty()
{    return numEntries == 0;}
0
public boolean containsKey(Object key)
{    return key != null && keys[find(key)] != null;}
0
public boolean containsValue(Object value)
{    if (value == null) {        return false;    }    for (V theValue : values) {        if (theValue != null && value.equals(theValue)) {            return true;        }    }    return false;}
0
public V put(K key, V value)
{    Preconditions.checkNotNull(key);    Preconditions.checkNotNull(value);        if (numSlotsUsed * loadFactor >= keys.length) {                if (numEntries * loadFactor >= numSlotsUsed) {            growAndRehash();        } else {                        rehash();        }    }        int index = findForAdd(key);    if (keys[index] == key) {        V oldValue = values[index];        values[index] = value;        return oldValue;    }        if (countingAccesses && numEntries >= maxSize) {                clearStaleEntry(index);    }    keys[index] = key;    values[index] = value;    numEntries++;    numSlotsUsed++;    return null;}
0
private void clearStaleEntry(int index)
{    while (true) {        K currentKey;        do {            if (index == 0) {                index = keys.length - 1;            } else {                index--;            }            currentKey = keys[index];        } while (currentKey == null || currentKey == REMOVED);        if (recentlyAccessed.get(index)) {            recentlyAccessed.clear(index);        } else {            break;        }    }        ((Object[]) keys)[index] = REMOVED;    numEntries--;    values[index] = null;}
0
public void putAll(Map<? extends K, ? extends V> map)
{    for (Entry<? extends K, ? extends V> entry : map.entrySet()) {        put(entry.getKey(), entry.getValue());    }}
0
public V remove(Object key)
{    if (key == null) {        return null;    }    int index = find(key);    if (keys[index] == null) {        return null;    } else {        ((Object[]) keys)[index] = REMOVED;        numEntries--;        V oldValue = values[index];        values[index] = null;                return oldValue;    }}
0
public void clear()
{    numEntries = 0;    numSlotsUsed = 0;    Arrays.fill(keys, null);    Arrays.fill(values, null);    if (countingAccesses) {        recentlyAccessed.clear();    }}
0
public Set<K> keySet()
{    return new KeySet();}
0
public Collection<V> values()
{    return new ValueCollection();}
0
public Set<Entry<K, V>> entrySet()
{    return new EntrySet();}
0
public void rehash()
{    rehash(RandomUtils.nextTwinPrime((int) (loadFactor * numEntries)));}
0
private void growAndRehash()
{    if (keys.length * loadFactor >= RandomUtils.MAX_INT_SMALLER_TWIN_PRIME) {        throw new IllegalStateException("Can't grow any more");    }    rehash(RandomUtils.nextTwinPrime((int) (loadFactor * keys.length)));}
0
private void rehash(int newHashSize)
{    K[] oldKeys = keys;    V[] oldValues = values;    numEntries = 0;    numSlotsUsed = 0;    if (countingAccesses) {        recentlyAccessed = new BitSet(newHashSize);    }    keys = (K[]) new Object[newHashSize];    values = (V[]) new Object[newHashSize];    int length = oldKeys.length;    for (int i = 0; i < length; i++) {        K key = oldKeys[i];        if (key != null && key != REMOVED) {            put(key, oldValues[i]);        }    }}
0
 void iteratorRemove(int lastNext)
{    if (lastNext >= values.length) {        throw new NoSuchElementException();    }    if (lastNext < 0) {        throw new IllegalStateException();    }    values[lastNext] = null;    ((Object[]) keys)[lastNext] = REMOVED;    numEntries--;}
0
public FastMap<K, V> clone()
{    FastMap<K, V> clone;    try {        clone = (FastMap<K, V>) super.clone();    } catch (CloneNotSupportedException cnse) {        throw new AssertionError();    }    clone.keys = keys.clone();    clone.values = values.clone();    clone.recentlyAccessed = countingAccesses ? new BitSet(keys.length) : null;    return clone;}
0
public int hashCode()
{    int hash = 0;    K[] keys = this.keys;    int max = keys.length;    for (int i = 0; i < max; i++) {        K key = keys[i];        if (key != null && key != REMOVED) {            hash = 31 * hash + key.hashCode();            hash = 31 * hash + values[i].hashCode();        }    }    return hash;}
0
public boolean equals(Object other)
{    if (!(other instanceof FastMap)) {        return false;    }    FastMap<K, V> otherMap = (FastMap<K, V>) other;    K[] otherKeys = otherMap.keys;    V[] otherValues = otherMap.values;    int length = keys.length;    int otherLength = otherKeys.length;    int max = Math.min(length, otherLength);    int i = 0;    while (i < max) {        K key = keys[i];        K otherKey = otherKeys[i];        if (key == null || key == REMOVED) {            if (otherKey != null && otherKey != REMOVED) {                return false;            }        } else {            if (key != otherKey || !values[i].equals(otherValues[i])) {                return false;            }        }        i++;    }    while (i < length) {        K key = keys[i];        if (key != null && key != REMOVED) {            return false;        }        i++;    }    while (i < otherLength) {        K key = otherKeys[i];        if (key != null && key != REMOVED) {            return false;        }        i++;    }    return true;}
0
public String toString()
{    if (isEmpty()) {        return "{}";    }    StringBuilder result = new StringBuilder();    result.append('{');    for (int i = 0; i < keys.length; i++) {        K key = keys[i];        if (key != null && key != REMOVED) {            result.append(key).append('=').append(values[i]).append(',');        }    }    result.setCharAt(result.length() - 1, '}');    return result.toString();}
0
public int size()
{    return FastMap.this.size();}
0
public boolean isEmpty()
{    return FastMap.this.isEmpty();}
0
public boolean contains(Object o)
{    return containsKey(o);}
0
public Iterator<Entry<K, V>> iterator()
{    return new EntryIterator();}
0
public boolean add(Entry<K, V> t)
{    throw new UnsupportedOperationException();}
0
public boolean remove(Object o)
{    throw new UnsupportedOperationException();}
0
public boolean addAll(Collection<? extends Entry<K, V>> ts)
{    throw new UnsupportedOperationException();}
0
public boolean retainAll(Collection<?> objects)
{    throw new UnsupportedOperationException();}
0
public boolean removeAll(Collection<?> objects)
{    throw new UnsupportedOperationException();}
0
public void clear()
{    FastMap.this.clear();}
0
public K getKey()
{    return keys[index];}
0
public V getValue()
{    return values[index];}
0
public V setValue(V value)
{    Preconditions.checkArgument(value != null);    V oldValue = values[index];    values[index] = value;    return oldValue;}
0
public boolean hasNext()
{    goToNext();    return position < keys.length;}
0
public Entry<K, V> next()
{    goToNext();    lastNext = position;    if (position >= keys.length) {        throw new NoSuchElementException();    }    return new MapEntry(position++);}
0
private void goToNext()
{    int length = values.length;    while (position < length && values[position] == null) {        position++;    }}
0
public void remove()
{    iteratorRemove(lastNext);}
0
public int size()
{    return FastMap.this.size();}
0
public boolean isEmpty()
{    return FastMap.this.isEmpty();}
0
public boolean contains(Object o)
{    return containsKey(o);}
0
public Iterator<K> iterator()
{    return new KeyIterator();}
0
public boolean add(K t)
{    throw new UnsupportedOperationException();}
0
public boolean remove(Object o)
{    throw new UnsupportedOperationException();}
0
public boolean addAll(Collection<? extends K> ts)
{    throw new UnsupportedOperationException();}
0
public boolean retainAll(Collection<?> objects)
{    throw new UnsupportedOperationException();}
0
public boolean removeAll(Collection<?> objects)
{    throw new UnsupportedOperationException();}
0
public void clear()
{    FastMap.this.clear();}
0
public boolean hasNext()
{    goToNext();    return position < keys.length;}
0
public K next()
{    goToNext();    lastNext = position;    if (position >= keys.length) {        throw new NoSuchElementException();    }    return keys[position++];}
0
private void goToNext()
{    int length = values.length;    while (position < length && values[position] == null) {        position++;    }}
0
public void remove()
{    iteratorRemove(lastNext);}
0
public int size()
{    return FastMap.this.size();}
0
public boolean isEmpty()
{    return FastMap.this.isEmpty();}
0
public boolean contains(Object o)
{    return containsValue(o);}
0
public Iterator<V> iterator()
{    return new ValueIterator();}
0
public boolean add(V v)
{    throw new UnsupportedOperationException();}
0
public boolean remove(Object o)
{    throw new UnsupportedOperationException();}
0
public boolean addAll(Collection<? extends V> vs)
{    throw new UnsupportedOperationException();}
0
public boolean removeAll(Collection<?> objects)
{    throw new UnsupportedOperationException();}
0
public boolean retainAll(Collection<?> objects)
{    throw new UnsupportedOperationException();}
0
public void clear()
{    FastMap.this.clear();}
0
public boolean hasNext()
{    goToNext();    return position < values.length;}
0
public V next()
{    goToNext();    lastNext = position;    if (position >= values.length) {        throw new NoSuchElementException();    }    return values[position++];}
0
private void goToNext()
{    int length = values.length;    while (position < length && values[position] == null) {        position++;    }}
0
public void remove()
{    iteratorRemove(lastNext);}
0
public synchronized void addDatum(double datum)
{    throw new UnsupportedOperationException();}
0
public synchronized void removeDatum(double datum)
{    throw new UnsupportedOperationException();}
0
public synchronized void changeDatum(double delta)
{    throw new UnsupportedOperationException();}
0
public synchronized int getCount()
{    return count;}
0
public synchronized double getAverage()
{    return average;}
0
public RunningAverage inverse()
{    return new InvertedRunningAverage(this);}
0
public synchronized String toString()
{    return String.valueOf(average);}
0
public RunningAverageAndStdDev inverse()
{    return new InvertedRunningAverageAndStdDev(this);}
0
public synchronized String toString()
{    return super.toString() + ',' + stdDev;}
0
public double getStandardDeviation()
{    return stdDev;}
0
public synchronized void addDatum(double datum)
{    if (++count == 1) {        average = datum;    } else {        average = average * (count - 1) / count + datum / count;    }}
0
public synchronized void removeDatum(double datum)
{    if (count == 0) {        throw new IllegalStateException();    }    if (--count == 0) {        average = Double.NaN;    } else {        average = average * (count + 1) / count - datum / count;    }}
0
public synchronized void changeDatum(double delta)
{    if (count == 0) {        throw new IllegalStateException();    }    average += delta / count;}
0
public synchronized int getCount()
{    return count;}
0
public synchronized double getAverage()
{    return average;}
0
public RunningAverage inverse()
{    return new InvertedRunningAverage(this);}
0
public synchronized String toString()
{    return String.valueOf(average);}
0
public double getMk()
{    return mk;}
0
public double getSk()
{    return sk;}
0
public synchronized double getStandardDeviation()
{    return stdDev;}
0
public synchronized void addDatum(double datum)
{    super.addDatum(datum);    int count = getCount();    if (count == 1) {        mk = datum;        sk = 0.0;    } else {        double oldmk = mk;        double diff = datum - oldmk;        mk += diff / count;        sk += diff * (datum - mk);    }    recomputeStdDev();}
0
public synchronized void removeDatum(double datum)
{    int oldCount = getCount();    super.removeDatum(datum);    double oldmk = mk;    mk = (oldCount * oldmk - datum) / (oldCount - 1);    sk -= (datum - mk) * (datum - oldmk);    recomputeStdDev();}
0
public void changeDatum(double delta)
{    throw new UnsupportedOperationException();}
0
private synchronized void recomputeStdDev()
{    int count = getCount();    stdDev = count > 1 ? Math.sqrt(sk / (count - 1)) : Double.NaN;}
0
public RunningAverageAndStdDev inverse()
{    return new InvertedRunningAverageAndStdDev(this);}
0
public synchronized String toString()
{    return String.valueOf(String.valueOf(getAverage()) + ',' + stdDev);}
0
public void addDatum(double datum)
{    throw new UnsupportedOperationException();}
0
public void removeDatum(double datum)
{    throw new UnsupportedOperationException();}
0
public void changeDatum(double delta)
{    throw new UnsupportedOperationException();}
0
public int getCount()
{    return delegate.getCount();}
0
public double getAverage()
{    return -delegate.getAverage();}
0
public RunningAverage inverse()
{    return delegate;}
0
public void addDatum(double datum)
{    throw new UnsupportedOperationException();}
0
public void removeDatum(double datum)
{    throw new UnsupportedOperationException();}
0
public void changeDatum(double delta)
{    throw new UnsupportedOperationException();}
0
public int getCount()
{    return delegate.getCount();}
0
public double getAverage()
{    return -delegate.getAverage();}
0
public double getStandardDeviation()
{    return delegate.getStandardDeviation();}
0
public RunningAverageAndStdDev inverse()
{    return delegate;}
0
protected static void checkNotNullAndLog(String argName, Object value)
{    Preconditions.checkArgument(value != null && !value.toString().isEmpty(), argName + " is null or empty");    }
1
protected static void checkNotNullAndLog(String argName, Object[] values)
{    Preconditions.checkArgument(values != null && values.length != 0, argName + " is null or zero-length");    for (Object value : values) {        checkNotNullAndLog(argName, value);    }}
0
protected int getFetchSize()
{    return DEFAULT_FETCH_SIZE;}
0
protected ResultSet computeNext()
{    try {        if (resultSet.next()) {            return resultSet;        } else {            close();            return null;        }    } catch (SQLException sqle) {        close();        throw new IllegalStateException(sqle);    }}
0
public void skip(int n) throws SQLException
{    try {        resultSet.relative(n);    } catch (SQLException sqle) {                int i = 0;        while (i < n && resultSet.next()) {            i++;        }    }}
0
public void close()
{    IOUtils.quietClose(resultSet, statement, connection);    endOfData();}
0
public T apply(ResultSet from)
{    try {        return parseElement(from);    } catch (SQLException sqle) {        throw new IllegalStateException(sqle);    }}
0
protected Iterator<T> delegate()
{    return delegate;}
0
public void skip(int n)
{    if (n >= 1) {        try {            rowDelegate.skip(n);        } catch (SQLException sqle) {            throw new IllegalStateException(sqle);        }    }}
0
public boolean hasNext()
{    return position < max;}
0
public Long next()
{    return nextLong();}
0
public long nextLong()
{    if (position >= array.length) {        throw new NoSuchElementException();    }    return array[position++];}
0
public long peek()
{    if (position >= array.length) {        throw new NoSuchElementException();    }    return array[position];}
0
public void remove()
{    throw new UnsupportedOperationException();}
0
public void skip(int n)
{    if (n > 0) {        position += n;    }}
0
public String toString()
{    return "LongPrimitiveArrayIterator";}
0
public void addDependency(Refreshable refreshable)
{    if (refreshable != null) {        dependencies.add(refreshable);    }}
0
public void removeDependency(Refreshable refreshable)
{    if (refreshable != null) {        dependencies.remove(refreshable);    }}
0
public void refresh(Collection<Refreshable> alreadyRefreshed)
{    if (refreshLock.tryLock()) {        try {            alreadyRefreshed = buildRefreshed(alreadyRefreshed);            for (Refreshable dependency : dependencies) {                maybeRefresh(alreadyRefreshed, dependency);            }            if (refreshRunnable != null) {                try {                    refreshRunnable.call();                } catch (Exception e) {                                    }            }        } finally {            refreshLock.unlock();        }    }}
1
public static Collection<Refreshable> buildRefreshed(Collection<Refreshable> currentAlreadyRefreshed)
{    return currentAlreadyRefreshed == null ? new HashSet<Refreshable>(3) : currentAlreadyRefreshed;}
0
public static void maybeRefresh(Collection<Refreshable> alreadyRefreshed, Refreshable refreshable)
{    if (!alreadyRefreshed.contains(refreshable)) {        alreadyRefreshed.add(refreshable);                refreshable.refresh(alreadyRefreshed);            }}
1
public boolean hasNext()
{    return hasNext;}
0
public long nextLong()
{    if (hasNext) {        long result = next;        doNext();        return result;    }    throw new NoSuchElementException();}
0
public long peek()
{    if (hasNext) {        return next;    }    throw new NoSuchElementException();}
0
private void doNext()
{    int toSkip = geometricDistribution.sample();    delegate.skip(toSkip);    if (delegate.hasNext()) {        next = delegate.next();    } else {        hasNext = false;    }}
0
public void remove()
{    throw new UnsupportedOperationException();}
0
public void skip(int n)
{    int toSkip = 0;    for (int i = 0; i < n; i++) {        toSkip += geometricDistribution.sample();    }    delegate.skip(toSkip);    if (delegate.hasNext()) {        next = delegate.next();    } else {        hasNext = false;    }}
0
public static LongPrimitiveIterator maybeWrapIterator(LongPrimitiveIterator delegate, double samplingRate)
{    return samplingRate >= 1.0 ? delegate : new SamplingLongPrimitiveIterator(delegate, samplingRate);}
0
public synchronized void addDatum(double datum)
{    addDatum(datum, 1.0);}
0
public synchronized void addDatum(double datum, double weight)
{    double oldTotalWeight = totalWeight;    totalWeight += weight;    if (oldTotalWeight <= 0.0) {        average = datum;    } else {        average = average * oldTotalWeight / totalWeight + datum * weight / totalWeight;    }}
0
public synchronized void removeDatum(double datum)
{    removeDatum(datum, 1.0);}
0
public synchronized void removeDatum(double datum, double weight)
{    double oldTotalWeight = totalWeight;    totalWeight -= weight;    if (totalWeight <= 0.0) {        average = Double.NaN;        totalWeight = 0.0;    } else {        average = average * oldTotalWeight / totalWeight - datum * weight / totalWeight;    }}
0
public synchronized void changeDatum(double delta)
{    changeDatum(delta, 1.0);}
0
public synchronized void changeDatum(double delta, double weight)
{    Preconditions.checkArgument(weight <= totalWeight, "weight must be <= totalWeight");    average += delta * weight / totalWeight;}
0
public synchronized double getTotalWeight()
{    return totalWeight;}
0
public synchronized int getCount()
{    return (int) totalWeight;}
0
public synchronized double getAverage()
{    return average;}
0
public RunningAverage inverse()
{    return new InvertedRunningAverage(this);}
0
public synchronized String toString()
{    return String.valueOf(average);}
0
public synchronized void addDatum(double datum, double weight)
{    super.addDatum(datum, weight);    totalSquaredWeight += weight * weight;    double weightedData = datum * weight;    totalWeightedData += weightedData;    totalWeightedSquaredData += weightedData * datum;}
0
public synchronized void removeDatum(double datum, double weight)
{    super.removeDatum(datum, weight);    totalSquaredWeight -= weight * weight;    if (totalSquaredWeight <= 0.0) {        totalSquaredWeight = 0.0;    }    double weightedData = datum * weight;    totalWeightedData -= weightedData;    if (totalWeightedData <= 0.0) {        totalWeightedData = 0.0;    }    totalWeightedSquaredData -= weightedData * datum;    if (totalWeightedSquaredData <= 0.0) {        totalWeightedSquaredData = 0.0;    }}
0
public synchronized void changeDatum(double delta, double weight)
{    throw new UnsupportedOperationException();}
0
public synchronized double getStandardDeviation()
{    double totalWeight = getTotalWeight();    return Math.sqrt((totalWeightedSquaredData * totalWeight - totalWeightedData * totalWeightedData) / (totalWeight * totalWeight - totalSquaredWeight));}
0
public RunningAverageAndStdDev inverse()
{    return new InvertedRunningAverageAndStdDev(this);}
0
public synchronized String toString()
{    return String.valueOf(String.valueOf(getAverage()) + ',' + getStandardDeviation());}
0
public final float getMaxPreference()
{    return maxPreference;}
0
public final void setMaxPreference(float maxPreference)
{    this.maxPreference = maxPreference;}
0
public final float getMinPreference()
{    return minPreference;}
0
public final void setMinPreference(float minPreference)
{    this.minPreference = minPreference;}
0
public double evaluate(RecommenderBuilder recommenderBuilder, DataModelBuilder dataModelBuilder, DataModel dataModel, double trainingPercentage, double evaluationPercentage) throws TasteException
{    Preconditions.checkNotNull(recommenderBuilder);    Preconditions.checkNotNull(dataModel);    Preconditions.checkArgument(trainingPercentage >= 0.0 && trainingPercentage <= 1.0, "Invalid trainingPercentage: " + trainingPercentage + ". Must be: 0.0 <= trainingPercentage <= 1.0");    Preconditions.checkArgument(evaluationPercentage >= 0.0 && evaluationPercentage <= 1.0, "Invalid evaluationPercentage: " + evaluationPercentage + ". Must be: 0.0 <= evaluationPercentage <= 1.0");        int numUsers = dataModel.getNumUsers();    FastByIDMap<PreferenceArray> trainingPrefs = new FastByIDMap<>(1 + (int) (evaluationPercentage * numUsers));    FastByIDMap<PreferenceArray> testPrefs = new FastByIDMap<>(1 + (int) (evaluationPercentage * numUsers));    LongPrimitiveIterator it = dataModel.getUserIDs();    while (it.hasNext()) {        long userID = it.nextLong();        if (random.nextDouble() < evaluationPercentage) {            splitOneUsersPrefs(trainingPercentage, trainingPrefs, testPrefs, userID, dataModel);        }    }    DataModel trainingModel = dataModelBuilder == null ? new GenericDataModel(trainingPrefs) : dataModelBuilder.buildDataModel(trainingPrefs);    Recommender recommender = recommenderBuilder.buildRecommender(trainingModel);    double result = getEvaluation(testPrefs, recommender);        return result;}
1
private void splitOneUsersPrefs(double trainingPercentage, FastByIDMap<PreferenceArray> trainingPrefs, FastByIDMap<PreferenceArray> testPrefs, long userID, DataModel dataModel) throws TasteException
{    List<Preference> oneUserTrainingPrefs = null;    List<Preference> oneUserTestPrefs = null;    PreferenceArray prefs = dataModel.getPreferencesFromUser(userID);    int size = prefs.length();    for (int i = 0; i < size; i++) {        Preference newPref = new GenericPreference(userID, prefs.getItemID(i), prefs.getValue(i));        if (random.nextDouble() < trainingPercentage) {            if (oneUserTrainingPrefs == null) {                oneUserTrainingPrefs = new ArrayList<>(3);            }            oneUserTrainingPrefs.add(newPref);        } else {            if (oneUserTestPrefs == null) {                oneUserTestPrefs = new ArrayList<>(3);            }            oneUserTestPrefs.add(newPref);        }    }    if (oneUserTrainingPrefs != null) {        trainingPrefs.put(userID, new GenericUserPreferenceArray(oneUserTrainingPrefs));        if (oneUserTestPrefs != null) {            testPrefs.put(userID, new GenericUserPreferenceArray(oneUserTestPrefs));        }    }}
0
private float capEstimatedPreference(float estimate)
{    if (estimate > maxPreference) {        return maxPreference;    }    if (estimate < minPreference) {        return minPreference;    }    return estimate;}
0
private double getEvaluation(FastByIDMap<PreferenceArray> testPrefs, Recommender recommender) throws TasteException
{    reset();    Collection<Callable<Void>> estimateCallables = new ArrayList<>();    AtomicInteger noEstimateCounter = new AtomicInteger();    for (Map.Entry<Long, PreferenceArray> entry : testPrefs.entrySet()) {        estimateCallables.add(new PreferenceEstimateCallable(recommender, entry.getKey(), entry.getValue(), noEstimateCounter));    }        RunningAverageAndStdDev timing = new FullRunningAverageAndStdDev();    execute(estimateCallables, noEstimateCounter, timing);    return computeFinalEvaluation();}
1
protected static void execute(Collection<Callable<Void>> callables, AtomicInteger noEstimateCounter, RunningAverageAndStdDev timing) throws TasteException
{    Collection<Callable<Void>> wrappedCallables = wrapWithStatsCallables(callables, noEstimateCounter, timing);    int numProcessors = Runtime.getRuntime().availableProcessors();    ExecutorService executor = Executors.newFixedThreadPool(numProcessors);        try {        List<Future<Void>> futures = executor.invokeAll(wrappedCallables);                for (Future<Void> future : futures) {            future.get();        }    } catch (InterruptedException ie) {        throw new TasteException(ie);    } catch (ExecutionException ee) {        throw new TasteException(ee.getCause());    }    executor.shutdown();    try {        executor.awaitTermination(10, TimeUnit.SECONDS);    } catch (InterruptedException e) {        throw new TasteException(e.getCause());    }}
1
private static Collection<Callable<Void>> wrapWithStatsCallables(Iterable<Callable<Void>> callables, AtomicInteger noEstimateCounter, RunningAverageAndStdDev timing)
{    Collection<Callable<Void>> wrapped = new ArrayList<>();    int count = 0;    for (Callable<Void> callable : callables) {                boolean logStats = count++ % 1000 == 0;        wrapped.add(new StatsCallable(callable, logStats, timing, noEstimateCounter));    }    return wrapped;}
0
public Void call() throws TasteException
{    for (Preference realPref : prefs) {        float estimatedPreference = Float.NaN;        try {            estimatedPreference = recommender.estimatePreference(testUserID, realPref.getItemID());        } catch (NoSuchUserException nsue) {                                            } catch (NoSuchItemException nsie) {                    }        if (Float.isNaN(estimatedPreference)) {            noEstimateCounter.incrementAndGet();        } else {            estimatedPreference = capEstimatedPreference(estimatedPreference);            processOneEstimate(estimatedPreference, realPref);        }    }    return null;}
1
protected void reset()
{    average = new FullRunningAverage();}
0
protected void processOneEstimate(float estimatedPreference, Preference realPref)
{    average.addDatum(Math.abs(realPref.getValue() - estimatedPreference));}
0
protected double computeFinalEvaluation()
{    return average.getAverage();}
0
public String toString()
{    return "AverageAbsoluteDifferenceRecommenderEvaluator";}
0
public IRStatistics evaluate(RecommenderBuilder recommenderBuilder, DataModelBuilder dataModelBuilder, DataModel dataModel, IDRescorer rescorer, int at, double relevanceThreshold, double evaluationPercentage) throws TasteException
{    Preconditions.checkArgument(recommenderBuilder != null, "recommenderBuilder is null");    Preconditions.checkArgument(dataModel != null, "dataModel is null");    Preconditions.checkArgument(at >= 1, "at must be at least 1");    Preconditions.checkArgument(evaluationPercentage > 0.0 && evaluationPercentage <= 1.0, "Invalid evaluationPercentage: " + evaluationPercentage + ". Must be: 0.0 < evaluationPercentage <= 1.0");    int numItems = dataModel.getNumItems();    RunningAverage precision = new FullRunningAverage();    RunningAverage recall = new FullRunningAverage();    RunningAverage fallOut = new FullRunningAverage();    RunningAverage nDCG = new FullRunningAverage();    int numUsersRecommendedFor = 0;    int numUsersWithRecommendations = 0;    LongPrimitiveIterator it = dataModel.getUserIDs();    while (it.hasNext()) {        long userID = it.nextLong();        if (random.nextDouble() >= evaluationPercentage) {                        continue;        }        long start = System.currentTimeMillis();        PreferenceArray prefs = dataModel.getPreferencesFromUser(userID);                double theRelevanceThreshold = Double.isNaN(relevanceThreshold) ? computeThreshold(prefs) : relevanceThreshold;        FastIDSet relevantItemIDs = dataSplitter.getRelevantItemsIDs(userID, at, theRelevanceThreshold, dataModel);        int numRelevantItems = relevantItemIDs.size();        if (numRelevantItems <= 0) {            continue;        }        FastByIDMap<PreferenceArray> trainingUsers = new FastByIDMap<>(dataModel.getNumUsers());        LongPrimitiveIterator it2 = dataModel.getUserIDs();        while (it2.hasNext()) {            dataSplitter.processOtherUser(userID, relevantItemIDs, trainingUsers, it2.nextLong(), dataModel);        }        DataModel trainingModel = dataModelBuilder == null ? new GenericDataModel(trainingUsers) : dataModelBuilder.buildDataModel(trainingUsers);        try {            trainingModel.getPreferencesFromUser(userID);        } catch (NoSuchUserException nsee) {                        continue;        }        int size = numRelevantItems + trainingModel.getItemIDsFromUser(userID).size();        if (size < 2 * at) {                        continue;        }        Recommender recommender = recommenderBuilder.buildRecommender(trainingModel);        int intersectionSize = 0;        List<RecommendedItem> recommendedItems = recommender.recommend(userID, at, rescorer);        for (RecommendedItem recommendedItem : recommendedItems) {            if (relevantItemIDs.contains(recommendedItem.getItemID())) {                intersectionSize++;            }        }        int numRecommendedItems = recommendedItems.size();                if (numRecommendedItems > 0) {            precision.addDatum((double) intersectionSize / (double) numRecommendedItems);        }                recall.addDatum((double) intersectionSize / (double) numRelevantItems);                if (numRelevantItems < size) {            fallOut.addDatum((double) (numRecommendedItems - intersectionSize) / (double) (numItems - numRelevantItems));        }                        double cumulativeGain = 0.0;        double idealizedGain = 0.0;        for (int i = 0; i < numRecommendedItems; i++) {            RecommendedItem item = recommendedItems.get(i);                        double discount = 1.0 / log2(i + 2.0);            if (relevantItemIDs.contains(item.getItemID())) {                cumulativeGain += discount;            }                        if (i < numRelevantItems) {                idealizedGain += discount;            }        }        if (idealizedGain > 0.0) {            nDCG.addDatum(cumulativeGain / idealizedGain);        }                numUsersRecommendedFor++;        if (numRecommendedItems > 0) {            numUsersWithRecommendations++;        }        long end = System.currentTimeMillis();                    }    return new IRStatisticsImpl(precision.getAverage(), recall.getAverage(), fallOut.getAverage(), nDCG.getAverage(), (double) numUsersWithRecommendations / (double) numUsersRecommendedFor);}
1
private static double computeThreshold(PreferenceArray prefs)
{    if (prefs.length() < 2) {                return Double.NEGATIVE_INFINITY;    }    RunningAverageAndStdDev stdDev = new FullRunningAverageAndStdDev();    int size = prefs.length();    for (int i = 0; i < size; i++) {        stdDev.addDatum(prefs.getValue(i));    }    return stdDev.getAverage() + stdDev.getStandardDeviation();}
0
private static double log2(double value)
{    return Math.log(value) / LOG2;}
0
public FastIDSet getRelevantItemsIDs(long userID, int at, double relevanceThreshold, DataModel dataModel) throws TasteException
{    PreferenceArray prefs = dataModel.getPreferencesFromUser(userID);    FastIDSet relevantItemIDs = new FastIDSet(at);    prefs.sortByValueReversed();    for (int i = 0; i < prefs.length() && relevantItemIDs.size() < at; i++) {        if (prefs.getValue(i) >= relevanceThreshold) {            relevantItemIDs.add(prefs.getItemID(i));        }    }    return relevantItemIDs;}
0
public void processOtherUser(long userID, FastIDSet relevantItemIDs, FastByIDMap<PreferenceArray> trainingUsers, long otherUserID, DataModel dataModel) throws TasteException
{    PreferenceArray prefs2Array = dataModel.getPreferencesFromUser(otherUserID);        if (userID == otherUserID) {                List<Preference> prefs2 = new ArrayList<>(prefs2Array.length());        for (Preference pref : prefs2Array) {            prefs2.add(pref);        }        for (Iterator<Preference> iterator = prefs2.iterator(); iterator.hasNext(); ) {            Preference pref = iterator.next();            if (relevantItemIDs.contains(pref.getItemID())) {                iterator.remove();            }        }        if (!prefs2.isEmpty()) {            trainingUsers.put(otherUserID, new GenericUserPreferenceArray(prefs2));        }    } else {                trainingUsers.put(otherUserID, prefs2Array);    }}
0
public double getPrecision()
{    return precision;}
0
public double getRecall()
{    return recall;}
0
public double getFallOut()
{    return fallOut;}
0
public double getF1Measure()
{    return getFNMeasure(1.0);}
0
public double getFNMeasure(double b)
{    double b2 = b * b;    double sum = b2 * precision + recall;    return sum == 0.0 ? Double.NaN : (1.0 + b2) * precision * recall / sum;}
0
public double getNormalizedDiscountedCumulativeGain()
{    return ndcg;}
0
public double getReach()
{    return reach;}
0
public String toString()
{    return "IRStatisticsImpl[precision:" + precision + ",recall:" + recall + ",fallOut:" + fallOut + ",nDCG:" + ndcg + ",reach:" + reach + ']';}
0
public Void call() throws Exception
{    recommender.recommend(userID, 10);    return null;}
0
public static LoadStatistics runLoad(Recommender recommender) throws TasteException
{    return runLoad(recommender, 10);}
0
public static LoadStatistics runLoad(Recommender recommender, int howMany) throws TasteException
{    DataModel dataModel = recommender.getDataModel();    int numUsers = dataModel.getNumUsers();    double sampleRate = 1000.0 / numUsers;    LongPrimitiveIterator userSampler = SamplingLongPrimitiveIterator.maybeWrapIterator(dataModel.getUserIDs(), sampleRate);        recommender.recommend(userSampler.next(), howMany);    Collection<Callable<Void>> callables = new ArrayList<>();    while (userSampler.hasNext()) {        callables.add(new LoadCallable(recommender, userSampler.next()));    }    AtomicInteger noEstimateCounter = new AtomicInteger();    RunningAverageAndStdDev timing = new FullRunningAverageAndStdDev();    AbstractDifferenceRecommenderEvaluator.execute(callables, noEstimateCounter, timing);    return new LoadStatistics(timing);}
0
public RunningAverage getTiming()
{    return timing;}
0
public static void evaluate(Recommender recommender1, Recommender recommender2, int samples, RunningAverage tracker, String tag) throws TasteException
{    printHeader();    LongPrimitiveIterator users = recommender1.getDataModel().getUserIDs();    while (users.hasNext()) {        long userID = users.nextLong();        List<RecommendedItem> recs1 = recommender1.recommend(userID, samples);        List<RecommendedItem> recs2 = recommender2.recommend(userID, samples);        FastIDSet commonSet = new FastIDSet();        long maxItemID = setBits(commonSet, recs1, samples);        FastIDSet otherSet = new FastIDSet();        maxItemID = Math.max(maxItemID, setBits(otherSet, recs2, samples));        int max = mask(commonSet, otherSet, maxItemID);        max = Math.min(max, samples);        if (max < 2) {            continue;        }        Long[] items1 = getCommonItems(commonSet, recs1, max);        Long[] items2 = getCommonItems(commonSet, recs2, max);        double variance = scoreCommonSubset(tag, userID, samples, max, items1, items2);        tracker.addDatum(variance);    }}
0
public static void evaluate(Recommender recommender, DataModel model, int samples, RunningAverage tracker, String tag) throws TasteException
{    printHeader();    LongPrimitiveIterator users = recommender.getDataModel().getUserIDs();    while (users.hasNext()) {        long userID = users.nextLong();        List<RecommendedItem> recs1 = recommender.recommend(userID, model.getNumItems());        PreferenceArray prefs2 = model.getPreferencesFromUser(userID);        prefs2.sortByValueReversed();        FastIDSet commonSet = new FastIDSet();        long maxItemID = setBits(commonSet, recs1, samples);        FastIDSet otherSet = new FastIDSet();        maxItemID = Math.max(maxItemID, setBits(otherSet, prefs2, samples));        int max = mask(commonSet, otherSet, maxItemID);        max = Math.min(max, samples);        if (max < 2) {            continue;        }        Long[] items1 = getCommonItems(commonSet, recs1, max);        Long[] items2 = getCommonItems(commonSet, prefs2, max);        double variance = scoreCommonSubset(tag, userID, samples, max, items1, items2);        tracker.addDatum(variance);    }}
0
public static void evaluate(DataModel model1, DataModel model2, int samples, RunningAverage tracker, String tag) throws TasteException
{    printHeader();    LongPrimitiveIterator users = model1.getUserIDs();    while (users.hasNext()) {        long userID = users.nextLong();        PreferenceArray prefs1 = model1.getPreferencesFromUser(userID);        PreferenceArray prefs2 = model2.getPreferencesFromUser(userID);        prefs1.sortByValueReversed();        prefs2.sortByValueReversed();        FastIDSet commonSet = new FastIDSet();        long maxItemID = setBits(commonSet, prefs1, samples);        FastIDSet otherSet = new FastIDSet();        maxItemID = Math.max(maxItemID, setBits(otherSet, prefs2, samples));        int max = mask(commonSet, otherSet, maxItemID);        max = Math.min(max, samples);        if (max < 2) {            continue;        }        Long[] items1 = getCommonItems(commonSet, prefs1, max);        Long[] items2 = getCommonItems(commonSet, prefs2, max);        double variance = scoreCommonSubset(tag, userID, samples, max, items1, items2);        tracker.addDatum(variance);    }}
0
private static int mask(FastIDSet commonSet, FastIDSet otherSet, long maxItemID)
{    int count = 0;    for (int i = 0; i <= maxItemID; i++) {        if (commonSet.contains(i)) {            if (otherSet.contains(i)) {                count++;            } else {                commonSet.remove(i);            }        }    }    return count;}
0
private static Long[] getCommonItems(FastIDSet commonSet, Iterable<RecommendedItem> recs, int max)
{    Long[] commonItems = new Long[max];    int index = 0;    for (RecommendedItem rec : recs) {        Long item = rec.getItemID();        if (commonSet.contains(item)) {            commonItems[index++] = item;        }        if (index == max) {            break;        }    }    return commonItems;}
0
private static Long[] getCommonItems(FastIDSet commonSet, PreferenceArray prefs1, int max)
{    Long[] commonItems = new Long[max];    int index = 0;    for (int i = 0; i < prefs1.length(); i++) {        Long item = prefs1.getItemID(i);        if (commonSet.contains(item)) {            commonItems[index++] = item;        }        if (index == max) {            break;        }    }    return commonItems;}
0
private static long setBits(FastIDSet modelSet, List<RecommendedItem> items, int max)
{    long maxItem = -1;    for (int i = 0; i < items.size() && i < max; i++) {        long itemID = items.get(i).getItemID();        modelSet.add(itemID);        if (itemID > maxItem) {            maxItem = itemID;        }    }    return maxItem;}
0
private static long setBits(FastIDSet modelSet, PreferenceArray prefs, int max)
{    long maxItem = -1;    for (int i = 0; i < prefs.length() && i < max; i++) {        long itemID = prefs.getItemID(i);        modelSet.add(itemID);        if (itemID > maxItem) {            maxItem = itemID;        }    }    return maxItem;}
0
private static void printHeader()
{    }
1
private static double scoreCommonSubset(String tag, long userID, int samples, int subset, Long[] itemsL, Long[] itemsR)
{    int[] vectorZ = new int[subset];    int[] vectorZabs = new int[subset];    long bubble = sort(itemsL, itemsR);    int hamming = slidingWindowHamming(itemsR, itemsL);    if (hamming > samples) {        throw new IllegalStateException();    }    getVectorZ(itemsR, itemsL, vectorZ, vectorZabs);    double normalW = normalWilcoxon(vectorZ, vectorZabs);    double meanRank = getMeanRank(vectorZabs);        double variance = Math.sqrt(meanRank);        return variance;}
1
private static int slidingWindowHamming(Long[] itemsR, Long[] itemsL)
{    int count = 0;    int samples = itemsR.length;    if (itemsR[0].equals(itemsL[0]) || itemsR[0].equals(itemsL[1])) {        count++;    }    for (int i = 1; i < samples - 1; i++) {        long itemID = itemsL[i];        if (itemsR[i] == itemID || itemsR[i - 1] == itemID || itemsR[i + 1] == itemID) {            count++;        }    }    if (itemsR[samples - 1].equals(itemsL[samples - 1]) || itemsR[samples - 1].equals(itemsL[samples - 2])) {        count++;    }    return count;}
0
 static double normalWilcoxon(int[] vectorZ, int[] vectorZabs)
{    int nitems = vectorZ.length;    double[] ranks = new double[nitems];    double[] ranksAbs = new double[nitems];    wilcoxonRanks(vectorZ, vectorZabs, ranks, ranksAbs);    return Math.min(getMeanWplus(ranks), getMeanWminus(ranks));}
0
private static void getVectorZ(Long[] itemsR, Long[] itemsL, int[] vectorZ, int[] vectorZabs)
{    int nitems = itemsR.length;    int bottom = 0;    int top = nitems - 1;    for (int i = 0; i < nitems; i++) {        long itemID = itemsR[i];        for (int j = bottom; j <= top; j++) {            if (itemsL[j] == null) {                continue;            }            long test = itemsL[j];            if (itemID == test) {                vectorZ[i] = i - j;                vectorZabs[i] = Math.abs(i - j);                if (j == bottom) {                    bottom++;                } else if (j == top) {                    top--;                } else {                    itemsL[j] = null;                }                break;            }        }    }}
0
private static void wilcoxonRanks(int[] vectorZ, int[] vectorZabs, double[] ranks, double[] ranksAbs)
{    int nitems = vectorZ.length;    int[] sorted = vectorZabs.clone();    Arrays.sort(sorted);    int zeros = 0;    for (; zeros < nitems; zeros++) {        if (sorted[zeros] > 0) {            break;        }    }    for (int i = 0; i < nitems; i++) {        double rank = 0.0;        int count = 0;        int score = vectorZabs[i];        for (int j = 0; j < nitems; j++) {            if (score == sorted[j]) {                rank += j + 1 - zeros;                count++;            } else if (score < sorted[j]) {                break;            }        }        if (vectorZ[i] != 0) {                        ranks[i] = (rank / count) * (vectorZ[i] < 0 ? -1 : 1);            ranksAbs[i] = Math.abs(ranks[i]);        }    }}
0
private static double getMeanRank(int[] ranks)
{    int nitems = ranks.length;    double sum = 0.0;    for (int rank : ranks) {        sum += rank;    }    return sum / nitems;}
0
private static double getMeanWplus(double[] ranks)
{    int nitems = ranks.length;    double sum = 0.0;    for (double rank : ranks) {        if (rank > 0) {            sum += rank;        }    }    return sum / nitems;}
0
private static double getMeanWminus(double[] ranks)
{    int nitems = ranks.length;    double sum = 0.0;    for (double rank : ranks) {        if (rank < 0) {            sum -= rank;        }    }    return sum / nitems;}
0
 static long sort(Long[] itemsL, Long[] itemsR)
{    int length = itemsL.length;    if (length < 2) {        return 0;    }    if (length == 2) {        return itemsL[0].longValue() == itemsR[0].longValue() ? 0 : 1;    }        long[] reference = new long[length];    long[] sortable = new long[length];    for (int i = 0; i < length; i++) {        reference[i] = itemsL[i];        sortable[i] = itemsR[i];    }    int sorted = 0;    long swaps = 0;    while (sorted < length - 1) {                while (length > 0 && reference[length - 1] == sortable[length - 1]) {            length--;        }        if (length == 0) {            break;        }        if (reference[sorted] == sortable[sorted]) {            sorted++;        } else {            for (int j = sorted; j < length - 1; j++) {                                int jump = 1;                if (reference[j] == sortable[j]) {                    while (j + jump < length && reference[j + jump] == sortable[j + jump]) {                        jump++;                    }                }                if (j + jump < length && !(reference[j] == sortable[j] && reference[j + jump] == sortable[j + jump])) {                    long tmp = sortable[j];                    sortable[j] = sortable[j + 1];                    sortable[j + 1] = tmp;                    swaps++;                }            }        }    }    return swaps;}
0
protected void reset()
{    average = new FullRunningAverage();}
0
protected void processOneEstimate(float estimatedPreference, Preference realPref)
{    double diff = realPref.getValue() - estimatedPreference;    average.addDatum(diff * diff);}
0
protected double computeFinalEvaluation()
{    return Math.sqrt(average.getAverage());}
0
public String toString()
{    return "RMSRecommenderEvaluator";}
0
public Void call() throws Exception
{    long start = System.currentTimeMillis();    delegate.call();    long end = System.currentTimeMillis();    timing.addDatum(end - start);    if (logStats) {        Runtime runtime = Runtime.getRuntime();        int average = (int) timing.getAverage();                long totalMemory = runtime.totalMemory();        long memory = totalMemory - runtime.freeMemory();                    }    return null;}
1
public float getMaxPreference()
{    return maxPreference;}
0
protected void setMaxPreference(float maxPreference)
{    this.maxPreference = maxPreference;}
0
public float getMinPreference()
{    return minPreference;}
0
protected void setMinPreference(float minPreference)
{    this.minPreference = minPreference;}
0
protected final long hash(String value)
{    byte[] md5hash;    synchronized (md5Digest) {        md5hash = md5Digest.digest(value.getBytes(Charsets.UTF_8));        md5Digest.reset();    }    long hash = 0L;    for (int i = 0; i < 8; i++) {        hash = hash << 8 | md5hash[i] & 0x00000000000000FFL;    }    return hash;}
0
public long toLongID(String stringID)
{    return hash(stringID);}
0
public void refresh(Collection<Refreshable> alreadyRefreshed)
{}
0
public final void storeMapping(long longID, String stringID) throws TasteException
{    Connection conn = null;    PreparedStatement stmt = null;    try {        conn = dataSource.getConnection();        stmt = conn.prepareStatement(storeMappingSQL);        stmt.setLong(1, longID);        stmt.setString(2, stringID);        stmt.executeUpdate();    } catch (SQLException sqle) {        throw new TasteException(sqle);    } finally {        IOUtils.quietClose(null, stmt, conn);    }}
0
public final String toStringID(long longID) throws TasteException
{    Connection conn = null;    PreparedStatement stmt = null;    ResultSet rs = null;    try {        conn = dataSource.getConnection();        stmt = conn.prepareStatement(getStringIDSQL, ResultSet.TYPE_FORWARD_ONLY, ResultSet.CONCUR_READ_ONLY);        stmt.setFetchDirection(ResultSet.FETCH_FORWARD);        stmt.setFetchSize(1);        stmt.setLong(1, longID);        rs = stmt.executeQuery();        if (rs.next()) {            return rs.getString(1);        } else {            return null;        }    } catch (SQLException sqle) {        throw new TasteException(sqle);    } finally {        IOUtils.quietClose(rs, stmt, conn);    }}
0
public void initialize(Iterable<String> stringIDs) throws TasteException
{    for (String stringID : stringIDs) {        storeMapping(toLongID(stringID), stringID);    }}
0
public int length()
{    return ids.length;}
0
public Preference get(int i)
{    return new PreferenceView(i);}
0
public void set(int i, Preference pref)
{    id = pref.getItemID();    ids[i] = pref.getUserID();}
0
public long getUserID(int i)
{    return ids[i];}
0
public void setUserID(int i, long userID)
{    ids[i] = userID;}
0
public long getItemID(int i)
{    return id;}
0
public void setItemID(int i, long itemID)
{    id = itemID;}
0
public long[] getIDs()
{    return ids;}
0
public float getValue(int i)
{    return 1.0f;}
0
public void setValue(int i, float value)
{    throw new UnsupportedOperationException();}
0
public void sortByUser()
{    Arrays.sort(ids);}
0
public void sortByItem()
{}
0
public void sortByValue()
{}
0
public void sortByValueReversed()
{}
0
public boolean hasPrefWithUserID(long userID)
{    for (long id : ids) {        if (userID == id) {            return true;        }    }    return false;}
0
public boolean hasPrefWithItemID(long itemID)
{    return id == itemID;}
0
public BooleanItemPreferenceArray clone()
{    return new BooleanItemPreferenceArray(ids.clone(), id);}
0
public int hashCode()
{    return (int) (id >> 32) ^ (int) id ^ Arrays.hashCode(ids);}
0
public boolean equals(Object other)
{    if (!(other instanceof BooleanItemPreferenceArray)) {        return false;    }    BooleanItemPreferenceArray otherArray = (BooleanItemPreferenceArray) other;    return id == otherArray.id && Arrays.equals(ids, otherArray.ids);}
0
public Iterator<Preference> iterator()
{    return Iterators.transform(new CountingIterator(length()), new Function<Integer, Preference>() {        @Override        public Preference apply(Integer from) {            return new PreferenceView(from);        }    });}
0
public Preference apply(Integer from)
{    return new PreferenceView(from);}
0
public String toString()
{    StringBuilder result = new StringBuilder(10 * ids.length);    result.append("BooleanItemPreferenceArray[itemID:");    result.append(id);    result.append(",{");    for (int i = 0; i < ids.length; i++) {        if (i > 0) {            result.append(',');        }        result.append(ids[i]);    }    result.append("}]");    return result.toString();}
0
public long getUserID()
{    return BooleanItemPreferenceArray.this.getUserID(i);}
0
public long getItemID()
{    return BooleanItemPreferenceArray.this.getItemID(i);}
0
public float getValue()
{    return 1.0f;}
0
public void setValue(float value)
{    throw new UnsupportedOperationException();}
0
public long getUserID()
{    return userID;}
0
public long getItemID()
{    return itemID;}
0
public float getValue()
{    return 1.0f;}
0
public void setValue(float value)
{    throw new UnsupportedOperationException();}
0
public String toString()
{    return "BooleanPreference[userID: " + userID + ", itemID:" + itemID + ']';}
0
public int length()
{    return ids.length;}
0
public Preference get(int i)
{    return new PreferenceView(i);}
0
public void set(int i, Preference pref)
{    id = pref.getUserID();    ids[i] = pref.getItemID();}
0
public long getUserID(int i)
{    return id;}
0
public void setUserID(int i, long userID)
{    id = userID;}
0
public long getItemID(int i)
{    return ids[i];}
0
public void setItemID(int i, long itemID)
{    ids[i] = itemID;}
0
public long[] getIDs()
{    return ids;}
0
public float getValue(int i)
{    return 1.0f;}
0
public void setValue(int i, float value)
{    throw new UnsupportedOperationException();}
0
public void sortByUser()
{}
0
public void sortByItem()
{    Arrays.sort(ids);}
0
public void sortByValue()
{}
0
public void sortByValueReversed()
{}
0
public boolean hasPrefWithUserID(long userID)
{    return id == userID;}
0
public boolean hasPrefWithItemID(long itemID)
{    for (long id : ids) {        if (itemID == id) {            return true;        }    }    return false;}
0
public BooleanUserPreferenceArray clone()
{    return new BooleanUserPreferenceArray(ids.clone(), id);}
0
public int hashCode()
{    return (int) (id >> 32) ^ (int) id ^ Arrays.hashCode(ids);}
0
public boolean equals(Object other)
{    if (!(other instanceof BooleanUserPreferenceArray)) {        return false;    }    BooleanUserPreferenceArray otherArray = (BooleanUserPreferenceArray) other;    return id == otherArray.id && Arrays.equals(ids, otherArray.ids);}
0
public Iterator<Preference> iterator()
{    return Iterators.transform(new CountingIterator(length()), new Function<Integer, Preference>() {        @Override        public Preference apply(Integer from) {            return new PreferenceView(from);        }    });}
0
public Preference apply(Integer from)
{    return new PreferenceView(from);}
0
public String toString()
{    StringBuilder result = new StringBuilder(10 * ids.length);    result.append("BooleanUserPreferenceArray[userID:");    result.append(id);    result.append(",{");    for (int i = 0; i < ids.length; i++) {        if (i > 0) {            result.append(',');        }        result.append(ids[i]);    }    result.append("}]");    return result.toString();}
0
public long getUserID()
{    return BooleanUserPreferenceArray.this.getUserID(i);}
0
public long getItemID()
{    return BooleanUserPreferenceArray.this.getItemID(i);}
0
public float getValue()
{    return 1.0f;}
0
public void setValue(float value)
{    throw new UnsupportedOperationException();}
0
public File getDataFile()
{    return dataFile;}
0
protected void reload()
{    if (reloadLock.tryLock()) {        try {            delegate = buildModel();        } catch (IOException ioe) {                    } finally {            reloadLock.unlock();        }    }}
1
protected DataModel buildModel() throws IOException
{    long newLastModified = dataFile.lastModified();    long newLastUpdateFileModified = readLastUpdateFileModified();    boolean loadFreshData = delegate == null || newLastModified > lastModified + minReloadIntervalMS;    long oldLastUpdateFileModifieid = lastUpdateFileModified;    lastModified = newLastModified;    lastUpdateFileModified = newLastUpdateFileModified;    FastByIDMap<FastByIDMap<Long>> timestamps = new FastByIDMap<>();    if (hasPrefValues) {        if (loadFreshData) {            FastByIDMap<Collection<Preference>> data = new FastByIDMap<>();            FileLineIterator iterator = new FileLineIterator(dataFile, false);            processFile(iterator, data, timestamps, false);            for (File updateFile : findUpdateFilesAfter(newLastModified)) {                processFile(new FileLineIterator(updateFile, false), data, timestamps, false);            }            return new GenericDataModel(GenericDataModel.toDataMap(data, true), timestamps);        } else {            FastByIDMap<PreferenceArray> rawData = ((GenericDataModel) delegate).getRawUserData();            for (File updateFile : findUpdateFilesAfter(Math.max(oldLastUpdateFileModifieid, newLastModified))) {                processFile(new FileLineIterator(updateFile, false), rawData, timestamps, true);            }            return new GenericDataModel(rawData, timestamps);        }    } else {        if (loadFreshData) {            FastByIDMap<FastIDSet> data = new FastByIDMap<>();            FileLineIterator iterator = new FileLineIterator(dataFile, false);            processFileWithoutID(iterator, data, timestamps);            for (File updateFile : findUpdateFilesAfter(newLastModified)) {                processFileWithoutID(new FileLineIterator(updateFile, false), data, timestamps);            }            return new GenericBooleanPrefDataModel(data, timestamps);        } else {            FastByIDMap<FastIDSet> rawData = ((GenericBooleanPrefDataModel) delegate).getRawUserData();            for (File updateFile : findUpdateFilesAfter(Math.max(oldLastUpdateFileModifieid, newLastModified))) {                processFileWithoutID(new FileLineIterator(updateFile, false), rawData, timestamps);            }            return new GenericBooleanPrefDataModel(rawData, timestamps);        }    }}
0
private Iterable<File> findUpdateFilesAfter(long minimumLastModified)
{    String dataFileName = dataFile.getName();    int period = dataFileName.indexOf('.');    String startName = period < 0 ? dataFileName : dataFileName.substring(0, period);    File parentDir = dataFile.getParentFile();    Map<Long, File> modTimeToUpdateFile = new TreeMap<>();    FileFilter onlyFiles = new FileFilter() {        @Override        public boolean accept(File file) {            return !file.isDirectory();        }    };    for (File updateFile : parentDir.listFiles(onlyFiles)) {        String updateFileName = updateFile.getName();        if (updateFileName.startsWith(startName) && !updateFileName.equals(dataFileName) && updateFile.lastModified() >= minimumLastModified) {            modTimeToUpdateFile.put(updateFile.lastModified(), updateFile);        }    }    return modTimeToUpdateFile.values();}
0
public boolean accept(File file)
{    return !file.isDirectory();}
0
private long readLastUpdateFileModified()
{    long mostRecentModification = Long.MIN_VALUE;    for (File updateFile : findUpdateFilesAfter(0L)) {        mostRecentModification = Math.max(mostRecentModification, updateFile.lastModified());    }    return mostRecentModification;}
0
public static char determineDelimiter(String line)
{    for (char possibleDelimieter : DELIMIETERS) {        if (line.indexOf(possibleDelimieter) >= 0) {            return possibleDelimieter;        }    }    throw new IllegalArgumentException("Did not find a delimiter in first line");}
0
protected void processFile(FileLineIterator dataOrUpdateFileIterator, FastByIDMap<?> data, FastByIDMap<FastByIDMap<Long>> timestamps, boolean fromPriorData)
{        int count = 0;    while (dataOrUpdateFileIterator.hasNext()) {        String line = dataOrUpdateFileIterator.next();        if (!line.isEmpty()) {            processLine(line, data, timestamps, fromPriorData);            if (++count % 1000000 == 0) {                            }        }    }    }
1
protected void processLine(String line, FastByIDMap<?> data, FastByIDMap<FastByIDMap<Long>> timestamps, boolean fromPriorData)
{        if (line.isEmpty() || line.charAt(0) == COMMENT_CHAR) {        return;    }    Iterator<String> tokens = delimiterPattern.split(line).iterator();    String userIDString = tokens.next();    String itemIDString = tokens.next();    String preferenceValueString = tokens.next();    boolean hasTimestamp = tokens.hasNext();    String timestampString = hasTimestamp ? tokens.next() : null;    long userID = readUserIDFromString(userIDString);    long itemID = readItemIDFromString(itemIDString);    if (transpose) {        long tmp = userID;        userID = itemID;        itemID = tmp;    }        Object maybePrefs = data.get(userID);    if (fromPriorData) {                PreferenceArray prefs = (PreferenceArray) maybePrefs;        if (!hasTimestamp && preferenceValueString.isEmpty()) {                        if (prefs != null) {                boolean exists = false;                int length = prefs.length();                for (int i = 0; i < length; i++) {                    if (prefs.getItemID(i) == itemID) {                        exists = true;                        break;                    }                }                if (exists) {                    if (length == 1) {                        data.remove(userID);                    } else {                        PreferenceArray newPrefs = new GenericUserPreferenceArray(length - 1);                        for (int i = 0, j = 0; i < length; i++, j++) {                            if (prefs.getItemID(i) == itemID) {                                j--;                            } else {                                newPrefs.set(j, prefs.get(i));                            }                        }                        ((FastByIDMap<PreferenceArray>) data).put(userID, newPrefs);                    }                }            }            removeTimestamp(userID, itemID, timestamps);        } else {            float preferenceValue = Float.parseFloat(preferenceValueString);            boolean exists = false;            if (prefs != null) {                for (int i = 0; i < prefs.length(); i++) {                    if (prefs.getItemID(i) == itemID) {                        exists = true;                        prefs.setValue(i, preferenceValue);                        break;                    }                }            }            if (!exists) {                if (prefs == null) {                    prefs = new GenericUserPreferenceArray(1);                } else {                    PreferenceArray newPrefs = new GenericUserPreferenceArray(prefs.length() + 1);                    for (int i = 0, j = 1; i < prefs.length(); i++, j++) {                        newPrefs.set(j, prefs.get(i));                    }                    prefs = newPrefs;                }                prefs.setUserID(0, userID);                prefs.setItemID(0, itemID);                prefs.setValue(0, preferenceValue);                ((FastByIDMap<PreferenceArray>) data).put(userID, prefs);            }        }        addTimestamp(userID, itemID, timestampString, timestamps);    } else {                Collection<Preference> prefs = (Collection<Preference>) maybePrefs;        if (!hasTimestamp && preferenceValueString.isEmpty()) {                        if (prefs != null) {                                Iterator<Preference> prefsIterator = prefs.iterator();                while (prefsIterator.hasNext()) {                    Preference pref = prefsIterator.next();                    if (pref.getItemID() == itemID) {                        prefsIterator.remove();                        break;                    }                }            }            removeTimestamp(userID, itemID, timestamps);        } else {            float preferenceValue = Float.parseFloat(preferenceValueString);            boolean exists = false;            if (prefs != null) {                for (Preference pref : prefs) {                    if (pref.getItemID() == itemID) {                        exists = true;                        pref.setValue(preferenceValue);                        break;                    }                }            }            if (!exists) {                if (prefs == null) {                    prefs = new ArrayList<>(2);                    ((FastByIDMap<Collection<Preference>>) data).put(userID, prefs);                }                prefs.add(new GenericPreference(userID, itemID, preferenceValue));            }            addTimestamp(userID, itemID, timestampString, timestamps);        }    }}
0
protected void processFileWithoutID(FileLineIterator dataOrUpdateFileIterator, FastByIDMap<FastIDSet> data, FastByIDMap<FastByIDMap<Long>> timestamps)
{        int count = 0;    while (dataOrUpdateFileIterator.hasNext()) {        String line = dataOrUpdateFileIterator.next();        if (!line.isEmpty()) {            processLineWithoutID(line, data, timestamps);            if (++count % 100000 == 0) {                            }        }    }    }
1
protected void processLineWithoutID(String line, FastByIDMap<FastIDSet> data, FastByIDMap<FastByIDMap<Long>> timestamps)
{    if (line.isEmpty() || line.charAt(0) == COMMENT_CHAR) {        return;    }    Iterator<String> tokens = delimiterPattern.split(line).iterator();    String userIDString = tokens.next();    String itemIDString = tokens.next();    boolean hasPreference = tokens.hasNext();    String preferenceValueString = hasPreference ? tokens.next() : "";    boolean hasTimestamp = tokens.hasNext();    String timestampString = hasTimestamp ? tokens.next() : null;    long userID = readUserIDFromString(userIDString);    long itemID = readItemIDFromString(itemIDString);    if (transpose) {        long tmp = userID;        userID = itemID;        itemID = tmp;    }    if (hasPreference && !hasTimestamp && preferenceValueString.isEmpty()) {                FastIDSet itemIDs = data.get(userID);        if (itemIDs != null) {            itemIDs.remove(itemID);        }        removeTimestamp(userID, itemID, timestamps);    } else {        FastIDSet itemIDs = data.get(userID);        if (itemIDs == null) {            itemIDs = new FastIDSet(2);            data.put(userID, itemIDs);        }        itemIDs.add(itemID);        addTimestamp(userID, itemID, timestampString, timestamps);    }}
0
private void addTimestamp(long userID, long itemID, String timestampString, FastByIDMap<FastByIDMap<Long>> timestamps)
{    if (timestampString != null) {        FastByIDMap<Long> itemTimestamps = timestamps.get(userID);        if (itemTimestamps == null) {            itemTimestamps = new FastByIDMap<>();            timestamps.put(userID, itemTimestamps);        }        long timestamp = readTimestampFromString(timestampString);        itemTimestamps.put(itemID, timestamp);    }}
0
private static void removeTimestamp(long userID, long itemID, FastByIDMap<FastByIDMap<Long>> timestamps)
{    FastByIDMap<Long> itemTimestamps = timestamps.get(userID);    if (itemTimestamps != null) {        itemTimestamps.remove(itemID);    }}
0
protected long readUserIDFromString(String value)
{    return Long.parseLong(value);}
0
protected long readItemIDFromString(String value)
{    return Long.parseLong(value);}
0
protected long readTimestampFromString(String value)
{    return Long.parseLong(value);}
0
public LongPrimitiveIterator getUserIDs() throws TasteException
{    return delegate.getUserIDs();}
0
public PreferenceArray getPreferencesFromUser(long userID) throws TasteException
{    return delegate.getPreferencesFromUser(userID);}
0
public FastIDSet getItemIDsFromUser(long userID) throws TasteException
{    return delegate.getItemIDsFromUser(userID);}
0
public LongPrimitiveIterator getItemIDs() throws TasteException
{    return delegate.getItemIDs();}
0
public PreferenceArray getPreferencesForItem(long itemID) throws TasteException
{    return delegate.getPreferencesForItem(itemID);}
0
public Float getPreferenceValue(long userID, long itemID) throws TasteException
{    return delegate.getPreferenceValue(userID, itemID);}
0
public Long getPreferenceTime(long userID, long itemID) throws TasteException
{    return delegate.getPreferenceTime(userID, itemID);}
0
public int getNumItems() throws TasteException
{    return delegate.getNumItems();}
0
public int getNumUsers() throws TasteException
{    return delegate.getNumUsers();}
0
public int getNumUsersWithPreferenceFor(long itemID) throws TasteException
{    return delegate.getNumUsersWithPreferenceFor(itemID);}
0
public int getNumUsersWithPreferenceFor(long itemID1, long itemID2) throws TasteException
{    return delegate.getNumUsersWithPreferenceFor(itemID1, itemID2);}
0
public void setPreference(long userID, long itemID, float value) throws TasteException
{    delegate.setPreference(userID, itemID, value);}
0
public void removePreference(long userID, long itemID) throws TasteException
{    delegate.removePreference(userID, itemID);}
0
public boolean hasPreferenceValues()
{    return delegate.hasPreferenceValues();}
0
public float getMaxPreference()
{    return delegate.getMaxPreference();}
0
public float getMinPreference()
{    return delegate.getMinPreference();}
0
public String toString()
{    return "FileDataModel[dataFile:" + dataFile + ']';}
0
public String toStringID(long longID)
{    return longToString.get(longID);}
0
private void reload()
{    if (reloadLock.tryLock()) {        try {            longToString = buildMapping();        } catch (IOException ioe) {            throw new IllegalStateException(ioe);        } finally {            reloadLock.unlock();        }    }}
0
private FastByIDMap<String> buildMapping() throws IOException
{    FastByIDMap<String> mapping = new FastByIDMap<>();    for (String line : new FileLineIterable(dataFile)) {        mapping.put(toLongID(line), line);    }    lastModified = dataFile.lastModified();    return mapping;}
0
public String toString()
{    return "FileIDMigrator[dataFile:" + dataFile + ']';}
0
public static FastByIDMap<FastIDSet> toDataMap(DataModel dataModel) throws TasteException
{    FastByIDMap<FastIDSet> data = new FastByIDMap<>(dataModel.getNumUsers());    LongPrimitiveIterator it = dataModel.getUserIDs();    while (it.hasNext()) {        long userID = it.nextLong();        data.put(userID, dataModel.getItemIDsFromUser(userID));    }    return data;}
0
public static FastByIDMap<FastIDSet> toDataMap(FastByIDMap<PreferenceArray> data)
{    for (Map.Entry<Long, Object> entry : ((FastByIDMap<Object>) (FastByIDMap<?>) data).entrySet()) {        PreferenceArray prefArray = (PreferenceArray) entry.getValue();        int size = prefArray.length();        FastIDSet itemIDs = new FastIDSet(size);        for (int i = 0; i < size; i++) {            itemIDs.add(prefArray.getItemID(i));        }        entry.setValue(itemIDs);    }    return (FastByIDMap<FastIDSet>) (FastByIDMap<?>) data;}
0
public FastByIDMap<FastIDSet> getRawUserData()
{    return this.preferenceFromUsers;}
0
public FastByIDMap<FastIDSet> getRawItemData()
{    return this.preferenceForItems;}
0
public LongPrimitiveArrayIterator getUserIDs()
{    return new LongPrimitiveArrayIterator(userIDs);}
0
public PreferenceArray getPreferencesFromUser(long userID) throws NoSuchUserException
{    FastIDSet itemIDs = preferenceFromUsers.get(userID);    if (itemIDs == null) {        throw new NoSuchUserException(userID);    }    PreferenceArray prefArray = new BooleanUserPreferenceArray(itemIDs.size());    int i = 0;    LongPrimitiveIterator it = itemIDs.iterator();    while (it.hasNext()) {        prefArray.setUserID(i, userID);        prefArray.setItemID(i, it.nextLong());        i++;    }    return prefArray;}
0
public FastIDSet getItemIDsFromUser(long userID) throws TasteException
{    FastIDSet itemIDs = preferenceFromUsers.get(userID);    if (itemIDs == null) {        throw new NoSuchUserException(userID);    }    return itemIDs;}
0
public LongPrimitiveArrayIterator getItemIDs()
{    return new LongPrimitiveArrayIterator(itemIDs);}
0
public PreferenceArray getPreferencesForItem(long itemID) throws NoSuchItemException
{    FastIDSet userIDs = preferenceForItems.get(itemID);    if (userIDs == null) {        throw new NoSuchItemException(itemID);    }    PreferenceArray prefArray = new BooleanItemPreferenceArray(userIDs.size());    int i = 0;    LongPrimitiveIterator it = userIDs.iterator();    while (it.hasNext()) {        prefArray.setUserID(i, it.nextLong());        prefArray.setItemID(i, itemID);        i++;    }    return prefArray;}
0
public Float getPreferenceValue(long userID, long itemID) throws NoSuchUserException
{    FastIDSet itemIDs = preferenceFromUsers.get(userID);    if (itemIDs == null) {        throw new NoSuchUserException(userID);    }    if (itemIDs.contains(itemID)) {        return 1.0f;    }    return null;}
0
public Long getPreferenceTime(long userID, long itemID) throws TasteException
{    if (timestamps == null) {        return null;    }    FastByIDMap<Long> itemTimestamps = timestamps.get(userID);    if (itemTimestamps == null) {        throw new NoSuchUserException(userID);    }    return itemTimestamps.get(itemID);}
0
public int getNumItems()
{    return itemIDs.length;}
0
public int getNumUsers()
{    return userIDs.length;}
0
public int getNumUsersWithPreferenceFor(long itemID)
{    FastIDSet userIDs1 = preferenceForItems.get(itemID);    return userIDs1 == null ? 0 : userIDs1.size();}
0
public int getNumUsersWithPreferenceFor(long itemID1, long itemID2)
{    FastIDSet userIDs1 = preferenceForItems.get(itemID1);    if (userIDs1 == null) {        return 0;    }    FastIDSet userIDs2 = preferenceForItems.get(itemID2);    if (userIDs2 == null) {        return 0;    }    return userIDs1.size() < userIDs2.size() ? userIDs2.intersectionSize(userIDs1) : userIDs1.intersectionSize(userIDs2);}
0
public void removePreference(long userID, long itemID)
{    throw new UnsupportedOperationException();}
0
public void setPreference(long userID, long itemID, float value)
{    throw new UnsupportedOperationException();}
0
public void refresh(Collection<Refreshable> alreadyRefreshed)
{}
0
public boolean hasPreferenceValues()
{    return false;}
0
public String toString()
{    StringBuilder result = new StringBuilder(200);    result.append("GenericBooleanPrefDataModel[users:");    for (int i = 0; i < Math.min(3, userIDs.length); i++) {        if (i > 0) {            result.append(',');        }        result.append(userIDs[i]);    }    if (userIDs.length > 3) {        result.append("...");    }    result.append(']');    return result.toString();}
0
public static FastByIDMap<PreferenceArray> toDataMap(FastByIDMap<Collection<Preference>> data, boolean byUser)
{    for (Map.Entry<Long, Object> entry : ((FastByIDMap<Object>) (FastByIDMap<?>) data).entrySet()) {        List<Preference> prefList = (List<Preference>) entry.getValue();        entry.setValue(byUser ? new GenericUserPreferenceArray(prefList) : new GenericItemPreferenceArray(prefList));    }    return (FastByIDMap<PreferenceArray>) (FastByIDMap<?>) data;}
0
public static FastByIDMap<PreferenceArray> toDataMap(DataModel dataModel) throws TasteException
{    FastByIDMap<PreferenceArray> data = new FastByIDMap<>(dataModel.getNumUsers());    LongPrimitiveIterator it = dataModel.getUserIDs();    while (it.hasNext()) {        long userID = it.nextLong();        data.put(userID, dataModel.getPreferencesFromUser(userID));    }    return data;}
0
public FastByIDMap<PreferenceArray> getRawUserData()
{    return this.preferenceFromUsers;}
0
public FastByIDMap<PreferenceArray> getRawItemData()
{    return this.preferenceForItems;}
0
public LongPrimitiveArrayIterator getUserIDs()
{    return new LongPrimitiveArrayIterator(userIDs);}
0
public PreferenceArray getPreferencesFromUser(long userID) throws NoSuchUserException
{    PreferenceArray prefs = preferenceFromUsers.get(userID);    if (prefs == null) {        throw new NoSuchUserException(userID);    }    return prefs;}
0
public FastIDSet getItemIDsFromUser(long userID) throws TasteException
{    PreferenceArray prefs = getPreferencesFromUser(userID);    int size = prefs.length();    FastIDSet result = new FastIDSet(size);    for (int i = 0; i < size; i++) {        result.add(prefs.getItemID(i));    }    return result;}
0
public LongPrimitiveArrayIterator getItemIDs()
{    return new LongPrimitiveArrayIterator(itemIDs);}
0
public PreferenceArray getPreferencesForItem(long itemID) throws NoSuchItemException
{    PreferenceArray prefs = preferenceForItems.get(itemID);    if (prefs == null) {        throw new NoSuchItemException(itemID);    }    return prefs;}
0
public Float getPreferenceValue(long userID, long itemID) throws TasteException
{    PreferenceArray prefs = getPreferencesFromUser(userID);    int size = prefs.length();    for (int i = 0; i < size; i++) {        if (prefs.getItemID(i) == itemID) {            return prefs.getValue(i);        }    }    return null;}
0
public Long getPreferenceTime(long userID, long itemID) throws TasteException
{    if (timestamps == null) {        return null;    }    FastByIDMap<Long> itemTimestamps = timestamps.get(userID);    if (itemTimestamps == null) {        throw new NoSuchUserException(userID);    }    return itemTimestamps.get(itemID);}
0
public int getNumItems()
{    return itemIDs.length;}
0
public int getNumUsers()
{    return userIDs.length;}
0
public int getNumUsersWithPreferenceFor(long itemID)
{    PreferenceArray prefs1 = preferenceForItems.get(itemID);    return prefs1 == null ? 0 : prefs1.length();}
0
public int getNumUsersWithPreferenceFor(long itemID1, long itemID2)
{    PreferenceArray prefs1 = preferenceForItems.get(itemID1);    if (prefs1 == null) {        return 0;    }    PreferenceArray prefs2 = preferenceForItems.get(itemID2);    if (prefs2 == null) {        return 0;    }    int size1 = prefs1.length();    int size2 = prefs2.length();    int count = 0;    int i = 0;    int j = 0;    long userID1 = prefs1.getUserID(0);    long userID2 = prefs2.getUserID(0);    while (true) {        if (userID1 < userID2) {            if (++i == size1) {                break;            }            userID1 = prefs1.getUserID(i);        } else if (userID1 > userID2) {            if (++j == size2) {                break;            }            userID2 = prefs2.getUserID(j);        } else {            count++;            if (++i == size1 || ++j == size2) {                break;            }            userID1 = prefs1.getUserID(i);            userID2 = prefs2.getUserID(j);        }    }    return count;}
0
public void removePreference(long userID, long itemID)
{    throw new UnsupportedOperationException();}
0
public void setPreference(long userID, long itemID, float value)
{    throw new UnsupportedOperationException();}
0
public void refresh(Collection<Refreshable> alreadyRefreshed)
{}
0
public boolean hasPreferenceValues()
{    return true;}
0
public String toString()
{    StringBuilder result = new StringBuilder(200);    result.append("GenericDataModel[users:");    for (int i = 0; i < Math.min(3, userIDs.length); i++) {        if (i > 0) {            result.append(',');        }        result.append(userIDs[i]);    }    if (userIDs.length > 3) {        result.append("...");    }    result.append(']');    return result.toString();}
0
public int length()
{    return ids.length;}
0
public Preference get(int i)
{    return new PreferenceView(i);}
0
public void set(int i, Preference pref)
{    id = pref.getItemID();    ids[i] = pref.getUserID();    values[i] = pref.getValue();}
0
public long getUserID(int i)
{    return ids[i];}
0
public void setUserID(int i, long userID)
{    ids[i] = userID;}
0
public long getItemID(int i)
{    return id;}
0
public void setItemID(int i, long itemID)
{    id = itemID;}
0
public long[] getIDs()
{    return ids;}
0
public float getValue(int i)
{    return values[i];}
0
public void setValue(int i, float value)
{    values[i] = value;}
0
public void sortByUser()
{    lateralSort(USER);}
0
public void sortByItem()
{}
0
public void sortByValue()
{    lateralSort(VALUE);}
0
public void sortByValueReversed()
{    lateralSort(VALUE_REVERSED);}
0
public boolean hasPrefWithUserID(long userID)
{    for (long id : ids) {        if (userID == id) {            return true;        }    }    return false;}
0
public boolean hasPrefWithItemID(long itemID)
{    return id == itemID;}
0
private void lateralSort(int type)
{        int length = length();    int gap = length;    boolean swapped = false;    while (gap > 1 || swapped) {        if (gap > 1) {                        gap /= 1.247330950103979;        }        swapped = false;        int max = length - gap;        for (int i = 0; i < max; i++) {            int other = i + gap;            if (isLess(other, i, type)) {                swap(i, other);                swapped = true;            }        }    }}
0
private boolean isLess(int i, int j, int type)
{    switch(type) {        case USER:            return ids[i] < ids[j];        case VALUE:            return values[i] < values[j];        case VALUE_REVERSED:            return values[i] > values[j];        default:            throw new IllegalStateException();    }}
0
private void swap(int i, int j)
{    long temp1 = ids[i];    float temp2 = values[i];    ids[i] = ids[j];    values[i] = values[j];    ids[j] = temp1;    values[j] = temp2;}
0
public GenericItemPreferenceArray clone()
{    return new GenericItemPreferenceArray(ids.clone(), id, values.clone());}
0
public int hashCode()
{    return (int) (id >> 32) ^ (int) id ^ Arrays.hashCode(ids) ^ Arrays.hashCode(values);}
0
public boolean equals(Object other)
{    if (!(other instanceof GenericItemPreferenceArray)) {        return false;    }    GenericItemPreferenceArray otherArray = (GenericItemPreferenceArray) other;    return id == otherArray.id && Arrays.equals(ids, otherArray.ids) && Arrays.equals(values, otherArray.values);}
0
public Iterator<Preference> iterator()
{    return Iterators.transform(new CountingIterator(length()), new Function<Integer, Preference>() {        @Override        public Preference apply(Integer from) {            return new PreferenceView(from);        }    });}
0
public Preference apply(Integer from)
{    return new PreferenceView(from);}
0
public String toString()
{    if (ids == null || ids.length == 0) {        return "GenericItemPreferenceArray[{}]";    }    StringBuilder result = new StringBuilder(20 * ids.length);    result.append("GenericItemPreferenceArray[itemID:");    result.append(id);    result.append(",{");    for (int i = 0; i < ids.length; i++) {        if (i > 0) {            result.append(',');        }        result.append(ids[i]);        result.append('=');        result.append(values[i]);    }    result.append("}]");    return result.toString();}
0
public long getUserID()
{    return GenericItemPreferenceArray.this.getUserID(i);}
0
public long getItemID()
{    return GenericItemPreferenceArray.this.getItemID(i);}
0
public float getValue()
{    return values[i];}
0
public void setValue(float value)
{    values[i] = value;}
0
public long getUserID()
{    return userID;}
0
public long getItemID()
{    return itemID;}
0
public float getValue()
{    return value;}
0
public void setValue(float value)
{    Preconditions.checkArgument(!Float.isNaN(value), "NaN value");    this.value = value;}
0
public String toString()
{    return "GenericPreference[userID: " + userID + ", itemID:" + itemID + ", value:" + value + ']';}
0
public int length()
{    return ids.length;}
0
public Preference get(int i)
{    return new PreferenceView(i);}
0
public void set(int i, Preference pref)
{    id = pref.getUserID();    ids[i] = pref.getItemID();    values[i] = pref.getValue();}
0
public long getUserID(int i)
{    return id;}
0
public void setUserID(int i, long userID)
{    id = userID;}
0
public long getItemID(int i)
{    return ids[i];}
0
public void setItemID(int i, long itemID)
{    ids[i] = itemID;}
0
public long[] getIDs()
{    return ids;}
0
public float getValue(int i)
{    return values[i];}
0
public void setValue(int i, float value)
{    values[i] = value;}
0
public void sortByUser()
{}
0
public void sortByItem()
{    lateralSort(ITEM);}
0
public void sortByValue()
{    lateralSort(VALUE);}
0
public void sortByValueReversed()
{    lateralSort(VALUE_REVERSED);}
0
public boolean hasPrefWithUserID(long userID)
{    return id == userID;}
0
public boolean hasPrefWithItemID(long itemID)
{    for (long id : ids) {        if (itemID == id) {            return true;        }    }    return false;}
0
private void lateralSort(int type)
{        int length = length();    int gap = length;    boolean swapped = false;    while (gap > 1 || swapped) {        if (gap > 1) {                        gap /= 1.247330950103979;        }        swapped = false;        int max = length - gap;        for (int i = 0; i < max; i++) {            int other = i + gap;            if (isLess(other, i, type)) {                swap(i, other);                swapped = true;            }        }    }}
0
private boolean isLess(int i, int j, int type)
{    switch(type) {        case ITEM:            return ids[i] < ids[j];        case VALUE:            return values[i] < values[j];        case VALUE_REVERSED:            return values[i] > values[j];        default:            throw new IllegalStateException();    }}
0
private void swap(int i, int j)
{    long temp1 = ids[i];    float temp2 = values[i];    ids[i] = ids[j];    values[i] = values[j];    ids[j] = temp1;    values[j] = temp2;}
0
public GenericUserPreferenceArray clone()
{    return new GenericUserPreferenceArray(ids.clone(), id, values.clone());}
0
public int hashCode()
{    return (int) (id >> 32) ^ (int) id ^ Arrays.hashCode(ids) ^ Arrays.hashCode(values);}
0
public boolean equals(Object other)
{    if (!(other instanceof GenericUserPreferenceArray)) {        return false;    }    GenericUserPreferenceArray otherArray = (GenericUserPreferenceArray) other;    return id == otherArray.id && Arrays.equals(ids, otherArray.ids) && Arrays.equals(values, otherArray.values);}
0
public Iterator<Preference> iterator()
{    return Iterators.transform(new CountingIterator(length()), new Function<Integer, Preference>() {        @Override        public Preference apply(Integer from) {            return new PreferenceView(from);        }    });}
0
public Preference apply(Integer from)
{    return new PreferenceView(from);}
0
public String toString()
{    if (ids == null || ids.length == 0) {        return "GenericUserPreferenceArray[{}]";    }    StringBuilder result = new StringBuilder(20 * ids.length);    result.append("GenericUserPreferenceArray[userID:");    result.append(id);    result.append(",{");    for (int i = 0; i < ids.length; i++) {        if (i > 0) {            result.append(',');        }        result.append(ids[i]);        result.append('=');        result.append(values[i]);    }    result.append("}]");    return result.toString();}
0
public long getUserID()
{    return GenericUserPreferenceArray.this.getUserID(i);}
0
public long getItemID()
{    return GenericUserPreferenceArray.this.getItemID(i);}
0
public float getValue()
{    return values[i];}
0
public void setValue(float value)
{    values[i] = value;}
0
public void storeMapping(long longID, String stringID)
{    synchronized (longToString) {        longToString.put(longID, stringID);    }}
0
public String toStringID(long longID)
{    synchronized (longToString) {        return longToString.get(longID);    }}
0
public void initialize(Iterable<String> stringIDs)
{    for (String stringID : stringIDs) {        storeMapping(toLongID(stringID), stringID);    }}
0
private void initializeUsersPools(int usersPoolSize)
{    usersPool = new ConcurrentLinkedQueue<>();    for (int i = 0; i < usersPoolSize; i++) {        usersPool.add(TEMP_USER_ID + i);    }}
0
public Long takeAvailableUser()
{    Long takenUserID = usersPool.poll();    if (takenUserID != null) {                tempPrefs.put(takenUserID, new GenericUserPreferenceArray(0));        return takenUserID;    }    return null;}
0
public boolean releaseUser(Long userID)
{    if (tempPrefs.containsKey(userID)) {        this.clearTempPrefs(userID);                usersPool.offer(userID);        return true;    }    return false;}
0
private boolean isAnonymousUser(long userID)
{    return tempPrefs.containsKey(userID);}
0
public void setTempPrefs(PreferenceArray prefs, long anonymousUserID)
{    Preconditions.checkArgument(prefs != null && prefs.length() > 0, "prefs is null or empty");    this.tempPrefs.put(anonymousUserID, prefs);    FastIDSet userPrefItemIDs = new FastIDSet();    for (int i = 0; i < prefs.length(); i++) {        userPrefItemIDs.add(prefs.getItemID(i));    }    this.prefItemIDs.put(anonymousUserID, userPrefItemIDs);}
0
public void clearTempPrefs(long anonymousUserID)
{    this.tempPrefs.remove(anonymousUserID);    this.prefItemIDs.remove(anonymousUserID);}
0
public LongPrimitiveIterator getUserIDs() throws TasteException
{        return getDelegate().getUserIDs();}
0
public PreferenceArray getPreferencesFromUser(long userID) throws TasteException
{    if (isAnonymousUser(userID)) {        return tempPrefs.get(userID);    }    return getDelegate().getPreferencesFromUser(userID);}
0
public FastIDSet getItemIDsFromUser(long userID) throws TasteException
{    if (isAnonymousUser(userID)) {        return prefItemIDs.get(userID);    }    return getDelegate().getItemIDsFromUser(userID);}
0
public PreferenceArray getPreferencesForItem(long itemID) throws TasteException
{    if (tempPrefs.isEmpty()) {        return getDelegate().getPreferencesForItem(itemID);    }    PreferenceArray delegatePrefs = null;    try {        delegatePrefs = getDelegate().getPreferencesForItem(itemID);    } catch (NoSuchItemException nsie) {                if (log.isDebugEnabled()) {                    }    }    List<Preference> anonymousPreferences = Lists.newArrayList();    for (Map.Entry<Long, PreferenceArray> prefsMap : tempPrefs.entrySet()) {        PreferenceArray singleUserTempPrefs = prefsMap.getValue();        for (int i = 0; i < singleUserTempPrefs.length(); i++) {            if (singleUserTempPrefs.getItemID(i) == itemID) {                anonymousPreferences.add(singleUserTempPrefs.get(i));            }        }    }    int delegateLength = delegatePrefs == null ? 0 : delegatePrefs.length();    int anonymousPrefsLength = anonymousPreferences.size();    int prefsCounter = 0;        PreferenceArray newPreferenceArray = new GenericItemPreferenceArray(delegateLength + anonymousPrefsLength);    for (int i = 0; i < delegateLength; i++) {        newPreferenceArray.set(prefsCounter++, delegatePrefs.get(i));    }    for (Preference anonymousPreference : anonymousPreferences) {        newPreferenceArray.set(prefsCounter++, anonymousPreference);    }    if (newPreferenceArray.length() == 0) {                throw new NoSuchItemException(itemID);    }    return newPreferenceArray;}
1
public Float getPreferenceValue(long userID, long itemID) throws TasteException
{    if (isAnonymousUser(userID)) {        PreferenceArray singleUserTempPrefs = tempPrefs.get(userID);        for (int i = 0; i < singleUserTempPrefs.length(); i++) {            if (singleUserTempPrefs.getItemID(i) == itemID) {                return singleUserTempPrefs.getValue(i);            }        }        return null;    }    return getDelegate().getPreferenceValue(userID, itemID);}
0
public Long getPreferenceTime(long userID, long itemID) throws TasteException
{    if (isAnonymousUser(userID)) {                return null;    }    return getDelegate().getPreferenceTime(userID, itemID);}
0
public int getNumUsers() throws TasteException
{        return getDelegate().getNumUsers();}
0
public int getNumUsersWithPreferenceFor(long itemID) throws TasteException
{    if (tempPrefs.isEmpty()) {        return getDelegate().getNumUsersWithPreferenceFor(itemID);    }    int countAnonymousUsersWithPreferenceFor = 0;    for (Map.Entry<Long, PreferenceArray> singleUserTempPrefs : tempPrefs.entrySet()) {        for (int i = 0; i < singleUserTempPrefs.getValue().length(); i++) {            if (singleUserTempPrefs.getValue().getItemID(i) == itemID) {                countAnonymousUsersWithPreferenceFor++;                break;            }        }    }    return getDelegate().getNumUsersWithPreferenceFor(itemID) + countAnonymousUsersWithPreferenceFor;}
0
public int getNumUsersWithPreferenceFor(long itemID1, long itemID2) throws TasteException
{    if (tempPrefs.isEmpty()) {        return getDelegate().getNumUsersWithPreferenceFor(itemID1, itemID2);    }    int countAnonymousUsersWithPreferenceFor = 0;    for (Map.Entry<Long, PreferenceArray> singleUserTempPrefs : tempPrefs.entrySet()) {        boolean found1 = false;        boolean found2 = false;        for (int i = 0; i < singleUserTempPrefs.getValue().length() && !(found1 && found2); i++) {            long itemID = singleUserTempPrefs.getValue().getItemID(i);            if (itemID == itemID1) {                found1 = true;            }            if (itemID == itemID2) {                found2 = true;            }        }        if (found1 && found2) {            countAnonymousUsersWithPreferenceFor++;        }    }    return getDelegate().getNumUsersWithPreferenceFor(itemID1, itemID2) + countAnonymousUsersWithPreferenceFor;}
0
public void setPreference(long userID, long itemID, float value) throws TasteException
{    if (isAnonymousUser(userID)) {        throw new UnsupportedOperationException();    }    getDelegate().setPreference(userID, itemID, value);}
0
public void removePreference(long userID, long itemID) throws TasteException
{    if (isAnonymousUser(userID)) {        throw new UnsupportedOperationException();    }    getDelegate().removePreference(userID, itemID);}
0
protected DataModel getDelegate()
{    return delegate;}
0
public void setTempPrefs(PreferenceArray prefs)
{    Preconditions.checkArgument(prefs != null && prefs.length() > 0, "prefs is null or empty");    this.tempPrefs = prefs;    this.prefItemIDs.clear();    for (int i = 0; i < prefs.length(); i++) {        this.prefItemIDs.add(prefs.getItemID(i));    }}
0
public void clearTempPrefs()
{    tempPrefs = null;    prefItemIDs.clear();}
0
public LongPrimitiveIterator getUserIDs() throws TasteException
{    if (tempPrefs == null) {        return delegate.getUserIDs();    }    return new PlusAnonymousUserLongPrimitiveIterator(delegate.getUserIDs(), TEMP_USER_ID);}
0
public PreferenceArray getPreferencesFromUser(long userID) throws TasteException
{    if (userID == TEMP_USER_ID) {        if (tempPrefs == null) {            throw new NoSuchUserException(TEMP_USER_ID);        }        return tempPrefs;    }    return delegate.getPreferencesFromUser(userID);}
0
public FastIDSet getItemIDsFromUser(long userID) throws TasteException
{    if (userID == TEMP_USER_ID) {        if (tempPrefs == null) {            throw new NoSuchUserException(TEMP_USER_ID);        }        return prefItemIDs;    }    return delegate.getItemIDsFromUser(userID);}
0
public LongPrimitiveIterator getItemIDs() throws TasteException
{    return delegate.getItemIDs();}
0
public PreferenceArray getPreferencesForItem(long itemID) throws TasteException
{    if (tempPrefs == null) {        return delegate.getPreferencesForItem(itemID);    }    PreferenceArray delegatePrefs = null;    try {        delegatePrefs = delegate.getPreferencesForItem(itemID);    } catch (NoSuchItemException nsie) {                if (log.isDebugEnabled()) {                    }    }    for (int i = 0; i < tempPrefs.length(); i++) {        if (tempPrefs.getItemID(i) == itemID) {            return cloneAndMergeInto(delegatePrefs, itemID, tempPrefs.getUserID(i), tempPrefs.getValue(i));        }    }    if (delegatePrefs == null) {                throw new NoSuchItemException(itemID);    }    return delegatePrefs;}
1
private static PreferenceArray cloneAndMergeInto(PreferenceArray delegatePrefs, long itemID, long newUserID, float value)
{    int length = delegatePrefs == null ? 0 : delegatePrefs.length();    int newLength = length + 1;    PreferenceArray newPreferenceArray = new GenericItemPreferenceArray(newLength);        newPreferenceArray.setItemID(0, itemID);    int positionToInsert = 0;    while (positionToInsert < length && newUserID > delegatePrefs.getUserID(positionToInsert)) {        positionToInsert++;    }    for (int i = 0; i < positionToInsert; i++) {        newPreferenceArray.setUserID(i, delegatePrefs.getUserID(i));        newPreferenceArray.setValue(i, delegatePrefs.getValue(i));    }    newPreferenceArray.setUserID(positionToInsert, newUserID);    newPreferenceArray.setValue(positionToInsert, value);    for (int i = positionToInsert + 1; i < newLength; i++) {        newPreferenceArray.setUserID(i, delegatePrefs.getUserID(i - 1));        newPreferenceArray.setValue(i, delegatePrefs.getValue(i - 1));    }    return newPreferenceArray;}
0
public Float getPreferenceValue(long userID, long itemID) throws TasteException
{    if (userID == TEMP_USER_ID) {        if (tempPrefs == null) {            throw new NoSuchUserException(TEMP_USER_ID);        }        for (int i = 0; i < tempPrefs.length(); i++) {            if (tempPrefs.getItemID(i) == itemID) {                return tempPrefs.getValue(i);            }        }        return null;    }    return delegate.getPreferenceValue(userID, itemID);}
0
public Long getPreferenceTime(long userID, long itemID) throws TasteException
{    if (userID == TEMP_USER_ID) {        if (tempPrefs == null) {            throw new NoSuchUserException(TEMP_USER_ID);        }        return null;    }    return delegate.getPreferenceTime(userID, itemID);}
0
public int getNumItems() throws TasteException
{    return delegate.getNumItems();}
0
public int getNumUsers() throws TasteException
{    return delegate.getNumUsers() + (tempPrefs == null ? 0 : 1);}
0
public int getNumUsersWithPreferenceFor(long itemID) throws TasteException
{    if (tempPrefs == null) {        return delegate.getNumUsersWithPreferenceFor(itemID);    }    boolean found = false;    for (int i = 0; i < tempPrefs.length(); i++) {        if (tempPrefs.getItemID(i) == itemID) {            found = true;            break;        }    }    return delegate.getNumUsersWithPreferenceFor(itemID) + (found ? 1 : 0);}
0
public int getNumUsersWithPreferenceFor(long itemID1, long itemID2) throws TasteException
{    if (tempPrefs == null) {        return delegate.getNumUsersWithPreferenceFor(itemID1, itemID2);    }    boolean found1 = false;    boolean found2 = false;    for (int i = 0; i < tempPrefs.length() && !(found1 && found2); i++) {        long itemID = tempPrefs.getItemID(i);        if (itemID == itemID1) {            found1 = true;        }        if (itemID == itemID2) {            found2 = true;        }    }    return delegate.getNumUsersWithPreferenceFor(itemID1, itemID2) + (found1 && found2 ? 1 : 0);}
0
public void setPreference(long userID, long itemID, float value) throws TasteException
{    if (userID == TEMP_USER_ID) {        if (tempPrefs == null) {            throw new NoSuchUserException(TEMP_USER_ID);        }        throw new UnsupportedOperationException();    }    delegate.setPreference(userID, itemID, value);}
0
public void removePreference(long userID, long itemID) throws TasteException
{    if (userID == TEMP_USER_ID) {        if (tempPrefs == null) {            throw new NoSuchUserException(TEMP_USER_ID);        }        throw new UnsupportedOperationException();    }    delegate.removePreference(userID, itemID);}
0
public void refresh(Collection<Refreshable> alreadyRefreshed)
{    delegate.refresh(alreadyRefreshed);}
0
public boolean hasPreferenceValues()
{    return delegate.hasPreferenceValues();}
0
public float getMaxPreference()
{    return delegate.getMaxPreference();}
0
public float getMinPreference()
{    return delegate.getMinPreference();}
0
public long nextLong()
{    if (datumConsumed) {        return delegate.nextLong();    } else {        if (delegate.hasNext()) {            long delegateNext = delegate.peek();            if (extraDatum <= delegateNext) {                datumConsumed = true;                return extraDatum;            } else {                return delegate.next();            }        } else {            datumConsumed = true;            return extraDatum;        }    }}
0
public long peek()
{    if (datumConsumed) {        return delegate.peek();    } else {        if (delegate.hasNext()) {            long delegateNext = delegate.peek();            if (extraDatum <= delegateNext) {                return extraDatum;            } else {                return delegateNext;            }        } else {            return extraDatum;        }    }}
0
public boolean hasNext()
{    return !datumConsumed || delegate.hasNext();}
0
public void remove()
{    throw new UnsupportedOperationException();}
0
public void skip(int n)
{    for (int i = 0; i < n; i++) {        nextLong();    }}
0
 final UserSimilarity getUserSimilarity()
{    return userSimilarity;}
0
 final DataModel getDataModel()
{    return dataModel;}
0
 final double getSamplingRate()
{    return samplingRate;}
0
public final void refresh(Collection<Refreshable> alreadyRefreshed)
{    refreshHelper.refresh(alreadyRefreshed);}
0
public long[] getUserNeighborhood(long userID) throws TasteException
{    return neighborhoodCache.get(userID);}
0
public void refresh(Collection<Refreshable> alreadyRefreshed)
{    neighborhoodCache.clear();    Collection<Refreshable> refreshed = RefreshHelper.buildRefreshed(alreadyRefreshed);    RefreshHelper.maybeRefresh(refreshed, neighborhood);}
0
public long[] get(Long key) throws TasteException
{    return neighborhood.getUserNeighborhood(key);}
0
public long[] getUserNeighborhood(long userID) throws TasteException
{    DataModel dataModel = getDataModel();    UserSimilarity userSimilarityImpl = getUserSimilarity();    TopItems.Estimator<Long> estimator = new Estimator(userSimilarityImpl, userID, minSimilarity);    LongPrimitiveIterator userIDs = SamplingLongPrimitiveIterator.maybeWrapIterator(dataModel.getUserIDs(), getSamplingRate());    return TopItems.getTopUsers(n, userIDs, null, estimator);}
0
public String toString()
{    return "NearestNUserNeighborhood";}
0
public double estimate(Long userID) throws TasteException
{    if (userID == theUserID) {        return Double.NaN;    }    double sim = userSimilarityImpl.userSimilarity(theUserID, userID);    return sim >= minSim ? sim : Double.NaN;}
0
public long[] getUserNeighborhood(long userID) throws TasteException
{    DataModel dataModel = getDataModel();    FastIDSet neighborhood = new FastIDSet();    LongPrimitiveIterator usersIterable = SamplingLongPrimitiveIterator.maybeWrapIterator(dataModel.getUserIDs(), getSamplingRate());    UserSimilarity userSimilarityImpl = getUserSimilarity();    while (usersIterable.hasNext()) {        long otherUserID = usersIterable.next();        if (userID != otherUserID) {            double theSimilarity = userSimilarityImpl.userSimilarity(userID, otherUserID);            if (!Double.isNaN(theSimilarity) && theSimilarity >= threshold) {                neighborhood.add(otherUserID);            }        }    }    return neighborhood.toArray();}
0
public String toString()
{    return "ThresholdUserNeighborhood";}
0
protected FastIDSet doGetCandidateItems(long[] preferredItemIDs, DataModel dataModel) throws TasteException
{    return doGetCandidateItems(preferredItemIDs, dataModel, false);}
0
public FastIDSet getCandidateItems(long userID, PreferenceArray preferencesFromUser, DataModel dataModel, boolean includeKnownItems) throws TasteException
{    return doGetCandidateItems(preferencesFromUser.getIDs(), dataModel, includeKnownItems);}
0
public FastIDSet getCandidateItems(long[] itemIDs, DataModel dataModel) throws TasteException
{    return doGetCandidateItems(itemIDs, dataModel, false);}
0
public void refresh(Collection<Refreshable> alreadyRefreshed)
{}
0
protected static CandidateItemsStrategy getDefaultCandidateItemsStrategy()
{    return new PreferredItemsNeighborhoodCandidateItemsStrategy();}
0
public List<RecommendedItem> recommend(long userID, int howMany) throws TasteException
{    return recommend(userID, howMany, null, false);}
0
public List<RecommendedItem> recommend(long userID, int howMany, boolean includeKnownItems) throws TasteException
{    return recommend(userID, howMany, null, includeKnownItems);}
0
public List<RecommendedItem> recommend(long userID, int howMany, IDRescorer rescorer) throws TasteException
{    return recommend(userID, howMany, rescorer, false);}
0
public void setPreference(long userID, long itemID, float value) throws TasteException
{    Preconditions.checkArgument(!Float.isNaN(value), "NaN value");        dataModel.setPreference(userID, itemID, value);}
1
public void removePreference(long userID, long itemID) throws TasteException
{        dataModel.removePreference(userID, itemID);}
1
public DataModel getDataModel()
{    return dataModel;}
0
protected FastIDSet getAllOtherItems(long userID, PreferenceArray preferencesFromUser, boolean includeKnownItems) throws TasteException
{    return candidateItemsStrategy.getCandidateItems(userID, preferencesFromUser, dataModel, includeKnownItems);}
0
protected FastIDSet doGetCandidateItems(long[] preferredItemIDs, DataModel dataModel, boolean includeKnownItems) throws TasteException
{    FastIDSet candidateItemIDs = new FastIDSet();    for (long itemID : preferredItemIDs) {        candidateItemIDs.addAll(similarity.allSimilarItemIDs(itemID));    }    if (!includeKnownItems) {        candidateItemIDs.removeAll(preferredItemIDs);    }    return candidateItemIDs;}
0
protected FastIDSet doGetCandidateItems(long[] preferredItemIDs, DataModel dataModel, boolean includeKnownItems) throws TasteException
{    FastIDSet possibleItemIDs = new FastIDSet(dataModel.getNumItems());    LongPrimitiveIterator allItemIDs = dataModel.getItemIDs();    while (allItemIDs.hasNext()) {        possibleItemIDs.add(allItemIDs.nextLong());    }    if (!includeKnownItems) {        possibleItemIDs.removeAll(preferredItemIDs);    }    return possibleItemIDs;}
0
public int compare(RecommendedItem o1, RecommendedItem o2)
{    double rescored1;    double rescored2;    if (rescorer == null) {        rescored1 = o1.getValue();        rescored2 = o2.getValue();    } else {        rescored1 = rescorer.rescore(o1.getItemID(), o1.getValue());        rescored2 = rescorer.rescore(o2.getItemID(), o2.getValue());    }    if (rescored1 < rescored2) {        return 1;    } else if (rescored1 > rescored2) {        return -1;    } else {        return 0;    }}
0
public String toString()
{    return "ByRescoreComparator[rescorer:" + rescorer + ']';}
0
public static Comparator<RecommendedItem> getInstance()
{    return INSTANCE;}
0
public int compare(RecommendedItem o1, RecommendedItem o2)
{    float value1 = o1.getValue();    float value2 = o2.getValue();    return value1 > value2 ? -1 : value1 < value2 ? 1 : 0;}
0
public Object call()
{    clear();    return null;}
0
private void setCurrentRescorer(IDRescorer rescorer)
{    if (rescorer == null) {        if (currentRescorer != null) {            currentRescorer = null;            clear();        }    } else {        if (!rescorer.equals(currentRescorer)) {            currentRescorer = rescorer;            clear();        }    }}
0
public void setCurrentlyIncludeKnownItems(boolean currentlyIncludeKnownItems)
{    this.currentlyIncludeKnownItems = currentlyIncludeKnownItems;}
0
public List<RecommendedItem> recommend(long userID, int howMany) throws TasteException
{    return recommend(userID, howMany, null, false);}
0
public List<RecommendedItem> recommend(long userID, int howMany, boolean includeKnownItems) throws TasteException
{    return recommend(userID, howMany, null, includeKnownItems);}
0
public List<RecommendedItem> recommend(long userID, int howMany, IDRescorer rescorer) throws TasteException
{    return recommend(userID, howMany, rescorer, false);}
0
public List<RecommendedItem> recommend(long userID, int howMany, IDRescorer rescorer, boolean includeKnownItems) throws TasteException
{    Preconditions.checkArgument(howMany >= 1, "howMany must be at least 1");    synchronized (maxHowMany) {        if (howMany > maxHowMany[0]) {            maxHowMany[0] = howMany;        }    }        if (userID == PlusAnonymousUserDataModel.TEMP_USER_ID) {        return recommendationsRetriever.get(PlusAnonymousUserDataModel.TEMP_USER_ID).getItems();    }    setCurrentRescorer(rescorer);    setCurrentlyIncludeKnownItems(includeKnownItems);    Recommendations recommendations = recommendationCache.get(userID);    if (recommendations.getItems().size() < howMany && !recommendations.isNoMoreRecommendableItems()) {        clear(userID);        recommendations = recommendationCache.get(userID);        if (recommendations.getItems().size() < howMany) {            recommendations.setNoMoreRecommendableItems(true);        }    }    List<RecommendedItem> recommendedItems = recommendations.getItems();    return recommendedItems.size() > howMany ? recommendedItems.subList(0, howMany) : recommendedItems;}
0
public float estimatePreference(long userID, long itemID) throws TasteException
{    return estimatedPrefCache.get(new LongPair(userID, itemID));}
0
public void setPreference(long userID, long itemID, float value) throws TasteException
{    recommender.setPreference(userID, itemID, value);    clear(userID);}
0
public void removePreference(long userID, long itemID) throws TasteException
{    recommender.removePreference(userID, itemID);    clear(userID);}
0
public DataModel getDataModel()
{    return recommender.getDataModel();}
0
public void refresh(Collection<Refreshable> alreadyRefreshed)
{    refreshHelper.refresh(alreadyRefreshed);}
0
public void clear(final long userID)
{        recommendationCache.remove(userID);    estimatedPrefCache.removeKeysMatching(new Cache.MatchPredicate<LongPair>() {        @Override        public boolean matches(LongPair userItemPair) {            return userItemPair.getFirst() == userID;        }    });}
1
public boolean matches(LongPair userItemPair)
{    return userItemPair.getFirst() == userID;}
0
public void clear()
{        recommendationCache.clear();    estimatedPrefCache.clear();}
1
public String toString()
{    return "CachingRecommender[recommender:" + recommender + ']';}
0
public Recommendations get(Long key) throws TasteException
{        int howMany = maxHowMany[0];    IDRescorer rescorer = currentRescorer;    List<RecommendedItem> recommendations = rescorer == null ? recommender.recommend(key, howMany, null, currentlyIncludeKnownItems) : recommender.recommend(key, howMany, rescorer, currentlyIncludeKnownItems);    return new Recommendations(Collections.unmodifiableList(recommendations));}
1
public Float get(LongPair key) throws TasteException
{    long userID = key.getFirst();    long itemID = key.getSecond();        return recommender.estimatePreference(userID, itemID);}
1
 List<RecommendedItem> getItems()
{    return items;}
0
 boolean isNoMoreRecommendableItems()
{    return noMoreRecommendableItems;}
0
 void setNoMoreRecommendableItems(boolean noMoreRecommendableItems)
{    this.noMoreRecommendableItems = noMoreRecommendableItems;}
0
public float capEstimate(float estimate)
{    if (estimate > max) {        estimate = max;    } else if (estimate < min) {        estimate = min;    }    return estimate;}
0
protected float doEstimatePreference(long userID, PreferenceArray preferencesFromUser, long itemID) throws TasteException
{    double[] similarities = getSimilarity().itemSimilarities(itemID, preferencesFromUser.getIDs());    boolean foundAPref = false;    double totalSimilarity = 0.0;    for (double theSimilarity : similarities) {        if (!Double.isNaN(theSimilarity)) {            foundAPref = true;            totalSimilarity += theSimilarity;        }    }    return foundAPref ? (float) totalSimilarity : Float.NaN;}
0
public String toString()
{    return "GenericBooleanPrefItemBasedRecommender";}
0
protected float doEstimatePreference(long theUserID, long[] theNeighborhood, long itemID) throws TasteException
{    if (theNeighborhood.length == 0) {        return Float.NaN;    }    DataModel dataModel = getDataModel();    UserSimilarity similarity = getSimilarity();    float totalSimilarity = 0.0f;    boolean foundAPref = false;    for (long userID : theNeighborhood) {                if (userID != theUserID && dataModel.getPreferenceValue(userID, itemID) != null) {            foundAPref = true;            totalSimilarity += (float) similarity.userSimilarity(theUserID, userID);        }    }    return foundAPref ? totalSimilarity : Float.NaN;}
0
protected FastIDSet getAllOtherItems(long[] theNeighborhood, long theUserID, boolean includeKnownItems) throws TasteException
{    DataModel dataModel = getDataModel();    FastIDSet possibleItemIDs = new FastIDSet();    for (long userID : theNeighborhood) {        possibleItemIDs.addAll(dataModel.getItemIDsFromUser(userID));    }    if (!includeKnownItems) {        possibleItemIDs.removeAll(dataModel.getItemIDsFromUser(theUserID));    }    return possibleItemIDs;}
0
public String toString()
{    return "GenericBooleanPrefUserBasedRecommender";}
0
public Void call()
{    capper = buildCapper();    return null;}
0
protected static MostSimilarItemsCandidateItemsStrategy getDefaultMostSimilarItemsCandidateItemsStrategy()
{    return new PreferredItemsNeighborhoodCandidateItemsStrategy();}
0
public ItemSimilarity getSimilarity()
{    return similarity;}
0
public List<RecommendedItem> recommend(long userID, int howMany, IDRescorer rescorer, boolean includeKnownItems) throws TasteException
{    Preconditions.checkArgument(howMany >= 1, "howMany must be at least 1");        PreferenceArray preferencesFromUser = getDataModel().getPreferencesFromUser(userID);    if (preferencesFromUser.length() == 0) {        return Collections.emptyList();    }    FastIDSet possibleItemIDs = getAllOtherItems(userID, preferencesFromUser, includeKnownItems);    TopItems.Estimator<Long> estimator = new Estimator(userID, preferencesFromUser);    List<RecommendedItem> topItems = TopItems.getTopItems(howMany, possibleItemIDs.iterator(), rescorer, estimator);        return topItems;}
1
public float estimatePreference(long userID, long itemID) throws TasteException
{    PreferenceArray preferencesFromUser = getDataModel().getPreferencesFromUser(userID);    Float actualPref = getPreferenceForItem(preferencesFromUser, itemID);    if (actualPref != null) {        return actualPref;    }    return doEstimatePreference(userID, preferencesFromUser, itemID);}
0
private static Float getPreferenceForItem(PreferenceArray preferencesFromUser, long itemID)
{    int size = preferencesFromUser.length();    for (int i = 0; i < size; i++) {        if (preferencesFromUser.getItemID(i) == itemID) {            return preferencesFromUser.getValue(i);        }    }    return null;}
0
public List<RecommendedItem> mostSimilarItems(long itemID, int howMany) throws TasteException
{    return mostSimilarItems(itemID, howMany, null);}
0
public List<RecommendedItem> mostSimilarItems(long itemID, int howMany, Rescorer<LongPair> rescorer) throws TasteException
{    TopItems.Estimator<Long> estimator = new MostSimilarEstimator(itemID, similarity, rescorer);    return doMostSimilarItems(new long[] { itemID }, howMany, estimator);}
0
public List<RecommendedItem> mostSimilarItems(long[] itemIDs, int howMany) throws TasteException
{    TopItems.Estimator<Long> estimator = new MultiMostSimilarEstimator(itemIDs, similarity, null, EXCLUDE_ITEM_IF_NOT_SIMILAR_TO_ALL_BY_DEFAULT);    return doMostSimilarItems(itemIDs, howMany, estimator);}
0
public List<RecommendedItem> mostSimilarItems(long[] itemIDs, int howMany, Rescorer<LongPair> rescorer) throws TasteException
{    TopItems.Estimator<Long> estimator = new MultiMostSimilarEstimator(itemIDs, similarity, rescorer, EXCLUDE_ITEM_IF_NOT_SIMILAR_TO_ALL_BY_DEFAULT);    return doMostSimilarItems(itemIDs, howMany, estimator);}
0
public List<RecommendedItem> mostSimilarItems(long[] itemIDs, int howMany, boolean excludeItemIfNotSimilarToAll) throws TasteException
{    TopItems.Estimator<Long> estimator = new MultiMostSimilarEstimator(itemIDs, similarity, null, excludeItemIfNotSimilarToAll);    return doMostSimilarItems(itemIDs, howMany, estimator);}
0
public List<RecommendedItem> mostSimilarItems(long[] itemIDs, int howMany, Rescorer<LongPair> rescorer, boolean excludeItemIfNotSimilarToAll) throws TasteException
{    TopItems.Estimator<Long> estimator = new MultiMostSimilarEstimator(itemIDs, similarity, rescorer, excludeItemIfNotSimilarToAll);    return doMostSimilarItems(itemIDs, howMany, estimator);}
0
public List<RecommendedItem> recommendedBecause(long userID, long itemID, int howMany) throws TasteException
{    Preconditions.checkArgument(howMany >= 1, "howMany must be at least 1");    DataModel model = getDataModel();    TopItems.Estimator<Long> estimator = new RecommendedBecauseEstimator(userID, itemID);    PreferenceArray prefs = model.getPreferencesFromUser(userID);    int size = prefs.length();    FastIDSet allUserItems = new FastIDSet(size);    for (int i = 0; i < size; i++) {        allUserItems.add(prefs.getItemID(i));    }    allUserItems.remove(itemID);    return TopItems.getTopItems(howMany, allUserItems.iterator(), null, estimator);}
0
private List<RecommendedItem> doMostSimilarItems(long[] itemIDs, int howMany, TopItems.Estimator<Long> estimator) throws TasteException
{    FastIDSet possibleItemIDs = mostSimilarItemsCandidateItemsStrategy.getCandidateItems(itemIDs, getDataModel());    return TopItems.getTopItems(howMany, possibleItemIDs.iterator(), null, estimator);}
0
protected float doEstimatePreference(long userID, PreferenceArray preferencesFromUser, long itemID) throws TasteException
{    double preference = 0.0;    double totalSimilarity = 0.0;    int count = 0;    double[] similarities = similarity.itemSimilarities(itemID, preferencesFromUser.getIDs());    for (int i = 0; i < similarities.length; i++) {        double theSimilarity = similarities[i];        if (!Double.isNaN(theSimilarity)) {                        preference += theSimilarity * preferencesFromUser.getValue(i);            totalSimilarity += theSimilarity;            count++;        }    }        if (count <= 1) {        return Float.NaN;    }    float estimate = (float) (preference / totalSimilarity);    if (capper != null) {        estimate = capper.capEstimate(estimate);    }    return estimate;}
0
public void refresh(Collection<Refreshable> alreadyRefreshed)
{    refreshHelper.refresh(alreadyRefreshed);}
0
public String toString()
{    return "GenericItemBasedRecommender[similarity:" + similarity + ']';}
0
private EstimatedPreferenceCapper buildCapper()
{    DataModel dataModel = getDataModel();    if (Float.isNaN(dataModel.getMinPreference()) && Float.isNaN(dataModel.getMaxPreference())) {        return null;    } else {        return new EstimatedPreferenceCapper(dataModel);    }}
0
public double estimate(Long itemID) throws TasteException
{    LongPair pair = new LongPair(toItemID, itemID);    if (rescorer != null && rescorer.isFiltered(pair)) {        return Double.NaN;    }    double originalEstimate = similarity.itemSimilarity(toItemID, itemID);    return rescorer == null ? originalEstimate : rescorer.rescore(pair, originalEstimate);}
0
public double estimate(Long itemID) throws TasteException
{    return doEstimatePreference(userID, preferencesFromUser, itemID);}
0
public double estimate(Long itemID) throws TasteException
{    RunningAverage average = new FullRunningAverage();    double[] similarities = similarity.itemSimilarities(itemID, toItemIDs);    for (int i = 0; i < toItemIDs.length; i++) {        long toItemID = toItemIDs[i];        LongPair pair = new LongPair(toItemID, itemID);        if (rescorer != null && rescorer.isFiltered(pair)) {            continue;        }        double estimate = similarities[i];        if (rescorer != null) {            estimate = rescorer.rescore(pair, estimate);        }        if (excludeItemIfNotSimilarToAll || !Double.isNaN(estimate)) {            average.addDatum(estimate);        }    }    double averageEstimate = average.getAverage();    return averageEstimate == 0 ? Double.NaN : averageEstimate;}
0
public double estimate(Long itemID) throws TasteException
{    Float pref = getDataModel().getPreferenceValue(userID, itemID);    if (pref == null) {        return Float.NaN;    }    double similarityValue = similarity.itemSimilarity(recommendedItemID, itemID);    return (1.0 + similarityValue) * pref;}
0
public long getItemID()
{    return itemID;}
0
public float getValue()
{    return value;}
0
public String toString()
{    return "RecommendedItem[item:" + itemID + ", value:" + value + ']';}
0
public int hashCode()
{    return (int) itemID ^ RandomUtils.hashFloat(value);}
0
public boolean equals(Object o)
{    if (!(o instanceof GenericRecommendedItem)) {        return false;    }    RecommendedItem other = (RecommendedItem) o;    return itemID == other.getItemID() && value == other.getValue();}
0
public Void call()
{    capper = buildCapper();    return null;}
0
public UserSimilarity getSimilarity()
{    return similarity;}
0
public List<RecommendedItem> recommend(long userID, int howMany, IDRescorer rescorer, boolean includeKnownItems) throws TasteException
{    Preconditions.checkArgument(howMany >= 1, "howMany must be at least 1");        long[] theNeighborhood = neighborhood.getUserNeighborhood(userID);    if (theNeighborhood.length == 0) {        return Collections.emptyList();    }    FastIDSet allItemIDs = getAllOtherItems(theNeighborhood, userID, includeKnownItems);    TopItems.Estimator<Long> estimator = new Estimator(userID, theNeighborhood);    List<RecommendedItem> topItems = TopItems.getTopItems(howMany, allItemIDs.iterator(), rescorer, estimator);        return topItems;}
1
public float estimatePreference(long userID, long itemID) throws TasteException
{    DataModel model = getDataModel();    Float actualPref = model.getPreferenceValue(userID, itemID);    if (actualPref != null) {        return actualPref;    }    long[] theNeighborhood = neighborhood.getUserNeighborhood(userID);    return doEstimatePreference(userID, theNeighborhood, itemID);}
0
public long[] mostSimilarUserIDs(long userID, int howMany) throws TasteException
{    return mostSimilarUserIDs(userID, howMany, null);}
0
public long[] mostSimilarUserIDs(long userID, int howMany, Rescorer<LongPair> rescorer) throws TasteException
{    TopItems.Estimator<Long> estimator = new MostSimilarEstimator(userID, similarity, rescorer);    return doMostSimilarUsers(howMany, estimator);}
0
private long[] doMostSimilarUsers(int howMany, TopItems.Estimator<Long> estimator) throws TasteException
{    DataModel model = getDataModel();    return TopItems.getTopUsers(howMany, model.getUserIDs(), null, estimator);}
0
protected float doEstimatePreference(long theUserID, long[] theNeighborhood, long itemID) throws TasteException
{    if (theNeighborhood.length == 0) {        return Float.NaN;    }    DataModel dataModel = getDataModel();    double preference = 0.0;    double totalSimilarity = 0.0;    int count = 0;    for (long userID : theNeighborhood) {        if (userID != theUserID) {                        Float pref = dataModel.getPreferenceValue(userID, itemID);            if (pref != null) {                double theSimilarity = similarity.userSimilarity(theUserID, userID);                if (!Double.isNaN(theSimilarity)) {                    preference += theSimilarity * pref;                    totalSimilarity += theSimilarity;                    count++;                }            }        }    }        if (count <= 1) {        return Float.NaN;    }    float estimate = (float) (preference / totalSimilarity);    if (capper != null) {        estimate = capper.capEstimate(estimate);    }    return estimate;}
0
protected FastIDSet getAllOtherItems(long[] theNeighborhood, long theUserID, boolean includeKnownItems) throws TasteException
{    DataModel dataModel = getDataModel();    FastIDSet possibleItemIDs = new FastIDSet();    for (long userID : theNeighborhood) {        possibleItemIDs.addAll(dataModel.getItemIDsFromUser(userID));    }    if (!includeKnownItems) {        possibleItemIDs.removeAll(dataModel.getItemIDsFromUser(theUserID));    }    return possibleItemIDs;}
0
public void refresh(Collection<Refreshable> alreadyRefreshed)
{    refreshHelper.refresh(alreadyRefreshed);}
0
public String toString()
{    return "GenericUserBasedRecommender[neighborhood:" + neighborhood + ']';}
0
private EstimatedPreferenceCapper buildCapper()
{    DataModel dataModel = getDataModel();    if (Float.isNaN(dataModel.getMinPreference()) && Float.isNaN(dataModel.getMaxPreference())) {        return null;    } else {        return new EstimatedPreferenceCapper(dataModel);    }}
0
public double estimate(Long userID) throws TasteException
{        if (userID == toUserID) {        return Double.NaN;    }    if (rescorer == null) {        return similarity.userSimilarity(toUserID, userID);    } else {        LongPair pair = new LongPair(toUserID, userID);        if (rescorer.isFiltered(pair)) {            return Double.NaN;        }        double originalEstimate = similarity.userSimilarity(toUserID, userID);        return rescorer.rescore(pair, originalEstimate);    }}
0
public double estimate(Long itemID) throws TasteException
{    return doEstimatePreference(theUserID, theNeighborhood, itemID);}
0
public Object call() throws TasteException
{    buildAverageDiffs();    return null;}
0
public List<RecommendedItem> recommend(long userID, int howMany, IDRescorer rescorer, boolean includeKnownItems) throws TasteException
{    Preconditions.checkArgument(howMany >= 1, "howMany must be at least 1");        PreferenceArray preferencesFromUser = getDataModel().getPreferencesFromUser(userID);    FastIDSet possibleItemIDs = getAllOtherItems(userID, preferencesFromUser, includeKnownItems);    TopItems.Estimator<Long> estimator = new Estimator();    List<RecommendedItem> topItems = TopItems.getTopItems(howMany, possibleItemIDs.iterator(), rescorer, estimator);        return topItems;}
1
public float estimatePreference(long userID, long itemID) throws TasteException
{    DataModel dataModel = getDataModel();    Float actualPref = dataModel.getPreferenceValue(userID, itemID);    if (actualPref != null) {        return actualPref;    }    return doEstimatePreference(itemID);}
0
private float doEstimatePreference(long itemID)
{    buildAveragesLock.readLock().lock();    try {        RunningAverage average = itemAverages.get(itemID);        return average == null ? Float.NaN : (float) average.getAverage();    } finally {        buildAveragesLock.readLock().unlock();    }}
0
private void buildAverageDiffs() throws TasteException
{    try {        buildAveragesLock.writeLock().lock();        DataModel dataModel = getDataModel();        LongPrimitiveIterator it = dataModel.getUserIDs();        while (it.hasNext()) {            PreferenceArray prefs = dataModel.getPreferencesFromUser(it.nextLong());            int size = prefs.length();            for (int i = 0; i < size; i++) {                long itemID = prefs.getItemID(i);                RunningAverage average = itemAverages.get(itemID);                if (average == null) {                    average = new FullRunningAverage();                    itemAverages.put(itemID, average);                }                average.addDatum(prefs.getValue(i));            }        }    } finally {        buildAveragesLock.writeLock().unlock();    }}
0
public void setPreference(long userID, long itemID, float value) throws TasteException
{    DataModel dataModel = getDataModel();    double prefDelta;    try {        Float oldPref = dataModel.getPreferenceValue(userID, itemID);        prefDelta = oldPref == null ? value : value - oldPref;    } catch (NoSuchUserException nsee) {        prefDelta = value;    }    super.setPreference(userID, itemID, value);    try {        buildAveragesLock.writeLock().lock();        RunningAverage average = itemAverages.get(itemID);        if (average == null) {            RunningAverage newAverage = new FullRunningAverage();            newAverage.addDatum(prefDelta);            itemAverages.put(itemID, newAverage);        } else {            average.changeDatum(prefDelta);        }    } finally {        buildAveragesLock.writeLock().unlock();    }}
0
public void removePreference(long userID, long itemID) throws TasteException
{    DataModel dataModel = getDataModel();    Float oldPref = dataModel.getPreferenceValue(userID, itemID);    super.removePreference(userID, itemID);    if (oldPref != null) {        try {            buildAveragesLock.writeLock().lock();            RunningAverage average = itemAverages.get(itemID);            if (average == null) {                throw new IllegalStateException("No preferences exist for item ID: " + itemID);            } else {                average.removeDatum(oldPref);            }        } finally {            buildAveragesLock.writeLock().unlock();        }    }}
0
public void refresh(Collection<Refreshable> alreadyRefreshed)
{    refreshHelper.refresh(alreadyRefreshed);}
0
public String toString()
{    return "ItemAverageRecommender";}
0
public double estimate(Long itemID)
{    return doEstimatePreference(itemID);}
0
public Object call() throws TasteException
{    buildAverageDiffs();    return null;}
0
public List<RecommendedItem> recommend(long userID, int howMany, IDRescorer rescorer, boolean includeKnownItems) throws TasteException
{    Preconditions.checkArgument(howMany >= 1, "howMany must be at least 1");        PreferenceArray preferencesFromUser = getDataModel().getPreferencesFromUser(userID);    FastIDSet possibleItemIDs = getAllOtherItems(userID, preferencesFromUser, includeKnownItems);    TopItems.Estimator<Long> estimator = new Estimator(userID);    List<RecommendedItem> topItems = TopItems.getTopItems(howMany, possibleItemIDs.iterator(), rescorer, estimator);        return topItems;}
1
public float estimatePreference(long userID, long itemID) throws TasteException
{    DataModel dataModel = getDataModel();    Float actualPref = dataModel.getPreferenceValue(userID, itemID);    if (actualPref != null) {        return actualPref;    }    return doEstimatePreference(userID, itemID);}
0
private float doEstimatePreference(long userID, long itemID)
{    buildAveragesLock.readLock().lock();    try {        RunningAverage itemAverage = itemAverages.get(itemID);        if (itemAverage == null) {            return Float.NaN;        }        RunningAverage userAverage = userAverages.get(userID);        if (userAverage == null) {            return Float.NaN;        }        double userDiff = userAverage.getAverage() - overallAveragePrefValue.getAverage();        return (float) (itemAverage.getAverage() + userDiff);    } finally {        buildAveragesLock.readLock().unlock();    }}
0
private void buildAverageDiffs() throws TasteException
{    try {        buildAveragesLock.writeLock().lock();        DataModel dataModel = getDataModel();        LongPrimitiveIterator it = dataModel.getUserIDs();        while (it.hasNext()) {            long userID = it.nextLong();            PreferenceArray prefs = dataModel.getPreferencesFromUser(userID);            int size = prefs.length();            for (int i = 0; i < size; i++) {                long itemID = prefs.getItemID(i);                float value = prefs.getValue(i);                addDatumAndCreateIfNeeded(itemID, value, itemAverages);                addDatumAndCreateIfNeeded(userID, value, userAverages);                overallAveragePrefValue.addDatum(value);            }        }    } finally {        buildAveragesLock.writeLock().unlock();    }}
0
private static void addDatumAndCreateIfNeeded(long itemID, float value, FastByIDMap<RunningAverage> averages)
{    RunningAverage itemAverage = averages.get(itemID);    if (itemAverage == null) {        itemAverage = new FullRunningAverage();        averages.put(itemID, itemAverage);    }    itemAverage.addDatum(value);}
0
public void setPreference(long userID, long itemID, float value) throws TasteException
{    DataModel dataModel = getDataModel();    double prefDelta;    try {        Float oldPref = dataModel.getPreferenceValue(userID, itemID);        prefDelta = oldPref == null ? value : value - oldPref;    } catch (NoSuchUserException nsee) {        prefDelta = value;    }    super.setPreference(userID, itemID, value);    try {        buildAveragesLock.writeLock().lock();        RunningAverage itemAverage = itemAverages.get(itemID);        if (itemAverage == null) {            RunningAverage newItemAverage = new FullRunningAverage();            newItemAverage.addDatum(prefDelta);            itemAverages.put(itemID, newItemAverage);        } else {            itemAverage.changeDatum(prefDelta);        }        RunningAverage userAverage = userAverages.get(userID);        if (userAverage == null) {            RunningAverage newUserAveragae = new FullRunningAverage();            newUserAveragae.addDatum(prefDelta);            userAverages.put(userID, newUserAveragae);        } else {            userAverage.changeDatum(prefDelta);        }        overallAveragePrefValue.changeDatum(prefDelta);    } finally {        buildAveragesLock.writeLock().unlock();    }}
0
public void removePreference(long userID, long itemID) throws TasteException
{    DataModel dataModel = getDataModel();    Float oldPref = dataModel.getPreferenceValue(userID, itemID);    super.removePreference(userID, itemID);    if (oldPref != null) {        try {            buildAveragesLock.writeLock().lock();            RunningAverage itemAverage = itemAverages.get(itemID);            if (itemAverage == null) {                throw new IllegalStateException("No preferences exist for item ID: " + itemID);            }            itemAverage.removeDatum(oldPref);            RunningAverage userAverage = userAverages.get(userID);            if (userAverage == null) {                throw new IllegalStateException("No preferences exist for user ID: " + userID);            }            userAverage.removeDatum(oldPref);            overallAveragePrefValue.removeDatum(oldPref);        } finally {            buildAveragesLock.writeLock().unlock();        }    }}
0
public void refresh(Collection<Refreshable> alreadyRefreshed)
{    refreshHelper.refresh(alreadyRefreshed);}
0
public String toString()
{    return "ItemUserAverageRecommender";}
0
public double estimate(Long itemID)
{    return doEstimatePreference(userID, itemID);}
0
public static IDRescorer getItemInstance()
{    return USER_OR_ITEM_INSTANCE;}
0
public static IDRescorer getUserInstance()
{    return USER_OR_ITEM_INSTANCE;}
0
public static Rescorer<LongPair> getItemItemPairInstance()
{    return ITEM_ITEM_PAIR_INSTANCE;}
0
public static Rescorer<LongPair> getUserUserPairInstance()
{    return USER_USER_PAIR_INSTANCE;}
0
public double rescore(T thing, double originalScore)
{    return originalScore;}
0
public boolean isFiltered(T thing)
{    return false;}
0
public double rescore(long id, double originalScore)
{    return originalScore;}
0
public boolean isFiltered(long id)
{    return false;}
0
public String toString()
{    return "NullRescorer";}
0
protected FastIDSet doGetCandidateItems(long[] preferredItemIDs, DataModel dataModel, boolean includeKnownItems) throws TasteException
{    FastIDSet possibleItemsIDs = new FastIDSet();    for (long itemID : preferredItemIDs) {        PreferenceArray itemPreferences = dataModel.getPreferencesForItem(itemID);        int numUsersPreferringItem = itemPreferences.length();        for (int index = 0; index < numUsersPreferringItem; index++) {            possibleItemsIDs.addAll(dataModel.getItemIDsFromUser(itemPreferences.getUserID(index)));        }    }    if (!includeKnownItems) {        possibleItemsIDs.removeAll(preferredItemIDs);    }    return possibleItemsIDs;}
0
public List<RecommendedItem> recommend(long userID, int howMany, IDRescorer rescorer, boolean includeKnownItems) throws TasteException
{    DataModel dataModel = getDataModel();    int numItems = dataModel.getNumItems();    List<RecommendedItem> result = new ArrayList<>(howMany);    while (result.size() < howMany) {        LongPrimitiveIterator it = dataModel.getItemIDs();        it.skip(random.nextInt(numItems));        long itemID = it.next();        if (includeKnownItems || dataModel.getPreferenceValue(userID, itemID) == null) {            result.add(new GenericRecommendedItem(itemID, randomPref()));        }    }    return result;}
0
public float estimatePreference(long userID, long itemID)
{    return randomPref();}
0
private float randomPref()
{    return minPref + random.nextFloat() * (maxPref - minPref);}
0
public void refresh(Collection<Refreshable> alreadyRefreshed)
{    getDataModel().refresh(alreadyRefreshed);}
0
private static int computeMaxFrom(int factor, int numThings)
{    if (factor == NO_LIMIT_FACTOR) {        return MAX_LIMIT;    }    long max = (long) (factor * (1.0 + Math.log(numThings) / LOG2));    return max > MAX_LIMIT ? MAX_LIMIT : (int) max;}
0
protected FastIDSet doGetCandidateItems(long[] preferredItemIDs, DataModel dataModel, boolean includeKnownItems) throws TasteException
{    LongPrimitiveIterator preferredItemIDsIterator = new LongPrimitiveArrayIterator(preferredItemIDs);    if (preferredItemIDs.length > maxItems) {        double samplingRate = (double) maxItems / preferredItemIDs.length;                preferredItemIDsIterator = new SamplingLongPrimitiveIterator(preferredItemIDsIterator, samplingRate);    }    FastIDSet possibleItemsIDs = new FastIDSet();    while (preferredItemIDsIterator.hasNext()) {        long itemID = preferredItemIDsIterator.nextLong();        PreferenceArray prefs = dataModel.getPreferencesForItem(itemID);        int prefsLength = prefs.length();        if (prefsLength > maxUsersPerItem) {            Iterator<Preference> sampledPrefs = new FixedSizeSamplingIterator<>(maxUsersPerItem, prefs.iterator());            while (sampledPrefs.hasNext()) {                addSomeOf(possibleItemsIDs, dataModel.getItemIDsFromUser(sampledPrefs.next().getUserID()));            }        } else {            for (int i = 0; i < prefsLength; i++) {                addSomeOf(possibleItemsIDs, dataModel.getItemIDsFromUser(prefs.getUserID(i)));            }        }    }    if (!includeKnownItems) {        possibleItemsIDs.removeAll(preferredItemIDs);    }    return possibleItemsIDs;}
0
private void addSomeOf(FastIDSet possibleItemIDs, FastIDSet itemIDs)
{    if (itemIDs.size() > maxItemsPerUser) {        LongPrimitiveIterator it = new SamplingLongPrimitiveIterator(itemIDs.iterator(), (double) maxItemsPerUser / itemIDs.size());        while (it.hasNext()) {            possibleItemIDs.add(it.nextLong());        }    } else {        possibleItemIDs.addAll(itemIDs);    }}
0
 long getUserID()
{    return userID;}
0
 double getSimilarity()
{    return similarity;}
0
public int hashCode()
{    return (int) userID ^ RandomUtils.hashDouble(similarity);}
0
public boolean equals(Object o)
{    if (!(o instanceof SimilarUser)) {        return false;    }    SimilarUser other = (SimilarUser) o;    return userID == other.getUserID() && similarity == other.getSimilarity();}
0
public String toString()
{    return "SimilarUser[user:" + userID + ", similarity:" + similarity + ']';}
0
public int compareTo(SimilarUser other)
{    double otherSimilarity = other.getSimilarity();    if (similarity > otherSimilarity) {        return -1;    }    if (similarity < otherSimilarity) {        return 1;    }    long otherUserID = other.getUserID();    if (userID < otherUserID) {        return -1;    }    if (userID > otherUserID) {        return 1;    }    return 0;}
0
public Object call() throws TasteException
{    buildMappings();    return null;}
0
private void buildMappings() throws TasteException
{    userIDMapping = createIDMapping(dataModel.getNumUsers(), dataModel.getUserIDs());    itemIDMapping = createIDMapping(dataModel.getNumItems(), dataModel.getItemIDs());}
0
protected Factorization createFactorization(double[][] userFeatures, double[][] itemFeatures)
{    return new Factorization(userIDMapping, itemIDMapping, userFeatures, itemFeatures);}
0
protected Integer userIndex(long userID)
{    Integer userIndex = userIDMapping.get(userID);    if (userIndex == null) {        userIndex = userIDMapping.size();        userIDMapping.put(userID, userIndex);    }    return userIndex;}
0
protected Integer itemIndex(long itemID)
{    Integer itemIndex = itemIDMapping.get(itemID);    if (itemIndex == null) {        itemIndex = itemIDMapping.size();        itemIDMapping.put(itemID, itemIndex);    }    return itemIndex;}
0
private static FastByIDMap<Integer> createIDMapping(int size, LongPrimitiveIterator idIterator)
{    FastByIDMap<Integer> mapping = new FastByIDMap<>(size);    int index = 0;    while (idIterator.hasNext()) {        mapping.put(idIterator.nextLong(), index++);    }    return mapping;}
0
public void refresh(Collection<Refreshable> alreadyRefreshed)
{    refreshHelper.refresh(alreadyRefreshed);}
0
 double[][] getM()
{    return M;}
0
 double[][] getU()
{    return U;}
0
 Vector getUserFeatureColumn(int index)
{    return new DenseVector(U[index]);}
0
 Vector getItemFeatureColumn(int index)
{    return new DenseVector(M[index]);}
0
 void setFeatureColumnInU(int idIndex, Vector vector)
{    setFeatureColumn(U, idIndex, vector);}
0
 void setFeatureColumnInM(int idIndex, Vector vector)
{    setFeatureColumn(M, idIndex, vector);}
0
protected void setFeatureColumn(double[][] matrix, int idIndex, Vector vector)
{    for (int feature = 0; feature < numFeatures; feature++) {        matrix[idIndex][feature] = vector.get(feature);    }}
0
protected double averateRating(long itemID) throws TasteException
{    PreferenceArray prefs = dataModel.getPreferencesForItem(itemID);    RunningAverage avg = new FullRunningAverage();    for (Preference pref : prefs) {        avg.addDatum(pref.getValue());    }    return avg.getAverage();}
0
public Factorization factorize() throws TasteException
{        final Features features = new Features(this);    /* feature maps necessary for solving for implicit feedback */    OpenIntObjectHashMap<Vector> userY = null;    OpenIntObjectHashMap<Vector> itemY = null;    if (usesImplicitFeedback) {        userY = userFeaturesMapping(dataModel.getUserIDs(), dataModel.getNumUsers(), features.getU());        itemY = itemFeaturesMapping(dataModel.getItemIDs(), dataModel.getNumItems(), features.getM());    }    for (int iteration = 0; iteration < numIterations; iteration++) {                /* fix M - compute U */        ExecutorService queue = createQueue();        LongPrimitiveIterator userIDsIterator = dataModel.getUserIDs();        try {            final ImplicitFeedbackAlternatingLeastSquaresSolver implicitFeedbackSolver = usesImplicitFeedback ? new ImplicitFeedbackAlternatingLeastSquaresSolver(numFeatures, lambda, alpha, itemY, numTrainingThreads) : null;            while (userIDsIterator.hasNext()) {                final long userID = userIDsIterator.nextLong();                final LongPrimitiveIterator itemIDsFromUser = dataModel.getItemIDsFromUser(userID).iterator();                final PreferenceArray userPrefs = dataModel.getPreferencesFromUser(userID);                queue.execute(new Runnable() {                    @Override                    public void run() {                        List<Vector> featureVectors = new ArrayList<>();                        while (itemIDsFromUser.hasNext()) {                            long itemID = itemIDsFromUser.nextLong();                            featureVectors.add(features.getItemFeatureColumn(itemIndex(itemID)));                        }                        Vector userFeatures = usesImplicitFeedback ? implicitFeedbackSolver.solve(sparseUserRatingVector(userPrefs)) : AlternatingLeastSquaresSolver.solve(featureVectors, ratingVector(userPrefs), lambda, numFeatures);                        features.setFeatureColumnInU(userIndex(userID), userFeatures);                    }                });            }        } finally {            queue.shutdown();            try {                queue.awaitTermination(dataModel.getNumUsers(), TimeUnit.SECONDS);            } catch (InterruptedException e) {                            }        }        /* fix U - compute M */        queue = createQueue();        LongPrimitiveIterator itemIDsIterator = dataModel.getItemIDs();        try {            final ImplicitFeedbackAlternatingLeastSquaresSolver implicitFeedbackSolver = usesImplicitFeedback ? new ImplicitFeedbackAlternatingLeastSquaresSolver(numFeatures, lambda, alpha, userY, numTrainingThreads) : null;            while (itemIDsIterator.hasNext()) {                final long itemID = itemIDsIterator.nextLong();                final PreferenceArray itemPrefs = dataModel.getPreferencesForItem(itemID);                queue.execute(new Runnable() {                    @Override                    public void run() {                        List<Vector> featureVectors = new ArrayList<>();                        for (Preference pref : itemPrefs) {                            long userID = pref.getUserID();                            featureVectors.add(features.getUserFeatureColumn(userIndex(userID)));                        }                        Vector itemFeatures = usesImplicitFeedback ? implicitFeedbackSolver.solve(sparseItemRatingVector(itemPrefs)) : AlternatingLeastSquaresSolver.solve(featureVectors, ratingVector(itemPrefs), lambda, numFeatures);                        features.setFeatureColumnInM(itemIndex(itemID), itemFeatures);                    }                });            }        } finally {            queue.shutdown();            try {                queue.awaitTermination(dataModel.getNumItems(), TimeUnit.SECONDS);            } catch (InterruptedException e) {                            }        }    }        return createFactorization(features.getU(), features.getM());}
1
public void run()
{    List<Vector> featureVectors = new ArrayList<>();    while (itemIDsFromUser.hasNext()) {        long itemID = itemIDsFromUser.nextLong();        featureVectors.add(features.getItemFeatureColumn(itemIndex(itemID)));    }    Vector userFeatures = usesImplicitFeedback ? implicitFeedbackSolver.solve(sparseUserRatingVector(userPrefs)) : AlternatingLeastSquaresSolver.solve(featureVectors, ratingVector(userPrefs), lambda, numFeatures);    features.setFeatureColumnInU(userIndex(userID), userFeatures);}
0
public void run()
{    List<Vector> featureVectors = new ArrayList<>();    for (Preference pref : itemPrefs) {        long userID = pref.getUserID();        featureVectors.add(features.getUserFeatureColumn(userIndex(userID)));    }    Vector itemFeatures = usesImplicitFeedback ? implicitFeedbackSolver.solve(sparseItemRatingVector(itemPrefs)) : AlternatingLeastSquaresSolver.solve(featureVectors, ratingVector(itemPrefs), lambda, numFeatures);    features.setFeatureColumnInM(itemIndex(itemID), itemFeatures);}
0
protected ExecutorService createQueue()
{    return Executors.newFixedThreadPool(numTrainingThreads);}
0
protected static Vector ratingVector(PreferenceArray prefs)
{    double[] ratings = new double[prefs.length()];    for (int n = 0; n < prefs.length(); n++) {        ratings[n] = prefs.get(n).getValue();    }    return new DenseVector(ratings, true);}
0
protected OpenIntObjectHashMap<Vector> itemFeaturesMapping(LongPrimitiveIterator itemIDs, int numItems, double[][] featureMatrix)
{    OpenIntObjectHashMap<Vector> mapping = new OpenIntObjectHashMap<>(numItems);    while (itemIDs.hasNext()) {        long itemID = itemIDs.next();        int itemIndex = itemIndex(itemID);        mapping.put(itemIndex, new DenseVector(featureMatrix[itemIndex(itemID)], true));    }    return mapping;}
0
protected OpenIntObjectHashMap<Vector> userFeaturesMapping(LongPrimitiveIterator userIDs, int numUsers, double[][] featureMatrix)
{    OpenIntObjectHashMap<Vector> mapping = new OpenIntObjectHashMap<>(numUsers);    while (userIDs.hasNext()) {        long userID = userIDs.next();        int userIndex = userIndex(userID);        mapping.put(userIndex, new DenseVector(featureMatrix[userIndex(userID)], true));    }    return mapping;}
0
protected Vector sparseItemRatingVector(PreferenceArray prefs)
{    SequentialAccessSparseVector ratings = new SequentialAccessSparseVector(Integer.MAX_VALUE, prefs.length());    for (Preference preference : prefs) {        ratings.set(userIndex(preference.getUserID()), preference.getValue());    }    return ratings;}
0
protected Vector sparseUserRatingVector(PreferenceArray prefs)
{    SequentialAccessSparseVector ratings = new SequentialAccessSparseVector(Integer.MAX_VALUE, prefs.length());    for (Preference preference : prefs) {        ratings.set(itemIndex(preference.getItemID()), preference.getValue());    }    return ratings;}
0
public double[][] allUserFeatures()
{    return userFeatures;}
0
public double[] getUserFeatures(long userID) throws NoSuchUserException
{    Integer index = userIDMapping.get(userID);    if (index == null) {        throw new NoSuchUserException(userID);    }    return userFeatures[index];}
0
public double[][] allItemFeatures()
{    return itemFeatures;}
0
public double[] getItemFeatures(long itemID) throws NoSuchItemException
{    Integer index = itemIDMapping.get(itemID);    if (index == null) {        throw new NoSuchItemException(itemID);    }    return itemFeatures[index];}
0
public int userIndex(long userID) throws NoSuchUserException
{    Integer index = userIDMapping.get(userID);    if (index == null) {        throw new NoSuchUserException(userID);    }    return index;}
0
public Iterable<Map.Entry<Long, Integer>> getUserIDMappings()
{    return userIDMapping.entrySet();}
0
public LongPrimitiveIterator getUserIDMappingKeys()
{    return userIDMapping.keySetIterator();}
0
public int itemIndex(long itemID) throws NoSuchItemException
{    Integer index = itemIDMapping.get(itemID);    if (index == null) {        throw new NoSuchItemException(itemID);    }    return index;}
0
public Iterable<Map.Entry<Long, Integer>> getItemIDMappings()
{    return itemIDMapping.entrySet();}
0
public LongPrimitiveIterator getItemIDMappingKeys()
{    return itemIDMapping.keySetIterator();}
0
public int numFeatures()
{    return userFeatures.length > 0 ? userFeatures[0].length : 0;}
0
public int numUsers()
{    return userIDMapping.size();}
0
public int numItems()
{    return itemIDMapping.size();}
0
public boolean equals(Object o)
{    if (o instanceof Factorization) {        Factorization other = (Factorization) o;        return userIDMapping.equals(other.userIDMapping) && itemIDMapping.equals(other.itemIDMapping) && Arrays.deepEquals(userFeatures, other.userFeatures) && Arrays.deepEquals(itemFeatures, other.itemFeatures);    }    return false;}
0
public int hashCode()
{    int hashCode = 31 * userIDMapping.hashCode() + itemIDMapping.hashCode();    hashCode = 31 * hashCode + Arrays.deepHashCode(userFeatures);    hashCode = 31 * hashCode + Arrays.deepHashCode(itemFeatures);    return hashCode;}
0
public Factorization load() throws IOException
{    if (!file.exists()) {                return null;    }    try (DataInputStream in = new DataInputStream(new BufferedInputStream(new FileInputStream(file)))) {                return readBinary(in);    }}
1
public void maybePersist(Factorization factorization) throws IOException
{    try (DataOutputStream out = new DataOutputStream(new BufferedOutputStream(new FileOutputStream(file)))) {                writeBinary(factorization, out);    }}
1
protected static void writeBinary(Factorization factorization, DataOutput out) throws IOException
{    out.writeInt(factorization.numFeatures());    out.writeInt(factorization.numUsers());    out.writeInt(factorization.numItems());    for (Map.Entry<Long, Integer> mappingEntry : factorization.getUserIDMappings()) {        long userID = mappingEntry.getKey();        out.writeInt(mappingEntry.getValue());        out.writeLong(userID);        try {            double[] userFeatures = factorization.getUserFeatures(userID);            for (int feature = 0; feature < factorization.numFeatures(); feature++) {                out.writeDouble(userFeatures[feature]);            }        } catch (NoSuchUserException e) {            throw new IOException("Unable to persist factorization", e);        }    }    for (Map.Entry<Long, Integer> entry : factorization.getItemIDMappings()) {        long itemID = entry.getKey();        out.writeInt(entry.getValue());        out.writeLong(itemID);        try {            double[] itemFeatures = factorization.getItemFeatures(itemID);            for (int feature = 0; feature < factorization.numFeatures(); feature++) {                out.writeDouble(itemFeatures[feature]);            }        } catch (NoSuchItemException e) {            throw new IOException("Unable to persist factorization", e);        }    }}
0
public static Factorization readBinary(DataInput in) throws IOException
{    int numFeatures = in.readInt();    int numUsers = in.readInt();    int numItems = in.readInt();    FastByIDMap<Integer> userIDMapping = new FastByIDMap<>(numUsers);    double[][] userFeatures = new double[numUsers][numFeatures];    for (int n = 0; n < numUsers; n++) {        int userIndex = in.readInt();        long userID = in.readLong();        userIDMapping.put(userID, userIndex);        for (int feature = 0; feature < numFeatures; feature++) {            userFeatures[userIndex][feature] = in.readDouble();        }    }    FastByIDMap<Integer> itemIDMapping = new FastByIDMap<>(numItems);    double[][] itemFeatures = new double[numItems][numFeatures];    for (int n = 0; n < numItems; n++) {        int itemIndex = in.readInt();        long itemID = in.readLong();        itemIDMapping.put(itemID, itemIndex);        for (int feature = 0; feature < numFeatures; feature++) {            itemFeatures[itemIndex][feature] = in.readDouble();        }    }    return new Factorization(userIDMapping, itemIDMapping, userFeatures, itemFeatures);}
0
public Factorization load() throws IOException
{    return null;}
0
public void maybePersist(Factorization factorization) throws IOException
{}
0
private int countPreferences(DataModel dataModel) throws TasteException
{    int numPreferences = 0;    LongPrimitiveIterator userIDs = dataModel.getUserIDs();    while (userIDs.hasNext()) {        PreferenceArray preferencesFromUser = dataModel.getPreferencesFromUser(userIDs.nextLong());        numPreferences += preferencesFromUser.length();    }    return numPreferences;}
0
private void cachePreferences(DataModel dataModel) throws TasteException
{    int numPreferences = countPreferences(dataModel);    preferences = new Preference[numPreferences];    LongPrimitiveIterator userIDs = dataModel.getUserIDs();    int index = 0;    while (userIDs.hasNext()) {        long userID = userIDs.nextLong();        PreferenceArray preferencesFromUser = dataModel.getPreferencesFromUser(userID);        for (Preference preference : preferencesFromUser) {            preferences[index++] = preference;        }    }}
0
public final void shuffle()
{    unstagedPreferences = preferences.clone();    /* Durstenfeld shuffle */    for (int i = unstagedPreferences.length - 1; i > 0; i--) {        int rand = random.nextInt(i + 1);        swapCachedPreferences(i, rand);    }}
0
private void swapCachedPreferences(int x, int y)
{    Preference p = unstagedPreferences[x];    unstagedPreferences[x] = unstagedPreferences[y];    unstagedPreferences[y] = p;}
0
public final void stage()
{    preferences = unstagedPreferences;}
0
public Preference get(int i)
{    return preferences[i];}
0
public int size()
{    return preferences.length;}
0
protected void initialize() throws TasteException
{    RandomWrapper random = RandomUtils.getRandom();    userVectors = new double[dataModel.getNumUsers()][rank];    itemVectors = new double[dataModel.getNumItems()][rank];    double globalAverage = getAveragePreference();    for (int userIndex = 0; userIndex < userVectors.length; userIndex++) {        userVectors[userIndex][0] = globalAverage;                userVectors[userIndex][USER_BIAS_INDEX] = 0;                userVectors[userIndex][ITEM_BIAS_INDEX] = 1;        for (int feature = FEATURE_OFFSET; feature < rank; feature++) {            userVectors[userIndex][feature] = random.nextGaussian() * NOISE;        }    }    for (int itemIndex = 0; itemIndex < itemVectors.length; itemIndex++) {                itemVectors[itemIndex][0] = 1;                itemVectors[itemIndex][USER_BIAS_INDEX] = 1;                itemVectors[itemIndex][ITEM_BIAS_INDEX] = 0;        for (int feature = FEATURE_OFFSET; feature < rank; feature++) {            itemVectors[itemIndex][feature] = random.nextGaussian() * NOISE;        }    }}
0
private double getMu(int i)
{    return mu0 * Math.pow(decayFactor, i - 1) * Math.pow(i + stepOffset, forgettingExponent);}
0
public Factorization factorize() throws TasteException
{    initialize();    if (logger.isInfoEnabled()) {            }    for (epoch = 1; epoch <= numEpochs; epoch++) {        shuffler.stage();        final double mu = getMu(epoch);        int subSize = shuffler.size() / numThreads + 1;        ExecutorService executor = Executors.newFixedThreadPool(numThreads);        try {            for (int t = 0; t < numThreads; t++) {                final int iStart = t * subSize;                final int iEnd = Math.min((t + 1) * subSize, shuffler.size());                executor.execute(new Runnable() {                    @Override                    public void run() {                        for (int i = iStart; i < iEnd; i++) {                            update(shuffler.get(i), mu);                        }                    }                });            }        } finally {            executor.shutdown();            shuffler.shuffle();            try {                boolean terminated = executor.awaitTermination(numEpochs * shuffler.size(), TimeUnit.MICROSECONDS);                if (!terminated) {                                    }            } catch (InterruptedException e) {                throw new TasteException("waiting fof termination interrupted", e);            }        }    }    return createFactorization(userVectors, itemVectors);}
1
public void run()
{    for (int i = iStart; i < iEnd; i++) {        update(shuffler.get(i), mu);    }}
0
 double getAveragePreference() throws TasteException
{    RunningAverage average = new FullRunningAverage();    LongPrimitiveIterator it = dataModel.getUserIDs();    while (it.hasNext()) {        for (Preference pref : dataModel.getPreferencesFromUser(it.nextLong())) {            average.addDatum(pref.getValue());        }    }    return average.getAverage();}
0
protected void update(Preference preference, double mu)
{    int userIndex = userIndex(preference.getUserID());    int itemIndex = itemIndex(preference.getItemID());    double[] userVector = userVectors[userIndex];    double[] itemVector = itemVectors[itemIndex];    double prediction = dot(userVector, itemVector);    double err = preference.getValue() - prediction;        for (int k = FEATURE_OFFSET; k < rank; k++) {        double userFeature = userVector[k];        double itemFeature = itemVector[k];        userVector[k] += mu * (err * itemFeature - lambda * userFeature);        itemVector[k] += mu * (err * userFeature - lambda * itemFeature);    }        userVector[USER_BIAS_INDEX] += biasMuRatio * mu * (err - biasLambdaRatio * lambda * userVector[USER_BIAS_INDEX]);    itemVector[ITEM_BIAS_INDEX] += biasMuRatio * mu * (err - biasLambdaRatio * lambda * itemVector[ITEM_BIAS_INDEX]);}
0
private double dot(double[] userVector, double[] itemVector)
{    double sum = 0;    for (int k = 0; k < rank; k++) {        sum += userVector[k] * itemVector[k];    }    return sum;}
0
protected void prepareTraining() throws TasteException
{    RandomWrapper random = RandomUtils.getRandom();    userVectors = new double[dataModel.getNumUsers()][numFeatures];    itemVectors = new double[dataModel.getNumItems()][numFeatures];    double globalAverage = getAveragePreference();    for (int userIndex = 0; userIndex < userVectors.length; userIndex++) {        userVectors[userIndex][0] = globalAverage;                userVectors[userIndex][USER_BIAS_INDEX] = 0;                userVectors[userIndex][ITEM_BIAS_INDEX] = 1;        for (int feature = FEATURE_OFFSET; feature < numFeatures; feature++) {            userVectors[userIndex][feature] = random.nextGaussian() * randomNoise;        }    }    for (int itemIndex = 0; itemIndex < itemVectors.length; itemIndex++) {                itemVectors[itemIndex][0] = 1;                itemVectors[itemIndex][USER_BIAS_INDEX] = 1;                itemVectors[itemIndex][ITEM_BIAS_INDEX] = 0;        for (int feature = FEATURE_OFFSET; feature < numFeatures; feature++) {            itemVectors[itemIndex][feature] = random.nextGaussian() * randomNoise;        }    }    cachePreferences();    shufflePreferences();}
0
private int countPreferences() throws TasteException
{    int numPreferences = 0;    LongPrimitiveIterator userIDs = dataModel.getUserIDs();    while (userIDs.hasNext()) {        PreferenceArray preferencesFromUser = dataModel.getPreferencesFromUser(userIDs.nextLong());        numPreferences += preferencesFromUser.length();    }    return numPreferences;}
0
private void cachePreferences() throws TasteException
{    int numPreferences = countPreferences();    cachedUserIDs = new long[numPreferences];    cachedItemIDs = new long[numPreferences];    LongPrimitiveIterator userIDs = dataModel.getUserIDs();    int index = 0;    while (userIDs.hasNext()) {        long userID = userIDs.nextLong();        PreferenceArray preferencesFromUser = dataModel.getPreferencesFromUser(userID);        for (Preference preference : preferencesFromUser) {            cachedUserIDs[index] = userID;            cachedItemIDs[index] = preference.getItemID();            index++;        }    }}
0
protected void shufflePreferences()
{    RandomWrapper random = RandomUtils.getRandom();    /* Durstenfeld shuffle */    for (int currentPos = cachedUserIDs.length - 1; currentPos > 0; currentPos--) {        int swapPos = random.nextInt(currentPos + 1);        swapCachedPreferences(currentPos, swapPos);    }}
0
private void swapCachedPreferences(int posA, int posB)
{    long tmpUserIndex = cachedUserIDs[posA];    long tmpItemIndex = cachedItemIDs[posA];    cachedUserIDs[posA] = cachedUserIDs[posB];    cachedItemIDs[posA] = cachedItemIDs[posB];    cachedUserIDs[posB] = tmpUserIndex;    cachedItemIDs[posB] = tmpItemIndex;}
0
public Factorization factorize() throws TasteException
{    prepareTraining();    double currentLearningRate = learningRate;    for (int it = 0; it < numIterations; it++) {        for (int index = 0; index < cachedUserIDs.length; index++) {            long userId = cachedUserIDs[index];            long itemId = cachedItemIDs[index];            float rating = dataModel.getPreferenceValue(userId, itemId);            updateParameters(userId, itemId, rating, currentLearningRate);        }        currentLearningRate *= learningRateDecay;    }    return createFactorization(userVectors, itemVectors);}
0
 double getAveragePreference() throws TasteException
{    RunningAverage average = new FullRunningAverage();    LongPrimitiveIterator it = dataModel.getUserIDs();    while (it.hasNext()) {        for (Preference pref : dataModel.getPreferencesFromUser(it.nextLong())) {            average.addDatum(pref.getValue());        }    }    return average.getAverage();}
0
protected void updateParameters(long userID, long itemID, float rating, double currentLearningRate)
{    int userIndex = userIndex(userID);    int itemIndex = itemIndex(itemID);    double[] userVector = userVectors[userIndex];    double[] itemVector = itemVectors[itemIndex];    double prediction = predictRating(userIndex, itemIndex);    double err = rating - prediction;        userVector[USER_BIAS_INDEX] += biasLearningRate * currentLearningRate * (err - biasReg * preventOverfitting * userVector[USER_BIAS_INDEX]);        itemVector[ITEM_BIAS_INDEX] += biasLearningRate * currentLearningRate * (err - biasReg * preventOverfitting * itemVector[ITEM_BIAS_INDEX]);        for (int feature = FEATURE_OFFSET; feature < numFeatures; feature++) {        double userFeature = userVector[feature];        double itemFeature = itemVector[feature];        double deltaUserFeature = err * itemFeature - preventOverfitting * userFeature;        userVector[feature] += currentLearningRate * deltaUserFeature;        double deltaItemFeature = err * userFeature - preventOverfitting * itemFeature;        itemVector[feature] += currentLearningRate * deltaItemFeature;    }}
0
private double predictRating(int userID, int itemID)
{    double sum = 0;    for (int feature = 0; feature < numFeatures; feature++) {        sum += userVectors[userID][feature] * itemVectors[itemID][feature];    }    return sum;}
0
protected void prepareTraining() throws TasteException
{    super.prepareTraining();    Random random = RandomUtils.getRandom();    p = new double[dataModel.getNumUsers()][numFeatures];    for (int i = 0; i < p.length; i++) {        for (int feature = 0; feature < FEATURE_OFFSET; feature++) {            p[i][feature] = 0;        }        for (int feature = FEATURE_OFFSET; feature < numFeatures; feature++) {            p[i][feature] = random.nextGaussian() * randomNoise;        }    }    y = new double[dataModel.getNumItems()][numFeatures];    for (int i = 0; i < y.length; i++) {        for (int feature = 0; feature < FEATURE_OFFSET; feature++) {            y[i][feature] = 0;        }        for (int feature = FEATURE_OFFSET; feature < numFeatures; feature++) {            y[i][feature] = random.nextGaussian() * randomNoise;        }    }    /* get internal item IDs which we will need several times */    itemsByUser = new HashMap<>();    LongPrimitiveIterator userIDs = dataModel.getUserIDs();    while (userIDs.hasNext()) {        long userId = userIDs.nextLong();        int userIndex = userIndex(userId);        FastIDSet itemIDsFromUser = dataModel.getItemIDsFromUser(userId);        List<Integer> itemIndexes = new ArrayList<>(itemIDsFromUser.size());        itemsByUser.put(userIndex, itemIndexes);        for (long itemID2 : itemIDsFromUser) {            int i2 = itemIndex(itemID2);            itemIndexes.add(i2);        }    }}
0
public Factorization factorize() throws TasteException
{    prepareTraining();    super.factorize();    for (int userIndex = 0; userIndex < userVectors.length; userIndex++) {        for (int itemIndex : itemsByUser.get(userIndex)) {            for (int feature = FEATURE_OFFSET; feature < numFeatures; feature++) {                userVectors[userIndex][feature] += y[itemIndex][feature];            }        }        double denominator = Math.sqrt(itemsByUser.get(userIndex).size());        for (int feature = 0; feature < userVectors[userIndex].length; feature++) {            userVectors[userIndex][feature] = (float) (userVectors[userIndex][feature] / denominator + p[userIndex][feature]);        }    }    return createFactorization(userVectors, itemVectors);}
0
protected void updateParameters(long userID, long itemID, float rating, double currentLearningRate)
{    int userIndex = userIndex(userID);    int itemIndex = itemIndex(itemID);    double[] userVector = p[userIndex];    double[] itemVector = itemVectors[itemIndex];    double[] pPlusY = new double[numFeatures];    for (int i2 : itemsByUser.get(userIndex)) {        for (int f = FEATURE_OFFSET; f < numFeatures; f++) {            pPlusY[f] += y[i2][f];        }    }    double denominator = Math.sqrt(itemsByUser.get(userIndex).size());    for (int feature = 0; feature < pPlusY.length; feature++) {        pPlusY[feature] = (float) (pPlusY[feature] / denominator + p[userIndex][feature]);    }    double prediction = predictRating(pPlusY, itemIndex);    double err = rating - prediction;    double normalized_error = err / denominator;        userVector[USER_BIAS_INDEX] += biasLearningRate * currentLearningRate * (err - biasReg * preventOverfitting * userVector[USER_BIAS_INDEX]);        itemVector[ITEM_BIAS_INDEX] += biasLearningRate * currentLearningRate * (err - biasReg * preventOverfitting * itemVector[ITEM_BIAS_INDEX]);        for (int feature = FEATURE_OFFSET; feature < numFeatures; feature++) {        double pF = userVector[feature];        double iF = itemVector[feature];        double deltaU = err * iF - preventOverfitting * pF;        userVector[feature] += currentLearningRate * deltaU;        double deltaI = err * pPlusY[feature] - preventOverfitting * iF;        itemVector[feature] += currentLearningRate * deltaI;        double commonUpdate = normalized_error * iF;        for (int itemIndex2 : itemsByUser.get(userIndex)) {            double deltaI2 = commonUpdate - preventOverfitting * y[itemIndex2][feature];            y[itemIndex2][feature] += learningRate * deltaI2;        }    }}
0
private double predictRating(double[] userVector, int itemID)
{    double sum = 0;    for (int feature = 0; feature < numFeatures; feature++) {        sum += userVector[feature] * itemVectors[itemID][feature];    }    return sum;}
0
public double getCache()
{    return cache;}
0
public void setCache(double value)
{    Preconditions.checkArgument(!Double.isNaN(value), "NaN cache value");    this.cache = value;}
0
public Object call() throws TasteException
{    train();    return null;}
0
 static PersistenceStrategy getDefaultPersistenceStrategy()
{    return new NoPersistenceStrategy();}
0
private void train() throws TasteException
{    factorization = factorizer.factorize();    try {        persistenceStrategy.maybePersist(factorization);    } catch (IOException e) {        throw new TasteException("Error persisting factorization", e);    }}
0
public List<RecommendedItem> recommend(long userID, int howMany, IDRescorer rescorer, boolean includeKnownItems) throws TasteException
{    Preconditions.checkArgument(howMany >= 1, "howMany must be at least 1");        PreferenceArray preferencesFromUser = getDataModel().getPreferencesFromUser(userID);    FastIDSet possibleItemIDs = getAllOtherItems(userID, preferencesFromUser, includeKnownItems);    List<RecommendedItem> topItems = TopItems.getTopItems(howMany, possibleItemIDs.iterator(), rescorer, new Estimator(userID));        return topItems;}
1
public float estimatePreference(long userID, long itemID) throws TasteException
{    double[] userFeatures = factorization.getUserFeatures(userID);    double[] itemFeatures = factorization.getItemFeatures(itemID);    double estimate = 0;    for (int feature = 0; feature < userFeatures.length; feature++) {        estimate += userFeatures[feature] * itemFeatures[feature];    }    return (float) estimate;}
0
public double estimate(Long itemID) throws TasteException
{    return estimatePreference(theUserID, itemID);}
0
public void refresh(Collection<Refreshable> alreadyRefreshed)
{    refreshHelper.refresh(alreadyRefreshed);}
0
public static List<RecommendedItem> getTopItems(int howMany, LongPrimitiveIterator possibleItemIDs, IDRescorer rescorer, Estimator<Long> estimator) throws TasteException
{    Preconditions.checkArgument(possibleItemIDs != null, "possibleItemIDs is null");    Preconditions.checkArgument(estimator != null, "estimator is null");    Queue<RecommendedItem> topItems = new PriorityQueue<>(howMany + 1, Collections.reverseOrder(ByValueRecommendedItemComparator.getInstance()));    boolean full = false;    double lowestTopValue = Double.NEGATIVE_INFINITY;    while (possibleItemIDs.hasNext()) {        long itemID = possibleItemIDs.next();        if (rescorer == null || !rescorer.isFiltered(itemID)) {            double preference;            try {                preference = estimator.estimate(itemID);            } catch (NoSuchItemException nsie) {                continue;            }            double rescoredPref = rescorer == null ? preference : rescorer.rescore(itemID, preference);            if (!Double.isNaN(rescoredPref) && (!full || rescoredPref > lowestTopValue)) {                topItems.add(new GenericRecommendedItem(itemID, (float) rescoredPref));                if (full) {                    topItems.poll();                } else if (topItems.size() > howMany) {                    full = true;                    topItems.poll();                }                lowestTopValue = topItems.peek().getValue();            }        }    }    int size = topItems.size();    if (size == 0) {        return Collections.emptyList();    }    List<RecommendedItem> result = new ArrayList<>(size);    result.addAll(topItems);    Collections.sort(result, ByValueRecommendedItemComparator.getInstance());    return result;}
0
public static long[] getTopUsers(int howMany, LongPrimitiveIterator allUserIDs, IDRescorer rescorer, Estimator<Long> estimator) throws TasteException
{    Queue<SimilarUser> topUsers = new PriorityQueue<>(howMany + 1, Collections.reverseOrder());    boolean full = false;    double lowestTopValue = Double.NEGATIVE_INFINITY;    while (allUserIDs.hasNext()) {        long userID = allUserIDs.next();        if (rescorer != null && rescorer.isFiltered(userID)) {            continue;        }        double similarity;        try {            similarity = estimator.estimate(userID);        } catch (NoSuchUserException nsue) {            continue;        }        double rescoredSimilarity = rescorer == null ? similarity : rescorer.rescore(userID, similarity);        if (!Double.isNaN(rescoredSimilarity) && (!full || rescoredSimilarity > lowestTopValue)) {            topUsers.add(new SimilarUser(userID, rescoredSimilarity));            if (full) {                topUsers.poll();            } else if (topUsers.size() > howMany) {                full = true;                topUsers.poll();            }            lowestTopValue = topUsers.peek().getSimilarity();        }    }    int size = topUsers.size();    if (size == 0) {        return NO_IDS;    }    List<SimilarUser> sorted = new ArrayList<>(size);    sorted.addAll(topUsers);    Collections.sort(sorted);    long[] result = new long[size];    int i = 0;    for (SimilarUser similarUser : sorted) {        result[i++] = similarUser.getUserID();    }    return result;}
0
public static List<GenericItemSimilarity.ItemItemSimilarity> getTopItemItemSimilarities(int howMany, Iterator<GenericItemSimilarity.ItemItemSimilarity> allSimilarities)
{    Queue<GenericItemSimilarity.ItemItemSimilarity> topSimilarities = new PriorityQueue<>(howMany + 1, Collections.reverseOrder());    boolean full = false;    double lowestTopValue = Double.NEGATIVE_INFINITY;    while (allSimilarities.hasNext()) {        GenericItemSimilarity.ItemItemSimilarity similarity = allSimilarities.next();        double value = similarity.getValue();        if (!Double.isNaN(value) && (!full || value > lowestTopValue)) {            topSimilarities.add(similarity);            if (full) {                topSimilarities.poll();            } else if (topSimilarities.size() > howMany) {                full = true;                topSimilarities.poll();            }            lowestTopValue = topSimilarities.peek().getValue();        }    }    int size = topSimilarities.size();    if (size == 0) {        return Collections.emptyList();    }    List<GenericItemSimilarity.ItemItemSimilarity> result = new ArrayList<>(size);    result.addAll(topSimilarities);    Collections.sort(result);    return result;}
0
public static List<GenericUserSimilarity.UserUserSimilarity> getTopUserUserSimilarities(int howMany, Iterator<GenericUserSimilarity.UserUserSimilarity> allSimilarities)
{    Queue<GenericUserSimilarity.UserUserSimilarity> topSimilarities = new PriorityQueue<>(howMany + 1, Collections.reverseOrder());    boolean full = false;    double lowestTopValue = Double.NEGATIVE_INFINITY;    while (allSimilarities.hasNext()) {        GenericUserSimilarity.UserUserSimilarity similarity = allSimilarities.next();        double value = similarity.getValue();        if (!Double.isNaN(value) && (!full || value > lowestTopValue)) {            topSimilarities.add(similarity);            if (full) {                topSimilarities.poll();            } else if (topSimilarities.size() > howMany) {                full = true;                topSimilarities.poll();            }            lowestTopValue = topSimilarities.peek().getValue();        }    }    int size = topSimilarities.size();    if (size == 0) {        return Collections.emptyList();    }    List<GenericUserSimilarity.UserUserSimilarity> result = new ArrayList<>(size);    result.addAll(topSimilarities);    Collections.sort(result);    return result;}
0
protected DataModel getDataModel()
{    return dataModel;}
0
public long[] allSimilarItemIDs(long itemID) throws TasteException
{    FastIDSet allSimilarItemIDs = new FastIDSet();    LongPrimitiveIterator allItemIDs = dataModel.getItemIDs();    while (allItemIDs.hasNext()) {        long possiblySimilarItemID = allItemIDs.nextLong();        if (!Double.isNaN(itemSimilarity(itemID, possiblySimilarItemID))) {            allSimilarItemIDs.add(possiblySimilarItemID);        }    }    return allSimilarItemIDs.toArray();}
0
public void refresh(Collection<Refreshable> alreadyRefreshed)
{    refreshHelper.refresh(alreadyRefreshed);}
0
public Object call() throws TasteException
{    cachedNumItems = dataModel.getNumItems();    cachedNumUsers = dataModel.getNumUsers();    return null;}
0
 final PreferenceInferrer getPreferenceInferrer()
{    return inferrer;}
0
public final void setPreferenceInferrer(PreferenceInferrer inferrer)
{    Preconditions.checkArgument(inferrer != null, "inferrer is null");    refreshHelper.addDependency(inferrer);    refreshHelper.removeDependency(this.inferrer);    this.inferrer = inferrer;}
0
 final boolean isWeighted()
{    return weighted;}
0
public double userSimilarity(long userID1, long userID2) throws TasteException
{    DataModel dataModel = getDataModel();    PreferenceArray xPrefs = dataModel.getPreferencesFromUser(userID1);    PreferenceArray yPrefs = dataModel.getPreferencesFromUser(userID2);    int xLength = xPrefs.length();    int yLength = yPrefs.length();    if (xLength == 0 || yLength == 0) {        return Double.NaN;    }    long xIndex = xPrefs.getItemID(0);    long yIndex = yPrefs.getItemID(0);    int xPrefIndex = 0;    int yPrefIndex = 0;    double sumX = 0.0;    double sumX2 = 0.0;    double sumY = 0.0;    double sumY2 = 0.0;    double sumXY = 0.0;    double sumXYdiff2 = 0.0;    int count = 0;    boolean hasInferrer = inferrer != null;    while (true) {        int compare = xIndex < yIndex ? -1 : xIndex > yIndex ? 1 : 0;        if (hasInferrer || compare == 0) {            double x;            double y;            if (xIndex == yIndex) {                                x = xPrefs.getValue(xPrefIndex);                y = yPrefs.getValue(yPrefIndex);            } else {                                if (compare < 0) {                                        x = xPrefs.getValue(xPrefIndex);                    y = inferrer.inferPreference(userID2, xIndex);                } else {                                                            x = inferrer.inferPreference(userID1, yIndex);                    y = yPrefs.getValue(yPrefIndex);                }            }            sumXY += x * y;            sumX += x;            sumX2 += x * x;            sumY += y;            sumY2 += y * y;            double diff = x - y;            sumXYdiff2 += diff * diff;            count++;        }        if (compare <= 0) {            if (++xPrefIndex >= xLength) {                if (hasInferrer) {                                        if (yIndex == Long.MAX_VALUE) {                                                break;                    }                    xIndex = Long.MAX_VALUE;                } else {                    break;                }            } else {                xIndex = xPrefs.getItemID(xPrefIndex);            }        }        if (compare >= 0) {            if (++yPrefIndex >= yLength) {                if (hasInferrer) {                                        if (xIndex == Long.MAX_VALUE) {                                                break;                    }                    yIndex = Long.MAX_VALUE;                } else {                    break;                }            } else {                yIndex = yPrefs.getItemID(yPrefIndex);            }        }    }        double result;    if (centerData) {        double meanX = sumX / count;        double meanY = sumY / count;                double centeredSumXY = sumXY - meanY * sumX;                double centeredSumX2 = sumX2 - meanX * sumX;                double centeredSumY2 = sumY2 - meanY * sumY;        result = computeResult(count, centeredSumXY, centeredSumX2, centeredSumY2, sumXYdiff2);    } else {        result = computeResult(count, sumXY, sumX2, sumY2, sumXYdiff2);    }    if (!Double.isNaN(result)) {        result = normalizeWeightResult(result, count, cachedNumItems);    }    return result;}
0
public final double itemSimilarity(long itemID1, long itemID2) throws TasteException
{    DataModel dataModel = getDataModel();    PreferenceArray xPrefs = dataModel.getPreferencesForItem(itemID1);    PreferenceArray yPrefs = dataModel.getPreferencesForItem(itemID2);    int xLength = xPrefs.length();    int yLength = yPrefs.length();    if (xLength == 0 || yLength == 0) {        return Double.NaN;    }    long xIndex = xPrefs.getUserID(0);    long yIndex = yPrefs.getUserID(0);    int xPrefIndex = 0;    int yPrefIndex = 0;    double sumX = 0.0;    double sumX2 = 0.0;    double sumY = 0.0;    double sumY2 = 0.0;    double sumXY = 0.0;    double sumXYdiff2 = 0.0;    int count = 0;    while (true) {        int compare = xIndex < yIndex ? -1 : xIndex > yIndex ? 1 : 0;        if (compare == 0) {                        double x = xPrefs.getValue(xPrefIndex);            double y = yPrefs.getValue(yPrefIndex);            sumXY += x * y;            sumX += x;            sumX2 += x * x;            sumY += y;            sumY2 += y * y;            double diff = x - y;            sumXYdiff2 += diff * diff;            count++;        }        if (compare <= 0) {            if (++xPrefIndex == xLength) {                break;            }            xIndex = xPrefs.getUserID(xPrefIndex);        }        if (compare >= 0) {            if (++yPrefIndex == yLength) {                break;            }            yIndex = yPrefs.getUserID(yPrefIndex);        }    }    double result;    if (centerData) {                double n = (double) count;        double meanX = sumX / n;        double meanY = sumY / n;                double centeredSumXY = sumXY - meanY * sumX;                double centeredSumX2 = sumX2 - meanX * sumX;                double centeredSumY2 = sumY2 - meanY * sumY;        result = computeResult(count, centeredSumXY, centeredSumX2, centeredSumY2, sumXYdiff2);    } else {        result = computeResult(count, sumXY, sumX2, sumY2, sumXYdiff2);    }    if (!Double.isNaN(result)) {        result = normalizeWeightResult(result, count, cachedNumUsers);    }    return result;}
0
public double[] itemSimilarities(long itemID1, long[] itemID2s) throws TasteException
{    int length = itemID2s.length;    double[] result = new double[length];    for (int i = 0; i < length; i++) {        result[i] = itemSimilarity(itemID1, itemID2s[i]);    }    return result;}
0
 final double normalizeWeightResult(double result, int count, int num)
{    double normalizedResult = result;    if (weighted) {        double scaleFactor = 1.0 - (double) count / (double) (num + 1);        if (normalizedResult < 0.0) {            normalizedResult = -1.0 + scaleFactor * (1.0 + normalizedResult);        } else {            normalizedResult = 1.0 - scaleFactor * (1.0 - normalizedResult);        }    }        if (normalizedResult < -1.0) {        normalizedResult = -1.0;    } else if (normalizedResult > 1.0) {        normalizedResult = 1.0;    }    return normalizedResult;}
0
public final void refresh(Collection<Refreshable> alreadyRefreshed)
{    super.refresh(alreadyRefreshed);    refreshHelper.refresh(alreadyRefreshed);}
0
public final String toString()
{    return this.getClass().getSimpleName() + "[dataModel:" + getDataModel() + ",inferrer:" + inferrer + ']';}
0
public float inferPreference(long userID, long itemID) throws TasteException
{    return averagePreferenceValue.get(userID);}
0
public void refresh(Collection<Refreshable> alreadyRefreshed)
{    averagePreferenceValue.clear();}
0
public Float get(Long key) throws TasteException
{    PreferenceArray prefs = dataModel.getPreferencesFromUser(key);    int size = prefs.length();    if (size == 0) {        return ZERO;    }    RunningAverage average = new FullRunningAverage();    for (int i = 0; i < size; i++) {        average.addDatum(prefs.getValue(i));    }    return (float) average.getAverage();}
0
public String toString()
{    return "AveragingPreferenceInferrer";}
0
public Void call()
{    similarityCache.clear();    return null;}
0
public double itemSimilarity(long itemID1, long itemID2) throws TasteException
{    LongPair key = itemID1 < itemID2 ? new LongPair(itemID1, itemID2) : new LongPair(itemID2, itemID1);    return similarityCache.get(key);}
0
public double[] itemSimilarities(long itemID1, long[] itemID2s) throws TasteException
{    int length = itemID2s.length;    double[] result = new double[length];    for (int i = 0; i < length; i++) {        result[i] = itemSimilarity(itemID1, itemID2s[i]);    }    return result;}
0
public long[] allSimilarItemIDs(long itemID) throws TasteException
{    return similarity.allSimilarItemIDs(itemID);}
0
public void refresh(Collection<Refreshable> alreadyRefreshed)
{    refreshHelper.refresh(alreadyRefreshed);}
0
public void clearCacheForItem(long itemID)
{    similarityCache.removeKeysMatching(new LongPairMatchPredicate(itemID));}
0
public Double get(LongPair key) throws TasteException
{    return similarity.itemSimilarity(key.getFirst(), key.getSecond());}
0
public Void call()
{    similarityCache.clear();    return null;}
0
public double userSimilarity(long userID1, long userID2) throws TasteException
{    LongPair key = userID1 < userID2 ? new LongPair(userID1, userID2) : new LongPair(userID2, userID1);    return similarityCache.get(key);}
0
public void setPreferenceInferrer(PreferenceInferrer inferrer)
{    similarityCache.clear();    similarity.setPreferenceInferrer(inferrer);}
0
public void clearCacheForUser(long userID)
{    similarityCache.removeKeysMatching(new LongPairMatchPredicate(userID));}
0
public void refresh(Collection<Refreshable> alreadyRefreshed)
{    refreshHelper.refresh(alreadyRefreshed);}
0
public Double get(LongPair key) throws TasteException
{    return similarity.userSimilarity(key.getFirst(), key.getSecond());}
0
public void setPreferenceInferrer(PreferenceInferrer inferrer)
{    throw new UnsupportedOperationException();}
0
public void refresh(Collection<Refreshable> alreadyRefreshed)
{    Collection<Refreshable> refreshed = RefreshHelper.buildRefreshed(alreadyRefreshed);    RefreshHelper.maybeRefresh(refreshed, getDataModel());}
0
public double itemSimilarity(long itemID1, long itemID2) throws TasteException
{    DataModel dataModel = getDataModel();    int preferring1 = dataModel.getNumUsersWithPreferenceFor(itemID1);    int preferring2 = dataModel.getNumUsersWithPreferenceFor(itemID2);    int intersection = dataModel.getNumUsersWithPreferenceFor(itemID1, itemID2);    return doSimilarity(preferring1, preferring2, intersection);}
0
public double[] itemSimilarities(long itemID1, long[] itemID2s) throws TasteException
{    DataModel dataModel = getDataModel();    int preferring1 = dataModel.getNumUsersWithPreferenceFor(itemID1);    double[] distance = new double[itemID2s.length];    for (int i = 0; i < itemID2s.length; ++i) {        int preferring2 = dataModel.getNumUsersWithPreferenceFor(itemID2s[i]);        int intersection = dataModel.getNumUsersWithPreferenceFor(itemID1, itemID2s[i]);        distance[i] = doSimilarity(preferring1, preferring2, intersection);    }    return distance;}
0
public double userSimilarity(long userID1, long userID2) throws TasteException
{    DataModel dataModel = getDataModel();    FastIDSet prefs1 = dataModel.getItemIDsFromUser(userID1);    FastIDSet prefs2 = dataModel.getItemIDsFromUser(userID2);    int prefs1Size = prefs1.size();    int prefs2Size = prefs2.size();    int intersectionSize = prefs1Size < prefs2Size ? prefs2.intersectionSize(prefs1) : prefs1.intersectionSize(prefs2);    return doSimilarity(prefs1Size, prefs2Size, intersectionSize);}
0
private static double doSimilarity(int pref1, int pref2, int intersection)
{    int distance = pref1 + pref2 - 2 * intersection;    return 1.0 / (1.0 + distance);}
0
 double computeResult(int n, double sumXY, double sumX2, double sumY2, double sumXYdiff2)
{    return 1.0 / (1.0 + Math.sqrt(sumXYdiff2) / Math.sqrt(n));}
0
public Iterator<GenericItemSimilarity.ItemItemSimilarity> iterator()
{    try {        return new FileItemItemSimilarityIterator(similaritiesFile);    } catch (IOException ioe) {        throw new IllegalStateException("Can't read " + similaritiesFile, ioe);    }}
0
public GenericItemSimilarity.ItemItemSimilarity apply(String from)
{    String[] tokens = SEPARATOR.split(from);    return new GenericItemSimilarity.ItemItemSimilarity(Long.parseLong(tokens[0]), Long.parseLong(tokens[1]), Double.parseDouble(tokens[2]));}
0
protected Iterator<GenericItemSimilarity.ItemItemSimilarity> delegate()
{    return delegate;}
0
public double[] itemSimilarities(long itemID1, long[] itemID2s) throws TasteException
{    return delegate.itemSimilarities(itemID1, itemID2s);}
0
public long[] allSimilarItemIDs(long itemID) throws TasteException
{    return delegate.allSimilarItemIDs(itemID);}
0
public double itemSimilarity(long itemID1, long itemID2) throws TasteException
{    return delegate.itemSimilarity(itemID1, itemID2);}
0
protected void reload()
{    if (reloadLock.tryLock()) {        try {            long newLastModified = dataFile.lastModified();            delegate = new GenericItemSimilarity(new FileItemItemSimilarityIterable(dataFile));            lastModified = newLastModified;        } finally {            reloadLock.unlock();        }    }}
0
public String toString()
{    return "FileItemSimilarity[dataFile:" + dataFile + ']';}
0
private void initSimilarityMaps(Iterator<ItemItemSimilarity> similarities)
{    while (similarities.hasNext()) {        ItemItemSimilarity iic = similarities.next();        long similarityItemID1 = iic.getItemID1();        long similarityItemID2 = iic.getItemID2();        if (similarityItemID1 != similarityItemID2) {                        long itemID1;            long itemID2;            if (similarityItemID1 < similarityItemID2) {                itemID1 = similarityItemID1;                itemID2 = similarityItemID2;            } else {                itemID1 = similarityItemID2;                itemID2 = similarityItemID1;            }            FastByIDMap<Double> map = similarityMaps.get(itemID1);            if (map == null) {                map = new FastByIDMap<>();                similarityMaps.put(itemID1, map);            }            map.put(itemID2, iic.getValue());            doIndex(itemID1, itemID2);            doIndex(itemID2, itemID1);        }        }}
0
private void doIndex(long fromItemID, long toItemID)
{    FastIDSet similarItemIDs = similarItemIDsIndex.get(fromItemID);    if (similarItemIDs == null) {        similarItemIDs = new FastIDSet();        similarItemIDsIndex.put(fromItemID, similarItemIDs);    }    similarItemIDs.add(toItemID);}
0
public double itemSimilarity(long itemID1, long itemID2)
{    if (itemID1 == itemID2) {        return 1.0;    }    long firstID;    long secondID;    if (itemID1 < itemID2) {        firstID = itemID1;        secondID = itemID2;    } else {        firstID = itemID2;        secondID = itemID1;    }    FastByIDMap<Double> nextMap = similarityMaps.get(firstID);    if (nextMap == null) {        return Double.NaN;    }    Double similarity = nextMap.get(secondID);    return similarity == null ? Double.NaN : similarity;}
0
public double[] itemSimilarities(long itemID1, long[] itemID2s)
{    int length = itemID2s.length;    double[] result = new double[length];    for (int i = 0; i < length; i++) {        result[i] = itemSimilarity(itemID1, itemID2s[i]);    }    return result;}
0
public long[] allSimilarItemIDs(long itemID)
{    FastIDSet similarItemIDs = similarItemIDsIndex.get(itemID);    return similarItemIDs != null ? similarItemIDs.toArray() : NO_IDS;}
0
public void refresh(Collection<Refreshable> alreadyRefreshed)
{}
0
public long getItemID1()
{    return itemID1;}
0
public long getItemID2()
{    return itemID2;}
0
public double getValue()
{    return value;}
0
public String toString()
{    return "ItemItemSimilarity[" + itemID1 + ',' + itemID2 + ':' + value + ']';}
0
public int compareTo(ItemItemSimilarity other)
{    double otherValue = other.getValue();    return value > otherValue ? -1 : value < otherValue ? 1 : 0;}
0
public boolean equals(Object other)
{    if (!(other instanceof ItemItemSimilarity)) {        return false;    }    ItemItemSimilarity otherSimilarity = (ItemItemSimilarity) other;    return otherSimilarity.getItemID1() == itemID1 && otherSimilarity.getItemID2() == itemID2 && otherSimilarity.getValue() == value;}
0
public int hashCode()
{    return (int) itemID1 ^ (int) itemID2 ^ RandomUtils.hashDouble(value);}
0
protected ItemItemSimilarity computeNext()
{    int size = itemIDs.length;    ItemItemSimilarity result = null;    while (result == null && i < size - 1) {        long itemID2 = itemIDs[j];        double similarity;        try {            similarity = otherSimilarity.itemSimilarity(itemID1, itemID2);        } catch (TasteException te) {                        throw new IllegalStateException(te);        }        if (!Double.isNaN(similarity)) {            result = new ItemItemSimilarity(itemID1, itemID2, similarity);        }        if (++j == size) {            itemID1 = itemIDs[++i];            j = i + 1;        }    }    if (result == null) {        return endOfData();    } else {        return result;    }}
0
 static long[] longIteratorToList(LongPrimitiveIterator iterator)
{    long[] result = new long[5];    int size = 0;    while (iterator.hasNext()) {        if (size == result.length) {            long[] newResult = new long[result.length << 1];            System.arraycopy(result, 0, newResult, 0, result.length);            result = newResult;        }        result[size++] = iterator.next();    }    if (size != result.length) {        long[] newResult = new long[size];        System.arraycopy(result, 0, newResult, 0, size);        result = newResult;    }    return result;}
0
private void initSimilarityMaps(Iterator<UserUserSimilarity> similarities)
{    while (similarities.hasNext()) {        UserUserSimilarity uuc = similarities.next();        long similarityUser1 = uuc.getUserID1();        long similarityUser2 = uuc.getUserID2();        if (similarityUser1 != similarityUser2) {                        long user1;            long user2;            if (similarityUser1 < similarityUser2) {                user1 = similarityUser1;                user2 = similarityUser2;            } else {                user1 = similarityUser2;                user2 = similarityUser1;            }            FastByIDMap<Double> map = similarityMaps.get(user1);            if (map == null) {                map = new FastByIDMap<>();                similarityMaps.put(user1, map);            }            map.put(user2, uuc.getValue());        }        }}
0
public double userSimilarity(long userID1, long userID2)
{    if (userID1 == userID2) {        return 1.0;    }    long first;    long second;    if (userID1 < userID2) {        first = userID1;        second = userID2;    } else {        first = userID2;        second = userID1;    }    FastByIDMap<Double> nextMap = similarityMaps.get(first);    if (nextMap == null) {        return Double.NaN;    }    Double similarity = nextMap.get(second);    return similarity == null ? Double.NaN : similarity;}
0
public void setPreferenceInferrer(PreferenceInferrer inferrer)
{    throw new UnsupportedOperationException();}
0
public void refresh(Collection<Refreshable> alreadyRefreshed)
{}
0
public long getUserID1()
{    return userID1;}
0
public long getUserID2()
{    return userID2;}
0
public double getValue()
{    return value;}
0
public String toString()
{    return "UserUserSimilarity[" + userID1 + ',' + userID2 + ':' + value + ']';}
0
public int compareTo(UserUserSimilarity other)
{    double otherValue = other.getValue();    return value > otherValue ? -1 : value < otherValue ? 1 : 0;}
0
public boolean equals(Object other)
{    if (!(other instanceof UserUserSimilarity)) {        return false;    }    UserUserSimilarity otherSimilarity = (UserUserSimilarity) other;    return otherSimilarity.getUserID1() == userID1 && otherSimilarity.getUserID2() == userID2 && otherSimilarity.getValue() == value;}
0
public int hashCode()
{    return (int) userID1 ^ (int) userID2 ^ RandomUtils.hashDouble(value);}
0
protected UserUserSimilarity computeNext()
{    int size = itemIDs.length;    while (i < size - 1) {        long itemID2 = itemIDs[j];        double similarity;        try {            similarity = otherSimilarity.userSimilarity(itemID1, itemID2);        } catch (TasteException te) {                        throw new IllegalStateException(te);        }        if (!Double.isNaN(similarity)) {            return new UserUserSimilarity(itemID1, itemID2, similarity);        }        if (++j == size) {            itemID1 = itemIDs[++i];            j = i + 1;        }    }    return endOfData();}
0
public void setPreferenceInferrer(PreferenceInferrer inferrer)
{    throw new UnsupportedOperationException();}
0
public double userSimilarity(long userID1, long userID2) throws TasteException
{    DataModel dataModel = getDataModel();    FastIDSet prefs1 = dataModel.getItemIDsFromUser(userID1);    FastIDSet prefs2 = dataModel.getItemIDsFromUser(userID2);    long prefs1Size = prefs1.size();    long prefs2Size = prefs2.size();    long intersectionSize = prefs1Size < prefs2Size ? prefs2.intersectionSize(prefs1) : prefs1.intersectionSize(prefs2);    if (intersectionSize == 0) {        return Double.NaN;    }    long numItems = dataModel.getNumItems();    double logLikelihood = LogLikelihood.logLikelihoodRatio(intersectionSize, prefs2Size - intersectionSize, prefs1Size - intersectionSize, numItems - prefs1Size - prefs2Size + intersectionSize);    return 1.0 - 1.0 / (1.0 + logLikelihood);}
0
public double itemSimilarity(long itemID1, long itemID2) throws TasteException
{    DataModel dataModel = getDataModel();    long preferring1 = dataModel.getNumUsersWithPreferenceFor(itemID1);    long numUsers = dataModel.getNumUsers();    return doItemSimilarity(itemID1, itemID2, preferring1, numUsers);}
0
public double[] itemSimilarities(long itemID1, long[] itemID2s) throws TasteException
{    DataModel dataModel = getDataModel();    long preferring1 = dataModel.getNumUsersWithPreferenceFor(itemID1);    long numUsers = dataModel.getNumUsers();    int length = itemID2s.length;    double[] result = new double[length];    for (int i = 0; i < length; i++) {        result[i] = doItemSimilarity(itemID1, itemID2s[i], preferring1, numUsers);    }    return result;}
0
private double doItemSimilarity(long itemID1, long itemID2, long preferring1, long numUsers) throws TasteException
{    DataModel dataModel = getDataModel();    long preferring1and2 = dataModel.getNumUsersWithPreferenceFor(itemID1, itemID2);    if (preferring1and2 == 0) {        return Double.NaN;    }    long preferring2 = dataModel.getNumUsersWithPreferenceFor(itemID2);    double logLikelihood = LogLikelihood.logLikelihoodRatio(preferring1and2, preferring2 - preferring1and2, preferring1 - preferring1and2, numUsers - preferring1 - preferring2 + preferring1and2);    return 1.0 - 1.0 / (1.0 + logLikelihood);}
0
public void refresh(Collection<Refreshable> alreadyRefreshed)
{    alreadyRefreshed = RefreshHelper.buildRefreshed(alreadyRefreshed);    RefreshHelper.maybeRefresh(alreadyRefreshed, getDataModel());}
0
public String toString()
{    return "LogLikelihoodSimilarity[dataModel:" + getDataModel() + ']';}
0
public boolean matches(LongPair pair)
{    return pair.getFirst() == id || pair.getSecond() == id;}
0
 double computeResult(int n, double sumXY, double sumX2, double sumY2, double sumXYdiff2)
{    if (n == 0) {        return Double.NaN;    }            double denominator = Math.sqrt(sumX2) * Math.sqrt(sumY2);    if (denominator == 0.0) {                return Double.NaN;    }    return sumXY / denominator;}
0
public void open() throws IOException
{    writer = new BufferedWriter(new OutputStreamWriter(new FileOutputStream(file), Charsets.UTF_8));}
0
public void add(SimilarItems similarItems) throws IOException
{    String itemID = String.valueOf(similarItems.getItemID());    for (SimilarItem similarItem : similarItems.getSimilarItems()) {        writer.write(itemID);        writer.write(',');        writer.write(String.valueOf(similarItem.getItemID()));        writer.write(',');        writer.write(String.valueOf(similarItem.getSimilarity()));        writer.newLine();    }}
0
public void close() throws IOException
{    Closeables.close(writer, false);}
0
public int computeItemSimilarities(int degreeOfParallelism, int maxDurationInHours, SimilarItemsWriter writer) throws IOException
{    ExecutorService executorService = Executors.newFixedThreadPool(degreeOfParallelism + 1);    Output output = null;    try {        writer.open();        DataModel dataModel = getRecommender().getDataModel();        BlockingQueue<long[]> itemsIDsInBatches = queueItemIDsInBatches(dataModel, batchSize, degreeOfParallelism);        BlockingQueue<List<SimilarItems>> results = new LinkedBlockingQueue<>();        AtomicInteger numActiveWorkers = new AtomicInteger(degreeOfParallelism);        for (int n = 0; n < degreeOfParallelism; n++) {            executorService.execute(new SimilarItemsWorker(n, itemsIDsInBatches, results, numActiveWorkers));        }        output = new Output(results, writer, numActiveWorkers);        executorService.execute(output);    } catch (Exception e) {        throw new IOException(e);    } finally {        executorService.shutdown();        try {            boolean succeeded = executorService.awaitTermination(maxDurationInHours, TimeUnit.HOURS);            if (!succeeded) {                throw new RuntimeException("Unable to complete the computation in " + maxDurationInHours + " hours!");            }        } catch (InterruptedException e) {            throw new RuntimeException(e);        }        Closeables.close(writer, false);    }    return output.getNumSimilaritiesProcessed();}
0
private static BlockingQueue<long[]> queueItemIDsInBatches(DataModel dataModel, int batchSize, int degreeOfParallelism) throws TasteException
{    LongPrimitiveIterator itemIDs = dataModel.getItemIDs();    int numItems = dataModel.getNumItems();    BlockingQueue<long[]> itemIDBatches = new LinkedBlockingQueue<>((numItems / batchSize) + 1);    long[] batch = new long[batchSize];    int pos = 0;    while (itemIDs.hasNext()) {        batch[pos] = itemIDs.nextLong();        pos++;        if (pos == batchSize) {            itemIDBatches.add(batch.clone());            pos = 0;        }    }    if (pos > 0) {        long[] lastBatch = new long[pos];        System.arraycopy(batch, 0, lastBatch, 0, pos);        itemIDBatches.add(lastBatch);    }    if (itemIDBatches.size() < degreeOfParallelism) {        throw new IllegalStateException("Degree of parallelism [" + degreeOfParallelism + "] " + " is larger than number of batches [" + itemIDBatches.size() + "].");    }        return itemIDBatches;}
1
private int getNumSimilaritiesProcessed()
{    return numSimilaritiesProcessed;}
0
public void run()
{    while (numActiveWorkers.get() != 0 || !results.isEmpty()) {        try {            List<SimilarItems> similarItemsOfABatch = results.poll(10, TimeUnit.MILLISECONDS);            if (similarItemsOfABatch != null) {                for (SimilarItems similarItems : similarItemsOfABatch) {                    writer.add(similarItems);                    numSimilaritiesProcessed += similarItems.numSimilarItems();                }            }        } catch (Exception e) {            throw new RuntimeException(e);        }    }}
0
public void run()
{    int numBatchesProcessed = 0;    while (!itemIDBatches.isEmpty()) {        try {            long[] itemIDBatch = itemIDBatches.take();            List<SimilarItems> similarItemsOfBatch = new ArrayList<>(itemIDBatch.length);            for (long itemID : itemIDBatch) {                List<RecommendedItem> similarItems = getRecommender().mostSimilarItems(itemID, getSimilarItemsPerItem());                similarItemsOfBatch.add(new SimilarItems(itemID, similarItems));            }            results.offer(similarItemsOfBatch);            if (++numBatchesProcessed % 5 == 0) {                            }        } catch (Exception e) {            throw new RuntimeException(e);        }    }        numActiveWorkers.decrementAndGet();}
1
public double userSimilarity(long userID1, long userID2) throws TasteException
{    PreferenceArray xPrefs = dataModel.getPreferencesFromUser(userID1);    PreferenceArray yPrefs = dataModel.getPreferencesFromUser(userID2);    int xLength = xPrefs.length();    int yLength = yPrefs.length();    if (xLength <= 1 || yLength <= 1) {        return Double.NaN;    }        xPrefs = xPrefs.clone();    yPrefs = yPrefs.clone();        xPrefs.sortByValue();    yPrefs.sortByValue();        float nextRank = 1.0f;    for (int i = 0; i < xLength; i++) {                if (yPrefs.hasPrefWithItemID(xPrefs.getItemID(i))) {            xPrefs.setValue(i, nextRank);            nextRank += 1.0f;        }        }    nextRank = 1.0f;    for (int i = 0; i < yLength; i++) {        if (xPrefs.hasPrefWithItemID(yPrefs.getItemID(i))) {            yPrefs.setValue(i, nextRank);            nextRank += 1.0f;        }    }    xPrefs.sortByItem();    yPrefs.sortByItem();    long xIndex = xPrefs.getItemID(0);    long yIndex = yPrefs.getItemID(0);    int xPrefIndex = 0;    int yPrefIndex = 0;    double sumXYRankDiff2 = 0.0;    int count = 0;    while (true) {        int compare = xIndex < yIndex ? -1 : xIndex > yIndex ? 1 : 0;        if (compare == 0) {            double diff = xPrefs.getValue(xPrefIndex) - yPrefs.getValue(yPrefIndex);            sumXYRankDiff2 += diff * diff;            count++;        }        if (compare <= 0) {            if (++xPrefIndex >= xLength) {                break;            }            xIndex = xPrefs.getItemID(xPrefIndex);        }        if (compare >= 0) {            if (++yPrefIndex >= yLength) {                break;            }            yIndex = yPrefs.getItemID(yPrefIndex);        }    }    if (count <= 1) {        return Double.NaN;    }        return 1.0 - 6.0 * sumXYRankDiff2 / (count * (count * count - 1));}
0
public void setPreferenceInferrer(PreferenceInferrer inferrer)
{    throw new UnsupportedOperationException();}
0
public void refresh(Collection<Refreshable> alreadyRefreshed)
{    alreadyRefreshed = RefreshHelper.buildRefreshed(alreadyRefreshed);    RefreshHelper.maybeRefresh(alreadyRefreshed, dataModel);}
0
public void setPreferenceInferrer(PreferenceInferrer inferrer)
{    throw new UnsupportedOperationException();}
0
public double userSimilarity(long userID1, long userID2) throws TasteException
{    DataModel dataModel = getDataModel();    FastIDSet xPrefs = dataModel.getItemIDsFromUser(userID1);    FastIDSet yPrefs = dataModel.getItemIDsFromUser(userID2);    int xPrefsSize = xPrefs.size();    int yPrefsSize = yPrefs.size();    if (xPrefsSize == 0 && yPrefsSize == 0) {        return Double.NaN;    }    if (xPrefsSize == 0 || yPrefsSize == 0) {        return 0.0;    }    int intersectionSize = xPrefsSize < yPrefsSize ? yPrefs.intersectionSize(xPrefs) : xPrefs.intersectionSize(yPrefs);    if (intersectionSize == 0) {        return Double.NaN;    }    int unionSize = xPrefsSize + yPrefsSize - intersectionSize;    return (double) intersectionSize / (double) unionSize;}
0
public double itemSimilarity(long itemID1, long itemID2) throws TasteException
{    int preferring1 = getDataModel().getNumUsersWithPreferenceFor(itemID1);    return doItemSimilarity(itemID1, itemID2, preferring1);}
0
public double[] itemSimilarities(long itemID1, long[] itemID2s) throws TasteException
{    int preferring1 = getDataModel().getNumUsersWithPreferenceFor(itemID1);    int length = itemID2s.length;    double[] result = new double[length];    for (int i = 0; i < length; i++) {        result[i] = doItemSimilarity(itemID1, itemID2s[i], preferring1);    }    return result;}
0
private double doItemSimilarity(long itemID1, long itemID2, int preferring1) throws TasteException
{    DataModel dataModel = getDataModel();    int preferring1and2 = dataModel.getNumUsersWithPreferenceFor(itemID1, itemID2);    if (preferring1and2 == 0) {        return Double.NaN;    }    int preferring2 = dataModel.getNumUsersWithPreferenceFor(itemID2);    return (double) preferring1and2 / (double) (preferring1 + preferring2 - preferring1and2);}
0
public void refresh(Collection<Refreshable> alreadyRefreshed)
{    alreadyRefreshed = RefreshHelper.buildRefreshed(alreadyRefreshed);    RefreshHelper.maybeRefresh(alreadyRefreshed, getDataModel());}
0
public String toString()
{    return "TanimotoCoefficientSimilarity[dataModel:" + getDataModel() + ']';}
0
 double computeResult(int n, double sumXY, double sumX2, double sumY2, double sumXYdiff2)
{    if (n == 0) {        return Double.NaN;    }    double denominator = Math.sqrt(sumX2) * Math.sqrt(sumY2);    if (denominator == 0.0) {                return Double.NaN;    }    return sumXY / denominator;}
0
protected ItemBasedRecommender getRecommender()
{    return recommender;}
0
protected int getSimilarItemsPerItem()
{    return similarItemsPerItem;}
0
public int compare(SimilarItem s1, SimilarItem s2)
{    return Doubles.compare(s1.similarity, s2.similarity);}
0
public void set(long itemID, double similarity)
{    this.itemID = itemID;    this.similarity = similarity;}
0
public long getItemID()
{    return itemID;}
0
public double getSimilarity()
{    return similarity;}
0
public long getItemID()
{    return itemID;}
0
public int numSimilarItems()
{    return similarItemIDs.length;}
0
public Iterable<SimilarItem> getSimilarItems()
{    return new Iterable<SimilarItem>() {        @Override        public Iterator<SimilarItem> iterator() {            return new SimilarItemsIterator();        }    };}
0
public Iterator<SimilarItem> iterator()
{    return new SimilarItemsIterator();}
0
public boolean hasNext()
{    return index < (similarItemIDs.length - 1);}
0
public SimilarItem next()
{    if (!hasNext()) {        throw new NoSuchElementException();    }    index++;    return new SimilarItem(similarItemIDs[index], similarities[index]);}
0
public Vector classifyNoLink(Vector features)
{    throw new UnsupportedOperationException(this.getClass().getName() + " doesn't support classification without a link");}
0
public Vector classifyFull(Vector instance)
{    return classifyFull(new DenseVector(numCategories()), instance);}
0
public Vector classifyFull(Vector r, Vector instance)
{    r.viewPart(1, numCategories() - 1).assign(classify(instance));    r.setQuick(0, 1.0 - r.zSum());    return r;}
0
public Matrix classify(Matrix data)
{    Matrix r = new DenseMatrix(data.numRows(), numCategories() - 1);    for (int row = 0; row < data.numRows(); row++) {        r.assignRow(row, classify(data.viewRow(row)));    }    return r;}
0
public Matrix classifyFull(Matrix data)
{    Matrix r = new DenseMatrix(data.numRows(), numCategories());    for (int row = 0; row < data.numRows(); row++) {        classifyFull(r.viewRow(row), data.viewRow(row));    }    return r;}
0
public Vector classifyScalar(Matrix data)
{    Preconditions.checkArgument(numCategories() == 2, "Can only call classifyScalar with two categories");    Vector r = new DenseVector(data.numRows());    for (int row = 0; row < data.numRows(); row++) {        r.set(row, classifyScalar(data.viewRow(row)));    }    return r;}
0
public double logLikelihood(int actual, Vector data)
{    if (numCategories() == 2) {        double p = classifyScalar(data);        if (actual > 0) {            return Math.max(MIN_LOG_LIKELIHOOD, Math.log(p));        } else {            return Math.max(MIN_LOG_LIKELIHOOD, Math.log1p(-p));        }    } else {        Vector p = classify(data);        if (actual > 0) {            return Math.max(MIN_LOG_LIKELIHOOD, Math.log(p.get(actual - 1)));        } else {            return Math.max(MIN_LOG_LIKELIHOOD, Math.log1p(-p.zSum()));        }    }}
0
public double getLogLikelihood()
{    return logLikelihood;}
0
public void setLogLikelihood(double logLikelihood)
{    this.logLikelihood = logLikelihood;}
0
public String getLabel()
{    return label;}
0
public double getScore()
{    return score;}
0
public void setLabel(String label)
{    this.label = label;}
0
public void setScore(double score)
{    this.score = score;}
0
public String toString()
{    return "ClassifierResult{" + "category='" + label + '\'' + ", score=" + score + '}';}
0
public int[][] getConfusionMatrix()
{    return confusionMatrix;}
0
public Collection<String> getLabels()
{    return Collections.unmodifiableCollection(labelMap.keySet());}
0
private int numLabels()
{    return labelMap.size();}
0
public double getAccuracy(String label)
{    int labelId = labelMap.get(label);    int labelTotal = 0;    int correct = 0;    for (int i = 0; i < numLabels(); i++) {        labelTotal += confusionMatrix[labelId][i];        if (i == labelId) {            correct += confusionMatrix[labelId][i];        }    }    return 100.0 * correct / labelTotal;}
0
public double getAccuracy()
{    int total = 0;    int correct = 0;    for (int i = 0; i < numLabels(); i++) {        for (int j = 0; j < numLabels(); j++) {            total += confusionMatrix[i][j];            if (i == j) {                correct += confusionMatrix[i][j];            }        }    }    return 100.0 * correct / total;}
0
private int getActualNumberOfTestExamplesForClass(String label)
{    int labelId = labelMap.get(label);    int sum = 0;    for (int i = 0; i < numLabels(); i++) {        sum += confusionMatrix[labelId][i];    }    return sum;}
0
public double getPrecision(String label)
{    int labelId = labelMap.get(label);    int truePositives = confusionMatrix[labelId][labelId];    int falsePositives = 0;    for (int i = 0; i < numLabels(); i++) {        if (i == labelId) {            continue;        }        falsePositives += confusionMatrix[i][labelId];    }    if (truePositives + falsePositives == 0) {        return 0;    }    return ((double) truePositives) / (truePositives + falsePositives);}
0
public double getWeightedPrecision()
{    double[] precisions = new double[numLabels()];    double[] weights = new double[numLabels()];    int index = 0;    for (String label : labelMap.keySet()) {        precisions[index] = getPrecision(label);        weights[index] = getActualNumberOfTestExamplesForClass(label);        index++;    }    return new Mean().evaluate(precisions, weights);}
0
public double getRecall(String label)
{    int labelId = labelMap.get(label);    int truePositives = confusionMatrix[labelId][labelId];    int falseNegatives = 0;    for (int i = 0; i < numLabels(); i++) {        if (i == labelId) {            continue;        }        falseNegatives += confusionMatrix[labelId][i];    }    if (truePositives + falseNegatives == 0) {        return 0;    }    return ((double) truePositives) / (truePositives + falseNegatives);}
0
public double getWeightedRecall()
{    double[] recalls = new double[numLabels()];    double[] weights = new double[numLabels()];    int index = 0;    for (String label : labelMap.keySet()) {        recalls[index] = getRecall(label);        weights[index] = getActualNumberOfTestExamplesForClass(label);        index++;    }    return new Mean().evaluate(recalls, weights);}
0
public double getF1score(String label)
{    double precision = getPrecision(label);    double recall = getRecall(label);    if (precision + recall == 0) {        return 0;    }    return 2 * precision * recall / (precision + recall);}
0
public double getWeightedF1score()
{    double[] f1Scores = new double[numLabels()];    double[] weights = new double[numLabels()];    int index = 0;    for (String label : labelMap.keySet()) {        f1Scores[index] = getF1score(label);        weights[index] = getActualNumberOfTestExamplesForClass(label);        index++;    }    return new Mean().evaluate(f1Scores, weights);}
0
public double getReliability()
{    int count = 0;    double accuracy = 0;    for (String label : labelMap.keySet()) {        if (!label.equals(defaultLabel)) {            accuracy += getAccuracy(label);        }        count++;    }    return accuracy / count;}
0
public double getKappa()
{    double a = 0.0;    double b = 0.0;    for (int i = 0; i < confusionMatrix.length; i++) {        a += confusionMatrix[i][i];        double br = 0;        for (int j = 0; j < confusionMatrix.length; j++) {            br += confusionMatrix[i][j];        }        double bc = 0;        for (int[] vec : confusionMatrix) {            bc += vec[i];        }        b += br * bc;    }    return (samples * a - b) / (samples * samples - b);}
0
public RunningAverageAndStdDev getNormalizedStats()
{    RunningAverageAndStdDev summer = new FullRunningAverageAndStdDev();    for (int d = 0; d < confusionMatrix.length; d++) {        double total = 0;        for (int j = 0; j < confusionMatrix.length; j++) {            total += confusionMatrix[d][j];        }        summer.addDatum(confusionMatrix[d][d] / (total + 0.000001));    }    return summer;}
0
public int getCorrect(String label)
{    int labelId = labelMap.get(label);    return confusionMatrix[labelId][labelId];}
0
public int getTotal(String label)
{    int labelId = labelMap.get(label);    int labelTotal = 0;    for (int i = 0; i < labelMap.size(); i++) {        labelTotal += confusionMatrix[labelId][i];    }    return labelTotal;}
0
public void addInstance(String correctLabel, ClassifierResult classifiedResult)
{    samples++;    incrementCount(correctLabel, classifiedResult.getLabel());}
0
public void addInstance(String correctLabel, String classifiedLabel)
{    samples++;    incrementCount(correctLabel, classifiedLabel);}
0
public int getCount(String correctLabel, String classifiedLabel)
{    if (!labelMap.containsKey(correctLabel)) {                return 0;    }    Preconditions.checkArgument(labelMap.containsKey(classifiedLabel), "Label not found: " + classifiedLabel);    int correctId = labelMap.get(correctLabel);    int classifiedId = labelMap.get(classifiedLabel);    return confusionMatrix[correctId][classifiedId];}
1
public void putCount(String correctLabel, String classifiedLabel, int count)
{    if (!labelMap.containsKey(correctLabel)) {                return;    }    Preconditions.checkArgument(labelMap.containsKey(classifiedLabel), "Label not found: " + classifiedLabel);    int correctId = labelMap.get(correctLabel);    int classifiedId = labelMap.get(classifiedLabel);    if (confusionMatrix[correctId][classifiedId] == 0.0 && count != 0) {        samples++;    }    confusionMatrix[correctId][classifiedId] = count;}
1
public String getDefaultLabel()
{    return defaultLabel;}
0
public void incrementCount(String correctLabel, String classifiedLabel, int count)
{    putCount(correctLabel, classifiedLabel, count + getCount(correctLabel, classifiedLabel));}
0
public void incrementCount(String correctLabel, String classifiedLabel)
{    incrementCount(correctLabel, classifiedLabel, 1);}
0
public ConfusionMatrix merge(ConfusionMatrix b)
{    Preconditions.checkArgument(labelMap.size() == b.getLabels().size(), "The label sizes do not match");    for (String correctLabel : this.labelMap.keySet()) {        for (String classifiedLabel : this.labelMap.keySet()) {            incrementCount(correctLabel, classifiedLabel, b.getCount(correctLabel, classifiedLabel));        }    }    return this;}
0
public Matrix getMatrix()
{    int length = confusionMatrix.length;    Matrix m = new DenseMatrix(length, length);    for (int r = 0; r < length; r++) {        for (int c = 0; c < length; c++) {            m.set(r, c, confusionMatrix[r][c]);        }    }    Map<String, Integer> labels = new HashMap<>();    for (Map.Entry<String, Integer> entry : labelMap.entrySet()) {        labels.put(entry.getKey(), entry.getValue());    }    m.setRowLabelBindings(labels);    m.setColumnLabelBindings(labels);    return m;}
0
public void setMatrix(Matrix m)
{    int length = confusionMatrix.length;    if (m.numRows() != m.numCols()) {        throw new IllegalArgumentException("ConfusionMatrix: matrix(" + m.numRows() + ',' + m.numCols() + ") must be square");    }    for (int r = 0; r < length; r++) {        for (int c = 0; c < length; c++) {            confusionMatrix[r][c] = (int) Math.round(m.get(r, c));        }    }    Map<String, Integer> labels = m.getRowLabelBindings();    if (labels == null) {        labels = m.getColumnLabelBindings();    }    if (labels != null) {        String[] sorted = sortLabels(labels);        verifyLabels(length, sorted);        labelMap.clear();        for (int i = 0; i < length; i++) {            labelMap.put(sorted[i], i);        }    }}
0
private static String[] sortLabels(Map<String, Integer> labels)
{    String[] sorted = new String[labels.size()];    for (Map.Entry<String, Integer> entry : labels.entrySet()) {        sorted[entry.getValue()] = entry.getKey();    }    return sorted;}
0
private static void verifyLabels(int length, String[] sorted)
{    Preconditions.checkArgument(sorted.length == length, "One label, one row");    for (int i = 0; i < length; i++) {        if (sorted[i] == null) {            Preconditions.checkArgument(false, "One label, one row");        }    }}
0
public String toString()
{    StringBuilder returnString = new StringBuilder(200);    returnString.append("=======================================================").append('\n');    returnString.append("Confusion Matrix\n");    returnString.append("-------------------------------------------------------").append('\n');    int unclassified = getTotal(defaultLabel);    for (Map.Entry<String, Integer> entry : this.labelMap.entrySet()) {        if (entry.getKey().equals(defaultLabel) && unclassified == 0) {            continue;        }        returnString.append(StringUtils.rightPad(getSmallLabel(entry.getValue()), 5)).append('\t');    }    returnString.append("<--Classified as").append('\n');    for (Map.Entry<String, Integer> entry : this.labelMap.entrySet()) {        if (entry.getKey().equals(defaultLabel) && unclassified == 0) {            continue;        }        String correctLabel = entry.getKey();        int labelTotal = 0;        for (String classifiedLabel : this.labelMap.keySet()) {            if (classifiedLabel.equals(defaultLabel) && unclassified == 0) {                continue;            }            returnString.append(StringUtils.rightPad(Integer.toString(getCount(correctLabel, classifiedLabel)), 5)).append('\t');            labelTotal += getCount(correctLabel, classifiedLabel);        }        returnString.append(" |  ").append(StringUtils.rightPad(String.valueOf(labelTotal), 6)).append('\t').append(StringUtils.rightPad(getSmallLabel(entry.getValue()), 5)).append(" = ").append(correctLabel).append('\n');    }    if (unclassified > 0) {        returnString.append("Default Category: ").append(defaultLabel).append(": ").append(unclassified).append('\n');    }    returnString.append('\n');    return returnString.toString();}
0
 static String getSmallLabel(int i)
{    int val = i;    StringBuilder returnString = new StringBuilder();    do {        int n = val % 26;        returnString.insert(0, (char) ('a' + n));        val /= 26;    } while (val > 0);    return returnString.toString();}
0
public Node build(Random rng)
{        Arrays.fill(sampled, false);    Data bag = data.bagging(rng, sampled);        return treeBuilder.build(rng, bag);}
1
public void setM(int m)
{    this.m = m;}
0
public void setIgSplit(IgSplit igSplit)
{    this.igSplit = igSplit;}
0
public void setComplemented(boolean complemented)
{    this.complemented = complemented;}
0
public void setMinSplitNum(int minSplitNum)
{    this.minSplitNum = minSplitNum;}
0
public void setMinVarianceProportion(double minVarianceProportion)
{    this.minVarianceProportion = minVarianceProportion;}
0
public Node build(Random rng, Data data)
{    if (selected == null) {        selected = new boolean[data.getDataset().nbAttributes()];                selected[data.getDataset().getLabelId()] = true;    }    if (m == 0) {                double e = data.getDataset().nbAttributes() - 1;        if (data.getDataset().isNumerical(data.getDataset().getLabelId())) {                        m = (int) Math.ceil(e / 3.0);        } else {                        m = (int) Math.ceil(Math.sqrt(e));        }    }    if (data.isEmpty()) {        return new Leaf(Double.NaN);    }    double sum = 0.0;    if (data.getDataset().isNumerical(data.getDataset().getLabelId())) {                        double sumSquared = 0.0;        for (int i = 0; i < data.size(); i++) {            double label = data.getDataset().getLabel(data.get(i));            sum += label;            sumSquared += label * label;        }                double var = sumSquared - (sum * sum) / data.size();                if (Double.compare(minVariance, Double.NaN) == 0) {            minVariance = var / data.size() * minVarianceProportion;                    }                if ((var / data.size()) < minVariance) {                        return new Leaf(sum / data.size());        }    } else {                if (isIdentical(data)) {            return new Leaf(data.majorityLabel(rng));        }        if (data.identicalLabel()) {            return new Leaf(data.getDataset().getLabel(data.get(0)));        }    }        if (fullSet == null) {        fullSet = data;    }    int[] attributes = randomAttributes(rng, selected, m);    if (attributes == null || attributes.length == 0) {                double label;        if (data.getDataset().isNumerical(data.getDataset().getLabelId())) {                        label = sum / data.size();        } else {                        label = data.majorityLabel(rng);        }                return new Leaf(label);    }    if (igSplit == null) {        if (data.getDataset().isNumerical(data.getDataset().getLabelId())) {                        igSplit = new RegressionSplit();        } else {                        igSplit = new OptIgSplit();        }    }        Split best = null;    for (int attr : attributes) {        Split split = igSplit.computeSplit(data, attr);        if (best == null || best.getIg() < split.getIg()) {            best = split;        }    }        if (best.getIg() < EPSILON) {        double label;        if (data.getDataset().isNumerical(data.getDataset().getLabelId())) {            label = sum / data.size();        } else {            label = data.majorityLabel(rng);        }                return new Leaf(label);    }        boolean alreadySelected = selected[best.getAttr()];    if (alreadySelected) {                    }    Node childNode;    if (data.getDataset().isNumerical(best.getAttr())) {        boolean[] temp = null;        Data loSubset = data.subset(Condition.lesser(best.getAttr(), best.getSplit()));        Data hiSubset = data.subset(Condition.greaterOrEquals(best.getAttr(), best.getSplit()));        if (loSubset.isEmpty() || hiSubset.isEmpty()) {                        selected[best.getAttr()] = true;        } else {                        temp = selected;            selected = cloneCategoricalAttributes(data.getDataset(), selected);        }                if (loSubset.size() < minSplitNum || hiSubset.size() < minSplitNum) {                        double label;            if (data.getDataset().isNumerical(data.getDataset().getLabelId())) {                label = sum / data.size();            } else {                label = data.majorityLabel(rng);            }                        return new Leaf(label);        }        Node loChild = build(rng, loSubset);        Node hiChild = build(rng, hiSubset);                if (temp != null) {            selected = temp;        } else {            selected[best.getAttr()] = alreadySelected;        }        childNode = new NumericalNode(best.getAttr(), best.getSplit(), loChild, hiChild);    } else {                double[] values = data.values(best.getAttr());                Collection<Double> subsetValues = null;        if (complemented) {            subsetValues = new HashSet<>();            for (double value : values) {                subsetValues.add(value);            }            values = fullSet.values(best.getAttr());        }        int cnt = 0;        Data[] subsets = new Data[values.length];        for (int index = 0; index < values.length; index++) {            if (complemented && !subsetValues.contains(values[index])) {                continue;            }            subsets[index] = data.subset(Condition.equals(best.getAttr(), values[index]));            if (subsets[index].size() >= minSplitNum) {                cnt++;            }        }                if (cnt < 2) {                        double label;            if (data.getDataset().isNumerical(data.getDataset().getLabelId())) {                label = sum / data.size();            } else {                label = data.majorityLabel(rng);            }                        return new Leaf(label);        }        selected[best.getAttr()] = true;        Node[] children = new Node[values.length];        for (int index = 0; index < values.length; index++) {            if (complemented && (subsetValues == null || !subsetValues.contains(values[index]))) {                                double label;                if (data.getDataset().isNumerical(data.getDataset().getLabelId())) {                    label = sum / data.size();                } else {                    label = data.majorityLabel(rng);                }                                children[index] = new Leaf(label);                continue;            }            children[index] = build(rng, subsets[index]);        }        selected[best.getAttr()] = alreadySelected;        childNode = new CategoricalNode(best.getAttr(), values, children);    }    return childNode;}
1
private boolean isIdentical(Data data)
{    if (data.isEmpty()) {        return true;    }    Instance instance = data.get(0);    for (int attr = 0; attr < selected.length; attr++) {        if (selected[attr]) {            continue;        }        for (int index = 1; index < data.size(); index++) {            if (data.get(index).get(attr) != instance.get(attr)) {                return false;            }        }    }    return true;}
0
private static boolean[] cloneCategoricalAttributes(Dataset dataset, boolean[] selected)
{    boolean[] cloned = new boolean[selected.length];    for (int i = 0; i < selected.length; i++) {        cloned[i] = !dataset.isNumerical(i) && selected[i];    }    cloned[dataset.getLabelId()] = true;    return cloned;}
0
private static int[] randomAttributes(Random rng, boolean[] selected, int m)
{        int nbNonSelected = 0;    for (boolean sel : selected) {        if (!sel) {            nbNonSelected++;        }    }    if (nbNonSelected == 0) {                return NO_ATTRIBUTES;    }    int[] result;    if (nbNonSelected <= m) {                result = new int[nbNonSelected];        int index = 0;        for (int attr = 0; attr < selected.length; attr++) {            if (!selected[attr]) {                result[index++] = attr;            }        }    } else {        result = new int[m];        for (int index = 0; index < m; index++) {                        int rind;            do {                rind = rng.nextInt(selected.length);            } while (selected[rind]);            result[index] = rind;                        selected[rind] = true;        }                for (int attr : result) {            selected[attr] = false;        }    }    return result;}
1
public void setM(int m)
{    this.m = m;}
0
public Node build(Random rng, Data data)
{    if (selected == null) {        selected = new boolean[data.getDataset().nbAttributes()];                selected[data.getDataset().getLabelId()] = true;    }    if (data.isEmpty()) {        return new Leaf(-1);    }    if (isIdentical(data)) {        return new Leaf(data.majorityLabel(rng));    }    if (data.identicalLabel()) {        return new Leaf(data.getDataset().getLabel(data.get(0)));    }    int[] attributes = randomAttributes(rng, selected, m);    if (attributes == null || attributes.length == 0) {                return new Leaf(data.majorityLabel(rng));    }        Split best = null;    for (int attr : attributes) {        Split split = igSplit.computeSplit(data, attr);        if (best == null || best.getIg() < split.getIg()) {            best = split;        }    }    boolean alreadySelected = selected[best.getAttr()];    if (alreadySelected) {                    }    Node childNode;    if (data.getDataset().isNumerical(best.getAttr())) {        boolean[] temp = null;        Data loSubset = data.subset(Condition.lesser(best.getAttr(), best.getSplit()));        Data hiSubset = data.subset(Condition.greaterOrEquals(best.getAttr(), best.getSplit()));        if (loSubset.isEmpty() || hiSubset.isEmpty()) {                        selected[best.getAttr()] = true;        } else {                        temp = selected;            selected = cloneCategoricalAttributes(data.getDataset(), selected);        }        Node loChild = build(rng, loSubset);        Node hiChild = build(rng, hiSubset);                if (temp != null) {            selected = temp;        } else {            selected[best.getAttr()] = alreadySelected;        }        childNode = new NumericalNode(best.getAttr(), best.getSplit(), loChild, hiChild);    } else {                selected[best.getAttr()] = true;        double[] values = data.values(best.getAttr());        Node[] children = new Node[values.length];        for (int index = 0; index < values.length; index++) {            Data subset = data.subset(Condition.equals(best.getAttr(), values[index]));            children[index] = build(rng, subset);        }        selected[best.getAttr()] = alreadySelected;        childNode = new CategoricalNode(best.getAttr(), values, children);    }    return childNode;}
1
private boolean isIdentical(Data data)
{    if (data.isEmpty()) {        return true;    }    Instance instance = data.get(0);    for (int attr = 0; attr < selected.length; attr++) {        if (selected[attr]) {            continue;        }        for (int index = 1; index < data.size(); index++) {            if (data.get(index).get(attr) != instance.get(attr)) {                return false;            }        }    }    return true;}
0
private static boolean[] cloneCategoricalAttributes(Dataset dataset, boolean[] selected)
{    boolean[] cloned = new boolean[selected.length];    for (int i = 0; i < selected.length; i++) {        cloned[i] = !dataset.isNumerical(i) && selected[i];    }    return cloned;}
0
protected static int[] randomAttributes(Random rng, boolean[] selected, int m)
{        int nbNonSelected = 0;    for (boolean sel : selected) {        if (!sel) {            nbNonSelected++;        }    }    if (nbNonSelected == 0) {                return NO_ATTRIBUTES;    }    int[] result;    if (nbNonSelected <= m) {                result = new int[nbNonSelected];        int index = 0;        for (int attr = 0; attr < selected.length; attr++) {            if (!selected[attr]) {                result[index++] = attr;            }        }    } else {        result = new int[m];        for (int index = 0; index < m; index++) {                        int rind;            do {                rind = rng.nextInt(selected.length);            } while (selected[rind]);            result[index] = rind;                        selected[rind] = true;        }                for (int attr : result) {            selected[attr] = false;        }    }    return result;}
1
public static Condition equals(int attr, double value)
{    return new Equals(attr, value);}
0
public static Condition lesser(int attr, double value)
{    return new Lesser(attr, value);}
0
public static Condition greaterOrEquals(int attr, double value)
{    return new GreaterOrEquals(attr, value);}
0
public boolean isTrueFor(Instance instance)
{    return instance.get(attr) == value;}
0
public boolean isTrueFor(Instance v)
{    return v.get(attr) >= value;}
0
public boolean isTrueFor(Instance instance)
{    return instance.get(attr) < value;}
0
public int size()
{    return instances.size();}
0
public boolean isEmpty()
{    return instances.isEmpty();}
0
public boolean contains(Instance v)
{    return instances.contains(v);}
0
public Instance get(int index)
{    return instances.get(index);}
0
public Data subset(Condition condition)
{    List<Instance> subset = new ArrayList<>();    for (Instance instance : instances) {        if (condition.isTrueFor(instance)) {            subset.add(instance);        }    }    return new Data(dataset, subset);}
0
public Data bagging(Random rng)
{    int datasize = size();    List<Instance> bag = new ArrayList<>(datasize);    for (int i = 0; i < datasize; i++) {        bag.add(instances.get(rng.nextInt(datasize)));    }    return new Data(dataset, bag);}
0
public Data bagging(Random rng, boolean[] sampled)
{    int datasize = size();    List<Instance> bag = new ArrayList<>(datasize);    for (int i = 0; i < datasize; i++) {        int index = rng.nextInt(datasize);        bag.add(instances.get(index));        sampled[index] = true;    }    return new Data(dataset, bag);}
0
public Data rsplit(Random rng, int subsize)
{    List<Instance> subset = new ArrayList<>(subsize);    for (int i = 0; i < subsize; i++) {        subset.add(instances.remove(rng.nextInt(instances.size())));    }    return new Data(dataset, subset);}
0
public boolean isIdentical()
{    if (isEmpty()) {        return true;    }    Instance instance = get(0);    for (int attr = 0; attr < dataset.nbAttributes(); attr++) {        for (int index = 1; index < size(); index++) {            if (get(index).get(attr) != instance.get(attr)) {                return false;            }        }    }    return true;}
0
public boolean identicalLabel()
{    if (isEmpty()) {        return true;    }    double label = dataset.getLabel(get(0));    for (int index = 1; index < size(); index++) {        if (dataset.getLabel(get(index)) != label) {            return false;        }    }    return true;}
0
public double[] values(int attr)
{    Collection<Double> result = new HashSet<>();    for (Instance instance : instances) {        result.add(instance.get(attr));    }    double[] values = new double[result.size()];    int index = 0;    for (Double value : result) {        values[index++] = value;    }    return values;}
0
public Data clone()
{    return new Data(dataset, new ArrayList<>(instances));}
0
public boolean equals(Object obj)
{    if (this == obj) {        return true;    }    if (!(obj instanceof Data)) {        return false;    }    Data data = (Data) obj;    return instances.equals(data.instances) && dataset.equals(data.dataset);}
0
public int hashCode()
{    return instances.hashCode() + dataset.hashCode();}
0
public double[] extractLabels()
{    double[] labels = new double[size()];    for (int index = 0; index < labels.length; index++) {        labels[index] = dataset.getLabel(get(index));    }    return labels;}
0
public int majorityLabel(Random rng)
{        int[] counts = new int[dataset.nblabels()];    for (int index = 0; index < size(); index++) {        counts[(int) dataset.getLabel(get(index))]++;    }        return DataUtils.maxindex(rng, counts);}
0
public void countLabels(int[] counts)
{    for (int index = 0; index < size(); index++) {        counts[(int) dataset.getLabel(get(index))]++;    }}
0
public Dataset getDataset()
{    return dataset;}
0
public Instance convert(CharSequence string)
{        int nball = dataset.nbAttributes() + dataset.getIgnored().length;    String[] tokens = COMMA_SPACE.split(string);    Preconditions.checkArgument(tokens.length == nball, "Wrong number of attributes in the string: " + tokens.length + ". Must be " + nball);    int nbattrs = dataset.nbAttributes();    DenseVector vector = new DenseVector(nbattrs);    int aId = 0;    for (int attr = 0; attr < nball; attr++) {        if (!ArrayUtils.contains(dataset.getIgnored(), attr)) {            String token = tokens[attr].trim();            if ("?".equals(token)) {                                return null;            }            if (dataset.isNumerical(aId)) {                vector.set(aId++, Double.parseDouble(token));            } else {                                vector.set(aId, dataset.valueOf(aId, token));                aId++;            }        }    }    return new Instance(vector);}
0
private static boolean parseString(Attribute[] attrs, Set<String>[] values, CharSequence string, boolean regression)
{    String[] tokens = SEPARATORS.split(string);    Preconditions.checkArgument(tokens.length == attrs.length, "Wrong number of attributes in the string: " + tokens.length + ". Must be: " + attrs.length);        for (int attr = 0; attr < attrs.length; attr++) {        if (!attrs[attr].isIgnored() && "?".equals(tokens[attr])) {                        return false;        }    }    for (int attr = 0; attr < attrs.length; attr++) {        if (!attrs[attr].isIgnored()) {            String token = tokens[attr];            if (attrs[attr].isCategorical() || (!regression && attrs[attr].isLabel())) {                                if (values[attr] == null) {                    values[attr] = new HashSet<>();                }                values[attr].add(token);            } else {                try {                    Double.parseDouble(token);                } catch (NumberFormatException e) {                    return false;                }            }        }    }    return true;}
0
public static Data loadData(Dataset dataset, FileSystem fs, Path fpath) throws IOException
{    FSDataInputStream input = fs.open(fpath);    Scanner scanner = new Scanner(input, "UTF-8");    List<Instance> instances = new ArrayList<>();    DataConverter converter = new DataConverter(dataset);    while (scanner.hasNextLine()) {        String line = scanner.nextLine();        if (!line.isEmpty()) {            Instance instance = converter.convert(line);            if (instance != null) {                instances.add(instance);            } else {                                            }        } else {                    }    }    scanner.close();    return new Data(dataset, instances);}
1
public static Data loadData(Dataset dataset, FileSystem fs, Path[] pathes) throws IOException
{    List<Instance> instances = new ArrayList<>();    for (Path path : pathes) {        Data loadedData = loadData(dataset, fs, path);        for (int index = 0; index <= loadedData.size(); index++) {            instances.add(loadedData.get(index));        }    }    return new Data(dataset, instances);}
0
public static Data loadData(Dataset dataset, String[] data)
{    List<Instance> instances = new ArrayList<>();    DataConverter converter = new DataConverter(dataset);    for (String line : data) {        if (!line.isEmpty()) {            Instance instance = converter.convert(line);            if (instance != null) {                instances.add(instance);            } else {                                            }        } else {                    }    }    return new Data(dataset, instances);}
1
public static Dataset generateDataset(CharSequence descriptor, boolean regression, FileSystem fs, Path path) throws DescriptorException, IOException
{    Attribute[] attrs = DescriptorUtils.parseDescriptor(descriptor);    FSDataInputStream input = fs.open(path);    Scanner scanner = new Scanner(input, "UTF-8");        @SuppressWarnings("unchecked")    Set<String>[] valsets = new Set[attrs.length];    int size = 0;    while (scanner.hasNextLine()) {        String line = scanner.nextLine();        if (!line.isEmpty()) {            if (parseString(attrs, valsets, line, regression)) {                size++;            }        }    }    scanner.close();    @SuppressWarnings("unchecked")    List<String>[] values = new List[attrs.length];    for (int i = 0; i < valsets.length; i++) {        if (valsets[i] != null) {            values[i] = Lists.newArrayList(valsets[i]);        }    }    return new Dataset(attrs, values, size, regression);}
0
public static Dataset generateDataset(CharSequence descriptor, boolean regression, String[] data) throws DescriptorException
{    Attribute[] attrs = DescriptorUtils.parseDescriptor(descriptor);        @SuppressWarnings("unchecked")    Set<String>[] valsets = new Set[attrs.length];    int size = 0;    for (String aData : data) {        if (!aData.isEmpty()) {            if (parseString(attrs, valsets, aData, regression)) {                size++;            }        }    }    @SuppressWarnings("unchecked")    List<String>[] values = new List[attrs.length];    for (int i = 0; i < valsets.length; i++) {        if (valsets[i] != null) {            values[i] = Lists.newArrayList(valsets[i]);        }    }    return new Dataset(attrs, values, size, regression);}
0
public boolean isNumerical()
{    return this == NUMERICAL;}
0
public boolean isCategorical()
{    return this == CATEGORICAL;}
0
public boolean isLabel()
{    return this == LABEL;}
0
public boolean isIgnored()
{    return this == IGNORED;}
0
private static Attribute fromString(String from)
{    Attribute toReturn = LABEL;    if (NUMERICAL.toString().equalsIgnoreCase(from)) {        toReturn = NUMERICAL;    } else if (CATEGORICAL.toString().equalsIgnoreCase(from)) {        toReturn = CATEGORICAL;    } else if (IGNORED.toString().equalsIgnoreCase(from)) {        toReturn = IGNORED;    }    return toReturn;}
0
public int nbValues(int attr)
{    return values[attr].length;}
0
public String[] labels()
{    return Arrays.copyOf(values[labelId], nblabels());}
0
public int nblabels()
{    return values[labelId].length;}
0
public int getLabelId()
{    return labelId;}
0
public double getLabel(Instance instance)
{    return instance.get(getLabelId());}
0
public Attribute getAttribute(int attr)
{    return attributes[attr];}
0
public int labelCode(String label)
{    return ArrayUtils.indexOf(values[labelId], label);}
0
public String getLabelString(double code)
{        if (Double.isNaN(code)) {        return "unknown";    }    return values[labelId][(int) code];}
0
public String toString()
{    return "attributes=" + Arrays.toString(attributes);}
0
public int valueOf(int attr, String token)
{    Preconditions.checkArgument(!isNumerical(attr), "Only for CATEGORICAL attributes");    Preconditions.checkArgument(values != null, "Values not found (equals null)");    return ArrayUtils.indexOf(values[attr], token);}
0
public int[] getIgnored()
{    return ignored;}
0
private static int countAttributes(Attribute[] attrs)
{    int nbattrs = 0;    for (Attribute attr : attrs) {        if (!attr.isIgnored()) {            nbattrs++;        }    }    return nbattrs;}
0
private static void validateValues(Attribute[] attrs, List<String>[] values)
{    Preconditions.checkArgument(attrs.length == values.length, "attrs.length != values.length");    for (int attr = 0; attr < attrs.length; attr++) {        Preconditions.checkArgument(!attrs[attr].isCategorical() || values[attr] != null, "values not found for attribute " + attr);    }}
0
public int nbAttributes()
{    return attributes.length;}
0
public boolean isNumerical(int attr)
{    return attributes[attr].isNumerical();}
0
public boolean equals(Object obj)
{    if (this == obj) {        return true;    }    if (!(obj instanceof Dataset)) {        return false;    }    Dataset dataset = (Dataset) obj;    if (!Arrays.equals(attributes, dataset.attributes)) {        return false;    }    for (int attr = 0; attr < nbAttributes(); attr++) {        if (!Arrays.equals(values[attr], dataset.values[attr])) {            return false;        }    }    return labelId == dataset.labelId && nbInstances == dataset.nbInstances;}
0
public int hashCode()
{    int hashCode = labelId + 31 * nbInstances;    for (Attribute attr : attributes) {        hashCode = 31 * hashCode + attr.hashCode();    }    for (String[] valueRow : values) {        if (valueRow == null) {            continue;        }        for (String value : valueRow) {            hashCode = 31 * hashCode + value.hashCode();        }    }    return hashCode;}
0
public static Dataset load(Configuration conf, Path path) throws IOException
{    FileSystem fs = path.getFileSystem(conf);    long bytesToRead = fs.getFileStatus(path).getLen();    byte[] buff = new byte[Long.valueOf(bytesToRead).intValue()];    FSDataInputStream input = fs.open(path);    try {        input.readFully(buff);    } finally {        Closeables.close(input, true);    }    String json = new String(buff, Charset.defaultCharset());    return fromJSON(json);}
0
public String toJSON()
{    List<Map<String, Object>> toWrite = new LinkedList<>();        int ignoredCount = 0;    for (int i = 0; i < attributes.length + ignored.length; i++) {        Map<String, Object> attribute;        int attributesIndex = i - ignoredCount;        if (ignoredCount < ignored.length && i == ignored[ignoredCount]) {                        attribute = getMap(Attribute.IGNORED, null, false);            ignoredCount++;        } else if (attributesIndex == labelId) {                        attribute = getMap(attributes[attributesIndex], values[attributesIndex], true);        } else {                        attribute = getMap(attributes[attributesIndex], values[attributesIndex], false);        }        toWrite.add(attribute);    }    try {        return OBJECT_MAPPER.writeValueAsString(toWrite);    } catch (Exception ex) {        throw new RuntimeException(ex);    }}
0
public static Dataset fromJSON(String json)
{    List<Map<String, Object>> fromJSON;    try {        fromJSON = OBJECT_MAPPER.readValue(json, new TypeReference<List<Map<String, Object>>>() {        });    } catch (Exception ex) {        throw new RuntimeException(ex);    }    List<Attribute> attributes = new LinkedList<>();    List<Integer> ignored = new LinkedList<>();    String[][] nominalValues = new String[fromJSON.size()][];    Dataset dataset = new Dataset();    for (int i = 0; i < fromJSON.size(); i++) {        Map<String, Object> attribute = fromJSON.get(i);        if (Attribute.fromString((String) attribute.get(TYPE)) == Attribute.IGNORED) {            ignored.add(i);        } else {            Attribute asAttribute = Attribute.fromString((String) attribute.get(TYPE));            attributes.add(asAttribute);            if ((Boolean) attribute.get(LABEL)) {                dataset.labelId = i - ignored.size();            }            if (attribute.get(VALUES) != null) {                List<String> get = (List<String>) attribute.get(VALUES);                String[] array = get.toArray(new String[get.size()]);                nominalValues[i - ignored.size()] = array;            }        }    }    dataset.attributes = attributes.toArray(new Attribute[attributes.size()]);    dataset.ignored = new int[ignored.size()];    dataset.values = nominalValues;    for (int i = 0; i < dataset.ignored.length; i++) {        dataset.ignored[i] = ignored.get(i);    }    return dataset;}
0
private Map<String, Object> getMap(Attribute type, String[] values, boolean isLabel)
{    Map<String, Object> attribute = new HashMap<>();    attribute.put(TYPE, type.toString().toLowerCase(Locale.getDefault()));    attribute.put(VALUES, values);    attribute.put(LABEL, isLabel);    return attribute;}
0
public static int sum(int[] values)
{    int sum = 0;    for (int value : values) {        sum += value;    }    return sum;}
0
public static void add(int[] array1, int[] array2)
{    Preconditions.checkArgument(array1.length == array2.length, "array1.length != array2.length");    for (int index = 0; index < array1.length; index++) {        array1[index] += array2[index];    }}
0
public static void dec(int[] array1, int[] array2)
{    Preconditions.checkArgument(array1.length == array2.length, "array1.length != array2.length");    for (int index = 0; index < array1.length; index++) {        array1[index] -= array2[index];    }}
0
public static int maxindex(Random rng, int[] values)
{    int max = 0;    List<Integer> maxindices = new ArrayList<>();    for (int index = 0; index < values.length; index++) {        if (values[index] > max) {            max = values[index];            maxindices.clear();            maxindices.add(index);        } else if (values[index] == max) {            maxindices.add(index);        }    }    return maxindices.size() > 1 ? maxindices.get(rng.nextInt(maxindices.size())) : maxindices.get(0);}
0
public static Attribute[] parseDescriptor(CharSequence descriptor) throws DescriptorException
{    List<Attribute> attributes = new ArrayList<>();    for (String token : SPACE.split(descriptor)) {        token = token.toUpperCase(Locale.ENGLISH);        if ("I".equals(token)) {            attributes.add(Attribute.IGNORED);        } else if ("N".equals(token)) {            attributes.add(Attribute.NUMERICAL);        } else if ("C".equals(token)) {            attributes.add(Attribute.CATEGORICAL);        } else if ("L".equals(token)) {            attributes.add(Attribute.LABEL);        } else {            throw new DescriptorException("Bad Token : " + token);        }    }    return attributes.toArray(new Attribute[attributes.size()]);}
0
public static String generateDescriptor(CharSequence description) throws DescriptorException
{    return generateDescriptor(SPACE.split(description));}
0
public static String generateDescriptor(Iterable<String> tokens) throws DescriptorException
{    StringBuilder descriptor = new StringBuilder();    int multiplicator = 0;    for (String token : tokens) {        try {                        int number = Integer.parseInt(token);            if (number <= 0) {                throw new DescriptorException("Multiplicator (" + number + ") must be > 0");            }            if (multiplicator > 0) {                throw new DescriptorException("A multiplicator cannot be followed by another multiplicator");            }            multiplicator = number;        } catch (NumberFormatException e) {                        if (multiplicator == 0) {                multiplicator = 1;            }            for (int index = 0; index < multiplicator; index++) {                descriptor.append(token).append(' ');            }            multiplicator = 0;        }    }    return descriptor.toString().trim();}
0
public double get(int index)
{    return attrs.getQuick(index);}
0
public void set(int index, double value)
{    attrs.set(index, value);}
0
public boolean equals(Object obj)
{    if (this == obj) {        return true;    }    if (!(obj instanceof Instance)) {        return false;    }    Instance instance = (Instance) obj;    return /*id == instance.id &&*/    attrs.equals(instance.attrs);}
0
public int hashCode()
{    return /*id +*/    attrs.hashCode();}
0
 List<Node> getTrees()
{    return trees;}
0
public void classify(Data data, double[][] predictions)
{    Preconditions.checkArgument(data.size() == predictions.length, "predictions.length must be equal to data.size()");    if (data.isEmpty()) {                return;    }    int treeId = 0;    for (Node tree : trees) {        for (int index = 0; index < data.size(); index++) {            if (predictions[index] == null) {                predictions[index] = new double[trees.size()];            }            predictions[index][treeId] = tree.classify(data.get(index));        }        treeId++;    }}
0
public double classify(Dataset dataset, Random rng, Instance instance)
{    if (dataset.isNumerical(dataset.getLabelId())) {        double sum = 0;        int cnt = 0;        for (Node tree : trees) {            double prediction = tree.classify(instance);            if (!Double.isNaN(prediction)) {                sum += prediction;                cnt++;            }        }        if (cnt > 0) {            return sum / cnt;        } else {            return Double.NaN;        }    } else {        int[] predictions = new int[dataset.nblabels()];        for (Node tree : trees) {            double prediction = tree.classify(instance);            if (!Double.isNaN(prediction)) {                predictions[(int) prediction]++;            }        }        if (DataUtils.sum(predictions) == 0) {                        return Double.NaN;        }        return DataUtils.maxindex(rng, predictions);    }}
0
public long meanNbNodes()
{    long sum = 0;    for (Node tree : trees) {        sum += tree.nbNodes();    }    return sum / trees.size();}
0
public long nbNodes()
{    long sum = 0;    for (Node tree : trees) {        sum += tree.nbNodes();    }    return sum;}
0
public long meanMaxDepth()
{    long sum = 0;    for (Node tree : trees) {        sum += tree.maxDepth();    }    return sum / trees.size();}
0
public boolean equals(Object obj)
{    if (this == obj) {        return true;    }    if (!(obj instanceof DecisionForest)) {        return false;    }    DecisionForest rf = (DecisionForest) obj;    return trees.size() == rf.getTrees().size() && trees.containsAll(rf.getTrees());}
0
public int hashCode()
{    return trees.hashCode();}
0
public void write(DataOutput dataOutput) throws IOException
{    dataOutput.writeInt(trees.size());    for (Node tree : trees) {        tree.write(dataOutput);    }}
0
public void readFields(DataInput dataInput) throws IOException
{    int size = dataInput.readInt();    for (int i = 0; i < size; i++) {        trees.add(Node.read(dataInput));    }}
0
public static DecisionForest read(DataInput dataInput) throws IOException
{    DecisionForest forest = new DecisionForest();    forest.readFields(dataInput);    return forest;}
0
public static DecisionForest load(Configuration conf, Path forestPath) throws IOException
{    FileSystem fs = forestPath.getFileSystem(conf);    Path[] files;    if (fs.getFileStatus(forestPath).isDir()) {        files = DFUtils.listOutputFiles(fs, forestPath);    } else {        files = new Path[] { forestPath };    }    DecisionForest forest = null;    for (Path path : files) {        try (FSDataInputStream dataInput = new FSDataInputStream(fs.open(path))) {            if (forest == null) {                forest = read(dataInput);            } else {                forest.readFields(dataInput);            }        }    }    return forest;}
0
public static void writeArray(DataOutput out, Node[] array) throws IOException
{    out.writeInt(array.length);    for (Node w : array) {        w.write(out);    }}
0
public static Node[] readNodeArray(DataInput in) throws IOException
{    int length = in.readInt();    Node[] nodes = new Node[length];    for (int index = 0; index < length; index++) {        nodes[index] = Node.read(in);    }    return nodes;}
0
public static void writeArray(DataOutput out, double[] array) throws IOException
{    out.writeInt(array.length);    for (double value : array) {        out.writeDouble(value);    }}
0
public static double[] readDoubleArray(DataInput in) throws IOException
{    int length = in.readInt();    double[] array = new double[length];    for (int index = 0; index < length; index++) {        array[index] = in.readDouble();    }    return array;}
0
public static void writeArray(DataOutput out, int[] array) throws IOException
{    out.writeInt(array.length);    for (int value : array) {        out.writeInt(value);    }}
0
public static int[] readIntArray(DataInput in) throws IOException
{    int length = in.readInt();    int[] array = new int[length];    for (int index = 0; index < length; index++) {        array[index] = in.readInt();    }    return array;}
0
public static Path[] listOutputFiles(FileSystem fs, Path outputPath) throws IOException
{    List<Path> outputFiles = new ArrayList<>();    for (FileStatus s : fs.listStatus(outputPath, PathFilters.logsCRCFilter())) {        if (!s.isDir() && !s.getPath().getName().startsWith("_")) {            outputFiles.add(s.getPath());        }    }    if (outputFiles.isEmpty()) {        throw new IOException("No output found !");    }    return outputFiles.toArray(new Path[outputFiles.size()]);}
0
public static String elapsedTime(long milli)
{    long seconds = milli / 1000;    milli %= 1000;    long minutes = seconds / 60;    seconds %= 60;    long hours = minutes / 60;    minutes %= 60;    return hours + "h " + minutes + "m " + seconds + "s " + milli;}
0
public static void storeWritable(Configuration conf, Path path, Writable writable) throws IOException
{    FileSystem fs = path.getFileSystem(conf);    try (FSDataOutputStream out = fs.create(path)) {        writable.write(out);    }}
0
public static void storeString(Configuration conf, Path path, String string) throws IOException
{    try (DataOutputStream out = path.getFileSystem(conf).create(path)) {        out.write(string.getBytes(Charset.defaultCharset()));    }}
0
public static double errorRate(double[] labels, double[] predictions)
{    Preconditions.checkArgument(labels.length == predictions.length, "labels.length != predictions.length");        double nberrors = 0;        double datasize = 0;    for (int index = 0; index < labels.length; index++) {        if (predictions[index] == -1) {                        continue;        }        if (predictions[index] != labels[index]) {            nberrors++;        }        datasize++;    }    return nberrors / datasize;}
0
protected Path getDataPath()
{    return dataPath;}
0
public static int getNumMaps(Configuration conf)
{    return conf.getInt("mapred.map.tasks", -1);}
0
protected static boolean isOutput(Configuration conf)
{    return conf.getBoolean("debug.mahout.rf.output", true);}
0
public static Long getRandomSeed(Configuration conf)
{    String seed = conf.get("mahout.rf.random.seed");    if (seed == null) {        return null;    }    return Long.valueOf(seed);}
0
private static void setRandomSeed(Configuration conf, long seed)
{    conf.setLong("mahout.rf.random.seed", seed);}
0
public static TreeBuilder getTreeBuilder(Configuration conf)
{    String string = conf.get("mahout.rf.treebuilder");    if (string == null) {        return null;    }    return StringUtils.fromString(string);}
0
private static void setTreeBuilder(Configuration conf, TreeBuilder treeBuilder)
{    conf.set("mahout.rf.treebuilder", StringUtils.toString(treeBuilder));}
0
public static int getNbTrees(Configuration conf)
{    return conf.getInt("mahout.rf.nbtrees", -1);}
0
public static void setNbTrees(Configuration conf, int nbTrees)
{    Preconditions.checkArgument(nbTrees > 0, "nbTrees should be greater than 0");    conf.setInt("mahout.rf.nbtrees", nbTrees);}
0
public void setOutputDirName(String name)
{    outputDirName = name;}
0
protected Path getOutputPath(Configuration conf) throws IOException
{            FileSystem fs = FileSystem.get(conf);    return new Path(fs.getWorkingDirectory(), outputDirName);}
0
public static Path getDistributedCacheFile(Configuration conf, int index) throws IOException
{    Path[] files = HadoopUtil.getCachedFiles(conf);    if (files.length <= index) {        throw new IOException("path not found in the DistributedCache");    }    return files[index];}
0
public static Dataset loadDataset(Configuration conf) throws IOException
{    Path datasetPath = getDistributedCacheFile(conf, 0);    return Dataset.load(conf, datasetPath);}
0
protected boolean runJob(Job job) throws ClassNotFoundException, IOException, InterruptedException
{    return job.waitForCompletion(true);}
0
public DecisionForest build(int nbTrees) throws IOException, ClassNotFoundException, InterruptedException
{        Path outputPath = getOutputPath(conf);    FileSystem fs = outputPath.getFileSystem(conf);        if (fs.exists(outputPath)) {        throw new IOException("Output path already exists : " + outputPath);    }    if (seed != null) {        setRandomSeed(conf, seed);    }    setNbTrees(conf, nbTrees);    setTreeBuilder(conf, treeBuilder);        DistributedCache.addCacheFile(datasetPath.toUri(), conf);    Job job = new Job(conf, "decision forest builder");        configureJob(job);        if (!runJob(job)) {                return null;    }    if (isOutput(conf)) {                DecisionForest forest = parseOutput(job);        HadoopUtil.delete(conf, outputPath);        return forest;    }    return null;}
1
public static void sortSplits(InputSplit[] splits)
{    Arrays.sort(splits, new Comparator<InputSplit>() {        @Override        public int compare(InputSplit a, InputSplit b) {            try {                long left = a.getLength();                long right = b.getLength();                if (left == right) {                    return 0;                } else if (left < right) {                    return 1;                } else {                    return -1;                }            } catch (IOException ie) {                throw new IllegalStateException("Problem getting input split size", ie);            } catch (InterruptedException ie) {                throw new IllegalStateException("Problem getting input split size", ie);            }        }    });}
0
public int compare(InputSplit a, InputSplit b)
{    try {        long left = a.getLength();        long right = b.getLength();        if (left == right) {            return 0;        } else if (left < right) {            return 1;        } else {            return -1;        }    } catch (IOException ie) {        throw new IllegalStateException("Problem getting input split size", ie);    } catch (InterruptedException ie) {        throw new IllegalStateException("Problem getting input split size", ie);    }}
0
public double[][] getResults()
{    return results;}
0
private void configureJob(Job job) throws IOException
{    job.setJarByClass(Classifier.class);    FileInputFormat.setInputPaths(job, inputPath);    FileOutputFormat.setOutputPath(job, mappersOutputPath);    job.setOutputKeyClass(DoubleWritable.class);    job.setOutputValueClass(Text.class);    job.setMapperClass(CMapper.class);        job.setNumReduceTasks(0);    job.setInputFormatClass(CTextInputFormat.class);    job.setOutputFormatClass(SequenceFileOutputFormat.class);}
0
public void run() throws IOException, ClassNotFoundException, InterruptedException
{    FileSystem fs = FileSystem.get(conf);        if (fs.exists(outputPath)) {        throw new IOException("Output path already exists : " + outputPath);    }            DistributedCache.addCacheFile(datasetPath.toUri(), conf);        DistributedCache.addCacheFile(forestPath.toUri(), conf);    Job job = new Job(conf, "decision forest classifier");        configureJob(job);        if (!job.waitForCompletion(true)) {        throw new IllegalStateException("Job failed!");    }    parseOutput(job);    HadoopUtil.delete(conf, mappersOutputPath);}
1
private void parseOutput(JobContext job) throws IOException
{    Configuration conf = job.getConfiguration();    FileSystem fs = mappersOutputPath.getFileSystem(conf);    Path[] outfiles = DFUtils.listOutputFiles(fs, mappersOutputPath);        List<double[]> resList = new ArrayList<>();    for (Path path : outfiles) {        FSDataOutputStream ofile = null;        try {            for (Pair<DoubleWritable, Text> record : new SequenceFileIterable<DoubleWritable, Text>(path, true, conf)) {                double key = record.getFirst().get();                String value = record.getSecond().toString();                if (ofile == null) {                                        ofile = fs.create(new Path(outputPath, value).suffix(".out"));                } else {                                                            ofile.writeChars(value);                    ofile.writeChar('\n');                    resList.add(new double[] { key, Double.valueOf(value) });                }            }        } finally {            Closeables.close(ofile, false);        }    }    results = new double[resList.size()][2];    resList.toArray(results);}
0
protected boolean isSplitable(JobContext jobContext, Path path)
{    return false;}
0
protected void setup(Context context) throws IOException, InterruptedException
{        super.setup(context);    Configuration conf = context.getConfiguration();    Path[] files = HadoopUtil.getCachedFiles(conf);    if (files.length < 2) {        throw new IOException("not enough paths in the DistributedCache");    }    dataset = Dataset.load(conf, files[0]);    converter = new DataConverter(dataset);    forest = DecisionForest.load(conf, files[1]);    if (forest == null) {        throw new InterruptedException("DecisionForest not found!");    }}
0
protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException
{    if (first) {        FileSplit split = (FileSplit) context.getInputSplit();                Path path = split.getPath();        lvalue.set(path.getName());        lkey.set(key.get());        context.write(lkey, lvalue);        first = false;    }    String line = value.toString();    if (!line.isEmpty()) {        Instance instance = converter.convert(line);        double prediction = forest.classify(dataset, rng, instance);        lkey.set(dataset.getLabel(instance));        lvalue.set(Double.toString(prediction));        context.write(lkey, lvalue);    }}
0
protected void configureJob(Job job) throws IOException
{    Configuration conf = job.getConfiguration();    job.setJarByClass(InMemBuilder.class);    FileOutputFormat.setOutputPath(job, getOutputPath(conf));        DistributedCache.addCacheFile(getDataPath().toUri(), conf);    job.setOutputKeyClass(IntWritable.class);    job.setOutputValueClass(MapredOutput.class);    job.setMapperClass(InMemMapper.class);        job.setNumReduceTasks(0);    job.setInputFormatClass(InMemInputFormat.class);    job.setOutputFormatClass(SequenceFileOutputFormat.class);}
0
protected DecisionForest parseOutput(Job job) throws IOException
{    Configuration conf = job.getConfiguration();    Map<Integer, MapredOutput> output = new HashMap<>();    Path outputPath = getOutputPath(conf);    FileSystem fs = outputPath.getFileSystem(conf);    Path[] outfiles = DFUtils.listOutputFiles(fs, outputPath);        for (Path path : outfiles) {        for (Pair<IntWritable, MapredOutput> record : new SequenceFileIterable<IntWritable, MapredOutput>(path, conf)) {            output.put(record.getFirst().get(), record.getSecond());        }    }    return processOutput(output);}
0
private static DecisionForest processOutput(Map<Integer, MapredOutput> output)
{    List<Node> trees = new ArrayList<>();    for (Map.Entry<Integer, MapredOutput> entry : output.entrySet()) {        MapredOutput value = entry.getValue();        trees.add(value.getTree());    }    return new DecisionForest(trees);}
0
private static boolean isSingleSeed(Configuration conf)
{    return conf.getBoolean("debug.mahout.rf.single.seed", false);}
0
public RecordReader<IntWritable, NullWritable> createRecordReader(InputSplit split, TaskAttemptContext context) throws IOException, InterruptedException
{    Preconditions.checkArgument(split instanceof InMemInputSplit);    return new InMemRecordReader((InMemInputSplit) split);}
0
public List<InputSplit> getSplits(JobContext context) throws IOException, InterruptedException
{    Configuration conf = context.getConfiguration();    int numSplits = conf.getInt("mapred.map.tasks", -1);    return getSplits(conf, numSplits);}
0
public List<InputSplit> getSplits(Configuration conf, int numSplits)
{    int nbTrees = Builder.getNbTrees(conf);    int splitSize = nbTrees / numSplits;    seed = Builder.getRandomSeed(conf);    isSingleSeed = isSingleSeed(conf);    if (rng != null && seed != null) {            }    rng = seed == null || isSingleSeed ? null : RandomUtils.getRandom(seed);    int id = 0;    List<InputSplit> splits = new ArrayList<>(numSplits);    for (int index = 0; index < numSplits - 1; index++) {        splits.add(new InMemInputSplit(id, splitSize, nextSeed()));        id += splitSize;    }        splits.add(new InMemInputSplit(id, nbTrees - id, nextSeed()));    return splits;}
1
private Long nextSeed()
{    if (seed == null) {        return null;    } else if (isSingleSeed) {        return seed;    } else {        return rng.nextLong();    }}
0
public float getProgress() throws IOException
{    return pos == 0 ? 0.0f : (float) (pos - 1) / split.nbTrees;}
0
public IntWritable getCurrentKey() throws IOException, InterruptedException
{    return key;}
0
public NullWritable getCurrentValue() throws IOException, InterruptedException
{    return value;}
0
public void initialize(InputSplit arg0, TaskAttemptContext arg1) throws IOException, InterruptedException
{    key = new IntWritable();    value = NullWritable.get();}
0
public boolean nextKeyValue() throws IOException, InterruptedException
{    if (pos < split.nbTrees) {        key.set(split.firstId + pos);        pos++;        return true;    } else {        return false;    }}
0
public void close() throws IOException
{}
0
public int getFirstId()
{    return firstId;}
0
public int getNbTrees()
{    return nbTrees;}
0
public Long getSeed()
{    return seed;}
0
public long getLength() throws IOException
{    return nbTrees;}
0
public String[] getLocations() throws IOException
{    return NO_LOCATIONS;}
0
public boolean equals(Object obj)
{    if (this == obj) {        return true;    }    if (!(obj instanceof InMemInputSplit)) {        return false;    }    InMemInputSplit split = (InMemInputSplit) obj;    if (firstId != split.firstId || nbTrees != split.nbTrees) {        return false;    }    if (seed == null) {        return split.seed == null;    } else {        return seed.equals(split.seed);    }}
0
public int hashCode()
{    return firstId + nbTrees + (seed == null ? 0 : seed.intValue());}
0
public String toString()
{    return String.format(Locale.ENGLISH, "[firstId:%d, nbTrees:%d, seed:%d]", firstId, nbTrees, seed);}
0
public void readFields(DataInput in) throws IOException
{    firstId = in.readInt();    nbTrees = in.readInt();    boolean isSeed = in.readBoolean();    seed = isSeed ? in.readLong() : null;}
0
public void write(DataOutput out) throws IOException
{    out.writeInt(firstId);    out.writeInt(nbTrees);    out.writeBoolean(seed != null);    if (seed != null) {        out.writeLong(seed);    }}
0
public static InMemInputSplit read(DataInput in) throws IOException
{    InMemInputSplit split = new InMemInputSplit();    split.readFields(in);    return split;}
0
private static Data loadData(Configuration conf, Dataset dataset) throws IOException
{    Path dataPath = Builder.getDistributedCacheFile(conf, 1);    FileSystem fs = FileSystem.get(dataPath.toUri(), conf);    return DataLoader.loadData(dataset, fs, dataPath);}
0
protected void setup(Context context) throws IOException, InterruptedException
{    super.setup(context);    Configuration conf = context.getConfiguration();        Data data = loadData(conf, getDataset());        bagging = new Bagging(getTreeBuilder(), data);}
1
protected void map(IntWritable key, NullWritable value, Context context) throws IOException, InterruptedException
{    map(key, context);}
0
 void map(IntWritable key, Context context) throws IOException, InterruptedException
{    initRandom((InMemInputSplit) context.getInputSplit());        Node tree = bagging.build(rng);    if (isOutput()) {                MapredOutput mrOut = new MapredOutput(tree);        context.write(key, mrOut);    }}
1
 void initRandom(InMemInputSplit split)
{    if (rng == null) {                Long seed = split.getSeed();                rng = seed == null ? RandomUtils.getRandom() : RandomUtils.getRandom(seed);    }}
1
protected boolean isOutput()
{    return !noOutput;}
0
protected TreeBuilder getTreeBuilder()
{    return treeBuilder;}
0
protected Dataset getDataset()
{    return dataset;}
0
protected void setup(Context context) throws IOException, InterruptedException
{    super.setup(context);    Configuration conf = context.getConfiguration();    configure(!Builder.isOutput(conf), Builder.getTreeBuilder(conf), Builder.loadDataset(conf));}
0
protected void configure(boolean noOutput, TreeBuilder treeBuilder, Dataset dataset)
{    Preconditions.checkArgument(treeBuilder != null, "TreeBuilder not found in the Job parameters");    this.noOutput = noOutput;    this.treeBuilder = treeBuilder;    this.dataset = dataset;}
0
public Node getTree()
{    return tree;}
0
 int[] getPredictions()
{    return predictions;}
0
public void readFields(DataInput in) throws IOException
{    boolean readTree = in.readBoolean();    if (readTree) {        tree = Node.read(in);    }    boolean readPredictions = in.readBoolean();    if (readPredictions) {        predictions = DFUtils.readIntArray(in);    }}
0
public void write(DataOutput out) throws IOException
{    out.writeBoolean(tree != null);    if (tree != null) {        tree.write(out);    }    out.writeBoolean(predictions != null);    if (predictions != null) {        DFUtils.writeArray(out, predictions);    }}
0
public MapredOutput clone()
{    return new MapredOutput(tree, predictions);}
0
public boolean equals(Object obj)
{    if (this == obj) {        return true;    }    if (!(obj instanceof MapredOutput)) {        return false;    }    MapredOutput mo = (MapredOutput) obj;    return ((tree == null && mo.getTree() == null) || (tree != null && tree.equals(mo.getTree()))) && Arrays.equals(predictions, mo.getPredictions());}
0
public int hashCode()
{    int hashCode = tree == null ? 1 : tree.hashCode();    for (int prediction : predictions) {        hashCode = 31 * hashCode + prediction;    }    return hashCode;}
0
public String toString()
{    return "{" + tree + " | " + Arrays.toString(predictions) + '}';}
0
protected void configureJob(Job job) throws IOException
{    Configuration conf = job.getConfiguration();    job.setJarByClass(PartialBuilder.class);    FileInputFormat.setInputPaths(job, getDataPath());    FileOutputFormat.setOutputPath(job, getOutputPath(conf));    job.setOutputKeyClass(TreeID.class);    job.setOutputValueClass(MapredOutput.class);    job.setMapperClass(Step1Mapper.class);        job.setNumReduceTasks(0);    job.setInputFormatClass(TextInputFormat.class);    job.setOutputFormatClass(SequenceFileOutputFormat.class);            TextInputFormat inputFormat = new TextInputFormat();    List<?> splits = inputFormat.getSplits(job);    if (splits == null || splits.isEmpty()) {            } else {        int numSplits = splits.size();                conf.setInt("mapred.map.tasks", numSplits);    }}
1
protected DecisionForest parseOutput(Job job) throws IOException
{    Configuration conf = job.getConfiguration();    int numTrees = Builder.getNbTrees(conf);    Path outputPath = getOutputPath(conf);    TreeID[] keys = new TreeID[numTrees];    Node[] trees = new Node[numTrees];    processOutput(job, outputPath, keys, trees);    return new DecisionForest(Arrays.asList(trees));}
0
protected static void processOutput(JobContext job, Path outputPath, TreeID[] keys, Node[] trees) throws IOException
{    Preconditions.checkArgument(keys == null && trees == null || keys != null && trees != null, "if keys is null, trees should also be null");    Preconditions.checkArgument(keys == null || keys.length == trees.length, "keys.length != trees.length");    Configuration conf = job.getConfiguration();    FileSystem fs = outputPath.getFileSystem(conf);    Path[] outfiles = DFUtils.listOutputFiles(fs, outputPath);        int index = 0;    for (Path path : outfiles) {        for (Pair<TreeID, MapredOutput> record : new SequenceFileIterable<TreeID, MapredOutput>(path, conf)) {            TreeID key = record.getFirst();            MapredOutput value = record.getSecond();            if (keys != null) {                keys[index] = key;            }            if (trees != null) {                trees[index] = value.getTree();            }            index++;        }    }        if (keys != null && index != keys.length) {        throw new IllegalStateException("Some key/values are missing from the output");    }}
0
public int getFirstTreeId()
{    return firstTreeId;}
0
protected void setup(Context context) throws IOException, InterruptedException
{    super.setup(context);    Configuration conf = context.getConfiguration();    configure(Builder.getRandomSeed(conf), conf.getInt("mapred.task.partition", -1), Builder.getNumMaps(conf), Builder.getNbTrees(conf));}
0
protected void configure(Long seed, int partition, int numMapTasks, int numTrees)
{    converter = new DataConverter(getDataset());            if (seed == null) {        rng = RandomUtils.getRandom();    } else {        rng = RandomUtils.getRandom(seed);    }        Preconditions.checkArgument(partition >= 0, "Wrong partition ID: " + partition + ". Partition must be >= 0!");    this.partition = partition;        nbTrees = nbTrees(numMapTasks, numTrees, partition);        firstTreeId = 0;    for (int p = 0; p < partition; p++) {        firstTreeId += nbTrees(numMapTasks, numTrees, p);    }            }
1
public static int nbTrees(int numMaps, int numTrees, int partition)
{    int treesPerMapper = numTrees / numMaps;    int remainder = numTrees - numMaps * treesPerMapper;    return treesPerMapper + (partition < remainder ? 1 : 0);}
0
protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException
{    instances.add(converter.convert(value.toString()));}
0
protected void cleanup(Context context) throws IOException, InterruptedException
{            Data data = new Data(getDataset(), instances);    Bagging bagging = new Bagging(getTreeBuilder(), data);    TreeID key = new TreeID();        for (int treeId = 0; treeId < nbTrees; treeId++) {                Node tree = bagging.build(rng);        key.set(partition, firstTreeId + treeId);        if (isOutput()) {            MapredOutput emOut = new MapredOutput(tree);            context.write(key, emOut);        }        context.progress();    }}
1
public void set(int partition, int treeId)
{    set((long) partition * MAX_TREEID + treeId);}
0
public int partition()
{    return (int) (get() / MAX_TREEID);}
0
public int treeId()
{    return (int) (get() % MAX_TREEID);}
0
public TreeID clone()
{    return new TreeID(partition(), treeId());}
0
public double classify(Instance instance)
{    int index = ArrayUtils.indexOf(values, instance.get(attr));    if (index == -1) {                return Double.NaN;    }    return childs[index].classify(instance);}
0
public long maxDepth()
{    long max = 0;    for (Node child : childs) {        long depth = child.maxDepth();        if (depth > max) {            max = depth;        }    }    return 1 + max;}
0
public long nbNodes()
{    long nbNodes = 1;    for (Node child : childs) {        nbNodes += child.nbNodes();    }    return nbNodes;}
0
protected Type getType()
{    return Type.CATEGORICAL;}
0
public boolean equals(Object obj)
{    if (this == obj) {        return true;    }    if (!(obj instanceof CategoricalNode)) {        return false;    }    CategoricalNode node = (CategoricalNode) obj;    return attr == node.attr && Arrays.equals(values, node.values) && Arrays.equals(childs, node.childs);}
0
public int hashCode()
{    int hashCode = attr;    for (double value : values) {        hashCode = 31 * hashCode + (int) Double.doubleToLongBits(value);    }    for (Node node : childs) {        hashCode = 31 * hashCode + node.hashCode();    }    return hashCode;}
0
protected String getString()
{    StringBuilder buffer = new StringBuilder();    for (Node child : childs) {        buffer.append(child).append(',');    }    return buffer.toString();}
0
public void readFields(DataInput in) throws IOException
{    attr = in.readInt();    values = DFUtils.readDoubleArray(in);    childs = DFUtils.readNodeArray(in);}
0
protected void writeNode(DataOutput out) throws IOException
{    out.writeInt(attr);    DFUtils.writeArray(out, values);    DFUtils.writeArray(out, childs);}
0
public double classify(Instance instance)
{    return label;}
0
public long maxDepth()
{    return 1;}
0
public long nbNodes()
{    return 1;}
0
protected Type getType()
{    return Type.LEAF;}
0
public boolean equals(Object obj)
{    if (this == obj) {        return true;    }    if (!(obj instanceof Leaf)) {        return false;    }    Leaf leaf = (Leaf) obj;    return Math.abs(label - leaf.label) < EPSILON;}
0
public int hashCode()
{    long bits = Double.doubleToLongBits(label);    return (int) (bits ^ (bits >>> 32));}
0
protected String getString()
{    return "";}
0
public void readFields(DataInput in) throws IOException
{    label = in.readDouble();}
0
protected void writeNode(DataOutput out) throws IOException
{    out.writeDouble(label);}
0
public static Node read(DataInput in) throws IOException
{    Type type = Type.values()[in.readInt()];    Node node;    switch(type) {        case LEAF:            node = new Leaf();            break;        case NUMERICAL:            node = new NumericalNode();            break;        case CATEGORICAL:            node = new CategoricalNode();            break;        default:            throw new IllegalStateException("This implementation is not currently supported");    }    node.readFields(in);    return node;}
0
public final String toString()
{    return getType() + ":" + getString() + ';';}
0
public final void write(DataOutput out) throws IOException
{    out.writeInt(getType().ordinal());    writeNode(out);}
0
public double classify(Instance instance)
{    if (instance.get(attr) < split) {        return loChild.classify(instance);    } else {        return hiChild.classify(instance);    }}
0
public long maxDepth()
{    return 1 + Math.max(loChild.maxDepth(), hiChild.maxDepth());}
0
public long nbNodes()
{    return 1 + loChild.nbNodes() + hiChild.nbNodes();}
0
protected Type getType()
{    return Type.NUMERICAL;}
0
public boolean equals(Object obj)
{    if (this == obj) {        return true;    }    if (!(obj instanceof NumericalNode)) {        return false;    }    NumericalNode node = (NumericalNode) obj;    return attr == node.attr && split == node.split && loChild.equals(node.loChild) && hiChild.equals(node.hiChild);}
0
public int hashCode()
{    return attr + (int) Double.doubleToLongBits(split) + loChild.hashCode() + hiChild.hashCode();}
0
protected String getString()
{    return loChild.toString() + ',' + hiChild.toString();}
0
public void readFields(DataInput in) throws IOException
{    attr = in.readInt();    split = in.readDouble();    loChild = Node.read(in);    hiChild = Node.read(in);}
0
protected void writeNode(DataOutput out) throws IOException
{    out.writeInt(attr);    out.writeDouble(split);    loChild.write(out);    hiChild.write(out);}
0
public DecisionForest build(int nbTrees)
{    List<Node> trees = new ArrayList<>();    for (int treeId = 0; treeId < nbTrees; treeId++) {        trees.add(bagging.build(rng));        logProgress(((float) treeId + 1) / nbTrees);    }    return new DecisionForest(trees);}
0
private static void logProgress(float progress)
{    int percent = (int) (progress * 100);    if (percent % 10 == 0) {            }}
1
public Split computeSplit(Data data, int attr)
{    if (data.getDataset().isNumerical(attr)) {        double[] values = data.values(attr);        double bestIg = -1;        double bestSplit = 0.0;        for (double value : values) {            double ig = numericalIg(data, attr, value);            if (ig > bestIg) {                bestIg = ig;                bestSplit = value;            }        }        return new Split(attr, bestIg, bestSplit);    } else {        double ig = categoricalIg(data, attr);        return new Split(attr, ig);    }}
0
 double categoricalIg(Data data, int attr)
{    double[] values = data.values(attr);        double hy = entropy(data);        double hyx = 0.0;    double invDataSize = 1.0 / data.size();    for (double value : values) {        Data subset = data.subset(Condition.equals(attr, value));        hyx += subset.size() * invDataSize * entropy(subset);    }    return hy - hyx;}
0
 double numericalIg(Data data, int attr, double split)
{    double hy = entropy(data);    double invDataSize = 1.0 / data.size();        Data subset = data.subset(Condition.lesser(attr, split));    hy -= subset.size() * invDataSize * entropy(subset);        subset = data.subset(Condition.greaterOrEquals(attr, split));    hy -= subset.size() * invDataSize * entropy(subset);    return hy;}
0
protected double entropy(Data data)
{    double invDataSize = 1.0 / data.size();    if (counts == null) {        counts = new int[data.getDataset().nblabels()];    }    Arrays.fill(counts, 0);    data.countLabels(counts);    double entropy = 0.0;    for (int label = 0; label < data.getDataset().nblabels(); label++) {        int count = counts[label];        if (count == 0) {                        continue;        }        double p = count * invDataSize;        entropy += -p * Math.log(p) / LOG2;    }    return entropy;}
0
public Split computeSplit(Data data, int attr)
{    if (data.getDataset().isNumerical(attr)) {        return numericalSplit(data, attr);    } else {        return categoricalSplit(data, attr);    }}
0
private static Split categoricalSplit(Data data, int attr)
{    double[] values = data.values(attr).clone();    double[] splitPoints = chooseCategoricalSplitPoints(values);    int numLabels = data.getDataset().nblabels();    int[][] counts = new int[splitPoints.length][numLabels];    int[] countAll = new int[numLabels];    computeFrequencies(data, attr, splitPoints, counts, countAll);    int size = data.size();        double hy = entropy(countAll, size);        double hyx = 0.0;    double invDataSize = 1.0 / size;    for (int index = 0; index < splitPoints.length; index++) {        size = DataUtils.sum(counts[index]);        hyx += size * invDataSize * entropy(counts[index], size);    }    double ig = hy - hyx;    return new Split(attr, ig);}
0
 static void computeFrequencies(Data data, int attr, double[] splitPoints, int[][] counts, int[] countAll)
{    Dataset dataset = data.getDataset();    for (int index = 0; index < data.size(); index++) {        Instance instance = data.get(index);        int label = (int) dataset.getLabel(instance);        double value = instance.get(attr);        int split = 0;        while (split < splitPoints.length && value > splitPoints[split]) {            split++;        }        if (split < splitPoints.length) {            counts[split][label]++;        }                countAll[label]++;    }}
0
 static Split numericalSplit(Data data, int attr)
{    double[] values = data.values(attr).clone();    Arrays.sort(values);    double[] splitPoints = chooseNumericSplitPoints(values);    int numLabels = data.getDataset().nblabels();    int[][] counts = new int[splitPoints.length][numLabels];    int[] countAll = new int[numLabels];    int[] countLess = new int[numLabels];    computeFrequencies(data, attr, splitPoints, counts, countAll);    int size = data.size();    double hy = entropy(countAll, size);    double invDataSize = 1.0 / size;    int best = -1;    double bestIg = -1.0;        for (int index = 0; index < splitPoints.length; index++) {        double ig = hy;        DataUtils.add(countLess, counts[index]);        DataUtils.dec(countAll, counts[index]);                size = DataUtils.sum(countLess);        ig -= size * invDataSize * entropy(countLess, size);                size = DataUtils.sum(countAll);        ig -= size * invDataSize * entropy(countAll, size);        if (ig > bestIg) {            bestIg = ig;            best = index;        }    }    if (best == -1) {        throw new IllegalStateException("no best split found !");    }    return new Split(attr, bestIg, splitPoints[best]);}
0
private static double[] chooseNumericSplitPoints(double[] values)
{    if (values.length <= 1) {        return values;    }    if (values.length <= MAX_NUMERIC_SPLITS + 1) {        double[] splitPoints = new double[values.length - 1];        for (int i = 1; i < values.length; i++) {            splitPoints[i - 1] = (values[i] + values[i - 1]) / 2.0;        }        return splitPoints;    }    Percentile distribution = new Percentile();    distribution.setData(values);    double[] percentiles = new double[MAX_NUMERIC_SPLITS];    for (int i = 0; i < percentiles.length; i++) {        double p = 100.0 * ((i + 1.0) / (MAX_NUMERIC_SPLITS + 1.0));        percentiles[i] = distribution.evaluate(p);    }    return percentiles;}
0
private static double[] chooseCategoricalSplitPoints(double[] values)
{                Collection<Double> uniqueOrderedCategories = new TreeSet<>();    for (double v : values) {        uniqueOrderedCategories.add(v);    }    double[] uniqueValues = new double[uniqueOrderedCategories.size()];    Iterator<Double> it = uniqueOrderedCategories.iterator();    for (int i = 0; i < uniqueValues.length; i++) {        uniqueValues[i] = it.next();    }    return uniqueValues;}
0
private static double entropy(int[] counts, int dataSize)
{    if (dataSize == 0) {        return 0.0;    }    double entropy = 0.0;    for (int count : counts) {        if (count > 0) {            double p = count / (double) dataSize;            entropy -= p * Math.log(p);        }    }    return entropy / LOG2;}
0
public int compare(Instance arg0, Instance arg1)
{    return Double.compare(arg0.get(attr), arg1.get(attr));}
0
public Split computeSplit(Data data, int attr)
{    if (data.getDataset().isNumerical(attr)) {        return numericalSplit(data, attr);    } else {        return categoricalSplit(data, attr);    }}
0
private static Split categoricalSplit(Data data, int attr)
{    FullRunningAverage[] ra = new FullRunningAverage[data.getDataset().nbValues(attr)];    double[] sk = new double[data.getDataset().nbValues(attr)];    for (int i = 0; i < ra.length; i++) {        ra[i] = new FullRunningAverage();    }    FullRunningAverage totalRa = new FullRunningAverage();    double totalSk = 0.0;    for (int i = 0; i < data.size(); i++) {                Instance instance = data.get(i);        int value = (int) instance.get(attr);        double xk = data.getDataset().getLabel(instance);        if (ra[value].getCount() == 0) {            ra[value].addDatum(xk);            sk[value] = 0.0;        } else {            double mk = ra[value].getAverage();            ra[value].addDatum(xk);            sk[value] += (xk - mk) * (xk - ra[value].getAverage());        }                if (i == 0) {            totalRa.addDatum(xk);            totalSk = 0.0;        } else {            double mk = totalRa.getAverage();            totalRa.addDatum(xk);            totalSk += (xk - mk) * (xk - totalRa.getAverage());        }    }        double ig = totalSk;    for (double aSk : sk) {        ig -= aSk;    }    return new Split(attr, ig);}
0
private static Split numericalSplit(Data data, int attr)
{    FullRunningAverage[] ra = new FullRunningAverage[2];    for (int i = 0; i < ra.length; i++) {        ra[i] = new FullRunningAverage();    }        Instance[] instances = new Instance[data.size()];    for (int i = 0; i < data.size(); i++) {        instances[i] = data.get(i);    }    Arrays.sort(instances, new InstanceComparator(attr));    double[] sk = new double[2];    for (Instance instance : instances) {        double xk = data.getDataset().getLabel(instance);        if (ra[1].getCount() == 0) {            ra[1].addDatum(xk);            sk[1] = 0.0;        } else {            double mk = ra[1].getAverage();            ra[1].addDatum(xk);            sk[1] += (xk - mk) * (xk - ra[1].getAverage());        }    }    double totalSk = sk[1];        double split = Double.NaN;    double preSplit = Double.NaN;    double bestVal = Double.MAX_VALUE;    double bestSk = 0.0;        for (Instance instance : instances) {        double xk = data.getDataset().getLabel(instance);        if (instance.get(attr) > preSplit) {            double curVal = sk[0] / ra[0].getCount() + sk[1] / ra[1].getCount();            if (curVal < bestVal) {                bestVal = curVal;                bestSk = sk[0] + sk[1];                split = (instance.get(attr) + preSplit) / 2.0;            }        }                if (ra[0].getCount() == 0) {            ra[0].addDatum(xk);            sk[0] = 0.0;        } else {            double mk = ra[0].getAverage();            ra[0].addDatum(xk);            sk[0] += (xk - mk) * (xk - ra[0].getAverage());        }        double mk = ra[1].getAverage();        ra[1].removeDatum(xk);        sk[1] -= (xk - mk) * (xk - ra[1].getAverage());        preSplit = instance.get(attr);    }        double ig = totalSk - bestSk;    return new Split(attr, ig, split);}
0
public int getAttr()
{    return attr;}
0
public double getIg()
{    return ig;}
0
public double getSplit()
{    return split;}
0
public String toString()
{    return String.format(Locale.ENGLISH, "attr: %d, ig: %f, split: %f", attr, ig, split);}
0
public static int main(String[] args) throws Exception
{    return ToolRunner.run(new Describe(), args);}
0
public int run(String[] args) throws Exception
{    DefaultOptionBuilder obuilder = new DefaultOptionBuilder();    ArgumentBuilder abuilder = new ArgumentBuilder();    GroupBuilder gbuilder = new GroupBuilder();    Option pathOpt = obuilder.withLongName("path").withShortName("p").withRequired(true).withArgument(abuilder.withName("path").withMinimum(1).withMaximum(1).create()).withDescription("Data path").create();    Option descriptorOpt = obuilder.withLongName("descriptor").withShortName("d").withRequired(true).withArgument(abuilder.withName("descriptor").withMinimum(1).create()).withDescription("data descriptor").create();    Option descPathOpt = obuilder.withLongName("file").withShortName("f").withRequired(true).withArgument(abuilder.withName("file").withMinimum(1).withMaximum(1).create()).withDescription("Path to generated descriptor file").create();    Option regOpt = obuilder.withLongName("regression").withDescription("Regression Problem").withShortName("r").create();    Option helpOpt = obuilder.withLongName("help").withDescription("Print out help").withShortName("h").create();    Group group = gbuilder.withName("Options").withOption(pathOpt).withOption(descPathOpt).withOption(descriptorOpt).withOption(regOpt).withOption(helpOpt).create();    try {        Parser parser = new Parser();        parser.setGroup(group);        CommandLine cmdLine = parser.parse(args);        if (cmdLine.hasOption(helpOpt)) {            CommandLineUtil.printHelp(group);            return -1;        }        String dataPath = cmdLine.getValue(pathOpt).toString();        String descPath = cmdLine.getValue(descPathOpt).toString();        List<String> descriptor = convert(cmdLine.getValues(descriptorOpt));        boolean regression = cmdLine.hasOption(regOpt);                                        runTool(dataPath, descriptor, descPath, regression);    } catch (OptionException e) {                CommandLineUtil.printHelp(group);    }    return 0;}
1
private void runTool(String dataPath, Iterable<String> description, String filePath, boolean regression) throws DescriptorException, IOException
{        String descriptor = DescriptorUtils.generateDescriptor(description);    Path fPath = validateOutput(filePath);        Dataset dataset = generateDataset(descriptor, dataPath, regression);        String json = dataset.toJSON();    DFUtils.storeString(conf, fPath, json);}
1
private Dataset generateDataset(String descriptor, String dataPath, boolean regression) throws IOException, DescriptorException
{    Path path = new Path(dataPath);    FileSystem fs = path.getFileSystem(conf);    return DataLoader.generateDataset(descriptor, regression, fs, path);}
0
private Path validateOutput(String filePath) throws IOException
{    Path path = new Path(filePath);    FileSystem fs = path.getFileSystem(conf);    if (fs.exists(path)) {        throw new IllegalStateException("Descriptor's file already exists");    }    return path;}
0
private static List<String> convert(Collection<?> values)
{    List<String> list = new ArrayList<>(values.size());    for (Object value : values) {        list.add(value.toString());    }    return list;}
0
public void setConf(Configuration entries)
{    this.conf = entries;}
0
public Configuration getConf()
{    return conf;}
0
public static String toString(DecisionForest forest, Dataset dataset, String[] attrNames)
{    List<Node> trees;    try {        Method getTrees = forest.getClass().getDeclaredMethod("getTrees");        getTrees.setAccessible(true);        trees = (List<Node>) getTrees.invoke(forest);    } catch (IllegalAccessException e) {        throw new IllegalStateException(e);    } catch (InvocationTargetException e) {        throw new IllegalStateException(e);    } catch (NoSuchMethodException e) {        throw new IllegalStateException(e);    }    int cnt = 1;    StringBuilder buff = new StringBuilder();    for (Node tree : trees) {        buff.append("Tree[").append(cnt).append("]:");        buff.append(TreeVisualizer.toString(tree, dataset, attrNames));        buff.append('\n');        cnt++;    }    return buff.toString();}
0
public static String toString(String forestPath, String datasetPath, String[] attrNames) throws IOException
{    Configuration conf = new Configuration();    DecisionForest forest = DecisionForest.load(conf, new Path(forestPath));    Dataset dataset = Dataset.load(conf, new Path(datasetPath));    return toString(forest, dataset, attrNames);}
0
public static void print(String forestPath, String datasetPath, String[] attrNames) throws IOException
{    System.out.println(toString(forestPath, datasetPath, attrNames));}
0
public static void main(String[] args)
{    DefaultOptionBuilder obuilder = new DefaultOptionBuilder();    ArgumentBuilder abuilder = new ArgumentBuilder();    GroupBuilder gbuilder = new GroupBuilder();    Option datasetOpt = obuilder.withLongName("dataset").withShortName("ds").withRequired(true).withArgument(abuilder.withName("dataset").withMinimum(1).withMaximum(1).create()).withDescription("Dataset path").create();    Option modelOpt = obuilder.withLongName("model").withShortName("m").withRequired(true).withArgument(abuilder.withName("path").withMinimum(1).withMaximum(1).create()).withDescription("Path to the Decision Forest").create();    Option attrNamesOpt = obuilder.withLongName("names").withShortName("n").withRequired(false).withArgument(abuilder.withName("names").withMinimum(1).create()).withDescription("Optional, Attribute names").create();    Option helpOpt = obuilder.withLongName("help").withShortName("h").withDescription("Print out help").create();    Group group = gbuilder.withName("Options").withOption(datasetOpt).withOption(modelOpt).withOption(attrNamesOpt).withOption(helpOpt).create();    try {        Parser parser = new Parser();        parser.setGroup(group);        CommandLine cmdLine = parser.parse(args);        if (cmdLine.hasOption("help")) {            CommandLineUtil.printHelp(group);            return;        }        String datasetName = cmdLine.getValue(datasetOpt).toString();        String modelName = cmdLine.getValue(modelOpt).toString();        String[] attrNames = null;        if (cmdLine.hasOption(attrNamesOpt)) {            Collection<String> names = (Collection<String>) cmdLine.getValues(attrNamesOpt);            if (!names.isEmpty()) {                attrNames = new String[names.size()];                names.toArray(attrNames);            }        }        print(modelName, datasetName, attrNames);    } catch (Exception e) {                CommandLineUtil.printHelp(group);    }}
1
public int run(String[] args) throws IOException, ClassNotFoundException, InterruptedException
{    DefaultOptionBuilder obuilder = new DefaultOptionBuilder();    ArgumentBuilder abuilder = new ArgumentBuilder();    GroupBuilder gbuilder = new GroupBuilder();    Option dataOpt = obuilder.withLongName("data").withShortName("d").withRequired(true).withArgument(abuilder.withName("path").withMinimum(1).withMaximum(1).create()).withDescription("Data path").create();    Option datasetOpt = obuilder.withLongName("dataset").withShortName("ds").withRequired(true).withArgument(abuilder.withName("path").withMinimum(1).create()).withDescription("dataset path").create();    Option helpOpt = obuilder.withLongName("help").withDescription("Print out help").withShortName("h").create();    Group group = gbuilder.withName("Options").withOption(dataOpt).withOption(datasetOpt).withOption(helpOpt).create();    try {        Parser parser = new Parser();        parser.setGroup(group);        CommandLine cmdLine = parser.parse(args);        if (cmdLine.hasOption(helpOpt)) {            CommandLineUtil.printHelp(group);            return 0;        }        String dataPath = cmdLine.getValue(dataOpt).toString();        String datasetPath = cmdLine.getValue(datasetOpt).toString();                        runTool(dataPath, datasetPath);    } catch (OptionException e) {                CommandLineUtil.printHelp(group);    }    return 0;}
1
private void runTool(String data, String dataset) throws IOException, ClassNotFoundException, InterruptedException
{    FileSystem fs = FileSystem.get(getConf());    Path workingDir = fs.getWorkingDirectory();    Path dataPath = new Path(data);    Path datasetPath = new Path(dataset);        FrequenciesJob job = new FrequenciesJob(new Path(workingDir, "output"), dataPath, datasetPath);    int[][] counts = job.run(getConf());            for (int[] count : counts) {            }}
1
public static void main(String[] args) throws Exception
{    ToolRunner.run(new Configuration(), new Frequencies(), args);}
0
public int[][] run(Configuration conf) throws IOException, ClassNotFoundException, InterruptedException
{        FileSystem fs = outputPath.getFileSystem(conf);    if (fs.exists(outputPath)) {        throw new IOException("Output path already exists : " + outputPath);    }        URI[] files = { datasetPath.toUri() };    DistributedCache.setCacheFiles(files, conf);    Job job = new Job(conf);    job.setJarByClass(FrequenciesJob.class);    FileInputFormat.setInputPaths(job, dataPath);    FileOutputFormat.setOutputPath(job, outputPath);    job.setMapOutputKeyClass(LongWritable.class);    job.setMapOutputValueClass(IntWritable.class);    job.setOutputKeyClass(LongWritable.class);    job.setOutputValueClass(Frequencies.class);    job.setMapperClass(FrequenciesMapper.class);    job.setReducerClass(FrequenciesReducer.class);    job.setInputFormatClass(TextInputFormat.class);    job.setOutputFormatClass(SequenceFileOutputFormat.class);        boolean succeeded = job.waitForCompletion(true);    if (!succeeded) {        throw new IllegalStateException("Job failed!");    }    int[][] counts = parseOutput(job);    HadoopUtil.delete(conf, outputPath);    return counts;}
0
 int[][] parseOutput(JobContext job) throws IOException
{    Configuration conf = job.getConfiguration();    int numMaps = conf.getInt("mapred.map.tasks", -1);        FileSystem fs = outputPath.getFileSystem(conf);    Path[] outfiles = DFUtils.listOutputFiles(fs, outputPath);    Frequencies[] values = new Frequencies[numMaps];        int index = 0;    for (Path path : outfiles) {        for (Frequencies value : new SequenceFileValueIterable<Frequencies>(path, conf)) {            values[index++] = value;        }    }    if (index < numMaps) {        throw new IllegalStateException("number of output Frequencies (" + index + ") is lesser than the number of mappers!");    }        Arrays.sort(values);    return Frequencies.extractCounts(values);}
1
protected void setup(Context context) throws IOException, InterruptedException
{    Configuration conf = context.getConfiguration();    dataset = Builder.loadDataset(conf);    setup(dataset);}
0
 void setup(Dataset dataset)
{    converter = new DataConverter(dataset);}
0
protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException
{    if (firstId == null) {        firstId = new LongWritable(key.get());    }    Instance instance = converter.convert(value.toString());    context.write(firstId, new IntWritable((int) dataset.getLabel(instance)));}
0
protected void setup(Context context) throws IOException, InterruptedException
{    Configuration conf = context.getConfiguration();    Dataset dataset = Builder.loadDataset(conf);    setup(dataset.nblabels());}
0
 void setup(int nblabels)
{    this.nblabels = nblabels;}
0
protected void reduce(LongWritable key, Iterable<IntWritable> values, Context context) throws IOException, InterruptedException
{    int[] counts = new int[nblabels];    for (IntWritable value : values) {        counts[value.get()]++;    }    context.write(key, new Frequencies(key.get(), counts));}
0
public void readFields(DataInput in) throws IOException
{    firstId = in.readLong();    counts = DFUtils.readIntArray(in);}
0
public void write(DataOutput out) throws IOException
{    out.writeLong(firstId);    DFUtils.writeArray(out, counts);}
0
public boolean equals(Object other)
{    return other instanceof Frequencies && firstId == ((Frequencies) other).firstId;}
0
public int hashCode()
{    return (int) firstId;}
0
protected Frequencies clone()
{    return new Frequencies(firstId, counts);}
0
public int compareTo(Frequencies obj)
{    if (firstId < obj.firstId) {        return -1;    } else if (firstId > obj.firstId) {        return 1;    } else {        return 0;    }}
0
public static int[][] extractCounts(Frequencies[] partitions)
{    int[][] counts = new int[partitions.length][];    for (int p = 0; p < partitions.length; p++) {        counts[p] = partitions[p].counts;    }    return counts;}
0
private static String doubleToString(double value)
{    DecimalFormat df = new DecimalFormat("0.##");    return df.format(value);}
0
private static String toStringNode(Node node, Dataset dataset, String[] attrNames, Map<String, Field> fields, int layer)
{    StringBuilder buff = new StringBuilder();    try {        if (node instanceof CategoricalNode) {            CategoricalNode cnode = (CategoricalNode) node;            int attr = (Integer) fields.get("CategoricalNode.attr").get(cnode);            double[] values = (double[]) fields.get("CategoricalNode.values").get(cnode);            Node[] childs = (Node[]) fields.get("CategoricalNode.childs").get(cnode);            String[][] attrValues = (String[][]) fields.get("Dataset.values").get(dataset);            for (int i = 0; i < attrValues[attr].length; i++) {                int index = ArrayUtils.indexOf(values, i);                if (index < 0) {                    continue;                }                buff.append('\n');                for (int j = 0; j < layer; j++) {                    buff.append("|   ");                }                buff.append(attrNames == null ? attr : attrNames[attr]).append(" = ").append(attrValues[attr][i]);                buff.append(toStringNode(childs[index], dataset, attrNames, fields, layer + 1));            }        } else if (node instanceof NumericalNode) {            NumericalNode nnode = (NumericalNode) node;            int attr = (Integer) fields.get("NumericalNode.attr").get(nnode);            double split = (Double) fields.get("NumericalNode.split").get(nnode);            Node loChild = (Node) fields.get("NumericalNode.loChild").get(nnode);            Node hiChild = (Node) fields.get("NumericalNode.hiChild").get(nnode);            buff.append('\n');            for (int j = 0; j < layer; j++) {                buff.append("|   ");            }            buff.append(attrNames == null ? attr : attrNames[attr]).append(" < ").append(doubleToString(split));            buff.append(toStringNode(loChild, dataset, attrNames, fields, layer + 1));            buff.append('\n');            for (int j = 0; j < layer; j++) {                buff.append("|   ");            }            buff.append(attrNames == null ? attr : attrNames[attr]).append(" >= ").append(doubleToString(split));            buff.append(toStringNode(hiChild, dataset, attrNames, fields, layer + 1));        } else if (node instanceof Leaf) {            Leaf leaf = (Leaf) node;            double label = (Double) fields.get("Leaf.label").get(leaf);            if (dataset.isNumerical(dataset.getLabelId())) {                buff.append(" : ").append(doubleToString(label));            } else {                buff.append(" : ").append(dataset.getLabelString(label));            }        }    } catch (IllegalAccessException iae) {        throw new IllegalStateException(iae);    }    return buff.toString();}
0
private static Map<String, Field> getReflectMap()
{    Map<String, Field> fields = new HashMap<>();    try {        Field m = CategoricalNode.class.getDeclaredField("attr");        m.setAccessible(true);        fields.put("CategoricalNode.attr", m);        m = CategoricalNode.class.getDeclaredField("values");        m.setAccessible(true);        fields.put("CategoricalNode.values", m);        m = CategoricalNode.class.getDeclaredField("childs");        m.setAccessible(true);        fields.put("CategoricalNode.childs", m);        m = NumericalNode.class.getDeclaredField("attr");        m.setAccessible(true);        fields.put("NumericalNode.attr", m);        m = NumericalNode.class.getDeclaredField("split");        m.setAccessible(true);        fields.put("NumericalNode.split", m);        m = NumericalNode.class.getDeclaredField("loChild");        m.setAccessible(true);        fields.put("NumericalNode.loChild", m);        m = NumericalNode.class.getDeclaredField("hiChild");        m.setAccessible(true);        fields.put("NumericalNode.hiChild", m);        m = Leaf.class.getDeclaredField("label");        m.setAccessible(true);        fields.put("Leaf.label", m);        m = Dataset.class.getDeclaredField("values");        m.setAccessible(true);        fields.put("Dataset.values", m);    } catch (NoSuchFieldException nsfe) {        throw new IllegalStateException(nsfe);    }    return fields;}
0
public static String toString(Node tree, Dataset dataset, String[] attrNames)
{    return toStringNode(tree, dataset, attrNames, getReflectMap(), 0);}
0
public static void print(Node tree, Dataset dataset, String[] attrNames)
{    System.out.println(toString(tree, dataset, attrNames));}
0
private static String toStringPredict(Node node, Instance instance, Dataset dataset, String[] attrNames, Map<String, Field> fields)
{    StringBuilder buff = new StringBuilder();    try {        if (node instanceof CategoricalNode) {            CategoricalNode cnode = (CategoricalNode) node;            int attr = (Integer) fields.get("CategoricalNode.attr").get(cnode);            double[] values = (double[]) fields.get("CategoricalNode.values").get(cnode);            Node[] childs = (Node[]) fields.get("CategoricalNode.childs").get(cnode);            String[][] attrValues = (String[][]) fields.get("Dataset.values").get(dataset);            int index = ArrayUtils.indexOf(values, instance.get(attr));            if (index >= 0) {                buff.append(attrNames == null ? attr : attrNames[attr]).append(" = ").append(attrValues[attr][(int) instance.get(attr)]);                buff.append(" -> ");                buff.append(toStringPredict(childs[index], instance, dataset, attrNames, fields));            }        } else if (node instanceof NumericalNode) {            NumericalNode nnode = (NumericalNode) node;            int attr = (Integer) fields.get("NumericalNode.attr").get(nnode);            double split = (Double) fields.get("NumericalNode.split").get(nnode);            Node loChild = (Node) fields.get("NumericalNode.loChild").get(nnode);            Node hiChild = (Node) fields.get("NumericalNode.hiChild").get(nnode);            if (instance.get(attr) < split) {                buff.append('(').append(attrNames == null ? attr : attrNames[attr]).append(" = ").append(doubleToString(instance.get(attr))).append(") < ").append(doubleToString(split));                buff.append(" -> ");                buff.append(toStringPredict(loChild, instance, dataset, attrNames, fields));            } else {                buff.append('(').append(attrNames == null ? attr : attrNames[attr]).append(" = ").append(doubleToString(instance.get(attr))).append(") >= ").append(doubleToString(split));                buff.append(" -> ");                buff.append(toStringPredict(hiChild, instance, dataset, attrNames, fields));            }        } else if (node instanceof Leaf) {            Leaf leaf = (Leaf) node;            double label = (Double) fields.get("Leaf.label").get(leaf);            if (dataset.isNumerical(dataset.getLabelId())) {                buff.append(doubleToString(label));            } else {                buff.append(dataset.getLabelString(label));            }        }    } catch (IllegalAccessException iae) {        throw new IllegalStateException(iae);    }    return buff.toString();}
0
public static String[] predictTrace(Node tree, Data data, String[] attrNames)
{    Map<String, Field> reflectMap = getReflectMap();    String[] prediction = new String[data.size()];    for (int i = 0; i < data.size(); i++) {        prediction[i] = toStringPredict(tree, data.get(i), data.getDataset(), attrNames, reflectMap);    }    return prediction;}
0
public static void predictTracePrint(Node tree, Data data, String[] attrNames)
{    Map<String, Field> reflectMap = getReflectMap();    for (int i = 0; i < data.size(); i++) {        System.out.println(toStringPredict(tree, data.get(i), data.getDataset(), attrNames, reflectMap));    }}
0
public static void main(String[] args) throws IOException
{    DefaultOptionBuilder obuilder = new DefaultOptionBuilder();    ArgumentBuilder abuilder = new ArgumentBuilder();    GroupBuilder gbuilder = new GroupBuilder();    Option dataOpt = obuilder.withLongName("data").withShortName("d").withRequired(true).withArgument(abuilder.withName("data").withMinimum(1).withMaximum(1).create()).withDescription("Data path").create();    Option datasetOpt = obuilder.withLongName("dataset").withShortName("ds").withRequired(true).withArgument(abuilder.withName("dataset").withMinimum(1).create()).withDescription("Dataset path").create();    Option outputOpt = obuilder.withLongName("output").withShortName("o").withRequired(true).withArgument(abuilder.withName("output").withMinimum(1).withMaximum(1).create()).withDescription("Path to generated files").create();    Option partitionsOpt = obuilder.withLongName("numpartitions").withShortName("p").withRequired(true).withArgument(abuilder.withName("numparts").withMinimum(1).withMinimum(1).create()).withDescription("Number of partitions to create").create();    Option helpOpt = obuilder.withLongName("help").withDescription("Print out help").withShortName("h").create();    Group group = gbuilder.withName("Options").withOption(dataOpt).withOption(outputOpt).withOption(datasetOpt).withOption(partitionsOpt).withOption(helpOpt).create();    try {        Parser parser = new Parser();        parser.setGroup(group);        CommandLine cmdLine = parser.parse(args);        if (cmdLine.hasOption(helpOpt)) {            CommandLineUtil.printHelp(group);            return;        }        String data = cmdLine.getValue(dataOpt).toString();        String dataset = cmdLine.getValue(datasetOpt).toString();        int numPartitions = Integer.parseInt(cmdLine.getValue(partitionsOpt).toString());        String output = cmdLine.getValue(outputOpt).toString();        runTool(data, dataset, output, numPartitions);    } catch (OptionException e) {                CommandLineUtil.printHelp(group);    }}
1
private static void runTool(String dataStr, String datasetStr, String output, int numPartitions) throws IOException
{    Preconditions.checkArgument(numPartitions > 0, "numPartitions <= 0");        Path outputPath = new Path(output);    Configuration conf = new Configuration();    FileSystem fs = outputPath.getFileSystem(conf);    Preconditions.checkArgument(!fs.exists(outputPath), "Output path already exists");                                File tempFile = FileUtil.createLocalTempFile(new File(""), "df.tools.UDistrib", true);    Path partsPath = new Path(tempFile.toString());    FileSystem pfs = partsPath.getFileSystem(conf);    Path[] partPaths = new Path[numPartitions];    FSDataOutputStream[] files = new FSDataOutputStream[numPartitions];    for (int p = 0; p < numPartitions; p++) {        partPaths[p] = new Path(partsPath, String.format(Locale.ENGLISH, "part.%03d", p));        files[p] = pfs.create(partPaths[p]);    }    Path datasetPath = new Path(datasetStr);    Dataset dataset = Dataset.load(conf, datasetPath);        int[] currents = new int[dataset.nblabels()];        Random random = RandomUtils.getRandom();    for (int c = 0; c < currents.length; c++) {        currents[c] = random.nextInt(numPartitions);    }        Path dataPath = new Path(dataStr);    FileSystem ifs = dataPath.getFileSystem(conf);    FSDataInputStream input = ifs.open(dataPath);    Scanner scanner = new Scanner(input, "UTF-8");    DataConverter converter = new DataConverter(dataset);    int id = 0;    while (scanner.hasNextLine()) {        if (id % 1000 == 0) {                    }        String line = scanner.nextLine();        if (line.isEmpty()) {                        continue;        }                Instance instance = converter.convert(line);        int label = (int) dataset.getLabel(instance);        files[currents[label]].writeBytes(line);        files[currents[label]].writeChar('\n');                currents[label]++;        if (currents[label] == numPartitions) {            currents[label] = 0;        }    }        scanner.close();    for (FSDataOutputStream file : files) {        Closeables.close(file, false);    }        FileUtil.copyMerge(pfs, partsPath, fs, outputPath, true, conf, null);/*     * FSDataOutputStream joined = fs.create(new Path(outputPath, "uniform.data")); for (int p = 0; p <     * numPartitions; p++) { FSDataInputStream partStream =     * fs.open(partPaths[p]);     *      * IOUtils.copyBytes(partStream, joined, conf, false);     *      * partStream.close(); }     *      * joined.close();     *      * fs.delete(partsPath, true);     */}
1
public void add(int trueValue, double score)
{    Preconditions.checkArgument(trueValue == 0 || trueValue == 1, "True value must be 0 or 1");    hasScore = true;    int predictedClass = score > threshold ? 1 : 0;    confusion.set(trueValue, predictedClass, confusion.get(trueValue, predictedClass) + 1);    samples++;    if (isProbabilityScore()) {        double limited = Math.max(1.0e-20, Math.min(score, 1 - 1.0e-20));        double v0 = entropy.get(trueValue, 0);        entropy.set(trueValue, 0, (Math.log1p(-limited) - v0) / samples + v0);        double v1 = entropy.get(trueValue, 1);        entropy.set(trueValue, 1, (Math.log(limited) - v1) / samples + v1);    }        DoubleArrayList buf = scores[trueValue];    if (buf.size() >= maxBufferSize) {                                                                                int index = rand.nextInt(samples);        if (index < buf.size()) {            buf.set(index, score);        }    } else {                                buf.add(score);    }}
0
public void add(int trueValue, int predictedClass)
{    hasScore = false;    Preconditions.checkArgument(trueValue == 0 || trueValue == 1, "True value must be 0 or 1");    confusion.set(trueValue, predictedClass, confusion.get(trueValue, predictedClass) + 1);}
0
public double auc()
{    Preconditions.checkArgument(hasScore, "Can't compute AUC for classifier without a score");    scores[0].sort();    scores[1].sort();    double n0 = scores[0].size();    double n1 = scores[1].size();    if (n0 == 0 || n1 == 0) {        return 0.5;    }        int i0 = 0;    int i1 = 0;    int rank = 1;    double rankSum = 0;    while (i0 < n0 && i1 < n1) {        double v0 = scores[0].get(i0);        double v1 = scores[1].get(i1);        if (v0 < v1) {            i0++;            rank++;        } else if (v1 < v0) {            i1++;            rankSum += rank;            rank++;        } else {                        double tieScore = v0;                        int k0 = 0;            while (i0 < n0 && scores[0].get(i0) == tieScore) {                k0++;                i0++;            }                        int k1 = 0;            while (i1 < n1 && scores[1].get(i1) == tieScore) {                k1++;                i1++;            }                                                rankSum += (rank + (k0 + k1 - 1) / 2.0) * k1;            rank += k0 + k1;        }    }    if (i1 < n1) {        rankSum += (rank + (n1 - i1 - 1) / 2.0) * (n1 - i1);        rank += (int) (n1 - i1);    }    return (rankSum / n1 - (n1 + 1) / 2) / n0;}
0
public Matrix confusion()
{    return confusion;}
0
public Matrix entropy()
{    if (!hasScore) {                        double p = (0.5 + confusion.get(1, 1)) / (1 + confusion.get(0, 0) + confusion.get(1, 1));        entropy.set(0, 0, confusion.get(0, 0) * Math.log1p(-p));        entropy.set(0, 1, confusion.get(0, 1) * Math.log(p));        entropy.set(1, 0, confusion.get(1, 0) * Math.log1p(-p));        entropy.set(1, 1, confusion.get(1, 1) * Math.log(p));    }    return entropy;}
0
public void setMaxBufferSize(int maxBufferSize)
{    this.maxBufferSize = maxBufferSize;}
0
public boolean isProbabilityScore()
{    return probabilityScore;}
0
public void setProbabilityScore(boolean probabilityScore)
{    this.probabilityScore = probabilityScore;}
0
protected NaiveBayesModel getModel()
{    return model;}
0
protected double getScoreForLabelInstance(int label, Vector instance)
{    double result = 0.0;    for (Element e : instance.nonZeroes()) {        result += e.get() * getScoreForLabelFeature(label, e.index());    }    return result;}
0
public int numCategories()
{    return model.numLabels();}
0
public Vector classifyFull(Vector instance)
{    return classifyFull(model.createScoringVector(), instance);}
0
public Vector classifyFull(Vector r, Vector instance)
{    for (int label = 0; label < model.numLabels(); label++) {        r.setQuick(label, getScoreForLabelInstance(label, instance));    }    return r;}
0
public double classifyScalar(Vector instance)
{    throw new UnsupportedOperationException("Not supported in Naive Bayes");}
0
public Vector classify(Vector instance)
{    throw new UnsupportedOperationException("probabilites not supported in Naive Bayes");}
0
public static NaiveBayesModel readModelFromDir(Path base, Configuration conf)
{    float alphaI = conf.getFloat(ThetaMapper.ALPHA_I, 1.0f);    boolean isComplementary = conf.getBoolean(NaiveBayesModel.COMPLEMENTARY_MODEL, true);        Vector scoresPerLabel = null;    Vector scoresPerFeature = null;    for (Pair<Text, VectorWritable> record : new SequenceFileDirIterable<Text, VectorWritable>(new Path(base, TrainNaiveBayesJob.WEIGHTS), PathType.LIST, PathFilters.partFilter(), conf)) {        String key = record.getFirst().toString();        VectorWritable value = record.getSecond();        if (key.equals(TrainNaiveBayesJob.WEIGHTS_PER_FEATURE)) {            scoresPerFeature = value.get();        } else if (key.equals(TrainNaiveBayesJob.WEIGHTS_PER_LABEL)) {            scoresPerLabel = value.get();        }    }    Preconditions.checkNotNull(scoresPerFeature);    Preconditions.checkNotNull(scoresPerLabel);    Matrix scoresPerLabelAndFeature = new SparseMatrix(scoresPerLabel.size(), scoresPerFeature.size());    for (Pair<IntWritable, VectorWritable> entry : new SequenceFileDirIterable<IntWritable, VectorWritable>(new Path(base, TrainNaiveBayesJob.SUMMED_OBSERVATIONS), PathType.LIST, PathFilters.partFilter(), conf)) {        scoresPerLabelAndFeature.assignRow(entry.getFirst().get(), entry.getSecond().get());    }        Vector perLabelThetaNormalizer = null;    if (isComplementary) {        perLabelThetaNormalizer = scoresPerLabel.like();        for (Pair<Text, VectorWritable> entry : new SequenceFileDirIterable<Text, VectorWritable>(new Path(base, TrainNaiveBayesJob.THETAS), PathType.LIST, PathFilters.partFilter(), conf)) {            if (entry.getFirst().toString().equals(TrainNaiveBayesJob.LABEL_THETA_NORMALIZER)) {                perLabelThetaNormalizer = entry.getSecond().get();            }        }        Preconditions.checkNotNull(perLabelThetaNormalizer);    }    return new NaiveBayesModel(scoresPerLabelAndFeature, scoresPerFeature, scoresPerLabel, perLabelThetaNormalizer, alphaI, isComplementary);}
0
public static int writeLabelIndex(Configuration conf, Iterable<String> labels, Path indexPath) throws IOException
{    FileSystem fs = FileSystem.get(indexPath.toUri(), conf);    int i = 0;    try (SequenceFile.Writer writer = SequenceFile.createWriter(fs.getConf(), SequenceFile.Writer.file(indexPath), SequenceFile.Writer.keyClass(Text.class), SequenceFile.Writer.valueClass(IntWritable.class))) {        for (String label : labels) {            writer.append(new Text(label), new IntWritable(i++));        }    }    return i;}
0
public static int writeLabelIndex(Configuration conf, Path indexPath, Iterable<Pair<Text, IntWritable>> labels) throws IOException
{    FileSystem fs = FileSystem.get(indexPath.toUri(), conf);    Collection<String> seen = new HashSet<>();    int i = 0;    try (SequenceFile.Writer writer = SequenceFile.createWriter(fs.getConf(), SequenceFile.Writer.file(indexPath), SequenceFile.Writer.keyClass(Text.class), SequenceFile.Writer.valueClass(IntWritable.class))) {        for (Object label : labels) {            String theLabel = SLASH.split(((Pair<?, ?>) label).getFirst().toString())[1];            if (!seen.contains(theLabel)) {                writer.append(new Text(theLabel), new IntWritable(i++));                seen.add(theLabel);            }        }    }    return i;}
0
public static Map<Integer, String> readLabelIndex(Configuration conf, Path indexPath)
{    Map<Integer, String> labelMap = new HashMap<>();    for (Pair<Text, IntWritable> pair : new SequenceFileIterable<Text, IntWritable>(indexPath, true, conf)) {        labelMap.put(pair.getSecond().get(), pair.getFirst().toString());    }    return labelMap;}
0
public static OpenObjectIntHashMap<String> readIndexFromCache(Configuration conf) throws IOException
{    OpenObjectIntHashMap<String> index = new OpenObjectIntHashMap<>();    for (Pair<Writable, IntWritable> entry : new SequenceFileIterable<Writable, IntWritable>(HadoopUtil.getSingleCachedFile(conf), conf)) {        index.put(entry.getFirst().toString(), entry.getSecond().get());    }    return index;}
0
public static Map<String, Vector> readScoresFromCache(Configuration conf) throws IOException
{    Map<String, Vector> sumVectors = new HashMap<>();    for (Pair<Text, VectorWritable> entry : new SequenceFileDirIterable<Text, VectorWritable>(HadoopUtil.getSingleCachedFile(conf), PathType.LIST, PathFilters.partFilter(), conf)) {        sumVectors.put(entry.getFirst().toString(), entry.getSecond().get());    }    return sumVectors;}
0
public double getScoreForLabelFeature(int label, int feature)
{    NaiveBayesModel model = getModel();    double weight = computeWeight(model.featureWeight(feature), model.weight(label, feature), model.totalWeightSum(), model.labelWeight(label), model.alphaI(), model.numFeatures());        return weight / model.thetaNormalizer(label);}
0
public static double computeWeight(double featureWeight, double featureLabelWeight, double totalWeight, double labelWeight, double alphaI, double numFeatures)
{    double numerator = featureWeight - featureLabelWeight + alphaI;    double denominator = totalWeight - labelWeight + alphaI * numFeatures;    return -Math.log(numerator / denominator);}
0
public double labelWeight(int label)
{    return weightsPerLabel.getQuick(label);}
0
public double thetaNormalizer(int label)
{    return perlabelThetaNormalizer.get(label);}
0
public double featureWeight(int feature)
{    return weightsPerFeature.getQuick(feature);}
0
public double weight(int label, int feature)
{    return weightsPerLabelAndFeature.getQuick(label, feature);}
0
public float alphaI()
{    return alphaI;}
0
public double numFeatures()
{    return numFeatures;}
0
public double totalWeightSum()
{    return totalWeightSum;}
0
public int numLabels()
{    return weightsPerLabel.size();}
0
public Vector createScoringVector()
{    return weightsPerLabel.like();}
0
public boolean isComplemtary()
{    return isComplementary;}
0
public static NaiveBayesModel materialize(Path output, Configuration conf) throws IOException
{    FileSystem fs = output.getFileSystem(conf);    Vector weightsPerLabel;    Vector perLabelThetaNormalizer = null;    Vector weightsPerFeature;    Matrix weightsPerLabelAndFeature;    float alphaI;    boolean isComplementary;    try (FSDataInputStream in = fs.open(new Path(output, "naiveBayesModel.bin"))) {        alphaI = in.readFloat();        isComplementary = in.readBoolean();        weightsPerFeature = VectorWritable.readVector(in);        weightsPerLabel = new DenseVector(VectorWritable.readVector(in));        if (isComplementary) {            perLabelThetaNormalizer = new DenseVector(VectorWritable.readVector(in));        }        weightsPerLabelAndFeature = new SparseRowMatrix(weightsPerLabel.size(), weightsPerFeature.size());        for (int label = 0; label < weightsPerLabelAndFeature.numRows(); label++) {            weightsPerLabelAndFeature.assignRow(label, VectorWritable.readVector(in));        }    }    NaiveBayesModel model = new NaiveBayesModel(weightsPerLabelAndFeature, weightsPerFeature, weightsPerLabel, perLabelThetaNormalizer, alphaI, isComplementary);    model.validate();    return model;}
0
public void serialize(Path output, Configuration conf) throws IOException
{    FileSystem fs = output.getFileSystem(conf);    try (FSDataOutputStream out = fs.create(new Path(output, "naiveBayesModel.bin"))) {        out.writeFloat(alphaI);        out.writeBoolean(isComplementary);        VectorWritable.writeVector(out, weightsPerFeature);        VectorWritable.writeVector(out, weightsPerLabel);        if (isComplementary) {            VectorWritable.writeVector(out, perlabelThetaNormalizer);        }        for (int row = 0; row < weightsPerLabelAndFeature.numRows(); row++) {            VectorWritable.writeVector(out, weightsPerLabelAndFeature.viewRow(row));        }    }}
0
public void validate()
{    Preconditions.checkState(alphaI > 0, "alphaI has to be greater than 0!");    Preconditions.checkArgument(numFeatures > 0, "the vocab count has to be greater than 0!");    Preconditions.checkArgument(totalWeightSum > 0, "the totalWeightSum has to be greater than 0!");    Preconditions.checkNotNull(weightsPerLabel, "the number of labels has to be defined!");    Preconditions.checkArgument(weightsPerLabel.getNumNondefaultElements() > 0, "the number of labels has to be greater than 0!");    Preconditions.checkNotNull(weightsPerFeature, "the feature sums have to be defined");    Preconditions.checkArgument(weightsPerFeature.getNumNondefaultElements() > 0, "the feature sums have to be greater than 0!");    if (isComplementary) {        Preconditions.checkArgument(perlabelThetaNormalizer != null, "the theta normalizers have to be defined");        Preconditions.checkArgument(perlabelThetaNormalizer.getNumNondefaultElements() > 0, "the number of theta normalizers has to be greater than 0!");        Preconditions.checkArgument(Math.signum(perlabelThetaNormalizer.minValue()) == Math.signum(perlabelThetaNormalizer.maxValue()), "Theta normalizers do not all have the same sign");        Preconditions.checkArgument(perlabelThetaNormalizer.getNumNonZeroElements() == perlabelThetaNormalizer.size(), "Theta normalizers can not have zero value.");    }}
0
public double getScoreForLabelFeature(int label, int feature)
{    NaiveBayesModel model = getModel();        return computeWeight(model.weight(label, feature), model.labelWeight(label), model.alphaI(), model.numFeatures());}
0
public static double computeWeight(double featureLabelWeight, double labelWeight, double alphaI, double numFeatures)
{    double numerator = featureLabelWeight + alphaI;    double denominator = labelWeight + alphaI * numFeatures;    return Math.log(numerator / denominator);}
0
protected void setup(Context context) throws IOException, InterruptedException
{    super.setup(context);    Configuration conf = context.getConfiguration();    Path modelPath = HadoopUtil.getSingleCachedFile(conf);    NaiveBayesModel model = NaiveBayesModel.materialize(modelPath, conf);    boolean isComplementary = Boolean.parseBoolean(conf.get(TestNaiveBayesDriver.COMPLEMENTARY));        if (isComplementary) {        Preconditions.checkArgument((model.isComplemtary()), "Complementary mode in model is different than test mode");    }    if (isComplementary) {        classifier = new ComplementaryNaiveBayesClassifier(model);    } else {        classifier = new StandardNaiveBayesClassifier(model);    }}
0
protected void map(Text key, VectorWritable value, Context context) throws IOException, InterruptedException
{    Vector result = classifier.classifyFull(value.get());        context.write(new Text(SLASH.split(key.toString())[1]), new VectorWritable(result));}
0
public static void main(String[] args) throws Exception
{    ToolRunner.run(new Configuration(), new TestNaiveBayesDriver(), args);}
0
public int run(String[] args) throws Exception
{    addInputOption();    addOutputOption();    addOption(addOption(DefaultOptionCreator.overwriteOption().create()));    addOption("model", "m", "The path to the model built during training", true);    addOption(buildOption("testComplementary", "c", "test complementary?", false, false, String.valueOf(false)));    addOption(buildOption("runSequential", "seq", "run sequential?", false, false, String.valueOf(false)));    addOption("labelIndex", "l", "The path to the location of the label index", true);    Map<String, List<String>> parsedArgs = parseArguments(args);    if (parsedArgs == null) {        return -1;    }    if (hasOption(DefaultOptionCreator.OVERWRITE_OPTION)) {        HadoopUtil.delete(getConf(), getOutputPath());    }    boolean sequential = hasOption("runSequential");    boolean succeeded;    if (sequential) {        runSequential();    } else {        succeeded = runMapReduce();        if (!succeeded) {            return -1;        }    }        Map<Integer, String> labelMap = BayesUtils.readLabelIndex(getConf(), new Path(getOption("labelIndex")));        SequenceFileDirIterable<Text, VectorWritable> dirIterable = new SequenceFileDirIterable<>(getOutputPath(), PathType.LIST, PathFilters.partFilter(), getConf());    ResultAnalyzer analyzer = new ResultAnalyzer(labelMap.values(), "DEFAULT");    analyzeResults(labelMap, dirIterable, analyzer);        return 0;}
1
private void runSequential() throws IOException
{    boolean complementary = hasOption("testComplementary");    FileSystem fs = FileSystem.get(getConf());    NaiveBayesModel model = NaiveBayesModel.materialize(new Path(getOption("model")), getConf());        if (complementary) {        Preconditions.checkArgument((model.isComplemtary()), "Complementary mode in model is different from test mode");    }    AbstractNaiveBayesClassifier classifier;    if (complementary) {        classifier = new ComplementaryNaiveBayesClassifier(model);    } else {        classifier = new StandardNaiveBayesClassifier(model);    }    try (SequenceFile.Writer writer = SequenceFile.createWriter(fs, getConf(), new Path(getOutputPath(), "part-r-00000"), Text.class, VectorWritable.class)) {        SequenceFileDirIterable<Text, VectorWritable> dirIterable = new SequenceFileDirIterable<>(getInputPath(), PathType.LIST, PathFilters.partFilter(), getConf());                for (Pair<Text, VectorWritable> pair : dirIterable) {            writer.append(new Text(SLASH.split(pair.getFirst().toString())[1]), new VectorWritable(classifier.classifyFull(pair.getSecond().get())));        }    }}
0
private boolean runMapReduce() throws IOException, InterruptedException, ClassNotFoundException
{    Path model = new Path(getOption("model"));    HadoopUtil.cacheFiles(model, getConf());        Job testJob = prepareJob(getInputPath(), getOutputPath(), SequenceFileInputFormat.class, BayesTestMapper.class, Text.class, VectorWritable.class, SequenceFileOutputFormat.class);        boolean complementary = hasOption("testComplementary");    testJob.getConfiguration().set(COMPLEMENTARY, String.valueOf(complementary));    return testJob.waitForCompletion(true);}
0
private static void analyzeResults(Map<Integer, String> labelMap, SequenceFileDirIterable<Text, VectorWritable> dirIterable, ResultAnalyzer analyzer)
{    for (Pair<Text, VectorWritable> pair : dirIterable) {        int bestIdx = Integer.MIN_VALUE;        double bestScore = Long.MIN_VALUE;        for (Vector.Element element : pair.getSecond().get().all()) {            if (element.get() > bestScore) {                bestScore = element.get();                bestIdx = element.index();            }        }        if (bestIdx != Integer.MIN_VALUE) {            ClassifierResult classifierResult = new ClassifierResult(labelMap.get(bestIdx), bestScore);            analyzer.addInstance(pair.getFirst().toString(), classifierResult);        }    }}
0
public void train(int label, Vector perLabelWeight)
{    double labelWeight = labelWeight(label);        for (int i = 0; i < perLabelWeight.size(); i++) {        Vector.Element perLabelWeightElement = perLabelWeight.getElement(i);        updatePerLabelThetaNormalizer(label, ComplementaryNaiveBayesClassifier.computeWeight(featureWeight(perLabelWeightElement.index()), perLabelWeightElement.get(), totalWeightSum(), labelWeight, alphaI(), numFeatures()));    }}
0
protected double alphaI()
{    return alphaI;}
0
protected double numFeatures()
{    return numFeatures;}
0
protected double labelWeight(int label)
{    return weightsPerLabel.get(label);}
0
protected double totalWeightSum()
{    return totalWeightSum;}
0
protected double featureWeight(int feature)
{    return weightsPerFeature.get(feature);}
0
protected void updatePerLabelThetaNormalizer(int label, double weight)
{    perLabelThetaNormalizer.set(label, perLabelThetaNormalizer.get(label) + Math.abs(weight));}
0
public Vector retrievePerLabelThetaNormalizer()
{    return perLabelThetaNormalizer.clone();}
0
protected void setup(Context ctx) throws IOException, InterruptedException
{    super.setup(ctx);    labelIndex = BayesUtils.readIndexFromCache(ctx.getConfiguration());}
0
protected void map(Text labelText, VectorWritable instance, Context ctx) throws IOException, InterruptedException
{    String label = SLASH.split(labelText.toString())[1];    if (labelIndex.containsKey(label)) {        ctx.write(new IntWritable(labelIndex.get(label)), instance);    } else {        ctx.getCounter(Counter.SKIPPED_INSTANCES).increment(1);    }}
0
protected void setup(Context ctx) throws IOException, InterruptedException
{    super.setup(ctx);    Configuration conf = ctx.getConfiguration();    float alphaI = conf.getFloat(ALPHA_I, 1.0f);    Map<String, Vector> scores = BayesUtils.readScoresFromCache(conf);    trainer = new ComplementaryThetaTrainer(scores.get(TrainNaiveBayesJob.WEIGHTS_PER_FEATURE), scores.get(TrainNaiveBayesJob.WEIGHTS_PER_LABEL), alphaI);}
0
protected void map(IntWritable key, VectorWritable value, Context ctx) throws IOException, InterruptedException
{    trainer.train(key.get(), value.get());}
0
protected void cleanup(Context ctx) throws IOException, InterruptedException
{    ctx.write(new Text(TrainNaiveBayesJob.LABEL_THETA_NORMALIZER), new VectorWritable(trainer.retrievePerLabelThetaNormalizer()));    super.cleanup(ctx);}
0
public static void main(String[] args) throws Exception
{    ToolRunner.run(new Configuration(), new TrainNaiveBayesJob(), args);}
0
public int run(String[] args) throws Exception
{    addInputOption();    addOutputOption();    addOption(ALPHA_I, "a", "smoothing parameter", String.valueOf(1.0f));    addOption(buildOption(TRAIN_COMPLEMENTARY, "c", "train complementary?", false, false, String.valueOf(false)));    addOption(LABEL_INDEX, "li", "The path to store the label index in", false);    addOption(DefaultOptionCreator.overwriteOption().create());    Map<String, List<String>> parsedArgs = parseArguments(args);    if (parsedArgs == null) {        return -1;    }    if (hasOption(DefaultOptionCreator.OVERWRITE_OPTION)) {        HadoopUtil.delete(getConf(), getOutputPath());        HadoopUtil.delete(getConf(), getTempPath());    }    Path labPath;    String labPathStr = getOption(LABEL_INDEX);    if (labPathStr != null) {        labPath = new Path(labPathStr);    } else {        labPath = getTempPath(LABEL_INDEX);    }    long labelSize = createLabelIndex(labPath);    float alphaI = Float.parseFloat(getOption(ALPHA_I));    boolean trainComplementary = hasOption(TRAIN_COMPLEMENTARY);    HadoopUtil.setSerializations(getConf());    HadoopUtil.cacheFiles(labPath, getConf());        Job indexInstances = prepareJob(getInputPath(), getTempPath(SUMMED_OBSERVATIONS), SequenceFileInputFormat.class, IndexInstancesMapper.class, IntWritable.class, VectorWritable.class, VectorSumReducer.class, IntWritable.class, VectorWritable.class, SequenceFileOutputFormat.class);    indexInstances.setCombinerClass(VectorSumReducer.class);    boolean succeeded = indexInstances.waitForCompletion(true);    if (!succeeded) {        return -1;    }        Job weightSummer = prepareJob(getTempPath(SUMMED_OBSERVATIONS), getTempPath(WEIGHTS), SequenceFileInputFormat.class, WeightsMapper.class, Text.class, VectorWritable.class, VectorSumReducer.class, Text.class, VectorWritable.class, SequenceFileOutputFormat.class);    weightSummer.getConfiguration().set(WeightsMapper.NUM_LABELS, String.valueOf(labelSize));    weightSummer.setCombinerClass(VectorSumReducer.class);    succeeded = weightSummer.waitForCompletion(true);    if (!succeeded) {        return -1;    }        HadoopUtil.cacheFiles(getTempPath(WEIGHTS), getConf());    if (trainComplementary) {                        Job thetaSummer = prepareJob(getTempPath(SUMMED_OBSERVATIONS), getTempPath(THETAS), SequenceFileInputFormat.class, ThetaMapper.class, Text.class, VectorWritable.class, VectorSumReducer.class, Text.class, VectorWritable.class, SequenceFileOutputFormat.class);        thetaSummer.setCombinerClass(VectorSumReducer.class);        thetaSummer.getConfiguration().setFloat(ThetaMapper.ALPHA_I, alphaI);        thetaSummer.getConfiguration().setBoolean(ThetaMapper.TRAIN_COMPLEMENTARY, trainComplementary);        succeeded = thetaSummer.waitForCompletion(true);        if (!succeeded) {            return -1;        }    }        HadoopUtil.cacheFiles(getTempPath(THETAS), getConf());        getConf().setFloat(ThetaMapper.ALPHA_I, alphaI);    getConf().setBoolean(NaiveBayesModel.COMPLEMENTARY_MODEL, trainComplementary);    NaiveBayesModel naiveBayesModel = BayesUtils.readModelFromDir(getTempPath(), getConf());    naiveBayesModel.validate();    naiveBayesModel.serialize(getOutputPath(), getConf());    return 0;}
0
private long createLabelIndex(Path labPath) throws IOException
{    long labelSize = 0;    Iterable<Pair<Text, IntWritable>> iterable = new SequenceFileDirIterable<>(getInputPath(), PathType.LIST, PathFilters.logsCRCFilter(), getConf());    labelSize = BayesUtils.writeLabelIndex(getConf(), labPath, iterable);    return labelSize;}
0
protected void setup(Context ctx) throws IOException, InterruptedException
{    super.setup(ctx);    int numLabels = Integer.parseInt(ctx.getConfiguration().get(NUM_LABELS));    Preconditions.checkArgument(numLabels > 0, "Wrong numLabels: " + numLabels + ". Must be > 0!");    weightsPerLabel = new DenseVector(numLabels);}
0
protected void map(IntWritable index, VectorWritable value, Context ctx) throws IOException, InterruptedException
{    Vector instance = value.get();    if (weightsPerFeature == null) {        weightsPerFeature = new RandomAccessSparseVector(instance.size(), instance.getNumNondefaultElements());    }    int label = index.get();    weightsPerFeature.assign(instance, Functions.PLUS);    weightsPerLabel.set(label, weightsPerLabel.get(label) + instance.zSum());}
0
protected void cleanup(Context ctx) throws IOException, InterruptedException
{    if (weightsPerFeature != null) {        ctx.write(new Text(TrainNaiveBayesJob.WEIGHTS_PER_FEATURE), new VectorWritable(weightsPerFeature));        ctx.write(new Text(TrainNaiveBayesJob.WEIGHTS_PER_LABEL), new VectorWritable(weightsPerLabel));    }    super.cleanup(ctx);}
0
 double getActual()
{    return actual;}
0
 double getResult()
{    return result;}
0
public void addInstance(double actual, double result)
{    if (results == null) {        results = new ArrayList<>();    }    results.add(new Result(actual, result));}
0
public void setInstances(double[][] results)
{    for (double[] res : results) {        addInstance(res[0], res[1]);    }}
0
public String toString()
{    double sumActual = 0.0;    double sumActualSquared = 0.0;    double sumResult = 0.0;    double sumResultSquared = 0.0;    double sumActualResult = 0.0;    double sumAbsolute = 0.0;    double sumAbsoluteSquared = 0.0;    int predictable = 0;    int unpredictable = 0;    for (Result res : results) {        double actual = res.getActual();        double result = res.getResult();        if (Double.isNaN(result)) {            unpredictable++;        } else {            sumActual += actual;            sumActualSquared += actual * actual;            sumResult += result;            sumResultSquared += result * result;            sumActualResult += actual * result;            double absolute = Math.abs(actual - result);            sumAbsolute += absolute;            sumAbsoluteSquared += absolute * absolute;            predictable++;        }    }    StringBuilder returnString = new StringBuilder();    returnString.append("=======================================================\n");    returnString.append("Summary\n");    returnString.append("-------------------------------------------------------\n");    if (predictable > 0) {        double varActual = sumActualSquared - sumActual * sumActual / predictable;        double varResult = sumResultSquared - sumResult * sumResult / predictable;        double varCo = sumActualResult - sumActual * sumResult / predictable;        double correlation;        if (varActual * varResult <= 0) {            correlation = 0.0;        } else {            correlation = varCo / Math.sqrt(varActual * varResult);        }        Locale.setDefault(Locale.US);        NumberFormat decimalFormatter = new DecimalFormat("0.####");        returnString.append(StringUtils.rightPad("Correlation coefficient", 40)).append(": ").append(StringUtils.leftPad(decimalFormatter.format(correlation), 10)).append('\n');        returnString.append(StringUtils.rightPad("Mean absolute error", 40)).append(": ").append(StringUtils.leftPad(decimalFormatter.format(sumAbsolute / predictable), 10)).append('\n');        returnString.append(StringUtils.rightPad("Root mean squared error", 40)).append(": ").append(StringUtils.leftPad(decimalFormatter.format(Math.sqrt(sumAbsoluteSquared / predictable)), 10)).append('\n');    }    returnString.append(StringUtils.rightPad("Predictable Instances", 40)).append(": ").append(StringUtils.leftPad(Integer.toString(predictable), 10)).append('\n');    returnString.append(StringUtils.rightPad("Unpredictable Instances", 40)).append(": ").append(StringUtils.leftPad(Integer.toString(unpredictable), 10)).append('\n');    returnString.append(StringUtils.rightPad("Total Regressed Instances", 40)).append(": ").append(StringUtils.leftPad(Integer.toString(results.size()), 10)).append('\n');    returnString.append('\n');    return returnString.toString();}
0
public ConfusionMatrix getConfusionMatrix()
{    return this.confusionMatrix;}
0
public boolean addInstance(String correctLabel, ClassifierResult classifiedResult)
{    boolean result = correctLabel.equals(classifiedResult.getLabel());    if (result) {        correctlyClassified++;    } else {        incorrectlyClassified++;    }    confusionMatrix.addInstance(correctLabel, classifiedResult);    if (classifiedResult.getLogLikelihood() != Double.MAX_VALUE) {        summarizer.add(classifiedResult.getLogLikelihood());        hasLL = true;    }    return result;}
0
public String toString()
{    StringBuilder returnString = new StringBuilder();    returnString.append('\n');    returnString.append("=======================================================\n");    returnString.append("Summary\n");    returnString.append("-------------------------------------------------------\n");    int totalClassified = correctlyClassified + incorrectlyClassified;    double percentageCorrect = (double) 100 * correctlyClassified / totalClassified;    double percentageIncorrect = (double) 100 * incorrectlyClassified / totalClassified;    NumberFormat decimalFormatter = new DecimalFormat("0.####");    returnString.append(StringUtils.rightPad("Correctly Classified Instances", 40)).append(": ").append(StringUtils.leftPad(Integer.toString(correctlyClassified), 10)).append('\t').append(StringUtils.leftPad(decimalFormatter.format(percentageCorrect), 10)).append("%\n");    returnString.append(StringUtils.rightPad("Incorrectly Classified Instances", 40)).append(": ").append(StringUtils.leftPad(Integer.toString(incorrectlyClassified), 10)).append('\t').append(StringUtils.leftPad(decimalFormatter.format(percentageIncorrect), 10)).append("%\n");    returnString.append(StringUtils.rightPad("Total Classified Instances", 40)).append(": ").append(StringUtils.leftPad(Integer.toString(totalClassified), 10)).append('\n');    returnString.append('\n');    returnString.append(confusionMatrix);    returnString.append("=======================================================\n");    returnString.append("Statistics\n");    returnString.append("-------------------------------------------------------\n");    RunningAverageAndStdDev normStats = confusionMatrix.getNormalizedStats();    returnString.append(StringUtils.rightPad("Kappa", 40)).append(StringUtils.leftPad(decimalFormatter.format(confusionMatrix.getKappa()), 10)).append('\n');    returnString.append(StringUtils.rightPad("Accuracy", 40)).append(StringUtils.leftPad(decimalFormatter.format(confusionMatrix.getAccuracy()), 10)).append("%\n");    returnString.append(StringUtils.rightPad("Reliability", 40)).append(StringUtils.leftPad(decimalFormatter.format(normStats.getAverage() * 100.00000001), 10)).append("%\n");    returnString.append(StringUtils.rightPad("Reliability (standard deviation)", 40)).append(StringUtils.leftPad(decimalFormatter.format(normStats.getStandardDeviation()), 10)).append('\n');    returnString.append(StringUtils.rightPad("Weighted precision", 40)).append(StringUtils.leftPad(decimalFormatter.format(confusionMatrix.getWeightedPrecision()), 10)).append('\n');    returnString.append(StringUtils.rightPad("Weighted recall", 40)).append(StringUtils.leftPad(decimalFormatter.format(confusionMatrix.getWeightedRecall()), 10)).append('\n');    returnString.append(StringUtils.rightPad("Weighted F1 score", 40)).append(StringUtils.leftPad(decimalFormatter.format(confusionMatrix.getWeightedF1score()), 10)).append('\n');    if (hasLL) {        returnString.append(StringUtils.rightPad("Log-likelihood", 30)).append("mean      : ").append(StringUtils.leftPad(decimalFormatter.format(summarizer.getMean()), 10)).append('\n');        returnString.append(StringUtils.rightPad("", 30)).append(StringUtils.rightPad("25%-ile   : ", 10)).append(StringUtils.leftPad(decimalFormatter.format(summarizer.getQuartile(1)), 10)).append('\n');        returnString.append(StringUtils.rightPad("", 30)).append(StringUtils.rightPad("75%-ile   : ", 10)).append(StringUtils.leftPad(decimalFormatter.format(summarizer.getQuartile(3)), 10)).append('\n');    }    return returnString.toString();}
0
public static void main(String[] args) throws IOException
{    DefaultOptionBuilder optionBuilder = new DefaultOptionBuilder();    ArgumentBuilder argumentBuilder = new ArgumentBuilder();    Option inputOption = DefaultOptionCreator.inputOption().create();    Option outputOption = DefaultOptionCreator.outputOption().create();    Option stateNumberOption = optionBuilder.withLongName("nrOfHiddenStates").withDescription("Number of hidden states").withShortName("nh").withArgument(argumentBuilder.withMaximum(1).withMinimum(1).withName("number").create()).withRequired(true).create();    Option observedStateNumberOption = optionBuilder.withLongName("nrOfObservedStates").withDescription("Number of observed states").withShortName("no").withArgument(argumentBuilder.withMaximum(1).withMinimum(1).withName("number").create()).withRequired(true).create();    Option epsilonOption = optionBuilder.withLongName("epsilon").withDescription("Convergence threshold").withShortName("e").withArgument(argumentBuilder.withMaximum(1).withMinimum(1).withName("number").create()).withRequired(true).create();    Option iterationsOption = optionBuilder.withLongName("max-iterations").withDescription("Maximum iterations number").withShortName("m").withArgument(argumentBuilder.withMaximum(1).withMinimum(1).withName("number").create()).withRequired(true).create();    Group optionGroup = new GroupBuilder().withOption(inputOption).withOption(outputOption).withOption(stateNumberOption).withOption(observedStateNumberOption).withOption(epsilonOption).withOption(iterationsOption).withName("Options").create();    try {        Parser parser = new Parser();        parser.setGroup(optionGroup);        CommandLine commandLine = parser.parse(args);        String input = (String) commandLine.getValue(inputOption);        String output = (String) commandLine.getValue(outputOption);        int nrOfHiddenStates = Integer.parseInt((String) commandLine.getValue(stateNumberOption));        int nrOfObservedStates = Integer.parseInt((String) commandLine.getValue(observedStateNumberOption));        double epsilon = Double.parseDouble((String) commandLine.getValue(epsilonOption));        int maxIterations = Integer.parseInt((String) commandLine.getValue(iterationsOption));                HmmModel model = new HmmModel(nrOfHiddenStates, nrOfObservedStates, new Date().getTime());        List<Integer> observations = new ArrayList<>();                try (Scanner scanner = new Scanner(new FileInputStream(input), "UTF-8")) {            while (scanner.hasNextInt()) {                observations.add(scanner.nextInt());            }        }        int[] observationsArray = new int[observations.size()];        for (int i = 0; i < observations.size(); ++i) {            observationsArray[i] = observations.get(i);        }                HmmModel trainedModel = HmmTrainer.trainBaumWelch(model, observationsArray, epsilon, maxIterations, true);                try (DataOutputStream stream = new DataOutputStream(new FileOutputStream(output))) {            LossyHmmSerializer.serialize(trainedModel, stream);        }                System.out.println("Initial probabilities: ");        for (int i = 0; i < trainedModel.getNrOfHiddenStates(); ++i) {            System.out.print(i + " ");        }        System.out.println();        for (int i = 0; i < trainedModel.getNrOfHiddenStates(); ++i) {            System.out.print(trainedModel.getInitialProbabilities().get(i) + " ");        }        System.out.println();        System.out.println("Transition matrix:");        System.out.print("  ");        for (int i = 0; i < trainedModel.getNrOfHiddenStates(); ++i) {            System.out.print(i + " ");        }        System.out.println();        for (int i = 0; i < trainedModel.getNrOfHiddenStates(); ++i) {            System.out.print(i + " ");            for (int j = 0; j < trainedModel.getNrOfHiddenStates(); ++j) {                System.out.print(trainedModel.getTransitionMatrix().get(i, j) + " ");            }            System.out.println();        }        System.out.println("Emission matrix: ");        System.out.print("  ");        for (int i = 0; i < trainedModel.getNrOfOutputStates(); ++i) {            System.out.print(i + " ");        }        System.out.println();        for (int i = 0; i < trainedModel.getNrOfHiddenStates(); ++i) {            System.out.print(i + " ");            for (int j = 0; j < trainedModel.getNrOfOutputStates(); ++j) {                System.out.print(trainedModel.getEmissionMatrix().get(i, j) + " ");            }            System.out.println();        }    } catch (OptionException e) {        CommandLineUtil.printHelp(optionGroup);    }}
0
public static Matrix forwardAlgorithm(HmmModel model, int[] observations, boolean scaled)
{    Matrix alpha = new DenseMatrix(observations.length, model.getNrOfHiddenStates());    forwardAlgorithm(alpha, model, observations, scaled);    return alpha;}
0
 static void forwardAlgorithm(Matrix alpha, HmmModel model, int[] observations, boolean scaled)
{        Vector ip = model.getInitialProbabilities();    Matrix b = model.getEmissionMatrix();    Matrix a = model.getTransitionMatrix();    if (scaled) {                for (int i = 0; i < model.getNrOfHiddenStates(); i++) {            alpha.setQuick(0, i, Math.log(ip.getQuick(i) * b.getQuick(i, observations[0])));        }                for (int t = 1; t < observations.length; t++) {            for (int i = 0; i < model.getNrOfHiddenStates(); i++) {                                double sum = Double.NEGATIVE_INFINITY;                for (int j = 0; j < model.getNrOfHiddenStates(); j++) {                    double tmp = alpha.getQuick(t - 1, j) + Math.log(a.getQuick(j, i));                    if (tmp > Double.NEGATIVE_INFINITY) {                                                sum = tmp + Math.log1p(Math.exp(sum - tmp));                    }                }                alpha.setQuick(t, i, sum + Math.log(b.getQuick(i, observations[t])));            }        }    } else {                for (int i = 0; i < model.getNrOfHiddenStates(); i++) {            alpha.setQuick(0, i, ip.getQuick(i) * b.getQuick(i, observations[0]));        }                for (int t = 1; t < observations.length; t++) {            for (int i = 0; i < model.getNrOfHiddenStates(); i++) {                double sum = 0.0;                for (int j = 0; j < model.getNrOfHiddenStates(); j++) {                    sum += alpha.getQuick(t - 1, j) * a.getQuick(j, i);                }                alpha.setQuick(t, i, sum * b.getQuick(i, observations[t]));            }        }    }}
0
public static Matrix backwardAlgorithm(HmmModel model, int[] observations, boolean scaled)
{        Matrix beta = new DenseMatrix(observations.length, model.getNrOfHiddenStates());        backwardAlgorithm(beta, model, observations, scaled);    return beta;}
0
 static void backwardAlgorithm(Matrix beta, HmmModel model, int[] observations, boolean scaled)
{        Matrix b = model.getEmissionMatrix();    Matrix a = model.getTransitionMatrix();    if (scaled) {                for (int i = 0; i < model.getNrOfHiddenStates(); i++) {            beta.setQuick(observations.length - 1, i, 0);        }                for (int t = observations.length - 2; t >= 0; t--) {            for (int i = 0; i < model.getNrOfHiddenStates(); i++) {                                double sum = Double.NEGATIVE_INFINITY;                for (int j = 0; j < model.getNrOfHiddenStates(); j++) {                    double tmp = beta.getQuick(t + 1, j) + Math.log(a.getQuick(i, j)) + Math.log(b.getQuick(j, observations[t + 1]));                    if (tmp > Double.NEGATIVE_INFINITY) {                                                sum = tmp + Math.log1p(Math.exp(sum - tmp));                    }                }                beta.setQuick(t, i, sum);            }        }    } else {                for (int i = 0; i < model.getNrOfHiddenStates(); i++) {            beta.setQuick(observations.length - 1, i, 1);        }                for (int t = observations.length - 2; t >= 0; t--) {            for (int i = 0; i < model.getNrOfHiddenStates(); i++) {                double sum = 0;                for (int j = 0; j < model.getNrOfHiddenStates(); j++) {                    sum += beta.getQuick(t + 1, j) * a.getQuick(i, j) * b.getQuick(j, observations[t + 1]);                }                beta.setQuick(t, i, sum);            }        }    }}
0
public static int[] viterbiAlgorithm(HmmModel model, int[] observations, boolean scaled)
{            double[][] delta = new double[observations.length][model.getNrOfHiddenStates()];            int[][] phi = new int[observations.length - 1][model.getNrOfHiddenStates()];        int[] sequence = new int[observations.length];    viterbiAlgorithm(sequence, delta, phi, model, observations, scaled);    return sequence;}
0
 static void viterbiAlgorithm(int[] sequence, double[][] delta, int[][] phi, HmmModel model, int[] observations, boolean scaled)
{        Vector ip = model.getInitialProbabilities();    Matrix b = model.getEmissionMatrix();    Matrix a = model.getTransitionMatrix();        if (scaled) {        for (int i = 0; i < model.getNrOfHiddenStates(); i++) {            delta[0][i] = Math.log(ip.getQuick(i) * b.getQuick(i, observations[0]));        }    } else {        for (int i = 0; i < model.getNrOfHiddenStates(); i++) {            delta[0][i] = ip.getQuick(i) * b.getQuick(i, observations[0]);        }    }        if (scaled) {        for (int t = 1; t < observations.length; t++) {                        for (int i = 0; i < model.getNrOfHiddenStates(); i++) {                                                                int maxState = 0;                double maxProb = delta[t - 1][0] + Math.log(a.getQuick(0, i));                for (int j = 1; j < model.getNrOfHiddenStates(); j++) {                    double prob = delta[t - 1][j] + Math.log(a.getQuick(j, i));                    if (prob > maxProb) {                        maxProb = prob;                        maxState = j;                    }                }                delta[t][i] = maxProb + Math.log(b.getQuick(i, observations[t]));                phi[t - 1][i] = maxState;            }        }    } else {        for (int t = 1; t < observations.length; t++) {                        for (int i = 0; i < model.getNrOfHiddenStates(); i++) {                                                                int maxState = 0;                double maxProb = delta[t - 1][0] * a.getQuick(0, i);                for (int j = 1; j < model.getNrOfHiddenStates(); j++) {                    double prob = delta[t - 1][j] * a.getQuick(j, i);                    if (prob > maxProb) {                        maxProb = prob;                        maxState = j;                    }                }                delta[t][i] = maxProb * b.getQuick(i, observations[t]);                phi[t - 1][i] = maxState;            }        }    }        double maxProb;    if (scaled) {        maxProb = Double.NEGATIVE_INFINITY;    } else {        maxProb = 0.0;    }    for (int i = 0; i < model.getNrOfHiddenStates(); i++) {        if (delta[observations.length - 1][i] > maxProb) {            maxProb = delta[observations.length - 1][i];            sequence[observations.length - 1] = i;        }    }        for (int t = observations.length - 2; t >= 0; t--) {        sequence[t] = phi[t][sequence[t + 1]];    }}
0
public static int[] predict(HmmModel model, int steps)
{    return predict(model, steps, RandomUtils.getRandom());}
0
public static int[] predict(HmmModel model, int steps, long seed)
{    return predict(model, steps, RandomUtils.getRandom(seed));}
0
private static int[] predict(HmmModel model, int steps, Random rand)
{        Vector cip = HmmUtils.getCumulativeInitialProbabilities(model);    Matrix ctm = HmmUtils.getCumulativeTransitionMatrix(model);    Matrix com = HmmUtils.getCumulativeOutputMatrix(model);        int[] result = new int[steps];        int hiddenState = 0;    double randnr = rand.nextDouble();    while (cip.get(hiddenState) < randnr) {        hiddenState++;    }        for (int step = 0; step < steps; ++step) {                randnr = rand.nextDouble();        int outputState = 0;        while (com.get(hiddenState, outputState) < randnr) {            outputState++;        }        result[step] = outputState;                randnr = rand.nextDouble();        int nextHiddenState = 0;        while (ctm.get(hiddenState, nextHiddenState) < randnr) {            nextHiddenState++;        }        hiddenState = nextHiddenState;    }    return result;}
0
public static double modelLikelihood(HmmModel model, int[] outputSequence, boolean scaled)
{    return modelLikelihood(HmmAlgorithms.forwardAlgorithm(model, outputSequence, scaled), scaled);}
0
public static double modelLikelihood(Matrix alpha, boolean scaled)
{    double likelihood = 0;    if (scaled) {        for (int i = 0; i < alpha.numCols(); ++i) {            likelihood += Math.exp(alpha.getQuick(alpha.numRows() - 1, i));        }    } else {        for (int i = 0; i < alpha.numCols(); ++i) {            likelihood += alpha.getQuick(alpha.numRows() - 1, i);        }    }    return likelihood;}
0
public static double modelLikelihood(HmmModel model, int[] outputSequence, Matrix beta, boolean scaled)
{    double likelihood = 0;        Matrix e = model.getEmissionMatrix();    Vector pi = model.getInitialProbabilities();    int firstOutput = outputSequence[0];    if (scaled) {        for (int i = 0; i < model.getNrOfHiddenStates(); ++i) {            likelihood += pi.getQuick(i) * Math.exp(beta.getQuick(0, i)) * e.getQuick(i, firstOutput);        }    } else {        for (int i = 0; i < model.getNrOfHiddenStates(); ++i) {            likelihood += pi.getQuick(i) * beta.getQuick(0, i) * e.getQuick(i, firstOutput);        }    }    return likelihood;}
0
public static int[] decode(HmmModel model, int[] observations, boolean scaled)
{    return HmmAlgorithms.viterbiAlgorithm(model, observations, scaled);}
0
public HmmModel clone()
{    HmmModel model = new HmmModel(transitionMatrix.clone(), emissionMatrix.clone(), initialProbabilities.clone());    if (hiddenStateNames != null) {        model.hiddenStateNames = HashBiMap.create(hiddenStateNames);    }    if (outputStateNames != null) {        model.outputStateNames = HashBiMap.create(outputStateNames);    }    return model;}
0
public void assign(HmmModel model)
{    this.nrOfHiddenStates = model.nrOfHiddenStates;    this.nrOfOutputStates = model.nrOfOutputStates;    this.hiddenStateNames = model.hiddenStateNames;    this.outputStateNames = model.outputStateNames;        this.initialProbabilities = model.initialProbabilities.clone();    this.emissionMatrix = model.emissionMatrix.clone();    this.transitionMatrix = model.transitionMatrix.clone();}
0
private void initRandomParameters(long seed)
{    Random rand;        if (seed == 0) {        rand = RandomUtils.getRandom();    } else {        rand = RandomUtils.getRandom(seed);    }            double sum = 0;    for (int i = 0; i < nrOfHiddenStates; i++) {        double nextRand = rand.nextDouble();        initialProbabilities.set(i, nextRand);        sum += nextRand;    }        initialProbabilities = initialProbabilities.divide(sum);        double[] values = new double[nrOfHiddenStates];    for (int i = 0; i < nrOfHiddenStates; i++) {        sum = 0;        for (int j = 0; j < nrOfHiddenStates; j++) {            values[j] = rand.nextDouble();            sum += values[j];        }                for (int j = 0; j < nrOfHiddenStates; j++) {            values[j] /= sum;        }                transitionMatrix.set(i, values);    }        values = new double[nrOfOutputStates];    for (int i = 0; i < nrOfHiddenStates; i++) {        sum = 0;        for (int j = 0; j < nrOfOutputStates; j++) {            values[j] = rand.nextDouble();            sum += values[j];        }                for (int j = 0; j < nrOfOutputStates; j++) {            values[j] /= sum;        }                emissionMatrix.set(i, values);    }}
0
public int getNrOfHiddenStates()
{    return nrOfHiddenStates;}
0
public int getNrOfOutputStates()
{    return nrOfOutputStates;}
0
public Matrix getTransitionMatrix()
{    return transitionMatrix;}
0
public Matrix getEmissionMatrix()
{    return emissionMatrix;}
0
public Vector getInitialProbabilities()
{    return initialProbabilities;}
0
public Map<String, Integer> getHiddenStateNames()
{    return hiddenStateNames;}
0
public void registerHiddenStateNames(String[] stateNames)
{    if (stateNames != null) {        hiddenStateNames = HashBiMap.create();        for (int i = 0; i < stateNames.length; ++i) {            hiddenStateNames.put(stateNames[i], i);        }    }}
0
public void registerHiddenStateNames(Map<String, Integer> stateNames)
{    if (stateNames != null) {        hiddenStateNames = HashBiMap.create(stateNames);    }}
0
public String getHiddenStateName(int id)
{    if (hiddenStateNames == null) {        return null;    }    return hiddenStateNames.inverse().get(id);}
0
public int getHiddenStateID(String name)
{    if (hiddenStateNames == null) {        return -1;    }    Integer tmp = hiddenStateNames.get(name);    return tmp == null ? -1 : tmp;}
0
public Map<String, Integer> getOutputStateNames()
{    return outputStateNames;}
0
public void registerOutputStateNames(String[] stateNames)
{    if (stateNames != null) {        outputStateNames = HashBiMap.create();        for (int i = 0; i < stateNames.length; ++i) {            outputStateNames.put(stateNames[i], i);        }    }}
0
public void registerOutputStateNames(Map<String, Integer> stateNames)
{    if (stateNames != null) {        outputStateNames = HashBiMap.create(stateNames);    }}
0
public String getOutputStateName(int id)
{    if (outputStateNames == null) {        return null;    }    return outputStateNames.inverse().get(id);}
0
public int getOutputStateID(String name)
{    if (outputStateNames == null) {        return -1;    }    Integer tmp = outputStateNames.get(name);    return tmp == null ? -1 : tmp;}
0
public static HmmModel trainSupervised(int nrOfHiddenStates, int nrOfOutputStates, int[] observedSequence, int[] hiddenSequence, double pseudoCount)
{        pseudoCount = pseudoCount == 0 ? Double.MIN_VALUE : pseudoCount;        DenseMatrix transitionMatrix = new DenseMatrix(nrOfHiddenStates, nrOfHiddenStates);    DenseMatrix emissionMatrix = new DenseMatrix(nrOfHiddenStates, nrOfOutputStates);            transitionMatrix.assign(pseudoCount);    emissionMatrix.assign(pseudoCount);            DenseVector initialProbabilities = new DenseVector(nrOfHiddenStates);    initialProbabilities.assign(1.0 / nrOfHiddenStates);        countTransitions(transitionMatrix, emissionMatrix, observedSequence, hiddenSequence);        for (int i = 0; i < nrOfHiddenStates; i++) {                double sum = 0;        for (int j = 0; j < nrOfHiddenStates; j++) {            sum += transitionMatrix.getQuick(i, j);        }                for (int j = 0; j < nrOfHiddenStates; j++) {            transitionMatrix.setQuick(i, j, transitionMatrix.getQuick(i, j) / sum);        }                sum = 0;        for (int j = 0; j < nrOfOutputStates; j++) {            sum += emissionMatrix.getQuick(i, j);        }                for (int j = 0; j < nrOfOutputStates; j++) {            emissionMatrix.setQuick(i, j, emissionMatrix.getQuick(i, j) / sum);        }    }        return new HmmModel(transitionMatrix, emissionMatrix, initialProbabilities);}
0
private static void countTransitions(Matrix transitionMatrix, Matrix emissionMatrix, int[] observedSequence, int[] hiddenSequence)
{    emissionMatrix.setQuick(hiddenSequence[0], observedSequence[0], emissionMatrix.getQuick(hiddenSequence[0], observedSequence[0]) + 1);    for (int i = 1; i < observedSequence.length; ++i) {        transitionMatrix.setQuick(hiddenSequence[i - 1], hiddenSequence[i], transitionMatrix.getQuick(hiddenSequence[i - 1], hiddenSequence[i]) + 1);        emissionMatrix.setQuick(hiddenSequence[i], observedSequence[i], emissionMatrix.getQuick(hiddenSequence[i], observedSequence[i]) + 1);    }}
0
public static HmmModel trainSupervisedSequence(int nrOfHiddenStates, int nrOfOutputStates, Collection<int[]> hiddenSequences, Collection<int[]> observedSequences, double pseudoCount)
{        pseudoCount = pseudoCount == 0 ? Double.MIN_VALUE : pseudoCount;        DenseMatrix transitionMatrix = new DenseMatrix(nrOfHiddenStates, nrOfHiddenStates);    DenseMatrix emissionMatrix = new DenseMatrix(nrOfHiddenStates, nrOfOutputStates);    DenseVector initialProbabilities = new DenseVector(nrOfHiddenStates);        transitionMatrix.assign(pseudoCount);    emissionMatrix.assign(pseudoCount);    initialProbabilities.assign(pseudoCount);        Iterator<int[]> hiddenSequenceIt = hiddenSequences.iterator();    Iterator<int[]> observedSequenceIt = observedSequences.iterator();    while (hiddenSequenceIt.hasNext() && observedSequenceIt.hasNext()) {                int[] hiddenSequence = hiddenSequenceIt.next();        int[] observedSequence = observedSequenceIt.next();                initialProbabilities.setQuick(hiddenSequence[0], initialProbabilities.getQuick(hiddenSequence[0]) + 1);        countTransitions(transitionMatrix, emissionMatrix, observedSequence, hiddenSequence);    }            double isum = 0;    for (int i = 0; i < nrOfHiddenStates; i++) {        isum += initialProbabilities.getQuick(i);                double sum = 0;        for (int j = 0; j < nrOfHiddenStates; j++) {            sum += transitionMatrix.getQuick(i, j);        }                for (int j = 0; j < nrOfHiddenStates; j++) {            transitionMatrix.setQuick(i, j, transitionMatrix.getQuick(i, j) / sum);        }                sum = 0;        for (int j = 0; j < nrOfOutputStates; j++) {            sum += emissionMatrix.getQuick(i, j);        }                for (int j = 0; j < nrOfOutputStates; j++) {            emissionMatrix.setQuick(i, j, emissionMatrix.getQuick(i, j) / sum);        }    }        for (int i = 0; i < nrOfHiddenStates; ++i) {        initialProbabilities.setQuick(i, initialProbabilities.getQuick(i) / isum);    }        return new HmmModel(transitionMatrix, emissionMatrix, initialProbabilities);}
0
public static HmmModel trainViterbi(HmmModel initialModel, int[] observedSequence, double pseudoCount, double epsilon, int maxIterations, boolean scaled)
{        pseudoCount = pseudoCount == 0 ? Double.MIN_VALUE : pseudoCount;        HmmModel lastIteration = initialModel.clone();    HmmModel iteration = initialModel.clone();        int[] viterbiPath = new int[observedSequence.length];    int[][] phi = new int[observedSequence.length - 1][initialModel.getNrOfHiddenStates()];    double[][] delta = new double[observedSequence.length][initialModel.getNrOfHiddenStates()];        for (int i = 0; i < maxIterations; ++i) {                HmmAlgorithms.viterbiAlgorithm(viterbiPath, delta, phi, lastIteration, observedSequence, scaled);                        Matrix emissionMatrix = iteration.getEmissionMatrix();        Matrix transitionMatrix = iteration.getTransitionMatrix();                emissionMatrix.assign(pseudoCount);        transitionMatrix.assign(pseudoCount);                countTransitions(transitionMatrix, emissionMatrix, observedSequence, viterbiPath);                for (int j = 0; j < iteration.getNrOfHiddenStates(); ++j) {            double sum = 0;                        for (int k = 0; k < iteration.getNrOfHiddenStates(); ++k) {                sum += transitionMatrix.getQuick(j, k);            }            for (int k = 0; k < iteration.getNrOfHiddenStates(); ++k) {                transitionMatrix.setQuick(j, k, transitionMatrix.getQuick(j, k) / sum);            }                        sum = 0;            for (int k = 0; k < iteration.getNrOfOutputStates(); ++k) {                sum += emissionMatrix.getQuick(j, k);            }            for (int k = 0; k < iteration.getNrOfOutputStates(); ++k) {                emissionMatrix.setQuick(j, k, emissionMatrix.getQuick(j, k) / sum);            }        }                if (checkConvergence(lastIteration, iteration, epsilon)) {            break;        }                lastIteration.assign(iteration);    }        return iteration;}
0
public static HmmModel trainBaumWelch(HmmModel initialModel, int[] observedSequence, double epsilon, int maxIterations, boolean scaled)
{        HmmModel lastIteration = initialModel.clone();    HmmModel iteration = initialModel.clone();        int hiddenCount = initialModel.getNrOfHiddenStates();    int visibleCount = observedSequence.length;    Matrix alpha = new DenseMatrix(visibleCount, hiddenCount);    Matrix beta = new DenseMatrix(visibleCount, hiddenCount);        for (int it = 0; it < maxIterations; ++it) {                Vector initialProbabilities = iteration.getInitialProbabilities();        Matrix emissionMatrix = iteration.getEmissionMatrix();        Matrix transitionMatrix = iteration.getTransitionMatrix();                HmmAlgorithms.forwardAlgorithm(alpha, iteration, observedSequence, scaled);        HmmAlgorithms.backwardAlgorithm(beta, iteration, observedSequence, scaled);        if (scaled) {            logScaledBaumWelch(observedSequence, iteration, alpha, beta);        } else {            unscaledBaumWelch(observedSequence, iteration, alpha, beta);        }                        double isum = 0;        for (int j = 0; j < iteration.getNrOfHiddenStates(); ++j) {            double sum = 0;                        for (int k = 0; k < iteration.getNrOfHiddenStates(); ++k) {                sum += transitionMatrix.getQuick(j, k);            }            for (int k = 0; k < iteration.getNrOfHiddenStates(); ++k) {                transitionMatrix.setQuick(j, k, transitionMatrix.getQuick(j, k) / sum);            }                        sum = 0;            for (int k = 0; k < iteration.getNrOfOutputStates(); ++k) {                sum += emissionMatrix.getQuick(j, k);            }            for (int k = 0; k < iteration.getNrOfOutputStates(); ++k) {                emissionMatrix.setQuick(j, k, emissionMatrix.getQuick(j, k) / sum);            }                        isum += initialProbabilities.getQuick(j);        }                for (int i = 0; i < iteration.getNrOfHiddenStates(); ++i) {            initialProbabilities.setQuick(i, initialProbabilities.getQuick(i) / isum);        }                if (checkConvergence(lastIteration, iteration, epsilon)) {            break;        }                lastIteration.assign(iteration);    }        return iteration;}
0
private static void unscaledBaumWelch(int[] observedSequence, HmmModel iteration, Matrix alpha, Matrix beta)
{    Vector initialProbabilities = iteration.getInitialProbabilities();    Matrix emissionMatrix = iteration.getEmissionMatrix();    Matrix transitionMatrix = iteration.getTransitionMatrix();    double modelLikelihood = HmmEvaluator.modelLikelihood(alpha, false);    for (int i = 0; i < iteration.getNrOfHiddenStates(); ++i) {        initialProbabilities.setQuick(i, alpha.getQuick(0, i) * beta.getQuick(0, i));    }        for (int i = 0; i < iteration.getNrOfHiddenStates(); ++i) {        for (int j = 0; j < iteration.getNrOfHiddenStates(); ++j) {            double temp = 0;            for (int t = 0; t < observedSequence.length - 1; ++t) {                temp += alpha.getQuick(t, i) * emissionMatrix.getQuick(j, observedSequence[t + 1]) * beta.getQuick(t + 1, j);            }            transitionMatrix.setQuick(i, j, transitionMatrix.getQuick(i, j) * temp / modelLikelihood);        }    }        for (int i = 0; i < iteration.getNrOfHiddenStates(); ++i) {        for (int j = 0; j < iteration.getNrOfOutputStates(); ++j) {            double temp = 0;            for (int t = 0; t < observedSequence.length; ++t) {                                if (observedSequence[t] == j) {                    temp += alpha.getQuick(t, i) * beta.getQuick(t, i);                }            }            emissionMatrix.setQuick(i, j, temp / modelLikelihood);        }    }}
0
private static void logScaledBaumWelch(int[] observedSequence, HmmModel iteration, Matrix alpha, Matrix beta)
{    Vector initialProbabilities = iteration.getInitialProbabilities();    Matrix emissionMatrix = iteration.getEmissionMatrix();    Matrix transitionMatrix = iteration.getTransitionMatrix();    double modelLikelihood = HmmEvaluator.modelLikelihood(alpha, true);    for (int i = 0; i < iteration.getNrOfHiddenStates(); ++i) {        initialProbabilities.setQuick(i, Math.exp(alpha.getQuick(0, i) + beta.getQuick(0, i)));    }        for (int i = 0; i < iteration.getNrOfHiddenStates(); ++i) {        for (int j = 0; j < iteration.getNrOfHiddenStates(); ++j) {                        double sum = Double.NEGATIVE_INFINITY;            for (int t = 0; t < observedSequence.length - 1; ++t) {                double temp = alpha.getQuick(t, i) + Math.log(emissionMatrix.getQuick(j, observedSequence[t + 1])) + beta.getQuick(t + 1, j);                if (temp > Double.NEGATIVE_INFINITY) {                                        sum = temp + Math.log1p(Math.exp(sum - temp));                }            }            transitionMatrix.setQuick(i, j, transitionMatrix.getQuick(i, j) * Math.exp(sum - modelLikelihood));        }    }        for (int i = 0; i < iteration.getNrOfHiddenStates(); ++i) {        for (int j = 0; j < iteration.getNrOfOutputStates(); ++j) {                        double sum = Double.NEGATIVE_INFINITY;            for (int t = 0; t < observedSequence.length; ++t) {                                if (observedSequence[t] == j) {                    double temp = alpha.getQuick(t, i) + beta.getQuick(t, i);                    if (temp > Double.NEGATIVE_INFINITY) {                                                sum = temp + Math.log1p(Math.exp(sum - temp));                    }                }            }            emissionMatrix.setQuick(i, j, Math.exp(sum - modelLikelihood));        }    }}
0
private static boolean checkConvergence(HmmModel oldModel, HmmModel newModel, double epsilon)
{        Matrix oldTransitionMatrix = oldModel.getTransitionMatrix();    Matrix newTransitionMatrix = newModel.getTransitionMatrix();    double diff = 0;    for (int i = 0; i < oldModel.getNrOfHiddenStates(); ++i) {        for (int j = 0; j < oldModel.getNrOfHiddenStates(); ++j) {            double tmp = oldTransitionMatrix.getQuick(i, j) - newTransitionMatrix.getQuick(i, j);            diff += tmp * tmp;        }    }    double norm = Math.sqrt(diff);    diff = 0;        Matrix oldEmissionMatrix = oldModel.getEmissionMatrix();    Matrix newEmissionMatrix = newModel.getEmissionMatrix();    for (int i = 0; i < oldModel.getNrOfHiddenStates(); i++) {        for (int j = 0; j < oldModel.getNrOfOutputStates(); j++) {            double tmp = oldEmissionMatrix.getQuick(i, j) - newEmissionMatrix.getQuick(i, j);            diff += tmp * tmp;        }    }    norm += Math.sqrt(diff);        return norm < epsilon;}
0
public static Matrix getCumulativeTransitionMatrix(HmmModel model)
{        int hiddenStates = model.getNrOfHiddenStates();    Matrix transitionMatrix = model.getTransitionMatrix();        Matrix resultMatrix = new DenseMatrix(hiddenStates, hiddenStates);    for (int i = 0; i < hiddenStates; ++i) {        double sum = 0;        for (int j = 0; j < hiddenStates; ++j) {            sum += transitionMatrix.get(i, j);            resultMatrix.set(i, j, sum);        }        resultMatrix.set(i, hiddenStates - 1, 1.0);                        }    return resultMatrix;}
0
public static Matrix getCumulativeOutputMatrix(HmmModel model)
{        int hiddenStates = model.getNrOfHiddenStates();    int outputStates = model.getNrOfOutputStates();    Matrix outputMatrix = model.getEmissionMatrix();        Matrix resultMatrix = new DenseMatrix(hiddenStates, outputStates);    for (int i = 0; i < hiddenStates; ++i) {        double sum = 0;        for (int j = 0; j < outputStates; ++j) {            sum += outputMatrix.get(i, j);            resultMatrix.set(i, j, sum);        }        resultMatrix.set(i, outputStates - 1, 1.0);                    }    return resultMatrix;}
0
public static Vector getCumulativeInitialProbabilities(HmmModel model)
{        int hiddenStates = model.getNrOfHiddenStates();    Vector initialProbabilities = model.getInitialProbabilities();        Vector resultVector = new DenseVector(initialProbabilities.size());    double sum = 0;    for (int i = 0; i < hiddenStates; ++i) {        sum += initialProbabilities.get(i);        resultVector.set(i, sum);    }        resultVector.set(hiddenStates - 1, 1.0);        return resultVector;}
0
public static void validate(HmmModel model)
{    if (model == null) {                return;    }    /*     * The number of hidden states is positive.     */    Preconditions.checkArgument(model.getNrOfHiddenStates() > 0, "Error: The number of hidden states has to be greater than 0");    /*     * The number of output states is positive.     */    Preconditions.checkArgument(model.getNrOfOutputStates() > 0, "Error: The number of output states has to be greater than 0!");    /*     * The size of the vector of initial probabilities is equal to the number of     * the hidden states. Each initial probability is non-negative. The sum of     * initial probabilities is equal to 1.     */    Preconditions.checkArgument(model.getInitialProbabilities() != null && model.getInitialProbabilities().size() == model.getNrOfHiddenStates(), "Error: The vector of initial probabilities is not initialized!");    double sum = 0;    for (int i = 0; i < model.getInitialProbabilities().size(); i++) {        Preconditions.checkArgument(model.getInitialProbabilities().get(i) >= 0, "Error: Initial probability of state %d is negative", i);        sum += model.getInitialProbabilities().get(i);    }    Preconditions.checkArgument(Math.abs(sum - 1) <= 0.00001, "Error: Initial probabilities do not add up to 1");    /*     * The row size of the output matrix is equal to the number of the hidden     * states. The column size is equal to the number of output states. Each     * probability of the matrix is non-negative. The sum of each row is equal     * to 1.     */    Preconditions.checkNotNull(model.getEmissionMatrix(), "Error: The output state matrix is not initialized!");    Preconditions.checkArgument(model.getEmissionMatrix().numRows() == model.getNrOfHiddenStates() && model.getEmissionMatrix().numCols() == model.getNrOfOutputStates(), "Error: The output state matrix is not of the form nrOfHiddenStates x nrOfOutputStates");    for (int i = 0; i < model.getEmissionMatrix().numRows(); i++) {        sum = 0;        for (int j = 0; j < model.getEmissionMatrix().numCols(); j++) {            Preconditions.checkArgument(model.getEmissionMatrix().get(i, j) >= 0, "The output state probability from hidden state " + i + " to output state " + j + " is negative");            sum += model.getEmissionMatrix().get(i, j);        }        Preconditions.checkArgument(Math.abs(sum - 1) <= 0.00001, "Error: The output state probabilities for hidden state %d don't add up to 1", i);    }    /*     * The size of both dimension of the transition matrix is equal to the     * number of the hidden states. Each probability of the matrix is     * non-negative. The sum of each row in transition matrix is equal to 1.     */    Preconditions.checkArgument(model.getTransitionMatrix() != null, "Error: The hidden state matrix is not initialized!");    Preconditions.checkArgument(model.getTransitionMatrix().numRows() == model.getNrOfHiddenStates() && model.getTransitionMatrix().numCols() == model.getNrOfHiddenStates(), "Error: The output state matrix is not of the form nrOfHiddenStates x nrOfHiddenStates");    for (int i = 0; i < model.getTransitionMatrix().numRows(); i++) {        sum = 0;        for (int j = 0; j < model.getTransitionMatrix().numCols(); j++) {            Preconditions.checkArgument(model.getTransitionMatrix().get(i, j) >= 0, "Error: The transition probability from hidden state %d to hidden state %d is negative", i, j);            sum += model.getTransitionMatrix().get(i, j);        }        Preconditions.checkArgument(Math.abs(sum - 1) <= 0.00001, "Error: The transition probabilities for hidden state " + i + " don't add up to 1.");    }}
0
public static int[] encodeStateSequence(HmmModel model, Collection<String> sequence, boolean observed, int defaultValue)
{    int[] encoded = new int[sequence.size()];    Iterator<String> seqIter = sequence.iterator();    for (int i = 0; i < sequence.size(); ++i) {        String nextState = seqIter.next();        int nextID;        if (observed) {            nextID = model.getOutputStateID(nextState);        } else {            nextID = model.getHiddenStateID(nextState);        }                encoded[i] = nextID < 0 ? defaultValue : nextID;    }    return encoded;}
0
public static List<String> decodeStateSequence(HmmModel model, int[] sequence, boolean observed, String defaultValue)
{    List<String> decoded = new ArrayList<>(sequence.length);    for (int position : sequence) {        String nextState;        if (observed) {            nextState = model.getOutputStateName(position);        } else {            nextState = model.getHiddenStateName(position);        }                decoded.add(nextState == null ? defaultValue : nextState);    }    return decoded;}
0
public static void normalizeModel(HmmModel model)
{    Vector ip = model.getInitialProbabilities();    Matrix emission = model.getEmissionMatrix();    Matrix transition = model.getTransitionMatrix();        double isum = 0;    for (int i = 0; i < model.getNrOfHiddenStates(); ++i) {        isum += ip.getQuick(i);        double sum = 0;        for (int j = 0; j < model.getNrOfHiddenStates(); ++j) {            sum += transition.getQuick(i, j);        }        if (sum != 1.0) {            for (int j = 0; j < model.getNrOfHiddenStates(); ++j) {                transition.setQuick(i, j, transition.getQuick(i, j) / sum);            }        }        sum = 0;        for (int j = 0; j < model.getNrOfOutputStates(); ++j) {            sum += emission.getQuick(i, j);        }        if (sum != 1.0) {            for (int j = 0; j < model.getNrOfOutputStates(); ++j) {                emission.setQuick(i, j, emission.getQuick(i, j) / sum);            }        }    }    if (isum != 1.0) {        for (int i = 0; i < model.getNrOfHiddenStates(); ++i) {            ip.setQuick(i, ip.getQuick(i) / isum);        }    }}
0
public static HmmModel truncateModel(HmmModel model, double threshold)
{    Vector ip = model.getInitialProbabilities();    Matrix em = model.getEmissionMatrix();    Matrix tr = model.getTransitionMatrix();        RandomAccessSparseVector sparseIp = new RandomAccessSparseVector(model.getNrOfHiddenStates());    SparseMatrix sparseEm = new SparseMatrix(model.getNrOfHiddenStates(), model.getNrOfOutputStates());    SparseMatrix sparseTr = new SparseMatrix(model.getNrOfHiddenStates(), model.getNrOfHiddenStates());        for (int i = 0; i < model.getNrOfHiddenStates(); ++i) {        double value = ip.getQuick(i);        if (value > threshold) {            sparseIp.setQuick(i, value);        }        for (int j = 0; j < model.getNrOfHiddenStates(); ++j) {            value = tr.getQuick(i, j);            if (value > threshold) {                sparseTr.setQuick(i, j, value);            }        }        for (int j = 0; j < model.getNrOfOutputStates(); ++j) {            value = em.getQuick(i, j);            if (value > threshold) {                sparseEm.setQuick(i, j, value);            }        }    }        HmmModel sparseModel = new HmmModel(sparseTr, sparseEm, sparseIp);        normalizeModel(sparseModel);        sparseModel.registerHiddenStateNames(model.getHiddenStateNames());    sparseModel.registerOutputStateNames(model.getOutputStateNames());        return sparseModel;}
0
 static void serialize(HmmModel model, DataOutput output) throws IOException
{    MatrixWritable matrix = new MatrixWritable(model.getEmissionMatrix());    matrix.write(output);    matrix.set(model.getTransitionMatrix());    matrix.write(output);    VectorWritable vector = new VectorWritable(model.getInitialProbabilities());    vector.write(output);}
0
 static HmmModel deserialize(DataInput input) throws IOException
{    MatrixWritable matrix = new MatrixWritable();    matrix.readFields(input);    Matrix emissionMatrix = matrix.get();    matrix.readFields(input);    Matrix transitionMatrix = matrix.get();    VectorWritable vector = new VectorWritable();    vector.readFields(input);    Vector initialProbabilities = vector.get();    return new HmmModel(transitionMatrix, emissionMatrix, initialProbabilities);}
0
public static void main(String[] args) throws IOException
{    DefaultOptionBuilder optionBuilder = new DefaultOptionBuilder();    ArgumentBuilder argumentBuilder = new ArgumentBuilder();    Option outputOption = optionBuilder.withLongName("output").withDescription("Output file with sequence of observed states").withShortName("o").withArgument(argumentBuilder.withMaximum(1).withMinimum(1).withName("path").create()).withRequired(false).create();    Option modelOption = optionBuilder.withLongName("model").withDescription("Path to serialized HMM model").withShortName("m").withArgument(argumentBuilder.withMaximum(1).withMinimum(1).withName("path").create()).withRequired(true).create();    Option lengthOption = optionBuilder.withLongName("length").withDescription("Length of generated sequence").withShortName("l").withArgument(argumentBuilder.withMaximum(1).withMinimum(1).withName("number").create()).withRequired(true).create();    Group optionGroup = new GroupBuilder().withOption(outputOption).withOption(modelOption).withOption(lengthOption).withName("Options").create();    try {        Parser parser = new Parser();        parser.setGroup(optionGroup);        CommandLine commandLine = parser.parse(args);        String output = (String) commandLine.getValue(outputOption);        String modelPath = (String) commandLine.getValue(modelOption);        int length = Integer.parseInt((String) commandLine.getValue(lengthOption));                HmmModel model;        try (DataInputStream modelStream = new DataInputStream(new FileInputStream(modelPath))) {            model = LossyHmmSerializer.deserialize(modelStream);        }                int[] observations = HmmEvaluator.predict(model, length, System.currentTimeMillis());                try (PrintWriter writer = new PrintWriter(new OutputStreamWriter(new FileOutputStream(output), Charsets.UTF_8), true)) {            for (int observation : observations) {                writer.print(observation);                writer.print(' ');            }        }    } catch (OptionException e) {        CommandLineUtil.printHelp(optionGroup);    }}
0
public static void main(String[] args) throws IOException
{    DefaultOptionBuilder optionBuilder = new DefaultOptionBuilder();    ArgumentBuilder argumentBuilder = new ArgumentBuilder();    Option inputOption = DefaultOptionCreator.inputOption().create();    Option outputOption = DefaultOptionCreator.outputOption().create();    Option modelOption = optionBuilder.withLongName("model").withDescription("Path to serialized HMM model").withShortName("m").withArgument(argumentBuilder.withMaximum(1).withMinimum(1).withName("path").create()).withRequired(true).create();    Option likelihoodOption = optionBuilder.withLongName("likelihood").withDescription("Compute likelihood of observed sequence").withShortName("l").withRequired(false).create();    Group optionGroup = new GroupBuilder().withOption(inputOption).withOption(outputOption).withOption(modelOption).withOption(likelihoodOption).withName("Options").create();    try {        Parser parser = new Parser();        parser.setGroup(optionGroup);        CommandLine commandLine = parser.parse(args);        String input = (String) commandLine.getValue(inputOption);        String output = (String) commandLine.getValue(outputOption);        String modelPath = (String) commandLine.getValue(modelOption);        boolean computeLikelihood = commandLine.hasOption(likelihoodOption);                ;        HmmModel model;        try (DataInputStream modelStream = new DataInputStream(new FileInputStream(modelPath))) {            model = LossyHmmSerializer.deserialize(modelStream);        }                List<Integer> observations = new ArrayList<>();        try (Scanner scanner = new Scanner(new FileInputStream(input), "UTF-8")) {            while (scanner.hasNextInt()) {                observations.add(scanner.nextInt());            }        }        int[] observationsArray = new int[observations.size()];        for (int i = 0; i < observations.size(); ++i) {            observationsArray[i] = observations.get(i);        }                int[] hiddenStates = HmmEvaluator.decode(model, observationsArray, true);                try (PrintWriter writer = new PrintWriter(new OutputStreamWriter(new FileOutputStream(output), Charsets.UTF_8), true)) {            for (int hiddenState : hiddenStates) {                writer.print(hiddenState);                writer.print(' ');            }        }        if (computeLikelihood) {            System.out.println("Likelihood: " + HmmEvaluator.modelLikelihood(model, observationsArray, true));        }    } catch (OptionException e) {        CommandLineUtil.printHelp(optionGroup);    }}
0
public AbstractOnlineLogisticRegression lambda(double lambda)
{    this.lambda = lambda;    return this;}
0
public static Vector link(Vector v)
{    double max = v.maxValue();    if (max >= 40) {                        v.assign(Functions.minus(max)).assign(Functions.EXP);        return v.divide(v.norm(1));    } else {        v.assign(Functions.EXP);        return v.divide(1 + v.norm(1));    }}
0
public static double link(double r)
{    if (r < 0.0) {        double s = Math.exp(r);        return s / (1.0 + s);    } else {        double s = Math.exp(-r);        return 1.0 / (1.0 + s);    }}
0
public Vector classifyNoLink(Vector instance)
{        regularize(instance);    return beta.times(instance);}
0
public double classifyScalarNoLink(Vector instance)
{    return beta.viewRow(0).dot(instance);}
0
public Vector classify(Vector instance)
{    return link(classifyNoLink(instance));}
0
public double classifyScalar(Vector instance)
{    Preconditions.checkArgument(numCategories() == 2, "Can only call classifyScalar with two categories");        regularize(instance);        return link(classifyScalarNoLink(instance));}
0
public void train(long trackingKey, String groupKey, int actual, Vector instance)
{    unseal();    double learningRate = currentLearningRate();        regularize(instance);        Vector gradient = this.gradient.apply(groupKey, actual, instance, this);    for (int i = 0; i < numCategories - 1; i++) {        double gradientBase = gradient.get(i);                for (Element updateLocation : instance.nonZeroes()) {            int j = updateLocation.index();            double newValue = beta.getQuick(i, j) + gradientBase * learningRate * perTermLearningRate(j) * instance.get(j);            beta.setQuick(i, j, newValue);        }    }        for (Element element : instance.nonZeroes()) {        int j = element.index();        updateSteps.setQuick(j, getStep());        updateCounts.incrementQuick(j, 1);    }    nextStep();}
0
public void train(long trackingKey, int actual, Vector instance)
{    train(trackingKey, null, actual, instance);}
0
public void train(int actual, Vector instance)
{    train(0, null, actual, instance);}
0
public void regularize(Vector instance)
{    if (updateSteps == null || isSealed()) {        return;    }        double learningRate = currentLearningRate();        for (int i = 0; i < numCategories - 1; i++) {        for (Element updateLocation : instance.nonZeroes()) {            int j = updateLocation.index();            double missingUpdates = getStep() - updateSteps.get(j);            if (missingUpdates > 0) {                double rate = getLambda() * learningRate * perTermLearningRate(j);                double newValue = prior.age(beta.get(i, j), missingUpdates, rate);                beta.set(i, j, newValue);                updateSteps.set(j, getStep());            }        }    }}
0
public void setPrior(PriorFunction prior)
{    this.prior = prior;}
0
public void setGradient(Gradient gradient)
{    this.gradient = gradient;}
0
public PriorFunction getPrior()
{    return prior;}
0
public Matrix getBeta()
{    close();    return beta;}
0
public void setBeta(int i, int j, double betaIJ)
{    beta.set(i, j, betaIJ);}
0
public int numCategories()
{    return numCategories;}
0
public int numFeatures()
{    return beta.numCols();}
0
public double getLambda()
{    return lambda;}
0
public int getStep()
{    return step;}
0
protected void nextStep()
{    step++;}
0
public boolean isSealed()
{    return sealed;}
0
protected void unseal()
{    sealed = false;}
0
private void regularizeAll()
{    Vector all = new DenseVector(beta.numCols());    all.assign(1);    regularize(all);}
0
public void close()
{    if (!sealed) {        step++;        regularizeAll();        sealed = true;    }}
0
public void copyFrom(AbstractOnlineLogisticRegression other)
{        Preconditions.checkArgument(numCategories == other.numCategories, "Can't copy unless number of target categories is the same");    beta.assign(other.beta);    step = other.step;    updateSteps.assign(other.updateSteps);    updateCounts.assign(other.updateCounts);}
0
public boolean validModel()
{    double k = beta.aggregate(Functions.PLUS, new DoubleFunction() {        @Override        public double apply(double v) {            return Double.isNaN(v) || Double.isInfinite(v) ? 1 : 0;        }    });    return k < 1;}
0
public double apply(double v)
{    return Double.isNaN(v) || Double.isInfinite(v) ? 1 : 0;}
0
public void train(int actual, Vector instance)
{    train(record, null, actual, instance);}
0
public void train(long trackingKey, int actual, Vector instance)
{    train(trackingKey, null, actual, instance);}
0
public void train(long trackingKey, String groupKey, int actual, Vector instance)
{    record++;    buffer.add(new TrainingExample(trackingKey, groupKey, actual, instance));        if (buffer.size() > bufferSize) {        trainWithBufferedExamples();    }}
0
private void trainWithBufferedExamples()
{    try {        this.best = ep.parallelDo(new EvolutionaryProcess.Function<Payload<CrossFoldLearner>>() {            @Override            public double apply(Payload<CrossFoldLearner> z, double[] params) {                Wrapper x = (Wrapper) z;                for (TrainingExample example : buffer) {                    x.train(example);                }                if (x.getLearner().validModel()) {                    if (x.getLearner().numCategories() == 2) {                        return x.wrapped.auc();                    } else {                        return x.wrapped.logLikelihood();                    }                } else {                    return Double.NaN;                }            }        });    } catch (InterruptedException e) {                    } catch (ExecutionException e) {        throw new IllegalStateException(e.getCause());    }    buffer.clear();    if (record > cutoff) {        cutoff = nextStep(record);                ep.mutatePopulation(SURVIVORS);        if (freezeSurvivors) {                        for (State<Wrapper, CrossFoldLearner> state : ep.getPopulation().subList(0, SURVIVORS)) {                Wrapper.freeze(state);            }        }    }}
1
public double apply(Payload<CrossFoldLearner> z, double[] params)
{    Wrapper x = (Wrapper) z;    for (TrainingExample example : buffer) {        x.train(example);    }    if (x.getLearner().validModel()) {        if (x.getLearner().numCategories() == 2) {            return x.wrapped.auc();        } else {            return x.wrapped.logLikelihood();        }    } else {        return Double.NaN;    }}
0
public int nextStep(int recordNumber)
{    int stepSize = stepSize(recordNumber, 2.6);    if (stepSize < minInterval) {        stepSize = minInterval;    }    if (stepSize > maxInterval) {        stepSize = maxInterval;    }    int newCutoff = stepSize * (recordNumber / stepSize + 1);    if (newCutoff < cutoff + currentStep) {        newCutoff = cutoff + currentStep;    } else {        this.currentStep = stepSize;    }    return newCutoff;}
0
public static int stepSize(int recordNumber, double multiplier)
{    int[] bumps = { 1, 2, 5 };    double log = Math.floor(multiplier * Math.log10(recordNumber));    int bump = bumps[(int) log % bumps.length];    int scale = (int) Math.pow(10, Math.floor(log / bumps.length));    return bump * scale;}
0
public void close()
{    trainWithBufferedExamples();    try {        ep.parallelDo(new EvolutionaryProcess.Function<Payload<CrossFoldLearner>>() {            @Override            public double apply(Payload<CrossFoldLearner> payload, double[] params) {                CrossFoldLearner learner = ((Wrapper) payload).getLearner();                learner.close();                return learner.logLikelihood();            }        });    } catch (InterruptedException e) {            } catch (ExecutionException e) {        throw new IllegalStateException(e);    } finally {        ep.close();    }}
1
public double apply(Payload<CrossFoldLearner> payload, double[] params)
{    CrossFoldLearner learner = ((Wrapper) payload).getLearner();    learner.close();    return learner.logLikelihood();}
0
public void setInterval(int interval)
{    setInterval(interval, interval);}
0
public void setInterval(int minInterval, int maxInterval)
{    this.minInterval = Math.max(200, minInterval);    this.maxInterval = Math.max(200, maxInterval);    this.cutoff = minInterval * (record / minInterval + 1);    this.currentStep = minInterval;    bufferSize = Math.min(minInterval, bufferSize);}
0
public final void setPoolSize(int poolSize)
{    this.poolSize = poolSize;    setupOptimizer(poolSize);}
0
public void setThreadCount(int threadCount)
{    this.threadCount = threadCount;    setupOptimizer(poolSize);}
0
public void setAucEvaluator(OnlineAuc auc)
{    seed.getPayload().setAucEvaluator(auc);    setupOptimizer(poolSize);}
0
private void setupOptimizer(int poolSize)
{    ep = new EvolutionaryProcess<>(threadCount, poolSize, seed);}
0
public int numFeatures()
{    return numFeatures;}
0
public double auc()
{    if (best == null) {        return Double.NaN;    } else {        Wrapper payload = best.getPayload();        return payload.getLearner().auc();    }}
0
public State<Wrapper, CrossFoldLearner> getBest()
{    return best;}
0
public void setBest(State<Wrapper, CrossFoldLearner> best)
{    this.best = best;}
0
public int getRecord()
{    return record;}
0
public void setRecord(int record)
{    this.record = record;}
0
public int getMinInterval()
{    return minInterval;}
0
public int getMaxInterval()
{    return maxInterval;}
0
public int getNumCategories()
{    return seed.getPayload().getLearner().numCategories();}
0
public PriorFunction getPrior()
{    return seed.getPayload().getLearner().getPrior();}
0
public void setBuffer(List<TrainingExample> buffer)
{    this.buffer = buffer;}
0
public List<TrainingExample> getBuffer()
{    return buffer;}
0
public EvolutionaryProcess<Wrapper, CrossFoldLearner> getEp()
{    return ep;}
0
public void setEp(EvolutionaryProcess<Wrapper, CrossFoldLearner> ep)
{    this.ep = ep;}
0
public State<Wrapper, CrossFoldLearner> getSeed()
{    return seed;}
0
public void setSeed(State<Wrapper, CrossFoldLearner> seed)
{    this.seed = seed;}
0
public int getNumFeatures()
{    return numFeatures;}
0
public void setAveragingWindow(int averagingWindow)
{    seed.getPayload().getLearner().setWindowSize(averagingWindow);    setupOptimizer(poolSize);}
0
public void setFreezeSurvivors(boolean freezeSurvivors)
{    this.freezeSurvivors = freezeSurvivors;}
0
public Wrapper copy()
{    Wrapper r = new Wrapper();    r.wrapped = wrapped.copy();    return r;}
0
public void update(double[] params)
{    int i = 0;    wrapped.lambda(params[i++]);    wrapped.learningRate(params[i]);    wrapped.stepOffset(1);    wrapped.alpha(1);    wrapped.decayExponent(0);}
0
public static void freeze(State<Wrapper, CrossFoldLearner> s)
{        double[] params = s.getParams();    params[1] -= 10;        s.setOmni(s.getOmni() / 20);    double[] step = s.getStep();    for (int i = 0; i < step.length; i++) {        step[i] /= 20;    }}
0
public static void setMappings(State<Wrapper, CrossFoldLearner> x)
{    int i = 0;        x.setMap(i++, Mapping.logLimit(1.0e-8, 0.1));        x.setMap(i, Mapping.logLimit(1.0e-8, 1));}
0
public void train(TrainingExample example)
{    wrapped.train(example.getKey(), example.getGroupKey(), example.getActual(), example.getInstance());}
0
public CrossFoldLearner getLearner()
{    return wrapped;}
0
public String toString()
{    return String.format(Locale.ENGLISH, "auc=%.2f", wrapped.auc());}
0
public void setAucEvaluator(OnlineAuc auc)
{    wrapped.setAucEvaluator(auc);}
0
public void write(DataOutput out) throws IOException
{    wrapped.write(out);}
0
public void readFields(DataInput input) throws IOException
{    wrapped = new CrossFoldLearner();    wrapped.readFields(input);}
0
public long getKey()
{    return key;}
0
public int getActual()
{    return actual;}
0
public Vector getInstance()
{    return instance;}
0
public String getGroupKey()
{    return groupKey;}
0
public void write(DataOutput out) throws IOException
{    out.writeLong(key);    if (groupKey != null) {        out.writeBoolean(true);        out.writeUTF(groupKey);    } else {        out.writeBoolean(false);    }    out.writeInt(actual);    VectorWritable.writeVector(out, instance, true);}
0
public void readFields(DataInput in) throws IOException
{    key = in.readLong();    if (in.readBoolean()) {        groupKey = in.readUTF();    }    actual = in.readInt();    instance = VectorWritable.readVector(in);}
0
public void write(DataOutput out) throws IOException
{    out.writeInt(record);    out.writeInt(cutoff);    out.writeInt(minInterval);    out.writeInt(maxInterval);    out.writeInt(currentStep);    out.writeInt(bufferSize);    out.writeInt(buffer.size());    for (TrainingExample example : buffer) {        example.write(out);    }    ep.write(out);    best.write(out);    out.writeInt(threadCount);    out.writeInt(poolSize);    seed.write(out);    out.writeInt(numFeatures);    out.writeBoolean(freezeSurvivors);}
0
public void readFields(DataInput in) throws IOException
{    record = in.readInt();    cutoff = in.readInt();    minInterval = in.readInt();    maxInterval = in.readInt();    currentStep = in.readInt();    bufferSize = in.readInt();    int n = in.readInt();    buffer = new ArrayList<>();    for (int i = 0; i < n; i++) {        TrainingExample example = new TrainingExample();        example.readFields(in);        buffer.add(example);    }    ep = new EvolutionaryProcess<>();    ep.readFields(in);    best = new State<>();    best.readFields(in);    threadCount = in.readInt();    poolSize = in.readInt();    seed = new State<>();    seed.readFields(in);    numFeatures = in.readInt();    freezeSurvivors = in.readBoolean();}
0
public CrossFoldLearner lambda(double v)
{    for (OnlineLogisticRegression model : models) {        model.lambda(v);    }    return this;}
0
public CrossFoldLearner learningRate(double x)
{    for (OnlineLogisticRegression model : models) {        model.learningRate(x);    }    return this;}
0
public CrossFoldLearner stepOffset(int x)
{    for (OnlineLogisticRegression model : models) {        model.stepOffset(x);    }    return this;}
0
public CrossFoldLearner decayExponent(double x)
{    for (OnlineLogisticRegression model : models) {        model.decayExponent(x);    }    return this;}
0
public CrossFoldLearner alpha(double alpha)
{    for (OnlineLogisticRegression model : models) {        model.alpha(alpha);    }    return this;}
0
public void train(int actual, Vector instance)
{    train(record, null, actual, instance);}
0
public void train(long trackingKey, int actual, Vector instance)
{    train(trackingKey, null, actual, instance);}
0
public void train(long trackingKey, String groupKey, int actual, Vector instance)
{    record++;    int k = 0;    for (OnlineLogisticRegression model : models) {        if (k == mod(trackingKey, models.size())) {            Vector v = model.classifyFull(instance);            double score = Math.max(v.get(actual), MIN_SCORE);            logLikelihood += (Math.log(score) - logLikelihood) / Math.min(record, windowSize);            int correct = v.maxValueIndex() == actual ? 1 : 0;            percentCorrect += (correct - percentCorrect) / Math.min(record, windowSize);            if (numCategories() == 2) {                auc.addSample(actual, groupKey, v.get(1));            }        } else {            model.train(trackingKey, groupKey, actual, instance);        }        k++;    }}
0
private static long mod(long x, int y)
{    long r = x % y;    return r < 0 ? r + y : r;}
0
public void close()
{    for (OnlineLogisticRegression m : models) {        m.close();    }}
0
public void resetLineCounter()
{    record = 0;}
0
public boolean validModel()
{    boolean r = true;    for (OnlineLogisticRegression model : models) {        r &= model.validModel();    }    return r;}
0
public Vector classify(Vector instance)
{    Vector r = new DenseVector(numCategories() - 1);    DoubleDoubleFunction scale = Functions.plusMult(1.0 / models.size());    for (OnlineLogisticRegression model : models) {        r.assign(model.classify(instance), scale);    }    return r;}
0
public Vector classifyNoLink(Vector instance)
{    Vector r = new DenseVector(numCategories() - 1);    DoubleDoubleFunction scale = Functions.plusMult(1.0 / models.size());    for (OnlineLogisticRegression model : models) {        r.assign(model.classifyNoLink(instance), scale);    }    return r;}
0
public double classifyScalar(Vector instance)
{    double r = 0;    int n = 0;    for (OnlineLogisticRegression model : models) {        n++;        r += model.classifyScalar(instance);    }    return r / n;}
0
public int numCategories()
{    return models.get(0).numCategories();}
0
public double auc()
{    return auc.auc();}
0
public double logLikelihood()
{    return logLikelihood;}
0
public double percentCorrect()
{    return percentCorrect;}
0
public CrossFoldLearner copy()
{    CrossFoldLearner r = new CrossFoldLearner(models.size(), numCategories(), numFeatures, prior);    r.models.clear();    for (OnlineLogisticRegression model : models) {        model.close();        OnlineLogisticRegression newModel = new OnlineLogisticRegression(model.numCategories(), model.numFeatures(), model.prior);        newModel.copyFrom(model);        r.models.add(newModel);    }    return r;}
0
public int getRecord()
{    return record;}
0
public void setRecord(int record)
{    this.record = record;}
0
public OnlineAuc getAucEvaluator()
{    return auc;}
0
public void setAucEvaluator(OnlineAuc auc)
{    this.auc = auc;}
0
public double getLogLikelihood()
{    return logLikelihood;}
0
public void setLogLikelihood(double logLikelihood)
{    this.logLikelihood = logLikelihood;}
0
public List<OnlineLogisticRegression> getModels()
{    return models;}
0
public void addModel(OnlineLogisticRegression model)
{    models.add(model);}
0
public double[] getParameters()
{    return parameters;}
0
public void setParameters(double[] parameters)
{    this.parameters = parameters;}
0
public int getNumFeatures()
{    return numFeatures;}
0
public void setNumFeatures(int numFeatures)
{    this.numFeatures = numFeatures;}
0
public void setWindowSize(int windowSize)
{    this.windowSize = windowSize;    auc.setWindowSize(windowSize);}
0
public PriorFunction getPrior()
{    return prior;}
0
public void setPrior(PriorFunction prior)
{    this.prior = prior;}
0
public void write(DataOutput out) throws IOException
{    out.writeInt(record);    PolymorphicWritable.write(out, auc);    out.writeDouble(logLikelihood);    out.writeInt(models.size());    for (OnlineLogisticRegression model : models) {        model.write(out);    }    for (double x : parameters) {        out.writeDouble(x);    }    out.writeInt(numFeatures);    PolymorphicWritable.write(out, prior);    out.writeDouble(percentCorrect);    out.writeInt(windowSize);}
0
public void readFields(DataInput in) throws IOException
{    record = in.readInt();    auc = PolymorphicWritable.read(in, OnlineAuc.class);    logLikelihood = in.readDouble();    int n = in.readInt();    for (int i = 0; i < n; i++) {        OnlineLogisticRegression olr = new OnlineLogisticRegression();        olr.readFields(in);        models.add(olr);    }    parameters = new double[4];    for (int i = 0; i < 4; i++) {        parameters[i] = in.readDouble();    }    numFeatures = in.readInt();    prior = PolymorphicWritable.read(in, PriorFunction.class);    percentCorrect = in.readDouble();    windowSize = in.readInt();}
0
private List<String> parseCsvLine(String line)
{    try {        return Arrays.asList(CSVUtils.parseLine(line));    } catch (IOException e) {        List<String> list = new ArrayList<>();        list.add(line);        return list;    }}
0
private List<String> parseCsvLine(CharSequence line)
{    return parseCsvLine(line.toString());}
0
public void defineTargetCategories(List<String> values)
{    Preconditions.checkArgument(values.size() <= maxTargetValue, "Must have less than or equal to " + maxTargetValue + " categories for target variable, but found " + values.size());    if (maxTargetValue == Integer.MAX_VALUE) {        maxTargetValue = values.size();    }    for (String value : values) {        targetDictionary.intern(value);    }}
0
public CsvRecordFactory maxTargetValue(int max)
{    maxTargetValue = max;    return this;}
0
public boolean usesFirstLineAsSchema()
{    return true;}
0
public void firstLine(String line)
{        final Map<String, Integer> vars = new HashMap<>();    variableNames = parseCsvLine(line);    int column = 0;    for (String var : variableNames) {        vars.put(var, column++);    }        target = vars.get(targetName);        if (idName != null) {        id = vars.get(idName);    }        predictors = new ArrayList<>(Collections2.transform(typeMap.keySet(), new Function<String, Integer>() {        @Override        public Integer apply(String from) {            Integer r = vars.get(from);            Preconditions.checkArgument(r != null, "Can't find variable %s, only know about %s", from, vars);            return r;        }    }));    if (includeBiasTerm) {        predictors.add(-1);    }    Collections.sort(predictors);        predictorEncoders = new HashMap<>();    for (Integer predictor : predictors) {        String name;        Class<? extends FeatureVectorEncoder> c;        if (predictor == -1) {            name = INTERCEPT_TERM;            c = ConstantValueEncoder.class;        } else {            name = variableNames.get(predictor);            c = TYPE_DICTIONARY.get(typeMap.get(name));        }        try {            Preconditions.checkArgument(c != null, "Invalid type of variable %s,  wanted one of %s", typeMap.get(name), TYPE_DICTIONARY.keySet());            Constructor<? extends FeatureVectorEncoder> constructor = c.getConstructor(String.class);            Preconditions.checkArgument(constructor != null, "Can't find correct constructor for %s", typeMap.get(name));            FeatureVectorEncoder encoder = constructor.newInstance(name);            predictorEncoders.put(predictor, encoder);            encoder.setTraceDictionary(traceDictionary);        } catch (InstantiationException e) {            throw new IllegalStateException(CANNOT_CONSTRUCT_CONVERTER, e);        } catch (IllegalAccessException e) {            throw new IllegalStateException(CANNOT_CONSTRUCT_CONVERTER, e);        } catch (InvocationTargetException e) {            throw new IllegalStateException(CANNOT_CONSTRUCT_CONVERTER, e);        } catch (NoSuchMethodException e) {            throw new IllegalStateException(CANNOT_CONSTRUCT_CONVERTER, e);        }    }}
0
public Integer apply(String from)
{    Integer r = vars.get(from);    Preconditions.checkArgument(r != null, "Can't find variable %s, only know about %s", from, vars);    return r;}
0
public int processLine(String line, Vector featureVector)
{    List<String> values = parseCsvLine(line);    int targetValue = targetDictionary.intern(values.get(target));    if (targetValue >= maxTargetValue) {        targetValue = maxTargetValue - 1;    }    for (Integer predictor : predictors) {        String value;        if (predictor >= 0) {            value = values.get(predictor);        } else {            value = null;        }        predictorEncoders.get(predictor).addToVector(value, featureVector);    }    return targetValue;}
0
public int processLine(CharSequence line, Vector featureVector, boolean returnTarget)
{    List<String> values = parseCsvLine(line);    int targetValue = -1;    if (returnTarget) {        targetValue = targetDictionary.intern(values.get(target));        if (targetValue >= maxTargetValue) {            targetValue = maxTargetValue - 1;        }    }    for (Integer predictor : predictors) {        String value = predictor >= 0 ? values.get(predictor) : null;        predictorEncoders.get(predictor).addToVector(value, featureVector);    }    return targetValue;}
0
public String getTargetString(CharSequence line)
{    List<String> values = parseCsvLine(line);    return values.get(target);}
0
public String getTargetLabel(int code)
{    for (String key : targetDictionary.values()) {        if (targetDictionary.intern(key) == code) {            return key;        }    }    return null;}
0
public String getIdString(CharSequence line)
{    List<String> values = parseCsvLine(line);    return values.get(id);}
0
public Iterable<String> getPredictors()
{    return Lists.transform(predictors, new Function<Integer, String>() {        @Override        public String apply(Integer v) {            if (v >= 0) {                return variableNames.get(v);            } else {                return INTERCEPT_TERM;            }        }    });}
0
public String apply(Integer v)
{    if (v >= 0) {        return variableNames.get(v);    } else {        return INTERCEPT_TERM;    }}
0
public Map<String, Set<Integer>> getTraceDictionary()
{    return traceDictionary;}
0
public CsvRecordFactory includeBiasTerm(boolean useBias)
{    includeBiasTerm = useBias;    return this;}
0
public List<String> getTargetCategories()
{    List<String> r = targetDictionary.values();    if (r.size() > maxTargetValue) {        r.subList(maxTargetValue, r.size()).clear();    }    return r;}
0
public String getIdName()
{    return idName;}
0
public void setIdName(String idName)
{    this.idName = idName;}
0
public final Vector apply(String groupKey, int actual, Vector instance, AbstractVectorClassifier classifier)
{        Vector v = classifier.classify(instance);    Vector r = v.like();    if (actual != 0) {        r.setQuick(actual - 1, 1);    }    r.assign(v, Functions.MINUS);    return r;}
0
public double age(double oldValue, double generations, double learningRate)
{    oldValue *= Math.pow(1 - alphaByLambda * learningRate, generations);    double newValue = oldValue - Math.signum(oldValue) * learningRate * generations;    if (newValue * oldValue < 0.0) {                return 0.0;    } else {        return newValue;    }}
0
public double logP(double betaIJ)
{    return l1.logP(betaIJ) + alphaByLambda * l2.logP(betaIJ);}
0
public void write(DataOutput out) throws IOException
{    out.writeDouble(alphaByLambda);    l1.write(out);    l2.write(out);}
0
public void readFields(DataInput in) throws IOException
{    alphaByLambda = in.readDouble();    l1 = new L1();    l1.readFields(in);    l2 = new L2();    l2.readFields(in);}
0
public void initWeights(Random gen)
{    double hiddenFanIn = 1.0 / Math.sqrt(numFeatures);    for (int i = 0; i < numHidden; i++) {        for (int j = 0; j < numFeatures; j++) {            double val = (2.0 * gen.nextDouble() - 1.0) * hiddenFanIn;            hiddenWeights[i].setQuick(j, val);        }    }    double outputFanIn = 1.0 / Math.sqrt(numHidden);    for (int i = 0; i < numOutput; i++) {        for (int j = 0; j < numHidden; j++) {            double val = (2.0 * gen.nextDouble() - 1.0) * outputFanIn;            outputWeights[i].setQuick(j, val);        }    }}
0
public GradientMachine learningRate(double learningRate)
{    this.learningRate = learningRate;    return this;}
0
public GradientMachine regularization(double regularization)
{    this.regularization = regularization;    return this;}
0
public GradientMachine sparsity(double sparsity)
{    this.sparsity = sparsity;    return this;}
0
public GradientMachine sparsityLearningRate(double sparsityLearningRate)
{    this.sparsityLearningRate = sparsityLearningRate;    return this;}
0
public void copyFrom(GradientMachine other)
{    numFeatures = other.numFeatures;    numHidden = other.numHidden;    numOutput = other.numOutput;    learningRate = other.learningRate;    regularization = other.regularization;    sparsity = other.sparsity;    sparsityLearningRate = other.sparsityLearningRate;    hiddenWeights = new DenseVector[numHidden];    for (int i = 0; i < numHidden; i++) {        hiddenWeights[i] = other.hiddenWeights[i].clone();    }    hiddenBias = other.hiddenBias.clone();    outputWeights = new DenseVector[numOutput];    for (int i = 0; i < numOutput; i++) {        outputWeights[i] = other.outputWeights[i].clone();    }    outputBias = other.outputBias.clone();}
0
public int numCategories()
{    return numOutput;}
0
public int numFeatures()
{    return numFeatures;}
0
public int numHidden()
{    return numHidden;}
0
public DenseVector inputToHidden(Vector input)
{    DenseVector activations = new DenseVector(numHidden);    for (int i = 0; i < numHidden; i++) {        activations.setQuick(i, hiddenWeights[i].dot(input));    }    activations.assign(hiddenBias, Functions.PLUS);    activations.assign(Functions.min(40.0)).assign(Functions.max(-40));    activations.assign(Functions.SIGMOID);    return activations;}
0
public DenseVector hiddenToOutput(Vector hiddenActivation)
{    DenseVector activations = new DenseVector(numOutput);    for (int i = 0; i < numOutput; i++) {        activations.setQuick(i, outputWeights[i].dot(hiddenActivation));    }    activations.assign(outputBias, Functions.PLUS);    return activations;}
0
public void updateRanking(Vector hiddenActivation, Collection<Integer> goodLabels, int numTrials, Random gen)
{        if (goodLabels.size() >= numOutput) {        return;    }    for (Integer good : goodLabels) {        double goodScore = outputWeights[good].dot(hiddenActivation);        int highestBad = -1;        double highestBadScore = Double.NEGATIVE_INFINITY;        for (int i = 0; i < numTrials; i++) {            int bad = gen.nextInt(numOutput);            while (goodLabels.contains(bad)) {                bad = gen.nextInt(numOutput);            }            double badScore = outputWeights[bad].dot(hiddenActivation);            if (badScore > highestBadScore) {                highestBadScore = badScore;                highestBad = bad;            }        }        int bad = highestBad;        double loss = 1.0 - goodScore + highestBadScore;        if (loss < 0.0) {            continue;        }                                                        Vector gradGood = outputWeights[good].clone();        gradGood.assign(Functions.NEGATE);        Vector propHidden = gradGood.clone();        Vector gradBad = outputWeights[bad].clone();        propHidden.assign(gradBad, Functions.PLUS);        gradGood.assign(Functions.mult(-learningRate * (1.0 - regularization)));        outputWeights[good].assign(gradGood, Functions.PLUS);        gradBad.assign(Functions.mult(-learningRate * (1.0 + regularization)));        outputWeights[bad].assign(gradBad, Functions.PLUS);        outputBias.setQuick(good, outputBias.get(good) + learningRate);        outputBias.setQuick(bad, outputBias.get(bad) - learningRate);                Vector gradSig = hiddenActivation.clone();        gradSig.assign(Functions.SIGMOIDGRADIENT);                for (int i = 0; i < numHidden; i++) {            gradSig.setQuick(i, gradSig.get(i) * propHidden.get(i));        }        for (int i = 0; i < numHidden; i++) {            for (int j = 0; j < numFeatures; j++) {                double v = hiddenWeights[i].get(j);                v -= learningRate * (gradSig.get(i) + regularization * v);                hiddenWeights[i].setQuick(j, v);            }        }    }}
0
public Vector classify(Vector instance)
{    Vector result = classifyNoLink(instance);        int max = result.maxValueIndex();    result.assign(0);    result.setQuick(max, 1.0);    return result.viewPart(1, result.size() - 1);}
0
public Vector classifyNoLink(Vector instance)
{    DenseVector hidden = inputToHidden(instance);    return hiddenToOutput(hidden);}
0
public double classifyScalar(Vector instance)
{    Vector output = classifyNoLink(instance);    if (output.get(0) > output.get(1)) {        return 0;    }    return 1;}
0
public GradientMachine copy()
{    close();    GradientMachine r = new GradientMachine(numFeatures(), numHidden(), numCategories());    r.copyFrom(this);    return r;}
0
public void write(DataOutput out) throws IOException
{    out.writeInt(WRITABLE_VERSION);    out.writeDouble(learningRate);    out.writeDouble(regularization);    out.writeDouble(sparsity);    out.writeDouble(sparsityLearningRate);    out.writeInt(numFeatures);    out.writeInt(numHidden);    out.writeInt(numOutput);    VectorWritable.writeVector(out, hiddenBias);    for (int i = 0; i < numHidden; i++) {        VectorWritable.writeVector(out, hiddenWeights[i]);    }    VectorWritable.writeVector(out, outputBias);    for (int i = 0; i < numOutput; i++) {        VectorWritable.writeVector(out, outputWeights[i]);    }}
0
public void readFields(DataInput in) throws IOException
{    int version = in.readInt();    if (version == WRITABLE_VERSION) {        learningRate = in.readDouble();        regularization = in.readDouble();        sparsity = in.readDouble();        sparsityLearningRate = in.readDouble();        numFeatures = in.readInt();        numHidden = in.readInt();        numOutput = in.readInt();        hiddenWeights = new DenseVector[numHidden];        hiddenBias = VectorWritable.readVector(in);        for (int i = 0; i < numHidden; i++) {            hiddenWeights[i] = VectorWritable.readVector(in);        }        outputWeights = new DenseVector[numOutput];        outputBias = VectorWritable.readVector(in);        for (int i = 0; i < numOutput; i++) {            outputWeights[i] = VectorWritable.readVector(in);        }    } else {        throw new IOException("Incorrect object version, wanted " + WRITABLE_VERSION + " got " + version);    }}
0
public void close()
{}
0
public void train(long trackingKey, String groupKey, int actual, Vector instance)
{    Vector hiddenActivation = inputToHidden(instance);    hiddenToOutput(hiddenActivation);    Collection<Integer> goodLabels = new HashSet<>();    goodLabels.add(actual);    updateRanking(hiddenActivation, goodLabels, 2, rnd);}
0
public void train(long trackingKey, int actual, Vector instance)
{    train(trackingKey, null, actual, instance);}
0
public void train(int actual, Vector instance)
{    train(0, null, actual, instance);}
0
public double age(double oldValue, double generations, double learningRate)
{    double newValue = oldValue - Math.signum(oldValue) * learningRate * generations;    if (newValue * oldValue < 0) {                return 0;    } else {        return newValue;    }}
0
public double logP(double betaIJ)
{    return -Math.abs(betaIJ);}
0
public void write(DataOutput out) throws IOException
{}
0
public void readFields(DataInput dataInput) throws IOException
{}
0
public double age(double oldValue, double generations, double learningRate)
{    return oldValue * Math.pow(1.0 - learningRate / s2, generations);}
0
public double logP(double betaIJ)
{    return -betaIJ * betaIJ / s2 / 2.0 - Math.log(s) - HALF_LOG_2PI;}
0
public void write(DataOutput out) throws IOException
{    out.writeDouble(s2);    out.writeDouble(s);}
0
public void readFields(DataInput in) throws IOException
{    s2 = in.readDouble();    s = in.readDouble();}
0
public Vector apply(String groupKey, int actual, Vector instance, AbstractVectorClassifier classifier)
{    if (random.nextDouble() < alpha) {                if (!hasZero || !hasOne) {            throw new IllegalStateException();        }        return rank.apply(groupKey, actual, instance, classifier);    } else {        hasZero |= actual == 0;        hasOne |= actual == 1;                rank.addToHistory(actual, instance);        return basic.apply(groupKey, actual, instance, classifier);    }}
0
public void update(Vector features, Map<String, Set<Integer>> traceDictionary, AbstractVectorClassifier learner)
{        features.assign(0);    for (Map.Entry<String, Set<Integer>> entry : traceDictionary.entrySet()) {                String key = entry.getKey();        Set<Integer> value = entry.getValue();                if (!weightMap.containsKey(key)) {                        for (Integer where : value) {                features.set(where, 1);            }                        Vector v = learner.classifyNoLink(features);            weightMap.put(key, v);                        for (Integer where : value) {                features.set(where, 0);            }        }    }}
0
public List<Weight> summary(int n)
{    Queue<Weight> pq = new PriorityQueue<>();    for (Map.Entry<String, Vector> entry : weightMap.entrySet()) {        pq.add(new Weight(entry.getKey(), entry.getValue()));        while (pq.size() > n) {            pq.poll();        }    }    List<Weight> r = new ArrayList<>(pq);    Collections.sort(r, Ordering.natural().reverse());    return r;}
0
public int compareTo(Category o)
{    int r = Double.compare(Math.abs(weight), Math.abs(o.weight));    if (r == 0) {        if (o.index < index) {            return -1;        }        if (o.index > index) {            return 1;        }        return 0;    }    return r;}
0
public boolean equals(Object o)
{    if (!(o instanceof Category)) {        return false;    }    Category other = (Category) o;    return index == other.index && weight == other.weight;}
0
public int hashCode()
{    return RandomUtils.hashDouble(weight) ^ index;}
0
public int compareTo(Weight other)
{    int r = Double.compare(Math.abs(this.value), Math.abs(other.value));    if (r == 0) {        return feature.compareTo(other.feature);    }    return r;}
0
public boolean equals(Object o)
{    if (!(o instanceof Weight)) {        return false;    }    Weight other = (Weight) o;    return feature.equals(other.feature) && value == other.value && maxIndex == other.maxIndex && categories.equals(other.categories);}
0
public int hashCode()
{    return feature.hashCode() ^ RandomUtils.hashDouble(value) ^ maxIndex ^ categories.hashCode();}
0
public String getFeature()
{    return feature;}
0
public double getWeight()
{    return value;}
0
public double getWeight(int n)
{    return categories.get(n).weight;}
0
public double getCategory(int n)
{    return categories.get(n).index;}
0
public int getMaxImpact()
{    return maxIndex;}
0
public static void writeBinary(String path, CrossFoldLearner model) throws IOException
{    try (DataOutputStream out = new DataOutputStream(new FileOutputStream(path))) {        PolymorphicWritable.write(out, model);    }}
0
public static void writeBinary(String path, OnlineLogisticRegression model) throws IOException
{    try (DataOutputStream out = new DataOutputStream(new FileOutputStream(path))) {        PolymorphicWritable.write(out, model);    }}
0
public static void writeBinary(String path, AdaptiveLogisticRegression model) throws IOException
{    try (DataOutputStream out = new DataOutputStream(new FileOutputStream(path))) {        PolymorphicWritable.write(out, model);    }}
0
public static T readBinary(InputStream in, Class<T> clazz) throws IOException
{    DataInput dataIn = new DataInputStream(in);    try {        return PolymorphicWritable.read(dataIn, clazz);    } finally {        Closeables.close(in, false);    }}
0
public OnlineLogisticRegression alpha(double alpha)
{    this.decayFactor = alpha;    return this;}
0
public OnlineLogisticRegression lambda(double lambda)
{        super.lambda(lambda);    return this;}
0
public OnlineLogisticRegression learningRate(double learningRate)
{    this.mu0 = learningRate;    return this;}
0
public OnlineLogisticRegression stepOffset(int stepOffset)
{    this.stepOffset = stepOffset;    return this;}
0
public OnlineLogisticRegression decayExponent(double decayExponent)
{    if (decayExponent > 0) {        decayExponent = -decayExponent;    }    this.forgettingExponent = decayExponent;    return this;}
0
public double perTermLearningRate(int j)
{    return Math.sqrt(perTermAnnealingOffset / updateCounts.get(j));}
0
public double currentLearningRate()
{    return mu0 * Math.pow(decayFactor, getStep()) * Math.pow(getStep() + stepOffset, forgettingExponent);}
0
public void copyFrom(OnlineLogisticRegression other)
{    super.copyFrom(other);    mu0 = other.mu0;    decayFactor = other.decayFactor;    stepOffset = other.stepOffset;    forgettingExponent = other.forgettingExponent;    perTermAnnealingOffset = other.perTermAnnealingOffset;}
0
public OnlineLogisticRegression copy()
{    close();    OnlineLogisticRegression r = new OnlineLogisticRegression(numCategories(), numFeatures(), prior);    r.copyFrom(this);    return r;}
0
public void write(DataOutput out) throws IOException
{    out.writeInt(WRITABLE_VERSION);    out.writeDouble(mu0);    out.writeDouble(getLambda());    out.writeDouble(decayFactor);    out.writeInt(stepOffset);    out.writeInt(step);    out.writeDouble(forgettingExponent);    out.writeInt(perTermAnnealingOffset);    out.writeInt(numCategories);    MatrixWritable.writeMatrix(out, beta);    PolymorphicWritable.write(out, prior);    VectorWritable.writeVector(out, updateCounts);    VectorWritable.writeVector(out, updateSteps);}
0
public void readFields(DataInput in) throws IOException
{    int version = in.readInt();    if (version == WRITABLE_VERSION) {        mu0 = in.readDouble();        lambda(in.readDouble());        decayFactor = in.readDouble();        stepOffset = in.readInt();        step = in.readInt();        forgettingExponent = in.readDouble();        perTermAnnealingOffset = in.readInt();        numCategories = in.readInt();        beta = MatrixWritable.readMatrix(in);        prior = PolymorphicWritable.read(in, PriorFunction.class);        updateCounts = VectorWritable.readVector(in);        updateSteps = VectorWritable.readVector(in);    } else {        throw new IOException("Incorrect object version, wanted " + WRITABLE_VERSION + " got " + version);    }}
0
public PassiveAggressive learningRate(double learningRate)
{    this.learningRate = learningRate;    return this;}
0
public void copyFrom(PassiveAggressive other)
{    learningRate = other.learningRate;    numCategories = other.numCategories;    weights = other.weights;}
0
public int numCategories()
{    return numCategories;}
0
public Vector classify(Vector instance)
{    Vector result = classifyNoLink(instance);        double max = result.maxValue();    result.assign(Functions.minus(max)).assign(Functions.EXP);    result = result.divide(result.norm(1));    return result.viewPart(1, result.size() - 1);}
0
public Vector classifyNoLink(Vector instance)
{    Vector result = new DenseVector(weights.numRows());    result.assign(0);    for (int i = 0; i < weights.numRows(); i++) {        result.setQuick(i, weights.viewRow(i).dot(instance));    }    return result;}
0
public double classifyScalar(Vector instance)
{    double v1 = weights.viewRow(0).dot(instance);    double v2 = weights.viewRow(1).dot(instance);    v1 = Math.exp(v1);    v2 = Math.exp(v2);    return v2 / (v1 + v2);}
0
public int numFeatures()
{    return weights.numCols();}
0
public PassiveAggressive copy()
{    close();    PassiveAggressive r = new PassiveAggressive(numCategories(), numFeatures());    r.copyFrom(this);    return r;}
0
public void write(DataOutput out) throws IOException
{    out.writeInt(WRITABLE_VERSION);    out.writeDouble(learningRate);    out.writeInt(numCategories);    MatrixWritable.writeMatrix(out, weights);}
0
public void readFields(DataInput in) throws IOException
{    int version = in.readInt();    if (version == WRITABLE_VERSION) {        learningRate = in.readDouble();        numCategories = in.readInt();        weights = MatrixWritable.readMatrix(in);    } else {        throw new IOException("Incorrect object version, wanted " + WRITABLE_VERSION + " got " + version);    }}
0
public void close()
{}
0
public void train(long trackingKey, String groupKey, int actual, Vector instance)
{    if (lossCount > 1000) {                lossCount = 0;        lossSum = 0;    }    Vector result = classifyNoLink(instance);    double myScore = result.get(actual);        int otherIndex = result.maxValueIndex();    double otherValue = result.get(otherIndex);    if (otherIndex == actual) {        result.setQuick(otherIndex, Double.NEGATIVE_INFINITY);        otherIndex = result.maxValueIndex();        otherValue = result.get(otherIndex);    }    double loss = 1.0 - myScore + otherValue;    lossCount += 1;    if (loss >= 0) {        lossSum += loss;        double tau = loss / (instance.dot(instance) + 0.5 / learningRate);        Vector delta = instance.clone();        delta.assign(Functions.mult(tau));        weights.viewRow(actual).assign(delta, Functions.PLUS);                delta.assign(Functions.mult(-1));        weights.viewRow(otherIndex).assign(delta, Functions.PLUS);        }}
1
public void train(long trackingKey, int actual, Vector instance)
{    train(trackingKey, null, actual, instance);}
0
public void train(int actual, Vector instance)
{    train(0, null, actual, instance);}
0
public static void write(DataOutput dataOutput, T value) throws IOException
{    dataOutput.writeUTF(value.getClass().getName());    value.write(dataOutput);}
0
public static T read(DataInput dataInput, Class<? extends T> clazz) throws IOException
{    String className = dataInput.readUTF();    T r = ClassUtils.instantiateAs(className, clazz);    r.readFields(dataInput);    return r;}
0
public final Vector apply(String groupKey, int actual, Vector instance, AbstractVectorClassifier classifier)
{    addToHistory(actual, instance);        Deque<Vector> otherSide = history.get(1 - actual);    int n = otherSide.size();    Vector r = null;    for (Vector other : otherSide) {        Vector g = BASIC.apply(groupKey, actual, instance.minus(other), classifier);        if (r == null) {            r = g;        } else {            r.assign(g, Functions.plusMult(1.0 / n));        }    }    return r;}
0
public void addToHistory(int actual, Vector instance)
{    while (history.size() <= actual) {        history.add(new ArrayDeque<Vector>(window));    }        Deque<Vector> ourSide = history.get(actual);    ourSide.add(instance);    while (ourSide.size() >= window) {        ourSide.pollFirst();    }}
0
public Gradient getBaseGradient()
{    return BASIC;}
0
public double age(double oldValue, double generations, double learningRate)
{    for (int i = 0; i < generations; i++) {        oldValue -= learningRate * oldValue * (df + 1.0) / (df + oldValue * oldValue);    }    return oldValue;}
0
public double logP(double betaIJ)
{    return Gamma.logGamma((df + 1.0) / 2.0) - Math.log(df * Math.PI) - Gamma.logGamma(df / 2.0) - (df + 1.0) / 2.0 * Math.log1p(betaIJ * betaIJ);}
0
public void write(DataOutput out) throws IOException
{    out.writeDouble(df);}
0
public void readFields(DataInput in) throws IOException
{    df = in.readDouble();}
0
public double age(double oldValue, double generations, double learningRate)
{    return oldValue;}
0
public double logP(double betaIJ)
{    return 0;}
0
public void write(DataOutput dataOutput) throws IOException
{}
0
public void readFields(DataInput dataInput) throws IOException
{}
0
public void write(DataOutput out) throws IOException
{    out.writeInt(id);    out.writeLong(getNumObservations());    out.writeLong(getTotalObservations());    VectorWritable.writeVector(out, getCenter());    VectorWritable.writeVector(out, getRadius());    out.writeDouble(s0);    VectorWritable.writeVector(out, s1);    VectorWritable.writeVector(out, s2);}
0
public void readFields(DataInput in) throws IOException
{    this.id = in.readInt();    this.setNumObservations(in.readLong());    this.setTotalObservations(in.readLong());    this.setCenter(VectorWritable.readVector(in));    this.setRadius(VectorWritable.readVector(in));    this.setS0(in.readDouble());    this.setS1(VectorWritable.readVector(in));    this.setS2(VectorWritable.readVector(in));}
0
public void configure(Configuration job)
{}
0
public Collection<Parameter<?>> getParameters()
{    return Collections.emptyList();}
0
public void createParameters(String prefix, Configuration jobConf)
{}
0
public int getId()
{    return id;}
0
protected void setId(int id)
{    this.id = id;}
0
public long getNumObservations()
{    return numObservations;}
0
protected void setNumObservations(long l)
{    this.numObservations = l;}
0
public long getTotalObservations()
{    return totalObservations;}
0
protected void setTotalObservations(long totalPoints)
{    this.totalObservations = totalPoints;}
0
public Vector getCenter()
{    return center;}
0
protected void setCenter(Vector center)
{    this.center = center;}
0
public Vector getRadius()
{    return radius;}
0
protected void setRadius(Vector radius)
{    this.radius = radius;}
0
protected double getS0()
{    return s0;}
0
protected void setS0(double s0)
{    this.s0 = s0;}
0
protected Vector getS1()
{    return s1;}
0
protected void setS1(Vector s1)
{    this.s1 = s1;}
0
protected Vector getS2()
{    return s2;}
0
protected void setS2(Vector s2)
{    this.s2 = s2;}
0
public void observe(Model<VectorWritable> x)
{    AbstractCluster cl = (AbstractCluster) x;    setS0(getS0() + cl.getS0());    setS1(getS1().plus(cl.getS1()));    setS2(getS2().plus(cl.getS2()));}
0
public void observe(VectorWritable x)
{    observe(x.get());}
0
public void observe(VectorWritable x, double weight)
{    observe(x.get(), weight);}
0
public void observe(Vector x, double weight)
{    if (weight == 1.0) {        observe(x);    } else {        setS0(getS0() + weight);        Vector weightedX = x.times(weight);        if (getS1() == null) {            setS1(weightedX);        } else {            getS1().assign(weightedX, Functions.PLUS);        }        Vector x2 = x.times(x).times(weight);        if (getS2() == null) {            setS2(x2);        } else {            getS2().assign(x2, Functions.PLUS);        }    }}
0
public void observe(Vector x)
{    setS0(getS0() + 1);    if (getS1() == null) {        setS1(x.clone());    } else {        getS1().assign(x, Functions.PLUS);    }    Vector x2 = x.times(x);    if (getS2() == null) {        setS2(x2);    } else {        getS2().assign(x2, Functions.PLUS);    }}
0
public void computeParameters()
{    if (getS0() == 0) {        return;    }    setNumObservations((long) getS0());    setTotalObservations(getTotalObservations() + getNumObservations());    setCenter(getS1().divide(getS0()));        if (getS0() > 1) {        setRadius(getS2().times(getS0()).minus(getS1().times(getS1())).assign(new SquareRootFunction()).divide(getS0()));    }    setS0(0);    setS1(center.like());    setS2(center.like());}
0
public String asFormatString(String[] bindings)
{    String fmtString = "";    try {        fmtString = jxn.writeValueAsString(asJson(bindings));    } catch (IOException e) {            }    return fmtString;}
1
public Map<String, Object> asJson(String[] bindings)
{    Map<String, Object> dict = new HashMap<>();    dict.put("identifier", getIdentifier());    dict.put("n", getNumObservations());    if (getCenter() != null) {        try {            dict.put("c", formatVectorAsJson(getCenter(), bindings));        } catch (IOException e) {                    }    }    if (getRadius() != null) {        try {            dict.put("r", formatVectorAsJson(getRadius(), bindings));        } catch (IOException e) {                    }    }    return dict;}
1
public Vector computeCentroid()
{    return getS0() == 0 ? getCenter() : getS1().divide(getS0());}
0
public static String formatVector(Vector v, String[] bindings)
{    String fmtString = "";    try {        fmtString = jxn.writeValueAsString(formatVectorAsJson(v, bindings));    } catch (IOException e) {            }    return fmtString;}
1
public static List<Object> formatVectorAsJson(Vector v, String[] bindings) throws IOException
{    boolean hasBindings = bindings != null;    boolean isSparse = v.getNumNonZeroElements() != v.size();        Vector provider = v.isSequentialAccess() ? v : new SequentialAccessSparseVector(v);    List<Object> terms = new LinkedList<>();    String term = "";    for (Element elem : provider.nonZeroes()) {        if (hasBindings && bindings.length >= elem.index() + 1 && bindings[elem.index()] != null) {            term = bindings[elem.index()];        } else if (hasBindings || isSparse) {            term = String.valueOf(elem.index());        }        Map<String, Object> term_entry = new HashMap<>();        double roundedWeight = (double) Math.round(elem.get() * 1000) / 1000;        if (hasBindings || isSparse) {            term_entry.put(term, roundedWeight);            terms.add(term_entry);        } else {            terms.add(roundedWeight);        }    }    return terms;}
0
public boolean isConverged()
{        return false;}
0
public String asFormatString()
{    return "C" + this.getId() + ": " + this.computeCentroid().asFormatString();}
0
public String toString()
{    return getIdentifier() + ": " + getCenter().asFormatString();}
0
public String getIdentifier()
{    return "C-" + getId();}
0
public double getT1()
{    return t1;}
0
public double getT2()
{    return t2;}
0
public double getT3()
{    return t3;}
0
public double getT4()
{    return t4;}
0
public void useT3T4()
{    t1 = t3;    t2 = t4;}
0
public void addPointToCanopies(Vector point, Collection<Canopy> canopies)
{    boolean pointStronglyBound = false;    for (Canopy canopy : canopies) {        double dist = measure.distance(canopy.getCenter().getLengthSquared(), canopy.getCenter(), point);        if (dist < t1) {            if (log.isDebugEnabled()) {                            }            canopy.observe(point);        }        pointStronglyBound = pointStronglyBound || dist < t2;    }    if (!pointStronglyBound) {        if (log.isDebugEnabled()) {                    }        canopies.add(new Canopy(point, nextCanopyId++, measure));    }}
1
public boolean canopyCovers(Canopy canopy, Vector point)
{    return measure.distance(canopy.getCenter().getLengthSquared(), canopy.getCenter(), point) < t1;}
0
public static List<Canopy> createCanopies(List<Vector> points, DistanceMeasure measure, double t1, double t2)
{    List<Canopy> canopies = Lists.newArrayList();    /**     * Reference Implementation: Given a distance metric, one can create     * canopies as follows: Start with a list of the data points in any     * order, and with two distance thresholds, T1 and T2, where T1 > T2.     * (These thresholds can be set by the user, or selected by     * cross-validation.) Pick a point on the list and measure its distance     * to all other points. Put all points that are within distance     * threshold T1 into a canopy. Remove from the list all points that are     * within distance threshold T2. Repeat until the list is empty.     */    int nextCanopyId = 0;    while (!points.isEmpty()) {        Iterator<Vector> ptIter = points.iterator();        Vector p1 = ptIter.next();        ptIter.remove();        Canopy canopy = new Canopy(p1, nextCanopyId++, measure);        canopies.add(canopy);        while (ptIter.hasNext()) {            Vector p2 = ptIter.next();            double dist = measure.distance(p1, p2);                        if (dist < t1) {                canopy.observe(p2);            }                        if (dist < t2) {                ptIter.remove();            }        }        for (Canopy c : canopies) {            c.computeParameters();        }    }    return canopies;}
0
public static List<Vector> getCenters(Iterable<Canopy> canopies)
{    List<Vector> result = Lists.newArrayList();    for (Canopy canopy : canopies) {        result.add(canopy.getCenter());    }    return result;}
0
public static void updateCentroids(Iterable<Canopy> canopies)
{    for (Canopy canopy : canopies) {        canopy.computeParameters();    }}
0
public void setT3(double t3)
{    this.t3 = t3;}
0
public void setT4(double t4)
{    this.t4 = t4;}
0
public static CanopyClusterer configureCanopyClusterer(Configuration configuration)
{    double t1 = Double.parseDouble(configuration.get(T1_KEY));    double t2 = Double.parseDouble(configuration.get(T2_KEY));    DistanceMeasure measure = ClassUtils.instantiateAs(configuration.get(DISTANCE_MEASURE_KEY), DistanceMeasure.class);    measure.configure(configuration);    CanopyClusterer canopyClusterer = new CanopyClusterer(measure, t1, t2);    String d = configuration.get(T3_KEY);    if (d != null) {        canopyClusterer.setT3(Double.parseDouble(d));    }    d = configuration.get(T4_KEY);    if (d != null) {        canopyClusterer.setT4(Double.parseDouble(d));    }    return canopyClusterer;}
0
public static void main(String[] args) throws Exception
{    ToolRunner.run(new Configuration(), new CanopyDriver(), args);}
0
public int run(String[] args) throws Exception
{    addInputOption();    addOutputOption();    addOption(DefaultOptionCreator.distanceMeasureOption().create());    addOption(DefaultOptionCreator.t1Option().create());    addOption(DefaultOptionCreator.t2Option().create());    addOption(DefaultOptionCreator.t3Option().create());    addOption(DefaultOptionCreator.t4Option().create());    addOption(DefaultOptionCreator.clusterFilterOption().create());    addOption(DefaultOptionCreator.overwriteOption().create());    addOption(DefaultOptionCreator.clusteringOption().create());    addOption(DefaultOptionCreator.methodOption().create());    addOption(DefaultOptionCreator.outlierThresholdOption().create());    if (parseArguments(args) == null) {        return -1;    }    Path input = getInputPath();    Path output = getOutputPath();    Configuration conf = getConf();    if (hasOption(DefaultOptionCreator.OVERWRITE_OPTION)) {        HadoopUtil.delete(conf, output);    }    String measureClass = getOption(DefaultOptionCreator.DISTANCE_MEASURE_OPTION);    double t1 = Double.parseDouble(getOption(DefaultOptionCreator.T1_OPTION));    double t2 = Double.parseDouble(getOption(DefaultOptionCreator.T2_OPTION));    double t3 = t1;    if (hasOption(DefaultOptionCreator.T3_OPTION)) {        t3 = Double.parseDouble(getOption(DefaultOptionCreator.T3_OPTION));    }    double t4 = t2;    if (hasOption(DefaultOptionCreator.T4_OPTION)) {        t4 = Double.parseDouble(getOption(DefaultOptionCreator.T4_OPTION));    }    int clusterFilter = 0;    if (hasOption(DefaultOptionCreator.CLUSTER_FILTER_OPTION)) {        clusterFilter = Integer.parseInt(getOption(DefaultOptionCreator.CLUSTER_FILTER_OPTION));    }    boolean runClustering = hasOption(DefaultOptionCreator.CLUSTERING_OPTION);    boolean runSequential = getOption(DefaultOptionCreator.METHOD_OPTION).equalsIgnoreCase(DefaultOptionCreator.SEQUENTIAL_METHOD);    DistanceMeasure measure = ClassUtils.instantiateAs(measureClass, DistanceMeasure.class);    double clusterClassificationThreshold = 0.0;    if (hasOption(DefaultOptionCreator.OUTLIER_THRESHOLD)) {        clusterClassificationThreshold = Double.parseDouble(getOption(DefaultOptionCreator.OUTLIER_THRESHOLD));    }    run(conf, input, output, measure, t1, t2, t3, t4, clusterFilter, runClustering, clusterClassificationThreshold, runSequential);    return 0;}
0
public static void run(Configuration conf, Path input, Path output, DistanceMeasure measure, double t1, double t2, double t3, double t4, int clusterFilter, boolean runClustering, double clusterClassificationThreshold, boolean runSequential) throws IOException, InterruptedException, ClassNotFoundException
{    Path clustersOut = buildClusters(conf, input, output, measure, t1, t2, t3, t4, clusterFilter, runSequential);    if (runClustering) {        clusterData(conf, input, clustersOut, output, clusterClassificationThreshold, runSequential);    }}
0
public static void run(Configuration conf, Path input, Path output, DistanceMeasure measure, double t1, double t2, boolean runClustering, double clusterClassificationThreshold, boolean runSequential) throws IOException, InterruptedException, ClassNotFoundException
{    run(conf, input, output, measure, t1, t2, t1, t2, 0, runClustering, clusterClassificationThreshold, runSequential);}
0
public static void run(Path input, Path output, DistanceMeasure measure, double t1, double t2, boolean runClustering, double clusterClassificationThreshold, boolean runSequential) throws IOException, InterruptedException, ClassNotFoundException
{    run(new Configuration(), input, output, measure, t1, t2, runClustering, clusterClassificationThreshold, runSequential);}
0
public static Path buildClusters(Configuration conf, Path input, Path output, DistanceMeasure measure, double t1, double t2, int clusterFilter, boolean runSequential) throws IOException, InterruptedException, ClassNotFoundException
{    return buildClusters(conf, input, output, measure, t1, t2, t1, t2, clusterFilter, runSequential);}
0
public static Path buildClusters(Configuration conf, Path input, Path output, DistanceMeasure measure, double t1, double t2, double t3, double t4, int clusterFilter, boolean runSequential) throws IOException, InterruptedException, ClassNotFoundException
{        if (runSequential) {        return buildClustersSeq(input, output, measure, t1, t2, clusterFilter);    } else {        return buildClustersMR(conf, input, output, measure, t1, t2, t3, t4, clusterFilter);    }}
1
private static Path buildClustersSeq(Path input, Path output, DistanceMeasure measure, double t1, double t2, int clusterFilter) throws IOException
{    CanopyClusterer clusterer = new CanopyClusterer(measure, t1, t2);    Collection<Canopy> canopies = Lists.newArrayList();    Configuration conf = new Configuration();    FileSystem fs = FileSystem.get(input.toUri(), conf);    for (VectorWritable vw : new SequenceFileDirValueIterable<VectorWritable>(input, PathType.LIST, PathFilters.logsCRCFilter(), conf)) {        clusterer.addPointToCanopies(vw.get(), canopies);    }    Path canopyOutputDir = new Path(output, Cluster.CLUSTERS_DIR + '0' + Cluster.FINAL_ITERATION_SUFFIX);    Path path = new Path(canopyOutputDir, "part-r-00000");    SequenceFile.Writer writer = new SequenceFile.Writer(fs, conf, path, Text.class, ClusterWritable.class);    try {        ClusterWritable clusterWritable = new ClusterWritable();        for (Canopy canopy : canopies) {            canopy.computeParameters();            if (log.isDebugEnabled()) {                            }            if (canopy.getNumObservations() > clusterFilter) {                clusterWritable.setValue(canopy);                writer.append(new Text(canopy.getIdentifier()), clusterWritable);            }        }    } finally {        Closeables.close(writer, false);    }    return canopyOutputDir;}
1
private static Path buildClustersMR(Configuration conf, Path input, Path output, DistanceMeasure measure, double t1, double t2, double t3, double t4, int clusterFilter) throws IOException, InterruptedException, ClassNotFoundException
{    conf.set(CanopyConfigKeys.DISTANCE_MEASURE_KEY, measure.getClass().getName());    conf.set(CanopyConfigKeys.T1_KEY, String.valueOf(t1));    conf.set(CanopyConfigKeys.T2_KEY, String.valueOf(t2));    conf.set(CanopyConfigKeys.T3_KEY, String.valueOf(t3));    conf.set(CanopyConfigKeys.T4_KEY, String.valueOf(t4));    conf.set(CanopyConfigKeys.CF_KEY, String.valueOf(clusterFilter));    Job job = new Job(conf, "Canopy Driver running buildClusters over input: " + input);    job.setInputFormatClass(SequenceFileInputFormat.class);    job.setOutputFormatClass(SequenceFileOutputFormat.class);    job.setMapperClass(CanopyMapper.class);    job.setMapOutputKeyClass(Text.class);    job.setMapOutputValueClass(VectorWritable.class);    job.setReducerClass(CanopyReducer.class);    job.setOutputKeyClass(Text.class);    job.setOutputValueClass(ClusterWritable.class);    job.setNumReduceTasks(1);    job.setJarByClass(CanopyDriver.class);    FileInputFormat.addInputPath(job, input);    Path canopyOutputDir = new Path(output, Cluster.CLUSTERS_DIR + '0' + Cluster.FINAL_ITERATION_SUFFIX);    FileOutputFormat.setOutputPath(job, canopyOutputDir);    if (!job.waitForCompletion(true)) {        throw new InterruptedException("Canopy Job failed processing " + input);    }    return canopyOutputDir;}
0
private static void clusterData(Configuration conf, Path points, Path canopies, Path output, double clusterClassificationThreshold, boolean runSequential) throws IOException, InterruptedException, ClassNotFoundException
{    ClusterClassifier.writePolicy(new CanopyClusteringPolicy(), canopies);    ClusterClassificationDriver.run(conf, points, output, new Path(output, PathDirectory.CLUSTERED_POINTS_DIRECTORY), clusterClassificationThreshold, true, runSequential);}
0
protected void map(WritableComparable<?> key, VectorWritable point, Context context) throws IOException, InterruptedException
{    canopyClusterer.addPointToCanopies(point.get(), canopies);}
0
protected void setup(Context context) throws IOException, InterruptedException
{    super.setup(context);    canopyClusterer = CanopyConfigKeys.configureCanopyClusterer(context.getConfiguration());    clusterFilter = Integer.parseInt(context.getConfiguration().get(CanopyConfigKeys.CF_KEY));}
0
protected void cleanup(Context context) throws IOException, InterruptedException
{    for (Canopy canopy : canopies) {        canopy.computeParameters();        if (canopy.getNumObservations() > clusterFilter) {            context.write(new Text("centroid"), new VectorWritable(canopy.getCenter()));        }    }    super.cleanup(context);}
0
 CanopyClusterer getCanopyClusterer()
{    return canopyClusterer;}
0
protected void reduce(Text arg0, Iterable<VectorWritable> values, Context context) throws IOException, InterruptedException
{    for (VectorWritable value : values) {        Vector point = value.get();        canopyClusterer.addPointToCanopies(point, canopies);    }    for (Canopy canopy : canopies) {        canopy.computeParameters();        if (canopy.getNumObservations() > clusterFilter) {            ClusterWritable clusterWritable = new ClusterWritable();            clusterWritable.setValue(canopy);            context.write(new Text(canopy.getIdentifier()), clusterWritable);        }    }}
0
protected void setup(Context context) throws IOException, InterruptedException
{    super.setup(context);    canopyClusterer = CanopyConfigKeys.configureCanopyClusterer(context.getConfiguration());    canopyClusterer.useT3T4();    clusterFilter = Integer.parseInt(context.getConfiguration().get(CanopyConfigKeys.CF_KEY));}
0
public int run(String[] args) throws Exception
{    addInputOption();    addOutputOption();    addOption(DefaultOptionCreator.methodOption().create());    addOption(DefaultOptionCreator.clustersInOption().withDescription("The input centroids, as Vectors.  Must be a SequenceFile of Writable, Cluster/Canopy.").create());    if (parseArguments(args) == null) {        return -1;    }    Path input = getInputPath();    Path output = getOutputPath();    if (getConf() == null) {        setConf(new Configuration());    }    Path clustersIn = new Path(getOption(DefaultOptionCreator.CLUSTERS_IN_OPTION));    boolean runSequential = getOption(DefaultOptionCreator.METHOD_OPTION).equalsIgnoreCase(DefaultOptionCreator.SEQUENTIAL_METHOD);    double clusterClassificationThreshold = 0.0;    if (hasOption(DefaultOptionCreator.OUTLIER_THRESHOLD)) {        clusterClassificationThreshold = Double.parseDouble(getOption(DefaultOptionCreator.OUTLIER_THRESHOLD));    }    run(getConf(), input, clustersIn, output, clusterClassificationThreshold, true, runSequential);    return 0;}
0
public static void main(String[] args) throws Exception
{    ToolRunner.run(new Configuration(), new ClusterClassificationDriver(), args);}
0
public static void run(Configuration conf, Path input, Path clusteringOutputPath, Path output, Double clusterClassificationThreshold, boolean emitMostLikely, boolean runSequential) throws IOException, InterruptedException, ClassNotFoundException
{    if (runSequential) {        classifyClusterSeq(conf, input, clusteringOutputPath, output, clusterClassificationThreshold, emitMostLikely);    } else {        classifyClusterMR(conf, input, clusteringOutputPath, output, clusterClassificationThreshold, emitMostLikely);    }}
0
private static void classifyClusterSeq(Configuration conf, Path input, Path clusters, Path output, Double clusterClassificationThreshold, boolean emitMostLikely) throws IOException
{    List<Cluster> clusterModels = populateClusterModels(clusters, conf);    ClusteringPolicy policy = ClusterClassifier.readPolicy(finalClustersPath(conf, clusters));    ClusterClassifier clusterClassifier = new ClusterClassifier(clusterModels, policy);    selectCluster(input, clusterModels, clusterClassifier, output, clusterClassificationThreshold, emitMostLikely);}
0
private static List<Cluster> populateClusterModels(Path clusterOutputPath, Configuration conf) throws IOException
{    List<Cluster> clusterModels = new ArrayList<>();    Path finalClustersPath = finalClustersPath(conf, clusterOutputPath);    Iterator<?> it = new SequenceFileDirValueIterator<>(finalClustersPath, PathType.LIST, PathFilters.partFilter(), null, false, conf);    while (it.hasNext()) {        ClusterWritable next = (ClusterWritable) it.next();        Cluster cluster = next.getValue();        cluster.configure(conf);        clusterModels.add(cluster);    }    return clusterModels;}
0
private static Path finalClustersPath(Configuration conf, Path clusterOutputPath) throws IOException
{    FileSystem fileSystem = clusterOutputPath.getFileSystem(conf);    FileStatus[] clusterFiles = fileSystem.listStatus(clusterOutputPath, PathFilters.finalPartFilter());    return clusterFiles[0].getPath();}
0
private static void selectCluster(Path input, List<Cluster> clusterModels, ClusterClassifier clusterClassifier, Path output, Double clusterClassificationThreshold, boolean emitMostLikely) throws IOException
{    Configuration conf = new Configuration();    SequenceFile.Writer writer = new SequenceFile.Writer(input.getFileSystem(conf), conf, new Path(output, "part-m-" + 0), IntWritable.class, WeightedPropertyVectorWritable.class);    for (Pair<Writable, VectorWritable> vw : new SequenceFileDirIterable<Writable, VectorWritable>(input, PathType.LIST, PathFilters.logsCRCFilter(), conf)) {                        Class<? extends Writable> keyClass = vw.getFirst().getClass();        Vector vector = vw.getSecond().get();        if (!keyClass.equals(NamedVector.class)) {            if (keyClass.equals(Text.class)) {                vector = new NamedVector(vector, vw.getFirst().toString());            } else if (keyClass.equals(IntWritable.class)) {                vector = new NamedVector(vector, Integer.toString(((IntWritable) vw.getFirst()).get()));            }        }        Vector pdfPerCluster = clusterClassifier.classify(vector);        if (shouldClassify(pdfPerCluster, clusterClassificationThreshold)) {            classifyAndWrite(clusterModels, clusterClassificationThreshold, emitMostLikely, writer, new VectorWritable(vector), pdfPerCluster);        }    }    writer.close();}
0
private static void classifyAndWrite(List<Cluster> clusterModels, Double clusterClassificationThreshold, boolean emitMostLikely, SequenceFile.Writer writer, VectorWritable vw, Vector pdfPerCluster) throws IOException
{    Map<Text, Text> props = new HashMap<>();    if (emitMostLikely) {        int maxValueIndex = pdfPerCluster.maxValueIndex();        WeightedPropertyVectorWritable weightedPropertyVectorWritable = new WeightedPropertyVectorWritable(pdfPerCluster.maxValue(), vw.get(), props);        write(clusterModels, writer, weightedPropertyVectorWritable, maxValueIndex);    } else {        writeAllAboveThreshold(clusterModels, clusterClassificationThreshold, writer, vw, pdfPerCluster);    }}
0
private static void writeAllAboveThreshold(List<Cluster> clusterModels, Double clusterClassificationThreshold, SequenceFile.Writer writer, VectorWritable vw, Vector pdfPerCluster) throws IOException
{    Map<Text, Text> props = new HashMap<>();    for (Element pdf : pdfPerCluster.nonZeroes()) {        if (pdf.get() >= clusterClassificationThreshold) {            WeightedPropertyVectorWritable wvw = new WeightedPropertyVectorWritable(pdf.get(), vw.get(), props);            int clusterIndex = pdf.index();            write(clusterModels, writer, wvw, clusterIndex);        }    }}
0
private static void write(List<Cluster> clusterModels, SequenceFile.Writer writer, WeightedPropertyVectorWritable weightedPropertyVectorWritable, int maxValueIndex) throws IOException
{    Cluster cluster = clusterModels.get(maxValueIndex);    DistanceMeasureCluster distanceMeasureCluster = (DistanceMeasureCluster) cluster;    DistanceMeasure distanceMeasure = distanceMeasureCluster.getMeasure();    double distance = distanceMeasure.distance(cluster.getCenter(), weightedPropertyVectorWritable.getVector());    weightedPropertyVectorWritable.getProperties().put(new Text("distance"), new Text(Double.toString(distance)));    writer.append(new IntWritable(cluster.getId()), weightedPropertyVectorWritable);}
0
private static boolean shouldClassify(Vector pdfPerCluster, Double clusterClassificationThreshold)
{    return pdfPerCluster.maxValue() >= clusterClassificationThreshold;}
0
private static void classifyClusterMR(Configuration conf, Path input, Path clustersIn, Path output, Double clusterClassificationThreshold, boolean emitMostLikely) throws IOException, InterruptedException, ClassNotFoundException
{    conf.setFloat(ClusterClassificationConfigKeys.OUTLIER_REMOVAL_THRESHOLD, clusterClassificationThreshold.floatValue());    conf.setBoolean(ClusterClassificationConfigKeys.EMIT_MOST_LIKELY, emitMostLikely);    conf.set(ClusterClassificationConfigKeys.CLUSTERS_IN, clustersIn.toUri().toString());    Job job = new Job(conf, "Cluster Classification Driver running over input: " + input);    job.setJarByClass(ClusterClassificationDriver.class);    job.setInputFormatClass(SequenceFileInputFormat.class);    job.setOutputFormatClass(SequenceFileOutputFormat.class);    job.setMapperClass(ClusterClassificationMapper.class);    job.setNumReduceTasks(0);    job.setOutputKeyClass(IntWritable.class);    job.setOutputValueClass(WeightedPropertyVectorWritable.class);    FileInputFormat.addInputPath(job, input);    FileOutputFormat.setOutputPath(job, output);    if (!job.waitForCompletion(true)) {        throw new InterruptedException("Cluster Classification Driver Job failed processing " + input);    }}
0
public static void run(Configuration conf, Path input, Path clusteringOutputPath, Path output, double clusterClassificationThreshold, boolean emitMostLikely, boolean runSequential) throws IOException, InterruptedException, ClassNotFoundException
{    if (runSequential) {        classifyClusterSeq(conf, input, clusteringOutputPath, output, clusterClassificationThreshold, emitMostLikely);    } else {        classifyClusterMR(conf, input, clusteringOutputPath, output, clusterClassificationThreshold, emitMostLikely);    }}
0
protected void setup(Context context) throws IOException, InterruptedException
{    super.setup(context);    Configuration conf = context.getConfiguration();    String clustersIn = conf.get(ClusterClassificationConfigKeys.CLUSTERS_IN);    threshold = conf.getFloat(ClusterClassificationConfigKeys.OUTLIER_REMOVAL_THRESHOLD, 0.0f);    emitMostLikely = conf.getBoolean(ClusterClassificationConfigKeys.EMIT_MOST_LIKELY, false);    clusterModels = new ArrayList<>();    if (clustersIn != null && !clustersIn.isEmpty()) {        Path clustersInPath = new Path(clustersIn);        clusterModels = populateClusterModels(clustersInPath, conf);        ClusteringPolicy policy = ClusterClassifier.readPolicy(finalClustersPath(clustersInPath));        clusterClassifier = new ClusterClassifier(clusterModels, policy);    }    clusterId = new IntWritable();}
0
protected void map(WritableComparable<?> key, VectorWritable vw, Context context) throws IOException, InterruptedException
{    if (!clusterModels.isEmpty()) {                        Class<? extends Vector> vectorClass = vw.get().getClass();        Vector vector = vw.get();        if (!vectorClass.equals(NamedVector.class)) {            if (key.getClass().equals(Text.class)) {                vector = new NamedVector(vector, key.toString());            } else if (key.getClass().equals(IntWritable.class)) {                vector = new NamedVector(vector, Integer.toString(((IntWritable) key).get()));            }        }        Vector pdfPerCluster = clusterClassifier.classify(vector);        if (shouldClassify(pdfPerCluster)) {            if (emitMostLikely) {                int maxValueIndex = pdfPerCluster.maxValueIndex();                write(new VectorWritable(vector), context, maxValueIndex, 1.0);            } else {                writeAllAboveThreshold(new VectorWritable(vector), context, pdfPerCluster);            }        }    }}
0
private void writeAllAboveThreshold(VectorWritable vw, Context context, Vector pdfPerCluster) throws IOException, InterruptedException
{    for (Element pdf : pdfPerCluster.nonZeroes()) {        if (pdf.get() >= threshold) {            int clusterIndex = pdf.index();            write(vw, context, clusterIndex, pdf.get());        }    }}
0
private void write(VectorWritable vw, Context context, int clusterIndex, double weight) throws IOException, InterruptedException
{    Cluster cluster = clusterModels.get(clusterIndex);    clusterId.set(cluster.getId());    DistanceMeasureCluster distanceMeasureCluster = (DistanceMeasureCluster) cluster;    DistanceMeasure distanceMeasure = distanceMeasureCluster.getMeasure();    double distance = distanceMeasure.distance(cluster.getCenter(), vw.get());    Map<Text, Text> props = new HashMap<>();    props.put(new Text("distance"), new Text(Double.toString(distance)));    context.write(clusterId, new WeightedPropertyVectorWritable(weight, vw.get(), props));}
0
public static List<Cluster> populateClusterModels(Path clusterOutputPath, Configuration conf) throws IOException
{    List<Cluster> clusters = new ArrayList<>();    FileSystem fileSystem = clusterOutputPath.getFileSystem(conf);    FileStatus[] clusterFiles = fileSystem.listStatus(clusterOutputPath, PathFilters.finalPartFilter());    Iterator<?> it = new SequenceFileDirValueIterator<>(clusterFiles[0].getPath(), PathType.LIST, PathFilters.partFilter(), null, false, conf);    while (it.hasNext()) {        ClusterWritable next = (ClusterWritable) it.next();        Cluster cluster = next.getValue();        cluster.configure(conf);        clusters.add(cluster);    }    return clusters;}
0
private boolean shouldClassify(Vector pdfPerCluster)
{    return pdfPerCluster.maxValue() >= threshold;}
0
private static Path finalClustersPath(Path clusterOutputPath) throws IOException
{    FileSystem fileSystem = clusterOutputPath.getFileSystem(new Configuration());    FileStatus[] clusterFiles = fileSystem.listStatus(clusterOutputPath, PathFilters.finalPartFilter());    return clusterFiles[0].getPath();}
0
public Vector classify(Vector instance)
{    return policy.classify(instance, this);}
0
public double classifyScalar(Vector instance)
{    if (models.size() == 2) {        double pdf0 = models.get(0).pdf(new VectorWritable(instance));        double pdf1 = models.get(1).pdf(new VectorWritable(instance));        return pdf0 / (pdf0 + pdf1);    }    throw new IllegalStateException();}
0
public int numCategories()
{    return models.size();}
0
public void write(DataOutput out) throws IOException
{    out.writeInt(models.size());    out.writeUTF(modelClass);    new ClusteringPolicyWritable(policy).write(out);    for (Cluster cluster : models) {        cluster.write(out);    }}
0
public void readFields(DataInput in) throws IOException
{    int size = in.readInt();    modelClass = in.readUTF();    models = new ArrayList<>();    ClusteringPolicyWritable clusteringPolicyWritable = new ClusteringPolicyWritable();    clusteringPolicyWritable.readFields(in);    policy = clusteringPolicyWritable.getValue();    for (int i = 0; i < size; i++) {        Cluster element = ClassUtils.instantiateAs(modelClass, Cluster.class);        element.readFields(in);        models.add(element);    }}
0
public void train(int actual, Vector instance)
{    models.get(actual).observe(new VectorWritable(instance));}
0
public void train(int actual, Vector data, double weight)
{    models.get(actual).observe(new VectorWritable(data), weight);}
0
public void train(long trackingKey, String groupKey, int actual, Vector instance)
{    models.get(actual).observe(new VectorWritable(instance));}
0
public void train(long trackingKey, int actual, Vector instance)
{    models.get(actual).observe(new VectorWritable(instance));}
0
public void close()
{    policy.close(this);}
0
public List<Cluster> getModels()
{    return models;}
0
public ClusteringPolicy getPolicy()
{    return policy;}
0
public void writeToSeqFiles(Path path) throws IOException
{    writePolicy(policy, path);    Configuration config = new Configuration();    FileSystem fs = FileSystem.get(path.toUri(), config);    ClusterWritable cw = new ClusterWritable();    for (int i = 0; i < models.size(); i++) {        try (SequenceFile.Writer writer = new SequenceFile.Writer(fs, config, new Path(path, "part-" + String.format(Locale.ENGLISH, "%05d", i)), IntWritable.class, ClusterWritable.class)) {            Cluster cluster = models.get(i);            cw.setValue(cluster);            Writable key = new IntWritable(i);            writer.append(key, cw);        }    }}
0
public void readFromSeqFiles(Configuration conf, Path path) throws IOException
{    Configuration config = new Configuration();    List<Cluster> clusters = new ArrayList<>();    for (ClusterWritable cw : new SequenceFileDirValueIterable<ClusterWritable>(path, PathType.LIST, PathFilters.logsCRCFilter(), config)) {        Cluster cluster = cw.getValue();        cluster.configure(conf);        clusters.add(cluster);    }    this.models = clusters;    modelClass = models.get(0).getClass().getName();    this.policy = readPolicy(path);}
0
public static ClusteringPolicy readPolicy(Path path) throws IOException
{    Path policyPath = new Path(path, POLICY_FILE_NAME);    Configuration config = new Configuration();    FileSystem fs = FileSystem.get(policyPath.toUri(), config);    SequenceFile.Reader reader = new SequenceFile.Reader(fs, policyPath, config);    Text key = new Text();    ClusteringPolicyWritable cpw = new ClusteringPolicyWritable();    reader.next(key, cpw);    Closeables.close(reader, true);    return cpw.getValue();}
0
public static void writePolicy(ClusteringPolicy policy, Path path) throws IOException
{    Path policyPath = new Path(path, POLICY_FILE_NAME);    Configuration config = new Configuration();    FileSystem fs = FileSystem.get(policyPath.toUri(), config);    SequenceFile.Writer writer = new SequenceFile.Writer(fs, config, policyPath, Text.class, ClusteringPolicyWritable.class);    writer.append(new Text(), new ClusteringPolicyWritable(policy));    Closeables.close(writer, false);}
0
public Map<Text, Text> getProperties()
{    return properties;}
0
public void setProperties(Map<Text, Text> properties)
{    this.properties = properties;}
0
public void readFields(DataInput in) throws IOException
{    super.readFields(in);    int size = in.readInt();    if (size > 0) {        properties = new HashMap<>();        for (int i = 0; i < size; i++) {            Text key = new Text(in.readUTF());            Text val = new Text(in.readUTF());            properties.put(key, val);        }    }}
0
public void write(DataOutput out) throws IOException
{    super.write(out);    out.writeInt(properties != null ? properties.size() : 0);    if (properties != null) {        for (Map.Entry<Text, Text> entry : properties.entrySet()) {            out.writeUTF(entry.getKey().toString());            out.writeUTF(entry.getValue().toString());        }    }}
0
public String toString()
{    Vector vector = getVector();    StringBuilder bldr = new StringBuilder("wt: ").append(getWeight()).append(' ');    if (properties != null && !properties.isEmpty()) {        for (Map.Entry<Text, Text> entry : properties.entrySet()) {            bldr.append(entry.getKey().toString()).append(": ").append(entry.getValue().toString()).append(' ');        }    }    bldr.append(" vec: ").append(vector == null ? "null" : AbstractCluster.formatVector(vector, null));    return bldr.toString();}
0
public Vector getVector()
{    return vectorWritable.get();}
0
public void setVector(Vector vector)
{    vectorWritable.set(vector);}
0
public double getWeight()
{    return weight;}
0
public void readFields(DataInput in) throws IOException
{    vectorWritable.readFields(in);    weight = in.readDouble();}
0
public void write(DataOutput out) throws IOException
{    vectorWritable.write(out);    out.writeDouble(weight);}
0
public String toString()
{    Vector vector = vectorWritable.get();    return weight + ": " + (vector == null ? "null" : AbstractCluster.formatVector(vector, null));}
0
public static List<OnlineSummarizer> summarizeClusterDistances(Iterable<? extends Vector> datapoints, Iterable<? extends Vector> centroids, DistanceMeasure distanceMeasure)
{    UpdatableSearcher searcher = new ProjectionSearch(distanceMeasure, 3, 1);    searcher.addAll(centroids);    List<OnlineSummarizer> summarizers = new ArrayList<>();    if (searcher.size() == 0) {        return summarizers;    }    for (int i = 0; i < searcher.size(); ++i) {        summarizers.add(new OnlineSummarizer());    }    for (Vector v : datapoints) {        Centroid closest = (Centroid) searcher.search(v, 1).get(0).getValue();        OnlineSummarizer summarizer = summarizers.get(closest.getIndex());        summarizer.add(distanceMeasure.distance(v, closest));    }    return summarizers;}
0
public static double totalClusterCost(Iterable<? extends Vector> datapoints, Iterable<? extends Vector> centroids)
{    DistanceMeasure distanceMeasure = new EuclideanDistanceMeasure();    UpdatableSearcher searcher = new ProjectionSearch(distanceMeasure, 3, 1);    searcher.addAll(centroids);    return totalClusterCost(datapoints, searcher);}
0
public static double totalClusterCost(Iterable<? extends Vector> datapoints, Searcher centroids)
{    double totalCost = 0;    for (Vector vector : datapoints) {        totalCost += centroids.searchFirst(vector, false).getWeight();    }    return totalCost;}
0
public static double estimateDistanceCutoff(List<? extends Vector> data, DistanceMeasure distanceMeasure)
{    BruteSearch searcher = new BruteSearch(distanceMeasure);    searcher.addAll(data);    double minDistance = Double.POSITIVE_INFINITY;    for (Vector vector : data) {        double closest = searcher.searchFirst(vector, true).getWeight();        if (minDistance > 0 && closest < minDistance) {            minDistance = closest;        }        searcher.add(vector);    }    return minDistance;}
0
public static double estimateDistanceCutoff(Iterable<T> data, DistanceMeasure distanceMeasure, int sampleLimit)
{    return estimateDistanceCutoff(Lists.newArrayList(Iterables.limit(data, sampleLimit)), distanceMeasure);}
0
public static double daviesBouldinIndex(List<? extends Vector> centroids, DistanceMeasure distanceMeasure, List<OnlineSummarizer> clusterDistanceSummaries)
{    Preconditions.checkArgument(centroids.size() == clusterDistanceSummaries.size(), "Number of centroids and cluster summaries differ.");    int n = centroids.size();    double totalDBIndex = 0;        for (int i = 0; i < n; ++i) {        double averageDistanceI = clusterDistanceSummaries.get(i).getMean();        double maxDBIndex = 0;        for (int j = 0; j < n; ++j) {            if (i != j) {                double dbIndex = (averageDistanceI + clusterDistanceSummaries.get(j).getMean()) / distanceMeasure.distance(centroids.get(i), centroids.get(j));                if (dbIndex > maxDBIndex) {                    maxDBIndex = dbIndex;                }            }        }        totalDBIndex += maxDBIndex;    }    return totalDBIndex / n;}
0
public static double dunnIndex(List<? extends Vector> centroids, DistanceMeasure distanceMeasure, List<OnlineSummarizer> clusterDistanceSummaries)
{    Preconditions.checkArgument(centroids.size() == clusterDistanceSummaries.size(), "Number of centroids and cluster summaries differ.");    int n = centroids.size();                        double maxIntraClusterDistance = 0;    for (OnlineSummarizer summarizer : clusterDistanceSummaries) {        if (summarizer.getCount() > 0) {            double intraClusterDistance;            if (summarizer.getCount() == 1) {                intraClusterDistance = summarizer.getMean();            } else {                intraClusterDistance = summarizer.getMedian();            }            if (maxIntraClusterDistance < intraClusterDistance) {                maxIntraClusterDistance = intraClusterDistance;            }        }    }    double minDunnIndex = Double.POSITIVE_INFINITY;    for (int i = 0; i < n; ++i) {                for (int j = i + 1; j < n; ++j) {            double dunnIndex = distanceMeasure.distance(centroids.get(i), centroids.get(j));            if (minDunnIndex > dunnIndex) {                minDunnIndex = dunnIndex;            }        }    }    return minDunnIndex / maxIntraClusterDistance;}
0
public static double choose2(double n)
{    return n * (n - 1) / 2;}
0
public static Matrix getConfusionMatrix(List<? extends Vector> rowCentroids, List<? extends Vector> columnCentroids, Iterable<? extends Vector> datapoints, DistanceMeasure distanceMeasure)
{    Searcher rowSearcher = new BruteSearch(distanceMeasure);    rowSearcher.addAll(rowCentroids);    Searcher columnSearcher = new BruteSearch(distanceMeasure);    columnSearcher.addAll(columnCentroids);    int numRows = rowCentroids.size();    int numCols = columnCentroids.size();    Matrix confusionMatrix = new DenseMatrix(numRows, numCols);    for (Vector vector : datapoints) {        WeightedThing<Vector> closestRowCentroid = rowSearcher.search(vector, 1).get(0);        WeightedThing<Vector> closestColumnCentroid = columnSearcher.search(vector, 1).get(0);        int row = ((Centroid) closestRowCentroid.getValue()).getIndex();        int column = ((Centroid) closestColumnCentroid.getValue()).getIndex();        double vectorWeight;        if (vector instanceof WeightedVector) {            vectorWeight = ((WeightedVector) vector).getWeight();        } else {            vectorWeight = 1;        }        confusionMatrix.set(row, column, confusionMatrix.get(row, column) + vectorWeight);    }    return confusionMatrix;}
0
public static double getAdjustedRandIndex(Matrix confusionMatrix)
{    int numRows = confusionMatrix.numRows();    int numCols = confusionMatrix.numCols();    double rowChoiceSum = 0;    double columnChoiceSum = 0;    double totalChoiceSum = 0;    double total = 0;    for (int i = 0; i < numRows; ++i) {        double rowSum = 0;        for (int j = 0; j < numCols; ++j) {            rowSum += confusionMatrix.get(i, j);            totalChoiceSum += choose2(confusionMatrix.get(i, j));        }        total += rowSum;        rowChoiceSum += choose2(rowSum);    }    for (int j = 0; j < numCols; ++j) {        double columnSum = 0;        for (int i = 0; i < numRows; ++i) {            columnSum += confusionMatrix.get(i, j);        }        columnChoiceSum += choose2(columnSum);    }    double rowColumnChoiceSumDivTotal = rowChoiceSum * columnChoiceSum / choose2(total);    return (totalChoiceSum - rowColumnChoiceSumDivTotal) / ((rowChoiceSum + columnChoiceSum) / 2 - rowColumnChoiceSumDivTotal);}
0
public static double totalWeight(Iterable<? extends Vector> data)
{    double sum = 0;    for (Vector row : data) {        Preconditions.checkNotNull(row);        if (row instanceof WeightedVector) {            sum += ((WeightedVector) row).getWeight();        } else {            sum++;        }    }    return sum;}
0
public Vector computePi(Collection<SoftCluster> clusters, List<Double> clusterDistanceList)
{    Vector pi = new DenseVector(clusters.size());    for (int i = 0; i < clusters.size(); i++) {        double probWeight = computeProbWeight(clusterDistanceList.get(i), clusterDistanceList);        pi.set(i, probWeight);    }    return pi;}
0
public double computeProbWeight(double clusterDistance, Iterable<Double> clusterDistanceList)
{    if (clusterDistance == 0) {        clusterDistance = MINIMAL_VALUE;    }    double denom = 0.0;    for (double eachCDist : clusterDistanceList) {        if (eachCDist == 0.0) {            eachCDist = MINIMAL_VALUE;        }        denom += Math.pow(clusterDistance / eachCDist, 2.0 / (m - 1));    }    return 1.0 / denom;}
0
public void setM(double m)
{    this.m = m;}
0
public static void main(String[] args) throws Exception
{    ToolRunner.run(new Configuration(), new FuzzyKMeansDriver(), args);}
0
public int run(String[] args) throws Exception
{    addInputOption();    addOutputOption();    addOption(DefaultOptionCreator.distanceMeasureOption().create());    addOption(DefaultOptionCreator.clustersInOption().withDescription("The input centroids, as Vectors.  Must be a SequenceFile of Writable, Cluster/Canopy.  " + "If k is also specified, then a random set of vectors will be selected" + " and written out to this path first").create());    addOption(DefaultOptionCreator.numClustersOption().withDescription("The k in k-Means.  If specified, then a random selection of k Vectors will be chosen" + " as the Centroid and written to the clusters input path.").create());    addOption(DefaultOptionCreator.convergenceOption().create());    addOption(DefaultOptionCreator.maxIterationsOption().create());    addOption(DefaultOptionCreator.overwriteOption().create());    addOption(M_OPTION, M_OPTION, "coefficient normalization factor, must be greater than 1", true);    addOption(DefaultOptionCreator.clusteringOption().create());    addOption(DefaultOptionCreator.emitMostLikelyOption().create());    addOption(DefaultOptionCreator.thresholdOption().create());    addOption(DefaultOptionCreator.methodOption().create());    addOption(DefaultOptionCreator.useSetRandomSeedOption().create());    if (parseArguments(args) == null) {        return -1;    }    Path input = getInputPath();    Path clusters = new Path(getOption(DefaultOptionCreator.CLUSTERS_IN_OPTION));    Path output = getOutputPath();    String measureClass = getOption(DefaultOptionCreator.DISTANCE_MEASURE_OPTION);    if (measureClass == null) {        measureClass = SquaredEuclideanDistanceMeasure.class.getName();    }    double convergenceDelta = Double.parseDouble(getOption(DefaultOptionCreator.CONVERGENCE_DELTA_OPTION));    float fuzziness = Float.parseFloat(getOption(M_OPTION));    int maxIterations = Integer.parseInt(getOption(DefaultOptionCreator.MAX_ITERATIONS_OPTION));    if (hasOption(DefaultOptionCreator.OVERWRITE_OPTION)) {        HadoopUtil.delete(getConf(), output);    }    boolean emitMostLikely = Boolean.parseBoolean(getOption(DefaultOptionCreator.EMIT_MOST_LIKELY_OPTION));    double threshold = Double.parseDouble(getOption(DefaultOptionCreator.THRESHOLD_OPTION));    DistanceMeasure measure = ClassUtils.instantiateAs(measureClass, DistanceMeasure.class);    if (hasOption(DefaultOptionCreator.NUM_CLUSTERS_OPTION)) {        int numClusters = Integer.parseInt(getOption(DefaultOptionCreator.NUM_CLUSTERS_OPTION));        Long seed = null;        if (hasOption(DefaultOptionCreator.RANDOM_SEED)) {            seed = Long.parseLong(getOption(DefaultOptionCreator.RANDOM_SEED));        }        clusters = RandomSeedGenerator.buildRandom(getConf(), input, clusters, numClusters, measure, seed);    }    boolean runClustering = hasOption(DefaultOptionCreator.CLUSTERING_OPTION);    boolean runSequential = getOption(DefaultOptionCreator.METHOD_OPTION).equalsIgnoreCase(DefaultOptionCreator.SEQUENTIAL_METHOD);    run(getConf(), input, clusters, output, convergenceDelta, maxIterations, fuzziness, runClustering, emitMostLikely, threshold, runSequential);    return 0;}
0
public static void run(Path input, Path clustersIn, Path output, double convergenceDelta, int maxIterations, float m, boolean runClustering, boolean emitMostLikely, double threshold, boolean runSequential) throws IOException, ClassNotFoundException, InterruptedException
{    Configuration conf = new Configuration();    Path clustersOut = buildClusters(conf, input, clustersIn, output, convergenceDelta, maxIterations, m, runSequential);    if (runClustering) {                clusterData(conf, input, clustersOut, output, convergenceDelta, m, emitMostLikely, threshold, runSequential);    }}
1
public static void run(Configuration conf, Path input, Path clustersIn, Path output, double convergenceDelta, int maxIterations, float m, boolean runClustering, boolean emitMostLikely, double threshold, boolean runSequential) throws IOException, ClassNotFoundException, InterruptedException
{    Path clustersOut = buildClusters(conf, input, clustersIn, output, convergenceDelta, maxIterations, m, runSequential);    if (runClustering) {                clusterData(conf, input, clustersOut, output, convergenceDelta, m, emitMostLikely, threshold, runSequential);    }}
1
public static Path buildClusters(Configuration conf, Path input, Path clustersIn, Path output, double convergenceDelta, int maxIterations, float m, boolean runSequential) throws IOException, InterruptedException, ClassNotFoundException
{    List<Cluster> clusters = new ArrayList<>();    FuzzyKMeansUtil.configureWithClusterInfo(conf, clustersIn, clusters);    if (conf == null) {        conf = new Configuration();    }    if (clusters.isEmpty()) {        throw new IllegalStateException("No input clusters found in " + clustersIn + ". Check your -c argument.");    }    Path priorClustersPath = new Path(output, Cluster.INITIAL_CLUSTERS_DIR);    ClusteringPolicy policy = new FuzzyKMeansClusteringPolicy(m, convergenceDelta);    ClusterClassifier prior = new ClusterClassifier(clusters, policy);    prior.writeToSeqFiles(priorClustersPath);    if (runSequential) {        ClusterIterator.iterateSeq(conf, input, priorClustersPath, output, maxIterations);    } else {        ClusterIterator.iterateMR(conf, input, priorClustersPath, output, maxIterations);    }    return output;}
0
public static void clusterData(Configuration conf, Path input, Path clustersIn, Path output, double convergenceDelta, float m, boolean emitMostLikely, double threshold, boolean runSequential) throws IOException, ClassNotFoundException, InterruptedException
{    ClusterClassifier.writePolicy(new FuzzyKMeansClusteringPolicy(m, convergenceDelta), clustersIn);    ClusterClassificationDriver.run(conf, input, output, new Path(output, PathDirectory.CLUSTERED_POINTS_DIRECTORY), threshold, emitMostLikely, runSequential);}
0
public static void configureWithClusterInfo(Configuration conf, Path clusterPath, List<Cluster> clusters)
{    for (Writable value : new SequenceFileDirValueIterable<>(clusterPath, PathType.LIST, PathFilters.partFilter(), conf)) {        Class<? extends Writable> valueClass = value.getClass();        if (valueClass.equals(ClusterWritable.class)) {            ClusterWritable clusterWritable = (ClusterWritable) value;            value = clusterWritable.getValue();            valueClass = value.getClass();        }        if (valueClass.equals(Kluster.class)) {                        Kluster cluster = (Kluster) value;            clusters.add(new SoftCluster(cluster.getCenter(), cluster.getId(), cluster.getMeasure()));        } else if (valueClass.equals(SoftCluster.class)) {                        clusters.add((SoftCluster) value);        } else if (valueClass.equals(Canopy.class)) {                        Canopy canopy = (Canopy) value;            clusters.add(new SoftCluster(canopy.getCenter(), canopy.getId(), canopy.getMeasure()));        } else {            throw new IllegalStateException("Bad value class: " + valueClass);        }    }}
0
public String asFormatString()
{    return this.getIdentifier() + ": " + this.computeCentroid().asFormatString();}
0
public String getIdentifier()
{    return (isConverged() ? "SV-" : "SC-") + getId();}
0
public double pdf(VectorWritable vw)
{        throw new UnsupportedOperationException("SoftCluster pdf cannot be calculated out of context. See FuzzyKMeansClusterer");}
0
public Vector select(Vector probabilities)
{    int maxValueIndex = probabilities.maxValueIndex();    Vector weights = new SequentialAccessSparseVector(probabilities.size());    weights.set(maxValueIndex, 1.0);    return weights;}
0
public void update(ClusterClassifier posterior)
{}
0
public Vector classify(Vector data, ClusterClassifier prior)
{    List<Cluster> models = prior.getModels();    int i = 0;    Vector pdfs = new DenseVector(models.size());    for (Cluster model : models) {        pdfs.set(i++, model.pdf(new VectorWritable(data)));    }    return pdfs.assign(new TimesFunction(), 1.0 / pdfs.zSum());}
0
public void close(ClusterClassifier posterior)
{    for (Cluster cluster : posterior.getModels()) {        cluster.computeParameters();    }}
0
public Vector select(Vector probabilities)
{    int maxValueIndex = probabilities.maxValueIndex();    Vector weights = new SequentialAccessSparseVector(probabilities.size());    weights.set(maxValueIndex, 1.0);    return weights;}
0
public void write(DataOutput out) throws IOException
{    out.writeDouble(t1);    out.writeDouble(t2);}
0
public void readFields(DataInput in) throws IOException
{    this.t1 = in.readDouble();    this.t2 = in.readDouble();}
0
protected void setup(Context context) throws IOException, InterruptedException
{    Configuration conf = context.getConfiguration();    String priorClustersPath = conf.get(ClusterIterator.PRIOR_PATH_KEY);    classifier = new ClusterClassifier();    classifier.readFromSeqFiles(conf, new Path(priorClustersPath));    policy = classifier.getPolicy();    policy.update(classifier);    super.setup(context);}
0
protected void map(WritableComparable<?> key, VectorWritable value, Context context) throws IOException, InterruptedException
{    Vector probabilities = classifier.classify(value.get());    Vector selections = policy.select(probabilities);    for (Element el : selections.nonZeroes()) {        classifier.train(el.index(), value.get(), el.get());    }}
0
protected void cleanup(Context context) throws IOException, InterruptedException
{    List<Cluster> clusters = classifier.getModels();    ClusterWritable cw = new ClusterWritable();    for (int index = 0; index < clusters.size(); index++) {        cw.setValue(clusters.get(index));        context.write(new IntWritable(index), cw);    }    super.cleanup(context);}
0
protected void reduce(IntWritable key, Iterable<ClusterWritable> values, Context context) throws IOException, InterruptedException
{    Iterator<ClusterWritable> iter = values.iterator();        Cluster first = iter.next().getValue();    while (iter.hasNext()) {        Cluster cluster = iter.next().getValue();        first.observe(cluster);    }    List<Cluster> models = new ArrayList<>();    models.add(first);    classifier = new ClusterClassifier(models, policy);    classifier.close();    context.write(key, new ClusterWritable(first));}
0
protected void setup(Context context) throws IOException, InterruptedException
{    Configuration conf = context.getConfiguration();    String priorClustersPath = conf.get(ClusterIterator.PRIOR_PATH_KEY);    classifier = new ClusterClassifier();    classifier.readFromSeqFiles(conf, new Path(priorClustersPath));    policy = classifier.getPolicy();    policy.update(classifier);    super.setup(context);}
0
public ClusteringPolicy getValue()
{    return value;}
0
public void setValue(ClusteringPolicy value)
{    this.value = value;}
0
public void write(DataOutput out) throws IOException
{    PolymorphicWritable.write(out, value);}
0
public void readFields(DataInput in) throws IOException
{    value = PolymorphicWritable.read(in, ClusteringPolicy.class);}
0
public static ClusterClassifier iterate(Iterable<Vector> data, ClusterClassifier classifier, int numIterations)
{    ClusteringPolicy policy = classifier.getPolicy();    for (int iteration = 1; iteration <= numIterations; iteration++) {        for (Vector vector : data) {                        policy.update(classifier);                        Vector probabilities = classifier.classify(vector);                        Vector weights = policy.select(probabilities);                        for (Vector.Element e : weights.nonZeroes()) {                int index = e.index();                classifier.train(index, vector, weights.get(index));            }        }                classifier.close();    }    return classifier;}
0
public static void iterateSeq(Configuration conf, Path inPath, Path priorPath, Path outPath, int numIterations) throws IOException
{    ClusterClassifier classifier = new ClusterClassifier();    classifier.readFromSeqFiles(conf, priorPath);    Path clustersOut = null;    int iteration = 1;    while (iteration <= numIterations) {        for (VectorWritable vw : new SequenceFileDirValueIterable<VectorWritable>(inPath, PathType.LIST, PathFilters.logsCRCFilter(), conf)) {            Vector vector = vw.get();                        Vector probabilities = classifier.classify(vector);                        Vector weights = classifier.getPolicy().select(probabilities);                        for (Vector.Element e : weights.nonZeroes()) {                int index = e.index();                classifier.train(index, vector, weights.get(index));            }        }                classifier.close();                classifier.getPolicy().update(classifier);                clustersOut = new Path(outPath, Cluster.CLUSTERS_DIR + iteration);        classifier.writeToSeqFiles(clustersOut);        FileSystem fs = FileSystem.get(outPath.toUri(), conf);        iteration++;        if (isConverged(clustersOut, conf, fs)) {            break;        }    }    Path finalClustersIn = new Path(outPath, Cluster.CLUSTERS_DIR + (iteration - 1) + Cluster.FINAL_ITERATION_SUFFIX);    FileSystem.get(clustersOut.toUri(), conf).rename(clustersOut, finalClustersIn);}
0
public static void iterateMR(Configuration conf, Path inPath, Path priorPath, Path outPath, int numIterations) throws IOException, InterruptedException, ClassNotFoundException
{    ClusteringPolicy policy = ClusterClassifier.readPolicy(priorPath);    Path clustersOut = null;    int iteration = 1;    while (iteration <= numIterations) {        conf.set(PRIOR_PATH_KEY, priorPath.toString());        String jobName = "Cluster Iterator running iteration " + iteration + " over priorPath: " + priorPath;        Job job = new Job(conf, jobName);        job.setMapOutputKeyClass(IntWritable.class);        job.setMapOutputValueClass(ClusterWritable.class);        job.setOutputKeyClass(IntWritable.class);        job.setOutputValueClass(ClusterWritable.class);        job.setInputFormatClass(SequenceFileInputFormat.class);        job.setOutputFormatClass(SequenceFileOutputFormat.class);        job.setMapperClass(CIMapper.class);        job.setReducerClass(CIReducer.class);        FileInputFormat.addInputPath(job, inPath);        clustersOut = new Path(outPath, Cluster.CLUSTERS_DIR + iteration);        priorPath = clustersOut;        FileOutputFormat.setOutputPath(job, clustersOut);        job.setJarByClass(ClusterIterator.class);        if (!job.waitForCompletion(true)) {            throw new InterruptedException("Cluster Iteration " + iteration + " failed processing " + priorPath);        }        ClusterClassifier.writePolicy(policy, clustersOut);        FileSystem fs = FileSystem.get(outPath.toUri(), conf);        iteration++;        if (isConverged(clustersOut, conf, fs)) {            break;        }    }    Path finalClustersIn = new Path(outPath, Cluster.CLUSTERS_DIR + (iteration - 1) + Cluster.FINAL_ITERATION_SUFFIX);    FileSystem.get(clustersOut.toUri(), conf).rename(clustersOut, finalClustersIn);}
0
private static boolean isConverged(Path filePath, Configuration conf, FileSystem fs) throws IOException
{    for (FileStatus part : fs.listStatus(filePath, PathFilters.partFilter())) {        SequenceFileValueIterator<ClusterWritable> iterator = new SequenceFileValueIterator<>(part.getPath(), true, conf);        while (iterator.hasNext()) {            ClusterWritable value = iterator.next();            if (!value.getValue().isConverged()) {                Closeables.close(iterator, true);                return false;            }        }    }    return true;}
0
public Cluster getValue()
{    return value;}
0
public void setValue(Cluster value)
{    this.value = value;}
0
public void write(DataOutput out) throws IOException
{    PolymorphicWritable.write(out, value);}
0
public void readFields(DataInput in) throws IOException
{    value = PolymorphicWritable.read(in, Cluster.class);}
0
public void configure(Configuration job)
{    if (measure != null) {        measure.configure(job);    }}
0
public void readFields(DataInput in) throws IOException
{    String dm = in.readUTF();    this.measure = ClassUtils.instantiateAs(dm, DistanceMeasure.class);    super.readFields(in);}
0
public void write(DataOutput out) throws IOException
{    out.writeUTF(measure.getClass().getName());    super.write(out);}
0
public double pdf(VectorWritable vw)
{    return 1 / (1 + measure.distance(vw.get(), getCenter()));}
0
public Model<VectorWritable> sampleFromPosterior()
{    return new DistanceMeasureCluster(getCenter(), getId(), measure);}
0
public DistanceMeasure getMeasure()
{    return measure;}
0
public void setMeasure(DistanceMeasure measure)
{    this.measure = measure;}
0
public String getIdentifier()
{    return "DMC:" + getId();}
0
public Vector select(Vector probabilities)
{    return probabilities;}
0
public Vector classify(Vector data, ClusterClassifier prior)
{    Collection<SoftCluster> clusters = new ArrayList<>();    List<Double> distances = new ArrayList<>();    for (Cluster model : prior.getModels()) {        SoftCluster sc = (SoftCluster) model;        clusters.add(sc);        distances.add(sc.getMeasure().distance(data, sc.getCenter()));    }    FuzzyKMeansClusterer fuzzyKMeansClusterer = new FuzzyKMeansClusterer();    fuzzyKMeansClusterer.setM(m);    return fuzzyKMeansClusterer.computePi(clusters, distances);}
0
public void write(DataOutput out) throws IOException
{    out.writeDouble(m);    out.writeDouble(convergenceDelta);}
0
public void readFields(DataInput in) throws IOException
{    this.m = in.readDouble();    this.convergenceDelta = in.readDouble();}
0
public void close(ClusterClassifier posterior)
{    for (Cluster cluster : posterior.getModels()) {        ((org.apache.mahout.clustering.kmeans.Kluster) cluster).calculateConvergence(convergenceDelta);        cluster.computeParameters();    }}
0
public void write(DataOutput out) throws IOException
{    out.writeDouble(convergenceDelta);}
0
public void readFields(DataInput in) throws IOException
{    this.convergenceDelta = in.readDouble();}
0
public void close(ClusterClassifier posterior)
{    boolean allConverged = true;    for (Cluster cluster : posterior.getModels()) {        org.apache.mahout.clustering.kmeans.Kluster kluster = (org.apache.mahout.clustering.kmeans.Kluster) cluster;        boolean converged = kluster.calculateConvergence(convergenceDelta);        allConverged = allConverged && converged;        cluster.computeParameters();    }}
0
public double calculateDerivativeValue(double distance, double h)
{    return distance < h ? 1.0 : 0.0;}
0
public static String formatCluster(Kluster cluster)
{    return cluster.getIdentifier() + ": " + cluster.computeCentroid().asFormatString();}
0
public String asFormatString()
{    return formatCluster(this);}
0
public void write(DataOutput out) throws IOException
{    super.write(out);    out.writeBoolean(converged);}
0
public void readFields(DataInput in) throws IOException
{    super.readFields(in);    this.converged = in.readBoolean();}
0
public String toString()
{    return asFormatString(null);}
0
public String getIdentifier()
{    return (converged ? "VL-" : "CL-") + getId();}
0
public boolean computeConvergence(DistanceMeasure measure, double convergenceDelta)
{    Vector centroid = computeCentroid();    converged = measure.distance(centroid.getLengthSquared(), centroid, getCenter()) <= convergenceDelta;    return converged;}
0
public boolean isConverged()
{    return converged;}
0
protected void setConverged(boolean converged)
{    this.converged = converged;}
0
public boolean calculateConvergence(double convergenceDelta)
{    Vector centroid = computeCentroid();    converged = getMeasure().distance(centroid.getLengthSquared(), centroid, getCenter()) <= convergenceDelta;    return converged;}
0
public static void main(String[] args) throws Exception
{    ToolRunner.run(new Configuration(), new KMeansDriver(), args);}
0
public int run(String[] args) throws Exception
{    addInputOption();    addOutputOption();    addOption(DefaultOptionCreator.distanceMeasureOption().create());    addOption(DefaultOptionCreator.clustersInOption().withDescription("The input centroids, as Vectors.  Must be a SequenceFile of Writable, Cluster/Canopy.  " + "If k is also specified, then a random set of vectors will be selected" + " and written out to this path first").create());    addOption(DefaultOptionCreator.numClustersOption().withDescription("The k in k-Means.  If specified, then a random selection of k Vectors will be chosen" + " as the Centroid and written to the clusters input path.").create());    addOption(DefaultOptionCreator.useSetRandomSeedOption().create());    addOption(DefaultOptionCreator.convergenceOption().create());    addOption(DefaultOptionCreator.maxIterationsOption().create());    addOption(DefaultOptionCreator.overwriteOption().create());    addOption(DefaultOptionCreator.clusteringOption().create());    addOption(DefaultOptionCreator.methodOption().create());    addOption(DefaultOptionCreator.outlierThresholdOption().create());    if (parseArguments(args) == null) {        return -1;    }    Path input = getInputPath();    Path clusters = new Path(getOption(DefaultOptionCreator.CLUSTERS_IN_OPTION));    Path output = getOutputPath();    String measureClass = getOption(DefaultOptionCreator.DISTANCE_MEASURE_OPTION);    if (measureClass == null) {        measureClass = SquaredEuclideanDistanceMeasure.class.getName();    }    double convergenceDelta = Double.parseDouble(getOption(DefaultOptionCreator.CONVERGENCE_DELTA_OPTION));    int maxIterations = Integer.parseInt(getOption(DefaultOptionCreator.MAX_ITERATIONS_OPTION));    if (hasOption(DefaultOptionCreator.OVERWRITE_OPTION)) {        HadoopUtil.delete(getConf(), output);    }    DistanceMeasure measure = ClassUtils.instantiateAs(measureClass, DistanceMeasure.class);    if (hasOption(DefaultOptionCreator.NUM_CLUSTERS_OPTION)) {        int numClusters = Integer.parseInt(getOption(DefaultOptionCreator.NUM_CLUSTERS_OPTION));        Long seed = null;        if (hasOption(DefaultOptionCreator.RANDOM_SEED)) {            seed = Long.parseLong(getOption(DefaultOptionCreator.RANDOM_SEED));        }        clusters = RandomSeedGenerator.buildRandom(getConf(), input, clusters, numClusters, measure, seed);    }    boolean runClustering = hasOption(DefaultOptionCreator.CLUSTERING_OPTION);    boolean runSequential = getOption(DefaultOptionCreator.METHOD_OPTION).equalsIgnoreCase(DefaultOptionCreator.SEQUENTIAL_METHOD);    double clusterClassificationThreshold = 0.0;    if (hasOption(DefaultOptionCreator.OUTLIER_THRESHOLD)) {        clusterClassificationThreshold = Double.parseDouble(getOption(DefaultOptionCreator.OUTLIER_THRESHOLD));    }    run(getConf(), input, clusters, output, convergenceDelta, maxIterations, runClustering, clusterClassificationThreshold, runSequential);    return 0;}
0
public static void run(Configuration conf, Path input, Path clustersIn, Path output, double convergenceDelta, int maxIterations, boolean runClustering, double clusterClassificationThreshold, boolean runSequential) throws IOException, InterruptedException, ClassNotFoundException
{        String delta = Double.toString(convergenceDelta);    if (log.isInfoEnabled()) {                    }    Path clustersOut = buildClusters(conf, input, clustersIn, output, maxIterations, delta, runSequential);    if (runClustering) {                clusterData(conf, input, clustersOut, output, clusterClassificationThreshold, runSequential);    }}
1
public static void run(Path input, Path clustersIn, Path output, double convergenceDelta, int maxIterations, boolean runClustering, double clusterClassificationThreshold, boolean runSequential) throws IOException, InterruptedException, ClassNotFoundException
{    run(new Configuration(), input, clustersIn, output, convergenceDelta, maxIterations, runClustering, clusterClassificationThreshold, runSequential);}
0
public static Path buildClusters(Configuration conf, Path input, Path clustersIn, Path output, int maxIterations, String delta, boolean runSequential) throws IOException, InterruptedException, ClassNotFoundException
{    double convergenceDelta = Double.parseDouble(delta);    List<Cluster> clusters = new ArrayList<>();    KMeansUtil.configureWithClusterInfo(conf, clustersIn, clusters);    if (clusters.isEmpty()) {        throw new IllegalStateException("No input clusters found in " + clustersIn + ". Check your -c argument.");    }    Path priorClustersPath = new Path(output, Cluster.INITIAL_CLUSTERS_DIR);    ClusteringPolicy policy = new KMeansClusteringPolicy(convergenceDelta);    ClusterClassifier prior = new ClusterClassifier(clusters, policy);    prior.writeToSeqFiles(priorClustersPath);    if (runSequential) {        ClusterIterator.iterateSeq(conf, input, priorClustersPath, output, maxIterations);    } else {        ClusterIterator.iterateMR(conf, input, priorClustersPath, output, maxIterations);    }    return output;}
0
public static void clusterData(Configuration conf, Path input, Path clustersIn, Path output, double clusterClassificationThreshold, boolean runSequential) throws IOException, InterruptedException, ClassNotFoundException
{    if (log.isInfoEnabled()) {                    }    ClusterClassifier.writePolicy(new KMeansClusteringPolicy(), clustersIn);    ClusterClassificationDriver.run(conf, input, output, new Path(output, PathDirectory.CLUSTERED_POINTS_DIRECTORY), clusterClassificationThreshold, true, runSequential);}
1
public static void configureWithClusterInfo(Configuration conf, Path clusterPath, Collection<Cluster> clusters)
{    for (Writable value : new SequenceFileDirValueIterable<>(clusterPath, PathType.LIST, PathFilters.partFilter(), conf)) {        Class<? extends Writable> valueClass = value.getClass();        if (valueClass.equals(ClusterWritable.class)) {            ClusterWritable clusterWritable = (ClusterWritable) value;            value = clusterWritable.getValue();            valueClass = value.getClass();        }                if (valueClass.equals(Kluster.class)) {                        clusters.add((Kluster) value);        } else if (valueClass.equals(Canopy.class)) {                        Canopy canopy = (Canopy) value;            clusters.add(new Kluster(canopy.getCenter(), canopy.getId(), canopy.getMeasure()));        } else {            throw new IllegalStateException("Bad value class: " + valueClass);        }    }}
1
public static Path buildRandom(Configuration conf, Path input, Path output, int k, DistanceMeasure measure) throws IOException
{    return buildRandom(conf, input, output, k, measure, null);}
0
public static Path buildRandom(Configuration conf, Path input, Path output, int k, DistanceMeasure measure, Long seed) throws IOException
{    Preconditions.checkArgument(k > 0, "Must be: k > 0, but k = " + k);        FileSystem fs = FileSystem.get(output.toUri(), conf);    HadoopUtil.delete(conf, output);    Path outFile = new Path(output, "part-randomSeed");    boolean newFile = fs.createNewFile(outFile);    if (newFile) {        Path inputPathPattern;        if (fs.getFileStatus(input).isDir()) {            inputPathPattern = new Path(input, "*");        } else {            inputPathPattern = input;        }        FileStatus[] inputFiles = fs.globStatus(inputPathPattern, PathFilters.logsCRCFilter());        Random random = (seed != null) ? RandomUtils.getRandom(seed) : RandomUtils.getRandom();        List<Text> chosenTexts = new ArrayList<>(k);        List<ClusterWritable> chosenClusters = new ArrayList<>(k);        int nextClusterId = 0;        int index = 0;        for (FileStatus fileStatus : inputFiles) {            if (!fileStatus.isDir()) {                for (Pair<Writable, VectorWritable> record : new SequenceFileIterable<Writable, VectorWritable>(fileStatus.getPath(), true, conf)) {                    Writable key = record.getFirst();                    VectorWritable value = record.getSecond();                    Kluster newCluster = new Kluster(value.get(), nextClusterId++, measure);                    newCluster.observe(value.get(), 1);                    Text newText = new Text(key.toString());                    int currentSize = chosenTexts.size();                    if (currentSize < k) {                        chosenTexts.add(newText);                        ClusterWritable clusterWritable = new ClusterWritable();                        clusterWritable.setValue(newCluster);                        chosenClusters.add(clusterWritable);                    } else {                        int j = random.nextInt(index);                        if (j < k) {                            chosenTexts.set(j, newText);                            ClusterWritable clusterWritable = new ClusterWritable();                            clusterWritable.setValue(newCluster);                            chosenClusters.set(j, clusterWritable);                        }                    }                    index++;                }            }        }        try (SequenceFile.Writer writer = SequenceFile.createWriter(fs, conf, outFile, Text.class, ClusterWritable.class)) {            for (int i = 0; i < chosenTexts.size(); i++) {                writer.append(chosenTexts.get(i), chosenClusters.get(i));            }                    }    }    return outFile;}
1
protected ModelTrainer getModelTrainer()
{    return modelTrainer;}
0
protected int getMaxIters()
{    return maxIters;}
0
protected int getNumTopics()
{    return numTopics;}
0
protected void setup(Context context) throws IOException, InterruptedException
{        Configuration conf = context.getConfiguration();    float eta = conf.getFloat(CVB0Driver.TERM_TOPIC_SMOOTHING, Float.NaN);    float alpha = conf.getFloat(CVB0Driver.DOC_TOPIC_SMOOTHING, Float.NaN);    long seed = conf.getLong(CVB0Driver.RANDOM_SEED, 1234L);    numTopics = conf.getInt(CVB0Driver.NUM_TOPICS, -1);    int numTerms = conf.getInt(CVB0Driver.NUM_TERMS, -1);    int numUpdateThreads = conf.getInt(CVB0Driver.NUM_UPDATE_THREADS, 1);    int numTrainThreads = conf.getInt(CVB0Driver.NUM_TRAIN_THREADS, 4);    maxIters = conf.getInt(CVB0Driver.MAX_ITERATIONS_PER_DOC, 10);    float modelWeight = conf.getFloat(CVB0Driver.MODEL_WEIGHT, 1.0f);        Path[] modelPaths = CVB0Driver.getModelPaths(conf);    if (modelPaths != null && modelPaths.length > 0) {        readModel = new TopicModel(conf, eta, alpha, null, numUpdateThreads, modelWeight, modelPaths);    } else {                readModel = new TopicModel(numTopics, numTerms, eta, alpha, RandomUtils.getRandom(seed), null, numTrainThreads, modelWeight);    }        writeModel = modelWeight == 1 ? new TopicModel(numTopics, numTerms, eta, alpha, null, numUpdateThreads) : readModel;        modelTrainer = new ModelTrainer(readModel, writeModel, numTrainThreads, numTopics, numTerms);    modelTrainer.start();}
1
public void map(IntWritable docId, VectorWritable document, Context context) throws IOException, InterruptedException
{    /* where to get docTopics? */    Vector topicVector = new DenseVector(numTopics).assign(1.0 / numTopics);    modelTrainer.train(document.get(), topicVector, true, maxIters);}
0
protected void cleanup(Context context) throws IOException, InterruptedException
{        modelTrainer.stop();        TopicModel readFrom = modelTrainer.getReadModel();    for (MatrixSlice topic : readFrom) {        context.write(new IntWritable(topic.index()), new VectorWritable(topic.vector()));    }    readModel.stop();    writeModel.stop();}
1
protected void setup(Context context) throws IOException, InterruptedException
{    MemoryUtil.startMemoryLogger(5000);        Configuration conf = context.getConfiguration();    float eta = conf.getFloat(CVB0Driver.TERM_TOPIC_SMOOTHING, Float.NaN);    float alpha = conf.getFloat(CVB0Driver.DOC_TOPIC_SMOOTHING, Float.NaN);    long seed = conf.getLong(CVB0Driver.RANDOM_SEED, 1234L);    random = RandomUtils.getRandom(seed);    numTopics = conf.getInt(CVB0Driver.NUM_TOPICS, -1);    int numTerms = conf.getInt(CVB0Driver.NUM_TERMS, -1);    int numUpdateThreads = conf.getInt(CVB0Driver.NUM_UPDATE_THREADS, 1);    int numTrainThreads = conf.getInt(CVB0Driver.NUM_TRAIN_THREADS, 4);    maxIters = conf.getInt(CVB0Driver.MAX_ITERATIONS_PER_DOC, 10);    float modelWeight = conf.getFloat(CVB0Driver.MODEL_WEIGHT, 1.0f);    testFraction = conf.getFloat(CVB0Driver.TEST_SET_FRACTION, 0.1f);        Path[] modelPaths = CVB0Driver.getModelPaths(conf);    if (modelPaths != null && modelPaths.length > 0) {        readModel = new TopicModel(conf, eta, alpha, null, numUpdateThreads, modelWeight, modelPaths);    } else {                readModel = new TopicModel(numTopics, numTerms, eta, alpha, RandomUtils.getRandom(seed), null, numTrainThreads, modelWeight);    }        modelTrainer = new ModelTrainer(readModel, null, numTrainThreads, numTopics, numTerms);        topicVector = new DenseVector(new double[numTopics]);}
1
protected void cleanup(Context context) throws IOException, InterruptedException
{    readModel.stop();    MemoryUtil.stopMemoryLogger();}
0
public void map(IntWritable docId, VectorWritable document, Context context) throws IOException, InterruptedException
{    if (testFraction < 1.0f && random.nextFloat() >= testFraction) {        return;    }    context.getCounter(Counters.SAMPLED_DOCUMENTS).increment(1);    outKey.set(document.get().norm(1));    outValue.set(modelTrainer.calculatePerplexity(document.get(), topicVector.assign(1.0 / numTopics), maxIters));    context.write(outKey, outValue);}
0
public void map(IntWritable docId, VectorWritable doc, Context context) throws IOException, InterruptedException
{    int numTopics = getNumTopics();    Vector docTopics = new DenseVector(numTopics).assign(1.0 / numTopics);    Matrix docModel = new SparseRowMatrix(numTopics, doc.get().size());    int maxIters = getMaxIters();    ModelTrainer modelTrainer = getModelTrainer();    for (int i = 0; i < maxIters; i++) {        modelTrainer.getReadModel().trainDocTopicModel(doc.get(), docTopics, docModel);    }    topics.set(docTopics);    context.write(docId, topics);}
0
protected void cleanup(Context context)
{    getModelTrainer().stop();}
0
public int run(String[] args) throws Exception
{    addInputOption();    addOutputOption();    addOption(DefaultOptionCreator.maxIterationsOption().create());    addOption(DefaultOptionCreator.CONVERGENCE_DELTA_OPTION, "cd", "The convergence delta value", String.valueOf(DEFAULT_CONVERGENCE_DELTA));    addOption(DefaultOptionCreator.overwriteOption().create());    addOption(NUM_TOPICS, "k", "Number of topics to learn", true);    addOption(NUM_TERMS, "nt", "Vocabulary size", false);    addOption(DOC_TOPIC_SMOOTHING, "a", "Smoothing for document/topic distribution", String.valueOf(DEFAULT_DOC_TOPIC_SMOOTHING));    addOption(TERM_TOPIC_SMOOTHING, "e", "Smoothing for topic/term distribution", String.valueOf(DEFAULT_TERM_TOPIC_SMOOTHING));    addOption(DICTIONARY, "dict", "Path to term-dictionary file(s) (glob expression supported)", false);    addOption(DOC_TOPIC_OUTPUT, "dt", "Output path for the training doc/topic distribution", false);    addOption(MODEL_TEMP_DIR, "mt", "Path to intermediate model path (useful for restarting)", false);    addOption(ITERATION_BLOCK_SIZE, "block", "Number of iterations per perplexity check", String.valueOf(DEFAULT_ITERATION_BLOCK_SIZE));    addOption(RANDOM_SEED, "seed", "Random seed", false);    addOption(TEST_SET_FRACTION, "tf", "Fraction of data to hold out for testing", String.valueOf(DEFAULT_TEST_SET_FRACTION));    addOption(NUM_TRAIN_THREADS, "ntt", "number of threads per mapper to train with", String.valueOf(DEFAULT_NUM_TRAIN_THREADS));    addOption(NUM_UPDATE_THREADS, "nut", "number of threads per mapper to update the model with", String.valueOf(DEFAULT_NUM_UPDATE_THREADS));    addOption(MAX_ITERATIONS_PER_DOC, "mipd", "max number of iterations per doc for p(topic|doc) learning", String.valueOf(DEFAULT_MAX_ITERATIONS_PER_DOC));    addOption(NUM_REDUCE_TASKS, null, "number of reducers to use during model estimation", String.valueOf(DEFAULT_NUM_REDUCE_TASKS));    addOption(buildOption(BACKFILL_PERPLEXITY, null, "enable backfilling of missing perplexity values", false, false, null));    if (parseArguments(args) == null) {        return -1;    }    int numTopics = Integer.parseInt(getOption(NUM_TOPICS));    Path inputPath = getInputPath();    Path topicModelOutputPath = getOutputPath();    int maxIterations = Integer.parseInt(getOption(DefaultOptionCreator.MAX_ITERATIONS_OPTION));    int iterationBlockSize = Integer.parseInt(getOption(ITERATION_BLOCK_SIZE));    double convergenceDelta = Double.parseDouble(getOption(DefaultOptionCreator.CONVERGENCE_DELTA_OPTION));    double alpha = Double.parseDouble(getOption(DOC_TOPIC_SMOOTHING));    double eta = Double.parseDouble(getOption(TERM_TOPIC_SMOOTHING));    int numTrainThreads = Integer.parseInt(getOption(NUM_TRAIN_THREADS));    int numUpdateThreads = Integer.parseInt(getOption(NUM_UPDATE_THREADS));    int maxItersPerDoc = Integer.parseInt(getOption(MAX_ITERATIONS_PER_DOC));    Path dictionaryPath = hasOption(DICTIONARY) ? new Path(getOption(DICTIONARY)) : null;    int numTerms = hasOption(NUM_TERMS) ? Integer.parseInt(getOption(NUM_TERMS)) : getNumTerms(getConf(), dictionaryPath);    Path docTopicOutputPath = hasOption(DOC_TOPIC_OUTPUT) ? new Path(getOption(DOC_TOPIC_OUTPUT)) : null;    Path modelTempPath = hasOption(MODEL_TEMP_DIR) ? new Path(getOption(MODEL_TEMP_DIR)) : getTempPath("topicModelState");    long seed = hasOption(RANDOM_SEED) ? Long.parseLong(getOption(RANDOM_SEED)) : System.nanoTime() % 10000;    float testFraction = hasOption(TEST_SET_FRACTION) ? Float.parseFloat(getOption(TEST_SET_FRACTION)) : 0.0f;    int numReduceTasks = Integer.parseInt(getOption(NUM_REDUCE_TASKS));    boolean backfillPerplexity = hasOption(BACKFILL_PERPLEXITY);    return run(getConf(), inputPath, topicModelOutputPath, numTopics, numTerms, alpha, eta, maxIterations, iterationBlockSize, convergenceDelta, dictionaryPath, docTopicOutputPath, modelTempPath, seed, testFraction, numTrainThreads, numUpdateThreads, maxItersPerDoc, numReduceTasks, backfillPerplexity);}
0
private static int getNumTerms(Configuration conf, Path dictionaryPath) throws IOException
{    FileSystem fs = dictionaryPath.getFileSystem(conf);    Text key = new Text();    IntWritable value = new IntWritable();    int maxTermId = -1;    for (FileStatus stat : fs.globStatus(dictionaryPath)) {        SequenceFile.Reader reader = new SequenceFile.Reader(fs, stat.getPath(), conf);        while (reader.next(key, value)) {            maxTermId = Math.max(maxTermId, value.get());        }    }    return maxTermId + 1;}
0
private static double rateOfChange(List<Double> perplexities)
{    int sz = perplexities.size();    if (sz < 2) {        return Double.MAX_VALUE;    }    return Math.abs(perplexities.get(sz - 1) - perplexities.get(sz - 2)) / perplexities.get(0);}
0
private double calculatePerplexity(Configuration conf, Path corpusPath, Path modelPath, int iteration) throws IOException, ClassNotFoundException, InterruptedException
{    String jobName = "Calculating perplexity for " + modelPath;        Path outputPath = perplexityPath(modelPath.getParent(), iteration);    Job job = prepareJob(corpusPath, outputPath, CachingCVB0PerplexityMapper.class, DoubleWritable.class, DoubleWritable.class, DualDoubleSumReducer.class, DoubleWritable.class, DoubleWritable.class);    job.setJobName(jobName);    job.setCombinerClass(DualDoubleSumReducer.class);    job.setNumReduceTasks(1);    setModelPaths(job, modelPath);    HadoopUtil.delete(conf, outputPath);    if (!job.waitForCompletion(true)) {        throw new InterruptedException("Failed to calculate perplexity for: " + modelPath);    }    return readPerplexity(conf, modelPath.getParent(), iteration);}
1
public void run(Context context) throws IOException, InterruptedException
{    double keySum = 0.0;    double valueSum = 0.0;    while (context.nextKey()) {        keySum += context.getCurrentKey().get();        for (DoubleWritable value : context.getValues()) {            valueSum += value.get();        }    }    outKey.set(keySum);    outValue.set(valueSum);    context.write(outKey, outValue);}
0
public static double readPerplexity(Configuration conf, Path topicModelStateTemp, int iteration) throws IOException
{    Path perplexityPath = perplexityPath(topicModelStateTemp, iteration);    FileSystem fs = FileSystem.get(perplexityPath.toUri(), conf);    if (!fs.exists(perplexityPath)) {                return Double.NaN;    }    double perplexity = 0;    double modelWeight = 0;    long n = 0;    for (Pair<DoubleWritable, DoubleWritable> pair : new SequenceFileDirIterable<DoubleWritable, DoubleWritable>(perplexityPath, PathType.LIST, PathFilters.partFilter(), null, true, conf)) {        modelWeight += pair.getFirst().get();        perplexity += pair.getSecond().get();        n++;    }        return perplexity / modelWeight;}
1
private Job writeTopicModel(Configuration conf, Path modelInput, Path output) throws IOException, InterruptedException, ClassNotFoundException
{    String jobName = String.format("Writing final topic/term distributions from %s to %s", modelInput, output);        Job job = prepareJob(modelInput, output, SequenceFileInputFormat.class, CVB0TopicTermVectorNormalizerMapper.class, IntWritable.class, VectorWritable.class, SequenceFileOutputFormat.class, jobName);    job.submit();    return job;}
1
private Job writeDocTopicInference(Configuration conf, Path corpus, Path modelInput, Path output) throws IOException, ClassNotFoundException, InterruptedException
{    String jobName = String.format("Writing final document/topic inference from %s to %s", corpus, output);        Job job = prepareJob(corpus, output, SequenceFileInputFormat.class, CVB0DocInferenceMapper.class, IntWritable.class, VectorWritable.class, SequenceFileOutputFormat.class, jobName);    FileSystem fs = FileSystem.get(corpus.toUri(), conf);    if (modelInput != null && fs.exists(modelInput)) {        FileStatus[] statuses = fs.listStatus(modelInput, PathFilters.partFilter());        URI[] modelUris = new URI[statuses.length];        for (int i = 0; i < statuses.length; i++) {            modelUris[i] = statuses[i].getPath().toUri();        }        DistributedCache.setCacheFiles(modelUris, conf);        setModelPaths(job, modelInput);    }    job.submit();    return job;}
1
public static Path modelPath(Path topicModelStateTempPath, int iterationNumber)
{    return new Path(topicModelStateTempPath, "model-" + iterationNumber);}
0
public static Path perplexityPath(Path topicModelStateTempPath, int iterationNumber)
{    return new Path(topicModelStateTempPath, "perplexity-" + iterationNumber);}
0
private static int getCurrentIterationNumber(Configuration config, Path modelTempDir, int maxIterations) throws IOException
{    FileSystem fs = FileSystem.get(modelTempDir.toUri(), config);    int iterationNumber = 1;    Path iterationPath = modelPath(modelTempDir, iterationNumber);    while (fs.exists(iterationPath) && iterationNumber <= maxIterations) {                iterationNumber++;        iterationPath = modelPath(modelTempDir, iterationNumber);    }    return iterationNumber - 1;}
1
public void runIteration(Configuration conf, Path corpusInput, Path modelInput, Path modelOutput, int iterationNumber, int maxIterations, int numReduceTasks) throws IOException, ClassNotFoundException, InterruptedException
{    String jobName = String.format("Iteration %d of %d, input path: %s", iterationNumber, maxIterations, modelInput);        Job job = prepareJob(corpusInput, modelOutput, CachingCVB0Mapper.class, IntWritable.class, VectorWritable.class, VectorSumReducer.class, IntWritable.class, VectorWritable.class);    job.setCombinerClass(VectorSumReducer.class);    job.setNumReduceTasks(numReduceTasks);    job.setJobName(jobName);    setModelPaths(job, modelInput);    HadoopUtil.delete(conf, modelOutput);    if (!job.waitForCompletion(true)) {        throw new InterruptedException(String.format("Failed to complete iteration %d stage 1", iterationNumber));    }}
1
private static void setModelPaths(Job job, Path modelPath) throws IOException
{    Configuration conf = job.getConfiguration();    if (modelPath == null || !FileSystem.get(modelPath.toUri(), conf).exists(modelPath)) {        return;    }    FileStatus[] statuses = FileSystem.get(modelPath.toUri(), conf).listStatus(modelPath, PathFilters.partFilter());    Preconditions.checkState(statuses.length > 0, "No part files found in model path '%s'", modelPath.toString());    String[] modelPaths = new String[statuses.length];    for (int i = 0; i < statuses.length; i++) {        modelPaths[i] = statuses[i].getPath().toUri().toString();    }    conf.setStrings(MODEL_PATHS, modelPaths);}
0
public static Path[] getModelPaths(Configuration conf)
{    String[] modelPathNames = conf.getStrings(MODEL_PATHS);    if (modelPathNames == null || modelPathNames.length == 0) {        return null;    }    Path[] modelPaths = new Path[modelPathNames.length];    for (int i = 0; i < modelPathNames.length; i++) {        modelPaths[i] = new Path(modelPathNames[i]);    }    return modelPaths;}
0
public static void main(String[] args) throws Exception
{    ToolRunner.run(new Configuration(), new CVB0Driver(), args);}
0
protected void map(IntWritable key, VectorWritable value, Context context) throws IOException, InterruptedException
{    value.get().assign(Functions.div(value.get().norm(1.0)));    context.write(key, value);}
0
public void setVerbose(boolean verbose)
{    this.verbose = verbose;}
0
private void postInitCorpus()
{    totalCorpusWeight = 0;    int numNonZero = 0;    for (int i = 0; i < numDocuments; i++) {        Vector v = corpusWeights.viewRow(i);        double norm;        if (v != null && (norm = v.norm(1)) != 0) {            numNonZero += v.getNumNondefaultElements();            totalCorpusWeight += norm;        }    }    String s = "Initializing corpus with %d docs, %d terms, %d nonzero entries, total termWeight %f";    }
1
private void initializeModel()
{    TopicModel topicModel = new TopicModel(numTopics, numTerms, eta, alpha, RandomUtils.getRandom(), terms, numUpdatingThreads, initialModelCorpusFraction == 0 ? 1 : initialModelCorpusFraction * totalCorpusWeight);    topicModel.setConf(getConf());    TopicModel updatedModel = initialModelCorpusFraction == 0 ? new TopicModel(numTopics, numTerms, eta, alpha, null, terms, numUpdatingThreads, 1) : topicModel;    updatedModel.setConf(getConf());    docTopicCounts = new DenseMatrix(numDocuments, numTopics);    docTopicCounts.assign(1.0 / numTopics);    modelTrainer = new ModelTrainer(topicModel, updatedModel, numTrainingThreads, numTopics, numTerms);}
0
public void trainDocuments()
{    trainDocuments(0);}
0
public void trainDocuments(double testFraction)
{    long start = System.nanoTime();    modelTrainer.start();    for (int docId = 0; docId < corpusWeights.numRows(); docId++) {        if (testFraction == 0 || docId % (1 / testFraction) != 0) {                        Vector docTopics = new DenseVector(numTopics).assign(1.0 / numTopics);            modelTrainer.trainSync(corpusWeights.viewRow(docId), docTopics, true, 10);        }    }    modelTrainer.stop();    logTime("train documents", System.nanoTime() - start);}
0
public double iterateUntilConvergence(double minFractionalErrorChange, int maxIterations, int minIter)
{    return iterateUntilConvergence(minFractionalErrorChange, maxIterations, minIter, 0);}
0
public double iterateUntilConvergence(double minFractionalErrorChange, int maxIterations, int minIter, double testFraction)
{    int iter = 0;    double oldPerplexity = 0;    while (iter < minIter) {        trainDocuments(testFraction);        if (verbose) {                    }                oldPerplexity = modelTrainer.calculatePerplexity(corpusWeights, docTopicCounts, testFraction);                iter++;    }    double newPerplexity = 0;    double fractionalChange = Double.MAX_VALUE;    while (iter < maxIterations && fractionalChange > minFractionalErrorChange) {        trainDocuments();        if (verbose) {                    }        newPerplexity = modelTrainer.calculatePerplexity(corpusWeights, docTopicCounts, testFraction);                iter++;        fractionalChange = Math.abs(newPerplexity - oldPerplexity) / oldPerplexity;                oldPerplexity = newPerplexity;    }    if (iter < maxIterations) {            } else {            }    return newPerplexity;}
1
public void writeModel(Path outputPath) throws IOException
{    modelTrainer.persist(outputPath);}
0
private static void logTime(String label, long nanos)
{    }
1
public static int main2(String[] args, Configuration conf) throws Exception
{    DefaultOptionBuilder obuilder = new DefaultOptionBuilder();    ArgumentBuilder abuilder = new ArgumentBuilder();    GroupBuilder gbuilder = new GroupBuilder();    Option helpOpt = DefaultOptionCreator.helpOption();    Option inputDirOpt = obuilder.withLongName("input").withRequired(true).withArgument(abuilder.withName("input").withMinimum(1).withMaximum(1).create()).withDescription("The Directory on HDFS containing the collapsed, properly formatted files having " + "one doc per line").withShortName("i").create();    Option dictOpt = obuilder.withLongName("dictionary").withRequired(false).withArgument(abuilder.withName("dictionary").withMinimum(1).withMaximum(1).create()).withDescription("The path to the term-dictionary format is ... ").withShortName("d").create();    Option dfsOpt = obuilder.withLongName("dfs").withRequired(false).withArgument(abuilder.withName("dfs").withMinimum(1).withMaximum(1).create()).withDescription("HDFS namenode URI").withShortName("dfs").create();    Option numTopicsOpt = obuilder.withLongName("numTopics").withRequired(true).withArgument(abuilder.withName("numTopics").withMinimum(1).withMaximum(1).create()).withDescription("Number of topics to learn").withShortName("top").create();    Option outputTopicFileOpt = obuilder.withLongName("topicOutputFile").withRequired(true).withArgument(abuilder.withName("topicOutputFile").withMinimum(1).withMaximum(1).create()).withDescription("File to write out p(term | topic)").withShortName("to").create();    Option outputDocFileOpt = obuilder.withLongName("docOutputFile").withRequired(true).withArgument(abuilder.withName("docOutputFile").withMinimum(1).withMaximum(1).create()).withDescription("File to write out p(topic | docid)").withShortName("do").create();    Option alphaOpt = obuilder.withLongName("alpha").withRequired(false).withArgument(abuilder.withName("alpha").withMinimum(1).withMaximum(1).withDefault("0.1").create()).withDescription("Smoothing parameter for p(topic | document) prior").withShortName("a").create();    Option etaOpt = obuilder.withLongName("eta").withRequired(false).withArgument(abuilder.withName("eta").withMinimum(1).withMaximum(1).withDefault("0.1").create()).withDescription("Smoothing parameter for p(term | topic)").withShortName("e").create();    Option maxIterOpt = obuilder.withLongName("maxIterations").withRequired(false).withArgument(abuilder.withName("maxIterations").withMinimum(1).withMaximum(1).withDefault("10").create()).withDescription("Maximum number of training passes").withShortName("m").create();    Option modelCorpusFractionOption = obuilder.withLongName("modelCorpusFraction").withRequired(false).withArgument(abuilder.withName("modelCorpusFraction").withMinimum(1).withMaximum(1).withDefault("0.0").create()).withShortName("mcf").withDescription("For online updates, initial value of |model|/|corpus|").create();    Option burnInOpt = obuilder.withLongName("burnInIterations").withRequired(false).withArgument(abuilder.withName("burnInIterations").withMinimum(1).withMaximum(1).withDefault("5").create()).withDescription("Minimum number of iterations").withShortName("b").create();    Option convergenceOpt = obuilder.withLongName("convergence").withRequired(false).withArgument(abuilder.withName("convergence").withMinimum(1).withMaximum(1).withDefault("0.0").create()).withDescription("Fractional rate of perplexity to consider convergence").withShortName("c").create();    Option reInferDocTopicsOpt = obuilder.withLongName("reInferDocTopics").withRequired(false).withArgument(abuilder.withName("reInferDocTopics").withMinimum(1).withMaximum(1).withDefault("no").create()).withDescription("re-infer p(topic | doc) : [no | randstart | continue]").withShortName("rdt").create();    Option numTrainThreadsOpt = obuilder.withLongName("numTrainThreads").withRequired(false).withArgument(abuilder.withName("numTrainThreads").withMinimum(1).withMaximum(1).withDefault("1").create()).withDescription("number of threads to train with").withShortName("ntt").create();    Option numUpdateThreadsOpt = obuilder.withLongName("numUpdateThreads").withRequired(false).withArgument(abuilder.withName("numUpdateThreads").withMinimum(1).withMaximum(1).withDefault("1").create()).withDescription("number of threads to update the model with").withShortName("nut").create();    Option verboseOpt = obuilder.withLongName("verbose").withRequired(false).withArgument(abuilder.withName("verbose").withMinimum(1).withMaximum(1).withDefault("false").create()).withDescription("print verbose information, like top-terms in each topic, during iteration").withShortName("v").create();    Group group = gbuilder.withName("Options").withOption(inputDirOpt).withOption(numTopicsOpt).withOption(alphaOpt).withOption(etaOpt).withOption(maxIterOpt).withOption(burnInOpt).withOption(convergenceOpt).withOption(dictOpt).withOption(reInferDocTopicsOpt).withOption(outputDocFileOpt).withOption(outputTopicFileOpt).withOption(dfsOpt).withOption(numTrainThreadsOpt).withOption(numUpdateThreadsOpt).withOption(modelCorpusFractionOption).withOption(verboseOpt).create();    try {        Parser parser = new Parser();        parser.setGroup(group);        parser.setHelpOption(helpOpt);        CommandLine cmdLine = parser.parse(args);        if (cmdLine.hasOption(helpOpt)) {            CommandLineUtil.printHelp(group);            return -1;        }        String inputDirString = (String) cmdLine.getValue(inputDirOpt);        String dictDirString = cmdLine.hasOption(dictOpt) ? (String) cmdLine.getValue(dictOpt) : null;        int numTopics = Integer.parseInt((String) cmdLine.getValue(numTopicsOpt));        double alpha = Double.parseDouble((String) cmdLine.getValue(alphaOpt));        double eta = Double.parseDouble((String) cmdLine.getValue(etaOpt));        int maxIterations = Integer.parseInt((String) cmdLine.getValue(maxIterOpt));        int burnInIterations = Integer.parseInt((String) cmdLine.getValue(burnInOpt));        double minFractionalErrorChange = Double.parseDouble((String) cmdLine.getValue(convergenceOpt));        int numTrainThreads = Integer.parseInt((String) cmdLine.getValue(numTrainThreadsOpt));        int numUpdateThreads = Integer.parseInt((String) cmdLine.getValue(numUpdateThreadsOpt));        String topicOutFile = (String) cmdLine.getValue(outputTopicFileOpt);        String docOutFile = (String) cmdLine.getValue(outputDocFileOpt);                boolean verbose = Boolean.parseBoolean((String) cmdLine.getValue(verboseOpt));        double modelCorpusFraction = Double.parseDouble((String) cmdLine.getValue(modelCorpusFractionOption));        long start = System.nanoTime();        if (conf.get("fs.default.name") == null) {            String dfsNameNode = (String) cmdLine.getValue(dfsOpt);            conf.set("fs.default.name", dfsNameNode);        }        String[] terms = loadDictionary(dictDirString, conf);        logTime("dictionary loading", System.nanoTime() - start);        start = System.nanoTime();        Matrix corpus = loadVectors(inputDirString, conf);        logTime("vector seqfile corpus loading", System.nanoTime() - start);        start = System.nanoTime();        InMemoryCollapsedVariationalBayes0 cvb0 = new InMemoryCollapsedVariationalBayes0(corpus, terms, numTopics, alpha, eta, numTrainThreads, numUpdateThreads, modelCorpusFraction);        logTime("cvb0 init", System.nanoTime() - start);        start = System.nanoTime();        cvb0.setVerbose(verbose);        cvb0.iterateUntilConvergence(minFractionalErrorChange, maxIterations, burnInIterations);        logTime("total training time", System.nanoTime() - start);        /*      if ("randstart".equalsIgnoreCase(reInferDocTopics)) {        cvb0.inferDocuments(0.0, 100, true);      } else if ("continue".equalsIgnoreCase(reInferDocTopics)) {        cvb0.inferDocuments(0.0, 100, false);      }       */        start = System.nanoTime();        cvb0.writeModel(new Path(topicOutFile));        DistributedRowMatrixWriter.write(new Path(docOutFile), conf, cvb0.docTopicCounts);        logTime("printTopics", System.nanoTime() - start);    } catch (OptionException e) {                CommandLineUtil.printHelp(group);    }    return 0;}
1
private static String[] loadDictionary(String dictionaryPath, Configuration conf)
{    if (dictionaryPath == null) {        return null;    }    Path dictionaryFile = new Path(dictionaryPath);    List<Pair<Integer, String>> termList = new ArrayList<>();    int maxTermId = 0;        for (Pair<Writable, IntWritable> record : new SequenceFileIterable<Writable, IntWritable>(dictionaryFile, true, conf)) {        termList.add(new Pair<>(record.getSecond().get(), record.getFirst().toString()));        maxTermId = Math.max(maxTermId, record.getSecond().get());    }    String[] terms = new String[maxTermId + 1];    for (Pair<Integer, String> pair : termList) {        terms[pair.getFirst()] = pair.getSecond();    }    return terms;}
0
public Configuration getConf()
{    return super.getConf();}
0
private static Matrix loadVectors(String vectorPathString, Configuration conf) throws IOException
{    Path vectorPath = new Path(vectorPathString);    FileSystem fs = vectorPath.getFileSystem(conf);    List<Path> subPaths = new ArrayList<>();    if (fs.isFile(vectorPath)) {        subPaths.add(vectorPath);    } else {        for (FileStatus fileStatus : fs.listStatus(vectorPath, PathFilters.logsCRCFilter())) {            subPaths.add(fileStatus.getPath());        }    }    List<Pair<Integer, Vector>> rowList = new ArrayList<>();    int numRows = Integer.MIN_VALUE;    int numCols = -1;    boolean sequentialAccess = false;    for (Path subPath : subPaths) {        for (Pair<IntWritable, VectorWritable> record : new SequenceFileIterable<IntWritable, VectorWritable>(subPath, true, conf)) {            int id = record.getFirst().get();            Vector vector = record.getSecond().get();            if (vector instanceof NamedVector) {                vector = ((NamedVector) vector).getDelegate();            }            if (numCols < 0) {                numCols = vector.size();                sequentialAccess = vector.isSequentialAccess();            }            rowList.add(Pair.of(id, vector));            numRows = Math.max(numRows, id);        }    }    numRows++;    Vector[] rowVectors = new Vector[numRows];    for (Pair<Integer, Vector> pair : rowList) {        rowVectors[pair.getFirst()] = pair.getSecond();    }    return new SparseRowMatrix(numRows, numCols, rowVectors, true, !sequentialAccess);}
0
public int run(String[] strings) throws Exception
{    return main2(strings, getConf());}
0
public static void main(String[] args) throws Exception
{    ToolRunner.run(new InMemoryCollapsedVariationalBayes0(), args);}
0
public TopicModel getReadModel()
{    return readModel;}
0
public void start()
{        workQueue = new ArrayBlockingQueue<>(numTrainThreads * 10);    threadPool = new ThreadPoolExecutor(numTrainThreads, numTrainThreads, 0, TimeUnit.SECONDS, workQueue);    threadPool.allowCoreThreadTimeOut(false);    threadPool.prestartAllCoreThreads();    writeModel.reset();}
1
public void train(VectorIterable matrix, VectorIterable docTopicCounts)
{    train(matrix, docTopicCounts, 1);}
0
public double calculatePerplexity(VectorIterable matrix, VectorIterable docTopicCounts)
{    return calculatePerplexity(matrix, docTopicCounts, 0);}
0
public double calculatePerplexity(VectorIterable matrix, VectorIterable docTopicCounts, double testFraction)
{    Iterator<MatrixSlice> docIterator = matrix.iterator();    Iterator<MatrixSlice> docTopicIterator = docTopicCounts.iterator();    double perplexity = 0;    double matrixNorm = 0;    while (docIterator.hasNext() && docTopicIterator.hasNext()) {        MatrixSlice docSlice = docIterator.next();        MatrixSlice topicSlice = docTopicIterator.next();        int docId = docSlice.index();        Vector document = docSlice.vector();        Vector topicDist = topicSlice.vector();        if (testFraction == 0 || docId % (1 / testFraction) == 0) {            trainSync(document, topicDist, false, 10);            perplexity += readModel.perplexity(document, topicDist);            matrixNorm += document.norm(1);        }    }    return perplexity / matrixNorm;}
0
public void train(VectorIterable matrix, VectorIterable docTopicCounts, int numDocTopicIters)
{    start();    Iterator<MatrixSlice> docIterator = matrix.iterator();    Iterator<MatrixSlice> docTopicIterator = docTopicCounts.iterator();    long startTime = System.nanoTime();    int i = 0;    double[] times = new double[100];    Map<Vector, Vector> batch = new HashMap<>();    int numTokensInBatch = 0;    long batchStart = System.nanoTime();    while (docIterator.hasNext() && docTopicIterator.hasNext()) {        i++;        Vector document = docIterator.next().vector();        Vector topicDist = docTopicIterator.next().vector();        if (isReadWrite) {            if (batch.size() < numTrainThreads) {                batch.put(document, topicDist);                if (log.isDebugEnabled()) {                    numTokensInBatch += document.getNumNondefaultElements();                }            } else {                batchTrain(batch, true, numDocTopicIters);                long time = System.nanoTime();                                batchStart = time;                numTokensInBatch = 0;            }        } else {            long start = System.nanoTime();            train(document, topicDist, true, numDocTopicIters);            if (log.isDebugEnabled()) {                times[i % times.length] = (System.nanoTime() - start) / (1.0e6 * document.getNumNondefaultElements());                if (i % 100 == 0) {                    long time = System.nanoTime() - startTime;                                        if (i % 500 == 0) {                        Arrays.sort(times);                                            }                }            }        }    }    stop();}
1
public void batchTrain(Map<Vector, Vector> batch, boolean update, int numDocTopicsIters)
{    while (true) {        try {            List<TrainerRunnable> runnables = new ArrayList<>();            for (Map.Entry<Vector, Vector> entry : batch.entrySet()) {                runnables.add(new TrainerRunnable(readModel, null, entry.getKey(), entry.getValue(), new SparseRowMatrix(numTopics, numTerms, true), numDocTopicsIters));            }            threadPool.invokeAll(runnables);            if (update) {                for (TrainerRunnable runnable : runnables) {                    writeModel.update(runnable.docTopicModel);                }            }            break;        } catch (InterruptedException e) {                    }    }}
1
public void train(Vector document, Vector docTopicCounts, boolean update, int numDocTopicIters)
{    while (true) {        try {            workQueue.put(new TrainerRunnable(readModel, update ? writeModel : null, document, docTopicCounts, new SparseRowMatrix(numTopics, numTerms, true), numDocTopicIters));            return;        } catch (InterruptedException e) {                    }    }}
1
public void trainSync(Vector document, Vector docTopicCounts, boolean update, int numDocTopicIters)
{    new TrainerRunnable(readModel, update ? writeModel : null, document, docTopicCounts, new SparseRowMatrix(numTopics, numTerms, true), numDocTopicIters).run();}
0
public double calculatePerplexity(Vector document, Vector docTopicCounts, int numDocTopicIters)
{    TrainerRunnable runner = new TrainerRunnable(readModel, null, document, docTopicCounts, new SparseRowMatrix(numTopics, numTerms, true), numDocTopicIters);    return runner.call();}
0
public void stop()
{    long startTime = System.nanoTime();        try {        threadPool.shutdown();        if (!threadPool.awaitTermination(60, TimeUnit.SECONDS)) {                    }        long newTime = System.nanoTime();                startTime = newTime;        readModel.stop();        newTime = System.nanoTime();                startTime = newTime;        writeModel.stop();        newTime = System.nanoTime();                TopicModel tmpModel = writeModel;        writeModel = readModel;        readModel = tmpModel;    } catch (InterruptedException e) {            }}
1
public void persist(Path outputPath) throws IOException
{    readModel.persist(outputPath, true);}
0
public void run()
{    for (int i = 0; i < numDocTopicIters; i++) {                readModel.trainDocTopicModel(document, docTopics, docTopicModel);    }    if (writeModel != null) {                                writeModel.update(docTopicModel);    }}
0
public Double call()
{    run();    return readModel.perplexity(document, docTopics);}
0
public int getNumTerms()
{    return numTerms;}
0
public int getNumTopics()
{    return numTopics;}
0
private static Vector viewRowSums(Matrix m)
{    Vector v = new DenseVector(m.numRows());    for (MatrixSlice slice : m) {        v.set(slice.index(), slice.vector().norm(1));    }    return v;}
0
private synchronized void initializeThreadPool()
{    if (threadPool != null) {        threadPool.shutdown();        try {            threadPool.awaitTermination(100, TimeUnit.SECONDS);        } catch (InterruptedException e) {                    }    }    threadPool = new ThreadPoolExecutor(numThreads, numThreads, 0, TimeUnit.SECONDS, new ArrayBlockingQueue<Runnable>(numThreads * 10));    threadPool.allowCoreThreadTimeOut(false);    updaters = new Updater[numThreads];    for (int i = 0; i < numThreads; i++) {        updaters[i] = new Updater();        threadPool.submit(updaters[i]);    }}
1
 Matrix topicTermCounts()
{    return topicTermCounts;}
0
public Iterator<MatrixSlice> iterator()
{    return topicTermCounts.iterateAll();}
0
public Vector topicSums()
{    return topicSums;}
0
private static Pair<Matrix, Vector> randomMatrix(int numTopics, int numTerms, Random random)
{    Matrix topicTermCounts = new DenseMatrix(numTopics, numTerms);    Vector topicSums = new DenseVector(numTopics);    if (random != null) {        for (int x = 0; x < numTopics; x++) {            for (int term = 0; term < numTerms; term++) {                topicTermCounts.viewRow(x).set(term, random.nextDouble());            }        }    }    for (int x = 0; x < numTopics; x++) {        topicSums.set(x, random == null ? 1.0 : topicTermCounts.viewRow(x).norm(1));    }    return Pair.of(topicTermCounts, topicSums);}
0
public static Pair<Matrix, Vector> loadModel(Configuration conf, Path... modelPaths) throws IOException
{    int numTopics = -1;    int numTerms = -1;    List<Pair<Integer, Vector>> rows = new ArrayList<>();    for (Path modelPath : modelPaths) {        for (Pair<IntWritable, VectorWritable> row : new SequenceFileIterable<IntWritable, VectorWritable>(modelPath, true, conf)) {            rows.add(Pair.of(row.getFirst().get(), row.getSecond().get()));            numTopics = Math.max(numTopics, row.getFirst().get());            if (numTerms < 0) {                numTerms = row.getSecond().get().size();            }        }    }    if (rows.isEmpty()) {        throw new IOException(Arrays.toString(modelPaths) + " have no vectors in it");    }    numTopics++;    Matrix model = new DenseMatrix(numTopics, numTerms);    Vector topicSums = new DenseVector(numTopics);    for (Pair<Integer, Vector> pair : rows) {        model.viewRow(pair.getFirst()).assign(pair.getSecond());        topicSums.set(pair.getFirst(), pair.getSecond().norm(1));    }    return Pair.of(model, topicSums);}
0
public String toString()
{    StringBuilder buf = new StringBuilder();    for (int x = 0; x < numTopics; x++) {        String v = dictionary != null ? vectorToSortedString(topicTermCounts.viewRow(x).normalize(1), dictionary) : topicTermCounts.viewRow(x).asFormatString();        buf.append(v).append('\n');    }    return buf.toString();}
0
public int sampleTerm(Vector topicDistribution)
{    return sampler.sample(topicTermCounts.viewRow(sampler.sample(topicDistribution)));}
0
public int sampleTerm(int topic)
{    return sampler.sample(topicTermCounts.viewRow(topic));}
0
public synchronized void reset()
{    for (int x = 0; x < numTopics; x++) {        topicTermCounts.assignRow(x, new SequentialAccessSparseVector(numTerms));    }    topicSums.assign(1.0);    if (threadPool.isTerminated()) {        initializeThreadPool();    }}
0
public synchronized void stop()
{    for (Updater updater : updaters) {        updater.shutdown();    }    threadPool.shutdown();    try {        if (!threadPool.awaitTermination(60, TimeUnit.SECONDS)) {                    }    } catch (InterruptedException e) {            }}
1
public void renormalize()
{    for (int x = 0; x < numTopics; x++) {        topicTermCounts.assignRow(x, topicTermCounts.viewRow(x).normalize(1));        topicSums.assign(1.0);    }}
0
public void trainDocTopicModel(Vector original, Vector topics, Matrix docTopicModel)
{            pTopicGivenTerm(original, topics, docTopicModel);    normalizeByTopic(docTopicModel);        for (Element e : original.nonZeroes()) {        for (int x = 0; x < numTopics; x++) {            Vector docTopicModelRow = docTopicModel.viewRow(x);            docTopicModelRow.setQuick(e.index(), docTopicModelRow.getQuick(e.index()) * e.get());        }    }        topics.assign(0.0);    for (int x = 0; x < numTopics; x++) {        topics.set(x, docTopicModel.viewRow(x).norm(1));    }        topics.assign(Functions.mult(1 / topics.norm(1)));}
0
public Vector infer(Vector original, Vector docTopics)
{    Vector pTerm = original.like();    for (Element e : original.nonZeroes()) {        int term = e.index();                double pA = 0;        for (int x = 0; x < numTopics; x++) {            pA += (topicTermCounts.viewRow(x).get(term) / topicSums.get(x)) * docTopics.get(x);        }        pTerm.set(term, pA);    }    return pTerm;}
0
public void update(Matrix docTopicCounts)
{    for (int x = 0; x < numTopics; x++) {        updaters[x % updaters.length].update(x, docTopicCounts.viewRow(x));    }}
0
public void updateTopic(int topic, Vector docTopicCounts)
{    topicTermCounts.viewRow(topic).assign(docTopicCounts, Functions.PLUS);    topicSums.set(topic, topicSums.get(topic) + docTopicCounts.norm(1));}
0
public void update(int termId, Vector topicCounts)
{    for (int x = 0; x < numTopics; x++) {        Vector v = topicTermCounts.viewRow(x);        v.set(termId, v.get(termId) + topicCounts.get(x));    }    topicSums.assign(topicCounts, Functions.PLUS);}
0
public void persist(Path outputDir, boolean overwrite) throws IOException
{    FileSystem fs = outputDir.getFileSystem(conf);    if (overwrite) {                fs.delete(outputDir, true);    }    DistributedRowMatrixWriter.write(outputDir, conf, topicTermCounts);}
0
private void pTopicGivenTerm(Vector document, Vector docTopics, Matrix termTopicDist)
{        for (int x = 0; x < numTopics; x++) {                double topicWeight = docTopics == null ? 1.0 : docTopics.get(x);                Vector topicTermRow = topicTermCounts.viewRow(x);                double topicSum = topicSums.get(x);                Vector termTopicRow = termTopicDist.viewRow(x);                for (Element e : document.nonZeroes()) {            int termIndex = e.index();                        double termTopicLikelihood = (topicTermRow.get(termIndex) + eta) * (topicWeight + alpha) / (topicSum + eta * numTerms);            termTopicRow.set(termIndex, termTopicLikelihood);        }    }}
0
public double perplexity(Vector document, Vector docTopics)
{    double perplexity = 0;    double norm = docTopics.norm(1) + (docTopics.size() * alpha);    for (Element e : document.nonZeroes()) {        int term = e.index();        double prob = 0;        for (int x = 0; x < numTopics; x++) {            double d = (docTopics.get(x) + alpha) / norm;            double p = d * (topicTermCounts.viewRow(x).get(term) + eta) / (topicSums.get(x) + eta * numTerms);            prob += p;        }        perplexity += e.get() * Math.log(prob);    }    return -perplexity;}
0
private void normalizeByTopic(Matrix perTopicSparseDistributions)
{        for (Element e : perTopicSparseDistributions.viewRow(0).nonZeroes()) {        int a = e.index();        double sum = 0;        for (int x = 0; x < numTopics; x++) {            sum += perTopicSparseDistributions.viewRow(x).get(a);        }        for (int x = 0; x < numTopics; x++) {            perTopicSparseDistributions.viewRow(x).set(a, perTopicSparseDistributions.viewRow(x).get(a) / sum);        }    }}
0
public static String vectorToSortedString(Vector vector, String[] dictionary)
{    List<Pair<String, Double>> vectorValues = new ArrayList<>(vector.getNumNondefaultElements());    for (Element e : vector.nonZeroes()) {        vectorValues.add(Pair.of(dictionary != null ? dictionary[e.index()] : String.valueOf(e.index()), e.get()));    }    Collections.sort(vectorValues, new Comparator<Pair<String, Double>>() {        @Override        public int compare(Pair<String, Double> x, Pair<String, Double> y) {            return y.getSecond().compareTo(x.getSecond());        }    });    Iterator<Pair<String, Double>> listIt = vectorValues.iterator();    StringBuilder bldr = new StringBuilder(2048);    bldr.append('{');    int i = 0;    while (listIt.hasNext() && i < 25) {        i++;        Pair<String, Double> p = listIt.next();        bldr.append(p.getFirst());        bldr.append(':');        bldr.append(p.getSecond());        bldr.append(',');    }    if (bldr.length() > 1) {        bldr.setCharAt(bldr.length() - 1, '}');    }    return bldr.toString();}
0
public int compare(Pair<String, Double> x, Pair<String, Double> y)
{    return y.getSecond().compareTo(x.getSecond());}
0
public void setConf(Configuration configuration)
{    this.conf = configuration;}
0
public Configuration getConf()
{    return conf;}
0
public void shutdown()
{    try {        synchronized (this) {            while (!shutdownComplete) {                shutdown = true;                                wait(10000L);            }        }    } catch (InterruptedException e) {            }}
1
public boolean update(int topic, Vector v)
{    if (shutdown) {                throw new IllegalStateException("In SHUTDOWN state: cannot submit tasks");    }    while (true) {                try {                        queue.put(Pair.of(topic, v));                        return true;        } catch (InterruptedException e) {                    }    }}
1
public void run()
{    while (!shutdown) {        try {            Pair<Integer, Vector> pair = queue.poll(1, TimeUnit.SECONDS);            if (pair != null) {                updateTopic(pair.getFirst(), pair.getSecond());            }        } catch (InterruptedException e) {                    }    }        for (Pair<Integer, Vector> pair : queue) {        updateTopic(pair.getFirst(), pair.getSecond());    }    synchronized (this) {        shutdownComplete = true;        notifyAll();    }}
1
public double getN()
{    return sumWeight;}
0
public Vector getMean()
{    return mean;}
0
public Vector getStd()
{    return variance.clone().assign(new SquareRootFunction());}
0
public void observe(Vector x, double weight)
{    double temp = weight + sumWeight;    Vector q;    if (mean == null) {        mean = x.like();        q = x.clone();    } else {        q = x.minus(mean);    }    Vector r = q.times(weight).divide(temp);    if (s == null) {        s = q.times(sumWeight).times(r);    } else {        s = s.plus(q.times(sumWeight).times(r));    }    mean = mean.plus(r);    sumWeight = temp;        variance = s.divide(sumWeight - 1);}
0
public void compute()
{}
0
public double getAverageStd()
{    if (sumWeight == 0.0) {        return 0.0;    } else {        Vector std = getStd();        return std.zSum() / std.size();    }}
0
public Vector getVariance()
{    return variance;}
0
public double getN()
{    return s0;}
0
public Vector getMean()
{    return mean;}
0
public Vector getStd()
{    return std;}
0
public double getAverageStd()
{    if (s0 == 0.0) {        return 0.0;    } else {        return std.zSum() / std.size();    }}
0
public Vector getVariance()
{    return std.times(std);}
0
public void observe(Vector x, double weight)
{    s0 += weight;    Vector weightedX = x.times(weight);    if (s1 == null) {        s1 = weightedX;    } else {        s1.assign(weightedX, Functions.PLUS);    }    Vector x2 = x.times(x).times(weight);    if (s2 == null) {        s2 = x2;    } else {        s2.assign(x2, Functions.PLUS);    }}
0
public void compute()
{    if (s0 != 0.0) {        mean = s1.divide(s0);        std = s2.times(s0).minus(s1.times(s1)).assign(new SquareRootFunction()).divide(s0);    }}
0
public static void runJob(Path input, Path output, int rows, int cols) throws IOException, InterruptedException, ClassNotFoundException
{    Configuration conf = new Configuration();    HadoopUtil.delete(conf, output);    conf.setInt(Keys.AFFINITY_DIMENSIONS, rows);    Job job = new Job(conf, "AffinityMatrixInputJob: " + input + " -> M/R -> " + output);    job.setMapOutputKeyClass(IntWritable.class);    job.setMapOutputValueClass(DistributedRowMatrix.MatrixEntryWritable.class);    job.setOutputKeyClass(IntWritable.class);    job.setOutputValueClass(VectorWritable.class);    job.setOutputFormatClass(SequenceFileOutputFormat.class);    job.setMapperClass(AffinityMatrixInputMapper.class);    job.setReducerClass(AffinityMatrixInputReducer.class);    FileInputFormat.addInputPath(job, input);    FileOutputFormat.setOutputPath(job, output);    job.setJarByClass(AffinityMatrixInputJob.class);    boolean succeeded = job.waitForCompletion(true);    if (!succeeded) {        throw new IllegalStateException("Job failed!");    }}
0
public static DistributedRowMatrix runJob(Path input, Path output, int dimensions) throws IOException, InterruptedException, ClassNotFoundException
{    Path seqFiles = new Path(output, "seqfiles-" + (System.nanoTime() & 0xFF));    runJob(input, seqFiles, dimensions, dimensions);    DistributedRowMatrix a = new DistributedRowMatrix(seqFiles, new Path(seqFiles, "seqtmp-" + (System.nanoTime() & 0xFF)), dimensions, dimensions);    a.setConf(new Configuration());    return a;}
0
protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException
{    String[] elements = COMMA_PATTERN.split(value.toString());            if (elements.length != 3) {        throw new IOException("Expected input of length 3, received " + elements.length + ". Please make sure you adhere to " + "the structure of (i,j,value) for representing a graph in text. " + "Input line was: '" + value + "'.");    }    if (elements[0].isEmpty() || elements[1].isEmpty() || elements[2].isEmpty()) {        throw new IOException("Found an element of 0 length. Please be sure you adhere to the structure of " + "(i,j,value) for  representing a graph in text.");    }                DistributedRowMatrix.MatrixEntryWritable toAdd = new DistributedRowMatrix.MatrixEntryWritable();    IntWritable row = new IntWritable(Integer.valueOf(elements[0]));        toAdd.setRow(-1);    toAdd.setCol(Integer.valueOf(elements[1]));    toAdd.setVal(Double.valueOf(elements[2]));    context.write(row, toAdd);}
1
protected void reduce(IntWritable row, Iterable<DistributedRowMatrix.MatrixEntryWritable> values, Context context) throws IOException, InterruptedException
{    int size = context.getConfiguration().getInt(Keys.AFFINITY_DIMENSIONS, Integer.MAX_VALUE);    RandomAccessSparseVector out = new RandomAccessSparseVector(size, 100);    for (DistributedRowMatrix.MatrixEntryWritable element : values) {        out.setQuick(element.getCol(), element.getVal());        if (log.isDebugEnabled()) {                    }    }    SequentialAccessSparseVector output = new SequentialAccessSparseVector(out);    context.write(row, new VectorWritable(output));}
1
public void setKey(int k)
{    this.key = k;}
0
public void setValue(double v)
{    this.value = v;}
0
public void readFields(DataInput in) throws IOException
{    this.key = in.readInt();    this.value = in.readDouble();}
0
public void write(DataOutput out) throws IOException
{    out.writeInt(key);    out.writeDouble(value);}
0
public int getKey()
{    return key;}
0
public double getValue()
{    return value;}
0
public static Path buildFromEigens(Configuration conf, Path input, Path output, int k, DistanceMeasure measure) throws IOException
{        FileSystem fs = FileSystem.get(output.toUri(), conf);    HadoopUtil.delete(conf, output);    Path outFile = new Path(output, "part-eigenSeed");    boolean newFile = fs.createNewFile(outFile);    if (newFile) {        Path inputPathPattern;        if (fs.getFileStatus(input).isDir()) {            inputPathPattern = new Path(input, "*");        } else {            inputPathPattern = input;        }        FileStatus[] inputFiles = fs.globStatus(inputPathPattern, PathFilters.logsCRCFilter());                Map<Integer, Double> maxEigens = new HashMap<>(k);                                                Map<Integer, Text> chosenTexts = new HashMap<>(k);        Map<Integer, ClusterWritable> chosenClusters = new HashMap<>(k);        for (FileStatus fileStatus : inputFiles) {            if (!fileStatus.isDir()) {                for (Pair<Writable, VectorWritable> record : new SequenceFileIterable<Writable, VectorWritable>(fileStatus.getPath(), true, conf)) {                    Writable key = record.getFirst();                    VectorWritable value = record.getSecond();                    for (Vector.Element e : value.get().nonZeroes()) {                        int index = e.index();                        double v = Math.abs(e.get());                        if (!maxEigens.containsKey(index) || v > maxEigens.get(index)) {                            maxEigens.put(index, v);                            Text newText = new Text(key.toString());                            chosenTexts.put(index, newText);                            Kluster newCluster = new Kluster(value.get(), index, measure);                            newCluster.observe(value.get(), 1);                            ClusterWritable clusterWritable = new ClusterWritable();                            clusterWritable.setValue(newCluster);                            chosenClusters.put(index, clusterWritable);                        }                    }                }            }        }        try (SequenceFile.Writer writer = SequenceFile.createWriter(fs, conf, outFile, Text.class, ClusterWritable.class)) {            for (Integer key : maxEigens.keySet()) {                writer.append(chosenTexts.get(key), chosenClusters.get(key));            }                    }    }    return outFile;}
1
public static void main(String[] args) throws Exception
{    ToolRunner.run(new SpectralKMeansDriver(), args);}
0
public int run(String[] arg0) throws Exception
{    Configuration conf = getConf();    addInputOption();    addOutputOption();    addOption("dimensions", "d", "Square dimensions of affinity matrix", true);    addOption("clusters", "k", "Number of clusters and top eigenvectors", true);    addOption(DefaultOptionCreator.distanceMeasureOption().create());    addOption(DefaultOptionCreator.convergenceOption().create());    addOption(DefaultOptionCreator.maxIterationsOption().create());    addOption(DefaultOptionCreator.overwriteOption().create());    addFlag("usessvd", "ssvd", "Uses SSVD as the eigensolver. Default is the Lanczos solver.");    addOption("reduceTasks", "t", "Number of reducers for SSVD", String.valueOf(REDUCERS));    addOption("outerProdBlockHeight", "oh", "Block height of outer products for SSVD", String.valueOf(BLOCKHEIGHT));    addOption("oversampling", "p", "Oversampling parameter for SSVD", String.valueOf(OVERSAMPLING));    addOption("powerIter", "q", "Additional power iterations for SSVD", String.valueOf(POWERITERS));    Map<String, List<String>> parsedArgs = parseArguments(arg0);    if (parsedArgs == null) {        return 0;    }    Path input = getInputPath();    Path output = getOutputPath();    if (hasOption(DefaultOptionCreator.OVERWRITE_OPTION)) {        HadoopUtil.delete(conf, getTempPath());        HadoopUtil.delete(conf, getOutputPath());    }    int numDims = Integer.parseInt(getOption("dimensions"));    int clusters = Integer.parseInt(getOption("clusters"));    String measureClass = getOption(DefaultOptionCreator.DISTANCE_MEASURE_OPTION);    DistanceMeasure measure = ClassUtils.instantiateAs(measureClass, DistanceMeasure.class);    double convergenceDelta = Double.parseDouble(getOption(DefaultOptionCreator.CONVERGENCE_DELTA_OPTION));    int maxIterations = Integer.parseInt(getOption(DefaultOptionCreator.MAX_ITERATIONS_OPTION));    Path tempdir = new Path(getOption("tempDir"));    int reducers = Integer.parseInt(getOption("reduceTasks"));    int blockheight = Integer.parseInt(getOption("outerProdBlockHeight"));    int oversampling = Integer.parseInt(getOption("oversampling"));    int poweriters = Integer.parseInt(getOption("powerIter"));    run(conf, input, output, numDims, clusters, measure, convergenceDelta, maxIterations, tempdir, reducers, blockheight, oversampling, poweriters);    return 0;}
0
public static void run(Configuration conf, Path input, Path output, int numDims, int clusters, DistanceMeasure measure, double convergenceDelta, int maxIterations, Path tempDir) throws IOException, InterruptedException, ClassNotFoundException
{    run(conf, input, output, numDims, clusters, measure, convergenceDelta, maxIterations, tempDir, REDUCERS, BLOCKHEIGHT, OVERSAMPLING, POWERITERS);}
0
public static void run(Configuration conf, Path input, Path output, int numDims, int clusters, DistanceMeasure measure, double convergenceDelta, int maxIterations, Path tempDir, int numReducers, int blockHeight, int oversampling, int poweriters) throws IOException, InterruptedException, ClassNotFoundException
{    HadoopUtil.delete(conf, tempDir);    Path outputCalc = new Path(tempDir, "calculations");    Path outputTmp = new Path(tempDir, "temporary");                Path affSeqFiles = new Path(outputCalc, "seqfile");    AffinityMatrixInputJob.runJob(input, affSeqFiles, numDims, numDims);        DistributedRowMatrix A = new DistributedRowMatrix(affSeqFiles, new Path(outputTmp, "afftmp"), numDims, numDims);    Configuration depConf = new Configuration(conf);    A.setConf(depConf);        Vector D = MatrixDiagonalizeJob.runJob(affSeqFiles, numDims);        DistributedRowMatrix L = VectorMatrixMultiplicationJob.runJob(affSeqFiles, D, new Path(outputCalc, "laplacian"), new Path(outputCalc, outputCalc));    L.setConf(depConf);    Path data;        Path[] LPath = new Path[1];    LPath[0] = L.getRowPath();    Path SSVDout = new Path(outputCalc, "SSVD");    SSVDSolver solveIt = new SSVDSolver(depConf, LPath, SSVDout, blockHeight, clusters, oversampling, numReducers);    solveIt.setComputeV(false);    solveIt.setComputeU(true);    solveIt.setOverwrite(true);    solveIt.setQ(poweriters);        solveIt.run();    data = new Path(solveIt.getUPath());            Path unitVectors = new Path(outputCalc, "unitvectors");    UnitVectorizerJob.runJob(data, unitVectors);    DistributedRowMatrix Wt = new DistributedRowMatrix(unitVectors, new Path(unitVectors, "tmp"), clusters, numDims);    Wt.setConf(depConf);    data = Wt.getRowPath();            Path initialclusters = EigenSeedGenerator.buildFromEigens(conf, data, new Path(output, Cluster.INITIAL_CLUSTERS_DIR), clusters, measure);        Path answer = new Path(output, "kmeans_out");    KMeansDriver.run(conf, data, initialclusters, answer, convergenceDelta, maxIterations, true, 0.0, false);        Path mappingPath = new Path(new Path(conf.get("hadoop.tmp.dir")), "generic_input_mapping");    List<String> mapping = new ArrayList<>();    FileSystem fs = FileSystem.get(mappingPath.toUri(), conf);    if (fs.exists(mappingPath)) {        SequenceFile.Reader reader = new SequenceFile.Reader(fs, mappingPath, conf);        Text mappingValue = new Text();        IntWritable mappingIndex = new IntWritable();        while (reader.next(mappingIndex, mappingValue)) {            String s = mappingValue.toString();            mapping.add(s);        }        HadoopUtil.delete(conf, mappingPath);    } else {            }    Path clusteredPointsPath = new Path(answer, "clusteredPoints");    Path inputPath = new Path(clusteredPointsPath, "part-m-00000");    int id = 0;    for (Pair<IntWritable, WeightedVectorWritable> record : new SequenceFileIterable<IntWritable, WeightedVectorWritable>(inputPath, conf)) {        if (!mapping.isEmpty()) {                    } else {                    }    }}
1
public static Vector runJob(Path affInput, int dimensions) throws IOException, ClassNotFoundException, InterruptedException
{        Configuration conf = new Configuration();    Path diagOutput = new Path(affInput.getParent(), "diagonal");    HadoopUtil.delete(conf, diagOutput);    conf.setInt(Keys.AFFINITY_DIMENSIONS, dimensions);    Job job = new Job(conf, "MatrixDiagonalizeJob");    job.setInputFormatClass(SequenceFileInputFormat.class);    job.setMapOutputKeyClass(NullWritable.class);    job.setMapOutputValueClass(IntDoublePairWritable.class);    job.setOutputKeyClass(NullWritable.class);    job.setOutputValueClass(VectorWritable.class);    job.setOutputFormatClass(SequenceFileOutputFormat.class);    job.setMapperClass(MatrixDiagonalizeMapper.class);    job.setReducerClass(MatrixDiagonalizeReducer.class);    FileInputFormat.addInputPath(job, affInput);    FileOutputFormat.setOutputPath(job, diagOutput);    job.setJarByClass(MatrixDiagonalizeJob.class);    boolean succeeded = job.waitForCompletion(true);    if (!succeeded) {        throw new IllegalStateException("Job failed!");    }        return VectorCache.load(conf, new Path(diagOutput, "part-r-00000"));}
0
protected void map(IntWritable key, VectorWritable row, Context context) throws IOException, InterruptedException
{        IntDoublePairWritable store = new IntDoublePairWritable(key.get(), row.get().zSum());    context.write(NullWritable.get(), store);}
0
protected void reduce(NullWritable key, Iterable<IntDoublePairWritable> values, Context context) throws IOException, InterruptedException
{        Vector retval = new DenseVector(context.getConfiguration().getInt(Keys.AFFINITY_DIMENSIONS, Integer.MAX_VALUE));        for (IntDoublePairWritable e : values) {        retval.setQuick(e.getKey(), e.getValue());    }        context.write(key, new VectorWritable(retval));}
0
public static void runJob(Path input, Path output) throws IOException, InterruptedException, ClassNotFoundException
{    Configuration conf = new Configuration();    Job job = new Job(conf, "UnitVectorizerJob");    job.setInputFormatClass(SequenceFileInputFormat.class);    job.setOutputKeyClass(IntWritable.class);    job.setOutputValueClass(VectorWritable.class);    job.setOutputFormatClass(SequenceFileOutputFormat.class);    job.setMapperClass(UnitVectorizerMapper.class);    job.setNumReduceTasks(0);    FileInputFormat.addInputPath(job, input);    FileOutputFormat.setOutputPath(job, output);    job.setJarByClass(UnitVectorizerJob.class);    boolean succeeded = job.waitForCompletion(true);    if (!succeeded) {        throw new IllegalStateException("Job failed!");    }}
0
protected void map(IntWritable row, VectorWritable vector, Context context) throws IOException, InterruptedException
{    context.write(row, new VectorWritable(vector.get().normalize(2)));}
0
public static void save(Writable key, Vector vector, Path output, Configuration conf, boolean overwritePath, boolean deleteOnExit) throws IOException
{    FileSystem fs = FileSystem.get(output.toUri(), conf);    output = fs.makeQualified(output);    if (overwritePath) {        HadoopUtil.delete(conf, output);    }        DistributedCache.setCacheFiles(new URI[] { output.toUri() }, conf);        try (SequenceFile.Writer writer = new SequenceFile.Writer(fs, conf, output, IntWritable.class, VectorWritable.class)) {        writer.append(key, new VectorWritable(vector));    }    if (deleteOnExit) {        fs.deleteOnExit(output);    }}
0
public static void save(Writable key, Vector vector, Path output, Configuration conf) throws IOException
{    save(key, vector, output, conf, true, true);}
0
public static Vector load(Configuration conf) throws IOException
{    Path[] files = HadoopUtil.getCachedFiles(conf);    if (files.length != 1) {        throw new IOException("Cannot read Frequency list from Distributed Cache (" + files.length + ')');    }    if (log.isInfoEnabled()) {            }    return load(conf, files[0]);}
1
public static Vector load(Configuration conf, Path input) throws IOException
{        try (SequenceFileValueIterator<VectorWritable> iterator = new SequenceFileValueIterator<>(input, true, conf)) {        return iterator.next().get();    }}
1
public static DistributedRowMatrix runJob(Path markovPath, Vector diag, Path outputPath) throws IOException, ClassNotFoundException, InterruptedException
{    return runJob(markovPath, diag, outputPath, new Path(outputPath, "tmp"));}
0
public static DistributedRowMatrix runJob(Path markovPath, Vector diag, Path outputPath, Path tmpPath) throws IOException, ClassNotFoundException, InterruptedException
{        Configuration conf = new Configuration();    FileSystem fs = FileSystem.get(markovPath.toUri(), conf);    markovPath = fs.makeQualified(markovPath);    outputPath = fs.makeQualified(outputPath);    Path vectorOutputPath = new Path(outputPath.getParent(), "vector");    VectorCache.save(new IntWritable(Keys.DIAGONAL_CACHE_INDEX), diag, vectorOutputPath, conf);        Job job = new Job(conf, "VectorMatrixMultiplication");    job.setInputFormatClass(SequenceFileInputFormat.class);    job.setOutputKeyClass(IntWritable.class);    job.setOutputValueClass(VectorWritable.class);    job.setOutputFormatClass(SequenceFileOutputFormat.class);    job.setMapperClass(VectorMatrixMultiplicationMapper.class);    job.setNumReduceTasks(0);    FileInputFormat.addInputPath(job, markovPath);    FileOutputFormat.setOutputPath(job, outputPath);    job.setJarByClass(VectorMatrixMultiplicationJob.class);    boolean succeeded = job.waitForCompletion(true);    if (!succeeded) {        throw new IllegalStateException("Job failed!");    }        return new DistributedRowMatrix(outputPath, tmpPath, diag.size(), diag.size());}
0
protected void setup(Context context) throws IOException, InterruptedException
{        super.setup(context);    Configuration config = context.getConfiguration();    diagonal = VectorCache.load(config);    if (diagonal == null) {        throw new IOException("No vector loaded from cache!");    }    if (!(diagonal instanceof DenseVector)) {        diagonal = new DenseVector(diagonal);    }}
0
protected void map(IntWritable key, VectorWritable row, Context ctx) throws IOException, InterruptedException
{    for (Vector.Element e : row.get().all()) {        double dii = Functions.SQRT.apply(diagonal.get(key.get()));        double djj = Functions.SQRT.apply(diagonal.get(e.index()));        double mij = e.get();        e.set(dii * mij * djj);    }    ctx.write(key, row);}
0
 void setup(Vector diag)
{    this.diagonal = diag;}
0
public int getRow()
{    return i;}
0
public void setRow(int i)
{    this.i = i;}
0
public int getCol()
{    return j;}
0
public void setCol(int j)
{    this.j = j;}
0
public double getValue()
{    return value;}
0
public void setValue(double v)
{    this.value = v;}
0
public String getType()
{    return type;}
0
public void setType(String t)
{    this.type = t;}
0
public void readFields(DataInput arg0) throws IOException
{    this.i = arg0.readInt();    this.j = arg0.readInt();    this.value = arg0.readDouble();    this.type = arg0.readUTF();}
0
public void write(DataOutput arg0) throws IOException
{    arg0.writeInt(i);    arg0.writeInt(j);    arg0.writeDouble(value);    arg0.writeUTF(type);}
0
public Pair<List<? extends WeightedVector>, List<? extends WeightedVector>> splitTrainTest(List<? extends WeightedVector> datapoints)
{        if (testProbability == 0) {        return new Pair<List<? extends WeightedVector>, List<? extends WeightedVector>>(datapoints, new ArrayList<WeightedVector>());    }    int numTest = (int) (testProbability * datapoints.size());    Preconditions.checkArgument(numTest > 0 && numTest < datapoints.size(), "Must have nonzero number of training and test vectors. Asked for %.1f %% of %d vectors for test", testProbability * 100, datapoints.size());    Collections.shuffle(datapoints);    return new Pair<List<? extends WeightedVector>, List<? extends WeightedVector>>(datapoints.subList(numTest, datapoints.size()), datapoints.subList(0, numTest));}
0
public UpdatableSearcher cluster(List<? extends WeightedVector> datapoints)
{    Pair<List<? extends WeightedVector>, List<? extends WeightedVector>> trainTestSplit = splitTrainTest(datapoints);    List<Vector> bestCentroids = new ArrayList<>();    double cost = Double.POSITIVE_INFINITY;    double bestCost = Double.POSITIVE_INFINITY;    for (int i = 0; i < numRuns; ++i) {        centroids.clear();        if (kMeansPlusPlusInit) {                        initializeSeedsKMeansPlusPlus(trainTestSplit.getFirst());        } else {                        initializeSeedsRandomly(trainTestSplit.getFirst());        }                if (numRuns > 1) {                        iterativeAssignment(trainTestSplit.getFirst());                        cost = ClusteringUtils.totalClusterCost(splitTrainTest ? datapoints : trainTestSplit.getSecond(), centroids);            if (cost < bestCost) {                bestCost = cost;                bestCentroids.clear();                Iterables.addAll(bestCentroids, centroids);            }        } else {                        iterativeAssignment(datapoints);            return centroids;        }    }    if (bestCost == Double.POSITIVE_INFINITY) {        throw new RuntimeException("No valid clustering was found");    }    if (cost != bestCost) {        centroids.clear();        centroids.addAll(bestCentroids);    }    if (correctWeights) {        for (WeightedVector testDatapoint : trainTestSplit.getSecond()) {            WeightedVector closest = (WeightedVector) centroids.searchFirst(testDatapoint, false).getValue();            closest.setWeight(closest.getWeight() + testDatapoint.getWeight());        }    }    return centroids;}
0
private void initializeSeedsRandomly(List<? extends WeightedVector> datapoints)
{    int numDatapoints = datapoints.size();    double totalWeight = 0;    for (WeightedVector datapoint : datapoints) {        totalWeight += datapoint.getWeight();    }    Multinomial<Integer> seedSelector = new Multinomial<>();    for (int i = 0; i < numDatapoints; ++i) {        seedSelector.add(i, datapoints.get(i).getWeight() / totalWeight);    }    for (int i = 0; i < numClusters; ++i) {        int sample = seedSelector.sample();        seedSelector.delete(sample);        Centroid centroid = new Centroid(datapoints.get(sample));        centroid.setIndex(i);        centroids.add(centroid);    }}
0
private void initializeSeedsKMeansPlusPlus(List<? extends WeightedVector> datapoints)
{    Preconditions.checkArgument(datapoints.size() > 1, "Must have at least two datapoints points to cluster " + "sensibly");    Preconditions.checkArgument(datapoints.size() >= numClusters, String.format("Must have more datapoints [%d] than clusters [%d]", datapoints.size(), numClusters));        Centroid center = new Centroid(datapoints.iterator().next());    for (WeightedVector row : Iterables.skip(datapoints, 1)) {        center.update(row);    }            double deltaX = 0;    DistanceMeasure distanceMeasure = centroids.getDistanceMeasure();    for (WeightedVector row : datapoints) {        deltaX += distanceMeasure.distance(row, center);    }                                                                    Multinomial<Integer> seedSelector = new Multinomial<>();    for (int i = 0; i < datapoints.size(); ++i) {        double selectionProbability = deltaX + datapoints.size() * distanceMeasure.distance(datapoints.get(i), center);        seedSelector.add(i, selectionProbability);    }    int selected = random.nextInt(datapoints.size());    Centroid c_1 = new Centroid(datapoints.get(selected).clone());    c_1.setIndex(0);        for (int i = 0; i < datapoints.size(); ++i) {        WeightedVector row = datapoints.get(i);        double w = distanceMeasure.distance(c_1, row) * 2 * Math.log(1 + row.getWeight());        seedSelector.set(i, w);    }                            centroids.add(c_1);    int clusterIndex = 1;    while (centroids.size() < numClusters) {                int seedIndex = seedSelector.sample();        Centroid nextSeed = new Centroid(datapoints.get(seedIndex));        nextSeed.setIndex(clusterIndex++);        centroids.add(nextSeed);                seedSelector.delete(seedIndex);                for (int currSeedIndex : seedSelector) {            WeightedVector curr = datapoints.get(currSeedIndex);            double newWeight = nextSeed.getWeight() * distanceMeasure.distance(nextSeed, curr);            if (newWeight < seedSelector.getWeight(currSeedIndex)) {                seedSelector.set(currSeedIndex, newWeight);            }        }    }}
0
private void iterativeAssignment(List<? extends WeightedVector> datapoints)
{    DistanceMeasure distanceMeasure = centroids.getDistanceMeasure();            List<Double> closestClusterDistances = new ArrayList<>(numClusters);                List<Integer> clusterAssignments = new ArrayList<>(Collections.nCopies(datapoints.size(), -1));    boolean changed = true;    for (int i = 0; changed && i < maxNumIterations; i++) {        changed = false;                                closestClusterDistances.clear();        for (Vector center : centroids) {                        Vector closestOtherCluster = centroids.searchFirst(center, true).getValue();            closestClusterDistances.add(distanceMeasure.distance(center, closestOtherCluster));        }                        List<Centroid> newCentroids = new ArrayList<>();        for (Vector centroid : centroids) {                        Centroid newCentroid = (Centroid) centroid.clone();            newCentroid.setWeight(0);            newCentroids.add(newCentroid);        }                for (int j = 0; j < datapoints.size(); ++j) {            WeightedVector datapoint = datapoints.get(j);                        WeightedThing<Vector> closestPair = centroids.searchFirst(datapoint, false);            int closestIndex = ((WeightedVector) closestPair.getValue()).getIndex();            double closestDistance = closestPair.getWeight();                        if (closestIndex != clusterAssignments.get(j)) {                changed = true;                clusterAssignments.set(j, closestIndex);            }                        if (closestDistance < trimFraction * closestClusterDistances.get(closestIndex)) {                newCentroids.get(closestIndex).update(datapoint);            }        }                centroids.clear();        centroids.addAll(newCentroids);    }    if (correctWeights) {        for (Vector v : centroids) {            ((Centroid) v).setWeight(0);        }        for (WeightedVector datapoint : datapoints) {            Centroid closestCentroid = (Centroid) centroids.searchFirst(datapoint, false).getValue();            closestCentroid.setWeight(closestCentroid.getWeight() + datapoint.getWeight());        }    }}
0
public Iterator<Centroid> iterator()
{    return Iterators.transform(centroids.iterator(), new Function<Vector, Centroid>() {        @Override        public Centroid apply(Vector input) {            Preconditions.checkArgument(input instanceof Centroid, "Non-centroid in centroids " + "searcher");                        return (Centroid) input;        }    });}
0
public Centroid apply(Vector input)
{    Preconditions.checkArgument(input instanceof Centroid, "Non-centroid in centroids " + "searcher");        return (Centroid) input;}
0
public Iterator<Centroid> iterator()
{    return Iterators.transform(centroids.iterator(), new Function<Vector, Centroid>() {        @Override        public Centroid apply(Vector input) {            return (Centroid) input;        }    });}
0
public Centroid apply(Vector input)
{    return (Centroid) input;}
0
public UpdatableSearcher cluster(Matrix data)
{    return cluster(Iterables.transform(data, new Function<MatrixSlice, Centroid>() {        @Override        public Centroid apply(MatrixSlice input) {                        return Centroid.create(input.index(), input.vector());        }    }));}
0
public Centroid apply(MatrixSlice input)
{        return Centroid.create(input.index(), input.vector());}
0
public UpdatableSearcher cluster(Iterable<Centroid> datapoints)
{    return clusterInternal(datapoints, false);}
0
public UpdatableSearcher cluster(final Centroid datapoint)
{    return cluster(new Iterable<Centroid>() {        @Override        public Iterator<Centroid> iterator() {            return new Iterator<Centroid>() {                private boolean accessed = false;                @Override                public boolean hasNext() {                    return !accessed;                }                @Override                public Centroid next() {                    accessed = true;                    return datapoint;                }                @Override                public void remove() {                    throw new UnsupportedOperationException();                }            };        }    });}
0
public Iterator<Centroid> iterator()
{    return new Iterator<Centroid>() {        private boolean accessed = false;        @Override        public boolean hasNext() {            return !accessed;        }        @Override        public Centroid next() {            accessed = true;            return datapoint;        }        @Override        public void remove() {            throw new UnsupportedOperationException();        }    };}
0
public boolean hasNext()
{    return !accessed;}
0
public Centroid next()
{    accessed = true;    return datapoint;}
0
public void remove()
{    throw new UnsupportedOperationException();}
0
public int getNumClusters()
{    return centroids.size();}
0
private UpdatableSearcher clusterInternal(Iterable<Centroid> datapoints, boolean collapseClusters)
{    Iterator<Centroid> datapointsIterator = datapoints.iterator();    if (!datapointsIterator.hasNext()) {        return centroids;    }    int oldNumProcessedDataPoints = numProcessedDatapoints;        if (collapseClusters) {        centroids.clear();        numProcessedDatapoints = 0;    }    if (centroids.size() == 0) {                                centroids.add(datapointsIterator.next().clone());        ++numProcessedDatapoints;    }        while (datapointsIterator.hasNext()) {        Centroid row = datapointsIterator.next();                                WeightedThing<Vector> closestPair = centroids.searchFirst(row, false);                                                                double sample = random.nextDouble();        if (sample < row.getWeight() * closestPair.getWeight() / distanceCutoff) {                        centroids.add(row.clone());        } else {                                                            Centroid centroid = (Centroid) closestPair.getValue();                        if (!centroids.remove(centroid, Constants.EPSILON)) {                throw new RuntimeException("Unable to remove centroid");            }            centroid.update(row);            centroids.add(centroid);        }        ++numProcessedDatapoints;        if (!collapseClusters && centroids.size() > clusterOvershoot * numClusters) {            numClusters = (int) Math.max(numClusters, clusterLogFactor * Math.log(numProcessedDatapoints));            List<Centroid> shuffled = new ArrayList<>();            for (Vector vector : centroids) {                shuffled.add((Centroid) vector);            }            Collections.shuffle(shuffled);                                    clusterInternal(shuffled, true);            if (centroids.size() > numClusters) {                distanceCutoff *= beta;            }        }    }    if (collapseClusters) {        numProcessedDatapoints = oldNumProcessedDataPoints;    }    return centroids;}
0
public void reindexCentroids()
{    int numCentroids = 0;    for (Centroid centroid : this) {        centroid.setIndex(numCentroids++);    }}
0
public double getDistanceCutoff()
{    return distanceCutoff;}
0
public void setDistanceCutoff(double distanceCutoff)
{    this.distanceCutoff = distanceCutoff;}
0
public DistanceMeasure getDistanceMeasure()
{    return centroids.getDistanceMeasure();}
0
public Centroid getCentroid()
{    return centroid;}
0
public void write(DataOutput dataOutput) throws IOException
{    dataOutput.writeInt(centroid.getIndex());    dataOutput.writeDouble(centroid.getWeight());    VectorWritable.writeVector(dataOutput, centroid.getVector());}
0
public void readFields(DataInput dataInput) throws IOException
{    if (centroid == null) {        centroid = read(dataInput);        return;    }    centroid.setIndex(dataInput.readInt());    centroid.setWeight(dataInput.readDouble());    centroid.assign(VectorWritable.readVector(dataInput));}
0
public static Centroid read(DataInput dataInput) throws IOException
{    int index = dataInput.readInt();    double weight = dataInput.readDouble();    Vector v = VectorWritable.readVector(dataInput);    return new Centroid(index, v, weight);}
0
public boolean equals(Object o)
{    if (this == o) {        return true;    }    if (!(o instanceof CentroidWritable)) {        return false;    }    CentroidWritable writable = (CentroidWritable) o;    return centroid.equals(writable.centroid);}
0
public int hashCode()
{    return centroid.hashCode();}
0
public String toString()
{    return centroid.toString();}
0
public int run(String[] args) throws Exception
{        addInputOption();    addOutputOption();    addOption(DefaultOptionCreator.overwriteOption().create());        addOption(DefaultOptionCreator.numClustersOption().withDescription("The k in k-Means. Approximately this many clusters will be generated.").create());                    addOption(ESTIMATED_NUM_MAP_CLUSTERS, "km", "The estimated number of clusters to use for the " + "Map phase of the job when running StreamingKMeans. This should be around k * log(n), " + "where k is the final number of clusters and n is the total number of data points to " + "cluster.", String.valueOf(1));    addOption(ESTIMATED_DISTANCE_CUTOFF, "e", "The initial estimated distance cutoff between two " + "points for forming new clusters. If no value is given, it's estimated from the data set", String.valueOf(INVALID_DISTANCE_CUTOFF));        addOption(MAX_NUM_ITERATIONS, "mi", "The maximum number of iterations to run for the " + "BallKMeans algorithm used by the reducer. If no value is given, defaults to 10.", String.valueOf(10));    addOption(TRIM_FRACTION, "tf", "The 'ball' aspect of ball k-means means that only the closest points " + "to the centroid will actually be used for updating. The fraction of the points to be used is those " + "points whose distance to the center is within trimFraction * distance to the closest other center. " + "If no value is given, defaults to 0.9.", String.valueOf(0.9));    addFlag(RANDOM_INIT, "ri", "Whether to use k-means++ initialization or random initialization " + "of the seed centroids. Essentially, k-means++ provides better clusters, but takes longer, whereas random " + "initialization takes less time, but produces worse clusters, and tends to fail more often and needs " + "multiple runs to compare to k-means++. If set, uses the random initialization.");    addFlag(IGNORE_WEIGHTS, "iw", "Whether to correct the weights of the centroids after the clustering is done. " + "The weights end up being wrong because of the trimFraction and possible train/test splits. In some cases, " + "especially in a pipeline, having an accurate count of the weights is useful. If set, ignores the final " + "weights");    addOption(TEST_PROBABILITY, "testp", "A double value between 0 and 1 that represents the percentage of " + "points to be used for 'testing' different clustering runs in the final BallKMeans " + "step. If no value is given, defaults to 0.1", String.valueOf(0.1));    addOption(NUM_BALLKMEANS_RUNS, "nbkm", "Number of BallKMeans runs to use at the end to try to cluster the " + "points. If no value is given, defaults to 4", String.valueOf(4));                        addOption(DefaultOptionCreator.distanceMeasureOption().create());            addOption(SEARCHER_CLASS_OPTION, "sc", "The type of searcher to be used when performing nearest " + "neighbor searches. Defaults to ProjectionSearch.", ProjectionSearch.class.getCanonicalName());        addOption(NUM_PROJECTIONS_OPTION, "np", "The number of projections considered in estimating the " + "distances between vectors. Only used when the distance measure requested is either " + "ProjectionSearch or FastProjectionSearch. If no value is given, defaults to 3.", String.valueOf(3));    addOption(SEARCH_SIZE_OPTION, "s", "In more efficient searches (non BruteSearch), " + "not all distances are calculated for determining the nearest neighbors. The number of " + "elements whose distances from the query vector is actually computer is proportional to " + "searchSize. If no value is given, defaults to 1.", String.valueOf(2));    addFlag(REDUCE_STREAMING_KMEANS, "rskm", "There might be too many intermediate clusters from the mapper " + "to fit into memory, so the reducer can run another pass of StreamingKMeans to collapse them down to a " + "fewer clusters");    addOption(DefaultOptionCreator.methodOption().create());    if (parseArguments(args) == null) {        return -1;    }    Path output = getOutputPath();    if (hasOption(DefaultOptionCreator.OVERWRITE_OPTION)) {        HadoopUtil.delete(getConf(), output);    }    configureOptionsForWorkers();    run(getConf(), getInputPath(), output);    return 0;}
0
private void configureOptionsForWorkers() throws ClassNotFoundException
{        String method = getOption(DefaultOptionCreator.METHOD_OPTION);    int numClusters = Integer.parseInt(getOption(DefaultOptionCreator.NUM_CLUSTERS_OPTION));        int estimatedNumMapClusters = Integer.parseInt(getOption(ESTIMATED_NUM_MAP_CLUSTERS));    float estimatedDistanceCutoff = Float.parseFloat(getOption(ESTIMATED_DISTANCE_CUTOFF));        int maxNumIterations = Integer.parseInt(getOption(MAX_NUM_ITERATIONS));    float trimFraction = Float.parseFloat(getOption(TRIM_FRACTION));    boolean randomInit = hasOption(RANDOM_INIT);    boolean ignoreWeights = hasOption(IGNORE_WEIGHTS);    float testProbability = Float.parseFloat(getOption(TEST_PROBABILITY));    int numBallKMeansRuns = Integer.parseInt(getOption(NUM_BALLKMEANS_RUNS));        String measureClass = getOption(DefaultOptionCreator.DISTANCE_MEASURE_OPTION);    String searcherClass = getOption(SEARCHER_CLASS_OPTION);                    boolean getSearchSize = false;    boolean getNumProjections = false;    if (!searcherClass.equals(BruteSearch.class.getName())) {        getSearchSize = true;        getNumProjections = true;    }        int searchSize = 0;    if (getSearchSize) {        searchSize = Integer.parseInt(getOption(SEARCH_SIZE_OPTION));    }                int numProjections = 0;    if (getNumProjections) {        numProjections = Integer.parseInt(getOption(NUM_PROJECTIONS_OPTION));    }    boolean reduceStreamingKMeans = hasOption(REDUCE_STREAMING_KMEANS);    configureOptionsForWorkers(getConf(), numClusters, /* StreamingKMeans */    estimatedNumMapClusters, estimatedDistanceCutoff, /* BallKMeans */    maxNumIterations, trimFraction, randomInit, ignoreWeights, testProbability, numBallKMeansRuns, /* Searcher */    measureClass, searcherClass, searchSize, numProjections, method, reduceStreamingKMeans);}
1
private static int runSequentially(Configuration conf, Path input, Path output) throws IOException, ExecutionException, InterruptedException
{    long start = System.currentTimeMillis();        ExecutorService pool = Executors.newCachedThreadPool();    List<Future<Iterable<Centroid>>> intermediateCentroidFutures = new ArrayList<>();    for (FileStatus status : HadoopUtil.listStatus(FileSystem.get(conf), input, PathFilters.logsCRCFilter())) {        intermediateCentroidFutures.add(pool.submit(new StreamingKMeansThread(status.getPath(), conf)));    }            List<Centroid> intermediateCentroids = new ArrayList<>();    for (Future<Iterable<Centroid>> futureIterable : intermediateCentroidFutures) {        for (Centroid centroid : futureIterable.get()) {            intermediateCentroids.add(centroid);        }    }    pool.shutdown();    pool.awaitTermination(Long.MAX_VALUE, TimeUnit.SECONDS);        SequenceFile.Writer writer = SequenceFile.createWriter(FileSystem.get(conf), conf, new Path(output, "part-r-00000"), IntWritable.class, CentroidWritable.class);    int numCentroids = 0;        for (Vector finalVector : StreamingKMeansReducer.getBestCentroids(intermediateCentroids, conf)) {        Centroid finalCentroid = (Centroid) finalVector;        writer.append(new IntWritable(numCentroids++), new CentroidWritable(finalCentroid));    }    writer.close();    long end = System.currentTimeMillis();        return 0;}
1
public static int runMapReduce(Configuration conf, Path input, Path output) throws IOException, ClassNotFoundException, InterruptedException
{        Job job = HadoopUtil.prepareJob(input, output, SequenceFileInputFormat.class, StreamingKMeansMapper.class, IntWritable.class, CentroidWritable.class, StreamingKMeansReducer.class, IntWritable.class, CentroidWritable.class, SequenceFileOutputFormat.class, conf);    job.setJobName(HadoopUtil.getCustomJobName(StreamingKMeansDriver.class.getSimpleName(), job, StreamingKMeansMapper.class, StreamingKMeansReducer.class));            job.setNumReduceTasks(1);        job.setJarByClass(StreamingKMeansDriver.class);        long start = System.currentTimeMillis();    if (!job.waitForCompletion(true)) {        return -1;    }    long end = System.currentTimeMillis();        return 0;}
1
public static void main(String[] args) throws Exception
{    ToolRunner.run(new StreamingKMeansDriver(), args);}
0
public void setup(Context context)
{            Configuration conf = context.getConfiguration();    UpdatableSearcher searcher = StreamingKMeansUtilsMR.searcherFromConfiguration(conf);    int numClusters = conf.getInt(StreamingKMeansDriver.ESTIMATED_NUM_MAP_CLUSTERS, 1);    double estimatedDistanceCutoff = conf.getFloat(StreamingKMeansDriver.ESTIMATED_DISTANCE_CUTOFF, StreamingKMeansDriver.INVALID_DISTANCE_CUTOFF);    if (estimatedDistanceCutoff == StreamingKMeansDriver.INVALID_DISTANCE_CUTOFF) {        estimateDistanceCutoff = true;        estimatePoints = new ArrayList<>();    }        clusterer = new StreamingKMeans(searcher, numClusters, estimatedDistanceCutoff);}
0
private void clusterEstimatePoints()
{    clusterer.setDistanceCutoff(ClusteringUtils.estimateDistanceCutoff(estimatePoints, clusterer.getDistanceMeasure()));    clusterer.cluster(estimatePoints);    estimateDistanceCutoff = false;}
0
public void map(Writable key, VectorWritable point, Context context)
{    Centroid centroid = new Centroid(numPoints++, point.get(), 1);    if (estimateDistanceCutoff) {        if (numPoints < NUM_ESTIMATE_POINTS) {            estimatePoints.add(centroid);        } else if (numPoints == NUM_ESTIMATE_POINTS) {            clusterEstimatePoints();        }    } else {        clusterer.cluster(centroid);    }}
0
public void cleanup(Context context) throws IOException, InterruptedException
{        if (estimateDistanceCutoff) {        clusterEstimatePoints();    }        clusterer.reindexCentroids();        for (Centroid centroid : clusterer) {        context.write(new IntWritable(0), new CentroidWritable(centroid));    }}
0
public void setup(Context context)
{            conf = context.getConfiguration();}
0
public void reduce(IntWritable key, Iterable<CentroidWritable> centroids, Context context) throws IOException, InterruptedException
{    List<Centroid> intermediateCentroids;        if (conf.getBoolean(StreamingKMeansDriver.REDUCE_STREAMING_KMEANS, false)) {        intermediateCentroids = Lists.newArrayList(new StreamingKMeansThread(Iterables.transform(centroids, new Function<CentroidWritable, Centroid>() {            @Override            public Centroid apply(CentroidWritable input) {                Preconditions.checkNotNull(input);                return input.getCentroid().clone();            }        }), conf).call());    } else {        intermediateCentroids = centroidWritablesToList(centroids);    }    int index = 0;    for (Vector centroid : getBestCentroids(intermediateCentroids, conf)) {        context.write(new IntWritable(index), new CentroidWritable((Centroid) centroid));        ++index;    }}
0
public Centroid apply(CentroidWritable input)
{    Preconditions.checkNotNull(input);    return input.getCentroid().clone();}
0
public static List<Centroid> centroidWritablesToList(Iterable<CentroidWritable> centroids)
{        return Lists.newArrayList(Iterables.transform(centroids, new Function<CentroidWritable, Centroid>() {        @Override        public Centroid apply(CentroidWritable input) {            Preconditions.checkNotNull(input);            return input.getCentroid().clone();        }    }));}
0
public Centroid apply(CentroidWritable input)
{    Preconditions.checkNotNull(input);    return input.getCentroid().clone();}
0
public static Iterable<Vector> getBestCentroids(List<Centroid> centroids, Configuration conf)
{    if (log.isInfoEnabled()) {            }    int numClusters = conf.getInt(DefaultOptionCreator.NUM_CLUSTERS_OPTION, 1);    int maxNumIterations = conf.getInt(StreamingKMeansDriver.MAX_NUM_ITERATIONS, 10);    float trimFraction = conf.getFloat(StreamingKMeansDriver.TRIM_FRACTION, 0.9f);    boolean kMeansPlusPlusInit = !conf.getBoolean(StreamingKMeansDriver.RANDOM_INIT, false);    boolean correctWeights = !conf.getBoolean(StreamingKMeansDriver.IGNORE_WEIGHTS, false);    float testProbability = conf.getFloat(StreamingKMeansDriver.TEST_PROBABILITY, 0.1f);    int numRuns = conf.getInt(StreamingKMeansDriver.NUM_BALLKMEANS_RUNS, 3);    BallKMeans ballKMeansCluster = new BallKMeans(StreamingKMeansUtilsMR.searcherFromConfiguration(conf), numClusters, maxNumIterations, trimFraction, kMeansPlusPlusInit, correctWeights, testProbability, numRuns);    return ballKMeansCluster.cluster(centroids);}
1
public Iterable<Centroid> call()
{    UpdatableSearcher searcher = StreamingKMeansUtilsMR.searcherFromConfiguration(conf);    int numClusters = conf.getInt(StreamingKMeansDriver.ESTIMATED_NUM_MAP_CLUSTERS, 1);    double estimateDistanceCutoff = conf.getFloat(StreamingKMeansDriver.ESTIMATED_DISTANCE_CUTOFF, StreamingKMeansDriver.INVALID_DISTANCE_CUTOFF);    Iterator<Centroid> dataPointsIterator = dataPoints.iterator();    if (estimateDistanceCutoff == StreamingKMeansDriver.INVALID_DISTANCE_CUTOFF) {        List<Centroid> estimatePoints = new ArrayList<>(NUM_ESTIMATE_POINTS);        while (dataPointsIterator.hasNext() && estimatePoints.size() < NUM_ESTIMATE_POINTS) {            Centroid centroid = dataPointsIterator.next();            estimatePoints.add(centroid);        }        if (log.isInfoEnabled()) {                    }        estimateDistanceCutoff = ClusteringUtils.estimateDistanceCutoff(estimatePoints, searcher.getDistanceMeasure());    }    StreamingKMeans streamingKMeans = new StreamingKMeans(searcher, numClusters, estimateDistanceCutoff);        if (!dataPointsIterator.hasNext()) {        dataPointsIterator = dataPoints.iterator();    }    while (dataPointsIterator.hasNext()) {        streamingKMeans.cluster(dataPointsIterator.next());    }    streamingKMeans.reindexCentroids();    return streamingKMeans;}
1
public static UpdatableSearcher searcherFromConfiguration(Configuration conf)
{    DistanceMeasure distanceMeasure;    String distanceMeasureClass = conf.get(DefaultOptionCreator.DISTANCE_MEASURE_OPTION);    try {        distanceMeasure = (DistanceMeasure) Class.forName(distanceMeasureClass).getConstructor().newInstance();    } catch (Exception e) {        throw new RuntimeException("Failed to instantiate distanceMeasure", e);    }    int numProjections = conf.getInt(StreamingKMeansDriver.NUM_PROJECTIONS_OPTION, 20);    int searchSize = conf.getInt(StreamingKMeansDriver.SEARCH_SIZE_OPTION, 10);    String searcherClass = conf.get(StreamingKMeansDriver.SEARCHER_CLASS_OPTION);    if (searcherClass.equals(BruteSearch.class.getName())) {        return ClassUtils.instantiateAs(searcherClass, UpdatableSearcher.class, new Class[] { DistanceMeasure.class }, new Object[] { distanceMeasure });    } else if (searcherClass.equals(FastProjectionSearch.class.getName()) || searcherClass.equals(ProjectionSearch.class.getName())) {        return ClassUtils.instantiateAs(searcherClass, UpdatableSearcher.class, new Class[] { DistanceMeasure.class, int.class, int.class }, new Object[] { distanceMeasure, numProjections, searchSize });    } else if (searcherClass.equals(LocalitySensitiveHashSearch.class.getName())) {        return ClassUtils.instantiateAs(searcherClass, LocalitySensitiveHashSearch.class, new Class[] { DistanceMeasure.class, int.class }, new Object[] { distanceMeasure, searchSize });    } else {        throw new IllegalStateException("Unknown class instantiation requested");    }}
0
public static Iterable<Centroid> getCentroidsFromVectorWritable(Iterable<VectorWritable> inputIterable)
{    return Iterables.transform(inputIterable, new Function<VectorWritable, Centroid>() {        private int numVectors = 0;        @Override        public Centroid apply(VectorWritable input) {            Preconditions.checkNotNull(input);            return new Centroid(numVectors++, new RandomAccessSparseVector(input.get()), 1);        }    });}
0
public Centroid apply(VectorWritable input)
{    Preconditions.checkNotNull(input);    return new Centroid(numVectors++, new RandomAccessSparseVector(input.get()), 1);}
0
public static Iterable<Centroid> castVectorsToCentroids(Iterable<Vector> input)
{    return Iterables.transform(input, new Function<Vector, Centroid>() {        private int numVectors = 0;        @Override        public Centroid apply(Vector input) {            Preconditions.checkNotNull(input);            if (input instanceof Centroid) {                return (Centroid) input;            } else {                return new Centroid(numVectors++, input, 1);            }        }    });}
0
public Centroid apply(Vector input)
{    Preconditions.checkNotNull(input);    if (input instanceof Centroid) {        return (Centroid) input;    } else {        return new Centroid(numVectors++, input, 1);    }}
0
public static void writeCentroidsToSequenceFile(Iterable<Centroid> centroids, Path path, Configuration conf) throws IOException
{    try (SequenceFile.Writer writer = SequenceFile.createWriter(FileSystem.get(conf), conf, path, IntWritable.class, CentroidWritable.class)) {        int i = 0;        for (Centroid centroid : centroids) {            writer.append(new IntWritable(i++), new CentroidWritable(centroid));        }    }}
0
public static void writeVectorsToSequenceFile(Iterable<? extends Vector> datapoints, Path path, Configuration conf) throws IOException
{    try (SequenceFile.Writer writer = SequenceFile.createWriter(FileSystem.get(conf), conf, path, IntWritable.class, VectorWritable.class)) {        int i = 0;        for (Vector vector : datapoints) {            writer.append(new IntWritable(i++), new VectorWritable(vector));        }    }}
0
private void writeSplit(Iterator<Pair<Writable, Writable>> inputIterator, int numSplit, int numEntriesPerSplit) throws IOException
{    SequenceFile.Writer splitWriter = null;    for (int j = 0; j < numEntriesPerSplit; ++j) {        Pair<Writable, Writable> item = inputIterator.next();        if (splitWriter == null) {            splitWriter = SequenceFile.createWriter(fs, conf, new Path(outputFileBase + "-" + numSplit), item.getFirst().getClass(), item.getSecond().getClass());        }        splitWriter.append(item.getFirst(), item.getSecond());    }    if (splitWriter != null) {        splitWriter.close();    }}
0
private void run(PrintWriter printWriter) throws IOException
{    conf = new Configuration();    SequenceFileDirIterable<Writable, Writable> inputIterable = new SequenceFileDirIterable<>(new Path(inputFile), PathType.LIST, conf);    fs = FileSystem.get(conf);    int numEntries = Iterables.size(inputIterable);    int numEntriesPerSplit = numEntries / numSplits;    int numEntriesLastSplit = numEntriesPerSplit + numEntries - numEntriesPerSplit * numSplits;    Iterator<Pair<Writable, Writable>> inputIterator = inputIterable.iterator();    printWriter.printf("Writing %d splits\n", numSplits);    for (int i = 0; i < numSplits - 1; ++i) {        printWriter.printf("Writing split %d\n", i);        writeSplit(inputIterator, i, numEntriesPerSplit);    }    printWriter.printf("Writing split %d\n", numSplits - 1);    writeSplit(inputIterator, numSplits - 1, numEntriesLastSplit);}
0
private boolean parseArgs(String[] args)
{    DefaultOptionBuilder builder = new DefaultOptionBuilder();    Option help = builder.withLongName("help").withDescription("print this list").create();    ArgumentBuilder argumentBuilder = new ArgumentBuilder();    Option inputFileOption = builder.withLongName("input").withShortName("i").withRequired(true).withArgument(argumentBuilder.withName("input").withMaximum(1).create()).withDescription("what the base folder for sequence files is (they all must have the same key/value type").create();    Option outputFileOption = builder.withLongName("output").withShortName("o").withRequired(true).withArgument(argumentBuilder.withName("output").withMaximum(1).create()).withDescription("the base name of the file split that the files will be split it; the i'th split has the " + "suffix -i").create();    Option numSplitsOption = builder.withLongName("numSplits").withShortName("ns").withRequired(true).withArgument(argumentBuilder.withName("numSplits").withMaximum(1).create()).withDescription("how many splits to use for the given files").create();    Group normalArgs = new GroupBuilder().withOption(help).withOption(inputFileOption).withOption(outputFileOption).withOption(numSplitsOption).create();    Parser parser = new Parser();    parser.setHelpOption(help);    parser.setHelpTrigger("--help");    parser.setGroup(normalArgs);    parser.setHelpFormatter(new HelpFormatter(" ", "", " ", 130));    CommandLine cmdLine = parser.parseAndHelp(args);    if (cmdLine == null) {        return false;    }    inputFile = (String) cmdLine.getValue(inputFileOption);    outputFileBase = (String) cmdLine.getValue(outputFileOption);    numSplits = Integer.parseInt((String) cmdLine.getValue(numSplitsOption));    return true;}
0
public static void main(String[] args) throws IOException
{    ResplitSequenceFiles runner = new ResplitSequenceFiles();    if (runner.parseArgs(args)) {        runner.run(new PrintWriter(new OutputStreamWriter(System.out, Charsets.UTF_8), true));    }}
0
public static Path getTopLevelClusterPath(Path output)
{    return new Path(output + File.separator + TOP_LEVEL_CLUSTER_DIRECTORY);}
0
public static Path getClusterPostProcessorOutputDirectory(Path outputPathProvidedByUser)
{    return new Path(outputPathProvidedByUser + File.separator + POST_PROCESS_DIRECTORY);}
0
public static Path getClusterOutputClusteredPoints(Path output)
{    return new Path(output + File.separator + CLUSTERED_POINTS_DIRECTORY + File.separator, "*");}
0
public static Path getBottomLevelClusterPath(Path output, String clusterId)
{    return new Path(output + File.separator + BOTTOM_LEVEL_CLUSTER_DIRECTORY + File.separator + clusterId);}
0
public static Path getClusterPathForClusterId(Path clusterPostProcessorOutput, String clusterId)
{    return new Path(clusterPostProcessorOutput + File.separator + clusterId);}
0
public static int getNumberOfClusters(Path clusterOutputPath, Configuration conf) throws IOException
{    FileSystem fileSystem = clusterOutputPath.getFileSystem(conf);    FileStatus[] clusterFiles = fileSystem.listStatus(clusterOutputPath, PathFilters.finalPartFilter());    int numberOfClusters = 0;    Iterator<?> it = new SequenceFileDirValueIterator<>(clusterFiles[0].getPath(), PathType.LIST, PathFilters.partFilter(), null, true, conf);    while (it.hasNext()) {        it.next();        numberOfClusters++;    }    return numberOfClusters;}
0
public static Map<Integer, Integer> getClusterIDs(Path clusterOutputPath, Configuration conf, boolean keyIsClusterId) throws IOException
{    Map<Integer, Integer> clusterIds = new HashMap<>();    FileSystem fileSystem = clusterOutputPath.getFileSystem(conf);    FileStatus[] clusterFiles = fileSystem.listStatus(clusterOutputPath, PathFilters.finalPartFilter());        Iterator<ClusterWritable> it = new SequenceFileDirValueIterator<>(clusterFiles[0].getPath(), PathType.LIST, PathFilters.partFilter(), null, true, conf);    int i = 0;    while (it.hasNext()) {        Integer key;        Integer value;        if (keyIsClusterId) {                        key = it.next().getValue().getId();            value = i;        } else {            key = i;            value = it.next().getValue().getId();        }        clusterIds.put(key, value);        i++;    }    return clusterIds;}
0
public void process() throws IOException
{    createPostProcessDirectory();    for (Pair<?, WeightedVectorWritable> record : new SequenceFileDirIterable<Writable, WeightedVectorWritable>(clusteredPoints, PathType.GLOB, PathFilters.partFilter(), null, false, conf)) {        String clusterId = record.getFirst().toString().trim();        putVectorInRespectiveCluster(clusterId, record.getSecond());    }    IOUtils.close(writersForClusters.values());    writersForClusters.clear();}
0
private void createPostProcessDirectory() throws IOException
{    if (!fileSystem.exists(clusterPostProcessorOutput) && !fileSystem.mkdirs(clusterPostProcessorOutput)) {        throw new IOException("Error creating cluster post processor directory");    }}
0
private void putVectorInRespectiveCluster(String clusterId, WeightedVectorWritable point) throws IOException
{    Writer writer = findWriterForVector(clusterId);    postProcessedClusterDirectories.put(clusterId, PathDirectory.getClusterPathForClusterId(clusterPostProcessorOutput, clusterId));    writeVectorToCluster(writer, point);}
0
private Writer findWriterForVector(String clusterId) throws IOException
{    Path clusterDirectory = PathDirectory.getClusterPathForClusterId(clusterPostProcessorOutput, clusterId);    Writer writer = writersForClusters.get(clusterId);    if (writer == null) {        Path pathToWrite = new Path(clusterDirectory, new Path("part-m-0"));        writer = new Writer(fileSystem, conf, pathToWrite, LongWritable.class, VectorWritable.class);        writersForClusters.put(clusterId, writer);    }    return writer;}
0
private void writeVectorToCluster(Writer writer, WeightedVectorWritable point) throws IOException
{    writer.append(new LongWritable(uniqueVectorId++), new VectorWritable(point.getVector()));    writer.sync();}
0
public Map<String, Path> getPostProcessedClusterDirectories()
{    return postProcessedClusterDirectories;}
0
public void setClusteredPoints(Path clusteredPoints)
{    this.clusteredPoints = clusteredPoints;}
0
public int run(String[] args) throws Exception
{    addInputOption();    addOutputOption();    addOption(DefaultOptionCreator.methodOption().create());    addOption(DefaultOptionCreator.overwriteOption().create());    if (parseArguments(args) == null) {        return -1;    }    Path input = getInputPath();    Path output = getOutputPath();    if (hasOption(DefaultOptionCreator.OVERWRITE_OPTION)) {        HadoopUtil.delete(getConf(), output);    }    boolean runSequential = getOption(DefaultOptionCreator.METHOD_OPTION).equalsIgnoreCase(DefaultOptionCreator.SEQUENTIAL_METHOD);    run(input, output, runSequential);    return 0;}
0
public static void main(String[] args) throws Exception
{    ToolRunner.run(new Configuration(), new ClusterOutputPostProcessorDriver(), args);}
0
public static void run(Path input, Path output, boolean runSequential) throws IOException, InterruptedException, ClassNotFoundException
{    if (runSequential) {        postProcessSeq(input, output);    } else {        Configuration conf = new Configuration();        postProcessMR(conf, input, output);        movePartFilesToRespectiveDirectories(conf, output);    }}
0
private static void postProcessSeq(Path input, Path output) throws IOException
{    ClusterOutputPostProcessor clusterOutputPostProcessor = new ClusterOutputPostProcessor(input, output, new Configuration());    clusterOutputPostProcessor.process();}
0
private static void postProcessMR(Configuration conf, Path input, Path output) throws IOException, InterruptedException, ClassNotFoundException
{    System.out.println("WARNING: If you are running in Hadoop local mode, please use the --sequential option, " + "as the MapReduce option will not work properly");    int numberOfClusters = ClusterCountReader.getNumberOfClusters(input, conf);    conf.set("clusterOutputPath", input.toString());    Job job = new Job(conf, "ClusterOutputPostProcessor Driver running over input: " + input);    job.setInputFormatClass(SequenceFileInputFormat.class);    job.setOutputFormatClass(SequenceFileOutputFormat.class);    job.setMapperClass(ClusterOutputPostProcessorMapper.class);    job.setMapOutputKeyClass(IntWritable.class);    job.setMapOutputValueClass(VectorWritable.class);    job.setReducerClass(ClusterOutputPostProcessorReducer.class);    job.setOutputKeyClass(IntWritable.class);    job.setOutputValueClass(VectorWritable.class);    job.setNumReduceTasks(numberOfClusters);    job.setJarByClass(ClusterOutputPostProcessorDriver.class);    FileInputFormat.addInputPath(job, new Path(input, new Path("clusteredPoints")));    FileOutputFormat.setOutputPath(job, output);    if (!job.waitForCompletion(true)) {        throw new InterruptedException("ClusterOutputPostProcessor Job failed processing " + input);    }}
0
private static void movePartFilesToRespectiveDirectories(Configuration conf, Path output) throws IOException
{    FileSystem fileSystem = output.getFileSystem(conf);    for (FileStatus fileStatus : fileSystem.listStatus(output, PathFilters.partFilter())) {        SequenceFileIterator<Writable, Writable> it = new SequenceFileIterator<>(fileStatus.getPath(), true, conf);        if (it.hasNext()) {            renameFile(it.next().getFirst(), fileStatus, conf);        }        it.close();    }}
0
private static void renameFile(Writable key, FileStatus fileStatus, Configuration conf) throws IOException
{    Path path = fileStatus.getPath();    FileSystem fileSystem = path.getFileSystem(conf);    Path subDir = new Path(key.toString());    Path renameTo = new Path(path.getParent(), subDir);    fileSystem.mkdirs(renameTo);    fileSystem.rename(path, renameTo);}
0
public void setup(Context context) throws IOException
{    Configuration conf = context.getConfiguration();        Path clusterOutputPath = new Path(conf.get("clusterOutputPath"));        newClusterMappings = ClusterCountReader.getClusterIDs(clusterOutputPath, conf, true);    outputVector = new VectorWritable();}
0
public void map(IntWritable key, WeightedVectorWritable val, Context context) throws IOException, InterruptedException
{            outputVector.set(val.getVector());    context.write(new IntWritable(newClusterMappings.get(key.get())), outputVector);}
0
public void setup(Context context) throws IOException
{    Configuration conf = context.getConfiguration();    Path clusterOutputPath = new Path(conf.get("clusterOutputPath"));        reverseClusterMappings = ClusterCountReader.getClusterIDs(clusterOutputPath, conf, false);}
0
protected void reduce(IntWritable key, Iterable<VectorWritable> values, Context context) throws IOException, InterruptedException
{                IntWritable outKey = new IntWritable(reverseClusterMappings.get(key.get()));    System.out.println(outKey + " this: " + this);    for (VectorWritable value : values) {        context.write(outKey, value);    }}
0
public static double rGamma(double k, double lambda)
{    boolean accept = false;    if (k >= 1.0) {                double b = k - Math.log(4.0);        double c = k + Math.sqrt(2.0 * k - 1.0);        double lam = Math.sqrt(2.0 * k - 1.0);        double cheng = 1.0 + Math.log(4.5);        double x;        do {            double u = RANDOM.nextDouble();            double v = RANDOM.nextDouble();            double y = 1.0 / lam * Math.log(v / (1.0 - v));            x = k * Math.exp(y);            double z = u * v * v;            double r = b + c * y - x;            if (r >= 4.5 * z - cheng || r >= Math.log(z)) {                accept = true;            }        } while (!accept);        return x / lambda;    } else {                double c = 1.0 / k;        double d = (1.0 - k) * Math.pow(k, k / (1.0 - k));        double x;        do {            double u = RANDOM.nextDouble();            double v = RANDOM.nextDouble();            double z = -Math.log(u);            double e = -Math.log(v);            x = Math.pow(z, c);            if (z + e >= d + x) {                accept = true;            }        } while (!accept);        return x / lambda;    }}
0
public static double rBeta(double shape1, double shape2)
{    double gam1 = rGamma(shape1, 1.0);    double gam2 = rGamma(shape2, 1.0);    return gam1 / (gam1 + gam2);}
0
public static double rNorm(double mean, double sd)
{    RealDistribution dist = new NormalDistribution(RANDOM.getRandomGenerator(), mean, sd, NormalDistribution.DEFAULT_INVERSE_ABSOLUTE_ACCURACY);    return dist.sample();}
0
public static int rBinomial(int n, double p)
{    if (p >= 1.0) {                return n;    }    double q = -Math.log1p(-p);    double sum = 0.0;    int x = 0;    while (sum <= q) {        double u = RANDOM.nextDouble();        double e = -Math.log(u);        sum += e / (n - x);        x++;    }    if (x == 0) {        return 0;    }    return x - 1;}
0
protected Path getInputPath()
{    return inputPath;}
0
protected Path getOutputPath()
{    return outputPath;}
0
protected Path getOutputPath(String path)
{    return new Path(outputPath, path);}
0
protected File getInputFile()
{    return inputFile;}
0
protected File getOutputFile()
{    return outputFile;}
0
protected Path getTempPath()
{    return tempPath;}
0
protected Path getTempPath(String directory)
{    return new Path(tempPath, directory);}
0
public Configuration getConf()
{    Configuration result = super.getConf();    if (result == null) {        return new Configuration();    }    return result;}
0
protected void addFlag(String name, String shortName, String description)
{    options.add(buildOption(name, shortName, description, false, false, null));}
0
protected void addOption(String name, String shortName, String description)
{    options.add(buildOption(name, shortName, description, true, false, null));}
0
protected void addOption(String name, String shortName, String description, boolean required)
{    options.add(buildOption(name, shortName, description, true, required, null));}
0
protected void addOption(String name, String shortName, String description, String defaultValue)
{    options.add(buildOption(name, shortName, description, true, false, defaultValue));}
0
protected Option addOption(Option option)
{    options.add(option);    return option;}
0
protected Group getGroup()
{    return group;}
0
protected void addInputOption()
{    this.inputOption = addOption(DefaultOptionCreator.inputOption().create());}
0
protected void addOutputOption()
{    this.outputOption = addOption(DefaultOptionCreator.outputOption().create());}
0
protected static Option buildOption(String name, String shortName, String description, boolean hasArg, boolean required, String defaultValue)
{    return buildOption(name, shortName, description, hasArg, 1, 1, required, defaultValue);}
0
protected static Option buildOption(String name, String shortName, String description, boolean hasArg, int min, int max, boolean required, String defaultValue)
{    DefaultOptionBuilder optBuilder = new DefaultOptionBuilder().withLongName(name).withDescription(description).withRequired(required);    if (shortName != null) {        optBuilder.withShortName(shortName);    }    if (hasArg) {        ArgumentBuilder argBuilder = new ArgumentBuilder().withName(name).withMinimum(min).withMaximum(max);        if (defaultValue != null) {            argBuilder = argBuilder.withDefault(defaultValue);        }        optBuilder.withArgument(argBuilder.create());    }    return optBuilder.create();}
0
protected Option getCLIOption(String name)
{    for (Option option : options) {        if (option.getPreferredName().equals(name)) {            return option;        }    }    return null;}
0
public Map<String, List<String>> parseArguments(String[] args) throws IOException
{    return parseArguments(args, false, false);}
0
public Map<String, List<String>> parseArguments(String[] args, boolean inputOptional, boolean outputOptional) throws IOException
{    Option helpOpt = addOption(DefaultOptionCreator.helpOption());    addOption("tempDir", null, "Intermediate output directory", "temp");    addOption("startPhase", null, "First phase to run", "0");    addOption("endPhase", null, "Last phase to run", String.valueOf(Integer.MAX_VALUE));    GroupBuilder gBuilder = new GroupBuilder().withName("Job-Specific Options:");    for (Option opt : options) {        gBuilder = gBuilder.withOption(opt);    }    group = gBuilder.create();    CommandLine cmdLine;    try {        Parser parser = new Parser();        parser.setGroup(group);        parser.setHelpOption(helpOpt);        cmdLine = parser.parse(args);    } catch (OptionException e) {                CommandLineUtil.printHelpWithGenericOptions(group, e);        return null;    }    if (cmdLine.hasOption(helpOpt)) {        CommandLineUtil.printHelpWithGenericOptions(group);        return null;    }    try {        parseDirectories(cmdLine, inputOptional, outputOptional);    } catch (IllegalArgumentException e) {                CommandLineUtil.printHelpWithGenericOptions(group);        return null;    }    argMap = new TreeMap<>();    maybePut(argMap, cmdLine, this.options.toArray(new Option[this.options.size()]));    this.tempPath = new Path(getOption("tempDir"));    if (!hasOption("quiet")) {            }    return argMap;}
1
public static String keyFor(String optionName)
{    return "--" + optionName;}
0
public String getOption(String optionName)
{    List<String> list = argMap.get(keyFor(optionName));    if (list != null && !list.isEmpty()) {        return list.get(0);    }    return null;}
0
public String getOption(String optionName, String defaultVal)
{    String res = getOption(optionName);    if (res == null) {        res = defaultVal;    }    return res;}
0
public int getInt(String optionName)
{    return Integer.parseInt(getOption(optionName));}
0
public int getInt(String optionName, int defaultVal)
{    return Integer.parseInt(getOption(optionName, String.valueOf(defaultVal)));}
0
public float getFloat(String optionName)
{    return Float.parseFloat(getOption(optionName));}
0
public float getFloat(String optionName, float defaultVal)
{    return Float.parseFloat(getOption(optionName, String.valueOf(defaultVal)));}
0
public List<String> getOptions(String optionName)
{    return argMap.get(keyFor(optionName));}
0
public boolean hasOption(String optionName)
{    return argMap.containsKey(keyFor(optionName));}
0
public int getDimensions(Path matrix) throws IOException
{    try (SequenceFile.Reader reader = new SequenceFile.Reader(FileSystem.get(getConf()), matrix, getConf())) {        Writable row = ClassUtils.instantiateAs(reader.getKeyClass().asSubclass(Writable.class), Writable.class);        Preconditions.checkArgument(reader.getValueClass().equals(VectorWritable.class), "value type of sequencefile must be a VectorWritable");        VectorWritable vectorWritable = new VectorWritable();        boolean hasAtLeastOneRow = reader.next(row, vectorWritable);        Preconditions.checkState(hasAtLeastOneRow, "matrix must have at least one row");        return vectorWritable.get().size();    }}
0
protected void parseDirectories(CommandLine cmdLine, boolean inputOptional, boolean outputOptional)
{    Configuration conf = getConf();    if (inputOption != null && cmdLine.hasOption(inputOption)) {        this.inputPath = new Path(cmdLine.getValue(inputOption).toString());        this.inputFile = new File(cmdLine.getValue(inputOption).toString());    }    if (inputPath == null && conf.get("mapred.input.dir") != null) {        this.inputPath = new Path(conf.get("mapred.input.dir"));    }    if (outputOption != null && cmdLine.hasOption(outputOption)) {        this.outputPath = new Path(cmdLine.getValue(outputOption).toString());        this.outputFile = new File(cmdLine.getValue(outputOption).toString());    }    if (outputPath == null && conf.get("mapred.output.dir") != null) {        this.outputPath = new Path(conf.get("mapred.output.dir"));    }    Preconditions.checkArgument(inputOptional || inputOption == null || inputPath != null, "No input specified or -Dmapred.input.dir must be provided to specify input directory");    Preconditions.checkArgument(outputOptional || outputOption == null || outputPath != null, "No output specified:  or -Dmapred.output.dir must be provided to specify output directory");}
0
protected static void maybePut(Map<String, List<String>> args, CommandLine cmdLine, Option... opt)
{    for (Option o : opt) {                if (cmdLine.hasOption(o) || cmdLine.getValue(o) != null || (cmdLine.getValues(o) != null && !cmdLine.getValues(o).isEmpty())) {                        List<?> vo = cmdLine.getValues(o);            if (vo != null && !vo.isEmpty()) {                List<String> vals = new ArrayList<>();                for (Object o1 : vo) {                    vals.add(o1.toString());                }                args.put(o.getPreferredName(), vals);            } else {                args.put(o.getPreferredName(), null);            }        }    }}
0
public static String getOption(Map<String, List<String>> args, String optName)
{    List<String> res = args.get(optName);    if (res != null && !res.isEmpty()) {        return res.get(0);    }    return null;}
0
protected static boolean shouldRunNextPhase(Map<String, List<String>> args, AtomicInteger currentPhase)
{    int phase = currentPhase.getAndIncrement();    String startPhase = getOption(args, "--startPhase");    String endPhase = getOption(args, "--endPhase");    boolean phaseSkipped = (startPhase != null && phase < Integer.parseInt(startPhase)) || (endPhase != null && phase > Integer.parseInt(endPhase));    if (phaseSkipped) {            }    return !phaseSkipped;}
1
protected Job prepareJob(Path inputPath, Path outputPath, Class<? extends InputFormat> inputFormat, Class<? extends Mapper> mapper, Class<? extends Writable> mapperKey, Class<? extends Writable> mapperValue, Class<? extends OutputFormat> outputFormat) throws IOException
{    return prepareJob(inputPath, outputPath, inputFormat, mapper, mapperKey, mapperValue, outputFormat, null);}
0
protected Job prepareJob(Path inputPath, Path outputPath, Class<? extends InputFormat> inputFormat, Class<? extends Mapper> mapper, Class<? extends Writable> mapperKey, Class<? extends Writable> mapperValue, Class<? extends OutputFormat> outputFormat, String jobname) throws IOException
{    Job job = HadoopUtil.prepareJob(inputPath, outputPath, inputFormat, mapper, mapperKey, mapperValue, outputFormat, getConf());    String name = jobname != null ? jobname : HadoopUtil.getCustomJobName(getClass().getSimpleName(), job, mapper, Reducer.class);    job.setJobName(name);    return job;}
0
protected Job prepareJob(Path inputPath, Path outputPath, Class<? extends Mapper> mapper, Class<? extends Writable> mapperKey, Class<? extends Writable> mapperValue, Class<? extends Reducer> reducer, Class<? extends Writable> reducerKey, Class<? extends Writable> reducerValue) throws IOException
{    return prepareJob(inputPath, outputPath, SequenceFileInputFormat.class, mapper, mapperKey, mapperValue, reducer, reducerKey, reducerValue, SequenceFileOutputFormat.class);}
0
protected Job prepareJob(Path inputPath, Path outputPath, Class<? extends InputFormat> inputFormat, Class<? extends Mapper> mapper, Class<? extends Writable> mapperKey, Class<? extends Writable> mapperValue, Class<? extends Reducer> reducer, Class<? extends Writable> reducerKey, Class<? extends Writable> reducerValue, Class<? extends OutputFormat> outputFormat) throws IOException
{    Job job = HadoopUtil.prepareJob(inputPath, outputPath, inputFormat, mapper, mapperKey, mapperValue, reducer, reducerKey, reducerValue, outputFormat, getConf());    job.setJobName(HadoopUtil.getCustomJobName(getClass().getSimpleName(), job, mapper, Reducer.class));    return job;}
0
public static void setS3SafeCombinedInputPath(Job job, Path referencePath, Path inputPathOne, Path inputPathTwo) throws IOException
{    FileSystem fs = FileSystem.get(referencePath.toUri(), job.getConfiguration());    FileInputFormat.setInputPaths(job, inputPathOne.makeQualified(fs), inputPathTwo.makeQualified(fs));}
0
protected Class<? extends Analyzer> getAnalyzerClassFromOption() throws ClassNotFoundException
{    Class<? extends Analyzer> analyzerClass = StandardAnalyzer.class;    if (hasOption(DefaultOptionCreator.ANALYZER_NAME_OPTION)) {        String className = getOption(DefaultOptionCreator.ANALYZER_NAME_OPTION);        analyzerClass = Class.forName(className).asSubclass(Analyzer.class);                                AnalyzerUtils.createAnalyzer(analyzerClass);    }    return analyzerClass;}
0
public void setConf(Configuration conf)
{    super.setConf(conf);            String oozieActionConfXml = System.getProperty("oozie.action.conf.xml");    if (oozieActionConfXml != null && conf != null) {        conf.addResource(new Path("file:///", oozieActionConfXml));            }}
1
public static T instantiateAs(String classname, Class<T> asSubclassOfClass)
{    try {        return instantiateAs(Class.forName(classname).asSubclass(asSubclassOfClass), asSubclassOfClass);    } catch (ClassNotFoundException e) {        throw new IllegalStateException(e);    }}
0
public static T instantiateAs(String classname, Class<T> asSubclassOfClass, Class<?>[] params, Object[] args)
{    try {        return instantiateAs(Class.forName(classname).asSubclass(asSubclassOfClass), asSubclassOfClass, params, args);    } catch (ClassNotFoundException e) {        throw new IllegalStateException(e);    }}
0
public static T instantiateAs(Class<? extends T> clazz, Class<T> asSubclassOfClass, Class<?>[] params, Object[] args)
{    try {        return clazz.asSubclass(asSubclassOfClass).getConstructor(params).newInstance(args);    } catch (InstantiationException | IllegalAccessException | NoSuchMethodException | InvocationTargetException ie) {        throw new IllegalStateException(ie);    }}
0
public static T instantiateAs(Class<? extends T> clazz, Class<T> asSubclassOfClass)
{    try {        return clazz.asSubclass(asSubclassOfClass).getConstructor().newInstance();    } catch (InstantiationException | IllegalAccessException | NoSuchMethodException | InvocationTargetException ie) {        throw new IllegalStateException(ie);    }}
0
public static Option helpOption()
{    return new DefaultOptionBuilder().withLongName("help").withDescription("Print out help").withShortName("h").create();}
0
public static DefaultOptionBuilder inputOption()
{    return new DefaultOptionBuilder().withLongName(INPUT_OPTION).withRequired(false).withShortName("i").withArgument(new ArgumentBuilder().withName(INPUT_OPTION).withMinimum(1).withMaximum(1).create()).withDescription("Path to job input directory.");}
0
public static DefaultOptionBuilder clustersInOption()
{    return new DefaultOptionBuilder().withLongName(CLUSTERS_IN_OPTION).withRequired(true).withArgument(new ArgumentBuilder().withName(CLUSTERS_IN_OPTION).withMinimum(1).withMaximum(1).create()).withDescription("The path to the initial clusters directory. Must be a SequenceFile of some type of Cluster").withShortName("c");}
0
public static DefaultOptionBuilder outputOption()
{    return new DefaultOptionBuilder().withLongName(OUTPUT_OPTION).withRequired(false).withShortName("o").withArgument(new ArgumentBuilder().withName(OUTPUT_OPTION).withMinimum(1).withMaximum(1).create()).withDescription("The directory pathname for output.");}
0
public static DefaultOptionBuilder overwriteOption()
{    return new DefaultOptionBuilder().withLongName(OVERWRITE_OPTION).withRequired(false).withDescription("If present, overwrite the output directory before running job").withShortName("ow");}
0
public static DefaultOptionBuilder distanceMeasureOption()
{    return new DefaultOptionBuilder().withLongName(DISTANCE_MEASURE_OPTION).withRequired(false).withShortName("dm").withArgument(new ArgumentBuilder().withName(DISTANCE_MEASURE_OPTION).withDefault(SquaredEuclideanDistanceMeasure.class.getName()).withMinimum(1).withMaximum(1).create()).withDescription("The classname of the DistanceMeasure. Default is SquaredEuclidean");}
0
public static DefaultOptionBuilder methodOption()
{    return new DefaultOptionBuilder().withLongName(METHOD_OPTION).withRequired(false).withShortName("xm").withArgument(new ArgumentBuilder().withName(METHOD_OPTION).withDefault(MAPREDUCE_METHOD).withMinimum(1).withMaximum(1).create()).withDescription("The execution method to use: sequential or mapreduce. Default is mapreduce");}
0
public static DefaultOptionBuilder t1Option()
{    return new DefaultOptionBuilder().withLongName(T1_OPTION).withRequired(true).withArgument(new ArgumentBuilder().withName(T1_OPTION).withMinimum(1).withMaximum(1).create()).withDescription("T1 threshold value").withShortName(T1_OPTION);}
0
public static DefaultOptionBuilder t2Option()
{    return new DefaultOptionBuilder().withLongName(T2_OPTION).withRequired(true).withArgument(new ArgumentBuilder().withName(T2_OPTION).withMinimum(1).withMaximum(1).create()).withDescription("T2 threshold value").withShortName(T2_OPTION);}
0
public static DefaultOptionBuilder t3Option()
{    return new DefaultOptionBuilder().withLongName(T3_OPTION).withRequired(false).withArgument(new ArgumentBuilder().withName(T3_OPTION).withMinimum(1).withMaximum(1).create()).withDescription("T3 (Reducer T1) threshold value").withShortName(T3_OPTION);}
0
public static DefaultOptionBuilder t4Option()
{    return new DefaultOptionBuilder().withLongName(T4_OPTION).withRequired(false).withArgument(new ArgumentBuilder().withName(T4_OPTION).withMinimum(1).withMaximum(1).create()).withDescription("T4 (Reducer T2) threshold value").withShortName(T4_OPTION);}
0
public static DefaultOptionBuilder clusterFilterOption()
{    return new DefaultOptionBuilder().withLongName(CLUSTER_FILTER_OPTION).withShortName("cf").withRequired(false).withArgument(new ArgumentBuilder().withName(CLUSTER_FILTER_OPTION).withMinimum(1).withMaximum(1).create()).withDescription("Cluster filter suppresses small canopies from mapper").withShortName(CLUSTER_FILTER_OPTION);}
0
public static DefaultOptionBuilder maxIterationsOption()
{        return new DefaultOptionBuilder().withLongName(MAX_ITERATIONS_OPTION).withRequired(true).withShortName("x").withArgument(new ArgumentBuilder().withName(MAX_ITERATIONS_OPTION).withDefault("-1").withMinimum(1).withMaximum(1).create()).withDescription("The maximum number of iterations.");}
0
public static DefaultOptionBuilder numClustersOption()
{    return new DefaultOptionBuilder().withLongName(NUM_CLUSTERS_OPTION).withRequired(false).withArgument(new ArgumentBuilder().withName("k").withMinimum(1).withMaximum(1).create()).withDescription("The number of clusters to create").withShortName("k");}
0
public static DefaultOptionBuilder useSetRandomSeedOption()
{    return new DefaultOptionBuilder().withLongName(RANDOM_SEED).withRequired(false).withArgument(new ArgumentBuilder().withName(RANDOM_SEED).create()).withDescription("Seed to initaize Random Number Generator with").withShortName("rs");}
0
public static DefaultOptionBuilder convergenceOption()
{    return new DefaultOptionBuilder().withLongName(CONVERGENCE_DELTA_OPTION).withRequired(false).withShortName("cd").withArgument(new ArgumentBuilder().withName(CONVERGENCE_DELTA_OPTION).withDefault("0.5").withMinimum(1).withMaximum(1).create()).withDescription("The convergence delta value. Default is 0.5");}
0
public static DefaultOptionBuilder numReducersOption()
{    return new DefaultOptionBuilder().withLongName(MAX_REDUCERS_OPTION).withRequired(false).withShortName("r").withArgument(new ArgumentBuilder().withName(MAX_REDUCERS_OPTION).withDefault("2").withMinimum(1).withMaximum(1).create()).withDescription("The number of reduce tasks. Defaults to 2");}
0
public static DefaultOptionBuilder clusteringOption()
{    return new DefaultOptionBuilder().withLongName(CLUSTERING_OPTION).withRequired(false).withDescription("If present, run clustering after the iterations have taken place").withShortName("cl");}
0
public static DefaultOptionBuilder analyzerOption()
{    return new DefaultOptionBuilder().withLongName(ANALYZER_NAME_OPTION).withRequired(false).withDescription("If present, the name of a Lucene analyzer class to use").withArgument(new ArgumentBuilder().withName(ANALYZER_NAME_OPTION).withDefault(StandardAnalyzer.class.getName()).withMinimum(1).withMaximum(1).create()).withShortName("an");}
0
public static DefaultOptionBuilder emitMostLikelyOption()
{    return new DefaultOptionBuilder().withLongName(EMIT_MOST_LIKELY_OPTION).withRequired(false).withShortName("e").withArgument(new ArgumentBuilder().withName(EMIT_MOST_LIKELY_OPTION).withDefault("true").withMinimum(1).withMaximum(1).create()).withDescription("True if clustering should emit the most likely point only, " + "false for threshold clustering. Default is true");}
0
public static DefaultOptionBuilder thresholdOption()
{    return new DefaultOptionBuilder().withLongName(THRESHOLD_OPTION).withRequired(false).withShortName("t").withArgument(new ArgumentBuilder().withName(THRESHOLD_OPTION).withDefault("0").withMinimum(1).withMaximum(1).create()).withDescription("The pdf threshold used for cluster determination. Default is 0");}
0
public static DefaultOptionBuilder kernelProfileOption()
{    return new DefaultOptionBuilder().withLongName(KERNEL_PROFILE_OPTION).withRequired(false).withShortName("kp").withArgument(new ArgumentBuilder().withName(KERNEL_PROFILE_OPTION).withDefault(TriangularKernelProfile.class.getName()).withMinimum(1).withMaximum(1).create()).withDescription("The classname of the IKernelProfile. Default is TriangularKernelProfile");}
0
public static DefaultOptionBuilder outlierThresholdOption()
{    return new DefaultOptionBuilder().withLongName(OUTLIER_THRESHOLD).withRequired(false).withArgument(new ArgumentBuilder().withName(OUTLIER_THRESHOLD).withMinimum(1).withMaximum(1).create()).withDescription("Outlier threshold value").withShortName(OUTLIER_THRESHOLD);}
0
public static void printHelp(Group group)
{    HelpFormatter formatter = new HelpFormatter();    formatter.setGroup(group);    formatter.print();}
0
public static void printHelpWithGenericOptions(Group group) throws IOException
{    new GenericOptionsParser(new Configuration(), new org.apache.commons.cli.Options(), new String[0]);    PrintWriter pw = new PrintWriter(new OutputStreamWriter(System.out, Charsets.UTF_8), true);    HelpFormatter formatter = new HelpFormatter();    formatter.setGroup(group);    formatter.setPrintWriter(pw);    formatter.setFooter("Specify HDFS directories while running on hadoop; else specify local file system directories");    formatter.print();}
0
public static void printHelpWithGenericOptions(Group group, OptionException oe) throws IOException
{    new GenericOptionsParser(new Configuration(), new org.apache.commons.cli.Options(), new String[0]);    PrintWriter pw = new PrintWriter(new OutputStreamWriter(System.out, Charsets.UTF_8), true);    HelpFormatter formatter = new HelpFormatter();    formatter.setGroup(group);    formatter.setPrintWriter(pw);    formatter.setException(oe);    formatter.print();}
0
public void configure(Configuration job)
{}
0
public Collection<Parameter<?>> getParameters()
{    return Collections.emptyList();}
0
public void createParameters(String prefix, Configuration jobConf)
{}
0
public double distance(Vector v1, Vector v2)
{    if (v1.size() != v2.size()) {        throw new CardinalityException(v1.size(), v2.size());    }    return v1.aggregate(v2, Functions.MAX_ABS, Functions.MINUS);}
0
public double distance(double centroidLengthSquare, Vector centroid, Vector v)
{        return distance(centroid, v);}
0
public void configure(Configuration job)
{}
0
public Collection<Parameter<?>> getParameters()
{    return Collections.emptyList();}
0
public void createParameters(String prefix, Configuration jobConf)
{}
0
public static double distance(double[] p1, double[] p2)
{    double dotProduct = 0.0;    double lengthSquaredp1 = 0.0;    double lengthSquaredp2 = 0.0;    for (int i = 0; i < p1.length; i++) {        lengthSquaredp1 += p1[i] * p1[i];        lengthSquaredp2 += p2[i] * p2[i];        dotProduct += p1[i] * p2[i];    }    double denominator = Math.sqrt(lengthSquaredp1) * Math.sqrt(lengthSquaredp2);        if (denominator < dotProduct) {        denominator = dotProduct;    }        if (denominator == 0 && dotProduct == 0) {        return 0;    }    return 1.0 - dotProduct / denominator;}
0
public double distance(Vector v1, Vector v2)
{    if (v1.size() != v2.size()) {        throw new CardinalityException(v1.size(), v2.size());    }    double lengthSquaredv1 = v1.getLengthSquared();    double lengthSquaredv2 = v2.getLengthSquared();    double dotProduct = v2.dot(v1);    double denominator = Math.sqrt(lengthSquaredv1) * Math.sqrt(lengthSquaredv2);        if (denominator < dotProduct) {        denominator = dotProduct;    }        if (denominator == 0 && dotProduct == 0) {        return 0;    }    return 1.0 - dotProduct / denominator;}
0
public double distance(double centroidLengthSquare, Vector centroid, Vector v)
{    double lengthSquaredv = v.getLengthSquared();    double dotProduct = v.dot(centroid);    double denominator = Math.sqrt(centroidLengthSquare) * Math.sqrt(lengthSquaredv);        if (denominator < dotProduct) {        denominator = dotProduct;    }        if (denominator == 0 && dotProduct == 0) {        return 0;    }    return 1.0 - dotProduct / denominator;}
0
public double distance(Vector v1, Vector v2)
{    return Math.sqrt(super.distance(v1, v2));}
0
public double distance(double centroidLengthSquare, Vector centroid, Vector v)
{    return Math.sqrt(super.distance(centroidLengthSquare, centroid, v));}
0
public void configure(Configuration jobConf)
{    if (parameters == null) {        ParameteredGeneralizations.configureParameters(this, jobConf);    }    try {        if (inverseCovarianceFile.get() != null) {            FileSystem fs = FileSystem.get(inverseCovarianceFile.get().toUri(), jobConf);            MatrixWritable inverseCovarianceMatrix = ClassUtils.instantiateAs((Class<? extends MatrixWritable>) matrixClass.get(), MatrixWritable.class);            if (!fs.exists(inverseCovarianceFile.get())) {                throw new FileNotFoundException(inverseCovarianceFile.get().toString());            }            try (DataInputStream in = fs.open(inverseCovarianceFile.get())) {                inverseCovarianceMatrix.readFields(in);            }            this.inverseCovarianceMatrix = inverseCovarianceMatrix.get();            Preconditions.checkArgument(this.inverseCovarianceMatrix != null, "inverseCovarianceMatrix not initialized");        }        if (meanVectorFile.get() != null) {            FileSystem fs = FileSystem.get(meanVectorFile.get().toUri(), jobConf);            VectorWritable meanVector = ClassUtils.instantiateAs((Class<? extends VectorWritable>) vectorClass.get(), VectorWritable.class);            if (!fs.exists(meanVectorFile.get())) {                throw new FileNotFoundException(meanVectorFile.get().toString());            }            try (DataInputStream in = fs.open(meanVectorFile.get())) {                meanVector.readFields(in);            }            this.meanVector = meanVector.get();            Preconditions.checkArgument(this.meanVector != null, "meanVector not initialized");        }    } catch (IOException e) {        throw new IllegalStateException(e);    }}
0
public Collection<Parameter<?>> getParameters()
{    return parameters;}
0
public void createParameters(String prefix, Configuration jobConf)
{    parameters = new ArrayList<>();    inverseCovarianceFile = new PathParameter(prefix, "inverseCovarianceFile", jobConf, null, "Path on DFS to a file containing the inverse covariance matrix.");    parameters.add(inverseCovarianceFile);    matrixClass = new ClassParameter(prefix, "maxtrixClass", jobConf, DenseMatrix.class, "Class<Matix> file specified in parameter inverseCovarianceFile has been serialized with.");    parameters.add(matrixClass);    meanVectorFile = new PathParameter(prefix, "meanVectorFile", jobConf, null, "Path on DFS to a file containing the mean Vector.");    parameters.add(meanVectorFile);    vectorClass = new ClassParameter(prefix, "vectorClass", jobConf, DenseVector.class, "Class file specified in parameter meanVectorFile has been serialized with.");    parameters.add(vectorClass);}
0
public double distance(Vector v)
{    return Math.sqrt(v.minus(meanVector).dot(Algebra.mult(inverseCovarianceMatrix, v.minus(meanVector))));}
0
public double distance(Vector v1, Vector v2)
{    if (v1.size() != v2.size()) {        throw new CardinalityException(v1.size(), v2.size());    }    return Math.sqrt(v1.minus(v2).dot(Algebra.mult(inverseCovarianceMatrix, v1.minus(v2))));}
0
public double distance(double centroidLengthSquare, Vector centroid, Vector v)
{        return distance(centroid, v);}
0
public void setInverseCovarianceMatrix(Matrix inverseCovarianceMatrix)
{    Preconditions.checkArgument(inverseCovarianceMatrix != null, "inverseCovarianceMatrix not initialized");    this.inverseCovarianceMatrix = inverseCovarianceMatrix;}
0
public void setCovarianceMatrix(Matrix m)
{    if (m.numRows() != m.numCols()) {        throw new CardinalityException(m.numRows(), m.numCols());    }                SingularValueDecomposition svd = new SingularValueDecomposition(m);    Matrix sInv = svd.getS();        for (int i = 0; i < sInv.numRows(); i++) {        double diagElem = sInv.get(i, i);        if (diagElem > 0.0) {            sInv.set(i, i, 1 / diagElem);        } else {            throw new IllegalStateException("Eigen Value equals to 0 found.");        }    }    inverseCovarianceMatrix = svd.getU().times(sInv.times(svd.getU().transpose()));    Preconditions.checkArgument(inverseCovarianceMatrix != null, "inverseCovarianceMatrix not initialized");}
0
public Matrix getInverseCovarianceMatrix()
{    return inverseCovarianceMatrix;}
0
public void setMeanVector(Vector meanVector)
{    Preconditions.checkArgument(meanVector != null, "meanVector not initialized");    this.meanVector = meanVector;}
0
public Vector getMeanVector()
{    return meanVector;}
0
public static double distance(double[] p1, double[] p2)
{    double result = 0.0;    for (int i = 0; i < p1.length; i++) {        result += Math.abs(p2[i] - p1[i]);    }    return result;}
0
public void configure(Configuration job)
{}
0
public Collection<Parameter<?>> getParameters()
{    return Collections.emptyList();}
0
public void createParameters(String prefix, Configuration jobConf)
{}
0
public double distance(Vector v1, Vector v2)
{    if (v1.size() != v2.size()) {        throw new CardinalityException(v1.size(), v2.size());    }    return v1.aggregate(v2, Functions.PLUS, Functions.MINUS_ABS);}
0
public double distance(double centroidLengthSquare, Vector centroid, Vector v)
{        return distance(centroid, v);}
0
public void createParameters(String prefix, Configuration conf)
{    parameters = new ArrayList<>();    Parameter<?> param = new DoubleParameter(prefix, "exponent", conf, EXPONENT, "Exponent for Fractional Lagrange distance");    parameters.add(param);}
0
public Collection<Parameter<?>> getParameters()
{    return parameters;}
0
public void configure(Configuration jobConf)
{    if (parameters == null) {        ParameteredGeneralizations.configureParameters(this, jobConf);    }}
0
public double getExponent()
{    return exponent;}
0
public void setExponent(double exponent)
{    this.exponent = exponent;}
0
public double distance(Vector v1, Vector v2)
{    return Math.pow(v1.aggregate(v2, Functions.PLUS, Functions.minusAbsPow(exponent)), 1.0 / exponent);}
0
public double distance(double centroidLengthSquare, Vector centroid, Vector v)
{        return distance(centroid, v);}
0
public void configure(Configuration job)
{}
0
public Collection<Parameter<?>> getParameters()
{    return Collections.emptyList();}
0
public void createParameters(String prefix, Configuration jobConf)
{}
0
public double distance(Vector v1, Vector v2)
{    return v2.getDistanceSquared(v1);}
0
public double distance(double centroidLengthSquare, Vector centroid, Vector v)
{    return centroidLengthSquare - 2 * v.dot(centroid) + v.getLengthSquared();}
0
public double distance(Vector a, Vector b)
{    double ab;    double denominator;    if (getWeights() != null) {        ab = a.times(b).aggregate(getWeights(), Functions.PLUS, Functions.MULT);        denominator = a.aggregate(getWeights(), Functions.PLUS, Functions.MULT_SQUARE_LEFT) + b.aggregate(getWeights(), Functions.PLUS, Functions.MULT_SQUARE_LEFT) - ab;    } else {                ab = b.dot(a);        denominator = a.getLengthSquared() + b.getLengthSquared() - ab;    }    if (denominator < ab) {                denominator = ab;    }    if (denominator > 0) {                return 1.0 - ab / denominator;    } else {        return 0.0;    }}
0
public double distance(double centroidLengthSquare, Vector centroid, Vector v)
{        return distance(centroid, v);}
0
public void createParameters(String prefix, Configuration jobConf)
{    parameters = new ArrayList<>();    weightsFile = new PathParameter(prefix, "weightsFile", jobConf, null, "Path on DFS to a file containing the weights.");    parameters.add(weightsFile);    vectorClass = new ClassParameter(prefix, "vectorClass", jobConf, DenseVector.class, "Class<Vector> file specified in parameter weightsFile has been serialized with.");    parameters.add(vectorClass);}
0
public Collection<Parameter<?>> getParameters()
{    return parameters;}
0
public void configure(Configuration jobConf)
{    if (parameters == null) {        ParameteredGeneralizations.configureParameters(this, jobConf);    }    try {        if (weightsFile.get() != null) {            FileSystem fs = FileSystem.get(weightsFile.get().toUri(), jobConf);            VectorWritable weights = ClassUtils.instantiateAs((Class<? extends VectorWritable>) vectorClass.get(), VectorWritable.class);            if (!fs.exists(weightsFile.get())) {                throw new FileNotFoundException(weightsFile.get().toString());            }            try (DataInputStream in = fs.open(weightsFile.get())) {                weights.readFields(in);            }            this.weights = weights.get();        }    } catch (IOException e) {        throw new IllegalStateException(e);    }}
0
public Vector getWeights()
{    return weights;}
0
public void setWeights(Vector weights)
{    this.weights = weights;}
0
public double distance(Vector p1, Vector p2)
{    double result = 0;    Vector res = p2.minus(p1);    Vector theWeights = getWeights();    if (theWeights == null) {        for (Element elt : res.nonZeroes()) {            result += elt.get() * elt.get();        }    } else {        for (Element elt : res.nonZeroes()) {            result += elt.get() * elt.get() * theWeights.get(elt.index());        }    }    return Math.sqrt(result);}
0
public double distance(double centroidLengthSquare, Vector centroid, Vector v)
{        return distance(centroid, v);}
0
public double distance(Vector p1, Vector p2)
{    double result = 0;    Vector res = p2.minus(p1);    if (getWeights() == null) {        for (Element elt : res.nonZeroes()) {            result += Math.abs(elt.get());        }    } else {        for (Element elt : res.nonZeroes()) {            result += Math.abs(elt.get() * getWeights().get(elt.index()));        }    }    return result;}
0
public double distance(double centroidLengthSquare, Vector centroid, Vector v)
{        return distance(centroid, v);}
0
public static Job prepareJob(Path inputPath, Path outputPath, Class<? extends InputFormat> inputFormat, Class<? extends Mapper> mapper, Class<? extends Writable> mapperKey, Class<? extends Writable> mapperValue, Class<? extends OutputFormat> outputFormat, Configuration conf) throws IOException
{    Job job = new Job(new Configuration(conf));    Configuration jobConf = job.getConfiguration();    if (mapper.equals(Mapper.class)) {        throw new IllegalStateException("Can't figure out the user class jar file from mapper/reducer");    }    job.setJarByClass(mapper);    job.setInputFormatClass(inputFormat);    jobConf.set("mapred.input.dir", inputPath.toString());    job.setMapperClass(mapper);    job.setMapOutputKeyClass(mapperKey);    job.setMapOutputValueClass(mapperValue);    job.setOutputKeyClass(mapperKey);    job.setOutputValueClass(mapperValue);    jobConf.setBoolean("mapred.compress.map.output", true);    job.setNumReduceTasks(0);    job.setOutputFormatClass(outputFormat);    jobConf.set("mapred.output.dir", outputPath.toString());    return job;}
0
public static Job prepareJob(Path inputPath, Path outputPath, Class<? extends InputFormat> inputFormat, Class<? extends Mapper> mapper, Class<? extends Writable> mapperKey, Class<? extends Writable> mapperValue, Class<? extends Reducer> reducer, Class<? extends Writable> reducerKey, Class<? extends Writable> reducerValue, Class<? extends OutputFormat> outputFormat, Configuration conf) throws IOException
{    Job job = new Job(new Configuration(conf));    Configuration jobConf = job.getConfiguration();    if (reducer.equals(Reducer.class)) {        if (mapper.equals(Mapper.class)) {            throw new IllegalStateException("Can't figure out the user class jar file from mapper/reducer");        }        job.setJarByClass(mapper);    } else {        job.setJarByClass(reducer);    }    job.setInputFormatClass(inputFormat);    jobConf.set("mapred.input.dir", inputPath.toString());    job.setMapperClass(mapper);    if (mapperKey != null) {        job.setMapOutputKeyClass(mapperKey);    }    if (mapperValue != null) {        job.setMapOutputValueClass(mapperValue);    }    jobConf.setBoolean("mapred.compress.map.output", true);    job.setReducerClass(reducer);    job.setOutputKeyClass(reducerKey);    job.setOutputValueClass(reducerValue);    job.setOutputFormatClass(outputFormat);    jobConf.set("mapred.output.dir", outputPath.toString());    return job;}
0
public static String getCustomJobName(String className, JobContext job, Class<? extends Mapper> mapper, Class<? extends Reducer> reducer)
{    StringBuilder name = new StringBuilder(100);    String customJobName = job.getJobName();    if (customJobName == null || customJobName.trim().isEmpty()) {        name.append(className);    } else {        name.append(customJobName);    }    name.append('-').append(mapper.getSimpleName());    name.append('-').append(reducer.getSimpleName());    return name.toString();}
0
public static void delete(Configuration conf, Iterable<Path> paths) throws IOException
{    if (conf == null) {        conf = new Configuration();    }    for (Path path : paths) {        FileSystem fs = path.getFileSystem(conf);        if (fs.exists(path)) {                        fs.delete(path, true);        }    }}
1
public static void delete(Configuration conf, Path... paths) throws IOException
{    delete(conf, Arrays.asList(paths));}
0
public static long countRecords(Path path, Configuration conf) throws IOException
{    long count = 0;    Iterator<?> iterator = new SequenceFileValueIterator<>(path, true, conf);    while (iterator.hasNext()) {        iterator.next();        count++;    }    return count;}
0
public static long countRecords(Path path, PathType pt, PathFilter filter, Configuration conf) throws IOException
{    long count = 0;    Iterator<?> iterator = new SequenceFileDirValueIterator<>(path, pt, filter, null, true, conf);    while (iterator.hasNext()) {        iterator.next();        count++;    }    return count;}
0
public static InputStream openStream(Path path, Configuration conf) throws IOException
{    FileSystem fs = FileSystem.get(path.toUri(), conf);    return fs.open(path.makeQualified(path.toUri(), path));}
0
public static FileStatus[] getFileStatus(Path path, PathType pathType, PathFilter filter, Comparator<FileStatus> ordering, Configuration conf) throws IOException
{    FileStatus[] statuses;    FileSystem fs = path.getFileSystem(conf);    if (filter == null) {        statuses = pathType == PathType.GLOB ? fs.globStatus(path) : listStatus(fs, path);    } else {        statuses = pathType == PathType.GLOB ? fs.globStatus(path, filter) : listStatus(fs, path, filter);    }    if (ordering != null) {        Arrays.sort(statuses, ordering);    }    return statuses;}
0
public static FileStatus[] listStatus(FileSystem fs, Path path) throws IOException
{    try {        return fs.listStatus(path);    } catch (FileNotFoundException e) {        return new FileStatus[0];    }}
0
public static FileStatus[] listStatus(FileSystem fs, Path path, PathFilter filter) throws IOException
{    try {        return fs.listStatus(path, filter);    } catch (FileNotFoundException e) {        return new FileStatus[0];    }}
0
public static void cacheFiles(Path fileToCache, Configuration conf)
{    DistributedCache.setCacheFiles(new URI[] { fileToCache.toUri() }, conf);}
0
public static Path getSingleCachedFile(Configuration conf) throws IOException
{    return getCachedFiles(conf)[0];}
0
public static Path[] getCachedFiles(Configuration conf) throws IOException
{    LocalFileSystem localFs = FileSystem.getLocal(conf);    Path[] cacheFiles = DistributedCache.getLocalCacheFiles(conf);    URI[] fallbackFiles = DistributedCache.getCacheFiles(conf);        if (cacheFiles == null) {        Preconditions.checkState(fallbackFiles != null, "Unable to find cached files!");        cacheFiles = new Path[fallbackFiles.length];        for (int n = 0; n < fallbackFiles.length; n++) {            cacheFiles[n] = new Path(fallbackFiles[n].getPath());        }    } else {        for (int n = 0; n < cacheFiles.length; n++) {            cacheFiles[n] = localFs.makeQualified(cacheFiles[n]);                        if (!localFs.exists(cacheFiles[n])) {                cacheFiles[n] = new Path(fallbackFiles[n].getPath());            }        }    }    Preconditions.checkState(cacheFiles.length > 0, "Unable to find cached files!");    return cacheFiles;}
0
public static void setSerializations(Configuration configuration)
{    configuration.set("io.serializations", "org.apache.hadoop.io.serializer.JavaSerialization," + "org.apache.hadoop.io.serializer.WritableSerialization");}
0
public static void writeInt(int value, Path path, Configuration configuration) throws IOException
{    FileSystem fs = FileSystem.get(path.toUri(), configuration);    try (FSDataOutputStream out = fs.create(path)) {        out.writeInt(value);    }}
0
public static int readInt(Path path, Configuration configuration) throws IOException
{    FileSystem fs = FileSystem.get(path.toUri(), configuration);    try (FSDataInputStream in = fs.open(path)) {        return in.readInt();    }}
0
public static String buildDirList(FileSystem fs, FileStatus fileStatus) throws IOException
{    boolean containsFiles = false;    List<String> directoriesList = new ArrayList<>();    for (FileStatus childFileStatus : fs.listStatus(fileStatus.getPath())) {        if (childFileStatus.isDir()) {            String subDirectoryList = buildDirList(fs, childFileStatus);            directoriesList.add(subDirectoryList);        } else {            containsFiles = true;        }    }    if (containsFiles) {        directoriesList.add(fileStatus.getPath().toUri().getPath());    }    return Joiner.on(',').skipNulls().join(directoriesList.iterator());}
0
public static String buildDirList(FileSystem fs, FileStatus fileStatus, PathFilter pathFilter) throws IOException
{    boolean containsFiles = false;    List<String> directoriesList = new ArrayList<>();    for (FileStatus childFileStatus : fs.listStatus(fileStatus.getPath(), pathFilter)) {        if (childFileStatus.isDir()) {            String subDirectoryList = buildDirList(fs, childFileStatus);            directoriesList.add(subDirectoryList);        } else {            containsFiles = true;        }    }    if (containsFiles) {        directoriesList.add(fileStatus.getPath().toUri().getPath());    }    return Joiner.on(',').skipNulls().join(directoriesList.iterator());}
0
public static String calcRelativeFilePath(Configuration configuration, Path filePath) throws IOException
{    FileSystem fs = filePath.getFileSystem(configuration);    FileStatus fst = fs.getFileStatus(filePath);    String currentPath = fst.getPath().toString().replaceFirst("file:", "");    String basePath = configuration.get("baseinputpath");    if (!basePath.endsWith("/")) {        basePath += "/";    }    basePath = basePath.replaceFirst("file:", "");    String[] parts = currentPath.split(basePath);    if (parts.length == 2) {        return parts[1];    } else if (parts.length == 1) {        return parts[0];    }    return currentPath;}
0
public static Path findInCacheByPartOfFilename(String partOfFilename, URI[] localFiles)
{    for (URI distCacheFile : localFiles) {                if (distCacheFile != null && distCacheFile.toString().contains(partOfFilename)) {                        return new Path(distCacheFile.getPath());        }    }    return null;}
1
public boolean add(Integer entry)
{    return tuple.add(entry);}
0
public Integer integerAt(int index)
{    return tuple.get(index);}
0
public Integer replaceAt(int index, Integer newInteger)
{    return tuple.set(index, newInteger);}
0
public List<Integer> getEntries()
{    return Collections.unmodifiableList(this.tuple);}
0
public int length()
{    return this.tuple.size();}
0
public String toString()
{    return tuple.toString();}
0
public int hashCode()
{    return tuple.hashCode();}
0
public boolean equals(Object obj)
{    if (this == obj) {        return true;    }    if (obj == null) {        return false;    }    if (getClass() != obj.getClass()) {        return false;    }    IntegerTuple other = (IntegerTuple) obj;    if (tuple == null) {        if (other.tuple != null) {            return false;        }    } else if (!tuple.equals(other.tuple)) {        return false;    }    return true;}
0
public void readFields(DataInput in) throws IOException
{    int len = in.readInt();    tuple = Lists.newArrayListWithCapacity(len);    for (int i = 0; i < len; i++) {        int data = in.readInt();        tuple.add(data);    }}
0
public void write(DataOutput out) throws IOException
{    out.writeInt(tuple.size());    for (Integer entry : tuple) {        out.writeInt(entry);    }}
0
public int compareTo(IntegerTuple otherTuple)
{    int thisLength = length();    int otherLength = otherTuple.length();    int min = Math.min(thisLength, otherLength);    for (int i = 0; i < min; i++) {        int ret = this.tuple.get(i).compareTo(otherTuple.integerAt(i));        if (ret == 0) {            continue;        }        return ret;    }    if (thisLength < otherLength) {        return -1;    } else if (thisLength > otherLength) {        return 1;    } else {        return 0;    }}
0
public void set(int x, int y)
{    putInt(x, b, 0);    putInt(y, b, INT_BYTE_LENGTH);}
0
public void setFirst(int x)
{    putInt(x, b, 0);}
0
public int getFirst()
{    return getInt(b, 0);}
0
public void setSecond(int y)
{    putInt(y, b, INT_BYTE_LENGTH);}
0
public int getSecond()
{    return getInt(b, INT_BYTE_LENGTH);}
0
public void readFields(DataInput in) throws IOException
{    in.readFully(b);}
0
public void write(DataOutput out) throws IOException
{    out.write(b);}
0
public int hashCode()
{    return Arrays.hashCode(b);}
0
public boolean equals(Object obj)
{    if (this == obj) {        return true;    }    if (!super.equals(obj)) {        return false;    }    if (!(obj instanceof IntPairWritable)) {        return false;    }    IntPairWritable other = (IntPairWritable) obj;    return Arrays.equals(b, other.b);}
0
public int compareTo(BinaryComparable other)
{    return Comparator.doCompare(b, 0, ((IntPairWritable) other).b, 0);}
0
public Object clone()
{    return new IntPairWritable(this);}
0
public String toString()
{    return "(" + getFirst() + ", " + getSecond() + ')';}
0
public byte[] getBytes()
{    return b;}
0
public int getLength()
{    return INT_PAIR_BYTE_LENGTH;}
0
private static void putInt(int value, byte[] b, int offset)
{    for (int i = offset, j = 24; j >= 0; i++, j -= 8) {        b[i] = (byte) (value >> j);    }}
0
private static int getInt(byte[] b, int offset)
{    int value = 0;    for (int i = offset, j = 24; j >= 0; i++, j -= 8) {        value |= (b[i] & 0xFF) << j;    }    return value;}
0
public int compare(byte[] b1, int s1, int l1, byte[] b2, int s2, int l2)
{    return doCompare(b1, s1, b2, s2);}
0
 static int doCompare(byte[] b1, int s1, byte[] b2, int s2)
{    int compare1 = compareInts(b1, s1, b2, s2);    if (compare1 != 0) {        return compare1;    }    return compareInts(b1, s1 + INT_BYTE_LENGTH, b2, s2 + INT_BYTE_LENGTH);}
0
private static int compareInts(byte[] b1, int s1, byte[] b2, int s2)
{        int end1 = s1 + INT_BYTE_LENGTH;    for (int i = s1, j = s2; i < end1; i++, j++) {        int a = b1[i];        int b = b2[j];        if (i > s1) {            a &= 0xff;            b &= 0xff;        }        if (a != b) {            return a - b;        }    }    return 0;}
0
public int compare(byte[] b1, int s1, int l1, byte[] b2, int s2, int l2)
{    int firstb1 = WritableComparator.readInt(b1, s1);    int firstb2 = WritableComparator.readInt(b2, s2);    if (firstb1 < firstb2) {        return -1;    } else if (firstb1 > firstb2) {        return 1;    } else {        return 0;    }}
0
public int compare(Object o1, Object o2)
{    int firstb1 = ((IntPairWritable) o1).getFirst();    int firstb2 = ((IntPairWritable) o2).getFirst();    if (firstb1 < firstb2) {        return -1;    }    if (firstb1 > firstb2) {        return 1;    }    return 0;}
0
public double getFrequency()
{    return frequency;}
0
public IntPairWritable getPair()
{    return pair;}
0
public int hashCode()
{    return pair.hashCode() + RandomUtils.hashDouble(frequency);}
0
public boolean equals(Object right)
{    if (!(right instanceof Frequency)) {        return false;    }    Frequency that = (Frequency) right;    return pair.equals(that.pair) && frequency == that.frequency;}
0
public int compareTo(Frequency that)
{    if (frequency < that.frequency) {        return -1;    }    if (frequency > that.frequency) {        return 1;    }    return 0;}
0
public String toString()
{    return pair + "\t" + frequency;}
0
public T apply(T from)
{    if (constructor == null) {        Class<T> elementClass = (Class<T>) from.getClass();        try {            constructor = elementClass.getConstructor(elementClass);        } catch (NoSuchMethodException e) {            throw new IllegalStateException(e);        }    }    try {        return constructor.newInstance(from);    } catch (InstantiationException | IllegalAccessException | InvocationTargetException e) {        throw new IllegalStateException(e);    }}
0
protected Iterator<T> delegate()
{    return delegate;}
0
protected Integer computeNext()
{    if (count < to) {        return count++;    } else {        return endOfData();    }}
0
public Iterator<String> iterator()
{    try {        return new FileLineIterator(is, encoding, skipFirstLine, this.origFilename);    } catch (IOException ioe) {        throw new IllegalStateException(ioe);    }}
0
 static InputStream getFileInputStream(File file) throws IOException
{    InputStream is = new FileInputStream(file);    String name = file.getName();    if ("gz".equalsIgnoreCase(Files.getFileExtension(name.toLowerCase()))) {        return new GZIPInputStream(is);    } else if ("zip".equalsIgnoreCase(Files.getFileExtension(name.toLowerCase()))) {        return new ZipInputStream(is);    } else {        return is;    }}
0
protected String computeNext()
{    String line;    try {        line = reader.readLine();    } catch (IOException ioe) {        try {            close();        } catch (IOException e) {                    }        throw new IllegalStateException(ioe);    }    return line == null ? endOfData() : line;}
1
public void skip(int n)
{    try {        for (int i = 0; i < n; i++) {            if (reader.readLine() == null) {                break;            }        }    } catch (IOException ioe) {        try {            close();        } catch (IOException e) {            throw new IllegalStateException(e);        }    }}
0
public void close() throws IOException
{    endOfData();    Closeables.close(reader, true);}
0
protected Iterator<T> delegate()
{    return delegate;}
0
public Iterator<T> iterator()
{    return new SamplingIterator<>(delegate.iterator(), samplingRate);}
0
public static Iterable<T> maybeWrapIterable(Iterable<T> delegate, double samplingRate)
{    return samplingRate >= 1.0 ? delegate : new SamplingIterable<>(delegate, samplingRate);}
0
protected T computeNext()
{    int toSkip = geometricDistribution.sample();    if (delegate instanceof SkippingIterator<?>) {        SkippingIterator<? extends T> skippingDelegate = (SkippingIterator<? extends T>) delegate;        skippingDelegate.skip(toSkip);        if (skippingDelegate.hasNext()) {            return skippingDelegate.next();        }    } else {        for (int i = 0; i < toSkip && delegate.hasNext(); i++) {            delegate.next();        }        if (delegate.hasNext()) {            return delegate.next();        }    }    return endOfData();}
0
public boolean accept(Path path)
{    String name = path.getName();    return name.startsWith("part-") && !name.endsWith(".crc");}
0
public boolean accept(Path path)
{    String name = path.getName();    return name.startsWith("clusters-") && name.endsWith("-final");}
0
public boolean accept(Path path)
{    String name = path.getName();    return !(name.endsWith(".crc") || name.startsWith(".") || name.startsWith("_"));}
0
public static PathFilter partFilter()
{    return PART_FILE_INSTANCE;}
0
public static PathFilter finalPartFilter()
{    return CLUSTER_FINAL;}
0
public static PathFilter logsCRCFilter()
{    return LOGS_CRC_INSTANCE;}
0
public Iterator<Pair<K, V>> iterator()
{    try {        return new SequenceFileDirIterator<>(path, pathType, filter, ordering, reuseKeyValueInstances, conf);    } catch (IOException ioe) {        throw new IllegalStateException(path.toString(), ioe);    }}
0
private void init(FileStatus[] statuses, final boolean reuseKeyValueInstances, final Configuration conf)
{    /*     * prevent NPEs. Unfortunately, Hadoop would return null for list if nothing     * was qualified. In this case, which is a corner case, we should assume an     * empty iterator, not an NPE.     */    if (statuses == null) {        statuses = NO_STATUSES;    }    Iterator<FileStatus> fileStatusIterator = Iterators.forArray(statuses);    Iterator<Iterator<Pair<K, V>>> fsIterators = Iterators.transform(fileStatusIterator, new Function<FileStatus, Iterator<Pair<K, V>>>() {        @Override        public Iterator<Pair<K, V>> apply(FileStatus from) {            try {                SequenceFileIterator<K, V> iterator = new SequenceFileIterator<>(from.getPath(), reuseKeyValueInstances, conf);                iterators.add(iterator);                return iterator;            } catch (IOException ioe) {                throw new IllegalStateException(from.getPath().toString(), ioe);            }        }    });        Collections.reverse(iterators);    delegate = Iterators.concat(fsIterators);}
0
public Iterator<Pair<K, V>> apply(FileStatus from)
{    try {        SequenceFileIterator<K, V> iterator = new SequenceFileIterator<>(from.getPath(), reuseKeyValueInstances, conf);        iterators.add(iterator);        return iterator;    } catch (IOException ioe) {        throw new IllegalStateException(from.getPath().toString(), ioe);    }}
0
protected Iterator<Pair<K, V>> delegate()
{    return delegate;}
0
public void close() throws IOException
{    IOUtils.close(iterators);    iterators.clear();}
0
public Iterator<V> iterator()
{    try {        return new SequenceFileDirValueIterator<>(path, pathType, filter, ordering, reuseKeyValueInstances, conf);    } catch (IOException ioe) {        throw new IllegalStateException(path.toString(), ioe);    }}
0
private void init(FileStatus[] statuses, Comparator<FileStatus> ordering, final boolean reuseKeyValueInstances, final Configuration conf) throws IOException
{    /*     * prevent NPEs. Unfortunately, Hadoop would return null for list if nothing     * was qualified. In this case, which is a corner case, we should assume an     * empty iterator, not an NPE.     */    if (statuses == null) {        statuses = NO_STATUSES;    }    if (ordering != null) {        Arrays.sort(statuses, ordering);    }    Iterator<FileStatus> fileStatusIterator = Iterators.forArray(statuses);    try {        Iterator<Iterator<V>> fsIterators = Iterators.transform(fileStatusIterator, new Function<FileStatus, Iterator<V>>() {            @Override            public Iterator<V> apply(FileStatus from) {                try {                    SequenceFileValueIterator<V> iterator = new SequenceFileValueIterator<>(from.getPath(), reuseKeyValueInstances, conf);                    iterators.add(iterator);                    return iterator;                } catch (IOException ioe) {                    throw new IllegalStateException(from.getPath().toString(), ioe);                }            }        });                Collections.reverse(iterators);        delegate = Iterators.concat(fsIterators);    } finally {        /*       * prevent file handle leaks in case one of handles fails to open. If some       * of the files fail to open, constructor will fail and close() will never       * be called. Thus, those handles that did open in constructor, would leak       * out, unless we specifically handle it here.       */        IOUtils.close(iterators);    }}
0
public Iterator<V> apply(FileStatus from)
{    try {        SequenceFileValueIterator<V> iterator = new SequenceFileValueIterator<>(from.getPath(), reuseKeyValueInstances, conf);        iterators.add(iterator);        return iterator;    } catch (IOException ioe) {        throw new IllegalStateException(from.getPath().toString(), ioe);    }}
0
protected Iterator<V> delegate()
{    return delegate;}
0
public void close() throws IOException
{    IOUtils.close(iterators);}
0
public Iterator<Pair<K, V>> iterator()
{    try {        return new SequenceFileIterator<>(path, reuseKeyValueInstances, conf);    } catch (IOException ioe) {        throw new IllegalStateException(path.toString(), ioe);    }}
0
public Class<K> getKeyClass()
{    return keyClass;}
0
public Class<V> getValueClass()
{    return valueClass;}
0
public void close() throws IOException
{    key = null;    value = null;    Closeables.close(reader, true);    endOfData();}
0
protected Pair<K, V> computeNext()
{    if (!reuseKeyValueInstances || value == null) {        key = ReflectionUtils.newInstance(keyClass, conf);        if (!noValue) {            value = ReflectionUtils.newInstance(valueClass, conf);        }    }    try {        boolean available;        if (noValue) {            available = reader.next(key);        } else {            available = reader.next(key, value);        }        if (!available) {            close();            return null;        }        return new Pair<>(key, value);    } catch (IOException ioe) {        try {            close();        } catch (IOException e) {                    }        throw new IllegalStateException(ioe);    }}
1
public Iterator<V> iterator()
{    try {        return new SequenceFileValueIterator<>(path, reuseKeyValueInstances, conf);    } catch (IOException ioe) {        throw new IllegalStateException(path.toString(), ioe);    }}
0
public Class<V> getValueClass()
{    return valueClass;}
0
public void close() throws IOException
{    value = null;    Closeables.close(reader, true);    endOfData();}
0
protected V computeNext()
{    if (!reuseKeyValueInstances || value == null) {        value = ReflectionUtils.newInstance(valueClass, conf);    }    try {        boolean available = reader.next(key, value);        if (!available) {            close();            return null;        }        return value;    } catch (IOException ioe) {        try {            close();        } catch (IOException e) {                    }        throw new IllegalStateException(ioe);    }}
1
public T apply(Pair<Integer, T> from)
{    return from.getSecond();}
0
protected Iterator<T> delegate()
{    return delegate;}
0
public Pair<List<String>, Long> apply(String from)
{    String[] items = splitter.split(from);    return new Pair<>(Arrays.asList(items), ONE);}
0
protected Iterator<Pair<List<String>, Long>> delegate()
{    return delegate;}
0
public long getFirst()
{    return first;}
0
public long getSecond()
{    return second;}
0
public LongPair swap()
{    return new LongPair(second, first);}
0
public boolean equals(Object obj)
{    if (!(obj instanceof LongPair)) {        return false;    }    LongPair otherPair = (LongPair) obj;    return first == otherPair.getFirst() && second == otherPair.getSecond();}
0
public int hashCode()
{    int firstHash = Longs.hashCode(first);        return (firstHash >>> 16 | firstHash << 16) ^ Longs.hashCode(second);}
0
public String toString()
{    return '(' + String.valueOf(first) + ',' + second + ')';}
0
public int compareTo(LongPair o)
{    if (first < o.getFirst()) {        return -1;    } else if (first > o.getFirst()) {        return 1;    } else {        return second < o.getSecond() ? -1 : second > o.getSecond() ? 1 : 0;    }}
0
public static Analyzer createAnalyzer(String analyzerClassName) throws ClassNotFoundException
{    return createAnalyzer(analyzerClassName, Version.LUCENE_5_5_2);}
0
public static Analyzer createAnalyzer(String analyzerClassName, Version version) throws ClassNotFoundException
{    Class<? extends Analyzer> analyzerClass = Class.forName(analyzerClassName).asSubclass(Analyzer.class);    return createAnalyzer(analyzerClass, version);}
0
public static Analyzer createAnalyzer(Class<? extends Analyzer> analyzerClass)
{    return createAnalyzer(analyzerClass, Version.LUCENE_5_5_2);}
0
public static Analyzer createAnalyzer(Class<? extends Analyzer> analyzerClass, Version version)
{    try {        return ClassUtils.instantiateAs(analyzerClass, Analyzer.class, new Class<?>[] { Version.class }, new Object[] { version });    } catch (IllegalStateException e) {        return ClassUtils.instantiateAs(analyzerClass, Analyzer.class);    }}
0
public boolean incrementToken()
{    if (iterator.hasNext()) {        clearAttributes();        termAtt.append(iterator.next());        return true;    } else {        return false;    }}
0
protected String computeNext()
{    try {        if (tokenStream.incrementToken()) {            return tokenStream.getAttribute(CharTermAttribute.class).toString();        } else {            tokenStream.end();            tokenStream.close();            return endOfData();        }    } catch (IOException e) {        throw new IllegalStateException("IO error while tokenizing", e);    }}
0
public void reduce(WritableComparable<?> key, Iterable<VectorWritable> vectors, Context ctx) throws IOException, InterruptedException
{    ctx.write(key, VectorWritable.merge(vectors.iterator()));}
0
public void reduce(WritableComparable<?> key, Iterable<VectorWritable> vectors, Context ctx) throws IOException, InterruptedException
{    Vector merged = VectorWritable.merge(vectors.iterator()).get();    result.set(new SequentialAccessSparseVector(merged));    ctx.write(key, result);}
0
protected void setup(Context ctx) throws IOException, InterruptedException
{    newNumCols = ctx.getConfiguration().getInt(NEW_NUM_COLS_PARAM, Integer.MAX_VALUE);}
0
protected void map(IntWritable r, VectorWritable v, Context ctx) throws IOException, InterruptedException
{    int row = r.get();    for (Vector.Element e : v.get().nonZeroes()) {        RandomAccessSparseVector tmp = new RandomAccessSparseVector(newNumCols, 1);        tmp.setQuick(row, e.get());        r.set(e.index());        ctx.write(r, new VectorWritable(tmp));    }}
0
protected void reduce(WritableComparable<?> key, Iterable<VectorWritable> values, Context ctx) throws IOException, InterruptedException
{    result.set(Vectors.sum(values.iterator()));    ctx.write(key, result);}
0
protected void reduce(WritableComparable<?> key, Iterable<VectorWritable> values, Context ctx) throws IOException, InterruptedException
{    ctx.write(key, new VectorWritable(Vectors.sum(values.iterator())));}
0
public static void logMemoryStatistics()
{    Runtime runtime = Runtime.getRuntime();    long freeBytes = runtime.freeMemory();    long maxBytes = runtime.maxMemory();    long totalBytes = runtime.totalMemory();    long usedBytes = totalBytes - freeBytes;    }
1
public static void startMemoryLogger(long rateInMillis)
{    stopMemoryLogger();    scheduler = Executors.newScheduledThreadPool(1, new ThreadFactory() {        private final ThreadFactory delegate = Executors.defaultThreadFactory();        @Override        public Thread newThread(Runnable r) {            Thread t = delegate.newThread(r);            t.setDaemon(true);            return t;        }    });    Runnable memoryLoogerRunnable = new Runnable() {        @Override        public void run() {            logMemoryStatistics();        }    };    scheduler.scheduleAtFixedRate(memoryLoogerRunnable, rateInMillis, rateInMillis, TimeUnit.MILLISECONDS);}
0
public Thread newThread(Runnable r)
{    Thread t = delegate.newThread(r);    t.setDaemon(true);    return t;}
0
public void run()
{    logMemoryStatistics();}
0
public static void startMemoryLogger()
{    startMemoryLogger(1000);}
0
public static void stopMemoryLogger()
{    if (scheduler != null) {        scheduler.shutdownNow();        scheduler = null;    }}
0
public Map<String, List<String>> generateNGrams()
{    Map<String, List<String>> returnDocument = Maps.newHashMap();    Iterator<String> tokenizer = SPACE_TAB.split(line).iterator();    List<String> tokens = Lists.newArrayList();    String labelName = tokenizer.next();    List<String> previousN1Grams = Lists.newArrayList();    while (tokenizer.hasNext()) {        String nextToken = tokenizer.next();        if (previousN1Grams.size() == gramSize) {            previousN1Grams.remove(0);        }        previousN1Grams.add(nextToken);        StringBuilder gramBuilder = new StringBuilder();        for (String gram : previousN1Grams) {            gramBuilder.append(gram);            String token = gramBuilder.toString();            tokens.add(token);            gramBuilder.append(' ');        }    }    returnDocument.put(labelName, tokens);    return returnDocument;}
0
public List<String> generateNGramsWithoutLabel()
{    List<String> tokens = Lists.newArrayList();    List<String> previousN1Grams = Lists.newArrayList();    for (String nextToken : SPACE_TAB.split(line)) {        if (previousN1Grams.size() == gramSize) {            previousN1Grams.remove(0);        }        previousN1Grams.add(nextToken);        StringBuilder gramBuilder = new StringBuilder();        for (String gram : previousN1Grams) {            gramBuilder.append(gram);            String token = gramBuilder.toString();            tokens.add(token);            gramBuilder.append(' ');        }    }    return tokens;}
0
public A getFirst()
{    return first;}
0
public B getSecond()
{    return second;}
0
public Pair<B, A> swap()
{    return new Pair<>(second, first);}
0
public static Pair<A, B> of(A a, B b)
{    return new Pair<>(a, b);}
0
public boolean equals(Object obj)
{    if (!(obj instanceof Pair<?, ?>)) {        return false;    }    Pair<?, ?> otherPair = (Pair<?, ?>) obj;    return isEqualOrNulls(first, otherPair.getFirst()) && isEqualOrNulls(second, otherPair.getSecond());}
0
private static boolean isEqualOrNulls(Object obj1, Object obj2)
{    return obj1 == null ? obj2 == null : obj1.equals(obj2);}
0
public int hashCode()
{    int firstHash = hashCodeNull(first);        return (firstHash >>> 16 | firstHash << 16) ^ hashCodeNull(second);}
0
private static int hashCodeNull(Object obj)
{    return obj == null ? 0 : obj.hashCode();}
0
public String toString()
{    return '(' + String.valueOf(first) + ',' + second + ')';}
0
public int compareTo(Pair<A, B> other)
{    Comparable<A> thisFirst = (Comparable<A>) first;    A thatFirst = other.getFirst();    int compare = thisFirst.compareTo(thatFirst);    if (compare != 0) {        return compare;    }    Comparable<B> thisSecond = (Comparable<B>) second;    B thatSecond = other.getSecond();    return thisSecond.compareTo(thatSecond);}
0
public void configure(Configuration jobConf)
{}
0
public void createParameters(String prefix, Configuration jobConf)
{}
0
public String getStringValue()
{    if (value == null) {        return null;    }    return value.toString();}
0
public Collection<Parameter<?>> getParameters()
{    return Collections.emptyList();}
0
public String prefix()
{    return prefix;}
0
public String name()
{    return name;}
0
public String description()
{    return description;}
0
public Class<T> type()
{    return type;}
0
public String defaultValue()
{    return defaultValue;}
0
public T get()
{    return value;}
0
public void set(T value)
{    this.value = value;}
0
public String toString()
{    if (value != null) {        return value.toString();    } else {        return super.toString();    }}
0
public void setStringValue(String stringValue)
{    try {        set(Class.forName(stringValue));    } catch (ClassNotFoundException e) {        throw new IllegalStateException(e);    }}
0
public String getStringValue()
{    if (get() == null) {        return null;    }    return get().getName();}
0
public void setStringValue(String stringValue)
{    set(Double.valueOf(stringValue));}
0
public static void configureParameters(Parametered parametered, Configuration jobConf)
{    configureParameters(parametered.getClass().getSimpleName() + '.', parametered, jobConf);}
0
public static void configureParameters(String prefix, Parametered parametered, Configuration jobConf)
{    parametered.createParameters(prefix, jobConf);    configureParametersRecursively(parametered, prefix, jobConf);}
0
private static void configureParametersRecursively(Parametered parametered, String prefix, Configuration jobConf)
{    for (Parameter<?> parameter : parametered.getParameters()) {        if (log.isDebugEnabled()) {                    }        String name = prefix + parameter.name() + '.';        parameter.createParameters(name, jobConf);        parameter.configure(jobConf);        if (!parameter.getParameters().isEmpty()) {            configureParametersRecursively(parameter, name, jobConf);        }    }}
1
public static String help(Parametered parametered)
{    return new Help(parametered).toString();}
0
public static String conf(Parametered parametered)
{    return new Conf(parametered).toString();}
0
public String toString()
{    return sb.toString();}
0
private void recurseCount(Parametered parametered)
{    for (Parameter<?> parameter : parametered.getParameters()) {        int parameterNameLength = parameter.name().length();        if (parameterNameLength > longestName) {            longestName = parameterNameLength;        }        recurseCount(parameter);        numChars += parameter.description().length();    }}
0
private void recurseWrite(Parametered parametered)
{    for (Parameter<?> parameter : parametered.getParameters()) {        sb.append(parameter.prefix());        sb.append(parameter.name());        int max = longestName - parameter.name().length() - parameter.prefix().length() + NAME_DESC_DISTANCE;        for (int i = 0; i < max; i++) {            sb.append(' ');        }        sb.append(parameter.description());        if (parameter.defaultValue() != null) {            sb.append(" (default value '");            sb.append(parameter.defaultValue());            sb.append("')");        }        sb.append('\n');        recurseWrite(parameter);    }}
0
public String toString()
{    return sb.toString();}
0
private void recurseCount(Parametered parametered)
{    for (Parameter<?> parameter : parametered.getParameters()) {        int parameterNameLength = parameter.prefix().length() + parameter.name().length();        if (parameterNameLength > longestName) {            longestName = parameterNameLength;        }        numChars += parameterNameLength;                numChars += 5;        numChars += parameter.description().length();        if (parameter.getStringValue() != null) {            numChars += parameter.getStringValue().length();        }        recurseCount(parameter);    }}
0
private void recurseWrite(Parametered parametered)
{    for (Parameter<?> parameter : parametered.getParameters()) {        sb.append("# ");        sb.append(parameter.description());        sb.append('\n');        sb.append(parameter.prefix());        sb.append(parameter.name());        sb.append(" = ");        if (parameter.getStringValue() != null) {            sb.append(parameter.getStringValue());        }        sb.append('\n');        sb.append('\n');        recurseWrite(parameter);    }}
0
public void setStringValue(String stringValue)
{    set(new Path(stringValue));}
0
public String get(String key)
{    return params.get(key);}
0
public String get(String key, String defaultValue)
{    String ret = params.get(key);    return ret == null ? defaultValue : ret;}
0
public void set(String key, String value)
{    params.put(key, value);}
0
public int getInt(String key, int defaultValue)
{    String ret = params.get(key);    return ret == null ? defaultValue : Integer.parseInt(ret);}
0
public String toString()
{    Configuration conf = new Configuration();    conf.set("io.serializations", "org.apache.hadoop.io.serializer.JavaSerialization," + "org.apache.hadoop.io.serializer.WritableSerialization");    DefaultStringifier<Map<String, String>> mapStringifier = new DefaultStringifier<>(conf, GenericsUtil.getClass(params));    try {        return mapStringifier.toString(params);    } catch (IOException e) {                return "";    }}
1
public String print()
{    return params.toString();}
0
public static Map<String, String> parseParams(String serializedString) throws IOException
{    Configuration conf = new Configuration();    conf.set("io.serializations", "org.apache.hadoop.io.serializer.JavaSerialization," + "org.apache.hadoop.io.serializer.WritableSerialization");    Map<String, String> params = Maps.newHashMap();    DefaultStringifier<Map<String, String>> mapStringifier = new DefaultStringifier<>(conf, GenericsUtil.getClass(params));    return mapStringifier.fromString(serializedString);}
0
public boolean add(String entry)
{    return tuple.add(entry);}
0
public String stringAt(int index)
{    return tuple.get(index);}
0
public String replaceAt(int index, String newString)
{    return tuple.set(index, newString);}
0
public List<String> getEntries()
{    return Collections.unmodifiableList(this.tuple);}
0
public int length()
{    return this.tuple.size();}
0
public String toString()
{    return tuple.toString();}
0
public int hashCode()
{    return tuple.hashCode();}
0
public boolean equals(Object obj)
{    if (this == obj) {        return true;    }    if (obj == null) {        return false;    }    if (getClass() != obj.getClass()) {        return false;    }    StringTuple other = (StringTuple) obj;    if (tuple == null) {        if (other.tuple != null) {            return false;        }    } else if (!tuple.equals(other.tuple)) {        return false;    }    return true;}
0
public void readFields(DataInput in) throws IOException
{    int len = in.readInt();    tuple = Lists.newArrayListWithCapacity(len);    Text value = new Text();    for (int i = 0; i < len; i++) {        value.readFields(in);        tuple.add(value.toString());    }}
0
public void write(DataOutput out) throws IOException
{    out.writeInt(tuple.size());    Text value = new Text();    for (String entry : tuple) {        value.set(entry);        value.write(out);    }}
0
public int compareTo(StringTuple otherTuple)
{    int thisLength = length();    int otherLength = otherTuple.length();    int min = Math.min(thisLength, otherLength);    for (int i = 0; i < min; i++) {        int ret = this.tuple.get(i).compareTo(otherTuple.stringAt(i));        if (ret != 0) {            return ret;        }    }    if (thisLength < otherLength) {        return -1;    } else if (thisLength > otherLength) {        return 1;    } else {        return 0;    }}
0
public static String toString(Object obj)
{    return NEWLINE_PATTERN.matcher(XSTREAM.toXML(obj)).replaceAll("");}
0
public static T fromString(String str)
{    return (T) XSTREAM.fromXML(str);}
0
public static String escapeXML(CharSequence input)
{    return XMLRESERVED.matcher(input).replaceAll("_");}
0
public synchronized int getNCalls()
{    return nCalls;}
0
public synchronized long getMinTime()
{    return Math.max(0, minTime);}
0
public synchronized long getMaxTime()
{    return maxTime;}
0
public synchronized long getSumTime()
{    return sumTime;}
0
public synchronized double getSumSquaredTime()
{    return sumSquaredTime;}
0
public synchronized long getMeanTime()
{    return nCalls == 0 ? 0 : sumTime / nCalls;}
0
public synchronized long getStdDevTime()
{    if (nCalls == 0) {        return 0;    }    double mean = getMeanTime();    double meanSquared = mean * mean;    double meanOfSquares = sumSquaredTime / nCalls;    double variance = meanOfSquares - meanSquared;    if (variance < 0) {                return 0;    }    return (long) Math.sqrt(variance);}
0
public synchronized String toString()
{    return '\n' + "nCalls = " + nCalls + ";\n" + "sum    = " + DF.format(sumTime / 1000000000.0) + "s;\n" + "min    = " + DF.format(minTime / 1000000.0) + "ms;\n" + "max    = " + DF.format(maxTime / 1000000.0) + "ms;\n" + "mean   = " + DF.format(getMeanTime() / 1000.0) + "us;\n" + "stdDev = " + DF.format(getStdDevTime() / 1000.0) + "us;";}
0
public Call newCall(long leadTimeUsec)
{    if (leadSumTime > leadTimeUsec) {        return new Call();    } else {        return new LeadTimeCall();    }}
0
public void end()
{    long elapsed = System.nanoTime() - startTime;    synchronized (TimingStatistics.this) {        leadSumTime += elapsed;    }}
0
public boolean end(long sumMaxUsec)
{    end();    return false;}
0
public void end()
{    long elapsed = System.nanoTime() - startTime;    synchronized (TimingStatistics.this) {        nCalls++;        if (elapsed < minTime || nCalls == 1) {            minTime = elapsed;        }        if (elapsed > maxTime) {            maxTime = elapsed;        }        sumTime += elapsed;        sumSquaredTime += elapsed * elapsed;    }}
0
public boolean end(long sumMaxUsec)
{    end();    return sumMaxUsec < sumTime;}
0
public static void main(String[] args) throws Throwable
{    Properties mainClasses = loadProperties("driver.classes.props");    if (mainClasses == null) {        mainClasses = loadProperties("driver.classes.default.props");    }    if (mainClasses == null) {        throw new IOException("Can't load any properties file?");    }    boolean foundShortName = false;    ProgramDriver programDriver = new ProgramDriver();    for (Object key : mainClasses.keySet()) {        String keyString = (String) key;        if (args.length > 0 && shortName(mainClasses.getProperty(keyString)).equals(args[0])) {            foundShortName = true;        }        if (args.length > 0 && keyString.equalsIgnoreCase(args[0]) && isDeprecated(mainClasses, keyString)) {                        return;        }        if (isDeprecated(mainClasses, keyString)) {            continue;        }        addClass(programDriver, keyString, mainClasses.getProperty(keyString));    }    if (args.length < 1 || args[0] == null || "-h".equals(args[0]) || "--help".equals(args[0])) {        programDriver.driver(args);        return;    }    String progName = args[0];    if (!foundShortName) {        addClass(programDriver, progName, progName);    }    shift(args);    Properties mainProps = loadProperties(progName + ".props");    if (mainProps == null) {                mainProps = new Properties();    }    Map<String, String[]> argMap = new HashMap<>();    int i = 0;    while (i < args.length && args[i] != null) {        List<String> argValues = new ArrayList<>();        String arg = args[i];        i++;        if (arg.startsWith("-D")) {                        String[] argSplit = arg.split("=");            arg = argSplit[0];            if (argSplit.length == 2) {                argValues.add(argSplit[1]);            }        } else {                        while (i < args.length && args[i] != null) {                if (args[i].startsWith("-")) {                    break;                }                argValues.add(args[i]);                i++;            }        }        argMap.put(arg, argValues.toArray(new String[argValues.size()]));    }        for (String key : mainProps.stringPropertyNames()) {        String[] argNamePair = key.split("\\|");        String shortArg = '-' + argNamePair[0].trim();        String longArg = argNamePair.length < 2 ? null : "--" + argNamePair[1].trim();        if (!argMap.containsKey(shortArg) && (longArg == null || !argMap.containsKey(longArg))) {            argMap.put(longArg, new String[] { mainProps.getProperty(key) });        }    }        List<String> argsList = new ArrayList<>();    argsList.add(progName);    for (Map.Entry<String, String[]> entry : argMap.entrySet()) {        String arg = entry.getKey();        if (arg.startsWith("-D")) {                        String[] argValues = entry.getValue();            if (argValues.length > 0 && !argValues[0].trim().isEmpty()) {                arg += '=' + argValues[0].trim();            }            argsList.add(1, arg);        } else {            argsList.add(arg);            for (String argValue : Arrays.asList(argMap.get(arg))) {                if (!argValue.isEmpty()) {                    argsList.add(argValue);                }            }        }    }    long start = System.currentTimeMillis();    programDriver.driver(argsList.toArray(new String[argsList.size()]));    if (log.isInfoEnabled()) {            }}
1
private static boolean isDeprecated(Properties mainClasses, String keyString)
{    return "deprecated".equalsIgnoreCase(shortName(mainClasses.getProperty(keyString)));}
0
private static Properties loadProperties(String resource) throws IOException
{    InputStream propsStream = Thread.currentThread().getContextClassLoader().getResourceAsStream(resource);    if (propsStream != null) {        try {            Properties properties = new Properties();            properties.load(propsStream);            return properties;        } finally {            Closeables.close(propsStream, true);        }    }    return null;}
0
private static String[] shift(String[] args)
{    System.arraycopy(args, 1, args, 0, args.length - 1);    args[args.length - 1] = null;    return args;}
0
private static String shortName(String valueString)
{    return valueString.contains(":") ? valueString.substring(0, valueString.indexOf(':')).trim() : valueString;}
0
private static String desc(String valueString)
{    return valueString.contains(":") ? valueString.substring(valueString.indexOf(':')).trim() : valueString;}
0
private static void addClass(ProgramDriver driver, String classString, String descString)
{    try {        Class<?> clazz = Class.forName(classString);        driver.addClass(shortName(descString), clazz, desc(descString));    } catch (Throwable t) {            }}
1
private void initializePopulation(int populationSize, State<T, U> seed)
{    population = Lists.newArrayList(seed);    for (int i = 0; i < populationSize; i++) {        population.add(seed.mutate());    }}
0
public void add(State<T, U> value)
{    population.add(value);}
0
public void mutatePopulation(int survivors)
{        Collections.sort(population);        List<State<T, U>> parents = new ArrayList<>(population.subList(0, survivors));    population.subList(survivors, population.size()).clear();        int i = 0;    while (population.size() < populationSize) {        population.add(parents.get(i % survivors).mutate());        i++;    }}
0
public State<T, U> parallelDo(final Function<Payload<U>> fn) throws InterruptedException, ExecutionException
{    Collection<Callable<State<T, U>>> tasks = new ArrayList<>();    for (final State<T, U> state : population) {        tasks.add(new Callable<State<T, U>>() {            @Override            public State<T, U> call() {                double v = fn.apply(state.getPayload(), state.getMappedParams());                state.setValue(v);                return state;            }        });    }    List<Future<State<T, U>>> r = pool.invokeAll(tasks);        double max = Double.NEGATIVE_INFINITY;    State<T, U> best = null;    for (Future<State<T, U>> future : r) {        State<T, U> s = future.get();        double value = s.getValue();        if (!Double.isNaN(value) && value >= max) {            max = value;            best = s;        }    }    if (best == null) {        best = r.get(0).get();    }    return best;}
0
public State<T, U> call()
{    double v = fn.apply(state.getPayload(), state.getMappedParams());    state.setValue(v);    return state;}
0
public void setThreadCount(int threadCount)
{    this.threadCount = threadCount;    pool = Executors.newFixedThreadPool(threadCount);}
0
public int getThreadCount()
{    return threadCount;}
0
public int getPopulationSize()
{    return populationSize;}
0
public List<State<T, U>> getPopulation()
{    return population;}
0
public void close()
{    List<Runnable> remainingTasks = pool.shutdownNow();    try {        pool.awaitTermination(10, TimeUnit.SECONDS);    } catch (InterruptedException e) {        throw new IllegalStateException("Had to forcefully shut down " + remainingTasks.size() + " tasks");    }    if (!remainingTasks.isEmpty()) {        throw new IllegalStateException("Had to forcefully shut down " + remainingTasks.size() + " tasks");    }}
0
public void write(DataOutput out) throws IOException
{    out.writeInt(threadCount);    out.writeInt(population.size());    for (State<T, U> state : population) {        PolymorphicWritable.write(out, state);    }}
0
public void readFields(DataInput input) throws IOException
{    setThreadCount(input.readInt());    int n = input.readInt();    population = new ArrayList<>();    for (int i = 0; i < n; i++) {        State<T, U> state = (State<T, U>) PolymorphicWritable.read(input, State.class);        population.add(state);    }}
0
public double apply(double v)
{    return min + (max - min) * 1 / (1 + Math.exp(-v * scale));}
0
public void write(DataOutput out) throws IOException
{    out.writeDouble(min);    out.writeDouble(max);    out.writeDouble(scale);}
0
public void readFields(DataInput in) throws IOException
{    min = in.readDouble();    max = in.readDouble();    scale = in.readDouble();}
0
public double apply(double v)
{    return Math.exp(wrapped.apply(v));}
0
public void write(DataOutput dataOutput) throws IOException
{    PolymorphicWritable.write(dataOutput, wrapped);}
0
public void readFields(DataInput in) throws IOException
{    wrapped = PolymorphicWritable.read(in, Mapping.class);}
0
public double apply(double v)
{    return Math.exp(v * scale);}
0
public void write(DataOutput out) throws IOException
{    out.writeDouble(scale);}
0
public void readFields(DataInput in) throws IOException
{    scale = in.readDouble();}
0
public double apply(double v)
{    return v;}
0
public void write(DataOutput dataOutput)
{}
0
public void readFields(DataInput dataInput)
{}
0
public static Mapping softLimit(double min, double max, double scale)
{    return new SoftLimit(min, max, scale);}
0
public static Mapping softLimit(double min, double max)
{    return softLimit(min, max, 1);}
0
public static Mapping logLimit(double low, double high)
{    Preconditions.checkArgument(low > 0, "Lower bound for log limit must be > 0 but was %f", low);    Preconditions.checkArgument(high > 0, "Upper bound for log limit must be > 0 but was %f", high);    return new LogLimit(low, high);}
0
public static Mapping exponential()
{    return exponential(1);}
0
public static Mapping exponential(double scale)
{    return new Exponential(scale);}
0
public static Mapping identity()
{    return new Identity();}
0
public State<T, U> copy()
{    State<T, U> r = new State<>();    r.params = Arrays.copyOf(this.params, this.params.length);    r.omni = this.omni;    r.step = Arrays.copyOf(this.step, this.step.length);    r.maps = Arrays.copyOf(this.maps, this.maps.length);    if (this.payload != null) {        r.payload = (T) this.payload.copy();    }    r.gen = this.gen;    return r;}
0
public State<T, U> mutate()
{    double sum = 0;    for (double v : step) {        sum += v * v;    }    sum = Math.sqrt(sum);    double lambda = 1 + gen.nextGaussian();    State<T, U> r = this.copy();    double magnitude = 0.9 * omni + sum / 10;    r.omni = magnitude * -Math.log1p(-gen.nextDouble());    for (int i = 0; i < step.length; i++) {        r.step[i] = lambda * step[i] + r.omni * gen.nextGaussian();        r.params[i] += r.step[i];    }    if (this.payload != null) {        r.payload.update(r.getMappedParams());    }    return r;}
0
public void setMap(int i, Mapping m)
{    maps[i] = m;}
0
public double get(int i)
{    Mapping m = maps[i];    return m == null ? params[i] : m.apply(params[i]);}
0
public int getId()
{    return id;}
0
public double[] getParams()
{    return params;}
0
public Mapping[] getMaps()
{    return maps;}
0
public double[] getMappedParams()
{    double[] r = Arrays.copyOf(params, params.length);    for (int i = 0; i < params.length; i++) {        r[i] = get(i);    }    return r;}
0
public double getOmni()
{    return omni;}
0
public double[] getStep()
{    return step;}
0
public T getPayload()
{    return payload;}
0
public double getValue()
{    return value;}
0
public void setOmni(double omni)
{    this.omni = omni;}
0
public void setId(int id)
{    this.id = id;}
0
public void setStep(double[] step)
{    this.step = step;}
0
public void setMaps(Mapping[] maps)
{    this.maps = maps;}
0
public void setMaps(Iterable<Mapping> maps)
{    Collection<Mapping> list = Lists.newArrayList(maps);    this.maps = list.toArray(new Mapping[list.size()]);}
0
public void setValue(double v)
{    value = v;}
0
public void setPayload(T payload)
{    this.payload = payload;}
0
public boolean equals(Object o)
{    if (!(o instanceof State)) {        return false;    }    State<?, ?> other = (State<?, ?>) o;    return id == other.id && value == other.value;}
0
public int hashCode()
{    return RandomUtils.hashDouble(value) ^ id;}
0
public int compareTo(State<T, U> other)
{    int r = Double.compare(other.value, this.value);    if (r != 0) {        return r;    }    if (this.id < other.id) {        return -1;    }    if (this.id > other.id) {        return 1;    }    return 0;}
0
public String toString()
{    double sum = 0;    for (double v : step) {        sum += v * v;    }    return String.format(Locale.ENGLISH, "<S/%s %.3f %.3f>", payload, omni + Math.sqrt(sum), value);}
0
public void write(DataOutput out) throws IOException
{    out.writeInt(id);    out.writeInt(params.length);    for (double v : params) {        out.writeDouble(v);    }    for (Mapping map : maps) {        PolymorphicWritable.write(out, map);    }    out.writeDouble(omni);    for (double v : step) {        out.writeDouble(v);    }    out.writeDouble(value);    PolymorphicWritable.write(out, payload);}
0
public void readFields(DataInput input) throws IOException
{    id = input.readInt();    int n = input.readInt();    params = new double[n];    for (int i = 0; i < n; i++) {        params[i] = input.readDouble();    }    maps = new Mapping[n];    for (int i = 0; i < n; i++) {        maps[i] = PolymorphicWritable.read(input, Mapping.class);    }    omni = input.readDouble();    step = new double[n];    for (int i = 0; i < n; i++) {        step[i] = input.readDouble();    }    value = input.readDouble();    payload = (T) PolymorphicWritable.read(input, Payload.class);}
0
public static Vector solve(Iterable<Vector> featureVectors, Vector ratingVector, double lambda, int numFeatures)
{    Preconditions.checkNotNull(featureVectors, "Feature Vectors cannot be null");    Preconditions.checkArgument(!Iterables.isEmpty(featureVectors));    Preconditions.checkNotNull(ratingVector, "Rating Vector cannot be null");    Preconditions.checkArgument(ratingVector.getNumNondefaultElements() > 0, "Rating Vector cannot be empty");    Preconditions.checkArgument(Iterables.size(featureVectors) == ratingVector.getNumNondefaultElements());    int nui = ratingVector.getNumNondefaultElements();    Matrix MiIi = createMiIi(featureVectors, numFeatures);    Matrix RiIiMaybeTransposed = createRiIiMaybeTransposed(ratingVector);    /* compute Ai = MiIi * t(MiIi) + lambda * nui * E */    Matrix Ai = miTimesMiTransposePlusLambdaTimesNuiTimesE(MiIi, lambda, nui);    /* compute Vi = MiIi * t(R(i,Ii)) */    Matrix Vi = MiIi.times(RiIiMaybeTransposed);    /* compute Ai * ui = Vi */    return solve(Ai, Vi);}
0
private static Vector solve(Matrix Ai, Matrix Vi)
{    return new QRDecomposition(Ai).solve(Vi).viewColumn(0);}
0
 static Matrix addLambdaTimesNuiTimesE(Matrix matrix, double lambda, int nui)
{    Preconditions.checkArgument(matrix.numCols() == matrix.numRows(), "Must be a Square Matrix");    double lambdaTimesNui = lambda * nui;    int numCols = matrix.numCols();    for (int n = 0; n < numCols; n++) {        matrix.setQuick(n, n, matrix.getQuick(n, n) + lambdaTimesNui);    }    return matrix;}
0
private static Matrix miTimesMiTransposePlusLambdaTimesNuiTimesE(Matrix MiIi, double lambda, int nui)
{    double lambdaTimesNui = lambda * nui;    int rows = MiIi.numRows();    double[][] result = new double[rows][rows];    for (int i = 0; i < rows; i++) {        for (int j = i; j < rows; j++) {            double dot = MiIi.viewRow(i).dot(MiIi.viewRow(j));            if (i != j) {                result[i][j] = dot;                result[j][i] = dot;            } else {                result[i][i] = dot + lambdaTimesNui;            }        }    }    return new DenseMatrix(result, true);}
0
 static Matrix createMiIi(Iterable<Vector> featureVectors, int numFeatures)
{    double[][] MiIi = new double[numFeatures][Iterables.size(featureVectors)];    int n = 0;    for (Vector featureVector : featureVectors) {        for (int m = 0; m < numFeatures; m++) {            MiIi[m][n] = featureVector.getQuick(m);        }        n++;    }    return new DenseMatrix(MiIi, true);}
0
 static Matrix createRiIiMaybeTransposed(Vector ratingVector)
{    Preconditions.checkArgument(ratingVector.isSequentialAccess(), "Ratings should be iterable in Index or Sequential Order");    double[][] RiIiMaybeTransposed = new double[ratingVector.getNumNondefaultElements()][1];    int index = 0;    for (Vector.Element elem : ratingVector.nonZeroes()) {        RiIiMaybeTransposed[index++][0] = elem.get();    }    return new DenseMatrix(RiIiMaybeTransposed, true);}
0
public Vector solve(Vector ratings)
{    return solve(YtransposeY.plus(getYtransponseCuMinusIYPlusLambdaI(ratings)), getYtransponseCuPu(ratings));}
0
private static Vector solve(Matrix A, Matrix y)
{    return new QRDecomposition(A).solve(y).viewColumn(0);}
0
 double confidence(double rating)
{    return 1 + alpha * rating;}
0
public Matrix getYtransposeY(final OpenIntObjectHashMap<Vector> Y)
{    ExecutorService queue = Executors.newFixedThreadPool(numTrainingThreads);    if (log.isInfoEnabled()) {            }    long startTime = System.nanoTime();    final IntArrayList indexes = Y.keys();    final int numIndexes = indexes.size();    final double[][] YtY = new double[numFeatures][numFeatures];        for (int i = 0; i < numFeatures; i++) {        for (int j = i; j < numFeatures; j++) {            final int ii = i;            final int jj = j;            queue.execute(new Runnable() {                @Override                public void run() {                    double dot = 0;                    for (int k = 0; k < numIndexes; k++) {                        Vector row = Y.get(indexes.getQuick(k));                        dot += row.getQuick(ii) * row.getQuick(jj);                    }                    YtY[ii][jj] = dot;                    if (ii != jj) {                        YtY[jj][ii] = dot;                    }                }            });        }    }    queue.shutdown();    try {        queue.awaitTermination(1, TimeUnit.DAYS);    } catch (InterruptedException e) {                throw new RuntimeException("Error during Y'Y queue shutdown");    }    if (log.isInfoEnabled()) {            }    return new DenseMatrix(YtY, true);}
1
public void run()
{    double dot = 0;    for (int k = 0; k < numIndexes; k++) {        Vector row = Y.get(indexes.getQuick(k));        dot += row.getQuick(ii) * row.getQuick(jj);    }    YtY[ii][jj] = dot;    if (ii != jj) {        YtY[jj][ii] = dot;    }}
0
private Matrix getYtransponseCuMinusIYPlusLambdaI(Vector userRatings)
{    Preconditions.checkArgument(userRatings.isSequentialAccess(), "need sequential access to ratings!");    /* (Cu -I) Y */    OpenIntObjectHashMap<Vector> CuMinusIY = new OpenIntObjectHashMap<>(userRatings.getNumNondefaultElements());    for (Element e : userRatings.nonZeroes()) {        CuMinusIY.put(e.index(), Y.get(e.index()).times(confidence(e.get()) - 1));    }    Matrix YtransponseCuMinusIY = new DenseMatrix(numFeatures, numFeatures);    /* Y' (Cu -I) Y by outer products */    for (Element e : userRatings.nonZeroes()) {        for (Element feature : Y.get(e.index()).all()) {            Vector partial = CuMinusIY.get(e.index()).times(feature.get());            YtransponseCuMinusIY.viewRow(feature.index()).assign(partial, Functions.PLUS);        }    }    /* Y' (Cu - I) Y +  I  add lambda on the diagonal */    for (int feature = 0; feature < numFeatures; feature++) {        YtransponseCuMinusIY.setQuick(feature, feature, YtransponseCuMinusIY.getQuick(feature, feature) + lambda);    }    return YtransponseCuMinusIY;}
0
private Matrix getYtransponseCuPu(Vector userRatings)
{    Preconditions.checkArgument(userRatings.isSequentialAccess(), "need sequential access to ratings!");    Vector YtransponseCuPu = new DenseVector(numFeatures);    for (Element e : userRatings.nonZeroes()) {        YtransponseCuPu.assign(Y.get(e.index()).times(confidence(e.get())), Functions.PLUS);    }    return columnVectorAsMatrix(YtransponseCuPu);}
0
private Matrix columnVectorAsMatrix(Vector v)
{    double[][] matrix = new double[numFeatures][1];    for (Element e : v.all()) {        matrix[e.index()][0] = e.get();    }    return new DenseMatrix(matrix, true);}
0
public synchronized EigenStatus verify(VectorIterable corpus, Vector vector)
{    if (!finished && !started) {                status = new EigenStatus(-1, 0);        Vector vectorCopy = vector.clone();        threadPool.execute(new VerifierRunnable(corpus, vectorCopy));        started = true;    }    if (finished) {        finished = false;    }    return status;}
0
public void close()
{    this.threadPool.shutdownNow();}
0
protected EigenStatus innerVerify(VectorIterable corpus, Vector vector)
{    return super.verify(corpus, vector);}
0
public void run()
{    EigenStatus status = innerVerify(corpus, vector);    synchronized (AsyncEigenVerifier.this) {        AsyncEigenVerifier.this.status = status;        finished = true;        started = false;    }}
0
public double getCosAngle()
{    return cosAngle;}
0
public double getEigenValue()
{    return eigenValue;}
0
public boolean inProgress()
{    return inProgress;}
0
 void setInProgress(boolean status)
{    inProgress = status;}
0
public TrainingState solve(Matrix corpus, int desiredRank)
{    int cols = corpus.numCols();    Matrix eigens = new DenseMatrix(desiredRank, cols);    List<Double> eigenValues = new ArrayList<>();        /*     * The corpusProjections matrix is a running cache of the residual projection of each corpus vector against all     * of the previously found singular vectors.  Without this, if multiple passes over the data is made (per     * singular vector), recalculating these projections eventually dominates the computational complexity of the     * solver.     */    Matrix corpusProjections = new DenseMatrix(corpus.numRows(), desiredRank);    TrainingState state = new TrainingState(eigens, corpusProjections);    for (int i = 0; i < desiredRank; i++) {        Vector currentEigen = new DenseVector(cols);        Vector previousEigen = null;        while (hasNotConverged(currentEigen, corpus, state)) {            int randomStartingIndex = getRandomStartingIndex(corpus, eigens);            Vector initialTrainingVector = corpus.viewRow(randomStartingIndex);            state.setTrainingIndex(randomStartingIndex);            updater.update(currentEigen, initialTrainingVector, state);            for (int corpusRow = 0; corpusRow < corpus.numRows(); corpusRow++) {                state.setTrainingIndex(corpusRow);                if (corpusRow != randomStartingIndex) {                    updater.update(currentEigen, corpus.viewRow(corpusRow), state);                }            }            state.setFirstPass(false);            if (DEBUG) {                if (previousEigen == null) {                    previousEigen = currentEigen.clone();                } else {                    double dot = currentEigen.dot(previousEigen);                    if (dot > 0.0) {                        dot /= currentEigen.norm(2) * previousEigen.norm(2);                    }                                }            }        }                double eigenValue = state.getStatusProgress().get(state.getStatusProgress().size() - 1).getEigenValue();                        currentEigen.assign(new TimesFunction(), 1 / currentEigen.norm(2));        eigens.assignRow(i, currentEigen);        eigenValues.add(eigenValue);        state.setCurrentEigenValues(eigenValues);                /**         *  TODO: Persist intermediate output!         */        state.setFirstPass(true);        state.setNumEigensProcessed(state.getNumEigensProcessed() + 1);        state.setActivationDenominatorSquared(0);        state.setActivationNumerator(0);        state.getStatusProgress().clear();        numPasses = 0;    }    return state;}
1
private int getRandomStartingIndex(Matrix corpus, Matrix eigens)
{    int index;    Vector v;    do {        double r = rng.nextDouble();        index = (int) (r * corpus.numRows());        v = corpus.viewRow(index);    } while (v == null || v.norm(2) == 0 || v.getNumNondefaultElements() < 5);    return index;}
0
protected boolean hasNotConverged(Vector currentPseudoEigen, Matrix corpus, TrainingState state)
{    numPasses++;    if (state.isFirstPass()) {                return true;    }    Matrix previousEigens = state.getCurrentEigens();        /*     * Step 1: orthogonalize currentPseudoEigen by subtracting off eigen(i) * helper.get(i)     * Step 2: zero-out the helper vector because it has already helped.     */    for (int i = 0; i < state.getNumEigensProcessed(); i++) {        Vector previousEigen = previousEigens.viewRow(i);        currentPseudoEigen.assign(previousEigen, new PlusMult(-state.getHelperVector().get(i)));        state.getHelperVector().set(i, 0);    }    if (currentPseudoEigen.norm(2) > 0) {        for (int i = 0; i < state.getNumEigensProcessed(); i++) {            Vector previousEigen = previousEigens.viewRow(i);                    }    }    /*     * Step 3: verify how eigen-like the prospective eigen is.  This is potentially asynchronous.     */    EigenStatus status = verify(corpus, currentPseudoEigen);    if (status.inProgress()) {            } else {                state.getStatusProgress().add(status);    }    return state.getStatusProgress().size() <= maxPassesPerEigen && 1.0 - status.getCosAngle() > convergenceTarget;}
1
protected EigenStatus verify(Matrix corpus, Vector currentPseudoEigen)
{    return verifier.verify(corpus, currentPseudoEigen);}
0
public static void main(String[] args)
{    Properties props = new Properties();    String propertiesFile = args.length > 0 ? args[0] : "config/solver.properties";        String corpusDir = props.getProperty("solver.input.dir");    String outputDir = props.getProperty("solver.output.dir");    if (corpusDir == null || corpusDir.isEmpty() || outputDir == null || outputDir.isEmpty()) {                return;    }        int rank = Integer.parseInt(props.getProperty("solver.output.desiredRank"));    double convergence = Double.parseDouble(props.getProperty("solver.convergence"));    int maxPasses = Integer.parseInt(props.getProperty("solver.maxPasses"));        HebbianUpdater updater = new HebbianUpdater();    SingularVectorVerifier verifier = new AsyncEigenVerifier();    HebbianSolver solver = new HebbianSolver(updater, verifier, convergence, maxPasses);    Matrix corpus = null;    /*    if (numThreads <= 1) {          } else {          }     */    long now = System.currentTimeMillis();    TrainingState finalState = solver.solve(corpus, rank);    long time = (System.currentTimeMillis() - now) / 1000;    }
1
public void update(Vector pseudoEigen, Vector trainingVector, TrainingState currentState)
{    double trainingVectorNorm = trainingVector.norm(2);    int numPreviousEigens = currentState.getNumEigensProcessed();    if (numPreviousEigens > 0 && currentState.isFirstPass()) {        updateTrainingProjectionsVector(currentState, trainingVector, numPreviousEigens - 1);    }    if (currentState.getActivationDenominatorSquared() == 0 || trainingVectorNorm == 0) {        if (currentState.getActivationDenominatorSquared() == 0) {            pseudoEigen.assign(trainingVector, new PlusMult(1));            currentState.setHelperVector(currentState.currentTrainingProjection().clone());            double helperNorm = currentState.getHelperVector().norm(2);            currentState.setActivationDenominatorSquared(trainingVectorNorm * trainingVectorNorm - helperNorm * helperNorm);        }        return;    }    currentState.setActivationNumerator(pseudoEigen.dot(trainingVector));    currentState.setActivationNumerator(currentState.getActivationNumerator() - currentState.getHelperVector().dot(currentState.currentTrainingProjection()));    double activation = currentState.getActivationNumerator() / Math.sqrt(currentState.getActivationDenominatorSquared());    currentState.setActivationDenominatorSquared(currentState.getActivationDenominatorSquared() + 2 * activation * currentState.getActivationNumerator() + activation * activation * (trainingVector.getLengthSquared() - currentState.currentTrainingProjection().getLengthSquared()));    if (numPreviousEigens > 0) {        currentState.getHelperVector().assign(currentState.currentTrainingProjection(), new PlusMult(activation));    }    pseudoEigen.assign(trainingVector, new PlusMult(activation));}
0
private static void updateTrainingProjectionsVector(TrainingState state, Vector trainingVector, int previousEigenIndex)
{    Vector previousEigen = state.mostRecentEigen();    Vector currentTrainingVectorProjection = state.currentTrainingProjection();    double projection = previousEigen.dot(trainingVector);    currentTrainingVectorProjection.set(previousEigenIndex, projection);}
0
public Vector mostRecentEigen()
{    return currentEigens.viewRow(numEigensProcessed - 1);}
0
public Vector currentTrainingProjection()
{    if (trainingProjections.viewRow(trainingIndex) == null) {        trainingProjections.assignRow(trainingIndex, new DenseVector(currentEigens.numCols()));    }    return trainingProjections.viewRow(trainingIndex);}
0
public Matrix getCurrentEigens()
{    return currentEigens;}
0
public void setCurrentEigens(Matrix currentEigens)
{    this.currentEigens = currentEigens;}
0
public int getNumEigensProcessed()
{    return numEigensProcessed;}
0
public void setNumEigensProcessed(int numEigensProcessed)
{    this.numEigensProcessed = numEigensProcessed;}
0
public List<Double> getCurrentEigenValues()
{    return currentEigenValues;}
0
public void setCurrentEigenValues(List<Double> currentEigenValues)
{    this.currentEigenValues = currentEigenValues;}
0
public Matrix getTrainingProjections()
{    return trainingProjections;}
0
public void setTrainingProjections(Matrix trainingProjections)
{    this.trainingProjections = trainingProjections;}
0
public int getTrainingIndex()
{    return trainingIndex;}
0
public void setTrainingIndex(int trainingIndex)
{    this.trainingIndex = trainingIndex;}
0
public Vector getHelperVector()
{    return helperVector;}
0
public void setHelperVector(Vector helperVector)
{    this.helperVector = helperVector;}
0
public boolean isFirstPass()
{    return firstPass;}
0
public void setFirstPass(boolean firstPass)
{    this.firstPass = firstPass;}
0
public List<EigenStatus> getStatusProgress()
{    return statusProgress;}
0
public void setStatusProgress(List<EigenStatus> statusProgress)
{    this.statusProgress = statusProgress;}
0
public double getActivationNumerator()
{    return activationNumerator;}
0
public void setActivationNumerator(double activationNumerator)
{    this.activationNumerator = activationNumerator;}
0
public double getActivationDenominatorSquared()
{    return activationDenominatorSquared;}
0
public void setActivationDenominatorSquared(double activationDenominatorSquared)
{    this.activationDenominatorSquared = activationDenominatorSquared;}
0
public double apply(double arg1)
{    return arg1 * d;}
0
public void solve(LanczosState state, int desiredRank)
{    solve(state, desiredRank, false);}
0
public void solve(LanczosState state, int desiredRank, boolean isSymmetric)
{    VectorIterable corpus = state.getCorpus();        int i = state.getIterationNumber();    Vector currentVector = state.getBasisVector(i - 1);    Vector previousVector = state.getBasisVector(i - 2);    double beta = 0;    Matrix triDiag = state.getDiagonalMatrix();    while (i < desiredRank) {        startTime(TimingSection.ITERATE);        Vector nextVector = isSymmetric ? corpus.times(currentVector) : corpus.timesSquared(currentVector);                if (state.getScaleFactor() <= 0) {            state.setScaleFactor(calculateScaleFactor(nextVector));        }        nextVector.assign(new Scale(1.0 / state.getScaleFactor()));        if (previousVector != null) {            nextVector.assign(previousVector, new PlusMult(-beta));        }                double alpha = currentVector.dot(nextVector);        nextVector.assign(currentVector, new PlusMult(-alpha));        endTime(TimingSection.ITERATE);        startTime(TimingSection.ORTHOGANLIZE);        orthoganalizeAgainstAllButLast(nextVector, state);        endTime(TimingSection.ORTHOGANLIZE);                beta = nextVector.norm(2);        if (outOfRange(beta) || outOfRange(alpha)) {                        break;        }        nextVector.assign(new Scale(1 / beta));        state.setBasisVector(i, nextVector);        previousVector = currentVector;        currentVector = nextVector;                triDiag.set(i - 1, i - 1, alpha);        if (i < desiredRank - 1) {            triDiag.set(i - 1, i, beta);            triDiag.set(i, i - 1, beta);        }        state.setIterationNumber(++i);    }    startTime(TimingSection.TRIDIAG_DECOMP);            EigenDecomposition decomp = new EigenDecomposition(triDiag);    Matrix eigenVects = decomp.getV();    Vector eigenVals = decomp.getRealEigenvalues();    endTime(TimingSection.TRIDIAG_DECOMP);    startTime(TimingSection.FINAL_EIGEN_CREATE);    for (int row = 0; row < i; row++) {        Vector realEigen = null;        Vector ejCol = eigenVects.viewColumn(row);        int size = Math.min(ejCol.size(), state.getBasisSize());        for (int j = 0; j < size; j++) {            double d = ejCol.get(j);            Vector rowJ = state.getBasisVector(j);            if (realEigen == null) {                realEigen = rowJ.like();            }            realEigen.assign(rowJ, new PlusMult(d));        }        Preconditions.checkState(realEigen != null);        assert realEigen != null;        realEigen = realEigen.normalize();        state.setRightSingularVector(row, realEigen);        double e = eigenVals.get(row) * state.getScaleFactor();        if (!isSymmetric) {            e = Math.sqrt(e);        }                state.setSingularValue(row, e);    }        endTime(TimingSection.FINAL_EIGEN_CREATE);}
1
protected static double calculateScaleFactor(Vector nextVector)
{    return nextVector.norm(2);}
0
private static boolean outOfRange(double d)
{    return Double.isNaN(d) || d > SAFE_MAX || -d > SAFE_MAX;}
0
protected static void orthoganalizeAgainstAllButLast(Vector nextVector, LanczosState state)
{    for (int i = 0; i < state.getIterationNumber(); i++) {        Vector basisVector = state.getBasisVector(i);        double alpha;        if (basisVector == null || (alpha = nextVector.dot(basisVector)) == 0.0) {            continue;        }        nextVector.assign(basisVector, new PlusMult(-alpha));    }}
0
private void startTime(TimingSection section)
{    startTimes.put(section, System.nanoTime());}
0
private void endTime(TimingSection section)
{    if (!times.containsKey(section)) {        times.put(section, 0L);    }    times.put(section, times.get(section) + System.nanoTime() - startTimes.get(section));}
0
private void intitializeBasisAndSingularVectors()
{    basis = Maps.newHashMap();    singularVectors = Maps.newHashMap();}
0
public Matrix getDiagonalMatrix()
{    return diagonalMatrix;}
0
public int getIterationNumber()
{    return iterationNumber;}
0
public double getScaleFactor()
{    return scaleFactor;}
0
public VectorIterable getCorpus()
{    return corpus;}
0
public Vector getRightSingularVector(int i)
{    return singularVectors.get(i);}
0
public Double getSingularValue(int i)
{    return singularValues.get(i);}
0
public Vector getBasisVector(int i)
{    return basis.get(i);}
0
public int getBasisSize()
{    return basis.size();}
0
public void setBasisVector(int i, Vector basisVector)
{    basis.put(i, basisVector);}
0
public void setScaleFactor(double scale)
{    scaleFactor = scale;}
0
public void setIterationNumber(int i)
{    iterationNumber = i;}
0
public void setRightSingularVector(int i, Vector vector)
{    singularVectors.put(i, vector);}
0
public void setSingularValue(int i, double value)
{    singularValues.put(i, value);}
0
public EigenStatus verify(VectorIterable corpus, Vector vector)
{    Vector resultantVector = corpus.timesSquared(vector);    double newNorm = resultantVector.norm(2);    double oldNorm = vector.norm(2);    double eigenValue;    double cosAngle;    if (newNorm > 0 && oldNorm > 0) {        eigenValue = newNorm / oldNorm;        cosAngle = resultantVector.dot(vector) / newNorm * oldNorm;    } else {        eigenValue = 1.0;        cosAngle = 0.0;    }    return new EigenStatus(eigenValue, cosAngle, false);}
0
public static void write(Path outputDir, Configuration conf, Iterable<MatrixSlice> matrix) throws IOException
{    FileSystem fs = outputDir.getFileSystem(conf);    SequenceFile.Writer writer = SequenceFile.createWriter(fs, conf, outputDir, IntWritable.class, VectorWritable.class);    IntWritable topic = new IntWritable();    VectorWritable vector = new VectorWritable();    for (MatrixSlice slice : matrix) {        topic.set(slice.index());        vector.set(slice.vector());        writer.append(topic, vector);    }    writer.close();}
0
public static Vector getInitialVector(VectorIterable corpus)
{    Vector initialVector = new DenseVector(corpus.numCols());    initialVector.assign(1.0 / Math.sqrt(corpus.numCols()));    return initialVector;}
0
public LanczosState runJob(Configuration originalConfig, LanczosState state, int desiredRank, boolean isSymmetric, String outputEigenVectorPathString) throws IOException
{    ((Configurable) state.getCorpus()).setConf(new Configuration(originalConfig));    setConf(originalConfig);    solve(state, desiredRank, isSymmetric);    serializeOutput(state, new Path(outputEigenVectorPathString));    return state;}
0
public LanczosState runJob(Configuration originalConfig, Path inputPath, Path outputTmpPath, int numRows, int numCols, boolean isSymmetric, int desiredRank, String outputEigenVectorPathString) throws IOException
{    DistributedRowMatrix matrix = new DistributedRowMatrix(inputPath, outputTmpPath, numRows, numCols);    matrix.setConf(new Configuration(originalConfig));    LanczosState state = new LanczosState(matrix, desiredRank, getInitialVector(matrix));    return runJob(originalConfig, state, desiredRank, isSymmetric, outputEigenVectorPathString);}
0
public int run(String[] strings) throws Exception
{    Path inputPath = new Path(AbstractJob.getOption(parsedArgs, "--input"));    Path outputPath = new Path(AbstractJob.getOption(parsedArgs, "--output"));    Path outputTmpPath = new Path(AbstractJob.getOption(parsedArgs, "--tempDir"));    Path workingDirPath = AbstractJob.getOption(parsedArgs, "--workingDir") != null ? new Path(AbstractJob.getOption(parsedArgs, "--workingDir")) : null;    int numRows = Integer.parseInt(AbstractJob.getOption(parsedArgs, "--numRows"));    int numCols = Integer.parseInt(AbstractJob.getOption(parsedArgs, "--numCols"));    boolean isSymmetric = Boolean.parseBoolean(AbstractJob.getOption(parsedArgs, "--symmetric"));    int desiredRank = Integer.parseInt(AbstractJob.getOption(parsedArgs, "--rank"));    boolean cleansvd = Boolean.parseBoolean(AbstractJob.getOption(parsedArgs, "--cleansvd"));    if (cleansvd) {        double maxError = Double.parseDouble(AbstractJob.getOption(parsedArgs, "--maxError"));        double minEigenvalue = Double.parseDouble(AbstractJob.getOption(parsedArgs, "--minEigenvalue"));        boolean inMemory = Boolean.parseBoolean(AbstractJob.getOption(parsedArgs, "--inMemory"));        return run(inputPath, outputPath, outputTmpPath, workingDirPath, numRows, numCols, isSymmetric, desiredRank, maxError, minEigenvalue, inMemory);    }    return run(inputPath, outputPath, outputTmpPath, workingDirPath, numRows, numCols, isSymmetric, desiredRank);}
0
public int run(Path inputPath, Path outputPath, Path outputTmpPath, Path workingDirPath, int numRows, int numCols, boolean isSymmetric, int desiredRank, double maxError, double minEigenvalue, boolean inMemory) throws Exception
{    int result = run(inputPath, outputPath, outputTmpPath, workingDirPath, numRows, numCols, isSymmetric, desiredRank);    if (result != 0) {        return result;    }    Path rawEigenVectorPath = new Path(outputPath, RAW_EIGENVECTORS);    return new EigenVerificationJob().run(inputPath, rawEigenVectorPath, outputPath, outputTmpPath, maxError, minEigenvalue, inMemory, getConf() != null ? new Configuration(getConf()) : new Configuration());}
0
public int run(Path inputPath, Path outputPath, Path outputTmpPath, Path workingDirPath, int numRows, int numCols, boolean isSymmetric, int desiredRank) throws Exception
{    DistributedRowMatrix matrix = new DistributedRowMatrix(inputPath, outputTmpPath, numRows, numCols);    matrix.setConf(new Configuration(getConf() != null ? getConf() : new Configuration()));    LanczosState state;    if (workingDirPath == null) {        state = new LanczosState(matrix, desiredRank, getInitialVector(matrix));    } else {        HdfsBackedLanczosState hState = new HdfsBackedLanczosState(matrix, desiredRank, getInitialVector(matrix), workingDirPath);        hState.setConf(matrix.getConf());        state = hState;    }    solve(state, desiredRank, isSymmetric);    Path outputEigenVectorPath = new Path(outputPath, RAW_EIGENVECTORS);    serializeOutput(state, outputEigenVectorPath);    return 0;}
0
public void serializeOutput(LanczosState state, Path outputPath) throws IOException
{    int numEigenVectors = state.getIterationNumber();        Configuration conf = getConf() != null ? getConf() : new Configuration();    FileSystem fs = FileSystem.get(outputPath.toUri(), conf);    SequenceFile.Writer seqWriter = new SequenceFile.Writer(fs, conf, outputPath, IntWritable.class, VectorWritable.class);    try {        IntWritable iw = new IntWritable();        for (int i = 0; i < numEigenVectors; i++) {                        NamedVector v = new NamedVector(state.getRightSingularVector(numEigenVectors - 1 - i), "eigenVector" + i + ", eigenvalue = " + state.getSingularValue(numEigenVectors - 1 - i));            Writable vw = new VectorWritable(v);            iw.set(i);            seqWriter.append(iw, vw);        }    } finally {        Closeables.close(seqWriter, false);    }}
1
public void setConf(Configuration configuration)
{    conf = configuration;}
0
public Configuration getConf()
{    return conf;}
0
public DistributedLanczosSolverJob job()
{    return new DistributedLanczosSolverJob();}
0
public void setConf(Configuration conf)
{    DistributedLanczosSolver.this.setConf(conf);}
0
public Configuration getConf()
{    return DistributedLanczosSolver.this.getConf();}
0
public int run(String[] args) throws Exception
{    addInputOption();    addOutputOption();    addOption("numRows", "nr", "Number of rows of the input matrix");    addOption("numCols", "nc", "Number of columns of the input matrix");    addOption("rank", "r", "Desired decomposition rank (note: only roughly 1/4 to 1/3 " + "of these will have the top portion of the spectrum)");    addOption("symmetric", "sym", "Is the input matrix square and symmetric?");    addOption("workingDir", "wd", "Working directory path to store Lanczos basis vectors " + "(to be used on restarts, and to avoid too much RAM usage)");        addOption("cleansvd", "cl", "Run the EigenVerificationJob to clean the eigenvectors after SVD", false);    addOption("maxError", "err", "Maximum acceptable error", "0.05");    addOption("minEigenvalue", "mev", "Minimum eigenvalue to keep the vector for", "0.0");    addOption("inMemory", "mem", "Buffer eigen matrix into memory (if you have enough!)", "false");    DistributedLanczosSolver.this.parsedArgs = parseArguments(args);    if (DistributedLanczosSolver.this.parsedArgs == null) {        return -1;    } else {        return DistributedLanczosSolver.this.run(args);    }}
0
public static void main(String[] args) throws Exception
{    ToolRunner.run(new DistributedLanczosSolver().job(), args);}
0
public double getEigenValue()
{    return getEigenValue(getName());}
0
public double getCosAngleError()
{    return getCosAngleError(getName());}
0
public int getIndex()
{    return getIndex(getName());}
0
public static double getEigenValue(CharSequence name)
{    return parseMetaData(name)[1];}
0
public static double getCosAngleError(CharSequence name)
{    return parseMetaData(name)[2];}
0
public static int getIndex(CharSequence name)
{    return (int) parseMetaData(name)[0];}
0
public static double[] parseMetaData(CharSequence name)
{    double[] m = new double[3];    String[] s = EQUAL_PATTERN.split(name);    m[0] = Double.parseDouble(PIPE_PATTERN.split(s[0])[1]);    m[1] = Double.parseDouble(PIPE_PATTERN.split(s[1])[1]);    m[2] = Double.parseDouble(s[2].substring(1));    return m;}
0
protected double[] parseMetaData()
{    return parseMetaData(getName());}
0
public void setEigensToVerify(VectorIterable eigens)
{    eigensToVerify = eigens;}
0
public int run(String[] args) throws Exception
{    Map<String, List<String>> argMap = handleArgs(args);    if (argMap == null) {        return -1;    }    if (argMap.isEmpty()) {        return 0;    }        runJob(getConf(), new Path(getOption("eigenInput")), new Path(getOption("corpusInput")), getOutputPath(), getOption("inMemory") != null, Double.parseDouble(getOption("maxError")),     Integer.parseInt(getOption("maxEigens")));    return 0;}
0
public int run(Path corpusInput, Path eigenInput, Path output, Path tempOut, double maxError, double minEigenValue, boolean inMemory, Configuration conf) throws IOException
{    this.outPath = output;    this.tmpOut = tempOut;    this.maxError = maxError;    this.minEigenValue = minEigenValue;    if (eigenInput != null && eigensToVerify == null) {        prepareEigens(conf, eigenInput, inMemory);    }    DistributedRowMatrix c = new DistributedRowMatrix(corpusInput, tempOut, 1, 1);    c.setConf(conf);    corpus = c;        eigenVerifier = new SimpleEigenVerifier();            Map<MatrixSlice, EigenStatus> eigenMetaData = verifyEigens();    List<Map.Entry<MatrixSlice, EigenStatus>> prunedEigenMeta = pruneEigens(eigenMetaData);    saveCleanEigens(new Configuration(), prunedEigenMeta);    return 0;}
0
private Map<String, List<String>> handleArgs(String[] args) throws IOException
{    addOutputOption();    addOption("eigenInput", "ei", "The Path for purported eigenVector input files (SequenceFile<WritableComparable,VectorWritable>.", null);    addOption("corpusInput", "ci", "The Path for corpus input files (SequenceFile<WritableComparable,VectorWritable>.");    addOption(DefaultOptionCreator.outputOption().create());    addOption(DefaultOptionCreator.helpOption());    addOption("inMemory", "mem", "Buffer eigen matrix into memory (if you have enough!)", "false");    addOption("maxError", "err", "Maximum acceptable error", "0.05");    addOption("minEigenvalue", "mev", "Minimum eigenvalue to keep the vector for", "0.0");    addOption("maxEigens", "max", "Maximum number of eigenvectors to keep (0 means all)", "0");    return parseArguments(args);}
0
private void saveCleanEigens(Configuration conf, Collection<Map.Entry<MatrixSlice, EigenStatus>> prunedEigenMeta) throws IOException
{    Path path = new Path(outPath, CLEAN_EIGENVECTORS);    FileSystem fs = FileSystem.get(path.toUri(), conf);    SequenceFile.Writer seqWriter = new SequenceFile.Writer(fs, conf, path, IntWritable.class, VectorWritable.class);    try {        IntWritable iw = new IntWritable();        int numEigensWritten = 0;        int index = 0;        for (Map.Entry<MatrixSlice, EigenStatus> pruneSlice : prunedEigenMeta) {            MatrixSlice s = pruneSlice.getKey();            EigenStatus meta = pruneSlice.getValue();            EigenVector ev = new EigenVector(s.vector(), meta.getEigenValue(), Math.abs(1 - meta.getCosAngle()), s.index());                        Writable vw = new VectorWritable(ev);            iw.set(index++);            seqWriter.append(iw, vw);                                                numEigensWritten++;            if (numEigensWritten == maxEigensToKeep) {                                break;            }        }    } finally {        Closeables.close(seqWriter, false);    }    cleanedEigensPath = path;}
1
private List<Map.Entry<MatrixSlice, EigenStatus>> pruneEigens(Map<MatrixSlice, EigenStatus> eigenMetaData)
{    List<Map.Entry<MatrixSlice, EigenStatus>> prunedEigenMeta = Lists.newArrayList();    for (Map.Entry<MatrixSlice, EigenStatus> entry : eigenMetaData.entrySet()) {        if (Math.abs(1 - entry.getValue().getCosAngle()) < maxError && entry.getValue().getEigenValue() > minEigenValue) {            prunedEigenMeta.add(entry);        }    }    Collections.sort(prunedEigenMeta, new Comparator<Map.Entry<MatrixSlice, EigenStatus>>() {        @Override        public int compare(Map.Entry<MatrixSlice, EigenStatus> e1, Map.Entry<MatrixSlice, EigenStatus> e2) {                        Double eg1 = e1.getValue().getEigenValue();            Double eg2 = e2.getValue().getEigenValue();            return eg1.compareTo(eg2);        }    });        List<Map.Entry<MatrixSlice, EigenStatus>> selectedEigenMeta = Lists.newArrayList();    Map.Entry<MatrixSlice, EigenStatus> e1 = prunedEigenMeta.remove(0);    selectedEigenMeta.add(e1);    int selectedEigenMetaLength = selectedEigenMeta.size();    int prunedEigenMetaLength = prunedEigenMeta.size();    while (prunedEigenMetaLength > 0) {        double sum = Double.MAX_VALUE;        int index = 0;        for (int i = 0; i < prunedEigenMetaLength; i++) {            Map.Entry<MatrixSlice, EigenStatus> e = prunedEigenMeta.get(i);            double tmp = 0;            for (int j = 0; j < selectedEigenMetaLength; j++) {                Map.Entry<MatrixSlice, EigenStatus> ee = selectedEigenMeta.get(j);                tmp += ee.getKey().vector().times(e.getKey().vector()).norm(2);            }            if (tmp < sum) {                sum = tmp;                index = i;            }        }        Map.Entry<MatrixSlice, EigenStatus> e = prunedEigenMeta.remove(index);        selectedEigenMeta.add(e);        selectedEigenMetaLength++;        prunedEigenMetaLength--;    }    return selectedEigenMeta;}
0
public int compare(Map.Entry<MatrixSlice, EigenStatus> e1, Map.Entry<MatrixSlice, EigenStatus> e2)
{        Double eg1 = e1.getValue().getEigenValue();    Double eg2 = e2.getValue().getEigenValue();    return eg1.compareTo(eg2);}
0
private Map<MatrixSlice, EigenStatus> verifyEigens()
{    Map<MatrixSlice, EigenStatus> eigenMetaData = Maps.newHashMap();    for (MatrixSlice slice : eigensToVerify) {        EigenStatus status = eigenVerifier.verify(corpus, slice.vector());        eigenMetaData.put(slice, status);    }    return eigenMetaData;}
0
private void prepareEigens(Configuration conf, Path eigenInput, boolean inMemory)
{    DistributedRowMatrix eigens = new DistributedRowMatrix(eigenInput, tmpOut, 1, 1);    eigens.setConf(conf);    if (inMemory) {        List<Vector> eigenVectors = Lists.newArrayList();        for (MatrixSlice slice : eigens) {            eigenVectors.add(slice.vector());        }        eigensToVerify = new SparseRowMatrix(eigenVectors.size(), eigenVectors.get(0).size(), eigenVectors.toArray(new Vector[eigenVectors.size()]), true, true);    } else {        eigensToVerify = eigens;    }}
0
public Path getCleanedEigensPath()
{    return cleanedEigensPath;}
0
public static void main(String[] args) throws Exception
{    ToolRunner.run(new EigenVerificationJob(), args);}
0
public void runJob(Configuration conf, Path eigenInput, Path corpusInput, Path output, boolean inMemory, double maxError, int maxEigens) throws IOException
{        outPath = output;    tmpOut = new Path(outPath, "tmp");    maxEigensToKeep = maxEigens;    this.maxError = maxError;    if (eigenInput != null && eigensToVerify == null) {        prepareEigens(new Configuration(conf), eigenInput, inMemory);    }    DistributedRowMatrix c = new DistributedRowMatrix(corpusInput, tmpOut, 1, 1);    c.setConf(new Configuration(conf));    corpus = c;    eigenVerifier = new SimpleEigenVerifier();    Map<MatrixSlice, EigenStatus> eigenMetaData = verifyEigens();    List<Map.Entry<MatrixSlice, EigenStatus>> prunedEigenMeta = pruneEigens(eigenMetaData);    saveCleanEigens(conf, prunedEigenMeta);}
0
public void setConf(Configuration configuration)
{    conf = configuration;    try {        setupDirs();        updateHdfsState();    } catch (IOException e) {            }}
1
public Configuration getConf()
{    return conf;}
0
private void setupDirs() throws IOException
{    fs = baseDir.getFileSystem(conf);    createDirIfNotExist(baseDir);    createDirIfNotExist(basisPath);    createDirIfNotExist(singularVectorPath);}
0
private void createDirIfNotExist(Path path) throws IOException
{    if (!fs.exists(path) && !fs.mkdirs(path)) {        throw new IOException("Unable to create: " + path);    }}
0
public void setIterationNumber(int i)
{    super.setIterationNumber(i);    try {        updateHdfsState();    } catch (IOException e) {            }}
1
protected void updateHdfsState() throws IOException
{    if (conf == null) {        return;    }    int numBasisVectorsOnDisk = 0;    Path nextBasisVectorPath = new Path(basisPath, BASIS_PREFIX + '_' + numBasisVectorsOnDisk);    while (fs.exists(nextBasisVectorPath)) {        nextBasisVectorPath = new Path(basisPath, BASIS_PREFIX + '_' + ++numBasisVectorsOnDisk);    }    Vector nextVector;    while (numBasisVectorsOnDisk < iterationNumber && (nextVector = getBasisVector(numBasisVectorsOnDisk)) != null) {        persistVector(nextBasisVectorPath, numBasisVectorsOnDisk, nextVector);        nextBasisVectorPath = new Path(basisPath, BASIS_PREFIX + '_' + ++numBasisVectorsOnDisk);    }    if (scaleFactor <= 0) {                scaleFactor = getScaleFactor();    }        diagonalMatrix = getDiagonalMatrix();    Vector norms = new DenseVector(diagonalMatrix.numCols() - 1);    Vector projections = new DenseVector(diagonalMatrix.numCols());    int i = 0;    while (i < diagonalMatrix.numCols() - 1) {        norms.set(i, diagonalMatrix.get(i, i + 1));        projections.set(i, diagonalMatrix.get(i, i));        i++;    }    projections.set(i, diagonalMatrix.get(i, i));    persistVector(new Path(baseDir, "projections"), 0, projections);    persistVector(new Path(baseDir, "norms"), 0, norms);    persistVector(new Path(baseDir, "scaleFactor"), 0, new DenseVector(new double[] { scaleFactor }));    for (Map.Entry<Integer, Vector> entry : singularVectors.entrySet()) {        persistVector(new Path(singularVectorPath, SINGULAR_PREFIX + '_' + entry.getKey()), entry.getKey(), entry.getValue());    }    super.setIterationNumber(numBasisVectorsOnDisk);}
0
protected void persistVector(Path p, int key, Vector vector) throws IOException
{    SequenceFile.Writer writer = null;    try {        if (fs.exists(p)) {                        fs.delete(p, true);        }        writer = new SequenceFile.Writer(fs, conf, p, IntWritable.class, VectorWritable.class);        writer.append(new IntWritable(key), new VectorWritable(vector));    } finally {        Closeables.close(writer, false);    }}
1
protected Vector fetchVector(Path p, int keyIndex) throws IOException
{    if (!fs.exists(p)) {        return null;    }    SequenceFile.Reader reader = new SequenceFile.Reader(fs, p, conf);    IntWritable key = new IntWritable();    VectorWritable vw = new VectorWritable();    while (reader.next(key, vw)) {        if (key.get() == keyIndex) {            return vw.get();        }    }    return null;}
0
public Vector getBasisVector(int i)
{    if (!basis.containsKey(i)) {        try {            Vector v = fetchVector(new Path(basisPath, BASIS_PREFIX + '_' + i), i);            basis.put(i, v);        } catch (IOException e) {                    }    }    return super.getBasisVector(i);}
1
public Vector getRightSingularVector(int i)
{    if (!singularVectors.containsKey(i)) {        try {            Vector v = fetchVector(new Path(singularVectorPath, BASIS_PREFIX + '_' + i), i);            singularVectors.put(i, v);        } catch (IOException e) {                    }    }    return super.getRightSingularVector(i);}
1
public double getScaleFactor()
{    if (scaleFactor <= 0) {        try {            Vector v = fetchVector(new Path(baseDir, "scaleFactor"), 0);            if (v != null && v.size() > 0) {                scaleFactor = v.get(0);            }        } catch (IOException e) {                    }    }    return scaleFactor;}
1
public Matrix getDiagonalMatrix()
{    if (diagonalMatrix == null) {        diagonalMatrix = new DenseMatrix(desiredRank, desiredRank);    }    if (diagonalMatrix.get(0, 1) <= 0) {        try {            Vector norms = fetchVector(new Path(baseDir, "norms"), 0);            Vector projections = fetchVector(new Path(baseDir, "projections"), 0);            if (norms != null && projections != null) {                int i = 0;                while (i < projections.size() - 1) {                    diagonalMatrix.set(i, i, projections.get(i));                    diagonalMatrix.set(i, i + 1, norms.get(i));                    diagonalMatrix.set(i + 1, i, norms.get(i));                    i++;                }                diagonalMatrix.set(i, i, projections.get(i));            }        } catch (IOException e) {                    }    }    return diagonalMatrix;}
1
public Configuration getConf()
{    return conf;}
0
public void setConf(Configuration conf)
{    this.conf = conf;    try {        FileSystem fs = FileSystem.get(inputPath.toUri(), conf);        rowPath = fs.makeQualified(inputPath);        outputTmpBasePath = fs.makeQualified(outputTmpPath);        keepTempFiles = conf.getBoolean(KEEP_TEMP_FILES, false);    } catch (IOException ioe) {        throw new IllegalStateException(ioe);    }}
0
public Path getRowPath()
{    return rowPath;}
0
public Path getOutputTempPath()
{    return outputTmpBasePath;}
0
public void setOutputTempPathString(String outPathString)
{    try {        outputTmpBasePath = FileSystem.get(conf).makeQualified(new Path(outPathString));    } catch (IOException ioe) {            }}
1
public Iterator<MatrixSlice> iterateNonEmpty()
{    return iterator();}
0
public Iterator<MatrixSlice> iterateAll()
{    try {        Path pathPattern = rowPath;        if (FileSystem.get(conf).getFileStatus(rowPath).isDir()) {            pathPattern = new Path(rowPath, "*");        }        return Iterators.transform(new SequenceFileDirIterator<IntWritable, VectorWritable>(pathPattern, PathType.GLOB, PathFilters.logsCRCFilter(), null, true, conf), new Function<Pair<IntWritable, VectorWritable>, MatrixSlice>() {            @Override            public MatrixSlice apply(Pair<IntWritable, VectorWritable> from) {                return new MatrixSlice(from.getSecond().get(), from.getFirst().get());            }        });    } catch (IOException ioe) {        throw new IllegalStateException(ioe);    }}
0
public MatrixSlice apply(Pair<IntWritable, VectorWritable> from)
{    return new MatrixSlice(from.getSecond().get(), from.getFirst().get());}
0
public int numSlices()
{    return numRows();}
0
public int numRows()
{    return numRows;}
0
public int numCols()
{    return numCols;}
0
public DistributedRowMatrix times(DistributedRowMatrix other) throws IOException
{    return times(other, new Path(outputTmpBasePath.getParent(), "productWith-" + (System.nanoTime() & 0xFF)));}
0
public DistributedRowMatrix times(DistributedRowMatrix other, Path outPath) throws IOException
{    if (numRows != other.numRows()) {        throw new CardinalityException(numRows, other.numRows());    }    Configuration initialConf = getConf() == null ? new Configuration() : getConf();    Configuration conf = MatrixMultiplicationJob.createMatrixMultiplyJobConf(initialConf, rowPath, other.rowPath, outPath, other.numCols);    JobClient.runJob(new JobConf(conf));    DistributedRowMatrix out = new DistributedRowMatrix(outPath, outputTmpPath, numCols, other.numCols());    out.setConf(conf);    return out;}
0
public Vector columnMeans() throws IOException
{    return columnMeans("SequentialAccessSparseVector");}
0
public Vector columnMeans(String vectorClass) throws IOException
{    Path outputVectorTmpPath = new Path(outputTmpBasePath, new Path(Long.toString(System.nanoTime())));    Configuration initialConf = getConf() == null ? new Configuration() : getConf();    String vectorClassFull = "org.apache.mahout.math." + vectorClass;    Vector mean = MatrixColumnMeansJob.run(initialConf, rowPath, outputVectorTmpPath, vectorClassFull);    if (!keepTempFiles) {        FileSystem fs = outputVectorTmpPath.getFileSystem(conf);        fs.delete(outputVectorTmpPath, true);    }    return mean;}
0
public DistributedRowMatrix transpose() throws IOException
{    Path outputPath = new Path(rowPath.getParent(), "transpose-" + (System.nanoTime() & 0xFF));    Configuration initialConf = getConf() == null ? new Configuration() : getConf();    Job transposeJob = TransposeJob.buildTransposeJob(initialConf, rowPath, outputPath, numRows);    try {        transposeJob.waitForCompletion(true);    } catch (Exception e) {        throw new IllegalStateException("transposition failed", e);    }    DistributedRowMatrix m = new DistributedRowMatrix(outputPath, outputTmpPath, numCols, numRows);    m.setConf(this.conf);    return m;}
0
public Vector times(Vector v)
{    try {        Configuration initialConf = getConf() == null ? new Configuration() : getConf();        Path outputVectorTmpPath = new Path(outputTmpBasePath, new Path(Long.toString(System.nanoTime())));        Job job = TimesSquaredJob.createTimesJob(initialConf, v, numRows, rowPath, outputVectorTmpPath);        try {            job.waitForCompletion(true);        } catch (Exception e) {            throw new IllegalStateException("times failed", e);        }        Vector result = TimesSquaredJob.retrieveTimesSquaredOutputVector(outputVectorTmpPath, conf);        if (!keepTempFiles) {            FileSystem fs = outputVectorTmpPath.getFileSystem(conf);            fs.delete(outputVectorTmpPath, true);        }        return result;    } catch (IOException ioe) {        throw new IllegalStateException(ioe);    }}
0
public Vector timesSquared(Vector v)
{    try {        Configuration initialConf = getConf() == null ? new Configuration() : getConf();        Path outputVectorTmpPath = new Path(outputTmpBasePath, new Path(Long.toString(System.nanoTime())));        Job job = TimesSquaredJob.createTimesSquaredJob(initialConf, v, rowPath, outputVectorTmpPath);        try {            job.waitForCompletion(true);        } catch (Exception e) {            throw new IllegalStateException("timesSquared failed", e);        }        Vector result = TimesSquaredJob.retrieveTimesSquaredOutputVector(outputVectorTmpPath, conf);        if (!keepTempFiles) {            FileSystem fs = outputVectorTmpPath.getFileSystem(conf);            fs.delete(outputVectorTmpPath, true);        }        return result;    } catch (IOException ioe) {        throw new IllegalStateException(ioe);    }}
0
public Iterator<MatrixSlice> iterator()
{    return iterateAll();}
0
public int getRow()
{    return row;}
0
public void setRow(int row)
{    this.row = row;}
0
public int getCol()
{    return col;}
0
public void setCol(int col)
{    this.col = col;}
0
public double getVal()
{    return val;}
0
public void setVal(double val)
{    this.val = val;}
0
public int compareTo(MatrixEntryWritable o)
{    if (row > o.row) {        return 1;    } else if (row < o.row) {        return -1;    } else {        if (col > o.col) {            return 1;        } else if (col < o.col) {            return -1;        } else {            return 0;        }    }}
0
public boolean equals(Object o)
{    if (!(o instanceof MatrixEntryWritable)) {        return false;    }    MatrixEntryWritable other = (MatrixEntryWritable) o;    return row == other.row && col == other.col;}
0
public int hashCode()
{    return row + 31 * col;}
0
public void write(DataOutput out) throws IOException
{    out.writeInt(row);    out.writeInt(col);    out.writeDouble(val);}
0
public void readFields(DataInput in) throws IOException
{    row = in.readInt();    col = in.readInt();    val = in.readDouble();}
0
public String toString()
{    return "(" + row + ',' + col + "):" + val;}
0
public static Vector run(Configuration conf, Path inputPath, Path outputVectorTmpPath) throws IOException
{    return run(conf, inputPath, outputVectorTmpPath, null);}
0
public static Vector run(Configuration initialConf, Path inputPath, Path outputVectorTmpPath, String vectorClass) throws IOException
{    try {        initialConf.set(VECTOR_CLASS, vectorClass == null ? DenseVector.class.getName() : vectorClass);        Job job = new Job(initialConf, "MatrixColumnMeansJob");        job.setJarByClass(MatrixColumnMeansJob.class);        FileOutputFormat.setOutputPath(job, outputVectorTmpPath);        outputVectorTmpPath.getFileSystem(job.getConfiguration()).delete(outputVectorTmpPath, true);        job.setNumReduceTasks(1);        FileOutputFormat.setOutputPath(job, outputVectorTmpPath);        FileInputFormat.addInputPath(job, inputPath);        job.setInputFormatClass(SequenceFileInputFormat.class);        job.setOutputFormatClass(SequenceFileOutputFormat.class);        FileOutputFormat.setOutputPath(job, outputVectorTmpPath);        job.setMapperClass(MatrixColumnMeansMapper.class);        job.setReducerClass(MatrixColumnMeansReducer.class);        job.setMapOutputKeyClass(NullWritable.class);        job.setMapOutputValueClass(VectorWritable.class);        job.setOutputKeyClass(IntWritable.class);        job.setOutputValueClass(VectorWritable.class);        job.submit();        job.waitForCompletion(true);        Path tmpFile = new Path(outputVectorTmpPath, "part-r-00000");        SequenceFileValueIterator<VectorWritable> iterator = new SequenceFileValueIterator<>(tmpFile, true, initialConf);        try {            if (iterator.hasNext()) {                return iterator.next().get();            } else {                return (Vector) Class.forName(vectorClass).getConstructor(int.class).newInstance(0);            }        } finally {            Closeables.close(iterator, true);        }    } catch (IOException ioe) {        throw ioe;    } catch (Throwable thr) {        throw new IOException(thr);    }}
0
public void setup(Context context)
{    vectorClass = context.getConfiguration().get(VECTOR_CLASS);}
0
public void map(Writable r, VectorWritable v, Context context) throws IOException
{    if (runningSum == null) {        /*           * If this is the first vector the mapper has seen, instantiate a new           * vector using the parameter VECTOR_CLASS           */        runningSum = ClassUtils.instantiateAs(vectorClass, Vector.class, new Class<?>[] { int.class }, new Object[] { v.get().size() + 1 });        runningSum.set(0, 1);        runningSum.viewPart(1, v.get().size()).assign(v.get());    } else {        runningSum.set(0, runningSum.get(0) + 1);        runningSum.viewPart(1, v.get().size()).assign(v.get(), Functions.PLUS);    }}
0
public void cleanup(Context context) throws InterruptedException, IOException
{    if (runningSum != null) {        context.write(NullWritable.get(), new VectorWritable(runningSum));    }}
0
public void setup(Context context)
{    vectorClass = context.getConfiguration().get(VECTOR_CLASS);}
0
public void reduce(NullWritable n, Iterable<VectorWritable> vectors, Context context) throws IOException, InterruptedException
{    /**     * Add together partial column-wise sums from mappers     */    for (VectorWritable v : vectors) {        if (outputVector == null) {            outputVector = v.get();        } else {            outputVector.assign(v.get(), Functions.PLUS);        }    }    /**     * Divide total column-wise sum by count of vectors, which corresponds to     * the number of rows in the DistributedRowMatrix     */    if (outputVector != null) {        outputVectorWritable.set(outputVector.viewPart(1, outputVector.size() - 1).divide(outputVector.get(0)));        context.write(ONE, outputVectorWritable);    } else {        Vector emptyVector = ClassUtils.instantiateAs(vectorClass, Vector.class, new Class<?>[] { int.class }, new Object[] { 0 });        context.write(ONE, new VectorWritable(emptyVector));    }}
0
public static Configuration createMatrixMultiplyJobConf(Path aPath, Path bPath, Path outPath, int outCardinality)
{    return createMatrixMultiplyJobConf(new Configuration(), aPath, bPath, outPath, outCardinality);}
0
public static Configuration createMatrixMultiplyJobConf(Configuration initialConf, Path aPath, Path bPath, Path outPath, int outCardinality)
{    JobConf conf = new JobConf(initialConf, MatrixMultiplicationJob.class);    conf.setInputFormat(CompositeInputFormat.class);    conf.set("mapred.join.expr", CompositeInputFormat.compose("inner", SequenceFileInputFormat.class, aPath, bPath));    conf.setInt(OUT_CARD, outCardinality);    conf.setOutputFormat(SequenceFileOutputFormat.class);    FileOutputFormat.setOutputPath(conf, outPath);    conf.setMapperClass(MatrixMultiplyMapper.class);    conf.setCombinerClass(MatrixMultiplicationReducer.class);    conf.setReducerClass(MatrixMultiplicationReducer.class);    conf.setMapOutputKeyClass(IntWritable.class);    conf.setMapOutputValueClass(VectorWritable.class);    conf.setOutputKeyClass(IntWritable.class);    conf.setOutputValueClass(VectorWritable.class);    return conf;}
0
public static void main(String[] args) throws Exception
{    ToolRunner.run(new MatrixMultiplicationJob(), args);}
0
public int run(String[] strings) throws Exception
{    addOption("numRowsA", "nra", "Number of rows of the first input matrix", true);    addOption("numColsA", "nca", "Number of columns of the first input matrix", true);    addOption("numRowsB", "nrb", "Number of rows of the second input matrix", true);    addOption("numColsB", "ncb", "Number of columns of the second input matrix", true);    addOption("inputPathA", "ia", "Path to the first input matrix", true);    addOption("inputPathB", "ib", "Path to the second input matrix", true);    addOption("outputPath", "op", "Path to the output matrix", false);    Map<String, List<String>> argMap = parseArguments(strings);    if (argMap == null) {        return -1;    }    DistributedRowMatrix a = new DistributedRowMatrix(new Path(getOption("inputPathA")), new Path(getOption("tempDir")), Integer.parseInt(getOption("numRowsA")), Integer.parseInt(getOption("numColsA")));    DistributedRowMatrix b = new DistributedRowMatrix(new Path(getOption("inputPathB")), new Path(getOption("tempDir")), Integer.parseInt(getOption("numRowsB")), Integer.parseInt(getOption("numColsB")));    a.setConf(new Configuration(getConf()));    b.setConf(new Configuration(getConf()));    if (hasOption("outputPath")) {        a.times(b, new Path(getOption("outputPath")));    } else {        a.times(b);    }    return 0;}
0
public void configure(JobConf conf)
{    outCardinality = conf.getInt(OUT_CARD, Integer.MAX_VALUE);}
0
public void map(IntWritable index, TupleWritable v, OutputCollector<IntWritable, VectorWritable> out, Reporter reporter) throws IOException
{    boolean firstIsOutFrag = ((VectorWritable) v.get(0)).get().size() == outCardinality;    Vector outFrag = firstIsOutFrag ? ((VectorWritable) v.get(0)).get() : ((VectorWritable) v.get(1)).get();    Vector multiplier = firstIsOutFrag ? ((VectorWritable) v.get(1)).get() : ((VectorWritable) v.get(0)).get();    VectorWritable outVector = new VectorWritable();    for (Vector.Element e : multiplier.nonZeroes()) {        row.set(e.index());        outVector.set(outFrag.times(e.get()));        out.collect(row, outVector);    }}
0
public void reduce(IntWritable rowNum, Iterator<VectorWritable> it, OutputCollector<IntWritable, VectorWritable> out, Reporter reporter) throws IOException
{    if (!it.hasNext()) {        return;    }    Vector accumulator = new RandomAccessSparseVector(it.next().get());    while (it.hasNext()) {        Vector row = it.next().get();        accumulator.assign(row, Functions.PLUS);    }    out.collect(rowNum, new VectorWritable(new SequentialAccessSparseVector(accumulator)));}
0
public double similarity(double dots, double normA, double normB, int numberOfColumns)
{    return 1.0 / (1.0 + normA + normB - 2 * dots);}
0
public double similarity(double dots, double normA, double normB, int numberOfColumns)
{    return dots;}
0
public boolean consider(int numNonZeroEntriesA, int numNonZeroEntriesB, double maxValueA, double maxValueB, double threshold)
{    return numNonZeroEntriesA >= threshold && numNonZeroEntriesB >= threshold;}
0
public Vector normalize(Vector vector)
{    return vector.normalize();}
0
public double norm(Vector vector)
{    return VectorSimilarityMeasure.NO_NORM;}
0
public double aggregate(double valueA, double nonZeroValueB)
{    return valueA * nonZeroValueB;}
0
public double similarity(double dots, double normA, double normB, int numberOfColumns)
{    return dots;}
0
public boolean consider(int numNonZeroEntriesA, int numNonZeroEntriesB, double maxValueA, double maxValueB, double threshold)
{    return numNonZeroEntriesB >= threshold / maxValueA && numNonZeroEntriesA >= threshold / maxValueB;}
0
public Vector normalize(Vector vector)
{    return vector;}
0
public double norm(Vector vector)
{    return vector.norm(0);}
0
public double aggregate(double valueA, double nonZeroValueB)
{    return 1;}
0
public boolean consider(int numNonZeroEntriesA, int numNonZeroEntriesB, double maxValueA, double maxValueB, double threshold)
{    return true;}
0
public Vector normalize(Vector vector)
{    return vector;}
0
public double norm(Vector vector)
{    double norm = 0;    for (Vector.Element e : vector.nonZeroes()) {        double value = e.get();        norm += value * value;    }    return norm;}
0
public double aggregate(double valueA, double nonZeroValueB)
{    return valueA * nonZeroValueB;}
0
public double similarity(double dots, double normA, double normB, int numberOfColumns)
{            double euclideanDistance = Math.sqrt(Math.max(0.0, normA - 2 * dots + normB));    return 1.0 / (1.0 + euclideanDistance);}
0
public boolean consider(int numNonZeroEntriesA, int numNonZeroEntriesB, double maxValueA, double maxValueB, double threshold)
{    return true;}
0
public double similarity(double summedAggregations, double normA, double normB, int numberOfColumns)
{    double logLikelihood = LogLikelihood.logLikelihoodRatio((long) summedAggregations, (long) (normB - summedAggregations), (long) (normA - summedAggregations), (long) (numberOfColumns - normA - normB + summedAggregations));    return 1.0 - 1.0 / (1.0 + logLikelihood);}
0
public Vector normalize(Vector vector)
{    if (vector.getNumNondefaultElements() == 0) {        return vector;    }        double average = vector.norm(1) / vector.getNumNonZeroElements();    for (Vector.Element e : vector.nonZeroes()) {        e.set(e.get() - average);    }    return super.normalize(vector);}
0
public double similarity(double dots, double normA, double normB, int numberOfColumns)
{        return dots / (normA + normB - dots);}
0
public boolean consider(int numNonZeroEntriesA, int numNonZeroEntriesB, double maxValueA, double maxValueB, double threshold)
{    return numNonZeroEntriesA >= numNonZeroEntriesB * threshold && numNonZeroEntriesB >= numNonZeroEntriesA * threshold;}
0
public String getClassname()
{    return implementingClass.getName();}
0
public static String list()
{    return Arrays.toString(values());}
0
public double get()
{    return value;}
0
public int index()
{    return index;}
0
public void setIndex(int index)
{    this.index = index;}
0
public void set(double value)
{    this.value = value;}
0
public static void main(String[] args) throws Exception
{    ToolRunner.run(new RowSimilarityJob(), args);}
0
public int run(String[] args) throws Exception
{    addInputOption();    addOutputOption();    addOption("numberOfColumns", "r", "Number of columns in the input matrix", false);    addOption("similarityClassname", "s", "Name of distributed similarity class to instantiate, alternatively use " + "one of the predefined similarities (" + VectorSimilarityMeasures.list() + ')');    addOption("maxSimilaritiesPerRow", "m", "Number of maximum similarities per row (default: " + DEFAULT_MAX_SIMILARITIES_PER_ROW + ')', String.valueOf(DEFAULT_MAX_SIMILARITIES_PER_ROW));    addOption("excludeSelfSimilarity", "ess", "compute similarity of rows to themselves?", String.valueOf(false));    addOption("threshold", "tr", "discard row pairs with a similarity value below this", false);    addOption("maxObservationsPerRow", null, "sample rows down to this number of entries", String.valueOf(DEFAULT_MAX_OBSERVATIONS_PER_ROW));    addOption("maxObservationsPerColumn", null, "sample columns down to this number of entries", String.valueOf(DEFAULT_MAX_OBSERVATIONS_PER_COLUMN));    addOption("randomSeed", null, "use this seed for sampling", false);    addOption(DefaultOptionCreator.overwriteOption().create());    Map<String, List<String>> parsedArgs = parseArguments(args);    if (parsedArgs == null) {        return -1;    }    int numberOfColumns;    if (hasOption("numberOfColumns")) {                numberOfColumns = Integer.parseInt(getOption("numberOfColumns"));    } else {                numberOfColumns = getDimensions(getInputPath());    }    String similarityClassnameArg = getOption("similarityClassname");    String similarityClassname;    try {        similarityClassname = VectorSimilarityMeasures.valueOf(similarityClassnameArg).getClassname();    } catch (IllegalArgumentException iae) {        similarityClassname = similarityClassnameArg;    }        if (hasOption(DefaultOptionCreator.OVERWRITE_OPTION)) {                HadoopUtil.delete(getConf(), getTempPath());                HadoopUtil.delete(getConf(), getOutputPath());    }    int maxSimilaritiesPerRow = Integer.parseInt(getOption("maxSimilaritiesPerRow"));    boolean excludeSelfSimilarity = Boolean.parseBoolean(getOption("excludeSelfSimilarity"));    double threshold = hasOption("threshold") ? Double.parseDouble(getOption("threshold")) : NO_THRESHOLD;    long randomSeed = hasOption("randomSeed") ? Long.parseLong(getOption("randomSeed")) : NO_FIXED_RANDOM_SEED;    int maxObservationsPerRow = Integer.parseInt(getOption("maxObservationsPerRow"));    int maxObservationsPerColumn = Integer.parseInt(getOption("maxObservationsPerColumn"));    Path weightsPath = getTempPath("weights");    Path normsPath = getTempPath("norms.bin");    Path numNonZeroEntriesPath = getTempPath("numNonZeroEntries.bin");    Path maxValuesPath = getTempPath("maxValues.bin");    Path pairwiseSimilarityPath = getTempPath("pairwiseSimilarity");    Path observationsPerColumnPath = getTempPath("observationsPerColumn.bin");    AtomicInteger currentPhase = new AtomicInteger();    Job countObservations = prepareJob(getInputPath(), getTempPath("notUsed"), CountObservationsMapper.class, NullWritable.class, VectorWritable.class, SumObservationsReducer.class, NullWritable.class, VectorWritable.class);    countObservations.setCombinerClass(VectorSumCombiner.class);    countObservations.getConfiguration().set(OBSERVATIONS_PER_COLUMN_PATH, observationsPerColumnPath.toString());    countObservations.setNumReduceTasks(1);    countObservations.waitForCompletion(true);    if (shouldRunNextPhase(parsedArgs, currentPhase)) {        Job normsAndTranspose = prepareJob(getInputPath(), weightsPath, VectorNormMapper.class, IntWritable.class, VectorWritable.class, MergeVectorsReducer.class, IntWritable.class, VectorWritable.class);        normsAndTranspose.setCombinerClass(MergeVectorsCombiner.class);        Configuration normsAndTransposeConf = normsAndTranspose.getConfiguration();        normsAndTransposeConf.set(THRESHOLD, String.valueOf(threshold));        normsAndTransposeConf.set(NORMS_PATH, normsPath.toString());        normsAndTransposeConf.set(NUM_NON_ZERO_ENTRIES_PATH, numNonZeroEntriesPath.toString());        normsAndTransposeConf.set(MAXVALUES_PATH, maxValuesPath.toString());        normsAndTransposeConf.set(SIMILARITY_CLASSNAME, similarityClassname);        normsAndTransposeConf.set(OBSERVATIONS_PER_COLUMN_PATH, observationsPerColumnPath.toString());        normsAndTransposeConf.set(MAX_OBSERVATIONS_PER_ROW, String.valueOf(maxObservationsPerRow));        normsAndTransposeConf.set(MAX_OBSERVATIONS_PER_COLUMN, String.valueOf(maxObservationsPerColumn));        normsAndTransposeConf.set(RANDOM_SEED, String.valueOf(randomSeed));        boolean succeeded = normsAndTranspose.waitForCompletion(true);        if (!succeeded) {            return -1;        }    }    if (shouldRunNextPhase(parsedArgs, currentPhase)) {        Job pairwiseSimilarity = prepareJob(weightsPath, pairwiseSimilarityPath, CooccurrencesMapper.class, IntWritable.class, VectorWritable.class, SimilarityReducer.class, IntWritable.class, VectorWritable.class);        pairwiseSimilarity.setCombinerClass(VectorSumReducer.class);        Configuration pairwiseConf = pairwiseSimilarity.getConfiguration();        pairwiseConf.set(THRESHOLD, String.valueOf(threshold));        pairwiseConf.set(NORMS_PATH, normsPath.toString());        pairwiseConf.set(NUM_NON_ZERO_ENTRIES_PATH, numNonZeroEntriesPath.toString());        pairwiseConf.set(MAXVALUES_PATH, maxValuesPath.toString());        pairwiseConf.set(SIMILARITY_CLASSNAME, similarityClassname);        pairwiseConf.setInt(NUMBER_OF_COLUMNS, numberOfColumns);        pairwiseConf.setBoolean(EXCLUDE_SELF_SIMILARITY, excludeSelfSimilarity);        boolean succeeded = pairwiseSimilarity.waitForCompletion(true);        if (!succeeded) {            return -1;        }    }    if (shouldRunNextPhase(parsedArgs, currentPhase)) {        Job asMatrix = prepareJob(pairwiseSimilarityPath, getOutputPath(), UnsymmetrifyMapper.class, IntWritable.class, VectorWritable.class, MergeToTopKSimilaritiesReducer.class, IntWritable.class, VectorWritable.class);        asMatrix.setCombinerClass(MergeToTopKSimilaritiesReducer.class);        asMatrix.getConfiguration().setInt(MAX_SIMILARITIES_PER_ROW, maxSimilaritiesPerRow);        boolean succeeded = asMatrix.waitForCompletion(true);        if (!succeeded) {            return -1;        }    }    return 0;}
0
protected void map(IntWritable rowIndex, VectorWritable rowVectorWritable, Context ctx) throws IOException, InterruptedException
{    Vector row = rowVectorWritable.get();    for (Vector.Element elem : row.nonZeroes()) {        columnCounts.setQuick(elem.index(), columnCounts.getQuick(elem.index()) + 1);    }}
0
protected void cleanup(Context ctx) throws IOException, InterruptedException
{    ctx.write(NullWritable.get(), new VectorWritable(columnCounts));}
0
protected void reduce(NullWritable nullWritable, Iterable<VectorWritable> partialVectors, Context ctx) throws IOException, InterruptedException
{    Vector counts = Vectors.sum(partialVectors.iterator());    Vectors.write(counts, new Path(ctx.getConfiguration().get(OBSERVATIONS_PER_COLUMN_PATH)), ctx.getConfiguration());}
0
protected void setup(Context ctx) throws IOException, InterruptedException
{    Configuration conf = ctx.getConfiguration();    similarity = ClassUtils.instantiateAs(conf.get(SIMILARITY_CLASSNAME), VectorSimilarityMeasure.class);    norms = new RandomAccessSparseVector(Integer.MAX_VALUE);    nonZeroEntries = new RandomAccessSparseVector(Integer.MAX_VALUE);    maxValues = new RandomAccessSparseVector(Integer.MAX_VALUE);    threshold = Double.parseDouble(conf.get(THRESHOLD));    observationsPerColumn = Vectors.readAsIntMap(new Path(conf.get(OBSERVATIONS_PER_COLUMN_PATH)), conf);    maxObservationsPerRow = conf.getInt(MAX_OBSERVATIONS_PER_ROW, DEFAULT_MAX_OBSERVATIONS_PER_ROW);    maxObservationsPerColumn = conf.getInt(MAX_OBSERVATIONS_PER_COLUMN, DEFAULT_MAX_OBSERVATIONS_PER_COLUMN);    long seed = Long.parseLong(conf.get(RANDOM_SEED));    if (seed == NO_FIXED_RANDOM_SEED) {        random = RandomUtils.getRandom();    } else {        random = RandomUtils.getRandom(seed);    }}
0
private Vector sampleDown(Vector rowVector, Context ctx)
{    int observationsPerRow = rowVector.getNumNondefaultElements();    double rowSampleRate = (double) Math.min(maxObservationsPerRow, observationsPerRow) / (double) observationsPerRow;    Vector downsampledRow = rowVector.like();    long usedObservations = 0;    long neglectedObservations = 0;    for (Vector.Element elem : rowVector.nonZeroes()) {        int columnCount = observationsPerColumn.get(elem.index());        double columnSampleRate = (double) Math.min(maxObservationsPerColumn, columnCount) / (double) columnCount;        if (random.nextDouble() <= Math.min(rowSampleRate, columnSampleRate)) {            downsampledRow.setQuick(elem.index(), elem.get());            usedObservations++;        } else {            neglectedObservations++;        }    }    ctx.getCounter(Counters.USED_OBSERVATIONS).increment(usedObservations);    ctx.getCounter(Counters.NEGLECTED_OBSERVATIONS).increment(neglectedObservations);    return downsampledRow;}
0
protected void map(IntWritable row, VectorWritable vectorWritable, Context ctx) throws IOException, InterruptedException
{    Vector sampledRowVector = sampleDown(vectorWritable.get(), ctx);    Vector rowVector = similarity.normalize(sampledRowVector);    int numNonZeroEntries = 0;    double maxValue = Double.MIN_VALUE;    for (Vector.Element element : rowVector.nonZeroes()) {        RandomAccessSparseVector partialColumnVector = new RandomAccessSparseVector(Integer.MAX_VALUE);        partialColumnVector.setQuick(row.get(), element.get());        ctx.write(new IntWritable(element.index()), new VectorWritable(partialColumnVector));        numNonZeroEntries++;        if (maxValue < element.get()) {            maxValue = element.get();        }    }    if (threshold != NO_THRESHOLD) {        nonZeroEntries.setQuick(row.get(), numNonZeroEntries);        maxValues.setQuick(row.get(), maxValue);    }    norms.setQuick(row.get(), similarity.norm(rowVector));    ctx.getCounter(Counters.ROWS).increment(1);}
0
protected void cleanup(Context ctx) throws IOException, InterruptedException
{    ctx.write(new IntWritable(NORM_VECTOR_MARKER), new VectorWritable(norms));    ctx.write(new IntWritable(NUM_NON_ZERO_ENTRIES_VECTOR_MARKER), new VectorWritable(nonZeroEntries));    ctx.write(new IntWritable(MAXVALUE_VECTOR_MARKER), new VectorWritable(maxValues));}
0
protected void reduce(IntWritable row, Iterable<VectorWritable> partialVectors, Context ctx) throws IOException, InterruptedException
{    ctx.write(row, new VectorWritable(Vectors.merge(partialVectors)));}
0
protected void setup(Context ctx) throws IOException, InterruptedException
{    normsPath = new Path(ctx.getConfiguration().get(NORMS_PATH));    numNonZeroEntriesPath = new Path(ctx.getConfiguration().get(NUM_NON_ZERO_ENTRIES_PATH));    maxValuesPath = new Path(ctx.getConfiguration().get(MAXVALUES_PATH));}
0
protected void reduce(IntWritable row, Iterable<VectorWritable> partialVectors, Context ctx) throws IOException, InterruptedException
{    Vector partialVector = Vectors.merge(partialVectors);    if (row.get() == NORM_VECTOR_MARKER) {        Vectors.write(partialVector, normsPath, ctx.getConfiguration());    } else if (row.get() == MAXVALUE_VECTOR_MARKER) {        Vectors.write(partialVector, maxValuesPath, ctx.getConfiguration());    } else if (row.get() == NUM_NON_ZERO_ENTRIES_VECTOR_MARKER) {        Vectors.write(partialVector, numNonZeroEntriesPath, ctx.getConfiguration(), true);    } else {        ctx.write(row, new VectorWritable(partialVector));    }}
0
public int compare(Vector.Element one, Vector.Element two)
{    return Ints.compare(one.index(), two.index());}
0
protected void setup(Context ctx) throws IOException, InterruptedException
{    similarity = ClassUtils.instantiateAs(ctx.getConfiguration().get(SIMILARITY_CLASSNAME), VectorSimilarityMeasure.class);    numNonZeroEntries = Vectors.readAsIntMap(new Path(ctx.getConfiguration().get(NUM_NON_ZERO_ENTRIES_PATH)), ctx.getConfiguration());    maxValues = Vectors.read(new Path(ctx.getConfiguration().get(MAXVALUES_PATH)), ctx.getConfiguration());    threshold = Double.parseDouble(ctx.getConfiguration().get(THRESHOLD));}
0
private boolean consider(Vector.Element occurrenceA, Vector.Element occurrenceB)
{    int numNonZeroEntriesA = numNonZeroEntries.get(occurrenceA.index());    int numNonZeroEntriesB = numNonZeroEntries.get(occurrenceB.index());    double maxValueA = maxValues.get(occurrenceA.index());    double maxValueB = maxValues.get(occurrenceB.index());    return similarity.consider(numNonZeroEntriesA, numNonZeroEntriesB, maxValueA, maxValueB, threshold);}
0
protected void map(IntWritable column, VectorWritable occurrenceVector, Context ctx) throws IOException, InterruptedException
{    Vector.Element[] occurrences = Vectors.toArray(occurrenceVector);    Arrays.sort(occurrences, BY_INDEX);    int cooccurrences = 0;    int prunedCooccurrences = 0;    for (int n = 0; n < occurrences.length; n++) {        Vector.Element occurrenceA = occurrences[n];        Vector dots = new RandomAccessSparseVector(Integer.MAX_VALUE);        for (int m = n; m < occurrences.length; m++) {            Vector.Element occurrenceB = occurrences[m];            if (threshold == NO_THRESHOLD || consider(occurrenceA, occurrenceB)) {                dots.setQuick(occurrenceB.index(), similarity.aggregate(occurrenceA.get(), occurrenceB.get()));                cooccurrences++;            } else {                prunedCooccurrences++;            }        }        ctx.write(new IntWritable(occurrenceA.index()), new VectorWritable(dots));    }    ctx.getCounter(Counters.COOCCURRENCES).increment(cooccurrences);    ctx.getCounter(Counters.PRUNED_COOCCURRENCES).increment(prunedCooccurrences);}
0
protected void setup(Context ctx) throws IOException, InterruptedException
{    similarity = ClassUtils.instantiateAs(ctx.getConfiguration().get(SIMILARITY_CLASSNAME), VectorSimilarityMeasure.class);    numberOfColumns = ctx.getConfiguration().getInt(NUMBER_OF_COLUMNS, -1);    Preconditions.checkArgument(numberOfColumns > 0, "Number of columns must be greater then 0! But numberOfColumns = " + numberOfColumns);    excludeSelfSimilarity = ctx.getConfiguration().getBoolean(EXCLUDE_SELF_SIMILARITY, false);    norms = Vectors.read(new Path(ctx.getConfiguration().get(NORMS_PATH)), ctx.getConfiguration());    treshold = Double.parseDouble(ctx.getConfiguration().get(THRESHOLD));}
0
protected void reduce(IntWritable row, Iterable<VectorWritable> partialDots, Context ctx) throws IOException, InterruptedException
{    Iterator<VectorWritable> partialDotsIterator = partialDots.iterator();    Vector dots = partialDotsIterator.next().get();    while (partialDotsIterator.hasNext()) {        Vector toAdd = partialDotsIterator.next().get();        for (Element nonZeroElement : toAdd.nonZeroes()) {            dots.setQuick(nonZeroElement.index(), dots.getQuick(nonZeroElement.index()) + nonZeroElement.get());        }    }    Vector similarities = dots.like();    double normA = norms.getQuick(row.get());    for (Element b : dots.nonZeroes()) {        double similarityValue = similarity.similarity(b.get(), normA, norms.getQuick(b.index()), numberOfColumns);        if (similarityValue >= treshold) {            similarities.set(b.index(), similarityValue);        }    }    if (excludeSelfSimilarity) {        similarities.setQuick(row.get(), 0);    }    ctx.write(row, new VectorWritable(similarities));}
0
protected void setup(Mapper.Context ctx) throws IOException, InterruptedException
{    maxSimilaritiesPerRow = ctx.getConfiguration().getInt(MAX_SIMILARITIES_PER_ROW, 0);    Preconditions.checkArgument(maxSimilaritiesPerRow > 0, "Maximum number of similarities per row must be greater then 0!");}
0
protected void map(IntWritable row, VectorWritable similaritiesWritable, Context ctx) throws IOException, InterruptedException
{    Vector similarities = similaritiesWritable.get();        Vector transposedPartial = new RandomAccessSparseVector(similarities.size(), 1);    TopElementsQueue topKQueue = new TopElementsQueue(maxSimilaritiesPerRow);    for (Element nonZeroElement : similarities.nonZeroes()) {        MutableElement top = topKQueue.top();        double candidateValue = nonZeroElement.get();        if (candidateValue > top.get()) {            top.setIndex(nonZeroElement.index());            top.set(candidateValue);            topKQueue.updateTop();        }        transposedPartial.setQuick(row.get(), candidateValue);        ctx.write(new IntWritable(nonZeroElement.index()), new VectorWritable(transposedPartial));        transposedPartial.setQuick(row.get(), 0.0);    }    Vector topKSimilarities = new RandomAccessSparseVector(similarities.size(), maxSimilaritiesPerRow);    for (Vector.Element topKSimilarity : topKQueue.getTopElements()) {        topKSimilarities.setQuick(topKSimilarity.index(), topKSimilarity.get());    }    ctx.write(row, new VectorWritable(topKSimilarities));}
0
protected void setup(Context ctx) throws IOException, InterruptedException
{    maxSimilaritiesPerRow = ctx.getConfiguration().getInt(MAX_SIMILARITIES_PER_ROW, 0);    Preconditions.checkArgument(maxSimilaritiesPerRow > 0, "Maximum number of similarities per row must be greater then 0!");}
0
protected void reduce(IntWritable row, Iterable<VectorWritable> partials, Context ctx) throws IOException, InterruptedException
{    Vector allSimilarities = Vectors.merge(partials);    Vector topKSimilarities = Vectors.topKElements(maxSimilaritiesPerRow, allSimilarities);    ctx.write(row, new VectorWritable(topKSimilarities));}
0
public List<MutableElement> getTopElements()
{    List<MutableElement> topElements = Lists.newArrayListWithCapacity(maxSize);    while (size() > 0) {        MutableElement top = pop();                if (top.index() != SENTINEL_INDEX) {            topElements.add(top);        }    }    Collections.reverse(topElements);    return topElements;}
0
protected MutableElement getSentinelObject()
{    return new MutableElement(SENTINEL_INDEX, Double.MIN_VALUE);}
0
protected boolean lessThan(MutableElement e1, MutableElement e2)
{    return e1.get() < e2.get();}
0
public static Vector maybeSample(Vector original, int sampleSize)
{    if (original.getNumNondefaultElements() <= sampleSize) {        return original;    }    Vector sample = new RandomAccessSparseVector(original.size(), sampleSize);    Iterator<Element> sampledElements = new FixedSizeSamplingIterator<>(sampleSize, original.nonZeroes().iterator());    while (sampledElements.hasNext()) {        Element elem = sampledElements.next();        sample.setQuick(elem.index(), elem.get());    }    return sample;}
0
public static Vector topKElements(int k, Vector original)
{    if (original.getNumNondefaultElements() <= k) {        return original;    }    TopElementsQueue topKQueue = new TopElementsQueue(k);    for (Element nonZeroElement : original.nonZeroes()) {        MutableElement top = topKQueue.top();        double candidateValue = nonZeroElement.get();        if (candidateValue > top.get()) {            top.setIndex(nonZeroElement.index());            top.set(candidateValue);            topKQueue.updateTop();        }    }    Vector topKSimilarities = new RandomAccessSparseVector(original.size(), k);    for (Vector.Element topKSimilarity : topKQueue.getTopElements()) {        topKSimilarities.setQuick(topKSimilarity.index(), topKSimilarity.get());    }    return topKSimilarities;}
0
public static Vector merge(Iterable<VectorWritable> partialVectors)
{    Iterator<VectorWritable> vectors = partialVectors.iterator();    Vector accumulator = vectors.next().get();    while (vectors.hasNext()) {        VectorWritable v = vectors.next();        if (v != null) {            for (Element nonZeroElement : v.get().nonZeroes()) {                accumulator.setQuick(nonZeroElement.index(), nonZeroElement.get());            }        }    }    return accumulator;}
0
public static Vector sum(Iterator<VectorWritable> vectors)
{    Vector sum = vectors.next().get();    while (vectors.hasNext()) {        sum.assign(vectors.next().get(), Functions.PLUS);    }    return sum;}
0
public double get()
{    return value;}
0
public int index()
{    return index;}
0
public void set(double value)
{    this.value = value;}
0
public static Vector.Element[] toArray(VectorWritable vectorWritable)
{    Vector.Element[] elements = new Vector.Element[vectorWritable.get().getNumNondefaultElements()];    int k = 0;    for (Element nonZeroElement : vectorWritable.get().nonZeroes()) {        elements[k++] = new TemporaryElement(nonZeroElement.index(), nonZeroElement.get());    }    return elements;}
0
public static void write(Vector vector, Path path, Configuration conf) throws IOException
{    write(vector, path, conf, false);}
0
public static void write(Vector vector, Path path, Configuration conf, boolean laxPrecision) throws IOException
{    FileSystem fs = FileSystem.get(path.toUri(), conf);    FSDataOutputStream out = fs.create(path);    try {        VectorWritable vectorWritable = new VectorWritable(vector);        vectorWritable.setWritesLaxPrecision(laxPrecision);        vectorWritable.write(out);    } finally {        Closeables.close(out, false);    }}
0
public static OpenIntIntHashMap readAsIntMap(Path path, Configuration conf) throws IOException
{    FileSystem fs = FileSystem.get(path.toUri(), conf);    FSDataInputStream in = fs.open(path);    try {        return readAsIntMap(in);    } finally {        Closeables.close(in, true);    }}
0
private static OpenIntIntHashMap readAsIntMap(DataInput in) throws IOException
{    int flags = in.readByte();    Preconditions.checkArgument(flags >> VectorWritable.NUM_FLAGS == 0, "Unknown flags set: %d", Integer.toString(flags, 2));    boolean dense = (flags & VectorWritable.FLAG_DENSE) != 0;    boolean sequential = (flags & VectorWritable.FLAG_SEQUENTIAL) != 0;    boolean laxPrecision = (flags & VectorWritable.FLAG_LAX_PRECISION) != 0;    Preconditions.checkState(!dense && !sequential, "Only for reading sparse vectors!");    Varint.readUnsignedVarInt(in);    OpenIntIntHashMap values = new OpenIntIntHashMap();    int numNonDefaultElements = Varint.readUnsignedVarInt(in);    for (int i = 0; i < numNonDefaultElements; i++) {        int index = Varint.readUnsignedVarInt(in);        double value = laxPrecision ? in.readFloat() : in.readDouble();        values.put(index, (int) value);    }    return values;}
0
public static Vector read(Path path, Configuration conf) throws IOException
{    FileSystem fs = FileSystem.get(path.toUri(), conf);    FSDataInputStream in = fs.open(path);    try {        return VectorWritable.readVector(in);    } finally {        Closeables.close(in, true);    }}
0
public static List<NamedVector> loadSeedVectors(Configuration conf)
{    String seedPathStr = conf.get(VectorDistanceSimilarityJob.SEEDS_PATH_KEY);    if (seedPathStr == null || seedPathStr.isEmpty()) {        return Collections.emptyList();    }    List<NamedVector> seedVectors = Lists.newArrayList();    long item = 0;    for (Writable value : new SequenceFileDirValueIterable<>(new Path(seedPathStr), PathType.LIST, PathFilters.partFilter(), conf)) {        Class<? extends Writable> valueClass = value.getClass();        if (valueClass.equals(Kluster.class)) {                        Kluster cluster = (Kluster) value;            Vector vector = cluster.getCenter();            if (vector instanceof NamedVector) {                seedVectors.add((NamedVector) vector);            } else {                seedVectors.add(new NamedVector(vector, cluster.getIdentifier()));            }        } else if (valueClass.equals(Canopy.class)) {                        Canopy canopy = (Canopy) value;            Vector vector = canopy.getCenter();            if (vector instanceof NamedVector) {                seedVectors.add((NamedVector) vector);            } else {                seedVectors.add(new NamedVector(vector, canopy.getIdentifier()));            }        } else if (valueClass.equals(Vector.class)) {            Vector vector = (Vector) value;            if (vector instanceof NamedVector) {                seedVectors.add((NamedVector) vector);            } else {                seedVectors.add(new NamedVector(vector, seedPathStr + '.' + item++));            }        } else if (valueClass.equals(VectorWritable.class) || valueClass.isInstance(VectorWritable.class)) {            VectorWritable vw = (VectorWritable) value;            Vector vector = vw.get();            if (vector instanceof NamedVector) {                seedVectors.add((NamedVector) vector);            } else {                seedVectors.add(new NamedVector(vector, seedPathStr + '.' + item++));            }        } else {            throw new IllegalStateException("Bad value class: " + valueClass);        }    }    if (seedVectors.isEmpty()) {        throw new IllegalStateException("No seeds found. Check your path: " + seedPathStr);    }        return seedVectors;}
1
protected void map(WritableComparable<?> key, VectorWritable value, Context context) throws IOException, InterruptedException
{    String keyName;    Vector valVec = value.get();    if (valVec instanceof NamedVector) {        keyName = ((NamedVector) valVec).getName();    } else {        keyName = key.toString();    }    Vector outVec = new DenseVector(new double[seedVectors.size()]);    int i = 0;    for (NamedVector seedVector : seedVectors) {        outVec.setQuick(i++, measure.distance(seedVector, valVec));    }    context.write(new Text(keyName), new VectorWritable(outVec));}
0
protected void setup(Context context) throws IOException, InterruptedException
{    super.setup(context);    Configuration conf = context.getConfiguration();    measure = ClassUtils.instantiateAs(conf.get(VectorDistanceSimilarityJob.DISTANCE_MEASURE_KEY), DistanceMeasure.class);    measure.configure(conf);    seedVectors = SeedVectorUtil.loadSeedVectors(conf);}
0
protected void map(WritableComparable<?> key, VectorWritable value, Context context) throws IOException, InterruptedException
{    String keyName;    Vector valVec = value.get();    if (valVec instanceof NamedVector) {        keyName = ((NamedVector) valVec).getName();    } else {        keyName = key.toString();    }    for (NamedVector seedVector : seedVectors) {        double distance = measure.distance(seedVector, valVec);        if (!usesThreshold || distance <= maxDistance) {            StringTuple outKey = new StringTuple();            outKey.add(seedVector.getName());            outKey.add(keyName);            context.write(outKey, new DoubleWritable(distance));        }    }}
0
protected void setup(Context context) throws IOException, InterruptedException
{    super.setup(context);    Configuration conf = context.getConfiguration();    String maxDistanceParam = conf.get(VectorDistanceSimilarityJob.MAX_DISTANCE);    if (maxDistanceParam != null) {        usesThreshold = true;        maxDistance = Double.parseDouble(maxDistanceParam);    }    measure = ClassUtils.instantiateAs(conf.get(VectorDistanceSimilarityJob.DISTANCE_MEASURE_KEY), DistanceMeasure.class);    measure.configure(conf);    seedVectors = SeedVectorUtil.loadSeedVectors(conf);}
0
public static void main(String[] args) throws Exception
{    ToolRunner.run(new Configuration(), new VectorDistanceSimilarityJob(), args);}
0
public int run(String[] args) throws Exception
{    addInputOption();    addOutputOption();    addOption(DefaultOptionCreator.distanceMeasureOption().create());    addOption(SEEDS, "s", "The set of vectors to compute distances against.  Must fit in memory on the mapper");    addOption(MAX_DISTANCE, "mx", "set an upper-bound on distance (double) such that any pair of vectors with a" + " distance greater than this value is ignored in the output. Ignored for non pairwise output!");    addOption(DefaultOptionCreator.overwriteOption().create());    addOption(OUT_TYPE_KEY, "ot", "[pw|v] -- Define the output style: pairwise, the default, (pw) or vector (v).  " + "Pairwise is a tuple of <seed, other, distance>, vector is <other, <Vector of size the number of seeds>>.", "pw");    if (parseArguments(args) == null) {        return -1;    }    Path input = getInputPath();    Path output = getOutputPath();    Path seeds = new Path(getOption(SEEDS));    String measureClass = getOption(DefaultOptionCreator.DISTANCE_MEASURE_OPTION);    if (measureClass == null) {        measureClass = SquaredEuclideanDistanceMeasure.class.getName();    }    if (hasOption(DefaultOptionCreator.OVERWRITE_OPTION)) {        HadoopUtil.delete(getConf(), output);    }    DistanceMeasure measure = ClassUtils.instantiateAs(measureClass, DistanceMeasure.class);    String outType = getOption(OUT_TYPE_KEY, "pw");    Double maxDistance = null;    if ("pw".equals(outType)) {        String maxDistanceArg = getOption(MAX_DISTANCE);        if (maxDistanceArg != null) {            maxDistance = Double.parseDouble(maxDistanceArg);            Preconditions.checkArgument(maxDistance > 0.0d, "value for " + MAX_DISTANCE + " must be greater than zero");        }    }    run(getConf(), input, seeds, output, measure, outType, maxDistance);    return 0;}
0
public static void run(Configuration conf, Path input, Path seeds, Path output, DistanceMeasure measure, String outType) throws IOException, ClassNotFoundException, InterruptedException
{    run(conf, input, seeds, output, measure, outType, null);}
0
public static void run(Configuration conf, Path input, Path seeds, Path output, DistanceMeasure measure, String outType, Double maxDistance) throws IOException, ClassNotFoundException, InterruptedException
{    if (maxDistance != null) {        conf.set(MAX_DISTANCE, String.valueOf(maxDistance));    }    conf.set(DISTANCE_MEASURE_KEY, measure.getClass().getName());    conf.set(SEEDS_PATH_KEY, seeds.toString());    Job job = new Job(conf, "Vector Distance Similarity: seeds: " + seeds + " input: " + input);    job.setInputFormatClass(SequenceFileInputFormat.class);    job.setOutputFormatClass(SequenceFileOutputFormat.class);    if ("pw".equalsIgnoreCase(outType)) {        job.setMapOutputKeyClass(StringTuple.class);        job.setOutputKeyClass(StringTuple.class);        job.setMapOutputValueClass(DoubleWritable.class);        job.setOutputValueClass(DoubleWritable.class);        job.setMapperClass(VectorDistanceMapper.class);    } else if ("v".equalsIgnoreCase(outType)) {        job.setMapOutputKeyClass(Text.class);        job.setOutputKeyClass(Text.class);        job.setMapOutputValueClass(VectorWritable.class);        job.setOutputValueClass(VectorWritable.class);        job.setMapperClass(VectorDistanceInvertedMapper.class);    } else {        throw new IllegalArgumentException("Invalid outType specified: " + outType);    }    job.setNumReduceTasks(0);    FileInputFormat.addInputPath(job, input);    FileOutputFormat.setOutputPath(job, output);    job.setJarByClass(VectorDistanceSimilarityJob.class);    HadoopUtil.delete(conf, output);    if (!job.waitForCompletion(true)) {        throw new IllegalStateException("VectorDistance Similarity failed processing " + seeds);    }}
0
public Vector runJob(Path inputPath, Path tempPath, int numRows, int numCols, Vector b, Preconditioner preconditioner, int maxIterations, double maxError)
{    DistributedRowMatrix matrix = new DistributedRowMatrix(inputPath, tempPath, numRows, numCols);    matrix.setConf(conf);    return solve(matrix, b, preconditioner, maxIterations, maxError);}
0
public Configuration getConf()
{    return conf;}
0
public void setConf(Configuration conf)
{    this.conf = conf;}
0
public int run(String[] strings) throws Exception
{    Path inputPath = new Path(AbstractJob.getOption(parsedArgs, "--input"));    Path outputPath = new Path(AbstractJob.getOption(parsedArgs, "--output"));    Path tempPath = new Path(AbstractJob.getOption(parsedArgs, "--tempDir"));    Path vectorPath = new Path(AbstractJob.getOption(parsedArgs, "--vector"));    int numRows = Integer.parseInt(AbstractJob.getOption(parsedArgs, "--numRows"));    int numCols = Integer.parseInt(AbstractJob.getOption(parsedArgs, "--numCols"));    int maxIterations = parsedArgs.containsKey("--maxIter") ? Integer.parseInt(AbstractJob.getOption(parsedArgs, "--maxIter")) : numCols + 2;    double maxError = parsedArgs.containsKey("--maxError") ? Double.parseDouble(AbstractJob.getOption(parsedArgs, "--maxError")) : ConjugateGradientSolver.DEFAULT_MAX_ERROR;    Vector b = loadInputVector(vectorPath);    Vector x = runJob(inputPath, tempPath, numRows, numCols, b, null, maxIterations, maxError);    saveOutputVector(outputPath, x);    tempPath.getFileSystem(conf).delete(tempPath, true);    return 0;}
0
public DistributedConjugateGradientSolverJob job()
{    return new DistributedConjugateGradientSolverJob();}
0
private Vector loadInputVector(Path path) throws IOException
{    FileSystem fs = path.getFileSystem(conf);    try (SequenceFile.Reader reader = new SequenceFile.Reader(fs, path, conf)) {        VectorWritable value = new VectorWritable();        if (!reader.next(new IntWritable(), value)) {            throw new IOException("Input vector file is empty.");        }        return value.get();    }}
0
private void saveOutputVector(Path path, Vector v) throws IOException
{    FileSystem fs = path.getFileSystem(conf);    try (SequenceFile.Writer writer = new SequenceFile.Writer(fs, conf, path, IntWritable.class, VectorWritable.class)) {        writer.append(new IntWritable(0), new VectorWritable(v));    }}
0
public void setConf(Configuration conf)
{    DistributedConjugateGradientSolver.this.setConf(conf);}
0
public Configuration getConf()
{    return DistributedConjugateGradientSolver.this.getConf();}
0
public int run(String[] args) throws Exception
{    addInputOption();    addOutputOption();    addOption("numRows", "nr", "Number of rows in the input matrix", true);    addOption("numCols", "nc", "Number of columns in the input matrix", true);    addOption("vector", "b", "Vector to solve against", true);    addOption("lambda", "l", "Scalar in A + lambda * I [default = 0]", "0.0");    addOption("symmetric", "sym", "Is the input matrix square and symmetric?", "true");    addOption("maxIter", "x", "Maximum number of iterations to run");    addOption("maxError", "err", "Maximum residual error to allow before stopping");    DistributedConjugateGradientSolver.this.parsedArgs = parseArguments(args);    if (DistributedConjugateGradientSolver.this.parsedArgs == null) {        return -1;    } else {        Configuration conf = getConf();        if (conf == null) {            conf = new Configuration();        }        DistributedConjugateGradientSolver.this.setConf(conf);        return DistributedConjugateGradientSolver.this.run(args);    }}
0
public static void main(String[] args) throws Exception
{    ToolRunner.run(new DistributedConjugateGradientSolver().job(), args);}
0
public static double variance(Path input, Path output, Configuration baseConf) throws IOException, InterruptedException, ClassNotFoundException
{    VarianceTotals varianceTotals = computeVarianceTotals(input, output, baseConf);    return varianceTotals.computeVariance();}
0
public static double varianceForGivenMean(Path input, Path output, double mean, Configuration baseConf) throws IOException, InterruptedException, ClassNotFoundException
{    VarianceTotals varianceTotals = computeVarianceTotals(input, output, baseConf);    return varianceTotals.computeVarianceForGivenMean(mean);}
0
private static VarianceTotals computeVarianceTotals(Path input, Path output, Configuration baseConf) throws IOException, InterruptedException, ClassNotFoundException
{    Configuration conf = new Configuration(baseConf);    conf.set("io.serializations", "org.apache.hadoop.io.serializer.JavaSerialization," + "org.apache.hadoop.io.serializer.WritableSerialization");    Job job = HadoopUtil.prepareJob(input, output, SequenceFileInputFormat.class, StandardDeviationCalculatorMapper.class, IntWritable.class, DoubleWritable.class, StandardDeviationCalculatorReducer.class, IntWritable.class, DoubleWritable.class, SequenceFileOutputFormat.class, conf);    HadoopUtil.delete(conf, output);    job.setCombinerClass(StandardDeviationCalculatorReducer.class);    boolean succeeded = job.waitForCompletion(true);    if (!succeeded) {        throw new IllegalStateException("Job failed!");    }        Path filesPattern = new Path(output, "part-*");    double sumOfSquares = 0;    double sum = 0;    double totalCount = 0;    for (Pair<Writable, Writable> record : new SequenceFileDirIterable<>(filesPattern, PathType.GLOB, null, null, true, conf)) {        int key = ((IntWritable) record.getFirst()).get();        if (key == StandardDeviationCalculatorMapper.SUM_OF_SQUARES.get()) {            sumOfSquares += ((DoubleWritable) record.getSecond()).get();        } else if (key == StandardDeviationCalculatorMapper.TOTAL_COUNT.get()) {            totalCount += ((DoubleWritable) record.getSecond()).get();        } else if (key == StandardDeviationCalculatorMapper.SUM.get()) {            sum += ((DoubleWritable) record.getSecond()).get();        }    }    VarianceTotals varianceTotals = new VarianceTotals();    varianceTotals.setSum(sum);    varianceTotals.setSumOfSquares(sumOfSquares);    varianceTotals.setTotalCount(totalCount);    return varianceTotals;}
0
public static double stdDev(Path input, Path output, Configuration baseConf) throws IOException, InterruptedException, ClassNotFoundException
{    return Math.sqrt(variance(input, output, baseConf));}
0
public static double stdDevForGivenMean(Path input, Path output, double mean, Configuration baseConf) throws IOException, InterruptedException, ClassNotFoundException
{    return Math.sqrt(varianceForGivenMean(input, output, mean, baseConf));}
0
protected void map(IntWritable key, Writable value, Context context) throws IOException, InterruptedException
{    if (key.get() == -1) {        return;    }        double df = Double.NaN;    if (value instanceof LongWritable) {        df = ((LongWritable) value).get();    } else if (value instanceof DoubleWritable) {        df = ((DoubleWritable) value).get();    }    if (!Double.isNaN(df)) {                context.write(SUM_OF_SQUARES, new DoubleWritable(df * df));        context.write(SUM, new DoubleWritable(df));                context.write(TOTAL_COUNT, new DoubleWritable(1));    }}
0
protected void reduce(IntWritable key, Iterable<DoubleWritable> values, Context context) throws IOException, InterruptedException
{    double sum = 0.0;    for (DoubleWritable value : values) {        sum += value.get();    }    context.write(key, new DoubleWritable(sum));}
0
public double getSumOfSquares()
{    return sumOfSquares;}
0
public void setSumOfSquares(double sumOfSquares)
{    this.sumOfSquares = sumOfSquares;}
0
public double getSum()
{    return sum;}
0
public void setSum(double sum)
{    this.sum = sum;}
0
public double getTotalCount()
{    return totalCount;}
0
public void setTotalCount(double totalCount)
{    this.totalCount = totalCount;}
0
public double computeMean()
{    return sum / totalCount;}
0
public double computeVariance()
{    return ((totalCount * sumOfSquares) - (sum * sum)) / (totalCount * (totalCount - 1.0));}
0
public double computeVarianceForGivenMean(double mean)
{    return (sumOfSquares - totalCount * mean * mean) / (totalCount - 1.0);}
0
protected void map(Writable key, VectorWritable value, Context context) throws IOException, InterruptedException
{    Vector vec = value.get();    int vecSize = vec.size();    if (aCols == null) {        aCols = new Vector[vecSize];    } else if (aCols.length < vecSize) {        aCols = Arrays.copyOf(aCols, vecSize);    }    if (vec.isDense()) {        for (int i = 0; i < vecSize; i++) {            extendAColIfNeeded(i, aRowCount + 1);            aCols[i].setQuick(aRowCount, vec.getQuick(i));        }    } else if (vec.size() > 0) {        for (Vector.Element vecEl : vec.nonZeroes()) {            int i = vecEl.index();            extendAColIfNeeded(i, aRowCount + 1);            aCols[i].setQuick(aRowCount, vecEl.get());        }    }    aRowCount++;}
0
private void extendAColIfNeeded(int col, int rowCount)
{    if (aCols[col] == null) {        aCols[col] = new SequentialAccessSparseVector(rowCount < blockHeight ? blockHeight : rowCount, 1);    } else if (aCols[col].size() < rowCount) {        Vector newVec = new SequentialAccessSparseVector(rowCount + blockHeight, aCols[col].getNumNondefaultElements() << 1);        newVec.viewPart(0, aCols[col].size()).assign(aCols[col]);        aCols[col] = newVec;    }}
0
protected void cleanup(Context context) throws IOException, InterruptedException
{    try {        yiCols = new double[kp][];        for (int i = 0; i < kp; i++) {            yiCols[i] = new double[Math.min(aRowCount, blockHeight)];        }        int numPasses = (aRowCount - 1) / blockHeight + 1;        String propBtPathStr = context.getConfiguration().get(PROP_BT_PATH);        Validate.notNull(propBtPathStr, "Bt input is not set");        Path btPath = new Path(propBtPathStr);        DenseBlockWritable dbw = new DenseBlockWritable();        /*         * so it turns out that it may be much more efficient to do a few         * independent passes over Bt accumulating the entire block in memory         * than pass huge amount of blocks out to combiner. so we aim of course         * to fit entire s x (k+p) dense block in memory where s is the number         * of A rows in this split. If A is much sparser than (k+p) avg # of         * elements per row then the block may exceed the split size. if this         * happens, and if the given blockHeight is not high enough to         * accomodate this (because of memory constraints), then we start         * splitting s into several passes. since computation is cpu-bound         * anyway, it should be o.k. for supersparse inputs. (as ok it can be         * that projection is thicker than the original anyway, why would one         * use that many k+p then).         */        int lastRowIndex = -1;        for (int pass = 0; pass < numPasses; pass++) {            if (distributedBt) {                btInput = new SequenceFileDirIterator<>(btLocalPath, true, localFsConfig);            } else {                btInput = new SequenceFileDirIterator<>(btPath, PathType.GLOB, null, null, true, context.getConfiguration());            }            closeables.addFirst(btInput);            Validate.isTrue(btInput.hasNext(), "Empty B' input!");            int aRowBegin = pass * blockHeight;            int bh = Math.min(blockHeight, aRowCount - aRowBegin);            /*           * check if we need to trim block allocation           */            if (pass > 0) {                if (bh == blockHeight) {                    for (int i = 0; i < kp; i++) {                        Arrays.fill(yiCols[i], 0.0);                    }                } else {                    for (int i = 0; i < kp; i++) {                        yiCols[i] = null;                    }                    for (int i = 0; i < kp; i++) {                        yiCols[i] = new double[bh];                    }                }            }            while (btInput.hasNext()) {                Pair<IntWritable, VectorWritable> btRec = btInput.next();                int btIndex = btRec.getFirst().get();                Vector btVec = btRec.getSecond().get();                Vector aCol;                if (btIndex > aCols.length || (aCol = aCols[btIndex]) == null || aCol.size() == 0) {                    /* 100% zero A column in the block, skip it as sparse */                    continue;                }                int j = -1;                for (Vector.Element aEl : aCol.nonZeroes()) {                    j = aEl.index();                    /*               * now we compute only swathes between aRowBegin..aRowBegin+bh               * exclusive. it seems like a deficiency but in fact i think it               * will balance itself out: either A is dense and then we               * shouldn't have more than one pass and therefore filter               * conditions will never kick in. Or, the only situation where we               * can't fit Y_i block in memory is when A input is much sparser               * than k+p per row. But if this is the case, then we'd be looking               * at very few elements without engaging them in any operations so               * even then it should be ok.               */                    if (j < aRowBegin) {                        continue;                    }                    if (j >= aRowBegin + bh) {                        break;                    }                    /*               * assume btVec is dense               */                    if (xi != null) {                        /*                 * MAHOUT-817: PCA correction for B'. I rewrite the whole                 * computation loop so i don't have to check if PCA correction                 * is needed at individual element level. It looks bulkier this                 * way but perhaps less wasteful on cpu.                 */                        for (int s = 0; s < kp; s++) {                                                        double xii = xi.size() > btIndex ? xi.get(btIndex) : 0.0;                            yiCols[s][j - aRowBegin] += aEl.get() * (btVec.getQuick(s) - xii * sq.get(s));                        }                    } else {                        /*                 * no PCA correction                 */                        for (int s = 0; s < kp; s++) {                            yiCols[s][j - aRowBegin] += aEl.get() * btVec.getQuick(s);                        }                    }                }                if (lastRowIndex < j) {                    lastRowIndex = j;                }            }            /*           * so now we have stuff in yi           */            dbw.setBlock(yiCols);            outKey.setTaskItemOrdinal(pass);            context.write(outKey, dbw);            closeables.remove(btInput);            btInput.close();        }    } finally {        IOUtils.close(closeables);    }}
0
protected void setup(Context context) throws IOException, InterruptedException
{    Configuration conf = context.getConfiguration();    int k = Integer.parseInt(conf.get(QRFirstStep.PROP_K));    int p = Integer.parseInt(conf.get(QRFirstStep.PROP_P));    kp = k + p;    outKey = new SplitPartitionedWritable(context);    blockHeight = conf.getInt(BtJob.PROP_OUTER_PROD_BLOCK_HEIGHT, -1);    distributedBt = conf.get(PROP_BT_BROADCAST) != null;    if (distributedBt) {        btLocalPath = HadoopUtil.getCachedFiles(conf);        localFsConfig = new Configuration();        localFsConfig.set("fs.default.name", "file:///");    }    /*       * PCA -related corrections (MAHOUT-817)       */    String xiPathStr = conf.get(PROP_XI_PATH);    if (xiPathStr != null) {        xi = SSVDHelper.loadAndSumUpVectors(new Path(xiPathStr), conf);        sq = SSVDHelper.loadAndSumUpVectors(new Path(conf.get(PROP_SQ_PATH)), conf);    }}
0
protected void setup(Context context) throws IOException, InterruptedException
{    Configuration conf = context.getConfiguration();    blockHeight = conf.getInt(BtJob.PROP_OUTER_PROD_BLOCK_HEIGHT, -1);    String sbPathStr = conf.get(PROP_SB_PATH);    /*       * PCA -related corrections (MAHOUT-817)       */    if (sbPathStr != null) {        sb = SSVDHelper.loadAndSumUpVectors(new Path(sbPathStr), conf);    }}
0
protected void setupBlock(Context context, SplitPartitionedWritable spw) throws InterruptedException, IOException
{    IOUtils.close(closeables);    qhatCollector = createOutputCollector(QJob.OUTPUT_QHAT, spw, context, DenseBlockWritable.class);    rhatCollector = createOutputCollector(QJob.OUTPUT_RHAT, spw, context, VectorWritable.class);    qr = new QRFirstStep(context.getConfiguration(), qhatCollector, rhatCollector);    closeables.addFirst(qr);    lastTaskId = spw.getTaskId();}
0
protected void reduce(SplitPartitionedWritable key, Iterable<DenseBlockWritable> values, Context context) throws IOException, InterruptedException
{    if (key.getTaskId() != lastTaskId) {        setupBlock(context, key);    }    Iterator<DenseBlockWritable> iter = values.iterator();    DenseBlockWritable dbw = iter.next();    double[][] yiCols = dbw.getBlock();    if (iter.hasNext()) {        throw new IOException("Unexpected extra Y_i block in reducer input.");    }    long blockBase = key.getTaskItemOrdinal() * blockHeight;    int bh = yiCols[0].length;    if (yiRow == null) {        yiRow = new DenseVector(yiCols.length);    }    for (int k = 0; k < bh; k++) {        for (int j = 0; j < yiCols.length; j++) {            yiRow.setQuick(j, yiCols[j][k]);        }        key.setTaskItemOrdinal(blockBase + k);                if (sb != null) {            yiRow.assign(sb, Functions.MINUS);        }        qr.collect(key, yiRow);    }}
0
private Path getSplitFilePath(String name, SplitPartitionedWritable spw, Context context) throws InterruptedException, IOException
{    String uniqueFileName = FileOutputFormat.getUniqueFile(context, name, "");    uniqueFileName = uniqueFileName.replaceFirst("-r-", "-m-");    uniqueFileName = uniqueFileName.replaceFirst("\\d+$", Matcher.quoteReplacement(NUMBER_FORMAT.format(spw.getTaskId())));    return new Path(FileOutputFormat.getWorkOutputPath(context), uniqueFileName);}
0
private OutputCollector<K, V> createOutputCollector(String name, final SplitPartitionedWritable spw, Context ctx, Class<V> valueClass) throws IOException, InterruptedException
{    Path outputPath = getSplitFilePath(name, spw, ctx);    final SequenceFile.Writer w = SequenceFile.createWriter(FileSystem.get(outputPath.toUri(), ctx.getConfiguration()), ctx.getConfiguration(), outputPath, SplitPartitionedWritable.class, valueClass);    closeables.addFirst(w);    return new OutputCollector<K, V>() {        @Override        public void collect(K key, V val) throws IOException {            w.append(spw, val);        }    };}
0
public void collect(K key, V val) throws IOException
{    w.append(spw, val);}
0
protected void cleanup(Context context) throws IOException, InterruptedException
{    IOUtils.close(closeables);}
0
public static void run(Configuration conf, Path[] inputAPaths, Path inputBtGlob, Path xiPath, Path sqPath, Path sbPath, Path outputPath, int aBlockRows, int minSplitSize, int k, int p, int outerProdBlockHeight, int numReduceTasks, boolean broadcastBInput) throws ClassNotFoundException, InterruptedException, IOException
{    JobConf oldApiJob = new JobConf(conf);    Job job = new Job(oldApiJob);    job.setJobName("ABt-job");    job.setJarByClass(ABtDenseOutJob.class);    job.setInputFormatClass(SequenceFileInputFormat.class);    FileInputFormat.setInputPaths(job, inputAPaths);    if (minSplitSize > 0) {        FileInputFormat.setMinInputSplitSize(job, minSplitSize);    }    FileOutputFormat.setOutputPath(job, outputPath);    SequenceFileOutputFormat.setOutputCompressionType(job, CompressionType.BLOCK);    job.setMapOutputKeyClass(SplitPartitionedWritable.class);    job.setMapOutputValueClass(DenseBlockWritable.class);    job.setOutputKeyClass(SplitPartitionedWritable.class);    job.setOutputValueClass(VectorWritable.class);    job.setMapperClass(ABtMapper.class);    job.setReducerClass(QRReducer.class);    job.getConfiguration().setInt(QJob.PROP_AROWBLOCK_SIZE, aBlockRows);    job.getConfiguration().setInt(BtJob.PROP_OUTER_PROD_BLOCK_HEIGHT, outerProdBlockHeight);    job.getConfiguration().setInt(QRFirstStep.PROP_K, k);    job.getConfiguration().setInt(QRFirstStep.PROP_P, p);    job.getConfiguration().set(PROP_BT_PATH, inputBtGlob.toString());    /*     * PCA-related options, MAHOUT-817     */    if (xiPath != null) {        job.getConfiguration().set(PROP_XI_PATH, xiPath.toString());        job.getConfiguration().set(PROP_SB_PATH, sbPath.toString());        job.getConfiguration().set(PROP_SQ_PATH, sqPath.toString());    }    job.setNumReduceTasks(numReduceTasks);        if (broadcastBInput) {        job.getConfiguration().set(PROP_BT_BROADCAST, "y");        FileSystem fs = FileSystem.get(inputBtGlob.toUri(), conf);        FileStatus[] fstats = fs.globStatus(inputBtGlob);        if (fstats != null) {            for (FileStatus fstat : fstats) {                /*           * new api is not enabled yet in our dependencies at this time, still           * using deprecated one           */                DistributedCache.addCacheFile(fstat.getPath().toUri(), job.getConfiguration());            }        }    }    job.submit();    job.waitForCompletion(false);    if (!job.isSuccessful()) {        throw new IOException("ABt job unsuccessful.");    }}
0
protected void map(Writable key, VectorWritable value, Context context) throws IOException, InterruptedException
{    Vector vec = value.get();    int vecSize = vec.size();    if (aCols == null) {        aCols = new Vector[vecSize];    } else if (aCols.length < vecSize) {        aCols = Arrays.copyOf(aCols, vecSize);    }    if (vec.isDense()) {        for (int i = 0; i < vecSize; i++) {            extendAColIfNeeded(i, aRowCount + 1);            aCols[i].setQuick(aRowCount, vec.getQuick(i));        }    } else {        for (Vector.Element vecEl : vec.nonZeroes()) {            int i = vecEl.index();            extendAColIfNeeded(i, aRowCount + 1);            aCols[i].setQuick(aRowCount, vecEl.get());        }    }    aRowCount++;}
0
private void extendAColIfNeeded(int col, int rowCount)
{    if (aCols[col] == null) {        aCols[col] = new SequentialAccessSparseVector(rowCount < 10000 ? 10000 : rowCount, 1);    } else if (aCols[col].size() < rowCount) {        Vector newVec = new SequentialAccessSparseVector(rowCount << 1, aCols[col].getNumNondefaultElements() << 1);        newVec.viewPart(0, aCols[col].size()).assign(aCols[col]);        aCols[col] = newVec;    }}
0
protected void cleanup(Context context) throws IOException, InterruptedException
{    try {                int lastRowIndex = -1;        while (btInput.hasNext()) {            Pair<IntWritable, VectorWritable> btRec = btInput.next();            int btIndex = btRec.getFirst().get();            Vector btVec = btRec.getSecond().get();            Vector aCol;            if (btIndex > aCols.length || (aCol = aCols[btIndex]) == null) {                continue;            }            int j = -1;            for (Vector.Element aEl : aCol.nonZeroes()) {                j = aEl.index();                                                                                yiCollector.collect((long) j, btVec.times(aEl.get()));            }            if (lastRowIndex < j) {                lastRowIndex = j;            }        }        aCols = null;                                        Vector yDummy = new SequentialAccessSparseVector(kp);                for (lastRowIndex += 1; lastRowIndex < aRowCount; lastRowIndex++) {                                    yiCollector.collect((long) lastRowIndex, yDummy);        }    } finally {        IOUtils.close(closeables);    }}
0
protected void setup(final Context context) throws IOException, InterruptedException
{    int k = Integer.parseInt(context.getConfiguration().get(QRFirstStep.PROP_K));    int p = Integer.parseInt(context.getConfiguration().get(QRFirstStep.PROP_P));    kp = k + p;    outKey = new SplitPartitionedWritable(context);    String propBtPathStr = context.getConfiguration().get(PROP_BT_PATH);    Validate.notNull(propBtPathStr, "Bt input is not set");    Path btPath = new Path(propBtPathStr);    boolean distributedBt = context.getConfiguration().get(PROP_BT_BROADCAST) != null;    if (distributedBt) {        Path[] btFiles = HadoopUtil.getCachedFiles(context.getConfiguration());                        StringBuilder btLocalPath = new StringBuilder();        for (Path btFile : btFiles) {            if (btLocalPath.length() > 0) {                btLocalPath.append(Path.SEPARATOR_CHAR);            }            btLocalPath.append(btFile);        }        btInput = new SequenceFileDirIterator<>(new Path(btLocalPath.toString()), PathType.LIST, null, null, true, context.getConfiguration());    } else {        btInput = new SequenceFileDirIterator<>(btPath, PathType.GLOB, null, null, true, context.getConfiguration());    }        closeables.addFirst(btInput);    OutputCollector<LongWritable, SparseRowBlockWritable> yiBlockCollector = new OutputCollector<LongWritable, SparseRowBlockWritable>() {        @Override        public void collect(LongWritable blockKey, SparseRowBlockWritable block) throws IOException {            outKey.setTaskItemOrdinal((int) blockKey.get());            try {                context.write(outKey, block);            } catch (InterruptedException exc) {                throw new IOException("Interrupted", exc);            }        }    };    blockHeight = context.getConfiguration().getInt(BtJob.PROP_OUTER_PROD_BLOCK_HEIGHT, -1);    yiCollector = new SparseRowBlockAccumulator(blockHeight, yiBlockCollector);    closeables.addFirst(yiCollector);}
0
public void collect(LongWritable blockKey, SparseRowBlockWritable block) throws IOException
{    outKey.setTaskItemOrdinal((int) blockKey.get());    try {        context.write(outKey, block);    } catch (InterruptedException exc) {        throw new IOException("Interrupted", exc);    }}
0
protected void setup(Context context) throws IOException, InterruptedException
{    blockHeight = context.getConfiguration().getInt(BtJob.PROP_OUTER_PROD_BLOCK_HEIGHT, -1);}
0
protected void setupBlock(Context context, SplitPartitionedWritable spw) throws InterruptedException, IOException
{    IOUtils.close(closeables);    qhatCollector = createOutputCollector(QJob.OUTPUT_QHAT, spw, context, DenseBlockWritable.class);    rhatCollector = createOutputCollector(QJob.OUTPUT_RHAT, spw, context, VectorWritable.class);    qr = new QRFirstStep(context.getConfiguration(), qhatCollector, rhatCollector);    closeables.addFirst(qr);    lastTaskId = spw.getTaskId();}
0
protected void reduce(SplitPartitionedWritable key, Iterable<SparseRowBlockWritable> values, Context context) throws IOException, InterruptedException
{    accum.clear();    for (SparseRowBlockWritable bw : values) {        accum.plusBlock(bw);    }    if (key.getTaskId() != lastTaskId) {        setupBlock(context, key);    }    long blockBase = key.getTaskItemOrdinal() * blockHeight;    for (int k = 0; k < accum.getNumRows(); k++) {        Vector yiRow = accum.getRows()[k];        key.setTaskItemOrdinal(blockBase + accum.getRowIndices()[k]);        qr.collect(key, yiRow);    }}
0
private Path getSplitFilePath(String name, SplitPartitionedWritable spw, Context context) throws InterruptedException, IOException
{    String uniqueFileName = FileOutputFormat.getUniqueFile(context, name, "");    uniqueFileName = uniqueFileName.replaceFirst("-r-", "-m-");    uniqueFileName = uniqueFileName.replaceFirst("\\d+$", Matcher.quoteReplacement(NUMBER_FORMAT.format(spw.getTaskId())));    return new Path(FileOutputFormat.getWorkOutputPath(context), uniqueFileName);}
0
private OutputCollector<K, V> createOutputCollector(String name, final SplitPartitionedWritable spw, Context ctx, Class<V> valueClass) throws IOException, InterruptedException
{    Path outputPath = getSplitFilePath(name, spw, ctx);    final SequenceFile.Writer w = SequenceFile.createWriter(FileSystem.get(outputPath.toUri(), ctx.getConfiguration()), ctx.getConfiguration(), outputPath, SplitPartitionedWritable.class, valueClass);    closeables.addFirst(w);    return new OutputCollector<K, V>() {        @Override        public void collect(K key, V val) throws IOException {            w.append(spw, val);        }    };}
0
public void collect(K key, V val) throws IOException
{    w.append(spw, val);}
0
protected void cleanup(Context context) throws IOException, InterruptedException
{    IOUtils.close(closeables);}
0
public static void run(Configuration conf, Path[] inputAPaths, Path inputBtGlob, Path outputPath, int aBlockRows, int minSplitSize, int k, int p, int outerProdBlockHeight, int numReduceTasks, boolean broadcastBInput) throws ClassNotFoundException, InterruptedException, IOException
{    JobConf oldApiJob = new JobConf(conf);                                                        Job job = new Job(oldApiJob);    job.setJobName("ABt-job");    job.setJarByClass(ABtJob.class);    job.setInputFormatClass(SequenceFileInputFormat.class);    FileInputFormat.setInputPaths(job, inputAPaths);    if (minSplitSize > 0) {        FileInputFormat.setMinInputSplitSize(job, minSplitSize);    }    FileOutputFormat.setOutputPath(job, outputPath);    SequenceFileOutputFormat.setOutputCompressionType(job, CompressionType.BLOCK);    job.setMapOutputKeyClass(SplitPartitionedWritable.class);    job.setMapOutputValueClass(SparseRowBlockWritable.class);    job.setOutputKeyClass(SplitPartitionedWritable.class);    job.setOutputValueClass(VectorWritable.class);    job.setMapperClass(ABtMapper.class);    job.setCombinerClass(BtJob.OuterProductCombiner.class);    job.setReducerClass(QRReducer.class);    job.getConfiguration().setInt(QJob.PROP_AROWBLOCK_SIZE, aBlockRows);    job.getConfiguration().setInt(BtJob.PROP_OUTER_PROD_BLOCK_HEIGHT, outerProdBlockHeight);    job.getConfiguration().setInt(QRFirstStep.PROP_K, k);    job.getConfiguration().setInt(QRFirstStep.PROP_P, p);    job.getConfiguration().set(PROP_BT_PATH, inputBtGlob.toString());            job.setNumReduceTasks(numReduceTasks);        if (broadcastBInput) {        job.getConfiguration().set(PROP_BT_BROADCAST, "y");        FileSystem fs = FileSystem.get(inputBtGlob.toUri(), conf);        FileStatus[] fstats = fs.globStatus(inputBtGlob);        if (fstats != null) {            for (FileStatus fstat : fstats) {                /*           * new api is not enabled yet in our dependencies at this time, still           * using deprecated one           */                DistributedCache.addCacheFile(fstat.getPath().toUri(), conf);            }        }    }    job.submit();    job.waitForCompletion(false);    if (!job.isSuccessful()) {        throw new IOException("ABt job unsuccessful.");    }}
0
protected void map(Writable key, VectorWritable value, Context context) throws IOException, InterruptedException
{    mapContext = context;        Vector aRow = value.get();    Vector qRow = qr.next();    int kp = qRow.size();        outputQRow(key, qRow, aRow);        if (computeSq) {        if (sqAccum == null) {            sqAccum = new DenseVector(kp);        }        sqAccum.assign(qRow, Functions.PLUS);    }    if (btRow == null) {        btRow = new DenseVector(kp);    }    if (!aRow.isDense()) {        for (Vector.Element el : aRow.nonZeroes()) {            double mul = el.get();            for (int j = 0; j < kp; j++) {                btRow.setQuick(j, mul * qRow.getQuick(j));            }            btCollector.collect((long) el.index(), btRow);        }    } else {        int n = aRow.size();        for (int i = 0; i < n; i++) {            double mul = aRow.getQuick(i);            for (int j = 0; j < kp; j++) {                btRow.setQuick(j, mul * qRow.getQuick(j));            }            btCollector.collect((long) i, btRow);        }    }}
0
protected void setup(Context context) throws IOException, InterruptedException
{    super.setup(context);    Configuration conf = context.getConfiguration();    Path qJobPath = new Path(conf.get(PROP_QJOB_PATH));    /*       * actually this is kind of dangerous because this routine thinks we need       * to create file name for our current job and this will use -m- so it's       * just serendipity we are calling it from the mapper too as the QJob did.       */    Path qInputPath = new Path(qJobPath, FileOutputFormat.getUniqueFile(context, QJob.OUTPUT_QHAT, ""));    blockNum = context.getTaskAttemptID().getTaskID().getId();    SequenceFileValueIterator<DenseBlockWritable> qhatInput = new SequenceFileValueIterator<>(qInputPath, true, conf);    closeables.addFirst(qhatInput);    /*       * read all r files _in order of task ids_, i.e. partitions (aka group       * nums).       *       * Note: if broadcast option is used, this comes from distributed cache       * files rather than hdfs path.       */    SequenceFileDirValueIterator<VectorWritable> rhatInput;    boolean distributedRHat = conf.get(PROP_RHAT_BROADCAST) != null;    if (distributedRHat) {        Path[] rFiles = HadoopUtil.getCachedFiles(conf);        Validate.notNull(rFiles, "no RHat files in distributed cache job definition");                Configuration lconf = new Configuration();        lconf.set("fs.default.name", "file:///");        rhatInput = new SequenceFileDirValueIterator<>(rFiles, SSVDHelper.PARTITION_COMPARATOR, true, lconf);    } else {        Path rPath = new Path(qJobPath, QJob.OUTPUT_RHAT + "-*");        rhatInput = new SequenceFileDirValueIterator<>(rPath, PathType.GLOB, null, SSVDHelper.PARTITION_COMPARATOR, true, conf);    }    Validate.isTrue(rhatInput.hasNext(), "Empty R-hat input!");    closeables.addFirst(rhatInput);    outputs = new MultipleOutputs(new JobConf(conf));    closeables.addFirst(new IOUtils.MultipleOutputsCloseableAdapter(outputs));    qr = new QRLastStep(qhatInput, rhatInput, blockNum);    closeables.addFirst(qr);    /*       * it's so happens that current QRLastStep's implementation preloads R       * sequence into memory in the constructor so it's ok to close rhat input       * now.       */    if (!rhatInput.hasNext()) {        closeables.remove(rhatInput);        rhatInput.close();    }    OutputCollector<LongWritable, SparseRowBlockWritable> btBlockCollector = new OutputCollector<LongWritable, SparseRowBlockWritable>() {        @Override        public void collect(LongWritable blockKey, SparseRowBlockWritable block) throws IOException {            try {                mapContext.write(blockKey, block);            } catch (InterruptedException exc) {                throw new IOException("Interrupted.", exc);            }        }    };    btCollector = new SparseRowBlockAccumulator(conf.getInt(PROP_OUTER_PROD_BLOCK_HEIGHT, -1), btBlockCollector);    closeables.addFirst(btCollector);        computeSq = conf.get(PROP_XI_PATH) != null;        nv = conf.getBoolean(PROP_NV, false);}
0
public void collect(LongWritable blockKey, SparseRowBlockWritable block) throws IOException
{    try {        mapContext.write(blockKey, block);    } catch (InterruptedException exc) {        throw new IOException("Interrupted.", exc);    }}
0
protected void cleanup(Context context) throws IOException, InterruptedException
{    try {        if (sqAccum != null) {            /*           * hack: we will output sq partial sums with index -1 for summation.           */            SparseRowBlockWritable sbrw = new SparseRowBlockWritable(1);            sbrw.plusRow(0, sqAccum);            LongWritable lw = new LongWritable(-1);            context.write(lw, sbrw);        }    } finally {        IOUtils.close(closeables);    }}
0
private void outputQRow(Writable key, Vector qRow, Vector aRow) throws IOException
{    if (nv && (aRow instanceof NamedVector)) {        qRowValue.set(new NamedVector(qRow, ((NamedVector) aRow).getName()));    } else {        qRowValue.set(qRow);    }    outputs.getCollector(OUTPUT_Q, null).collect(key, qRowValue);}
0
protected void setup(Context context) throws IOException, InterruptedException
{    blockHeight = context.getConfiguration().getInt(PROP_OUTER_PROD_BLOCK_HEIGHT, -1);}
0
protected void reduce(Writable key, Iterable<SparseRowBlockWritable> values, Context context) throws IOException, InterruptedException
{    for (SparseRowBlockWritable bw : values) {        accum.plusBlock(bw);    }    context.write(key, accum);    accum.clear();}
0
protected void cleanup(Context context) throws IOException, InterruptedException
{    IOUtils.close(closeables);}
0
protected void setup(Context context) throws IOException, InterruptedException
{    Configuration conf = context.getConfiguration();    blockHeight = conf.getInt(PROP_OUTER_PROD_BLOCK_HEIGHT, -1);    outputBBt = conf.getBoolean(PROP_OUPTUT_BBT_PRODUCTS, false);    if (outputBBt) {        int k = conf.getInt(QJob.PROP_K, -1);        int p = conf.getInt(QJob.PROP_P, -1);        Validate.isTrue(k > 0, "invalid k parameter");        Validate.isTrue(p >= 0, "invalid p parameter");        mBBt = new UpperTriangular(k + p);    }    String xiPathStr = conf.get(PROP_XI_PATH);    if (xiPathStr != null) {        xi = SSVDHelper.loadAndSumUpVectors(new Path(xiPathStr), conf);        if (xi == null) {            throw new IOException(String.format("unable to load mean path xi from %s.", xiPathStr));        }    }    if (outputBBt || xi != null) {        outputs = new MultipleOutputs(new JobConf(conf));        closeables.addFirst(new IOUtils.MultipleOutputsCloseableAdapter(outputs));    }}
0
protected void reduce(LongWritable key, Iterable<SparseRowBlockWritable> values, Context context) throws IOException, InterruptedException
{    accum.clear();    for (SparseRowBlockWritable bw : values) {        accum.plusBlock(bw);    }        if (key.get() == -1L) {        Vector sq = accum.getRows()[0];        @SuppressWarnings("unchecked")        OutputCollector<IntWritable, VectorWritable> sqOut = outputs.getCollector(OUTPUT_SQ, null);        sqOut.collect(new IntWritable(0), new VectorWritable(sq));        return;    }    for (int k = 0; k < accum.getNumRows(); k++) {        Vector btRow = accum.getRows()[k];        btKey.set((int) (key.get() * blockHeight + accum.getRowIndices()[k]));        btValue.set(btRow);        context.write(btKey, btValue);        if (outputBBt) {            int kp = mBBt.numRows();                        for (int i = 0; i < kp; i++) {                double vi = btRow.get(i);                if (vi != 0.0) {                    for (int j = i; j < kp; j++) {                        double vj = btRow.get(j);                        if (vj != 0.0) {                            mBBt.setQuick(i, j, mBBt.getQuick(i, j) + vi * vj);                        }                    }                }            }        }                if (xi != null) {                        int btIndex = btKey.get();            double xii = xi.size() > btIndex ? xi.getQuick(btIndex) : 0.0;                        pmult.setMultiplicator(xii);            if (sbAccum == null) {                sbAccum = new DenseVector(btRow.size());            }            sbAccum.assign(btRow, pmult);        }    }}
0
protected void cleanup(Context context) throws IOException, InterruptedException
{        try {        if (outputBBt) {            @SuppressWarnings("unchecked")            OutputCollector<Writable, Writable> collector = outputs.getCollector(OUTPUT_BBT, null);            collector.collect(new IntWritable(), new VectorWritable(new DenseVector(mBBt.getData())));        }                if (sbAccum != null) {            @SuppressWarnings("unchecked")            OutputCollector<IntWritable, VectorWritable> collector = outputs.getCollector(OUTPUT_SB, null);            collector.collect(new IntWritable(), new VectorWritable(sbAccum));        }    } finally {        IOUtils.close(closeables);    }}
0
public static void run(Configuration conf, Path[] inputPathA, Path inputPathQJob, Path xiPath, Path outputPath, int minSplitSize, int k, int p, int btBlockHeight, int numReduceTasks, boolean broadcast, Class<? extends Writable> labelClass, boolean outputBBtProducts) throws ClassNotFoundException, InterruptedException, IOException
{    JobConf oldApiJob = new JobConf(conf);    MultipleOutputs.addNamedOutput(oldApiJob, OUTPUT_Q, org.apache.hadoop.mapred.SequenceFileOutputFormat.class, labelClass, VectorWritable.class);    if (outputBBtProducts) {        MultipleOutputs.addNamedOutput(oldApiJob, OUTPUT_BBT, org.apache.hadoop.mapred.SequenceFileOutputFormat.class, IntWritable.class, VectorWritable.class);        /*       * MAHOUT-1067: if we are asked to output BBT products then named vector       * names should be propagated to Q too so that UJob could pick them up       * from there.       */        oldApiJob.setBoolean(PROP_NV, true);    }    if (xiPath != null) {                MultipleOutputs.addNamedOutput(oldApiJob, OUTPUT_SQ, org.apache.hadoop.mapred.SequenceFileOutputFormat.class, IntWritable.class, VectorWritable.class);        MultipleOutputs.addNamedOutput(oldApiJob, OUTPUT_SB, org.apache.hadoop.mapred.SequenceFileOutputFormat.class, IntWritable.class, VectorWritable.class);    }    /*     * HACK: we use old api multiple outputs since they are not available in the     * new api of either 0.20.2 or 0.20.203 but wrap it into a new api job so we     * can use new api interfaces.     */    Job job = new Job(oldApiJob);    job.setJobName("Bt-job");    job.setJarByClass(BtJob.class);    job.setInputFormatClass(SequenceFileInputFormat.class);    job.setOutputFormatClass(SequenceFileOutputFormat.class);    FileInputFormat.setInputPaths(job, inputPathA);    if (minSplitSize > 0) {        FileInputFormat.setMinInputSplitSize(job, minSplitSize);    }    FileOutputFormat.setOutputPath(job, outputPath);        job.getConfiguration().set("mapreduce.output.basename", OUTPUT_BT);    FileOutputFormat.setOutputCompressorClass(job, DefaultCodec.class);    SequenceFileOutputFormat.setOutputCompressionType(job, CompressionType.BLOCK);    job.setMapOutputKeyClass(LongWritable.class);    job.setMapOutputValueClass(SparseRowBlockWritable.class);    job.setOutputKeyClass(IntWritable.class);    job.setOutputValueClass(VectorWritable.class);    job.setMapperClass(BtMapper.class);    job.setCombinerClass(OuterProductCombiner.class);    job.setReducerClass(OuterProductReducer.class);    job.getConfiguration().setInt(QJob.PROP_K, k);    job.getConfiguration().setInt(QJob.PROP_P, p);    job.getConfiguration().set(PROP_QJOB_PATH, inputPathQJob.toString());    job.getConfiguration().setBoolean(PROP_OUPTUT_BBT_PRODUCTS, outputBBtProducts);    job.getConfiguration().setInt(PROP_OUTER_PROD_BLOCK_HEIGHT, btBlockHeight);    job.setNumReduceTasks(numReduceTasks);    /*     * PCA-related options, MAHOUT-817     */    if (xiPath != null) {        job.getConfiguration().set(PROP_XI_PATH, xiPath.toString());    }    if (broadcast) {        job.getConfiguration().set(PROP_RHAT_BROADCAST, "y");        FileSystem fs = FileSystem.get(inputPathQJob.toUri(), conf);        FileStatus[] fstats = fs.globStatus(new Path(inputPathQJob, QJob.OUTPUT_RHAT + "-*"));        if (fstats != null) {            for (FileStatus fstat : fstats) {                /*           * new api is not enabled yet in our dependencies at this time, still           * using deprecated one           */                DistributedCache.addCacheFile(fstat.getPath().toUri(), job.getConfiguration());            }        }    }    job.submit();    job.waitForCompletion(false);    if (!job.isSuccessful()) {        throw new IOException("Bt job unsuccessful.");    }}
0
public void setBlock(double[][] block)
{    this.block = block;}
0
public double[][] getBlock()
{    return block;}
0
public void readFields(DataInput in) throws IOException
{    int m = in.readInt();    int n = in.readInt();    if (block == null) {        block = new double[m][0];    } else if (block.length != m) {        block = Arrays.copyOf(block, m);    }    for (int i = 0; i < m; i++) {        if (block[i] == null || block[i].length != n) {            block[i] = new double[n];        }        for (int j = 0; j < n; j++) {            block[i][j] = in.readDouble();        }    }}
0
public void write(DataOutput out) throws IOException
{    int m = block.length;    int n = block.length == 0 ? 0 : block[0].length;    out.writeInt(m);    out.writeInt(n);    for (double[] aBlock : block) {        for (int j = 0; j < n; j++) {            out.writeDouble(aBlock[j]);        }    }}
0
public double getQuick(int row, int column)
{    long hash = murmur64((long) row << Integer.SIZE | column, 8, seed);    return hash / UNIFORM_DIVISOR;}
0
public void computeYRow(Vector aRow, double[] yRow)
{        Arrays.fill(yRow, 0.0);    if (aRow.isDense()) {        int n = aRow.size();        for (int j = 0; j < n; j++) {            accumDots(j, aRow.getQuick(j), yRow);        }    } else {        for (Element el : aRow.nonZeroes()) {            accumDots(el.index(), el.get(), yRow);        }    }}
0
public void computeYRow(Vector aRow, Vector yRowOut)
{    yRowOut.assign(0.0);    if (aRow.isDense()) {        int n = aRow.size();        for (int j = 0; j < n; j++) {            accumDots(j, aRow.getQuick(j), yRowOut);        }    } else {        for (Element el : aRow.nonZeroes()) {            accumDots(el.index(), el.get(), yRowOut);        }    }}
0
public Vector mutlithreadedTRightMultiply(final Vector v)
{    int nThreads = Runtime.getRuntime().availableProcessors();    ExecutorService es = new ThreadPoolExecutor(nThreads, nThreads, 1, TimeUnit.SECONDS, new ArrayBlockingQueue<Runnable>(kp));    try {        List<Future<Double>> dotFutures = Lists.newArrayListWithCapacity(kp);        for (int i = 0; i < kp; i++) {            final int index = i;            Future<Double> dotFuture = es.submit(new Callable<Double>() {                @Override                public Double call() throws Exception {                    double result = 0.0;                    if (v.isDense()) {                        for (int k = 0; k < v.size(); k++) {                                                        result += getQuick(k, index) * v.getQuick(k);                        }                    } else {                        for (Element el : v.nonZeroes()) {                            int k = el.index();                            result += getQuick(k, index) * el.get();                        }                    }                    return result;                }            });            dotFutures.add(dotFuture);        }        try {            Vector res = new DenseVector(kp);            for (int i = 0; i < kp; i++) {                res.setQuick(i, dotFutures.get(i).get());            }            return res;        } catch (InterruptedException exc) {            throw new IllegalStateException("Interrupted", exc);        } catch (ExecutionException exc) {            if (exc.getCause() instanceof RuntimeException) {                throw (RuntimeException) exc.getCause();            } else {                throw new IllegalStateException(exc.getCause());            }        }    } finally {        es.shutdown();    }}
0
public Double call() throws Exception
{    double result = 0.0;    if (v.isDense()) {        for (int k = 0; k < v.size(); k++) {                        result += getQuick(k, index) * v.getQuick(k);        }    } else {        for (Element el : v.nonZeroes()) {            int k = el.index();            result += getQuick(k, index) * el.get();        }    }    return result;}
0
protected void accumDots(int aIndex, double aElement, double[] yRow)
{    for (int i = 0; i < kp; i++) {        yRow[i] += getQuick(aIndex, i) * aElement;    }}
0
protected void accumDots(int aIndex, double aElement, Vector yRow)
{    for (int i = 0; i < kp; i++) {        yRow.setQuick(i, yRow.getQuick(i) + getQuick(aIndex, i) * aElement);    }}
0
public static long murmur64(long val, int len, long seed)
{        long m = 0xc6a4a7935bd1e995L;    long h = seed ^ len * m;    long k = val;    k *= m;    int r = 47;    k ^= k >>> r;    k *= m;    h ^= k;    h *= m;    h ^= h >>> r;    h *= m;    h ^= h >>> r;    return h;}
0
public static long murmur64(byte[] val, int offset, int len, long seed)
{    long m = 0xc6a4a7935bd1e995L;    int r = 47;    long h = seed ^ (len * m);    int lt = len >>> 3;    for (int i = 0; i < lt; i++, offset += 8) {        long k = 0;        for (int j = 0; j < 8; j++) {            k <<= 8;            k |= val[offset + j] & 0xff;        }        k *= m;        k ^= k >>> r;        k *= m;        h ^= k;        h *= m;    }    if (offset < len) {        long k = 0;        while (offset < len) {            k <<= 8;            k |= val[offset] & 0xff;            offset++;        }        h ^= k;        h *= m;    }    h ^= h >>> r;    h *= m;    h ^= h >>> r;    return h;}
0
protected void setup(Context context) throws IOException, InterruptedException
{    Configuration conf = context.getConfiguration();    int k = Integer.parseInt(conf.get(PROP_K));    int p = Integer.parseInt(conf.get(PROP_P));    kp = k + p;    long omegaSeed = Long.parseLong(conf.get(PROP_OMEGA_SEED));    omega = new Omega(omegaSeed, k + p);    String sbPathStr = conf.get(PROP_SB_PATH);    if (sbPathStr != null) {        sb = SSVDHelper.loadAndSumUpVectors(new Path(sbPathStr), conf);        if (sb == null)            throw new IOException(String.format("Unable to load s_omega from path %s.", sbPathStr));    }    outputs = new MultipleOutputs(new JobConf(conf));    closeables.addFirst(new Closeable() {        @Override        public void close() throws IOException {            outputs.close();        }    });    qHatKey = new SplitPartitionedWritable(context);    rHatKey = new SplitPartitionedWritable(context);    OutputCollector<Writable, DenseBlockWritable> qhatCollector = new OutputCollector<Writable, DenseBlockWritable>() {        @Override        @SuppressWarnings("unchecked")        public void collect(Writable nil, DenseBlockWritable dbw) throws IOException {            outputs.getCollector(OUTPUT_QHAT, null).collect(qHatKey, dbw);            qHatKey.incrementItemOrdinal();        }    };    OutputCollector<Writable, VectorWritable> rhatCollector = new OutputCollector<Writable, VectorWritable>() {        @Override        @SuppressWarnings("unchecked")        public void collect(Writable nil, VectorWritable rhat) throws IOException {            outputs.getCollector(OUTPUT_RHAT, null).collect(rHatKey, rhat);            rHatKey.incrementItemOrdinal();        }    };    qr = new QRFirstStep(conf, qhatCollector, rhatCollector);        closeables.addFirst(qr);    yRow = new DenseVector(kp);}
0
public void close() throws IOException
{    outputs.close();}
0
public void collect(Writable nil, DenseBlockWritable dbw) throws IOException
{    outputs.getCollector(OUTPUT_QHAT, null).collect(qHatKey, dbw);    qHatKey.incrementItemOrdinal();}
0
public void collect(Writable nil, VectorWritable rhat) throws IOException
{    outputs.getCollector(OUTPUT_RHAT, null).collect(rHatKey, rhat);    rHatKey.incrementItemOrdinal();}
0
protected void map(Writable key, VectorWritable value, Context context) throws IOException, InterruptedException
{    omega.computeYRow(value.get(), yRow);    if (sb != null) {        yRow.assign(sb, Functions.MINUS);    }    qr.collect(key, yRow);}
0
protected void cleanup(Context context) throws IOException, InterruptedException
{    IOUtils.close(closeables);}
0
public static void run(Configuration conf, Path[] inputPaths, Path sbPath, Path outputPath, int aBlockRows, int minSplitSize, int k, int p, long seed, int numReduceTasks) throws ClassNotFoundException, InterruptedException, IOException
{    JobConf oldApiJob = new JobConf(conf);    MultipleOutputs.addNamedOutput(oldApiJob, OUTPUT_QHAT, org.apache.hadoop.mapred.SequenceFileOutputFormat.class, SplitPartitionedWritable.class, DenseBlockWritable.class);    MultipleOutputs.addNamedOutput(oldApiJob, OUTPUT_RHAT, org.apache.hadoop.mapred.SequenceFileOutputFormat.class, SplitPartitionedWritable.class, VectorWritable.class);    Job job = new Job(oldApiJob);    job.setJobName("Q-job");    job.setJarByClass(QJob.class);    job.setInputFormatClass(SequenceFileInputFormat.class);    FileInputFormat.setInputPaths(job, inputPaths);    if (minSplitSize > 0) {        FileInputFormat.setMinInputSplitSize(job, minSplitSize);    }    FileOutputFormat.setOutputPath(job, outputPath);    FileOutputFormat.setCompressOutput(job, true);    FileOutputFormat.setOutputCompressorClass(job, DefaultCodec.class);    SequenceFileOutputFormat.setOutputCompressionType(job, CompressionType.BLOCK);    job.setMapOutputKeyClass(SplitPartitionedWritable.class);    job.setMapOutputValueClass(VectorWritable.class);    job.setOutputKeyClass(SplitPartitionedWritable.class);    job.setOutputValueClass(VectorWritable.class);    job.setMapperClass(QMapper.class);    job.getConfiguration().setInt(PROP_AROWBLOCK_SIZE, aBlockRows);    job.getConfiguration().setLong(PROP_OMEGA_SEED, seed);    job.getConfiguration().setInt(PROP_K, k);    job.getConfiguration().setInt(PROP_P, p);    if (sbPath != null) {        job.getConfiguration().set(PROP_SB_PATH, sbPath.toString());    }    /*     * number of reduce tasks doesn't matter. we don't actually send anything to     * reducers.     */    job.setNumReduceTasks(0);    job.submit();    job.waitForCompletion(false);    if (!job.isSuccessful()) {        throw new IOException("Q job unsuccessful.");    }}
0
public void reset()
{    cnt = 0;}
0
public void solve(Matrix a)
{    assert a.rowSize() == m;    assert a.columnSize() == n;    double[] aRow = new double[n];    for (int i = 0; i < m; i++) {        Vector aRowV = a.viewRow(i);        for (int j = 0; j < n; j++) {            aRow[j] = aRowV.getQuick(j);        }        appendRow(aRow);    }}
0
public boolean isFull()
{    return cnt == m;}
0
public int getM()
{    return m;}
0
public int getN()
{    return n;}
0
public int getCnt()
{    return cnt;}
0
public void adjust(int newM)
{    if (newM == m) {                return;    }    if (newM < n) {        throw new IllegalArgumentException("new m can't be less than n");    }    if (newM < cnt) {        throw new IllegalArgumentException("new m can't be less than rows accumulated");    }    vQtRow = new double[newM];        if (newM > m) {                for (int i = 0; i < n; i++) {            mQt[i] = Arrays.copyOf(mQt[i], newM);            System.arraycopy(mQt[i], 0, mQt[i], newM - m, m);            Arrays.fill(mQt[i], 0, newM - m, 0);        }    } else {                for (int i = 0; i < n; i++) {            mQt[i] = Arrays.copyOfRange(mQt[i], m - newM, m);        }    }    m = newM;}
0
public void trim()
{    adjust(cnt);}
0
public void appendRow(double[] aRow)
{    if (cnt >= m) {        throw new IllegalStateException("thin QR solver fed more rows than initialized for");    }    try {        /*       * moving pointers around is inefficient but for the sanity's sake i am       * keeping it this way so i don't have to guess how R-tilde index maps to       * actual block index       */        Arrays.fill(vQtRow, 0);        vQtRow[m - cnt - 1] = 1;        int height = cnt > n ? n : cnt;        System.arraycopy(aRow, 0, vARow, 0, n);        if (height > 0) {            givens(vARow[0], getRRow(0)[0], cs);            applyGivensInPlace(cs[0], cs[1], vARow, getRRow(0), 0, n);            applyGivensInPlace(cs[0], cs[1], vQtRow, getQtRow(0), 0, m);        }        for (int i = 1; i < height; i++) {            givens(getRRow(i - 1)[i], getRRow(i)[i], cs);            applyGivensInPlace(cs[0], cs[1], getRRow(i - 1), getRRow(i), i, n - i);            applyGivensInPlace(cs[0], cs[1], getQtRow(i - 1), getQtRow(i), 0, m);        }        /*       * push qt and r-tilde 1 row down       *        * just swap the references to reduce GC churning       */        pushQtDown();        double[] swap = getQtRow(0);        setQtRow(0, vQtRow);        vQtRow = swap;        pushRDown();        swap = getRRow(0);        setRRow(0, vARow);        vARow = swap;    } finally {        cnt++;    }}
0
private double[] getQtRow(int row)
{    return mQt[(row += qtStartRow) >= n ? row - n : row];}
0
private void setQtRow(int row, double[] qtRow)
{    mQt[(row += qtStartRow) >= n ? row - n : row] = qtRow;}
0
private void pushQtDown()
{    qtStartRow = qtStartRow == 0 ? n - 1 : qtStartRow - 1;}
0
private double[] getRRow(int row)
{    row += rStartRow;    return mR[row >= n ? row - n : row];}
0
private void setRRow(int row, double[] rrow)
{    mR[(row += rStartRow) >= n ? row - n : row] = rrow;}
0
private void pushRDown()
{    rStartRow = rStartRow == 0 ? n - 1 : rStartRow - 1;}
0
public UpperTriangular getRTilde()
{    UpperTriangular packedR = new UpperTriangular(n);    for (int i = 0; i < n; i++) {        packedR.assignNonZeroElementsInRow(i, getRRow(i));    }    return packedR;}
0
public double[][] getThinQtTilde()
{    if (qtStartRow != 0) {        /*       * rotate qt rows into place       *        * double[~500][], once per block, not a big deal.       */        double[][] qt = new double[n][];        System.arraycopy(mQt, qtStartRow, qt, 0, n - qtStartRow);        System.arraycopy(mQt, 0, qt, n - qtStartRow, qtStartRow);        return qt;    }    return mQt;}
0
public static void applyGivensInPlace(double c, double s, double[] row1, double[] row2, int offset, int len)
{    int n = offset + len;    for (int j = offset; j < n; j++) {        double tau1 = row1[j];        double tau2 = row2[j];        row1[j] = c * tau1 - s * tau2;        row2[j] = s * tau1 + c * tau2;    }}
0
public static void applyGivensInPlace(double c, double s, Vector row1, Vector row2, int offset, int len)
{    int n = offset + len;    for (int j = offset; j < n; j++) {        double tau1 = row1.getQuick(j);        double tau2 = row2.getQuick(j);        row1.setQuick(j, c * tau1 - s * tau2);        row2.setQuick(j, s * tau1 + c * tau2);    }}
0
public static void applyGivensInPlace(double c, double s, int i, int k, Matrix mx)
{    int n = mx.columnSize();    for (int j = 0; j < n; j++) {        double tau1 = mx.get(i, j);        double tau2 = mx.get(k, j);        mx.set(i, j, c * tau1 - s * tau2);        mx.set(k, j, s * tau1 + c * tau2);    }}
0
public static void fromRho(double rho, double[] csOut)
{    if (rho == 1) {        csOut[0] = 0;        csOut[1] = 1;        return;    }    if (Math.abs(rho) < 1) {        csOut[1] = 2 * rho;        csOut[0] = Math.sqrt(1 - csOut[1] * csOut[1]);        return;    }    csOut[0] = 2 / rho;    csOut[1] = Math.sqrt(1 - csOut[0] * csOut[0]);}
0
public static void givens(double a, double b, double[] csOut)
{    if (b == 0) {        csOut[0] = 1;        csOut[1] = 0;        return;    }    if (Math.abs(b) > Math.abs(a)) {        double tau = -a / b;        csOut[1] = 1 / Math.sqrt(1 + tau * tau);        csOut[0] = csOut[1] * tau;    } else {        double tau = -b / a;        csOut[0] = 1 / Math.sqrt(1 + tau * tau);        csOut[1] = csOut[0] * tau;    }}
0
public static double toRho(double c, double s)
{    if (c == 0) {        return 1;    }    if (Math.abs(s) < Math.abs(c)) {        return Math.signum(c) * s / 2;    } else {        return Math.signum(s) * 2 / c;    }}
0
public static void mergeR(UpperTriangular r1, UpperTriangular r2)
{    TriangularRowView r1Row = new TriangularRowView(r1);    TriangularRowView r2Row = new TriangularRowView(r2);    int kp = r1Row.size();    assert kp == r2Row.size();    double[] cs = new double[2];    for (int v = 0; v < kp; v++) {        for (int u = v; u < kp; u++) {            givens(r1Row.setViewedRow(u).get(u), r2Row.setViewedRow(u - v).get(u), cs);            applyGivensInPlace(cs[0], cs[1], r1Row, r2Row, u, kp - u);        }    }}
0
public static void mergeR(double[][] r1, double[][] r2)
{    int kp = r1[0].length;    assert kp == r2[0].length;    double[] cs = new double[2];    for (int v = 0; v < kp; v++) {        for (int u = v; u < kp; u++) {            givens(r1[u][u], r2[u - v][u], cs);            applyGivensInPlace(cs[0], cs[1], r1[u], r2[u - v], u, kp - u);        }    }}
0
public static void mergeRonQ(UpperTriangular r1, UpperTriangular r2, double[][] qt1, double[][] qt2)
{    TriangularRowView r1Row = new TriangularRowView(r1);    TriangularRowView r2Row = new TriangularRowView(r2);    int kp = r1Row.size();    assert kp == r2Row.size();    assert kp == qt1.length;    assert kp == qt2.length;    int r = qt1[0].length;    assert qt2[0].length == r;    double[] cs = new double[2];    for (int v = 0; v < kp; v++) {        for (int u = v; u < kp; u++) {            givens(r1Row.setViewedRow(u).get(u), r2Row.setViewedRow(u - v).get(u), cs);            applyGivensInPlace(cs[0], cs[1], r1Row, r2Row, u, kp - u);            applyGivensInPlace(cs[0], cs[1], qt1[u], qt2[u - v], 0, r);        }    }}
0
public static void mergeRonQ(double[][] r1, double[][] r2, double[][] qt1, double[][] qt2)
{    int kp = r1[0].length;    assert kp == r2[0].length;    assert kp == qt1.length;    assert kp == qt2.length;    int r = qt1[0].length;    assert qt2[0].length == r;    double[] cs = new double[2];    /*     * pairwise givens(a,b) so that a come off main diagonal in r1 and bs come     * off u-th upper subdiagonal in r2.     */    for (int v = 0; v < kp; v++) {        for (int u = v; u < kp; u++) {            givens(r1[u][u], r2[u - v][u], cs);            applyGivensInPlace(cs[0], cs[1], r1[u], r2[u - v], u, kp - u);            applyGivensInPlace(cs[0], cs[1], qt1[u], qt2[u - v], 0, r);        }    }}
0
public static double[][] mergeQrUp(double[][] qt1, double[][] r1, double[][] r2)
{    int kp = qt1.length;    int r = qt1[0].length;    double[][] qTilde = new double[kp][];    for (int i = 0; i < kp; i++) {        qTilde[i] = new double[r];    }    mergeRonQ(r1, r2, qt1, qTilde);    return qt1;}
0
public static double[][] mergeQrUp(double[][] qt1, UpperTriangular r1, UpperTriangular r2)
{    int kp = qt1.length;    int r = qt1[0].length;    double[][] qTilde = new double[kp][];    for (int i = 0; i < kp; i++) {        qTilde[i] = new double[r];    }    mergeRonQ(r1, r2, qt1, qTilde);    return qt1;}
0
public static double[][] mergeQrDown(double[][] r1, double[][] qt2, double[][] r2)
{    int kp = qt2.length;    int r = qt2[0].length;    double[][] qTilde = new double[kp][];    for (int i = 0; i < kp; i++) {        qTilde[i] = new double[r];    }    mergeRonQ(r1, r2, qTilde, qt2);    return qTilde;}
0
public static double[][] mergeQrDown(UpperTriangular r1, double[][] qt2, UpperTriangular r2)
{    int kp = qt2.length;    int r = qt2[0].length;    double[][] qTilde = new double[kp][];    for (int i = 0; i < kp; i++) {        qTilde[i] = new double[r];    }    mergeRonQ(r1, r2, qTilde, qt2);    return qTilde;}
0
public static double[][] computeQtHat(double[][] qt, int i, Iterator<UpperTriangular> rIter)
{    UpperTriangular rTilde = rIter.next();    for (int j = 1; j < i; j++) {        mergeR(rTilde, rIter.next());    }    if (i > 0) {        qt = mergeQrDown(rTilde, qt, rIter.next());    }    while (rIter.hasNext()) {        qt = mergeQrUp(qt, rTilde, rIter.next());    }    return qt;}
0
public static boolean isOrthonormal(double[][] qt, boolean insufficientRank, double epsilon)
{    int n = qt.length;    int rank = 0;    for (int i = 0; i < n; i++) {        Vector ei = new DenseVector(qt[i], true);        double norm = ei.norm(2);        if (Math.abs(1.0 - norm) < epsilon) {            rank++;        } else if (Math.abs(norm) > epsilon) {                        return false;        }        for (int j = 0; j <= i; j++) {            Vector ej = new DenseVector(qt[j], true);            double dot = ei.dot(ej);            if (!(Math.abs((i == j && rank > j ? 1.0 : 0.0) - dot) < epsilon)) {                return false;            }        }    }    return insufficientRank ? rank < n : rank == n;}
0
public static boolean isOrthonormalBlocked(Iterable<double[][]> qtHats, boolean insufficientRank, double epsilon)
{    int n = qtHats.iterator().next().length;    int rank = 0;    for (int i = 0; i < n; i++) {        List<Vector> ei = Lists.newArrayList();                for (double[][] qtHat : qtHats) {            ei.add(new DenseVector(qtHat[i], true));        }        double norm = 0;        for (Vector v : ei) {            norm += v.dot(v);        }        norm = Math.sqrt(norm);        if (Math.abs(1 - norm) < epsilon) {            rank++;        } else if (Math.abs(norm) > epsilon) {                        return false;        }        for (int j = 0; j <= i; j++) {            List<Vector> ej = Lists.newArrayList();            for (double[][] qtHat : qtHats) {                ej.add(new DenseVector(qtHat[j], true));            }                        double dot = 0;            for (int k = 0; k < ei.size(); k++) {                dot += ei.get(k).dot(ej.get(k));            }            if (!(Math.abs((i == j && rank > j ? 1 : 0) - dot) < epsilon)) {                return false;            }        }    }    return insufficientRank ? rank < n : rank == n;}
0
 TriangularRowView setViewedRow(int row)
{    rowNum = row;    return this;}
0
public boolean isDense()
{    return true;}
0
public boolean isSequentialAccess()
{    return false;}
0
public Iterator<Element> iterator()
{    throw new UnsupportedOperationException();}
0
public Iterator<Element> iterateNonZero()
{    throw new UnsupportedOperationException();}
0
public double getQuick(int index)
{    return viewed.getQuick(rowNum, index);}
0
public Vector like()
{    throw new UnsupportedOperationException();}
0
public Vector like(int cardinality)
{    throw new UnsupportedOperationException();}
0
public void setQuick(int index, double value)
{    viewed.setQuick(rowNum, index, value);}
0
public int getNumNondefaultElements()
{    throw new UnsupportedOperationException();}
0
public double getLookupCost()
{    return 1;}
0
public double getIteratorAdvanceCost()
{    return 1;}
0
public boolean isAddConstantTime()
{    return true;}
0
public Matrix matrixLike(int rows, int columns)
{    throw new UnsupportedOperationException();}
0
public void mergeUpdates(OrderedIntDoubleMapping updates)
{    int[] indices = updates.getIndices();    double[] values = updates.getValues();    for (int i = 0; i < updates.getNumMappings(); ++i) {        viewed.setQuick(rowNum, indices[i], values[i]);    }}
0
public static void orthonormalizeColumns(Matrix mx)
{    int n = mx.numCols();    for (int c = 0; c < n; c++) {        Vector col = mx.viewColumn(c);        for (int c1 = 0; c1 < c; c1++) {            Vector viewC1 = mx.viewColumn(c1);            col.assign(col.minus(viewC1.times(viewC1.dot(col))));        }        final double norm2 = col.norm(2);        col.assign(new DoubleFunction() {            @Override            public double apply(double x) {                return x / norm2;            }        });    }}
0
public double apply(double x)
{    return x / norm2;}
0
public void close() throws IOException
{    cleanup();}
0
public int getKP()
{    return kp;}
0
private void flushSolver() throws IOException
{    UpperTriangular r = qSolver.getRTilde();    double[][] qt = qSolver.getThinQtTilde();    rSubseq.add(r);    value.setBlock(qt);    getTempQw().append(tempKey, value);    /*     * this probably should be a sparse row matrix, but compressor should get it     * for disk and in memory we want it dense anyway, sparse random     * implementations would be a mostly a memory management disaster consisting     * of rehashes and GC      */    value.setBlock(null);    qSolver.reset();}
0
private void flushQBlocks() throws IOException
{    if (blockCnt == 1) {        /*       * only one block, no temp file, no second pass. should be the default       * mode for efficiency in most cases. Sure mapper should be able to load       * the entire split in memory -- and we don't require even that.       */        value.setBlock(qSolver.getThinQtTilde());        outputQHat(value);        outputR(new VectorWritable(new DenseVector(qSolver.getRTilde().getData(), true)));    } else {        secondPass();    }}
0
private void outputQHat(DenseBlockWritable value) throws IOException
{    qtHatOut.collect(NullWritable.get(), value);}
0
private void outputR(VectorWritable value) throws IOException
{    rHatOut.collect(NullWritable.get(), value);}
0
private void secondPass() throws IOException
{        qSolver = null;    FileSystem localFs = FileSystem.getLocal(jobConf);    SequenceFile.Reader tempQr = new SequenceFile.Reader(localFs, tempQPath, jobConf);    closeables.addFirst(tempQr);    int qCnt = 0;    while (tempQr.next(tempKey, value)) {        value.setBlock(GivensThinSolver.computeQtHat(value.getBlock(), qCnt, new CopyConstructorIterator<>(rSubseq.iterator())));        if (qCnt == 1) {            /*         * just merge r[0] <- r[1] so it doesn't have to repeat in subsequent         * computeQHat iterators         */            GivensThinSolver.mergeR(rSubseq.get(0), rSubseq.remove(1));        } else {            qCnt++;        }        outputQHat(value);    }    assert rSubseq.size() == 1;    outputR(new VectorWritable(new DenseVector(rSubseq.get(0).getData(), true)));}
0
protected void map(Vector incomingYRow) throws IOException
{    double[] yRow;    if (yLookahead.size() == kp) {        if (qSolver.isFull()) {            flushSolver();            blockCnt++;        }        yRow = yLookahead.remove(0);        qSolver.appendRow(yRow);    } else {        yRow = new double[kp];    }    if (incomingYRow.isDense()) {        for (int i = 0; i < kp; i++) {            yRow[i] = incomingYRow.get(i);        }    } else {        Arrays.fill(yRow, 0);        for (Element yEl : incomingYRow.nonZeroes()) {            yRow[yEl.index()] = yEl.get();        }    }    yLookahead.add(yRow);}
0
protected void setup()
{    int r = Integer.parseInt(jobConf.get(PROP_AROWBLOCK_SIZE));    int k = Integer.parseInt(jobConf.get(PROP_K));    int p = Integer.parseInt(jobConf.get(PROP_P));    kp = k + p;    yLookahead = Lists.newArrayListWithCapacity(kp);    qSolver = new GivensThinSolver(r, kp);    outputs = new MultipleOutputs(new JobConf(jobConf));    closeables.addFirst(new Closeable() {        @Override        public void close() throws IOException {            outputs.close();        }    });}
0
public void close() throws IOException
{    outputs.close();}
0
protected void cleanup() throws IOException
{    try {        if (qSolver == null && yLookahead.isEmpty()) {            return;        }        if (qSolver == null) {            qSolver = new GivensThinSolver(yLookahead.size(), kp);        }                qSolver.adjust(qSolver.getCnt() + yLookahead.size());        while (!yLookahead.isEmpty()) {            qSolver.appendRow(yLookahead.remove(0));        }        assert qSolver.isFull();        if (++blockCnt > 1) {            flushSolver();            assert tempQw != null;            closeables.remove(tempQw);            Closeables.close(tempQw, false);        }        flushQBlocks();    } finally {        IOUtils.close(closeables);    }}
0
private SequenceFile.Writer getTempQw() throws IOException
{    if (tempQw == null) {        /*       * temporary Q output hopefully will not exceed size of IO cache in which       * case it is only good since it is going to be managed by kernel, not       * java GC. And if IO cache is not good enough, then at least it is always       * sequential.       */        String taskTmpDir = System.getProperty("java.io.tmpdir");        FileSystem localFs = FileSystem.getLocal(jobConf);        Path parent = new Path(taskTmpDir);        Path sub = new Path(parent, "qw_" + System.currentTimeMillis());        tempQPath = new Path(sub, "q-temp.seq");        tempQw = SequenceFile.createWriter(localFs, jobConf, tempQPath, IntWritable.class, DenseBlockWritable.class, CompressionType.BLOCK);        closeables.addFirst(tempQw);        closeables.addFirst(new IOUtils.DeleteFileOnClose(new File(tempQPath.toString())));    }    return tempQw;}
0
public void collect(Writable key, Vector vw) throws IOException
{    map(vw);}
0
private boolean loadNextQt()
{    boolean more = qHatInput.hasNext();    if (!more) {        return false;    }    DenseBlockWritable v = qHatInput.next();    mQt = GivensThinSolver.computeQtHat(v.getBlock(), blockNum == 0 ? 0 : 1, new CopyConstructorIterator<>(mRs.iterator()));    r = mQt[0].length;    kp = mQt.length;    if (qRow == null) {        qRow = new DenseVector(kp);    }    return true;}
0
public boolean hasNext()
{    if (mQt != null && cnt == r) {        mQt = null;    }    boolean result = true;    if (mQt == null) {        result = loadNextQt();        cnt = 0;    }    return result;}
0
public Vector next()
{    if (!hasNext()) {        throw new NoSuchElementException();    }    Validate.isTrue(hasNext(), "Q input overrun");    /*     * because Q blocks are initially stored in inverse order     */    int qRowIndex = r - cnt - 1;    for (int j = 0; j < kp; j++) {        qRow.setQuick(j, mQt[j][qRowIndex]);    }    cnt++;    return qRow;}
0
public void remove()
{    throw new UnsupportedOperationException();}
0
public void close() throws IOException
{    mQt = null;    mRs.clear();}
0
private void flushBlock() throws IOException
{    if (block == null || block.getNumRows() == 0) {        return;    }    blockKeyW.set(currentBlockNum);    delegate.collect(blockKeyW, block);    block.clear();}
0
public void collect(Long rowIndex, Vector v) throws IOException
{    long blockKey = rowIndex / height;    if (blockKey != currentBlockNum) {        flushBlock();        if (block == null) {            block = new SparseRowBlockWritable(100);        }        currentBlockNum = blockKey;    }    block.plusRow((int) (rowIndex % height), v);}
0
public void close() throws IOException
{    flushBlock();}
0
public int[] getRowIndices()
{    return rowIndices;}
0
public Vector[] getRows()
{    return rows;}
0
public void readFields(DataInput in) throws IOException
{    numRows = Varint.readUnsignedVarInt(in);    if (rows == null || rows.length < numRows) {        rows = new Vector[numRows];        rowIndices = new int[numRows];    }    VectorWritable vw = new VectorWritable();    for (int i = 0; i < numRows; i++) {        rowIndices[i] = Varint.readUnsignedVarInt(in);        vw.readFields(in);        rows[i] = vw.get().clone();    }}
0
public void write(DataOutput out) throws IOException
{    Varint.writeUnsignedVarInt(numRows, out);    VectorWritable vw = new VectorWritable();    for (int i = 0; i < numRows; i++) {        Varint.writeUnsignedVarInt(rowIndices[i], out);        vw.set(rows[i]);        vw.write(out);    }}
0
public void plusRow(int index, Vector row)
{    /*     * often accumulation goes in row-increasing order, so check for this to     * avoid binary search (another log Height multiplier).     */    int pos = numRows == 0 || rowIndices[numRows - 1] < index ? -numRows - 1 : Arrays.binarySearch(rowIndices, 0, numRows, index);    if (pos >= 0) {        rows[pos].assign(row, PlusMult.plusMult(1));    } else {        insertIntoPos(-pos - 1, index, row);    }}
0
private void insertIntoPos(int pos, int rowIndex, Vector row)
{        if (numRows == rows.length) {        rows = Arrays.copyOf(rows, numRows + 1 << 1);        rowIndices = Arrays.copyOf(rowIndices, numRows + 1 << 1);    }        System.arraycopy(rows, pos, rows, pos + 1, numRows - pos);    System.arraycopy(rowIndices, pos, rowIndices, pos + 1, numRows - pos);        rowIndices[pos] = rowIndex;    rows[pos] = row.clone();    numRows++;}
0
public void plusBlock(SparseRowBlockWritable bOther)
{    /*     * since we maintained row indices in a sorted order, we can run sort merge     * to expedite this operation     */    int i = 0;    int j = 0;    while (i < numRows && j < bOther.numRows) {        while (i < numRows && rowIndices[i] < bOther.rowIndices[j]) {            i++;        }        if (i < numRows) {            if (rowIndices[i] == bOther.rowIndices[j]) {                rows[i].assign(bOther.rows[j], PlusMult.plusMult(1));            } else {                                insertIntoPos(i, bOther.rowIndices[j], bOther.rows[j]);            }                        i++;            j++;        }    }    for (; j < bOther.numRows; j++) {        insertIntoPos(numRows, bOther.rowIndices[j], bOther.rows[j]);    }}
0
public int getNumRows()
{    return numRows;}
0
public void clear()
{    numRows = 0;    Arrays.fill(rows, null);}
0
public int getTaskId()
{    return taskId;}
0
public long getTaskItemOrdinal()
{    return taskItemOrdinal;}
0
public void incrementItemOrdinal()
{    taskItemOrdinal++;}
0
public void setTaskItemOrdinal(long taskItemOrdinal)
{    this.taskItemOrdinal = taskItemOrdinal;}
0
public void readFields(DataInput in) throws IOException
{    taskId = Varint.readUnsignedVarInt(in);    taskItemOrdinal = Varint.readUnsignedVarLong(in);}
0
public void write(DataOutput out) throws IOException
{    Varint.writeUnsignedVarInt(taskId, out);    Varint.writeUnsignedVarLong(taskItemOrdinal, out);}
0
public int hashCode()
{    int prime = 31;    int result = 1;    result = prime * result + taskId;    return result;}
0
public boolean equals(Object obj)
{    if (this == obj) {        return true;    }    if (obj == null) {        return false;    }    if (getClass() != obj.getClass()) {        return false;    }    SplitPartitionedWritable other = (SplitPartitionedWritable) obj;    return taskId == other.taskId;}
0
public int compareTo(SplitPartitionedWritable o)
{    if (taskId < o.taskId) {        return -1;    }    if (taskId > o.taskId) {        return 1;    }    if (taskItemOrdinal < o.taskItemOrdinal) {        return -1;    }    if (taskItemOrdinal > o.taskItemOrdinal) {        return 1;    }    return 0;}
0
public int compare(Object a, Object b)
{    SplitPartitionedWritable o1 = (SplitPartitionedWritable) a;    SplitPartitionedWritable o2 = (SplitPartitionedWritable) b;    if (o1.taskId < o2.taskId) {        return -1;    }    if (o1.taskId > o2.taskId) {        return 1;    }    return 0;}
0
public int run(String[] args) throws Exception
{    addInputOption();    addOutputOption();    addOption("rank", "k", "decomposition rank", true);    addOption("oversampling", "p", "oversampling", String.valueOf(15));    addOption("blockHeight", "r", "Y block height (must be > (k+p))", String.valueOf(10000));    addOption("outerProdBlockHeight", "oh", "block height of outer products during multiplication, increase for sparse inputs", String.valueOf(30000));    addOption("abtBlockHeight", "abth", "block height of Y_i in ABtJob during AB' multiplication, increase for extremely sparse inputs", String.valueOf(200000));    addOption("minSplitSize", "s", "minimum split size", String.valueOf(-1));    addOption("computeU", "U", "compute U (true/false)", String.valueOf(true));    addOption("uHalfSigma", "uhs", "Compute U * Sigma^0.5", String.valueOf(false));    addOption("uSigma", "us", "Compute U * Sigma", String.valueOf(false));    addOption("computeV", "V", "compute V (true/false)", String.valueOf(true));    addOption("vHalfSigma", "vhs", "compute V * Sigma^0.5", String.valueOf(false));    addOption("reduceTasks", "t", "number of reduce tasks (where applicable)", true);    addOption("powerIter", "q", "number of additional power iterations (0..2 is good)", String.valueOf(0));    addOption("broadcast", "br", "whether use distributed cache to broadcast matrices wherever possible", String.valueOf(true));    addOption("pca", "pca", "run in pca mode: compute column-wise mean and subtract from input", String.valueOf(false));    addOption("pcaOffset", "xi", "path(glob) of external pca mean (optional, dont compute, use external mean");    addOption(DefaultOptionCreator.overwriteOption().create());    Map<String, List<String>> pargs = parseArguments(args);    if (pargs == null) {        return -1;    }    int k = Integer.parseInt(getOption("rank"));    int p = Integer.parseInt(getOption("oversampling"));    int r = Integer.parseInt(getOption("blockHeight"));    int h = Integer.parseInt(getOption("outerProdBlockHeight"));    int abh = Integer.parseInt(getOption("abtBlockHeight"));    int q = Integer.parseInt(getOption("powerIter"));    int minSplitSize = Integer.parseInt(getOption("minSplitSize"));    boolean computeU = Boolean.parseBoolean(getOption("computeU"));    boolean computeV = Boolean.parseBoolean(getOption("computeV"));    boolean cUHalfSigma = Boolean.parseBoolean(getOption("uHalfSigma"));    boolean cUSigma = Boolean.parseBoolean(getOption("uSigma"));    boolean cVHalfSigma = Boolean.parseBoolean(getOption("vHalfSigma"));    int reduceTasks = Integer.parseInt(getOption("reduceTasks"));    boolean broadcast = Boolean.parseBoolean(getOption("broadcast"));    String xiPathStr = getOption("pcaOffset");    Path xiPath = xiPathStr == null ? null : new Path(xiPathStr);    boolean pca = Boolean.parseBoolean(getOption("pca")) || xiPath != null;    boolean overwrite = hasOption(DefaultOptionCreator.OVERWRITE_OPTION);    Configuration conf = getConf();    if (conf == null) {        throw new IOException("No Hadoop configuration present");    }    Path[] inputPaths = { getInputPath() };    Path tempPath = getTempPath();    FileSystem fs = FileSystem.get(getTempPath().toUri(), conf);        if (overwrite) {                HadoopUtil.delete(getConf(), getOutputPath());                HadoopUtil.delete(getConf(), getTempPath());    }    fs.mkdirs(getOutputPath());        if (pca && xiPath == null) {        xiPath = new Path(tempPath, "xi");        if (overwrite) {            fs.delete(xiPath, true);        }        MatrixColumnMeansJob.run(conf, inputPaths[0], xiPath);    }    SSVDSolver solver = new SSVDSolver(conf, inputPaths, new Path(tempPath, "ssvd"), r, k, p, reduceTasks);    solver.setMinSplitSize(minSplitSize);    solver.setComputeU(computeU);    solver.setComputeV(computeV);    solver.setcUHalfSigma(cUHalfSigma);    solver.setcVHalfSigma(cVHalfSigma);    solver.setcUSigma(cUSigma);    solver.setOuterBlockHeight(h);    solver.setAbtBlockHeight(abh);    solver.setQ(q);    solver.setBroadcast(broadcast);    solver.setOverwrite(overwrite);    if (xiPath != null) {        solver.setPcaMeanPath(new Path(xiPath, "part-*"));    }    solver.run();    Vector svalues = solver.getSingularValues().viewPart(0, k);    SSVDHelper.saveVector(svalues, getOutputPath("sigma"), conf);    if (computeU && !fs.rename(new Path(solver.getUPath()), getOutputPath())) {        throw new IOException("Unable to move U results to the output path.");    }    if (cUHalfSigma && !fs.rename(new Path(solver.getuHalfSigmaPath()), getOutputPath())) {        throw new IOException("Unable to move U*Sigma^0.5 results to the output path.");    }    if (cUSigma && !fs.rename(new Path(solver.getuSigmaPath()), getOutputPath())) {        throw new IOException("Unable to move U*Sigma results to the output path.");    }    if (computeV && !fs.rename(new Path(solver.getVPath()), getOutputPath())) {        throw new IOException("Unable to move V results to the output path.");    }    if (cVHalfSigma && !fs.rename(new Path(solver.getvHalfSigmaPath()), getOutputPath())) {        throw new IOException("Unable to move V*Sigma^0.5 results to the output path.");    }        fs.deleteOnExit(getTempPath());    return 0;}
0
public static void main(String[] args) throws Exception
{    ToolRunner.run(new SSVDCli(), args);}
0
 static Vector loadVector(Path glob, Configuration conf) throws IOException
{    SequenceFileDirValueIterator<VectorWritable> iter = new SequenceFileDirValueIterator<>(glob, PathType.GLOB, null, null, true, conf);    try {        if (!iter.hasNext()) {            throw new IOException("Empty input while reading vector");        }        VectorWritable vw = iter.next();        if (iter.hasNext()) {            throw new IOException("Unexpected data after the end of vector file");        }        return vw.get();    } finally {        Closeables.close(iter, true);    }}
0
public static void saveVector(Vector v, Path vectorFilePath, Configuration conf) throws IOException
{    VectorWritable vw = new VectorWritable(v);    FileSystem fs = FileSystem.get(conf);    try (SequenceFile.Writer w = new SequenceFile.Writer(fs, conf, vectorFilePath, IntWritable.class, VectorWritable.class)) {        w.append(new IntWritable(), vw);    }/*       * this is a writer, no quiet close please. we must bail out on incomplete       * close.       */}
0
 static Class<? extends Writable> sniffInputLabelType(Path[] inputPath, Configuration conf) throws IOException
{    FileSystem fs = FileSystem.get(conf);    for (Path p : inputPath) {        FileStatus[] fstats = fs.globStatus(p);        if (fstats == null || fstats.length == 0) {            continue;        }        FileStatus firstSeqFile;        if (fstats[0].isDir()) {            firstSeqFile = fs.listStatus(fstats[0].getPath(), PathFilters.logsCRCFilter())[0];        } else {            firstSeqFile = fstats[0];        }        SequenceFile.Reader r = null;        try {            r = new SequenceFile.Reader(fs, firstSeqFile.getPath(), conf);            return r.getKeyClass().asSubclass(Writable.class);        } finally {            Closeables.close(r, true);        }    }    throw new IOException("Unable to open input files to determine input label type.");}
0
public int compare(FileStatus o1, FileStatus o2)
{    matcher.reset(o1.getPath().getName());    if (!matcher.matches()) {        throw new IllegalArgumentException("Unexpected file name, unable to deduce partition #:" + o1.getPath());    }    int p1 = Integer.parseInt(matcher.group(3));    matcher.reset(o2.getPath().getName());    if (!matcher.matches()) {        throw new IllegalArgumentException("Unexpected file name, unable to deduce partition #:" + o2.getPath());    }    int p2 = Integer.parseInt(matcher.group(3));    return p1 - p2;}
0
public static Iterator<Pair<Writable, Vector>> drmIterator(FileSystem fs, Path glob, Configuration conf, Deque<Closeable> closeables) throws IOException
{    SequenceFileDirIterator<Writable, VectorWritable> ret = new SequenceFileDirIterator<>(glob, PathType.GLOB, PathFilters.logsCRCFilter(), PARTITION_COMPARATOR, true, conf);    closeables.addFirst(ret);    return Iterators.transform(ret, new Function<Pair<Writable, VectorWritable>, Pair<Writable, Vector>>() {        @Override        public Pair<Writable, Vector> apply(Pair<Writable, VectorWritable> p) {            return new Pair(p.getFirst(), p.getSecond().get());        }    });}
0
public Pair<Writable, Vector> apply(Pair<Writable, VectorWritable> p)
{    return new Pair(p.getFirst(), p.getSecond().get());}
0
public static DenseMatrix drmLoadAsDense(FileSystem fs, Path glob, Configuration conf) throws IOException
{    Deque<Closeable> closeables = new ArrayDeque<>();    try {        List<double[]> denseData = new ArrayList<>();        for (Iterator<Pair<Writable, Vector>> iter = drmIterator(fs, glob, conf, closeables); iter.hasNext(); ) {            Pair<Writable, Vector> p = iter.next();            Vector v = p.getSecond();            double[] dd = new double[v.size()];            if (v.isDense()) {                for (int i = 0; i < v.size(); i++) {                    dd[i] = v.getQuick(i);                }            } else {                for (Vector.Element el : v.nonZeroes()) {                    dd[el.index()] = el.get();                }            }            denseData.add(dd);        }        if (denseData.size() == 0) {            return null;        } else {            return new DenseMatrix(denseData.toArray(new double[denseData.size()][]));        }    } finally {        IOUtils.close(closeables);    }}
0
public static DenseSymmetricMatrix loadAndSumUpperTriangularMatricesAsSymmetric(Path glob, Configuration conf) throws IOException
{    Vector v = loadAndSumUpVectors(glob, conf);    return v == null ? null : new DenseSymmetricMatrix(v);}
0
public static Vector loadAndSumUpVectors(Path glob, Configuration conf) throws IOException
{    SequenceFileDirValueIterator<VectorWritable> iter = new SequenceFileDirValueIterator<>(glob, PathType.GLOB, null, PARTITION_COMPARATOR, true, conf);    try {        Vector v = null;        while (iter.hasNext()) {            if (v == null) {                v = new DenseVector(iter.next().get());            } else {                v.assign(iter.next().get(), Functions.PLUS);            }        }        return v;    } finally {        Closeables.close(iter, true);    }}
0
public static UpperTriangular loadUpperTriangularMatrix(Path glob, Configuration conf) throws IOException
{    try (SequenceFileDirValueIterator<VectorWritable> iter = new SequenceFileDirValueIterator<>(glob, PathType.GLOB, null, null, true, conf)) {        if (!iter.hasNext()) {            throw new IOException("No triangular matrices found");        }        Vector v = iter.next().get();        UpperTriangular result = new UpperTriangular(v);        if (iter.hasNext()) {            throw new IOException("Unexpected overrun in upper triangular matrix files");        }        return result;    }}
0
public static double[][] extractRawData(Matrix m)
{    int rows = m.numRows();    int cols = m.numCols();    double[][] result = new double[rows][];    for (int i = 0; i < rows; i++) {        result[i] = new double[cols];        for (int j = 0; j < cols; j++) {            result[i][j] = m.getQuick(i, j);        }    }    return result;}
0
public int getQ()
{    return q;}
0
public void setQ(int q)
{    this.q = q;}
0
public void setComputeU(boolean val)
{    computeU = val;}
0
public void setComputeV(boolean val)
{    computeV = val;}
0
public void setcUHalfSigma(boolean cUHat)
{    this.cUHalfSigma = cUHat;}
0
public void setcVHalfSigma(boolean cVHat)
{    this.cVHalfSigma = cVHat;}
0
public void setcUSigma(boolean cUSigma)
{    this.cUSigma = cUSigma;}
0
public void setcVSigma(boolean cVSigma)
{    this.cVSigma = cVSigma;}
0
public void setMinSplitSize(int size)
{    minSplitSize = size;}
0
public Vector getSingularValues()
{    return svalues;}
0
public String getUPath()
{    return uPath;}
0
public String getVPath()
{    return vPath;}
0
public String getuSigmaPath()
{    return uSigmaPath;}
0
public String getuHalfSigmaPath()
{    return uHalfSigmaPath;}
0
public String getvSigmaPath()
{    return vSigmaPath;}
0
public String getvHalfSigmaPath()
{    return vHalfSigmaPath;}
0
public boolean isOverwrite()
{    return overwrite;}
0
public void setOverwrite(boolean overwrite)
{    this.overwrite = overwrite;}
0
public int getOuterBlockHeight()
{    return outerBlockHeight;}
0
public void setOuterBlockHeight(int outerBlockHeight)
{    this.outerBlockHeight = outerBlockHeight;}
0
public int getAbtBlockHeight()
{    return abtBlockHeight;}
0
public void setAbtBlockHeight(int abtBlockHeight)
{    this.abtBlockHeight = abtBlockHeight;}
0
public boolean isBroadcast()
{    return broadcast;}
0
public void setBroadcast(boolean broadcast)
{    this.broadcast = broadcast;}
0
public Path getPcaMeanPath()
{    return pcaMeanPath;}
0
public void setPcaMeanPath(Path pcaMeanPath)
{    this.pcaMeanPath = pcaMeanPath;}
0
 long getOmegaSeed()
{    return omegaSeed;}
0
public void run() throws IOException
{    Deque<Closeable> closeables = Lists.newLinkedList();    try {        Class<? extends Writable> labelType = SSVDHelper.sniffInputLabelType(inputPath, conf);        FileSystem fs = FileSystem.get(conf);        Path qPath = new Path(outputPath, "Q-job");        Path btPath = new Path(outputPath, "Bt-job");        Path uHatPath = new Path(outputPath, "UHat");        Path svPath = new Path(outputPath, "Sigma");        Path uPath = new Path(outputPath, "U");        Path uSigmaPath = new Path(outputPath, "USigma");        Path uHalfSigmaPath = new Path(outputPath, "UHalfSigma");        Path vPath = new Path(outputPath, "V");        Path vHalfSigmaPath = new Path(outputPath, "VHalfSigma");        Path vSigmaPath = new Path(outputPath, "VSigma");        Path pcaBasePath = new Path(outputPath, "pca");        if (overwrite) {            fs.delete(outputPath, true);        }        if (pcaMeanPath != null) {            fs.mkdirs(pcaBasePath);        }        Random rnd = RandomUtils.getRandom();        omegaSeed = rnd.nextLong();        Path sbPath = null;        double xisquaredlen = 0.0;        if (pcaMeanPath != null) {            /*         * combute s_b0 if pca offset present.         *          * Just in case, we treat xi path as a possible reduce or otherwise         * multiple task output that we assume we need to sum up partial         * components. If it is just one file, it will work too.         */            Vector xi = SSVDHelper.loadAndSumUpVectors(pcaMeanPath, conf);            if (xi == null) {                throw new IOException(String.format("unable to load mean path xi from %s.", pcaMeanPath.toString()));            }            xisquaredlen = xi.dot(xi);            Omega omega = new Omega(omegaSeed, k + p);            Vector s_b0 = omega.mutlithreadedTRightMultiply(xi);            SSVDHelper.saveVector(s_b0, sbPath = new Path(pcaBasePath, "somega.seq"), conf);        }        /*       * if we work with pca offset, we need to precompute s_bq0 aka s_omega for       * jobs to use.       */        QJob.run(conf, inputPath, sbPath, qPath, ablockRows, minSplitSize, k, p, omegaSeed, reduceTasks);        /*       * restrict number of reducers to a reasonable number so we don't have to       * run too many additions in the frontend when reconstructing BBt for the       * last B' and BB' computations. The user may not realize that and gives a       * bit too many (I would be happy i that were ever the case though).       */        BtJob.run(conf, inputPath, qPath, pcaMeanPath, btPath, minSplitSize, k, p, outerBlockHeight, q <= 0 ? Math.min(1000, reduceTasks) : reduceTasks, broadcast, labelType, q <= 0);        sbPath = new Path(btPath, BtJob.OUTPUT_SB + "-*");        Path sqPath = new Path(btPath, BtJob.OUTPUT_SQ + "-*");                for (int i = 0; i < q; i++) {            qPath = new Path(outputPath, String.format("ABt-job-%d", i + 1));            Path btPathGlob = new Path(btPath, BtJob.OUTPUT_BT + "-*");            ABtDenseOutJob.run(conf, inputPath, btPathGlob, pcaMeanPath, sqPath, sbPath, qPath, ablockRows, minSplitSize, k, p, abtBlockHeight, reduceTasks, broadcast);            btPath = new Path(outputPath, String.format("Bt-job-%d", i + 1));            BtJob.run(conf, inputPath, qPath, pcaMeanPath, btPath, minSplitSize, k, p, outerBlockHeight, i == q - 1 ? Math.min(1000, reduceTasks) : reduceTasks, broadcast, labelType, i == q - 1);            sbPath = new Path(btPath, BtJob.OUTPUT_SB + "-*");            sqPath = new Path(btPath, BtJob.OUTPUT_SQ + "-*");        }        DenseSymmetricMatrix bbt = SSVDHelper.loadAndSumUpperTriangularMatricesAsSymmetric(new Path(btPath, BtJob.OUTPUT_BBT + "-*"), conf);                assert bbt.columnSize() == k + p;        /*       * we currently use a 3rd party in-core eigensolver. So we need just a       * dense array representation for it.       */        Matrix bbtSquare = new DenseMatrix(k + p, k + p);        bbtSquare.assign(bbt);                if (pcaMeanPath != null) {            Vector sq = SSVDHelper.loadAndSumUpVectors(sqPath, conf);            Vector sb = SSVDHelper.loadAndSumUpVectors(sbPath, conf);            Matrix mC = sq.cross(sb);            bbtSquare.assign(mC, Functions.MINUS);            bbtSquare.assign(mC.transpose(), Functions.MINUS);            Matrix outerSq = sq.cross(sq);            outerSq.assign(Functions.mult(xisquaredlen));            bbtSquare.assign(outerSq, Functions.PLUS);        }        EigenDecomposition eigen = new EigenDecomposition(bbtSquare);        Matrix uHat = eigen.getV();        svalues = eigen.getRealEigenvalues().clone();        svalues.assign(Functions.SQRT);                fs.mkdirs(uHatPath);        DistributedRowMatrixWriter.write(uHatPath = new Path(uHatPath, "uhat.seq"), conf, uHat);                SSVDHelper.saveVector(svalues, svPath = new Path(svPath, "svalues.seq"), conf);        UJob ujob = null;        if (computeU) {            ujob = new UJob();            ujob.run(conf, new Path(btPath, BtJob.OUTPUT_Q + "-*"), uHatPath, svPath, uPath, k, reduceTasks, labelType, OutputScalingEnum.NOSCALING);                }        UJob uhsjob = null;        if (cUHalfSigma) {            uhsjob = new UJob();            uhsjob.run(conf, new Path(btPath, BtJob.OUTPUT_Q + "-*"), uHatPath, svPath, uHalfSigmaPath, k, reduceTasks, labelType, OutputScalingEnum.HALFSIGMA);        }        UJob usjob = null;        if (cUSigma) {            usjob = new UJob();            usjob.run(conf, new Path(btPath, BtJob.OUTPUT_Q + "-*"), uHatPath, svPath, uSigmaPath, k, reduceTasks, labelType, OutputScalingEnum.SIGMA);        }        VJob vjob = null;        if (computeV) {            vjob = new VJob();            vjob.run(conf, new Path(btPath, BtJob.OUTPUT_BT + "-*"), pcaMeanPath, sqPath, uHatPath, svPath, vPath, k, reduceTasks, OutputScalingEnum.NOSCALING);        }        VJob vhsjob = null;        if (cVHalfSigma) {            vhsjob = new VJob();            vhsjob.run(conf, new Path(btPath, BtJob.OUTPUT_BT + "-*"), pcaMeanPath, sqPath, uHatPath, svPath, vHalfSigmaPath, k, reduceTasks, OutputScalingEnum.HALFSIGMA);        }        VJob vsjob = null;        if (cVSigma) {            vsjob = new VJob();            vsjob.run(conf, new Path(btPath, BtJob.OUTPUT_BT + "-*"), pcaMeanPath, sqPath, uHatPath, svPath, vSigmaPath, k, reduceTasks, OutputScalingEnum.SIGMA);        }        if (ujob != null) {            ujob.waitForCompletion();            this.uPath = uPath.toString();        }        if (uhsjob != null) {            uhsjob.waitForCompletion();            this.uHalfSigmaPath = uHalfSigmaPath.toString();        }        if (usjob != null) {            usjob.waitForCompletion();            this.uSigmaPath = uSigmaPath.toString();        }        if (vjob != null) {            vjob.waitForCompletion();            this.vPath = vPath.toString();        }        if (vhsjob != null) {            vhsjob.waitForCompletion();            this.vHalfSigmaPath = vHalfSigmaPath.toString();        }        if (vsjob != null) {            vsjob.waitForCompletion();            this.vSigmaPath = vSigmaPath.toString();        }    } catch (InterruptedException exc) {        throw new IOException("Interrupted", exc);    } catch (ClassNotFoundException exc) {        throw new IOException(exc);    } finally {        IOUtils.close(closeables);    }}
0
public void run(Configuration conf, Path inputPathQ, Path inputUHatPath, Path sigmaPath, Path outputPath, int k, int numReduceTasks, Class<? extends Writable> labelClass, SSVDSolver.OutputScalingEnum outputScaling) throws ClassNotFoundException, InterruptedException, IOException
{    job = new Job(conf);    job.setJobName("U-job");    job.setJarByClass(UJob.class);    job.setInputFormatClass(SequenceFileInputFormat.class);    job.setOutputFormatClass(SequenceFileOutputFormat.class);    FileInputFormat.setInputPaths(job, inputPathQ);    FileOutputFormat.setOutputPath(job, outputPath);        job.getConfiguration().set("mapreduce.output.basename", OUTPUT_U);    FileOutputFormat.setCompressOutput(job, true);    FileOutputFormat.setOutputCompressorClass(job, DefaultCodec.class);    SequenceFileOutputFormat.setOutputCompressionType(job, CompressionType.BLOCK);    job.setMapperClass(UMapper.class);    job.setMapOutputKeyClass(IntWritable.class);    job.setMapOutputValueClass(VectorWritable.class);    job.setOutputKeyClass(labelClass);    job.setOutputValueClass(VectorWritable.class);    job.getConfiguration().set(PROP_UHAT_PATH, inputUHatPath.toString());    job.getConfiguration().set(PROP_SIGMA_PATH, sigmaPath.toString());    job.getConfiguration().set(PROP_OUTPUT_SCALING, outputScaling.name());    job.getConfiguration().setInt(PROP_K, k);    job.setNumReduceTasks(0);    job.submit();}
0
public void waitForCompletion() throws IOException, ClassNotFoundException, InterruptedException
{    job.waitForCompletion(false);    if (!job.isSuccessful()) {        throw new IOException("U job unsuccessful.");    }}
0
protected void map(Writable key, VectorWritable value, Context context) throws IOException, InterruptedException
{    Vector qRow = value.get();    if (sValues != null) {        for (int i = 0; i < k; i++) {            uRow.setQuick(i, qRow.dot(uHat.viewColumn(i)) * sValues.getQuick(i));        }    } else {        for (int i = 0; i < k; i++) {            uRow.setQuick(i, qRow.dot(uHat.viewColumn(i)));        }    }    /*       * MAHOUT-1067: inherit A names too.       */    if (qRow instanceof NamedVector) {        uRowWritable.set(new NamedVector(uRow, ((NamedVector) qRow).getName()));    } else {        uRowWritable.set(uRow);    }        context.write(key, uRowWritable);}
0
protected void setup(Context context) throws IOException, InterruptedException
{    super.setup(context);    Path uHatPath = new Path(context.getConfiguration().get(PROP_UHAT_PATH));    Path sigmaPath = new Path(context.getConfiguration().get(PROP_SIGMA_PATH));    FileSystem fs = FileSystem.get(uHatPath.toUri(), context.getConfiguration());    uHat = SSVDHelper.drmLoadAsDense(fs, uHatPath, context.getConfiguration());        kp = uHat.columnSize();    k = context.getConfiguration().getInt(PROP_K, kp);    uRow = new DenseVector(k);    uRowWritable = new VectorWritable(uRow);    SSVDSolver.OutputScalingEnum outputScaling = SSVDSolver.OutputScalingEnum.valueOf(context.getConfiguration().get(PROP_OUTPUT_SCALING));    switch(outputScaling) {        case SIGMA:            sValues = SSVDHelper.loadVector(sigmaPath, context.getConfiguration());            break;        case HALFSIGMA:            sValues = SSVDHelper.loadVector(sigmaPath, context.getConfiguration());            sValues.assign(Functions.SQRT);            break;        default:    }}
0
protected void map(IntWritable key, VectorWritable value, Context context) throws IOException, InterruptedException
{    Vector bCol = value.get();    /*       * MAHOUT-817: PCA correction for B': b_{col=i} -= s_q * xi_{i}       */    if (xi != null) {        /*         * code defensively against shortened xi which may be externally         * supplied         */        int btIndex = key.get();        double xii = xi.size() > btIndex ? xi.getQuick(btIndex) : 0.0;        plusMult.setMultiplicator(-xii);        bCol.assign(sq, plusMult);    }    for (int i = 0; i < k; i++) {        vRow.setQuick(i, bCol.dot(uHat.viewColumn(i)) / sValues.getQuick(i));    }    context.write(key, vRowWritable);}
0
protected void setup(Context context) throws IOException, InterruptedException
{    super.setup(context);    Configuration conf = context.getConfiguration();    FileSystem fs = FileSystem.get(conf);    Path uHatPath = new Path(conf.get(PROP_UHAT_PATH));    Path sigmaPath = new Path(conf.get(PROP_SIGMA_PATH));    uHat = SSVDHelper.drmLoadAsDense(fs, uHatPath, conf);        kp = uHat.columnSize();    k = context.getConfiguration().getInt(PROP_K, kp);    vRow = new DenseVector(k);    vRowWritable = new VectorWritable(vRow);    sValues = SSVDHelper.loadVector(sigmaPath, conf);    SSVDSolver.OutputScalingEnum outputScaling = SSVDSolver.OutputScalingEnum.valueOf(context.getConfiguration().get(PROP_OUTPUT_SCALING));    switch(outputScaling) {        case SIGMA:            sValues.assign(1.0);            break;        case HALFSIGMA:            sValues = SSVDHelper.loadVector(sigmaPath, context.getConfiguration());            sValues.assign(Functions.SQRT);            break;        default:    }    /*       * PCA -related corrections (MAHOUT-817)       */    String xiPathStr = conf.get(PROP_XI_PATH);    if (xiPathStr != null) {        xi = SSVDHelper.loadAndSumUpVectors(new Path(xiPathStr), conf);        sq = SSVDHelper.loadAndSumUpVectors(new Path(conf.get(PROP_SQ_PATH)), conf);    }}
0
public void run(Configuration conf, Path inputPathBt, Path xiPath, Path sqPath, Path inputUHatPath, Path inputSigmaPath, Path outputPath, int k, int numReduceTasks, SSVDSolver.OutputScalingEnum outputScaling) throws ClassNotFoundException, InterruptedException, IOException
{    job = new Job(conf);    job.setJobName("V-job");    job.setJarByClass(VJob.class);    job.setInputFormatClass(SequenceFileInputFormat.class);    job.setOutputFormatClass(SequenceFileOutputFormat.class);    FileInputFormat.setInputPaths(job, inputPathBt);    FileOutputFormat.setOutputPath(job, outputPath);        job.getConfiguration().set("mapreduce.output.basename", OUTPUT_V);    FileOutputFormat.setCompressOutput(job, true);    FileOutputFormat.setOutputCompressorClass(job, DefaultCodec.class);    SequenceFileOutputFormat.setOutputCompressionType(job, CompressionType.BLOCK);    job.setMapOutputKeyClass(IntWritable.class);    job.setMapOutputValueClass(VectorWritable.class);    job.setOutputKeyClass(IntWritable.class);    job.setOutputValueClass(VectorWritable.class);    job.setMapperClass(VMapper.class);    job.getConfiguration().set(PROP_UHAT_PATH, inputUHatPath.toString());    job.getConfiguration().set(PROP_SIGMA_PATH, inputSigmaPath.toString());    job.getConfiguration().set(PROP_OUTPUT_SCALING, outputScaling.name());    job.getConfiguration().setInt(PROP_K, k);    job.setNumReduceTasks(0);    /*     * PCA-related options, MAHOUT-817     */    if (xiPath != null) {        job.getConfiguration().set(PROP_XI_PATH, xiPath.toString());        job.getConfiguration().set(PROP_SQ_PATH, sqPath.toString());    }    job.submit();}
0
public void waitForCompletion() throws IOException, ClassNotFoundException, InterruptedException
{    job.waitForCompletion(false);    if (!job.isSuccessful()) {        throw new IOException("V job unsuccessful.");    }}
0
protected void setup(Context context) throws IOException, InterruptedException
{    int k = context.getConfiguration().getInt(PROP_K, -1);    int p = context.getConfiguration().getInt(PROP_P, -1);    Validate.isTrue(k > 0, "invalid k parameter");    Validate.isTrue(p > 0, "invalid p parameter");    kp = k + p;    long omegaSeed = Long.parseLong(context.getConfiguration().get(PROP_OMEGA_SEED));    omega = new Omega(omegaSeed, k + p);    mYtY = new UpperTriangular(kp);            yRow = new DenseVector(kp);}
0
protected void map(Writable key, VectorWritable value, Context context) throws IOException, InterruptedException
{    omega.computeYRow(value.get(), yRow);    if (yRow.isDense()) {        for (int i = 0; i < kp; i++) {            double yi;            if ((yi = yRow.getQuick(i)) == 0.0) {                                continue;            }            for (int j = i; j < kp; j++) {                double yj;                if ((yj = yRow.getQuick(j)) != 0.0) {                    mYtY.setQuick(i, j, mYtY.getQuick(i, j) + yi * yj);                }            }        }    } else {        /*         * the disadvantage of using sparse vector (aside from the fact that we         * are creating some short-lived references) here is that we obviously         * do two times more iterations then necessary if y row is pretty dense.         */        for (Vector.Element eli : yRow.nonZeroes()) {            int i = eli.index();            for (Vector.Element elj : yRow.nonZeroes()) {                int j = elj.index();                if (j < i) {                    continue;                }                mYtY.setQuick(i, j, mYtY.getQuick(i, j) + eli.get() * elj.get());            }        }    }}
0
protected void cleanup(Context context) throws IOException, InterruptedException
{    context.write(new IntWritable(context.getTaskAttemptID().getTaskID().getId()), new VectorWritable(new DenseVector(mYtY.getData())));}
0
protected void setup(Context context) throws IOException, InterruptedException
{    int k = context.getConfiguration().getInt(PROP_K, -1);    int p = context.getConfiguration().getInt(PROP_P, -1);    Validate.isTrue(k > 0, "invalid k parameter");    Validate.isTrue(p > 0, "invalid p parameter");    accum.set(acc = new DenseVector(k + p));}
0
protected void cleanup(Context context) throws IOException, InterruptedException
{    context.write(new IntWritable(), accum);}
0
protected void reduce(IntWritable key, Iterable<VectorWritable> values, Context arg2) throws IOException, InterruptedException
{    for (VectorWritable vw : values) {        acc.addAll(vw.get());    }}
0
public static void run(Configuration conf, Path[] inputPaths, Path outputPath, int k, int p, long seed) throws ClassNotFoundException, InterruptedException, IOException
{    Job job = new Job(conf);    job.setJobName("YtY-job");    job.setJarByClass(YtYJob.class);    job.setInputFormatClass(SequenceFileInputFormat.class);    FileInputFormat.setInputPaths(job, inputPaths);    FileOutputFormat.setOutputPath(job, outputPath);    SequenceFileOutputFormat.setOutputCompressionType(job, CompressionType.BLOCK);    job.setMapOutputKeyClass(IntWritable.class);    job.setMapOutputValueClass(VectorWritable.class);    job.setOutputKeyClass(IntWritable.class);    job.setOutputValueClass(VectorWritable.class);    job.setMapperClass(YtYMapper.class);    job.getConfiguration().setLong(PROP_OMEGA_SEED, seed);    job.getConfiguration().setInt(PROP_K, k);    job.getConfiguration().setInt(PROP_P, p);    /*     * we must reduce to just one matrix which means we need only one reducer.     * But it's ok since each mapper outputs only one vector (a packed     * UpperTriangular) so even if there're thousands of mappers, one reducer     * should cope just fine.     */    job.setNumReduceTasks(1);    job.submit();    job.waitForCompletion(false);    if (!job.isSuccessful()) {        throw new IOException("YtY job unsuccessful.");    }}
0
public static Job createTimesSquaredJob(Vector v, Path matrixInputPath, Path outputVectorPath) throws IOException
{    return createTimesSquaredJob(new Configuration(), v, matrixInputPath, outputVectorPath);}
0
public static Job createTimesSquaredJob(Configuration initialConf, Vector v, Path matrixInputPath, Path outputVectorPath) throws IOException
{    return createTimesSquaredJob(initialConf, v, matrixInputPath, outputVectorPath, TimesSquaredMapper.class, VectorSummingReducer.class);}
0
public static Job createTimesJob(Vector v, int outDim, Path matrixInputPath, Path outputVectorPath) throws IOException
{    return createTimesJob(new Configuration(), v, outDim, matrixInputPath, outputVectorPath);}
0
public static Job createTimesJob(Configuration initialConf, Vector v, int outDim, Path matrixInputPath, Path outputVectorPath) throws IOException
{    return createTimesSquaredJob(initialConf, v, outDim, matrixInputPath, outputVectorPath, TimesMapper.class, VectorSummingReducer.class);}
0
public static Job createTimesSquaredJob(Vector v, Path matrixInputPath, Path outputVectorPathBase, Class<? extends TimesSquaredMapper> mapClass, Class<? extends VectorSummingReducer> redClass) throws IOException
{    return createTimesSquaredJob(new Configuration(), v, matrixInputPath, outputVectorPathBase, mapClass, redClass);}
0
public static Job createTimesSquaredJob(Configuration initialConf, Vector v, Path matrixInputPath, Path outputVectorPathBase, Class<? extends TimesSquaredMapper> mapClass, Class<? extends VectorSummingReducer> redClass) throws IOException
{    return createTimesSquaredJob(initialConf, v, v.size(), matrixInputPath, outputVectorPathBase, mapClass, redClass);}
0
public static Job createTimesSquaredJob(Vector v, int outputVectorDim, Path matrixInputPath, Path outputVectorPathBase, Class<? extends TimesSquaredMapper> mapClass, Class<? extends VectorSummingReducer> redClass) throws IOException
{    return createTimesSquaredJob(new Configuration(), v, outputVectorDim, matrixInputPath, outputVectorPathBase, mapClass, redClass);}
0
public static Job createTimesSquaredJob(Configuration initialConf, Vector v, int outputVectorDim, Path matrixInputPath, Path outputVectorPathBase, Class<? extends TimesSquaredMapper> mapClass, Class<? extends VectorSummingReducer> redClass) throws IOException
{    FileSystem fs = FileSystem.get(matrixInputPath.toUri(), initialConf);    matrixInputPath = fs.makeQualified(matrixInputPath);    outputVectorPathBase = fs.makeQualified(outputVectorPathBase);    long now = System.nanoTime();    Path inputVectorPath = new Path(outputVectorPathBase, INPUT_VECTOR + '/' + now);    SequenceFile.Writer inputVectorPathWriter = null;    try {        inputVectorPathWriter = new SequenceFile.Writer(fs, initialConf, inputVectorPath, NullWritable.class, VectorWritable.class);        inputVectorPathWriter.append(NullWritable.get(), new VectorWritable(v));    } finally {        Closeables.close(inputVectorPathWriter, false);    }    URI ivpURI = inputVectorPath.toUri();    DistributedCache.setCacheFiles(new URI[] { ivpURI }, initialConf);    Job job = HadoopUtil.prepareJob(matrixInputPath, new Path(outputVectorPathBase, OUTPUT_VECTOR_FILENAME), SequenceFileInputFormat.class, mapClass, NullWritable.class, VectorWritable.class, redClass, NullWritable.class, VectorWritable.class, SequenceFileOutputFormat.class, initialConf);    job.setCombinerClass(redClass);    job.setJobName("TimesSquaredJob: " + matrixInputPath);    Configuration conf = job.getConfiguration();    conf.set(INPUT_VECTOR, ivpURI.toString());    conf.setBoolean(IS_SPARSE_OUTPUT, !v.isDense());    conf.setInt(OUTPUT_VECTOR_DIMENSION, outputVectorDim);    return job;}
0
public static Vector retrieveTimesSquaredOutputVector(Path outputVectorTmpPath, Configuration conf) throws IOException
{    Path outputFile = new Path(outputVectorTmpPath, OUTPUT_VECTOR_FILENAME + "/part-r-00000");    SequenceFileValueIterator<VectorWritable> iterator = new SequenceFileValueIterator<>(outputFile, true, conf);    try {        return iterator.next().get();    } finally {        Closeables.close(iterator, true);    }}
0
 Vector getOutputVector()
{    return outputVector;}
0
protected void setup(Context ctx) throws IOException, InterruptedException
{    try {        Configuration conf = ctx.getConfiguration();        Path[] localFiles = DistributedCache.getLocalCacheFiles(conf);        Preconditions.checkArgument(localFiles != null && localFiles.length >= 1, "missing paths from the DistributedCache");        Path inputVectorPath = HadoopUtil.getSingleCachedFile(conf);        SequenceFileValueIterator<VectorWritable> iterator = new SequenceFileValueIterator<>(inputVectorPath, true, conf);        try {            inputVector = iterator.next().get();        } finally {            Closeables.close(iterator, true);        }        int outDim = conf.getInt(OUTPUT_VECTOR_DIMENSION, Integer.MAX_VALUE);        outputVector = conf.getBoolean(IS_SPARSE_OUTPUT, false) ? new RandomAccessSparseVector(outDim, 10) : new DenseVector(outDim);    } catch (IOException ioe) {        throw new IllegalStateException(ioe);    }}
0
protected void map(T key, VectorWritable v, Context context) throws IOException, InterruptedException
{    double d = scale(v);    if (d == 1.0) {        outputVector.assign(v.get(), Functions.PLUS);    } else if (d != 0.0) {        outputVector.assign(v.get(), Functions.plusMult(d));    }}
0
protected double scale(VectorWritable v)
{    return v.get().dot(inputVector);}
0
protected void cleanup(Context ctx) throws IOException, InterruptedException
{    ctx.write(NullWritable.get(), new VectorWritable(outputVector));}
0
protected void map(IntWritable rowNum, VectorWritable v, Context context) throws IOException, InterruptedException
{    double d = scale(v);    if (d != 0.0) {        getOutputVector().setQuick(rowNum.get(), d);    }}
0
protected void setup(Context ctx) throws IOException, InterruptedException
{    Configuration conf = ctx.getConfiguration();    int outputDimension = conf.getInt(OUTPUT_VECTOR_DIMENSION, Integer.MAX_VALUE);    outputVector = conf.getBoolean(IS_SPARSE_OUTPUT, false) ? new RandomAccessSparseVector(outputDimension, 10) : new DenseVector(outputDimension);}
0
protected void reduce(NullWritable key, Iterable<VectorWritable> vectors, Context ctx) throws IOException, InterruptedException
{    for (VectorWritable v : vectors) {        if (v != null) {            outputVector.assign(v.get(), Functions.PLUS);        }    }    ctx.write(NullWritable.get(), new VectorWritable(outputVector));}
0
public static void main(String[] args) throws Exception
{    ToolRunner.run(new TransposeJob(), args);}
0
public int run(String[] strings) throws Exception
{    addInputOption();    addOption("numRows", "nr", "Number of rows of the input matrix");    addOption("numCols", "nc", "Number of columns of the input matrix");    Map<String, List<String>> parsedArgs = parseArguments(strings);    if (parsedArgs == null) {        return -1;    }    int numRows = Integer.parseInt(getOption("numRows"));    int numCols = Integer.parseInt(getOption("numCols"));    DistributedRowMatrix matrix = new DistributedRowMatrix(getInputPath(), getTempPath(), numRows, numCols);    matrix.setConf(new Configuration(getConf()));    matrix.transpose();    return 0;}
0
public static Job buildTransposeJob(Path matrixInputPath, Path matrixOutputPath, int numInputRows) throws IOException
{    return buildTransposeJob(new Configuration(), matrixInputPath, matrixOutputPath, numInputRows);}
0
public static Job buildTransposeJob(Configuration initialConf, Path matrixInputPath, Path matrixOutputPath, int numInputRows) throws IOException
{    Job job = HadoopUtil.prepareJob(matrixInputPath, matrixOutputPath, SequenceFileInputFormat.class, TransposeMapper.class, IntWritable.class, VectorWritable.class, MergeVectorsReducer.class, IntWritable.class, VectorWritable.class, SequenceFileOutputFormat.class, initialConf);    job.setCombinerClass(MergeVectorsCombiner.class);    job.getConfiguration().setInt(TransposeMapper.NEW_NUM_COLS_PARAM, numInputRows);    job.setJobName("TransposeJob: " + matrixInputPath);    return job;}
0
public static void write(Path outputDir, Configuration conf, VectorIterable matrix) throws IOException
{    FileSystem fs = outputDir.getFileSystem(conf);    fs.delete(outputDir, true);    SequenceFile.Writer writer = SequenceFile.createWriter(fs, conf, outputDir, IntWritable.class, VectorWritable.class);    IntWritable topic = new IntWritable();    VectorWritable vector = new VectorWritable();    for (MatrixSlice slice : matrix) {        topic.set(slice.index());        vector.set(slice.vector());        writer.append(topic, vector);    }    writer.close();}
0
public static Matrix read(Configuration conf, Path... modelPaths) throws IOException
{    int numRows = -1;    int numCols = -1;    boolean sparse = false;    List<Pair<Integer, Vector>> rows = Lists.newArrayList();    for (Path modelPath : modelPaths) {        for (Pair<IntWritable, VectorWritable> row : new SequenceFileIterable<IntWritable, VectorWritable>(modelPath, true, conf)) {            rows.add(Pair.of(row.getFirst().get(), row.getSecond().get()));            numRows = Math.max(numRows, row.getFirst().get());            sparse = !row.getSecond().get().isDense();            if (numCols < 0) {                numCols = row.getSecond().get().size();            }        }    }    if (rows.isEmpty()) {        throw new IOException(Arrays.toString(modelPaths) + " have no vectors in it");    }    numRows++;    Vector[] arrayOfRows = new Vector[numRows];    for (Pair<Integer, Vector> pair : rows) {        arrayOfRows[pair.getFirst()] = pair.getSecond();    }    Matrix matrix;    if (sparse) {        matrix = new SparseRowMatrix(numRows, numCols, arrayOfRows);    } else {        matrix = new DenseMatrix(numRows, numCols);        for (int i = 0; i < numRows; i++) {            matrix.assignRow(i, arrayOfRows[i]);        }    }    return matrix;}
0
public static OpenObjectIntHashMap<String> readDictionary(Configuration conf, Path... dictPath)
{    OpenObjectIntHashMap<String> dictionary = new OpenObjectIntHashMap<>();    for (Path dictionaryFile : dictPath) {        for (Pair<Writable, IntWritable> record : new SequenceFileIterable<Writable, IntWritable>(dictionaryFile, true, conf)) {            dictionary.put(record.getFirst().toString(), record.getSecond().get());        }    }    return dictionary;}
0
public static String[] invertDictionary(OpenObjectIntHashMap<String> termIdMap)
{    int maxTermId = -1;    for (String term : termIdMap.keys()) {        maxTermId = Math.max(maxTermId, termIdMap.get(term));    }    maxTermId++;    String[] dictionary = new String[maxTermId];    for (String term : termIdMap.keys()) {        dictionary[termIdMap.get(term)] = term;    }    return dictionary;}
0
public Vector getVector()
{    return vectorWritable.get();}
0
public void setVector(Vector vector)
{    vectorWritable.set(vector);}
0
public void setLabels(int[] labels)
{    this.labels = labels;}
0
public int[] getLabels()
{    return labels;}
0
public void readFields(DataInput in) throws IOException
{    vectorWritable.readFields(in);    int labelSize = in.readInt();    labels = new int[labelSize];    for (int i = 0; i < labelSize; i++) {        labels[i] = in.readInt();    }}
0
public void write(DataOutput out) throws IOException
{    vectorWritable.write(out);    out.writeInt(labels.length);    for (int label : labels) {        out.writeInt(label);    }}
0
public static MultiLabelVectorWritable read(DataInput in) throws IOException
{    MultiLabelVectorWritable writable = new MultiLabelVectorWritable();    writable.readFields(in);    return writable;}
0
public static void write(DataOutput out, SequentialAccessSparseVector ssv, int[] labels) throws IOException
{    new MultiLabelVectorWritable(ssv, labels).write(out);}
0
public void add(Vector vector)
{    referenceVectors.add(vector);}
0
public int size()
{    return referenceVectors.size();}
0
public List<WeightedThing<Vector>> search(Vector query, int limit)
{    Preconditions.checkArgument(limit > 0, "limit must be greater then 0!");    limit = Math.min(limit, referenceVectors.size());            PriorityQueue<WeightedThing<Integer>> bestNeighbors = new PriorityQueue<>(limit, Ordering.natural().reverse());        List<WeightedThing<Vector>> results = Lists.newArrayListWithCapacity(limit);    int rowNumber = 0;    for (Vector row : referenceVectors) {        double distance = distanceMeasure.distance(query, row);                if (bestNeighbors.size() < limit || bestNeighbors.peek().getWeight() > distance) {            bestNeighbors.add(new WeightedThing<>(rowNumber, distance));            if (bestNeighbors.size() > limit) {                bestNeighbors.poll();            } else {                                                results.add(null);            }        }        ++rowNumber;    }    for (int i = limit - 1; i >= 0; --i) {        WeightedThing<Integer> neighbor = bestNeighbors.poll();        results.set(i, new WeightedThing<>(referenceVectors.get(neighbor.getValue()), neighbor.getWeight()));    }    return results;}
0
public WeightedThing<Vector> searchFirst(Vector query, boolean differentThanQuery)
{    double bestDistance = Double.POSITIVE_INFINITY;    Vector bestVector = null;    for (Vector row : referenceVectors) {        double distance = distanceMeasure.distance(query, row);        if (distance < bestDistance && (!differentThanQuery || !row.equals(query))) {            bestDistance = distance;            bestVector = row;        }    }    return new WeightedThing<>(bestVector, bestDistance);}
0
public List<List<WeightedThing<Vector>>> search(Iterable<WeightedVector> queries, final int limit, int numThreads) throws InterruptedException
{    ExecutorService executor = Executors.newFixedThreadPool(numThreads);    List<Callable<Object>> tasks = Lists.newArrayList();    final List<List<WeightedThing<Vector>>> results = Lists.newArrayList();    int i = 0;    for (final Vector query : queries) {        results.add(null);        final int index = i++;        tasks.add(new Callable<Object>() {            @Override            public Object call() throws Exception {                results.set(index, BruteSearch.this.search(query, limit));                return null;            }        });    }    executor.invokeAll(tasks);    executor.shutdown();    return results;}
0
public Object call() throws Exception
{    results.set(index, BruteSearch.this.search(query, limit));    return null;}
0
public Iterator<Vector> iterator()
{    return referenceVectors.iterator();}
0
public boolean remove(Vector query, double epsilon)
{    int rowNumber = 0;    for (Vector row : referenceVectors) {        double distance = distanceMeasure.distance(query, row);        if (distance < epsilon) {            referenceVectors.remove(rowNumber);            return true;        }        rowNumber++;    }    return false;}
0
public void clear()
{    referenceVectors.clear();}
0
private void initialize(int numDimensions)
{    if (initialized) {        return;    }    basisMatrix = RandomProjector.generateBasisNormal(numProjections, numDimensions);    initialized = true;}
0
public void add(Vector vector)
{    initialize(vector.size());    pendingAdditions.add(vector);}
0
public int size()
{    return pendingAdditions.size() + scalarProjections.get(0).size() - numPendingRemovals;}
0
public List<WeightedThing<Vector>> search(Vector query, int limit)
{    reindex(false);    Set<Vector> candidates = Sets.newHashSet();    Vector projection = basisMatrix.times(query);    for (int i = 0; i < basisMatrix.numRows(); ++i) {        List<WeightedThing<Vector>> currProjections = scalarProjections.get(i);        int middle = Collections.binarySearch(currProjections, new WeightedThing<Vector>(projection.get(i)));        if (middle < 0) {            middle = -(middle + 1);        }        for (int j = Math.max(0, middle - searchSize); j < Math.min(currProjections.size(), middle + searchSize + 1); ++j) {            if (currProjections.get(j).getValue() == null) {                continue;            }            candidates.add(currProjections.get(j).getValue());        }    }    List<WeightedThing<Vector>> top = Lists.newArrayListWithCapacity(candidates.size() + pendingAdditions.size());    for (Vector candidate : Iterables.concat(candidates, pendingAdditions)) {        top.add(new WeightedThing<>(candidate, distanceMeasure.distance(candidate, query)));    }    Collections.sort(top);    return top.subList(0, Math.min(top.size(), limit));}
0
public WeightedThing<Vector> searchFirst(Vector query, boolean differentThanQuery)
{    reindex(false);    double bestDistance = Double.POSITIVE_INFINITY;    Vector bestVector = null;    Vector projection = basisMatrix.times(query);    for (int i = 0; i < basisMatrix.numRows(); ++i) {        List<WeightedThing<Vector>> currProjections = scalarProjections.get(i);        int middle = Collections.binarySearch(currProjections, new WeightedThing<Vector>(projection.get(i)));        if (middle < 0) {            middle = -(middle + 1);        }        for (int j = Math.max(0, middle - searchSize); j < Math.min(currProjections.size(), middle + searchSize + 1); ++j) {            if (currProjections.get(j).getValue() == null) {                continue;            }            Vector vector = currProjections.get(j).getValue();            double distance = distanceMeasure.distance(vector, query);            if (distance < bestDistance && (!differentThanQuery || !vector.equals(query))) {                bestDistance = distance;                bestVector = vector;            }        }    }    for (Vector vector : pendingAdditions) {        double distance = distanceMeasure.distance(vector, query);        if (distance < bestDistance && (!differentThanQuery || !vector.equals(query))) {            bestDistance = distance;            bestVector = vector;        }    }    return new WeightedThing<>(bestVector, bestDistance);}
0
public boolean remove(Vector vector, double epsilon)
{    WeightedThing<Vector> closestPair = searchFirst(vector, false);    if (distanceMeasure.distance(closestPair.getValue(), vector) > epsilon) {        return false;    }    boolean isProjected = true;    Vector projection = basisMatrix.times(vector);    for (int i = 0; i < basisMatrix.numRows(); ++i) {        List<WeightedThing<Vector>> currProjections = scalarProjections.get(i);        WeightedThing<Vector> searchedThing = new WeightedThing<>(projection.get(i));        int middle = Collections.binarySearch(currProjections, searchedThing);        if (middle < 0) {            isProjected = false;            break;        }                        scalarProjections.get(i).set(middle, searchedThing);    }    if (isProjected) {        ++numPendingRemovals;        return true;    }    for (int i = 0; i < pendingAdditions.size(); ++i) {        if (pendingAdditions.get(i).equals(vector)) {            pendingAdditions.remove(i);            break;        }    }    return true;}
0
private void reindex(boolean force)
{    int numProjected = scalarProjections.get(0).size();    if (force || pendingAdditions.size() > ADDITION_THRESHOLD * numProjected || numPendingRemovals > REMOVAL_THRESHOLD * numProjected) {                                List<List<WeightedThing<Vector>>> scalarProjections = Lists.newArrayListWithCapacity(numProjections);        for (int i = 0; i < numProjections; ++i) {            if (i == 0) {                scalarProjections.add(Lists.newArrayList(this.scalarProjections.get(i)));            } else {                scalarProjections.add(this.scalarProjections.get(i));            }        }                for (Vector pending : pendingAdditions) {            Vector projection = basisMatrix.times(pending);            for (int i = 0; i < numProjections; ++i) {                scalarProjections.get(i).add(new WeightedThing<>(pending, projection.get(i)));            }        }        pendingAdditions.clear();                for (int i = 0; i < numProjections; ++i) {            List<WeightedThing<Vector>> currProjections = scalarProjections.get(i);            for (WeightedThing<Vector> v : currProjections) {                if (v.getValue() == null) {                    v.setWeight(Double.POSITIVE_INFINITY);                }            }            Collections.sort(currProjections);            for (int j = 0; j < numPendingRemovals; ++j) {                currProjections.remove(currProjections.size() - 1);            }        }        numPendingRemovals = 0;        this.scalarProjections = scalarProjections;    }}
0
public void clear()
{    pendingAdditions.clear();    for (int i = 0; i < numProjections; ++i) {        scalarProjections.get(i).clear();    }    numPendingRemovals = 0;}
0
public Iterator<Vector> iterator()
{    reindex(true);    return new AbstractIterator<Vector>() {        private final Iterator<WeightedThing<Vector>> data = scalarProjections.get(0).iterator();        @Override        protected Vector computeNext() {            do {                if (!data.hasNext()) {                    return endOfData();                }                WeightedThing<Vector> next = data.next();                if (next.getValue() != null) {                    return next.getValue();                }            } while (true);        }    };}
0
protected Vector computeNext()
{    do {        if (!data.hasNext()) {            return endOfData();        }        WeightedThing<Vector> next = data.next();        if (next.getValue() != null) {            return next.getValue();        }    } while (true);}
0
public static long computeHash64(Vector vector, Matrix projection)
{    long hash = 0;    for (Element element : projection.times(vector).nonZeroes()) {        if (element.get() > 0) {            hash += 1L << element.index();        }    }    return hash;}
0
public static HashedVector hash(WeightedVector v, Matrix projection)
{    return hash(v, projection, 0);}
0
public static HashedVector hash(WeightedVector v, Matrix projection, long mask)
{    return new HashedVector(v, projection, mask);}
0
public int hammingDistance(long otherHash)
{    return Long.bitCount(hash ^ otherHash);}
0
public long getHash()
{    return hash;}
0
public String toString()
{    return String.format("index=%d, hash=%08x, v=%s", getIndex(), hash, getVector());}
0
public boolean equals(Object o)
{    if (this == o) {        return true;    }    if (!(o instanceof HashedVector)) {        return o instanceof Vector && this.minus((Vector) o).norm(1) == 0;    }    HashedVector v = (HashedVector) o;    return v.hash == this.hash && this.minus(v).norm(1) == 0;}
0
public int hashCode()
{    int result = super.hashCode();    result = 31 * result + (int) (hash ^ (hash >>> 32));    return result;}
0
private void initialize(int numDimensions)
{    if (initialized) {        return;    }    initialized = true;    projection = RandomProjector.generateBasisNormal(BITS, numDimensions);}
0
private PriorityQueue<WeightedThing<Vector>> searchInternal(Vector query)
{    long queryHash = HashedVector.computeHash64(query, projection);        PriorityQueue<WeightedThing<Vector>> top = Searcher.getCandidateQueue(getSearchSize());                OnlineSummarizer[] distribution = new OnlineSummarizer[BITS + 1];    for (int i = 0; i < BITS + 1; i++) {        distribution[i] = new OnlineSummarizer();    }    distanceEvaluations = 0;            int[] hashCounts = new int[BITS + 1];            int hashLimit = BITS;    int limitCount = 0;    double distanceLimit = Double.POSITIVE_INFINITY;        for (HashedVector vector : trainingVectors) {                        int bitDot = vector.hammingDistance(queryHash);        if (bitDot <= hashLimit) {            distanceEvaluations++;            double distance = distanceMeasure.distance(query, vector);            distribution[bitDot].add(distance);            if (distance < distanceLimit) {                top.insertWithOverflow(new WeightedThing<Vector>(vector, distance));                if (top.size() == searchSize) {                    distanceLimit = top.top().getWeight();                }                hashCounts[bitDot]++;                limitCount++;                while (hashLimit > 0 && limitCount - hashCounts[hashLimit - 1] > searchSize) {                    hashLimit--;                    limitCount -= hashCounts[hashLimit];                }                if (hashLimitStrategy >= 0) {                    while (hashLimit < MAX_HASH_LIMIT && distribution[hashLimit].getCount() > MIN_DISTRIBUTION_COUNT && ((1 - hashLimitStrategy) * distribution[hashLimit].getQuartile(0) + hashLimitStrategy * distribution[hashLimit].getQuartile(1)) < distanceLimit) {                        limitCount += hashCounts[hashLimit];                        hashLimit++;                    }                }            }        }    }    return top;}
0
public List<WeightedThing<Vector>> search(Vector query, int limit)
{    PriorityQueue<WeightedThing<Vector>> top = searchInternal(query);    List<WeightedThing<Vector>> results = Lists.newArrayListWithExpectedSize(top.size());    while (top.size() != 0) {        WeightedThing<Vector> wv = top.pop();        results.add(new WeightedThing<>(((HashedVector) wv.getValue()).getVector(), wv.getWeight()));    }    Collections.reverse(results);    if (limit < results.size()) {        results = results.subList(0, limit);    }    return results;}
0
public WeightedThing<Vector> searchFirst(Vector query, boolean differentThanQuery)
{        PriorityQueue<WeightedThing<Vector>> top = searchInternal(query);        while (top.size() > 2) {        top.pop();    }        if (top.size() < 2) {        return removeHash(top.pop());    }        WeightedThing<Vector> secondBest = top.pop();    WeightedThing<Vector> best = top.pop();        if (differentThanQuery && best.getValue().equals(query)) {        best = secondBest;    }    return removeHash(best);}
0
protected static WeightedThing<Vector> removeHash(WeightedThing<Vector> input)
{    return new WeightedThing<>(((HashedVector) input.getValue()).getVector(), input.getWeight());}
0
public void add(Vector vector)
{    initialize(vector.size());    trainingVectors.add(new HashedVector(vector, projection, HashedVector.INVALID_INDEX, BIT_MASK));}
0
public int size()
{    return trainingVectors.size();}
0
public int getSearchSize()
{    return searchSize;}
0
public void setSearchSize(int size)
{    searchSize = size;}
0
public void setRaiseHashLimitStrategy(double strategy)
{    hashLimitStrategy = strategy;}
0
public int resetEvaluationCount()
{    int result = distanceEvaluations;    distanceEvaluations = 0;    return result;}
0
public Iterator<Vector> iterator()
{    return Iterators.transform(trainingVectors.iterator(), new Function<HashedVector, Vector>() {        @Override        public Vector apply(org.apache.mahout.math.neighborhood.HashedVector input) {            Preconditions.checkNotNull(input);                        return input.getVector();        }    });}
0
public Vector apply(org.apache.mahout.math.neighborhood.HashedVector input)
{    Preconditions.checkNotNull(input);        return input.getVector();}
0
public boolean remove(Vector v, double epsilon)
{    return trainingVectors.remove(new HashedVector(v, projection, HashedVector.INVALID_INDEX, BIT_MASK));}
0
public void clear()
{    trainingVectors.clear();}
0
private void initialize(int numDimensions)
{    if (initialized) {        return;    }    initialized = true;    basisMatrix = RandomProjector.generateBasisNormal(numProjections, numDimensions);    scalarProjections = Lists.newArrayList();    for (int i = 0; i < numProjections; ++i) {        scalarProjections.add(TreeMultiset.<WeightedThing<Vector>>create());    }}
0
public void add(Vector vector)
{    initialize(vector.size());    Vector projection = basisMatrix.times(vector);        int i = 0;    for (TreeMultiset<WeightedThing<Vector>> s : scalarProjections) {        s.add(new WeightedThing<>(vector, projection.get(i++)));    }    int numVectors = scalarProjections.get(0).size();    for (TreeMultiset<WeightedThing<Vector>> s : scalarProjections) {        Preconditions.checkArgument(s.size() == numVectors, "Number of vectors in projection sets " + "differ");        double firstWeight = s.firstEntry().getElement().getWeight();        for (WeightedThing<Vector> w : s) {            Preconditions.checkArgument(firstWeight <= w.getWeight(), "Weights not in non-decreasing " + "order");            firstWeight = w.getWeight();        }    }}
0
public int size()
{    if (scalarProjections == null) {        return 0;    }    return scalarProjections.get(0).size();}
0
public List<WeightedThing<Vector>> search(Vector query, int limit)
{    Set<Vector> candidates = Sets.newHashSet();    Iterator<? extends Vector> projections = basisMatrix.iterator();    for (TreeMultiset<WeightedThing<Vector>> v : scalarProjections) {        Vector basisVector = projections.next();        WeightedThing<Vector> projectedQuery = new WeightedThing<>(query, query.dot(basisVector));        for (WeightedThing<Vector> candidate : Iterables.concat(Iterables.limit(v.tailMultiset(projectedQuery, BoundType.CLOSED), searchSize), Iterables.limit(v.headMultiset(projectedQuery, BoundType.OPEN).descendingMultiset(), searchSize))) {            candidates.add(candidate.getValue());        }    }            List<WeightedThing<Vector>> top = Lists.newArrayList();    for (Vector candidate : candidates) {        top.add(new WeightedThing<>(candidate, distanceMeasure.distance(query, candidate)));    }    Collections.sort(top);    return top.subList(0, Math.min(limit, top.size()));}
0
public WeightedThing<Vector> searchFirst(Vector query, boolean differentThanQuery)
{    double bestDistance = Double.POSITIVE_INFINITY;    Vector bestVector = null;    Iterator<? extends Vector> projections = basisMatrix.iterator();    for (TreeMultiset<WeightedThing<Vector>> v : scalarProjections) {        Vector basisVector = projections.next();        WeightedThing<Vector> projectedQuery = new WeightedThing<>(query, query.dot(basisVector));        for (WeightedThing<Vector> candidate : Iterables.concat(Iterables.limit(v.tailMultiset(projectedQuery, BoundType.CLOSED), searchSize), Iterables.limit(v.headMultiset(projectedQuery, BoundType.OPEN).descendingMultiset(), searchSize))) {            double distance = distanceMeasure.distance(query, candidate.getValue());            if (distance < bestDistance && (!differentThanQuery || !candidate.getValue().equals(query))) {                bestDistance = distance;                bestVector = candidate.getValue();            }        }    }    return new WeightedThing<>(bestVector, bestDistance);}
0
public Iterator<Vector> iterator()
{    return new AbstractIterator<Vector>() {        private final Iterator<WeightedThing<Vector>> projected = scalarProjections.get(0).iterator();        @Override        protected Vector computeNext() {            if (!projected.hasNext()) {                return endOfData();            }            return projected.next().getValue();        }    };}
0
protected Vector computeNext()
{    if (!projected.hasNext()) {        return endOfData();    }    return projected.next().getValue();}
0
public boolean remove(Vector vector, double epsilon)
{    WeightedThing<Vector> toRemove = searchFirst(vector, false);    if (toRemove.getWeight() < epsilon) {        Iterator<? extends Vector> basisVectors = basisMatrix.iterator();        for (TreeMultiset<WeightedThing<Vector>> projection : scalarProjections) {            if (!projection.remove(new WeightedThing<>(vector, vector.dot(basisVectors.next())))) {                throw new RuntimeException("Internal inconsistency in ProjectionSearch");            }        }        return true;    } else {        return false;    }}
0
public void clear()
{    if (scalarProjections == null) {        return;    }    for (TreeMultiset<WeightedThing<Vector>> set : scalarProjections) {        set.clear();    }}
0
public DistanceMeasure getDistanceMeasure()
{    return distanceMeasure;}
0
public List<List<WeightedThing<Vector>>> search(Iterable<? extends Vector> queries, int limit)
{    List<List<WeightedThing<Vector>>> results = Lists.newArrayListWithExpectedSize(Iterables.size(queries));    for (Vector query : queries) {        results.add(search(query, limit));    }    return results;}
0
public List<WeightedThing<Vector>> searchFirst(Iterable<? extends Vector> queries, boolean differentThanQuery)
{    List<WeightedThing<Vector>> results = Lists.newArrayListWithExpectedSize(Iterables.size(queries));    for (Vector query : queries) {        results.add(searchFirst(query, differentThanQuery));    }    return results;}
0
public void addAll(Iterable<? extends Vector> data)
{    for (Vector vector : data) {        add(vector);    }}
0
public void addAllMatrixSlices(Iterable<MatrixSlice> data)
{    for (MatrixSlice slice : data) {        add(slice.vector());    }}
0
public void addAllMatrixSlicesAsWeightedVectors(Iterable<MatrixSlice> data)
{    for (MatrixSlice slice : data) {        add(new WeightedVector(slice.vector(), 1, slice.index()));    }}
0
public boolean remove(Vector v, double epsilon)
{    throw new UnsupportedOperationException("Can't remove a vector from a " + this.getClass().getName());}
0
public void clear()
{    throw new UnsupportedOperationException("Can't remove vectors from a " + this.getClass().getName());}
0
public static PriorityQueue<WeightedThing<Vector>> getCandidateQueue(int limit)
{    return new PriorityQueue<WeightedThing<Vector>>(limit) {        @Override        protected boolean lessThan(WeightedThing<Vector> a, WeightedThing<Vector> b) {            return a.getWeight() > b.getWeight();        }    };}
0
protected boolean lessThan(WeightedThing<Vector> a, WeightedThing<Vector> b)
{    return a.getWeight() > b.getWeight();}
0
public double apply(double ignored)
{    return sample();}
0
public Integer sample()
{    double u = rand.nextDouble() * (alpha + weight);    for (int j = 0; j < weights.size(); j++) {                if (u < weights.get(j) - discount) {            weights.set(j, weights.get(j) + 1);            weight++;            return j;        } else {            u -= weights.get(j) - discount;        }    }            weights.add(1);    weight++;    return weights.size() - 1;}
0
public int size()
{    return weights.size();}
0
public int count()
{    return (int) weight;}
0
public int count(int j)
{    Preconditions.checkArgument(j >= 0);    if (j < weights.size()) {        return (int) weights.get(j);    } else {        return 0;    }}
0
public Double sample()
{    return sample(gen.nextDouble());}
0
public double sample(double u)
{    if (exceedMinimum && u < x[0]) {                if (u == 0) {            u = 1.0e-16;        }        return y[0] + Math.log(u / x[0]) * x[0] * (y[1] - y[0]) / (x[1] - x[0]);    } else if (exceedMaximum && u > x[n - 1]) {        if (u == 1) {            u = 1 - 1.0e-16;        }                double dy = y[n - 1] - y[n - 2];        double dx = x[n - 1] - x[n - 2];        return y[n - 1] - Math.log((1 - u) / (1 - x[n - 1])) * (1 - x[n - 1]) * dy / dx;    } else {                for (int i = 1; i < n; i++) {            if (x[i] > u) {                double dy = y[i] - y[i - 1];                double dx = x[i] - x[i - 1];                return y[i - 1] + (u - x[i - 1]) * dy / dx;            }        }        throw new RuntimeException(String.format("Can't happen (%.3f is not in [%.3f,%.3f]", u, x[0], x[n - 1]));    }}
0
public static IndianBuffet<Integer> createIntegerDocumentSampler(double alpha)
{    return new IndianBuffet<>(alpha, new IdentityConverter());}
0
public static IndianBuffet<String> createTextDocumentSampler(double alpha)
{    return new IndianBuffet<>(alpha, new WordConverter());}
0
public List<T> sample()
{    List<T> r = Lists.newArrayList();    if (documents == 0) {        double n = new PoissonSampler(alpha).sample();        for (int i = 0; i < n; i++) {            r.add(converter.convert(i));            count.add(1);        }        documents++;    } else {        documents++;        int i = 0;        for (double cnt : count) {            if (gen.nextDouble() < cnt / documents) {                r.add(converter.convert(i));                count.set(i, count.get(i) + 1);            }            i++;        }        int newItems = new PoissonSampler(alpha / documents).sample().intValue();        for (int j = 0; j < newItems; j++) {            r.add(converter.convert(i + j));            count.add(1);        }    }    return r;}
0
public Integer convert(int i)
{    return i;}
0
public String convert(int i)
{    return String.valueOf(i);}
0
public boolean processLine(String line)
{    Iterables.addAll(theWords, onSpace.split(line));    return true;}
0
public List<String> getResult()
{    return theWords;}
0
public String convert(int i)
{    if (i < words.size()) {        return words.get(i);    } else {        return "w_" + i;    }}
0
public T sample()
{    if (gen.nextDouble() >= p) {        return delegate.sample();    } else {        return missingMarker;    }}
0
public void add(T value, double w)
{    Preconditions.checkNotNull(value);    Preconditions.checkArgument(!items.containsKey(value));    int n = this.weight.size();    if (n == 1) {        weight.add(w);        values.add(value);        items.put(value, 1);    } else {                weight.add(weight.get(n / 2));        values.add(values.get(n / 2));        items.put(values.get(n / 2), n);        n++;                items.put(value, n);        this.weight.add(w);        values.add(value);                while (n > 1) {            n /= 2;            this.weight.set(n, this.weight.get(n) + w);        }    }}
0
public double getWeight(T value)
{    if (items.containsKey(value)) {        return weight.get(items.get(value));    } else {        return 0;    }}
0
public double getProbability(T value)
{    if (items.containsKey(value)) {        return weight.get(items.get(value)) / weight.get(1);    } else {        return 0;    }}
0
public double getWeight()
{    if (weight.size() > 1) {        return weight.get(1);    } else {        return 0;    }}
0
public void delete(T value)
{    set(value, 0);}
0
public void set(T value, double newP)
{    Preconditions.checkArgument(items.containsKey(value));    int n = items.get(value);    if (newP <= 0) {                        items.remove(value);    }    double oldP = weight.get(n);    while (n > 0) {        weight.set(n, weight.get(n) - oldP + newP);        n /= 2;    }}
0
public T sample()
{    Preconditions.checkArgument(!weight.isEmpty());    return sample(rand.nextDouble());}
0
public T sample(double u)
{    u *= weight.get(1);    int n = 1;    while (2 * n < weight.size()) {                double left = weight.get(2 * n);        if (u <= left) {            n = 2 * n;        } else {            u -= left;            n = 2 * n + 1;        }    }    return values.get(n);}
0
 List<Double> getWeights()
{    List<Double> r = Lists.newArrayList();    int i = Integer.highestOneBit(weight.size());    while (i < weight.size()) {        r.add(weight.get(i));        i++;    }    i /= 2;    while (i < Integer.highestOneBit(weight.size())) {        r.add(weight.get(i));        i++;    }    return r;}
0
public Iterator<T> iterator()
{    return new AbstractIterator<T>() {        Iterator<T> valuesIterator = Iterables.skip(values, 1).iterator();        @Override        protected T computeNext() {            while (valuesIterator.hasNext()) {                T next = valuesIterator.next();                if (items.containsKey(next)) {                    return next;                }            }            return endOfData();        }    };}
0
protected T computeNext()
{    while (valuesIterator.hasNext()) {        T next = valuesIterator.next();        if (items.containsKey(next)) {            return next;        }    }    return endOfData();}
0
public Vector sample()
{    Vector v = new DenseVector(dimension).assign(new DoubleFunction() {        @Override        public double apply(double ignored) {            return gen.nextGaussian();        }    });    if (mean != null) {        if (scale != null) {            return scale.times(v).plus(mean);        } else {            return v.plus(mean);        }    } else {        if (scale != null) {            return scale.times(v);        } else {            return v;        }    }}
0
public double apply(double ignored)
{    return gen.nextGaussian();}
0
public Vector getScale()
{    return mean;}
0
public Double sample()
{    return rand.nextGaussian() * sd + mean;}
0
public Double sample()
{    return sample(gen.nextDouble());}
0
 double sample(double u)
{    if (u < limit) {        List<WeightedThing<Integer>> steps = Lists.newArrayList();        limit = 1;        int i = 0;        while (u / 20 < limit) {            double pdf = pd.probability(i);            limit -= pdf;            steps.add(new WeightedThing<>(i, pdf));            i++;        }        steps.add(new WeightedThing<>(steps.size(), limit));        partial = new Multinomial<>(steps);    }    return partial.sample(u);}
0
public static Matrix generateBasisNormal(int projectedVectorSize, int vectorSize)
{    Matrix basisMatrix = new DenseMatrix(projectedVectorSize, vectorSize);    basisMatrix.assign(new Normal());    for (MatrixSlice row : basisMatrix) {        row.vector().assign(row.normalize());    }    return basisMatrix;}
0
public static Matrix generateBasisPlusMinusOne(int projectedVectorSize, int vectorSize)
{    Matrix basisMatrix = new DenseMatrix(projectedVectorSize, vectorSize);    for (int i = 0; i < projectedVectorSize; ++i) {        for (int j = 0; j < vectorSize; ++j) {            basisMatrix.set(i, j, RandomUtils.nextInt(2) == 0 ? 1 : -1);        }    }    for (MatrixSlice row : basisMatrix) {        row.vector().assign(row.normalize());    }    return basisMatrix;}
0
public static Matrix generateBasisZeroPlusMinusOne(int projectedVectorSize, int vectorSize)
{    Matrix basisMatrix = new DenseMatrix(projectedVectorSize, vectorSize);    Multinomial<Double> choice = new Multinomial<>();    choice.add(0.0, 2 / 3.0);    choice.add(Math.sqrt(3.0), 1 / 6.0);    choice.add(-Math.sqrt(3.0), 1 / 6.0);    for (int i = 0; i < projectedVectorSize; ++i) {        for (int j = 0; j < vectorSize; ++j) {            basisMatrix.set(i, j, choice.sample());        }    }    for (MatrixSlice row : basisMatrix) {        row.vector().assign(row.normalize());    }    return basisMatrix;}
0
public static List<Vector> generateVectorBasis(int projectedVectorSize, int vectorSize)
{    DoubleFunction random = new Normal();    List<Vector> basisVectors = Lists.newArrayList();    for (int i = 0; i < projectedVectorSize; ++i) {        Vector basisVector = new DenseVector(vectorSize);        basisVector.assign(random);        basisVector.normalize();        basisVectors.add(basisVector);    }    return basisVectors;}
0
public T getValue()
{    return value;}
0
public double getWeight()
{    return weight;}
0
public void setWeight(double weight)
{    this.weight = weight;}
0
public int compareTo(WeightedThing<T> other)
{    return Double.compare(this.weight, other.weight);}
0
public boolean equals(Object o)
{    if (o instanceof WeightedThing) {        @SuppressWarnings("unchecked")        WeightedThing<T> other = (WeightedThing<T>) o;        return weight == other.weight && value.equals(other.value);    }    return false;}
0
public int hashCode()
{    return 31 * RandomUtils.hashDouble(weight) + value.hashCode();}
0
public Vector getSingularValues()
{    return new DenseVector(svd.getSingularValues());}
0
public Matrix getU()
{        return cd1.solveRight(y).times(svd.getU());}
0
public Matrix getV()
{        return cd2.solveRight(b.transpose()).times(svd.getV());}
0
public void computeV(File tmpDir, int ncols) throws IOException
{        for (int j = 0; j < ncols; j += columnsPerSlice) {        File bPath = bFile(tmpDir, j);        if (bPath.exists()) {            MatrixWritable m = new MatrixWritable();            try (DataInputStream in = new DataInputStream(new FileInputStream(bPath))) {                m.readFields(in);            }            m.set(l2.solveRight(m.get().transpose()).times(svd.getV()));            try (DataOutputStream out = new DataOutputStream(new FileOutputStream(new File(tmpDir, String.format("V-%s", bPath.getName().replaceAll(".*-", "")))))) {                m.write(out);            }        }    }}
0
public void computeU(Iterable<File> partsOfA, File tmpDir) throws IOException
{        for (File file : partsOfA) {        MatrixWritable m = new MatrixWritable();        m.readFields(new DataInputStream(new FileInputStream(file)));        Matrix aI = m.get();        Matrix y = aI.times(new RandomTrinaryMatrix(seed, aI.numCols(), dim, false));        Matrix uI = r2.solveRight(y).times(svd.getU());        m.set(uI);        try (DataOutputStream out = new DataOutputStream(new FileOutputStream(new File(tmpDir, String.format("U-%s", file.getName().replaceAll(".*-", "")))))) {            m.write(out);        }    }}
0
private static void addToSavedCopy(File file, Matrix matrix) throws IOException
{    MatrixWritable mw = new MatrixWritable();    if (file.exists()) {        try (DataInputStream in = new DataInputStream(new FileInputStream(file))) {            mw.readFields(in);        }        mw.get().assign(matrix, Functions.PLUS);    } else {        mw.set(matrix);    }    try (DataOutputStream out = new DataOutputStream(new FileOutputStream(file))) {        mw.write(out);    }}
0
private static File bFile(File tmpDir, int j)
{    return new File(tmpDir, String.format("B-%09d", j));}
0
public Vector getSingularValues()
{    return new DenseVector(svd.getSingularValues());}
0
public double addSample(int category, String groupKey, double score)
{    return addSample(category, score);}
0
public double addSample(int category, double score)
{    int n = (int) samples.get(category);    if (n < HISTORY) {        scores.set(category, n, score);    } else {        switch(policy) {            case FIFO:                scores.set(category, n % HISTORY, score);                break;            case FAIR:                int j1 = random.nextInt(n + 1);                if (j1 < HISTORY) {                    scores.set(category, j1, score);                }                break;            case RANDOM:                int j2 = random.nextInt(HISTORY);                scores.set(category, j2, score);                break;            default:                throw new IllegalStateException("Unknown policy: " + policy);        }    }    samples.set(category, n + 1);    if (samples.minValue() >= 1) {                Vector row = scores.viewRow(1 - category);        double m = 0.0;        double count = 0.0;        for (Vector.Element element : row.all()) {            double v = element.get();            if (Double.isNaN(v)) {                continue;            }            count++;            if (score > v) {                m++;                                    } else if (score == v) {                m += 0.5;            }        }        averages.set(category, averages.get(category) + (m / count - averages.get(category)) / Math.min(windowSize, samples.get(category)));    }    return auc();}
0
public double auc()
{        return (1 - averages.get(0) + averages.get(1)) / 2;}
0
public double value()
{    return auc();}
0
public void setPolicy(ReplacementPolicy policy)
{    this.policy = policy;}
0
public void setWindowSize(int windowSize)
{    this.windowSize = windowSize;}
0
public void write(DataOutput out) throws IOException
{    out.writeInt(windowSize);    out.writeInt(policy.ordinal());    MatrixWritable.writeMatrix(out, scores);    VectorWritable.writeVector(out, averages);    VectorWritable.writeVector(out, samples);}
0
public void readFields(DataInput in) throws IOException
{    windowSize = in.readInt();    policy = ReplacementPolicy.values()[in.readInt()];    scores = MatrixWritable.readMatrix(in);    averages = VectorWritable.readVector(in);    samples = VectorWritable.readVector(in);}
0
public double addSample(int category, String groupKey, double score)
{    if (groupKey == null) {        addSample(category, score);    }    OnlineAuc group = map.get(groupKey);    if (group == null) {        group = new GlobalOnlineAuc();        if (policy != null) {            group.setPolicy(policy);        }        if (windowSize > 0) {            group.setWindowSize(windowSize);        }        map.put(groupKey, group);    }    return group.addSample(category, score);}
0
public double addSample(int category, double score)
{    throw new UnsupportedOperationException("Can't add to " + this.getClass() + " without group key");}
0
public double auc()
{    double sum = 0;    for (OnlineAuc auc : map.values()) {        sum += auc.auc();    }    return sum / map.size();}
0
public void setPolicy(GlobalOnlineAuc.ReplacementPolicy policy)
{    this.policy = policy;    for (OnlineAuc auc : map.values()) {        auc.setPolicy(policy);    }}
0
public void setWindowSize(int windowSize)
{    this.windowSize = windowSize;    for (OnlineAuc auc : map.values()) {        auc.setWindowSize(windowSize);    }}
0
public void write(DataOutput out) throws IOException
{    out.writeInt(map.size());    for (Map.Entry<String, OnlineAuc> entry : map.entrySet()) {        out.writeUTF(entry.getKey());        PolymorphicWritable.write(out, entry.getValue());    }    out.writeInt(policy.ordinal());    out.writeInt(windowSize);}
0
public void readFields(DataInput in) throws IOException
{    int n = in.readInt();    map.clear();    for (int i = 0; i < n; i++) {        String key = in.readUTF();        map.put(key, PolymorphicWritable.read(in, OnlineAuc.class));    }    policy = GlobalOnlineAuc.ReplacementPolicy.values()[in.readInt()];    windowSize = in.readInt();}
0
public void add(double sample)
{    n++;    double oldMean = mean;    mean += (sample - mean) / n;    double diff = (sample - mean) * (sample - oldMean);    variance += (diff - variance) / n;    quantiles.add(sample);}
0
public int getCount()
{    return n;}
0
public double getMean()
{    return mean;}
0
public double getSD()
{    return Math.sqrt(variance);}
0
public double getMin()
{    return getQuartile(0);}
0
public double getMax()
{    return getQuartile(4);}
0
public double getQuartile(int i)
{    return quantiles.quantile(0.25 * i);}
0
public double quantile(double q)
{    return quantiles.quantile(q);}
0
public double getMedian()
{    return getQuartile(2);}
0
public int sample(Vector distribution)
{    return sample(samplerFor(distribution));}
0
public int sample()
{    Preconditions.checkNotNull(sampler, "Sampler must have been constructed with a distribution, or else sample(Vector) should be used to sample");    return sample(sampler);}
0
private static double[] samplerFor(Vector vectorDistribution)
{    int size = vectorDistribution.size();    double[] partition = new double[size];    double norm = vectorDistribution.norm(1);    double sum = 0;    for (int i = 0; i < size; i++) {        sum += vectorDistribution.get(i) / norm;        partition[i] = sum;    }    return partition;}
0
private int sample(double[] sampler)
{    int index = Arrays.binarySearch(sampler, random.nextDouble());    return index < 0 ? -(index + 1) : index;}
0
protected void reduce(GramKey key, Iterable<Gram> values, Context context) throws IOException, InterruptedException
{    int freq = 0;    Gram value = null;        for (Gram value1 : values) {        value = value1;        freq += value.getFrequency();    }    if (value != null) {        value.setFrequency(freq);        context.write(key, value);    }}
0
public static void main(String[] args) throws Exception
{    ToolRunner.run(new CollocDriver(), args);}
0
public int run(String[] args) throws Exception
{    addInputOption();    addOutputOption();    addOption(DefaultOptionCreator.numReducersOption().create());    addOption("maxNGramSize", "ng", "(Optional) The max size of ngrams to create (2 = bigrams, 3 = trigrams, etc) default: 2", String.valueOf(DEFAULT_MAX_NGRAM_SIZE));    addOption("minSupport", "s", "(Optional) Minimum Support. Default Value: " + CollocReducer.DEFAULT_MIN_SUPPORT, String.valueOf(CollocReducer.DEFAULT_MIN_SUPPORT));    addOption("minLLR", "ml", "(Optional)The minimum Log Likelihood Ratio(Float)  Default is " + LLRReducer.DEFAULT_MIN_LLR, String.valueOf(LLRReducer.DEFAULT_MIN_LLR));    addOption(DefaultOptionCreator.overwriteOption().create());    addOption("analyzerName", "a", "The class name of the analyzer to use for preprocessing", null);    addFlag("preprocess", "p", "If set, input is SequenceFile<Text,Text> where the value is the document, " + " which will be tokenized using the specified analyzer.");    addFlag("unigram", "u", "If set, unigrams will be emitted in the final output alongside collocations");    Map<String, List<String>> argMap = parseArguments(args);    if (argMap == null) {        return -1;    }    Path input = getInputPath();    Path output = getOutputPath();    int maxNGramSize = DEFAULT_MAX_NGRAM_SIZE;    if (hasOption("maxNGramSize")) {        try {            maxNGramSize = Integer.parseInt(getOption("maxNGramSize"));        } catch (NumberFormatException ex) {                    }    }        if (hasOption(DefaultOptionCreator.OVERWRITE_OPTION)) {        HadoopUtil.delete(getConf(), output);    }    int minSupport = CollocReducer.DEFAULT_MIN_SUPPORT;    if (getOption("minSupport") != null) {        minSupport = Integer.parseInt(getOption("minSupport"));    }        float minLLRValue = LLRReducer.DEFAULT_MIN_LLR;    if (getOption("minLLR") != null) {        minLLRValue = Float.parseFloat(getOption("minLLR"));    }        int reduceTasks = DEFAULT_PASS1_NUM_REDUCE_TASKS;    if (getOption("maxRed") != null) {        reduceTasks = Integer.parseInt(getOption("maxRed"));    }        boolean emitUnigrams = argMap.containsKey("emitUnigrams");    if (argMap.containsKey("preprocess")) {                Class<? extends Analyzer> analyzerClass = StandardAnalyzer.class;        if (getOption("analyzerName") != null) {            String className = getOption("analyzerName");            analyzerClass = Class.forName(className).asSubclass(Analyzer.class);                                    AnalyzerUtils.createAnalyzer(analyzerClass);        }        Path tokenizedPath = new Path(output, DocumentProcessor.TOKENIZED_DOCUMENT_OUTPUT_FOLDER);        DocumentProcessor.tokenizeDocuments(input, analyzerClass, tokenizedPath, getConf());        input = tokenizedPath;    } else {            }        long ngramCount = generateCollocations(input, output, getConf(), emitUnigrams, maxNGramSize, reduceTasks, minSupport);        computeNGramsPruneByLLR(output, getConf(), ngramCount, emitUnigrams, minLLRValue, reduceTasks);    return 0;}
1
public static void generateAllGrams(Path input, Path output, Configuration baseConf, int maxNGramSize, int minSupport, float minLLRValue, int reduceTasks) throws IOException, InterruptedException, ClassNotFoundException
{        long ngramCount = generateCollocations(input, output, baseConf, true, maxNGramSize, reduceTasks, minSupport);        computeNGramsPruneByLLR(output, baseConf, ngramCount, true, minLLRValue, reduceTasks);}
0
private static long generateCollocations(Path input, Path output, Configuration baseConf, boolean emitUnigrams, int maxNGramSize, int reduceTasks, int minSupport) throws IOException, ClassNotFoundException, InterruptedException
{    Configuration con = new Configuration(baseConf);    con.setBoolean(EMIT_UNIGRAMS, emitUnigrams);    con.setInt(CollocMapper.MAX_SHINGLE_SIZE, maxNGramSize);    con.setInt(CollocReducer.MIN_SUPPORT, minSupport);    Job job = new Job(con);    job.setJobName(CollocDriver.class.getSimpleName() + ".generateCollocations:" + input);    job.setJarByClass(CollocDriver.class);    job.setMapOutputKeyClass(GramKey.class);    job.setMapOutputValueClass(Gram.class);    job.setPartitionerClass(GramKeyPartitioner.class);    job.setGroupingComparatorClass(GramKeyGroupComparator.class);    job.setOutputKeyClass(Gram.class);    job.setOutputValueClass(Gram.class);    job.setCombinerClass(CollocCombiner.class);    FileInputFormat.setInputPaths(job, input);    Path outputPath = new Path(output, SUBGRAM_OUTPUT_DIRECTORY);    FileOutputFormat.setOutputPath(job, outputPath);    job.setInputFormatClass(SequenceFileInputFormat.class);    job.setMapperClass(CollocMapper.class);    job.setOutputFormatClass(SequenceFileOutputFormat.class);    job.setReducerClass(CollocReducer.class);    job.setNumReduceTasks(reduceTasks);    boolean succeeded = job.waitForCompletion(true);    if (!succeeded) {        throw new IllegalStateException("Job failed!");    }    return job.getCounters().findCounter(CollocMapper.Count.NGRAM_TOTAL).getValue();}
0
private static void computeNGramsPruneByLLR(Path output, Configuration baseConf, long nGramTotal, boolean emitUnigrams, float minLLRValue, int reduceTasks) throws IOException, InterruptedException, ClassNotFoundException
{    Configuration conf = new Configuration(baseConf);    conf.setLong(LLRReducer.NGRAM_TOTAL, nGramTotal);    conf.setBoolean(EMIT_UNIGRAMS, emitUnigrams);    conf.setFloat(LLRReducer.MIN_LLR, minLLRValue);    Job job = new Job(conf);    job.setJobName(CollocDriver.class.getSimpleName() + ".computeNGrams: " + output);    job.setJarByClass(CollocDriver.class);    job.setMapOutputKeyClass(Gram.class);    job.setMapOutputValueClass(Gram.class);    job.setOutputKeyClass(Text.class);    job.setOutputValueClass(DoubleWritable.class);    FileInputFormat.setInputPaths(job, new Path(output, SUBGRAM_OUTPUT_DIRECTORY));    Path outPath = new Path(output, NGRAM_OUTPUT_DIRECTORY);    FileOutputFormat.setOutputPath(job, outPath);    job.setMapperClass(Mapper.class);    job.setInputFormatClass(SequenceFileInputFormat.class);    job.setOutputFormatClass(SequenceFileOutputFormat.class);    job.setReducerClass(LLRReducer.class);    job.setNumReduceTasks(reduceTasks);    boolean succeeded = job.waitForCompletion(true);    if (!succeeded) {        throw new IllegalStateException("Job failed!");    }}
0
protected void map(Text key, StringTuple value, final Context context) throws IOException, InterruptedException
{    try (ShingleFilter sf = new ShingleFilter(new IteratorTokenStream(value.getEntries().iterator()), maxShingleSize)) {        sf.reset();                int count = 0;        OpenObjectIntHashMap<String> ngrams = new OpenObjectIntHashMap<>(value.getEntries().size() * (maxShingleSize - 1));        OpenObjectIntHashMap<String> unigrams = new OpenObjectIntHashMap<>(value.getEntries().size());        do {            String term = sf.getAttribute(CharTermAttribute.class).toString();            String type = sf.getAttribute(TypeAttribute.class).type();            if ("shingle".equals(type)) {                count++;                ngrams.adjustOrPutValue(term, 1, 1);            } else if (emitUnigrams && !term.isEmpty()) {                                unigrams.adjustOrPutValue(term, 1, 1);            }        } while (sf.incrementToken());        final GramKey gramKey = new GramKey();        ngrams.forEachPair(new ObjectIntProcedure<String>() {            @Override            public boolean apply(String term, int frequency) {                                                int i = term.lastIndexOf(' ');                if (i != -1) {                    try {                        Gram ngram = new Gram(term, frequency, Gram.Type.NGRAM);                        Gram head = new Gram(term.substring(0, i), frequency, Gram.Type.HEAD);                        Gram tail = new Gram(term.substring(i + 1), frequency, Gram.Type.TAIL);                        gramKey.set(head, EMPTY);                        context.write(gramKey, head);                        gramKey.set(head, ngram.getBytes());                        context.write(gramKey, ngram);                        gramKey.set(tail, EMPTY);                        context.write(gramKey, tail);                        gramKey.set(tail, ngram.getBytes());                        context.write(gramKey, ngram);                    } catch (IOException | InterruptedException e) {                        throw new IllegalStateException(e);                    }                }                return true;            }        });        unigrams.forEachPair(new ObjectIntProcedure<String>() {            @Override            public boolean apply(String term, int frequency) {                try {                    Gram unigram = new Gram(term, frequency, Gram.Type.UNIGRAM);                    gramKey.set(unigram, EMPTY);                    context.write(gramKey, unigram);                } catch (IOException | InterruptedException e) {                    throw new IllegalStateException(e);                }                return true;            }        });        context.getCounter(Count.NGRAM_TOTAL).increment(count);        sf.end();    }}
0
public boolean apply(String term, int frequency)
{            int i = term.lastIndexOf(' ');    if (i != -1) {        try {            Gram ngram = new Gram(term, frequency, Gram.Type.NGRAM);            Gram head = new Gram(term.substring(0, i), frequency, Gram.Type.HEAD);            Gram tail = new Gram(term.substring(i + 1), frequency, Gram.Type.TAIL);            gramKey.set(head, EMPTY);            context.write(gramKey, head);            gramKey.set(head, ngram.getBytes());            context.write(gramKey, ngram);            gramKey.set(tail, EMPTY);            context.write(gramKey, tail);            gramKey.set(tail, ngram.getBytes());            context.write(gramKey, ngram);        } catch (IOException | InterruptedException e) {            throw new IllegalStateException(e);        }    }    return true;}
0
public boolean apply(String term, int frequency)
{    try {        Gram unigram = new Gram(term, frequency, Gram.Type.UNIGRAM);        gramKey.set(unigram, EMPTY);        context.write(gramKey, unigram);    } catch (IOException | InterruptedException e) {        throw new IllegalStateException(e);    }    return true;}
0
protected void setup(Context context) throws IOException, InterruptedException
{    super.setup(context);    Configuration conf = context.getConfiguration();    this.maxShingleSize = conf.getInt(MAX_SHINGLE_SIZE, DEFAULT_MAX_SHINGLE_SIZE);    this.emitUnigrams = conf.getBoolean(CollocDriver.EMIT_UNIGRAMS, CollocDriver.DEFAULT_EMIT_UNIGRAMS);    if (log.isInfoEnabled()) {                    }}
1
protected void reduce(GramKey key, Iterable<Gram> values, Context context) throws IOException, InterruptedException
{    Gram.Type keyType = key.getType();    if (keyType == Gram.Type.UNIGRAM) {                processUnigram(values.iterator(), context);    } else if (keyType == Gram.Type.HEAD || keyType == Gram.Type.TAIL) {                processSubgram(values.iterator(), context);    } else {        context.getCounter(Skipped.MALFORMED_TYPES).increment(1);    }}
0
protected void setup(Context context) throws IOException, InterruptedException
{    super.setup(context);    Configuration conf = context.getConfiguration();    this.minSupport = conf.getInt(MIN_SUPPORT, DEFAULT_MIN_SUPPORT);    boolean emitUnigrams = conf.getBoolean(CollocDriver.EMIT_UNIGRAMS, CollocDriver.DEFAULT_EMIT_UNIGRAMS);        }
1
protected void processUnigram(Iterator<Gram> values, Context context) throws IOException, InterruptedException
{    int freq = 0;    Gram value = null;        while (values.hasNext()) {        value = values.next();        freq += value.getFrequency();    }    if (freq < minSupport) {        context.getCounter(Skipped.LESS_THAN_MIN_SUPPORT).increment(1);        return;    }    value.setFrequency(freq);    context.write(value, value);}
0
protected void processSubgram(Iterator<Gram> values, Context context) throws IOException, InterruptedException
{    Gram subgram = null;    Gram currentNgram = null;    while (values.hasNext()) {        Gram value = values.next();        if (value.getType() == Gram.Type.HEAD || value.getType() == Gram.Type.TAIL) {                        if (subgram == null) {                subgram = new Gram(value);            } else {                subgram.incrementFrequency(value.getFrequency());            }        } else if (!value.equals(currentNgram)) {                        if (currentNgram != null) {                if (currentNgram.getFrequency() < minSupport) {                    context.getCounter(Skipped.LESS_THAN_MIN_SUPPORT).increment(1);                } else {                    context.write(currentNgram, subgram);                }            }            currentNgram = new Gram(value);        } else {            currentNgram.incrementFrequency(value.getFrequency());        }    }        if (currentNgram != null) {        if (currentNgram.getFrequency() < minSupport) {            context.getCounter(Skipped.LESS_THAN_MIN_SUPPORT).increment(1);            return;        }        context.write(currentNgram, subgram);    }}
0
public String toString()
{    return String.valueOf(x);}
0
public byte[] getBytes()
{    return bytes;}
0
public int getLength()
{    return length;}
0
public Type getType()
{    return decodeType(bytes, 0);}
0
public String getString()
{    try {        return Text.decode(bytes, 1, length - 1);    } catch (CharacterCodingException e) {        throw new IllegalStateException("Should not have happened " + e);    }}
0
public int getFrequency()
{    return frequency;}
0
public void setFrequency(int frequency)
{    this.frequency = frequency;}
0
public void incrementFrequency(int i)
{    this.frequency += i;}
0
public void readFields(DataInput in) throws IOException
{    int newLength = Varint.readUnsignedVarInt(in);    setCapacity(newLength, false);    in.readFully(bytes, 0, newLength);    int newFrequency = Varint.readUnsignedVarInt(in);    length = newLength;    frequency = newFrequency;}
0
public void write(DataOutput out) throws IOException
{    Varint.writeUnsignedVarInt(length, out);    out.write(bytes, 0, length);    Varint.writeUnsignedVarInt(frequency, out);}
0
private void setCapacity(int len, boolean keepData)
{        len++;    if (bytes == null || bytes.length < len) {        byte[] newBytes = new byte[len];        if (bytes != null && keepData) {            System.arraycopy(bytes, 0, newBytes, 0, length);        }        bytes = newBytes;    }}
0
public String toString()
{    return '\'' + getString() + "'[" + getType() + "]:" + frequency;}
0
public static void encodeType(Type type, byte[] buf, int offset)
{    switch(type) {        case HEAD:            buf[offset] = 0x1;            break;        case TAIL:            buf[offset] = 0x2;            break;        case UNIGRAM:            buf[offset] = 0x3;            break;        case NGRAM:            buf[offset] = 0x4;            break;        default:            throw new IllegalStateException("switch/case problem in encodeType");    }}
0
public static Type decodeType(byte[] buf, int offset)
{    switch(buf[offset]) {        case 0x1:            return Type.HEAD;        case 0x2:            return Type.TAIL;        case 0x3:            return Type.UNIGRAM;        case 0x4:            return Type.NGRAM;        default:            throw new IllegalStateException("switch/case problem in decodeType");    }}
0
public void set(Gram gram, byte[] order)
{    primaryLength = gram.getLength();    length = primaryLength + order.length;    setCapacity(length, false);    System.arraycopy(gram.getBytes(), 0, bytes, 0, primaryLength);    if (order.length > 0) {        System.arraycopy(order, 0, bytes, primaryLength, order.length);    }}
0
public byte[] getBytes()
{    return bytes;}
0
public int getLength()
{    return length;}
0
public int getPrimaryLength()
{    return primaryLength;}
0
public void readFields(DataInput in) throws IOException
{    int newLength = Varint.readUnsignedVarInt(in);    int newPrimaryLength = Varint.readUnsignedVarInt(in);    setCapacity(newLength, false);    in.readFully(bytes, 0, newLength);    length = newLength;    primaryLength = newPrimaryLength;}
0
public void write(DataOutput out) throws IOException
{    Varint.writeUnsignedVarInt(length, out);    Varint.writeUnsignedVarInt(primaryLength, out);    out.write(bytes, 0, length);}
0
private void setCapacity(int len, boolean keepData)
{    if (bytes == null || bytes.length < len) {        byte[] newBytes = new byte[len];        if (bytes != null && keepData) {            System.arraycopy(bytes, 0, newBytes, 0, length);        }        bytes = newBytes;    }}
0
public Type getType()
{    return Gram.decodeType(bytes, 0);}
0
public String getPrimaryString()
{    try {        return Text.decode(bytes, 1, primaryLength - 1);    } catch (CharacterCodingException e) {        throw new IllegalStateException(e);    }}
0
public String toString()
{    return '\'' + getPrimaryString() + "'[" + getType() + ']';}
0
public int compare(WritableComparable a, WritableComparable b)
{    GramKey gka = (GramKey) a;    GramKey gkb = (GramKey) b;    return WritableComparator.compareBytes(gka.getBytes(), 0, gka.getPrimaryLength(), gkb.getBytes(), 0, gkb.getPrimaryLength());}
0
public int getPartition(GramKey key, Gram value, int numPartitions)
{    int hash = 1;    byte[] bytes = key.getBytes();    int length = key.getPrimaryLength();        for (int i = 1; i < length; i++) {        hash = (31 * hash) + bytes[i];    }    return (hash & Integer.MAX_VALUE) % numPartitions;}
0
protected void reduce(Gram ngram, Iterable<Gram> values, Context context) throws IOException, InterruptedException
{    int[] gramFreq = { -1, -1 };    if (ngram.getType() == Gram.Type.UNIGRAM && emitUnigrams) {        DoubleWritable dd = new DoubleWritable(ngram.getFrequency());        Text t = new Text(ngram.getString());        context.write(t, dd);        return;    }            String[] gram = new String[2];    for (Gram value : values) {        int pos = value.getType() == Gram.Type.HEAD ? 0 : 1;        if (gramFreq[pos] != -1) {                        if (value.getType() == Gram.Type.HEAD) {                context.getCounter(Skipped.EXTRA_HEAD).increment(1);            } else {                context.getCounter(Skipped.EXTRA_TAIL).increment(1);            }            return;        }        gram[pos] = value.getString();        gramFreq[pos] = value.getFrequency();    }    if (gramFreq[0] == -1) {                context.getCounter(Skipped.MISSING_HEAD).increment(1);        return;    }    if (gramFreq[1] == -1) {                context.getCounter(Skipped.MISSING_TAIL).increment(1);        return;    }    long k11 = ngram.getFrequency();    /* a&b */    long k12 = gramFreq[0] - ngram.getFrequency();    /* a&!b */    long k21 = gramFreq[1] - ngram.getFrequency();    /* !b&a */    long k22 = ngramTotal - (gramFreq[0] + gramFreq[1] - ngram.getFrequency());    /* !a&!b */    double llr;    try {        llr = ll.logLikelihoodRatio(k11, k12, k21, k22);    } catch (IllegalArgumentException ex) {        context.getCounter(Skipped.LLR_CALCULATION_ERROR).increment(1);                return;    }    if (llr < minLLRValue) {        context.getCounter(Skipped.LESS_THAN_MIN_LLR).increment(1);    } else {        context.write(new Text(ngram.getString()), new DoubleWritable(llr));    }}
1
protected void setup(Context context) throws IOException, InterruptedException
{    super.setup(context);    Configuration conf = context.getConfiguration();    this.ngramTotal = conf.getLong(NGRAM_TOTAL, -1);    this.minLLRValue = conf.getFloat(MIN_LLR, DEFAULT_MIN_LLR);    this.emitUnigrams = conf.getBoolean(CollocDriver.EMIT_UNIGRAMS, CollocDriver.DEFAULT_EMIT_UNIGRAMS);        if (ngramTotal == -1) {        throw new IllegalStateException("No NGRAM_TOTAL available in job config");    }}
1
public double logLikelihoodRatio(long k11, long k12, long k21, long k22)
{    return LogLikelihood.logLikelihoodRatio(k11, k12, k21, k22);}
0
public static void mergePartialVectors(Iterable<Path> partialVectorPaths, Path output, Configuration baseConf, float normPower, boolean logNormalize, int dimension, boolean sequentialAccess, boolean namedVector, int numReducers) throws IOException, InterruptedException, ClassNotFoundException
{    Preconditions.checkArgument(normPower == NO_NORMALIZING || normPower >= 0, "If specified normPower must be nonnegative", normPower);    Preconditions.checkArgument(normPower == NO_NORMALIZING || (normPower > 1 && !Double.isInfinite(normPower)) || !logNormalize, "normPower must be > 1 and not infinite if log normalization is chosen", normPower);    Configuration conf = new Configuration(baseConf);        conf.set("io.serializations", "org.apache.hadoop.io.serializer.JavaSerialization," + "org.apache.hadoop.io.serializer.WritableSerialization");    conf.setBoolean(SEQUENTIAL_ACCESS, sequentialAccess);    conf.setBoolean(NAMED_VECTOR, namedVector);    conf.setInt(DIMENSION, dimension);    conf.setFloat(NORMALIZATION_POWER, normPower);    conf.setBoolean(LOG_NORMALIZE, logNormalize);    Job job = new Job(conf);    job.setJobName("PartialVectorMerger::MergePartialVectors");    job.setJarByClass(PartialVectorMerger.class);    job.setOutputKeyClass(Text.class);    job.setOutputValueClass(VectorWritable.class);    FileInputFormat.setInputPaths(job, getCommaSeparatedPaths(partialVectorPaths));    FileOutputFormat.setOutputPath(job, output);    job.setMapperClass(Mapper.class);    job.setInputFormatClass(SequenceFileInputFormat.class);    job.setReducerClass(PartialVectorMergeReducer.class);    job.setOutputFormatClass(SequenceFileOutputFormat.class);    job.setNumReduceTasks(numReducers);    HadoopUtil.delete(conf, output);    boolean succeeded = job.waitForCompletion(true);    if (!succeeded) {        throw new IllegalStateException("Job failed!");    }}
0
private static String getCommaSeparatedPaths(Iterable<Path> paths)
{    StringBuilder commaSeparatedPaths = new StringBuilder(100);    String sep = "";    for (Path path : paths) {        commaSeparatedPaths.append(sep).append(path.toString());        sep = ",";    }    return commaSeparatedPaths.toString();}
0
protected void reduce(WritableComparable<?> key, Iterable<VectorWritable> values, Context context) throws IOException, InterruptedException
{    Vector vector = new RandomAccessSparseVector(dimension, 10);    for (VectorWritable value : values) {        vector.assign(value.get(), Functions.PLUS);    }    if (normPower != PartialVectorMerger.NO_NORMALIZING) {        if (logNormalize) {            vector = vector.logNormalize(normPower);        } else {            vector = vector.normalize(normPower);        }    }    if (sequentialAccess) {        vector = new SequentialAccessSparseVector(vector);    }    if (namedVector) {        vector = new NamedVector(vector, key.toString());    }        if (vector.getNumNondefaultElements() > 0) {        VectorWritable vectorWritable = new VectorWritable(vector);        context.write(key, vectorWritable);    }}
0
protected void setup(Context context) throws IOException, InterruptedException
{    super.setup(context);    Configuration conf = context.getConfiguration();    normPower = conf.getFloat(PartialVectorMerger.NORMALIZATION_POWER, PartialVectorMerger.NO_NORMALIZING);    dimension = conf.getInt(PartialVectorMerger.DIMENSION, Integer.MAX_VALUE);    sequentialAccess = conf.getBoolean(PartialVectorMerger.SEQUENTIAL_ACCESS, false);    namedVector = conf.getBoolean(PartialVectorMerger.NAMED_VECTOR, false);    logNormalize = conf.getBoolean(PartialVectorMerger.LOG_NORMALIZE, false);}
0
public void createVectors(Path input, Path output, VectorizerConfig config) throws IOException, ClassNotFoundException, InterruptedException
{    createTermFrequencyVectors(input, output, config.getTfDirName(), config.getConf(), config.getMinSupport(), config.getMaxNGramSize(), config.getMinLLRValue(), config.getNormPower(), config.isLogNormalize(), config.getNumReducers(), config.getChunkSizeInMegabytes(), config.isSequentialAccess(), config.isNamedVectors());}
0
public static void createTermFrequencyVectors(Path input, Path output, String tfVectorsFolderName, Configuration baseConf, int minSupport, int maxNGramSize, float minLLRValue, float normPower, boolean logNormalize, int numReducers, int chunkSizeInMegabytes, boolean sequentialAccess, boolean namedVectors) throws IOException, InterruptedException, ClassNotFoundException
{    Preconditions.checkArgument(normPower == PartialVectorMerger.NO_NORMALIZING || normPower >= 0, "If specified normPower must be nonnegative", normPower);    Preconditions.checkArgument(normPower == PartialVectorMerger.NO_NORMALIZING || (normPower > 1 && !Double.isInfinite(normPower)) || !logNormalize, "normPower must be > 1 and not infinite if log normalization is chosen", normPower);    if (chunkSizeInMegabytes < MIN_CHUNKSIZE) {        chunkSizeInMegabytes = MIN_CHUNKSIZE;    } else if (chunkSizeInMegabytes > MAX_CHUNKSIZE) {                chunkSizeInMegabytes = MAX_CHUNKSIZE;    }    if (minSupport < 0) {        minSupport = DEFAULT_MIN_SUPPORT;    }    Path dictionaryJobPath = new Path(output, DICTIONARY_JOB_FOLDER);        int[] maxTermDimension = new int[1];    List<Path> dictionaryChunks;    if (maxNGramSize == 1) {        startWordCounting(input, dictionaryJobPath, baseConf, minSupport);        dictionaryChunks = createDictionaryChunks(dictionaryJobPath, output, baseConf, chunkSizeInMegabytes, maxTermDimension);    } else {        CollocDriver.generateAllGrams(input, dictionaryJobPath, baseConf, maxNGramSize, minSupport, minLLRValue, numReducers);        dictionaryChunks = createDictionaryChunks(new Path(new Path(output, DICTIONARY_JOB_FOLDER), CollocDriver.NGRAM_OUTPUT_DIRECTORY), output, baseConf, chunkSizeInMegabytes, maxTermDimension);    }    int partialVectorIndex = 0;    Collection<Path> partialVectorPaths = Lists.newArrayList();    for (Path dictionaryChunk : dictionaryChunks) {        Path partialVectorOutputPath = new Path(output, VECTOR_OUTPUT_FOLDER + partialVectorIndex++);        partialVectorPaths.add(partialVectorOutputPath);        makePartialVectors(input, baseConf, maxNGramSize, dictionaryChunk, partialVectorOutputPath, maxTermDimension[0], sequentialAccess, namedVectors, numReducers);    }    Configuration conf = new Configuration(baseConf);    Path outputDir = new Path(output, tfVectorsFolderName);    PartialVectorMerger.mergePartialVectors(partialVectorPaths, outputDir, conf, normPower, logNormalize, maxTermDimension[0], sequentialAccess, namedVectors, numReducers);    HadoopUtil.delete(conf, partialVectorPaths);}
1
private static List<Path> createDictionaryChunks(Path wordCountPath, Path dictionaryPathBase, Configuration baseConf, int chunkSizeInMegabytes, int[] maxTermDimension) throws IOException
{    List<Path> chunkPaths = Lists.newArrayList();    Configuration conf = new Configuration(baseConf);    FileSystem fs = FileSystem.get(wordCountPath.toUri(), conf);    long chunkSizeLimit = chunkSizeInMegabytes * 1024L * 1024L;    int chunkIndex = 0;    Path chunkPath = new Path(dictionaryPathBase, DICTIONARY_FILE + chunkIndex);    chunkPaths.add(chunkPath);    SequenceFile.Writer dictWriter = new SequenceFile.Writer(fs, conf, chunkPath, Text.class, IntWritable.class);    try {        long currentChunkSize = 0;        Path filesPattern = new Path(wordCountPath, OUTPUT_FILES_PATTERN);        int i = 0;        for (Pair<Writable, Writable> record : new SequenceFileDirIterable<>(filesPattern, PathType.GLOB, null, null, true, conf)) {            if (currentChunkSize > chunkSizeLimit) {                Closeables.close(dictWriter, false);                chunkIndex++;                chunkPath = new Path(dictionaryPathBase, DICTIONARY_FILE + chunkIndex);                chunkPaths.add(chunkPath);                dictWriter = new SequenceFile.Writer(fs, conf, chunkPath, Text.class, IntWritable.class);                currentChunkSize = 0;            }            Writable key = record.getFirst();            int fieldSize = DICTIONARY_BYTE_OVERHEAD + key.toString().length() * 2 + Integer.SIZE / 8;            currentChunkSize += fieldSize;            dictWriter.append(key, new IntWritable(i++));        }        maxTermDimension[0] = i;    } finally {        Closeables.close(dictWriter, false);    }    return chunkPaths;}
0
private static void makePartialVectors(Path input, Configuration baseConf, int maxNGramSize, Path dictionaryFilePath, Path output, int dimension, boolean sequentialAccess, boolean namedVectors, int numReducers) throws IOException, InterruptedException, ClassNotFoundException
{    Configuration conf = new Configuration(baseConf);        conf.set("io.serializations", "org.apache.hadoop.io.serializer.JavaSerialization," + "org.apache.hadoop.io.serializer.WritableSerialization");    conf.setInt(PartialVectorMerger.DIMENSION, dimension);    conf.setBoolean(PartialVectorMerger.SEQUENTIAL_ACCESS, sequentialAccess);    conf.setBoolean(PartialVectorMerger.NAMED_VECTOR, namedVectors);    conf.setInt(MAX_NGRAMS, maxNGramSize);    DistributedCache.addCacheFile(dictionaryFilePath.toUri(), conf);    Job job = new Job(conf);    job.setJobName("DictionaryVectorizer::MakePartialVectors: input-folder: " + input + ", dictionary-file: " + dictionaryFilePath);    job.setJarByClass(DictionaryVectorizer.class);    job.setMapOutputKeyClass(Text.class);    job.setMapOutputValueClass(StringTuple.class);    job.setOutputKeyClass(Text.class);    job.setOutputValueClass(VectorWritable.class);    FileInputFormat.setInputPaths(job, input);    FileOutputFormat.setOutputPath(job, output);    job.setMapperClass(Mapper.class);    job.setInputFormatClass(SequenceFileInputFormat.class);    job.setReducerClass(TFPartialVectorReducer.class);    job.setOutputFormatClass(SequenceFileOutputFormat.class);    job.setNumReduceTasks(numReducers);    HadoopUtil.delete(conf, output);    boolean succeeded = job.waitForCompletion(true);    if (!succeeded) {        throw new IllegalStateException("Job failed!");    }}
0
private static void startWordCounting(Path input, Path output, Configuration baseConf, int minSupport) throws IOException, InterruptedException, ClassNotFoundException
{    Configuration conf = new Configuration(baseConf);        conf.set("io.serializations", "org.apache.hadoop.io.serializer.JavaSerialization," + "org.apache.hadoop.io.serializer.WritableSerialization");    conf.setInt(MIN_SUPPORT, minSupport);    Job job = new Job(conf);    job.setJobName("DictionaryVectorizer::WordCount: input-folder: " + input);    job.setJarByClass(DictionaryVectorizer.class);    job.setOutputKeyClass(Text.class);    job.setOutputValueClass(LongWritable.class);    FileInputFormat.setInputPaths(job, input);    FileOutputFormat.setOutputPath(job, output);    job.setMapperClass(TermCountMapper.class);    job.setInputFormatClass(SequenceFileInputFormat.class);    job.setCombinerClass(TermCountCombiner.class);    job.setReducerClass(TermCountReducer.class);    job.setOutputFormatClass(SequenceFileOutputFormat.class);    HadoopUtil.delete(conf, output);    boolean succeeded = job.waitForCompletion(true);    if (!succeeded) {        throw new IllegalStateException("Job failed!");    }}
0
public int run(String[] args) throws Exception
{    addInputOption();    addOutputOption();    addOption("tfDirName", "tf", "The folder to store the TF calculations", "tfDirName");    addOption("minSupport", "s", "(Optional) Minimum Support. Default Value: 2", "2");    addOption("maxNGramSize", "ng", "(Optional) The maximum size of ngrams to create" + " (2 = bigrams, 3 = trigrams, etc) Default Value:1");    addOption("minLLR", "ml", "(Optional)The minimum Log Likelihood Ratio(Float)  Default is " + LLRReducer.DEFAULT_MIN_LLR);    addOption("norm", "n", "The norm to use, expressed as either a float or \"INF\" " + "if you want to use the Infinite norm.  " + "Must be greater or equal to 0.  The default is not to normalize");    addOption("logNormalize", "lnorm", "(Optional) Whether output vectors should be logNormalize. " + "If set true else false", "false");    addOption(DefaultOptionCreator.numReducersOption().create());    addOption("chunkSize", "chunk", "The chunkSize in MegaBytes. 100-10000 MB", "100");    addOption(DefaultOptionCreator.methodOption().create());    addOption("namedVector", "nv", "(Optional) Whether output vectors should be NamedVectors. " + "If set true else false", "false");    if (parseArguments(args) == null) {        return -1;    }    String tfDirName = getOption("tfDirName", "tfDir");    int minSupport = getInt("minSupport", 2);    int maxNGramSize = getInt("maxNGramSize", 1);    float minLLRValue = getFloat("minLLR", LLRReducer.DEFAULT_MIN_LLR);    float normPower = getFloat("norm", PartialVectorMerger.NO_NORMALIZING);    boolean logNormalize = hasOption("logNormalize");    int numReducers = getInt(DefaultOptionCreator.MAX_REDUCERS_OPTION);    int chunkSizeInMegs = getInt("chunkSize", 100);    boolean sequential = hasOption("sequential");    boolean namedVecs = hasOption("namedVectors");        createTermFrequencyVectors(getInputPath(), getOutputPath(), tfDirName, getConf(), minSupport, maxNGramSize, minLLRValue, normPower, logNormalize, numReducers, chunkSizeInMegs, sequential, namedVecs);    return 0;}
0
public static void main(String[] args) throws Exception
{    ToolRunner.run(new DictionaryVectorizer(), args);}
0
protected void map(Text key, Text value, Context context) throws IOException, InterruptedException
{    TokenStream stream = analyzer.tokenStream(key.toString(), new StringReader(value.toString()));    CharTermAttribute termAtt = stream.addAttribute(CharTermAttribute.class);    stream.reset();    StringTuple document = new StringTuple();    while (stream.incrementToken()) {        if (termAtt.length() > 0) {            document.add(new String(termAtt.buffer(), 0, termAtt.length()));        }    }    stream.end();    Closeables.close(stream, true);    context.write(key, document);}
0
protected void setup(Context context) throws IOException, InterruptedException
{    super.setup(context);    String analyzerClassName = context.getConfiguration().get(DocumentProcessor.ANALYZER_CLASS, StandardAnalyzer.class.getName());    try {        analyzer = AnalyzerUtils.createAnalyzer(analyzerClassName);    } catch (ClassNotFoundException e) {        throw new IOException("Unable to create analyzer: " + analyzerClassName, e);    }}
0
public static void tokenizeDocuments(Path input, Class<? extends Analyzer> analyzerClass, Path output, Configuration baseConf) throws IOException, InterruptedException, ClassNotFoundException
{    Configuration conf = new Configuration(baseConf);        conf.set("io.serializations", "org.apache.hadoop.io.serializer.JavaSerialization," + "org.apache.hadoop.io.serializer.WritableSerialization");    conf.set(ANALYZER_CLASS, analyzerClass.getName());    Job job = new Job(conf);    job.setJobName("DocumentProcessor::DocumentTokenizer: input-folder: " + input);    job.setJarByClass(DocumentProcessor.class);    job.setOutputKeyClass(Text.class);    job.setOutputValueClass(StringTuple.class);    FileInputFormat.setInputPaths(job, input);    FileOutputFormat.setOutputPath(job, output);    job.setMapperClass(SequenceFileTokenizerMapper.class);    job.setInputFormatClass(SequenceFileInputFormat.class);    job.setNumReduceTasks(0);    job.setOutputFormatClass(SequenceFileOutputFormat.class);    HadoopUtil.delete(conf, output);    boolean succeeded = job.waitForCompletion(true);    if (!succeeded) {        throw new IllegalStateException("Job failed!");    }}
0
public static void main(String[] args) throws Exception
{    ToolRunner.run(new Configuration(), new EncodedVectorsFromSequenceFiles(), args);}
0
public int run(String[] args) throws Exception
{    addInputOption();    addOutputOption();    addOption(DefaultOptionCreator.analyzerOption().create());    addOption(buildOption("sequentialAccessVector", "seq", "(Optional) Whether output vectors should be SequentialAccessVectors. " + "If set true else false", false, false, null));    addOption(buildOption("namedVector", "nv", "Create named vectors using the key.  False by default", false, false, null));    addOption("cardinality", "c", "The cardinality to use for creating the vectors.  Default is 5000", "5000");    addOption("encoderFieldName", "en", "The name of the encoder to be passed to the FeatureVectorEncoder constructor. Default is text. " + "Note this is not the class name of a FeatureValueEncoder, but is instead the construction " + "argument.", "text");    addOption("encoderClass", "ec", "The class name of the encoder to be used. Default is " + LuceneTextValueEncoder.class.getName(), LuceneTextValueEncoder.class.getName());    addOption(DefaultOptionCreator.overwriteOption().create());    if (parseArguments(args) == null) {        return -1;    }    Path input = getInputPath();    Path output = getOutputPath();    if (hasOption(DefaultOptionCreator.OVERWRITE_OPTION)) {        HadoopUtil.delete(getConf(), output);    }    Class<? extends Analyzer> analyzerClass = getAnalyzerClassFromOption();    Configuration conf = getConf();    boolean sequentialAccessOutput = hasOption("sequentialAccessVector");    boolean namedVectors = hasOption("namedVector");    int cardinality = 5000;    if (hasOption("cardinality")) {        cardinality = Integer.parseInt(getOption("cardinality"));    }    String encoderName = "text";    if (hasOption("encoderFieldName")) {        encoderName = getOption("encoderFieldName");    }    String encoderClass = LuceneTextValueEncoder.class.getName();    if (hasOption("encoderClass")) {        encoderClass = getOption("encoderClass");        ClassUtils.instantiateAs(encoderClass, FeatureVectorEncoder.class, new Class[] { String.class },         new Object[] { encoderName });    }    SimpleTextEncodingVectorizer vectorizer = new SimpleTextEncodingVectorizer();    VectorizerConfig config = new VectorizerConfig(conf, analyzerClass.getName(), encoderClass, encoderName, sequentialAccessOutput, namedVectors, cardinality);    vectorizer.createVectors(input, output, config);    return 0;}
0
public void addToVector(String originalForm, double weight, Vector data)
{    dictionary.add(originalForm);    super.addToVector(originalForm, weight, data);}
0
protected double getWeight(byte[] originalForm, double w)
{    return w * weight(originalForm);}
0
protected double weight(byte[] originalForm)
{                double thisWord = dictionary.count(new String(originalForm, Charsets.UTF_8)) + 0.5;    double allWords = dictionary.size() + dictionary.elementSet().size() * 0.5 + 0.5;    return -Math.log(thisWord / allWords);}
0
public Multiset<String> getDictionary()
{    return dictionary;}
0
private void initCaches()
{    this.caches = new OpenIntIntHashMap[getProbes()];    for (int probe = 0; probe < getProbes(); probe++) {        caches[probe] = new OpenIntIntHashMap();    }}
0
 OpenIntIntHashMap[] getCaches()
{    return caches;}
0
public void setProbes(int probes)
{    super.setProbes(probes);    initCaches();}
0
protected int hashForProbe(byte[] originalForm, int dataSize, String name, int probe)
{    Preconditions.checkArgument(dataSize == this.dataSize, "dataSize argument [" + dataSize + "] does not match expected dataSize [" + this.dataSize + ']');    int originalHashcode = Arrays.hashCode(originalForm);    if (caches[probe].containsKey(originalHashcode)) {        return caches[probe].get(originalHashcode);    }    int hash = super.hashForProbe(originalForm, dataSize, name, probe);    caches[probe].put(originalHashcode, hash);    return hash;}
0
private void initCaches()
{    caches = new OpenIntIntHashMap[getProbes()];    for (int probe = 0; probe < getProbes(); probe++) {        caches[probe] = new OpenIntIntHashMap();    }}
0
 OpenIntIntHashMap[] getCaches()
{    return caches;}
0
public void setProbes(int probes)
{    super.setProbes(probes);    initCaches();}
0
protected int hashForProbe(byte[] originalForm, int dataSize, String name, int probe)
{    Preconditions.checkArgument(dataSize == this.dataSize, "dataSize argument [" + dataSize + "] does not match expected dataSize [" + this.dataSize + ']');    int originalHashcode = Arrays.hashCode(originalForm);    if (caches[probe].containsKey(originalHashcode)) {        return caches[probe].get(originalHashcode);    }    int hash = super.hashForProbe(originalForm, dataSize, name, probe);    caches[probe].put(originalHashcode, hash);    return hash;}
0
public void setProbes(int probes)
{    super.setProbes(probes);    cacheProbeLocations(getSeed());}
0
private void cacheProbeLocations(int seed)
{    cachedProbes = new int[getProbes()];    for (int i = 0; i < getProbes(); i++) {                cachedProbes[i] = (int) MurmurHash.hash64A(bytesForString(getName()), seed + i);    }}
0
protected int hashForProbe(byte[] originalForm, int dataSize, String name, int probe)
{    int h = cachedProbes[probe] % dataSize;    if (h < 0) {        h += dataSize;    }    return h;}
0
public void addToVector(byte[] originalForm, double weight, Vector data)
{    int probes = getProbes();    String name = getName();    for (int i = 0; i < probes; i++) {        int n = hashForProbe(originalForm, data.size(), name, i);        if (isTraceEnabled()) {            trace((String) null, n);        }        data.set(n, data.get(n) + getWeight(originalForm, weight));    }}
0
protected double getWeight(byte[] originalForm, double w)
{    return w;}
0
public String asString(String originalForm)
{    return getName();}
0
protected int getSeed()
{    return 0;}
0
public void addToVector(byte[] originalForm, double weight, Vector data)
{    int probes = getProbes();    String name = getName();    for (int i = 0; i < probes; i++) {        int n = hashForProbe(originalForm, data.size(), name, i);        if (isTraceEnabled()) {            trace((String) null, n);        }        data.set(n, data.get(n) + getWeight(originalForm, weight));    }}
0
protected double getWeight(byte[] originalForm, double w)
{    if (originalForm == null) {        return w;    }    return w * Double.parseDouble(new String(originalForm, Charsets.UTF_8));}
0
public String asString(String originalForm)
{    return getName() + ':' + originalForm;}
0
protected int getSeed()
{    return CONTINUOUS_VALUE_HASH_SEED;}
0
public int intern(String s)
{    if (!dict.containsKey(s)) {        dict.put(s, dict.size());    }    return dict.get(s);}
0
public List<String> values()
{        return new ArrayList<>(dict.keySet());}
0
public int size()
{    return dict.size();}
0
public static Dictionary fromList(Iterable<String> values)
{    Dictionary dict = new Dictionary();    for (String value : values) {        dict.intern(value);    }    return dict;}
0
public void addToVector(String originalForm, Vector data)
{    addToVector(originalForm, 1.0, data);}
0
public void addToVector(byte[] originalForm, Vector data)
{    addToVector(originalForm, 1.0, data);}
0
public void addToVector(String originalForm, double weight, Vector data)
{    addToVector(bytesForString(originalForm), weight, data);}
0
protected Iterable<Integer> hashesForProbe(byte[] originalForm, int dataSize, String name, int probe)
{    return Collections.singletonList(hashForProbe(originalForm, dataSize, name, probe));}
0
protected double getWeight(byte[] originalForm, double w)
{    return 1.0;}
0
protected int hash(String term, int probe, int numFeatures)
{    long r = MurmurHash.hash64A(bytesForString(term), probe) % numFeatures;    if (r < 0) {        r += numFeatures;    }    return (int) r;}
0
protected static int hash(byte[] term, int probe, int numFeatures)
{    long r = MurmurHash.hash64A(term, probe) % numFeatures;    if (r < 0) {        r += numFeatures;    }    return (int) r;}
0
protected static int hash(String term1, String term2, int probe, int numFeatures)
{    long r = MurmurHash.hash64A(bytesForString(term1), probe);    r = MurmurHash.hash64A(bytesForString(term2), (int) r) % numFeatures;    if (r < 0) {        r += numFeatures;    }    return (int) r;}
0
protected int hash(byte[] term1, byte[] term2, int probe, int numFeatures)
{    long r = MurmurHash.hash64A(term1, probe);    r = MurmurHash.hash64A(term2, (int) r) % numFeatures;    if (r < 0) {        r += numFeatures;    }    return (int) r;}
0
protected int hash(String term1, String term2, String term3, String term4, int probe, int numFeatures)
{    long r = MurmurHash.hash64A(bytesForString(term1), probe);    r = MurmurHash.hash64A(bytesForString(term2), (int) r) % numFeatures;    r = MurmurHash.hash64A(bytesForString(term3), (int) r) % numFeatures;    r = MurmurHash.hash64A(bytesForString(term4), (int) r) % numFeatures;    if (r < 0) {        r += numFeatures;    }    return (int) r;}
0
public int getProbes()
{    return probes;}
0
public void setProbes(int probes)
{    this.probes = probes;}
0
public String getName()
{    return name;}
0
protected boolean isTraceEnabled()
{    return traceDictionary != null;}
0
protected void trace(String subName, int n)
{    if (traceDictionary != null) {        String key = name;        if (subName != null) {            key = name + '=' + subName;        }        Set<Integer> trace = traceDictionary.get(key);        if (trace == null) {            trace = Sets.newHashSet(n);            traceDictionary.put(key, trace);        } else {            trace.add(n);        }    }}
0
protected void trace(byte[] subName, int n)
{    trace(new String(subName, Charsets.UTF_8), n);}
0
public void setTraceDictionary(Map<String, Set<Integer>> traceDictionary)
{    this.traceDictionary = traceDictionary;}
0
protected static byte[] bytesForString(String x)
{    return x == null ? EMPTY_ARRAY : x.getBytes(Charsets.UTF_8);}
0
public void addToVector(String originalForm, double w, Vector data)
{    throw new UnsupportedOperationException("addToVector is not supported for InteractionVectorEncoder");}
0
public void addToVector(byte[] originalForm, double w, Vector data)
{    throw new UnsupportedOperationException("addToVector is not supported for InteractionVectorEncoder");}
0
public void addInteractionToVector(String original1, String original2, double weight, Vector data)
{    byte[] originalForm1 = bytesForString(original1);    byte[] originalForm2 = bytesForString(original2);    addInteractionToVector(originalForm1, originalForm2, weight, data);}
0
public void addInteractionToVector(byte[] originalForm1, byte[] originalForm2, double weight, Vector data)
{    String name = getName();    double w = getWeight(originalForm1, originalForm2, weight);    for (int i = 0; i < probes(); i++) {        Iterable<Integer> jValues = secondEncoder.hashesForProbe(originalForm2, data.size(), name, i % secondEncoder.getProbes());        for (Integer k : firstEncoder.hashesForProbe(originalForm1, data.size(), name, i % firstEncoder.getProbes())) {            for (Integer j : jValues) {                int n = (k + j) % data.size();                if (isTraceEnabled()) {                    trace(String.format("%s:%s", new String(originalForm1, Charsets.UTF_8), new String(originalForm2, Charsets.UTF_8)), n);                }                data.set(n, data.get(n) + w);            }        }    }}
0
private int probes()
{    return getProbes();}
0
protected double getWeight(byte[] originalForm1, byte[] originalForm2, double w)
{    return firstEncoder.getWeight(originalForm1, 1.0) * secondEncoder.getWeight(originalForm2, 1.0) * w;}
0
public String asString(String originalForm)
{    return String.format(Locale.ENGLISH, "%s:%s", getName(), originalForm);}
0
protected int hashForProbe(byte[] originalForm, int dataSize, String name, int probe)
{    return hash(name, probe, dataSize);}
0
public void setAnalyzer(Analyzer analyzer)
{    this.analyzer = analyzer;}
0
protected Iterable<String> tokenize(CharSequence originalForm)
{    TokenStream ts = analyzer.tokenStream(getName(), new CharSequenceReader(originalForm));    ts.addAttribute(CharTermAttribute.class);    return new LuceneTokenIterable(ts, false);}
0
public int read(char[] cbuf, int off, int len)
{    int toRead = Math.min(len, buf.remaining());    if (toRead > 0) {        buf.get(cbuf, off, toRead);        return toRead;    } else {        return -1;    }}
0
public void close()
{}
0
public Iterator<String> iterator()
{    if (firstTime) {        firstTime = false;    } else {        try {            tokenStream.reset();        } catch (IOException e) {            throw new IllegalStateException("This token stream can't be reset");        }    }    return new TokenStreamIterator(tokenStream);}
0
protected int hashForProbe(byte[] originalForm, int dataSize, String name, int probe)
{    return hash(nameBytes, originalForm, WORD_LIKE_VALUE_HASH_SEED + probe, dataSize);}
0
public void setDictionary(Map<String, Double> dictionary)
{    this.dictionary = dictionary;    setMissingValueWeight(Collections.min(dictionary.values()) / 2);}
0
public void setMissingValueWeight(double missingValueWeight)
{    this.missingValueWeight = missingValueWeight;}
0
protected double weight(byte[] originalForm)
{    double weight = missingValueWeight;    if (dictionary != null) {        String s = new String(originalForm, Charsets.UTF_8);        if (dictionary.containsKey(s)) {            weight = dictionary.get(s);        }    }    return weight;}
0
public void addToVector(byte[] originalForm, double weight, Vector data)
{    addText(originalForm);    flush(weight, data);}
0
public void addText(byte[] originalForm)
{    addText(new String(originalForm, Charsets.UTF_8));}
0
public void addText(CharSequence text)
{    for (String word : tokenize(text)) {        counts.add(word);    }}
0
public void flush(double weight, Vector data)
{    for (String word : counts.elementSet()) {                wordEncoder.addToVector(word, weight * Math.log1p(counts.count(word)) / LOG_2, data);    }    counts.clear();}
0
protected int hashForProbe(byte[] originalForm, int dataSize, String name, int probe)
{    return 0;}
0
protected Iterable<Integer> hashesForProbe(byte[] originalForm, int dataSize, String name, int probe)
{    Collection<Integer> hashes = Lists.newArrayList();    for (String word : tokenize(new String(originalForm, Charsets.UTF_8))) {        hashes.add(hashForProbe(bytesForString(word), dataSize, name, probe));    }    return hashes;}
0
protected Iterable<String> tokenize(CharSequence originalForm)
{    return ON_NON_WORD.split(originalForm);}
0
public String asString(String originalForm)
{    StringBuilder r = new StringBuilder();    r.append('[');    for (String word : tokenize(originalForm)) {        if (r.length() > 1) {            r.append(", ");        }        r.append(wordEncoder.asString(word));    }    r.append(']');    return r.toString();}
0
public final void setWordEncoder(FeatureVectorEncoder wordEncoder)
{    this.wordEncoder = wordEncoder;}
0
public void addToVector(byte[] originalForm, double w, Vector data)
{    int probes = getProbes();    String name = getName();    double weight = getWeight(originalForm, w);    for (int i = 0; i < probes; i++) {        int n = hashForProbe(originalForm, data.size(), name, i);        if (isTraceEnabled()) {            trace(originalForm, n);        }        data.set(n, data.get(n) + weight);    }}
0
protected double getWeight(byte[] originalForm, double w)
{    return w * weight(originalForm);}
0
protected int hashForProbe(byte[] originalForm, int dataSize, String name, int probe)
{    return hash(nameBytes, originalForm, WORD_LIKE_VALUE_HASH_SEED + probe, dataSize);}
0
public String asString(String originalForm)
{    return String.format(Locale.ENGLISH, "%s:%s:%.4f", getName(), originalForm, weight(bytesForString(originalForm)));}
0
protected void setup(Context context) throws IOException, InterruptedException
{    Configuration conf = context.getConfiguration();    sequentialVectors = conf.getBoolean(USE_SEQUENTIAL, false);    namedVectors = conf.getBoolean(USE_NAMED_VECTORS, false);    String analyzerName = conf.get(ANALYZER_NAME, StandardAnalyzer.class.getName());    Analyzer analyzer;    try {        analyzer = AnalyzerUtils.createAnalyzer(analyzerName);    } catch (ClassNotFoundException e) {                throw new IOException("Unable to create Analyzer for name: " + analyzerName, e);    }    String encoderName = conf.get(ENCODER_FIELD_NAME, "text");    cardinality = conf.getInt(CARDINALITY, 5000);    String encClass = conf.get(ENCODER_CLASS);    encoder = ClassUtils.instantiateAs(encClass, FeatureVectorEncoder.class, new Class[] { String.class }, new Object[] { encoderName });    if (encoder instanceof LuceneTextValueEncoder) {        ((LuceneTextValueEncoder) encoder).setAnalyzer(analyzer);    }}
0
protected void map(Text key, Text value, Context context) throws IOException, InterruptedException
{    Vector vector;    if (sequentialVectors) {        vector = new SequentialAccessSparseVector(cardinality);    } else {        vector = new RandomAccessSparseVector(cardinality);    }    if (namedVectors) {        vector = new NamedVector(vector, key.toString());    }    encoder.addToVector(value.toString(), vector);    context.write(new Text(key.toString()), new VectorWritable(vector));}
0
public static void pruneVectors(Path tfDir, Path prunedTFDir, Path prunedPartialTFDir, long maxDF, long minDF, Configuration baseConf, Pair<Long[], List<Path>> docFrequenciesFeatures, float normPower, boolean logNormalize, int numReducers) throws IOException, InterruptedException, ClassNotFoundException
{    int partialVectorIndex = 0;    List<Path> partialVectorPaths = new ArrayList<>();    for (Path path : docFrequenciesFeatures.getSecond()) {        Path partialVectorOutputPath = new Path(prunedPartialTFDir, "partial-" + partialVectorIndex++);        partialVectorPaths.add(partialVectorOutputPath);        pruneVectorsPartial(tfDir, partialVectorOutputPath, path, maxDF, minDF, baseConf);    }    mergePartialVectors(partialVectorPaths, prunedTFDir, baseConf, normPower, logNormalize, numReducers);    HadoopUtil.delete(new Configuration(baseConf), prunedPartialTFDir);}
0
private static void pruneVectorsPartial(Path input, Path output, Path dictionaryFilePath, long maxDF, long minDF, Configuration baseConf) throws IOException, InterruptedException, ClassNotFoundException
{    Configuration conf = new Configuration(baseConf);            conf.set("io.serializations", "org.apache.hadoop.io.serializer.JavaSerialization," + "org.apache.hadoop.io.serializer.WritableSerialization");    conf.setLong(MAX_DF, maxDF);    conf.setLong(MIN_DF, minDF);    DistributedCache.addCacheFile(dictionaryFilePath.toUri(), conf);    Job job = HadoopUtil.prepareJob(input, output, SequenceFileInputFormat.class, Mapper.class, null, null, WordsPrunerReducer.class, Text.class, VectorWritable.class, SequenceFileOutputFormat.class, conf);    job.setJobName(": Prune Vectors: input-folder: " + input + ", dictionary-file: " + dictionaryFilePath.toString());    HadoopUtil.delete(conf, output);    boolean succeeded = job.waitForCompletion(true);    if (!succeeded) {        throw new IllegalStateException("Job failed!");    }}
0
public static void mergePartialVectors(Iterable<Path> partialVectorPaths, Path output, Configuration baseConf, float normPower, boolean logNormalize, int numReducers) throws IOException, InterruptedException, ClassNotFoundException
{    Configuration conf = new Configuration(baseConf);        conf.set("io.serializations", "org.apache.hadoop.io.serializer.JavaSerialization," + "org.apache.hadoop.io.serializer.WritableSerialization");    conf.setFloat(PartialVectorMerger.NORMALIZATION_POWER, normPower);    conf.setBoolean(PartialVectorMerger.LOG_NORMALIZE, logNormalize);    Job job = new Job(conf);    job.setJobName("PrunerPartialVectorMerger::MergePartialVectors");    job.setJarByClass(PartialVectorMerger.class);    job.setOutputKeyClass(Text.class);    job.setOutputValueClass(VectorWritable.class);    FileInputFormat.setInputPaths(job, getCommaSeparatedPaths(partialVectorPaths));    FileOutputFormat.setOutputPath(job, output);    job.setMapperClass(Mapper.class);    job.setInputFormatClass(SequenceFileInputFormat.class);    job.setReducerClass(PrunedPartialVectorMergeReducer.class);    job.setOutputFormatClass(SequenceFileOutputFormat.class);    job.setNumReduceTasks(numReducers);    HadoopUtil.delete(conf, output);    boolean succeeded = job.waitForCompletion(true);    if (!succeeded) {        throw new IllegalStateException("Job failed!");    }}
0
private static String getCommaSeparatedPaths(Iterable<Path> paths)
{    StringBuilder commaSeparatedPaths = new StringBuilder(100);    String sep = "";    for (Path path : paths) {        commaSeparatedPaths.append(sep).append(path.toString());        sep = ",";    }    return commaSeparatedPaths.toString();}
0
protected void reduce(WritableComparable<?> key, Iterable<VectorWritable> values, Context context) throws IOException, InterruptedException
{    Vector vector = null;    for (VectorWritable value : values) {        if (vector == null) {            vector = value.get().clone();            continue;        }                vector.assign(value.get(), Functions.PLUS);    }    if (vector != null && normPower != PartialVectorMerger.NO_NORMALIZING) {        vector = logNormalize ? vector.logNormalize(normPower) : vector.normalize(normPower);    }    VectorWritable vectorWritable = new VectorWritable(vector);    context.write(key, vectorWritable);}
0
protected void setup(Context context) throws IOException, InterruptedException
{    super.setup(context);    Configuration conf = context.getConfiguration();    normPower = conf.getFloat(PartialVectorMerger.NORMALIZATION_POWER, PartialVectorMerger.NO_NORMALIZING);    logNormalize = conf.getBoolean(PartialVectorMerger.LOG_NORMALIZE, false);}
0
protected void reduce(WritableComparable<?> key, Iterable<VectorWritable> values, Context context) throws IOException, InterruptedException
{    Iterator<VectorWritable> it = values.iterator();    if (!it.hasNext()) {        return;    }    Vector value = it.next().get();    Vector vector = value.clone();    if (maxDf != Long.MAX_VALUE || minDf > -1) {        for (Vector.Element e : value.nonZeroes()) {            if (!dictionary.containsKey(e.index())) {                vector.setQuick(e.index(), 0.0);                continue;            }            long df = dictionary.get(e.index());            if (df > maxDf || df < minDf) {                vector.setQuick(e.index(), 0.0);            }        }    }    VectorWritable vectorWritable = new VectorWritable(vector);    context.write(key, vectorWritable);}
0
protected void setup(Context context) throws IOException, InterruptedException
{    super.setup(context);    Configuration conf = context.getConfiguration();        maxDf = conf.getLong(HighDFWordsPruner.MAX_DF, Long.MAX_VALUE);    minDf = conf.getLong(HighDFWordsPruner.MIN_DF, -1);    Path dictionaryFile = HadoopUtil.getSingleCachedFile(conf);        for (Pair<IntWritable, LongWritable> record : new SequenceFileIterable<IntWritable, LongWritable>(dictionaryFile, true, conf)) {        dictionary.put(record.getFirst().get(), record.getSecond().get());    }}
0
public void createVectors(Path input, Path output, VectorizerConfig config) throws IOException, ClassNotFoundException, InterruptedException
{        Job job = HadoopUtil.prepareJob(input, output, SequenceFileInputFormat.class, EncodingMapper.class, Text.class, VectorWritable.class, SequenceFileOutputFormat.class, config.getConf());    Configuration conf = job.getConfiguration();    conf.set(EncodingMapper.USE_SEQUENTIAL, String.valueOf(config.isSequentialAccess()));    conf.set(EncodingMapper.USE_NAMED_VECTORS, String.valueOf(config.isNamedVectors()));    conf.set(EncodingMapper.ANALYZER_NAME, config.getAnalyzerClassName());    conf.set(EncodingMapper.ENCODER_FIELD_NAME, config.getEncoderName());    conf.set(EncodingMapper.ENCODER_CLASS, config.getEncoderClass());    conf.set(EncodingMapper.CARDINALITY, String.valueOf(config.getCardinality()));    job.setNumReduceTasks(0);    boolean finished = job.waitForCompletion(true);        if (!finished) {        throw new IllegalStateException("Job failed!");    }}
1
public static void main(String[] args) throws Exception
{    ToolRunner.run(new SparseVectorsFromSequenceFiles(), args);}
0
public int run(String[] args) throws Exception
{    DefaultOptionBuilder obuilder = new DefaultOptionBuilder();    ArgumentBuilder abuilder = new ArgumentBuilder();    GroupBuilder gbuilder = new GroupBuilder();    Option inputDirOpt = DefaultOptionCreator.inputOption().create();    Option outputDirOpt = DefaultOptionCreator.outputOption().create();    Option minSupportOpt = obuilder.withLongName("minSupport").withArgument(abuilder.withName("minSupport").withMinimum(1).withMaximum(1).create()).withDescription("(Optional) Minimum Support. Default Value: 2").withShortName("s").create();    Option analyzerNameOpt = obuilder.withLongName("analyzerName").withArgument(abuilder.withName("analyzerName").withMinimum(1).withMaximum(1).create()).withDescription("The class name of the analyzer").withShortName("a").create();    Option chunkSizeOpt = obuilder.withLongName("chunkSize").withArgument(abuilder.withName("chunkSize").withMinimum(1).withMaximum(1).create()).withDescription("The chunkSize in MegaBytes. Default Value: 100MB").withShortName("chunk").create();    Option weightOpt = obuilder.withLongName("weight").withRequired(false).withArgument(abuilder.withName("weight").withMinimum(1).withMaximum(1).create()).withDescription("The kind of weight to use. Currently TF or TFIDF. Default: TFIDF").withShortName("wt").create();    Option minDFOpt = obuilder.withLongName("minDF").withRequired(false).withArgument(abuilder.withName("minDF").withMinimum(1).withMaximum(1).create()).withDescription("The minimum document frequency.  Default is 1").withShortName("md").create();    Option maxDFPercentOpt = obuilder.withLongName("maxDFPercent").withRequired(false).withArgument(abuilder.withName("maxDFPercent").withMinimum(1).withMaximum(1).create()).withDescription("The max percentage of docs for the DF.  Can be used to remove really high frequency terms." + " Expressed as an integer between 0 and 100. Default is 99.  If maxDFSigma is also set, " + "it will override this value.").withShortName("x").create();    Option maxDFSigmaOpt = obuilder.withLongName("maxDFSigma").withRequired(false).withArgument(abuilder.withName("maxDFSigma").withMinimum(1).withMaximum(1).create()).withDescription("What portion of the tf (tf-idf) vectors to be used, expressed in times the standard deviation (sigma) " + "of the document frequencies of these vectors. Can be used to remove really high frequency terms." + " Expressed as a double value. Good value to be specified is 3.0. In case the value is less " + "than 0 no vectors will be filtered out. Default is -1.0.  Overrides maxDFPercent").withShortName("xs").create();    Option minLLROpt = obuilder.withLongName("minLLR").withRequired(false).withArgument(abuilder.withName("minLLR").withMinimum(1).withMaximum(1).create()).withDescription("(Optional)The minimum Log Likelihood Ratio(Float)  Default is " + LLRReducer.DEFAULT_MIN_LLR).withShortName("ml").create();    Option numReduceTasksOpt = obuilder.withLongName("numReducers").withArgument(abuilder.withName("numReducers").withMinimum(1).withMaximum(1).create()).withDescription("(Optional) Number of reduce tasks. Default Value: 1").withShortName("nr").create();    Option powerOpt = obuilder.withLongName("norm").withRequired(false).withArgument(abuilder.withName("norm").withMinimum(1).withMaximum(1).create()).withDescription("The norm to use, expressed as either a float or \"INF\" if you want to use the Infinite norm.  " + "Must be greater or equal to 0.  The default is not to normalize").withShortName("n").create();    Option logNormalizeOpt = obuilder.withLongName("logNormalize").withRequired(false).withDescription("(Optional) Whether output vectors should be logNormalize. If set true else false").withShortName("lnorm").create();    Option maxNGramSizeOpt = obuilder.withLongName("maxNGramSize").withRequired(false).withArgument(abuilder.withName("ngramSize").withMinimum(1).withMaximum(1).create()).withDescription("(Optional) The maximum size of ngrams to create" + " (2 = bigrams, 3 = trigrams, etc) Default Value:1").withShortName("ng").create();    Option sequentialAccessVectorOpt = obuilder.withLongName("sequentialAccessVector").withRequired(false).withDescription("(Optional) Whether output vectors should be SequentialAccessVectors. If set true else false").withShortName("seq").create();    Option namedVectorOpt = obuilder.withLongName("namedVector").withRequired(false).withDescription("(Optional) Whether output vectors should be NamedVectors. If set true else false").withShortName("nv").create();    Option overwriteOutput = obuilder.withLongName("overwrite").withRequired(false).withDescription("If set, overwrite the output directory").withShortName("ow").create();    Option helpOpt = obuilder.withLongName("help").withDescription("Print out help").withShortName("h").create();    Group group = gbuilder.withName("Options").withOption(minSupportOpt).withOption(analyzerNameOpt).withOption(chunkSizeOpt).withOption(outputDirOpt).withOption(inputDirOpt).withOption(minDFOpt).withOption(maxDFSigmaOpt).withOption(maxDFPercentOpt).withOption(weightOpt).withOption(powerOpt).withOption(minLLROpt).withOption(numReduceTasksOpt).withOption(maxNGramSizeOpt).withOption(overwriteOutput).withOption(helpOpt).withOption(sequentialAccessVectorOpt).withOption(namedVectorOpt).withOption(logNormalizeOpt).create();    try {        Parser parser = new Parser();        parser.setGroup(group);        parser.setHelpOption(helpOpt);        CommandLine cmdLine = parser.parse(args);        if (cmdLine.hasOption(helpOpt)) {            CommandLineUtil.printHelp(group);            return -1;        }        Path inputDir = new Path((String) cmdLine.getValue(inputDirOpt));        Path outputDir = new Path((String) cmdLine.getValue(outputDirOpt));        int chunkSize = 100;        if (cmdLine.hasOption(chunkSizeOpt)) {            chunkSize = Integer.parseInt((String) cmdLine.getValue(chunkSizeOpt));        }        int minSupport = 2;        if (cmdLine.hasOption(minSupportOpt)) {            String minSupportString = (String) cmdLine.getValue(minSupportOpt);            minSupport = Integer.parseInt(minSupportString);        }        int maxNGramSize = 1;        if (cmdLine.hasOption(maxNGramSizeOpt)) {            try {                maxNGramSize = Integer.parseInt(cmdLine.getValue(maxNGramSizeOpt).toString());            } catch (NumberFormatException ex) {                            }        }                if (cmdLine.hasOption(overwriteOutput)) {            HadoopUtil.delete(getConf(), outputDir);        }        float minLLRValue = LLRReducer.DEFAULT_MIN_LLR;        if (cmdLine.hasOption(minLLROpt)) {            minLLRValue = Float.parseFloat(cmdLine.getValue(minLLROpt).toString());        }                int reduceTasks = 1;        if (cmdLine.hasOption(numReduceTasksOpt)) {            reduceTasks = Integer.parseInt(cmdLine.getValue(numReduceTasksOpt).toString());        }                Class<? extends Analyzer> analyzerClass = StandardAnalyzer.class;        if (cmdLine.hasOption(analyzerNameOpt)) {            String className = cmdLine.getValue(analyzerNameOpt).toString();            analyzerClass = Class.forName(className).asSubclass(Analyzer.class);                                    AnalyzerUtils.createAnalyzer(analyzerClass);        }        boolean processIdf;        if (cmdLine.hasOption(weightOpt)) {            String wString = cmdLine.getValue(weightOpt).toString();            if ("tf".equalsIgnoreCase(wString)) {                processIdf = false;            } else if ("tfidf".equalsIgnoreCase(wString)) {                processIdf = true;            } else {                throw new OptionException(weightOpt);            }        } else {            processIdf = true;        }        int minDf = 1;        if (cmdLine.hasOption(minDFOpt)) {            minDf = Integer.parseInt(cmdLine.getValue(minDFOpt).toString());        }        int maxDFPercent = 99;        if (cmdLine.hasOption(maxDFPercentOpt)) {            maxDFPercent = Integer.parseInt(cmdLine.getValue(maxDFPercentOpt).toString());        }        double maxDFSigma = -1.0;        if (cmdLine.hasOption(maxDFSigmaOpt)) {            maxDFSigma = Double.parseDouble(cmdLine.getValue(maxDFSigmaOpt).toString());        }        float norm = PartialVectorMerger.NO_NORMALIZING;        if (cmdLine.hasOption(powerOpt)) {            String power = cmdLine.getValue(powerOpt).toString();            if ("INF".equals(power)) {                norm = Float.POSITIVE_INFINITY;            } else {                norm = Float.parseFloat(power);            }        }        boolean logNormalize = false;        if (cmdLine.hasOption(logNormalizeOpt)) {            logNormalize = true;        }                Configuration conf = getConf();        Path tokenizedPath = new Path(outputDir, DocumentProcessor.TOKENIZED_DOCUMENT_OUTPUT_FOLDER);                        DocumentProcessor.tokenizeDocuments(inputDir, analyzerClass, tokenizedPath, conf);        boolean sequentialAccessOutput = false;        if (cmdLine.hasOption(sequentialAccessVectorOpt)) {            sequentialAccessOutput = true;        }        boolean namedVectors = false;        if (cmdLine.hasOption(namedVectorOpt)) {            namedVectors = true;        }        boolean shouldPrune = maxDFSigma >= 0.0 || maxDFPercent > 0.00;        String tfDirName = shouldPrune ? DictionaryVectorizer.DOCUMENT_VECTOR_OUTPUT_FOLDER + "-toprune" : DictionaryVectorizer.DOCUMENT_VECTOR_OUTPUT_FOLDER;                if (processIdf) {            DictionaryVectorizer.createTermFrequencyVectors(tokenizedPath, outputDir, tfDirName, conf, minSupport, maxNGramSize, minLLRValue, -1.0f, false, reduceTasks, chunkSize, sequentialAccessOutput, namedVectors);        } else {            DictionaryVectorizer.createTermFrequencyVectors(tokenizedPath, outputDir, tfDirName, conf, minSupport, maxNGramSize, minLLRValue, norm, logNormalize, reduceTasks, chunkSize, sequentialAccessOutput, namedVectors);        }        Pair<Long[], List<Path>> docFrequenciesFeatures = null;                if (shouldPrune || processIdf) {                        docFrequenciesFeatures = TFIDFConverter.calculateDF(new Path(outputDir, tfDirName), outputDir, conf, chunkSize);        }                long maxDF = maxDFPercent;        if (shouldPrune) {            long vectorCount = docFrequenciesFeatures.getFirst()[1];            if (maxDFSigma >= 0.0) {                Path dfDir = new Path(outputDir, TFIDFConverter.WORDCOUNT_OUTPUT_FOLDER);                Path stdCalcDir = new Path(outputDir, HighDFWordsPruner.STD_CALC_DIR);                                double stdDev = BasicStats.stdDevForGivenMean(dfDir, stdCalcDir, 0.0, conf);                maxDF = (int) (100.0 * maxDFSigma * stdDev / vectorCount);            }            long maxDFThreshold = (long) (vectorCount * (maxDF / 100.0f));                        Path tfDir = new Path(outputDir, tfDirName);            Path prunedTFDir = new Path(outputDir, DictionaryVectorizer.DOCUMENT_VECTOR_OUTPUT_FOLDER);            Path prunedPartialTFDir = new Path(outputDir, DictionaryVectorizer.DOCUMENT_VECTOR_OUTPUT_FOLDER + "-partial");                        if (processIdf) {                HighDFWordsPruner.pruneVectors(tfDir, prunedTFDir, prunedPartialTFDir, maxDFThreshold, minDf, conf, docFrequenciesFeatures, -1.0f, false, reduceTasks);            } else {                HighDFWordsPruner.pruneVectors(tfDir, prunedTFDir, prunedPartialTFDir, maxDFThreshold, minDf, conf, docFrequenciesFeatures, norm, logNormalize, reduceTasks);            }            HadoopUtil.delete(new Configuration(conf), tfDir);        }        if (processIdf) {            TFIDFConverter.processTfIdf(new Path(outputDir, DictionaryVectorizer.DOCUMENT_VECTOR_OUTPUT_FOLDER), outputDir, conf, docFrequenciesFeatures, minDf, maxDF, norm, logNormalize, sequentialAccessOutput, namedVectors, reduceTasks);        }    } catch (OptionException e) {                CommandLineUtil.printHelp(group);    }    return 0;}
1
protected void reduce(Text key, Iterable<LongWritable> values, Context context) throws IOException, InterruptedException
{    long sum = 0;    for (LongWritable value : values) {        sum += value.get();    }    context.write(key, new LongWritable(sum));}
0
protected void map(Text key, StringTuple value, final Context context) throws IOException, InterruptedException
{    OpenObjectLongHashMap<String> wordCount = new OpenObjectLongHashMap<>();    for (String word : value.getEntries()) {        if (wordCount.containsKey(word)) {            wordCount.put(word, wordCount.get(word) + 1);        } else {            wordCount.put(word, 1);        }    }    wordCount.forEachPair(new ObjectLongProcedure<String>() {        @Override        public boolean apply(String first, long second) {            try {                context.write(new Text(first), new LongWritable(second));            } catch (IOException e) {                context.getCounter("Exception", "Output IO Exception").increment(1);            } catch (InterruptedException e) {                context.getCounter("Exception", "Interrupted Exception").increment(1);            }            return true;        }    });}
0
public boolean apply(String first, long second)
{    try {        context.write(new Text(first), new LongWritable(second));    } catch (IOException e) {        context.getCounter("Exception", "Output IO Exception").increment(1);    } catch (InterruptedException e) {        context.getCounter("Exception", "Interrupted Exception").increment(1);    }    return true;}
0
protected void reduce(Text key, Iterable<LongWritable> values, Context context) throws IOException, InterruptedException
{    long sum = 0;    for (LongWritable value : values) {        sum += value.get();    }    if (sum >= minSupport) {        context.write(key, new LongWritable(sum));    }}
0
protected void setup(Context context) throws IOException, InterruptedException
{    super.setup(context);    minSupport = context.getConfiguration().getInt(DictionaryVectorizer.MIN_SUPPORT, DictionaryVectorizer.DEFAULT_MIN_SUPPORT);}
0
protected void map(WritableComparable<?> key, VectorWritable value, Context context) throws IOException, InterruptedException
{    Vector vector = value.get();    for (Vector.Element e : vector.nonZeroes()) {        out.set(e.index());        context.write(out, ONE);    }    context.write(TOTAL_COUNT, ONE);}
0
protected void reduce(IntWritable key, Iterable<LongWritable> values, Context context) throws IOException, InterruptedException
{    long sum = 0;    for (LongWritable value : values) {        sum += value.get();    }    context.write(key, new LongWritable(sum));}
0
protected void reduce(Text key, Iterable<StringTuple> values, Context context) throws IOException, InterruptedException
{    Iterator<StringTuple> it = values.iterator();    if (!it.hasNext()) {        return;    }    List<String> value = Lists.newArrayList();    while (it.hasNext()) {        value.addAll(it.next().getEntries());    }        Vector vector = new RandomAccessSparseVector(dimension, value.size());    if (maxNGramSize >= 2) {        ShingleFilter sf = new ShingleFilter(new IteratorTokenStream(value.iterator()), maxNGramSize);        sf.reset();        try {            do {                String term = sf.getAttribute(CharTermAttribute.class).toString();                if (!term.isEmpty() && dictionary.containsKey(term)) {                                        int termId = dictionary.get(term);                    vector.setQuick(termId, vector.getQuick(termId) + 1);                }            } while (sf.incrementToken());            sf.end();        } finally {            Closeables.close(sf, true);        }    } else {        for (String term : value) {            if (!term.isEmpty() && dictionary.containsKey(term)) {                                int termId = dictionary.get(term);                vector.setQuick(termId, vector.getQuick(termId) + 1);            }        }    }    if (sequentialAccess) {        vector = new SequentialAccessSparseVector(vector);    }    if (namedVector) {        vector = new NamedVector(vector, key.toString());    }        if (vector.getNumNondefaultElements() > 0) {        VectorWritable vectorWritable = new VectorWritable(vector);        context.write(key, vectorWritable);    } else {        context.getCounter("TFPartialVectorReducer", "emptyVectorCount").increment(1);    }}
0
protected void setup(Context context) throws IOException, InterruptedException
{    super.setup(context);    Configuration conf = context.getConfiguration();    dimension = conf.getInt(PartialVectorMerger.DIMENSION, Integer.MAX_VALUE);    sequentialAccess = conf.getBoolean(PartialVectorMerger.SEQUENTIAL_ACCESS, false);    namedVector = conf.getBoolean(PartialVectorMerger.NAMED_VECTOR, false);    maxNGramSize = conf.getInt(DictionaryVectorizer.MAX_NGRAMS, maxNGramSize);    URI[] localFiles = DistributedCache.getCacheFiles(conf);    Path dictionaryFile = HadoopUtil.findInCacheByPartOfFilename(DictionaryVectorizer.DICTIONARY_FILE, localFiles);        for (Pair<Writable, IntWritable> record : new SequenceFileIterable<Writable, IntWritable>(dictionaryFile, true, conf)) {        dictionary.put(record.getFirst().toString(), record.getSecond().get());    }}
0
public double calculate(int tf, int df, int length, int numDocs)
{        return tf;}
0
public static void processTfIdf(Path input, Path output, Configuration baseConf, Pair<Long[], List<Path>> datasetFeatures, int minDf, long maxDF, float normPower, boolean logNormalize, boolean sequentialAccessOutput, boolean namedVector, int numReducers) throws IOException, InterruptedException, ClassNotFoundException
{    Preconditions.checkArgument(normPower == PartialVectorMerger.NO_NORMALIZING || normPower >= 0, "If specified normPower must be nonnegative", normPower);    Preconditions.checkArgument(normPower == PartialVectorMerger.NO_NORMALIZING || (normPower > 1 && !Double.isInfinite(normPower)) || !logNormalize, "normPower must be > 1 and not infinite if log normalization is chosen", normPower);    int partialVectorIndex = 0;    List<Path> partialVectorPaths = Lists.newArrayList();    List<Path> dictionaryChunks = datasetFeatures.getSecond();    for (Path dictionaryChunk : dictionaryChunks) {        Path partialVectorOutputPath = new Path(output, VECTOR_OUTPUT_FOLDER + partialVectorIndex++);        partialVectorPaths.add(partialVectorOutputPath);        makePartialVectors(input, baseConf, datasetFeatures.getFirst()[0], datasetFeatures.getFirst()[1], minDf, maxDF, dictionaryChunk, partialVectorOutputPath, sequentialAccessOutput, namedVector);    }    Configuration conf = new Configuration(baseConf);    Path outputDir = new Path(output, DOCUMENT_VECTOR_OUTPUT_FOLDER);    PartialVectorMerger.mergePartialVectors(partialVectorPaths, outputDir, baseConf, normPower, logNormalize, datasetFeatures.getFirst()[0].intValue(), sequentialAccessOutput, namedVector, numReducers);    HadoopUtil.delete(conf, partialVectorPaths);}
0
public static Pair<Long[], List<Path>> calculateDF(Path input, Path output, Configuration baseConf, int chunkSizeInMegabytes) throws IOException, InterruptedException, ClassNotFoundException
{    if (chunkSizeInMegabytes < MIN_CHUNKSIZE) {        chunkSizeInMegabytes = MIN_CHUNKSIZE;    } else if (chunkSizeInMegabytes > MAX_CHUNKSIZE) {                chunkSizeInMegabytes = MAX_CHUNKSIZE;    }    Path wordCountPath = new Path(output, WORDCOUNT_OUTPUT_FOLDER);    startDFCounting(input, wordCountPath, baseConf);    return createDictionaryChunks(wordCountPath, output, baseConf, chunkSizeInMegabytes);}
0
private static Pair<Long[], List<Path>> createDictionaryChunks(Path featureCountPath, Path dictionaryPathBase, Configuration baseConf, int chunkSizeInMegabytes) throws IOException
{    List<Path> chunkPaths = Lists.newArrayList();    Configuration conf = new Configuration(baseConf);    FileSystem fs = FileSystem.get(featureCountPath.toUri(), conf);    long chunkSizeLimit = chunkSizeInMegabytes * 1024L * 1024L;    int chunkIndex = 0;    Path chunkPath = new Path(dictionaryPathBase, FREQUENCY_FILE + chunkIndex);    chunkPaths.add(chunkPath);    SequenceFile.Writer freqWriter = new SequenceFile.Writer(fs, conf, chunkPath, IntWritable.class, LongWritable.class);    try {        long currentChunkSize = 0;        long featureCount = 0;        long vectorCount = Long.MAX_VALUE;        Path filesPattern = new Path(featureCountPath, OUTPUT_FILES_PATTERN);        for (Pair<IntWritable, LongWritable> record : new SequenceFileDirIterable<IntWritable, LongWritable>(filesPattern, PathType.GLOB, null, null, true, conf)) {            if (currentChunkSize > chunkSizeLimit) {                Closeables.close(freqWriter, false);                chunkIndex++;                chunkPath = new Path(dictionaryPathBase, FREQUENCY_FILE + chunkIndex);                chunkPaths.add(chunkPath);                freqWriter = new SequenceFile.Writer(fs, conf, chunkPath, IntWritable.class, LongWritable.class);                currentChunkSize = 0;            }            int fieldSize = SEQUENCEFILE_BYTE_OVERHEAD + Integer.SIZE / 8 + Long.SIZE / 8;            currentChunkSize += fieldSize;            IntWritable key = record.getFirst();            LongWritable value = record.getSecond();            if (key.get() >= 0) {                freqWriter.append(key, value);            } else if (key.get() == -1) {                vectorCount = value.get();            }            featureCount = Math.max(key.get(), featureCount);        }        featureCount++;        Long[] counts = { featureCount, vectorCount };        return new Pair<>(counts, chunkPaths);    } finally {        Closeables.close(freqWriter, false);    }}
0
private static void makePartialVectors(Path input, Configuration baseConf, Long featureCount, Long vectorCount, int minDf, long maxDF, Path dictionaryFilePath, Path output, boolean sequentialAccess, boolean namedVector) throws IOException, InterruptedException, ClassNotFoundException
{    Configuration conf = new Configuration(baseConf);        conf.set("io.serializations", "org.apache.hadoop.io.serializer.JavaSerialization," + "org.apache.hadoop.io.serializer.WritableSerialization");    conf.setLong(FEATURE_COUNT, featureCount);    conf.setLong(VECTOR_COUNT, vectorCount);    conf.setInt(MIN_DF, minDf);    conf.setLong(MAX_DF, maxDF);    conf.setBoolean(PartialVectorMerger.SEQUENTIAL_ACCESS, sequentialAccess);    conf.setBoolean(PartialVectorMerger.NAMED_VECTOR, namedVector);    DistributedCache.addCacheFile(dictionaryFilePath.toUri(), conf);    Job job = new Job(conf);    job.setJobName(": MakePartialVectors: input-folder: " + input + ", dictionary-file: " + dictionaryFilePath.toString());    job.setJarByClass(TFIDFConverter.class);    job.setOutputKeyClass(Text.class);    job.setOutputValueClass(VectorWritable.class);    FileInputFormat.setInputPaths(job, input);    FileOutputFormat.setOutputPath(job, output);    job.setMapperClass(Mapper.class);    job.setInputFormatClass(SequenceFileInputFormat.class);    job.setReducerClass(TFIDFPartialVectorReducer.class);    job.setOutputFormatClass(SequenceFileOutputFormat.class);    HadoopUtil.delete(conf, output);    boolean succeeded = job.waitForCompletion(true);    if (!succeeded) {        throw new IllegalStateException("Job failed!");    }}
0
private static void startDFCounting(Path input, Path output, Configuration baseConf) throws IOException, InterruptedException, ClassNotFoundException
{    Configuration conf = new Configuration(baseConf);        conf.set("io.serializations", "org.apache.hadoop.io.serializer.JavaSerialization," + "org.apache.hadoop.io.serializer.WritableSerialization");    Job job = new Job(conf);    job.setJobName("VectorTfIdf Document Frequency Count running over input: " + input);    job.setJarByClass(TFIDFConverter.class);    job.setOutputKeyClass(IntWritable.class);    job.setOutputValueClass(LongWritable.class);    FileInputFormat.setInputPaths(job, input);    FileOutputFormat.setOutputPath(job, output);    job.setMapperClass(TermDocumentCountMapper.class);    job.setInputFormatClass(SequenceFileInputFormat.class);    job.setCombinerClass(TermDocumentCountReducer.class);    job.setReducerClass(TermDocumentCountReducer.class);    job.setOutputFormatClass(SequenceFileOutputFormat.class);    HadoopUtil.delete(conf, output);    boolean succeeded = job.waitForCompletion(true);    if (!succeeded) {        throw new IllegalStateException("Job failed!");    }}
0
protected void reduce(WritableComparable<?> key, Iterable<VectorWritable> values, Context context) throws IOException, InterruptedException
{    Iterator<VectorWritable> it = values.iterator();    if (!it.hasNext()) {        return;    }    Vector value = it.next().get();    Vector vector = new RandomAccessSparseVector((int) featureCount, value.getNumNondefaultElements());    for (Vector.Element e : value.nonZeroes()) {        if (!dictionary.containsKey(e.index())) {            continue;        }        long df = dictionary.get(e.index());        if (maxDf > -1 && (100.0 * df) / vectorCount > maxDf) {            continue;        }        if (df < minDf) {            df = minDf;        }        vector.setQuick(e.index(), tfidf.calculate((int) e.get(), (int) df, (int) featureCount, (int) vectorCount));    }    if (sequentialAccess) {        vector = new SequentialAccessSparseVector(vector);    }    if (namedVector) {        vector = new NamedVector(vector, key.toString());    }    VectorWritable vectorWritable = new VectorWritable(vector);    context.write(key, vectorWritable);}
0
protected void setup(Context context) throws IOException, InterruptedException
{    super.setup(context);    Configuration conf = context.getConfiguration();    vectorCount = conf.getLong(TFIDFConverter.VECTOR_COUNT, 1);    featureCount = conf.getLong(TFIDFConverter.FEATURE_COUNT, 1);    minDf = conf.getInt(TFIDFConverter.MIN_DF, 1);    maxDf = conf.getLong(TFIDFConverter.MAX_DF, -1);    sequentialAccess = conf.getBoolean(PartialVectorMerger.SEQUENTIAL_ACCESS, false);    namedVector = conf.getBoolean(PartialVectorMerger.NAMED_VECTOR, false);    URI[] localFiles = DistributedCache.getCacheFiles(conf);    Path dictionaryFile = HadoopUtil.findInCacheByPartOfFilename(TFIDFConverter.FREQUENCY_FILE, localFiles);        for (Pair<IntWritable, LongWritable> record : new SequenceFileIterable<IntWritable, LongWritable>(dictionaryFile, true, conf)) {        dictionary.put(record.getFirst().get(), record.getSecond().get());    }}
0
public double calculate(int tf, int df, int length, int numDocs)
{        return sim.tf(tf) * sim.idf(df, numDocs);}
0
public Configuration getConf()
{    return conf;}
0
public void setConf(Configuration conf)
{    this.conf = conf;}
0
public String getAnalyzerClassName()
{    return analyzerClassName;}
0
public void setAnalyzerClassName(String analyzerClassName)
{    this.analyzerClassName = analyzerClassName;}
0
public String getEncoderName()
{    return encoderName;}
0
public void setEncoderName(String encoderName)
{    this.encoderName = encoderName;}
0
public boolean isSequentialAccess()
{    return sequentialAccess;}
0
public void setSequentialAccess(boolean sequentialAccess)
{    this.sequentialAccess = sequentialAccess;}
0
public String getTfDirName()
{    return tfDirName;}
0
public void setTfDirName(String tfDirName)
{    this.tfDirName = tfDirName;}
0
public boolean isNamedVectors()
{    return namedVectors;}
0
public void setNamedVectors(boolean namedVectors)
{    this.namedVectors = namedVectors;}
0
public int getCardinality()
{    return cardinality;}
0
public void setCardinality(int cardinality)
{    this.cardinality = cardinality;}
0
public String getEncoderClass()
{    return encoderClass;}
0
public void setEncoderClass(String encoderClass)
{    this.encoderClass = encoderClass;}
0
public int getMinSupport()
{    return minSupport;}
0
public void setMinSupport(int minSupport)
{    this.minSupport = minSupport;}
0
public int getMaxNGramSize()
{    return maxNGramSize;}
0
public void setMaxNGramSize(int maxNGramSize)
{    this.maxNGramSize = maxNGramSize;}
0
public float getMinLLRValue()
{    return minLLRValue;}
0
public void setMinLLRValue(float minLLRValue)
{    this.minLLRValue = minLLRValue;}
0
public float getNormPower()
{    return normPower;}
0
public void setNormPower(float normPower)
{    this.normPower = normPower;}
0
public boolean isLogNormalize()
{    return logNormalize;}
0
public void setLogNormalize(boolean logNormalize)
{    this.logNormalize = logNormalize;}
0
public int getNumReducers()
{    return numReducers;}
0
public void setNumReducers(int numReducers)
{    this.numReducers = numReducers;}
0
public int getChunkSizeInMegabytes()
{    return chunkSizeInMegabytes;}
0
public void setChunkSizeInMegabytes(int chunkSizeInMegabytes)
{    this.chunkSizeInMegabytes = chunkSizeInMegabytes;}
0
public static String version()
{    return Version.class.getPackage().getImplementationVersion();}
0
public static String versionFromResource() throws IOException
{    return Resources.toString(Resources.getResource("version"), Charsets.UTF_8);}
0
public static void main(String[] args) throws IOException
{    System.out.println(version() + ' ' + versionFromResource());}
0
public void testTasteException()
{        TasteException te1 = new TasteException();    TasteException te2 = new TasteException(te1);    TasteException te3 = new TasteException(te2.toString(), te2);    TasteException te4 = new TasteException(te3.toString());    te4.printStackTrace(new PrintStream(new ByteArrayOutputStream()));    te4.printStackTrace(new PrintWriter(new OutputStreamWriter(new ByteArrayOutputStream())));}
0
public void testNSUException()
{        TasteException te1 = new NoSuchUserException();    TasteException te4 = new NoSuchUserException(te1.toString());    te4.printStackTrace(new PrintStream(new ByteArrayOutputStream()));    te4.printStackTrace(new PrintWriter(new OutputStreamWriter(new ByteArrayOutputStream())));}
0
public void testNSIException()
{        TasteException te1 = new NoSuchItemException();    TasteException te4 = new NoSuchItemException(te1.toString());    te4.printStackTrace(new PrintStream(new ByteArrayOutputStream()));    te4.printStackTrace(new PrintWriter(new OutputStreamWriter(new ByteArrayOutputStream())));}
0
public void setUp() throws Exception
{    super.setUp();    inputFile = getTestTempFile("prefs.txt");    intermediateDir = getTestTempDir("intermediate");    intermediateDir.delete();    outputDir = getTestTempDir("output");    outputDir.delete();    tmpDir = getTestTempDir("tmp");    conf = getConfiguration();        SharingMapper.reset();}
0
public void completeJobToyExample() throws Exception
{    explicitExample(1);}
0
public void completeJobToyExampleMultithreaded() throws Exception
{    explicitExample(2);}
0
private void explicitExample(int numThreads) throws Exception
{    Double na = Double.NaN;    Matrix preferences = new SparseRowMatrix(4, 4, new Vector[] { new DenseVector(new double[] { 5.0, 5.0, 2.0, na }), new DenseVector(new double[] { 2.0, na, 3.0, 5.0 }), new DenseVector(new double[] { na, 5.0, na, 3.0 }), new DenseVector(new double[] { 3.0, na, na, 5.0 }) });    writeLines(inputFile, preferencesAsText(preferences));    ParallelALSFactorizationJob alsFactorization = new ParallelALSFactorizationJob();    alsFactorization.setConf(conf);    int numFeatures = 3;    int numIterations = 5;    double lambda = 0.065;    alsFactorization.run(new String[] { "--input", inputFile.getAbsolutePath(), "--output", outputDir.getAbsolutePath(), "--tempDir", tmpDir.getAbsolutePath(), "--lambda", String.valueOf(lambda), "--numFeatures", String.valueOf(numFeatures), "--numIterations", String.valueOf(numIterations), "--numThreadsPerSolver", String.valueOf(numThreads) });    Matrix u = MathHelper.readMatrix(conf, new Path(outputDir.getAbsolutePath(), "U/part-m-00000"), preferences.numRows(), numFeatures);    Matrix m = MathHelper.readMatrix(conf, new Path(outputDir.getAbsolutePath(), "M/part-m-00000"), preferences.numCols(), numFeatures);    StringBuilder info = new StringBuilder();    info.append("\nA - users x items\n\n");    info.append(MathHelper.nice(preferences));    info.append("\nU - users x features\n\n");    info.append(MathHelper.nice(u));    info.append("\nM - items x features\n\n");    info.append(MathHelper.nice(m));    Matrix Ak = u.times(m.transpose());    info.append("\nAk - users x items\n\n");    info.append(MathHelper.nice(Ak));    info.append('\n');        RunningAverage avg = new FullRunningAverage();    for (MatrixSlice slice : preferences) {        for (Element e : slice.nonZeroes()) {            if (!Double.isNaN(e.get())) {                double pref = e.get();                double estimate = u.viewRow(slice.index()).dot(m.viewRow(e.index()));                double err = pref - estimate;                avg.addDatum(err * err);                            }        }    }    double rmse = Math.sqrt(avg.getAverage());        assertTrue(rmse < 0.2);}
1
public void completeJobImplicitToyExample() throws Exception
{    implicitExample(1);}
0
public void completeJobImplicitToyExampleMultithreaded() throws Exception
{    implicitExample(2);}
0
public void implicitExample(int numThreads) throws Exception
{    Matrix observations = new SparseRowMatrix(4, 4, new Vector[] { new DenseVector(new double[] { 5.0, 5.0, 2.0, 0 }), new DenseVector(new double[] { 2.0, 0, 3.0, 5.0 }), new DenseVector(new double[] { 0, 5.0, 0, 3.0 }), new DenseVector(new double[] { 3.0, 0, 0, 5.0 }) });    Matrix preferences = new SparseRowMatrix(4, 4, new Vector[] { new DenseVector(new double[] { 1.0, 1.0, 1.0, 0 }), new DenseVector(new double[] { 1.0, 0, 1.0, 1.0 }), new DenseVector(new double[] { 0, 1.0, 0, 1.0 }), new DenseVector(new double[] { 1.0, 0, 0, 1.0 }) });    writeLines(inputFile, preferencesAsText(observations));    ParallelALSFactorizationJob alsFactorization = new ParallelALSFactorizationJob();    alsFactorization.setConf(conf);    int numFeatures = 3;    int numIterations = 5;    double lambda = 0.065;    double alpha = 20;    alsFactorization.run(new String[] { "--input", inputFile.getAbsolutePath(), "--output", outputDir.getAbsolutePath(), "--tempDir", tmpDir.getAbsolutePath(), "--lambda", String.valueOf(lambda), "--implicitFeedback", String.valueOf(true), "--alpha", String.valueOf(alpha), "--numFeatures", String.valueOf(numFeatures), "--numIterations", String.valueOf(numIterations), "--numThreadsPerSolver", String.valueOf(numThreads) });    Matrix u = MathHelper.readMatrix(conf, new Path(outputDir.getAbsolutePath(), "U/part-m-00000"), observations.numRows(), numFeatures);    Matrix m = MathHelper.readMatrix(conf, new Path(outputDir.getAbsolutePath(), "M/part-m-00000"), observations.numCols(), numFeatures);    StringBuilder info = new StringBuilder();    info.append("\nObservations - users x items\n");    info.append(MathHelper.nice(observations));    info.append("\nA - users x items\n\n");    info.append(MathHelper.nice(preferences));    info.append("\nU - users x features\n\n");    info.append(MathHelper.nice(u));    info.append("\nM - items x features\n\n");    info.append(MathHelper.nice(m));    Matrix Ak = u.times(m.transpose());    info.append("\nAk - users x items\n\n");    info.append(MathHelper.nice(Ak));    info.append('\n');        RunningAverage avg = new FullRunningAverage();    for (MatrixSlice slice : preferences) {        for (Element e : slice.nonZeroes()) {            if (!Double.isNaN(e.get())) {                double pref = e.get();                double estimate = u.viewRow(slice.index()).dot(m.viewRow(e.index()));                double confidence = 1 + alpha * observations.getQuick(slice.index(), e.index());                double err = confidence * (pref - estimate) * (pref - estimate);                avg.addDatum(err);                            }        }    }    double rmse = Math.sqrt(avg.getAverage());        assertTrue(rmse < 0.4);}
1
public void exampleWithIDMapping() throws Exception
{    String[] preferencesWithLongIDs = { "5568227754922264005,-4758971626494767444,5.0", "5568227754922264005,3688396615879561990,5.0", "5568227754922264005,4594226737871995304,2.0", "550945997885173934,-4758971626494767444,2.0", "550945997885173934,4594226737871995304,3.0", "550945997885173934,706816485922781596,5.0", "2448095297482319463,3688396615879561990,5.0", "2448095297482319463,706816485922781596,3.0", "6839920411763636962,-4758971626494767444,3.0", "6839920411763636962,706816485922781596,5.0" };    writeLines(inputFile, preferencesWithLongIDs);    ParallelALSFactorizationJob alsFactorization = new ParallelALSFactorizationJob();    alsFactorization.setConf(conf);    int numFeatures = 3;    int numIterations = 5;    double lambda = 0.065;    alsFactorization.run(new String[] { "--input", inputFile.getAbsolutePath(), "--output", outputDir.getAbsolutePath(), "--tempDir", tmpDir.getAbsolutePath(), "--lambda", String.valueOf(lambda), "--numFeatures", String.valueOf(numFeatures), "--numIterations", String.valueOf(numIterations), "--numThreadsPerSolver", String.valueOf(1), "--usesLongIDs", String.valueOf(true) });    OpenIntLongHashMap userIDIndex = TasteHadoopUtils.readIDIndexMap(outputDir.getAbsolutePath() + "/userIDIndex/part-r-00000", conf);    assertEquals(4, userIDIndex.size());    OpenIntLongHashMap itemIDIndex = TasteHadoopUtils.readIDIndexMap(outputDir.getAbsolutePath() + "/itemIDIndex/part-r-00000", conf);    assertEquals(4, itemIDIndex.size());    OpenIntObjectHashMap<Vector> u = MathHelper.readMatrixRows(conf, new Path(outputDir.getAbsolutePath(), "U/part-m-00000"));    OpenIntObjectHashMap<Vector> m = MathHelper.readMatrixRows(conf, new Path(outputDir.getAbsolutePath(), "M/part-m-00000"));    assertEquals(4, u.size());    assertEquals(4, m.size());    RunningAverage avg = new FullRunningAverage();    for (String line : preferencesWithLongIDs) {        String[] tokens = TasteHadoopUtils.splitPrefTokens(line);        long userID = Long.parseLong(tokens[TasteHadoopUtils.USER_ID_POS]);        long itemID = Long.parseLong(tokens[TasteHadoopUtils.ITEM_ID_POS]);        double rating = Double.parseDouble(tokens[2]);        Vector userFeatures = u.get(TasteHadoopUtils.idToIndex(userID));        Vector itemFeatures = m.get(TasteHadoopUtils.idToIndex(itemID));        double estimate = userFeatures.dot(itemFeatures);        double err = rating - estimate;        avg.addDatum(err * err);    }    double rmse = Math.sqrt(avg.getAverage());        assertTrue(rmse < 0.2);}
1
protected static String preferencesAsText(Matrix preferences)
{    StringBuilder prefsAsText = new StringBuilder();    String separator = "";    for (MatrixSlice slice : preferences) {        for (Element e : slice.nonZeroes()) {            if (!Double.isNaN(e.get())) {                prefsAsText.append(separator).append(slice.index()).append(',').append(e.index()).append(',').append(e.get());                separator = "\n";            }        }    }    System.out.println(prefsAsText.toString());    return prefsAsText.toString();}
0
public void recommenderJobWithIDMapping() throws Exception
{    String[] preferencesWithLongIDs = { "5568227754922264005,-4758971626494767444,5.0", "5568227754922264005,3688396615879561990,5.0", "5568227754922264005,4594226737871995304,2.0", "550945997885173934,-4758971626494767444,2.0", "550945997885173934,4594226737871995304,3.0", "550945997885173934,706816485922781596,5.0", "2448095297482319463,3688396615879561990,5.0", "2448095297482319463,706816485922781596,3.0", "6839920411763636962,-4758971626494767444,3.0", "6839920411763636962,706816485922781596,5.0" };    writeLines(inputFile, preferencesWithLongIDs);    ParallelALSFactorizationJob alsFactorization = new ParallelALSFactorizationJob();    alsFactorization.setConf(conf);    int numFeatures = 3;    int numIterations = 5;    double lambda = 0.065;    Configuration conf = getConfiguration();    int success = ToolRunner.run(alsFactorization, new String[] { "-Dhadoop.tmp.dir=" + conf.get("hadoop.tmp.dir"), "--input", inputFile.getAbsolutePath(), "--output", intermediateDir.getAbsolutePath(), "--tempDir", tmpDir.getAbsolutePath(), "--lambda", String.valueOf(lambda), "--numFeatures", String.valueOf(numFeatures), "--numIterations", String.valueOf(numIterations), "--numThreadsPerSolver", String.valueOf(1), "--usesLongIDs", String.valueOf(true) });    assertEquals(0, success);        SharingMapper.reset();    RecommenderJob recommender = new RecommenderJob();    success = ToolRunner.run(recommender, new String[] { "-Dhadoop.tmp.dir=" + conf.get("hadoop.tmp.dir"), "--input", intermediateDir.getAbsolutePath() + "/userRatings/", "--userFeatures", intermediateDir.getAbsolutePath() + "/U/", "--itemFeatures", intermediateDir.getAbsolutePath() + "/M/", "--numRecommendations", String.valueOf(2), "--maxRating", String.valueOf(5.0), "--numThreads", String.valueOf(2), "--usesLongIDs", String.valueOf(true), "--userIDIndex", intermediateDir.getAbsolutePath() + "/userIDIndex/", "--itemIDIndex", intermediateDir.getAbsolutePath() + "/itemIDIndex/", "--output", outputDir.getAbsolutePath() });    assertEquals(0, success);}
0
public void testUserItemFilter() throws Exception
{    Configuration conf = getConfiguration();    IDReader idReader = new IDReader(conf);    Map<Long, FastIDSet> userItemFilter = new HashMap<>();    long user1 = 1;    long user2 = 2;    idReader.addUserAndItemIdToUserItemFilter(userItemFilter, user1, 100L);    idReader.addUserAndItemIdToUserItemFilter(userItemFilter, user1, 200L);    idReader.addUserAndItemIdToUserItemFilter(userItemFilter, user2, 300L);    FastIDSet userIds = IDReader.extractAllUserIdsFromUserItemFilter(userItemFilter);    assertEquals(2, userIds.size());    assertTrue(userIds.contains(user1));    assertTrue(userIds.contains(user1));    setField(idReader, USER_ITEM_FILTER_FIELD, userItemFilter);    FastIDSet itemsForUser1 = idReader.getItemsToRecommendForUser(user1);    assertEquals(2, itemsForUser1.size());    assertTrue(itemsForUser1.contains(100L));    assertTrue(itemsForUser1.contains(200L));    FastIDSet itemsForUser2 = idReader.getItemsToRecommendForUser(user2);    assertEquals(1, itemsForUser2.size());    assertTrue(itemsForUser2.contains(300L));    FastIDSet itemsForNonExistingUser = idReader.getItemsToRecommendForUser(3L);    assertTrue(itemsForNonExistingUser.isEmpty());}
0
public void testItemIDIndexMapper() throws Exception
{    Mapper<LongWritable, Text, VarIntWritable, VarLongWritable>.Context context = EasyMock.createMock(Mapper.Context.class);    context.write(new VarIntWritable(TasteHadoopUtils.idToIndex(789L)), new VarLongWritable(789L));    EasyMock.replay(context);    new ItemIDIndexMapper().map(new LongWritable(123L), new Text("456,789,5.0"), context);    EasyMock.verify(context);}
0
public void testItemIDIndexReducer() throws Exception
{    Reducer<VarIntWritable, VarLongWritable, VarIntWritable, VarLongWritable>.Context context = EasyMock.createMock(Reducer.Context.class);    context.write(new VarIntWritable(123), new VarLongWritable(45L));    EasyMock.replay(context);    new ItemIDIndexReducer().reduce(new VarIntWritable(123), Arrays.asList(new VarLongWritable(67L), new VarLongWritable(89L), new VarLongWritable(45L)), context);    EasyMock.verify(context);}
0
public void testToItemPrefsMapper() throws Exception
{    Mapper<LongWritable, Text, VarLongWritable, VarLongWritable>.Context context = EasyMock.createMock(Mapper.Context.class);    context.write(new VarLongWritable(12L), new EntityPrefWritable(34L, 1.0f));    context.write(new VarLongWritable(56L), new EntityPrefWritable(78L, 2.0f));    EasyMock.replay(context);    ToItemPrefsMapper mapper = new ToItemPrefsMapper();    mapper.map(new LongWritable(123L), new Text("12,34,1"), context);    mapper.map(new LongWritable(456L), new Text("56,78,2"), context);    EasyMock.verify(context);}
0
public void testToItemPrefsMapperBooleanData() throws Exception
{    Mapper<LongWritable, Text, VarLongWritable, VarLongWritable>.Context context = EasyMock.createMock(Mapper.Context.class);    context.write(new VarLongWritable(12L), new VarLongWritable(34L));    context.write(new VarLongWritable(56L), new VarLongWritable(78L));    EasyMock.replay(context);    ToItemPrefsMapper mapper = new ToItemPrefsMapper();    setField(mapper, "booleanData", true);    mapper.map(new LongWritable(123L), new Text("12,34"), context);    mapper.map(new LongWritable(456L), new Text("56,78"), context);    EasyMock.verify(context);}
0
public void testToUserVectorReducer() throws Exception
{    Reducer<VarLongWritable, VarLongWritable, VarLongWritable, VectorWritable>.Context context = EasyMock.createMock(Reducer.Context.class);    Counter userCounters = EasyMock.createMock(Counter.class);    EasyMock.expect(context.getCounter(ToUserVectorsReducer.Counters.USERS)).andReturn(userCounters);    userCounters.increment(1);    context.write(EasyMock.eq(new VarLongWritable(12L)), MathHelper.vectorMatches(MathHelper.elem(TasteHadoopUtils.idToIndex(34L), 1.0), MathHelper.elem(TasteHadoopUtils.idToIndex(56L), 2.0)));    EasyMock.replay(context, userCounters);    Collection<VarLongWritable> varLongWritables = Lists.newLinkedList();    varLongWritables.add(new EntityPrefWritable(34L, 1.0f));    varLongWritables.add(new EntityPrefWritable(56L, 2.0f));    new ToUserVectorsReducer().reduce(new VarLongWritable(12L), varLongWritables, context);    EasyMock.verify(context, userCounters);}
0
public void testToUserVectorReducerWithBooleanData() throws Exception
{    Reducer<VarLongWritable, VarLongWritable, VarLongWritable, VectorWritable>.Context context = EasyMock.createMock(Reducer.Context.class);    Counter userCounters = EasyMock.createMock(Counter.class);    EasyMock.expect(context.getCounter(ToUserVectorsReducer.Counters.USERS)).andReturn(userCounters);    userCounters.increment(1);    context.write(EasyMock.eq(new VarLongWritable(12L)), MathHelper.vectorMatches(MathHelper.elem(TasteHadoopUtils.idToIndex(34L), 1.0), MathHelper.elem(TasteHadoopUtils.idToIndex(56L), 1.0)));    EasyMock.replay(context, userCounters);    new ToUserVectorsReducer().reduce(new VarLongWritable(12L), Arrays.asList(new VarLongWritable(34L), new VarLongWritable(56L)), context);    EasyMock.verify(context, userCounters);}
0
public void testSimilarityMatrixRowWrapperMapper() throws Exception
{    Mapper<IntWritable, VectorWritable, VarIntWritable, VectorOrPrefWritable>.Context context = EasyMock.createMock(Mapper.Context.class);    context.write(EasyMock.eq(new VarIntWritable(12)), vectorOfVectorOrPrefWritableMatches(MathHelper.elem(34, 0.5), MathHelper.elem(56, 0.7)));    EasyMock.replay(context);    RandomAccessSparseVector vector = new RandomAccessSparseVector(Integer.MAX_VALUE, 100);    vector.set(12, 1.0);    vector.set(34, 0.5);    vector.set(56, 0.7);    new SimilarityMatrixRowWrapperMapper().map(new IntWritable(12), new VectorWritable(vector), context);    EasyMock.verify(context);}
0
private static VectorOrPrefWritable vectorOfVectorOrPrefWritableMatches(final Vector.Element... elements)
{    EasyMock.reportMatcher(new IArgumentMatcher() {        @Override        public boolean matches(Object argument) {            if (argument instanceof VectorOrPrefWritable) {                Vector v = ((VectorOrPrefWritable) argument).getVector();                return MathHelper.consistsOf(v, elements);            }            return false;        }        @Override        public void appendTo(StringBuffer buffer) {        }    });    return null;}
0
public boolean matches(Object argument)
{    if (argument instanceof VectorOrPrefWritable) {        Vector v = ((VectorOrPrefWritable) argument).getVector();        return MathHelper.consistsOf(v, elements);    }    return false;}
0
public void appendTo(StringBuffer buffer)
{}
0
public void testUserVectorSplitterMapper() throws Exception
{    Mapper<VarLongWritable, VectorWritable, VarIntWritable, VectorOrPrefWritable>.Context context = EasyMock.createMock(Mapper.Context.class);    context.write(EasyMock.eq(new VarIntWritable(34)), prefOfVectorOrPrefWritableMatches(123L, 0.5f));    context.write(EasyMock.eq(new VarIntWritable(56)), prefOfVectorOrPrefWritableMatches(123L, 0.7f));    EasyMock.replay(context);    UserVectorSplitterMapper mapper = new UserVectorSplitterMapper();    setField(mapper, "maxPrefsPerUserConsidered", 10);    RandomAccessSparseVector vector = new RandomAccessSparseVector(Integer.MAX_VALUE, 100);    vector.set(34, 0.5);    vector.set(56, 0.7);    mapper.map(new VarLongWritable(123L), new VectorWritable(vector), context);    EasyMock.verify(context);}
0
private static VectorOrPrefWritable prefOfVectorOrPrefWritableMatches(final long userID, final float prefValue)
{    EasyMock.reportMatcher(new IArgumentMatcher() {        @Override        public boolean matches(Object argument) {            if (argument instanceof VectorOrPrefWritable) {                VectorOrPrefWritable pref = (VectorOrPrefWritable) argument;                return pref.getUserID() == userID && pref.getValue() == prefValue;            }            return false;        }        @Override        public void appendTo(StringBuffer buffer) {        }    });    return null;}
0
public boolean matches(Object argument)
{    if (argument instanceof VectorOrPrefWritable) {        VectorOrPrefWritable pref = (VectorOrPrefWritable) argument;        return pref.getUserID() == userID && pref.getValue() == prefValue;    }    return false;}
0
public void appendTo(StringBuffer buffer)
{}
0
public void testUserVectorSplitterMapperUserExclusion() throws Exception
{    Mapper<VarLongWritable, VectorWritable, VarIntWritable, VectorOrPrefWritable>.Context context = EasyMock.createMock(Mapper.Context.class);    context.write(EasyMock.eq(new VarIntWritable(34)), prefOfVectorOrPrefWritableMatches(123L, 0.5f));    context.write(EasyMock.eq(new VarIntWritable(56)), prefOfVectorOrPrefWritableMatches(123L, 0.7f));    EasyMock.replay(context);    FastIDSet usersToRecommendFor = new FastIDSet();    usersToRecommendFor.add(123L);    UserVectorSplitterMapper mapper = new UserVectorSplitterMapper();    setField(mapper, "maxPrefsPerUserConsidered", 10);    setField(mapper, "usersToRecommendFor", usersToRecommendFor);    RandomAccessSparseVector vector = new RandomAccessSparseVector(Integer.MAX_VALUE, 100);    vector.set(34, 0.5);    vector.set(56, 0.7);    mapper.map(new VarLongWritable(123L), new VectorWritable(vector), context);    mapper.map(new VarLongWritable(456L), new VectorWritable(vector), context);    EasyMock.verify(context);}
0
public void testUserVectorSplitterMapperOnlySomePrefsConsidered() throws Exception
{    Mapper<VarLongWritable, VectorWritable, VarIntWritable, VectorOrPrefWritable>.Context context = EasyMock.createMock(Mapper.Context.class);    context.write(EasyMock.eq(new VarIntWritable(34)), prefOfVectorOrPrefWritableMatchesNaN(123L));    context.write(EasyMock.eq(new VarIntWritable(56)), prefOfVectorOrPrefWritableMatches(123L, 0.7f));    EasyMock.replay(context);    UserVectorSplitterMapper mapper = new UserVectorSplitterMapper();    setField(mapper, "maxPrefsPerUserConsidered", 1);    RandomAccessSparseVector vector = new RandomAccessSparseVector(Integer.MAX_VALUE, 100);    vector.set(34, 0.5);    vector.set(56, 0.7);    mapper.map(new VarLongWritable(123L), new VectorWritable(vector), context);    EasyMock.verify(context);}
0
private static VectorOrPrefWritable prefOfVectorOrPrefWritableMatchesNaN(final long userID)
{    EasyMock.reportMatcher(new IArgumentMatcher() {        @Override        public boolean matches(Object argument) {            if (argument instanceof VectorOrPrefWritable) {                VectorOrPrefWritable pref = (VectorOrPrefWritable) argument;                return pref.getUserID() == userID && Float.isNaN(pref.getValue());            }            return false;        }        @Override        public void appendTo(StringBuffer buffer) {        }    });    return null;}
0
public boolean matches(Object argument)
{    if (argument instanceof VectorOrPrefWritable) {        VectorOrPrefWritable pref = (VectorOrPrefWritable) argument;        return pref.getUserID() == userID && Float.isNaN(pref.getValue());    }    return false;}
0
public void appendTo(StringBuffer buffer)
{}
0
public void testToVectorAndPrefReducer() throws Exception
{    Reducer<VarIntWritable, VectorOrPrefWritable, VarIntWritable, VectorAndPrefsWritable>.Context context = EasyMock.createMock(Reducer.Context.class);    context.write(EasyMock.eq(new VarIntWritable(1)), vectorAndPrefsWritableMatches(Arrays.asList(123L, 456L), Arrays.asList(1.0f, 2.0f), MathHelper.elem(3, 0.5), MathHelper.elem(7, 0.8)));    EasyMock.replay(context);    Vector similarityColumn = new RandomAccessSparseVector(Integer.MAX_VALUE, 100);    similarityColumn.set(3, 0.5);    similarityColumn.set(7, 0.8);    VectorOrPrefWritable itemPref1 = new VectorOrPrefWritable(123L, 1.0f);    VectorOrPrefWritable itemPref2 = new VectorOrPrefWritable(456L, 2.0f);    VectorOrPrefWritable similarities = new VectorOrPrefWritable(similarityColumn);    new ToVectorAndPrefReducer().reduce(new VarIntWritable(1), Arrays.asList(itemPref1, itemPref2, similarities), context);    EasyMock.verify(context);}
0
private static VectorAndPrefsWritable vectorAndPrefsWritableMatches(final List<Long> userIDs, final List<Float> prefValues, final Vector.Element... elements)
{    EasyMock.reportMatcher(new IArgumentMatcher() {        @Override        public boolean matches(Object argument) {            if (argument instanceof VectorAndPrefsWritable) {                VectorAndPrefsWritable vectorAndPrefs = (VectorAndPrefsWritable) argument;                if (!vectorAndPrefs.getUserIDs().equals(userIDs)) {                    return false;                }                if (!vectorAndPrefs.getValues().equals(prefValues)) {                    return false;                }                return MathHelper.consistsOf(vectorAndPrefs.getVector(), elements);            }            return false;        }        @Override        public void appendTo(StringBuffer buffer) {        }    });    return null;}
0
public boolean matches(Object argument)
{    if (argument instanceof VectorAndPrefsWritable) {        VectorAndPrefsWritable vectorAndPrefs = (VectorAndPrefsWritable) argument;        if (!vectorAndPrefs.getUserIDs().equals(userIDs)) {            return false;        }        if (!vectorAndPrefs.getValues().equals(prefValues)) {            return false;        }        return MathHelper.consistsOf(vectorAndPrefs.getVector(), elements);    }    return false;}
0
public void appendTo(StringBuffer buffer)
{}
0
public void testToVectorAndPrefReducerExceptionOn2Vectors() throws Exception
{    Reducer<VarIntWritable, VectorOrPrefWritable, VarIntWritable, VectorAndPrefsWritable>.Context context = EasyMock.createMock(Reducer.Context.class);    EasyMock.replay(context);    Vector similarityColumn1 = new RandomAccessSparseVector(Integer.MAX_VALUE, 100);    Vector similarityColumn2 = new RandomAccessSparseVector(Integer.MAX_VALUE, 100);    VectorOrPrefWritable similarities1 = new VectorOrPrefWritable(similarityColumn1);    VectorOrPrefWritable similarities2 = new VectorOrPrefWritable(similarityColumn2);    try {        new ToVectorAndPrefReducer().reduce(new VarIntWritable(1), Arrays.asList(similarities1, similarities2), context);        fail();    } catch (IllegalStateException e) {        }    EasyMock.verify(context);}
0
public void testItemFilterMapper() throws Exception
{    Mapper<LongWritable, Text, VarLongWritable, VarLongWritable>.Context context = EasyMock.createMock(Mapper.Context.class);    context.write(new VarLongWritable(34L), new VarLongWritable(12L));    context.write(new VarLongWritable(78L), new VarLongWritable(56L));    EasyMock.replay(context);    ItemFilterMapper mapper = new ItemFilterMapper();    mapper.map(null, new Text("12,34"), context);    mapper.map(null, new Text("56,78"), context);    EasyMock.verify(context);}
0
public void testItemFilterAsVectorAndPrefsReducer() throws Exception
{    Reducer<VarLongWritable, VarLongWritable, VarIntWritable, VectorAndPrefsWritable>.Context context = EasyMock.createMock(Reducer.Context.class);    int itemIDIndex = TasteHadoopUtils.idToIndex(123L);    context.write(EasyMock.eq(new VarIntWritable(itemIDIndex)), vectorAndPrefsForFilteringMatches(123L, 456L, 789L));    EasyMock.replay(context);    new ItemFilterAsVectorAndPrefsReducer().reduce(new VarLongWritable(123L), Arrays.asList(new VarLongWritable(456L), new VarLongWritable(789L)), context);    EasyMock.verify(context);}
0
 static VectorAndPrefsWritable vectorAndPrefsForFilteringMatches(final long itemID, final long... userIDs)
{    EasyMock.reportMatcher(new IArgumentMatcher() {        @Override        public boolean matches(Object argument) {            if (argument instanceof VectorAndPrefsWritable) {                VectorAndPrefsWritable vectorAndPrefs = (VectorAndPrefsWritable) argument;                Vector vector = vectorAndPrefs.getVector();                if (vector.getNumNondefaultElements() != 1) {                    return false;                }                if (!Double.isNaN(vector.get(TasteHadoopUtils.idToIndex(itemID)))) {                    return false;                }                if (userIDs.length != vectorAndPrefs.getUserIDs().size()) {                    return false;                }                for (long userID : userIDs) {                    if (!vectorAndPrefs.getUserIDs().contains(userID)) {                        return false;                    }                }                return true;            }            return false;        }        @Override        public void appendTo(StringBuffer buffer) {        }    });    return null;}
0
public boolean matches(Object argument)
{    if (argument instanceof VectorAndPrefsWritable) {        VectorAndPrefsWritable vectorAndPrefs = (VectorAndPrefsWritable) argument;        Vector vector = vectorAndPrefs.getVector();        if (vector.getNumNondefaultElements() != 1) {            return false;        }        if (!Double.isNaN(vector.get(TasteHadoopUtils.idToIndex(itemID)))) {            return false;        }        if (userIDs.length != vectorAndPrefs.getUserIDs().size()) {            return false;        }        for (long userID : userIDs) {            if (!vectorAndPrefs.getUserIDs().contains(userID)) {                return false;            }        }        return true;    }    return false;}
0
public void appendTo(StringBuffer buffer)
{}
0
public void testPartialMultiplyMapper() throws Exception
{    Vector similarityColumn = new RandomAccessSparseVector(Integer.MAX_VALUE, 100);    similarityColumn.set(3, 0.5);    similarityColumn.set(7, 0.8);    Mapper<VarIntWritable, VectorAndPrefsWritable, VarLongWritable, PrefAndSimilarityColumnWritable>.Context context = EasyMock.createMock(Mapper.Context.class);    PrefAndSimilarityColumnWritable one = new PrefAndSimilarityColumnWritable();    PrefAndSimilarityColumnWritable two = new PrefAndSimilarityColumnWritable();    one.set(1.0f, similarityColumn);    two.set(3.0f, similarityColumn);    context.write(EasyMock.eq(new VarLongWritable(123L)), EasyMock.eq(one));    context.write(EasyMock.eq(new VarLongWritable(456L)), EasyMock.eq(two));    EasyMock.replay(context);    VectorAndPrefsWritable vectorAndPrefs = new VectorAndPrefsWritable(similarityColumn, Arrays.asList(123L, 456L), Arrays.asList(1.0f, 3.0f));    new PartialMultiplyMapper().map(new VarIntWritable(1), vectorAndPrefs, context);    EasyMock.verify(context);}
0
public void testAggregateAndRecommendReducer() throws Exception
{    Reducer<VarLongWritable, PrefAndSimilarityColumnWritable, VarLongWritable, RecommendedItemsWritable>.Context context = EasyMock.createMock(Reducer.Context.class);    context.write(EasyMock.eq(new VarLongWritable(123L)), recommendationsMatch(new MutableRecommendedItem(1L, 2.8f), new MutableRecommendedItem(2L, 2.0f)));    EasyMock.replay(context);    RandomAccessSparseVector similarityColumnOne = new RandomAccessSparseVector(Integer.MAX_VALUE, 100);    similarityColumnOne.set(1, 0.1);    similarityColumnOne.set(2, 0.5);    RandomAccessSparseVector similarityColumnTwo = new RandomAccessSparseVector(Integer.MAX_VALUE, 100);    similarityColumnTwo.set(1, 0.9);    similarityColumnTwo.set(2, 0.5);    List<PrefAndSimilarityColumnWritable> values = Arrays.asList(new PrefAndSimilarityColumnWritable(1.0f, similarityColumnOne), new PrefAndSimilarityColumnWritable(3.0f, similarityColumnTwo));    OpenIntLongHashMap indexItemIDMap = new OpenIntLongHashMap();    indexItemIDMap.put(1, 1L);    indexItemIDMap.put(2, 2L);    AggregateAndRecommendReducer reducer = new AggregateAndRecommendReducer();    setField(reducer, "indexItemIDMap", indexItemIDMap);    setField(reducer, "recommendationsPerUser", 3);    reducer.reduce(new VarLongWritable(123L), values, context);    EasyMock.verify(context);}
0
public void testAggregateAndRecommendReducerExcludeRecommendationsBasedOnOneItem() throws Exception
{    Reducer<VarLongWritable, PrefAndSimilarityColumnWritable, VarLongWritable, RecommendedItemsWritable>.Context context = EasyMock.createMock(Reducer.Context.class);    context.write(EasyMock.eq(new VarLongWritable(123L)), recommendationsMatch(new MutableRecommendedItem(1L, 2.8f)));    EasyMock.replay(context);    RandomAccessSparseVector similarityColumnOne = new RandomAccessSparseVector(Integer.MAX_VALUE, 100);    similarityColumnOne.set(1, 0.1);    RandomAccessSparseVector similarityColumnTwo = new RandomAccessSparseVector(Integer.MAX_VALUE, 100);    similarityColumnTwo.set(1, 0.9);    similarityColumnTwo.set(2, 0.5);    List<PrefAndSimilarityColumnWritable> values = Arrays.asList(new PrefAndSimilarityColumnWritable(1.0f, similarityColumnOne), new PrefAndSimilarityColumnWritable(3.0f, similarityColumnTwo));    OpenIntLongHashMap indexItemIDMap = new OpenIntLongHashMap();    indexItemIDMap.put(1, 1L);    indexItemIDMap.put(2, 2L);    AggregateAndRecommendReducer reducer = new AggregateAndRecommendReducer();    setField(reducer, "indexItemIDMap", indexItemIDMap);    setField(reducer, "recommendationsPerUser", 3);    reducer.reduce(new VarLongWritable(123L), values, context);    EasyMock.verify(context);}
0
public void testAggregateAndRecommendReducerLimitNumberOfRecommendations() throws Exception
{    Reducer<VarLongWritable, PrefAndSimilarityColumnWritable, VarLongWritable, RecommendedItemsWritable>.Context context = EasyMock.createMock(Reducer.Context.class);    context.write(EasyMock.eq(new VarLongWritable(123L)), recommendationsMatch(new MutableRecommendedItem(1L, 2.8f)));    EasyMock.replay(context);    RandomAccessSparseVector similarityColumnOne = new RandomAccessSparseVector(Integer.MAX_VALUE, 100);    similarityColumnOne.set(1, 0.1);    similarityColumnOne.set(2, 0.5);    RandomAccessSparseVector similarityColumnTwo = new RandomAccessSparseVector(Integer.MAX_VALUE, 100);    similarityColumnTwo.set(1, 0.9);    similarityColumnTwo.set(2, 0.5);    List<PrefAndSimilarityColumnWritable> values = Arrays.asList(new PrefAndSimilarityColumnWritable(1.0f, similarityColumnOne), new PrefAndSimilarityColumnWritable(3.0f, similarityColumnTwo));    OpenIntLongHashMap indexItemIDMap = new OpenIntLongHashMap();    indexItemIDMap.put(1, 1L);    indexItemIDMap.put(2, 2L);    AggregateAndRecommendReducer reducer = new AggregateAndRecommendReducer();    setField(reducer, "indexItemIDMap", indexItemIDMap);    setField(reducer, "recommendationsPerUser", 1);    reducer.reduce(new VarLongWritable(123L), values, context);    EasyMock.verify(context);}
0
 static RecommendedItemsWritable recommendationsMatch(final RecommendedItem... items)
{    EasyMock.reportMatcher(new IArgumentMatcher() {        @Override        public boolean matches(Object argument) {            if (argument instanceof RecommendedItemsWritable) {                RecommendedItemsWritable recommendedItemsWritable = (RecommendedItemsWritable) argument;                List<RecommendedItem> expectedItems = Arrays.asList(items);                return expectedItems.equals(recommendedItemsWritable.getRecommendedItems());            }            return false;        }        @Override        public void appendTo(StringBuffer buffer) {        }    });    return null;}
0
public boolean matches(Object argument)
{    if (argument instanceof RecommendedItemsWritable) {        RecommendedItemsWritable recommendedItemsWritable = (RecommendedItemsWritable) argument;        List<RecommendedItem> expectedItems = Arrays.asList(items);        return expectedItems.equals(recommendedItemsWritable.getRecommendedItems());    }    return false;}
0
public void appendTo(StringBuffer buffer)
{}
0
public void testCompleteJob() throws Exception
{    File inputFile = getTestTempFile("prefs.txt");    File outputDir = getTestTempDir("output");    outputDir.delete();    File similaritiesOutputDir = getTestTempDir("outputSimilarities");    similaritiesOutputDir.delete();    File tmpDir = getTestTempDir("tmp");    writeLines(inputFile, "1,1,5", "1,2,5", "1,3,2", "2,1,2", "2,3,3", "2,4,5", "3,2,5", "3,4,3", "4,1,3", "4,4,5");    RecommenderJob recommenderJob = new RecommenderJob();    Configuration conf = getConfiguration();    conf.set("mapred.input.dir", inputFile.getAbsolutePath());    conf.set("mapred.output.dir", outputDir.getAbsolutePath());    conf.setBoolean("mapred.output.compress", false);    recommenderJob.setConf(conf);    recommenderJob.run(new String[] { "--tempDir", tmpDir.getAbsolutePath(), "--similarityClassname", TanimotoCoefficientSimilarity.class.getName(), "--numRecommendations", "4", "--outputPathForSimilarityMatrix", similaritiesOutputDir.getAbsolutePath() });    Map<Long, List<RecommendedItem>> recommendations = readRecommendations(new File(outputDir, "part-r-00000"));    assertEquals(4, recommendations.size());    for (Entry<Long, List<RecommendedItem>> entry : recommendations.entrySet()) {        long userID = entry.getKey();        List<RecommendedItem> items = entry.getValue();        assertNotNull(items);        RecommendedItem item1 = items.get(0);        if (userID == 1L) {            assertEquals(1, items.size());            assertEquals(4L, item1.getItemID());            assertEquals(4.3, item1.getValue(), 0.05);        }        if (userID == 2L) {            assertEquals(1, items.size());            assertEquals(2L, item1.getItemID());            assertEquals(3.3, item1.getValue(), 0.05);        }        if (userID == 3L) {            assertEquals(2, items.size());            assertEquals(3L, item1.getItemID());            assertEquals(4.1, item1.getValue(), 0.05);            RecommendedItem item2 = items.get(1);            assertEquals(1L, item2.getItemID());            assertEquals(3.7, item2.getValue(), 0.05);        }        if (userID == 4L) {            assertEquals(2, items.size());            assertEquals(2L, item1.getItemID());            assertEquals(4.0, item1.getValue(), 0.05);            RecommendedItem item2 = items.get(1);            assertEquals(3L, item2.getItemID());            assertEquals(3.5, item2.getValue(), 0.05);        }    }    Map<Pair<Long, Long>, Double> similarities = readSimilarities(new File(similaritiesOutputDir, "part-r-00000"));    assertEquals(6, similarities.size());    assertEquals(0.25, similarities.get(new Pair<>(1L, 2L)), EPSILON);    assertEquals(0.6666666666666666, similarities.get(new Pair<>(1L, 3L)), EPSILON);    assertEquals(0.5, similarities.get(new Pair<>(1L, 4L)), EPSILON);    assertEquals(0.3333333333333333, similarities.get(new Pair<>(2L, 3L)), EPSILON);    assertEquals(0.25, similarities.get(new Pair<>(2L, 4L)), EPSILON);    assertEquals(0.25, similarities.get(new Pair<>(3L, 4L)), EPSILON);}
0
public void testCompleteJobBoolean() throws Exception
{    File inputFile = getTestTempFile("prefs.txt");    File outputDir = getTestTempDir("output");    outputDir.delete();    File tmpDir = getTestTempDir("tmp");    File usersFile = getTestTempFile("users.txt");    writeLines(usersFile, "3");    writeLines(inputFile, "1,1", "1,2", "1,3", "2,1", "2,3", "2,4", "3,2", "3,4", "4,1", "4,4");    RecommenderJob recommenderJob = new RecommenderJob();    Configuration conf = getConfiguration();    conf.set("mapred.input.dir", inputFile.getAbsolutePath());    conf.set("mapred.output.dir", outputDir.getAbsolutePath());    conf.setBoolean("mapred.output.compress", false);    recommenderJob.setConf(conf);    recommenderJob.run(new String[] { "--tempDir", tmpDir.getAbsolutePath(), "--similarityClassname", CooccurrenceCountSimilarity.class.getName(), "--booleanData", "true", "--usersFile", usersFile.getAbsolutePath() });    Map<Long, List<RecommendedItem>> recommendations = readRecommendations(new File(outputDir, "part-r-00000"));    List<RecommendedItem> recommendedToCow = recommendations.get(3L);    assertEquals(2, recommendedToCow.size());    RecommendedItem item1 = recommendedToCow.get(0);    RecommendedItem item2 = recommendedToCow.get(1);    assertEquals(1L, item1.getItemID());    assertEquals(3L, item2.getItemID());    /* predicted pref must be the sum of similarities:    *    item1: coocc(burger, hotdog) + coocc(burger, icecream) = 3     *    item2: coocc(berries, hotdog) + coocc(berries, icecream) = 2 */    assertEquals(3, item1.getValue(), 0.05);    assertEquals(2, item2.getValue(), 0.05);}
0
public void testCompleteJobWithFiltering() throws Exception
{    File inputFile = getTestTempFile("prefs.txt");    File userFile = getTestTempFile("users.txt");    File filterFile = getTestTempFile("filter.txt");    File outputDir = getTestTempDir("output");    outputDir.delete();    File tmpDir = getTestTempDir("tmp");    writeLines(inputFile, "1,1,5", "1,2,5", "1,3,2", "2,1,2", "2,3,3", "2,4,5", "3,2,5", "3,4,3", "4,1,3", "4,4,5");    /* only compute recommendations for the donkey */    writeLines(userFile, "4");    /* do not recommend the hotdog for the donkey */    writeLines(filterFile, "4,2");    RecommenderJob recommenderJob = new RecommenderJob();    Configuration conf = getConfiguration();    conf.set("mapred.input.dir", inputFile.getAbsolutePath());    conf.set("mapred.output.dir", outputDir.getAbsolutePath());    conf.setBoolean("mapred.output.compress", false);    recommenderJob.setConf(conf);    recommenderJob.run(new String[] { "--tempDir", tmpDir.getAbsolutePath(), "--similarityClassname", TanimotoCoefficientSimilarity.class.getName(), "--numRecommendations", "1", "--usersFile", userFile.getAbsolutePath(), "--filterFile", filterFile.getAbsolutePath() });    Map<Long, List<RecommendedItem>> recommendations = readRecommendations(new File(outputDir, "part-r-00000"));    assertEquals(1, recommendations.size());    assertTrue(recommendations.containsKey(4L));    assertEquals(1, recommendations.get(4L).size());    /* berries should have been recommended to the donkey */    RecommendedItem recommendedItem = recommendations.get(4L).get(0);    assertEquals(3L, recommendedItem.getItemID());    assertEquals(3.5, recommendedItem.getValue(), 0.05);}
0
 static Map<Pair<Long, Long>, Double> readSimilarities(File file) throws IOException
{    Map<Pair<Long, Long>, Double> similarities = Maps.newHashMap();    for (String line : new FileLineIterable(file)) {        String[] parts = line.split("\t");        similarities.put(new Pair<>(Long.parseLong(parts[0]), Long.parseLong(parts[1])), Double.parseDouble(parts[2]));    }    return similarities;}
0
 static Map<Long, List<RecommendedItem>> readRecommendations(File file) throws IOException
{    Map<Long, List<RecommendedItem>> recommendations = Maps.newHashMap();    for (String line : new FileLineIterable(file)) {        String[] keyValue = line.split("\t");        long userID = Long.parseLong(keyValue[0]);        String[] tokens = keyValue[1].replaceAll("\\[", "").replaceAll("\\]", "").split(",");        List<RecommendedItem> items = Lists.newLinkedList();        for (String token : tokens) {            String[] itemTokens = token.split(":");            long itemID = Long.parseLong(itemTokens[0]);            float value = Float.parseFloat(itemTokens[1]);            items.add(new GenericRecommendedItem(itemID, value));        }        recommendations.put(userID, items);    }    return recommendations;}
0
public void testToUsersReducerMinPreferencesUserIgnored() throws Exception
{    Reducer<VarLongWritable, VarLongWritable, VarLongWritable, VectorWritable>.Context context = EasyMock.createMock(Reducer.Context.class);    ToUserVectorsReducer reducer = new ToUserVectorsReducer();    setField(reducer, "minPreferences", 2);    EasyMock.replay(context);    reducer.reduce(new VarLongWritable(123), Collections.singletonList(new VarLongWritable(456)), context);    EasyMock.verify(context);}
0
public void testToUsersReducerMinPreferencesUserPasses() throws Exception
{    Reducer<VarLongWritable, VarLongWritable, VarLongWritable, VectorWritable>.Context context = EasyMock.createMock(Reducer.Context.class);    Counter userCounters = EasyMock.createMock(Counter.class);    ToUserVectorsReducer reducer = new ToUserVectorsReducer();    setField(reducer, "minPreferences", 2);    EasyMock.expect(context.getCounter(ToUserVectorsReducer.Counters.USERS)).andReturn(userCounters);    userCounters.increment(1);    context.write(EasyMock.eq(new VarLongWritable(123)), MathHelper.vectorMatches(MathHelper.elem(TasteHadoopUtils.idToIndex(456L), 1.0), MathHelper.elem(TasteHadoopUtils.idToIndex(789L), 1.0)));    EasyMock.replay(context, userCounters);    reducer.reduce(new VarLongWritable(123), Arrays.asList(new VarLongWritable(456), new VarLongWritable(789)), context);    EasyMock.verify(context, userCounters);}
0
public void testMostSimilarItemsPairsMapper() throws Exception
{    OpenIntLongHashMap indexItemIDMap = new OpenIntLongHashMap();    indexItemIDMap.put(12, 12L);    indexItemIDMap.put(34, 34L);    indexItemIDMap.put(56, 56L);    Mapper<IntWritable, VectorWritable, EntityEntityWritable, DoubleWritable>.Context context = EasyMock.createMock(Mapper.Context.class);    context.write(new EntityEntityWritable(34L, 56L), new DoubleWritable(0.9));    EasyMock.replay(context);    Vector vector = new RandomAccessSparseVector(Integer.MAX_VALUE);    vector.set(12, 0.2);    vector.set(56, 0.9);    ItemSimilarityJob.MostSimilarItemPairsMapper mapper = new ItemSimilarityJob.MostSimilarItemPairsMapper();    setField(mapper, "indexItemIDMap", indexItemIDMap);    setField(mapper, "maxSimilarItemsPerItem", 1);    mapper.map(new IntWritable(34), new VectorWritable(vector), context);    EasyMock.verify(context);}
0
public void testMostSimilarItemPairsReducer() throws Exception
{    Reducer<EntityEntityWritable, DoubleWritable, EntityEntityWritable, DoubleWritable>.Context context = EasyMock.createMock(Reducer.Context.class);    context.write(new EntityEntityWritable(123L, 456L), new DoubleWritable(0.5));    EasyMock.replay(context);    new ItemSimilarityJob.MostSimilarItemPairsReducer().reduce(new EntityEntityWritable(123L, 456L), Arrays.asList(new DoubleWritable(0.5), new DoubleWritable(0.5)), context);    EasyMock.verify(context);}
0
public void testCompleteJob() throws Exception
{    File inputFile = getTestTempFile("prefs.txt");    File outputDir = getTestTempDir("output");    outputDir.delete();    File tmpDir = getTestTempDir("tmp");    writeLines(inputFile, "2,1,1", "1,2,1", "3,4,1", "1,3,2", "2,3,1");    ItemSimilarityJob similarityJob = new ItemSimilarityJob();    Configuration conf = getConfiguration();    conf.set("mapred.input.dir", inputFile.getAbsolutePath());    conf.set("mapred.output.dir", outputDir.getAbsolutePath());    conf.setBoolean("mapred.output.compress", false);    similarityJob.setConf(conf);    similarityJob.run(new String[] { "--tempDir", tmpDir.getAbsolutePath(), "--similarityClassname", CosineSimilarity.class.getName() });    File outPart = outputDir.listFiles(new FilenameFilter() {        @Override        public boolean accept(File dir, String name) {            return name.startsWith("part-");        }    })[0];    BufferedReader reader = Files.newReader(outPart, Charsets.UTF_8);    String line;    int currentLine = 1;    while ((line = reader.readLine()) != null) {        String[] tokens = TAB.split(line);        long itemAID = Long.parseLong(tokens[0]);        long itemBID = Long.parseLong(tokens[1]);        double similarity = Double.parseDouble(tokens[2]);        if (currentLine == 1) {            assertEquals(1L, itemAID);            assertEquals(3L, itemBID);            assertEquals(0.45, similarity, 0.01);        }        if (currentLine == 2) {            assertEquals(2L, itemAID);            assertEquals(3L, itemBID);            assertEquals(0.89, similarity, 0.01);        }        currentLine++;    }    int linesWritten = currentLine - 1;    assertEquals(2, linesWritten);}
0
public boolean accept(File dir, String name)
{    return name.startsWith("part-");}
0
public void testMaxSimilaritiesPerItem() throws Exception
{    File inputFile = getTestTempFile("prefsForMaxSimilarities.txt");    File outputDir = getTestTempDir("output");    outputDir.delete();    File tmpDir = getTestTempDir("tmp");    writeLines(inputFile, "1,1,1", "1,3,1", "2,2,1", "2,3,1", "3,1,1", "3,2,1", "4,1,1", "4,2,1", "4,3,1", "5,2,1", "6,1,1", "6,2,1");    ItemSimilarityJob similarityJob = new ItemSimilarityJob();    Configuration conf = getConfiguration();    conf.set("mapred.input.dir", inputFile.getAbsolutePath());    conf.set("mapred.output.dir", outputDir.getAbsolutePath());    conf.setBoolean("mapred.output.compress", false);    similarityJob.setConf(conf);    similarityJob.run(new String[] { "--tempDir", tmpDir.getAbsolutePath(), "--similarityClassname", TanimotoCoefficientSimilarity.class.getName(), "--maxSimilaritiesPerItem", "1" });    File outPart = outputDir.listFiles(new FilenameFilter() {        @Override        public boolean accept(File dir, String name) {            return name.startsWith("part-");        }    })[0];    BufferedReader reader = Files.newReader(outPart, Charsets.UTF_8);    String line;    int currentLine = 1;    while ((line = reader.readLine()) != null) {        String[] tokens = TAB.split(line);        long itemAID = Long.parseLong(tokens[0]);        long itemBID = Long.parseLong(tokens[1]);        double similarity = Double.parseDouble(tokens[2]);        if (currentLine == 1) {            assertEquals(1L, itemAID);            assertEquals(2L, itemBID);            assertEquals(0.5, similarity, 0.0001);        }        if (currentLine == 2) {            assertEquals(1L, itemAID);            assertEquals(3L, itemBID);            assertEquals(0.4, similarity, 0.0001);        }        currentLine++;    }    int linesWritten = currentLine - 1;    assertEquals(2, linesWritten);}
0
public boolean accept(File dir, String name)
{    return name.startsWith("part-");}
0
public void testWithinRange()
{    assertTrue(TasteHadoopUtils.idToIndex(0) >= 0);    assertTrue(TasteHadoopUtils.idToIndex(0) < Integer.MAX_VALUE);    assertTrue(TasteHadoopUtils.idToIndex(1) >= 0);    assertTrue(TasteHadoopUtils.idToIndex(1) < Integer.MAX_VALUE);    assertTrue(TasteHadoopUtils.idToIndex(Long.MAX_VALUE) >= 0);    assertTrue(TasteHadoopUtils.idToIndex(Long.MAX_VALUE) < Integer.MAX_VALUE);    assertTrue(TasteHadoopUtils.idToIndex(Integer.MAX_VALUE) >= 0);    assertTrue(TasteHadoopUtils.idToIndex(Integer.MAX_VALUE) < Integer.MAX_VALUE);}
0
public void topK()
{    float[] ratings = { 0.5f, 0.6f, 0.7f, 2.0f, 0.0f };    List<RecommendedItem> topItems = findTop(ratings, 2);    assertEquals(2, topItems.size());    assertEquals(3L, topItems.get(0).getItemID());    assertEquals(2.0f, topItems.get(0).getValue(), MahoutTestCase.EPSILON);    assertEquals(2L, topItems.get(1).getItemID());    assertEquals(0.7f, topItems.get(1).getValue(), MahoutTestCase.EPSILON);}
0
public void topKInputSmallerThanK()
{    float[] ratings = { 0.7f, 2.0f };    List<RecommendedItem> topItems = findTop(ratings, 3);    assertEquals(2, topItems.size());    assertEquals(1L, topItems.get(0).getItemID());    assertEquals(2.0f, topItems.get(0).getValue(), MahoutTestCase.EPSILON);    assertEquals(0L, topItems.get(1).getItemID());    assertEquals(0.7f, topItems.get(1).getValue(), MahoutTestCase.EPSILON);}
0
private static List<RecommendedItem> findTop(float[] ratings, int k)
{    TopItemsQueue queue = new TopItemsQueue(k);    for (int item = 0; item < ratings.length; item++) {        MutableRecommendedItem top = queue.top();        if (ratings[item] > top.getValue()) {            top.set(item, ratings[item]);            queue.updateTop();        }    }    return queue.getTopItems();}
0
public void testGetSet()
{    BitSet bitSet = new BitSet(NUM_BITS);    for (int i = 0; i < NUM_BITS; i++) {        assertFalse(bitSet.get(i));    }    bitSet.set(0);    bitSet.set(NUM_BITS - 1);    assertTrue(bitSet.get(0));    assertTrue(bitSet.get(NUM_BITS - 1));}
0
public void testBounds1()
{    BitSet bitSet = new BitSet(NUM_BITS);    bitSet.set(1000);}
0
public void testBounds2()
{    BitSet bitSet = new BitSet(NUM_BITS);    bitSet.set(-1);}
0
public void testClear()
{    BitSet bitSet = new BitSet(NUM_BITS);    for (int i = 0; i < NUM_BITS; i++) {        bitSet.set(i);    }    for (int i = 0; i < NUM_BITS; i++) {        assertTrue(bitSet.get(i));    }    bitSet.clear();    for (int i = 0; i < NUM_BITS; i++) {        assertFalse(bitSet.get(i));    }}
0
public void testClone()
{    BitSet bitSet = new BitSet(NUM_BITS);    bitSet.set(NUM_BITS - 1);    bitSet = bitSet.clone();    assertTrue(bitSet.get(NUM_BITS - 1));}
0
public void testLotsOfGets() throws TasteException
{    Retriever<Object, Object> retriever = new IdentityRetriever();    Cache<Object, Object> cache = new Cache<>(retriever, 1000);    for (int i = 0; i < 1000000; i++) {        assertEquals(i, cache.get(i));    }}
0
public void testMixedUsage() throws TasteException
{    Random random = RandomUtils.getRandom();    Retriever<Object, Object> retriever = new IdentityRetriever();    Cache<Object, Object> cache = new Cache<>(retriever, 1000);    for (int i = 0; i < 1000000; i++) {        double r = random.nextDouble();        if (r < 0.01) {            cache.clear();        } else if (r < 0.1) {            cache.remove(r - 100);        } else {            assertEquals(i, cache.get(i));        }    }}
0
public Object get(Object key) throws TasteException
{    return key;}
0
public void testPutAndGet()
{    FastByIDMap<Long> map = new FastByIDMap<>();    assertNull(map.get(500000L));    map.put(500000L, 2L);    assertEquals(2L, (long) map.get(500000L));}
0
public void testRemove()
{    FastByIDMap<Long> map = new FastByIDMap<>();    map.put(500000L, 2L);    map.remove(500000L);    assertEquals(0, map.size());    assertTrue(map.isEmpty());    assertNull(map.get(500000L));}
0
public void testClear()
{    FastByIDMap<Long> map = new FastByIDMap<>();    map.put(500000L, 2L);    map.clear();    assertEquals(0, map.size());    assertTrue(map.isEmpty());    assertNull(map.get(500000L));}
0
public void testSizeEmpty()
{    FastByIDMap<Long> map = new FastByIDMap<>();    assertEquals(0, map.size());    assertTrue(map.isEmpty());    map.put(500000L, 2L);    assertEquals(1, map.size());    assertFalse(map.isEmpty());    map.remove(500000L);    assertEquals(0, map.size());    assertTrue(map.isEmpty());}
0
public void testContains()
{    FastByIDMap<String> map = buildTestFastMap();    assertTrue(map.containsKey(500000L));    assertTrue(map.containsKey(47L));    assertTrue(map.containsKey(2L));    assertTrue(map.containsValue("alpha"));    assertTrue(map.containsValue("bang"));    assertTrue(map.containsValue("beta"));    assertFalse(map.containsKey(999));    assertFalse(map.containsValue("something"));}
0
public void testRehash()
{    FastByIDMap<String> map = buildTestFastMap();    map.remove(500000L);    map.rehash();    assertNull(map.get(500000L));    assertEquals("bang", map.get(47L));}
0
public void testGrow()
{    FastByIDMap<String> map = new FastByIDMap<>(1, 1);    map.put(500000L, "alpha");    map.put(47L, "bang");    assertNull(map.get(500000L));    assertEquals("bang", map.get(47L));}
0
public void testVersusHashMap()
{    FastByIDMap<String> actual = new FastByIDMap<>();    Map<Long, String> expected = new HashMap<>(1000000);    Random r = RandomUtils.getRandom();    for (int i = 0; i < 1000000; i++) {        double d = r.nextDouble();        Long key = (long) r.nextInt(100);        if (d < 0.4) {            assertEquals(expected.get(key), actual.get(key));        } else {            if (d < 0.7) {                assertEquals(expected.put(key, "bang"), actual.put(key, "bang"));            } else {                assertEquals(expected.remove(key), actual.remove(key));            }            assertEquals(expected.size(), actual.size());            assertEquals(expected.isEmpty(), actual.isEmpty());        }    }}
0
public void testMaxSize()
{    FastByIDMap<String> map = new FastByIDMap<>();    map.put(4, "bang");    assertEquals(1, map.size());    map.put(47L, "bang");    assertEquals(2, map.size());    assertNull(map.get(500000L));    map.put(47L, "buzz");    assertEquals(2, map.size());    assertEquals("buzz", map.get(47L));}
0
private static FastByIDMap<String> buildTestFastMap()
{    FastByIDMap<String> map = new FastByIDMap<>();    map.put(500000L, "alpha");    map.put(47L, "bang");    map.put(2L, "beta");    return map;}
0
public void testContainsAndAdd()
{    FastIDSet set = new FastIDSet();    assertFalse(set.contains(1));    set.add(1);    assertTrue(set.contains(1));}
0
public void testRemove()
{    FastIDSet set = new FastIDSet();    set.add(1);    set.remove(1);    assertEquals(0, set.size());    assertTrue(set.isEmpty());    assertFalse(set.contains(1));}
0
public void testClear()
{    FastIDSet set = new FastIDSet();    set.add(1);    set.clear();    assertEquals(0, set.size());    assertTrue(set.isEmpty());    assertFalse(set.contains(1));}
0
public void testSizeEmpty()
{    FastIDSet set = new FastIDSet();    assertEquals(0, set.size());    assertTrue(set.isEmpty());    set.add(1);    assertEquals(1, set.size());    assertFalse(set.isEmpty());    set.remove(1);    assertEquals(0, set.size());    assertTrue(set.isEmpty());}
0
public void testContains()
{    FastIDSet set = buildTestFastSet();    assertTrue(set.contains(1));    assertTrue(set.contains(2));    assertTrue(set.contains(3));    assertFalse(set.contains(4));}
0
public void testReservedValues()
{    FastIDSet set = new FastIDSet();    try {        set.add(Long.MIN_VALUE);        fail("Should have thrown IllegalArgumentException");    } catch (IllegalArgumentException iae) {        }    assertFalse(set.contains(Long.MIN_VALUE));    try {        set.add(Long.MAX_VALUE);        fail("Should have thrown IllegalArgumentException");    } catch (IllegalArgumentException iae) {        }    assertFalse(set.contains(Long.MAX_VALUE));}
0
public void testRehash()
{    FastIDSet set = buildTestFastSet();    set.remove(1);    set.rehash();    assertFalse(set.contains(1));}
0
public void testGrow()
{    FastIDSet set = new FastIDSet(1);    set.add(1);    set.add(2);    assertTrue(set.contains(1));    assertTrue(set.contains(2));}
0
public void testIterator()
{    FastIDSet set = buildTestFastSet();    Collection<Long> expected = Sets.newHashSetWithExpectedSize(3);    expected.add(1L);    expected.add(2L);    expected.add(3L);    LongPrimitiveIterator it = set.iterator();    while (it.hasNext()) {        expected.remove(it.nextLong());    }    assertTrue(expected.isEmpty());}
0
public void testVersusHashSet()
{    FastIDSet actual = new FastIDSet(1);    Collection<Integer> expected = Sets.newHashSetWithExpectedSize(1000000);    Random r = RandomUtils.getRandom();    for (int i = 0; i < 1000000; i++) {        double d = r.nextDouble();        Integer key = r.nextInt(100);        if (d < 0.4) {            assertEquals(expected.contains(key), actual.contains(key));        } else {            if (d < 0.7) {                assertEquals(expected.add(key), actual.add(key));            } else {                assertEquals(expected.remove(key), actual.remove(key));            }            assertEquals(expected.size(), actual.size());            assertEquals(expected.isEmpty(), actual.isEmpty());        }    }}
0
private static FastIDSet buildTestFastSet()
{    FastIDSet set = new FastIDSet();    set.add(1);    set.add(2);    set.add(3);    return set;}
0
public void testPutAndGet()
{    Map<String, String> map = new FastMap<>();    assertNull(map.get("foo"));    map.put("foo", "bar");    assertEquals("bar", map.get("foo"));}
0
public void testRemove()
{    Map<String, String> map = new FastMap<>();    map.put("foo", "bar");    map.remove("foo");    assertEquals(0, map.size());    assertTrue(map.isEmpty());    assertNull(map.get("foo"));}
0
public void testClear()
{    Map<String, String> map = new FastMap<>();    map.put("foo", "bar");    map.clear();    assertEquals(0, map.size());    assertTrue(map.isEmpty());    assertNull(map.get("foo"));}
0
public void testSizeEmpty()
{    Map<String, String> map = new FastMap<>();    assertEquals(0, map.size());    assertTrue(map.isEmpty());    map.put("foo", "bar");    assertEquals(1, map.size());    assertFalse(map.isEmpty());    map.remove("foo");    assertEquals(0, map.size());    assertTrue(map.isEmpty());}
0
public void testContains()
{    FastMap<String, String> map = buildTestFastMap();    assertTrue(map.containsKey("foo"));    assertTrue(map.containsKey("baz"));    assertTrue(map.containsKey("alpha"));    assertTrue(map.containsValue("bar"));    assertTrue(map.containsValue("bang"));    assertTrue(map.containsValue("beta"));    assertFalse(map.containsKey("something"));    assertFalse(map.containsValue("something"));}
0
public void testNull1()
{    Map<String, String> map = new FastMap<>();    assertNull(map.get(null));    map.put(null, "bar");}
0
public void testNull2()
{    Map<String, String> map = new FastMap<>();    map.put("foo", null);}
0
public void testRehash()
{    FastMap<String, String> map = buildTestFastMap();    map.remove("foo");    map.rehash();    assertNull(map.get("foo"));    assertEquals("bang", map.get("baz"));}
0
public void testGrow()
{    Map<String, String> map = new FastMap<>(1, FastMap.NO_MAX_SIZE);    map.put("foo", "bar");    map.put("baz", "bang");    assertEquals("bar", map.get("foo"));    assertEquals("bang", map.get("baz"));}
0
public void testKeySet()
{    FastMap<String, String> map = buildTestFastMap();    Collection<String> expected = Sets.newHashSetWithExpectedSize(3);    expected.add("foo");    expected.add("baz");    expected.add("alpha");    Set<String> actual = map.keySet();    assertTrue(expected.containsAll(actual));    assertTrue(actual.containsAll(expected));    Iterator<String> it = actual.iterator();    while (it.hasNext()) {        String value = it.next();        if (!"baz".equals(value)) {            it.remove();        }    }    assertTrue(map.containsKey("baz"));    assertFalse(map.containsKey("foo"));    assertFalse(map.containsKey("alpha"));}
0
public void testValues()
{    FastMap<String, String> map = buildTestFastMap();    Collection<String> expected = Sets.newHashSetWithExpectedSize(3);    expected.add("bar");    expected.add("bang");    expected.add("beta");    Collection<String> actual = map.values();    assertTrue(expected.containsAll(actual));    assertTrue(actual.containsAll(expected));    Iterator<String> it = actual.iterator();    while (it.hasNext()) {        String value = it.next();        if (!"bang".equals(value)) {            it.remove();        }    }    assertTrue(map.containsValue("bang"));    assertFalse(map.containsValue("bar"));    assertFalse(map.containsValue("beta"));}
0
public void testEntrySet()
{    FastMap<String, String> map = buildTestFastMap();    Set<Map.Entry<String, String>> actual = map.entrySet();    Collection<String> expectedKeys = Sets.newHashSetWithExpectedSize(3);    expectedKeys.add("foo");    expectedKeys.add("baz");    expectedKeys.add("alpha");    Collection<String> expectedValues = Sets.newHashSetWithExpectedSize(3);    expectedValues.add("bar");    expectedValues.add("bang");    expectedValues.add("beta");    assertEquals(3, actual.size());    for (Map.Entry<String, String> entry : actual) {        expectedKeys.remove(entry.getKey());        expectedValues.remove(entry.getValue());    }    assertEquals(0, expectedKeys.size());    assertEquals(0, expectedValues.size());}
0
public void testVersusHashMap()
{    Map<Integer, String> actual = new FastMap<>(1, 1000000);    Map<Integer, String> expected = Maps.newHashMapWithExpectedSize(1000000);    Random r = RandomUtils.getRandom();    for (int i = 0; i < 1000000; i++) {        double d = r.nextDouble();        Integer key = r.nextInt(100);        if (d < 0.4) {            assertEquals(expected.get(key), actual.get(key));        } else {            if (d < 0.7) {                assertEquals(expected.put(key, "foo"), actual.put(key, "foo"));            } else {                assertEquals(expected.remove(key), actual.remove(key));            }            assertEquals(expected.size(), actual.size());            assertEquals(expected.isEmpty(), actual.isEmpty());        }    }}
0
public void testMaxSize()
{    Map<String, String> map = new FastMap<>(1, 1);    map.put("foo", "bar");    assertEquals(1, map.size());    map.put("baz", "bang");    assertEquals(1, map.size());    assertNull(map.get("foo"));    map.put("baz", "buzz");    assertEquals(1, map.size());    assertEquals("buzz", map.get("baz"));}
0
private static FastMap<String, String> buildTestFastMap()
{    FastMap<String, String> map = new FastMap<>();    map.put("foo", "bar");    map.put("baz", "bang");    map.put("alpha", "beta");    return map;}
0
public void testAverage()
{    RunningAverage avg = new FullRunningAverage();    RunningAverage inverted = new InvertedRunningAverage(avg);    assertEquals(0, inverted.getCount());    avg.addDatum(1.0);    assertEquals(1, inverted.getCount());    assertEquals(-1.0, inverted.getAverage(), EPSILON);    avg.addDatum(2.0);    assertEquals(2, inverted.getCount());    assertEquals(-1.5, inverted.getAverage(), EPSILON);}
0
public void testUnsupported1()
{    RunningAverage inverted = new InvertedRunningAverage(new FullRunningAverage());    inverted.addDatum(1.0);}
0
public void testUnsupported2()
{    RunningAverage inverted = new InvertedRunningAverage(new FullRunningAverage());    inverted.changeDatum(1.0);}
0
public void testUnsupported3()
{    RunningAverage inverted = new InvertedRunningAverage(new FullRunningAverage());    inverted.removeDatum(1.0);}
0
public void testAverageAndStdDev()
{    RunningAverageAndStdDev avg = new FullRunningAverageAndStdDev();    RunningAverageAndStdDev inverted = new InvertedRunningAverageAndStdDev(avg);    assertEquals(0, inverted.getCount());    avg.addDatum(1.0);    assertEquals(1, inverted.getCount());    assertEquals(-1.0, inverted.getAverage(), EPSILON);    avg.addDatum(2.0);    assertEquals(2, inverted.getCount());    assertEquals(-1.5, inverted.getAverage(), EPSILON);    assertEquals(Math.sqrt(2.0) / 2.0, inverted.getStandardDeviation(), EPSILON);}
0
public void testAndStdDevUnsupported1()
{    RunningAverage inverted = new InvertedRunningAverageAndStdDev(new FullRunningAverageAndStdDev());    inverted.addDatum(1.0);}
0
public void testAndStdDevUnsupported2()
{    RunningAverage inverted = new InvertedRunningAverageAndStdDev(new FullRunningAverageAndStdDev());    inverted.changeDatum(1.0);}
0
public void testAndStdDevUnsupported3()
{    RunningAverage inverted = new InvertedRunningAverageAndStdDev(new FullRunningAverageAndStdDev());    inverted.removeDatum(1.0);}
0
public void testEmpty()
{    LongPrimitiveIterator it = new LongPrimitiveArrayIterator(new long[0]);    assertFalse(it.hasNext());    it.next();}
0
public void testNext()
{    LongPrimitiveIterator it = new LongPrimitiveArrayIterator(new long[] { 3, 2, 1 });    assertTrue(it.hasNext());    assertEquals(3, (long) it.next());    assertTrue(it.hasNext());    assertEquals(2, it.nextLong());    assertTrue(it.hasNext());    assertEquals(1, (long) it.next());    assertFalse(it.hasNext());    it.nextLong();}
0
public void testPeekSkip()
{    LongPrimitiveIterator it = new LongPrimitiveArrayIterator(new long[] { 3, 2, 1 });    assertEquals(3, it.peek());    it.skip(2);    assertEquals(1, it.nextLong());    assertFalse(it.hasNext());}
0
public void refresh(Collection<Refreshable> alreadyRefreshed)
{    call();}
0
public Object call()
{    callCount++;    return null;}
0
 int getCallCount()
{    return callCount;}
0
public void testCallable()
{    MockRefreshable mock = new MockRefreshable();    Refreshable helper = new RefreshHelper(mock);    helper.refresh(null);    assertEquals(1, mock.getCallCount());}
0
public void testNoCallable()
{    Refreshable helper = new RefreshHelper(null);    helper.refresh(null);}
0
public void testDependencies()
{    RefreshHelper helper = new RefreshHelper(null);    MockRefreshable mock1 = new MockRefreshable();    MockRefreshable mock2 = new MockRefreshable();    helper.addDependency(mock1);    helper.addDependency(mock2);    helper.refresh(null);    assertEquals(1, mock1.getCallCount());    assertEquals(1, mock2.getCallCount());}
0
public void testAlreadyRefreshed()
{    RefreshHelper helper = new RefreshHelper(null);    MockRefreshable mock1 = new MockRefreshable();    MockRefreshable mock2 = new MockRefreshable();    helper.addDependency(mock1);    helper.addDependency(mock2);    Collection<Refreshable> alreadyRefreshed = Sets.newHashSetWithExpectedSize(1);    alreadyRefreshed.add(mock1);    helper.refresh(alreadyRefreshed);    assertEquals(0, mock1.getCallCount());    assertEquals(1, mock2.getCallCount());}
0
public void testFull()
{    RunningAverageAndStdDev average = new FullRunningAverageAndStdDev();    assertEquals(0, average.getCount());    assertTrue(Double.isNaN(average.getAverage()));    assertTrue(Double.isNaN(average.getStandardDeviation()));    average.addDatum(6.0);    assertEquals(1, average.getCount());    assertEquals(6.0, average.getAverage(), EPSILON);    assertTrue(Double.isNaN(average.getStandardDeviation()));    average.addDatum(6.0);    assertEquals(2, average.getCount());    assertEquals(6.0, average.getAverage(), EPSILON);    assertEquals(0.0, average.getStandardDeviation(), EPSILON);    average.removeDatum(6.0);    assertEquals(1, average.getCount());    assertEquals(6.0, average.getAverage(), EPSILON);    assertTrue(Double.isNaN(average.getStandardDeviation()));    average.addDatum(-4.0);    assertEquals(2, average.getCount());    assertEquals(1.0, average.getAverage(), EPSILON);    assertEquals(5.0 * 1.4142135623730951, average.getStandardDeviation(), EPSILON);    average.removeDatum(4.0);    assertEquals(1, average.getCount());    assertEquals(-2.0, average.getAverage(), EPSILON);    assertTrue(Double.isNaN(average.getStandardDeviation()));}
0
public void testFullBig()
{    RunningAverageAndStdDev average = new FullRunningAverageAndStdDev();    Random r = RandomUtils.getRandom();    for (int i = 0; i < 100000; i++) {        average.addDatum(r.nextDouble() * 1000.0);    }    assertEquals(500.0, average.getAverage(), SMALL_EPSILON);    assertEquals(1000.0 / Math.sqrt(12.0), average.getStandardDeviation(), SMALL_EPSILON);}
0
public void testStddev()
{    RunningAverageAndStdDev runningAverage = new FullRunningAverageAndStdDev();    assertEquals(0, runningAverage.getCount());    assertTrue(Double.isNaN(runningAverage.getAverage()));    runningAverage.addDatum(1.0);    assertEquals(1, runningAverage.getCount());    assertEquals(1.0, runningAverage.getAverage(), EPSILON);    assertTrue(Double.isNaN(runningAverage.getStandardDeviation()));    runningAverage.addDatum(1.0);    assertEquals(2, runningAverage.getCount());    assertEquals(1.0, runningAverage.getAverage(), EPSILON);    assertEquals(0.0, runningAverage.getStandardDeviation(), EPSILON);    runningAverage.addDatum(7.0);    assertEquals(3, runningAverage.getCount());    assertEquals(3.0, runningAverage.getAverage(), EPSILON);    assertEquals(3.464101552963257, runningAverage.getStandardDeviation(), EPSILON);    runningAverage.addDatum(5.0);    assertEquals(4, runningAverage.getCount());    assertEquals(3.5, runningAverage.getAverage(), EPSILON);    assertEquals(3.0, runningAverage.getStandardDeviation(), EPSILON);}
0
public void testFull()
{    RunningAverage runningAverage = new FullRunningAverage();    assertEquals(0, runningAverage.getCount());    assertTrue(Double.isNaN(runningAverage.getAverage()));    runningAverage.addDatum(1.0);    assertEquals(1, runningAverage.getCount());    assertEquals(1.0, runningAverage.getAverage(), EPSILON);    runningAverage.addDatum(1.0);    assertEquals(2, runningAverage.getCount());    assertEquals(1.0, runningAverage.getAverage(), EPSILON);    runningAverage.addDatum(4.0);    assertEquals(3, runningAverage.getCount());    assertEquals(2.0, runningAverage.getAverage(), EPSILON);    runningAverage.addDatum(-4.0);    assertEquals(4, runningAverage.getCount());    assertEquals(0.5, runningAverage.getAverage(), EPSILON);    runningAverage.removeDatum(-4.0);    assertEquals(3, runningAverage.getCount());    assertEquals(2.0, runningAverage.getAverage(), EPSILON);    runningAverage.removeDatum(4.0);    assertEquals(2, runningAverage.getCount());    assertEquals(1.0, runningAverage.getAverage(), EPSILON);    runningAverage.changeDatum(0.0);    assertEquals(2, runningAverage.getCount());    assertEquals(1.0, runningAverage.getAverage(), EPSILON);    runningAverage.changeDatum(2.0);    assertEquals(2, runningAverage.getCount());    assertEquals(2.0, runningAverage.getAverage(), EPSILON);}
0
public void testCopyConstructor()
{    RunningAverage runningAverage = new FullRunningAverage();    runningAverage.addDatum(1.0);    runningAverage.addDatum(1.0);    assertEquals(2, runningAverage.getCount());    assertEquals(1.0, runningAverage.getAverage(), EPSILON);    RunningAverage copy = new FullRunningAverage(runningAverage.getCount(), runningAverage.getAverage());    assertEquals(2, copy.getCount());    assertEquals(1.0, copy.getAverage(), EPSILON);}
0
public void testEmptyCase()
{    assertFalse(new SamplingLongPrimitiveIterator(countingIterator(0), 0.9999).hasNext());    assertFalse(new SamplingLongPrimitiveIterator(countingIterator(0), 1).hasNext());}
0
public void testSmallInput()
{    SamplingLongPrimitiveIterator t = new SamplingLongPrimitiveIterator(countingIterator(1), 0.9999);    assertTrue(t.hasNext());    assertEquals(0L, t.nextLong());    assertFalse(t.hasNext());}
0
public void testBadRate1()
{    new SamplingLongPrimitiveIterator(countingIterator(1), 0.0);}
0
public void testBadRate2()
{    new SamplingLongPrimitiveIterator(countingIterator(1), 1.1);}
0
public void testExactSizeMatch()
{    SamplingLongPrimitiveIterator t = new SamplingLongPrimitiveIterator(countingIterator(10), 1);    for (int i = 0; i < 10; i++) {        assertTrue(t.hasNext());        assertEquals(i, t.next().intValue());    }    assertFalse(t.hasNext());}
0
public void testSample()
{    double p = 0.1;    int n = 1000;    double sd = Math.sqrt(n * p * (1.0 - p));    for (int i = 0; i < 1000; i++) {        SamplingLongPrimitiveIterator t = new SamplingLongPrimitiveIterator(countingIterator(n), p);        int k = 0;        while (t.hasNext()) {            long v = t.nextLong();            k++;            assertTrue(v >= 0L);            assertTrue(v < 1000L);        }                assertTrue(k >= 100 - 5 * sd);        assertTrue(k <= 100 + 5 * sd);    }}
0
private static LongPrimitiveArrayIterator countingIterator(int to)
{    long[] data = new long[to];    for (int i = 0; i < to; i++) {        data[i] = i;    }    return new LongPrimitiveArrayIterator(data);}
0
public void testWeighted()
{    WeightedRunningAverage runningAverage = new WeightedRunningAverage();    assertEquals(0, runningAverage.getCount());    assertTrue(Double.isNaN(runningAverage.getAverage()));    runningAverage.addDatum(1.0, 2.0);    assertEquals(1.0, runningAverage.getAverage(), EPSILON);    runningAverage.addDatum(1.0);    assertEquals(1.0, runningAverage.getAverage(), EPSILON);    runningAverage.addDatum(8.0, 0.5);    assertEquals(2.0, runningAverage.getAverage(), EPSILON);    runningAverage.addDatum(-4.0);    assertEquals(2.0 / 3.0, runningAverage.getAverage(), EPSILON);    runningAverage.removeDatum(-4.0);    assertEquals(2.0, runningAverage.getAverage(), EPSILON);    runningAverage.removeDatum(2.0, 2.0);    assertEquals(2.0, runningAverage.getAverage(), EPSILON);    runningAverage.changeDatum(0.0);    assertEquals(2.0, runningAverage.getAverage(), EPSILON);    runningAverage.changeDatum(4.0, 0.5);    assertEquals(5.0 / 1.5, runningAverage.getAverage(), EPSILON);}
0
public void testWeightedAndStdDev()
{    WeightedRunningAverageAndStdDev runningAverage = new WeightedRunningAverageAndStdDev();    assertEquals(0, runningAverage.getCount());    assertTrue(Double.isNaN(runningAverage.getAverage()));    assertTrue(Double.isNaN(runningAverage.getStandardDeviation()));    runningAverage.addDatum(1.0);    assertEquals(1.0, runningAverage.getAverage(), EPSILON);    assertTrue(Double.isNaN(runningAverage.getStandardDeviation()));    runningAverage.addDatum(1.0, 2.0);    assertEquals(1.0, runningAverage.getAverage(), EPSILON);    assertEquals(0.0, runningAverage.getStandardDeviation(), EPSILON);    runningAverage.addDatum(8.0, 0.5);    assertEquals(2.0, runningAverage.getAverage(), EPSILON);    assertEquals(Math.sqrt(10.5), runningAverage.getStandardDeviation(), EPSILON);    runningAverage.addDatum(-4.0);    assertEquals(2.0 / 3.0, runningAverage.getAverage(), EPSILON);    assertEquals(Math.sqrt(15.75), runningAverage.getStandardDeviation(), EPSILON);    runningAverage.removeDatum(-4.0);    assertEquals(2.0, runningAverage.getAverage(), EPSILON);    assertEquals(Math.sqrt(10.5), runningAverage.getStandardDeviation(), EPSILON);    runningAverage.removeDatum(2.0, 2.0);    assertEquals(2.0, runningAverage.getAverage(), EPSILON);    assertEquals(Math.sqrt(31.5), runningAverage.getStandardDeviation(), EPSILON);}
0
public void testBoolean() throws Exception
{    DataModel model = getBooleanDataModel();    RecommenderBuilder builder = new RecommenderBuilder() {        @Override        public Recommender buildRecommender(DataModel dataModel) {            return new GenericBooleanPrefItemBasedRecommender(dataModel, new LogLikelihoodSimilarity(dataModel));        }    };    DataModelBuilder dataModelBuilder = new DataModelBuilder() {        @Override        public DataModel buildDataModel(FastByIDMap<PreferenceArray> trainingData) {            return new GenericBooleanPrefDataModel(GenericBooleanPrefDataModel.toDataMap(trainingData));        }    };    RecommenderIRStatsEvaluator evaluator = new GenericRecommenderIRStatsEvaluator();    IRStatistics stats = evaluator.evaluate(builder, dataModelBuilder, model, null, 1, GenericRecommenderIRStatsEvaluator.CHOOSE_THRESHOLD, 1.0);    assertNotNull(stats);    assertEquals(0.666666666, stats.getPrecision(), EPSILON);    assertEquals(0.666666666, stats.getRecall(), EPSILON);    assertEquals(0.666666666, stats.getF1Measure(), EPSILON);    assertEquals(0.666666666, stats.getFNMeasure(2.0), EPSILON);    assertEquals(0.666666666, stats.getNormalizedDiscountedCumulativeGain(), EPSILON);}
0
public Recommender buildRecommender(DataModel dataModel)
{    return new GenericBooleanPrefItemBasedRecommender(dataModel, new LogLikelihoodSimilarity(dataModel));}
0
public DataModel buildDataModel(FastByIDMap<PreferenceArray> trainingData)
{    return new GenericBooleanPrefDataModel(GenericBooleanPrefDataModel.toDataMap(trainingData));}
0
public void testIRStats()
{    IRStatistics stats = new IRStatisticsImpl(0.3, 0.1, 0.2, 0.05, 0.15);    assertEquals(0.3, stats.getPrecision(), EPSILON);    assertEquals(0.1, stats.getRecall(), EPSILON);    assertEquals(0.15, stats.getF1Measure(), EPSILON);    assertEquals(0.11538461538462, stats.getFNMeasure(2.0), EPSILON);    assertEquals(0.05, stats.getNormalizedDiscountedCumulativeGain(), EPSILON);}
0
public static void main(String[] args) throws Exception
{    DataModel model = new FileDataModel(new File(args[0]));    int howMany = 10;    if (args.length > 1) {        howMany = Integer.parseInt(args[1]);    }    System.out.println("Run Items");    ItemSimilarity similarity = new EuclideanDistanceSimilarity(model);        Recommender recommender = new GenericItemBasedRecommender(model, similarity);    for (int i = 0; i < LOOPS; i++) {        LoadStatistics loadStats = LoadEvaluator.runLoad(recommender, howMany);        System.out.println(loadStats);    }    System.out.println("Run Users");    UserSimilarity userSim = new EuclideanDistanceSimilarity(model);    UserNeighborhood neighborhood = new NearestNUserNeighborhood(10, userSim, model);    recommender = new GenericUserBasedRecommender(model, neighborhood, userSim);    for (int i = 0; i < LOOPS; i++) {        LoadStatistics loadStats = LoadEvaluator.runLoad(recommender, howMany);        System.out.println(loadStats);    }}
0
public void testUserID()
{    PreferenceArray prefs = new BooleanItemPreferenceArray(3);    assertEquals(3, prefs.length());    prefs.setItemID(0, 1L);    assertEquals(1L, prefs.getItemID(0));    assertEquals(1L, prefs.getItemID(1));    assertEquals(1L, prefs.getItemID(2));}
0
public void testItemID()
{    PreferenceArray prefs = new BooleanItemPreferenceArray(3);    assertEquals(3, prefs.length());    prefs.setUserID(0, 1L);    prefs.setUserID(1, 2L);    prefs.setUserID(2, 3L);    assertEquals(1L, prefs.getUserID(0));    assertEquals(2L, prefs.getUserID(1));    assertEquals(3L, prefs.getUserID(2));}
0
public void testSetValue()
{    PreferenceArray prefs = new BooleanItemPreferenceArray(3);    assertEquals(3, prefs.length());    assertEquals(1.0f, prefs.getValue(2), EPSILON);    prefs.setValue(0, 1.0f);}
0
public void testHasPref()
{    PreferenceArray prefs = new BooleanItemPreferenceArray(3);    prefs.set(0, new GenericPreference(1L, 3L, 5.0f));    assertTrue(prefs.hasPrefWithItemID(3L));    assertTrue(prefs.hasPrefWithUserID(1L));    assertFalse(prefs.hasPrefWithItemID(2L));    assertFalse(prefs.hasPrefWithUserID(2L));}
0
public void testSort()
{    PreferenceArray prefs = new BooleanItemPreferenceArray(3);    prefs.set(0, new GenericPreference(3L, 1L, 5.0f));    prefs.set(1, new GenericPreference(1L, 1L, 5.0f));    prefs.set(2, new GenericPreference(2L, 1L, 5.0f));    prefs.sortByUser();    assertEquals(1L, prefs.getUserID(0));    assertEquals(2L, prefs.getUserID(1));    assertEquals(3L, prefs.getUserID(2));}
0
public void testClone()
{    BooleanItemPreferenceArray prefs = new BooleanItemPreferenceArray(3);    prefs.set(0, new BooleanPreference(3L, 1L));    prefs.set(1, new BooleanPreference(1L, 1L));    prefs.set(2, new BooleanPreference(2L, 1L));    prefs = prefs.clone();    assertEquals(3L, prefs.getUserID(0));    assertEquals(1L, prefs.getItemID(1));}
0
public void testUserID()
{    PreferenceArray prefs = new BooleanUserPreferenceArray(3);    assertEquals(3, prefs.length());    prefs.setUserID(0, 1L);    assertEquals(1L, prefs.getUserID(0));    assertEquals(1L, prefs.getUserID(1));    assertEquals(1L, prefs.getUserID(2));}
0
public void testItemID()
{    PreferenceArray prefs = new BooleanUserPreferenceArray(3);    assertEquals(3, prefs.length());    prefs.setItemID(0, 1L);    prefs.setItemID(1, 2L);    prefs.setItemID(2, 3L);    assertEquals(1L, prefs.getItemID(0));    assertEquals(2L, prefs.getItemID(1));    assertEquals(3L, prefs.getItemID(2));}
0
public void testSetValue()
{    PreferenceArray prefs = new BooleanUserPreferenceArray(3);    assertEquals(1.0f, prefs.getValue(2), EPSILON);    assertEquals(3, prefs.length());    prefs.setValue(0, 1.0f);}
0
public void testHasPref()
{    PreferenceArray prefs = new BooleanUserPreferenceArray(3);    prefs.set(0, new GenericPreference(1L, 3L, 5.0f));    assertTrue(prefs.hasPrefWithItemID(3L));    assertTrue(prefs.hasPrefWithUserID(1L));    assertFalse(prefs.hasPrefWithItemID(2L));    assertFalse(prefs.hasPrefWithUserID(2L));}
0
public void testSort()
{    PreferenceArray prefs = new BooleanUserPreferenceArray(3);    prefs.set(0, new BooleanPreference(1L, 3L));    prefs.set(1, new BooleanPreference(1L, 1L));    prefs.set(2, new BooleanPreference(1L, 2L));    prefs.sortByItem();    assertEquals(1L, prefs.getItemID(0));    assertEquals(2L, prefs.getItemID(1));    assertEquals(3L, prefs.getItemID(2));}
0
public void testClone()
{    BooleanUserPreferenceArray prefs = new BooleanUserPreferenceArray(3);    prefs.set(0, new BooleanPreference(1L, 3L));    prefs.set(1, new BooleanPreference(1L, 1L));    prefs.set(2, new BooleanPreference(1L, 2L));    prefs = prefs.clone();    assertEquals(3L, prefs.getItemID(0));    assertEquals(1L, prefs.getUserID(1));}
0
public void setUp() throws Exception
{    super.setUp();    testFile = getTestTempFile("test.txt");    writeLines(testFile, DATA);    model = new FileDataModel(testFile);}
0
public void testReadRegexSplittedFile() throws Exception
{    File testFile = getTestTempFile("testRegex.txt");    writeLines(testFile, DATA_SPLITTED_WITH_TWO_SPACES);    FileDataModel model = new FileDataModel(testFile, "\\s+");    assertEquals(model.getItemIDsFromUser(123).size(), 3);    assertEquals(model.getItemIDsFromUser(456).size(), 4);}
0
public void testFile() throws Exception
{    UserSimilarity userSimilarity = new PearsonCorrelationSimilarity(model);    UserNeighborhood neighborhood = new NearestNUserNeighborhood(3, userSimilarity, model);    Recommender recommender = new GenericUserBasedRecommender(model, neighborhood, userSimilarity);    assertEquals(1, recommender.recommend(123, 3).size());    assertEquals(0, recommender.recommend(234, 3).size());    assertEquals(1, recommender.recommend(345, 3).size());        model.refresh(null);}
0
public void testTranspose() throws Exception
{    FileDataModel tModel = new FileDataModel(testFile, true, FileDataModel.DEFAULT_MIN_RELOAD_INTERVAL_MS);    PreferenceArray userPrefs = tModel.getPreferencesFromUser(456);    assertNotNull("user prefs are null and it shouldn't be", userPrefs);    PreferenceArray pref = tModel.getPreferencesForItem(123);    assertNotNull("pref is null and it shouldn't be", pref);    assertEquals("pref Size: " + pref.length() + " is not: " + 3, 3, pref.length());}
0
public void testGetItems() throws Exception
{    LongPrimitiveIterator it = model.getItemIDs();    assertNotNull(it);    assertTrue(it.hasNext());    assertEquals(123, it.nextLong());    assertTrue(it.hasNext());    assertEquals(234, it.nextLong());    assertTrue(it.hasNext());    assertEquals(456, it.nextLong());    assertTrue(it.hasNext());    assertEquals(654, it.nextLong());    assertTrue(it.hasNext());    assertEquals(789, it.nextLong());    assertTrue(it.hasNext());    assertEquals(999, it.nextLong());    assertFalse(it.hasNext());    it.next();}
0
public void testPreferencesForItem() throws Exception
{    PreferenceArray prefs = model.getPreferencesForItem(456);    assertNotNull(prefs);    Preference pref1 = prefs.get(0);    assertEquals(123, pref1.getUserID());    assertEquals(456, pref1.getItemID());    Preference pref2 = prefs.get(1);    assertEquals(456, pref2.getUserID());    assertEquals(456, pref2.getItemID());    assertEquals(2, prefs.length());}
0
public void testGetNumUsers() throws Exception
{    assertEquals(4, model.getNumUsers());}
0
public void testNumUsersPreferring() throws Exception
{    assertEquals(2, model.getNumUsersWithPreferenceFor(456));    assertEquals(0, model.getNumUsersWithPreferenceFor(111));    assertEquals(0, model.getNumUsersWithPreferenceFor(111, 456));    assertEquals(2, model.getNumUsersWithPreferenceFor(123, 234));}
0
public void testRefresh() throws Exception
{    final MutableBoolean initialized = new MutableBoolean(false);    Runnable initializer = new Runnable() {        @Override        public void run() {            try {                model.getNumUsers();                initialized.setValue(true);            } catch (TasteException te) {                        }        }    };    new Thread(initializer).start();        Thread.sleep(1000L);        model.getNumUsers();    assertTrue(initialized.booleanValue());    assertEquals(4, model.getNumUsers());}
0
public void run()
{    try {        model.getNumUsers();        initialized.setValue(true);    } catch (TasteException te) {        }}
0
public void testExplicitRefreshAfterCompleteFileUpdate() throws Exception
{    File file = getTestTempFile("refresh");    writeLines(file, "123,456,3.0");    /* create a FileDataModel that always reloads when the underlying file has changed */    FileDataModel dataModel = new FileDataModel(file, false, 0L);    assertEquals(3.0f, dataModel.getPreferenceValue(123L, 456L), EPSILON);    /* change the underlying file,     * we have to wait at least a second to see the change in the file's lastModified timestamp */    Thread.sleep(2000L);    writeLines(file, "123,456,5.0");    dataModel.refresh(null);    assertEquals(5.0f, dataModel.getPreferenceValue(123L, 456L), EPSILON);}
0
public void testToString()
{    assertFalse(model.toString().isEmpty());}
0
public void testEmptyFile() throws Exception
{    File file = getTestTempFile("empty");        writeLines(file);    new FileDataModel(file);}
0
public void setUp() throws Exception
{    super.setUp();    testFile = getTestTempFile("test.txt");    writeLines(testFile, STRING_IDS);}
0
public void testLoadFromFile() throws Exception
{    IDMigrator migrator = new FileIDMigrator(testFile);    long dogAsLong = migrator.toLongID("dog");    long cowAsLong = migrator.toLongID("cow");    long donkeyAsLong = migrator.toLongID("donkey");    assertEquals("dog", migrator.toStringID(dogAsLong));    assertEquals("cow", migrator.toStringID(cowAsLong));    assertNull(migrator.toStringID(donkeyAsLong));}
0
public void testNoRefreshAfterFileUpdate() throws Exception
{    IDMigrator migrator = new FileIDMigrator(testFile, 0L);    /* call a method to make sure the original file is loaded */    long dogAsLong = migrator.toLongID("dog");    migrator.toStringID(dogAsLong);    /* change the underlying file,     * we have to wait at least a second to see the change in the file's lastModified timestamp */    Thread.sleep(2000L);    writeLines(testFile, UPDATED_STRING_IDS);    /* we shouldn't see any changes in the data as we have not yet refreshed */    long cowAsLong = migrator.toLongID("cow");    long donkeyAsLong = migrator.toLongID("donkey");    assertEquals("dog", migrator.toStringID(dogAsLong));    assertEquals("cow", migrator.toStringID(cowAsLong));    assertNull(migrator.toStringID(donkeyAsLong));}
0
public void testRefreshAfterFileUpdate() throws Exception
{    IDMigrator migrator = new FileIDMigrator(testFile, 0L);    /* call a method to make sure the original file is loaded */    long dogAsLong = migrator.toLongID("dog");    migrator.toStringID(dogAsLong);    /* change the underlying file,     * we have to wait at least a second to see the change in the file's lastModified timestamp */    Thread.sleep(2000L);    writeLines(testFile, UPDATED_STRING_IDS);    migrator.refresh(null);    long cowAsLong = migrator.toLongID("cow");    long donkeyAsLong = migrator.toLongID("donkey");    assertEquals("dog", migrator.toStringID(dogAsLong));    assertEquals("cow", migrator.toStringID(cowAsLong));    assertEquals("donkey", migrator.toStringID(donkeyAsLong));}
0
public void testSerialization() throws Exception
{    GenericDataModel model = (GenericDataModel) getDataModel();    ByteArrayOutputStream baos = new ByteArrayOutputStream();    ObjectOutputStream out = new ObjectOutputStream(baos);    out.writeObject(model);    ByteArrayInputStream bais = new ByteArrayInputStream(baos.toByteArray());    ObjectInputStream in = new ObjectInputStream(bais);    GenericDataModel newModel = (GenericDataModel) in.readObject();    assertEquals(model.getNumItems(), newModel.getNumItems());    assertEquals(model.getNumUsers(), newModel.getNumUsers());    assertEquals(model.getPreferencesFromUser(1L), newModel.getPreferencesFromUser(1L));    assertEquals(model.getPreferencesForItem(1L), newModel.getPreferencesForItem(1L));    assertEquals(model.getRawUserData(), newModel.getRawUserData());}
0
public void testUserID()
{    PreferenceArray prefs = new GenericItemPreferenceArray(3);    assertEquals(3, prefs.length());    prefs.setItemID(0, 1L);    assertEquals(1L, prefs.getItemID(0));    assertEquals(1L, prefs.getItemID(1));    assertEquals(1L, prefs.getItemID(2));}
0
public void testItemID()
{    PreferenceArray prefs = new GenericItemPreferenceArray(3);    assertEquals(3, prefs.length());    prefs.setUserID(0, 1L);    prefs.setUserID(1, 2L);    prefs.setUserID(2, 3L);    assertEquals(1L, prefs.getUserID(0));    assertEquals(2L, prefs.getUserID(1));    assertEquals(3L, prefs.getUserID(2));}
0
public void testSetValue()
{    PreferenceArray prefs = new GenericItemPreferenceArray(3);    assertEquals(3, prefs.length());    prefs.setValue(0, 1.0f);    prefs.setValue(1, 2.0f);    prefs.setValue(2, 3.0f);    assertEquals(1.0f, prefs.getValue(0), EPSILON);    assertEquals(2.0f, prefs.getValue(1), EPSILON);    assertEquals(3.0f, prefs.getValue(2), EPSILON);}
0
public void testHasPref()
{    PreferenceArray prefs = new GenericItemPreferenceArray(3);    prefs.set(0, new GenericPreference(1L, 3L, 5.0f));    assertTrue(prefs.hasPrefWithItemID(3L));    assertTrue(prefs.hasPrefWithUserID(1L));    assertFalse(prefs.hasPrefWithItemID(2L));    assertFalse(prefs.hasPrefWithUserID(2L));}
0
public void testSort()
{    PreferenceArray prefs = new GenericItemPreferenceArray(3);    prefs.set(0, new GenericPreference(3L, 1L, 5.0f));    prefs.set(1, new GenericPreference(1L, 1L, 5.0f));    prefs.set(2, new GenericPreference(2L, 1L, 5.0f));    prefs.sortByUser();    assertEquals(1L, prefs.getUserID(0));    assertEquals(2L, prefs.getUserID(1));    assertEquals(3L, prefs.getUserID(2));}
0
public void testSortValue()
{    PreferenceArray prefs = new GenericItemPreferenceArray(3);    prefs.set(0, new GenericPreference(3L, 1L, 5.0f));    prefs.set(1, new GenericPreference(1L, 1L, 4.0f));    prefs.set(2, new GenericPreference(2L, 1L, 3.0f));    prefs.sortByValue();    assertEquals(2L, prefs.getUserID(0));    assertEquals(1L, prefs.getUserID(1));    assertEquals(3L, prefs.getUserID(2));    prefs.sortByValueReversed();    assertEquals(3L, prefs.getUserID(0));    assertEquals(1L, prefs.getUserID(1));    assertEquals(2L, prefs.getUserID(2));}
0
public void testClone()
{    GenericItemPreferenceArray prefs = new GenericItemPreferenceArray(3);    prefs.set(0, new GenericPreference(3L, 1L, 5.0f));    prefs.set(1, new GenericPreference(1L, 1L, 4.0f));    prefs.set(2, new GenericPreference(2L, 1L, 3.0f));    prefs = prefs.clone();    assertEquals(3L, prefs.getUserID(0));    assertEquals(1L, prefs.getItemID(1));    assertEquals(3.0f, prefs.getValue(2), EPSILON);}
0
public void testUserID()
{    PreferenceArray prefs = new GenericUserPreferenceArray(3);    assertEquals(3, prefs.length());    prefs.setUserID(0, 1L);    assertEquals(1L, prefs.getUserID(0));    assertEquals(1L, prefs.getUserID(1));    assertEquals(1L, prefs.getUserID(2));}
0
public void testItemID()
{    PreferenceArray prefs = new GenericUserPreferenceArray(3);    assertEquals(3, prefs.length());    prefs.setItemID(0, 1L);    prefs.setItemID(1, 2L);    prefs.setItemID(2, 3L);    assertEquals(1L, prefs.getItemID(0));    assertEquals(2L, prefs.getItemID(1));    assertEquals(3L, prefs.getItemID(2));}
0
public void testSetValue()
{    PreferenceArray prefs = new GenericUserPreferenceArray(3);    assertEquals(3, prefs.length());    prefs.setValue(0, 1.0f);    prefs.setValue(1, 2.0f);    prefs.setValue(2, 3.0f);    assertEquals(1.0f, prefs.getValue(0), EPSILON);    assertEquals(2.0f, prefs.getValue(1), EPSILON);    assertEquals(3.0f, prefs.getValue(2), EPSILON);}
0
public void testHasPref()
{    PreferenceArray prefs = new GenericUserPreferenceArray(3);    prefs.set(0, new GenericPreference(1L, 3L, 5.0f));    assertTrue(prefs.hasPrefWithItemID(3L));    assertTrue(prefs.hasPrefWithUserID(1L));    assertFalse(prefs.hasPrefWithItemID(2L));    assertFalse(prefs.hasPrefWithUserID(2L));}
0
public void testSort()
{    PreferenceArray prefs = new GenericUserPreferenceArray(3);    prefs.set(0, new GenericPreference(1L, 3L, 5.0f));    prefs.set(1, new GenericPreference(1L, 1L, 5.0f));    prefs.set(2, new GenericPreference(1L, 2L, 5.0f));    prefs.sortByItem();    assertEquals(1L, prefs.getItemID(0));    assertEquals(2L, prefs.getItemID(1));    assertEquals(3L, prefs.getItemID(2));}
0
public void testSortValue()
{    PreferenceArray prefs = new GenericUserPreferenceArray(3);    prefs.set(0, new GenericPreference(1L, 3L, 5.0f));    prefs.set(1, new GenericPreference(1L, 1L, 4.0f));    prefs.set(2, new GenericPreference(1L, 2L, 3.0f));    prefs.sortByValue();    assertEquals(2L, prefs.getItemID(0));    assertEquals(1L, prefs.getItemID(1));    assertEquals(3L, prefs.getItemID(2));    prefs.sortByValueReversed();    assertEquals(3L, prefs.getItemID(0));    assertEquals(1L, prefs.getItemID(1));    assertEquals(2L, prefs.getItemID(2));}
0
public void testClone()
{    GenericUserPreferenceArray prefs = new GenericUserPreferenceArray(3);    prefs.set(0, new GenericPreference(1L, 3L, 5.0f));    prefs.set(1, new GenericPreference(1L, 1L, 4.0f));    prefs.set(2, new GenericPreference(1L, 2L, 3.0f));    prefs = prefs.clone();    assertEquals(3L, prefs.getItemID(0));    assertEquals(1L, prefs.getUserID(1));    assertEquals(3.0f, prefs.getValue(2), EPSILON);}
0
public void testToLong()
{    IDMigrator migrator = new MemoryIDMigrator();    long id = migrator.toLongID(DUMMY_STRING);    assertEquals(DUMMY_ID, id);}
0
public void testStore() throws Exception
{    UpdatableIDMigrator migrator = new MemoryIDMigrator();    long id = migrator.toLongID(DUMMY_STRING);    assertNull(migrator.toStringID(id));    migrator.storeMapping(id, DUMMY_STRING);    assertEquals(DUMMY_STRING, migrator.toStringID(id));}
0
public void testInitialize() throws Exception
{    UpdatableIDMigrator migrator = new MemoryIDMigrator();    long id = migrator.toLongID(DUMMY_STRING);    assertNull(migrator.toStringID(id));    migrator.initialize(Collections.singleton(DUMMY_STRING));    assertEquals(DUMMY_STRING, migrator.toStringID(id));}
0
private static PlusAnonymousConcurrentUserDataModel getTestableWithoutDelegateData(int maxConcurrentUsers)
{    FastByIDMap<PreferenceArray> delegatePreferences = new FastByIDMap<>();    return new PlusAnonymousConcurrentUserDataModel(new GenericDataModel(delegatePreferences), maxConcurrentUsers);}
0
private static PlusAnonymousConcurrentUserDataModel getTestableWithDelegateData(int maxConcurrentUsers, FastByIDMap<PreferenceArray> delegatePreferences)
{    return new PlusAnonymousConcurrentUserDataModel(new GenericDataModel(delegatePreferences), maxConcurrentUsers);}
0
public void testTakeFirstAvailableUser()
{    PlusAnonymousConcurrentUserDataModel instance = getTestableWithoutDelegateData(10);    Long expResult = PlusAnonymousUserDataModel.TEMP_USER_ID;    Long result = instance.takeAvailableUser();    assertEquals(expResult, result);}
0
public void testTakeNextAvailableUser()
{    PlusAnonymousConcurrentUserDataModel instance = getTestableWithoutDelegateData(10);        instance.takeAvailableUser();    Long result = instance.takeAvailableUser();    Long expResult = PlusAnonymousUserDataModel.TEMP_USER_ID + 1;    assertEquals(expResult, result);}
0
public void testTakeUnavailableUser()
{    PlusAnonymousConcurrentUserDataModel instance = getTestableWithoutDelegateData(1);        instance.takeAvailableUser();        assertNull(instance.takeAvailableUser());}
0
public void testReleaseValidUser()
{    PlusAnonymousConcurrentUserDataModel instance = getTestableWithoutDelegateData(10);    Long takenUserID = instance.takeAvailableUser();    assertTrue(instance.releaseUser(takenUserID));}
0
public void testReleaseInvalidUser()
{    PlusAnonymousConcurrentUserDataModel instance = getTestableWithoutDelegateData(10);    assertFalse(instance.releaseUser(Long.MAX_VALUE));}
0
public void testReleasePreviouslyReleasedUser()
{    PlusAnonymousConcurrentUserDataModel instance = getTestableWithoutDelegateData(10);    Long takenUserID = instance.takeAvailableUser();    assertTrue(instance.releaseUser(takenUserID));    assertFalse(instance.releaseUser(takenUserID));}
0
public void testSetAndGetTempPreferences() throws TasteException
{    PlusAnonymousConcurrentUserDataModel instance = getTestableWithoutDelegateData(10);    Long anonymousUserID = instance.takeAvailableUser();    PreferenceArray tempPrefs = new GenericUserPreferenceArray(1);    tempPrefs.setUserID(0, anonymousUserID);    tempPrefs.setItemID(0, 1);    instance.setTempPrefs(tempPrefs, anonymousUserID);    assertEquals(tempPrefs, instance.getPreferencesFromUser(anonymousUserID));    instance.releaseUser(anonymousUserID);}
0
public void testSetMultipleTempPreferences() throws TasteException
{    PlusAnonymousConcurrentUserDataModel instance = getTestableWithoutDelegateData(10);    Long anonymousUserID1 = instance.takeAvailableUser();    Long anonymousUserID2 = instance.takeAvailableUser();    PreferenceArray tempPrefs1 = new GenericUserPreferenceArray(1);    tempPrefs1.setUserID(0, anonymousUserID1);    tempPrefs1.setItemID(0, 1);    PreferenceArray tempPrefs2 = new GenericUserPreferenceArray(2);    tempPrefs2.setUserID(0, anonymousUserID2);    tempPrefs2.setItemID(0, 2);    tempPrefs2.setUserID(1, anonymousUserID2);    tempPrefs2.setItemID(1, 3);    instance.setTempPrefs(tempPrefs1, anonymousUserID1);    instance.setTempPrefs(tempPrefs2, anonymousUserID2);    assertEquals(tempPrefs1, instance.getPreferencesFromUser(anonymousUserID1));    assertEquals(tempPrefs2, instance.getPreferencesFromUser(anonymousUserID2));}
0
public void testGetNumUsersWithDelegateUsersOnly() throws TasteException
{    PreferenceArray prefs = new GenericUserPreferenceArray(1);    long sampleUserID = 1;    prefs.setUserID(0, sampleUserID);    long sampleItemID = 11;    prefs.setItemID(0, sampleItemID);    FastByIDMap<PreferenceArray> delegatePreferences = new FastByIDMap<>();    delegatePreferences.put(sampleUserID, prefs);    PlusAnonymousConcurrentUserDataModel instance = getTestableWithDelegateData(10, delegatePreferences);    assertEquals(1, instance.getNumUsers());}
0
public void testGetNumAnonymousUsers() throws TasteException
{    PlusAnonymousConcurrentUserDataModel instance = getTestableWithoutDelegateData(10);    Long anonymousUserID1 = instance.takeAvailableUser();    PreferenceArray tempPrefs1 = new GenericUserPreferenceArray(1);    tempPrefs1.setUserID(0, anonymousUserID1);    tempPrefs1.setItemID(0, 1);    instance.setTempPrefs(tempPrefs1, anonymousUserID1);        assertEquals(0, instance.getNumUsers());}
0
public void testGetPreferenceValue() throws TasteException
{    PlusAnonymousConcurrentUserDataModel instance = getTestableWithoutDelegateData(10);    Long anonymousUserID = instance.takeAvailableUser();    PreferenceArray tempPrefs = new GenericUserPreferenceArray(1);    tempPrefs.setUserID(0, anonymousUserID);    long sampleItemID = 1;    tempPrefs.setItemID(0, sampleItemID);    tempPrefs.setValue(0, Float.MAX_VALUE);    instance.setTempPrefs(tempPrefs, anonymousUserID);    assertEquals(Float.MAX_VALUE, instance.getPreferenceValue(anonymousUserID, sampleItemID), EPSILON);}
0
public void testGetPreferencesForNonAnonymousUser() throws TasteException
{    PreferenceArray prefs = new GenericUserPreferenceArray(1);    long sampleUserID = 1;    prefs.setUserID(0, sampleUserID);    long sampleItemID = 11;    prefs.setItemID(0, sampleItemID);    FastByIDMap<PreferenceArray> delegatePreferences = new FastByIDMap<>();    delegatePreferences.put(sampleUserID, prefs);    PlusAnonymousConcurrentUserDataModel instance = getTestableWithDelegateData(10, delegatePreferences);    assertEquals(prefs, instance.getPreferencesFromUser(sampleUserID));}
0
public void testGetPreferencesForNonExistingUser() throws TasteException
{    PlusAnonymousConcurrentUserDataModel instance = getTestableWithoutDelegateData(10);        instance.getPreferencesFromUser(1);}
0
public void testGetUserIDs() throws TasteException
{    PreferenceArray prefs = new GenericUserPreferenceArray(1);    long sampleUserID = 1;    prefs.setUserID(0, sampleUserID);    long sampleItemID = 11;    prefs.setItemID(0, sampleItemID);    FastByIDMap<PreferenceArray> delegatePreferences = new FastByIDMap<>();    delegatePreferences.put(sampleUserID, prefs);    PlusAnonymousConcurrentUserDataModel instance = getTestableWithDelegateData(10, delegatePreferences);    Long anonymousUserID = instance.takeAvailableUser();    PreferenceArray tempPrefs = new GenericUserPreferenceArray(1);    tempPrefs.setUserID(0, anonymousUserID);    tempPrefs.setItemID(0, 22);    instance.setTempPrefs(tempPrefs, anonymousUserID);    Iterator<Long> userIDs = instance.getUserIDs();    assertSame(sampleUserID, userIDs.next());    assertFalse(userIDs.hasNext());}
0
public void testGetPreferencesForItem() throws TasteException
{    PreferenceArray prefs = new GenericUserPreferenceArray(2);    long sampleUserID = 4;    prefs.setUserID(0, sampleUserID);    long sampleItemID = 11;    prefs.setItemID(0, sampleItemID);    prefs.setUserID(1, sampleUserID);    long sampleItemID2 = 22;    prefs.setItemID(1, sampleItemID2);    FastByIDMap<PreferenceArray> delegatePreferences = new FastByIDMap<>();    delegatePreferences.put(sampleUserID, prefs);    PlusAnonymousConcurrentUserDataModel instance = getTestableWithDelegateData(10, delegatePreferences);    Long anonymousUserID = instance.takeAvailableUser();    PreferenceArray tempPrefs = new GenericUserPreferenceArray(2);    tempPrefs.setUserID(0, anonymousUserID);    tempPrefs.setItemID(0, sampleItemID);    tempPrefs.setUserID(1, anonymousUserID);    long sampleItemID3 = 33;    tempPrefs.setItemID(1, sampleItemID3);    instance.setTempPrefs(tempPrefs, anonymousUserID);    assertEquals(sampleUserID, instance.getPreferencesForItem(sampleItemID).get(0).getUserID());    assertEquals(2, instance.getPreferencesForItem(sampleItemID).length());    assertEquals(1, instance.getPreferencesForItem(sampleItemID2).length());    assertEquals(1, instance.getPreferencesForItem(sampleItemID3).length());    assertEquals(2, instance.getNumUsersWithPreferenceFor(sampleItemID));    assertEquals(1, instance.getNumUsersWithPreferenceFor(sampleItemID, sampleItemID2));    assertEquals(1, instance.getNumUsersWithPreferenceFor(sampleItemID, sampleItemID3));}
0
public double userSimilarity(long userID1, long userID2) throws TasteException
{    DataModel dataModel = getDataModel();    return 1.0 / (1.0 + Math.abs(dataModel.getPreferencesFromUser(userID1).get(0).getValue() - dataModel.getPreferencesFromUser(userID2).get(0).getValue()));}
0
public double itemSimilarity(long itemID1, long itemID2)
{        return 1.0 / (1.0 + Math.abs(itemID1 - itemID2));}
0
public double[] itemSimilarities(long itemID1, long[] itemID2s)
{    int length = itemID2s.length;    double[] result = new double[length];    for (int i = 0; i < length; i++) {        result[i] = itemSimilarity(itemID1, itemID2s[i]);    }    return result;}
0
public void setPreferenceInferrer(PreferenceInferrer inferrer)
{    throw new UnsupportedOperationException();}
0
public void refresh(Collection<Refreshable> alreadyRefreshed)
{}
0
public void testNeighborhood() throws Exception
{    DataModel dataModel = getDataModel();    long[] neighborhood = new NearestNUserNeighborhood(1, new DummySimilarity(dataModel), dataModel).getUserNeighborhood(1);    assertNotNull(neighborhood);    assertEquals(1, neighborhood.length);    assertTrue(arrayContains(neighborhood, 2));    long[] neighborhood2 = new NearestNUserNeighborhood(2, new DummySimilarity(dataModel), dataModel).getUserNeighborhood(2);    assertNotNull(neighborhood2);    assertEquals(2, neighborhood2.length);    assertTrue(arrayContains(neighborhood2, 1));    assertTrue(arrayContains(neighborhood2, 3));    long[] neighborhood3 = new NearestNUserNeighborhood(4, new DummySimilarity(dataModel), dataModel).getUserNeighborhood(4);    assertNotNull(neighborhood3);    assertEquals(3, neighborhood3.length);    assertTrue(arrayContains(neighborhood3, 1));    assertTrue(arrayContains(neighborhood3, 2));    assertTrue(arrayContains(neighborhood3, 3));}
0
public void testNeighborhood() throws Exception
{    DataModel dataModel = getDataModel();    long[] neighborhood = new ThresholdUserNeighborhood(1.0, new DummySimilarity(dataModel), dataModel).getUserNeighborhood(1);    assertNotNull(neighborhood);    assertEquals(0, neighborhood.length);    long[] neighborhood2 = new ThresholdUserNeighborhood(0.8, new DummySimilarity(dataModel), dataModel).getUserNeighborhood(1);    assertNotNull(neighborhood2);    assertEquals(1, neighborhood2.length);    assertTrue(arrayContains(neighborhood2, 2));    long[] neighborhood3 = new ThresholdUserNeighborhood(0.6, new DummySimilarity(dataModel), dataModel).getUserNeighborhood(2);    assertNotNull(neighborhood3);    assertEquals(3, neighborhood3.length);    assertTrue(arrayContains(neighborhood3, 1));    assertTrue(arrayContains(neighborhood3, 3));    assertTrue(arrayContains(neighborhood3, 4));}
0
public void testStrategy() throws TasteException
{    FastIDSet allItemIDs = new FastIDSet();    allItemIDs.addAll(new long[] { 1L, 2L, 3L });    FastIDSet preferredItemIDs = new FastIDSet(1);    preferredItemIDs.add(2L);    DataModel dataModel = EasyMock.createMock(DataModel.class);    EasyMock.expect(dataModel.getNumItems()).andReturn(3);    EasyMock.expect(dataModel.getItemIDs()).andReturn(allItemIDs.iterator());    PreferenceArray prefArrayOfUser123 = new GenericUserPreferenceArray(Collections.singletonList(new GenericPreference(123L, 2L, 1.0f)));    CandidateItemsStrategy strategy = new AllUnknownItemsCandidateItemsStrategy();    EasyMock.replay(dataModel);    FastIDSet candidateItems = strategy.getCandidateItems(123L, prefArrayOfUser123, dataModel, false);    assertEquals(2, candidateItems.size());    assertTrue(candidateItems.contains(1L));    assertTrue(candidateItems.contains(3L));    EasyMock.verify(dataModel);}
0
public void testRecommender() throws Exception
{    MutableInt recommendCount = new MutableInt();    Recommender mockRecommender = new MockRecommender(recommendCount);    Recommender cachingRecommender = new CachingRecommender(mockRecommender);    cachingRecommender.recommend(1, 1);    assertEquals(1, recommendCount.intValue());    cachingRecommender.recommend(2, 1);    assertEquals(2, recommendCount.intValue());    cachingRecommender.recommend(1, 1);    assertEquals(2, recommendCount.intValue());    cachingRecommender.recommend(2, 1);    assertEquals(2, recommendCount.intValue());    cachingRecommender.refresh(null);    cachingRecommender.recommend(1, 1);    assertEquals(3, recommendCount.intValue());    cachingRecommender.recommend(2, 1);    assertEquals(4, recommendCount.intValue());    cachingRecommender.recommend(3, 1);    assertEquals(5, recommendCount.intValue());        IDRescorer rescorer = NullRescorer.getItemInstance();    cachingRecommender.refresh(null);    cachingRecommender.recommend(1, 1, rescorer);    assertEquals(6, recommendCount.intValue());    cachingRecommender.recommend(2, 1, rescorer);    assertEquals(7, recommendCount.intValue());    cachingRecommender.recommend(1, 1, rescorer);    assertEquals(7, recommendCount.intValue());    cachingRecommender.recommend(2, 1, rescorer);    assertEquals(7, recommendCount.intValue());        cachingRecommender.recommend(1, 1, null);    assertEquals(8, recommendCount.intValue());    cachingRecommender.recommend(2, 1, null);    assertEquals(9, recommendCount.intValue());    cachingRecommender.refresh(null);    cachingRecommender.estimatePreference(1, 1);    assertEquals(10, recommendCount.intValue());    cachingRecommender.estimatePreference(1, 2);    assertEquals(11, recommendCount.intValue());    cachingRecommender.estimatePreference(1, 2);    assertEquals(11, recommendCount.intValue());}
0
public void testRecommender() throws Exception
{    Recommender recommender = buildRecommender();    List<RecommendedItem> recommended = recommender.recommend(1, 1);    assertNotNull(recommended);    assertEquals(1, recommended.size());    RecommendedItem firstRecommended = recommended.get(0);    assertEquals(2, firstRecommended.getItemID());    assertEquals(0.1f, firstRecommended.getValue(), EPSILON);    recommender.refresh(null);    recommended = recommender.recommend(1, 1);    firstRecommended = recommended.get(0);    assertEquals(2, firstRecommended.getItemID());    assertEquals(0.1f, firstRecommended.getValue(), EPSILON);}
0
public void testHowMany() throws Exception
{    DataModel dataModel = getDataModel(new long[] { 1, 2, 3, 4, 5 }, new Double[][] { { 0.1, 0.2 }, { 0.2, 0.3, 0.3, 0.6 }, { 0.4, 0.4, 0.5, 0.9 }, { 0.1, 0.4, 0.5, 0.8, 0.9, 1.0 }, { 0.2, 0.3, 0.6, 0.7, 0.1, 0.2 } });    Collection<GenericItemSimilarity.ItemItemSimilarity> similarities = Lists.newArrayList();    for (int i = 0; i < 6; i++) {        for (int j = i + 1; j < 6; j++) {            similarities.add(new GenericItemSimilarity.ItemItemSimilarity(i, j, 1.0 / (1.0 + i + j)));        }    }    ItemSimilarity similarity = new GenericItemSimilarity(similarities);    Recommender recommender = new GenericItemBasedRecommender(dataModel, similarity);    List<RecommendedItem> fewRecommended = recommender.recommend(1, 2);    List<RecommendedItem> moreRecommended = recommender.recommend(1, 4);    for (int i = 0; i < fewRecommended.size(); i++) {        assertEquals(fewRecommended.get(i).getItemID(), moreRecommended.get(i).getItemID());    }    recommender.refresh(null);    for (int i = 0; i < fewRecommended.size(); i++) {        assertEquals(fewRecommended.get(i).getItemID(), moreRecommended.get(i).getItemID());    }}
0
public void testRescorer() throws Exception
{    DataModel dataModel = getDataModel(new long[] { 1, 2, 3 }, new Double[][] { { 0.1, 0.2 }, { 0.2, 0.3, 0.3, 0.6 }, { 0.4, 0.4, 0.5, 0.9 } });    Collection<GenericItemSimilarity.ItemItemSimilarity> similarities = Lists.newArrayList();    similarities.add(new GenericItemSimilarity.ItemItemSimilarity(0, 1, 1.0));    similarities.add(new GenericItemSimilarity.ItemItemSimilarity(0, 2, 0.5));    similarities.add(new GenericItemSimilarity.ItemItemSimilarity(0, 3, 0.2));    similarities.add(new GenericItemSimilarity.ItemItemSimilarity(1, 2, 0.7));    similarities.add(new GenericItemSimilarity.ItemItemSimilarity(1, 3, 0.5));    similarities.add(new GenericItemSimilarity.ItemItemSimilarity(2, 3, 0.9));    ItemSimilarity similarity = new GenericItemSimilarity(similarities);    Recommender recommender = new GenericItemBasedRecommender(dataModel, similarity);    List<RecommendedItem> originalRecommended = recommender.recommend(1, 2);    List<RecommendedItem> rescoredRecommended = recommender.recommend(1, 2, new ReversingRescorer<Long>());    assertNotNull(originalRecommended);    assertNotNull(rescoredRecommended);    assertEquals(2, originalRecommended.size());    assertEquals(2, rescoredRecommended.size());    assertEquals(originalRecommended.get(0).getItemID(), rescoredRecommended.get(1).getItemID());    assertEquals(originalRecommended.get(1).getItemID(), rescoredRecommended.get(0).getItemID());}
0
public void testIncludeKnownItems() throws Exception
{    DataModel dataModel = getDataModel(new long[] { 1, 2, 3 }, new Double[][] { { 0.1, 0.2 }, { 0.2, 0.3, 0.3, 0.6 }, { 0.4, 0.4, 0.5, 0.9 } });    Collection<GenericItemSimilarity.ItemItemSimilarity> similarities = Lists.newArrayList();    similarities.add(new GenericItemSimilarity.ItemItemSimilarity(0, 1, 0.8));    similarities.add(new GenericItemSimilarity.ItemItemSimilarity(0, 2, 0.5));    similarities.add(new GenericItemSimilarity.ItemItemSimilarity(0, 3, 0.2));    similarities.add(new GenericItemSimilarity.ItemItemSimilarity(1, 2, 0.7));    similarities.add(new GenericItemSimilarity.ItemItemSimilarity(1, 3, 0.5));    similarities.add(new GenericItemSimilarity.ItemItemSimilarity(2, 3, 0.9));    ItemSimilarity similarity = new GenericItemSimilarity(similarities);    Recommender recommender = new GenericItemBasedRecommender(dataModel, similarity);    List<RecommendedItem> originalRecommended = recommender.recommend(1, 4, null, true);    List<RecommendedItem> rescoredRecommended = recommender.recommend(1, 4, new ReversingRescorer<Long>(), true);    assertNotNull(originalRecommended);    assertNotNull(rescoredRecommended);    assertEquals(4, originalRecommended.size());    assertEquals(4, rescoredRecommended.size());    assertEquals(originalRecommended.get(0).getItemID(), rescoredRecommended.get(3).getItemID());    assertEquals(originalRecommended.get(3).getItemID(), rescoredRecommended.get(0).getItemID());}
0
public void testEstimatePref() throws Exception
{    Recommender recommender = buildRecommender();    assertEquals(0.1f, recommender.estimatePreference(1, 2), EPSILON);}
0
public void testBestRating() throws Exception
{    Recommender recommender = buildRecommender();    List<RecommendedItem> recommended = recommender.recommend(1, 1);    assertNotNull(recommended);    assertEquals(1, recommended.size());    RecommendedItem firstRecommended = recommended.get(0);        assertEquals(2, firstRecommended.getItemID());    assertEquals(0.1f, firstRecommended.getValue(), EPSILON);}
0
public void testMostSimilar() throws Exception
{    ItemBasedRecommender recommender = buildRecommender();    List<RecommendedItem> similar = recommender.mostSimilarItems(0, 2);    assertNotNull(similar);    assertEquals(2, similar.size());    RecommendedItem first = similar.get(0);    RecommendedItem second = similar.get(1);    assertEquals(1, first.getItemID());    assertEquals(1.0f, first.getValue(), EPSILON);    assertEquals(2, second.getItemID());    assertEquals(0.5f, second.getValue(), EPSILON);}
0
public void testMostSimilarToMultiple() throws Exception
{    ItemBasedRecommender recommender = buildRecommender2();    List<RecommendedItem> similar = recommender.mostSimilarItems(new long[] { 0, 1 }, 2);    assertNotNull(similar);    assertEquals(2, similar.size());    RecommendedItem first = similar.get(0);    RecommendedItem second = similar.get(1);    assertEquals(2, first.getItemID());    assertEquals(0.85f, first.getValue(), EPSILON);    assertEquals(3, second.getItemID());    assertEquals(-0.3f, second.getValue(), EPSILON);}
0
public void testMostSimilarToMultipleExcludeIfNotSimilarToAll() throws Exception
{    ItemBasedRecommender recommender = buildRecommender2();    List<RecommendedItem> similar = recommender.mostSimilarItems(new long[] { 3, 4 }, 2);    assertNotNull(similar);    assertEquals(1, similar.size());    RecommendedItem first = similar.get(0);    assertEquals(0, first.getItemID());    assertEquals(0.2f, first.getValue(), EPSILON);}
0
public void testMostSimilarToMultipleDontExcludeIfNotSimilarToAll() throws Exception
{    ItemBasedRecommender recommender = buildRecommender2();    List<RecommendedItem> similar = recommender.mostSimilarItems(new long[] { 1, 2, 4 }, 10, false);    assertNotNull(similar);    assertEquals(2, similar.size());    RecommendedItem first = similar.get(0);    RecommendedItem second = similar.get(1);    assertEquals(0, first.getItemID());    assertEquals(0.933333333f, first.getValue(), EPSILON);    assertEquals(3, second.getItemID());    assertEquals(-0.2f, second.getValue(), EPSILON);}
0
public void testRecommendedBecause() throws Exception
{    ItemBasedRecommender recommender = buildRecommender2();    List<RecommendedItem> recommendedBecause = recommender.recommendedBecause(1, 4, 3);    assertNotNull(recommendedBecause);    assertEquals(3, recommendedBecause.size());    RecommendedItem first = recommendedBecause.get(0);    RecommendedItem second = recommendedBecause.get(1);    RecommendedItem third = recommendedBecause.get(2);    assertEquals(2, first.getItemID());    assertEquals(0.99f, first.getValue(), EPSILON);    assertEquals(3, second.getItemID());    assertEquals(0.4f, second.getValue(), EPSILON);    assertEquals(0, third.getItemID());    assertEquals(0.2f, third.getValue(), EPSILON);}
0
private static ItemBasedRecommender buildRecommender()
{    DataModel dataModel = getDataModel();    Collection<GenericItemSimilarity.ItemItemSimilarity> similarities = Lists.newArrayList();    similarities.add(new GenericItemSimilarity.ItemItemSimilarity(0, 1, 1.0));    similarities.add(new GenericItemSimilarity.ItemItemSimilarity(0, 2, 0.5));    similarities.add(new GenericItemSimilarity.ItemItemSimilarity(1, 2, 0.0));    ItemSimilarity similarity = new GenericItemSimilarity(similarities);    return new GenericItemBasedRecommender(dataModel, similarity);}
0
private static ItemBasedRecommender buildRecommender2()
{    DataModel dataModel = getDataModel(new long[] { 1, 2, 3, 4 }, new Double[][] { { 0.1, 0.3, 0.9, 0.8 }, { 0.2, 0.3, 0.3, 0.4 }, { 0.4, 0.3, 0.5, 0.1, 0.1 }, { 0.7, 0.3, 0.8, 0.5, 0.6 } });    Collection<GenericItemSimilarity.ItemItemSimilarity> similarities = Lists.newArrayList();    similarities.add(new GenericItemSimilarity.ItemItemSimilarity(0, 1, 1.0));    similarities.add(new GenericItemSimilarity.ItemItemSimilarity(0, 2, 0.8));    similarities.add(new GenericItemSimilarity.ItemItemSimilarity(0, 3, -0.6));    similarities.add(new GenericItemSimilarity.ItemItemSimilarity(0, 4, 1.0));    similarities.add(new GenericItemSimilarity.ItemItemSimilarity(1, 2, 0.9));    similarities.add(new GenericItemSimilarity.ItemItemSimilarity(1, 3, 0.0));    similarities.add(new GenericItemSimilarity.ItemItemSimilarity(1, 1, 1.0));    similarities.add(new GenericItemSimilarity.ItemItemSimilarity(2, 3, -0.1));    similarities.add(new GenericItemSimilarity.ItemItemSimilarity(2, 4, 0.1));    similarities.add(new GenericItemSimilarity.ItemItemSimilarity(3, 4, -0.5));    ItemSimilarity similarity = new GenericItemSimilarity(similarities);    return new GenericItemBasedRecommender(dataModel, similarity);}
0
public void preferencesFetchedOnlyOnce() throws Exception
{    DataModel dataModel = EasyMock.createMock(DataModel.class);    ItemSimilarity itemSimilarity = EasyMock.createMock(ItemSimilarity.class);    CandidateItemsStrategy candidateItemsStrategy = EasyMock.createMock(CandidateItemsStrategy.class);    MostSimilarItemsCandidateItemsStrategy mostSimilarItemsCandidateItemsStrategy = EasyMock.createMock(MostSimilarItemsCandidateItemsStrategy.class);    PreferenceArray preferencesFromUser = new GenericUserPreferenceArray(Arrays.asList(new GenericPreference(1L, 1L, 5.0f), new GenericPreference(1L, 2L, 4.0f)));    EasyMock.expect(dataModel.getMinPreference()).andReturn(Float.NaN);    EasyMock.expect(dataModel.getMaxPreference()).andReturn(Float.NaN);    EasyMock.expect(dataModel.getPreferencesFromUser(1L)).andReturn(preferencesFromUser);    EasyMock.expect(candidateItemsStrategy.getCandidateItems(1L, preferencesFromUser, dataModel, false)).andReturn(new FastIDSet(new long[] { 3L, 4L }));    EasyMock.expect(itemSimilarity.itemSimilarities(3L, preferencesFromUser.getIDs())).andReturn(new double[] { 0.5, 0.3 });    EasyMock.expect(itemSimilarity.itemSimilarities(4L, preferencesFromUser.getIDs())).andReturn(new double[] { 0.4, 0.1 });    EasyMock.replay(dataModel, itemSimilarity, candidateItemsStrategy, mostSimilarItemsCandidateItemsStrategy);    Recommender recommender = new GenericItemBasedRecommender(dataModel, itemSimilarity, candidateItemsStrategy, mostSimilarItemsCandidateItemsStrategy);    recommender.recommend(1L, 3);    EasyMock.verify(dataModel, itemSimilarity, candidateItemsStrategy, mostSimilarItemsCandidateItemsStrategy);}
0
public void testRecommender() throws Exception
{    Recommender recommender = buildRecommender();    List<RecommendedItem> recommended = recommender.recommend(1, 1);    assertNotNull(recommended);    assertEquals(1, recommended.size());    RecommendedItem firstRecommended = recommended.get(0);    assertEquals(2, firstRecommended.getItemID());    assertEquals(0.1f, firstRecommended.getValue(), EPSILON);    recommender.refresh(null);    assertEquals(2, firstRecommended.getItemID());    assertEquals(0.1f, firstRecommended.getValue(), EPSILON);}
0
public void testHowMany() throws Exception
{    DataModel dataModel = getDataModel(new long[] { 1, 2, 3, 4, 5 }, new Double[][] { { 0.1, 0.2 }, { 0.2, 0.3, 0.3, 0.6 }, { 0.4, 0.4, 0.5, 0.9 }, { 0.1, 0.4, 0.5, 0.8, 0.9, 1.0 }, { 0.2, 0.3, 0.6, 0.7, 0.1, 0.2 } });    UserSimilarity similarity = new PearsonCorrelationSimilarity(dataModel);    UserNeighborhood neighborhood = new NearestNUserNeighborhood(2, similarity, dataModel);    Recommender recommender = new GenericUserBasedRecommender(dataModel, neighborhood, similarity);    List<RecommendedItem> fewRecommended = recommender.recommend(1, 2);    List<RecommendedItem> moreRecommended = recommender.recommend(1, 4);    for (int i = 0; i < fewRecommended.size(); i++) {        assertEquals(fewRecommended.get(i).getItemID(), moreRecommended.get(i).getItemID());    }    recommender.refresh(null);    for (int i = 0; i < fewRecommended.size(); i++) {        assertEquals(fewRecommended.get(i).getItemID(), moreRecommended.get(i).getItemID());    }}
0
public void testRescorer() throws Exception
{    DataModel dataModel = getDataModel(new long[] { 1, 2, 3 }, new Double[][] { { 0.1, 0.2 }, { 0.2, 0.3, 0.3, 0.6 }, { 0.4, 0.5, 0.5, 0.9 } });    UserSimilarity similarity = new PearsonCorrelationSimilarity(dataModel);    UserNeighborhood neighborhood = new NearestNUserNeighborhood(2, similarity, dataModel);    Recommender recommender = new GenericUserBasedRecommender(dataModel, neighborhood, similarity);    List<RecommendedItem> originalRecommended = recommender.recommend(1, 2);    List<RecommendedItem> rescoredRecommended = recommender.recommend(1, 2, new ReversingRescorer<Long>());    assertNotNull(originalRecommended);    assertNotNull(rescoredRecommended);    assertEquals(2, originalRecommended.size());    assertEquals(2, rescoredRecommended.size());    assertEquals(originalRecommended.get(0).getItemID(), rescoredRecommended.get(1).getItemID());    assertEquals(originalRecommended.get(1).getItemID(), rescoredRecommended.get(0).getItemID());}
0
public void testIncludeKnownItems() throws Exception
{    DataModel dataModel = getDataModel(new long[] { 1, 2, 3 }, new Double[][] { { 0.1, 0.2 }, { 0.2, 0.3, 0.3, 0.6 }, { 0.4, 0.5, 0.5, 0.9 } });    UserSimilarity similarity = new PearsonCorrelationSimilarity(dataModel);    UserNeighborhood neighborhood = new NearestNUserNeighborhood(2, similarity, dataModel);    Recommender recommender = new GenericUserBasedRecommender(dataModel, neighborhood, similarity);    List<RecommendedItem> originalRecommended = recommender.recommend(1, 4, null, true);    List<RecommendedItem> rescoredRecommended = recommender.recommend(1, 4, new ReversingRescorer<Long>(), true);    assertNotNull(originalRecommended);    assertNotNull(rescoredRecommended);    assertEquals(4, originalRecommended.size());    assertEquals(4, rescoredRecommended.size());    assertEquals(originalRecommended.get(0).getItemID(), rescoredRecommended.get(3).getItemID());    assertEquals(originalRecommended.get(3).getItemID(), rescoredRecommended.get(0).getItemID());}
0
public void testEstimatePref() throws Exception
{    Recommender recommender = buildRecommender();    assertEquals(0.1f, recommender.estimatePreference(1, 2), EPSILON);}
0
public void testBestRating() throws Exception
{    Recommender recommender = buildRecommender();    List<RecommendedItem> recommended = recommender.recommend(1, 1);    assertNotNull(recommended);    assertEquals(1, recommended.size());    RecommendedItem firstRecommended = recommended.get(0);        assertEquals(2, firstRecommended.getItemID());    assertEquals(0.1f, firstRecommended.getValue(), EPSILON);}
0
public void testMostSimilar() throws Exception
{    UserBasedRecommender recommender = buildRecommender();    long[] similar = recommender.mostSimilarUserIDs(1, 2);    assertNotNull(similar);    assertEquals(2, similar.length);    assertEquals(2, similar[0]);    assertEquals(3, similar[1]);}
0
public void testIsolatedUser() throws Exception
{    DataModel dataModel = getDataModel(new long[] { 1, 2, 3, 4 }, new Double[][] { { 0.1, 0.2 }, { 0.2, 0.3, 0.3, 0.6 }, { 0.4, 0.4, 0.5, 0.9 }, { null, null, null, null, 1.0 } });    UserSimilarity similarity = new PearsonCorrelationSimilarity(dataModel);    UserNeighborhood neighborhood = new NearestNUserNeighborhood(3, similarity, dataModel);    UserBasedRecommender recommender = new GenericUserBasedRecommender(dataModel, neighborhood, similarity);    long[] mostSimilar = recommender.mostSimilarUserIDs(4, 3);    assertNotNull(mostSimilar);    assertEquals(0, mostSimilar.length);}
0
private static UserBasedRecommender buildRecommender() throws TasteException
{    DataModel dataModel = getDataModel();    UserSimilarity similarity = new PearsonCorrelationSimilarity(dataModel);    UserNeighborhood neighborhood = new NearestNUserNeighborhood(2, similarity, dataModel);    return new GenericUserBasedRecommender(dataModel, neighborhood, similarity);}
0
public void testRecommender() throws Exception
{    Recommender recommender = new ItemAverageRecommender(getDataModel());    List<RecommendedItem> recommended = recommender.recommend(1, 1);    assertNotNull(recommended);    assertEquals(1, recommended.size());    RecommendedItem firstRecommended = recommended.get(0);    assertEquals(2, firstRecommended.getItemID());    assertEquals(0.53333336f, firstRecommended.getValue(), EPSILON);    recommender.refresh(null);    assertEquals(2, firstRecommended.getItemID());    assertEquals(0.53333336f, firstRecommended.getValue(), EPSILON);}
0
public void testRecommender() throws Exception
{    Recommender recommender = new ItemUserAverageRecommender(getDataModel());    List<RecommendedItem> recommended = recommender.recommend(1, 1);    assertNotNull(recommended);    assertEquals(1, recommended.size());    RecommendedItem firstRecommended = recommended.get(0);    assertEquals(2, firstRecommended.getItemID());    assertEquals(0.35151517f, firstRecommended.getValue(), EPSILON);    recommender.refresh(null);    assertEquals(2, firstRecommended.getItemID());    assertEquals(0.35151517f, firstRecommended.getValue(), EPSILON);}
0
public List<RecommendedItem> recommend(long userID, int howMany)
{    recommendCount.increment();    return Collections.<RecommendedItem>singletonList(new GenericRecommendedItem(1, 1.0f));}
0
public List<RecommendedItem> recommend(long userID, int howMany, boolean includeKnownItems)
{    return recommend(userID, howMany);}
0
public List<RecommendedItem> recommend(long userID, int howMany, IDRescorer rescorer)
{    return recommend(userID, howMany);}
0
public List<RecommendedItem> recommend(long userID, int howMany, IDRescorer rescorer, boolean includeKnownItems)
{    return recommend(userID, howMany);}
0
public float estimatePreference(long userID, long itemID)
{    recommendCount.increment();    return 0.0f;}
0
public void setPreference(long userID, long itemID, float value)
{}
0
public void removePreference(long userID, long itemID)
{}
0
public DataModel getDataModel()
{    return TasteTestCase.getDataModel(new long[] { 1, 2, 3 }, new Double[][] { { 1.0 }, { 2.0 }, { 3.0 } });}
0
public void refresh(Collection<Refreshable> alreadyRefreshed)
{}
0
public void testItemRescorer() throws Exception
{    IDRescorer rescorer = NullRescorer.getItemInstance();    assertNotNull(rescorer);    assertEquals(1.0, rescorer.rescore(1L, 1.0), EPSILON);    assertEquals(1.0, rescorer.rescore(0L, 1.0), EPSILON);    assertEquals(0.0, rescorer.rescore(1L, 0.0), EPSILON);    assertTrue(Double.isNaN(rescorer.rescore(1L, Double.NaN)));}
0
public void testUserRescorer() throws Exception
{    IDRescorer rescorer = NullRescorer.getUserInstance();    assertNotNull(rescorer);    assertEquals(1.0, rescorer.rescore(1L, 1.0), EPSILON);    assertEquals(1.0, rescorer.rescore(0L, 1.0), EPSILON);    assertEquals(0.0, rescorer.rescore(1L, 0.0), EPSILON);    assertTrue(Double.isNaN(rescorer.rescore(1L, Double.NaN)));}
0
public void testStrategy() throws TasteException
{    FastIDSet itemIDsFromUser123 = new FastIDSet();    itemIDsFromUser123.add(1L);    FastIDSet itemIDsFromUser456 = new FastIDSet();    itemIDsFromUser456.add(1L);    itemIDsFromUser456.add(2L);    List<Preference> prefs = Lists.newArrayList();    prefs.add(new GenericPreference(123L, 1L, 1.0f));    prefs.add(new GenericPreference(456L, 1L, 1.0f));    PreferenceArray preferencesForItem1 = new GenericItemPreferenceArray(prefs);    DataModel dataModel = EasyMock.createMock(DataModel.class);    EasyMock.expect(dataModel.getPreferencesForItem(1L)).andReturn(preferencesForItem1);    EasyMock.expect(dataModel.getItemIDsFromUser(123L)).andReturn(itemIDsFromUser123);    EasyMock.expect(dataModel.getItemIDsFromUser(456L)).andReturn(itemIDsFromUser456);    PreferenceArray prefArrayOfUser123 = new GenericUserPreferenceArray(Collections.singletonList(new GenericPreference(123L, 1L, 1.0f)));    CandidateItemsStrategy strategy = new PreferredItemsNeighborhoodCandidateItemsStrategy();    EasyMock.replay(dataModel);    FastIDSet candidateItems = strategy.getCandidateItems(123L, prefArrayOfUser123, dataModel, false);    assertEquals(1, candidateItems.size());    assertTrue(candidateItems.contains(2L));    EasyMock.verify(dataModel);}
0
public void testRecommender() throws Exception
{    Recommender recommender = new RandomRecommender(getDataModel());    List<RecommendedItem> recommended = recommender.recommend(1, 1);    assertNotNull(recommended);    assertEquals(1, recommended.size());    RecommendedItem firstRecommended = recommended.get(0);    assertEquals(2, firstRecommended.getItemID());    recommender.refresh(null);    assertEquals(2, firstRecommended.getItemID());}
0
public double rescore(T thing, double originalScore)
{    return -originalScore;}
0
public boolean isFiltered(T thing)
{    return false;}
0
public double rescore(long ID, double originalScore)
{    return -originalScore;}
0
public boolean isFiltered(long ID)
{    return false;}
0
public void testStrategy() throws TasteException
{    List<Preference> prefsOfUser123 = Lists.newArrayList();    prefsOfUser123.add(new GenericPreference(123L, 1L, 1.0f));    List<Preference> prefsOfUser456 = Lists.newArrayList();    prefsOfUser456.add(new GenericPreference(456L, 1L, 1.0f));    prefsOfUser456.add(new GenericPreference(456L, 2L, 1.0f));    List<Preference> prefsOfUser789 = Lists.newArrayList();    prefsOfUser789.add(new GenericPreference(789L, 1L, 0.5f));    prefsOfUser789.add(new GenericPreference(789L, 3L, 1.0f));    PreferenceArray prefArrayOfUser123 = new GenericUserPreferenceArray(prefsOfUser123);    FastByIDMap<PreferenceArray> userData = new FastByIDMap<>();    userData.put(123L, prefArrayOfUser123);    userData.put(456L, new GenericUserPreferenceArray(prefsOfUser456));    userData.put(789L, new GenericUserPreferenceArray(prefsOfUser789));    DataModel dataModel = new GenericDataModel(userData);    CandidateItemsStrategy strategy = new SamplingCandidateItemsStrategy(1, 1, 1, dataModel.getNumUsers(), dataModel.getNumItems());    FastIDSet candidateItems = strategy.getCandidateItems(123L, prefArrayOfUser123, dataModel, false);    /* result can be either item2 or item3 or empty */    assertTrue(candidateItems.size() <= 1);    assertFalse(candidateItems.contains(1L));}
0
public void setUp() throws Exception
{    super.setUp();    FastByIDMap<PreferenceArray> userData = new FastByIDMap<>();    userData.put(1L, new GenericUserPreferenceArray(Arrays.asList(new GenericPreference(1L, 1L, 5.0f), new GenericPreference(1L, 2L, 5.0f), new GenericPreference(1L, 3L, 2.0f))));    userData.put(2L, new GenericUserPreferenceArray(Arrays.asList(new GenericPreference(2L, 1L, 2.0f), new GenericPreference(2L, 3L, 3.0f), new GenericPreference(2L, 4L, 5.0f))));    userData.put(3L, new GenericUserPreferenceArray(Arrays.asList(new GenericPreference(3L, 2L, 5.0f), new GenericPreference(3L, 4L, 3.0f))));    userData.put(4L, new GenericUserPreferenceArray(Arrays.asList(new GenericPreference(4L, 1L, 3.0f), new GenericPreference(4L, 4L, 5.0f))));    dataModel = new GenericDataModel(userData);    factorizer = new ALSWRFactorizer(dataModel, 3, 0.065, 10);}
0
public void setFeatureColumn() throws Exception
{    ALSWRFactorizer.Features features = new ALSWRFactorizer.Features(factorizer);    Vector vector = new DenseVector(new double[] { 0.5, 2.0, 1.5 });    int index = 1;    features.setFeatureColumnInM(index, vector);    double[][] matrix = features.getM();    assertEquals(vector.get(0), matrix[index][0], EPSILON);    assertEquals(vector.get(1), matrix[index][1], EPSILON);    assertEquals(vector.get(2), matrix[index][2], EPSILON);}
0
public void ratingVector() throws Exception
{    PreferenceArray prefs = dataModel.getPreferencesFromUser(1);    Vector ratingVector = ALSWRFactorizer.ratingVector(prefs);    assertEquals(prefs.length(), ratingVector.getNumNondefaultElements());    assertEquals(prefs.get(0).getValue(), ratingVector.get(0), EPSILON);    assertEquals(prefs.get(1).getValue(), ratingVector.get(1), EPSILON);    assertEquals(prefs.get(2).getValue(), ratingVector.get(2), EPSILON);}
0
public void averageRating() throws Exception
{    ALSWRFactorizer.Features features = new ALSWRFactorizer.Features(factorizer);    assertEquals(2.5, features.averateRating(3L), EPSILON);}
0
public void initializeM() throws Exception
{    ALSWRFactorizer.Features features = new ALSWRFactorizer.Features(factorizer);    double[][] M = features.getM();    assertEquals(3.333333333, M[0][0], EPSILON);    assertEquals(5, M[1][0], EPSILON);    assertEquals(2.5, M[2][0], EPSILON);    assertEquals(4.333333333, M[3][0], EPSILON);    for (int itemIndex = 0; itemIndex < dataModel.getNumItems(); itemIndex++) {        for (int feature = 1; feature < 3; feature++) {            assertTrue(M[itemIndex][feature] >= 0);            assertTrue(M[itemIndex][feature] <= 0.1);        }    }}
0
public void toyExample() throws Exception
{    SVDRecommender svdRecommender = new SVDRecommender(dataModel, factorizer);    /* a hold out test would be better, but this is just a toy example so we only check that the    * factorization is close to the original matrix */    RunningAverage avg = new FullRunningAverage();    LongPrimitiveIterator userIDs = dataModel.getUserIDs();    while (userIDs.hasNext()) {        long userID = userIDs.nextLong();        for (Preference pref : dataModel.getPreferencesFromUser(userID)) {            double rating = pref.getValue();            double estimate = svdRecommender.estimatePreference(userID, pref.getItemID());            double err = rating - estimate;            avg.addDatum(err * err);        }    }    double rmse = Math.sqrt(avg.getAverage());    assertTrue(rmse < 0.2);}
0
public void toyExampleImplicit() throws Exception
{    Matrix observations = new SparseRowMatrix(4, 4, new Vector[] { new DenseVector(new double[] { 5.0, 5.0, 2.0, 0 }), new DenseVector(new double[] { 2.0, 0, 3.0, 5.0 }), new DenseVector(new double[] { 0, 5.0, 0, 3.0 }), new DenseVector(new double[] { 3.0, 0, 0, 5.0 }) });    Matrix preferences = new SparseRowMatrix(4, 4, new Vector[] { new DenseVector(new double[] { 1.0, 1.0, 1.0, 0 }), new DenseVector(new double[] { 1.0, 0, 1.0, 1.0 }), new DenseVector(new double[] { 0, 1.0, 0, 1.0 }), new DenseVector(new double[] { 1.0, 0, 0, 1.0 }) });    double alpha = 20;    ALSWRFactorizer factorizer = new ALSWRFactorizer(dataModel, 3, 0.065, 5, true, alpha);    SVDRecommender svdRecommender = new SVDRecommender(dataModel, factorizer);    RunningAverage avg = new FullRunningAverage();    Iterator<MatrixSlice> sliceIterator = preferences.iterateAll();    while (sliceIterator.hasNext()) {        MatrixSlice slice = sliceIterator.next();        for (Vector.Element e : slice.vector().all()) {            long userID = slice.index() + 1;            long itemID = e.index() + 1;            if (!Double.isNaN(e.get())) {                double pref = e.get();                double estimate = svdRecommender.estimatePreference(userID, itemID);                double confidence = 1 + alpha * observations.getQuick(slice.index(), e.index());                double err = confidence * (pref - estimate) * (pref - estimate);                avg.addDatum(err);                            }        }    }    double rmse = Math.sqrt(avg.getAverage());        assertTrue(rmse < 0.4);}
1
public void persistAndLoad() throws Exception
{    FastByIDMap<Integer> userIDMapping = new FastByIDMap<>();    FastByIDMap<Integer> itemIDMapping = new FastByIDMap<>();    userIDMapping.put(123, 0);    userIDMapping.put(456, 1);    itemIDMapping.put(12, 0);    itemIDMapping.put(34, 1);    double[][] userFeatures = { { 0.1, 0.2, 0.3 }, { 0.4, 0.5, 0.6 } };    double[][] itemFeatures = { { 0.7, 0.8, 0.9 }, { 1.0, 1.1, 1.2 } };    Factorization original = new Factorization(userIDMapping, itemIDMapping, userFeatures, itemFeatures);    File storage = getTestTempFile("storage.bin");    PersistenceStrategy persistenceStrategy = new FilePersistenceStrategy(storage);    assertNull(persistenceStrategy.load());    persistenceStrategy.maybePersist(original);    Factorization clone = persistenceStrategy.load();    assertEquals(original, clone);}
0
private Matrix randomMatrix(int numRows, int numColumns, double range)
{    double[][] data = new double[numRows][numColumns];    for (int i = 0; i < numRows; i++) {        for (int j = 0; j < numColumns; j++) {            double sqrtUniform = random.nextDouble();            data[i][j] = sqrtUniform * range;        }    }    return new DenseMatrix(data);}
0
private void normalize(Matrix source, final double range)
{    final double max = source.aggregateColumns(new VectorFunction() {        @Override        public double apply(Vector column) {            return column.maxValue();        }    }).maxValue();    final double min = source.aggregateColumns(new VectorFunction() {        @Override        public double apply(Vector column) {            return column.minValue();        }    }).minValue();    source.assign(new DoubleFunction() {        @Override        public double apply(double value) {            return (value - min) * range / (max - min);        }    });}
0
public double apply(Vector column)
{    return column.maxValue();}
0
public double apply(Vector column)
{    return column.minValue();}
0
public double apply(double value)
{    return (value - min) * range / (max - min);}
0
public void setUpSyntheticData() throws Exception
{    int numUsers = 2000;    int numItems = 1000;    double sparsity = 0.5;    this.rank = 20;    this.lambda = 0.000000001;    this.numIterations = 100;    Matrix users = randomMatrix(numUsers, rank, 1);    Matrix items = randomMatrix(rank, numItems, 1);    Matrix ratings = users.times(items);    normalize(ratings, 5);    FastByIDMap<PreferenceArray> userData = new FastByIDMap<>();    for (int userIndex = 0; userIndex < numUsers; userIndex++) {        List<Preference> row = Lists.newArrayList();        for (int itemIndex = 0; itemIndex < numItems; itemIndex++) {            if (random.nextDouble() <= sparsity) {                row.add(new GenericPreference(userIndex, itemIndex, (float) ratings.get(userIndex, itemIndex)));            }        }        userData.put(userIndex, new GenericUserPreferenceArray(row));    }    dataModel = new GenericDataModel(userData);}
0
public void setUpToyData() throws Exception
{    this.rank = 3;    this.lambda = 0.01;    this.numIterations = 1000;    FastByIDMap<PreferenceArray> userData = new FastByIDMap<>();    userData.put(1L, new GenericUserPreferenceArray(Arrays.asList(new GenericPreference(1L, 1L, 5.0f), new GenericPreference(1L, 2L, 5.0f), new GenericPreference(1L, 3L, 2.0f))));    userData.put(2L, new GenericUserPreferenceArray(Arrays.asList(new GenericPreference(2L, 1L, 2.0f), new GenericPreference(2L, 3L, 3.0f), new GenericPreference(2L, 4L, 5.0f))));    userData.put(3L, new GenericUserPreferenceArray(Arrays.asList(new GenericPreference(3L, 2L, 5.0f), new GenericPreference(3L, 4L, 3.0f))));    userData.put(4L, new GenericUserPreferenceArray(Arrays.asList(new GenericPreference(4L, 1L, 3.0f), new GenericPreference(4L, 4L, 5.0f))));    dataModel = new GenericDataModel(userData);}
0
public void testPreferenceShufflerWithSyntheticData() throws Exception
{    setUpSyntheticData();    ParallelSGDFactorizer.PreferenceShuffler shuffler = new PreferenceShuffler(dataModel);    shuffler.shuffle();    shuffler.stage();    FastByIDMap<FastByIDMap<Boolean>> checked = new FastByIDMap<>();    for (int i = 0; i < shuffler.size(); i++) {        Preference pref = shuffler.get(i);        float value = dataModel.getPreferenceValue(pref.getUserID(), pref.getItemID());        assertEquals(pref.getValue(), value, 0.0);        if (!checked.containsKey(pref.getUserID())) {            checked.put(pref.getUserID(), new FastByIDMap<Boolean>());        }        assertNull(checked.get(pref.getUserID()).get(pref.getItemID()));        checked.get(pref.getUserID()).put(pref.getItemID(), true);    }    LongPrimitiveIterator userIDs = dataModel.getUserIDs();    int index = 0;    while (userIDs.hasNext()) {        long userID = userIDs.nextLong();        PreferenceArray preferencesFromUser = dataModel.getPreferencesFromUser(userID);        for (Preference preference : preferencesFromUser) {            assertTrue(checked.get(preference.getUserID()).get(preference.getItemID()));            index++;        }    }    assertEquals(index, shuffler.size());}
0
public void testRecommenderWithToyData() throws Exception
{    setUpToyData();    factorizer = new ParallelSGDFactorizer(dataModel, rank, lambda, numIterations, 0.01, 1, 0, 0);    svdRecommender = new SVDRecommender(dataModel, factorizer);    /* a hold out test would be better, but this is just a toy example so we only check that the     * factorization is close to the original matrix */    RunningAverage avg = new FullRunningAverage();    LongPrimitiveIterator userIDs = dataModel.getUserIDs();    while (userIDs.hasNext()) {        long userID = userIDs.nextLong();        for (Preference pref : dataModel.getPreferencesFromUser(userID)) {            double rating = pref.getValue();            double estimate = svdRecommender.estimatePreference(userID, pref.getItemID());            double err = rating - estimate;            avg.addDatum(err * err);        }    }    double rmse = Math.sqrt(avg.getAverage());        assertTrue(rmse < 0.2);}
1
public void testRecommenderWithSyntheticData() throws Exception
{    setUpSyntheticData();    factorizer = new ParallelSGDFactorizer(dataModel, rank, lambda, numIterations, 0.01, 1, 0, 0);    svdRecommender = new SVDRecommender(dataModel, factorizer);    /* a hold out test would be better, but this is just a toy example so we only check that the     * factorization is close to the original matrix */    RunningAverage avg = new FullRunningAverage();    LongPrimitiveIterator userIDs = dataModel.getUserIDs();    while (userIDs.hasNext()) {        long userID = userIDs.nextLong();        for (Preference pref : dataModel.getPreferencesFromUser(userID)) {            double rating = pref.getValue();            double estimate = svdRecommender.estimatePreference(userID, pref.getItemID());            double err = rating - estimate;            avg.addDatum(err * err);        }    }    double rmse = Math.sqrt(avg.getAverage());        assertTrue(rmse < 0.2);}
1
public void estimatePreference() throws Exception
{    DataModel dataModel = EasyMock.createMock(DataModel.class);    Factorizer factorizer = EasyMock.createMock(Factorizer.class);    Factorization factorization = EasyMock.createMock(Factorization.class);    EasyMock.expect(factorizer.factorize()).andReturn(factorization);    EasyMock.expect(factorization.getUserFeatures(1L)).andReturn(new double[] { 0.4, 2 });    EasyMock.expect(factorization.getItemFeatures(5L)).andReturn(new double[] { 1, 0.3 });    EasyMock.replay(dataModel, factorizer, factorization);    SVDRecommender svdRecommender = new SVDRecommender(dataModel, factorizer);    float estimate = svdRecommender.estimatePreference(1L, 5L);    assertEquals(1, estimate, EPSILON);    EasyMock.verify(dataModel, factorizer, factorization);}
0
public void recommend() throws Exception
{    DataModel dataModel = EasyMock.createMock(DataModel.class);    PreferenceArray preferencesFromUser = EasyMock.createMock(PreferenceArray.class);    CandidateItemsStrategy candidateItemsStrategy = EasyMock.createMock(CandidateItemsStrategy.class);    Factorizer factorizer = EasyMock.createMock(Factorizer.class);    Factorization factorization = EasyMock.createMock(Factorization.class);    FastIDSet candidateItems = new FastIDSet();    candidateItems.add(5L);    candidateItems.add(3L);    EasyMock.expect(factorizer.factorize()).andReturn(factorization);    EasyMock.expect(dataModel.getPreferencesFromUser(1L)).andReturn(preferencesFromUser);    EasyMock.expect(candidateItemsStrategy.getCandidateItems(1L, preferencesFromUser, dataModel, false)).andReturn(candidateItems);    EasyMock.expect(factorization.getUserFeatures(1L)).andReturn(new double[] { 0.4, 2 });    EasyMock.expect(factorization.getItemFeatures(5L)).andReturn(new double[] { 1, 0.3 });    EasyMock.expect(factorization.getUserFeatures(1L)).andReturn(new double[] { 0.4, 2 });    EasyMock.expect(factorization.getItemFeatures(3L)).andReturn(new double[] { 2, 0.6 });    EasyMock.replay(dataModel, candidateItemsStrategy, factorizer, factorization);    SVDRecommender svdRecommender = new SVDRecommender(dataModel, factorizer, candidateItemsStrategy);    List<RecommendedItem> recommendedItems = svdRecommender.recommend(1L, 5);    assertEquals(2, recommendedItems.size());    assertEquals(3L, recommendedItems.get(0).getItemID());    assertEquals(2.0f, recommendedItems.get(0).getValue(), EPSILON);    assertEquals(5L, recommendedItems.get(1).getItemID());    assertEquals(1.0f, recommendedItems.get(1).getValue(), EPSILON);    EasyMock.verify(dataModel, candidateItemsStrategy, factorizer, factorization);}
0
public void testTopItems() throws Exception
{    long[] ids = new long[100];    for (int i = 0; i < 100; i++) {        ids[i] = i;    }    LongPrimitiveIterator possibleItemIds = new LongPrimitiveArrayIterator(ids);    TopItems.Estimator<Long> estimator = new TopItems.Estimator<Long>() {        @Override        public double estimate(Long thing) {            return thing;        }    };    List<RecommendedItem> topItems = TopItems.getTopItems(10, possibleItemIds, null, estimator);    int gold = 99;    for (RecommendedItem topItem : topItems) {        assertEquals(gold, topItem.getItemID());        assertEquals(gold--, topItem.getValue(), 0.01);    }}
0
public double estimate(Long thing)
{    return thing;}
0
public void testTopItemsRandom() throws Exception
{    long[] ids = new long[100];    for (int i = 0; i < 100; i++) {        ids[i] = i;    }    LongPrimitiveIterator possibleItemIds = new LongPrimitiveArrayIterator(ids);    final Random random = RandomUtils.getRandom();    TopItems.Estimator<Long> estimator = new TopItems.Estimator<Long>() {        @Override        public double estimate(Long thing) {            return random.nextDouble();        }    };    List<RecommendedItem> topItems = TopItems.getTopItems(10, possibleItemIds, null, estimator);    assertEquals(10, topItems.size());    double last = 2.0;    for (RecommendedItem topItem : topItems) {        assertTrue(topItem.getValue() <= last);        last = topItem.getItemID();    }}
0
public double estimate(Long thing)
{    return random.nextDouble();}
0
public void testTopUsers() throws Exception
{    long[] ids = new long[100];    for (int i = 0; i < 100; i++) {        ids[i] = i;    }    LongPrimitiveIterator possibleItemIds = new LongPrimitiveArrayIterator(ids);    TopItems.Estimator<Long> estimator = new TopItems.Estimator<Long>() {        @Override        public double estimate(Long thing) {            return thing;        }    };    long[] topItems = TopItems.getTopUsers(10, possibleItemIds, null, estimator);    int gold = 99;    for (long topItem : topItems) {        assertEquals(gold--, topItem);    }}
0
public double estimate(Long thing)
{    return thing;}
0
public void testTopItemItem() throws Exception
{    List<GenericItemSimilarity.ItemItemSimilarity> sims = Lists.newArrayList();    for (int i = 0; i < 99; i++) {        sims.add(new GenericItemSimilarity.ItemItemSimilarity(i, i + 1, i / 99.0));    }    List<GenericItemSimilarity.ItemItemSimilarity> res = TopItems.getTopItemItemSimilarities(10, sims.iterator());    int gold = 99;    for (GenericItemSimilarity.ItemItemSimilarity re : res) {                assertEquals(gold--, re.getItemID2());    }}
0
public void testTopItemItemAlt() throws Exception
{    List<GenericItemSimilarity.ItemItemSimilarity> sims = Lists.newArrayList();    for (int i = 0; i < 99; i++) {        sims.add(new GenericItemSimilarity.ItemItemSimilarity(i, i + 1, 1 - (i / 99.0)));    }    List<GenericItemSimilarity.ItemItemSimilarity> res = TopItems.getTopItemItemSimilarities(10, sims.iterator());    int gold = 0;    for (GenericItemSimilarity.ItemItemSimilarity re : res) {                assertEquals(gold++, re.getItemID1());    }}
0
public void testTopUserUser() throws Exception
{    List<GenericUserSimilarity.UserUserSimilarity> sims = Lists.newArrayList();    for (int i = 0; i < 99; i++) {        sims.add(new GenericUserSimilarity.UserUserSimilarity(i, i + 1, i / 99.0));    }    List<GenericUserSimilarity.UserUserSimilarity> res = TopItems.getTopUserUserSimilarities(10, sims.iterator());    int gold = 99;    for (GenericUserSimilarity.UserUserSimilarity re : res) {                assertEquals(gold--, re.getUserID2());    }}
0
public void testTopUserUserAlt() throws Exception
{    List<GenericUserSimilarity.UserUserSimilarity> sims = Lists.newArrayList();    for (int i = 0; i < 99; i++) {        sims.add(new GenericUserSimilarity.UserUserSimilarity(i, i + 1, 1 - (i / 99.0)));    }    List<GenericUserSimilarity.UserUserSimilarity> res = TopItems.getTopUserUserSimilarities(10, sims.iterator());    int gold = 0;    for (GenericUserSimilarity.UserUserSimilarity re : res) {                assertEquals(gold++, re.getUserID1());    }}
0
public void testInferrer() throws TasteException
{    DataModel model = getDataModel(new long[] { 1 }, new Double[][] { { 3.0, -2.0, 5.0 } });    PreferenceInferrer inferrer = new AveragingPreferenceInferrer(model);    double inferred = inferrer.inferPreference(1, 3);    assertEquals(2.0, inferred, EPSILON);}
0
public void testFullCorrelation1() throws Exception
{    DataModel dataModel = getDataModel(new long[] { 1, 2 }, new Double[][] { { 3.0, -2.0 }, { 3.0, -2.0 } });    double correlation = new EuclideanDistanceSimilarity(dataModel).userSimilarity(1, 2);    assertCorrelationEquals(1.0, correlation);}
0
public void testFullCorrelation1Weighted() throws Exception
{    DataModel dataModel = getDataModel(new long[] { 1, 2 }, new Double[][] { { 3.0, -2.0 }, { 3.0, -2.0 } });    double correlation = new EuclideanDistanceSimilarity(dataModel, Weighting.WEIGHTED).userSimilarity(1, 2);    assertCorrelationEquals(1.0, correlation);}
0
public void testFullCorrelation2() throws Exception
{    DataModel dataModel = getDataModel(new long[] { 1, 2 }, new Double[][] { { 3.0, 3.0 }, { 3.0, 3.0 } });    double correlation = new EuclideanDistanceSimilarity(dataModel).userSimilarity(1, 2);    assertEquals(1.0, correlation, EPSILON);}
0
public void testNoCorrelation1() throws Exception
{    DataModel dataModel = getDataModel(new long[] { 1, 2 }, new Double[][] { { 3.0, -2.0 }, { -3.0, 2.0 } });    double correlation = new EuclideanDistanceSimilarity(dataModel).userSimilarity(1, 2);    assertCorrelationEquals(0.1639607805437114, correlation);}
0
public void testNoCorrelation1Weighted() throws Exception
{    DataModel dataModel = getDataModel(new long[] { 1, 2 }, new Double[][] { { 3.0, -2.0 }, { -3.0, 2.0 } });    double correlation = new EuclideanDistanceSimilarity(dataModel, Weighting.WEIGHTED).userSimilarity(1, 2);    assertCorrelationEquals(0.7213202601812372, correlation);}
0
public void testNoCorrelation2() throws Exception
{    DataModel dataModel = getDataModel(new long[] { 1, 2 }, new Double[][] { { null, 1.0, null }, { null, null, 1.0 } });    double correlation = new EuclideanDistanceSimilarity(dataModel).userSimilarity(1, 2);    assertTrue(Double.isNaN(correlation));}
0
public void testNoCorrelation3() throws Exception
{    DataModel dataModel = getDataModel(new long[] { 1, 2 }, new Double[][] { { 90.0, 80.0, 70.0 }, { 70.0, 80.0, 90.0 } });    double correlation = new EuclideanDistanceSimilarity(dataModel).userSimilarity(1, 2);    assertCorrelationEquals(0.05770363219029305, correlation);}
0
public void testSimple() throws Exception
{    DataModel dataModel = getDataModel(new long[] { 1, 2 }, new Double[][] { { 1.0, 2.0, 3.0 }, { 2.0, 5.0, 6.0 } });    double correlation = new EuclideanDistanceSimilarity(dataModel).userSimilarity(1, 2);    assertCorrelationEquals(0.2843646522044218, correlation);}
0
public void testSimpleWeighted() throws Exception
{    DataModel dataModel = getDataModel(new long[] { 1, 2 }, new Double[][] { { 1.0, 2.0, 3.0 }, { 2.0, 5.0, 6.0 } });    double correlation = new EuclideanDistanceSimilarity(dataModel, Weighting.WEIGHTED).userSimilarity(1, 2);    assertCorrelationEquals(0.8210911630511055, correlation);}
0
public void testFullItemCorrelation1() throws Exception
{    DataModel dataModel = getDataModel(new long[] { 1, 2 }, new Double[][] { { 3.0, 3.0 }, { -2.0, -2.0 } });    double correlation = new EuclideanDistanceSimilarity(dataModel).itemSimilarity(0, 1);    assertCorrelationEquals(1.0, correlation);}
0
public void testFullItemCorrelation2() throws Exception
{    DataModel dataModel = getDataModel(new long[] { 1, 2 }, new Double[][] { { 3.0, 3.0 }, { 3.0, 3.0 } });    double correlation = new EuclideanDistanceSimilarity(dataModel).itemSimilarity(0, 1);    assertEquals(1.0, correlation, EPSILON);}
0
public void testNoItemCorrelation1() throws Exception
{    DataModel dataModel = getDataModel(new long[] { 1, 2 }, new Double[][] { { 3.0, -3.0 }, { -2.0, 2.0 } });    double correlation = new EuclideanDistanceSimilarity(dataModel).itemSimilarity(0, 1);    assertCorrelationEquals(0.1639607805437114, correlation);}
0
public void testNoItemCorrelation2() throws Exception
{    DataModel dataModel = getDataModel(new long[] { 1, 2 }, new Double[][] { { null, 1.0, null }, { null, null, 1.0 } });    double correlation = new EuclideanDistanceSimilarity(dataModel).itemSimilarity(1, 2);    assertTrue(Double.isNaN(correlation));}
0
public void testNoItemCorrelation3() throws Exception
{    DataModel dataModel = getDataModel(new long[] { 1, 2, 3 }, new Double[][] { { 90.0, 70.0 }, { 80.0, 80.0 }, { 70.0, 90.0 } });    double correlation = new EuclideanDistanceSimilarity(dataModel).itemSimilarity(0, 1);    assertCorrelationEquals(0.05770363219029305, correlation);}
0
public void testSimpleItem() throws Exception
{    DataModel dataModel = getDataModel(new long[] { 1, 2, 3 }, new Double[][] { { 1.0, 2.0 }, { 2.0, 5.0 }, { 3.0, 6.0 } });    double correlation = new EuclideanDistanceSimilarity(dataModel).itemSimilarity(0, 1);    assertCorrelationEquals(0.2843646522044218, correlation);}
0
public void testSimpleItemWeighted() throws Exception
{    DataModel dataModel = getDataModel(new long[] { 1, 2, 3 }, new Double[][] { { 1.0, 2.0 }, { 2.0, 5.0 }, { 3.0, 6.0 } });    ItemSimilarity itemSimilarity = new EuclideanDistanceSimilarity(dataModel, Weighting.WEIGHTED);    double correlation = itemSimilarity.itemSimilarity(0, 1);    assertCorrelationEquals(0.8210911630511055, correlation);}
0
public void testRefresh() throws TasteException
{        new EuclideanDistanceSimilarity(getDataModel()).refresh(null);}
0
public void setUp() throws Exception
{    super.setUp();    testFile = getTestTempFile("test.txt");    writeLines(testFile, data);}
0
public void testLoadFromFile() throws Exception
{    ItemSimilarity similarity = new FileItemSimilarity(testFile);    assertEquals(0.125, similarity.itemSimilarity(1L, 5L), EPSILON);    assertEquals(0.125, similarity.itemSimilarity(5L, 1L), EPSILON);    assertEquals(0.5, similarity.itemSimilarity(1L, 7L), EPSILON);    assertEquals(0.5, similarity.itemSimilarity(7L, 1L), EPSILON);    assertTrue(Double.isNaN(similarity.itemSimilarity(7L, 8L)));    double[] valuesForOne = similarity.itemSimilarities(1L, new long[] { 5L, 7L });    assertNotNull(valuesForOne);    assertEquals(2, valuesForOne.length);    assertEquals(0.125, valuesForOne[0], EPSILON);    assertEquals(0.5, valuesForOne[1], EPSILON);}
0
public void testNoRefreshAfterFileUpdate() throws Exception
{    ItemSimilarity similarity = new FileItemSimilarity(testFile, 0L);    /* call a method to make sure the original file is loaded*/    similarity.itemSimilarity(1L, 5L);    /* change the underlying file,     * we have to wait at least a second to see the change in the file's lastModified timestamp */    Thread.sleep(2000L);    writeLines(testFile, changedData);    /* we shouldn't see any changes in the data as we have not yet refreshed */    assertEquals(0.5, similarity.itemSimilarity(1L, 7L), EPSILON);    assertEquals(0.5, similarity.itemSimilarity(7L, 1L), EPSILON);    assertTrue(Double.isNaN(similarity.itemSimilarity(7L, 8L)));}
0
public void testRefreshAfterFileUpdate() throws Exception
{    ItemSimilarity similarity = new FileItemSimilarity(testFile, 0L);    /* call a method to make sure the original file is loaded */    similarity.itemSimilarity(1L, 5L);    /* change the underlying file,     * we have to wait at least a second to see the change in the file's lastModified timestamp */    Thread.sleep(2000L);    writeLines(testFile, changedData);    similarity.refresh(null);    /* we should now see the changes in the data */    assertEquals(0.9, similarity.itemSimilarity(1L, 7L), EPSILON);    assertEquals(0.9, similarity.itemSimilarity(7L, 1L), EPSILON);    assertEquals(0.125, similarity.itemSimilarity(1L, 5L), EPSILON);    assertEquals(0.125, similarity.itemSimilarity(5L, 1L), EPSILON);    assertFalse(Double.isNaN(similarity.itemSimilarity(7L, 8L)));    assertEquals(0.112, similarity.itemSimilarity(7L, 8L), EPSILON);    assertEquals(0.112, similarity.itemSimilarity(8L, 7L), EPSILON);}
0
public void testFileNotFoundExceptionForNonExistingFile() throws Exception
{    new FileItemSimilarity(new File("xKsdfksdfsdf"));}
0
public void testFileItemItemSimilarityIterable() throws Exception
{    Iterable<ItemItemSimilarity> similarityIterable = new FileItemItemSimilarityIterable(testFile);    GenericItemSimilarity similarity = new GenericItemSimilarity(similarityIterable);    assertEquals(0.125, similarity.itemSimilarity(1L, 5L), EPSILON);    assertEquals(0.125, similarity.itemSimilarity(5L, 1L), EPSILON);    assertEquals(0.5, similarity.itemSimilarity(1L, 7L), EPSILON);    assertEquals(0.5, similarity.itemSimilarity(7L, 1L), EPSILON);    assertTrue(Double.isNaN(similarity.itemSimilarity(7L, 8L)));    double[] valuesForOne = similarity.itemSimilarities(1L, new long[] { 5L, 7L });    assertNotNull(valuesForOne);    assertEquals(2, valuesForOne.length);    assertEquals(0.125, valuesForOne[0], EPSILON);    assertEquals(0.5, valuesForOne[1], EPSILON);}
0
public void testToString() throws Exception
{    ItemSimilarity similarity = new FileItemSimilarity(testFile);    assertTrue(!similarity.toString().isEmpty());}
0
public void testSimple()
{    List<GenericItemSimilarity.ItemItemSimilarity> similarities = Lists.newArrayList();    similarities.add(new GenericItemSimilarity.ItemItemSimilarity(1, 2, 0.5));    similarities.add(new GenericItemSimilarity.ItemItemSimilarity(2, 1, 0.6));    similarities.add(new GenericItemSimilarity.ItemItemSimilarity(1, 1, 0.5));    similarities.add(new GenericItemSimilarity.ItemItemSimilarity(1, 3, 0.3));    GenericItemSimilarity itemCorrelation = new GenericItemSimilarity(similarities);    assertEquals(1.0, itemCorrelation.itemSimilarity(1, 1), EPSILON);    assertEquals(0.6, itemCorrelation.itemSimilarity(1, 2), EPSILON);    assertEquals(0.6, itemCorrelation.itemSimilarity(2, 1), EPSILON);    assertEquals(0.3, itemCorrelation.itemSimilarity(1, 3), EPSILON);    assertTrue(Double.isNaN(itemCorrelation.itemSimilarity(3, 4)));}
0
public void testFromCorrelation() throws Exception
{    DataModel dataModel = getDataModel(new long[] { 1, 2, 3 }, new Double[][] { { 1.0, 2.0 }, { 2.0, 5.0 }, { 3.0, 6.0 } });    ItemSimilarity otherSimilarity = new PearsonCorrelationSimilarity(dataModel);    ItemSimilarity itemSimilarity = new GenericItemSimilarity(otherSimilarity, dataModel);    assertCorrelationEquals(1.0, itemSimilarity.itemSimilarity(0, 0));    assertCorrelationEquals(0.960768922830523, itemSimilarity.itemSimilarity(0, 1));}
0
public void testAllSimilaritiesWithoutIndex() throws TasteException
{    List<GenericItemSimilarity.ItemItemSimilarity> itemItemSimilarities = Arrays.asList(new GenericItemSimilarity.ItemItemSimilarity(1L, 2L, 0.2), new GenericItemSimilarity.ItemItemSimilarity(1L, 3L, 0.2), new GenericItemSimilarity.ItemItemSimilarity(2L, 1L, 0.2), new GenericItemSimilarity.ItemItemSimilarity(3L, 5L, 0.2), new GenericItemSimilarity.ItemItemSimilarity(3L, 4L, 0.2));    ItemSimilarity similarity = new GenericItemSimilarity(itemItemSimilarities);    assertTrue(containsExactly(similarity.allSimilarItemIDs(1L), 2L, 3L));    assertTrue(containsExactly(similarity.allSimilarItemIDs(2L), 1L));    assertTrue(containsExactly(similarity.allSimilarItemIDs(3L), 1L, 5L, 4L));    assertTrue(containsExactly(similarity.allSimilarItemIDs(4L), 3L));    assertTrue(containsExactly(similarity.allSimilarItemIDs(5L), 3L));}
0
public void testAllSimilaritiesWithIndex() throws TasteException
{    List<GenericItemSimilarity.ItemItemSimilarity> itemItemSimilarities = Arrays.asList(new GenericItemSimilarity.ItemItemSimilarity(1L, 2L, 0.2), new GenericItemSimilarity.ItemItemSimilarity(1L, 3L, 0.2), new GenericItemSimilarity.ItemItemSimilarity(2L, 1L, 0.2), new GenericItemSimilarity.ItemItemSimilarity(3L, 5L, 0.2), new GenericItemSimilarity.ItemItemSimilarity(3L, 4L, 0.2));    ItemSimilarity similarity = new GenericItemSimilarity(itemItemSimilarities);    assertTrue(containsExactly(similarity.allSimilarItemIDs(1L), 2L, 3L));    assertTrue(containsExactly(similarity.allSimilarItemIDs(2L), 1L));    assertTrue(containsExactly(similarity.allSimilarItemIDs(3L), 1L, 5L, 4L));    assertTrue(containsExactly(similarity.allSimilarItemIDs(4L), 3L));    assertTrue(containsExactly(similarity.allSimilarItemIDs(5L), 3L));}
0
private static boolean containsExactly(long[] allIDs, long... shouldContainID)
{    return new FastIDSet(allIDs).intersectionSize(new FastIDSet(shouldContainID)) == shouldContainID.length;}
0
public void testCorrelation() throws Exception
{    DataModel dataModel = getDataModel(new long[] { 1, 2, 3, 4, 5 }, new Double[][] { { 1.0, 1.0 }, { 1.0, null, 1.0 }, { null, null, 1.0, 1.0, 1.0 }, { 1.0, 1.0, 1.0, 1.0, 1.0 }, { null, 1.0, 1.0, 1.0, 1.0 } });    LogLikelihoodSimilarity similarity = new LogLikelihoodSimilarity(dataModel);    assertCorrelationEquals(0.12160727029227925, similarity.itemSimilarity(1, 0));    assertCorrelationEquals(0.12160727029227925, similarity.itemSimilarity(0, 1));    assertCorrelationEquals(0.5423213660693732, similarity.itemSimilarity(1, 2));    assertCorrelationEquals(0.5423213660693732, similarity.itemSimilarity(2, 1));    assertCorrelationEquals(0.6905400104897509, similarity.itemSimilarity(2, 3));    assertCorrelationEquals(0.6905400104897509, similarity.itemSimilarity(3, 2));    assertCorrelationEquals(0.8706358464330881, similarity.itemSimilarity(3, 4));    assertCorrelationEquals(0.8706358464330881, similarity.itemSimilarity(4, 3));}
0
public void testNoSimilarity() throws Exception
{    DataModel dataModel = getDataModel(new long[] { 1, 2, 3, 4 }, new Double[][] { { 1.0, null, 1.0, 1.0 }, { 1.0, null, 1.0, 1.0 }, { null, 1.0, 1.0, 1.0 }, { null, 1.0, 1.0, 1.0 } });    LogLikelihoodSimilarity similarity = new LogLikelihoodSimilarity(dataModel);    assertCorrelationEquals(Double.NaN, similarity.itemSimilarity(1, 0));    assertCorrelationEquals(Double.NaN, similarity.itemSimilarity(0, 1));    assertCorrelationEquals(0.0, similarity.itemSimilarity(2, 3));    assertCorrelationEquals(0.0, similarity.itemSimilarity(3, 2));}
0
public void testRefresh()
{        new LogLikelihoodSimilarity(getDataModel()).refresh(null);}
0
public void testFullCorrelation1() throws Exception
{    DataModel dataModel = getDataModel(new long[] { 1, 2 }, new Double[][] { { 3.0, -2.0 }, { 3.0, -2.0 } });    double correlation = new PearsonCorrelationSimilarity(dataModel).userSimilarity(1, 2);    assertCorrelationEquals(1.0, correlation);}
0
public void testFullCorrelation1Weighted() throws Exception
{    DataModel dataModel = getDataModel(new long[] { 1, 2 }, new Double[][] { { 3.0, -2.0 }, { 3.0, -2.0 } });    double correlation = new PearsonCorrelationSimilarity(dataModel, Weighting.WEIGHTED).userSimilarity(1, 2);    assertCorrelationEquals(1.0, correlation);}
0
public void testFullCorrelation2() throws Exception
{    DataModel dataModel = getDataModel(new long[] { 1, 2 }, new Double[][] { { 3.0, 3.0 }, { 3.0, 3.0 } });    double correlation = new PearsonCorrelationSimilarity(dataModel).userSimilarity(1, 2);        assertTrue(Double.isNaN(correlation));}
0
public void testNoCorrelation1() throws Exception
{    DataModel dataModel = getDataModel(new long[] { 1, 2 }, new Double[][] { { 3.0, -2.0 }, { -3.0, 2.0 } });    double correlation = new PearsonCorrelationSimilarity(dataModel).userSimilarity(1, 2);    assertCorrelationEquals(-1.0, correlation);}
0
public void testNoCorrelation1Weighted() throws Exception
{    DataModel dataModel = getDataModel(new long[] { 1, 2 }, new Double[][] { { 3.0, -2.0 }, { -3.0, 2.0 } });    double correlation = new PearsonCorrelationSimilarity(dataModel, Weighting.WEIGHTED).userSimilarity(1, 2);    assertCorrelationEquals(-1.0, correlation);}
0
public void testNoCorrelation2() throws Exception
{    DataModel dataModel = getDataModel(new long[] { 1, 2 }, new Double[][] { { null, 1.0, null }, { null, null, 1.0 } });    double correlation = new PearsonCorrelationSimilarity(dataModel).userSimilarity(1, 2);    assertTrue(Double.isNaN(correlation));}
0
public void testNoCorrelation3() throws Exception
{    DataModel dataModel = getDataModel(new long[] { 1, 2 }, new Double[][] { { 90.0, 80.0, 70.0 }, { 70.0, 80.0, 90.0 } });    double correlation = new PearsonCorrelationSimilarity(dataModel).userSimilarity(1, 2);    assertCorrelationEquals(-1.0, correlation);}
0
public void testSimple() throws Exception
{    DataModel dataModel = getDataModel(new long[] { 1, 2 }, new Double[][] { { 1.0, 2.0, 3.0 }, { 2.0, 5.0, 6.0 } });    double correlation = new PearsonCorrelationSimilarity(dataModel).userSimilarity(1, 2);    assertCorrelationEquals(0.9607689228305227, correlation);}
0
public void testSimpleWeighted() throws Exception
{    DataModel dataModel = getDataModel(new long[] { 1, 2 }, new Double[][] { { 1.0, 2.0, 3.0 }, { 2.0, 5.0, 6.0 } });    double correlation = new PearsonCorrelationSimilarity(dataModel, Weighting.WEIGHTED).userSimilarity(1, 2);    assertCorrelationEquals(0.9901922307076306, correlation);}
0
public void testFullItemCorrelation1() throws Exception
{    DataModel dataModel = getDataModel(new long[] { 1, 2 }, new Double[][] { { 3.0, 3.0 }, { -2.0, -2.0 } });    double correlation = new PearsonCorrelationSimilarity(dataModel).itemSimilarity(0, 1);    assertCorrelationEquals(1.0, correlation);}
0
public void testFullItemCorrelation2() throws Exception
{    DataModel dataModel = getDataModel(new long[] { 1, 2 }, new Double[][] { { 3.0, 3.0 }, { 3.0, 3.0 } });    double correlation = new PearsonCorrelationSimilarity(dataModel).itemSimilarity(0, 1);        assertTrue(Double.isNaN(correlation));}
0
public void testNoItemCorrelation1() throws Exception
{    DataModel dataModel = getDataModel(new long[] { 1, 2 }, new Double[][] { { 3.0, -3.0 }, { 2.0, -2.0 } });    double correlation = new PearsonCorrelationSimilarity(dataModel).itemSimilarity(0, 1);    assertCorrelationEquals(-1.0, correlation);}
0
public void testNoItemCorrelation2() throws Exception
{    DataModel dataModel = getDataModel(new long[] { 1, 2 }, new Double[][] { { null, 1.0, null }, { null, null, 1.0 } });    double correlation = new PearsonCorrelationSimilarity(dataModel).itemSimilarity(1, 2);    assertTrue(Double.isNaN(correlation));}
0
public void testNoItemCorrelation3() throws Exception
{    DataModel dataModel = getDataModel(new long[] { 1, 2, 3 }, new Double[][] { { 90.0, 70.0 }, { 80.0, 80.0 }, { 70.0, 90.0 } });    double correlation = new PearsonCorrelationSimilarity(dataModel).itemSimilarity(0, 1);    assertCorrelationEquals(-1.0, correlation);}
0
public void testSimpleItem() throws Exception
{    DataModel dataModel = getDataModel(new long[] { 1, 2, 3 }, new Double[][] { { 1.0, 2.0 }, { 2.0, 5.0 }, { 3.0, 6.0 } });    double correlation = new PearsonCorrelationSimilarity(dataModel).itemSimilarity(0, 1);    assertCorrelationEquals(0.9607689228305227, correlation);}
0
public void testSimpleItemWeighted() throws Exception
{    DataModel dataModel = getDataModel(new long[] { 1, 2, 3 }, new Double[][] { { 1.0, 2.0 }, { 2.0, 5.0 }, { 3.0, 6.0 } });    ItemSimilarity itemSimilarity = new PearsonCorrelationSimilarity(dataModel, Weighting.WEIGHTED);    double correlation = itemSimilarity.itemSimilarity(0, 1);    assertCorrelationEquals(0.9901922307076306, correlation);}
0
public void testRefresh() throws Exception
{        new PearsonCorrelationSimilarity(getDataModel()).refresh(null);}
0
public void testInferrer() throws Exception
{    DataModel dataModel = getDataModel(new long[] { 1, 2 }, new Double[][] { { null, 1.0, 2.0, null, null, 6.0 }, { 1.0, 8.0, null, 3.0, 4.0, null } });    UserSimilarity similarity = new PearsonCorrelationSimilarity(dataModel);    similarity.setPreferenceInferrer(new PreferenceInferrer() {        @Override        public float inferPreference(long userID, long itemID) {            return 1.0f;        }        @Override        public void refresh(Collection<Refreshable> alreadyRefreshed) {        }    });    assertEquals(-0.435285750066007, similarity.userSimilarity(1L, 2L), EPSILON);}
0
public float inferPreference(long userID, long itemID)
{    return 1.0f;}
0
public void refresh(Collection<Refreshable> alreadyRefreshed)
{}
0
public void lessItemsThanBatchSize() throws Exception
{    FastByIDMap<PreferenceArray> userData = new FastByIDMap<>();    userData.put(1, new GenericUserPreferenceArray(Arrays.asList(new GenericPreference(1, 1, 1), new GenericPreference(1, 2, 1), new GenericPreference(1, 3, 1))));    userData.put(2, new GenericUserPreferenceArray(Arrays.asList(new GenericPreference(2, 1, 1), new GenericPreference(2, 2, 1), new GenericPreference(2, 4, 1))));    DataModel dataModel = new GenericDataModel(userData);    ItemBasedRecommender recommender = new GenericItemBasedRecommender(dataModel, new TanimotoCoefficientSimilarity(dataModel));    BatchItemSimilarities batchSimilarities = new MultithreadedBatchItemSimilarities(recommender, 10);    batchSimilarities.computeItemSimilarities(1, 1, mock(SimilarItemsWriter.class));}
0
public void higherDegreeOfParallelismThanBatches() throws Exception
{    FastByIDMap<PreferenceArray> userData = new FastByIDMap<>();    userData.put(1, new GenericUserPreferenceArray(Arrays.asList(new GenericPreference(1, 1, 1), new GenericPreference(1, 2, 1), new GenericPreference(1, 3, 1))));    userData.put(2, new GenericUserPreferenceArray(Arrays.asList(new GenericPreference(2, 1, 1), new GenericPreference(2, 2, 1), new GenericPreference(2, 4, 1))));    DataModel dataModel = new GenericDataModel(userData);    ItemBasedRecommender recommender = new GenericItemBasedRecommender(dataModel, new TanimotoCoefficientSimilarity(dataModel));    BatchItemSimilarities batchSimilarities = new MultithreadedBatchItemSimilarities(recommender, 10);        batchSimilarities.computeItemSimilarities(2, 1, mock(SimilarItemsWriter.class));    fail();}
0
public void testCorrectNumberOfOutputSimilarities() throws Exception
{    FastByIDMap<PreferenceArray> userData = new FastByIDMap<>();    userData.put(1, new GenericUserPreferenceArray(Arrays.asList(new GenericPreference(1, 1, 1), new GenericPreference(1, 2, 1), new GenericPreference(1, 3, 1))));    userData.put(2, new GenericUserPreferenceArray(Arrays.asList(new GenericPreference(2, 1, 1), new GenericPreference(2, 2, 1), new GenericPreference(2, 4, 1))));    DataModel dataModel = new GenericDataModel(userData);    ItemBasedRecommender recommender = new GenericItemBasedRecommender(dataModel, new TanimotoCoefficientSimilarity(dataModel));    BatchItemSimilarities batchSimilarities = new MultithreadedBatchItemSimilarities(recommender, 10, 2);    int numOutputSimilarities = batchSimilarities.computeItemSimilarities(2, 1, mock(SimilarItemsWriter.class));    assertEquals(numOutputSimilarities, 10);}
0
 static void assertCorrelationEquals(double expected, double actual)
{    if (Double.isNaN(expected)) {        assertTrue("Correlation is not NaN", Double.isNaN(actual));    } else {        assertTrue("Correlation is NaN", !Double.isNaN(actual));        assertTrue("Correlation > 1.0", actual <= 1.0);        assertTrue("Correlation < -1.0", actual >= -1.0);        assertEquals(expected, actual, EPSILON);    }}
0
public void testFullCorrelation1() throws Exception
{    DataModel dataModel = getDataModel(new long[] { 1, 2 }, new Double[][] { { 1.0, 2.0, 3.0 }, { 1.0, 2.0, 3.0 } });    double correlation = new SpearmanCorrelationSimilarity(dataModel).userSimilarity(1, 2);    assertCorrelationEquals(1.0, correlation);}
0
public void testFullCorrelation2() throws Exception
{    DataModel dataModel = getDataModel(new long[] { 1, 2 }, new Double[][] { { 1.0, 2.0, 3.0 }, { 4.0, 5.0, 6.0 } });    double correlation = new SpearmanCorrelationSimilarity(dataModel).userSimilarity(1, 2);    assertCorrelationEquals(1.0, correlation);}
0
public void testAnticorrelation() throws Exception
{    DataModel dataModel = getDataModel(new long[] { 1, 2 }, new Double[][] { { 1.0, 2.0, 3.0 }, { 3.0, 2.0, 1.0 } });    double correlation = new SpearmanCorrelationSimilarity(dataModel).userSimilarity(1, 2);    assertCorrelationEquals(-1.0, correlation);}
0
public void testSimple() throws Exception
{    DataModel dataModel = getDataModel(new long[] { 1, 2 }, new Double[][] { { 1.0, 2.0, 3.0 }, { 2.0, 3.0, 1.0 } });    double correlation = new SpearmanCorrelationSimilarity(dataModel).userSimilarity(1, 2);    assertCorrelationEquals(-0.5, correlation);}
0
public void testRefresh()
{        new SpearmanCorrelationSimilarity(getDataModel()).refresh(null);}
0
public void testNoCorrelation() throws Exception
{    DataModel dataModel = getDataModel(new long[] { 1, 2 }, new Double[][] { { null, 2.0, 3.0 }, { 1.0 } });    double correlation = new TanimotoCoefficientSimilarity(dataModel).userSimilarity(1, 2);    assertCorrelationEquals(Double.NaN, correlation);}
0
public void testFullCorrelation1() throws Exception
{    DataModel dataModel = getDataModel(new long[] { 1, 2 }, new Double[][] { { 1.0 }, { 1.0 } });    double correlation = new TanimotoCoefficientSimilarity(dataModel).userSimilarity(1, 2);    assertCorrelationEquals(1.0, correlation);}
0
public void testFullCorrelation2() throws Exception
{    DataModel dataModel = getDataModel(new long[] { 1, 2 }, new Double[][] { { 1.0, 2.0, 3.0 }, { 1.0 } });    double correlation = new TanimotoCoefficientSimilarity(dataModel).userSimilarity(1, 2);    assertCorrelationEquals(0.3333333333333333, correlation);}
0
public void testCorrelation1() throws Exception
{    DataModel dataModel = getDataModel(new long[] { 1, 2 }, new Double[][] { { null, 2.0, 3.0 }, { 1.0, 1.0 } });    double correlation = new TanimotoCoefficientSimilarity(dataModel).userSimilarity(1, 2);    assertEquals(0.3333333333333333, correlation, EPSILON);}
0
public void testCorrelation2() throws Exception
{    DataModel dataModel = getDataModel(new long[] { 1, 2 }, new Double[][] { { null, 2.0, 3.0, 1.0 }, { 1.0, 1.0, null, 0.0 } });    double correlation = new TanimotoCoefficientSimilarity(dataModel).userSimilarity(1, 2);    assertEquals(0.5, correlation, EPSILON);}
0
public void testRefresh()
{        new TanimotoCoefficientSimilarity(getDataModel()).refresh(null);}
0
public void testReturnNaNDoubleWhenNoSimilaritiesForTwoItems() throws Exception
{    DataModel dataModel = getDataModel(new long[] { 1, 2 }, new Double[][] { { null, null, 3.0 }, { 1.0, 1.0, null } });    Double similarity = new TanimotoCoefficientSimilarity(dataModel).itemSimilarity(1, 2);    assertEquals(Double.NaN, similarity, EPSILON);}
0
public void testItemsSimilarities() throws Exception
{    DataModel dataModel = getDataModel(new long[] { 1, 2 }, new Double[][] { { 2.0, null, 2.0 }, { 1.0, 1.0, 1.0 } });    TanimotoCoefficientSimilarity tCS = new TanimotoCoefficientSimilarity(dataModel);    assertEquals(0.5, tCS.itemSimilarity(0, 1), EPSILON);    assertEquals(1, tCS.itemSimilarity(0, 2), EPSILON);    double[] similarities = tCS.itemSimilarities(0, new long[] { 1, 2 });    assertEquals(0.5, similarities[0], EPSILON);    assertEquals(1, similarities[1], EPSILON);}
0
public static DataModel getDataModel(long[] userIDs, Double[][] prefValues)
{    FastByIDMap<PreferenceArray> result = new FastByIDMap<>();    for (int i = 0; i < userIDs.length; i++) {        List<Preference> prefsList = Lists.newArrayList();        for (int j = 0; j < prefValues[i].length; j++) {            if (prefValues[i][j] != null) {                prefsList.add(new GenericPreference(userIDs[i], j, prefValues[i][j].floatValue()));            }        }        if (!prefsList.isEmpty()) {            result.put(userIDs[i], new GenericUserPreferenceArray(prefsList));        }    }    return new GenericDataModel(result);}
0
public static DataModel getBooleanDataModel(long[] userIDs, boolean[][] prefs)
{    FastByIDMap<FastIDSet> result = new FastByIDMap<>();    for (int i = 0; i < userIDs.length; i++) {        FastIDSet prefsSet = new FastIDSet();        for (int j = 0; j < prefs[i].length; j++) {            if (prefs[i][j]) {                prefsSet.add(j);            }        }        if (!prefsSet.isEmpty()) {            result.put(userIDs[i], prefsSet);        }    }    return new GenericBooleanPrefDataModel(result);}
0
protected static DataModel getDataModel()
{    return getDataModel(new long[] { 1, 2, 3, 4 }, new Double[][] { { 0.1, 0.3 }, { 0.2, 0.3, 0.3 }, { 0.4, 0.3, 0.5 }, { 0.7, 0.3, 0.8 } });}
0
protected static DataModel getBooleanDataModel()
{    return getBooleanDataModel(new long[] { 1, 2, 3, 4 }, new boolean[][] { { false, true, false }, { false, true, true, false }, { true, false, false, true }, { true, false, true, true } });}
0
protected static boolean arrayContains(long[] array, long value)
{    for (long l : array) {        if (l == value) {            return true;        }    }    return false;}
0
public void testIterator()
{    List<RecommendedItem> recommendedItems = new ArrayList<>();    for (long itemId = 2; itemId < 10; itemId++) {        recommendedItems.add(new GenericRecommendedItem(itemId, itemId));    }    SimilarItems similarItems = new SimilarItems(1, recommendedItems);    assertThat(similarItems.getSimilarItems(), Matchers.<SimilarItem>iterableWithSize(recommendedItems.size()));    int byHandIndex = 0;    for (SimilarItem simItem : similarItems.getSimilarItems()) {        RecommendedItem recItem = recommendedItems.get(byHandIndex++);        assertEquals(simItem.getItemID(), recItem.getItemID());        assertEquals(simItem.getSimilarity(), recItem.getValue(), EPSILON);    }}
0
public void testBuild()
{    ConfusionMatrix confusionMatrix = fillConfusionMatrix(VALUES, LABELS, DEFAULT_LABEL);    checkValues(confusionMatrix);    checkAccuracy(confusionMatrix);}
0
public void testGetMatrix()
{    ConfusionMatrix confusionMatrix = fillConfusionMatrix(VALUES, LABELS, DEFAULT_LABEL);    Matrix m = confusionMatrix.getMatrix();    Map<String, Integer> rowLabels = m.getRowLabelBindings();    assertEquals(confusionMatrix.getLabels().size(), m.numCols());    assertTrue(rowLabels.keySet().contains(LABELS[0]));    assertTrue(rowLabels.keySet().contains(LABELS[1]));    assertTrue(rowLabels.keySet().contains(DEFAULT_LABEL));    assertEquals(2, confusionMatrix.getCorrect(LABELS[0]));    assertEquals(20, confusionMatrix.getCorrect(LABELS[1]));    assertEquals(0, confusionMatrix.getCorrect(DEFAULT_LABEL));}
0
public void testPrecisionRecallAndF1ScoreAsScikitLearn()
{    Collection<String> labelList = Arrays.asList("0", "1", "2");    ConfusionMatrix confusionMatrix = new ConfusionMatrix(labelList, "DEFAULT");    confusionMatrix.putCount("0", "0", 2);    confusionMatrix.putCount("1", "0", 1);    confusionMatrix.putCount("1", "2", 1);    confusionMatrix.putCount("2", "1", 2);    double delta = 0.001;    assertEquals(0.222, confusionMatrix.getWeightedPrecision(), delta);    assertEquals(0.333, confusionMatrix.getWeightedRecall(), delta);    assertEquals(0.266, confusionMatrix.getWeightedF1score(), delta);}
0
private static void checkValues(ConfusionMatrix cm)
{    int[][] counts = cm.getConfusionMatrix();    cm.toString();    assertEquals(counts.length, counts[0].length);    assertEquals(3, counts.length);    assertEquals(VALUES[0][0], counts[0][0]);    assertEquals(VALUES[0][1], counts[0][1]);    assertEquals(VALUES[1][0], counts[1][0]);    assertEquals(VALUES[1][1], counts[1][1]);        assertTrue(Arrays.equals(new int[3], counts[2]));    assertEquals(OTHER[0], counts[0][2]);    assertEquals(OTHER[1], counts[1][2]);    assertEquals(3, cm.getLabels().size());    assertTrue(cm.getLabels().contains(LABELS[0]));    assertTrue(cm.getLabels().contains(LABELS[1]));    assertTrue(cm.getLabels().contains(DEFAULT_LABEL));}
0
private static void checkAccuracy(ConfusionMatrix cm)
{    Collection<String> labelstrs = cm.getLabels();    assertEquals(3, labelstrs.size());    assertEquals(25.0, cm.getAccuracy("Label1"), EPSILON);    assertEquals(55.5555555, cm.getAccuracy("Label2"), EPSILON);    assertTrue(Double.isNaN(cm.getAccuracy("other")));}
0
private static ConfusionMatrix fillConfusionMatrix(int[][] values, String[] labels, String defaultLabel)
{    Collection<String> labelList = Lists.newArrayList();    labelList.add(labels[0]);    labelList.add(labels[1]);    ConfusionMatrix confusionMatrix = new ConfusionMatrix(labelList, defaultLabel);    confusionMatrix.putCount("Label1", "Label1", values[0][0]);    confusionMatrix.putCount("Label1", "Label2", values[0][1]);    confusionMatrix.putCount("Label2", "Label1", values[1][0]);    confusionMatrix.putCount("Label2", "Label2", values[1][1]);    confusionMatrix.putCount("Label1", DEFAULT_LABEL, OTHER[0]);    confusionMatrix.putCount("Label2", DEFAULT_LABEL, OTHER[1]);    return confusionMatrix;}
0
public void testRandomAttributes() throws Exception
{    Random rng = RandomUtils.getRandom();    int nbAttributes = rng.nextInt(100) + 1;    boolean[] selected = new boolean[nbAttributes];    for (int nloop = 0; nloop < 100; nloop++) {        Arrays.fill(selected, false);                int nbSelected = rng.nextInt(nbAttributes - 1);        for (int index = 0; index < nbSelected; index++) {            int attr;            do {                attr = rng.nextInt(nbAttributes);            } while (selected[attr]);            selected[attr] = true;        }        int m = rng.nextInt(nbAttributes);        Method randomAttributes = DecisionTreeBuilder.class.getDeclaredMethod("randomAttributes", Random.class, boolean[].class, int.class);        randomAttributes.setAccessible(true);        int[] attrs = (int[]) randomAttributes.invoke(null, rng, selected, m);        assertNotNull(attrs);        assertEquals(Math.min(m, nbAttributes - nbSelected), attrs.length);        for (int attr : attrs) {                        assertFalse("an attribute has already been selected", selected[attr]);                        assertTrue(attr >= 0);            assertTrue(attr < nbAttributes);                        assertEquals(ArrayUtils.indexOf(attrs, attr), ArrayUtils.lastIndexOf(attrs, attr));        }    }}
0
public void testRandomAttributes() throws Exception
{    Random rng = RandomUtils.getRandom();    int nbAttributes = rng.nextInt(100) + 1;    boolean[] selected = new boolean[nbAttributes];    for (int nloop = 0; nloop < 100; nloop++) {        Arrays.fill(selected, false);                int nbSelected = rng.nextInt(nbAttributes - 1);        for (int index = 0; index < nbSelected; index++) {            int attr;            do {                attr = rng.nextInt(nbAttributes);            } while (selected[attr]);            selected[attr] = true;        }        int m = rng.nextInt(nbAttributes);        int[] attrs = DefaultTreeBuilder.randomAttributes(rng, selected, m);        assertNotNull(attrs);        assertEquals(Math.min(m, nbAttributes - nbSelected), attrs.length);        for (int attr : attrs) {                        assertFalse("an attribute has already been selected", selected[attr]);                        assertTrue(attr >= 0);            assertTrue(attr < nbAttributes);                        assertEquals(ArrayUtils.indexOf(attrs, attr), ArrayUtils.lastIndexOf(attrs, attr));        }    }}
0
public void testBuild() throws Exception
{    Random rng = RandomUtils.getRandom();    String[] source = Utils.double2String(dData);    String descriptor = "N N N N N N N N L";    Dataset dataset = DataLoader.generateDataset(descriptor, false, source);    Data data = DataLoader.loadData(dataset, source);    TreeBuilder builder = new DecisionTreeBuilder();    builder.build(rng, data);        dataset = DataLoader.generateDataset(descriptor, true, source);    data = DataLoader.loadData(dataset, source);    builder = new DecisionTreeBuilder();    builder.build(rng, data);}
0
public void testConvert() throws Exception
{    Random rng = RandomUtils.getRandom();    String descriptor = Utils.randomDescriptor(rng, ATTRIBUTE_COUNT);    double[][] source = Utils.randomDoubles(rng, descriptor, false, INSTANCE_COUNT);    String[] sData = Utils.double2String(source);    Dataset dataset = DataLoader.generateDataset(descriptor, false, sData);    Data data = DataLoader.loadData(dataset, sData);    DataConverter converter = new DataConverter(dataset);    for (int index = 0; index < data.size(); index++) {        assertEquals(data.get(index), converter.convert(sData[index]));    }        source = Utils.randomDoubles(rng, descriptor, true, INSTANCE_COUNT);    sData = Utils.double2String(source);    dataset = DataLoader.generateDataset(descriptor, true, sData);    data = DataLoader.loadData(dataset, sData);    converter = new DataConverter(dataset);    for (int index = 0; index < data.size(); index++) {        assertEquals(data.get(index), converter.convert(sData[index]));    }}
0
public void setUp() throws Exception
{    super.setUp();    rng = RandomUtils.getRandom();}
0
public void testLoadDataWithDescriptor() throws Exception
{    int nbAttributes = 10;    int datasize = 100;        String descriptor = Utils.randomDescriptor(rng, nbAttributes);    Attribute[] attrs = DescriptorUtils.parseDescriptor(descriptor);        double[][] data = Utils.randomDoubles(rng, descriptor, false, datasize);    Collection<Integer> missings = Lists.newArrayList();    String[] sData = prepareData(data, attrs, missings);    Dataset dataset = DataLoader.generateDataset(descriptor, false, sData);    Data loaded = DataLoader.loadData(dataset, sData);    testLoadedData(data, attrs, missings, loaded);    testLoadedDataset(data, attrs, missings, loaded);        data = Utils.randomDoubles(rng, descriptor, true, datasize);    missings = Lists.newArrayList();    sData = prepareData(data, attrs, missings);    dataset = DataLoader.generateDataset(descriptor, true, sData);    loaded = DataLoader.loadData(dataset, sData);    testLoadedData(data, attrs, missings, loaded);    testLoadedDataset(data, attrs, missings, loaded);}
0
public void testGenerateDataset() throws Exception
{    int nbAttributes = 10;    int datasize = 100;        String descriptor = Utils.randomDescriptor(rng, nbAttributes);    Attribute[] attrs = DescriptorUtils.parseDescriptor(descriptor);        double[][] data = Utils.randomDoubles(rng, descriptor, false, datasize);    Collection<Integer> missings = Lists.newArrayList();    String[] sData = prepareData(data, attrs, missings);    Dataset expected = DataLoader.generateDataset(descriptor, false, sData);    Dataset dataset = DataLoader.generateDataset(descriptor, false, sData);    assertEquals(expected, dataset);        data = Utils.randomDoubles(rng, descriptor, true, datasize);    missings = Lists.newArrayList();    sData = prepareData(data, attrs, missings);    expected = DataLoader.generateDataset(descriptor, true, sData);    dataset = DataLoader.generateDataset(descriptor, true, sData);    assertEquals(expected, dataset);}
0
private String[] prepareData(double[][] data, Attribute[] attrs, Collection<Integer> missings)
{    int nbAttributes = attrs.length;    String[] sData = new String[data.length];    for (int index = 0; index < data.length; index++) {        int missingAttr;        if (rng.nextDouble() < 0.0) {                        missings.add(index);                        do {                missingAttr = rng.nextInt(nbAttributes);            } while (attrs[missingAttr].isIgnored());        } else {            missingAttr = -1;        }        StringBuilder builder = new StringBuilder();        for (int attr = 0; attr < nbAttributes; attr++) {            if (attr == missingAttr) {                                builder.append('?').append(',');            } else {                builder.append(data[index][attr]).append(',');            }        }        sData[index] = builder.toString();    }    return sData;}
0
 static void testLoadedData(double[][] data, Attribute[] attrs, Collection<Integer> missings, Data loaded)
{    int nbAttributes = attrs.length;        assertEquals("number of instance", data.length - missings.size(), loaded.size());        int lind = 0;    for (int index = 0; index < data.length; index++) {        if (missings.contains(index)) {            continue;        }                double[] vector = data[index];        Instance instance = loaded.get(lind);        int aId = 0;        for (int attr = 0; attr < nbAttributes; attr++) {            if (attrs[attr].isIgnored()) {                continue;            }            if (attrs[attr].isNumerical()) {                assertEquals(vector[attr], instance.get(aId), EPSILON);                aId++;            } else if (attrs[attr].isCategorical()) {                checkCategorical(data, missings, loaded, attr, aId, vector[attr], instance.get(aId));                aId++;            } else if (attrs[attr].isLabel()) {                if (loaded.getDataset().isNumerical(aId)) {                    assertEquals(vector[attr], instance.get(aId), EPSILON);                } else {                    checkCategorical(data, missings, loaded, attr, aId, vector[attr], instance.get(aId));                }                aId++;            }        }        lind++;    }}
0
 static void testLoadedDataset(double[][] data, Attribute[] attrs, Collection<Integer> missings, Data loaded)
{    int nbAttributes = attrs.length;    int iId = 0;    for (int index = 0; index < data.length; index++) {        if (missings.contains(index)) {            continue;        }        Instance instance = loaded.get(iId++);        int aId = 0;        for (int attr = 0; attr < nbAttributes; attr++) {            if (attrs[attr].isIgnored()) {                continue;            }            if (attrs[attr].isLabel()) {                if (!loaded.getDataset().isNumerical(aId)) {                    double nValue = instance.get(aId);                    String oValue = Double.toString(data[index][attr]);                    assertEquals(loaded.getDataset().valueOf(aId, oValue), nValue, EPSILON);                }            } else {                assertEquals(attrs[attr].isNumerical(), loaded.getDataset().isNumerical(aId));                if (attrs[attr].isCategorical()) {                    double nValue = instance.get(aId);                    String oValue = Double.toString(data[index][attr]);                    assertEquals(loaded.getDataset().valueOf(aId, oValue), nValue, EPSILON);                }            }            aId++;        }    }}
0
public void testLoadDataFromFile() throws Exception
{    int nbAttributes = 10;    int datasize = 100;        String descriptor = Utils.randomDescriptor(rng, nbAttributes);    Attribute[] attrs = DescriptorUtils.parseDescriptor(descriptor);        double[][] source = Utils.randomDoubles(rng, descriptor, false, datasize);    Collection<Integer> missings = Lists.newArrayList();    String[] sData = prepareData(source, attrs, missings);    Dataset dataset = DataLoader.generateDataset(descriptor, false, sData);    Path dataPath = Utils.writeDataToTestFile(sData);    FileSystem fs = dataPath.getFileSystem(getConfiguration());    Data loaded = DataLoader.loadData(dataset, fs, dataPath);    testLoadedData(source, attrs, missings, loaded);        source = Utils.randomDoubles(rng, descriptor, true, datasize);    missings = Lists.newArrayList();    sData = prepareData(source, attrs, missings);    dataset = DataLoader.generateDataset(descriptor, true, sData);    dataPath = Utils.writeDataToTestFile(sData);    fs = dataPath.getFileSystem(getConfiguration());    loaded = DataLoader.loadData(dataset, fs, dataPath);    testLoadedData(source, attrs, missings, loaded);}
0
public void testGenerateDatasetFromFile() throws Exception
{    int nbAttributes = 10;    int datasize = 100;        String descriptor = Utils.randomDescriptor(rng, nbAttributes);    Attribute[] attrs = DescriptorUtils.parseDescriptor(descriptor);        double[][] source = Utils.randomDoubles(rng, descriptor, false, datasize);    Collection<Integer> missings = Lists.newArrayList();    String[] sData = prepareData(source, attrs, missings);    Dataset expected = DataLoader.generateDataset(descriptor, false, sData);    Path path = Utils.writeDataToTestFile(sData);    FileSystem fs = path.getFileSystem(getConfiguration());    Dataset dataset = DataLoader.generateDataset(descriptor, false, fs, path);    assertEquals(expected, dataset);        source = Utils.randomDoubles(rng, descriptor, false, datasize);    missings = Lists.newArrayList();    sData = prepareData(source, attrs, missings);    expected = DataLoader.generateDataset(descriptor, false, sData);    path = Utils.writeDataToTestFile(sData);    fs = path.getFileSystem(getConfiguration());    dataset = DataLoader.generateDataset(descriptor, false, fs, path);    assertEquals(expected, dataset);}
0
 static void checkCategorical(double[][] source, Collection<Integer> missings, Data loaded, int attr, int aId, double oValue, double nValue)
{    int lind = 0;    for (int index = 0; index < source.length; index++) {        if (missings.contains(index)) {            continue;        }        if (source[index][attr] == oValue) {            assertEquals(nValue, loaded.get(lind).get(aId), EPSILON);        } else {            assertFalse(nValue == loaded.get(lind).get(aId));        }        lind++;    }}
0
public void jsonEncoding() throws DescriptorException
{    Dataset to = DataLoader.generateDataset("N C I L", true, new String[] { "1 foo 2 3", "4 bar 5 6" });            assertEquals(3, to.nbAttributes());    assertEquals(1, to.getIgnored().length);    assertEquals(2, to.getIgnored()[0]);    assertEquals(2, to.getLabelId());    assertTrue(to.isNumerical(0));        Dataset fromJson = Dataset.fromJSON(to.toJSON());    assertEquals(3, fromJson.nbAttributes());    assertEquals(1, fromJson.getIgnored().length);    assertEquals(2, fromJson.getIgnored()[0]);    assertTrue(fromJson.isNumerical(0));        assertNotEquals(fromJson.valueOf(1, "bar"), fromJson.valueOf(1, "foo"));}
0
public void jsonEncodingIgnoreFeatures() throws DescriptorException
{    ;    Dataset to = DataLoader.generateDataset("N C I L", false, new String[] { "1 foo 2 Red", "4 bar 5 Blue" });            assertEquals(3, to.nbAttributes());    assertEquals(1, to.getIgnored().length);    assertEquals(2, to.getIgnored()[0]);    assertEquals(2, to.getLabelId());    assertTrue(to.isNumerical(0));    assertNotEquals(to.valueOf(1, "bar"), to.valueOf(1, "foo"));    assertNotEquals(to.valueOf(2, "Red"), to.valueOf(2, "Blue"));        Dataset fromJson = Dataset.fromJSON(to.toJSON());    assertEquals(3, fromJson.nbAttributes());    assertEquals(1, fromJson.getIgnored().length);    assertEquals(2, fromJson.getIgnored()[0]);    assertTrue(fromJson.isNumerical(0));        assertNotEquals(fromJson.valueOf(1, "bar"), fromJson.valueOf(1, "foo"));    assertNotEquals(fromJson.valueOf(2, "Red"), fromJson.valueOf(2, "Blue"));}
0
public void setUp() throws Exception
{    super.setUp();    rng = RandomUtils.getRandom();    classifierData = Utils.randomData(rng, ATTRIBUTE_COUNT, false, DATA_SIZE);    regressionData = Utils.randomData(rng, ATTRIBUTE_COUNT, true, DATA_SIZE);}
0
public void testSubset()
{    int n = 10;    for (int nloop = 0; nloop < n; nloop++) {        int attr = rng.nextInt(classifierData.getDataset().nbAttributes());        double[] values = classifierData.values(attr);        double value = values[rng.nextInt(values.length)];        Data eSubset = classifierData.subset(Condition.equals(attr, value));        Data lSubset = classifierData.subset(Condition.lesser(attr, value));        Data gSubset = classifierData.subset(Condition.greaterOrEquals(attr, value));        for (int index = 0; index < DATA_SIZE; index++) {            Instance instance = classifierData.get(index);            if (instance.get(attr) < value) {                assertTrue(lSubset.contains(instance));                assertFalse(eSubset.contains(instance));                assertFalse(gSubset.contains(instance));            } else if (instance.get(attr) == value) {                assertFalse(lSubset.contains(instance));                assertTrue(eSubset.contains(instance));                assertTrue(gSubset.contains(instance));            } else {                assertFalse(lSubset.contains(instance));                assertFalse(eSubset.contains(instance));                assertTrue(gSubset.contains(instance));            }        }                attr = rng.nextInt(regressionData.getDataset().nbAttributes());        values = regressionData.values(attr);        value = values[rng.nextInt(values.length)];        eSubset = regressionData.subset(Condition.equals(attr, value));        lSubset = regressionData.subset(Condition.lesser(attr, value));        gSubset = regressionData.subset(Condition.greaterOrEquals(attr, value));        for (int index = 0; index < DATA_SIZE; index++) {            Instance instance = regressionData.get(index);            if (instance.get(attr) < value) {                assertTrue(lSubset.contains(instance));                assertFalse(eSubset.contains(instance));                assertFalse(gSubset.contains(instance));            } else if (instance.get(attr) == value) {                assertFalse(lSubset.contains(instance));                assertTrue(eSubset.contains(instance));                assertTrue(gSubset.contains(instance));            } else {                assertFalse(lSubset.contains(instance));                assertFalse(eSubset.contains(instance));                assertTrue(gSubset.contains(instance));            }        }    }}
0
public void testValues() throws Exception
{    for (int attr = 0; attr < classifierData.getDataset().nbAttributes(); attr++) {        double[] values = classifierData.values(attr);                for (int index = 0; index < DATA_SIZE; index++) {            assertEquals(1, count(values, classifierData.get(index).get(attr)));        }    }    for (int attr = 0; attr < regressionData.getDataset().nbAttributes(); attr++) {        double[] values = regressionData.values(attr);                for (int index = 0; index < DATA_SIZE; index++) {            assertEquals(1, count(values, regressionData.get(index).get(attr)));        }    }}
0
private static int count(double[] values, double value)
{    int count = 0;    for (double v : values) {        if (v == value) {            count++;        }    }    return count;}
0
public void testIdenticalTrue() throws Exception
{        Dataset dataset = Utils.randomData(rng, ATTRIBUTE_COUNT, false, 1).getDataset();        Data empty = new Data(dataset);    assertTrue(empty.isIdentical());        Data identical = Utils.randomData(rng, ATTRIBUTE_COUNT, false, DATA_SIZE);    Instance model = identical.get(0);    for (int index = 1; index < DATA_SIZE; index++) {        for (int attr = 0; attr < identical.getDataset().nbAttributes(); attr++) {            identical.get(index).set(attr, model.get(attr));        }    }    assertTrue(identical.isIdentical());}
0
public void testIdenticalFalse() throws Exception
{    int n = 10;    for (int nloop = 0; nloop < n; nloop++) {        Data data = Utils.randomData(rng, ATTRIBUTE_COUNT, false, DATA_SIZE);                int index = rng.nextInt(DATA_SIZE);        Instance instance = data.get(index);                int attr = rng.nextInt(data.getDataset().nbAttributes());        instance.set(attr, instance.get(attr) + 1);        assertFalse(data.isIdentical());    }}
0
public void testIdenticalLabelTrue() throws Exception
{        Dataset dataset = Utils.randomData(rng, ATTRIBUTE_COUNT, false, 1).getDataset();        Data empty = new Data(dataset);    assertTrue(empty.identicalLabel());        String descriptor = Utils.randomDescriptor(rng, ATTRIBUTE_COUNT);    double[][] source = Utils.randomDoublesWithSameLabel(rng, descriptor, false, DATA_SIZE, rng.nextInt());    String[] sData = Utils.double2String(source);    dataset = DataLoader.generateDataset(descriptor, false, sData);    Data data = DataLoader.loadData(dataset, sData);    assertTrue(data.identicalLabel());}
0
public void testIdenticalLabelFalse() throws Exception
{    int n = 10;    for (int nloop = 0; nloop < n; nloop++) {        String descriptor = Utils.randomDescriptor(rng, ATTRIBUTE_COUNT);        int label = Utils.findLabel(descriptor);        double[][] source = Utils.randomDoublesWithSameLabel(rng, descriptor, false, DATA_SIZE, rng.nextInt());                int index = rng.nextInt(DATA_SIZE);        source[index][label]++;        String[] sData = Utils.double2String(source);        Dataset dataset = DataLoader.generateDataset(descriptor, false, sData);        Data data = DataLoader.loadData(dataset, sData);        assertFalse(data.identicalLabel());    }}
0
public void testBagging()
{    Data bag = classifierData.bagging(rng);        assertEquals(classifierData.size(), bag.size());        boolean found = false;    for (int index = 0; index < classifierData.size() && !found; index++) {        found = !bag.contains(classifierData.get(index));    }    assertTrue("some instances from data should not be in the bag", found);        bag = regressionData.bagging(rng);        assertEquals(regressionData.size(), bag.size());        found = false;    for (int index = 0; index < regressionData.size() && !found; index++) {        found = !bag.contains(regressionData.get(index));    }    assertTrue("some instances from data should not be in the bag", found);}
0
public void testRsplit()
{        Data source = classifierData.clone();    Data subset = source.rsplit(rng, 0);    assertTrue("subset should be empty", subset.isEmpty());    assertEquals("source.size is incorrect", DATA_SIZE, source.size());        source = classifierData.clone();    subset = source.rsplit(rng, DATA_SIZE);    assertEquals("subset.size is incorrect", DATA_SIZE, subset.size());    assertTrue("source should be empty", source.isEmpty());        int subsize = rng.nextInt(DATA_SIZE);    source = classifierData.clone();    subset = source.rsplit(rng, subsize);    assertEquals("subset.size is incorrect", subsize, subset.size());    assertEquals("source.size is incorrect", DATA_SIZE - subsize, source.size());            source = regressionData.clone();    subset = source.rsplit(rng, 0);    assertTrue("subset should be empty", subset.isEmpty());    assertEquals("source.size is incorrect", DATA_SIZE, source.size());        source = regressionData.clone();    subset = source.rsplit(rng, DATA_SIZE);    assertEquals("subset.size is incorrect", DATA_SIZE, subset.size());    assertTrue("source should be empty", source.isEmpty());        subsize = rng.nextInt(DATA_SIZE);    source = regressionData.clone();    subset = source.rsplit(rng, subsize);    assertEquals("subset.size is incorrect", subsize, subset.size());    assertEquals("source.size is incorrect", DATA_SIZE - subsize, source.size());}
0
public void testCountLabel() throws Exception
{    Dataset dataset = classifierData.getDataset();    int[] counts = new int[dataset.nblabels()];    int n = 10;    for (int nloop = 0; nloop < n; nloop++) {        Arrays.fill(counts, 0);        classifierData.countLabels(counts);        for (int index = 0; index < classifierData.size(); index++) {            counts[(int) dataset.getLabel(classifierData.get(index))]--;        }        for (int label = 0; label < classifierData.getDataset().nblabels(); label++) {            assertEquals("Wrong label 'equals' count", 0, counts[0]);        }    }}
0
public void testMajorityLabel() throws Exception
{        String descriptor = Utils.randomDescriptor(rng, ATTRIBUTE_COUNT);    int label = Utils.findLabel(descriptor);    int label1 = rng.nextInt();    double[][] source = Utils.randomDoublesWithSameLabel(rng, descriptor, false, 100, label1);    String[] sData = Utils.double2String(source);    Dataset dataset = DataLoader.generateDataset(descriptor, false, sData);    Data data = DataLoader.loadData(dataset, sData);    int code1 = dataset.labelCode(Double.toString(label1));    assertEquals(code1, data.majorityLabel(rng));        int label2 = label1 + 1;    int nblabel2 = 51;    while (nblabel2 > 0) {        double[] vector = source[rng.nextInt(100)];        if (vector[label] != label2) {            vector[label] = label2;            nblabel2--;        }    }    sData = Utils.double2String(source);    dataset = DataLoader.generateDataset(descriptor, false, sData);    data = DataLoader.loadData(dataset, sData);    int code2 = dataset.labelCode(Double.toString(label2));        assertEquals(code2, data.majorityLabel(rng));        do {        double[] vector = source[rng.nextInt(100)];        if (vector[label] == label2) {            vector[label] = label1;            break;        }    } while (true);    sData = Utils.double2String(source);    data = DataLoader.loadData(dataset, sData);    code1 = dataset.labelCode(Double.toString(label1));    code2 = dataset.labelCode(Double.toString(label2));        boolean found1 = false;    boolean found2 = false;    for (int index = 0; index < 10 && (!found1 || !found2); index++) {        int major = data.majorityLabel(rng);        if (major == code1) {            found1 = true;        }        if (major == code2) {            found2 = true;        }    }    assertTrue(found1 && found2);}
0
public void testParseDescriptor() throws Exception
{    int n = 10;    int maxnbAttributes = 100;    Random rng = RandomUtils.getRandom();    for (int nloop = 0; nloop < n; nloop++) {        int nbAttributes = rng.nextInt(maxnbAttributes) + 1;        char[] tokens = Utils.randomTokens(rng, nbAttributes);        Attribute[] attrs = DescriptorUtils.parseDescriptor(Utils.generateDescriptor(tokens));                assertEquals("attributes size", nbAttributes, attrs.length);        for (int attr = 0; attr < nbAttributes; attr++) {            switch(tokens[attr]) {                case 'I':                    assertTrue(attrs[attr].isIgnored());                    break;                case 'N':                    assertTrue(attrs[attr].isNumerical());                    break;                case 'C':                    assertTrue(attrs[attr].isCategorical());                    break;                case 'L':                    assertTrue(attrs[attr].isLabel());                    break;            }        }    }}
0
public void testGenerateDescription() throws Exception
{    validate("", "");    validate("I L C C N N N C", "I L C C N N N C");    validate("I L C C N N N C", "I L 2 C 3 N C");    validate("I L C C N N N C", " I L  2 C 3 N C ");    try {        validate("", "I L 2 2 C 2 N C");        fail("2 consecutive multiplicators");    } catch (DescriptorException e) {    }    try {        validate("", "I L 2 C -2 N C");        fail("negative multiplicator");    } catch (DescriptorException e) {    }}
0
private static void validate(String descriptor, CharSequence description) throws DescriptorException
{    assertEquals(descriptor, DescriptorUtils.generateDescriptor(description));}
0
public static char[] randomTokens(Random rng, int nbTokens)
{    char[] result = new char[nbTokens];    for (int token = 0; token < nbTokens; token++) {        double rand = rng.nextDouble();        if (rand < 0.1) {                        result[token] = 'I';        } else if (rand >= 0.5) {            result[token] = 'C';        } else {                        result[token] = 'N';        }        }        result[rng.nextInt(nbTokens)] = 'L';    return result;}
0
public static String generateDescriptor(char[] tokens)
{    StringBuilder builder = new StringBuilder();    for (char token : tokens) {        builder.append(token).append(' ');    }    return builder.toString();}
0
public static String randomDescriptor(Random rng, int nbAttributes)
{    return generateDescriptor(randomTokens(rng, nbAttributes));}
0
public static double[][] randomDoubles(Random rng, CharSequence descriptor, boolean regression, int number) throws DescriptorException
{    Attribute[] attrs = DescriptorUtils.parseDescriptor(descriptor);    double[][] data = new double[number][];    for (int index = 0; index < number; index++) {        data[index] = randomVector(rng, attrs, regression);    }    return data;}
0
public static Data randomData(Random rng, int nbAttributes, boolean regression, int size) throws DescriptorException
{    String descriptor = randomDescriptor(rng, nbAttributes);    double[][] source = randomDoubles(rng, descriptor, regression, size);    String[] sData = double2String(source);    Dataset dataset = DataLoader.generateDataset(descriptor, regression, sData);    return DataLoader.loadData(dataset, sData);}
0
private static double[] randomVector(Random rng, Attribute[] attrs, boolean regression)
{    double[] vector = new double[attrs.length];    for (int attr = 0; attr < attrs.length; attr++) {        if (attrs[attr].isIgnored()) {            vector[attr] = Double.NaN;        } else if (attrs[attr].isNumerical()) {            vector[attr] = rng.nextDouble();        } else if (attrs[attr].isCategorical()) {            vector[attr] = rng.nextInt(CATEGORICAL_RANGE);        } else {                        if (regression) {                vector[attr] = rng.nextDouble();            } else {                vector[attr] = rng.nextInt(CATEGORICAL_RANGE);            }        }    }    return vector;}
0
private static String double2String(double[] v)
{    StringBuilder builder = new StringBuilder();    for (double aV : v) {        builder.append(aV).append(',');    }    return builder.toString();}
0
public static String[] double2String(double[][] source)
{    String[] output = new String[source.length];    for (int index = 0; index < source.length; index++) {        output[index] = double2String(source[index]);    }    return output;}
0
public static double[][] randomDoublesWithSameLabel(Random rng, CharSequence descriptor, boolean regression, int number, int value) throws DescriptorException
{    int label = findLabel(descriptor);    double[][] source = randomDoubles(rng, descriptor, regression, number);    for (int index = 0; index < number; index++) {        source[index][label] = value;    }    return source;}
0
public static int findLabel(CharSequence descriptor) throws DescriptorException
{    Attribute[] attrs = DescriptorUtils.parseDescriptor(descriptor);    return ArrayUtils.indexOf(attrs, Attribute.LABEL);}
0
private static void writeDataToFile(String[] sData, Path path) throws IOException
{    BufferedWriter output = null;    try {        output = Files.newWriter(new File(path.toString()), Charsets.UTF_8);        for (String line : sData) {            output.write(line);            output.write('\n');        }    } finally {        Closeables.close(output, false);    }}
0
public static Path writeDataToTestFile(String[] sData) throws IOException
{    Path testData = new Path("testdata/Data");    MahoutTestCase ca = new MahoutTestCase();    FileSystem fs = testData.getFileSystem(ca.getConfiguration());    if (!fs.exists(testData)) {        fs.mkdirs(testData);    }    Path path = new Path(testData, "DataLoaderTest.data");    writeDataToFile(sData, path);    return path;}
0
public static String[][] splitData(String[] sData, int numMaps)
{    int nbInstances = sData.length;    int partitionSize = nbInstances / numMaps;    String[][] splits = new String[numMaps][];    for (int partition = 0; partition < numMaps; partition++) {        int from = partition * partitionSize;        int to = partition == (numMaps - 1) ? nbInstances : (partition + 1) * partitionSize;        splits[partition] = Arrays.copyOfRange(sData, from, to);    }    return splits;}
0
public void setUp() throws Exception
{    super.setUp();    rng = RandomUtils.getRandom();}
0
private static Data[] generateTrainingDataA() throws DescriptorException
{        Dataset dataset = DataLoader.generateDataset("C N N C L", false, TRAIN_DATA);        Data data = DataLoader.loadData(dataset, TRAIN_DATA);    @SuppressWarnings("unchecked")    List<Instance>[] instances = new List[3];    for (int i = 0; i < instances.length; i++) {        instances[i] = Lists.newArrayList();    }    for (int i = 0; i < data.size(); i++) {        if (data.get(i).get(0) == 0.0d) {            instances[0].add(data.get(i));        } else {            instances[1].add(data.get(i));        }    }    Data[] datas = new Data[instances.length];    for (int i = 0; i < datas.length; i++) {        datas[i] = new Data(dataset, instances[i]);    }    return datas;}
0
private static Data[] generateTrainingDataB() throws DescriptorException
{        String[] trainData = new String[20];    for (int i = 0; i < trainData.length; i++) {        if (i % 3 == 0) {            trainData[i] = "A," + (40 - i) + ',' + (i + 20);        } else if (i % 3 == 1) {            trainData[i] = "B," + (i + 20) + ',' + (40 - i);        } else {            trainData[i] = "C," + (i + 20) + ',' + (i + 20);        }    }        Dataset dataset = DataLoader.generateDataset("C N L", true, trainData);    Data[] datas = new Data[3];    datas[0] = DataLoader.loadData(dataset, trainData);        trainData = new String[20];    for (int i = 0; i < trainData.length; i++) {        if (i % 2 == 0) {            trainData[i] = "A," + (50 - i) + ',' + (i + 10);        } else {            trainData[i] = "B," + (i + 10) + ',' + (50 - i);        }    }    datas[1] = DataLoader.loadData(dataset, trainData);        trainData = new String[10];    for (int i = 0; i < trainData.length; i++) {        trainData[i] = "A," + (40 - i) + ',' + (i + 20);    }    datas[2] = DataLoader.loadData(dataset, trainData);    return datas;}
0
private DecisionForest buildForest(Data[] datas)
{    List<Node> trees = Lists.newArrayList();    for (Data data : datas) {                DecisionTreeBuilder builder = new DecisionTreeBuilder();        builder.setM(data.getDataset().nbAttributes() - 1);        builder.setMinSplitNum(0);        builder.setComplemented(false);        trees.add(builder.build(rng, data));    }    return new DecisionForest(trees);}
0
public void testClassify() throws DescriptorException
{        Data[] datas = generateTrainingDataA();        DecisionForest forest = buildForest(datas);        Dataset dataset = datas[0].getDataset();    Data testData = DataLoader.loadData(dataset, TEST_DATA);    double noValue = dataset.valueOf(4, "no");    double yesValue = dataset.valueOf(4, "yes");    assertEquals(noValue, forest.classify(testData.getDataset(), rng, testData.get(0)), EPSILON);            assertEquals(noValue, forest.classify(testData.getDataset(), rng, testData.get(2)), EPSILON);}
0
public void testClassifyData() throws DescriptorException
{        Data[] datas = generateTrainingDataA();        DecisionForest forest = buildForest(datas);        Dataset dataset = datas[0].getDataset();    Data testData = DataLoader.loadData(dataset, TEST_DATA);    double[][] predictions = new double[testData.size()][];    forest.classify(testData, predictions);    double noValue = dataset.valueOf(4, "no");    double yesValue = dataset.valueOf(4, "yes");    assertArrayEquals(new double[][] { { noValue, Double.NaN, Double.NaN }, { noValue, yesValue, Double.NaN }, { noValue, noValue, Double.NaN } }, predictions);}
0
public void testRegression() throws DescriptorException
{    Data[] datas = generateTrainingDataB();    DecisionForest[] forests = new DecisionForest[datas.length];    for (int i = 0; i < datas.length; i++) {        Data[] subDatas = new Data[datas.length - 1];        int k = 0;        for (int j = 0; j < datas.length; j++) {            if (j != i) {                subDatas[k] = datas[j];                k++;            }        }        forests[i] = buildForest(subDatas);    }    double[][] predictions = new double[datas[0].size()][];    forests[0].classify(datas[0], predictions);    assertArrayEquals(new double[] { 20.0, 20.0 }, predictions[0], EPSILON);    assertArrayEquals(new double[] { 39.0, 29.0 }, predictions[1], EPSILON);    assertArrayEquals(new double[] { Double.NaN, 29.0 }, predictions[2], EPSILON);    assertArrayEquals(new double[] { Double.NaN, 23.0 }, predictions[17], EPSILON);    predictions = new double[datas[1].size()][];    forests[1].classify(datas[1], predictions);    assertArrayEquals(new double[] { 30.0, 29.0 }, predictions[19], EPSILON);    predictions = new double[datas[2].size()][];    forests[2].classify(datas[2], predictions);    assertArrayEquals(new double[] { 29.0, 28.0 }, predictions[9], EPSILON);    assertEquals(20.0, forests[0].classify(datas[0].getDataset(), rng, datas[0].get(0)), EPSILON);    assertEquals(34.0, forests[0].classify(datas[0].getDataset(), rng, datas[0].get(1)), EPSILON);    assertEquals(29.0, forests[0].classify(datas[0].getDataset(), rng, datas[0].get(2)), EPSILON);}
0
public void testSplits() throws Exception
{    int n = 1;    int maxNumSplits = 100;    int maxNbTrees = 1000;    Random rng = RandomUtils.getRandom();    for (int nloop = 0; nloop < n; nloop++) {        int numSplits = rng.nextInt(maxNumSplits) + 1;        int nbTrees = rng.nextInt(maxNbTrees) + 1;        Configuration conf = getConfiguration();        Builder.setNbTrees(conf, nbTrees);        InMemInputFormat inputFormat = new InMemInputFormat();        List<InputSplit> splits = inputFormat.getSplits(conf, numSplits);        assertEquals(numSplits, splits.size());        int nbTreesPerSplit = nbTrees / numSplits;        int totalTrees = 0;        int expectedId = 0;        for (int index = 0; index < numSplits; index++) {            assertTrue(splits.get(index) instanceof InMemInputSplit);            InMemInputSplit split = (InMemInputSplit) splits.get(index);            assertEquals(expectedId, split.getFirstId());            if (index < numSplits - 1) {                assertEquals(nbTreesPerSplit, split.getNbTrees());            } else {                assertEquals(nbTrees - totalTrees, split.getNbTrees());            }            totalTrees += split.getNbTrees();            expectedId += split.getNbTrees();        }    }}
0
public void testRecordReader() throws Exception
{    int n = 1;    int maxNumSplits = 100;    int maxNbTrees = 1000;    Random rng = RandomUtils.getRandom();    for (int nloop = 0; nloop < n; nloop++) {        int numSplits = rng.nextInt(maxNumSplits) + 1;        int nbTrees = rng.nextInt(maxNbTrees) + 1;        Configuration conf = getConfiguration();        Builder.setNbTrees(conf, nbTrees);        InMemInputFormat inputFormat = new InMemInputFormat();        List<InputSplit> splits = inputFormat.getSplits(conf, numSplits);        for (int index = 0; index < numSplits; index++) {            InMemInputSplit split = (InMemInputSplit) splits.get(index);            InMemRecordReader reader = new InMemRecordReader(split);            reader.initialize(split, null);            for (int tree = 0; tree < split.getNbTrees(); tree++) {                                assertEquals(tree < split.getNbTrees(), reader.nextKeyValue());                assertEquals(split.getFirstId() + tree, reader.getCurrentKey().get());            }        }    }}
0
public void setUp() throws Exception
{    super.setUp();    rng = RandomUtils.getRandom();    byteOutStream = new ByteArrayOutputStream();    out = new DataOutputStream(byteOutStream);}
0
public void testWritable() throws Exception
{    InMemInputSplit split = new InMemInputSplit(rng.nextInt(), rng.nextInt(1000), rng.nextLong());    split.write(out);    assertEquals(split, readSplit());}
0
public void testNullSeed() throws Exception
{    InMemInputSplit split = new InMemInputSplit(rng.nextInt(), rng.nextInt(1000), null);    split.write(out);    assertEquals(split, readSplit());}
0
private InMemInputSplit readSplit() throws IOException
{    ByteArrayInputStream byteInStream = new ByteArrayInputStream(byteOutStream.toByteArray());    DataInput in = new DataInputStream(byteInStream);    return InMemInputSplit.read(in);}
0
public void testProcessOutput() throws Exception
{    Configuration conf = getConfiguration();    conf.setInt("mapred.map.tasks", NUM_MAPS);    Random rng = RandomUtils.getRandom();        TreeID[] keys = new TreeID[NUM_TREES];    MapredOutput[] values = new MapredOutput[NUM_TREES];    int[] firstIds = new int[NUM_MAPS];    randomKeyValues(rng, keys, values, firstIds);        Path base = getTestTempDirPath("testdata");    FileSystem fs = base.getFileSystem(conf);    Path outputFile = new Path(base, "PartialBuilderTest.seq");    Writer writer = SequenceFile.createWriter(fs, conf, outputFile, TreeID.class, MapredOutput.class);    try {        for (int index = 0; index < NUM_TREES; index++) {            writer.append(keys[index], values[index]);        }    } finally {        Closeables.close(writer, false);    }        TreeID[] newKeys = new TreeID[NUM_TREES];    Node[] newTrees = new Node[NUM_TREES];    PartialBuilder.processOutput(new Job(conf), base, newKeys, newTrees);        for (int tree = 0; tree < NUM_TREES; tree++) {        assertEquals(values[tree].getTree(), newTrees[tree]);    }    assertTrue("keys not equal", Arrays.deepEquals(keys, newKeys));}
0
public void testConfigure()
{    TreeBuilder treeBuilder = new DefaultTreeBuilder();    Path dataPath = new Path("notUsedDataPath");    Path datasetPath = new Path("notUsedDatasetPath");    Long seed = 5L;    new PartialBuilderChecker(treeBuilder, dataPath, datasetPath, seed);}
0
private static void randomKeyValues(Random rng, TreeID[] keys, MapredOutput[] values, int[] firstIds)
{    int index = 0;    int firstId = 0;    Collection<Integer> partitions = Lists.newArrayList();    for (int p = 0; p < NUM_MAPS; p++) {                int partition;        do {            partition = rng.nextInt(NUM_MAPS);        } while (partitions.contains(partition));        partitions.add(partition);        int nbTrees = Step1Mapper.nbTrees(NUM_MAPS, NUM_TREES, partition);        for (int treeId = 0; treeId < nbTrees; treeId++) {            Node tree = new Leaf(rng.nextInt(100));            keys[index] = new TreeID(partition, treeId);            values[index] = new MapredOutput(tree, nextIntArray(rng, NUM_INSTANCES));            index++;        }        firstIds[p] = firstId;        firstId += NUM_INSTANCES;    }}
0
private static int[] nextIntArray(Random rng, int size)
{    int[] array = new int[size];    for (int index = 0; index < size; index++) {        array[index] = rng.nextInt(101) - 1;    }    return array;}
0
protected boolean runJob(Job job) throws IOException
{        Configuration conf = job.getConfiguration();    assertEquals(seed, getRandomSeed(conf));            assertEquals(1, conf.getInt("mapred.map.tasks", -1));    assertEquals(NUM_TREES, getNbTrees(conf));    assertFalse(isOutput(conf));    assertEquals(treeBuilder, getTreeBuilder(conf));    assertEquals(datasetPath, getDistributedCacheFile(conf, 0));    return true;}
0
public void setExpected(Data data)
{    expected = data;}
0
public Node build(Random rng, Data data)
{    for (int index = 0; index < data.size(); index++) {        assertTrue(expected.contains(data.get(index)));    }    return new Leaf(Double.NaN);}
0
public void setValue(final TreeID value)
{    super.setValue(value.clone());}
0
public void testMapper() throws Exception
{    Random rng = RandomUtils.getRandom();        String descriptor = Utils.randomDescriptor(rng, NUM_ATTRIBUTES);    double[][] source = Utils.randomDoubles(rng, descriptor, false, NUM_INSTANCES);    String[] sData = Utils.double2String(source);    Dataset dataset = DataLoader.generateDataset(descriptor, false, sData);    String[][] splits = Utils.splitData(sData, NUM_MAPPERS);    MockTreeBuilder treeBuilder = new MockTreeBuilder();    LongWritable key = new LongWritable();    Text value = new Text();    int treeIndex = 0;    for (int partition = 0; partition < NUM_MAPPERS; partition++) {        String[] split = splits[partition];        treeBuilder.setExpected(DataLoader.loadData(dataset, split));                int mapNbTrees = Step1Mapper.nbTrees(NUM_MAPPERS, NUM_TREES, partition);        Mapper.Context context = EasyMock.createNiceMock(Mapper.Context.class);        Capture<TreeID> capturedKeys = new TreeIDCapture();        context.write(EasyMock.capture(capturedKeys), EasyMock.anyObject());        EasyMock.expectLastCall().anyTimes();        EasyMock.replay(context);        MockStep1Mapper mapper = new MockStep1Mapper(treeBuilder, dataset, null, partition, NUM_MAPPERS, NUM_TREES);                assertEquals(treeIndex, mapper.getFirstTreeId());        for (int index = 0; index < split.length; index++) {            key.set(index);            value.set(split[index]);            mapper.map(key, value, context);        }        mapper.cleanup(context);        EasyMock.verify(context);                assertEquals(mapNbTrees, capturedKeys.getValues().size());                for (TreeID k : capturedKeys.getValues()) {            assertEquals(partition, k.partition());            assertEquals(treeIndex, k.treeId());            treeIndex++;        }    }}
0
public void testTreeID()
{    Random rng = RandomUtils.getRandom();    for (int nloop = 0; nloop < 1000000; nloop++) {        int partition = Math.abs(rng.nextInt());        int treeId = rng.nextInt(TreeID.MAX_TREEID);        TreeID t1 = new TreeID(partition, treeId);        assertEquals(partition, t1.partition());        assertEquals(treeId, t1.treeId());        TreeID t2 = new TreeID();        t2.set(partition, treeId);        assertEquals(partition, t2.partition());        assertEquals(treeId, t2.treeId());    }}
0
public void setUp() throws Exception
{    super.setUp();    rng = RandomUtils.getRandom();    byteOutStream = new ByteArrayOutputStream();    out = new DataOutputStream(byteOutStream);}
0
public void testReadTree() throws Exception
{    Node node1 = new CategoricalNode(rng.nextInt(), new double[] { rng.nextDouble(), rng.nextDouble() }, new Node[] { new Leaf(rng.nextDouble()), new Leaf(rng.nextDouble()) });    Node node2 = new NumericalNode(rng.nextInt(), rng.nextDouble(), new Leaf(rng.nextDouble()), new Leaf(rng.nextDouble()));    Node root = new CategoricalNode(rng.nextInt(), new double[] { rng.nextDouble(), rng.nextDouble(), rng.nextDouble() }, new Node[] { node1, node2, new Leaf(rng.nextDouble()) });        root.write(out);        assertEquals(root, readNode());}
0
 Node readNode() throws IOException
{    ByteArrayInputStream byteInStream = new ByteArrayInputStream(byteOutStream.toByteArray());    DataInput in = new DataInputStream(byteInStream);    return Node.read(in);}
0
public void testReadLeaf() throws Exception
{    Node leaf = new Leaf(rng.nextDouble());    leaf.write(out);    assertEquals(leaf, readNode());}
0
public void testParseNumerical() throws Exception
{    Node node = new NumericalNode(rng.nextInt(), rng.nextDouble(), new Leaf(rng.nextInt()), new Leaf(rng.nextDouble()));    node.write(out);    assertEquals(node, readNode());}
0
public void testCategoricalNode() throws Exception
{    Node node = new CategoricalNode(rng.nextInt(), new double[] { rng.nextDouble(), rng.nextDouble(), rng.nextDouble() }, new Node[] { new Leaf(rng.nextDouble()), new Leaf(rng.nextDouble()), new Leaf(rng.nextDouble()) });    node.write(out);    assertEquals(node, readNode());}
0
public void testEntropy() throws Exception
{    Random rng = RandomUtils.getRandom();    String descriptor = Utils.randomDescriptor(rng, NUM_ATTRIBUTES);    int label = Utils.findLabel(descriptor);        double[][] temp = Utils.randomDoublesWithSameLabel(rng, descriptor, false, 100, 0);    String[] sData = Utils.double2String(temp);    Dataset dataset = DataLoader.generateDataset(descriptor, false, sData);    Data data = DataLoader.loadData(dataset, sData);    DefaultIgSplit iG = new DefaultIgSplit();    double expected = 0.0 - 1.0 * Math.log(1.0) / Math.log(2.0);    assertEquals(expected, iG.entropy(data), EPSILON);        for (int index = 0; index < 50; index++) {        temp[index][label] = 1.0;    }    sData = Utils.double2String(temp);    dataset = DataLoader.generateDataset(descriptor, false, sData);    data = DataLoader.loadData(dataset, sData);    iG = new DefaultIgSplit();    expected = 2.0 * -0.5 * Math.log(0.5) / Math.log(2.0);    assertEquals(expected, iG.entropy(data), EPSILON);        for (int index = 0; index < 15; index++) {        temp[index][label] = 2.0;    }    sData = Utils.double2String(temp);    dataset = DataLoader.generateDataset(descriptor, false, sData);    data = DataLoader.loadData(dataset, sData);    iG = new DefaultIgSplit();    expected = -0.15 * Math.log(0.15) / Math.log(2.0) - 0.35 * Math.log(0.35) / Math.log(2.0) - 0.5 * Math.log(0.5) / Math.log(2.0);    assertEquals(expected, iG.entropy(data), EPSILON);}
0
private static Data[] generateTrainingData() throws DescriptorException
{        String[] trainData = new String[20];    for (int i = 0; i < trainData.length; i++) {        if (i % 3 == 0) {            trainData[i] = "A," + (40 - i) + ',' + (i + 20);        } else if (i % 3 == 1) {            trainData[i] = "B," + (i + 20) + ',' + (40 - i);        } else {            trainData[i] = "C," + (i + 20) + ',' + (i + 20);        }    }        Dataset dataset = DataLoader.generateDataset("C N L", true, trainData);    Data[] datas = new Data[3];    datas[0] = DataLoader.loadData(dataset, trainData);        trainData = new String[20];    for (int i = 0; i < trainData.length; i++) {        if (i % 2 == 0) {            trainData[i] = "A," + (50 - i) + ',' + (i + 10);        } else {            trainData[i] = "B," + (i + 10) + ',' + (50 - i);        }    }    datas[1] = DataLoader.loadData(dataset, trainData);        trainData = new String[10];    for (int i = 0; i < trainData.length; i++) {        trainData[i] = "A," + (40 - i) + ',' + (i + 20);    }    datas[2] = DataLoader.loadData(dataset, trainData);    return datas;}
0
public void testComputeSplit() throws DescriptorException
{    Data[] datas = generateTrainingData();    RegressionSplit igSplit = new RegressionSplit();    Split split = igSplit.computeSplit(datas[0], 1);    assertEquals(180.0, split.getIg(), EPSILON);    assertEquals(38.0, split.getSplit(), EPSILON);    split = igSplit.computeSplit(datas[0].subset(Condition.lesser(1, 38.0)), 1);    assertEquals(76.5, split.getIg(), EPSILON);    assertEquals(21.5, split.getSplit(), EPSILON);    split = igSplit.computeSplit(datas[1], 0);    assertEquals(2205.0, split.getIg(), EPSILON);    assertEquals(Double.NaN, split.getSplit(), EPSILON);    split = igSplit.computeSplit(datas[1].subset(Condition.equals(0, 0.0)), 1);    assertEquals(250.0, split.getIg(), EPSILON);    assertEquals(41.0, split.getSplit(), EPSILON);}
0
public void setUp() throws Exception
{    super.setUp();    randomNumberGenerator = RandomUtils.getRandom(1);    Dataset dataset = DataLoader.generateDataset("C N N C L", false, TRAIN_DATA);    trainingData = DataLoader.loadData(dataset, TRAIN_DATA);    testData = DataLoader.loadData(dataset, TEST_DATA);}
0
public void testTreeVisualize() throws Exception
{        DecisionTreeBuilder builder = new DecisionTreeBuilder();    builder.setM(trainingData.getDataset().nbAttributes() - 1);    Node tree = builder.build(randomNumberGenerator, trainingData);    String visualization = TreeVisualizer.toString(tree, trainingData.getDataset(), ATTRIBUTE_NAMES);    assertTrue((String.format("\n" + "outlook = rainy\n" + "|   windy = FALSE : yes\n" + "|   windy = TRUE : no\n" + "outlook = sunny\n" + "|   humidity < 77%s5 : yes\n" + "|   humidity >= 77%s5 : no\n" + "outlook = overcast : yes", DECIMAL_SEPARATOR, DECIMAL_SEPARATOR)).equals(visualization) || (String.format("\n" + "outlook = rainy\n" + "|   windy = TRUE : no\n" + "|   windy = FALSE : yes\n" + "outlook = overcast : yes\n" + "outlook = sunny\n" + "|   humidity < 77%s5 : yes\n" + "|   humidity >= 77%s5 : no", DECIMAL_SEPARATOR, DECIMAL_SEPARATOR)).equals(visualization));}
0
public void testPredictTrace() throws Exception
{        DecisionTreeBuilder builder = new DecisionTreeBuilder();    builder.setM(trainingData.getDataset().nbAttributes() - 1);    Node tree = builder.build(randomNumberGenerator, trainingData);    String[] prediction = TreeVisualizer.predictTrace(tree, testData, ATTRIBUTE_NAMES);    Assert.assertArrayEquals(new String[] { "outlook = rainy -> windy = TRUE -> no", "outlook = overcast -> yes", String.format("outlook = sunny -> (humidity = 90) >= 77%s5 -> no", DECIMAL_SEPARATOR) }, prediction);}
0
public void testForestVisualize() throws Exception
{        NumericalNode root = new NumericalNode(2, 90, new Leaf(0), new CategoricalNode(0, new double[] { 0, 1, 2 }, new Node[] { new NumericalNode(1, 71, new Leaf(0), new Leaf(1)), new Leaf(1), new Leaf(0) }));    List<Node> trees = new ArrayList<>();    trees.add(root);        DecisionForest forest = new DecisionForest(trees);    String visualization = ForestVisualizer.toString(forest, trainingData.getDataset(), null);    assertTrue(("Tree[1]:\n2 < 90 : yes\n2 >= 90\n" + "|   0 = rainy\n" + "|   |   1 < 71 : yes\n" + "|   |   1 >= 71 : no\n" + "|   0 = sunny : no\n" + "|   0 = overcast : yes\n").equals(visualization) || ("Tree[1]:\n" + "2 < 90 : no\n" + "2 >= 90\n" + "|   0 = rainy\n" + "|   |   1 < 71 : no\n" + "|   |   1 >= 71 : yes\n" + "|   0 = overcast : yes\n" + "|   0 = sunny : no\n").equals(visualization));    visualization = ForestVisualizer.toString(forest, trainingData.getDataset(), ATTRIBUTE_NAMES);    assertTrue(("Tree[1]:\n" + "humidity < 90 : yes\n" + "humidity >= 90\n" + "|   outlook = rainy\n" + "|   |   temperature < 71 : yes\n" + "|   |   temperature >= 71 : no\n" + "|   outlook = sunny : no\n" + "|   outlook = overcast : yes\n").equals(visualization) || ("Tree[1]:\n" + "humidity < 90 : no\n" + "humidity >= 90\n" + "|   outlook = rainy\n" + "|   |   temperature < 71 : no\n" + "|   |   temperature >= 71 : yes\n" + "|   outlook = overcast : yes\n" + "|   outlook = sunny : no\n").equals(visualization));}
0
public void testLeafless() throws Exception
{    List<Instance> instances = new ArrayList<>();    for (int i = 0; i < trainingData.size(); i++) {        if (trainingData.get(i).get(0) != 0.0d) {            instances.add(trainingData.get(i));        }    }    Data lessData = new Data(trainingData.getDataset(), instances);        DecisionTreeBuilder builder = new DecisionTreeBuilder();    builder.setM(trainingData.getDataset().nbAttributes() - 1);    builder.setMinSplitNum(0);    builder.setComplemented(false);    Node tree = builder.build(randomNumberGenerator, lessData);    String visualization = TreeVisualizer.toString(tree, trainingData.getDataset(), ATTRIBUTE_NAMES);    assertTrue((String.format("\noutlook = sunny\n" + "|   humidity < 77%s5 : yes\n" + "|   humidity >= 77%s5 : no\n" + "outlook = overcast : yes", DECIMAL_SEPARATOR, DECIMAL_SEPARATOR)).equals(visualization) || (String.format("\noutlook = overcast : yes\n" + "outlook = sunny\n" + "|   humidity < 77%s5 : yes\n" + "|   humidity >= 77%s5 : no", DECIMAL_SEPARATOR, DECIMAL_SEPARATOR)).equals(visualization));}
0
public void testEmpty() throws Exception
{    Data emptyData = new Data(trainingData.getDataset());        DecisionTreeBuilder builder = new DecisionTreeBuilder();    Node tree = builder.build(randomNumberGenerator, emptyData);    assertEquals(" : unknown", TreeVisualizer.toString(tree, trainingData.getDataset(), ATTRIBUTE_NAMES));}
0
public void testAuc()
{    Auc auc = new Auc();    Random gen = RandomUtils.getRandom();    auc.setProbabilityScore(false);    for (int i = 0; i < 100000; i++) {        auc.add(0, gen.nextGaussian());        auc.add(1, gen.nextGaussian() + 1);    }    assertEquals(0.76, auc.auc(), 0.01);}
0
public void testTies()
{    Auc auc = new Auc();    Random gen = RandomUtils.getRandom();    auc.setProbabilityScore(false);    for (int i = 0; i < 100000; i++) {        auc.add(0, gen.nextGaussian());        auc.add(1, gen.nextGaussian() + 1);    }        auc.add(0, 5.0);    auc.add(0, 5.0);    auc.add(0, 5.0);    auc.add(0, 5.0);    auc.add(1, 5.0);    auc.add(1, 5.0);    auc.add(1, 5.0);    assertEquals(0.76, auc.auc(), 0.05);}
0
public void testEntropy()
{    Auc auc = new Auc();    Random gen = RandomUtils.getRandom();    Normal n0 = new Normal(-1, 1, gen);    Normal n1 = new Normal(1, 1, gen);    for (int i = 0; i < 100000; i++) {        double score = n0.nextDouble();        double p = n1.pdf(score) / (n0.pdf(score) + n1.pdf(score));        auc.add(0, p);        score = n1.nextDouble();        p = n1.pdf(score) / (n0.pdf(score) + n1.pdf(score));        auc.add(1, p);    }    Matrix m = auc.entropy();    assertEquals(-0.35, m.get(0, 0), 0.02);    assertEquals(-2.36, m.get(0, 1), 0.02);    assertEquals(-2.36, m.get(1, 0), 0.02);    assertEquals(-0.35, m.get(1, 1), 0.02);}
0
public void setUp() throws Exception
{    super.setUp();    NaiveBayesModel model = createComplementaryNaiveBayesModel();    classifier = new ComplementaryNaiveBayesClassifier(model);}
0
public void testNaiveBayes() throws Exception
{    assertEquals(4, classifier.numCategories());    assertEquals(0, maxIndex(classifier.classifyFull(new DenseVector(new double[] { 1.0, 0.0, 0.0, 0.0 }))));    assertEquals(1, maxIndex(classifier.classifyFull(new DenseVector(new double[] { 0.0, 1.0, 0.0, 0.0 }))));    assertEquals(2, maxIndex(classifier.classifyFull(new DenseVector(new double[] { 0.0, 0.0, 1.0, 0.0 }))));    assertEquals(3, maxIndex(classifier.classifyFull(new DenseVector(new double[] { 0.0, 0.0, 0.0, 1.0 }))));}
0
public void testRandomModelGeneration()
{        NaiveBayesModel standardModel = getStandardModel();        standardModel.validate();        NaiveBayesModel complementaryModel = getComplementaryModel();    complementaryModel.validate();}
0
public void setUp() throws Exception
{    super.setUp();    conf = getConfiguration();    inputFile = getTestTempFile("trainingInstances.seq");    outputDir = getTestTempDir("output");    outputDir.delete();    tempDir = getTestTempDir("tmp");    SequenceFile.Writer writer = new SequenceFile.Writer(FileSystem.get(conf), conf, new Path(inputFile.getAbsolutePath()), Text.class, VectorWritable.class);    try {        writer.append(LABEL_STOLEN, trainingInstance(COLOR_RED, TYPE_SPORTS, ORIGIN_DOMESTIC));        writer.append(LABEL_NOT_STOLEN, trainingInstance(COLOR_RED, TYPE_SPORTS, ORIGIN_DOMESTIC));        writer.append(LABEL_STOLEN, trainingInstance(COLOR_RED, TYPE_SPORTS, ORIGIN_DOMESTIC));        writer.append(LABEL_NOT_STOLEN, trainingInstance(COLOR_YELLOW, TYPE_SPORTS, ORIGIN_DOMESTIC));        writer.append(LABEL_STOLEN, trainingInstance(COLOR_YELLOW, TYPE_SPORTS, ORIGIN_IMPORTED));        writer.append(LABEL_NOT_STOLEN, trainingInstance(COLOR_YELLOW, TYPE_SUV, ORIGIN_IMPORTED));        writer.append(LABEL_STOLEN, trainingInstance(COLOR_YELLOW, TYPE_SUV, ORIGIN_IMPORTED));        writer.append(LABEL_NOT_STOLEN, trainingInstance(COLOR_YELLOW, TYPE_SUV, ORIGIN_DOMESTIC));        writer.append(LABEL_NOT_STOLEN, trainingInstance(COLOR_RED, TYPE_SUV, ORIGIN_IMPORTED));        writer.append(LABEL_STOLEN, trainingInstance(COLOR_RED, TYPE_SPORTS, ORIGIN_IMPORTED));    } finally {        Closeables.close(writer, false);    }}
0
public void toyData() throws Exception
{    TrainNaiveBayesJob trainNaiveBayes = new TrainNaiveBayesJob();    trainNaiveBayes.setConf(conf);    trainNaiveBayes.run(new String[] { "--input", inputFile.getAbsolutePath(), "--output", outputDir.getAbsolutePath(), "--tempDir", tempDir.getAbsolutePath() });    NaiveBayesModel naiveBayesModel = NaiveBayesModel.materialize(new Path(outputDir.getAbsolutePath()), conf);    AbstractVectorClassifier classifier = new StandardNaiveBayesClassifier(naiveBayesModel);    assertEquals(2, classifier.numCategories());    Vector prediction = classifier.classifyFull(trainingInstance(COLOR_RED, TYPE_SUV, ORIGIN_DOMESTIC).get());        assertTrue(prediction.get(0) < prediction.get(1));}
0
public void toyDataComplementary() throws Exception
{    TrainNaiveBayesJob trainNaiveBayes = new TrainNaiveBayesJob();    trainNaiveBayes.setConf(conf);    trainNaiveBayes.run(new String[] { "--input", inputFile.getAbsolutePath(), "--output", outputDir.getAbsolutePath(), "--trainComplementary", "--tempDir", tempDir.getAbsolutePath() });    NaiveBayesModel naiveBayesModel = NaiveBayesModel.materialize(new Path(outputDir.getAbsolutePath()), conf);    AbstractVectorClassifier classifier = new ComplementaryNaiveBayesClassifier(naiveBayesModel);    assertEquals(2, classifier.numCategories());    Vector prediction = classifier.classifyFull(trainingInstance(COLOR_RED, TYPE_SUV, ORIGIN_DOMESTIC).get());        assertTrue(prediction.get(0) < prediction.get(1));}
0
 static VectorWritable trainingInstance(Vector.Element... elems)
{    DenseVector trainingInstance = new DenseVector(6);    for (Vector.Element elem : elems) {        trainingInstance.set(elem.index(), elem.get());    }    return new VectorWritable(trainingInstance);}
0
public void setUp() throws Exception
{    super.setUp();    standardModel = createStandardNaiveBayesModel();    standardModel.validate();    complementaryModel = createComplementaryNaiveBayesModel();    complementaryModel.validate();}
0
protected NaiveBayesModel getStandardModel()
{    return standardModel;}
0
protected NaiveBayesModel getComplementaryModel()
{    return complementaryModel;}
0
protected static double complementaryNaiveBayesThetaWeight(int label, Matrix weightMatrix, Vector labelSum, Vector featureSum)
{    double weight = 0.0;    double alpha = 1.0;    for (int i = 0; i < featureSum.size(); i++) {        double score = weightMatrix.get(i, label);        double lSum = labelSum.get(label);        double fSum = featureSum.get(i);        double totalSum = featureSum.zSum();        double numerator = fSum - score + alpha;        double denominator = totalSum - lSum + featureSum.size();        weight += Math.abs(Math.log(numerator / denominator));    }    return weight;}
0
protected static double naiveBayesThetaWeight(int label, Matrix weightMatrix, Vector labelSum, Vector featureSum)
{    double weight = 0.0;    double alpha = 1.0;    for (int feature = 0; feature < featureSum.size(); feature++) {        double score = weightMatrix.get(feature, label);        double lSum = labelSum.get(label);        double numerator = score + alpha;        double denominator = lSum + featureSum.size();        weight += Math.abs(Math.log(numerator / denominator));    }    return weight;}
0
protected static NaiveBayesModel createStandardNaiveBayesModel()
{    double[][] matrix = { { 0.7, 0.1, 0.1, 0.3 }, { 0.4, 0.4, 0.1, 0.1 }, { 0.1, 0.0, 0.8, 0.1 }, { 0.1, 0.1, 0.1, 0.7 } };    double[] labelSumArray = { 1.2, 1.0, 1.0, 1.0 };    double[] featureSumArray = { 1.3, 0.6, 1.1, 1.2 };    DenseMatrix weightMatrix = new DenseMatrix(matrix);    DenseVector labelSum = new DenseVector(labelSumArray);    DenseVector featureSum = new DenseVector(featureSumArray);        return new NaiveBayesModel(weightMatrix, featureSum, labelSum, null, 1.0f, false);}
0
protected static NaiveBayesModel createComplementaryNaiveBayesModel()
{    double[][] matrix = { { 0.7, 0.1, 0.1, 0.3 }, { 0.4, 0.4, 0.1, 0.1 }, { 0.1, 0.0, 0.8, 0.1 }, { 0.1, 0.1, 0.1, 0.7 } };    double[] labelSumArray = { 1.2, 1.0, 1.0, 1.0 };    double[] featureSumArray = { 1.3, 0.6, 1.1, 1.2 };    DenseMatrix weightMatrix = new DenseMatrix(matrix);    DenseVector labelSum = new DenseVector(labelSumArray);    DenseVector featureSum = new DenseVector(featureSumArray);    double[] thetaNormalizerSum = { complementaryNaiveBayesThetaWeight(0, weightMatrix, labelSum, featureSum), complementaryNaiveBayesThetaWeight(1, weightMatrix, labelSum, featureSum), complementaryNaiveBayesThetaWeight(2, weightMatrix, labelSum, featureSum), complementaryNaiveBayesThetaWeight(3, weightMatrix, labelSum, featureSum) };        return new NaiveBayesModel(weightMatrix, featureSum, labelSum, new DenseVector(thetaNormalizerSum), 1.0f, true);}
0
protected static int maxIndex(Vector instance)
{    int maxIndex = -1;    double maxScore = Integer.MIN_VALUE;    for (Element label : instance.all()) {        if (label.get() >= maxScore) {            maxIndex = label.index();            maxScore = label.get();        }    }    return maxIndex;}
0
public void setUp() throws Exception
{    super.setUp();    NaiveBayesModel model = createStandardNaiveBayesModel();    classifier = new StandardNaiveBayesClassifier(model);}
0
public void testNaiveBayes() throws Exception
{    assertEquals(4, classifier.numCategories());    assertEquals(0, maxIndex(classifier.classifyFull(new DenseVector(new double[] { 1.0, 0.0, 0.0, 0.0 }))));    assertEquals(1, maxIndex(classifier.classifyFull(new DenseVector(new double[] { 0.0, 1.0, 0.0, 0.0 }))));    assertEquals(2, maxIndex(classifier.classifyFull(new DenseVector(new double[] { 0.0, 0.0, 1.0, 0.0 }))));    assertEquals(3, maxIndex(classifier.classifyFull(new DenseVector(new double[] { 0.0, 0.0, 0.0, 1.0 }))));}
0
public void setUp() throws Exception
{    super.setUp();    ctx = EasyMock.createMock(Mapper.Context.class);    instance = new VectorWritable(new DenseVector(new double[] { 1, 0, 1, 1, 0 }));    labelIndex = new OpenObjectIntHashMap<>();    labelIndex.put("bird", 0);    labelIndex.put("cat", 1);}
0
public void index() throws Exception
{    ctx.write(new IntWritable(0), instance);    EasyMock.replay(ctx);    IndexInstancesMapper indexInstances = new IndexInstancesMapper();    setField(indexInstances, "labelIndex", labelIndex);    indexInstances.map(new Text("/bird/"), instance, ctx);    EasyMock.verify(ctx);}
0
public void skip() throws Exception
{    Counter skippedInstances = EasyMock.createMock(Counter.class);    EasyMock.expect(ctx.getCounter(IndexInstancesMapper.Counter.SKIPPED_INSTANCES)).andReturn(skippedInstances);    skippedInstances.increment(1);    EasyMock.replay(ctx, skippedInstances);    IndexInstancesMapper indexInstances = new IndexInstancesMapper();    setField(indexInstances, "labelIndex", labelIndex);    indexInstances.map(new Text("/fish/"), instance, ctx);    EasyMock.verify(ctx, skippedInstances);}
0
public void standard() throws Exception
{    Mapper.Context ctx = EasyMock.createMock(Mapper.Context.class);    ComplementaryThetaTrainer trainer = EasyMock.createMock(ComplementaryThetaTrainer.class);    Vector instance1 = new DenseVector(new double[] { 1, 2, 3 });    Vector instance2 = new DenseVector(new double[] { 4, 5, 6 });    Vector perLabelThetaNormalizer = new DenseVector(new double[] { 7, 8 });    ThetaMapper thetaMapper = new ThetaMapper();    setField(thetaMapper, "trainer", trainer);    trainer.train(0, instance1);    trainer.train(1, instance2);    EasyMock.expect(trainer.retrievePerLabelThetaNormalizer()).andReturn(perLabelThetaNormalizer);    ctx.write(new Text(TrainNaiveBayesJob.LABEL_THETA_NORMALIZER), new VectorWritable(perLabelThetaNormalizer));    EasyMock.replay(ctx, trainer);    thetaMapper.map(new IntWritable(0), new VectorWritable(instance1), ctx);    thetaMapper.map(new IntWritable(1), new VectorWritable(instance2), ctx);    thetaMapper.cleanup(ctx);    EasyMock.verify(ctx, trainer);}
0
public void scores() throws Exception
{    Mapper.Context ctx = EasyMock.createMock(Mapper.Context.class);    Vector instance1 = new DenseVector(new double[] { 1, 0, 0.5, 0.5, 0 });    Vector instance2 = new DenseVector(new double[] { 0, 0.5, 0, 0, 0 });    Vector instance3 = new DenseVector(new double[] { 1, 0.5, 1, 1.5, 1 });    Vector weightsPerLabel = new DenseVector(new double[] { 0, 0 });    ctx.write(new Text(TrainNaiveBayesJob.WEIGHTS_PER_FEATURE), new VectorWritable(new DenseVector(new double[] { 2, 1, 1.5, 2, 1 })));    ctx.write(new Text(TrainNaiveBayesJob.WEIGHTS_PER_LABEL), new VectorWritable(new DenseVector(new double[] { 2.5, 5 })));    EasyMock.replay(ctx);    WeightsMapper weights = new WeightsMapper();    setField(weights, "weightsPerLabel", weightsPerLabel);    weights.map(new IntWritable(0), new VectorWritable(instance1), ctx);    weights.map(new IntWritable(0), new VectorWritable(instance2), ctx);    weights.map(new IntWritable(1), new VectorWritable(instance3), ctx);    weights.cleanup(ctx);    EasyMock.verify(ctx);}
0
private static double[] parseAnalysis(CharSequence analysis)
{    double[] results = new double[3];    Matcher m = p1.matcher(analysis);    if (m.find()) {        results[0] = Double.parseDouble(m.group(1));    } else {        return null;    }    m = p2.matcher(analysis);    if (m.find()) {        results[1] = Double.parseDouble(m.group(1));    } else {        return null;    }    m = p3.matcher(analysis);    if (m.find()) {        results[2] = Double.parseDouble(m.group(1));    } else {        return null;    }    return results;}
0
private static int[] parseAnalysisCount(CharSequence analysis)
{    int[] results = new int[3];    Matcher m = p4.matcher(analysis);    if (m.find()) {        results[0] = Integer.parseInt(m.group(1));    }    m = p5.matcher(analysis);    if (m.find()) {        results[1] = Integer.parseInt(m.group(1));    }    m = p6.matcher(analysis);    if (m.find()) {        results[2] = Integer.parseInt(m.group(1));    }    return results;}
0
public void testAnalyze()
{    double[][] results = new double[10][2];    for (int i = 0; i < results.length; i++) {        results[i][0] = i;        results[i][1] = i + 1;    }    RegressionResultAnalyzer analyzer = new RegressionResultAnalyzer();    analyzer.setInstances(results);    String analysis = analyzer.toString();    assertArrayEquals(new double[] { 1.0, 1.0, 1.0 }, parseAnalysis(analysis), 0);    for (int i = 0; i < results.length; i++) {        results[i][1] = Math.sqrt(i);    }    analyzer = new RegressionResultAnalyzer();    analyzer.setInstances(results);    analysis = analyzer.toString();    assertArrayEquals(new double[] { 0.9573, 2.5694, 3.2848 }, parseAnalysis(analysis), 0);    for (int i = 0; i < results.length; i++) {        results[i][0] = results.length - i;    }    analyzer = new RegressionResultAnalyzer();    analyzer.setInstances(results);    analysis = analyzer.toString();    assertArrayEquals(new double[] { -0.9573, 4.1351, 5.1573 }, parseAnalysis(analysis), 0);}
0
public void testUnpredictable()
{    double[][] results = new double[10][2];    for (int i = 0; i < results.length; i++) {        results[i][0] = i;        results[i][1] = Double.NaN;    }    RegressionResultAnalyzer analyzer = new RegressionResultAnalyzer();    analyzer.setInstances(results);    String analysis = analyzer.toString();    assertNull(parseAnalysis(analysis));    assertArrayEquals(new int[] { 0, 10, 10 }, parseAnalysisCount(analysis));    for (int i = 0; i < results.length - 3; i++) {        results[i][1] = Math.sqrt(i);    }    analyzer = new RegressionResultAnalyzer();    analyzer.setInstances(results);    analysis = analyzer.toString();    assertArrayEquals(new double[] { 0.9552, 1.4526, 1.9345 }, parseAnalysis(analysis), 0);    assertArrayEquals(new int[] { 7, 3, 10 }, parseAnalysisCount(analysis));}
0
public void testForwardAlgorithm()
{        double[][] alphaExpectedA = { { 0.02, 0.0392, 0.002438, 0.00035456, 0.0011554672, 7.158497e-04, 4.614927e-05 }, { 0.01, 0.0054, 0.001824, 0.00069486, 0.0007586904, 2.514137e-04, 1.721505e-05 }, { 0.32, 0.0262, 0.002542, 0.00038026, 0.0001360234, 3.002345e-05, 9.659608e-05 }, { 0.03, 0.0000, 0.013428, 0.00951084, 0.0000000000, 0.000000e+00, 2.428986e-05 } };        Matrix alpha = HmmAlgorithms.forwardAlgorithm(getModel(), getSequence(), false);        assertNotNull(alpha);    assertEquals(4, alpha.numCols());    assertEquals(7, alpha.numRows());        for (int i = 0; i < 4; ++i) {        for (int j = 0; j < 7; ++j) {            assertEquals(alphaExpectedA[i][j], alpha.get(j, i), EPSILON);        }    }}
0
public void testLogScaledForwardAlgorithm()
{        double[][] alphaExpectedA = { { 0.02, 0.0392, 0.002438, 0.00035456, 0.0011554672, 7.158497e-04, 4.614927e-05 }, { 0.01, 0.0054, 0.001824, 0.00069486, 0.0007586904, 2.514137e-04, 1.721505e-05 }, { 0.32, 0.0262, 0.002542, 0.00038026, 0.0001360234, 3.002345e-05, 9.659608e-05 }, { 0.03, 0.0000, 0.013428, 0.00951084, 0.0000000000, 0.000000e+00, 2.428986e-05 } };        Matrix alpha = HmmAlgorithms.forwardAlgorithm(getModel(), getSequence(), true);        assertNotNull(alpha);    assertEquals(4, alpha.numCols());    assertEquals(7, alpha.numRows());        for (int i = 0; i < 4; ++i) {        for (int j = 0; j < 7; ++j) {            assertEquals(Math.log(alphaExpectedA[i][j]), alpha.get(j, i), EPSILON);        }    }}
0
public void testBackwardAlgorithm()
{        double[][] betaExpectedA = { { 0.0015730559, 0.003543656, 0.00738264, 0.040692, 0.0848, 0.17, 1 }, { 0.0017191865, 0.002386795, 0.00923652, 0.052232, 0.1018, 0.17, 1 }, { 0.0003825772, 0.001238558, 0.00259464, 0.012096, 0.0664, 0.66, 1 }, { 0.0004390858, 0.007076994, 0.01063512, 0.013556, 0.0304, 0.17, 1 } };        Matrix beta = HmmAlgorithms.backwardAlgorithm(getModel(), getSequence(), false);        assertNotNull(beta);    assertEquals(4, beta.numCols());    assertEquals(7, beta.numRows());        for (int i = 0; i < 4; ++i) {        for (int j = 0; j < 7; ++j) {            assertEquals(betaExpectedA[i][j], beta.get(j, i), EPSILON);        }    }}
0
public void testLogScaledBackwardAlgorithm()
{        double[][] betaExpectedA = { { 0.0015730559, 0.003543656, 0.00738264, 0.040692, 0.0848, 0.17, 1 }, { 0.0017191865, 0.002386795, 0.00923652, 0.052232, 0.1018, 0.17, 1 }, { 0.0003825772, 0.001238558, 0.00259464, 0.012096, 0.0664, 0.66, 1 }, { 0.0004390858, 0.007076994, 0.01063512, 0.013556, 0.0304, 0.17, 1 } };        Matrix beta = HmmAlgorithms.backwardAlgorithm(getModel(), getSequence(), true);        assertNotNull(beta);    assertEquals(4, beta.numCols());    assertEquals(7, beta.numRows());        for (int i = 0; i < 4; ++i) {        for (int j = 0; j < 7; ++j) {            assertEquals(Math.log(betaExpectedA[i][j]), beta.get(j, i), EPSILON);        }    }}
0
public void testViterbiAlgorithm()
{        int[] expected = { 2, 0, 3, 3, 0, 0, 2 };        int[] computed = HmmAlgorithms.viterbiAlgorithm(getModel(), getSequence(), false);        assertNotNull(computed);    assertEquals(computed.length, getSequence().length);        for (int i = 0; i < getSequence().length; ++i) {        assertEquals(expected[i], computed[i]);    }}
0
public void testLogScaledViterbiAlgorithm()
{        int[] expected = { 2, 0, 3, 3, 0, 0, 2 };        int[] computed = HmmAlgorithms.viterbiAlgorithm(getModel(), getSequence(), true);        assertNotNull(computed);    assertEquals(computed.length, getSequence().length);        for (int i = 0; i < getSequence().length; ++i) {        assertEquals(expected[i], computed[i]);    }}
0
public void testModelLikelihood()
{        Matrix alpha = HmmAlgorithms.forwardAlgorithm(getModel(), getSequence(), false);    Matrix beta = HmmAlgorithms.backwardAlgorithm(getModel(), getSequence(), false);        double forwardLikelihood = HmmEvaluator.modelLikelihood(alpha, false);    double backwardLikelihood = HmmEvaluator.modelLikelihood(getModel(), getSequence(), beta, false);    assertEquals(forwardLikelihood, backwardLikelihood, EPSILON);        assertEquals(1.8425e-4, forwardLikelihood, EPSILON);}
0
public void testScaledModelLikelihood()
{        Matrix alpha = HmmAlgorithms.forwardAlgorithm(getModel(), getSequence(), true);    Matrix beta = HmmAlgorithms.backwardAlgorithm(getModel(), getSequence(), true);        double forwardLikelihood = HmmEvaluator.modelLikelihood(alpha, true);    double backwardLikelihood = HmmEvaluator.modelLikelihood(getModel(), getSequence(), beta, true);    assertEquals(forwardLikelihood, backwardLikelihood, EPSILON);        assertEquals(1.8425e-4, forwardLikelihood, EPSILON);}
0
public void testRandomModelGeneration()
{        HmmModel model = new HmmModel(10, 20);        HmmUtils.validate(model);}
0
public void setUp() throws Exception
{    super.setUp();        String[] hiddenNames = { "H0", "H1", "H2", "H3" };    String[] outputNames = { "O0", "O1", "O2" };        double[][] transitionP = { { 0.5, 0.1, 0.1, 0.3 }, { 0.4, 0.4, 0.1, 0.1 }, { 0.1, 0.0, 0.8, 0.1 }, { 0.1, 0.1, 0.1, 0.7 } };        double[][] emissionP = { { 0.8, 0.1, 0.1 }, { 0.6, 0.1, 0.3 }, { 0.1, 0.8, 0.1 }, { 0.0, 0.1, 0.9 } };        double[] initialP = { 0.2, 0.1, 0.4, 0.3 };        model = new HmmModel(new DenseMatrix(transitionP), new DenseMatrix(emissionP), new DenseVector(initialP));    model.registerHiddenStateNames(hiddenNames);    model.registerOutputStateNames(outputNames);        HmmUtils.validate(model);}
0
protected HmmModel getModel()
{    return model;}
0
protected int[] getSequence()
{    return sequence;}
0
public void testViterbiTraining()
{            double[][] transitionE = { { 0.3125, 0.0625, 0.3125, 0.3125 }, { 0.25, 0.25, 0.25, 0.25 }, { 0.5, 0.071429, 0.357143, 0.071429 }, { 0.5, 0.1, 0.1, 0.3 } };        double[][] emissionE = { { 0.882353, 0.058824, 0.058824 }, { 0.333333, 0.333333, 0.3333333 }, { 0.076923, 0.846154, 0.076923 }, { 0.111111, 0.111111, 0.777778 } };        int[] observed = { 1, 0, 2, 2, 0, 0, 1, 1, 1, 0, 2, 0, 1, 0, 0 };    HmmModel trained = HmmTrainer.trainViterbi(getModel(), observed, 0.5, 0.1, 10, false);        Matrix emissionMatrix = trained.getEmissionMatrix();    Matrix transitionMatrix = trained.getTransitionMatrix();    for (int i = 0; i < trained.getNrOfHiddenStates(); ++i) {        for (int j = 0; j < trained.getNrOfHiddenStates(); ++j) {            assertEquals(transitionMatrix.getQuick(i, j), transitionE[i][j], EPSILON);        }        for (int j = 0; j < trained.getNrOfOutputStates(); ++j) {            assertEquals(emissionMatrix.getQuick(i, j), emissionE[i][j], EPSILON);        }    }}
0
public void testScaledViterbiTraining()
{            double[][] transitionE = { { 0.3125, 0.0625, 0.3125, 0.3125 }, { 0.25, 0.25, 0.25, 0.25 }, { 0.5, 0.071429, 0.357143, 0.071429 }, { 0.5, 0.1, 0.1, 0.3 } };        double[][] emissionE = { { 0.882353, 0.058824, 0.058824 }, { 0.333333, 0.333333, 0.3333333 }, { 0.076923, 0.846154, 0.076923 }, { 0.111111, 0.111111, 0.777778 } };        int[] observed = { 1, 0, 2, 2, 0, 0, 1, 1, 1, 0, 2, 0, 1, 0, 0 };    HmmModel trained = HmmTrainer.trainViterbi(getModel(), observed, 0.5, 0.1, 10, true);        Matrix emissionMatrix = trained.getEmissionMatrix();    Matrix transitionMatrix = trained.getTransitionMatrix();    for (int i = 0; i < trained.getNrOfHiddenStates(); ++i) {        for (int j = 0; j < trained.getNrOfHiddenStates(); ++j) {            assertEquals(transitionMatrix.getQuick(i, j), transitionE[i][j], EPSILON);        }        for (int j = 0; j < trained.getNrOfOutputStates(); ++j) {            assertEquals(emissionMatrix.getQuick(i, j), emissionE[i][j], EPSILON);        }    }}
0
public void testBaumWelchTraining()
{        int[] observed = { 1, 0, 2, 2, 0, 0, 1, 1, 1, 0, 2, 0, 1, 0, 0 };        double[] initialExpected = { 0, 0, 1.0, 0 };    double[][] transitionExpected = { { 0.2319, 0.0993, 0.0005, 0.6683 }, { 0.0001, 0.3345, 0.6654, 0 }, { 0.5975, 0, 0.4025, 0 }, { 0.0024, 0.6657, 0, 0.3319 } };    double[][] emissionExpected = { { 0.9995, 0.0004, 0.0001 }, { 0.9943, 0.0036, 0.0021 }, { 0.0059, 0.9941, 0 }, { 0, 0, 1 } };    HmmModel trained = HmmTrainer.trainBaumWelch(getModel(), observed, 0.1, 10, false);    Vector initialProbabilities = trained.getInitialProbabilities();    Matrix emissionMatrix = trained.getEmissionMatrix();    Matrix transitionMatrix = trained.getTransitionMatrix();    for (int i = 0; i < trained.getNrOfHiddenStates(); ++i) {        assertEquals(initialProbabilities.get(i), initialExpected[i], 0.0001);        for (int j = 0; j < trained.getNrOfHiddenStates(); ++j) {            assertEquals(transitionMatrix.getQuick(i, j), transitionExpected[i][j], 0.0001);        }        for (int j = 0; j < trained.getNrOfOutputStates(); ++j) {            assertEquals(emissionMatrix.getQuick(i, j), emissionExpected[i][j], 0.0001);        }    }}
0
public void testScaledBaumWelchTraining()
{        int[] observed = { 1, 0, 2, 2, 0, 0, 1, 1, 1, 0, 2, 0, 1, 0, 0 };        double[] initialExpected = { 0, 0, 1.0, 0 };    double[][] transitionExpected = { { 0.2319, 0.0993, 0.0005, 0.6683 }, { 0.0001, 0.3345, 0.6654, 0 }, { 0.5975, 0, 0.4025, 0 }, { 0.0024, 0.6657, 0, 0.3319 } };    double[][] emissionExpected = { { 0.9995, 0.0004, 0.0001 }, { 0.9943, 0.0036, 0.0021 }, { 0.0059, 0.9941, 0 }, { 0, 0, 1 } };    HmmModel trained = HmmTrainer.trainBaumWelch(getModel(), observed, 0.1, 10, true);    Vector initialProbabilities = trained.getInitialProbabilities();    Matrix emissionMatrix = trained.getEmissionMatrix();    Matrix transitionMatrix = trained.getTransitionMatrix();    for (int i = 0; i < trained.getNrOfHiddenStates(); ++i) {        assertEquals(initialProbabilities.get(i), initialExpected[i], 0.0001);        for (int j = 0; j < trained.getNrOfHiddenStates(); ++j) {            assertEquals(transitionMatrix.getQuick(i, j), transitionExpected[i][j], 0.0001);        }        for (int j = 0; j < trained.getNrOfOutputStates(); ++j) {            assertEquals(emissionMatrix.getQuick(i, j), emissionExpected[i][j], 0.0001);        }    }}
0
public void setUp() throws Exception
{    super.setUp();    legal22 = new DenseMatrix(new double[][] { { 0.5, 0.5 }, { 0.3, 0.7 } });    legal23 = new DenseMatrix(new double[][] { { 0.2, 0.2, 0.6 }, { 0.3, 0.3, 0.4 } });    legal33 = new DenseMatrix(new double[][] { { 0.1, 0.1, 0.8 }, { 0.1, 0.2, 0.7 }, { 0.2, 0.3, 0.5 } });    legal2 = new DenseVector(new double[] { 0.4, 0.6 });    illegal22 = new DenseMatrix(new double[][] { { 1, 2 }, { 3, 4 } });}
0
public void testValidatorLegal()
{    HmmUtils.validate(new HmmModel(legal22, legal23, legal2));}
0
public void testValidatorDimensionError()
{    try {        HmmUtils.validate(new HmmModel(legal33, legal23, legal2));    } catch (IllegalArgumentException e) {                return;    }    fail();}
0
public void testValidatorIllegelMatrixError()
{    try {        HmmUtils.validate(new HmmModel(illegal22, legal23, legal2));    } catch (IllegalArgumentException e) {                return;    }    fail();}
0
public void testEncodeStateSequence()
{    String[] hiddenSequence = { "H1", "H2", "H0", "H3", "H4" };    String[] outputSequence = { "O1", "O2", "O4", "O0" };        int[] hiddenSequenceEnc = HmmUtils.encodeStateSequence(getModel(), Arrays.asList(hiddenSequence), false, -1);    int[] outputSequenceEnc = HmmUtils.encodeStateSequence(getModel(), Arrays.asList(outputSequence), true, -1);        int[] hiddenSequenceExp = { 1, 2, 0, 3, -1 };    int[] outputSequenceExp = { 1, 2, -1, 0 };        for (int i = 0; i < hiddenSequenceEnc.length; ++i) {        assertEquals(hiddenSequenceExp[i], hiddenSequenceEnc[i]);    }    for (int i = 0; i < outputSequenceEnc.length; ++i) {        assertEquals(outputSequenceExp[i], outputSequenceEnc[i]);    }}
0
public void testDecodeStateSequence()
{    int[] hiddenSequence = { 1, 2, 0, 3, 10 };    int[] outputSequence = { 1, 2, 10, 0 };        List<String> hiddenSequenceDec = HmmUtils.decodeStateSequence(getModel(), hiddenSequence, false, "unknown");    List<String> outputSequenceDec = HmmUtils.decodeStateSequence(getModel(), outputSequence, true, "unknown");        String[] hiddenSequenceExp = { "H1", "H2", "H0", "H3", "unknown" };    String[] outputSequenceExp = { "O1", "O2", "unknown", "O0" };        for (int i = 0; i < hiddenSequenceExp.length; ++i) {        assertEquals(hiddenSequenceExp[i], hiddenSequenceDec.get(i));    }    for (int i = 0; i < outputSequenceExp.length; ++i) {        assertEquals(outputSequenceExp[i], outputSequenceDec.get(i));    }}
0
public void testNormalizeModel()
{    DenseVector ip = new DenseVector(new double[] { 10, 20 });    DenseMatrix tr = new DenseMatrix(new double[][] { { 10, 10 }, { 20, 25 } });    DenseMatrix em = new DenseMatrix(new double[][] { { 5, 7 }, { 10, 15 } });    HmmModel model = new HmmModel(tr, em, ip);    HmmUtils.normalizeModel(model);        HmmUtils.validate(model);}
0
public void testTruncateModel()
{    DenseVector ip = new DenseVector(new double[] { 0.0001, 0.0001, 0.9998 });    DenseMatrix tr = new DenseMatrix(new double[][] { { 0.9998, 0.0001, 0.0001 }, { 0.0001, 0.9998, 0.0001 }, { 0.0001, 0.0001, 0.9998 } });    DenseMatrix em = new DenseMatrix(new double[][] { { 0.9998, 0.0001, 0.0001 }, { 0.0001, 0.9998, 0.0001 }, { 0.0001, 0.0001, 0.9998 } });    HmmModel model = new HmmModel(tr, em, ip);        HmmModel sparseModel = HmmUtils.truncateModel(model, 0.01);        HmmUtils.validate(sparseModel);        Vector sparse_ip = sparseModel.getInitialProbabilities();    Matrix sparse_tr = sparseModel.getTransitionMatrix();    Matrix sparse_em = sparseModel.getEmissionMatrix();    for (int i = 0; i < sparseModel.getNrOfHiddenStates(); ++i) {        assertEquals(i == 2 ? 1.0 : 0.0, sparse_ip.getQuick(i), EPSILON);        for (int j = 0; j < sparseModel.getNrOfHiddenStates(); ++j) {            if (i == j) {                assertEquals(1.0, sparse_tr.getQuick(i, j), EPSILON);                assertEquals(1.0, sparse_em.getQuick(i, j), EPSILON);            } else {                assertEquals(0.0, sparse_tr.getQuick(i, j), EPSILON);                assertEquals(0.0, sparse_em.getQuick(i, j), EPSILON);            }        }    }}
0
public void testTrain()
{    Random gen = RandomUtils.getRandom();    Exponential exp = new Exponential(0.5, gen);    Vector beta = new DenseVector(200);    for (Vector.Element element : beta.all()) {        int sign = 1;        if (gen.nextDouble() < 0.5) {            sign = -1;        }        element.set(sign * exp.nextDouble());    }    AdaptiveLogisticRegression.Wrapper cl = new AdaptiveLogisticRegression.Wrapper(2, 200, new L1());    cl.update(new double[] { 1.0e-5, 1 });    for (int i = 0; i < 10000; i++) {        AdaptiveLogisticRegression.TrainingExample r = getExample(i, gen, beta);        cl.train(r);        if (i % 1000 == 0) {            System.out.printf("%10d %10.3f\n", i, cl.getLearner().auc());        }    }    assertEquals(1, cl.getLearner().auc(), 0.1);    AdaptiveLogisticRegression adaptiveLogisticRegression = new AdaptiveLogisticRegression(2, 200, new L1());    adaptiveLogisticRegression.setInterval(1000);    for (int i = 0; i < 20000; i++) {        AdaptiveLogisticRegression.TrainingExample r = getExample(i, gen, beta);        adaptiveLogisticRegression.train(r.getKey(), r.getActual(), r.getInstance());        if (i % 1000 == 0 && adaptiveLogisticRegression.getBest() != null) {            System.out.printf("%10d %10.4f %10.8f %.3f\n", i, adaptiveLogisticRegression.auc(), Math.log10(adaptiveLogisticRegression.getBest().getMappedParams()[0]), adaptiveLogisticRegression.getBest().getMappedParams()[1]);        }    }    assertEquals(1, adaptiveLogisticRegression.auc(), 0.1);    adaptiveLogisticRegression.close();}
0
private static AdaptiveLogisticRegression.TrainingExample getExample(int i, Random gen, Vector beta)
{    Vector data = new DenseVector(200);    for (Vector.Element element : data.all()) {        element.set(gen.nextDouble() < 0.3 ? 1 : 0);    }    double p = 1 / (1 + Math.exp(1.5 - data.dot(beta)));    int target = 0;    if (gen.nextDouble() < p) {        target = 1;    }    return new AdaptiveLogisticRegression.TrainingExample(i, null, target, data);}
0
public void copyLearnsAsExpected()
{    Random gen = RandomUtils.getRandom();    Exponential exp = new Exponential(0.5, gen);    Vector beta = new DenseVector(200);    for (Vector.Element element : beta.all()) {        int sign = 1;        if (gen.nextDouble() < 0.5) {            sign = -1;        }        element.set(sign * exp.nextDouble());    }        AdaptiveLogisticRegression.Wrapper w = new AdaptiveLogisticRegression.Wrapper(2, 200, new L1());    for (int i = 0; i < 3000; i++) {        AdaptiveLogisticRegression.TrainingExample r = getExample(i, gen, beta);        w.train(r);        if (i % 1000 == 0) {            System.out.printf("%10d %.3f\n", i, w.getLearner().auc());        }    }    System.out.printf("%10d %.3f\n", 3000, w.getLearner().auc());    double auc1 = w.getLearner().auc();        AdaptiveLogisticRegression.Wrapper w2 = w.copy();    for (int i = 0; i < 5000; i++) {        if (i % 1000 == 0) {            if (i == 0) {                assertEquals("Should have started with no data", 0.5, w2.getLearner().auc(), 0.0001);            }            if (i == 1000) {                double auc2 = w2.getLearner().auc();                assertTrue("Should have had head-start", Math.abs(auc2 - 0.5) > 0.1);                assertTrue("AUC should improve quickly on copy", auc1 < auc2);            }            System.out.printf("%10d %.3f\n", i, w2.getLearner().auc());        }        AdaptiveLogisticRegression.TrainingExample r = getExample(i, gen, beta);        w2.train(r);    }    assertEquals("Original should not change after copy is updated", auc1, w.getLearner().auc(), 1.0e-5);        assertTrue("AUC should improve significantly on copy", auc1 < w2.getLearner().auc() - 0.05);        assertEquals(auc1, w.getLearner().auc(), 0);}
0
public void stepSize()
{    assertEquals(500, AdaptiveLogisticRegression.stepSize(15000, 2));    assertEquals(2000, AdaptiveLogisticRegression.stepSize(15000, 2.6));    assertEquals(5000, AdaptiveLogisticRegression.stepSize(24000, 2.6));    assertEquals(10000, AdaptiveLogisticRegression.stepSize(15000, 3));}
0
public void constantStep()
{    AdaptiveLogisticRegression lr = new AdaptiveLogisticRegression(2, 1000, new L1());    lr.setInterval(5000);    assertEquals(20000, lr.nextStep(15000));    assertEquals(20000, lr.nextStep(15001));    assertEquals(20000, lr.nextStep(16500));    assertEquals(20000, lr.nextStep(19999));    lr.close();}
0
public void growingStep()
{    AdaptiveLogisticRegression lr = new AdaptiveLogisticRegression(2, 1000, new L1());    lr.setInterval(2000, 10000);        for (int i = 2000; i < 20000; i += 2000) {        assertEquals(i + 2000, lr.nextStep(i));    }        for (int i = 20000; i < 50000; i += 5000) {        assertEquals(i + 5000, lr.nextStep(i));    }        for (int i = 50000; i < 500000; i += 10000) {        assertEquals(i + 10000, lr.nextStep(i));    }    lr.close();}
0
public void testAddToVector()
{    RecordFactory csv = new CsvRecordFactory("y", ImmutableMap.of("x1", "n", "x2", "w", "x3", "t"));    csv.firstLine("z,x1,y,x2,x3,q");    csv.maxTargetValue(2);    Vector v = new DenseVector(2000);    int t = csv.processLine("ignore,3.1,yes,tiger, \"this is text\",ignore", v);    assertEquals(0, t);        assertEquals(9.0, v.norm(0), 0);        assertEquals(3.1, v.maxValue(), 0);    v.set(v.maxValueIndex(), 0);    assertEquals(8.0, v.norm(0), 0);    assertEquals(8.0, v.norm(1), 0);    assertEquals(1.0, v.maxValue(), 0);    v.assign(0);    t = csv.processLine("ignore,5.3,no,line, \"and more text and more\",ignore", v);    assertEquals(1, t);        assertEquals(9.0, v.norm(0), 0);        assertEquals(5.3, v.maxValue(), 0);    v.set(v.maxValueIndex(), 0);    assertEquals(8.0, v.norm(0), 0);    assertEquals(10.339850002884626, v.norm(1), 1.0e-6);    assertEquals(1.5849625007211563, v.maxValue(), 1.0e-6);    v.assign(0);    t = csv.processLine("ignore,5.3,invalid,line, \"and more text and more\",ignore", v);    assertEquals(1, t);        assertEquals(9.0, v.norm(0), 0);        assertEquals(5.3, v.maxValue(), 0);    v.set(v.maxValueIndex(), 0);    assertEquals(8.0, v.norm(0), 0);    assertEquals(10.339850002884626, v.norm(1), 1.0e-6);    assertEquals(1.5849625007211563, v.maxValue(), 1.0e-6);}
0
public void testDictionaryOrder()
{    Dictionary dict = new Dictionary();    dict.intern("a");    dict.intern("d");    dict.intern("c");    dict.intern("b");    dict.intern("qrz");    assertEquals("[a, d, c, b, qrz]", dict.values().toString());    Dictionary dict2 = Dictionary.fromList(dict.values());    assertEquals("[a, d, c, b, qrz]", dict2.values().toString());}
0
public void testGradientmachine() throws IOException
{    Vector target = readStandardData();    GradientMachine grad = new GradientMachine(8, 4, 2).learningRate(0.1).regularization(0.01);    Random gen = RandomUtils.getRandom();    grad.initWeights(gen);    train(getInput(), target, grad);        test(getInput(), target, grad, 1.0, 1);}
0
private static T roundTrip(T m, Class<T> clazz) throws IOException
{    ByteArrayOutputStream buf = new ByteArrayOutputStream(1000);    DataOutputStream dos = new DataOutputStream(buf);    try {        PolymorphicWritable.write(dos, m);    } finally {        Closeables.close(dos, false);    }    return PolymorphicWritable.read(new DataInputStream(new ByteArrayInputStream(buf.toByteArray())), clazz);}
0
public void onlineAucRoundtrip() throws IOException
{    RandomUtils.useTestSeed();    OnlineAuc auc1 = new GlobalOnlineAuc();    Random gen = RandomUtils.getRandom();    for (int i = 0; i < 10000; i++) {        auc1.addSample(0, gen.nextGaussian());        auc1.addSample(1, gen.nextGaussian() + 1);    }    assertEquals(0.76, auc1.auc(), 0.01);    OnlineAuc auc3 = roundTrip(auc1, OnlineAuc.class);    assertEquals(auc1.auc(), auc3.auc(), 0);    for (int i = 0; i < 1000; i++) {        auc1.addSample(0, gen.nextGaussian());        auc1.addSample(1, gen.nextGaussian() + 1);        auc3.addSample(0, gen.nextGaussian());        auc3.addSample(1, gen.nextGaussian() + 1);    }    assertEquals(auc1.auc(), auc3.auc(), 0.01);}
0
public void onlineLogisticRegressionRoundTrip() throws IOException
{    OnlineLogisticRegression olr = new OnlineLogisticRegression(2, 5, new L1());    train(olr, 100);    OnlineLogisticRegression olr3 = roundTrip(olr, OnlineLogisticRegression.class);    assertEquals(0, olr.getBeta().minus(olr3.getBeta()).aggregate(Functions.MAX, Functions.IDENTITY), 1.0e-6);    train(olr, 100);    train(olr3, 100);    assertEquals(0, olr.getBeta().minus(olr3.getBeta()).aggregate(Functions.MAX, Functions.IDENTITY), 1.0e-6);    olr.close();    olr3.close();}
0
public void crossFoldLearnerRoundTrip() throws IOException
{    CrossFoldLearner learner = new CrossFoldLearner(5, 2, 5, new L1());    train(learner, 100);    CrossFoldLearner olr3 = roundTrip(learner, CrossFoldLearner.class);    double auc1 = learner.auc();    assertTrue(auc1 > 0.85);    assertEquals(auc1, learner.auc(), 1.0e-6);    assertEquals(auc1, olr3.auc(), 1.0e-6);    train(learner, 100);    train(learner, 100);    train(olr3, 100);    assertEquals(learner.auc(), learner.auc(), 0.02);    assertEquals(learner.auc(), olr3.auc(), 0.02);    double auc2 = learner.auc();    assertTrue(auc2 > auc1);    learner.close();    olr3.close();}
0
public void adaptiveLogisticRegressionRoundTrip() throws IOException
{    AdaptiveLogisticRegression learner = new AdaptiveLogisticRegression(2, 5, new L1());    learner.setInterval(200);    train(learner, 400);    AdaptiveLogisticRegression olr3 = roundTrip(learner, AdaptiveLogisticRegression.class);    double auc1 = learner.auc();    assertTrue(auc1 > 0.85);    assertEquals(auc1, learner.auc(), 1.0e-6);    assertEquals(auc1, olr3.auc(), 1.0e-6);    train(learner, 1000);    train(learner, 1000);    train(olr3, 1000);    assertEquals(learner.auc(), learner.auc(), 0.005);    assertEquals(learner.auc(), olr3.auc(), 0.005);    double auc2 = learner.auc();    assertTrue(String.format("%.3f > %.3f", auc2, auc1), auc2 > auc1);    learner.close();    olr3.close();}
0
private static void train(OnlineLearner olr, int n)
{    Vector beta = new DenseVector(new double[] { 1, -1, 0, 0.5, -0.5 });    Random gen = RandomUtils.getRandom();    for (int i = 0; i < n; i++) {        Vector x = randomVector(gen, 5);        int target = gen.nextDouble() < beta.dot(x) ? 1 : 0;        olr.train(target, x);    }}
0
private static Vector randomVector(final Random gen, int n)
{    Vector x = new DenseVector(n);    x.assign(new DoubleFunction() {        @Override        public double apply(double v) {            return gen.nextGaussian();        }    });    return x;}
0
public double apply(double v)
{    return gen.nextGaussian();}
0
 Matrix getInput()
{    return input;}
0
 Vector readStandardData() throws IOException
{                input = readCsv("sgd.csv");        Vector target = new DenseVector(60);    target.assign(0);    target.viewPart(30, 30).assign(1);    return target;}
0
 static void train(Matrix input, Vector target, OnlineLearner lr)
{    RandomUtils.useTestSeed();    Random gen = RandomUtils.getRandom();        for (int row : permute(gen, 60)) {        lr.train((int) target.get(row), input.viewRow(row));    }    lr.close();}
0
 static void test(Matrix input, Vector target, AbstractVectorClassifier lr, double expected_mean_error, double expected_absolute_error)
{        Matrix tmp = lr.classify(input);        double meanAbsoluteError = tmp.viewColumn(0).minus(target).aggregate(Functions.PLUS, Functions.ABS) / 60;        double maxAbsoluteError = tmp.viewColumn(0).minus(target).aggregate(Functions.MAX, Functions.ABS);    System.out.printf("mAE = %.4f, maxAE = %.4f\n", meanAbsoluteError, maxAbsoluteError);    assertEquals(0, meanAbsoluteError, expected_mean_error);    assertEquals(0, maxAbsoluteError, expected_absolute_error);        Vector v = lr.classifyScalar(input);    assertEquals(0, v.minus(tmp.viewColumn(0)).norm(1), 1.0e-5);    v = lr.classifyFull(input).viewColumn(1);    assertEquals(0, v.minus(tmp.viewColumn(0)).norm(1), 1.0e-4);}
0
 static int[] permute(Random gen, int max)
{    int[] permutation = new int[max];    permutation[0] = 0;    for (int i = 1; i < max; i++) {        int n = gen.nextInt(i + 1);        if (n == i) {            permutation[i] = i;        } else {            permutation[i] = permutation[n];            permutation[n] = i;        }    }    return permutation;}
0
 static Matrix readCsv(String resourceName) throws IOException
{    Splitter onCommas = Splitter.on(',').trimResults(CharMatcher.anyOf(" \""));    Readable isr = new InputStreamReader(Resources.getResource(resourceName).openStream(), Charsets.UTF_8);    List<String> data = CharStreams.readLines(isr);    String first = data.get(0);    data = data.subList(1, data.size());    List<String> values = Lists.newArrayList(onCommas.split(first));    Matrix r = new DenseMatrix(data.size(), values.size());    int column = 0;    Map<String, Integer> labels = Maps.newHashMap();    for (String value : values) {        labels.put(value, column);        column++;    }    r.setColumnLabelBindings(labels);    int row = 0;    for (String line : data) {        column = 0;        values = Lists.newArrayList(onCommas.split(line));        for (String value : values) {            r.set(row, column, Double.parseDouble(value));            column++;        }        row++;    }    return r;}
0
public void crossValidation() throws IOException
{    Vector target = readStandardData();    CrossFoldLearner lr = new CrossFoldLearner(5, 2, 8, new L1()).lambda(1 * 1.0e-3).learningRate(50);    train(getInput(), target, lr);    System.out.printf("%.2f %.5f\n", lr.auc(), lr.logLikelihood());    test(getInput(), target, lr, 0.05, 0.3);}
0
public void crossValidatedAuc() throws IOException
{    RandomUtils.useTestSeed();    Random gen = RandomUtils.getRandom();    Matrix data = readCsv("cancer.csv");    CrossFoldLearner lr = new CrossFoldLearner(5, 2, 10, new L1()).stepOffset(10).decayExponent(0.7).lambda(1 * 1.0e-3).learningRate(5);    int k = 0;    int[] ordering = permute(gen, data.numRows());    for (int epoch = 0; epoch < 100; epoch++) {        for (int row : ordering) {            lr.train(row, (int) data.get(row, 9), data.viewRow(row));            System.out.printf("%d,%d,%.3f\n", epoch, k++, lr.auc());        }        assertEquals(1, lr.auc(), 0.2);    }    assertEquals(1, lr.auc(), 0.1);}
0
public void testClassify()
{    OnlineLogisticRegression lr = new OnlineLogisticRegression(3, 2, new L2(1));        lr.setBeta(0, 0, -1);    lr.setBeta(1, 0, -2);        Vector v = lr.classify(new DenseVector(new double[] { 0, 0 }));    assertEquals(1 / 3.0, v.get(0), 1.0e-8);    assertEquals(1 / 3.0, v.get(1), 1.0e-8);    v = lr.classifyFull(new DenseVector(new double[] { 0, 0 }));    assertEquals(1.0, v.zSum(), 1.0e-8);    assertEquals(1 / 3.0, v.get(0), 1.0e-8);    assertEquals(1 / 3.0, v.get(1), 1.0e-8);    assertEquals(1 / 3.0, v.get(2), 1.0e-8);        v = lr.classify(new DenseVector(new double[] { 0, 1 }));    assertEquals(1 / 3.0, v.get(0), 1.0e-3);    assertEquals(1 / 3.0, v.get(1), 1.0e-3);    v = lr.classifyFull(new DenseVector(new double[] { 0, 1 }));    assertEquals(1.0, v.zSum(), 1.0e-8);    assertEquals(1 / 3.0, v.get(0), 1.0e-3);    assertEquals(1 / 3.0, v.get(1), 1.0e-3);    assertEquals(1 / 3.0, v.get(2), 1.0e-3);        v = lr.classify(new DenseVector(new double[] { 1, 0 }));    assertEquals(Math.exp(-1) / (1 + Math.exp(-1) + Math.exp(-2)), v.get(0), 1.0e-8);    assertEquals(Math.exp(-2) / (1 + Math.exp(-1) + Math.exp(-2)), v.get(1), 1.0e-8);    v = lr.classifyFull(new DenseVector(new double[] { 1, 0 }));    assertEquals(1.0, v.zSum(), 1.0e-8);    assertEquals(1 / (1 + Math.exp(-1) + Math.exp(-2)), v.get(0), 1.0e-8);    assertEquals(Math.exp(-1) / (1 + Math.exp(-1) + Math.exp(-2)), v.get(1), 1.0e-8);    assertEquals(Math.exp(-2) / (1 + Math.exp(-1) + Math.exp(-2)), v.get(2), 1.0e-8);    lr.setBeta(0, 1, 1);    v = lr.classifyFull(new DenseVector(new double[] { 1, 1 }));    assertEquals(1.0, v.zSum(), 1.0e-8);    assertEquals(Math.exp(0) / (1 + Math.exp(0) + Math.exp(-2)), v.get(1), 1.0e-3);    assertEquals(Math.exp(-2) / (1 + Math.exp(0) + Math.exp(-2)), v.get(2), 1.0e-3);    assertEquals(1 / (1 + Math.exp(0) + Math.exp(-2)), v.get(0), 1.0e-3);    lr.setBeta(1, 1, 3);    v = lr.classifyFull(new DenseVector(new double[] { 1, 1 }));    assertEquals(1.0, v.zSum(), 1.0e-8);    assertEquals(Math.exp(0) / (1 + Math.exp(0) + Math.exp(1)), v.get(1), 1.0e-8);    assertEquals(Math.exp(1) / (1 + Math.exp(0) + Math.exp(1)), v.get(2), 1.0e-8);    assertEquals(1 / (1 + Math.exp(0) + Math.exp(1)), v.get(0), 1.0e-8);}
0
public void iris() throws IOException
{                                                                            RandomUtils.useTestSeed();    Splitter onComma = Splitter.on(",");        List<String> raw = Resources.readLines(Resources.getResource("iris.csv"), Charsets.UTF_8);        List<Vector> data = Lists.newArrayList();        List<Integer> target = Lists.newArrayList();        Dictionary dict = new Dictionary();        List<Integer> order = Lists.newArrayList();    for (String line : raw.subList(1, raw.size())) {                order.add(order.size());                Vector v = new DenseVector(5);        v.set(0, 1);        int i = 1;        Iterable<String> values = onComma.split(line);        for (String value : Iterables.limit(values, 4)) {            v.set(i++, Double.parseDouble(value));        }        data.add(v);                target.add(dict.intern(Iterables.get(values, 4)));    }            Random random = RandomUtils.getRandom();    Collections.shuffle(order, random);        List<Integer> train = order.subList(0, 100);    List<Integer> test = order.subList(100, 150);                int[] correct = new int[test.size() + 1];    for (int run = 0; run < 200; run++) {        OnlineLogisticRegression lr = new OnlineLogisticRegression(3, 5, new L2(1));                for (int pass = 0; pass < 30; pass++) {            Collections.shuffle(train, random);            for (int k : train) {                lr.train(target.get(k), data.get(k));            }        }                int x = 0;        int[] count = new int[3];        for (Integer k : test) {            int r = lr.classifyFull(data.get(k)).maxValueIndex();            count[r]++;            x += r == target.get(k) ? 1 : 0;        }        correct[x]++;    }        for (int i = 0; i < Math.floor(0.95 * test.size()); i++) {        assertEquals(String.format("%d trials had unacceptable accuracy of only %.0f%%: ", correct[i], 100.0 * i / test.size()), 0, correct[i]);    }        assertEquals(String.format("%d trials had unrealistic accuracy of 100%%", correct[test.size() - 1]), 0, correct[test.size()]);}
1
public void testTrain() throws Exception
{    Vector target = readStandardData();                        OnlineLogisticRegression lr = new OnlineLogisticRegression(2, 8, new L1()).lambda(1 * 1.0e-3).learningRate(50);    train(getInput(), target, lr);    test(getInput(), target, lr, 0.05, 0.3);}
0
public void testSerializationAndDeSerialization() throws Exception
{    OnlineLogisticRegression lr = new OnlineLogisticRegression(2, 8, new L1()).lambda(1 * 1.0e-3).stepOffset(11).alpha(0.01).learningRate(50).decayExponent(-0.02);    lr.close();    byte[] output;    try (ByteArrayOutputStream byteArrayOutputStream = new ByteArrayOutputStream();        DataOutputStream dataOutputStream = new DataOutputStream(byteArrayOutputStream)) {        PolymorphicWritable.write(dataOutputStream, lr);        output = byteArrayOutputStream.toByteArray();    }    OnlineLogisticRegression read;    try (ByteArrayInputStream byteArrayInputStream = new ByteArrayInputStream(output);        DataInputStream dataInputStream = new DataInputStream(byteArrayInputStream)) {        read = PolymorphicWritable.read(dataInputStream, OnlineLogisticRegression.class);    }        Assert.assertEquals((1.0e-3), read.getLambda(), 1.0e-7);            Field stepOffset = lr.getClass().getDeclaredField("stepOffset");    stepOffset.setAccessible(true);    int stepOffsetVal = (Integer) stepOffset.get(lr);    Assert.assertEquals(11, stepOffsetVal);        Field decayFactor = lr.getClass().getDeclaredField("decayFactor");    decayFactor.setAccessible(true);    double decayFactorVal = (Double) decayFactor.get(lr);    Assert.assertEquals(0.01, decayFactorVal, 1.0e-7);        Field mu0 = lr.getClass().getDeclaredField("mu0");    mu0.setAccessible(true);    double mu0Val = (Double) mu0.get(lr);    Assert.assertEquals(50, mu0Val, 1.0e-7);        Field forgettingExponent = lr.getClass().getDeclaredField("forgettingExponent");    forgettingExponent.setAccessible(true);    double forgettingExponentVal = (Double) forgettingExponent.get(lr);    Assert.assertEquals(-0.02, forgettingExponentVal, 1.0e-7);}
0
public void testPassiveAggressive() throws IOException
{    Vector target = readStandardData();    PassiveAggressive pa = new PassiveAggressive(2, 8).learningRate(0.1);    train(getInput(), target, pa);    test(getInput(), target, pa, 0.11, 0.31);}
0
private static List<VectorWritable> getPointsWritable()
{    List<VectorWritable> points = Lists.newArrayList();    for (double[] fr : RAW) {        Vector vec = new RandomAccessSparseVector(fr.length);        vec.assign(fr);        points.add(new VectorWritable(vec));    }    return points;}
0
private static List<Vector> getPoints()
{    List<Vector> points = Lists.newArrayList();    for (double[] fr : RAW) {        Vector vec = new RandomAccessSparseVector(fr.length);        vec.assign(fr);        points.add(vec);    }    return points;}
0
private static void printCanopies(Iterable<Canopy> canopies)
{    for (Canopy canopy : canopies) {        System.out.println(canopy.asFormatString(null));    }}
0
public void setUp() throws Exception
{    super.setUp();    fs = FileSystem.get(getConfiguration());    referenceManhattan = CanopyClusterer.createCanopies(getPoints(), manhattanDistanceMeasure, 3.1, 2.1);    manhattanCentroids = CanopyClusterer.getCenters(referenceManhattan);    referenceEuclidean = CanopyClusterer.createCanopies(getPoints(), euclideanDistanceMeasure, 3.1, 2.1);    euclideanCentroids = CanopyClusterer.getCenters(referenceEuclidean);}
0
public void testReferenceManhattan() throws Exception
{        printCanopies(referenceManhattan);    assertEquals("number of canopies", 3, referenceManhattan.size());    for (int canopyIx = 0; canopyIx < referenceManhattan.size(); canopyIx++) {        Canopy testCanopy = referenceManhattan.get(canopyIx);        int[] expectedNumPoints = { 4, 4, 3 };        double[][] expectedCentroids = { { 1.5, 1.5 }, { 4.0, 4.0 }, { 4.666666666666667, 4.6666666666666667 } };        assertEquals("canopy points " + canopyIx, testCanopy.getNumObservations(), expectedNumPoints[canopyIx]);        double[] refCentroid = expectedCentroids[canopyIx];        Vector testCentroid = testCanopy.computeCentroid();        for (int pointIx = 0; pointIx < refCentroid.length; pointIx++) {            assertEquals("canopy centroid " + canopyIx + '[' + pointIx + ']', refCentroid[pointIx], testCentroid.get(pointIx), EPSILON);        }    }}
0
public void testReferenceEuclidean() throws Exception
{        printCanopies(referenceEuclidean);    assertEquals("number of canopies", 3, referenceEuclidean.size());    int[] expectedNumPoints = { 5, 5, 3 };    double[][] expectedCentroids = { { 1.8, 1.8 }, { 4.2, 4.2 }, { 4.666666666666667, 4.666666666666667 } };    for (int canopyIx = 0; canopyIx < referenceEuclidean.size(); canopyIx++) {        Canopy testCanopy = referenceEuclidean.get(canopyIx);        assertEquals("canopy points " + canopyIx, testCanopy.getNumObservations(), expectedNumPoints[canopyIx]);        double[] refCentroid = expectedCentroids[canopyIx];        Vector testCentroid = testCanopy.computeCentroid();        for (int pointIx = 0; pointIx < refCentroid.length; pointIx++) {            assertEquals("canopy centroid " + canopyIx + '[' + pointIx + ']', refCentroid[pointIx], testCentroid.get(pointIx), EPSILON);        }    }}
0
public void testCanopyMapperManhattan() throws Exception
{    CanopyMapper mapper = new CanopyMapper();    Configuration conf = getConfiguration();    conf.set(CanopyConfigKeys.DISTANCE_MEASURE_KEY, manhattanDistanceMeasure.getClass().getName());    conf.set(CanopyConfigKeys.T1_KEY, String.valueOf(3.1));    conf.set(CanopyConfigKeys.T2_KEY, String.valueOf(2.1));    conf.set(CanopyConfigKeys.CF_KEY, "0");    DummyRecordWriter<Text, VectorWritable> writer = new DummyRecordWriter<>();    Mapper<WritableComparable<?>, VectorWritable, Text, VectorWritable>.Context context = DummyRecordWriter.build(mapper, conf, writer);    mapper.setup(context);    List<VectorWritable> points = getPointsWritable();        for (VectorWritable point : points) {        mapper.map(new Text(), point, context);    }    mapper.cleanup(context);    assertEquals("Number of map results", 1, writer.getData().size());        List<VectorWritable> data = writer.getValue(new Text("centroid"));    assertEquals("Number of centroids", 3, data.size());    for (int i = 0; i < data.size(); i++) {        assertEquals("Centroid error", manhattanCentroids.get(i).asFormatString(), data.get(i).get().asFormatString());    }}
0
public void testCanopyMapperEuclidean() throws Exception
{    CanopyMapper mapper = new CanopyMapper();    Configuration conf = getConfiguration();    conf.set(CanopyConfigKeys.DISTANCE_MEASURE_KEY, euclideanDistanceMeasure.getClass().getName());    conf.set(CanopyConfigKeys.T1_KEY, String.valueOf(3.1));    conf.set(CanopyConfigKeys.T2_KEY, String.valueOf(2.1));    conf.set(CanopyConfigKeys.CF_KEY, "0");    DummyRecordWriter<Text, VectorWritable> writer = new DummyRecordWriter<>();    Mapper<WritableComparable<?>, VectorWritable, Text, VectorWritable>.Context context = DummyRecordWriter.build(mapper, conf, writer);    mapper.setup(context);    List<VectorWritable> points = getPointsWritable();        for (VectorWritable point : points) {        mapper.map(new Text(), point, context);    }    mapper.cleanup(context);    assertEquals("Number of map results", 1, writer.getData().size());        List<VectorWritable> data = writer.getValue(new Text("centroid"));    assertEquals("Number of centroids", 3, data.size());    for (int i = 0; i < data.size(); i++) {        assertEquals("Centroid error", euclideanCentroids.get(i).asFormatString(), data.get(i).get().asFormatString());    }}
0
public void testCanopyReducerManhattan() throws Exception
{    CanopyReducer reducer = new CanopyReducer();    Configuration conf = getConfiguration();    conf.set(CanopyConfigKeys.DISTANCE_MEASURE_KEY, "org.apache.mahout.common.distance.ManhattanDistanceMeasure");    conf.set(CanopyConfigKeys.T1_KEY, String.valueOf(3.1));    conf.set(CanopyConfigKeys.T2_KEY, String.valueOf(2.1));    conf.set(CanopyConfigKeys.CF_KEY, "0");    DummyRecordWriter<Text, ClusterWritable> writer = new DummyRecordWriter<>();    Reducer<Text, VectorWritable, Text, ClusterWritable>.Context context = DummyRecordWriter.build(reducer, conf, writer, Text.class, VectorWritable.class);    reducer.setup(context);    List<VectorWritable> points = getPointsWritable();    reducer.reduce(new Text("centroid"), points, context);    Iterable<Text> keys = writer.getKeysInInsertionOrder();    assertEquals("Number of centroids", 3, Iterables.size(keys));    int i = 0;    for (Text key : keys) {        List<ClusterWritable> data = writer.getValue(key);        ClusterWritable clusterWritable = data.get(0);        Canopy canopy = (Canopy) clusterWritable.getValue();        assertEquals(manhattanCentroids.get(i).asFormatString() + " is not equal to " + canopy.computeCentroid().asFormatString(), manhattanCentroids.get(i), canopy.computeCentroid());        i++;    }}
0
public void testCanopyReducerEuclidean() throws Exception
{    CanopyReducer reducer = new CanopyReducer();    Configuration conf = getConfiguration();    conf.set(CanopyConfigKeys.DISTANCE_MEASURE_KEY, "org.apache.mahout.common.distance.EuclideanDistanceMeasure");    conf.set(CanopyConfigKeys.T1_KEY, String.valueOf(3.1));    conf.set(CanopyConfigKeys.T2_KEY, String.valueOf(2.1));    conf.set(CanopyConfigKeys.CF_KEY, "0");    DummyRecordWriter<Text, ClusterWritable> writer = new DummyRecordWriter<>();    Reducer<Text, VectorWritable, Text, ClusterWritable>.Context context = DummyRecordWriter.build(reducer, conf, writer, Text.class, VectorWritable.class);    reducer.setup(context);    List<VectorWritable> points = getPointsWritable();    reducer.reduce(new Text("centroid"), points, context);    Iterable<Text> keys = writer.getKeysInInsertionOrder();    assertEquals("Number of centroids", 3, Iterables.size(keys));    int i = 0;    for (Text key : keys) {        List<ClusterWritable> data = writer.getValue(key);        ClusterWritable clusterWritable = data.get(0);        Canopy canopy = (Canopy) clusterWritable.getValue();        assertEquals(euclideanCentroids.get(i).asFormatString() + " is not equal to " + canopy.computeCentroid().asFormatString(), euclideanCentroids.get(i), canopy.computeCentroid());        i++;    }}
0
public void testCanopyGenManhattanMR() throws Exception
{    List<VectorWritable> points = getPointsWritable();    Configuration config = getConfiguration();    ClusteringTestUtils.writePointsToFile(points, getTestTempFilePath("testdata/file1"), fs, config);    ClusteringTestUtils.writePointsToFile(points, getTestTempFilePath("testdata/file2"), fs, config);        Path output = getTestTempDirPath("output");    CanopyDriver.run(config, getTestTempDirPath("testdata"), output, manhattanDistanceMeasure, 3.1, 2.1, false, 0.0, false);        Path path = new Path(output, "clusters-0-final/part-r-00000");    FileSystem fs = FileSystem.get(path.toUri(), config);    SequenceFile.Reader reader = new SequenceFile.Reader(fs, path, config);    try {        Writable key = new Text();        ClusterWritable clusterWritable = new ClusterWritable();        assertTrue("more to come", reader.next(key, clusterWritable));        assertEquals("1st key", "C-0", key.toString());        List<Pair<Double, Double>> refCenters = Lists.newArrayList();        refCenters.add(new Pair<>(1.5, 1.5));        refCenters.add(new Pair<>(4.333333333333334, 4.333333333333334));        Pair<Double, Double> c = new Pair<>(clusterWritable.getValue().getCenter().get(0), clusterWritable.getValue().getCenter().get(1));        assertTrue("center " + c + " not found", findAndRemove(c, refCenters, EPSILON));        assertTrue("more to come", reader.next(key, clusterWritable));        assertEquals("2nd key", "C-1", key.toString());        c = new Pair<>(clusterWritable.getValue().getCenter().get(0), clusterWritable.getValue().getCenter().get(1));        assertTrue("center " + c + " not found", findAndRemove(c, refCenters, EPSILON));        assertFalse("more to come", reader.next(key, clusterWritable));    } finally {        Closeables.close(reader, true);    }}
0
 static boolean findAndRemove(Pair<Double, Double> target, Collection<Pair<Double, Double>> list, double epsilon)
{    for (Pair<Double, Double> curr : list) {        if ((Math.abs(target.getFirst() - curr.getFirst()) < epsilon) && (Math.abs(target.getSecond() - curr.getSecond()) < epsilon)) {            list.remove(curr);            return true;        }    }    return false;}
0
public void testCanopyGenEuclideanMR() throws Exception
{    List<VectorWritable> points = getPointsWritable();    Configuration config = getConfiguration();    ClusteringTestUtils.writePointsToFile(points, getTestTempFilePath("testdata/file1"), fs, config);    ClusteringTestUtils.writePointsToFile(points, getTestTempFilePath("testdata/file2"), fs, config);        Path output = getTestTempDirPath("output");    CanopyDriver.run(config, getTestTempDirPath("testdata"), output, euclideanDistanceMeasure, 3.1, 2.1, false, 0.0, false);        Path path = new Path(output, "clusters-0-final/part-r-00000");    FileSystem fs = FileSystem.get(path.toUri(), config);    SequenceFile.Reader reader = new SequenceFile.Reader(fs, path, config);    try {        Writable key = new Text();        ClusterWritable clusterWritable = new ClusterWritable();        assertTrue("more to come", reader.next(key, clusterWritable));        assertEquals("1st key", "C-0", key.toString());        List<Pair<Double, Double>> refCenters = Lists.newArrayList();        refCenters.add(new Pair<>(1.8, 1.8));        refCenters.add(new Pair<>(4.433333333333334, 4.433333333333334));        Pair<Double, Double> c = new Pair<>(clusterWritable.getValue().getCenter().get(0), clusterWritable.getValue().getCenter().get(1));        assertTrue("center " + c + " not found", findAndRemove(c, refCenters, EPSILON));        assertTrue("more to come", reader.next(key, clusterWritable));        assertEquals("2nd key", "C-1", key.toString());        c = new Pair<>(clusterWritable.getValue().getCenter().get(0), clusterWritable.getValue().getCenter().get(1));        assertTrue("center " + c + " not found", findAndRemove(c, refCenters, EPSILON));        assertFalse("more to come", reader.next(key, clusterWritable));    } finally {        Closeables.close(reader, true);    }}
0
public void testClusteringManhattanSeq() throws Exception
{    List<VectorWritable> points = getPointsWritable();    Configuration config = getConfiguration();    ClusteringTestUtils.writePointsToFile(points, getTestTempFilePath("testdata/file1"), fs, config);        Path output = getTestTempDirPath("output");    CanopyDriver.run(config, getTestTempDirPath("testdata"), output, manhattanDistanceMeasure, 3.1, 2.1, true, 0.0, true);        Path path = new Path(output, "clusters-0-final/part-r-00000");    int ix = 0;    for (ClusterWritable clusterWritable : new SequenceFileValueIterable<ClusterWritable>(path, true, config)) {        assertEquals("Center [" + ix + ']', manhattanCentroids.get(ix), clusterWritable.getValue().getCenter());        ix++;    }    path = new Path(output, "clusteredPoints/part-m-0");    long count = HadoopUtil.countRecords(path, config);    assertEquals("number of points", points.size(), count);}
0
public void testClusteringEuclideanSeq() throws Exception
{    List<VectorWritable> points = getPointsWritable();    Configuration config = getConfiguration();    ClusteringTestUtils.writePointsToFile(points, getTestTempFilePath("testdata/file1"), fs, config);        Path output = getTestTempDirPath("output");    String[] args = { optKey(DefaultOptionCreator.INPUT_OPTION), getTestTempDirPath("testdata").toString(), optKey(DefaultOptionCreator.OUTPUT_OPTION), output.toString(), optKey(DefaultOptionCreator.DISTANCE_MEASURE_OPTION), EuclideanDistanceMeasure.class.getName(), optKey(DefaultOptionCreator.T1_OPTION), "3.1", optKey(DefaultOptionCreator.T2_OPTION), "2.1", optKey(DefaultOptionCreator.CLUSTERING_OPTION), optKey(DefaultOptionCreator.OVERWRITE_OPTION), optKey(DefaultOptionCreator.METHOD_OPTION), DefaultOptionCreator.SEQUENTIAL_METHOD };    ToolRunner.run(config, new CanopyDriver(), args);        Path path = new Path(output, "clusters-0-final/part-r-00000");    int ix = 0;    for (ClusterWritable clusterWritable : new SequenceFileValueIterable<ClusterWritable>(path, true, config)) {        assertEquals("Center [" + ix + ']', euclideanCentroids.get(ix), clusterWritable.getValue().getCenter());        ix++;    }    path = new Path(output, "clusteredPoints/part-m-0");    long count = HadoopUtil.countRecords(path, config);    assertEquals("number of points", points.size(), count);}
0
public void testClusteringEuclideanWithOutlierRemovalSeq() throws Exception
{    List<VectorWritable> points = getPointsWritable();    Configuration config = getConfiguration();    ClusteringTestUtils.writePointsToFile(points, getTestTempFilePath("testdata/file1"), fs, config);        Path output = getTestTempDirPath("output");    String[] args = { optKey(DefaultOptionCreator.INPUT_OPTION), getTestTempDirPath("testdata").toString(), optKey(DefaultOptionCreator.OUTPUT_OPTION), output.toString(), optKey(DefaultOptionCreator.DISTANCE_MEASURE_OPTION), EuclideanDistanceMeasure.class.getName(), optKey(DefaultOptionCreator.T1_OPTION), "3.1", optKey(DefaultOptionCreator.T2_OPTION), "2.1", optKey(DefaultOptionCreator.OUTLIER_THRESHOLD), "0.5", optKey(DefaultOptionCreator.CLUSTERING_OPTION), optKey(DefaultOptionCreator.OVERWRITE_OPTION), optKey(DefaultOptionCreator.METHOD_OPTION), DefaultOptionCreator.SEQUENTIAL_METHOD };    ToolRunner.run(config, new CanopyDriver(), args);        Path path = new Path(output, "clusters-0-final/part-r-00000");    int ix = 0;    for (ClusterWritable clusterWritable : new SequenceFileValueIterable<ClusterWritable>(path, true, config)) {        assertEquals("Center [" + ix + ']', euclideanCentroids.get(ix), clusterWritable.getValue().getCenter());        ix++;    }    path = new Path(output, "clusteredPoints/part-m-0");    long count = HadoopUtil.countRecords(path, config);    int expectedPointsHavingPDFGreaterThanThreshold = 6;    assertEquals("number of points", expectedPointsHavingPDFGreaterThanThreshold, count);}
0
public void testClusteringManhattanMR() throws Exception
{    List<VectorWritable> points = getPointsWritable();    Configuration conf = getConfiguration();    ClusteringTestUtils.writePointsToFile(points, true, getTestTempFilePath("testdata/file1"), fs, conf);    ClusteringTestUtils.writePointsToFile(points, true, getTestTempFilePath("testdata/file2"), fs, conf);        Path output = getTestTempDirPath("output");    CanopyDriver.run(conf, getTestTempDirPath("testdata"), output, manhattanDistanceMeasure, 3.1, 2.1, true, 0.0, false);    Path path = new Path(output, "clusteredPoints/part-m-00000");    long count = HadoopUtil.countRecords(path, conf);    assertEquals("number of points", points.size(), count);}
0
public void testClusteringEuclideanMR() throws Exception
{    List<VectorWritable> points = getPointsWritable();    Configuration conf = getConfiguration();    ClusteringTestUtils.writePointsToFile(points, true, getTestTempFilePath("testdata/file1"), fs, conf);    ClusteringTestUtils.writePointsToFile(points, true, getTestTempFilePath("testdata/file2"), fs, conf);        Path output = getTestTempDirPath("output");    String[] args = { optKey(DefaultOptionCreator.INPUT_OPTION), getTestTempDirPath("testdata").toString(), optKey(DefaultOptionCreator.OUTPUT_OPTION), output.toString(), optKey(DefaultOptionCreator.DISTANCE_MEASURE_OPTION), EuclideanDistanceMeasure.class.getName(), optKey(DefaultOptionCreator.T1_OPTION), "3.1", optKey(DefaultOptionCreator.T2_OPTION), "2.1", optKey(DefaultOptionCreator.CLUSTERING_OPTION), optKey(DefaultOptionCreator.OVERWRITE_OPTION) };    ToolRunner.run(getConfiguration(), new CanopyDriver(), args);    Path path = new Path(output, "clusteredPoints/part-m-00000");    long count = HadoopUtil.countRecords(path, conf);    assertEquals("number of points", points.size(), count);}
0
public void testClusteringEuclideanWithOutlierRemovalMR() throws Exception
{    List<VectorWritable> points = getPointsWritable();    Configuration conf = getConfiguration();    ClusteringTestUtils.writePointsToFile(points, true, getTestTempFilePath("testdata/file1"), fs, conf);    ClusteringTestUtils.writePointsToFile(points, true, getTestTempFilePath("testdata/file2"), fs, conf);        Path output = getTestTempDirPath("output");    String[] args = { optKey(DefaultOptionCreator.INPUT_OPTION), getTestTempDirPath("testdata").toString(), optKey(DefaultOptionCreator.OUTPUT_OPTION), output.toString(), optKey(DefaultOptionCreator.DISTANCE_MEASURE_OPTION), EuclideanDistanceMeasure.class.getName(), optKey(DefaultOptionCreator.T1_OPTION), "3.1", optKey(DefaultOptionCreator.T2_OPTION), "2.1", optKey(DefaultOptionCreator.OUTLIER_THRESHOLD), "0.7", optKey(DefaultOptionCreator.CLUSTERING_OPTION), optKey(DefaultOptionCreator.OVERWRITE_OPTION) };    ToolRunner.run(getConfiguration(), new CanopyDriver(), args);    Path path = new Path(output, "clusteredPoints/part-m-00000");    long count = HadoopUtil.countRecords(path, conf);    int expectedPointsAfterOutlierRemoval = 8;    assertEquals("number of points", expectedPointsAfterOutlierRemoval, count);}
0
public void testCanopyReducerT3T4Configuration() throws Exception
{    CanopyReducer reducer = new CanopyReducer();    Configuration conf = getConfiguration();    conf.set(CanopyConfigKeys.DISTANCE_MEASURE_KEY, "org.apache.mahout.common.distance.ManhattanDistanceMeasure");    conf.set(CanopyConfigKeys.T1_KEY, String.valueOf(3.1));    conf.set(CanopyConfigKeys.T2_KEY, String.valueOf(2.1));    conf.set(CanopyConfigKeys.T3_KEY, String.valueOf(1.1));    conf.set(CanopyConfigKeys.T4_KEY, String.valueOf(0.1));    conf.set(CanopyConfigKeys.CF_KEY, "0");    DummyRecordWriter<Text, ClusterWritable> writer = new DummyRecordWriter<>();    Reducer<Text, VectorWritable, Text, ClusterWritable>.Context context = DummyRecordWriter.build(reducer, conf, writer, Text.class, VectorWritable.class);    reducer.setup(context);    assertEquals(1.1, reducer.getCanopyClusterer().getT1(), EPSILON);    assertEquals(0.1, reducer.getCanopyClusterer().getT2(), EPSILON);}
0
public void testCanopyMapperClusterFilter() throws Exception
{    CanopyMapper mapper = new CanopyMapper();    Configuration conf = getConfiguration();    conf.set(CanopyConfigKeys.DISTANCE_MEASURE_KEY, manhattanDistanceMeasure.getClass().getName());    conf.set(CanopyConfigKeys.T1_KEY, String.valueOf(3.1));    conf.set(CanopyConfigKeys.T2_KEY, String.valueOf(2.1));    conf.set(CanopyConfigKeys.CF_KEY, "3");    DummyRecordWriter<Text, VectorWritable> writer = new DummyRecordWriter<>();    Mapper<WritableComparable<?>, VectorWritable, Text, VectorWritable>.Context context = DummyRecordWriter.build(mapper, conf, writer);    mapper.setup(context);    List<VectorWritable> points = getPointsWritable();        for (VectorWritable point : points) {        mapper.map(new Text(), point, context);    }    mapper.cleanup(context);    assertEquals("Number of map results", 1, writer.getData().size());        List<VectorWritable> data = writer.getValue(new Text("centroid"));    assertEquals("Number of centroids", 2, data.size());}
0
public void testCanopyReducerClusterFilter() throws Exception
{    CanopyReducer reducer = new CanopyReducer();    Configuration conf = getConfiguration();    conf.set(CanopyConfigKeys.DISTANCE_MEASURE_KEY, "org.apache.mahout.common.distance.ManhattanDistanceMeasure");    conf.set(CanopyConfigKeys.T1_KEY, String.valueOf(3.1));    conf.set(CanopyConfigKeys.T2_KEY, String.valueOf(2.1));    conf.set(CanopyConfigKeys.CF_KEY, "3");    DummyRecordWriter<Text, ClusterWritable> writer = new DummyRecordWriter<>();    Reducer<Text, VectorWritable, Text, ClusterWritable>.Context context = DummyRecordWriter.build(reducer, conf, writer, Text.class, VectorWritable.class);    reducer.setup(context);    List<VectorWritable> points = getPointsWritable();    reducer.reduce(new Text("centroid"), points, context);    Set<Text> keys = writer.getKeys();    assertEquals("Number of centroids", 2, keys.size());}
0
public void setUp() throws Exception
{    super.setUp();    Configuration conf = getConfiguration();    fs = FileSystem.get(conf);    firstCluster = Lists.newArrayList();    secondCluster = Lists.newArrayList();    thirdCluster = Lists.newArrayList();}
0
private static List<VectorWritable> getPointsWritable(double[][] raw)
{    List<VectorWritable> points = Lists.newArrayList();    for (double[] fr : raw) {        Vector vec = new RandomAccessSparseVector(fr.length);        vec.assign(fr);        points.add(new VectorWritable(vec));    }    return points;}
0
public void testVectorClassificationWithOutlierRemovalMR() throws Exception
{    List<VectorWritable> points = getPointsWritable(REFERENCE);    pointsPath = getTestTempDirPath("points");    clusteringOutputPath = getTestTempDirPath("output");    classifiedOutputPath = getTestTempDirPath("classifiedClusters");    HadoopUtil.delete(conf, classifiedOutputPath);    conf = getConfiguration();    ClusteringTestUtils.writePointsToFile(points, true, new Path(pointsPath, "file1"), fs, conf);    runClustering(pointsPath, conf, false);    runClassificationWithOutlierRemoval(false);    collectVectorsForAssertion();    assertVectorsWithOutlierRemoval();}
0
public void testVectorClassificationWithoutOutlierRemoval() throws Exception
{    List<VectorWritable> points = getPointsWritable(REFERENCE);    pointsPath = getTestTempDirPath("points");    clusteringOutputPath = getTestTempDirPath("output");    classifiedOutputPath = getTestTempDirPath("classify");    conf = getConfiguration();    ClusteringTestUtils.writePointsToFile(points, new Path(pointsPath, "file1"), fs, conf);    runClustering(pointsPath, conf, true);    runClassificationWithoutOutlierRemoval();    collectVectorsForAssertion();    assertVectorsWithoutOutlierRemoval();}
0
public void testVectorClassificationWithOutlierRemoval() throws Exception
{    List<VectorWritable> points = getPointsWritable(REFERENCE);    pointsPath = getTestTempDirPath("points");    clusteringOutputPath = getTestTempDirPath("output");    classifiedOutputPath = getTestTempDirPath("classify");    conf = getConfiguration();    ClusteringTestUtils.writePointsToFile(points, new Path(pointsPath, "file1"), fs, conf);    runClustering(pointsPath, conf, true);    runClassificationWithOutlierRemoval(true);    collectVectorsForAssertion();    assertVectorsWithOutlierRemoval();}
0
private void runClustering(Path pointsPath, Configuration conf, Boolean runSequential) throws IOException, InterruptedException, ClassNotFoundException
{    CanopyDriver.run(conf, pointsPath, clusteringOutputPath, new ManhattanDistanceMeasure(), 3.1, 2.1, false, 0.0, runSequential);    Path finalClustersPath = new Path(clusteringOutputPath, "clusters-0-final");    ClusterClassifier.writePolicy(new CanopyClusteringPolicy(), finalClustersPath);}
0
private void runClassificationWithoutOutlierRemoval() throws IOException, InterruptedException, ClassNotFoundException
{    ClusterClassificationDriver.run(getConfiguration(), pointsPath, clusteringOutputPath, classifiedOutputPath, 0.0, true, true);}
0
private void runClassificationWithOutlierRemoval(boolean runSequential) throws IOException, InterruptedException, ClassNotFoundException
{    ClusterClassificationDriver.run(getConfiguration(), pointsPath, clusteringOutputPath, classifiedOutputPath, 0.73, true, runSequential);}
0
private void collectVectorsForAssertion() throws IOException
{    Path[] partFilePaths = FileUtil.stat2Paths(fs.globStatus(classifiedOutputPath));    FileStatus[] listStatus = fs.listStatus(partFilePaths, PathFilters.partFilter());    for (FileStatus partFile : listStatus) {        SequenceFile.Reader classifiedVectors = new SequenceFile.Reader(fs, partFile.getPath(), conf);        Writable clusterIdAsKey = new IntWritable();        WeightedPropertyVectorWritable point = new WeightedPropertyVectorWritable();        while (classifiedVectors.next(clusterIdAsKey, point)) {            collectVector(clusterIdAsKey.toString(), point.getVector());        }    }}
0
private void collectVector(String clusterId, Vector vector)
{    if ("0".equals(clusterId)) {        firstCluster.add(vector);    } else if ("1".equals(clusterId)) {        secondCluster.add(vector);    } else if ("2".equals(clusterId)) {        thirdCluster.add(vector);    }}
0
private void assertVectorsWithOutlierRemoval()
{    checkClustersWithOutlierRemoval();}
0
private void assertVectorsWithoutOutlierRemoval()
{    assertFirstClusterWithoutOutlierRemoval();    assertSecondClusterWithoutOutlierRemoval();    assertThirdClusterWithoutOutlierRemoval();}
0
private void assertThirdClusterWithoutOutlierRemoval()
{    Assert.assertEquals(2, thirdCluster.size());    for (Vector vector : thirdCluster) {        Assert.assertTrue(ArrayUtils.contains(new String[] { "{0:9.0,1:9.0}", "{0:8.0,1:8.0}" }, vector.asFormatString()));    }}
0
private void assertSecondClusterWithoutOutlierRemoval()
{    Assert.assertEquals(4, secondCluster.size());    for (Vector vector : secondCluster) {        Assert.assertTrue(ArrayUtils.contains(new String[] { "{0:4.0,1:4.0}", "{0:5.0,1:4.0}", "{0:4.0,1:5.0}", "{0:5.0,1:5.0}" }, vector.asFormatString()));    }}
0
private void assertFirstClusterWithoutOutlierRemoval()
{    Assert.assertEquals(3, firstCluster.size());    for (Vector vector : firstCluster) {        Assert.assertTrue(ArrayUtils.contains(new String[] { "{0:1.0,1:1.0}", "{0:2.0,1:1.0}", "{0:1.0,1:2.0}" }, vector.asFormatString()));    }}
0
private void checkClustersWithOutlierRemoval()
{    Set<String> reference = Sets.newHashSet("{0:9.0,1:9.0}", "{0:1.0,1:1.0}");    List<List<Vector>> clusters = Lists.newArrayList();    clusters.add(firstCluster);    clusters.add(secondCluster);    clusters.add(thirdCluster);    int singletonCnt = 0;    int emptyCnt = 0;    for (List<Vector> vList : clusters) {        if (vList.isEmpty()) {            emptyCnt++;        } else {            singletonCnt++;            assertEquals("expecting only singleton clusters; got size=" + vList.size(), 1, vList.size());            if (vList.get(0).getClass().equals(NamedVector.class)) {                Assert.assertTrue("not expecting cluster:" + ((NamedVector) vList.get(0)).getDelegate().asFormatString(), reference.contains(((NamedVector) vList.get(0)).getDelegate().asFormatString()));                reference.remove(((NamedVector) vList.get(0)).getDelegate().asFormatString());            } else if (vList.get(0).getClass().equals(RandomAccessSparseVector.class)) {                Assert.assertTrue("not expecting cluster:" + vList.get(0).asFormatString(), reference.contains(vList.get(0).asFormatString()));                reference.remove(vList.get(0).asFormatString());            }        }    }    Assert.assertEquals("Different number of empty clusters than expected!", 1, emptyCnt);    Assert.assertEquals("Different number of singletons than expected!", 2, singletonCnt);    Assert.assertEquals("Didn't match all reference clusters!", 0, reference.size());}
0
public static void writePointsToFile(Iterable<VectorWritable> points, Path path, FileSystem fs, Configuration conf) throws IOException
{    writePointsToFile(points, false, path, fs, conf);}
0
public static void writePointsToFile(Iterable<VectorWritable> points, boolean intWritable, Path path, FileSystem fs, Configuration conf) throws IOException
{    SequenceFile.Writer writer = new SequenceFile.Writer(fs, conf, path, intWritable ? IntWritable.class : LongWritable.class, VectorWritable.class);    try {        int recNum = 0;        for (VectorWritable point : points) {            writer.append(intWritable ? new IntWritable(recNum++) : new LongWritable(recNum++), point);        }    } finally {        Closeables.close(writer, false);    }}
0
public static Matrix sampledCorpus(Matrix matrix, Random random, int numDocs, int numSamples, int numTopicsPerDoc)
{    Matrix corpus = new SparseRowMatrix(numDocs, matrix.numCols());    LDASampler modelSampler = new LDASampler(matrix, random);    Vector topicVector = new DenseVector(matrix.numRows());    for (int i = 0; i < numTopicsPerDoc; i++) {        int topic = random.nextInt(topicVector.size());        topicVector.set(topic, topicVector.get(topic) + 1);    }    for (int docId = 0; docId < numDocs; docId++) {        for (int sample : modelSampler.sample(topicVector, numSamples)) {            corpus.set(docId, sample, corpus.get(docId, sample) + 1);        }    }    return corpus;}
0
public static Matrix randomStructuredModel(int numTopics, int numTerms)
{    return randomStructuredModel(numTopics, numTerms, new DoubleFunction() {        @Override        public double apply(double d) {            return 1.0 / (1 + Math.abs(d));        }    });}
0
public double apply(double d)
{    return 1.0 / (1 + Math.abs(d));}
0
public static Matrix randomStructuredModel(int numTopics, int numTerms, DoubleFunction decay)
{    Matrix model = new DenseMatrix(numTopics, numTerms);    int width = numTerms / numTopics;    for (int topic = 0; topic < numTopics; topic++) {        int topicCentroid = width * (1 + topic);        for (int i = 0; i < numTerms; i++) {            int distance = Math.abs(topicCentroid - i);            if (distance > numTerms / 2) {                distance = numTerms - distance;            }            double v = decay.apply(distance);            model.set(topic, i, v);        }    }    return model;}
0
public int[] sample(Vector topicDistribution, int numSamples)
{    Preconditions.checkNotNull(topicDistribution);    Preconditions.checkArgument(numSamples > 0, "numSamples must be positive");    Preconditions.checkArgument(topicDistribution.size() == samplers.length, "topicDistribution must have same cardinality as the sampling model");    int[] samples = new int[numSamples];    Sampler topicSampler = new Sampler(random, topicDistribution);    for (int i = 0; i < numSamples; i++) {        samples[i] = samplers[topicSampler.sample()].sample();    }    return samples;}
0
public void setUp() throws Exception
{    super.setUp();    Configuration conf = getConfiguration();    fs = FileSystem.get(conf);}
0
private static Vector tweakValue(Vector point)
{    return point.plus(0.1);}
0
public void testFuzzyKMeansSeqJob() throws Exception
{    List<VectorWritable> points = TestKmeansClustering.getPointsWritable(TestKmeansClustering.REFERENCE);    Path pointsPath = getTestTempDirPath("points");    Path clustersPath = getTestTempDirPath("clusters");    Configuration conf = getConfiguration();    ClusteringTestUtils.writePointsToFile(points, new Path(pointsPath, "file1"), fs, conf);    for (int k = 0; k < points.size(); k++) {        System.out.println("testKFuzzyKMeansMRJob k= " + k);                SequenceFile.Writer writer = new SequenceFile.Writer(fs, conf, new Path(clustersPath, "part-00000"), Text.class, SoftCluster.class);        try {            for (int i = 0; i < k + 1; i++) {                Vector vec = tweakValue(points.get(i).get());                SoftCluster cluster = new SoftCluster(vec, i, measure);                /* add the center so the centroid will be correct upon output */                cluster.observe(cluster.getCenter(), 1);                                writer.append(new Text(cluster.getIdentifier()), cluster);            }        } finally {            Closeables.close(writer, false);        }                Path output = getTestTempDirPath("output" + k);        /*      FuzzyKMeansDriver.runJob(pointsPath,                                     clustersPath,                                     output,                                     EuclideanDistanceMeasure.class.getName(),                                     0.001,                                     2,                                     k + 1,                                     2,                                     false,                                     true,                                     0);      */        String[] args = { optKey(DefaultOptionCreator.INPUT_OPTION), pointsPath.toString(), optKey(DefaultOptionCreator.CLUSTERS_IN_OPTION), clustersPath.toString(), optKey(DefaultOptionCreator.OUTPUT_OPTION), output.toString(), optKey(DefaultOptionCreator.DISTANCE_MEASURE_OPTION), EuclideanDistanceMeasure.class.getName(), optKey(DefaultOptionCreator.CONVERGENCE_DELTA_OPTION), "0.001", optKey(DefaultOptionCreator.MAX_ITERATIONS_OPTION), "2", optKey(FuzzyKMeansDriver.M_OPTION), "2.0", optKey(DefaultOptionCreator.CLUSTERING_OPTION), optKey(DefaultOptionCreator.EMIT_MOST_LIKELY_OPTION), optKey(DefaultOptionCreator.OVERWRITE_OPTION), optKey(DefaultOptionCreator.METHOD_OPTION), DefaultOptionCreator.SEQUENTIAL_METHOD };        FuzzyKMeansDriver.main(args);        long count = HadoopUtil.countRecords(new Path(output, "clusteredPoints/part-m-0"), conf);        assertTrue(count > 0);    }}
0
public void testFuzzyKMeansMRJob() throws Exception
{    List<VectorWritable> points = TestKmeansClustering.getPointsWritable(TestKmeansClustering.REFERENCE);    Path pointsPath = getTestTempDirPath("points");    Path clustersPath = getTestTempDirPath("clusters");    Configuration conf = getConfiguration();    ClusteringTestUtils.writePointsToFile(points, new Path(pointsPath, "file1"), fs, conf);    for (int k = 0; k < points.size(); k++) {        System.out.println("testKFuzzyKMeansMRJob k= " + k);                SequenceFile.Writer writer = new SequenceFile.Writer(fs, conf, new Path(clustersPath, "part-00000"), Text.class, SoftCluster.class);        try {            for (int i = 0; i < k + 1; i++) {                Vector vec = tweakValue(points.get(i).get());                SoftCluster cluster = new SoftCluster(vec, i, measure);                /* add the center so the centroid will be correct upon output */                cluster.observe(cluster.getCenter(), 1);                                writer.append(new Text(cluster.getIdentifier()), cluster);            }        } finally {            Closeables.close(writer, false);        }                Path output = getTestTempDirPath("output" + k);        /*      FuzzyKMeansDriver.runJob(pointsPath,                                     clustersPath,                                     output,                                     EuclideanDistanceMeasure.class.getName(),                                     0.001,                                     2,                                     k + 1,                                     2,                                     false,                                     true,                                     0);      */        String[] args = { optKey(DefaultOptionCreator.INPUT_OPTION), pointsPath.toString(), optKey(DefaultOptionCreator.CLUSTERS_IN_OPTION), clustersPath.toString(), optKey(DefaultOptionCreator.OUTPUT_OPTION), output.toString(), optKey(DefaultOptionCreator.DISTANCE_MEASURE_OPTION), EuclideanDistanceMeasure.class.getName(), optKey(DefaultOptionCreator.CONVERGENCE_DELTA_OPTION), "0.001", optKey(DefaultOptionCreator.MAX_ITERATIONS_OPTION), "2", optKey(FuzzyKMeansDriver.M_OPTION), "2.0", optKey(DefaultOptionCreator.CLUSTERING_OPTION), optKey(DefaultOptionCreator.EMIT_MOST_LIKELY_OPTION), optKey(DefaultOptionCreator.OVERWRITE_OPTION) };        ToolRunner.run(getConfiguration(), new FuzzyKMeansDriver(), args);        long count = HadoopUtil.countRecords(new Path(output, "clusteredPoints/part-m-00000"), conf);        assertTrue(count > 0);    }}
0
private static ClusterClassifier newDMClassifier()
{    List<Cluster> models = Lists.newArrayList();    DistanceMeasure measure = new ManhattanDistanceMeasure();    models.add(new DistanceMeasureCluster(new DenseVector(2).assign(1), 0, measure));    models.add(new DistanceMeasureCluster(new DenseVector(2), 1, measure));    models.add(new DistanceMeasureCluster(new DenseVector(2).assign(-1), 2, measure));    return new ClusterClassifier(models, new KMeansClusteringPolicy());}
0
private static ClusterClassifier newKlusterClassifier()
{    List<Cluster> models = Lists.newArrayList();    DistanceMeasure measure = new ManhattanDistanceMeasure();    models.add(new org.apache.mahout.clustering.kmeans.Kluster(new DenseVector(2).assign(1), 0, measure));    models.add(new org.apache.mahout.clustering.kmeans.Kluster(new DenseVector(2), 1, measure));    models.add(new org.apache.mahout.clustering.kmeans.Kluster(new DenseVector(2).assign(-1), 2, measure));    return new ClusterClassifier(models, new KMeansClusteringPolicy());}
0
private static ClusterClassifier newCosineKlusterClassifier()
{    List<Cluster> models = Lists.newArrayList();    DistanceMeasure measure = new CosineDistanceMeasure();    models.add(new org.apache.mahout.clustering.kmeans.Kluster(new DenseVector(2).assign(1), 0, measure));    models.add(new org.apache.mahout.clustering.kmeans.Kluster(new DenseVector(2), 1, measure));    models.add(new org.apache.mahout.clustering.kmeans.Kluster(new DenseVector(2).assign(-1), 2, measure));    return new ClusterClassifier(models, new KMeansClusteringPolicy());}
0
private static ClusterClassifier newSoftClusterClassifier()
{    List<Cluster> models = Lists.newArrayList();    DistanceMeasure measure = new ManhattanDistanceMeasure();    models.add(new SoftCluster(new DenseVector(2).assign(1), 0, measure));    models.add(new SoftCluster(new DenseVector(2), 1, measure));    models.add(new SoftCluster(new DenseVector(2).assign(-1), 2, measure));    return new ClusterClassifier(models, new FuzzyKMeansClusteringPolicy());}
0
private ClusterClassifier writeAndRead(ClusterClassifier classifier) throws IOException
{    Path path = new Path(getTestTempDirPath(), "output");    classifier.writeToSeqFiles(path);    ClusterClassifier newClassifier = new ClusterClassifier();    newClassifier.readFromSeqFiles(getConfiguration(), path);    return newClassifier;}
0
public void testDMClusterClassification()
{    ClusterClassifier classifier = newDMClassifier();    Vector pdf = classifier.classify(new DenseVector(2));    assertEquals("[0,0]", "[0.2,0.6,0.2]", AbstractCluster.formatVector(pdf, null));    pdf = classifier.classify(new DenseVector(2).assign(2));    assertEquals("[2,2]", "[0.493,0.296,0.211]", AbstractCluster.formatVector(pdf, null));}
0
public void testClusterClassification()
{    ClusterClassifier classifier = newKlusterClassifier();    Vector pdf = classifier.classify(new DenseVector(2));    assertEquals("[0,0]", "[0.2,0.6,0.2]", AbstractCluster.formatVector(pdf, null));    pdf = classifier.classify(new DenseVector(2).assign(2));    assertEquals("[2,2]", "[0.493,0.296,0.211]", AbstractCluster.formatVector(pdf, null));}
0
public void testSoftClusterClassification()
{    ClusterClassifier classifier = newSoftClusterClassifier();    Vector pdf = classifier.classify(new DenseVector(2));    assertEquals("[0,0]", "[0.0,1.0,0.0]", AbstractCluster.formatVector(pdf, null));    pdf = classifier.classify(new DenseVector(2).assign(2));    assertEquals("[2,2]", "[0.735,0.184,0.082]", AbstractCluster.formatVector(pdf, null));}
0
public void testDMClassifierSerialization() throws Exception
{    ClusterClassifier classifier = newDMClassifier();    ClusterClassifier classifierOut = writeAndRead(classifier);    assertEquals(classifier.getModels().size(), classifierOut.getModels().size());    assertEquals(classifier.getModels().get(0).getClass().getName(), classifierOut.getModels().get(0).getClass().getName());}
0
public void testClusterClassifierSerialization() throws Exception
{    ClusterClassifier classifier = newKlusterClassifier();    ClusterClassifier classifierOut = writeAndRead(classifier);    assertEquals(classifier.getModels().size(), classifierOut.getModels().size());    assertEquals(classifier.getModels().get(0).getClass().getName(), classifierOut.getModels().get(0).getClass().getName());}
0
public void testSoftClusterClassifierSerialization() throws Exception
{    ClusterClassifier classifier = newSoftClusterClassifier();    ClusterClassifier classifierOut = writeAndRead(classifier);    assertEquals(classifier.getModels().size(), classifierOut.getModels().size());    assertEquals(classifier.getModels().get(0).getClass().getName(), classifierOut.getModels().get(0).getClass().getName());}
0
public void testClusterIteratorKMeans()
{    List<Vector> data = TestKmeansClustering.getPoints(TestKmeansClustering.REFERENCE);    ClusterClassifier prior = newKlusterClassifier();    ClusterClassifier posterior = ClusterIterator.iterate(data, prior, 5);    assertEquals(3, posterior.getModels().size());    for (Cluster cluster : posterior.getModels()) {        System.out.println(cluster.asFormatString(null));    }}
0
public void testClusterIteratorDirichlet()
{    List<Vector> data = TestKmeansClustering.getPoints(TestKmeansClustering.REFERENCE);    ClusterClassifier prior = newKlusterClassifier();    ClusterClassifier posterior = ClusterIterator.iterate(data, prior, 5);    assertEquals(3, posterior.getModels().size());    for (Cluster cluster : posterior.getModels()) {        System.out.println(cluster.asFormatString(null));    }}
0
public void testSeqFileClusterIteratorKMeans() throws IOException
{    Path pointsPath = getTestTempDirPath("points");    Path priorPath = getTestTempDirPath("prior");    Path outPath = getTestTempDirPath("output");    Configuration conf = getConfiguration();    FileSystem fs = FileSystem.get(pointsPath.toUri(), conf);    List<VectorWritable> points = TestKmeansClustering.getPointsWritable(TestKmeansClustering.REFERENCE);    ClusteringTestUtils.writePointsToFile(points, new Path(pointsPath, "file1"), fs, conf);    Path path = new Path(priorPath, "priorClassifier");    ClusterClassifier prior = newKlusterClassifier();    prior.writeToSeqFiles(path);    assertEquals(3, prior.getModels().size());    System.out.println("Prior");    for (Cluster cluster : prior.getModels()) {        System.out.println(cluster.asFormatString(null));    }    ClusterIterator.iterateSeq(conf, pointsPath, path, outPath, 5);    for (int i = 1; i <= 4; i++) {        System.out.println("Classifier-" + i);        ClusterClassifier posterior = new ClusterClassifier();        String name = i == 4 ? "clusters-4-final" : "clusters-" + i;        posterior.readFromSeqFiles(conf, new Path(outPath, name));        assertEquals(3, posterior.getModels().size());        for (Cluster cluster : posterior.getModels()) {            System.out.println(cluster.asFormatString(null));        }    }}
0
public void testMRFileClusterIteratorKMeans() throws Exception
{    Path pointsPath = getTestTempDirPath("points");    Path priorPath = getTestTempDirPath("prior");    Path outPath = getTestTempDirPath("output");    Configuration conf = getConfiguration();    FileSystem fs = FileSystem.get(pointsPath.toUri(), conf);    List<VectorWritable> points = TestKmeansClustering.getPointsWritable(TestKmeansClustering.REFERENCE);    ClusteringTestUtils.writePointsToFile(points, new Path(pointsPath, "file1"), fs, conf);    Path path = new Path(priorPath, "priorClassifier");    ClusterClassifier prior = newKlusterClassifier();    prior.writeToSeqFiles(path);    ClusteringPolicy policy = new KMeansClusteringPolicy();    ClusterClassifier.writePolicy(policy, path);    assertEquals(3, prior.getModels().size());    System.out.println("Prior");    for (Cluster cluster : prior.getModels()) {        System.out.println(cluster.asFormatString(null));    }    ClusterIterator.iterateMR(conf, pointsPath, path, outPath, 5);    for (int i = 1; i <= 4; i++) {        System.out.println("Classifier-" + i);        ClusterClassifier posterior = new ClusterClassifier();        String name = i == 4 ? "clusters-4-final" : "clusters-" + i;        posterior.readFromSeqFiles(conf, new Path(outPath, name));        assertEquals(3, posterior.getModels().size());        for (Cluster cluster : posterior.getModels()) {            System.out.println(cluster.asFormatString(null));        }    }}
0
public void testCosineKlusterClassification()
{    ClusterClassifier classifier = newCosineKlusterClassifier();    Vector pdf = classifier.classify(new DenseVector(2));    assertEquals("[0,0]", "[0.333,0.333,0.333]", AbstractCluster.formatVector(pdf, null));    pdf = classifier.classify(new DenseVector(2).assign(2));    assertEquals("[2,2]", "[0.429,0.429,0.143]", AbstractCluster.formatVector(pdf, null));}
0
public void setUp() throws Exception
{    super.setUp();    Configuration conf = getConfiguration();    fs = FileSystem.get(conf);}
0
public static List<VectorWritable> getPointsWritable(double[][] raw)
{    List<VectorWritable> points = Lists.newArrayList();    for (double[] fr : raw) {        Vector vec = new RandomAccessSparseVector(fr.length);        vec.assign(fr);        points.add(new VectorWritable(vec));    }    return points;}
0
public static List<VectorWritable> getPointsWritableDenseVector(double[][] raw)
{    List<VectorWritable> points = Lists.newArrayList();    for (double[] fr : raw) {        Vector vec = new DenseVector(fr.length);        vec.assign(fr);        points.add(new VectorWritable(vec));    }    return points;}
0
public static List<Vector> getPoints(double[][] raw)
{    List<Vector> points = Lists.newArrayList();    for (double[] fr : raw) {        Vector vec = new SequentialAccessSparseVector(fr.length);        vec.assign(fr);        points.add(vec);    }    return points;}
0
public void testKMeansSeqJob() throws Exception
{    DistanceMeasure measure = new EuclideanDistanceMeasure();    List<VectorWritable> points = getPointsWritable(REFERENCE);    Path pointsPath = getTestTempDirPath("points");    Path clustersPath = getTestTempDirPath("clusters");    Configuration conf = getConfiguration();    ClusteringTestUtils.writePointsToFile(points, true, new Path(pointsPath, "file1"), fs, conf);    ClusteringTestUtils.writePointsToFile(points, true, new Path(pointsPath, "file2"), fs, conf);    for (int k = 1; k < points.size(); k++) {        System.out.println("testKMeansMRJob k= " + k);                Path path = new Path(clustersPath, "part-00000");        FileSystem fs = FileSystem.get(path.toUri(), conf);        SequenceFile.Writer writer = new SequenceFile.Writer(fs, conf, path, Text.class, Kluster.class);        try {            for (int i = 0; i < k + 1; i++) {                Vector vec = points.get(i).get();                Kluster cluster = new Kluster(vec, i, measure);                                cluster.observe(cluster.getCenter(), 1);                writer.append(new Text(cluster.getIdentifier()), cluster);            }        } finally {            Closeables.close(writer, false);        }                Path outputPath = getTestTempDirPath("output" + k);        String[] args = { optKey(DefaultOptionCreator.INPUT_OPTION), pointsPath.toString(), optKey(DefaultOptionCreator.CLUSTERS_IN_OPTION), clustersPath.toString(), optKey(DefaultOptionCreator.OUTPUT_OPTION), outputPath.toString(), optKey(DefaultOptionCreator.DISTANCE_MEASURE_OPTION), EuclideanDistanceMeasure.class.getName(), optKey(DefaultOptionCreator.CONVERGENCE_DELTA_OPTION), "0.001", optKey(DefaultOptionCreator.MAX_ITERATIONS_OPTION), "2", optKey(DefaultOptionCreator.CLUSTERING_OPTION), optKey(DefaultOptionCreator.OVERWRITE_OPTION), optKey(DefaultOptionCreator.METHOD_OPTION), DefaultOptionCreator.SEQUENTIAL_METHOD };        ToolRunner.run(conf, new KMeansDriver(), args);                Path clusteredPointsPath = new Path(outputPath, "clusteredPoints");        int[] expect = EXPECTED_NUM_POINTS[k];        DummyOutputCollector<IntWritable, WeightedPropertyVectorWritable> collector = new DummyOutputCollector<>();                for (Pair<IntWritable, WeightedPropertyVectorWritable> record : new SequenceFileIterable<IntWritable, WeightedPropertyVectorWritable>(new Path(clusteredPointsPath, "part-m-0"), conf)) {            collector.collect(record.getFirst(), record.getSecond());        }        assertEquals("clusters[" + k + ']', expect.length, collector.getKeys().size());    }}
0
public void testKMeansSeqJobDenseVector() throws Exception
{    DistanceMeasure measure = new EuclideanDistanceMeasure();    List<VectorWritable> points = getPointsWritableDenseVector(REFERENCE);    Path pointsPath = getTestTempDirPath("points");    Path clustersPath = getTestTempDirPath("clusters");    Configuration conf = getConfiguration();    ClusteringTestUtils.writePointsToFile(points, true, new Path(pointsPath, "file1"), fs, conf);    ClusteringTestUtils.writePointsToFile(points, true, new Path(pointsPath, "file2"), fs, conf);    for (int k = 1; k < points.size(); k++) {        System.out.println("testKMeansMRJob k= " + k);                Path path = new Path(clustersPath, "part-00000");        FileSystem fs = FileSystem.get(path.toUri(), conf);        SequenceFile.Writer writer = new SequenceFile.Writer(fs, conf, path, Text.class, Kluster.class);        try {            for (int i = 0; i < k + 1; i++) {                Vector vec = points.get(i).get();                Kluster cluster = new Kluster(vec, i, measure);                                cluster.observe(cluster.getCenter(), 1);                writer.append(new Text(cluster.getIdentifier()), cluster);            }        } finally {            Closeables.close(writer, false);        }                Path outputPath = getTestTempDirPath("output" + k);        String[] args = { optKey(DefaultOptionCreator.INPUT_OPTION), pointsPath.toString(), optKey(DefaultOptionCreator.CLUSTERS_IN_OPTION), clustersPath.toString(), optKey(DefaultOptionCreator.OUTPUT_OPTION), outputPath.toString(), optKey(DefaultOptionCreator.DISTANCE_MEASURE_OPTION), EuclideanDistanceMeasure.class.getName(), optKey(DefaultOptionCreator.CONVERGENCE_DELTA_OPTION), "0.001", optKey(DefaultOptionCreator.MAX_ITERATIONS_OPTION), "2", optKey(DefaultOptionCreator.CLUSTERING_OPTION), optKey(DefaultOptionCreator.OVERWRITE_OPTION), optKey(DefaultOptionCreator.METHOD_OPTION), DefaultOptionCreator.SEQUENTIAL_METHOD };        ToolRunner.run(conf, new KMeansDriver(), args);                Path clusteredPointsPath = new Path(outputPath, "clusteredPoints");        int[] expect = EXPECTED_NUM_POINTS[k];        DummyOutputCollector<IntWritable, WeightedPropertyVectorWritable> collector = new DummyOutputCollector<>();                for (Pair<IntWritable, WeightedPropertyVectorWritable> record : new SequenceFileIterable<IntWritable, WeightedPropertyVectorWritable>(new Path(clusteredPointsPath, "part-m-0"), conf)) {            collector.collect(record.getFirst(), record.getSecond());        }        assertEquals("clusters[" + k + ']', expect.length, collector.getKeys().size());    }}
0
public void testKMeansMRJob() throws Exception
{    DistanceMeasure measure = new EuclideanDistanceMeasure();    List<VectorWritable> points = getPointsWritable(REFERENCE);    Path pointsPath = getTestTempDirPath("points");    Path clustersPath = getTestTempDirPath("clusters");    Configuration conf = getConfiguration();    ClusteringTestUtils.writePointsToFile(points, true, new Path(pointsPath, "file1"), fs, conf);    ClusteringTestUtils.writePointsToFile(points, true, new Path(pointsPath, "file2"), fs, conf);    for (int k = 1; k < points.size(); k += 3) {        System.out.println("testKMeansMRJob k= " + k);                Path path = new Path(clustersPath, "part-00000");        FileSystem fs = FileSystem.get(path.toUri(), conf);        SequenceFile.Writer writer = new SequenceFile.Writer(fs, conf, path, Text.class, Kluster.class);        try {            for (int i = 0; i < k + 1; i++) {                Vector vec = points.get(i).get();                Kluster cluster = new Kluster(vec, i, measure);                                cluster.observe(cluster.getCenter(), 1);                writer.append(new Text(cluster.getIdentifier()), cluster);            }        } finally {            Closeables.close(writer, false);        }                Path outputPath = getTestTempDirPath("output" + k);        String[] args = { optKey(DefaultOptionCreator.INPUT_OPTION), pointsPath.toString(), optKey(DefaultOptionCreator.CLUSTERS_IN_OPTION), clustersPath.toString(), optKey(DefaultOptionCreator.OUTPUT_OPTION), outputPath.toString(), optKey(DefaultOptionCreator.DISTANCE_MEASURE_OPTION), EuclideanDistanceMeasure.class.getName(), optKey(DefaultOptionCreator.CONVERGENCE_DELTA_OPTION), "0.001", optKey(DefaultOptionCreator.MAX_ITERATIONS_OPTION), "2", optKey(DefaultOptionCreator.CLUSTERING_OPTION), optKey(DefaultOptionCreator.OVERWRITE_OPTION) };        ToolRunner.run(getConfiguration(), new KMeansDriver(), args);                Path clusteredPointsPath = new Path(outputPath, "clusteredPoints");                int[] expect = EXPECTED_NUM_POINTS[k];        DummyOutputCollector<IntWritable, WeightedPropertyVectorWritable> collector = new DummyOutputCollector<>();                for (Pair<IntWritable, WeightedPropertyVectorWritable> record : new SequenceFileIterable<IntWritable, WeightedPropertyVectorWritable>(new Path(clusteredPointsPath, "part-m-00000"), conf)) {            collector.collect(record.getFirst(), record.getSecond());        }        assertEquals("clusters[" + k + ']', expect.length, collector.getKeys().size());    }}
0
public void testKMeansWithCanopyClusterInput() throws Exception
{    List<VectorWritable> points = getPointsWritable(REFERENCE);    Path pointsPath = getTestTempDirPath("points");    Configuration conf = getConfiguration();    ClusteringTestUtils.writePointsToFile(points, true, new Path(pointsPath, "file1"), fs, conf);    ClusteringTestUtils.writePointsToFile(points, true, new Path(pointsPath, "file2"), fs, conf);    Path outputPath = getTestTempDirPath("output");        CanopyDriver.run(conf, pointsPath, outputPath, new ManhattanDistanceMeasure(), 3.1, 2.1, false, 0.0, false);    DummyOutputCollector<Text, ClusterWritable> collector1 = new DummyOutputCollector<>();    FileStatus[] outParts = FileSystem.get(conf).globStatus(new Path(outputPath, "clusters-0-final/*-0*"));    for (FileStatus outPartStat : outParts) {        for (Pair<Text, ClusterWritable> record : new SequenceFileIterable<Text, ClusterWritable>(outPartStat.getPath(), conf)) {            collector1.collect(record.getFirst(), record.getSecond());        }    }    boolean got15 = false;    boolean got43 = false;    int count = 0;    for (Text k : collector1.getKeys()) {        count++;        List<ClusterWritable> vl = collector1.getValue(k);        assertEquals("non-singleton centroid!", 1, vl.size());        ClusterWritable clusterWritable = vl.get(0);        Vector v = clusterWritable.getValue().getCenter();        assertEquals("cetriod vector is wrong length", 2, v.size());        if ((Math.abs(v.get(0) - 1.5) < EPSILON) && (Math.abs(v.get(1) - 1.5) < EPSILON) && !got15) {            got15 = true;        } else if ((Math.abs(v.get(0) - 4.333333333333334) < EPSILON) && (Math.abs(v.get(1) - 4.333333333333334) < EPSILON) && !got43) {            got43 = true;        } else {            fail("got unexpected center: " + v + " [" + v.getClass().toString() + ']');        }    }    assertEquals("got unexpected number of centers", 2, count);        Path kmeansOutput = new Path(outputPath, "kmeans");    KMeansDriver.run(getConfiguration(), pointsPath, new Path(outputPath, "clusters-0-final"), kmeansOutput, 0.001, 10, true, 0.0, false);        Path clusteredPointsPath = new Path(kmeansOutput, "clusteredPoints");    DummyOutputCollector<IntWritable, WeightedPropertyVectorWritable> collector = new DummyOutputCollector<>();        for (Pair<IntWritable, WeightedPropertyVectorWritable> record : new SequenceFileIterable<IntWritable, WeightedPropertyVectorWritable>(new Path(clusteredPointsPath, "part-m-00000"), conf)) {        collector.collect(record.getFirst(), record.getSecond());    }    for (IntWritable k : collector.getKeys()) {        List<WeightedPropertyVectorWritable> wpvList = collector.getValue(k);        assertTrue("empty cluster!", !wpvList.isEmpty());        if (wpvList.get(0).getVector().get(0) <= 2.0) {            for (WeightedPropertyVectorWritable wv : wpvList) {                Vector v = wv.getVector();                int idx = v.maxValueIndex();                assertTrue("bad cluster!", v.get(idx) <= 2.0);            }            assertEquals("Wrong size cluster", 4, wpvList.size());        } else {            for (WeightedPropertyVectorWritable wv : wpvList) {                Vector v = wv.getVector();                int idx = v.minValueIndex();                assertTrue("bad cluster!", v.get(idx) > 2.0);            }            assertEquals("Wrong size cluster", 5, wpvList.size());        }    }}
0
private static List<VectorWritable> getPoints()
{    List<VectorWritable> points = Lists.newArrayList();    for (double[] fr : RAW) {        Vector vec = new RandomAccessSparseVector(fr.length);        vec.assign(fr);        points.add(new VectorWritable(vec));    }    return points;}
0
public void setUp() throws Exception
{    super.setUp();    Configuration conf = getConfiguration();    fs = FileSystem.get(conf);}
0
public void testRandomSeedGenerator() throws Exception
{    List<VectorWritable> points = getPoints();    Job job = new Job();    Configuration conf = job.getConfiguration();    job.setMapOutputValueClass(VectorWritable.class);    Path input = getTestTempFilePath("random-input");    Path output = getTestTempDirPath("random-output");    ClusteringTestUtils.writePointsToFile(points, input, fs, conf);    RandomSeedGenerator.buildRandom(conf, input, output, 4, new ManhattanDistanceMeasure());    int clusterCount = 0;    Collection<Integer> set = Sets.newHashSet();    for (ClusterWritable clusterWritable : new SequenceFileValueIterable<ClusterWritable>(new Path(output, "part-randomSeed"), true, conf)) {        clusterCount++;        Cluster cluster = clusterWritable.getValue();        int id = cluster.getId();                assertTrue(set.add(id));        Vector v = cluster.getCenter();                assertVectorEquals(RAW[id], v);    }        assertEquals(4, clusterCount);}
0
public void testRandomSeedGeneratorSeeded() throws Exception
{    List<VectorWritable> points = getPoints();    Job job = new Job();    Configuration conf = job.getConfiguration();    job.setMapOutputValueClass(VectorWritable.class);    Path input = getTestTempFilePath("random-input");    Path output = getTestTempDirPath("random-output");    ClusteringTestUtils.writePointsToFile(points, input, fs, conf);    RandomSeedGenerator.buildRandom(conf, input, output, 4, new ManhattanDistanceMeasure(), 1L);    int clusterCount = 0;    Collection<Integer> set = Sets.newHashSet();    for (ClusterWritable clusterWritable : new SequenceFileValueIterable<ClusterWritable>(new Path(output, "part-randomSeed"), true, conf)) {        clusterCount++;        Cluster cluster = clusterWritable.getValue();        int id = cluster.getId();                assertTrue(set.add(id));        Vector v = cluster.getCenter();                assertVectorEquals(RAW[id], v);    }        assertEquals(4, clusterCount);}
0
public void testBuildRandomSeededSameInitalClusters() throws Exception
{    List<VectorWritable> points = getPoints();    Job job = new Job();    Configuration conf = job.getConfiguration();    job.setMapOutputValueClass(VectorWritable.class);    Path input = getTestTempFilePath("random-input");    Path output = getTestTempDirPath("random-output");    ClusteringTestUtils.writePointsToFile(points, input, fs, conf);    long randSeed = 1;    RandomSeedGenerator.buildRandom(conf, input, output, 4, new ManhattanDistanceMeasure(), randSeed);    int[] clusterIDSeq = new int[4];    /**     * run through all clusters once and set sequence of IDs     */    int clusterCount = 0;    for (ClusterWritable clusterWritable : new SequenceFileValueIterable<ClusterWritable>(new Path(output, "part-randomSeed"), true, conf)) {        Cluster cluster = clusterWritable.getValue();        clusterIDSeq[clusterCount] = cluster.getId();        clusterCount++;    }    /* Rebuild cluster and run through again making sure all IDs are in the same random sequence     * Needs a better test because in this case passes when seeded with 1 and 2  fails with 1, 3     * passes when set to two */    RandomSeedGenerator.buildRandom(conf, input, output, 4, new ManhattanDistanceMeasure(), randSeed);    clusterCount = 0;    for (ClusterWritable clusterWritable : new SequenceFileValueIterable<ClusterWritable>(new Path(output, "part-randomSeed"), true, conf)) {        Cluster cluster = clusterWritable.getValue();                assertEquals(clusterIDSeq[clusterCount], cluster.getId());        clusterCount++;    }}
0
private static void assertVectorEquals(double[] raw, Vector v)
{    assertEquals(raw.length, v.size());    for (int i = 0; i < raw.length; i++) {        assertEquals(raw[i], v.getQuick(i), EPSILON);    }}
0
public void testInMemoryCVB0() throws Exception
{    String[] terms = new String[26];    for (int i = 0; i < terms.length; i++) {        terms[i] = String.valueOf((char) (i + 'a'));    }    int numGeneratingTopics = 3;    int numTerms = 26;    Matrix matrix = ClusteringTestUtils.randomStructuredModel(numGeneratingTopics, numTerms, new DoubleFunction() {        @Override        public double apply(double d) {            return 1.0 / Math.pow(d + 1.0, 2);        }    });    int numDocs = 100;    int numSamples = 20;    int numTopicsPerDoc = 1;    Matrix sampledCorpus = ClusteringTestUtils.sampledCorpus(matrix, RandomUtils.getRandom(), numDocs, numSamples, numTopicsPerDoc);    List<Double> perplexities = Lists.newArrayList();    int numTrials = 1;    for (int numTestTopics = 1; numTestTopics < 2 * numGeneratingTopics; numTestTopics++) {        double[] perps = new double[numTrials];        for (int trial = 0; trial < numTrials; trial++) {            InMemoryCollapsedVariationalBayes0 cvb = new InMemoryCollapsedVariationalBayes0(sampledCorpus, terms, numTestTopics, ALPHA, ETA, 2, 1, 0);            cvb.setVerbose(true);            perps[trial] = cvb.iterateUntilConvergence(0, 5, 0, 0.2);            System.out.println(perps[trial]);        }        Arrays.sort(perps);        System.out.println(Arrays.toString(perps));        perplexities.add(perps[0]);    }    System.out.println(Joiner.on(",").join(perplexities));}
0
public double apply(double d)
{    return 1.0 / Math.pow(d + 1.0, 2);}
0
public void testRandomStructuredModelViaMR() throws Exception
{    int numGeneratingTopics = 3;    int numTerms = 9;    Matrix matrix = ClusteringTestUtils.randomStructuredModel(numGeneratingTopics, numTerms, new DoubleFunction() {        @Override        public double apply(double d) {            return 1.0 / Math.pow(d + 1.0, 3);        }    });    int numDocs = 500;    int numSamples = 10;    int numTopicsPerDoc = 1;    Matrix sampledCorpus = ClusteringTestUtils.sampledCorpus(matrix, RandomUtils.getRandom(1234), numDocs, numSamples, numTopicsPerDoc);    Path sampleCorpusPath = getTestTempDirPath("corpus");    Configuration configuration = getConfiguration();    MatrixUtils.write(sampleCorpusPath, configuration, sampledCorpus);    int numIterations = 5;    List<Double> perplexities = Lists.newArrayList();    int startTopic = numGeneratingTopics - 1;    int numTestTopics = startTopic;    while (numTestTopics < numGeneratingTopics + 2) {        Path topicModelStateTempPath = getTestTempDirPath("topicTemp" + numTestTopics);        Configuration conf = getConfiguration();        CVB0Driver cvb0Driver = new CVB0Driver();        cvb0Driver.run(conf, sampleCorpusPath, null, numTestTopics, numTerms, ALPHA, ETA, numIterations, 1, 0, null, null, topicModelStateTempPath, 1234, 0.2f, 2, 1, 3, 1, false);        perplexities.add(lowestPerplexity(conf, topicModelStateTempPath));        numTestTopics++;    }    int bestTopic = -1;    double lowestPerplexity = Double.MAX_VALUE;    for (int t = 0; t < perplexities.size(); t++) {        if (perplexities.get(t) < lowestPerplexity) {            lowestPerplexity = perplexities.get(t);            bestTopic = t + startTopic;        }    }    assertEquals("The optimal number of topics is not that of the generating distribution", 4, bestTopic);    System.out.println("Perplexities: " + Joiner.on(", ").join(perplexities));}
0
public double apply(double d)
{    return 1.0 / Math.pow(d + 1.0, 3);}
0
private static double lowestPerplexity(Configuration conf, Path topicModelTemp) throws IOException
{    double lowest = Double.MAX_VALUE;    double current;    int iteration = 2;    while (!Double.isNaN(current = CVB0Driver.readPerplexity(conf, topicModelTemp, iteration))) {        lowest = Math.min(current, lowest);        iteration++;    }    return lowest;}
0
private static List<VectorWritable> getPoints()
{    List<VectorWritable> points = Lists.newArrayList();    for (double[] fr : RAW) {        Vector vec = new RandomAccessSparseVector(fr.length);        vec.assign(fr);        points.add(new VectorWritable(vec));    }    return points;}
0
public void setUp() throws Exception
{    super.setUp();    Configuration conf = getConfiguration();    fs = FileSystem.get(conf);}
0
public void testEigenSeedGenerator() throws Exception
{    List<VectorWritable> points = getPoints();    Job job = new Job();    Configuration conf = job.getConfiguration();    job.setMapOutputValueClass(VectorWritable.class);    Path input = getTestTempFilePath("eigen-input");    Path output = getTestTempDirPath("eigen-output");    ClusteringTestUtils.writePointsToFile(points, input, fs, conf);    EigenSeedGenerator.buildFromEigens(conf, input, output, 3, new ManhattanDistanceMeasure());    int clusterCount = 0;    Collection<Integer> set = new HashSet<>();    Vector[] v = new Vector[3];    for (ClusterWritable clusterWritable : new SequenceFileValueIterable<ClusterWritable>(new Path(output, "part-eigenSeed"), true, conf)) {        Cluster cluster = clusterWritable.getValue();        int id = cluster.getId();                assertTrue(set.add(id));        v[id] = cluster.getCenter();        clusterCount++;    }        assertEquals(3, clusterCount);        assertEquals(0, v[0].dot(v[1]), 1E-10);    assertEquals(0, v[1].dot(v[2]), 1E-10);    assertEquals(0, v[0].dot(v[2]), 1E-10);}
0
public void testAffinityMatrixInputMapper() throws Exception
{    AffinityMatrixInputMapper mapper = new AffinityMatrixInputMapper();    Configuration conf = getConfiguration();    conf.setInt(Keys.AFFINITY_DIMENSIONS, RAW_DIMENSIONS);        DummyRecordWriter<IntWritable, MatrixEntryWritable> writer = new DummyRecordWriter<>();    Mapper<LongWritable, Text, IntWritable, MatrixEntryWritable>.Context context = DummyRecordWriter.build(mapper, conf, writer);        for (String s : RAW) {        mapper.map(new LongWritable(), new Text(s), context);    }        assertEquals("Number of map results", RAW_DIMENSIONS, writer.getData().size());    Set<IntWritable> keys = writer.getData().keySet();    for (IntWritable i : keys) {        List<MatrixEntryWritable> row = writer.getData().get(i);        assertEquals("Number of items in row", RAW_DIMENSIONS, row.size());    }}
0
public void testAffinitymatrixInputReducer() throws Exception
{    AffinityMatrixInputMapper mapper = new AffinityMatrixInputMapper();    Configuration conf = getConfiguration();    conf.setInt(Keys.AFFINITY_DIMENSIONS, RAW_DIMENSIONS);        DummyRecordWriter<IntWritable, MatrixEntryWritable> mapWriter = new DummyRecordWriter<>();    Mapper<LongWritable, Text, IntWritable, MatrixEntryWritable>.Context mapContext = DummyRecordWriter.build(mapper, conf, mapWriter);        for (String s : RAW) {        mapper.map(new LongWritable(), new Text(s), mapContext);    }        Map<IntWritable, List<MatrixEntryWritable>> map = mapWriter.getData();        AffinityMatrixInputReducer reducer = new AffinityMatrixInputReducer();    DummyRecordWriter<IntWritable, VectorWritable> redWriter = new DummyRecordWriter<>();    Reducer<IntWritable, MatrixEntryWritable, IntWritable, VectorWritable>.Context redContext = DummyRecordWriter.build(reducer, conf, redWriter, IntWritable.class, MatrixEntryWritable.class);    for (IntWritable key : mapWriter.getKeys()) {        reducer.reduce(key, mapWriter.getValue(key), redContext);    }        assertEquals("Number of reduce results", RAW_DIMENSIONS, redWriter.getData().size());    for (IntWritable row : redWriter.getKeys()) {        List<VectorWritable> list = redWriter.getValue(row);        assertEquals("Should only be one vector", 1, list.size());                Vector v = list.get(0).get();        for (Vector.Element e : v.all()) {                        MatrixEntryWritable toCompare = new MatrixEntryWritable();            toCompare.setRow(-1);            toCompare.setCol(e.index());            toCompare.setVal(e.get());            assertTrue("This entry was correctly placed in its row", map.get(row).contains(toCompare));        }    }}
0
private static double rowSum(double[] row)
{    double sum = 0;    for (double r : row) {        sum += r;    }    return sum;}
0
public void testMatrixDiagonalizeMapper() throws Exception
{    MatrixDiagonalizeMapper mapper = new MatrixDiagonalizeMapper();    Configuration conf = getConfiguration();    conf.setInt(Keys.AFFINITY_DIMENSIONS, RAW_DIMENSIONS);        DummyRecordWriter<NullWritable, IntDoublePairWritable> writer = new DummyRecordWriter<>();    Mapper<IntWritable, VectorWritable, NullWritable, IntDoublePairWritable>.Context context = DummyRecordWriter.build(mapper, conf, writer);        for (int i = 0; i < RAW_DIMENSIONS; i++) {        RandomAccessSparseVector toAdd = new RandomAccessSparseVector(RAW_DIMENSIONS);        toAdd.assign(RAW[i]);        mapper.map(new IntWritable(i), new VectorWritable(toAdd), context);    }        assertEquals("Number of map results", RAW_DIMENSIONS, writer.getValue(NullWritable.get()).size());}
0
public void testMatrixDiagonalizeReducer() throws Exception
{    MatrixDiagonalizeMapper mapper = new MatrixDiagonalizeMapper();    Configuration conf = getConfiguration();    conf.setInt(Keys.AFFINITY_DIMENSIONS, RAW_DIMENSIONS);        DummyRecordWriter<NullWritable, IntDoublePairWritable> mapWriter = new DummyRecordWriter<>();    Mapper<IntWritable, VectorWritable, NullWritable, IntDoublePairWritable>.Context mapContext = DummyRecordWriter.build(mapper, conf, mapWriter);        for (int i = 0; i < RAW_DIMENSIONS; i++) {        RandomAccessSparseVector toAdd = new RandomAccessSparseVector(RAW_DIMENSIONS);        toAdd.assign(RAW[i]);        mapper.map(new IntWritable(i), new VectorWritable(toAdd), mapContext);    }        MatrixDiagonalizeReducer reducer = new MatrixDiagonalizeReducer();    DummyRecordWriter<NullWritable, VectorWritable> redWriter = new DummyRecordWriter<>();    Reducer<NullWritable, IntDoublePairWritable, NullWritable, VectorWritable>.Context redContext = DummyRecordWriter.build(reducer, conf, redWriter, NullWritable.class, IntDoublePairWritable.class);        reducer.reduce(NullWritable.get(), mapWriter.getValue(NullWritable.get()), redContext);        List<VectorWritable> list = redWriter.getValue(NullWritable.get());    assertEquals("Only a single resulting vector", 1, list.size());    Vector v = list.get(0).get();    for (int i = 0; i < v.size(); i++) {        assertEquals("Element sum is correct", rowSum(RAW[i]), v.get(i), 0.01);    }}
0
public void testUnitVectorizerMapper() throws Exception
{    UnitVectorizerMapper mapper = new UnitVectorizerMapper();    Configuration conf = getConfiguration();        DummyRecordWriter<IntWritable, VectorWritable> writer = new DummyRecordWriter<>();    Mapper<IntWritable, VectorWritable, IntWritable, VectorWritable>.Context context = DummyRecordWriter.build(mapper, conf, writer);        for (int i = 0; i < RAW.length; i++) {        Vector vector = new RandomAccessSparseVector(RAW[i].length);        vector.assign(RAW[i]);        mapper.map(new IntWritable(i), new VectorWritable(vector), context);    }        assertEquals("Number of map results", RAW.length, writer.getData().size());    for (int i = 0; i < RAW.length; i++) {        IntWritable key = new IntWritable(i);        List<VectorWritable> list = writer.getValue(key);        assertEquals("Only one element per row", 1, list.size());        Vector v = list.get(0).get();        assertTrue("Unit vector sum is 1 or differs by 0.0001", Math.abs(v.norm(2) - 1) < 0.000001);    }}
0
public void testSave() throws Exception
{    Configuration conf = getConfiguration();    Writable key = new IntWritable(0);    Vector value = new DenseVector(VECTOR);    Path path = getTestTempDirPath("output");        VectorCache.save(key, value, path, conf, true, true);        SequenceFileValueIterator<VectorWritable> iterator = new SequenceFileValueIterator<>(path, true, conf);    try {        VectorWritable old = iterator.next();                assertEquals("Saved vector is identical to original", old.get(), value);    } finally {        Closeables.close(iterator, true);    }}
0
public void testLoad() throws Exception
{        Configuration conf = getConfiguration();    Writable key = new IntWritable(0);    Vector value = new DenseVector(VECTOR);    Path path = getTestTempDirPath("output");    FileSystem fs = FileSystem.get(path.toUri(), conf);        path = fs.makeQualified(path);    fs.deleteOnExit(path);    HadoopUtil.delete(conf, path);    SequenceFile.Writer writer = new SequenceFile.Writer(fs, conf, path, IntWritable.class, VectorWritable.class);    try {        writer.append(key, new VectorWritable(value));    } finally {        Closeables.close(writer, false);    }    DistributedCache.setCacheFiles(new URI[] { path.toUri() }, conf);        Vector result = VectorCache.load(conf);        assertNotNull("Vector is null", result);    assertEquals("Loaded vector is not identical to original", result, value);}
0
public void testAll() throws Exception
{    Configuration conf = getConfiguration();    Vector v = new DenseVector(VECTOR);    Path toSave = getTestTempDirPath("output");    Writable key = new IntWritable(0);        VectorCache.save(key, v, toSave, conf);        Vector v2 = VectorCache.load(conf);        assertNotNull("Vector is null", v2);    assertEquals("Vectors are not identical", v2, v);}
0
public void testVectorMatrixMultiplicationMapper() throws Exception
{    VectorMatrixMultiplicationMapper mapper = new VectorMatrixMultiplicationMapper();    Configuration conf = getConfiguration();        Vector toSave = new DenseVector(VECTOR);    DummyRecordWriter<IntWritable, VectorWritable> writer = new DummyRecordWriter<>();    Mapper<IntWritable, VectorWritable, IntWritable, VectorWritable>.Context context = DummyRecordWriter.build(mapper, conf, writer);    mapper.setup(toSave);        for (int i = 0; i < MATRIX.length; i++) {        Vector v = new RandomAccessSparseVector(MATRIX[i].length);        v.assign(MATRIX[i]);        mapper.map(new IntWritable(i), new VectorWritable(v), context);    }        assertEquals("Number of map results", MATRIX.length, writer.getData().size());    for (int i = 0; i < MATRIX.length; i++) {        List<VectorWritable> list = writer.getValue(new IntWritable(i));        assertEquals("Only one vector per key", 1, list.size());        Vector v = list.get(0).get();        for (int j = 0; j < MATRIX[i].length; j++) {            double total = Math.sqrt(VECTOR[i]) * Math.sqrt(VECTOR[j]) * MATRIX[i][j];            assertEquals("Product matrix elements", total, v.get(j), EPSILON);        }    }}
0
public static void setUp()
{    RandomUtils.useTestSeed();    syntheticData = DataUtils.sampleMultiNormalHypercube(NUM_DIMENSIONS, NUM_DATA_POINTS, DISTRIBUTION_RADIUS);}
0
public void testClusteringMultipleRuns()
{    for (int i = 1; i <= 10; ++i) {        BallKMeans clusterer = new BallKMeans(new BruteSearch(new SquaredEuclideanDistanceMeasure()), 1 << NUM_DIMENSIONS, NUM_ITERATIONS, true, i);        clusterer.cluster(syntheticData.getFirst());        double costKMeansPlusPlus = ClusteringUtils.totalClusterCost(syntheticData.getFirst(), clusterer);        clusterer = new BallKMeans(new BruteSearch(new SquaredEuclideanDistanceMeasure()), 1 << NUM_DIMENSIONS, NUM_ITERATIONS, false, i);        clusterer.cluster(syntheticData.getFirst());        double costKMeansRandom = ClusteringUtils.totalClusterCost(syntheticData.getFirst(), clusterer);        System.out.printf("%d runs; kmeans++: %f; random: %f\n", i, costKMeansPlusPlus, costKMeansRandom);        assertTrue("kmeans++ cost should be less than random cost", costKMeansPlusPlus < costKMeansRandom);    }}
0
public void testClustering()
{    UpdatableSearcher searcher = new BruteSearch(new SquaredEuclideanDistanceMeasure());    BallKMeans clusterer = new BallKMeans(searcher, 1 << NUM_DIMENSIONS, NUM_ITERATIONS);    long startTime = System.currentTimeMillis();    Pair<List<Centroid>, List<Centroid>> data = syntheticData;    clusterer.cluster(data.getFirst());    long endTime = System.currentTimeMillis();    long hash = 0;    for (Centroid centroid : data.getFirst()) {        for (Vector.Element element : centroid.all()) {            hash = 31 * hash + 17 * element.index() + Double.toHexString(element.get()).hashCode();        }    }    System.out.printf("Hash = %08x\n", hash);    assertEquals("Total weight not preserved", totalWeight(syntheticData.getFirst()), totalWeight(clusterer), 1.0e-9);            OnlineSummarizer summarizer = new OnlineSummarizer();    for (Vector mean : syntheticData.getSecond()) {        WeightedThing<Vector> v = searcher.search(mean, 1).get(0);        summarizer.add(v.getWeight());    }    assertTrue(String.format("Median weight [%f] too large [>%f]", summarizer.getMedian(), DISTRIBUTION_RADIUS), summarizer.getMedian() < DISTRIBUTION_RADIUS);    double clusterTime = (endTime - startTime) / 1000.0;    System.out.printf("%s\n%.2f for clustering\n%.1f us per row\n\n", searcher.getClass().getName(), clusterTime, clusterTime / syntheticData.getFirst().size() * 1.0e6);        double[] cornerWeights = new double[1 << NUM_DIMENSIONS];    Searcher trueFinder = new BruteSearch(new EuclideanDistanceMeasure());    for (Vector trueCluster : syntheticData.getSecond()) {        trueFinder.add(trueCluster);    }    for (Centroid centroid : clusterer) {        WeightedThing<Vector> closest = trueFinder.search(centroid, 1).get(0);        cornerWeights[((Centroid) closest.getValue()).getIndex()] += centroid.getWeight();    }    int expectedNumPoints = NUM_DATA_POINTS / (1 << NUM_DIMENSIONS);    for (double v : cornerWeights) {        System.out.printf("%f ", v);    }    System.out.println();    for (double v : cornerWeights) {        assertEquals(expectedNumPoints, v, 0);    }}
0
public void testInitialization()
{        List<? extends WeightedVector> data = cubishTestData(0.01);        BallKMeans r = new BallKMeans(new BruteSearch(new SquaredEuclideanDistanceMeasure()), 6, 20);    r.cluster(data);        Matrix x = new DenseMatrix(6, 5);    int row = 0;    for (Centroid c : r) {        x.viewRow(row).assign(c.viewPart(0, 5));        row++;    }        final Vector columnNorms = x.aggregateColumns(new VectorFunction() {        @Override        public double apply(Vector f) {                        return Math.abs(f.minValue()) + Math.abs(f.maxValue() - 6) + Math.abs(f.norm(1) - 6);        }    });        assertEquals(0, columnNorms.norm(1) / columnNorms.size(), 0.1);        SingularValueDecomposition svd = new SingularValueDecomposition(x);    Vector s = svd.getS().viewDiagonal().assign(Functions.div(6));    assertEquals(5, s.getLengthSquared(), 0.05);    assertEquals(5, s.norm(1), 0.05);}
0
public double apply(Vector f)
{        return Math.abs(f.minValue()) + Math.abs(f.maxValue() - 6) + Math.abs(f.norm(1) - 6);}
0
private static List<? extends WeightedVector> cubishTestData(double radius)
{    List<WeightedVector> data = Lists.newArrayListWithCapacity(K1 + 5000);    int row = 0;    MultiNormal g = new MultiNormal(radius, new ConstantVector(0, 10));    for (int i = 0; i < K1; i++) {        data.add(new WeightedVector(g.sample(), 1, row++));    }    for (int i = 0; i < 5; i++) {        Vector m = new DenseVector(10);                m.set(i, 6);        MultiNormal gx = new MultiNormal(radius, m);        for (int j = 0; j < 1000; j++) {            data.add(new WeightedVector(gx.sample(), 1, row++));        }    }    return data;}
0
public static Pair<List<Centroid>, List<Centroid>> sampleMultiNormalHypercube(int numDimensions, int numDatapoints, double distributionRadius)
{    int pow2N = 1 << numDimensions;                List<Centroid> mean = Lists.newArrayListWithCapacity(pow2N);    List<MultiNormal> rowSamplers = Lists.newArrayList();    for (int i = 0; i < pow2N; i++) {        Vector v = new DenseVector(numDimensions);                int pow2J = 1 << (numDimensions - 1);        for (int j = 0; j < numDimensions; ++j) {            v.set(j, 1.0 / pow2J * (i & pow2J));            pow2J >>= 1;        }        mean.add(new Centroid(i, v, 1));        rowSamplers.add(new MultiNormal(distributionRadius, v));    }        List<Centroid> data = Lists.newArrayListWithCapacity(numDatapoints);    for (int i = 0; i < numDatapoints; ++i) {        data.add(new Centroid(i, rowSamplers.get(i % pow2N).sample(), 1));    }    return new Pair<>(data, mean);}
0
public static Pair<List<Centroid>, List<Centroid>> sampleMultiNormalHypercube(int numDimensions, int numDatapoints)
{    return sampleMultiNormalHypercube(numDimensions, numDatapoints, 0.01);}
0
public void setUp()
{    RandomUtils.useTestSeed();    syntheticData = DataUtils.sampleMultiNormalHypercube(NUM_DIMENSIONS, NUM_DATA_POINTS);}
0
public static List<Object[]> generateData()
{    return Arrays.asList(new Object[][] { { new ProjectionSearch(new SquaredEuclideanDistanceMeasure(), NUM_PROJECTIONS, SEARCH_SIZE), true }, { new FastProjectionSearch(new SquaredEuclideanDistanceMeasure(), NUM_PROJECTIONS, SEARCH_SIZE), true }, { new ProjectionSearch(new SquaredEuclideanDistanceMeasure(), NUM_PROJECTIONS, SEARCH_SIZE), false }, { new FastProjectionSearch(new SquaredEuclideanDistanceMeasure(), NUM_PROJECTIONS, SEARCH_SIZE), false } });}
0
public void testAverageDistanceCutoff()
{    double avgDistanceCutoff = 0;    double avgNumClusters = 0;    int numTests = 1;    System.out.printf("Distance cutoff for %s\n", searcher.getClass().getName());    for (int i = 0; i < numTests; ++i) {        searcher.clear();        int numStreamingClusters = (int) Math.log(syntheticData.getFirst().size()) * (1 << NUM_DIMENSIONS);        double distanceCutoff = 1.0e-6;        double estimatedCutoff = ClusteringUtils.estimateDistanceCutoff(syntheticData.getFirst(), searcher.getDistanceMeasure(), 100);        System.out.printf("[%d] Generated synthetic data [magic] %f [estimate] %f\n", i, distanceCutoff, estimatedCutoff);        StreamingKMeans clusterer = new StreamingKMeans(searcher, numStreamingClusters, estimatedCutoff);        clusterer.cluster(syntheticData.getFirst());        avgDistanceCutoff += clusterer.getDistanceCutoff();        avgNumClusters += clusterer.getNumClusters();        System.out.printf("[%d] %f\n", i, clusterer.getDistanceCutoff());    }    avgDistanceCutoff /= numTests;    avgNumClusters /= numTests;    System.out.printf("Final: distanceCutoff: %f estNumClusters: %f\n", avgDistanceCutoff, avgNumClusters);}
0
public void testClustering()
{    searcher.clear();    int numStreamingClusters = (int) Math.log(syntheticData.getFirst().size()) * (1 << NUM_DIMENSIONS);    System.out.printf("k log n = %d\n", numStreamingClusters);    double estimatedCutoff = ClusteringUtils.estimateDistanceCutoff(syntheticData.getFirst(), searcher.getDistanceMeasure(), 100);    StreamingKMeans clusterer = new StreamingKMeans(searcher, numStreamingClusters, estimatedCutoff);    long startTime = System.currentTimeMillis();    if (allAtOnce) {        clusterer.cluster(syntheticData.getFirst());    } else {        for (Centroid datapoint : syntheticData.getFirst()) {            clusterer.cluster(datapoint);        }    }    long endTime = System.currentTimeMillis();    System.out.printf("%s %s\n", searcher.getClass().getName(), searcher.getDistanceMeasure().getClass().getName());    System.out.printf("Total number of clusters %d\n", clusterer.getNumClusters());    System.out.printf("Weights: %f %f\n", ClusteringUtils.totalWeight(syntheticData.getFirst()), ClusteringUtils.totalWeight(clusterer));    assertEquals("Total weight not preserved", ClusteringUtils.totalWeight(syntheticData.getFirst()), ClusteringUtils.totalWeight(clusterer), 1.0e-9);        double maxWeight = 0;    for (Vector mean : syntheticData.getSecond()) {        WeightedThing<Vector> v = searcher.search(mean, 1).get(0);        maxWeight = Math.max(v.getWeight(), maxWeight);    }    assertTrue("Maximum weight too large " + maxWeight, maxWeight < 0.05);    double clusterTime = (endTime - startTime) / 1000.0;    System.out.printf("%s\n%.2f for clustering\n%.1f us per row\n\n", searcher.getClass().getName(), clusterTime, clusterTime / syntheticData.getFirst().size() * 1.0e6);        double[] cornerWeights = new double[1 << NUM_DIMENSIONS];    Searcher trueFinder = new BruteSearch(new EuclideanDistanceMeasure());    for (Vector trueCluster : syntheticData.getSecond()) {        trueFinder.add(trueCluster);    }    for (Centroid centroid : clusterer) {        WeightedThing<Vector> closest = trueFinder.search(centroid, 1).get(0);        cornerWeights[((Centroid) closest.getValue()).getIndex()] += centroid.getWeight();    }    int expectedNumPoints = NUM_DATA_POINTS / (1 << NUM_DIMENSIONS);    for (double v : cornerWeights) {        System.out.printf("%f ", v);    }    System.out.println();    for (double v : cornerWeights) {        assertEquals(expectedNumPoints, v, 0);    }}
0
public void setUp()
{    RandomUtils.useTestSeed();    syntheticData = DataUtils.sampleMultiNormalHypercube(NUM_DIMENSIONS, NUM_DATA_POINTS, 1.0e-4);}
0
private void configure(Configuration configuration)
{    configuration.set(DefaultOptionCreator.DISTANCE_MEASURE_OPTION, distanceMeasureClassName);    configuration.setInt(StreamingKMeansDriver.SEARCH_SIZE_OPTION, SEARCH_SIZE);    configuration.setInt(StreamingKMeansDriver.NUM_PROJECTIONS_OPTION, NUM_PROJECTIONS);    configuration.set(StreamingKMeansDriver.SEARCHER_CLASS_OPTION, searcherClassName);    configuration.setInt(DefaultOptionCreator.NUM_CLUSTERS_OPTION, 1 << NUM_DIMENSIONS);    configuration.setInt(StreamingKMeansDriver.ESTIMATED_NUM_MAP_CLUSTERS, (1 << NUM_DIMENSIONS) * (int) Math.log(NUM_DATA_POINTS));    configuration.setFloat(StreamingKMeansDriver.ESTIMATED_DISTANCE_CUTOFF, (float) DISTANCE_CUTOFF);    configuration.setInt(StreamingKMeansDriver.MAX_NUM_ITERATIONS, MAX_NUM_ITERATIONS);        configuration.setBoolean(StreamingKMeansDriver.REDUCE_STREAMING_KMEANS, true);}
0
public static List<Object[]> generateData()
{    return Arrays.asList(new Object[][] { { ProjectionSearch.class.getName(), SquaredEuclideanDistanceMeasure.class.getName() }, { FastProjectionSearch.class.getName(), SquaredEuclideanDistanceMeasure.class.getName() }, { LocalitySensitiveHashSearch.class.getName(), SquaredEuclideanDistanceMeasure.class.getName() } });}
0
public void testHypercubeMapper() throws IOException
{    MapDriver<Writable, VectorWritable, IntWritable, CentroidWritable> mapDriver = MapDriver.newMapDriver(new StreamingKMeansMapper());    configure(mapDriver.getConfiguration());    System.out.printf("%s mapper test\n", mapDriver.getConfiguration().get(StreamingKMeansDriver.SEARCHER_CLASS_OPTION));    for (Centroid datapoint : syntheticData.getFirst()) {        mapDriver.addInput(new IntWritable(0), new VectorWritable(datapoint));    }    List<org.apache.hadoop.mrunit.types.Pair<IntWritable, CentroidWritable>> results = mapDriver.run();    BruteSearch resultSearcher = new BruteSearch(new SquaredEuclideanDistanceMeasure());    for (org.apache.hadoop.mrunit.types.Pair<IntWritable, CentroidWritable> result : results) {        resultSearcher.add(result.getSecond().getCentroid());    }    System.out.printf("Clustered the data into %d clusters\n", results.size());    for (Vector mean : syntheticData.getSecond()) {        WeightedThing<Vector> closest = resultSearcher.search(mean, 1).get(0);        assertTrue("Weight " + closest.getWeight() + " not less than 0.5", closest.getWeight() < 0.5);    }}
0
public void testMapperVsLocal() throws IOException
{        MapDriver<Writable, VectorWritable, IntWritable, CentroidWritable> mapDriver = MapDriver.newMapDriver(new StreamingKMeansMapper());    Configuration configuration = mapDriver.getConfiguration();    configure(configuration);    System.out.printf("%s mapper vs local test\n", mapDriver.getConfiguration().get(StreamingKMeansDriver.SEARCHER_CLASS_OPTION));    for (Centroid datapoint : syntheticData.getFirst()) {        mapDriver.addInput(new IntWritable(0), new VectorWritable(datapoint));    }    List<Centroid> mapperCentroids = Lists.newArrayList();    for (org.apache.hadoop.mrunit.types.Pair<IntWritable, CentroidWritable> pair : mapDriver.run()) {        mapperCentroids.add(pair.getSecond().getCentroid());    }        StreamingKMeans batchClusterer = new StreamingKMeans(StreamingKMeansUtilsMR.searcherFromConfiguration(configuration), mapDriver.getConfiguration().getInt("estimatedNumMapClusters", -1), DISTANCE_CUTOFF);    batchClusterer.cluster(syntheticData.getFirst());    List<Centroid> batchCentroids = Lists.newArrayList();    for (Vector v : batchClusterer) {        batchCentroids.add((Centroid) v);    }        StreamingKMeans perPointClusterer = new StreamingKMeans(StreamingKMeansUtilsMR.searcherFromConfiguration(configuration), (1 << NUM_DIMENSIONS) * (int) Math.log(NUM_DATA_POINTS), DISTANCE_CUTOFF);    for (Centroid datapoint : syntheticData.getFirst()) {        perPointClusterer.cluster(datapoint);    }    List<Centroid> perPointCentroids = Lists.newArrayList();    for (Vector v : perPointClusterer) {        perPointCentroids.add((Centroid) v);    }        double mapperCost = ClusteringUtils.totalClusterCost(syntheticData.getFirst(), mapperCentroids);    double localCost = ClusteringUtils.totalClusterCost(syntheticData.getFirst(), batchCentroids);    double perPointCost = ClusteringUtils.totalClusterCost(syntheticData.getFirst(), perPointCentroids);    System.out.printf("[Total cost] Mapper %f [%d] Local %f [%d] Perpoint local %f [%d];" + "[ratio m-vs-l %f] [ratio pp-vs-l %f]\n", mapperCost, mapperCentroids.size(), localCost, batchCentroids.size(), perPointCost, perPointCentroids.size(), mapperCost / localCost, perPointCost / localCost);            assertEquals("Mapper StreamingKMeans / Batch local StreamingKMeans total cost ratio too far from 1", 1.0, mapperCost / localCost, 0.8);    assertEquals("One by one local StreamingKMeans / Batch local StreamingKMeans total cost ratio too high", 1.0, perPointCost / localCost, 0.8);}
0
public void testHypercubeReducer() throws IOException
{    ReduceDriver<IntWritable, CentroidWritable, IntWritable, CentroidWritable> reduceDriver = ReduceDriver.newReduceDriver(new StreamingKMeansReducer());    Configuration configuration = reduceDriver.getConfiguration();    configure(configuration);    System.out.printf("%s reducer test\n", configuration.get(StreamingKMeansDriver.SEARCHER_CLASS_OPTION));    StreamingKMeans clusterer = new StreamingKMeans(StreamingKMeansUtilsMR.searcherFromConfiguration(configuration), (1 << NUM_DIMENSIONS) * (int) Math.log(NUM_DATA_POINTS), DISTANCE_CUTOFF);    long start = System.currentTimeMillis();    clusterer.cluster(syntheticData.getFirst());    long end = System.currentTimeMillis();    System.out.printf("%f [s]\n", (end - start) / 1000.0);    List<CentroidWritable> reducerInputs = Lists.newArrayList();    int postMapperTotalWeight = 0;    for (Centroid intermediateCentroid : clusterer) {        reducerInputs.add(new CentroidWritable(intermediateCentroid));        postMapperTotalWeight += intermediateCentroid.getWeight();    }    reduceDriver.addInput(new IntWritable(0), reducerInputs);    List<org.apache.hadoop.mrunit.types.Pair<IntWritable, CentroidWritable>> results = reduceDriver.run();    testReducerResults(postMapperTotalWeight, results);}
0
public void testHypercubeMapReduce() throws IOException
{    MapReduceDriver<Writable, VectorWritable, IntWritable, CentroidWritable, IntWritable, CentroidWritable> mapReduceDriver = new MapReduceDriver<>(new StreamingKMeansMapper(), new StreamingKMeansReducer());    Configuration configuration = mapReduceDriver.getConfiguration();    configure(configuration);    System.out.printf("%s full test\n", configuration.get(StreamingKMeansDriver.SEARCHER_CLASS_OPTION));    for (Centroid datapoint : syntheticData.getFirst()) {        mapReduceDriver.addInput(new IntWritable(0), new VectorWritable(datapoint));    }    List<org.apache.hadoop.mrunit.types.Pair<IntWritable, CentroidWritable>> results = mapReduceDriver.run();    testReducerResults(syntheticData.getFirst().size(), results);}
0
public void testHypercubeMapReduceRunSequentially() throws Exception
{    Configuration configuration = getConfiguration();    configure(configuration);    configuration.set(DefaultOptionCreator.METHOD_OPTION, DefaultOptionCreator.SEQUENTIAL_METHOD);    Path inputPath = new Path("testInput");    Path outputPath = new Path("testOutput");    StreamingKMeansUtilsMR.writeVectorsToSequenceFile(syntheticData.getFirst(), inputPath, configuration);    StreamingKMeansDriver.run(configuration, inputPath, outputPath);    testReducerResults(syntheticData.getFirst().size(), Lists.newArrayList(Iterables.transform(new SequenceFileIterable<IntWritable, CentroidWritable>(outputPath, configuration), new Function<Pair<IntWritable, CentroidWritable>, org.apache.hadoop.mrunit.types.Pair<IntWritable, CentroidWritable>>() {        @Override        public org.apache.hadoop.mrunit.types.Pair<IntWritable, CentroidWritable> apply(org.apache.mahout.common.Pair<IntWritable, CentroidWritable> input) {            return new org.apache.hadoop.mrunit.types.Pair<>(input.getFirst(), input.getSecond());        }    })));}
0
public org.apache.hadoop.mrunit.types.Pair<IntWritable, CentroidWritable> apply(org.apache.mahout.common.Pair<IntWritable, CentroidWritable> input)
{    return new org.apache.hadoop.mrunit.types.Pair<>(input.getFirst(), input.getSecond());}
0
private static void testReducerResults(int totalWeight, List<org.apache.hadoop.mrunit.types.Pair<IntWritable, CentroidWritable>> results)
{    int expectedNumClusters = 1 << NUM_DIMENSIONS;    double expectedWeight = (double) totalWeight / expectedNumClusters;    int numClusters = 0;    int numUnbalancedClusters = 0;    int totalReducerWeight = 0;    for (org.apache.hadoop.mrunit.types.Pair<IntWritable, CentroidWritable> result : results) {        if (result.getSecond().getCentroid().getWeight() != expectedWeight) {            System.out.printf("Unbalanced weight %f in centroid %d\n", result.getSecond().getCentroid().getWeight(), result.getSecond().getCentroid().getIndex());            ++numUnbalancedClusters;        }        assertEquals("Final centroid index is invalid", numClusters, result.getFirst().get());        totalReducerWeight += result.getSecond().getCentroid().getWeight();        ++numClusters;    }    System.out.printf("%d clusters are unbalanced\n", numUnbalancedClusters);    assertEquals("Invalid total weight", totalWeight, totalReducerWeight);    assertEquals("Invalid number of clusters", 1 << NUM_DIMENSIONS, numClusters);}
0
public void testSplitting() throws Exception
{    Path inputFile = new Path(getTestTempDirPath("input"), "test.seq");    Path output = getTestTempDirPath("output");    Configuration conf = new Configuration();    LocalFileSystem fs = FileSystem.getLocal(conf);    SequenceFile.Writer writer = null;    try {        writer = SequenceFile.createWriter(fs, conf, inputFile, IntWritable.class, IntWritable.class);        writer.append(new IntWritable(1), new IntWritable(1));        writer.append(new IntWritable(2), new IntWritable(2));        writer.append(new IntWritable(3), new IntWritable(3));        writer.append(new IntWritable(4), new IntWritable(4));        writer.append(new IntWritable(5), new IntWritable(5));        writer.append(new IntWritable(6), new IntWritable(6));        writer.append(new IntWritable(7), new IntWritable(7));        writer.append(new IntWritable(8), new IntWritable(8));    } finally {        Closeables.close(writer, false);    }    String splitPattern = "split";    int numSplits = 4;    ResplitSequenceFiles.main(new String[] { "--input", inputFile.toString(), "--output", output.toString() + "/" + splitPattern, "--numSplits", String.valueOf(numSplits) });    FileStatus[] statuses = HadoopUtil.getFileStatus(output, PathType.LIST, PathFilters.logsCRCFilter(), null, conf);    for (FileStatus status : statuses) {        String name = status.getPath().getName();        assertTrue(name.startsWith(splitPattern));        assertEquals(2, numEntries(status, conf));    }    assertEquals(numSplits, statuses.length);}
0
private int numEntries(FileStatus status, Configuration conf)
{    return Iterables.size(new SequenceFileIterable(status.getPath(), conf));}
0
public void testClusterAsFormatString()
{    double[] d = { 1.1, 2.2, 3.3 };    Vector m = new DenseVector(d);    Cluster cluster = new org.apache.mahout.clustering.kmeans.Kluster(m, 123, measure);    String formatString = cluster.asFormatString(null);    assertTrue(formatString.contains("\"r\":[]"));    assertTrue(formatString.contains("\"c\":[1.1,2.2,3.3]"));    assertTrue(formatString.contains("\"n\":0"));    assertTrue(formatString.contains("\"identifier\":\"CL-123\""));}
0
public void testClusterAsFormatStringSparse()
{    double[] d = { 1.1, 0.0, 3.3 };    Vector m = new SequentialAccessSparseVector(3);    m.assign(d);    Cluster cluster = new org.apache.mahout.clustering.kmeans.Kluster(m, 123, measure);    String formatString = cluster.asFormatString(null);    assertTrue(formatString.contains("\"r\":[]"));    assertTrue(formatString.contains("\"c\":[{\"0\":1.1},{\"2\":3.3}]"));    assertTrue(formatString.contains("\"n\":0"));    assertTrue(formatString.contains("\"identifier\":\"CL-123\""));}
0
public void testClusterAsFormatStringWithBindings()
{    double[] d = { 1.1, 2.2, 3.3 };    Vector m = new DenseVector(d);    Cluster cluster = new org.apache.mahout.clustering.kmeans.Kluster(m, 123, measure);    String[] bindings = { "fee", null, "foo" };    String formatString = cluster.asFormatString(bindings);    assertTrue(formatString.contains("\"r\":[]"));    assertTrue(formatString.contains("\"c\":[{\"fee\":1.1},{\"1\":2.2},{\"foo\":3.3}]"));    assertTrue(formatString.contains("\"n\":0"));    assertTrue(formatString.contains("\"identifier\":\"CL-123\""));}
0
public void testClusterAsFormatStringSparseWithBindings()
{    double[] d = { 1.1, 0.0, 3.3 };    Vector m = new SequentialAccessSparseVector(3);    m.assign(d);    Cluster cluster = new org.apache.mahout.clustering.kmeans.Kluster(m, 123, measure);    String formatString = cluster.asFormatString(null);    assertTrue(formatString.contains("\"r\":[]"));    assertTrue(formatString.contains("\"c\":[{\"0\":1.1},{\"2\":3.3}]"));    assertTrue(formatString.contains("\"n\":0"));    assertTrue(formatString.contains("\"identifier\":\"CL-123\""));}
0
public void setUp() throws Exception
{    super.setUp();    sampleData = Lists.newArrayList();    generateSamples();    sampleN = 0;    Vector sum = new DenseVector(2);    for (VectorWritable v : sampleData) {        sum.assign(v.get(), Functions.PLUS);        sampleN++;    }    sampleMean = sum.divide(sampleN);    Vector sampleVar = new DenseVector(2);    for (VectorWritable v : sampleData) {        Vector delta = v.get().minus(sampleMean);        sampleVar.assign(delta.times(delta), Functions.PLUS);    }    sampleVar = sampleVar.divide(sampleN - 1);    sampleStd = sampleVar.clone();    sampleStd.assign(new SquareRootFunction());    }
1
private void generate2dSamples(int num, double mx, double my, double sdx, double sdy)
{        for (int i = 0; i < num; i++) {        sampleData.add(new VectorWritable(new DenseVector(new double[] { UncommonDistributions.rNorm(mx, sdx), UncommonDistributions.rNorm(my, sdy) })));    }}
1
private void generateSamples()
{    generate2dSamples(50000, 1, 2, 3, 4);}
0
public void testAccumulatorNoSamples()
{    GaussianAccumulator accumulator0 = new RunningSumsGaussianAccumulator();    GaussianAccumulator accumulator1 = new OnlineGaussianAccumulator();    accumulator0.compute();    accumulator1.compute();    assertEquals("N", accumulator0.getN(), accumulator1.getN(), EPSILON);    assertEquals("Means", accumulator0.getMean(), accumulator1.getMean());    assertEquals("Avg Stds", accumulator0.getAverageStd(), accumulator1.getAverageStd(), EPSILON);}
0
public void testAccumulatorOneSample()
{    GaussianAccumulator accumulator0 = new RunningSumsGaussianAccumulator();    GaussianAccumulator accumulator1 = new OnlineGaussianAccumulator();    Vector sample = new DenseVector(2);    accumulator0.observe(sample, 1.0);    accumulator1.observe(sample, 1.0);    accumulator0.compute();    accumulator1.compute();    assertEquals("N", accumulator0.getN(), accumulator1.getN(), EPSILON);    assertEquals("Means", accumulator0.getMean(), accumulator1.getMean());    assertEquals("Avg Stds", accumulator0.getAverageStd(), accumulator1.getAverageStd(), EPSILON);}
0
public void testOLAccumulatorResults()
{    GaussianAccumulator accumulator = new OnlineGaussianAccumulator();    for (VectorWritable vw : sampleData) {        accumulator.observe(vw.get(), 1.0);    }    accumulator.compute();        assertEquals("OL N", sampleN, accumulator.getN(), EPSILON);    assertEquals("OL Mean", sampleMean.zSum(), accumulator.getMean().zSum(), EPSILON);    assertEquals("OL Std", sampleStd.zSum(), accumulator.getStd().zSum(), EPSILON);}
1
public void testRSAccumulatorResults()
{    GaussianAccumulator accumulator = new RunningSumsGaussianAccumulator();    for (VectorWritable vw : sampleData) {        accumulator.observe(vw.get(), 1.0);    }    accumulator.compute();        assertEquals("OL N", sampleN, accumulator.getN(), EPSILON);    assertEquals("OL Mean", sampleMean.zSum(), accumulator.getMean().zSum(), EPSILON);    assertEquals("OL Std", sampleStd.zSum(), accumulator.getStd().zSum(), 0.0001);}
1
public void testAccumulatorWeightedResults()
{    GaussianAccumulator accumulator0 = new RunningSumsGaussianAccumulator();    GaussianAccumulator accumulator1 = new OnlineGaussianAccumulator();    for (VectorWritable vw : sampleData) {        accumulator0.observe(vw.get(), 0.5);        accumulator1.observe(vw.get(), 0.5);    }    accumulator0.compute();    accumulator1.compute();    assertEquals("N", accumulator0.getN(), accumulator1.getN(), EPSILON);    assertEquals("Means", accumulator0.getMean().zSum(), accumulator1.getMean().zSum(), EPSILON);    assertEquals("Stds", accumulator0.getStd().zSum(), accumulator1.getStd().zSum(), 0.001);    assertEquals("Variance", accumulator0.getVariance().zSum(), accumulator1.getVariance().zSum(), 0.01);}
0
public void testAccumulatorWeightedResults2()
{    GaussianAccumulator accumulator0 = new RunningSumsGaussianAccumulator();    GaussianAccumulator accumulator1 = new OnlineGaussianAccumulator();    for (VectorWritable vw : sampleData) {        accumulator0.observe(vw.get(), 1.5);        accumulator1.observe(vw.get(), 1.5);    }    accumulator0.compute();    accumulator1.compute();    assertEquals("N", accumulator0.getN(), accumulator1.getN(), EPSILON);    assertEquals("Means", accumulator0.getMean().zSum(), accumulator1.getMean().zSum(), EPSILON);    assertEquals("Stds", accumulator0.getStd().zSum(), accumulator1.getStd().zSum(), 0.001);    assertEquals("Variance", accumulator0.getVariance().zSum(), accumulator1.getVariance().zSum(), 0.01);}
0
public void shouldReturnTopLevelClusterPath()
{    Path expectedPath = new Path(output, PathDirectory.TOP_LEVEL_CLUSTER_DIRECTORY);    assertEquals(expectedPath, PathDirectory.getTopLevelClusterPath(output));}
0
public void shouldReturnClusterPostProcessorOutputDirectory()
{    Path expectedPath = new Path(output, PathDirectory.POST_PROCESS_DIRECTORY);    assertEquals(expectedPath, PathDirectory.getClusterPostProcessorOutputDirectory(output));}
0
public void shouldReturnClusterOutputClusteredPoints()
{    Path expectedPath = new Path(output, PathDirectory.CLUSTERED_POINTS_DIRECTORY + File.separator + '*');    assertEquals(expectedPath, PathDirectory.getClusterOutputClusteredPoints(output));}
0
public void shouldReturnBottomLevelClusterPath()
{    Path expectedPath = new Path(output + File.separator + PathDirectory.BOTTOM_LEVEL_CLUSTER_DIRECTORY + File.separator + '1');    assertEquals(expectedPath, PathDirectory.getBottomLevelClusterPath(output, "1"));}
0
public void shouldReturnClusterPathForClusterId()
{    Path expectedPath = new Path(PathDirectory.getClusterPostProcessorOutputDirectory(output), new Path("1"));    assertEquals(expectedPath, PathDirectory.getClusterPathForClusterId(PathDirectory.getClusterPostProcessorOutputDirectory(output), "1"));}
0
public void setUp() throws Exception
{    super.setUp();    Configuration conf = getConfiguration();    fs = FileSystem.get(conf);}
0
public static List<VectorWritable> getPointsWritable(double[][] raw)
{    List<VectorWritable> points = Lists.newArrayList();    for (double[] fr : raw) {        Vector vec = new RandomAccessSparseVector(fr.length);        vec.assign(fr);        points.add(new VectorWritable(vec));    }    return points;}
0
public void testGetNumberOfClusters() throws Exception
{    List<VectorWritable> points = getPointsWritable(REFERENCE);    Path pointsPath = getTestTempDirPath("points");    Configuration conf = getConfiguration();    ClusteringTestUtils.writePointsToFile(points, new Path(pointsPath, "file1"), fs, conf);    ClusteringTestUtils.writePointsToFile(points, new Path(pointsPath, "file2"), fs, conf);    outputPathForCanopy = getTestTempDirPath("canopy");    outputPathForKMeans = getTestTempDirPath("kmeans");    topLevelClustering(pointsPath, conf);    int numberOfClusters = ClusterCountReader.getNumberOfClusters(outputPathForKMeans, conf);    Assert.assertEquals(2, numberOfClusters);    verifyThatNumberOfClustersIsCorrect(conf, new Path(outputPathForKMeans, new Path("clusteredPoints")));}
0
private void topLevelClustering(Path pointsPath, Configuration conf) throws IOException, InterruptedException, ClassNotFoundException
{    DistanceMeasure measure = new ManhattanDistanceMeasure();    CanopyDriver.run(conf, pointsPath, outputPathForCanopy, measure, 4.0, 3.0, true, 0.0, true);    Path clustersIn = new Path(outputPathForCanopy, new Path(Cluster.CLUSTERS_DIR + '0' + Cluster.FINAL_ITERATION_SUFFIX));    KMeansDriver.run(conf, pointsPath, clustersIn, outputPathForKMeans, 1, 1, true, 0.0, true);}
0
private static void verifyThatNumberOfClustersIsCorrect(Configuration conf, Path clusteredPointsPath)
{    DummyOutputCollector<IntWritable, WeightedVectorWritable> collector = new DummyOutputCollector<>();        for (Pair<IntWritable, WeightedVectorWritable> record : new SequenceFileIterable<IntWritable, WeightedVectorWritable>(new Path(clusteredPointsPath, "part-m-0"), conf)) {        collector.collect(record.getFirst(), record.getSecond());    }    int clusterSize = collector.getKeys().size();    assertEquals(2, clusterSize);}
0
public void setUp() throws Exception
{    super.setUp();    Configuration conf = getConfiguration();    fs = FileSystem.get(conf);}
0
private static List<VectorWritable> getPointsWritable(double[][] raw)
{    List<VectorWritable> points = Lists.newArrayList();    for (double[] fr : raw) {        Vector vec = new RandomAccessSparseVector(fr.length);        vec.assign(fr);        points.add(new VectorWritable(vec));    }    return points;}
0
public void testTopDownClustering() throws Exception
{    List<VectorWritable> points = getPointsWritable(REFERENCE);    Path pointsPath = getTestTempDirPath("points");    conf = getConfiguration();    ClusteringTestUtils.writePointsToFile(points, new Path(pointsPath, "file1"), fs, conf);    ClusteringTestUtils.writePointsToFile(points, new Path(pointsPath, "file2"), fs, conf);    outputPath = getTestTempDirPath("output");    topLevelClustering(pointsPath, conf);    Map<String, Path> postProcessedClusterDirectories = ouputPostProcessing(conf);    assertPostProcessedOutput(postProcessedClusterDirectories);    bottomLevelClustering(postProcessedClusterDirectories);}
0
private void assertTopLevelCluster(Entry<String, Path> cluster)
{    String clusterId = cluster.getKey();    Path clusterPath = cluster.getValue();    try {        if ("0".equals(clusterId)) {            assertPointsInFirstTopLevelCluster(clusterPath);        } else if ("1".equals(clusterId)) {            assertPointsInSecondTopLevelCluster(clusterPath);        }    } catch (IOException e) {        Assert.fail("Exception occurred while asserting top level cluster.");    }}
0
private void assertPointsInFirstTopLevelCluster(Path clusterPath) throws IOException
{    List<Vector> vectorsInCluster = getVectorsInCluster(clusterPath);    for (Vector vector : vectorsInCluster) {        Assert.assertTrue(ArrayUtils.contains(new String[] { "{0:1.0,1:1.0}", "{0:2.0,1:1.0}", "{0:1.0,1:2.0}" }, vector.asFormatString()));    }}
0
private void assertPointsInSecondTopLevelCluster(Path clusterPath) throws IOException
{    List<Vector> vectorsInCluster = getVectorsInCluster(clusterPath);    for (Vector vector : vectorsInCluster) {        Assert.assertTrue(ArrayUtils.contains(new String[] { "{0:4.0,1:4.0}", "{0:5.0,1:4.0}", "{0:4.0,1:5.0}", "{0:5.0,1:5.0}" }, vector.asFormatString()));    }}
0
private List<Vector> getVectorsInCluster(Path clusterPath) throws IOException
{    Path[] partFilePaths = FileUtil.stat2Paths(fs.globStatus(clusterPath));    FileStatus[] listStatus = fs.listStatus(partFilePaths);    List<Vector> vectors = Lists.newArrayList();    for (FileStatus partFile : listStatus) {        SequenceFile.Reader topLevelClusterReader = new SequenceFile.Reader(fs, partFile.getPath(), conf);        Writable clusterIdAsKey = new LongWritable();        VectorWritable point = new VectorWritable();        while (topLevelClusterReader.next(clusterIdAsKey, point)) {            vectors.add(point.get());        }    }    return vectors;}
0
private void bottomLevelClustering(Map<String, Path> postProcessedClusterDirectories) throws IOException, InterruptedException, ClassNotFoundException
{    for (Entry<String, Path> topLevelCluster : postProcessedClusterDirectories.entrySet()) {        String clusterId = topLevelCluster.getKey();        Path topLevelclusterPath = topLevelCluster.getValue();        Path bottomLevelCluster = PathDirectory.getBottomLevelClusterPath(outputPath, clusterId);        CanopyDriver.run(conf, topLevelclusterPath, bottomLevelCluster, new ManhattanDistanceMeasure(), 2.1, 2.0, true, 0.0, true);        assertBottomLevelCluster(bottomLevelCluster);    }}
0
private void assertBottomLevelCluster(Path bottomLevelCluster)
{    Path clusteredPointsPath = new Path(bottomLevelCluster, "clusteredPoints");    DummyOutputCollector<IntWritable, WeightedVectorWritable> collector = new DummyOutputCollector<>();        for (Pair<IntWritable, WeightedVectorWritable> record : new SequenceFileIterable<IntWritable, WeightedVectorWritable>(new Path(clusteredPointsPath, "part-m-0"), conf)) {        collector.collect(record.getFirst(), record.getSecond());    }    int clusterSize = collector.getKeys().size();        assertTrue(clusterSize == 1 || clusterSize == 2);}
0
private void assertPostProcessedOutput(Map<String, Path> postProcessedClusterDirectories)
{    for (Entry<String, Path> cluster : postProcessedClusterDirectories.entrySet()) {        assertTopLevelCluster(cluster);    }}
0
private Map<String, Path> ouputPostProcessing(Configuration conf) throws IOException
{    ClusterOutputPostProcessor clusterOutputPostProcessor = new ClusterOutputPostProcessor(outputPath, outputPath, conf);    clusterOutputPostProcessor.process();    return clusterOutputPostProcessor.getPostProcessedClusterDirectories();}
0
private void topLevelClustering(Path pointsPath, Configuration conf) throws IOException, InterruptedException, ClassNotFoundException
{    CanopyDriver.run(conf, pointsPath, outputPath, new ManhattanDistanceMeasure(), 3.1, 2.1, true, 0.0, true);}
0
public void testFlag() throws Exception
{    final Map<String, List<String>> testMap = Maps.newHashMap();    AbstractJobFactory fact = new AbstractJobFactory() {        @Override        public AbstractJob getJob() {            return new AbstractJob() {                @Override                public int run(String[] args) throws IOException {                    addFlag("testFlag", "t", "a simple test flag");                    Map<String, List<String>> argMap = parseArguments(args);                    testMap.clear();                    testMap.putAll(argMap);                    return 1;                }            };        }    };        ToolRunner.run(fact.getJob(), new String[0]);    assertFalse("test map for absent flag", testMap.containsKey("--testFlag"));    String[] withFlag = { "--testFlag" };    ToolRunner.run(fact.getJob(), withFlag);    assertTrue("test map for present flag", testMap.containsKey("--testFlag"));}
0
public AbstractJob getJob()
{    return new AbstractJob() {        @Override        public int run(String[] args) throws IOException {            addFlag("testFlag", "t", "a simple test flag");            Map<String, List<String>> argMap = parseArguments(args);            testMap.clear();            testMap.putAll(argMap);            return 1;        }    };}
0
public int run(String[] args) throws IOException
{    addFlag("testFlag", "t", "a simple test flag");    Map<String, List<String>> argMap = parseArguments(args);    testMap.clear();    testMap.putAll(argMap);    return 1;}
0
public void testOptions() throws Exception
{    final Map<String, List<String>> testMap = Maps.newHashMap();    AbstractJobFactory fact = new AbstractJobFactory() {        @Override        public AbstractJob getJob() {            return new AbstractJob() {                @Override                public int run(String[] args) throws IOException {                    this.addOption(DefaultOptionCreator.overwriteOption().create());                    this.addOption("option", "o", "option");                    this.addOption("required", "r", "required", true);                    this.addOption("notRequired", "nr", "not required", false);                    this.addOption("hasDefault", "hd", "option w/ default", "defaultValue");                    Map<String, List<String>> argMap = parseArguments(args);                    if (argMap == null) {                        return -1;                    }                    testMap.clear();                    testMap.putAll(argMap);                    return 0;                }            };        }    };    int ret = ToolRunner.run(fact.getJob(), new String[0]);    assertEquals("-1 for missing required options", -1, ret);    ret = ToolRunner.run(fact.getJob(), new String[] { "--required", "requiredArg" });    assertEquals("0 for no missing required options", 0, ret);    assertEquals(Collections.singletonList("requiredArg"), testMap.get("--required"));    assertEquals(Collections.singletonList("defaultValue"), testMap.get("--hasDefault"));    assertNull(testMap.get("--option"));    assertNull(testMap.get("--notRequired"));    assertFalse(testMap.containsKey("--overwrite"));    ret = ToolRunner.run(fact.getJob(), new String[] { "--required", "requiredArg", "--unknownArg" });    assertEquals("-1 for including unknown options", -1, ret);    ret = ToolRunner.run(fact.getJob(), new String[] { "--required", "requiredArg", "--required", "requiredArg2" });    assertEquals("-1 for including duplicate options", -1, ret);    ret = ToolRunner.run(fact.getJob(), new String[] { "--required", "requiredArg", "--overwrite", "--hasDefault", "nonDefault", "--option", "optionValue", "--notRequired", "notRequired" });    assertEquals("0 for no missing required options", 0, ret);    assertEquals(Collections.singletonList("requiredArg"), testMap.get("--required"));    assertEquals(Collections.singletonList("nonDefault"), testMap.get("--hasDefault"));    assertEquals(Collections.singletonList("optionValue"), testMap.get("--option"));    assertEquals(Collections.singletonList("notRequired"), testMap.get("--notRequired"));    assertTrue(testMap.containsKey("--overwrite"));    ret = ToolRunner.run(fact.getJob(), new String[] { "-r", "requiredArg", "-ow", "-hd", "nonDefault", "-o", "optionValue", "-nr", "notRequired" });    assertEquals("0 for no missing required options", 0, ret);    assertEquals(Collections.singletonList("requiredArg"), testMap.get("--required"));    assertEquals(Collections.singletonList("nonDefault"), testMap.get("--hasDefault"));    assertEquals(Collections.singletonList("optionValue"), testMap.get("--option"));    assertEquals(Collections.singletonList("notRequired"), testMap.get("--notRequired"));    assertTrue(testMap.containsKey("--overwrite"));}
0
public AbstractJob getJob()
{    return new AbstractJob() {        @Override        public int run(String[] args) throws IOException {            this.addOption(DefaultOptionCreator.overwriteOption().create());            this.addOption("option", "o", "option");            this.addOption("required", "r", "required", true);            this.addOption("notRequired", "nr", "not required", false);            this.addOption("hasDefault", "hd", "option w/ default", "defaultValue");            Map<String, List<String>> argMap = parseArguments(args);            if (argMap == null) {                return -1;            }            testMap.clear();            testMap.putAll(argMap);            return 0;        }    };}
0
public int run(String[] args) throws IOException
{    this.addOption(DefaultOptionCreator.overwriteOption().create());    this.addOption("option", "o", "option");    this.addOption("required", "r", "required", true);    this.addOption("notRequired", "nr", "not required", false);    this.addOption("hasDefault", "hd", "option w/ default", "defaultValue");    Map<String, List<String>> argMap = parseArguments(args);    if (argMap == null) {        return -1;    }    testMap.clear();    testMap.putAll(argMap);    return 0;}
0
public void testInputOutputPaths() throws Exception
{    AbstractJobFactory fact = new AbstractJobFactory() {        @Override        public AbstractJob getJob() {            return new AbstractJob() {                @Override                public int run(String[] args) throws IOException {                    addInputOption();                    addOutputOption();                                        Map<String, List<String>> argMap = parseArguments(args);                    if (argMap == null) {                        return -1;                    }                    Path inputPath = getInputPath();                    assertNotNull("getInputPath() returns non-null", inputPath);                    Path outputPath = getInputPath();                    assertNotNull("getOutputPath() returns non-null", outputPath);                    return 0;                }            };        }    };    int ret = ToolRunner.run(fact.getJob(), new String[0]);    assertEquals("-1 for missing input option", -1, ret);    String testInputPath = "testInputPath";    AbstractJob job = fact.getJob();    ret = ToolRunner.run(job, new String[] { "--input", testInputPath });    assertEquals("-1 for missing output option", -1, ret);    assertEquals("input path is correct", testInputPath, job.getInputPath().toString());    job = fact.getJob();    String testOutputPath = "testOutputPath";    ret = ToolRunner.run(job, new String[] { "--output", testOutputPath });    assertEquals("-1 for missing input option", -1, ret);    assertEquals("output path is correct", testOutputPath, job.getOutputPath().toString());    job = fact.getJob();    ret = ToolRunner.run(job, new String[] { "--input", testInputPath, "--output", testOutputPath });    assertEquals("0 for complete options", 0, ret);    assertEquals("input path is correct", testInputPath, job.getInputPath().toString());    assertEquals("output path is correct", testOutputPath, job.getOutputPath().toString());    job = fact.getJob();    ret = ToolRunner.run(job, new String[] { "--input", testInputPath, "--output", testOutputPath });    assertEquals("0 for complete options", 0, ret);    assertEquals("input path is correct", testInputPath, job.getInputPath().toString());    assertEquals("output path is correct", testOutputPath, job.getOutputPath().toString());    job = fact.getJob();    String testInputPropertyPath = "testInputPropertyPath";    String testOutputPropertyPath = "testOutputPropertyPath";    ret = ToolRunner.run(job, new String[] { "-Dmapred.input.dir=" + testInputPropertyPath, "-Dmapred.output.dir=" + testOutputPropertyPath });    assertEquals("0 for complete options", 0, ret);    assertEquals("input path from property is correct", testInputPropertyPath, job.getInputPath().toString());    assertEquals("output path from property is correct", testOutputPropertyPath, job.getOutputPath().toString());    job = fact.getJob();    ret = ToolRunner.run(job, new String[] { "-Dmapred.input.dir=" + testInputPropertyPath, "-Dmapred.output.dir=" + testOutputPropertyPath, "--input", testInputPath, "--output", testOutputPath });    assertEquals("0 for complete options", 0, ret);    assertEquals("input command-line option precedes property", testInputPath, job.getInputPath().toString());    assertEquals("output command-line option precedes property", testOutputPath, job.getOutputPath().toString());}
0
public AbstractJob getJob()
{    return new AbstractJob() {        @Override        public int run(String[] args) throws IOException {            addInputOption();            addOutputOption();                        Map<String, List<String>> argMap = parseArguments(args);            if (argMap == null) {                return -1;            }            Path inputPath = getInputPath();            assertNotNull("getInputPath() returns non-null", inputPath);            Path outputPath = getInputPath();            assertNotNull("getOutputPath() returns non-null", outputPath);            return 0;        }    };}
0
public int run(String[] args) throws IOException
{    addInputOption();    addOutputOption();        Map<String, List<String>> argMap = parseArguments(args);    if (argMap == null) {        return -1;    }    Path inputPath = getInputPath();    assertNotNull("getInputPath() returns non-null", inputPath);    Path outputPath = getInputPath();    assertNotNull("getOutputPath() returns non-null", outputPath);    return 0;}
0
public void testMeasure()
{    DistanceMeasure distanceMeasure = new CosineDistanceMeasure();    Vector[] vectors = { new DenseVector(new double[] { 1, 0, 0, 0, 0, 0 }), new DenseVector(new double[] { 1, 1, 1, 0, 0, 0 }), new DenseVector(new double[] { 1, 1, 1, 1, 1, 1 }) };    double[][] distanceMatrix = new double[3][3];    for (int a = 0; a < 3; a++) {        for (int b = 0; b < 3; b++) {            distanceMatrix[a][b] = distanceMeasure.distance(vectors[a], vectors[b]);        }    }    assertEquals(0.0, distanceMatrix[0][0], EPSILON);    assertTrue(distanceMatrix[0][0] < distanceMatrix[0][1]);    assertTrue(distanceMatrix[0][1] < distanceMatrix[0][2]);    assertEquals(0.0, distanceMatrix[1][1], EPSILON);    assertTrue(distanceMatrix[1][0] > distanceMatrix[1][1]);    assertTrue(distanceMatrix[1][2] < distanceMatrix[1][0]);    assertEquals(0.0, distanceMatrix[2][2], EPSILON);    assertTrue(distanceMatrix[2][0] > distanceMatrix[2][1]);    assertTrue(distanceMatrix[2][1] > distanceMatrix[2][2]);        assertEquals(0, distanceMeasure.distance(new SequentialAccessSparseVector(1), new SequentialAccessSparseVector(1)), EPSILON);}
0
public void testMeasure()
{    DistanceMeasure distanceMeasure = distanceMeasureFactory();    Vector[] vectors = { new DenseVector(new double[] { 1, 1, 1, 1, 1, 1 }), new DenseVector(new double[] { 2, 2, 2, 2, 2, 2 }), new DenseVector(new double[] { 6, 6, 6, 6, 6, 6 }), new DenseVector(new double[] { -1, -1, -1, -1, -1, -1 }) };    compare(distanceMeasure, vectors);    vectors = new Vector[4];    vectors[0] = new RandomAccessSparseVector(5);    vectors[0].setQuick(0, 1);    vectors[0].setQuick(3, 1);    vectors[0].setQuick(4, 1);    vectors[1] = new RandomAccessSparseVector(5);    vectors[1].setQuick(0, 2);    vectors[1].setQuick(3, 2);    vectors[1].setQuick(4, 2);    vectors[2] = new RandomAccessSparseVector(5);    vectors[2].setQuick(0, 6);    vectors[2].setQuick(3, 6);    vectors[2].setQuick(4, 6);    vectors[3] = new RandomAccessSparseVector(5);    compare(distanceMeasure, vectors);}
0
private static void compare(DistanceMeasure distanceMeasure, Vector[] vectors)
{    double[][] distanceMatrix = new double[4][4];    for (int a = 0; a < 4; a++) {        for (int b = 0; b < 4; b++) {            distanceMatrix[a][b] = distanceMeasure.distance(vectors[a], vectors[b]);        }    }    assertEquals("Distance from first vector to itself is not zero", 0.0, distanceMatrix[0][0], EPSILON);    assertTrue(distanceMatrix[0][0] < distanceMatrix[0][1]);    assertTrue(distanceMatrix[0][1] < distanceMatrix[0][2]);    assertEquals("Distance from second vector to itself is not zero", 0.0, distanceMatrix[1][1], EPSILON);    assertTrue(distanceMatrix[1][0] > distanceMatrix[1][1]);    assertTrue(distanceMatrix[1][2] > distanceMatrix[1][0]);    assertEquals("Distance from third vector to itself is not zero", 0.0, distanceMatrix[2][2], EPSILON);    assertTrue(distanceMatrix[2][0] > distanceMatrix[2][1]);    assertTrue(distanceMatrix[2][1] > distanceMatrix[2][2]);    for (int a = 0; a < 4; a++) {        for (int b = 0; b < 4; b++) {            assertTrue("Distance between vectors less than zero: " + distanceMatrix[a][b] + " = " + distanceMeasure + ".distance(" + vectors[a].asFormatString() + ", " + vectors[b].asFormatString() + ')', distanceMatrix[a][b] >= 0);            if (vectors[a].plus(vectors[b]).norm(2) == 0 && vectors[a].norm(2) > 0) {                assertTrue("Distance from v to -v is equal to zero" + vectors[a].asFormatString() + " = -" + vectors[b].asFormatString(), distanceMatrix[a][b] > 0);            }        }    }}
0
public void testMeasureWeighted()
{    WeightedDistanceMeasure distanceMeasure = distanceMeasureFactory();    Vector[] vectors = { new DenseVector(new double[] { 9, 9, 1 }), new DenseVector(new double[] { 1, 9, 9 }), new DenseVector(new double[] { 9, 1, 9 }) };    distanceMeasure.setWeights(new DenseVector(new double[] { 1, 1000, 1 }));    double[][] distanceMatrix = new double[3][3];    for (int a = 0; a < 3; a++) {        for (int b = 0; b < 3; b++) {            distanceMatrix[a][b] = distanceMeasure.distance(vectors[a], vectors[b]);        }    }    assertEquals(0.0, distanceMatrix[0][0], EPSILON);    assertTrue(distanceMatrix[0][1] < distanceMatrix[0][2]);}
0
public void testMeasure()
{    DistanceMeasure chebyshevDistanceMeasure = new ChebyshevDistanceMeasure();    Vector[] vectors = { new DenseVector(new double[] { 1, 0, 0, 0, 0, 0 }), new DenseVector(new double[] { 1, 1, 1, 0, 0, 0 }), new DenseVector(new double[] { 1, 1, 1, 1, 1, 1 }) };    double[][] distances = { { 0.0, 1.0, 1.0 }, { 1.0, 0.0, 1.0 }, { 1.0, 1.0, 0.0 } };    double[][] chebyshevDistanceMatrix = new double[3][3];    for (int a = 0; a < 3; a++) {        for (int b = 0; b < 3; b++) {            chebyshevDistanceMatrix[a][b] = chebyshevDistanceMeasure.distance(vectors[a], vectors[b]);        }    }    for (int a = 0; a < 3; a++) {        for (int b = 0; b < 3; b++) {            assertEquals(distances[a][b], chebyshevDistanceMatrix[a][b], EPSILON);        }    }    assertEquals(0.0, chebyshevDistanceMatrix[0][0], EPSILON);}
0
public DistanceMeasure distanceMeasureFactory()
{    return new EuclideanDistanceMeasure();}
0
public void testMeasure()
{    double[][] invCovValues = { { 2.2, 0.4 }, { 0.4, 2.8 } };    double[] meanValues = { -2.3, -0.9 };    Matrix invCov = new DenseMatrix(invCovValues);    Vector meanVector = new DenseVector(meanValues);    MahalanobisDistanceMeasure distanceMeasure = new MahalanobisDistanceMeasure();    distanceMeasure.setInverseCovarianceMatrix(invCov);    distanceMeasure.setMeanVector(meanVector);    double[] v1 = { -1.9, -2.3 };    double[] v2 = { -2.9, -1.3 };    double dist = distanceMeasure.distance(new DenseVector(v1), new DenseVector(v2));    assertEquals(2.0493901531919194, dist, EPSILON);        distanceMeasure.setCovarianceMatrix(invCov);        Matrix identity = distanceMeasure.getInverseCovarianceMatrix().times(invCov);    assertEquals(1, identity.get(0, 0), EPSILON);    assertEquals(1, identity.get(1, 1), EPSILON);    assertEquals(0, identity.get(1, 0), EPSILON);    assertEquals(0, identity.get(0, 1), EPSILON);}
0
public DistanceMeasure distanceMeasureFactory()
{    return new ManhattanDistanceMeasure();}
0
public void testMeasure()
{    DistanceMeasure minkowskiDistanceMeasure = new MinkowskiDistanceMeasure(1.5);    DistanceMeasure manhattanDistanceMeasure = new ManhattanDistanceMeasure();    DistanceMeasure euclideanDistanceMeasure = new EuclideanDistanceMeasure();    Vector[] vectors = { new DenseVector(new double[] { 1, 0, 0, 0, 0, 0 }), new DenseVector(new double[] { 1, 1, 1, 0, 0, 0 }), new DenseVector(new double[] { 1, 1, 1, 1, 1, 1 }) };    double[][] minkowskiDistanceMatrix = new double[3][3];    double[][] manhattanDistanceMatrix = new double[3][3];    double[][] euclideanDistanceMatrix = new double[3][3];    for (int a = 0; a < 3; a++) {        for (int b = 0; b < 3; b++) {            minkowskiDistanceMatrix[a][b] = minkowskiDistanceMeasure.distance(vectors[a], vectors[b]);            manhattanDistanceMatrix[a][b] = manhattanDistanceMeasure.distance(vectors[a], vectors[b]);            euclideanDistanceMatrix[a][b] = euclideanDistanceMeasure.distance(vectors[a], vectors[b]);        }    }    for (int a = 0; a < 3; a++) {        for (int b = 0; b < 3; b++) {            assertTrue(minkowskiDistanceMatrix[a][b] <= manhattanDistanceMatrix[a][b]);            assertTrue(minkowskiDistanceMatrix[a][b] >= euclideanDistanceMatrix[a][b]);        }    }    assertEquals(0.0, minkowskiDistanceMatrix[0][0], EPSILON);    assertTrue(minkowskiDistanceMatrix[0][0] < minkowskiDistanceMatrix[0][1]);    assertTrue(minkowskiDistanceMatrix[0][1] < minkowskiDistanceMatrix[0][2]);}
0
public TanimotoDistanceMeasure distanceMeasureFactory()
{    return new TanimotoDistanceMeasure();}
0
public WeightedDistanceMeasure distanceMeasureFactory()
{    return new WeightedEuclideanDistanceMeasure();}
0
public WeightedManhattanDistanceMeasure distanceMeasureFactory()
{    return new WeightedManhattanDistanceMeasure();}
0
public void nonExistingFile()
{    Path path = HadoopUtil.findInCacheByPartOfFilename("no such file", DISTRIBUTED_CACHE_FILES);    assertNull(path);}
0
public void existingFile()
{    Path path = HadoopUtil.findInCacheByPartOfFilename("want_to_find", DISTRIBUTED_CACHE_FILES);    assertNotNull(path);    assertEquals(FILE_I_WANT_TO_FIND.getName(), path.getName());}
0
public void collect(K key, V values)
{    List<V> points = data.get(key);    if (points == null) {        points = Lists.newArrayList();        data.put(key, points);    }    points.add(values);}
0
public Map<K, List<V>> getData()
{    return data;}
0
public List<V> getValue(K key)
{    return data.get(key);}
0
public Set<K> getKeys()
{    return data.keySet();}
0
public void write(K key, V value)
{        try {        K keyToUse = key instanceof NullWritable ? key : (K) cloneWritable(key);        V valueToUse = (V) cloneWritable(value);        keysInInsertionOrder.add(keyToUse);        List<V> points = data.get(key);        if (points == null) {            points = Lists.newArrayList();            data.put(keyToUse, points);        }        points.add(valueToUse);    } catch (IOException e) {        throw new RuntimeException(e.getMessage(), e);    }}
0
private Writable cloneWritable(Writable original) throws IOException
{    Writable clone;    try {        clone = original.getClass().asSubclass(Writable.class).newInstance();    } catch (Exception e) {        throw new RuntimeException("Unable to instantiate writable!", e);    }    ByteArrayOutputStream bytes = new ByteArrayOutputStream();    original.write(new DataOutputStream(bytes));    clone.readFields(new DataInputStream(new ByteArrayInputStream(bytes.toByteArray())));    return clone;}
0
public void close(TaskAttemptContext context)
{}
0
public Map<K, List<V>> getData()
{    return data;}
0
public List<V> getValue(K key)
{    return data.get(key);}
0
public Set<K> getKeys()
{    return data.keySet();}
0
public Iterable<K> getKeysInInsertionOrder()
{    return keysInInsertionOrder;}
0
public static Mapper<K1, V1, K2, V2>.Context build(Mapper<K1, V1, K2, V2> mapper, Configuration configuration, RecordWriter<K2, V2> output)
{        try {        return buildNewMapperContext(configuration, output);    } catch (Exception | IncompatibleClassChangeError e) {        try {            return buildOldMapperContext(mapper, configuration, output);        } catch (Exception ex) {            throw new IllegalStateException(ex);        }    }}
0
public static Reducer<K1, V1, K2, V2>.Context build(Reducer<K1, V1, K2, V2> reducer, Configuration configuration, RecordWriter<K2, V2> output, Class<K1> keyClass, Class<V1> valueClass)
{        try {        return buildNewReducerContext(configuration, output, keyClass, valueClass);    } catch (Exception | IncompatibleClassChangeError e) {        try {            return buildOldReducerContext(reducer, configuration, output, keyClass, valueClass);        } catch (Exception ex) {            throw new IllegalStateException(ex);        }    }}
0
private static Mapper<K1, V1, K2, V2>.Context buildNewMapperContext(Configuration configuration, RecordWriter<K2, V2> output) throws Exception
{    Class<?> mapContextImplClass = Class.forName("org.apache.hadoop.mapreduce.task.MapContextImpl");    Constructor<?> cons = mapContextImplClass.getConstructors()[0];    Object mapContextImpl = cons.newInstance(configuration, new TaskAttemptID(), null, output, null, new DummyStatusReporter(), null);    Class<?> wrappedMapperClass = Class.forName("org.apache.hadoop.mapreduce.lib.map.WrappedMapper");    Object wrappedMapper = wrappedMapperClass.getConstructor().newInstance();    Method getMapContext = wrappedMapperClass.getMethod("getMapContext", MapContext.class);    return (Mapper.Context) getMapContext.invoke(wrappedMapper, mapContextImpl);}
0
private static Mapper<K1, V1, K2, V2>.Context buildOldMapperContext(Mapper<K1, V1, K2, V2> mapper, Configuration configuration, RecordWriter<K2, V2> output) throws Exception
{    Constructor<?> cons = getNestedContextConstructor(mapper.getClass());        return (Mapper.Context) cons.newInstance(mapper, configuration, new TaskAttemptID(), null, output, null, new DummyStatusReporter(), null);}
0
private static Reducer<K1, V1, K2, V2>.Context buildNewReducerContext(Configuration configuration, RecordWriter<K2, V2> output, Class<K1> keyClass, Class<V1> valueClass) throws Exception
{    Class<?> reduceContextImplClass = Class.forName("org.apache.hadoop.mapreduce.task.ReduceContextImpl");    Constructor<?> cons = reduceContextImplClass.getConstructors()[0];    Object reduceContextImpl = cons.newInstance(configuration, new TaskAttemptID(), new MockIterator(), null, null, output, null, new DummyStatusReporter(), null, keyClass, valueClass);    Class<?> wrappedReducerClass = Class.forName("org.apache.hadoop.mapreduce.lib.reduce.WrappedReducer");    Object wrappedReducer = wrappedReducerClass.getConstructor().newInstance();    Method getReducerContext = wrappedReducerClass.getMethod("getReducerContext", ReduceContext.class);    return (Reducer.Context) getReducerContext.invoke(wrappedReducer, reduceContextImpl);}
0
private static Reducer<K1, V1, K2, V2>.Context buildOldReducerContext(Reducer<K1, V1, K2, V2> reducer, Configuration configuration, RecordWriter<K2, V2> output, Class<K1> keyClass, Class<V1> valueClass) throws Exception
{    Constructor<?> cons = getNestedContextConstructor(reducer.getClass());        return (Reducer.Context) cons.newInstance(reducer, configuration, new TaskAttemptID(), new MockIterator(), null, null, output, null, new DummyStatusReporter(), null, keyClass, valueClass);}
0
private static Constructor<?> getNestedContextConstructor(Class<?> outerClass)
{    for (Class<?> nestedClass : outerClass.getClasses()) {        if ("Context".equals(nestedClass.getSimpleName())) {            return nestedClass.getConstructors()[0];        }    }    throw new IllegalStateException("Cannot find context class for " + outerClass);}
0
public void testWrite()
{    DummyRecordWriter<IntWritable, VectorWritable> writer = new DummyRecordWriter<>();    IntWritable reusableIntWritable = new IntWritable();    VectorWritable reusableVectorWritable = new VectorWritable();    reusableIntWritable.set(0);    reusableVectorWritable.set(new DenseVector(new double[] { 1, 2, 3 }));    writer.write(reusableIntWritable, reusableVectorWritable);    reusableIntWritable.set(1);    reusableVectorWritable.set(new DenseVector(new double[] { 4, 5, 6 }));    writer.write(reusableIntWritable, reusableVectorWritable);    Assert.assertEquals("The writer must remember the two keys that is written to it", 2, writer.getKeys().size());}
0
private static Counter newCounter()
{    try {                String c = "org.apache.hadoop.mapreduce.counters.GenericCounter";        return (Counter) EasyMock.createMockBuilder(Class.forName(c)).createMock();    } catch (ClassNotFoundException e) {                return EasyMock.createMockBuilder(Counter.class).createMock();    }}
0
public Counter getCounter(Enum<?> name)
{    if (!counters.containsKey(name)) {        counters.put(name, newCounter());    }    return counters.get(name);}
0
public Counter getCounter(String group, String name)
{    if (!counterGroups.containsKey(group + name)) {        counterGroups.put(group + name, newCounter());    }    return counterGroups.get(group + name);}
0
public void progress()
{}
0
public void setStatus(String status)
{}
0
public float getProgress()
{    return 0.0f;}
0
public void testGetSet()
{    IntPairWritable n = new IntPairWritable();    assertEquals(0, n.getFirst());    assertEquals(0, n.getSecond());    n.setFirst(5);    n.setSecond(10);    assertEquals(5, n.getFirst());    assertEquals(10, n.getSecond());    n = new IntPairWritable(2, 4);    assertEquals(2, n.getFirst());    assertEquals(4, n.getSecond());}
0
public void testWritable() throws Exception
{    IntPairWritable one = new IntPairWritable(1, 2);    IntPairWritable two = new IntPairWritable(3, 4);    assertEquals(1, one.getFirst());    assertEquals(2, one.getSecond());    assertEquals(3, two.getFirst());    assertEquals(4, two.getSecond());    ByteArrayOutputStream bout = new ByteArrayOutputStream();    DataOutput out = new DataOutputStream(bout);    two.write(out);    byte[] b = bout.toByteArray();    ByteArrayInputStream bin = new ByteArrayInputStream(b);    DataInput din = new DataInputStream(bin);    one.readFields(din);    assertEquals(two.getFirst(), one.getFirst());    assertEquals(two.getSecond(), one.getSecond());}
0
public void testComparable()
{    IntPairWritable[] input = { new IntPairWritable(2, 3), new IntPairWritable(2, 2), new IntPairWritable(1, 3), new IntPairWritable(1, 2), new IntPairWritable(2, 1), new IntPairWritable(2, 2), new IntPairWritable(1, -2), new IntPairWritable(1, -1), new IntPairWritable(-2, -2), new IntPairWritable(-2, -1), new IntPairWritable(-1, -1), new IntPairWritable(-1, -2), new IntPairWritable(Integer.MAX_VALUE, 1), new IntPairWritable(Integer.MAX_VALUE / 2, 1), new IntPairWritable(Integer.MIN_VALUE, 1), new IntPairWritable(Integer.MIN_VALUE / 2, 1) };    IntPairWritable[] sorted = new IntPairWritable[input.length];    System.arraycopy(input, 0, sorted, 0, input.length);    Arrays.sort(sorted);    int[] expected = { 14, 15, 8, 9, 11, 10, 6, 7, 3, 2, 4, 1, 5, 0, 13, 12 };    for (int i = 0; i < input.length; i++) {        assertSame(input[expected[i]], sorted[i]);    }}
0
public void testEmptyCase()
{    assertFalse(new CountingIterator(0).hasNext());}
0
public void testCount()
{    Iterator<Integer> it = new CountingIterator(3);    assertTrue(it.hasNext());    assertEquals(0, (int) it.next());    assertTrue(it.hasNext());    assertEquals(1, (int) it.next());    assertTrue(it.hasNext());    assertEquals(2, (int) it.next());    assertFalse(it.hasNext());}
0
public void testEmptyCase()
{    assertFalse(createSampler(100, new CountingIterator(0)).hasNext());}
0
public void testSmallInput()
{    Iterator<Integer> t = createSampler(10, new CountingIterator(1));    assertTrue(t.hasNext());    assertEquals(0, t.next().intValue());    assertFalse(t.hasNext());    t = createSampler(10, new CountingIterator(1));    assertTrue(t.hasNext());    assertEquals(0, t.next().intValue());    assertFalse(t.hasNext());}
0
public void testAbsurdSize()
{    Iterator<Integer> t = createSampler(0, new CountingIterator(2));    assertFalse(t.hasNext());}
0
public void testExactSizeMatch()
{    Iterator<Integer> t = createSampler(10, new CountingIterator(10));    for (int i = 0; i < 10; i++) {        assertTrue(t.hasNext());        assertEquals(i, t.next().intValue());    }    assertFalse(t.hasNext());}
0
public void testSample()
{    Iterator<Integer> source = new CountingIterator(100);    Iterator<Integer> t = createSampler(15, source);        List<Integer> expectedValues = Arrays.asList(52, 28, 2, 60, 50, 32, 65, 79, 78, 9, 40, 33, 96, 25, 48);    if (isSorted()) {        Collections.sort(expectedValues);    }    Iterator<Integer> expected = expectedValues.iterator();    int last = Integer.MIN_VALUE;    for (int i = 0; i < 15; i++) {        assertTrue(t.hasNext());        int actual = t.next();        if (isSorted()) {            assertTrue(actual >= last);            last = actual;        } else {                        if (actual < 15) {                assertEquals(i, actual);            }        }        assertTrue(actual >= 0 && actual < 100);                assertEquals(expected.next().intValue(), actual);        assertFalse(source.hasNext());    }    assertFalse(t.hasNext());}
0
protected Iterator<Integer> createSampler(int n, Iterator<Integer> source)
{    return new FixedSizeSamplingIterator<>(n, source);}
0
protected boolean isSorted()
{    return false;}
0
public void testEmptyCase()
{    assertFalse(new SamplingIterator<>(new CountingIterator(0), 0.9999).hasNext());    assertFalse(new SamplingIterator<>(new CountingIterator(0), 1).hasNext());}
0
public void testSmallInput()
{    Iterator<Integer> t = new SamplingIterator<>(new CountingIterator(1), 0.9999);    assertTrue(t.hasNext());    assertEquals(0, t.next().intValue());    assertFalse(t.hasNext());}
0
public void testBadRate1()
{    new SamplingIterator<>(new CountingIterator(1), 0.0);}
0
public void testBadRate2()
{    new SamplingIterator<>(new CountingIterator(1), 1.1);}
0
public void testExactSizeMatch()
{    Iterator<Integer> t = new SamplingIterator<>(new CountingIterator(10), 1);    for (int i = 0; i < 10; i++) {        assertTrue(t.hasNext());        assertEquals(i, t.next().intValue());    }    assertFalse(t.hasNext());}
0
public void testSample()
{    for (int i = 0; i < 1000; i++) {        Iterator<Integer> t = new SamplingIterator<>(new CountingIterator(1000), 0.1);        int k = 0;        while (t.hasNext()) {            int v = t.next();            k++;            assertTrue(v >= 0);            assertTrue(v < 1000);        }        double sd = Math.sqrt(0.9 * 0.1 * 1000);        assertTrue(k >= 100 - 4 * sd);        assertTrue(k <= 100 + 4 * sd);    }}
0
protected Iterator<Integer> createSampler(int n, Iterator<Integer> source)
{    return new StableFixedSizeSamplingIterator<>(n, source);}
0
protected boolean isSorted()
{    return true;}
0
public void createStandardAnalyzer() throws Exception
{    assertNotNull(AnalyzerUtils.createAnalyzer(StandardAnalyzer.class.getName()));}
0
public void createCJKAnalyzer() throws Exception
{    assertNotNull(AnalyzerUtils.createAnalyzer(CJKAnalyzer.class.getName()));}
0
public void setUp() throws Exception
{    super.setUp();    RandomUtils.useTestSeed();    testTempDirPath = null;    fs = null;}
0
public void tearDown() throws Exception
{    if (testTempDirPath != null) {        try {            fs.delete(testTempDirPath, true);        } catch (IOException e) {            throw new IllegalStateException("Test file not found");        }        testTempDirPath = null;        fs = null;    }    super.tearDown();}
0
public final Configuration getConfiguration() throws IOException
{    Configuration conf = new Configuration();    conf.set("hadoop.tmp.dir", getTestTempDir("hadoop" + Math.random()).getAbsolutePath());    return conf;}
0
protected final Path getTestTempDirPath() throws IOException
{    if (testTempDirPath == null) {        fs = FileSystem.get(getConfiguration());        long simpleRandomLong = (long) (Long.MAX_VALUE * Math.random());        testTempDirPath = fs.makeQualified(new Path("/tmp/mahout-" + getClass().getSimpleName() + '-' + simpleRandomLong));        if (!fs.mkdirs(testTempDirPath)) {            throw new IOException("Could not create " + testTempDirPath);        }        fs.deleteOnExit(testTempDirPath);    }    return testTempDirPath;}
0
protected final Path getTestTempFilePath(String name) throws IOException
{    return getTestTempFileOrDirPath(name, false);}
0
protected final Path getTestTempDirPath(String name) throws IOException
{    return getTestTempFileOrDirPath(name, true);}
0
private Path getTestTempFileOrDirPath(String name, boolean dir) throws IOException
{    Path testTempDirPath = getTestTempDirPath();    Path tempFileOrDir = fs.makeQualified(new Path(testTempDirPath, name));    fs.deleteOnExit(tempFileOrDir);    if (dir && !fs.mkdirs(tempFileOrDir)) {        throw new IOException("Could not create " + tempFileOrDir);    }    return tempFileOrDir;}
0
protected static void setField(Object target, String fieldname, Object value) throws NoSuchFieldException, IllegalAccessException
{    Field field = findDeclaredField(target.getClass(), fieldname);    field.setAccessible(true);    field.set(target, value);}
0
private static Field findDeclaredField(Class<?> inClass, String fieldname) throws NoSuchFieldException
{    while (!Object.class.equals(inClass)) {        for (Field field : inClass.getDeclaredFields()) {            if (field.getName().equalsIgnoreCase(fieldname)) {                return field;            }        }        inClass = inClass.getSuperclass();    }    throw new NoSuchFieldException();}
0
protected static String optKey(String optionName)
{    return AbstractJob.keyFor(optionName);}
0
protected static void writeLines(File file, String... lines) throws IOException
{    Writer writer = new OutputStreamWriter(new FileOutputStream(file), Charsets.UTF_8);    try {        for (String line : lines) {            writer.write(line);            writer.write('\n');        }    } finally {        Closeables.close(writer, false);    }}
0
public void close()
{}
0
public DataInputBuffer getKey()
{    return null;}
0
public Progress getProgress()
{    return null;}
0
public DataInputBuffer getValue()
{    return null;}
0
public boolean next()
{    return true;}
0
public boolean equals(Object obj)
{    if (this == obj) {        return true;    }    if (!(obj instanceof DummyTest)) {        return false;    }    DummyTest dt = (DummyTest) obj;    return field == dt.field;}
0
public int hashCode()
{    return field;}
0
public int getField()
{    return field;}
0
public void testStringConversion() throws Exception
{    List<String> expected = Lists.newArrayList("A", "B", "C");    assertEquals(expected, StringUtils.fromString(StringUtils.toString(expected)));        DummyTest test = new DummyTest();    assertEquals(test, StringUtils.fromString(StringUtils.toString(test)));}
0
public void testEscape() throws Exception
{    String res = StringUtils.escapeXML("\",\',&,>,<");    assertEquals("_,_,_,_,_", res);}
0
public void testMain() throws Throwable
{    MahoutDriver.main(new String[] { "canopy", "help" });}
0
public void testConverges() throws Exception
{    State<Foo, Double> s0 = new State<>(new double[5], 1);    s0.setPayload(new Foo());    EvolutionaryProcess<Foo, Double> ep = new EvolutionaryProcess<>(10, 100, s0);    State<Foo, Double> best = null;    for (int i = 0; i < 20; i++) {        best = ep.parallelDo(new EvolutionaryProcess.Function<Payload<Double>>() {            @Override            public double apply(Payload<Double> payload, double[] params) {                int i = 1;                double sum = 0;                for (double x : params) {                    sum += i * (x - i) * (x - i);                    i++;                }                return -sum;            }        });        ep.mutatePopulation(3);        System.out.printf("%10.3f %.3f\n", best.getValue(), best.getOmni());    }    ep.close();    assertNotNull(best);    assertEquals(0.0, best.getValue(), 0.02);}
0
public double apply(Payload<Double> payload, double[] params)
{    int i = 1;    double sum = 0;    for (double x : params) {        sum += i * (x - i) * (x - i);        i++;    }    return -sum;}
0
public Foo copy()
{    return this;}
0
public void update(double[] params)
{}
0
public void write(DataOutput dataOutput) throws IOException
{}
0
public void readFields(DataInput dataInput) throws IOException
{}
0
public void setUp() throws Exception
{    super.setUp();    File symTestData = getTestTempDir("symTestData");    File asymTestData = getTestTempDir("asymTestData");    symCorpus = new TestDistributedRowMatrix().randomDistributedMatrix(100, 90, 80, 2, 10.0, true, symTestData.getAbsolutePath());    asymCorpus = new TestDistributedRowMatrix().randomDistributedMatrix(100, 90, 80, 2, 10.0, false, asymTestData.getAbsolutePath());}
0
private static String suf(boolean symmetric)
{    return symmetric ? "_sym" : "_asym";}
0
private DistributedRowMatrix getCorpus(boolean symmetric)
{    return symmetric ? symCorpus : asymCorpus;}
0
private LanczosState doTestDistributedLanczosSolver(boolean symmetric, int desiredRank, boolean hdfsBackedState) throws IOException
{    DistributedRowMatrix corpus = getCorpus(symmetric);    Configuration conf = getConfiguration();    corpus.setConf(conf);    DistributedLanczosSolver solver = new DistributedLanczosSolver();    Vector intitialVector = DistributedLanczosSolver.getInitialVector(corpus);    LanczosState state;    if (hdfsBackedState) {        HdfsBackedLanczosState hState = new HdfsBackedLanczosState(corpus, desiredRank, intitialVector, new Path(getTestTempDirPath(), "lanczosStateDir" + suf(symmetric) + counter));        hState.setConf(conf);        state = hState;    } else {        state = new LanczosState(corpus, desiredRank, intitialVector);    }    solver.solve(state, desiredRank, symmetric);    SolverTest.assertOrthonormal(state);    for (int i = 0; i < desiredRank / 2; i++) {        SolverTest.assertEigen(i, state.getRightSingularVector(i), corpus, 0.1, symmetric);    }    counter++;    return state;}
0
public void doTestResumeIteration(boolean symmetric) throws IOException
{    DistributedRowMatrix corpus = getCorpus(symmetric);    Configuration conf = getConfiguration();    corpus.setConf(conf);    DistributedLanczosSolver solver = new DistributedLanczosSolver();    int rank = 10;    Vector intitialVector = DistributedLanczosSolver.getInitialVector(corpus);    HdfsBackedLanczosState state = new HdfsBackedLanczosState(corpus, rank, intitialVector, new Path(getTestTempDirPath(), "lanczosStateDir" + suf(symmetric) + counter));    solver.solve(state, rank, symmetric);    rank *= 2;    state = new HdfsBackedLanczosState(corpus, rank, intitialVector, new Path(getTestTempDirPath(), "lanczosStateDir" + suf(symmetric) + counter));    solver = new DistributedLanczosSolver();    solver.solve(state, rank, symmetric);    LanczosState allAtOnceState = doTestDistributedLanczosSolver(symmetric, rank, false);    for (int i = 0; i < state.getIterationNumber(); i++) {        Vector v = state.getBasisVector(i).normalize();        Vector w = allAtOnceState.getBasisVector(i).normalize();        double diff = v.minus(w).norm(2);        assertTrue("basis " + i + " is too long: " + diff, diff < 0.1);    }    counter++;}
0
public void testDistributedLanczosSolverCLI() throws Exception
{    Path testData = getTestTempDirPath("testdata");    DistributedRowMatrix corpus = new TestDistributedRowMatrix().randomDenseHierarchicalDistributedMatrix(10, 9, false, testData.toString());    corpus.setConf(getConfiguration());    Path output = getTestTempDirPath("output");    Path tmp = getTestTempDirPath("tmp");    Path workingDir = getTestTempDirPath("working");    String[] args = { "-i", new Path(testData, "distMatrix").toString(), "-o", output.toString(), "--tempDir", tmp.toString(), "--numRows", "10", "--numCols", "9", "--rank", "6", "--symmetric", "false", "--workingDir", workingDir.toString() };    ToolRunner.run(getConfiguration(), new DistributedLanczosSolver().new DistributedLanczosSolverJob(), args);    output = getTestTempDirPath("output2");    tmp = getTestTempDirPath("tmp2");    args = new String[] { "-i", new Path(testData, "distMatrix").toString(), "-o", output.toString(), "--tempDir", tmp.toString(), "--numRows", "10", "--numCols", "9", "--rank", "7", "--symmetric", "false", "--workingDir", workingDir.toString() };    ToolRunner.run(getConfiguration(), new DistributedLanczosSolver().new DistributedLanczosSolverJob(), args);    Path rawEigenvectors = new Path(output, DistributedLanczosSolver.RAW_EIGENVECTORS);    Matrix eigenVectors = new DenseMatrix(7, corpus.numCols());    Configuration conf = getConfiguration();    int i = 0;    for (VectorWritable value : new SequenceFileValueIterable<VectorWritable>(rawEigenvectors, conf)) {        Vector v = value.get();        eigenVectors.assignRow(i, v);        i++;    }    assertEquals("number of eigenvectors", 7, i);}
0
public void testDistributedLanczosSolverEVJCLI() throws Exception
{    Path testData = getTestTempDirPath("testdata");    DistributedRowMatrix corpus = new TestDistributedRowMatrix().randomDenseHierarchicalDistributedMatrix(10, 9, false, testData.toString());    corpus.setConf(getConfiguration());    Path output = getTestTempDirPath("output");    Path tmp = getTestTempDirPath("tmp");    String[] args = { "-i", new Path(testData, "distMatrix").toString(), "-o", output.toString(), "--tempDir", tmp.toString(), "--numRows", "10", "--numCols", "9", "--rank", "6", "--symmetric", "false", "--cleansvd", "true" };    ToolRunner.run(getConfiguration(), new DistributedLanczosSolver().new DistributedLanczosSolverJob(), args);    Path cleanEigenvectors = new Path(output, EigenVerificationJob.CLEAN_EIGENVECTORS);    Matrix eigenVectors = new DenseMatrix(6, corpus.numCols());    Collection<Double> eigenvalues = Lists.newArrayList();    output = getTestTempDirPath("output2");    tmp = getTestTempDirPath("tmp2");    args = new String[] { "-i", new Path(testData, "distMatrix").toString(), "-o", output.toString(), "--tempDir", tmp.toString(), "--numRows", "10", "--numCols", "9", "--rank", "7", "--symmetric", "false", "--cleansvd", "true" };    ToolRunner.run(getConfiguration(), new DistributedLanczosSolver().new DistributedLanczosSolverJob(), args);    Path cleanEigenvectors2 = new Path(output, EigenVerificationJob.CLEAN_EIGENVECTORS);    Matrix eigenVectors2 = new DenseMatrix(7, corpus.numCols());    Configuration conf = getConfiguration();    Collection<Double> newEigenValues = Lists.newArrayList();    int i = 0;    for (VectorWritable value : new SequenceFileValueIterable<VectorWritable>(cleanEigenvectors, conf)) {        NamedVector v = (NamedVector) value.get();        eigenVectors.assignRow(i, v);                if (EigenVector.getCosAngleError(v.getName()) < 1.0e-3) {            eigenvalues.add(EigenVector.getEigenValue(v.getName()));        }        i++;    }    assertEquals("number of clean eigenvectors", 3, i);    i = 0;    for (VectorWritable value : new SequenceFileValueIterable<VectorWritable>(cleanEigenvectors2, conf)) {        NamedVector v = (NamedVector) value.get();                eigenVectors2.assignRow(i, v);        newEigenValues.add(EigenVector.getEigenValue(v.getName()));        i++;    }    Collection<Integer> oldEigensFound = Lists.newArrayList();    for (int row = 0; row < eigenVectors.numRows(); row++) {        Vector oldEigen = eigenVectors.viewRow(row);        if (oldEigen == null) {            break;        }        for (int newRow = 0; newRow < eigenVectors2.numRows(); newRow++) {            Vector newEigen = eigenVectors2.viewRow(newRow);            if (newEigen != null && oldEigen.dot(newEigen) > 0.9) {                oldEigensFound.add(row);                break;            }        }    }    assertEquals("the number of new eigenvectors", 5, i);    Collection<Double> oldEigenValuesNotFound = Lists.newArrayList();    for (double d : eigenvalues) {        boolean found = false;        for (double newD : newEigenValues) {            if (Math.abs((d - newD) / d) < 0.1) {                found = true;            }        }        if (!found) {            oldEigenValuesNotFound.add(d);        }    }    assertEquals("number of old eigenvalues not found: " + Arrays.toString(oldEigenValuesNotFound.toArray(new Double[oldEigenValuesNotFound.size()])), 0, oldEigenValuesNotFound.size());    assertEquals("did not find enough old eigenvectors", 3, oldEigensFound.size());}
1
public static Vector.Element elem(int index, double value)
{    return new ElementToCheck(index, value);}
0
public double get()
{    return value;}
0
public int index()
{    return index;}
0
public void set(double value)
{    this.value = value;}
0
public static VectorWritable vectorMatches(final Vector.Element... elements)
{    EasyMock.reportMatcher(new IArgumentMatcher() {        @Override        public boolean matches(Object argument) {            if (argument instanceof VectorWritable) {                Vector v = ((VectorWritable) argument).get();                return consistsOf(v, elements);            }            return false;        }        @Override        public void appendTo(StringBuffer buffer) {        }    });    return null;}
0
public boolean matches(Object argument)
{    if (argument instanceof VectorWritable) {        Vector v = ((VectorWritable) argument).get();        return consistsOf(v, elements);    }    return false;}
0
public void appendTo(StringBuffer buffer)
{}
0
public static boolean consistsOf(Vector vector, Vector.Element... elements)
{    if (elements.length != numberOfNoNZeroNonNaNElements(vector)) {        return false;    }    for (Vector.Element element : elements) {        if (Math.abs(element.get() - vector.get(element.index())) > MahoutTestCase.EPSILON) {            return false;        }    }    return true;}
0
public static int numberOfNoNZeroNonNaNElements(Vector vector)
{    int elementsInVector = 0;    for (Element currentElement : vector.nonZeroes()) {        if (!Double.isNaN(currentElement.get())) {            elementsInVector++;        }    }    return elementsInVector;}
0
public static Matrix readMatrix(Configuration conf, Path path, int rows, int columns)
{    boolean readOneRow = false;    Matrix matrix = new DenseMatrix(rows, columns);    for (Pair<IntWritable, VectorWritable> record : new SequenceFileIterable<IntWritable, VectorWritable>(path, true, conf)) {        IntWritable key = record.getFirst();        VectorWritable value = record.getSecond();        readOneRow = true;        int row = key.get();        for (Element element : value.get().nonZeroes()) {            matrix.set(row, element.index(), element.get());        }    }    if (!readOneRow) {        throw new IllegalStateException("Not a single row read!");    }    return matrix;}
0
public static OpenIntObjectHashMap<Vector> readMatrixRows(Configuration conf, Path path)
{    boolean readOneRow = false;    OpenIntObjectHashMap<Vector> rows = new OpenIntObjectHashMap<>();    for (Pair<IntWritable, VectorWritable> record : new SequenceFileIterable<IntWritable, VectorWritable>(path, true, conf)) {        IntWritable key = record.getFirst();        readOneRow = true;        rows.put(key.get(), record.getSecond().get());    }    if (!readOneRow) {        throw new IllegalStateException("Not a single row read!");    }    return rows;}
0
public static void writeDistributedRowMatrix(double[][] entries, FileSystem fs, Configuration conf, Path path) throws IOException
{    SequenceFile.Writer writer = null;    try {        writer = new SequenceFile.Writer(fs, conf, path, IntWritable.class, VectorWritable.class);        for (int n = 0; n < entries.length; n++) {            Vector v = new RandomAccessSparseVector(entries[n].length);            for (int m = 0; m < entries[n].length; m++) {                v.setQuick(m, entries[n][m]);            }            writer.append(new IntWritable(n), new VectorWritable(v));        }    } finally {        Closeables.close(writer, false);    }}
0
public static void assertMatrixEquals(Matrix expected, Matrix actual)
{    Assert.assertEquals(expected.numRows(), actual.numRows());    Assert.assertEquals(actual.numCols(), actual.numCols());    for (int row = 0; row < expected.numRows(); row++) {        for (int col = 0; col < expected.numCols(); col++) {            Assert.assertEquals("Non-matching values in [" + row + ',' + col + ']', expected.get(row, col), actual.get(row, col), MahoutTestCase.EPSILON);        }    }}
0
public static String nice(Vector v)
{    if (!v.isSequentialAccess()) {        v = new DenseVector(v);    }    DecimalFormat df = new DecimalFormat("0.00", DecimalFormatSymbols.getInstance(Locale.ENGLISH));    StringBuilder buffer = new StringBuilder("[");    String separator = "";    for (Vector.Element e : v.all()) {        buffer.append(separator);        if (Double.isNaN(e.get())) {            buffer.append("  -  ");        } else {            if (e.get() >= 0) {                buffer.append(' ');            }            buffer.append(df.format(e.get()));        }        separator = "\t";    }    buffer.append(" ]");    return buffer.toString();}
0
public static String nice(Matrix matrix)
{    StringBuilder info = new StringBuilder();    for (int n = 0; n < matrix.numRows(); n++) {        info.append(nice(matrix.viewRow(n))).append('\n');    }    return info.toString();}
0
 static double distributedSimilarity(double[] one, double[] two, Class<? extends VectorSimilarityMeasure> similarityMeasureClass)
{    double rand = computeSimilarity(one, two, similarityMeasureClass, new RandomAccessSparseVector(one.length));    double seq = computeSimilarity(one, two, similarityMeasureClass, new SequentialAccessSparseVector(one.length));    double dense = computeSimilarity(one, two, similarityMeasureClass, new DenseVector(one.length));    assertEquals(seq, rand, 1.0e-10);    assertEquals(seq, dense, 1.0e-10);    assertEquals(dense, rand, 1.0e-10);    return seq;}
0
private static double computeSimilarity(double[] one, double[] two, Class<? extends VectorSimilarityMeasure> similarityMeasureClass, Vector like)
{    VectorSimilarityMeasure similarityMeasure = ClassUtils.instantiateAs(similarityMeasureClass, VectorSimilarityMeasure.class);    Vector oneNormalized = similarityMeasure.normalize(asVector(one, like));    Vector twoNormalized = similarityMeasure.normalize(asVector(two, like));    double normOne = similarityMeasure.norm(oneNormalized);    double normTwo = similarityMeasure.norm(twoNormalized);    double dot = 0;    for (int n = 0; n < one.length; n++) {        if (oneNormalized.get(n) != 0 && twoNormalized.get(n) != 0) {            dot += similarityMeasure.aggregate(oneNormalized.get(n), twoNormalized.get(n));        }    }    return similarityMeasure.similarity(dot, normOne, normTwo, one.length);}
0
 static Vector asVector(double[] values, Vector like)
{    Vector vector = like.like();    for (int dim = 0; dim < values.length; dim++) {        vector.set(dim, values[dim]);    }    return vector;}
0
public void testCooccurrenceCountSimilarity()
{    double similarity = distributedSimilarity(new double[] { 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0 }, new double[] { 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1 }, CooccurrenceCountSimilarity.class);    assertEquals(5.0, similarity, 0);}
0
public void testTanimotoCoefficientSimilarity()
{    double similarity = distributedSimilarity(new double[] { 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0 }, new double[] { 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1 }, TanimotoCoefficientSimilarity.class);    assertEquals(0.454545455, similarity, EPSILON);}
0
public void testCityblockSimilarity()
{    double similarity = distributedSimilarity(new double[] { 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0 }, new double[] { 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1 }, CityBlockSimilarity.class);    assertEquals(0.142857143, similarity, EPSILON);}
0
public void testLoglikelihoodSimilarity()
{    double similarity = distributedSimilarity(new double[] { 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0 }, new double[] { 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1 }, LoglikelihoodSimilarity.class);    assertEquals(0.03320155369284261, similarity, EPSILON);}
0
public void testCosineSimilarity()
{    double similarity = distributedSimilarity(new double[] { 0, 2, 0, 0, 8, 3, 0, 6, 0, 1, 2, 2, 0 }, new double[] { 3, 0, 0, 0, 7, 0, 2, 2, 1, 3, 2, 1, 1 }, CosineSimilarity.class);    assertEquals(0.769846046, similarity, EPSILON);}
0
public void testPearsonCorrelationSimilarity()
{    double similarity = distributedSimilarity(new double[] { 0, 2, 0, 0, 8, 3, 0, 6, 0, 1, 1, 2, 1 }, new double[] { 3, 0, 0, 0, 7, 0, 2, 2, 1, 3, 2, 4, 3 }, PearsonCorrelationSimilarity.class);    assertEquals(0.5303300858899108, similarity, EPSILON);}
0
public void testEuclideanDistanceSimilarity()
{    double similarity = distributedSimilarity(new double[] { 0, 2, 0, 0, 8, 3, 0, 6, 0, 1, 1, 2, 1 }, new double[] { 3, 0, 0, 0, 7, 0, 2, 2, 1, 3, 2, 4, 4 }, EuclideanDistanceSimilarity.class);    assertEquals(0.11268865367232477, similarity, EPSILON);}
0
public void toyIntegration() throws Exception
{    File inputFile = getTestTempFile("rows");    File outputDir = getTestTempDir("output");    outputDir.delete();    File tmpDir = getTestTempDir("tmp");    Configuration conf = getConfiguration();    Path inputPath = new Path(inputFile.getAbsolutePath());    FileSystem fs = FileSystem.get(inputPath.toUri(), conf);    MathHelper.writeDistributedRowMatrix(new double[][] { new double[] { 1, 0, 1, 1, 0 }, new double[] { 0, 0, 1, 1, 0 }, new double[] { 0, 0, 0, 0, 1 } }, fs, conf, inputPath);    RowSimilarityJob rowSimilarityJob = new RowSimilarityJob();    rowSimilarityJob.setConf(conf);    rowSimilarityJob.run(new String[] { "--input", inputFile.getAbsolutePath(), "--output", outputDir.getAbsolutePath(), "--numberOfColumns", String.valueOf(5), "--similarityClassname", TanimotoCoefficientSimilarity.class.getName(), "--tempDir", tmpDir.getAbsolutePath() });    OpenIntIntHashMap observationsPerColumn = Vectors.readAsIntMap(new Path(tmpDir.getAbsolutePath(), "observationsPerColumn.bin"), conf);    assertEquals(4, observationsPerColumn.size());    assertEquals(1, observationsPerColumn.get(0));    assertEquals(2, observationsPerColumn.get(2));    assertEquals(2, observationsPerColumn.get(3));    assertEquals(1, observationsPerColumn.get(4));    Matrix similarityMatrix = MathHelper.readMatrix(conf, new Path(outputDir.getAbsolutePath(), "part-r-00000"), 3, 3);    assertNotNull(similarityMatrix);    assertEquals(3, similarityMatrix.numCols());    assertEquals(3, similarityMatrix.numRows());    assertEquals(1.0, similarityMatrix.get(0, 0), EPSILON);    assertEquals(1.0, similarityMatrix.get(1, 1), EPSILON);    assertEquals(1.0, similarityMatrix.get(2, 2), EPSILON);    assertEquals(0.0, similarityMatrix.get(2, 0), EPSILON);    assertEquals(0.0, similarityMatrix.get(2, 1), EPSILON);    assertEquals(0.0, similarityMatrix.get(0, 2), EPSILON);    assertEquals(0.0, similarityMatrix.get(1, 2), EPSILON);    assertEquals(0.666666, similarityMatrix.get(0, 1), EPSILON);    assertEquals(0.666666, similarityMatrix.get(1, 0), EPSILON);}
0
public void toyIntegrationMaxSimilaritiesPerRow() throws Exception
{    File inputFile = getTestTempFile("rows");    File outputDir = getTestTempDir("output");    outputDir.delete();    File tmpDir = getTestTempDir("tmp");    Configuration conf = getConfiguration();    Path inputPath = new Path(inputFile.getAbsolutePath());    FileSystem fs = FileSystem.get(inputPath.toUri(), conf);    MathHelper.writeDistributedRowMatrix(new double[][] { new double[] { 1, 0, 1, 1, 0, 1 }, new double[] { 0, 1, 1, 1, 1, 1 }, new double[] { 1, 1, 0, 1, 0, 0 } }, fs, conf, inputPath);    RowSimilarityJob rowSimilarityJob = new RowSimilarityJob();    rowSimilarityJob.setConf(conf);    rowSimilarityJob.run(new String[] { "--input", inputFile.getAbsolutePath(), "--output", outputDir.getAbsolutePath(), "--numberOfColumns", String.valueOf(6), "--similarityClassname", TanimotoCoefficientSimilarity.class.getName(), "--maxSimilaritiesPerRow", String.valueOf(1), "--excludeSelfSimilarity", String.valueOf(true), "--tempDir", tmpDir.getAbsolutePath() });    Matrix similarityMatrix = MathHelper.readMatrix(conf, new Path(outputDir.getAbsolutePath(), "part-r-00000"), 3, 3);    assertNotNull(similarityMatrix);    assertEquals(3, similarityMatrix.numCols());    assertEquals(3, similarityMatrix.numRows());    assertEquals(0.0, similarityMatrix.get(0, 0), EPSILON);    assertEquals(0.5, similarityMatrix.get(0, 1), EPSILON);    assertEquals(0.0, similarityMatrix.get(0, 2), EPSILON);    assertEquals(0.5, similarityMatrix.get(1, 0), EPSILON);    assertEquals(0.0, similarityMatrix.get(1, 1), EPSILON);    assertEquals(0.0, similarityMatrix.get(1, 2), EPSILON);    assertEquals(0.4, similarityMatrix.get(2, 0), EPSILON);    assertEquals(0.0, similarityMatrix.get(2, 1), EPSILON);    assertEquals(0.0, similarityMatrix.get(2, 2), EPSILON);}
0
public void toyIntegrationWithThreshold() throws Exception
{    File inputFile = getTestTempFile("rows");    File outputDir = getTestTempDir("output");    outputDir.delete();    File tmpDir = getTestTempDir("tmp");    Configuration conf = getConfiguration();    Path inputPath = new Path(inputFile.getAbsolutePath());    FileSystem fs = FileSystem.get(inputPath.toUri(), conf);    MathHelper.writeDistributedRowMatrix(new double[][] { new double[] { 1, 0, 1, 1, 0, 1 }, new double[] { 0, 1, 1, 1, 1, 1 }, new double[] { 1, 1, 0, 1, 0, 0 } }, fs, conf, inputPath);    RowSimilarityJob rowSimilarityJob = new RowSimilarityJob();    rowSimilarityJob.setConf(conf);    rowSimilarityJob.run(new String[] { "--input", inputFile.getAbsolutePath(), "--output", outputDir.getAbsolutePath(), "--numberOfColumns", String.valueOf(6), "--similarityClassname", TanimotoCoefficientSimilarity.class.getName(), "--excludeSelfSimilarity", String.valueOf(true), "--threshold", String.valueOf(0.5), "--tempDir", tmpDir.getAbsolutePath() });    Matrix similarityMatrix = MathHelper.readMatrix(conf, new Path(outputDir.getAbsolutePath(), "part-r-00000"), 3, 3);    assertNotNull(similarityMatrix);    assertEquals(3, similarityMatrix.numCols());    assertEquals(3, similarityMatrix.numRows());    assertEquals(0.0, similarityMatrix.get(0, 0), EPSILON);    assertEquals(0.5, similarityMatrix.get(0, 1), EPSILON);    assertEquals(0.0, similarityMatrix.get(0, 2), EPSILON);    assertEquals(0.5, similarityMatrix.get(1, 0), EPSILON);    assertEquals(0.0, similarityMatrix.get(1, 1), EPSILON);    assertEquals(0.0, similarityMatrix.get(1, 2), EPSILON);    assertEquals(0.0, similarityMatrix.get(2, 0), EPSILON);    assertEquals(0.0, similarityMatrix.get(2, 1), EPSILON);    assertEquals(0.0, similarityMatrix.get(2, 2), EPSILON);}
0
public void testVectorDimensions() throws Exception
{    File inputFile = getTestTempFile("rows");    Configuration conf = getConfiguration();    Path inputPath = new Path(inputFile.getAbsolutePath());    FileSystem fs = FileSystem.get(inputPath.toUri(), conf);    MathHelper.writeDistributedRowMatrix(new double[][] { new double[] { 1, 0, 1, 1, 0, 1 }, new double[] { 0, 1, 1, 1, 1, 1 }, new double[] { 1, 1, 0, 1, 0, 0 } }, fs, conf, inputPath);    RowSimilarityJob rowSimilarityJob = new RowSimilarityJob();    rowSimilarityJob.setConf(conf);    int numberOfColumns = rowSimilarityJob.getDimensions(inputPath);    assertEquals(6, numberOfColumns);}
0
public void setUp() throws Exception
{    super.setUp();    fs = FileSystem.get(getConfiguration());}
0
public void testVectorDistanceMapper() throws Exception
{    Mapper<WritableComparable<?>, VectorWritable, StringTuple, DoubleWritable>.Context context = EasyMock.createMock(Mapper.Context.class);    StringTuple tuple = new StringTuple();    tuple.add("foo");    tuple.add("123");    context.write(tuple, new DoubleWritable(Math.sqrt(2.0)));    tuple = new StringTuple();    tuple.add("foo2");    tuple.add("123");    context.write(tuple, new DoubleWritable(1));    EasyMock.replay(context);    Vector vector = new RandomAccessSparseVector(2);    vector.set(0, 2);    vector.set(1, 2);    VectorDistanceMapper mapper = new VectorDistanceMapper();    setField(mapper, "measure", new EuclideanDistanceMeasure());    Collection<NamedVector> seedVectors = Lists.newArrayList();    Vector seed1 = new RandomAccessSparseVector(2);    seed1.set(0, 1);    seed1.set(1, 1);    Vector seed2 = new RandomAccessSparseVector(2);    seed2.set(0, 2);    seed2.set(1, 1);    seedVectors.add(new NamedVector(seed1, "foo"));    seedVectors.add(new NamedVector(seed2, "foo2"));    setField(mapper, "seedVectors", seedVectors);    mapper.map(new IntWritable(123), new VectorWritable(vector), context);    EasyMock.verify(context);}
0
public void testVectorDistanceInvertedMapper() throws Exception
{    Mapper<WritableComparable<?>, VectorWritable, Text, VectorWritable>.Context context = EasyMock.createMock(Mapper.Context.class);    Vector expectVec = new DenseVector(new double[] { Math.sqrt(2.0), 1.0 });    context.write(new Text("other"), new VectorWritable(expectVec));    EasyMock.replay(context);    Vector vector = new NamedVector(new RandomAccessSparseVector(2), "other");    vector.set(0, 2);    vector.set(1, 2);    VectorDistanceInvertedMapper mapper = new VectorDistanceInvertedMapper();    setField(mapper, "measure", new EuclideanDistanceMeasure());    Collection<NamedVector> seedVectors = Lists.newArrayList();    Vector seed1 = new RandomAccessSparseVector(2);    seed1.set(0, 1);    seed1.set(1, 1);    Vector seed2 = new RandomAccessSparseVector(2);    seed2.set(0, 2);    seed2.set(1, 1);    seedVectors.add(new NamedVector(seed1, "foo"));    seedVectors.add(new NamedVector(seed2, "foo2"));    setField(mapper, "seedVectors", seedVectors);    mapper.map(new IntWritable(123), new VectorWritable(vector), context);    EasyMock.verify(context);}
0
public void testRun() throws Exception
{    Path input = getTestTempDirPath("input");    Path output = getTestTempDirPath("output");    Path seedsPath = getTestTempDirPath("seeds");    List<VectorWritable> points = getPointsWritable(REFERENCE);    List<VectorWritable> seeds = getPointsWritable(SEEDS);    Configuration conf = getConfiguration();    ClusteringTestUtils.writePointsToFile(points, true, new Path(input, "file1"), fs, conf);    ClusteringTestUtils.writePointsToFile(seeds, true, new Path(seedsPath, "part-seeds"), fs, conf);    String[] args = { optKey(DefaultOptionCreator.INPUT_OPTION), input.toString(), optKey(VectorDistanceSimilarityJob.SEEDS), seedsPath.toString(), optKey(DefaultOptionCreator.OUTPUT_OPTION), output.toString(), optKey(DefaultOptionCreator.DISTANCE_MEASURE_OPTION), EuclideanDistanceMeasure.class.getName() };    ToolRunner.run(getConfiguration(), new VectorDistanceSimilarityJob(), args);    int expectedOutputSize = SEEDS.length * REFERENCE.length;    int outputSize = Iterables.size(new SequenceFileIterable<StringTuple, DoubleWritable>(new Path(output, "part-m-00000"), conf));    assertEquals(expectedOutputSize, outputSize);}
0
public void testMaxDistance() throws Exception
{    Path input = getTestTempDirPath("input");    Path output = getTestTempDirPath("output");    Path seedsPath = getTestTempDirPath("seeds");    List<VectorWritable> points = getPointsWritable(REFERENCE);    List<VectorWritable> seeds = getPointsWritable(SEEDS);    Configuration conf = getConfiguration();    ClusteringTestUtils.writePointsToFile(points, true, new Path(input, "file1"), fs, conf);    ClusteringTestUtils.writePointsToFile(seeds, true, new Path(seedsPath, "part-seeds"), fs, conf);    double maxDistance = 10;    String[] args = { optKey(DefaultOptionCreator.INPUT_OPTION), input.toString(), optKey(VectorDistanceSimilarityJob.SEEDS), seedsPath.toString(), optKey(DefaultOptionCreator.OUTPUT_OPTION), output.toString(), optKey(DefaultOptionCreator.DISTANCE_MEASURE_OPTION), EuclideanDistanceMeasure.class.getName(), optKey(VectorDistanceSimilarityJob.MAX_DISTANCE), String.valueOf(maxDistance) };    ToolRunner.run(getConfiguration(), new VectorDistanceSimilarityJob(), args);    int outputSize = 0;    for (Pair<StringTuple, DoubleWritable> record : new SequenceFileIterable<StringTuple, DoubleWritable>(new Path(output, "part-m-00000"), conf)) {        outputSize++;        assertTrue(record.getSecond().get() <= maxDistance);    }    assertEquals(14, outputSize);}
0
public void testRunInverted() throws Exception
{    Path input = getTestTempDirPath("input");    Path output = getTestTempDirPath("output");    Path seedsPath = getTestTempDirPath("seeds");    List<VectorWritable> points = getPointsWritable(REFERENCE);    List<VectorWritable> seeds = getPointsWritable(SEEDS);    Configuration conf = getConfiguration();    ClusteringTestUtils.writePointsToFile(points, true, new Path(input, "file1"), fs, conf);    ClusteringTestUtils.writePointsToFile(seeds, true, new Path(seedsPath, "part-seeds"), fs, conf);    String[] args = { optKey(DefaultOptionCreator.INPUT_OPTION), input.toString(), optKey(VectorDistanceSimilarityJob.SEEDS), seedsPath.toString(), optKey(DefaultOptionCreator.OUTPUT_OPTION), output.toString(), optKey(DefaultOptionCreator.DISTANCE_MEASURE_OPTION), EuclideanDistanceMeasure.class.getName(), optKey(VectorDistanceSimilarityJob.OUT_TYPE_KEY), "v" };    ToolRunner.run(getConfiguration(), new VectorDistanceSimilarityJob(), args);    DummyOutputCollector<Text, VectorWritable> collector = new DummyOutputCollector<>();    for (Pair<Text, VectorWritable> record : new SequenceFileIterable<Text, VectorWritable>(new Path(output, "part-m-00000"), conf)) {        collector.collect(record.getFirst(), record.getSecond());    }    assertEquals(REFERENCE.length, collector.getData().size());    for (Map.Entry<Text, List<VectorWritable>> entry : collector.getData().entrySet()) {        assertEquals(SEEDS.length, entry.getValue().iterator().next().get().size());    }}
0
private static List<VectorWritable> getPointsWritable(double[][] raw)
{    List<VectorWritable> points = Lists.newArrayList();    for (double[] fr : raw) {        Vector vec = new RandomAccessSparseVector(fr.length);        vec.assign(fr);        points.add(new VectorWritable(vec));    }    return points;}
0
private static Vector randomVector(int size, double entryMean)
{    DenseVector v = new DenseVector(size);    Random r = RandomUtils.getRandom();    for (int i = 0; i < size; ++i) {        v.setQuick(i, r.nextGaussian() * entryMean);    }    return v;}
0
public void testSolver() throws Exception
{    File testData = getTestTempDir("testdata");    DistributedRowMatrix matrix = new TestDistributedRowMatrix().randomDistributedMatrix(10, 10, 10, 10, 10.0, true, testData.getAbsolutePath());    matrix.setConf(getConfiguration());    Vector vector = randomVector(matrix.numCols(), 10.0);    DistributedConjugateGradientSolver solver = new DistributedConjugateGradientSolver();    Vector x = solver.solve(matrix, vector);    Vector solvedVector = matrix.times(x);    double distance = Math.sqrt(vector.getDistanceSquared(solvedVector));    assertEquals(0.0, distance, EPSILON);}
0
private static Vector randomVector(int size, double entryMean)
{    Vector v = new DenseVector(size);    Random r = RandomUtils.getRandom();    for (int i = 0; i < size; ++i) {        v.setQuick(i, r.nextGaussian() * entryMean);    }    return v;}
0
private static Path saveVector(Configuration conf, Path path, Vector v) throws IOException
{    FileSystem fs = path.getFileSystem(conf);    SequenceFile.Writer writer = new SequenceFile.Writer(fs, conf, path, IntWritable.class, VectorWritable.class);    try {        writer.append(new IntWritable(0), new VectorWritable(v));    } finally {        writer.close();    }    return path;}
0
private static Vector loadVector(Configuration conf, Path path) throws IOException
{    FileSystem fs = path.getFileSystem(conf);    SequenceFile.Reader reader = new SequenceFile.Reader(fs, path, conf);    Writable key = new IntWritable();    VectorWritable value = new VectorWritable();    try {        if (!reader.next(key, value)) {            throw new IOException("Input vector file is empty.");        }        return value.get();    } finally {        reader.close();    }}
0
public void testSolver() throws Exception
{    Configuration conf = getConfiguration();    Path testData = getTestTempDirPath("testdata");    DistributedRowMatrix matrix = new TestDistributedRowMatrix().randomDistributedMatrix(10, 10, 10, 10, 10.0, true, testData.toString());    matrix.setConf(conf);    Path output = getTestTempFilePath("output");    Path vectorPath = getTestTempFilePath("vector");    Path tempPath = getTestTempDirPath("tmp");    Vector vector = randomVector(matrix.numCols(), 10.0);    saveVector(conf, vectorPath, vector);    String[] args = { "-i", matrix.getRowPath().toString(), "-o", output.toString(), "--tempDir", tempPath.toString(), "--vector", vectorPath.toString(), "--numRows", "10", "--numCols", "10", "--symmetric", "true" };    DistributedConjugateGradientSolver solver = new DistributedConjugateGradientSolver();    ToolRunner.run(getConfiguration(), solver.job(), args);    Vector x = loadVector(conf, output);    Vector solvedVector = matrix.times(x);    double distance = Math.sqrt(vector.getDistanceSquared(solvedVector));    assertEquals(0.0, distance, EPSILON);}
0
public void setUp() throws Exception
{    super.setUp();    conf = getConfiguration();}
0
public void testVar() throws Exception
{    Path input = getTestTempFilePath("stdDev/counts.file");    Path output = getTestTempFilePath("stdDev/output.file");    produceTestData(input);    double v = BasicStats.variance(input, output, conf);    assertEquals(2.44, v, 0.01);}
0
public void testStdDev() throws Exception
{    Path input = getTestTempFilePath("stdDev/counts.file");    Path output = getTestTempFilePath("stdDev/output.file");    produceTestData(input);    double v = BasicStats.stdDev(input, output, conf);        assertEquals(1.56, v, 0.01);}
0
public void testStdDevForGivenMean() throws Exception
{    Path input = getTestTempFilePath("stdDev/counts.file");    Path output = getTestTempFilePath("stdDev/output.file");    produceTestData(input);    double v = BasicStats.stdDevForGivenMean(input, output, 0.0D, conf);        assertEquals(10.65, v, 0.01);}
0
private void produceTestData(Path input) throws Exception
{    FileSystem fs = FileSystem.get(input.toUri(), conf);    SequenceFile.Writer writer = new SequenceFile.Writer(fs, conf, input, IntWritable.class, DoubleWritable.class);        /*Normal normal = new Normal(5, 3, random);    for (int i = 0; i < 10000; i++) {      writer.append(new IntWritable(i), new DoubleWritable((long)normal.nextDouble()));    }*/    int i = 0;    writer.append(new IntWritable(i++), new DoubleWritable(7));    writer.append(new IntWritable(i++), new DoubleWritable(9));    writer.append(new IntWritable(i++), new DoubleWritable(9));    writer.append(new IntWritable(i++), new DoubleWritable(10));    writer.append(new IntWritable(i++), new DoubleWritable(10));    writer.append(new IntWritable(i++), new DoubleWritable(10));    writer.append(new IntWritable(i++), new DoubleWritable(10));    writer.append(new IntWritable(i++), new DoubleWritable(11));    writer.append(new IntWritable(i++), new DoubleWritable(11));    writer.append(new IntWritable(i++), new DoubleWritable(13));    writer.close();}
0
public void testStdDev2() throws Exception
{    Path input = getTestTempFilePath("stdDev/counts.file");    Path output = getTestTempFilePath("stdDev/output.file");    FileSystem fs = FileSystem.get(input.toUri(), conf);    SequenceFile.Writer writer = new SequenceFile.Writer(fs, conf, input, IntWritable.class, DoubleWritable.class);    Random random = RandomUtils.getRandom();    Normal normal = new Normal(5, 3, random);    for (int i = 0; i < 1000000; i++) {        writer.append(new IntWritable(i), new DoubleWritable((long) normal.nextInt()));    }    writer.close();    double v = BasicStats.stdDev(input, output, conf);    assertEquals(3, v, 0.02);}
0
public void testOmegaTRightMultiply()
{    final Random rnd = RandomUtils.getRandom();    final long seed = rnd.nextLong();    final int n = 2000;    final int kp = 100;    final Omega omega = new Omega(seed, kp);    final Matrix materializedOmega = new DenseMatrix(n, kp);    for (int i = 0; i < n; i++) for (int j = 0; j < kp; j++) materializedOmega.setQuick(i, j, omega.getQuick(i, j));    Vector xi = new DenseVector(n);    xi.assign(new DoubleFunction() {        @Override        public double apply(double x) {            return rnd.nextDouble() * 100;        }    });    Vector s_o = omega.mutlithreadedTRightMultiply(xi);    Matrix xiVector = new DenseMatrix(n, 1);    xiVector.assignColumn(0, xi);    Vector s_o_control = materializedOmega.transpose().times(xiVector).viewColumn(0);    assertEquals(0, s_o.minus(s_o_control).aggregate(Functions.PLUS, Functions.ABS), 1e-10);    System.out.printf("s_omega=\n%s\n", s_o);    System.out.printf("s_omega_control=\n%s\n", s_o_control);}
0
public double apply(double x)
{    return rnd.nextDouble() * 100;}
0
public void runPCATest1() throws IOException
{    runSSVDSolver(1);}
0
public void runPCATest0() throws IOException
{    runSSVDSolver(0);}
0
public void runSSVDSolver(int q) throws IOException
{    Configuration conf = new Configuration();    conf.set("mapred.job.tracker", "local");    conf.set("fs.default.name", "file:///");            Deque<Closeable> closeables = Lists.newLinkedList();    try {        Random rnd = RandomUtils.getRandom();        File tmpDir = getTestTempDir("svdtmp");        conf.set("hadoop.tmp.dir", tmpDir.getAbsolutePath());        Path aLocPath = new Path(getTestTempDirPath("svdtmp/A"), "A.seq");                SequenceFile.Writer w = SequenceFile.createWriter(FileSystem.getLocal(conf), conf, aLocPath, Text.class, VectorWritable.class, CompressionType.BLOCK, new DefaultCodec());        closeables.addFirst(w);        int n = 100;        int m = 2000;        double percent = 5;        VectorWritable vw = new VectorWritable();        Text rkey = new Text();        Vector xi = new DenseVector(n);        double muAmplitude = 50.0;        for (int i = 0; i < m; i++) {            Vector dv = new SequentialAccessSparseVector(n);            String rowname = "row-" + i;            NamedVector namedRow = new NamedVector(dv, rowname);            for (int j = 0; j < n * percent / 100; j++) {                dv.setQuick(rnd.nextInt(n), muAmplitude * (rnd.nextDouble() - 0.25));            }            rkey.set("row-i" + i);            vw.set(namedRow);            w.append(rkey, vw);            xi.assign(dv, Functions.PLUS);        }        closeables.remove(w);        Closeables.close(w, false);        xi.assign(Functions.mult(1.0 / m));        FileSystem fs = FileSystem.get(conf);        Path tempDirPath = getTestTempDirPath("svd-proc");        Path aPath = new Path(tempDirPath, "A/A.seq");        fs.copyFromLocalFile(aLocPath, aPath);        Path xiPath = new Path(tempDirPath, "xi/xi.seq");        SSVDHelper.saveVector(xi, xiPath, conf);        Path svdOutPath = new Path(tempDirPath, "SSVD-out");                fs.delete(svdOutPath, true);                System.out.println("Input prepared, starting solver...");        int ablockRows = 867;        int p = 60;        int k = 40;        SSVDSolver ssvd = new SSVDSolver(conf, new Path[] { aPath }, svdOutPath, ablockRows, k, p, 3);        ssvd.setOuterBlockHeight(500);        ssvd.setAbtBlockHeight(251);        ssvd.setPcaMeanPath(xiPath);        /*     * Removing V,U jobs from this test to reduce running time. i will keep them     * put in the dense test though.     *     * For PCA test, we also want to request U*Sigma output and check it for named     * vector propagation.     */        ssvd.setComputeU(false);        ssvd.setComputeV(false);        ssvd.setcUSigma(true);        ssvd.setOverwrite(true);        ssvd.setQ(q);        ssvd.setBroadcast(true);        ssvd.run();        Vector stochasticSValues = ssvd.getSingularValues();                Matrix a = SSVDHelper.drmLoadAsDense(fs, aPath, conf);        verifyInternals(svdOutPath, a, new Omega(ssvd.getOmegaSeed(), k + p), k + p, q);                for (int i = 0; i < m; i++) {            a.viewRow(i).assign(xi, Functions.MINUS);        }        SingularValueDecomposition svd2 = new SingularValueDecomposition(a);        Vector svalues2 = new DenseVector(svd2.getSingularValues());        System.out.println("--SSVD solver singular values:");        LocalSSVDSolverSparseSequentialTest.dumpSv(stochasticSValues);        System.out.println("--SVD solver singular values:");        LocalSSVDSolverSparseSequentialTest.dumpSv(svalues2);        for (int i = 0; i < k + p; i++) {            assertTrue(Math.abs(svalues2.getQuick(i) - stochasticSValues.getQuick(i)) <= s_epsilon);        }        DenseMatrix mQ = SSVDHelper.drmLoadAsDense(fs, new Path(svdOutPath, "Bt-job/" + BtJob.OUTPUT_Q + "-*"), conf);        SSVDCommonTest.assertOrthonormality(mQ, false, s_epsilon);                for (Iterator<Pair<Writable, Vector>> iter = SSVDHelper.drmIterator(fs, new Path(ssvd.getuSigmaPath() + "/*"), conf, closeables); iter.hasNext(); ) {            Pair<Writable, Vector> pair = iter.next();            Writable key = pair.getFirst();            Vector v = pair.getSecond();            assertTrue(v instanceof NamedVector);            assertTrue(key instanceof Text);        }    } finally {        IOUtils.close(closeables);    }}
0
private void verifyInternals(Path tempDir, Matrix a, Omega omega, int kp, int q)
{    int m = a.numRows();    int n = a.numCols();    Vector xi = a.aggregateColumns(new VectorFunction() {        @Override        public double apply(Vector v) {            return v.zSum() / v.size();        }    });        Matrix momega = new DenseMatrix(n, kp);    for (int i = 0; i < n; i++) for (int j = 0; j < kp; j++) momega.setQuick(i, j, omega.getQuick(i, j));    Vector s_o = omega.mutlithreadedTRightMultiply(xi);    System.out.printf("s_omega=\n%s\n", s_o);    Matrix y = a.times(momega);    for (int i = 0; i < n; i++) y.viewRow(i).assign(s_o, Functions.MINUS);    QRDecomposition qr = new QRDecomposition(y);    Matrix qm = qr.getQ();    Vector s_q = qm.aggregateColumns(new VectorFunction() {        @Override        public double apply(Vector v) {            return v.zSum();        }    });    System.out.printf("s_q=\n%s\n", s_q);    Matrix b = qm.transpose().times(a);    Vector s_b = b.times(xi);    System.out.printf("s_b=\n%s\n", s_b);}
0
public double apply(Vector v)
{    return v.zSum() / v.size();}
0
public double apply(Vector v)
{    return v.zSum();}
0
public void testSSVDSolverDense() throws IOException
{    runSSVDSolver(0);}
0
public void testSSVDSolverPowerIterations1() throws IOException
{    runSSVDSolver(1);}
0
public void runSSVDSolver(int q) throws IOException
{    Configuration conf = getConfiguration();    conf.set("mapred.job.tracker", "local");    conf.set("fs.default.name", "file:///");            File tmpDir = getTestTempDir("svdtmp");    conf.set("hadoop.tmp.dir", tmpDir.getAbsolutePath());    Path aLocPath = new Path(getTestTempDirPath("svdtmp/A"), "A.seq");                                    int n = 100;    int m = 2000;    Vector singularValues = new DenseVector(new double[] { 10, 4, 1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1 });    SSVDTestsHelper.generateDenseInput(aLocPath, FileSystem.getLocal(conf), singularValues, m, n);    FileSystem fs = FileSystem.get(aLocPath.toUri(), conf);    Path tempDirPath = getTestTempDirPath("svd-proc");    Path aPath = new Path(tempDirPath, "A/A.seq");    fs.copyFromLocalFile(aLocPath, aPath);    Path svdOutPath = new Path(tempDirPath, "SSVD-out");        System.out.println("Input prepared, starting solver...");    int ablockRows = 867;    int p = 10;    int k = 3;    SSVDSolver ssvd = new SSVDSolver(conf, new Path[] { aPath }, svdOutPath, ablockRows, k, p, 3);    /*     * these are only tiny-test values to simulate high load cases, in reality     * one needs much bigger     */    ssvd.setOuterBlockHeight(500);    ssvd.setAbtBlockHeight(400);    ssvd.setOverwrite(true);    ssvd.setQ(q);    ssvd.setBroadcast(false);    ssvd.run();    Vector stochasticSValues = ssvd.getSingularValues();    System.out.println("--SSVD solver singular values:");    dumpSv(stochasticSValues);    for (int i = 0; i < k; i++) {        assertTrue(Math.abs((singularValues.getQuick(i) - stochasticSValues.getQuick(i)) / singularValues.getQuick(i)) <= s_precisionPct / 100);    }    DenseMatrix mQ = SSVDHelper.drmLoadAsDense(fs, new Path(svdOutPath, "Bt-job/" + BtJob.OUTPUT_Q + "-*"), conf);    SSVDCommonTest.assertOrthonormality(mQ, false, s_epsilon);    DenseMatrix u = SSVDHelper.drmLoadAsDense(fs, new Path(svdOutPath, "U/*"), conf);    SSVDCommonTest.assertOrthonormality(u, false, s_epsilon);    DenseMatrix v = SSVDHelper.drmLoadAsDense(fs, new Path(svdOutPath, "V/*"), conf);    SSVDCommonTest.assertOrthonormality(v, false, s_epsilon);}
0
 static void dumpSv(Vector s)
{    System.out.printf("svs: ");    for (Vector.Element el : s.all()) {        System.out.printf("%f  ", el.get());    }    System.out.println();}
0
public void testSSVDSolverPowerIterations1() throws IOException
{    runSSVDSolver(1);}
0
public void runSSVDSolver(int q) throws IOException
{    Configuration conf = getConfiguration();    conf.set("mapred.job.tracker", "local");    conf.set("fs.default.name", "file:///");            Deque<Closeable> closeables = Lists.newLinkedList();    ;    Random rnd = RandomUtils.getRandom();    File tmpDir = getTestTempDir("svdtmp");    conf.set("hadoop.tmp.dir", tmpDir.getAbsolutePath());    Path aLocPath = new Path(getTestTempDirPath("svdtmp/A"), "A.seq");        SequenceFile.Writer w = SequenceFile.createWriter(FileSystem.getLocal(conf), conf, aLocPath, IntWritable.class, VectorWritable.class, CompressionType.BLOCK, new DefaultCodec());    closeables.addFirst(w);    int n = 100;    int m = 2000;    double percent = 5;    VectorWritable vw = new VectorWritable();    IntWritable roww = new IntWritable();    double muAmplitude = 50.0;    for (int i = 0; i < m; i++) {        Vector dv = new SequentialAccessSparseVector(n);        for (int j = 0; j < n * percent / 100; j++) {            dv.setQuick(rnd.nextInt(n), muAmplitude * (rnd.nextDouble() - 0.5));        }        roww.set(i);        vw.set(dv);        w.append(roww, vw);    }    closeables.remove(w);    Closeables.close(w, false);    FileSystem fs = FileSystem.get(aLocPath.toUri(), conf);    Path tempDirPath = getTestTempDirPath("svd-proc");    Path aPath = new Path(tempDirPath, "A/A.seq");    fs.copyFromLocalFile(aLocPath, aPath);    Path svdOutPath = new Path(tempDirPath, "SSVD-out");        fs.delete(svdOutPath, true);        System.out.println("Input prepared, starting solver...");    int ablockRows = 867;    int p = 60;    int k = 40;    SSVDSolver ssvd = new SSVDSolver(conf, new Path[] { aPath }, svdOutPath, ablockRows, k, p, 3);    ssvd.setOuterBlockHeight(500);    ssvd.setAbtBlockHeight(251);    /*     * removing V,U jobs from this test to reduce running time. i will keep them     * put in the dense test though.     */    ssvd.setComputeU(false);    ssvd.setComputeV(false);    ssvd.setOverwrite(true);    ssvd.setQ(q);    ssvd.setBroadcast(true);    ssvd.run();    Vector stochasticSValues = ssvd.getSingularValues();    System.out.println("--SSVD solver singular values:");    dumpSv(stochasticSValues);    System.out.println("--Colt SVD solver singular values:");        DenseMatrix a = SSVDHelper.drmLoadAsDense(fs, aPath, conf);            SingularValueDecomposition svd2 = new SingularValueDecomposition(a);    Vector svalues2 = new DenseVector(svd2.getSingularValues());    dumpSv(svalues2);    for (int i = 0; i < k + p; i++) {        assertTrue(Math.abs(svalues2.getQuick(i) - stochasticSValues.getQuick(i)) <= s_epsilon);    }    DenseMatrix mQ = SSVDHelper.drmLoadAsDense(fs, new Path(svdOutPath, "Bt-job/" + BtJob.OUTPUT_Q + "-*"), conf);    SSVDCommonTest.assertOrthonormality(mQ, false, s_epsilon);    IOUtils.close(closeables);}
0
 static void dumpSv(Vector s)
{    System.out.printf("svs: ");    for (Vector.Element el : s.all()) {        System.out.printf("%f  ", el.get());    }    System.out.println();}
0
 static void dump(double[][] matrix)
{    for (double[] aMatrix : matrix) {        for (double anAMatrix : aMatrix) {            System.out.printf("%f  ", anAMatrix);        }        System.out.println();    }}
0
public void testGivensQR() throws Exception
{        Matrix m = new DenseMatrix(3, 3);    m.assign(new DoubleFunction() {        private final Random rnd = RandomUtils.getRandom();        @Override        public double apply(double arg0) {            return rnd.nextDouble() * SCALE;        }    });    m.setQuick(0, 0, 1);    m.setQuick(0, 1, 2);    m.setQuick(0, 2, 3);    m.setQuick(1, 0, 4);    m.setQuick(1, 1, 5);    m.setQuick(1, 2, 6);    m.setQuick(2, 0, 7);    m.setQuick(2, 1, 8);    m.setQuick(2, 2, 9);    GivensThinSolver qrSolver = new GivensThinSolver(m.rowSize(), m.columnSize());    qrSolver.solve(m);    Matrix qtm = new DenseMatrix(qrSolver.getThinQtTilde());    assertOrthonormality(qtm.transpose(), false, SVD_EPSILON);    Matrix aClone = new DenseMatrix(qrSolver.getThinQtTilde()).transpose().times(qrSolver.getRTilde());    System.out.println("aclone : " + aClone);}
0
public double apply(double arg0)
{    return rnd.nextDouble() * SCALE;}
0
public static void assertOrthonormality(Matrix mtx, boolean insufficientRank, double epsilon)
{    int n = mtx.columnSize();    int rank = 0;    for (int i = 0; i < n; i++) {        Vector ei = mtx.viewColumn(i);        double norm = ei.norm(2);        if (Math.abs(1 - norm) < epsilon) {            rank++;        } else {            assertTrue(Math.abs(norm) < epsilon);        }        for (int j = 0; j <= i; j++) {            Vector e_j = mtx.viewColumn(j);            double dot = ei.dot(e_j);            assertTrue(Math.abs((i == j && rank > j ? 1 : 0) - dot) < epsilon);        }    }    assertTrue((!insufficientRank && rank == n) || (insufficientRank && rank < n));}
0
 static void generateDenseInput(Path outputPath, FileSystem dfs, Vector svalues, int m, int n) throws IOException
{    generateDenseInput(outputPath, dfs, svalues, m, n, 0);}
0
 static void generateDenseInput(Path outputPath, FileSystem dfs, Vector svalues, int m, int n, int startRowKey) throws IOException
{    Random rnd = RandomUtils.getRandom();    int svCnt = svalues.size();    Matrix v = generateDenseOrthonormalRandom(n, svCnt, rnd);    Matrix u = generateDenseOrthonormalRandom(m, svCnt, rnd);        Matrix mx = m > n ? v : u;    for (int i = 0; i < svCnt; i++) {        mx.assignColumn(i, mx.viewColumn(i).times(svalues.getQuick(i)));    }    SequenceFile.Writer w = SequenceFile.createWriter(dfs, dfs.getConf(), outputPath, IntWritable.class, VectorWritable.class);    try {        Vector outV = new DenseVector(n);        Writable vw = new VectorWritable(outV);        IntWritable iw = new IntWritable();        for (int i = 0; i < m; i++) {            iw.set(startRowKey + i);            for (int j = 0; j < n; j++) {                outV.setQuick(j, u.viewRow(i).dot(v.viewRow(j)));            }            w.append(iw, vw);        }    } finally {        w.close();    }}
0
 static Matrix generateDenseOrthonormalRandom(int m, int n, Random rnd)
{    Matrix result = new DenseMatrix(m, n);    for (int j = 0; j < n; j++) {        for (int i = 0; i < m; i++) {            result.setQuick(i, j, rnd.nextDouble() - 0.5);        }    }    GramSchmidt.orthonormalizeColumns(result);    SSVDCommonTest.assertOrthonormality(result, false, 1.0e-10);    return result;}
0
public static void main(String[] args) throws Exception
{        MahoutTestCase ca = new MahoutTestCase();    Configuration conf = ca.getConfiguration();    FileSystem dfs = FileSystem.getLocal(conf);    Path outputDir = new Path("/tmp/DRM");    dfs.mkdirs(outputDir);                            /*     *  create 2Gb sparse 4.5 m x 4.5m input . (similar to wikipedia graph).     *       *  In order to get at 2Gb, we need to generate ~ 40 non-zero items per row average.     *        */    outputDir = new Path("/tmp/DRM-sparse");    Random rnd = RandomUtils.getRandom();    SequenceFile.Writer w = SequenceFile.createWriter(dfs, dfs.getConf(), new Path(outputDir, "sparse.seq"), IntWritable.class, VectorWritable.class);    try {        IntWritable iw = new IntWritable();        VectorWritable vw = new VectorWritable();        int avgNZero = 40;        int n = 4500000;        for (int i = 1; i < n; i++) {            Vector vector = new RandomAccessSparseVector(n);            double nz = Math.round(avgNZero * (rnd.nextGaussian() + 1));            if (nz < 0) {                nz = 0;            }            for (int j = 1; j < nz; j++) {                vector.set(rnd.nextInt(n), rnd.nextGaussian() * 25 + 3);            }            iw.set(i);            vw.set(vector);            w.append(iw, vw);        }    } finally {        w.close();    }}
0
private static void assertEquals(VectorIterable m, VectorIterable mtt, double errorTolerance)
{    Iterator<MatrixSlice> mIt = m.iterateAll();    Iterator<MatrixSlice> mttIt = mtt.iterateAll();    Map<Integer, Vector> mMap = Maps.newHashMap();    Map<Integer, Vector> mttMap = Maps.newHashMap();    while (mIt.hasNext() && mttIt.hasNext()) {        MatrixSlice ms = mIt.next();        mMap.put(ms.index(), ms.vector());        MatrixSlice mtts = mttIt.next();        mttMap.put(mtts.index(), mtts.vector());    }    for (Map.Entry<Integer, Vector> entry : mMap.entrySet()) {        Integer key = entry.getKey();        Vector value = entry.getValue();        if (value == null || mttMap.get(key) == null) {            assertTrue(value == null || value.norm(2) == 0);            assertTrue(mttMap.get(key) == null || mttMap.get(key).norm(2) == 0);        } else {            assertTrue(value.getDistanceSquared(mttMap.get(key)) < errorTolerance);        }    }}
0
public void testTranspose() throws Exception
{    DistributedRowMatrix m = randomDistributedMatrix(10, 9, 5, 4, 1.0, false);    m.setConf(getConfiguration());    DistributedRowMatrix mt = m.transpose();    mt.setConf(getConfiguration());    Path tmpPath = getTestTempDirPath();    m.setOutputTempPathString(tmpPath.toString());    Path tmpOutPath = new Path(tmpPath, "/tmpOutTranspose");    mt.setOutputTempPathString(tmpOutPath.toString());    HadoopUtil.delete(getConfiguration(), tmpOutPath);    DistributedRowMatrix mtt = mt.transpose();    assertEquals(m, mtt, EPSILON);}
0
public void testMatrixColumnMeansJob() throws Exception
{    Matrix m = SolverTest.randomSequentialAccessSparseMatrix(100, 90, 50, 20, 1.0);    DistributedRowMatrix dm = randomDistributedMatrix(100, 90, 50, 20, 1.0, false);    dm.setConf(getConfiguration());    Vector expected = new DenseVector(50);    for (int i = 0; i < m.numRows(); i++) {        expected.assign(m.viewRow(i), Functions.PLUS);    }    expected.assign(Functions.DIV, m.numRows());    Vector actual = dm.columnMeans("DenseVector");    assertEquals(0.0, expected.getDistanceSquared(actual), EPSILON);}
0
public void testNullMatrixColumnMeansJob() throws Exception
{    Matrix m = SolverTest.randomSequentialAccessSparseMatrix(100, 90, 0, 0, 1.0);    DistributedRowMatrix dm = randomDistributedMatrix(100, 90, 0, 0, 1.0, false);    dm.setConf(getConfiguration());    Vector expected = new DenseVector(0);    for (int i = 0; i < m.numRows(); i++) {        expected.assign(m.viewRow(i), Functions.PLUS);    }    expected.assign(Functions.DIV, m.numRows());    Vector actual = dm.columnMeans();    assertEquals(0.0, expected.getDistanceSquared(actual), EPSILON);}
0
public void testMatrixTimesVector() throws Exception
{    Vector v = new RandomAccessSparseVector(50);    v.assign(1.0);    Matrix m = SolverTest.randomSequentialAccessSparseMatrix(100, 90, 50, 20, 1.0);    DistributedRowMatrix dm = randomDistributedMatrix(100, 90, 50, 20, 1.0, false);    dm.setConf(getConfiguration());    Vector expected = m.times(v);    Vector actual = dm.times(v);    assertEquals(0.0, expected.getDistanceSquared(actual), EPSILON);}
0
public void testMatrixTimesSquaredVector() throws Exception
{    Vector v = new RandomAccessSparseVector(50);    v.assign(1.0);    Matrix m = SolverTest.randomSequentialAccessSparseMatrix(100, 90, 50, 20, 1.0);    DistributedRowMatrix dm = randomDistributedMatrix(100, 90, 50, 20, 1.0, false);    dm.setConf(getConfiguration());    Vector expected = m.timesSquared(v);    Vector actual = dm.timesSquared(v);    assertEquals(0.0, expected.getDistanceSquared(actual), 1.0e-9);}
0
public void testMatrixTimesMatrix() throws Exception
{    Matrix inputA = SolverTest.randomSequentialAccessSparseMatrix(20, 19, 15, 5, 10.0);    Matrix inputB = SolverTest.randomSequentialAccessSparseMatrix(20, 13, 25, 10, 5.0);    Matrix expected = inputA.transpose().times(inputB);    DistributedRowMatrix distA = randomDistributedMatrix(20, 19, 15, 5, 10.0, false, "distA");    distA.setConf(getConfiguration());    DistributedRowMatrix distB = randomDistributedMatrix(20, 13, 25, 10, 5.0, false, "distB");    distB.setConf(getConfiguration());    DistributedRowMatrix product = distA.times(distB);    assertEquals(expected, product, EPSILON);}
0
public void testMatrixMultiplactionJobConfBuilder() throws Exception
{    Configuration initialConf = createInitialConf();    Path baseTmpDirPath = getTestTempDirPath("testpaths");    Path aPath = new Path(baseTmpDirPath, "a");    Path bPath = new Path(baseTmpDirPath, "b");    Path outPath = new Path(baseTmpDirPath, "out");    Configuration mmJobConf = MatrixMultiplicationJob.createMatrixMultiplyJobConf(aPath, bPath, outPath, 10);    Configuration mmCustomJobConf = MatrixMultiplicationJob.createMatrixMultiplyJobConf(initialConf, aPath, bPath, outPath, 10);    assertNull(mmJobConf.get(TEST_PROPERTY_KEY));    assertEquals(TEST_PROPERTY_VALUE, mmCustomJobConf.get(TEST_PROPERTY_KEY));}
0
public void testTransposeJobConfBuilder() throws Exception
{    Configuration initialConf = createInitialConf();    Path baseTmpDirPath = getTestTempDirPath("testpaths");    Path inputPath = new Path(baseTmpDirPath, "input");    Path outputPath = new Path(baseTmpDirPath, "output");    Configuration transposeJobConf = TransposeJob.buildTransposeJob(inputPath, outputPath, 10).getConfiguration();    Configuration transposeCustomJobConf = TransposeJob.buildTransposeJob(initialConf, inputPath, outputPath, 10).getConfiguration();    assertNull(transposeJobConf.get(TEST_PROPERTY_KEY));    assertEquals(TEST_PROPERTY_VALUE, transposeCustomJobConf.get(TEST_PROPERTY_KEY));}
0
public void testTimesSquaredJobConfBuilders() throws Exception
{    Configuration initialConf = createInitialConf();    Path baseTmpDirPath = getTestTempDirPath("testpaths");    Path inputPath = new Path(baseTmpDirPath, "input");    Path outputPath = new Path(baseTmpDirPath, "output");    Vector v = new RandomAccessSparseVector(50);    v.assign(1.0);    Job timesSquaredJob1 = TimesSquaredJob.createTimesSquaredJob(v, inputPath, outputPath);    Job customTimesSquaredJob1 = TimesSquaredJob.createTimesSquaredJob(initialConf, v, inputPath, outputPath);    assertNull(timesSquaredJob1.getConfiguration().get(TEST_PROPERTY_KEY));    assertEquals(TEST_PROPERTY_VALUE, customTimesSquaredJob1.getConfiguration().get(TEST_PROPERTY_KEY));    Job timesJob = TimesSquaredJob.createTimesJob(v, 50, inputPath, outputPath);    Job customTimesJob = TimesSquaredJob.createTimesJob(initialConf, v, 50, inputPath, outputPath);    assertNull(timesJob.getConfiguration().get(TEST_PROPERTY_KEY));    assertEquals(TEST_PROPERTY_VALUE, customTimesJob.getConfiguration().get(TEST_PROPERTY_KEY));    Job timesSquaredJob2 = TimesSquaredJob.createTimesSquaredJob(v, inputPath, outputPath, TimesSquaredJob.TimesSquaredMapper.class, TimesSquaredJob.VectorSummingReducer.class);    Job customTimesSquaredJob2 = TimesSquaredJob.createTimesSquaredJob(initialConf, v, inputPath, outputPath, TimesSquaredJob.TimesSquaredMapper.class, TimesSquaredJob.VectorSummingReducer.class);    assertNull(timesSquaredJob2.getConfiguration().get(TEST_PROPERTY_KEY));    assertEquals(TEST_PROPERTY_VALUE, customTimesSquaredJob2.getConfiguration().get(TEST_PROPERTY_KEY));    Job timesSquaredJob3 = TimesSquaredJob.createTimesSquaredJob(v, 50, inputPath, outputPath, TimesSquaredJob.TimesSquaredMapper.class, TimesSquaredJob.VectorSummingReducer.class);    Job customTimesSquaredJob3 = TimesSquaredJob.createTimesSquaredJob(initialConf, v, 50, inputPath, outputPath, TimesSquaredJob.TimesSquaredMapper.class, TimesSquaredJob.VectorSummingReducer.class);    assertNull(timesSquaredJob3.getConfiguration().get(TEST_PROPERTY_KEY));    assertEquals(TEST_PROPERTY_VALUE, customTimesSquaredJob3.getConfiguration().get(TEST_PROPERTY_KEY));}
0
public void testTimesVectorTempDirDeletion() throws Exception
{    Configuration conf = getConfiguration();    Vector v = new RandomAccessSparseVector(50);    v.assign(1.0);    DistributedRowMatrix dm = randomDistributedMatrix(100, 90, 50, 20, 1.0, false);    dm.setConf(conf);    Path outputPath = dm.getOutputTempPath();    FileSystem fs = outputPath.getFileSystem(conf);    deleteContentsOfPath(conf, outputPath);    assertEquals(0, HadoopUtil.listStatus(fs, outputPath).length);    Vector result1 = dm.times(v);    assertEquals(0, HadoopUtil.listStatus(fs, outputPath).length);    deleteContentsOfPath(conf, outputPath);    assertEquals(0, HadoopUtil.listStatus(fs, outputPath).length);    conf.setBoolean(DistributedRowMatrix.KEEP_TEMP_FILES, true);    dm.setConf(conf);    Vector result2 = dm.times(v);    FileStatus[] outputStatuses = fs.listStatus(outputPath);    assertEquals(1, outputStatuses.length);    Path outputTempPath = outputStatuses[0].getPath();    Path inputVectorPath = new Path(outputTempPath, TimesSquaredJob.INPUT_VECTOR);    Path outputVectorPath = new Path(outputTempPath, TimesSquaredJob.OUTPUT_VECTOR_FILENAME);    assertEquals(1, fs.listStatus(inputVectorPath, PathFilters.logsCRCFilter()).length);    assertEquals(1, fs.listStatus(outputVectorPath, PathFilters.logsCRCFilter()).length);    assertEquals(0.0, result1.getDistanceSquared(result2), EPSILON);}
0
public void testTimesSquaredVectorTempDirDeletion() throws Exception
{    Configuration conf = getConfiguration();    Vector v = new RandomAccessSparseVector(50);    v.assign(1.0);    DistributedRowMatrix dm = randomDistributedMatrix(100, 90, 50, 20, 1.0, false);    dm.setConf(getConfiguration());    Path outputPath = dm.getOutputTempPath();    FileSystem fs = outputPath.getFileSystem(conf);    deleteContentsOfPath(conf, outputPath);    assertEquals(0, HadoopUtil.listStatus(fs, outputPath).length);    Vector result1 = dm.timesSquared(v);    assertEquals(0, HadoopUtil.listStatus(fs, outputPath).length);    deleteContentsOfPath(conf, outputPath);    assertEquals(0, HadoopUtil.listStatus(fs, outputPath).length);    conf.setBoolean(DistributedRowMatrix.KEEP_TEMP_FILES, true);    dm.setConf(conf);    Vector result2 = dm.timesSquared(v);    FileStatus[] outputStatuses = fs.listStatus(outputPath);    assertEquals(1, outputStatuses.length);    Path outputTempPath = outputStatuses[0].getPath();    Path inputVectorPath = new Path(outputTempPath, TimesSquaredJob.INPUT_VECTOR);    Path outputVectorPath = new Path(outputTempPath, TimesSquaredJob.OUTPUT_VECTOR_FILENAME);    assertEquals(1, fs.listStatus(inputVectorPath, PathFilters.logsCRCFilter()).length);    assertEquals(1, fs.listStatus(outputVectorPath, PathFilters.logsCRCFilter()).length);    assertEquals(0.0, result1.getDistanceSquared(result2), EPSILON);}
0
public Configuration createInitialConf() throws IOException
{    Configuration initialConf = getConfiguration();    initialConf.set(TEST_PROPERTY_KEY, TEST_PROPERTY_VALUE);    return initialConf;}
0
private static void deleteContentsOfPath(Configuration conf, Path path) throws Exception
{    FileSystem fs = path.getFileSystem(conf);    FileStatus[] statuses = HadoopUtil.listStatus(fs, path);    for (FileStatus status : statuses) {        fs.delete(status.getPath(), true);    }}
0
public DistributedRowMatrix randomDistributedMatrix(int numRows, int nonNullRows, int numCols, int entriesPerRow, double entryMean, boolean isSymmetric) throws IOException
{    return randomDistributedMatrix(numRows, nonNullRows, numCols, entriesPerRow, entryMean, isSymmetric, "testdata");}
0
public DistributedRowMatrix randomDenseHierarchicalDistributedMatrix(int numRows, int numCols, boolean isSymmetric, String baseTmpDirSuffix) throws IOException
{    Path baseTmpDirPath = getTestTempDirPath(baseTmpDirSuffix);    Matrix c = SolverTest.randomHierarchicalMatrix(numRows, numCols, isSymmetric);    return saveToFs(c, baseTmpDirPath);}
0
public DistributedRowMatrix randomDistributedMatrix(int numRows, int nonNullRows, int numCols, int entriesPerRow, double entryMean, boolean isSymmetric, String baseTmpDirSuffix) throws IOException
{    Path baseTmpDirPath = getTestTempDirPath(baseTmpDirSuffix);    Matrix c = SolverTest.randomSequentialAccessSparseMatrix(numRows, nonNullRows, numCols, entriesPerRow, entryMean);    if (isSymmetric) {        c = c.times(c.transpose());    }    return saveToFs(c, baseTmpDirPath);}
0
private DistributedRowMatrix saveToFs(final Matrix m, Path baseTmpDirPath) throws IOException
{    Configuration conf = getConfiguration();    FileSystem fs = FileSystem.get(baseTmpDirPath.toUri(), conf);    ClusteringTestUtils.writePointsToFile(new Iterable<VectorWritable>() {        @Override        public Iterator<VectorWritable> iterator() {            return Iterators.transform(m.iterator(), new Function<MatrixSlice, VectorWritable>() {                @Override                public VectorWritable apply(MatrixSlice input) {                    return new VectorWritable(input.vector());                }            });        }    }, true, new Path(baseTmpDirPath, "distMatrix/part-00000"), fs, conf);    DistributedRowMatrix distMatrix = new DistributedRowMatrix(new Path(baseTmpDirPath, "distMatrix"), new Path(baseTmpDirPath, "tmpOut"), m.numRows(), m.numCols());    distMatrix.setConf(new Configuration(conf));    return distMatrix;}
0
public Iterator<VectorWritable> iterator()
{    return Iterators.transform(m.iterator(), new Function<MatrixSlice, VectorWritable>() {        @Override        public VectorWritable apply(MatrixSlice input) {            return new VectorWritable(input.vector());        }    });}
0
public VectorWritable apply(MatrixSlice input)
{    return new VectorWritable(input.vector());}
0
public void testSparseMatrixWritable() throws Exception
{    Matrix m = new SparseMatrix(5, 5);    m.set(1, 2, 3.0);    m.set(3, 4, 5.0);    Map<String, Integer> bindings = Maps.newHashMap();    bindings.put("A", 0);    bindings.put("B", 1);    bindings.put("C", 2);    bindings.put("D", 3);    bindings.put("default", 4);    m.setRowLabelBindings(bindings);    m.setColumnLabelBindings(bindings);    doTestMatrixWritableEquals(m);}
0
public void testSparseRowMatrixWritable() throws Exception
{    Matrix m = new SparseRowMatrix(5, 5);    m.set(1, 2, 3.0);    m.set(3, 4, 5.0);    Map<String, Integer> bindings = Maps.newHashMap();    bindings.put("A", 0);    bindings.put("B", 1);    bindings.put("C", 2);    bindings.put("D", 3);    bindings.put("default", 4);    m.setRowLabelBindings(bindings);    m.setColumnLabelBindings(bindings);    doTestMatrixWritableEquals(m);}
0
public void testDenseMatrixWritable() throws Exception
{    Matrix m = new DenseMatrix(5, 5);    m.set(1, 2, 3.0);    m.set(3, 4, 5.0);    Map<String, Integer> bindings = Maps.newHashMap();    bindings.put("A", 0);    bindings.put("B", 1);    bindings.put("C", 2);    bindings.put("D", 3);    bindings.put("default", 4);    m.setRowLabelBindings(bindings);    m.setColumnLabelBindings(bindings);    doTestMatrixWritableEquals(m);}
0
private static void doTestMatrixWritableEquals(Matrix m) throws IOException
{    Writable matrixWritable = new MatrixWritable(m);    MatrixWritable matrixWritable2 = new MatrixWritable();    writeAndRead(matrixWritable, matrixWritable2);    Matrix m2 = matrixWritable2.get();    compareMatrices(m, m2);    doCheckBindings(m2.getRowLabelBindings());    doCheckBindings(m2.getColumnLabelBindings());}
0
private static void compareMatrices(Matrix m, Matrix m2)
{    assertEquals(m.numRows(), m2.numRows());    assertEquals(m.numCols(), m2.numCols());    for (int r = 0; r < m.numRows(); r++) {        for (int c = 0; c < m.numCols(); c++) {            assertEquals(m.get(r, c), m2.get(r, c), EPSILON);        }    }    Map<String, Integer> bindings = m.getRowLabelBindings();    Map<String, Integer> bindings2 = m2.getRowLabelBindings();    assertEquals(bindings == null, bindings2 == null);    if (bindings != null) {        assertEquals(bindings.size(), m.numRows());        assertEquals(bindings.size(), bindings2.size());        for (Map.Entry<String, Integer> entry : bindings.entrySet()) {            assertEquals(entry.getValue(), bindings2.get(entry.getKey()));        }    }    bindings = m.getColumnLabelBindings();    bindings2 = m2.getColumnLabelBindings();    assertEquals(bindings == null, bindings2 == null);    if (bindings != null) {        assertEquals(bindings.size(), bindings2.size());        for (Map.Entry<String, Integer> entry : bindings.entrySet()) {            assertEquals(entry.getValue(), bindings2.get(entry.getKey()));        }    }}
0
private static void doCheckBindings(Map<String, Integer> labels)
{    assertTrue("Missing label", labels.keySet().contains("A"));    assertTrue("Missing label", labels.keySet().contains("B"));    assertTrue("Missing label", labels.keySet().contains("C"));    assertTrue("Missing label", labels.keySet().contains("D"));    assertTrue("Missing label", labels.keySet().contains("default"));}
0
private static void writeAndRead(Writable toWrite, Writable toRead) throws IOException
{    ByteArrayOutputStream baos = new ByteArrayOutputStream();    DataOutputStream dos = new DataOutputStream(baos);    try {        toWrite.write(dos);    } finally {        Closeables.close(dos, false);    }    ByteArrayInputStream bais = new ByteArrayInputStream(baos.toByteArray());    DataInputStream dis = new DataInputStream(bais);    try {        toRead.readFields(dis);    } finally {        Closeables.close(dis, true);    }}
0
public void testNormal()
{    Matrix testData = new DenseMatrix(100000, 10);    final Normal gen = new Normal();    testData.assign(gen);    final EuclideanDistanceMeasure distance = new EuclideanDistanceMeasure();    BruteSearch ref = new BruteSearch(distance);    ref.addAllMatrixSlicesAsWeightedVectors(testData);    LocalitySensitiveHashSearch cut = new LocalitySensitiveHashSearch(distance, 10);    cut.addAllMatrixSlicesAsWeightedVectors(testData);    cut.setSearchSize(200);    cut.resetEvaluationCount();    System.out.printf("speedup,q1,q2,q3\n");    for (int i = 0; i < 12; i++) {        double strategy = (i - 1.0) / 10.0;        cut.setRaiseHashLimitStrategy(strategy);        OnlineSummarizer t1 = evaluateStrategy(testData, ref, cut);        int evals = cut.resetEvaluationCount();        final double speedup = 10.0e6 / evals;        System.out.printf("%.1f,%.2f,%.2f,%.2f\n", speedup, t1.getQuartile(1), t1.getQuartile(2), t1.getQuartile(3));        assertTrue(t1.getQuartile(2) > 0.45);        assertTrue(speedup > 4 || t1.getQuartile(2) > 0.9);        assertTrue(speedup > 15 || t1.getQuartile(2) > 0.8);    }}
0
private static OnlineSummarizer evaluateStrategy(Matrix testData, BruteSearch ref, LocalitySensitiveHashSearch cut)
{    OnlineSummarizer t1 = new OnlineSummarizer();    for (int i = 0; i < 100; i++) {        final Vector q = testData.viewRow(i);        List<WeightedThing<Vector>> v1 = cut.search(q, 150);        BitSet b1 = new BitSet();        for (WeightedThing<Vector> v : v1) {            b1.set(((WeightedVector) v.getValue()).getIndex());        }        List<WeightedThing<Vector>> v2 = ref.search(q, 100);        BitSet b2 = new BitSet();        for (WeightedThing<Vector> v : v2) {            b2.set(((WeightedVector) v.getValue()).getIndex());        }        b1.and(b2);        t1.add(b1.cardinality());    }    return t1;}
0
public void testDotCorrelation()
{    final Normal gen = new Normal();    Matrix projection = new DenseMatrix(64, 10);    projection.assign(gen);    Vector query = new DenseVector(10);    query.assign(gen);    long qhash = HashedVector.computeHash64(query, projection);    int[] count = new int[65];    Vector v = new DenseVector(10);    for (int i = 0; i < 500000; i++) {        v.assign(gen);        long hash = HashedVector.computeHash64(v, projection);        final int bitDot = Long.bitCount(qhash ^ hash);        count[bitDot]++;        if (count[bitDot] < 200) {            System.out.printf("%d, %.3f\n", bitDot, v.dot(query) / Math.sqrt(v.getLengthSquared() * query.getLengthSquared()));        }    }    for (int i = 0; i < 65; ++i) {        System.out.printf("%d, %d\n", i, count[i]);    }}
0
public static Matrix lumpyRandomData(int numDataPoints, int numDimensions)
{    final Matrix data = new DenseMatrix(numDataPoints, numDimensions);    final LumpyData clusters = new LumpyData(numDimensions, 0.05, 10);    for (MatrixSlice row : data) {        row.vector().assign(clusters.sample());    }    return data;}
0
public Vector sample()
{    int id = cluster.sample();    if (id >= centroids.size()) {                centroids.add(new MultiNormal(radius, centers.sample()));    }    return centroids.get(id).sample();}
0
public static List<Object[]> generateData()
{    RandomUtils.useTestSeed();    Matrix dataPoints = LumpyData.lumpyRandomData(NUM_DATA_POINTS, NUM_DIMENSIONS);    Matrix queries = LumpyData.lumpyRandomData(NUM_QUERIES, NUM_DIMENSIONS);    DistanceMeasure distanceMeasure = new CosineDistanceMeasure();    Searcher bruteSearcher = new BruteSearch(distanceMeasure);    bruteSearcher.addAll(dataPoints);    Pair<List<List<WeightedThing<Vector>>>, Long> reference = getResultsAndRuntime(bruteSearcher, queries);    Pair<List<WeightedThing<Vector>>, Long> referenceSearchFirst = getResultsAndRuntimeSearchFirst(bruteSearcher, queries);    double bruteSearchAvgTime = reference.getSecond() / (queries.numRows() * 1.0);    System.out.printf("BruteSearch: avg_time(1 query) %f[s]\n", bruteSearchAvgTime);    return Arrays.asList(new Object[][] {     { new ProjectionSearch(distanceMeasure, 3, 10), dataPoints, queries, reference, referenceSearchFirst }, { new FastProjectionSearch(distanceMeasure, 3, 10), dataPoints, queries, reference, referenceSearchFirst },     { new ProjectionSearch(distanceMeasure, 5, 5), dataPoints, queries, reference, referenceSearchFirst }, { new FastProjectionSearch(distanceMeasure, 5, 5), dataPoints, queries, reference, referenceSearchFirst } });}
0
public void testOverlapAndRuntimeSearchFirst()
{    searcher.clear();    searcher.addAll(dataPoints);    Pair<List<WeightedThing<Vector>>, Long> results = getResultsAndRuntimeSearchFirst(searcher, queries);    int numFirstMatches = 0;    for (int i = 0; i < queries.numRows(); ++i) {        WeightedThing<Vector> referenceVector = referenceSearchFirst.getFirst().get(i);        WeightedThing<Vector> resultVector = results.getFirst().get(i);        if (referenceVector.getValue().equals(resultVector.getValue())) {            ++numFirstMatches;        }    }    double bruteSearchAvgTime = reference.getSecond() / (queries.numRows() * 1.0);    double searcherAvgTime = results.getSecond() / (queries.numRows() * 1.0);    System.out.printf("%s: first matches %d [%d]; avg_time(1 query) %f(s) [%f]\n", searcher.getClass().getName(), numFirstMatches, queries.numRows(), searcherAvgTime, bruteSearchAvgTime);    assertEquals("Closest vector returned doesn't match", queries.numRows(), numFirstMatches);    assertTrue("Searcher " + searcher.getClass().getName() + " slower than brute", bruteSearchAvgTime > searcherAvgTime);}
0
public void testOverlapAndRuntime()
{    searcher.clear();    searcher.addAll(dataPoints);    Pair<List<List<WeightedThing<Vector>>>, Long> results = getResultsAndRuntime(searcher, queries);    int numFirstMatches = 0;    int numMatches = 0;    StripWeight stripWeight = new StripWeight();    for (int i = 0; i < queries.numRows(); ++i) {        List<WeightedThing<Vector>> referenceVectors = reference.getFirst().get(i);        List<WeightedThing<Vector>> resultVectors = results.getFirst().get(i);        if (referenceVectors.get(0).getValue().equals(resultVectors.get(0).getValue())) {            ++numFirstMatches;        }        for (Vector v : Iterables.transform(referenceVectors, stripWeight)) {            for (Vector w : Iterables.transform(resultVectors, stripWeight)) {                if (v.equals(w)) {                    ++numMatches;                }            }        }    }    double bruteSearchAvgTime = reference.getSecond() / (queries.numRows() * 1.0);    double searcherAvgTime = results.getSecond() / (queries.numRows() * 1.0);    System.out.printf("%s: first matches %d [%d]; total matches %d [%d]; avg_time(1 query) %f(s) [%f]\n", searcher.getClass().getName(), numFirstMatches, queries.numRows(), numMatches, queries.numRows() * NUM_RESULTS, searcherAvgTime, bruteSearchAvgTime);    assertEquals("Closest vector returned doesn't match", queries.numRows(), numFirstMatches);    assertTrue("Searcher " + searcher.getClass().getName() + " slower than brute", bruteSearchAvgTime > searcherAvgTime);}
0
public static Pair<List<List<WeightedThing<Vector>>>, Long> getResultsAndRuntime(Searcher searcher, Iterable<? extends Vector> queries)
{    long start = System.currentTimeMillis();    List<List<WeightedThing<Vector>>> results = searcher.search(queries, NUM_RESULTS);    long end = System.currentTimeMillis();    return new Pair<>(results, end - start);}
0
public static Pair<List<WeightedThing<Vector>>, Long> getResultsAndRuntimeSearchFirst(Searcher searcher, Iterable<? extends Vector> queries)
{    long start = System.currentTimeMillis();    List<WeightedThing<Vector>> results = searcher.searchFirst(queries, false);    long end = System.currentTimeMillis();    return new Pair<>(results, end - start);}
0
public Vector apply(WeightedThing<Vector> input)
{    Preconditions.checkArgument(input != null, "input is null");        return input.getValue();}
0
protected static Matrix multiNormalRandomData(int numDataPoints, int numDimensions)
{    Matrix data = new DenseMatrix(numDataPoints, numDimensions);    MultiNormal gen = new MultiNormal(20);    for (MatrixSlice slice : data) {        slice.vector().assign(gen.sample());    }    return data;}
0
public static List<Object[]> generateData()
{    RandomUtils.useTestSeed();    Matrix dataPoints = multiNormalRandomData(NUM_DATA_POINTS, NUM_DIMENSIONS);    return Arrays.asList(new Object[][] { { new ProjectionSearch(new EuclideanDistanceMeasure(), NUM_PROJECTIONS, SEARCH_SIZE), dataPoints }, { new FastProjectionSearch(new EuclideanDistanceMeasure(), NUM_PROJECTIONS, SEARCH_SIZE), dataPoints }, { new LocalitySensitiveHashSearch(new EuclideanDistanceMeasure(), SEARCH_SIZE), dataPoints } });}
0
public void testExactMatch()
{    searcher.clear();    Iterable<MatrixSlice> data = dataPoints;    final Iterable<MatrixSlice> batch1 = Iterables.limit(data, 300);    List<MatrixSlice> queries = Lists.newArrayList(Iterables.limit(batch1, 100));        searcher.addAllMatrixSlices(batch1);    assertEquals(300, searcher.size());    Vector q = Iterables.get(data, 0).vector();    List<WeightedThing<Vector>> r = searcher.search(q, 2);    assertEquals(0, r.get(0).getValue().minus(q).norm(1), 1.0e-8);    final Iterable<MatrixSlice> batch2 = Iterables.limit(Iterables.skip(data, 300), 10);    searcher.addAllMatrixSlices(batch2);    assertEquals(310, searcher.size());    q = Iterables.get(data, 302).vector();    r = searcher.search(q, 2);    assertEquals(0, r.get(0).getValue().minus(q).norm(1), 1.0e-8);    searcher.addAllMatrixSlices(Iterables.skip(data, 310));    assertEquals(dataPoints.numRows(), searcher.size());    for (MatrixSlice query : queries) {        r = searcher.search(query.vector(), 2);        assertEquals("Distance has to be about zero", 0, r.get(0).getWeight(), 1.0e-6);        assertEquals("Answer must be substantially the same as query", 0, r.get(0).getValue().minus(query.vector()).norm(1), 1.0e-8);        assertTrue("Wrong answer must have non-zero distance", r.get(1).getWeight() > r.get(0).getWeight());    }}
0
public void testNearMatch()
{    searcher.clear();    List<MatrixSlice> queries = Lists.newArrayList(Iterables.limit(dataPoints, 100));    searcher.addAllMatrixSlicesAsWeightedVectors(dataPoints);    MultiNormal noise = new MultiNormal(0.01, new DenseVector(20));    for (MatrixSlice slice : queries) {        Vector query = slice.vector();        final Vector epsilon = noise.sample();        List<WeightedThing<Vector>> r = searcher.search(query, 2);        query = query.plus(epsilon);        assertEquals("Distance has to be small", epsilon.norm(2), r.get(0).getWeight(), 1.0e-1);        assertEquals("Answer must be substantially the same as query", epsilon.norm(2), r.get(0).getValue().minus(query).norm(2), 1.0e-1);        assertTrue("Wrong answer must be further away", r.get(1).getWeight() > r.get(0).getWeight());    }}
0
public void testOrdering()
{    searcher.clear();    Matrix queries = new DenseMatrix(100, 20);    MultiNormal gen = new MultiNormal(20);    for (int i = 0; i < 100; i++) {        queries.viewRow(i).assign(gen.sample());    }    searcher.addAllMatrixSlices(dataPoints);    for (MatrixSlice query : queries) {        List<WeightedThing<Vector>> r = searcher.search(query.vector(), 200);        double x = 0;        for (WeightedThing<Vector> thing : r) {            assertTrue("Scores must be monotonic increasing", thing.getWeight() >= x);            x = thing.getWeight();        }    }}
0
public void testRemoval()
{    searcher.clear();    searcher.addAllMatrixSlices(dataPoints);        if (searcher instanceof UpdatableSearcher) {        List<Vector> x = Lists.newArrayList(Iterables.limit(searcher, 2));        int size0 = searcher.size();        List<WeightedThing<Vector>> r0 = searcher.search(x.get(0), 2);        searcher.remove(x.get(0), 1.0e-7);        assertEquals(size0 - 1, searcher.size());        List<WeightedThing<Vector>> r = searcher.search(x.get(0), 1);        assertTrue("Vector should be gone", r.get(0).getWeight() > 0);        assertEquals("Previous second neighbor should be first", 0, r.get(0).getValue().minus(r0.get(1).getValue()).norm(1), 1.0e-8);        searcher.remove(x.get(1), 1.0e-7);        assertEquals(size0 - 2, searcher.size());        r = searcher.search(x.get(1), 1);        assertTrue("Vector should be gone", r.get(0).getWeight() > 0);                for (Vector v : searcher) {            assertTrue(x.get(0).minus(v).norm(1) > 1.0e-6);            assertTrue(x.get(1).minus(v).norm(1) > 1.0e-6);        }    } else {        try {            List<Vector> x = Lists.newArrayList(Iterables.limit(searcher, 2));            searcher.remove(x.get(0), 1.0e-7);            fail("Shouldn't be able to delete from " + searcher.getClass().getName());        } catch (UnsupportedOperationException e) {                }    }}
0
public void testSearchFirst()
{    searcher.clear();    searcher.addAll(dataPoints);    for (Vector datapoint : dataPoints) {        WeightedThing<Vector> first = searcher.searchFirst(datapoint, false);        WeightedThing<Vector> second = searcher.searchFirst(datapoint, true);        List<WeightedThing<Vector>> firstTwo = searcher.search(datapoint, 2);        assertEquals("First isn't self", 0, first.getWeight(), 0);        assertEquals("First isn't self", datapoint, first.getValue());        assertEquals("First doesn't match", first, firstTwo.get(0));        assertEquals("Second doesn't match", second, firstTwo.get(1));    }}
0
public void testSearchLimiting()
{    searcher.clear();    searcher.addAll(dataPoints);    for (Vector datapoint : dataPoints) {        List<WeightedThing<Vector>> firstTwo = searcher.search(datapoint, 2);        assertThat("Search limit isn't respected", firstTwo.size(), is(lessThanOrEqualTo(2)));    }}
0
public void testRemove()
{    searcher.clear();    for (int i = 0; i < dataPoints.rowSize(); ++i) {        Vector datapoint = dataPoints.viewRow(i);        searcher.add(datapoint);                if (i % 2 == 0) {            assertTrue("Failed to find self [search]", searcher.search(datapoint, 1).get(0).getWeight() < Constants.EPSILON);            assertTrue("Failed to find self [searchFirst]", searcher.searchFirst(datapoint, false).getWeight() < Constants.EPSILON);            assertTrue("Failed to remove self", searcher.remove(datapoint, Constants.EPSILON));        }    }}
0
public void setUp() throws Exception
{    super.setUp();    tmpDir = getTestTempDir("matrix");}
0
public void testSingularValues() throws IOException
{    Matrix A = lowRankMatrix(tmpDir, "A", 200, 970, 1020);    List<File> partsOfA = Arrays.asList(tmpDir.listFiles(new FilenameFilter() {        @Override        public boolean accept(File file, String fileName) {            return fileName.matches("A-.*");        }    }));        partsOfA = Lists.reverse(partsOfA);    SequentialOutOfCoreSvd s = new SequentialOutOfCoreSvd(partsOfA, tmpDir, 100, 210);    SequentialBigSvd svd = new SequentialBigSvd(A, 100);    Vector reference = new DenseVector(svd.getSingularValues()).viewPart(0, 6);    Vector actual = s.getSingularValues().viewPart(0, 6);    assertEquals(0, reference.minus(actual).maxValue(), 1.0e-9);    s.computeU(partsOfA, tmpDir);    Matrix u = readBlockMatrix(Arrays.asList(tmpDir.listFiles(new FilenameFilter() {        @Override        public boolean accept(File file, String fileName) {            return fileName.matches("U-.*");        }    })));    s.computeV(tmpDir, A.columnSize());    Matrix v = readBlockMatrix(Arrays.asList(tmpDir.listFiles(new FilenameFilter() {        @Override        public boolean accept(File file, String fileName) {            return fileName.matches("V-.*");        }    })));        assertEquals(0, A.minus(u.times(new DiagonalMatrix(s.getSingularValues())).times(v.transpose())).aggregate(Functions.PLUS, Functions.ABS), 1.0e-7);}
0
public boolean accept(File file, String fileName)
{    return fileName.matches("A-.*");}
0
public boolean accept(File file, String fileName)
{    return fileName.matches("U-.*");}
0
public boolean accept(File file, String fileName)
{    return fileName.matches("V-.*");}
0
private static Matrix readBlockMatrix(List<File> files) throws IOException
{        Collections.sort(files);        int nrows = -1;    int ncols = -1;    Matrix r = null;    MatrixWritable m = new MatrixWritable();    int row = 0;    for (File file : files) {        DataInputStream in = new DataInputStream(new FileInputStream(file));        m.readFields(in);        in.close();        if (nrows == -1) {                        nrows = m.get().rowSize() * files.size();            ncols = m.get().columnSize();            r = new DenseMatrix(nrows, ncols);        }        r.viewPart(row, m.get().rowSize(), 0, r.columnSize()).assign(m.get());        row += m.get().rowSize();    }        if (row != nrows && r != null) {                r = r.viewPart(0, row, 0, ncols);    }    return r;}
0
public void testLeftVectors() throws IOException
{    Matrix A = lowRankMatrixInMemory(20, 20);    SequentialBigSvd s = new SequentialBigSvd(A, 6);    SingularValueDecomposition svd = new SingularValueDecomposition(A);        Matrix u1 = svd.getU().viewPart(0, 20, 0, 3).assign(Functions.ABS);    Matrix u2 = s.getU().viewPart(0, 20, 0, 3).assign(Functions.ABS);    assertEquals(u1, u2);}
0
private static Matrix lowRankMatrixInMemory(int rows, int columns) throws IOException
{    return lowRankMatrix(null, null, 0, rows, columns);}
0
private static void assertEquals(Matrix u1, Matrix u2)
{    assertEquals(0.0, u1.minus(u2).aggregate(Functions.MAX, Functions.ABS), 1.0e-10);}
0
public void testRightVectors() throws IOException
{    Matrix A = lowRankMatrixInMemory(20, 20);    SequentialBigSvd s = new SequentialBigSvd(A, 6);    SingularValueDecomposition svd = new SingularValueDecomposition(A);    Matrix v1 = svd.getV().viewPart(0, 20, 0, 3).assign(Functions.ABS);    Matrix v2 = s.getV().viewPart(0, 20, 0, 3).assign(Functions.ABS);    assertEquals(v1, v2);}
0
private static Matrix lowRankMatrix(File tmpDir, String aBase, int rowsPerSlice, int rows, int columns) throws IOException
{    int rank = 10;    Matrix u = new RandomTrinaryMatrix(1, rows, rank, false);    Matrix d = new DenseMatrix(rank, rank);    d.set(0, 0, 5);    d.set(1, 1, 3);    d.set(2, 2, 1);    d.set(3, 3, 0.5);    Matrix v = new RandomTrinaryMatrix(2, columns, rank, false);    Matrix a = u.times(d).times(v.transpose());    if (tmpDir != null) {        for (int i = 0; i < a.rowSize(); i += rowsPerSlice) {            MatrixWritable m = new MatrixWritable(a.viewPart(i, Math.min(a.rowSize() - i, rowsPerSlice), 0, a.columnSize()));            DataOutputStream out = new DataOutputStream(new FileOutputStream(new File(tmpDir, String.format("%s-%09d", aBase, i))));            try {                m.write(out);            } finally {                out.close();            }        }    }    return a;}
0
public void testBinaryCase()
{    Random gen = RandomUtils.getRandom();    OnlineSummarizer[] stats = new OnlineSummarizer[4];    for (int i = 0; i < 4; i++) {        stats[i] = new OnlineSummarizer();    }    for (int i = 0; i < 100; i++) {        OnlineAuc a1 = new GlobalOnlineAuc();        a1.setPolicy(GlobalOnlineAuc.ReplacementPolicy.FAIR);        OnlineAuc a2 = new GlobalOnlineAuc();        a2.setPolicy(GlobalOnlineAuc.ReplacementPolicy.FIFO);        OnlineAuc a3 = new GlobalOnlineAuc();        a3.setPolicy(GlobalOnlineAuc.ReplacementPolicy.RANDOM);        Auc a4 = new Auc();        for (int j = 0; j < 10000; j++) {            double x = gen.nextGaussian();            a1.addSample(0, x);            a2.addSample(0, x);            a3.addSample(0, x);            a4.add(0, x);            x = gen.nextGaussian() + 1;            a1.addSample(1, x);            a2.addSample(1, x);            a3.addSample(1, x);            a4.add(1, x);        }        stats[0].add(a1.auc());        stats[1].add(a2.auc());        stats[2].add(a3.auc());        stats[3].add(a4.auc());    }    int i = 0;    for (GlobalOnlineAuc.ReplacementPolicy policy : new GlobalOnlineAuc.ReplacementPolicy[] { GlobalOnlineAuc.ReplacementPolicy.FAIR, GlobalOnlineAuc.ReplacementPolicy.FIFO, GlobalOnlineAuc.ReplacementPolicy.RANDOM, null }) {        OnlineSummarizer summary = stats[i++];        System.out.printf("%s,%.4f (min = %.4f, 25%%-ile=%.4f, 75%%-ile=%.4f, max=%.4f)\n", policy, summary.getMean(), summary.getQuartile(0), summary.getQuartile(1), summary.getQuartile(2), summary.getQuartile(3));    }        assertEquals(0.7603, stats[0].getMean(), 0.03);    assertEquals(0.7603, stats[0].getQuartile(1), 0.03);    assertEquals(0.7603, stats[0].getQuartile(3), 0.03);        assertEquals(0.7603, stats[1].getMean(), 0.001);    assertEquals(0.7603, stats[1].getQuartile(1), 0.006);    assertEquals(0.7603, stats[1].getQuartile(3), 0.006);        assertEquals(0.7603, stats[2].getMean(), 0.001);    assertEquals(0.7603, stats[2].getQuartile(1), 0.006);    assertEquals(0.7603, stats[2].getQuartile(1), 0.006);}
0
public void mustNotOmitGroup()
{    OnlineAuc x = new GroupedOnlineAuc();    x.addSample(0, 3.14);}
0
public void groupedAuc()
{    Random gen = RandomUtils.getRandom();    OnlineAuc x = new GroupedOnlineAuc();    OnlineAuc y = new GlobalOnlineAuc();    for (int i = 0; i < 10000; i++) {        x.addSample(0, "a", gen.nextGaussian());        x.addSample(1, "a", gen.nextGaussian() + 1);        x.addSample(0, "b", gen.nextGaussian() + 10);        x.addSample(1, "b", gen.nextGaussian() + 11);        y.addSample(0, "a", gen.nextGaussian());        y.addSample(1, "a", gen.nextGaussian() + 1);        y.addSample(0, "b", gen.nextGaussian() + 10);        y.addSample(1, "b", gen.nextGaussian() + 11);    }    assertEquals(0.7603, x.auc(), 0.01);    assertEquals((0.7603 + 0.5) / 2, y.auc(), 0.02);}
0
public void testDiscreteSampler()
{    Vector distribution = new DenseVector(new double[] { 1, 0, 2, 3, 5, 0 });    Sampler sampler = new Sampler(RandomUtils.getRandom(), distribution);    Vector sampledDistribution = distribution.like();    int i = 0;    while (i < 100000) {        int index = sampler.sample();        sampledDistribution.set(index, sampledDistribution.get(index) + 1);        i++;    }    assertTrue("sampled distribution is far from the original", l1Dist(distribution, sampledDistribution) < 1.0e-2);}
0
private static double l1Dist(Vector v, Vector w)
{    return v.normalize(1.0).minus(w.normalize(1)).norm(1.0);}
0
public void testUnsignedLong() throws Exception
{    ByteArrayOutputStream baos = new ByteArrayOutputStream();    DataOutput out = new DataOutputStream(baos);    Varint.writeUnsignedVarLong(0L, out);    for (long i = 1L; i > 0L && i <= (1L << 62); i <<= 1) {        Varint.writeUnsignedVarLong(i - 1, out);        Varint.writeUnsignedVarLong(i, out);    }    Varint.writeUnsignedVarLong(Long.MAX_VALUE, out);    DataInput in = new DataInputStream(new ByteArrayInputStream(baos.toByteArray()));    assertEquals(0L, Varint.readUnsignedVarLong(in));    for (long i = 1L; i > 0L && i <= (1L << 62); i <<= 1) {        assertEquals(i - 1, Varint.readUnsignedVarLong(in));        assertEquals(i, Varint.readUnsignedVarLong(in));    }    assertEquals(Long.MAX_VALUE, Varint.readUnsignedVarLong(in));}
0
public void testSignedPositiveLong() throws Exception
{    ByteArrayOutputStream baos = new ByteArrayOutputStream();    DataOutput out = new DataOutputStream(baos);    Varint.writeSignedVarLong(0L, out);    for (long i = 1L; i <= (1L << 61); i <<= 1) {        Varint.writeSignedVarLong(i - 1, out);        Varint.writeSignedVarLong(i, out);    }    Varint.writeSignedVarLong((1L << 62) - 1, out);    Varint.writeSignedVarLong((1L << 62), out);    Varint.writeSignedVarLong(Long.MAX_VALUE, out);    DataInput in = new DataInputStream(new ByteArrayInputStream(baos.toByteArray()));    assertEquals(0L, Varint.readSignedVarLong(in));    for (long i = 1L; i <= (1L << 61); i <<= 1) {        assertEquals(i - 1, Varint.readSignedVarLong(in));        assertEquals(i, Varint.readSignedVarLong(in));    }    assertEquals((1L << 62) - 1, Varint.readSignedVarLong(in));    assertEquals((1L << 62), Varint.readSignedVarLong(in));    assertEquals(Long.MAX_VALUE, Varint.readSignedVarLong(in));}
0
public void testSignedNegativeLong() throws Exception
{    ByteArrayOutputStream baos = new ByteArrayOutputStream();    DataOutput out = new DataOutputStream(baos);    for (long i = -1L; i >= -(1L << 62); i <<= 1) {        Varint.writeSignedVarLong(i, out);        Varint.writeSignedVarLong(i + 1, out);    }    Varint.writeSignedVarLong(Long.MIN_VALUE, out);    Varint.writeSignedVarLong(Long.MIN_VALUE + 1, out);    DataInput in = new DataInputStream(new ByteArrayInputStream(baos.toByteArray()));    for (long i = -1L; i >= -(1L << 62); i <<= 1) {        assertEquals(i, Varint.readSignedVarLong(in));        assertEquals(i + 1, Varint.readSignedVarLong(in));    }    assertEquals(Long.MIN_VALUE, Varint.readSignedVarLong(in));    assertEquals(Long.MIN_VALUE + 1, Varint.readSignedVarLong(in));}
0
public void testUnsignedInt() throws Exception
{    ByteArrayOutputStream baos = new ByteArrayOutputStream();    DataOutput out = new DataOutputStream(baos);    Varint.writeUnsignedVarInt(0, out);    for (int i = 1; i > 0 && i <= (1 << 30); i <<= 1) {        Varint.writeUnsignedVarLong(i - 1, out);        Varint.writeUnsignedVarLong(i, out);    }    Varint.writeUnsignedVarLong(Integer.MAX_VALUE, out);    DataInput in = new DataInputStream(new ByteArrayInputStream(baos.toByteArray()));    assertEquals(0, Varint.readUnsignedVarInt(in));    for (int i = 1; i > 0 && i <= (1 << 30); i <<= 1) {        assertEquals(i - 1, Varint.readUnsignedVarInt(in));        assertEquals(i, Varint.readUnsignedVarInt(in));    }    assertEquals(Integer.MAX_VALUE, Varint.readUnsignedVarInt(in));}
0
public void testSignedPositiveInt() throws Exception
{    ByteArrayOutputStream baos = new ByteArrayOutputStream();    DataOutput out = new DataOutputStream(baos);    Varint.writeSignedVarInt(0, out);    for (int i = 1; i <= (1 << 29); i <<= 1) {        Varint.writeSignedVarLong(i - 1, out);        Varint.writeSignedVarLong(i, out);    }    Varint.writeSignedVarInt((1 << 30) - 1, out);    Varint.writeSignedVarInt((1 << 30), out);    Varint.writeSignedVarInt(Integer.MAX_VALUE, out);    DataInput in = new DataInputStream(new ByteArrayInputStream(baos.toByteArray()));    assertEquals(0, Varint.readSignedVarInt(in));    for (int i = 1; i <= (1 << 29); i <<= 1) {        assertEquals(i - 1, Varint.readSignedVarInt(in));        assertEquals(i, Varint.readSignedVarInt(in));    }    assertEquals((1L << 30) - 1, Varint.readSignedVarInt(in));    assertEquals((1L << 30), Varint.readSignedVarInt(in));    assertEquals(Integer.MAX_VALUE, Varint.readSignedVarInt(in));}
0
public void testSignedNegativeInt() throws Exception
{    ByteArrayOutputStream baos = new ByteArrayOutputStream();    DataOutput out = new DataOutputStream(baos);    for (int i = -1; i >= -(1 << 30); i <<= 1) {        Varint.writeSignedVarInt(i, out);        Varint.writeSignedVarInt(i + 1, out);    }    Varint.writeSignedVarInt(Integer.MIN_VALUE, out);    Varint.writeSignedVarInt(Integer.MIN_VALUE + 1, out);    DataInput in = new DataInputStream(new ByteArrayInputStream(baos.toByteArray()));    for (int i = -1; i >= -(1 << 30); i <<= 1) {        assertEquals(i, Varint.readSignedVarInt(in));        assertEquals(i + 1, Varint.readSignedVarInt(in));    }    assertEquals(Integer.MIN_VALUE, Varint.readSignedVarInt(in));    assertEquals(Integer.MIN_VALUE + 1, Varint.readSignedVarInt(in));}
0
public void testUnsignedSize() throws Exception
{    ByteArrayOutputStream baos = new ByteArrayOutputStream();    DataOutput out = new DataOutputStream(baos);    int expectedSize = 0;    for (int exponent = 0; exponent <= 62; exponent++) {        Varint.writeUnsignedVarLong(1L << exponent, out);        expectedSize += 1 + exponent / 7;        assertEquals(expectedSize, baos.size());    }}
0
public void testSignedSize() throws Exception
{    ByteArrayOutputStream baos = new ByteArrayOutputStream();    DataOutput out = new DataOutputStream(baos);    int expectedSize = 0;    for (int exponent = 0; exponent <= 61; exponent++) {        Varint.writeSignedVarLong(1L << exponent, out);        expectedSize += 1 + ((exponent + 1) / 7);        assertEquals(expectedSize, baos.size());    }    for (int exponent = 0; exponent <= 61; exponent++) {        Varint.writeSignedVarLong(-(1L << exponent) - 1, out);        expectedSize += 1 + ((exponent + 1) / 7);        assertEquals(expectedSize, baos.size());    }}
0
public void createRandom(Vector v)
{    int size = randomInt(v.size() - 1);    for (int i = 0; i < size; ++i) {        v.set(randomInt(v.size() - 1), randomDouble());    }    int zeros = Math.max(2, size / 4);    for (Element e : v.nonZeroes()) {        if (e.index() % zeros == 0) {            e.set(0.0);        }    }}
0
public void testViewSequentialAccessSparseVectorWritable() throws Exception
{    Vector v = new SequentialAccessSparseVector(MAX_VECTOR_SIZE);    createRandom(v);    Vector view = new VectorView(v, 0, v.size());    doTestVectorWritableEquals(view);}
0
public void testSequentialAccessSparseVectorWritable() throws Exception
{    Vector v = new SequentialAccessSparseVector(MAX_VECTOR_SIZE);    createRandom(v);    doTestVectorWritableEquals(v);}
0
public void testRandomAccessSparseVectorWritable() throws Exception
{    Vector v = new RandomAccessSparseVector(MAX_VECTOR_SIZE);    createRandom(v);    doTestVectorWritableEquals(v);}
0
public void testDenseVectorWritable() throws Exception
{    Vector v = new DenseVector(MAX_VECTOR_SIZE);    createRandom(v);    doTestVectorWritableEquals(v);}
0
public void testNamedVectorWritable() throws Exception
{    Vector v = new DenseVector(MAX_VECTOR_SIZE);    v = new NamedVector(v, "Victor");    createRandom(v);    doTestVectorWritableEquals(v);}
0
private static void doTestVectorWritableEquals(Vector v) throws IOException
{    Writable vectorWritable = new VectorWritable(v);    VectorWritable vectorWritable2 = new VectorWritable();    writeAndRead(vectorWritable, vectorWritable2);    Vector v2 = vectorWritable2.get();    if (v instanceof NamedVector) {        assertTrue(v2 instanceof NamedVector);        NamedVector nv = (NamedVector) v;        NamedVector nv2 = (NamedVector) v2;        assertEquals(nv.getName(), nv2.getName());        assertEquals("Victor", nv.getName());    }    assertEquals(v, v2);}
0
private static void writeAndRead(Writable toWrite, Writable toRead) throws IOException
{    ByteArrayOutputStream baos = new ByteArrayOutputStream();    DataOutputStream dos = new DataOutputStream(baos);    try {        toWrite.write(dos);    } finally {        Closeables.close(dos, false);    }    ByteArrayInputStream bais = new ByteArrayInputStream(baos.toByteArray());    DataInputStream dis = new DataInputStream(bais);    try {        toRead.readFields(dis);    } finally {        Closeables.close(dos, true);    }}
0
public void setUp() throws Exception
{    super.setUp();    counter = EasyMock.createMock(Counter.class);    context = EasyMock.createMock(Context.class);}
0
public void testCollectNgrams() throws Exception
{    Text key = new Text();    key.set("dummy-key");    String[] input = { "the", "best", "of", "times", "the", "worst", "of", "times" };    StringTuple inputTuple = new StringTuple();    for (String i : input) {        inputTuple.add(i);    }    String[][] values = { { "h_the", "the best" }, { "t_best", "the best" }, { "h_of", "of times" }, { "t_times", "of times" }, { "h_best", "best of" }, { "t_of", "best of" }, { "h_the", "the worst" }, { "t_worst", "the worst" }, { "h_times", "times the" }, { "t_the", "times the" }, { "h_worst", "worst of" }, { "t_of", "worst of" } };        Configuration conf = getConfiguration();    conf.set(CollocMapper.MAX_SHINGLE_SIZE, "2");    EasyMock.expect(context.getConfiguration()).andReturn(conf);    for (String[] v : values) {        Type p = v[0].startsWith("h") ? Gram.Type.HEAD : Gram.Type.TAIL;        int frequency = 1;        if ("of times".equals(v[1])) {            frequency = 2;        }        Gram subgram = new Gram(v[0].substring(2), frequency, p);        Gram ngram = new Gram(v[1], frequency, Gram.Type.NGRAM);        GramKey subgramKey = new GramKey(subgram, new byte[0]);        GramKey subgramNgramKey = new GramKey(subgram, ngram.getBytes());        context.write(subgramKey, subgram);        context.write(subgramNgramKey, ngram);    }    EasyMock.expect(context.getCounter(CollocMapper.Count.NGRAM_TOTAL)).andReturn(counter);    counter.increment(7);    EasyMock.replay(context, counter);    CollocMapper c = new CollocMapper();    c.setup(context);    c.map(key, inputTuple, context);    EasyMock.verify(context);}
0
public void testCollectNgramsWithUnigrams() throws Exception
{    Text key = new Text();    key.set("dummy-key");    String[] input = { "the", "best", "of", "times", "the", "worst", "of", "times" };    StringTuple inputTuple = new StringTuple();    for (String i : input) {        inputTuple.add(i);    }    String[][] values = { { "h_the", "the best" }, { "t_best", "the best" }, { "h_of", "of times" }, { "t_times", "of times" }, { "h_best", "best of" }, { "t_of", "best of" }, { "h_the", "the worst" }, { "t_worst", "the worst" }, { "h_times", "times the" }, { "t_the", "times the" }, { "h_worst", "worst of" }, { "t_of", "worst of" }, { "u_worst", "worst" }, { "u_of", "of" }, { "u_the", "the" }, { "u_best", "best" }, { "u_times", "times" } };        Configuration conf = getConfiguration();    conf.set(CollocMapper.MAX_SHINGLE_SIZE, "2");    conf.setBoolean(CollocDriver.EMIT_UNIGRAMS, true);    EasyMock.expect(context.getConfiguration()).andReturn(conf);    for (String[] v : values) {        Type p = v[0].startsWith("h") ? Gram.Type.HEAD : Gram.Type.TAIL;        p = v[0].startsWith("u") ? Gram.Type.UNIGRAM : p;        int frequency = 1;        if ("of times".equals(v[1]) || "of".equals(v[1]) || "times".equals(v[1]) || "the".equals(v[1])) {            frequency = 2;        }        if (p == Gram.Type.UNIGRAM) {            Gram unigram = new Gram(v[1], frequency, Gram.Type.UNIGRAM);            GramKey unigramKey = new GramKey(unigram, new byte[0]);            context.write(unigramKey, unigram);        } else {            Gram subgram = new Gram(v[0].substring(2), frequency, p);            Gram ngram = new Gram(v[1], frequency, Gram.Type.NGRAM);            GramKey subgramKey = new GramKey(subgram, new byte[0]);            GramKey subgramNgramKey = new GramKey(subgram, ngram.getBytes());            context.write(subgramKey, subgram);            context.write(subgramNgramKey, ngram);        }    }    EasyMock.expect(context.getCounter(CollocMapper.Count.NGRAM_TOTAL)).andReturn(counter);    counter.increment(7);    EasyMock.replay(context, counter);    CollocMapper c = new CollocMapper();    c.setup(context);    c.map(key, inputTuple, context);    EasyMock.verify(context);}
0
public void setUp() throws Exception
{    super.setUp();    context = EasyMock.createMock(Context.class);}
0
public void testReduce() throws Exception
{                Gram[][] input = { { new Gram("the", Gram.Type.UNIGRAM), new Gram("the", Gram.Type.UNIGRAM), new Gram("the", Gram.Type.UNIGRAM) }, { new Gram("the", Gram.Type.HEAD), new Gram("the best", Gram.Type.NGRAM), new Gram("the worst", Gram.Type.NGRAM) }, { new Gram("of", Gram.Type.HEAD), new Gram("of times", Gram.Type.NGRAM), new Gram("of times", Gram.Type.NGRAM) }, { new Gram("times", Gram.Type.TAIL), new Gram("of times", Gram.Type.NGRAM), new Gram("of times", Gram.Type.NGRAM) } };        Gram[][] values = { { new Gram("the", 2, Gram.Type.UNIGRAM), new Gram("the", 2, Gram.Type.UNIGRAM) }, { new Gram("the best", 1, Gram.Type.NGRAM), new Gram("the", 2, Gram.Type.HEAD) }, { new Gram("the worst", 1, Gram.Type.NGRAM), new Gram("the", 2, Gram.Type.HEAD) }, { new Gram("of times", 2, Gram.Type.NGRAM), new Gram("of", 2, Gram.Type.HEAD) }, { new Gram("of times", 2, Gram.Type.NGRAM), new Gram("times", 2, Gram.Type.TAIL) } };        for (Gram[] v : values) {        context.write(v[0], v[1]);    }    EasyMock.replay(context);        CollocReducer c = new CollocReducer();    GramKey key = new GramKey();    byte[] empty = new byte[0];    for (Gram[] ii : input) {        key.set(ii[0], empty);        Collection<Gram> vv = Lists.newLinkedList();        vv.addAll(Arrays.asList(ii));        c.reduce(key, vv, context);    }    EasyMock.verify(context);}
0
public void testComparator()
{    byte[] foo = new byte[1];    foo[0] = 1;    byte[] empty = new byte[0];        GramKey a = new GramKey(new Gram("foo", 1, Gram.Type.HEAD), empty);        GramKey b = new GramKey(new Gram("foo", 1, Gram.Type.HEAD), foo);        GramKey c = new GramKey(new Gram("foo", 2, Gram.Type.HEAD), empty);        GramKey d = new GramKey(new Gram("foo", 1, Gram.Type.TAIL), empty);        GramKey e = new GramKey(new Gram("bar", 5, Gram.Type.HEAD), empty);    GramKeyGroupComparator cmp = new GramKeyGroupComparator();    assertEquals(0, cmp.compare(a, b));    assertEquals(0, cmp.compare(a, c));    assertTrue(cmp.compare(a, d) < 0);    assertTrue(cmp.compare(a, e) > 0);    assertTrue(cmp.compare(d, e) > 0);}
0
public void testPartition()
{    byte[] foo = new byte[1];    foo[0] = 1;    foo[0] = 2;    byte[] empty = new byte[0];    GramKey a = new GramKey(new Gram("foo", 1, Gram.Type.HEAD), empty);    GramKey b = new GramKey(new Gram("foo", 1, Gram.Type.HEAD), foo);    byte[] bar = new byte[1];    GramKey c = new GramKey(new Gram("foo", 2, Gram.Type.HEAD), bar);    GramKey d = new GramKey(new Gram("foo", 1, Gram.Type.TAIL), empty);    GramKey e = new GramKey(new Gram("foo", 2, Gram.Type.TAIL), foo);    Partitioner<GramKey, Gram> p = new GramKeyPartitioner();    int numPartitions = 5;    int ap = p.getPartition(a, null, numPartitions);    int bp = p.getPartition(b, null, numPartitions);    int cp = p.getPartition(c, null, numPartitions);    int dp = p.getPartition(d, null, numPartitions);    int ep = p.getPartition(e, null, numPartitions);    assertEquals(ap, bp);    assertEquals(ap, cp);    assertEquals(dp, ep);}
0
public void testGramKeySort()
{    byte[] foo = { 1 };            byte[] empty = new byte[0];    GramKey[] input = { new GramKey(new Gram("bar", 1, Gram.Type.UNIGRAM), empty), new GramKey(new Gram("bar", 1, Gram.Type.UNIGRAM), empty), new GramKey(new Gram("bar", 1, Gram.Type.UNIGRAM), foo), new GramKey(new Gram("bar", 8, Gram.Type.NGRAM), foo), new GramKey(new Gram("bar", 8, Gram.Type.NGRAM), empty), new GramKey(new Gram("foo", 2, Gram.Type.HEAD), foo), new GramKey(new Gram("foo", 3, Gram.Type.HEAD), empty), new GramKey(new Gram("foo", 4, Gram.Type.TAIL), foo), new GramKey(new Gram("foo", 5, Gram.Type.TAIL), foo), new GramKey(new Gram("bar", 6, Gram.Type.HEAD), foo), new GramKey(new Gram("bar", 7, Gram.Type.TAIL), empty) };    int[] expect = { 9, 6, 5, 10, 7, 8, 0, 1, 2, 4, 3 };    GramKey[] sorted = new GramKey[input.length];    System.arraycopy(input, 0, sorted, 0, input.length);    Arrays.sort(sorted);    for (int i = 0; i < input.length; i++) {        assertSame(input[expect[i]], sorted[i]);    }}
0
public void testWritable() throws Exception
{    byte[] foo = new byte[0];    byte[] bar = { 2 };    GramKey one = new GramKey(new Gram("foo", 2, Gram.Type.HEAD), foo);    GramKey two = new GramKey(new Gram("foobar", 3, Gram.Type.UNIGRAM), bar);    assertEquals("foo", one.getPrimaryString());    assertEquals("foobar", two.getPrimaryString());    assertEquals(Gram.Type.UNIGRAM, two.getType());    ByteArrayOutputStream bout = new ByteArrayOutputStream();    DataOutputStream out = new DataOutputStream(bout);    try {        two.write(out);    } finally {        Closeables.close(out, false);    }    byte[] b = bout.toByteArray();    ByteArrayInputStream bin = new ByteArrayInputStream(b);    DataInputStream din = new DataInputStream(bin);    try {        one.readFields(din);    } finally {        Closeables.close(din, true);    }    assertTrue(Arrays.equals(two.getBytes(), one.getBytes()));    assertEquals(Gram.Type.UNIGRAM, one.getType());}
0
public void testConstructorsGetters()
{    Gram one = new Gram("foo", 2, Gram.Type.HEAD);    assertEquals("foo", one.getString());    assertEquals(2, one.getFrequency());    assertEquals(Gram.Type.HEAD, one.getType());    Gram oneClone = new Gram(one);    assertEquals("foo", oneClone.getString());    assertEquals(2, oneClone.getFrequency());    assertEquals(Gram.Type.HEAD, oneClone.getType());    Gram two = new Gram("foo", 3, Gram.Type.TAIL);    assertEquals(Gram.Type.TAIL, two.getType());    Gram three = new Gram("foo", 4, Gram.Type.UNIGRAM);    assertEquals(Gram.Type.UNIGRAM, three.getType());    Gram four = new Gram("foo", 5, Gram.Type.NGRAM);    assertEquals(Gram.Type.NGRAM, four.getType());}
0
public void testNull1()
{    new Gram(null, 4, Gram.Type.UNIGRAM);}
0
public void testNull2()
{    new Gram("foo", 4, null);}
0
public void testEquality()
{    Gram one = new Gram("foo", 2, Gram.Type.HEAD);    Gram two = new Gram("foo", 3, Gram.Type.HEAD);    assertEquals(one, two);    assertEquals(two, one);    Gram three = new Gram("foo", 4, Gram.Type.TAIL);    Gram four = new Gram("foo", Gram.Type.UNIGRAM);    assertTrue(!three.equals(two));    assertTrue(!four.equals(one));    assertTrue(!one.equals(four));    Gram five = new Gram("foo", 5, Gram.Type.UNIGRAM);    assertEquals(four, five);    Gram six = new Gram("foo", 6, Gram.Type.NGRAM);    Gram seven = new Gram("foo", 7, Gram.Type.NGRAM);    assertTrue(!five.equals(six));    assertEquals(six, seven);    Gram eight = new Gram("foobar", 4, Gram.Type.TAIL);    assertTrue(!eight.equals(four));    assertTrue(!eight.equals(three));    assertTrue(!eight.equals(two));    assertTrue(!eight.equals(one));}
0
public void testHashing()
{    Gram[] input = { new Gram("foo", 2, Gram.Type.HEAD), new Gram("foo", 3, Gram.Type.HEAD), new Gram("foo", 4, Gram.Type.TAIL), new Gram("foo", 5, Gram.Type.TAIL), new Gram("bar", 6, Gram.Type.HEAD), new Gram("bar", 7, Gram.Type.TAIL), new Gram("bar", 8, Gram.Type.NGRAM), new Gram("bar", Gram.Type.UNIGRAM) };    HashMap<Gram, Gram> map = new HashMap<>();    for (Gram n : input) {        Gram val = map.get(n);        if (val != null) {            val.incrementFrequency(n.getFrequency());        } else {            map.put(n, n);        }    }        int[] freq = { 5, 3, 9, 5, 6, 7, 8, 1 };        boolean[] memb = { true, false, true, false, true, true, true, true };    for (int i = 0; i < input.length; i++) {        assertEquals(freq[i], input[i].getFrequency());        assertEquals(memb[i], input[i] == map.get(input[i]));    }}
0
public void testWritable() throws Exception
{    Gram one = new Gram("foo", 2, Gram.Type.HEAD);    Gram two = new Gram("foobar", 3, Gram.Type.UNIGRAM);    assertEquals("foo", one.getString());    assertEquals(2, one.getFrequency());    assertEquals(Gram.Type.HEAD, one.getType());    assertEquals("foobar", two.getString());    assertEquals(3, two.getFrequency());    assertEquals(Gram.Type.UNIGRAM, two.getType());    ByteArrayOutputStream bout = new ByteArrayOutputStream();    DataOutput out = new DataOutputStream(bout);    two.write(out);    byte[] b = bout.toByteArray();    ByteArrayInputStream bin = new ByteArrayInputStream(b);    DataInput din = new DataInputStream(bin);    one.readFields(din);    assertEquals("foobar", one.getString());    assertEquals(3, one.getFrequency());    assertEquals(Gram.Type.UNIGRAM, one.getType());}
0
public void testSorting()
{    Gram[] input = { new Gram("foo", 2, Gram.Type.HEAD), new Gram("foo", 3, Gram.Type.HEAD), new Gram("foo", 4, Gram.Type.TAIL), new Gram("foo", 5, Gram.Type.TAIL), new Gram("bar", 6, Gram.Type.HEAD), new Gram("bar", 7, Gram.Type.TAIL), new Gram("bar", 8, Gram.Type.NGRAM), new Gram("bar", Gram.Type.UNIGRAM) };    Gram[] sorted = new Gram[input.length];    int[] expectations = { 4, 0, 1, 5, 2, 3, 7, 6 };    System.arraycopy(input, 0, sorted, 0, input.length);    Arrays.sort(sorted);    for (int i = 0; i < sorted.length; i++) {        assertSame(input[expectations[i]], sorted[i]);    }}
0
public void setUp() throws Exception
{    super.setUp();    context = EasyMock.createMock(Reducer.Context.class);    ll = EasyMock.createMock(LLCallback.class);    cl = new LLCallback() {        @Override        public double logLikelihoodRatio(long k11, long k12, long k21, long k22) {                        return LogLikelihood.logLikelihoodRatio(k11, k12, k21, k22);        }    };}
1
public double logLikelihoodRatio(long k11, long k12, long k21, long k22)
{        return LogLikelihood.logLikelihoodRatio(k11, k12, k21, k22);}
1
public void testReduce() throws Exception
{    LLRReducer reducer = new LLRReducer(ll);                Gram[][] input = { { new Gram("the best", 1, Gram.Type.NGRAM), new Gram("the", 2, Gram.Type.HEAD), new Gram("best", 1, Gram.Type.TAIL) }, { new Gram("best of", 1, Gram.Type.NGRAM), new Gram("best", 1, Gram.Type.HEAD), new Gram("of", 2, Gram.Type.TAIL) }, { new Gram("of times", 2, Gram.Type.NGRAM), new Gram("of", 2, Gram.Type.HEAD), new Gram("times", 2, Gram.Type.TAIL) }, { new Gram("times the", 1, Gram.Type.NGRAM), new Gram("times", 1, Gram.Type.HEAD), new Gram("the", 1, Gram.Type.TAIL) }, { new Gram("the worst", 1, Gram.Type.NGRAM), new Gram("the", 2, Gram.Type.HEAD), new Gram("worst", 1, Gram.Type.TAIL) }, { new Gram("worst of", 1, Gram.Type.NGRAM), new Gram("worst", 1, Gram.Type.HEAD), new Gram("of", 2, Gram.Type.TAIL) } };    int[][] expectations = {     { 1, 1, 0, 5 },     { 1, 0, 1, 5 },     { 2, 0, 0, 5 },     { 1, 0, 0, 6 },     { 1, 1, 0, 5 },     { 1, 0, 1, 5 } };    Configuration config = getConfiguration();    config.set(LLRReducer.NGRAM_TOTAL, "7");    EasyMock.expect(context.getConfiguration()).andReturn(config);    for (int i = 0; i < expectations.length; i++) {        int[] ee = expectations[i];        context.write(EasyMock.eq(new Text(input[i][0].getString())), (DoubleWritable) EasyMock.anyObject());        EasyMock.expect(ll.logLikelihoodRatio(ee[0], ee[1], ee[2], ee[3])).andDelegateTo(cl);    }    EasyMock.replay(context, ll);    reducer.setup(context);    for (Gram[] ii : input) {        Collection<Gram> vv = Lists.newLinkedList();        vv.addAll(Arrays.asList(ii).subList(1, ii.length));        reducer.reduce(ii[0], vv, context);    }    EasyMock.verify(ll);}
0
public void setUp() throws Exception
{    super.setUp();    Configuration conf = getConfiguration();    inputPath = getTestTempFilePath("documents/docs.file");    FileSystem fs = FileSystem.get(inputPath.toUri(), conf);    SequenceFile.Writer writer = new SequenceFile.Writer(fs, conf, inputPath, Text.class, Text.class);    try {        RandomDocumentGenerator gen = new RandomDocumentGenerator();        for (int i = 0; i < NUM_DOCS; i++) {            writer.append(new Text("Document::ID::" + i), new Text(gen.getRandomDocument()));            writer.append(new Text("Document::ID::" + i), new Text(SECOND_TEXT_BLOCK_IDENTIFIER));        }    } finally {        Closeables.close(writer, false);    }}
0
public void testCreateTermFrequencyVectors() throws Exception
{    runTest(false, false);}
0
public void testCreateTermFrequencyVectorsNam() throws Exception
{    runTest(false, true);}
0
public void testCreateTermFrequencyVectorsSeq() throws Exception
{    runTest(true, false);}
0
public void testCreateTermFrequencyVectorsSeqNam() throws Exception
{    runTest(true, true);}
0
private void runTest(boolean sequential, boolean named) throws IOException, ClassNotFoundException, InterruptedException
{    Class<? extends Analyzer> analyzer = StandardAnalyzer.class;    Path tokenizedDocuments = getTestTempDirPath("output/tokenized-documents");    Path wordCount = getTestTempDirPath("output/wordcount");    Path tfVectors = new Path(wordCount, "tf-vectors");    Path tfidf = getTestTempDirPath("output/tfidf");    Path tfidfVectors = new Path(tfidf, "tfidf-vectors");    Configuration conf = getConfiguration();    DocumentProcessor.tokenizeDocuments(inputPath, analyzer, tokenizedDocuments, conf);    DictionaryVectorizer.createTermFrequencyVectors(tokenizedDocuments, wordCount, DictionaryVectorizer.DOCUMENT_VECTOR_OUTPUT_FOLDER, conf, 2, 1, 0.0f, -1.0f, true, 1, 100, sequential, named);    validateVectors(conf, NUM_DOCS, tfVectors, sequential, named);    Pair<Long[], List<Path>> docFrequenciesFeatures = TFIDFConverter.calculateDF(tfVectors, tfidf, conf, 100);    TFIDFConverter.processTfIdf(tfVectors, tfidf, conf, docFrequenciesFeatures, 1, -1, 2.0f, false, sequential, named, 1);    validateVectors(conf, NUM_DOCS, tfidfVectors, sequential, named);    Integer secondTextBlockIdentifierDimensionId = validateDictionary(wordCount, conf);    validateVectorContainingSecondTextBlock(conf, tfVectors, secondTextBlockIdentifierDimensionId);}
0
public static void validateVectors(Configuration conf, int numDocs, Path vectorPath, boolean sequential, boolean named)
{    int count = 0;    for (VectorWritable value : new SequenceFileDirValueIterable<VectorWritable>(vectorPath, PathType.LIST, PathFilters.partFilter(), null, true, conf)) {        count++;        Vector v = value.get();        if (named) {            assertTrue("Expected NamedVector", v instanceof NamedVector);            v = ((NamedVector) v).getDelegate();        }        if (sequential) {            assertTrue("Expected SequentialAccessSparseVector", v instanceof SequentialAccessSparseVector);        } else {            assertTrue("Expected RandomAccessSparseVector", v instanceof RandomAccessSparseVector);        }    }    assertEquals("Expected " + numDocs + " documents", numDocs, count);}
0
private Integer validateDictionary(Path dictionaryDirectoryPath, Configuration conf)
{    PathFilter dictionaryChunkPathFilter = new PathFilter() {        @Override        public boolean accept(Path path) {            String name = path.getName();            return name.startsWith("dictionary.file");        }    };    Map<String, Integer> dictionary = new HashMap<>();    for (Pair<Text, IntWritable> value : new SequenceFileDirIterable<Text, IntWritable>(dictionaryDirectoryPath, PathType.LIST, dictionaryChunkPathFilter, null, true, conf)) {        dictionary.put(value.getFirst().toString(), value.getSecond().get());    }    Integer secondTextBlockIdentifierDimensionId = dictionary.get(SECOND_TEXT_BLOCK_IDENTIFIER.toLowerCase());    assertNotNull("Token '" + SECOND_TEXT_BLOCK_IDENTIFIER + "' must be in dictionary ", secondTextBlockIdentifierDimensionId);    assertTrue("Dictionary must contain more than just 1 element!", dictionary.size() > 1);    return secondTextBlockIdentifierDimensionId;}
0
public boolean accept(Path path)
{    String name = path.getName();    return name.startsWith("dictionary.file");}
0
public static void validateVectorContainingSecondTextBlock(Configuration conf, Path vectorPath, int dimensionId)
{    for (VectorWritable value : new SequenceFileDirValueIterable<VectorWritable>(vectorPath, PathType.LIST, PathFilters.partFilter(), null, true, conf)) {        assertTrue("The vector must contain the second text block", value.get().get(dimensionId) > 0);    }}
0
public void testTokenizeDocuments() throws Exception
{    Configuration configuration = getConfiguration();    Path input = new Path(getTestTempDirPath(), "inputDir");    Path output = new Path(getTestTempDirPath(), "outputDir");    FileSystem fs = FileSystem.get(input.toUri(), configuration);    String documentId1 = "123";    String documentId2 = "456";    SequenceFile.Writer writer = new SequenceFile.Writer(fs, configuration, input, Text.class, Text.class);    try {        String text1 = "A test for the document processor";        writer.append(new Text(documentId1), new Text(text1));        String text2 = "and another one";        writer.append(new Text(documentId2), new Text(text2));    } finally {        Closeables.close(writer, false);    }    DocumentProcessor.tokenizeDocuments(input, StandardAnalyzer.class, output, configuration);    FileStatus[] statuses = fs.listStatus(output, PathFilters.logsCRCFilter());    assertEquals(1, statuses.length);    Path filePath = statuses[0].getPath();    SequenceFile.Reader reader = new SequenceFile.Reader(fs, filePath, configuration);    Text key = ClassUtils.instantiateAs((Class<? extends Text>) reader.getKeyClass(), Text.class);    StringTuple value = ClassUtils.instantiateAs((Class<? extends StringTuple>) reader.getValueClass(), StringTuple.class);    reader.next(key, value);    assertEquals(documentId1, key.toString());    assertEquals(Arrays.asList("test", "document", "processor"), value.getEntries());    reader.next(key, value);    assertEquals(documentId2, key.toString());    assertEquals(Arrays.asList("another", "one"), value.getEntries());}
0
public void setUp() throws Exception
{    super.setUp();    conf = getConfiguration();    inputPath = getTestTempFilePath("documents/docs.file");    FileSystem fs = FileSystem.get(inputPath.toUri(), conf);    SequenceFile.Writer writer = new SequenceFile.Writer(fs, conf, inputPath, Text.class, Text.class);    RandomDocumentGenerator gen = new RandomDocumentGenerator();    try {        for (int i = 0; i < NUM_DOCS; i++) {            writer.append(new Text("Document::ID::" + i), new Text(gen.getRandomDocument()));        }    } finally {        Closeables.close(writer, false);    }}
0
public void testCreate() throws Exception
{    runTest(false, false);}
0
public void testCreateNamed() throws Exception
{    runTest(false, true);}
0
public void testCreateSeq() throws Exception
{    runTest(true, false);}
0
public void testCreateSeqNamed() throws Exception
{    runTest(true, true);}
0
private void runTest(boolean sequential, boolean named) throws Exception
{    Path tmpPath = getTestTempDirPath();    Path outputPath = new Path(tmpPath, "output");    List<String> argList = Lists.newLinkedList();    ;    argList.add("-i");    argList.add(inputPath.toString());    argList.add("-o");    argList.add(outputPath.toString());    if (sequential) {        argList.add("-seq");    }    if (named) {        argList.add("-nv");    }    String[] args = argList.toArray(new String[argList.size()]);    ToolRunner.run(getConfiguration(), new EncodedVectorsFromSequenceFiles(), args);    SequenceFileDirIterator<Text, VectorWritable> iter = new SequenceFileDirIterator<>(outputPath, PathType.LIST, PathFilters.partFilter(), null, true, conf);    int seen = 0;    while (iter.hasNext()) {        Pair<Text, VectorWritable> next = iter.next();        if (sequential && !named) {            assertTrue(next.getSecond().get() instanceof SequentialAccessSparseVector);        } else if (named) {            assertTrue(next.getSecond().get() instanceof NamedVector);        }        seen++;    }    assertEquals("Missed some vectors", NUM_DOCS, seen);}
0
public void testCacheAreUsedStaticWord()
{    CachingStaticWordValueEncoder encoder = new CachingStaticWordValueEncoder(NAME, CARDINALITY);    Vector v = new DenseVector(CARDINALITY);    encoder.addToVector(WORD, v);    assertFalse("testCacheAreUsedStaticWord: cache should have values", encoder.getCaches()[0].isEmpty());}
0
public void testCacheAreUsedContinuous()
{    CachingContinuousValueEncoder encoder = new CachingContinuousValueEncoder(NAME, CARDINALITY);    Vector v = new DenseVector(CARDINALITY);    encoder.addToVector(CONTINUOUSVAL, 1.0, v);    assertFalse("testCacheAreUsedContinuous: cache should have values", encoder.getCaches()[0].isEmpty());}
0
public void testAddToVector()
{    FeatureVectorEncoder enc = new ConstantValueEncoder("foo");    Vector v1 = new DenseVector(20);    enc.addToVector((byte[]) null, -123, v1);    assertEquals(-123, v1.minValue(), 0);    assertEquals(0, v1.maxValue(), 0);    assertEquals(123, v1.norm(1), 0);    v1 = new DenseVector(20);    enc.addToVector((byte[]) null, 123, v1);    assertEquals(123, v1.maxValue(), 0);    assertEquals(0, v1.minValue(), 0);    assertEquals(123, v1.norm(1), 0);    Vector v2 = new DenseVector(20);    enc.setProbes(2);    enc.addToVector((byte[]) null, 123, v2);    assertEquals(123, v2.maxValue(), 0);    assertEquals(2 * 123, v2.norm(1), 0);            v1 = v2.minus(v1);    assertEquals(123, v1.maxValue(), 0);    assertEquals(123, v1.norm(1), 0);    Vector v3 = new DenseVector(20);    enc.setProbes(2);    enc.addToVector((byte[]) null, 100, v3);    v1 = v2.minus(v3);    assertEquals(23, v1.maxValue(), 0);    assertEquals(2 * 23, v1.norm(1), 0);    enc.addToVector((byte[]) null, 7, v1);    assertEquals(30, v1.maxValue(), 0);    assertEquals(2 * 30, v1.norm(1), 0);    assertEquals(30, v1.get(9), 0);    assertEquals(30, v1.get(10), 0);}
0
public void testAsString()
{    FeatureVectorEncoder enc = new ConstantValueEncoder("foo");    assertEquals("foo", enc.asString("123"));}
0
public void testAddToVector()
{    FeatureVectorEncoder enc = new ContinuousValueEncoder("foo");    Vector v1 = new DenseVector(20);    enc.addToVector("-123", v1);    assertEquals(-123, v1.minValue(), 0);    assertEquals(0, v1.maxValue(), 0);    assertEquals(123, v1.norm(1), 0);    v1 = new DenseVector(20);    enc.addToVector("123", v1);    assertEquals(123, v1.maxValue(), 0);    assertEquals(0, v1.minValue(), 0);    assertEquals(123, v1.norm(1), 0);    Vector v2 = new DenseVector(20);    enc.setProbes(2);    enc.addToVector("123", v2);    assertEquals(123, v2.maxValue(), 0);    assertEquals(2 * 123, v2.norm(1), 0);            v1 = v2.minus(v1);    assertEquals(123, v1.maxValue(), 0);    assertEquals(123, v1.norm(1), 0);    Vector v3 = new DenseVector(20);    enc.setProbes(2);    enc.addToVector("100", v3);    v1 = v2.minus(v3);    assertEquals(23, v1.maxValue(), 0);    assertEquals(2 * 23, v1.norm(1), 0);    enc.addToVector("7", v1);    assertEquals(30, v1.maxValue(), 0);    assertEquals(2 * 30, v1.norm(1), 0);    assertEquals(30, v1.get(10), 0);    assertEquals(30, v1.get(18), 0);    v2 = new DenseVector(20);    v3 = new DenseVector(20);    enc.setProbes(6);    enc.addToVector("145", v2);    enc.addToVector((byte[]) null, 145, v3);    assertEquals(0, v2.minus(v3).norm(1), 0);    try {        enc.addToVector("foobar", v1);        fail("Should have noticed bad numeric format");    } catch (NumberFormatException e) {        assertEquals("For input string: \"foobar\"", e.getMessage());    }}
0
public void testAsString()
{    FeatureVectorEncoder enc = new ContinuousValueEncoder("foo");    assertEquals("foo:123", enc.asString("123"));}
0
public void testAddToVector()
{    WordValueEncoder wv = new StaticWordValueEncoder("word");    ContinuousValueEncoder cv = new ContinuousValueEncoder("cont");    InteractionValueEncoder enc = new InteractionValueEncoder("interactions", wv, cv);    Vector v1 = new DenseVector(200);    enc.addInteractionToVector("a", "1.0", 1.0, v1);    int k = enc.getProbes();        assertEquals((float) k, v1.norm(1), 0);    assertEquals(1.0, v1.maxValue(), 0);        enc.addInteractionToVector("a", "1.0", 1.0, v1);    assertEquals((float) k * 2, v1.norm(1), 0);    assertEquals(2.0, v1.maxValue(), 0);    Vector v2 = new DenseVector(20000);    enc.addInteractionToVector("a", "1.0", 1.0, v2);    wv.addToVector("a", v2);    cv.addToVector("1.0", v2);    k = enc.getProbes();        assertEquals((float) (k + wv.getProbes() + cv.getProbes()), v2.norm(1), 1.0e-3);}
0
public void testAddToVectorUsesProductOfWeights()
{    WordValueEncoder wv = new StaticWordValueEncoder("word");    ContinuousValueEncoder cv = new ContinuousValueEncoder("cont");    InteractionValueEncoder enc = new InteractionValueEncoder("interactions", wv, cv);    Vector v1 = new DenseVector(200);    enc.addInteractionToVector("a", "0.9", 0.5, v1);    int k = enc.getProbes();        assertEquals((float) k * 0.5 * 0.9, v1.norm(1), 0);    assertEquals(0.5 * 0.9, v1.maxValue(), 0);}
0
public void testAddToVectorWithTextValueEncoder()
{    WordValueEncoder wv = new StaticWordValueEncoder("word");    TextValueEncoder tv = new TextValueEncoder("text");    InteractionValueEncoder enc = new InteractionValueEncoder("interactions", wv, tv);    Vector v1 = new DenseVector(200);    enc.addInteractionToVector("a", "some text here", 1.0, v1);    int k = enc.getProbes();        assertEquals((float) k * 3, v1.norm(1), 0);}
0
public void testTraceDictionary()
{    StaticWordValueEncoder encoder1 = new StaticWordValueEncoder("first");    StaticWordValueEncoder encoder2 = new StaticWordValueEncoder("second");    Map<String, Set<Integer>> traceDictionary = Maps.newHashMap();    InteractionValueEncoder interactions = new InteractionValueEncoder("interactions", encoder1, encoder2);    interactions.setProbes(1);    interactions.setTraceDictionary(traceDictionary);    Vector v = new DenseVector(10);    interactions.addInteractionToVector("a", "b", 1, v);    assertEquals(1, v.getNumNonZeroElements());    assertEquals(1, traceDictionary.size());    assertEquals("interactions=a:b", getFirst(traceDictionary.keySet(), null));}
0
public void testAddToVector()
{    TextValueEncoder enc = new TextValueEncoder("text");    Vector v1 = new DenseVector(200);    enc.addToVector("test1 and more", v1);    enc.flush(1, v1);        assertEquals(6.0, v1.norm(1), 0);    assertEquals(1.0, v1.maxValue(), 0);        StaticWordValueEncoder w = new StaticWordValueEncoder("text");    w.setDictionary(ImmutableMap.<String, Double>of("word1", 3.0, "word2", 1.5));    enc.setWordEncoder(w);        Vector v2 = new DenseVector(200);    enc.addToVector("test1 and more", v2);    enc.flush(1, v2);        Vector v3 = new DenseVector(200);    w.addToVector("test1", v3);    w.addToVector("and", v3);    w.addToVector("more", v3);    assertEquals(0, v3.minus(v2).norm(1), 0);        assertEquals(v3.zSum(), v3.dot(v1), 0);}
0
public void testAsString()
{    Locale.setDefault(Locale.ENGLISH);    FeatureVectorEncoder enc = new TextValueEncoder("text");    assertEquals("[text:test1:1.0000, text:and:1.0000, text:more:1.0000]", enc.asString("test1 and more"));}
0
public void testLuceneEncoding() throws Exception
{    LuceneTextValueEncoder enc = new LuceneTextValueEncoder("text");    enc.setAnalyzer(new WhitespaceAnalyzer());    Vector v1 = new DenseVector(200);    enc.addToVector("test1 and more", v1);    enc.flush(1, v1);            assertEquals(6.0, v1.norm(1), 0);    assertEquals(1.0, v1.maxValue(), 0);    v1 = new DenseVector(200);    enc.addToVector("", v1);    enc.flush(1, v1);    assertEquals(0.0, v1.norm(1), 0);    assertEquals(0.0, v1.maxValue(), 0);    v1 = new DenseVector(200);    StringBuilder builder = new StringBuilder(5000);    for (int i = 0; i < 1000; i++) {                builder.append("token_").append(i).append(' ');    }    enc.addToVector(builder.toString(), v1);    enc.flush(1, v1);        assertEquals(2000.0, v1.norm(1), 0);    assertEquals(19.0, v1.maxValue(), 0);}
0
public void testAddToVector()
{    FeatureVectorEncoder enc = new StaticWordValueEncoder("word");    Vector v = new DenseVector(200);    enc.addToVector("word1", v);    enc.addToVector("word2", v);    Iterator<Vector.Element> i = v.nonZeroes().iterator();    Iterator<Integer> j = ImmutableList.of(7, 118, 119, 199).iterator();    while (i.hasNext()) {        Vector.Element element = i.next();        assertEquals(j.next().intValue(), element.index());        assertEquals(1, element.get(), 0);    }    assertFalse(j.hasNext());}
0
public void testAsString()
{    Locale.setDefault(Locale.ENGLISH);    FeatureVectorEncoder enc = new StaticWordValueEncoder("word");    assertEquals("word:w1:1.0000", enc.asString("w1"));}
0
public void testStaticWeights()
{    StaticWordValueEncoder enc = new StaticWordValueEncoder("word");    enc.setDictionary(ImmutableMap.<String, Double>of("word1", 3.0, "word2", 1.5));    Vector v = new DenseVector(200);    enc.addToVector("word1", v);    enc.addToVector("word2", v);    enc.addToVector("word3", v);    Iterator<Vector.Element> i = v.nonZeroes().iterator();    Iterator<Integer> j = ImmutableList.of(7, 101, 118, 119, 152, 199).iterator();    Iterator<Double> k = ImmutableList.of(3.0, 0.75, 1.5, 1.5, 0.75, 3.0).iterator();    while (i.hasNext()) {        Vector.Element element = i.next();        assertEquals(j.next().intValue(), element.index());    }    i = v.nonZeroes().iterator();    while (i.hasNext()) {        Vector.Element element = i.next();        assertEquals(String.format("checking v[%d]", element.index()), k.next(), element.get(), 0);    }    assertFalse(j.hasNext());}
0
public void testDynamicWeights()
{    FeatureVectorEncoder enc = new AdaptiveWordValueEncoder("word");    Vector v = new DenseVector(200);        enc.addToVector("word1", v);        enc.addToVector("word2", v);        enc.addToVector("word1", v);        enc.addToVector("word3", v);    Iterator<Vector.Element> i = v.nonZeroes().iterator();    Iterator<Integer> j = ImmutableList.of(7, 101, 118, 119, 152, 199).iterator();    Iterator<Double> k = ImmutableList.of(Math.log(2 / 1.5) + Math.log(4.5 / 2.5), Math.log(6 / 1.5), Math.log(3.5 / 1.5), Math.log(3.5 / 1.5), Math.log(6 / 1.5), Math.log(2 / 1.5) + Math.log(4.5 / 2.5)).iterator();    while (i.hasNext()) {        Vector.Element element = i.next();        assertEquals(j.next().intValue(), element.index());        assertEquals(k.next(), element.get(), 1.0e-6);    }    assertFalse(j.hasNext());}
0
public void setUp() throws Exception
{    super.setUp();    conf = getConfiguration();    inputPath = getTestTempFilePath("documents/docs.file");    FileSystem fs = FileSystem.get(inputPath.toUri(), conf);    SequenceFile.Writer writer = new SequenceFile.Writer(fs, conf, inputPath, Text.class, Text.class);    RandomDocumentGenerator gen = new RandomDocumentGenerator();    for (int i = 0; i < NUM_DOCS; i++) {        writer.append(new Text("Document::ID::" + i), new Text(enhanceWithHighDFWords(gen.getRandomDocument())));    }    writer.close();}
0
private static String enhanceWithHighDFWords(String initialDoc)
{    StringBuilder sb = new StringBuilder(initialDoc);    for (String word : HIGH_DF_WORDS) {        sb.append(' ').append(word);    }    return sb.toString();}
0
public void testHighDFWordsPreserving() throws Exception
{    runTest(false);}
0
public void testHighDFWordsPruning() throws Exception
{    runTest(true);}
0
private void runTest(boolean prune) throws Exception
{    Path outputPath = getTestTempFilePath("output");    List<String> argList = Lists.newLinkedList();    argList.add("-i");    argList.add(inputPath.toString());    argList.add("-o");    argList.add(outputPath.toString());    if (prune) {        argList.add("-xs");                argList.add("3");    } else {        argList.add("--maxDFPercent");                argList.add("100");    }    argList.add("-seq");    argList.add("-nv");    String[] args = argList.toArray(new String[argList.size()]);    ToolRunner.run(conf, new SparseVectorsFromSequenceFiles(), args);    Path dictionary = new Path(outputPath, "dictionary.file-0");    Path tfVectors = new Path(outputPath, "tf-vectors");    Path tfidfVectors = new Path(outputPath, "tfidf-vectors");    int[] highDFWordsDictionaryIndices = getHighDFWordsDictionaryIndices(dictionary);    validateVectors(tfVectors, highDFWordsDictionaryIndices, prune);    validateVectors(tfidfVectors, highDFWordsDictionaryIndices, prune);}
0
private int[] getHighDFWordsDictionaryIndices(Path dictionaryPath)
{    int[] highDFWordsDictionaryIndices = new int[HIGH_DF_WORDS.length];    List<String> highDFWordsList = Arrays.asList(HIGH_DF_WORDS);    for (Pair<Text, IntWritable> record : new SequenceFileDirIterable<Text, IntWritable>(dictionaryPath, PathType.GLOB, null, null, true, conf)) {        int index = highDFWordsList.indexOf(record.getFirst().toString());        if (index > -1) {            highDFWordsDictionaryIndices[index] = record.getSecond().get();        }    }    return highDFWordsDictionaryIndices;}
0
private void validateVectors(Path vectorPath, int[] highDFWordsDictionaryIndices, boolean prune) throws Exception
{    assertTrue("Path does not exist", vectorPath.getFileSystem(conf).exists(vectorPath));    for (VectorWritable value : new SequenceFileDirValueIterable<VectorWritable>(vectorPath, PathType.LIST, PathFilters.partFilter(), null, true, conf)) {        Vector v = ((NamedVector) value.get()).getDelegate();        for (int i = 0; i < highDFWordsDictionaryIndices.length; i++) {            if (prune) {                assertEquals("Found vector for which word '" + HIGH_DF_WORDS[i] + "' is not pruned", 0.0, v.get(highDFWordsDictionaryIndices[i]), 0.0);            } else {                assertTrue("Found vector for which word '" + HIGH_DF_WORDS[i] + "' is pruned, and shouldn't have been", v.get(highDFWordsDictionaryIndices[i]) != 0.0);            }        }    }}
0
private char getRandomDelimiter()
{    return DELIM.charAt(random.nextInt(DELIM.length()));}
0
public String getRandomDocument()
{    int length = (AVG_DOCUMENT_LENGTH >> 1) + random.nextInt(AVG_DOCUMENT_LENGTH);    StringBuilder sb = new StringBuilder(length * AVG_SENTENCE_LENGTH * AVG_WORD_LENGTH);    for (int i = 0; i < length; i++) {        sb.append(getRandomSentence());    }    return sb.toString();}
0
public String getRandomSentence()
{    int length = (AVG_SENTENCE_LENGTH >> 1) + random.nextInt(AVG_SENTENCE_LENGTH);    StringBuilder sb = new StringBuilder(length * AVG_WORD_LENGTH);    for (int i = 0; i < length; i++) {        sb.append(getRandomString()).append(' ');    }    sb.append(getRandomDelimiter());    return sb.toString();}
0
public String getRandomString()
{    int length = (AVG_WORD_LENGTH >> 1) + random.nextInt(AVG_WORD_LENGTH);    StringBuilder sb = new StringBuilder(length);    for (int i = 0; i < length; i++) {        sb.append(CHARSET.charAt(random.nextInt(CHARSET.length())));    }    if (random.nextInt(10) == 0) {        sb.append(ERRORSET.charAt(random.nextInt(ERRORSET.length())));    }    return sb.toString();}
0
private void setupDocs() throws IOException
{    conf = getConfiguration();    inputPath = getTestTempFilePath("documents/docs.file");    FileSystem fs = FileSystem.get(inputPath.toUri(), conf);    SequenceFile.Writer writer = new SequenceFile.Writer(fs, conf, inputPath, Text.class, Text.class);    RandomDocumentGenerator gen = new RandomDocumentGenerator();    try {        for (int i = 0; i < NUM_DOCS; i++) {            writer.append(new Text("Document::ID::" + i), new Text(gen.getRandomDocument()));        }    } finally {        Closeables.close(writer, false);    }}
0
public void testCreateTermFrequencyVectors() throws Exception
{    setupDocs();    runTest(false, false, false, -1, NUM_DOCS);}
0
public void testCreateTermFrequencyVectorsNam() throws Exception
{    setupDocs();    runTest(false, false, true, -1, NUM_DOCS);}
0
public void testCreateTermFrequencyVectorsSeq() throws Exception
{    setupDocs();    runTest(false, true, false, -1, NUM_DOCS);}
0
public void testCreateTermFrequencyVectorsSeqNam() throws Exception
{    setupDocs();    runTest(false, true, true, -1, NUM_DOCS);}
0
public void testPruning() throws Exception
{    conf = getConfiguration();    inputPath = getTestTempFilePath("documents/docs.file");    FileSystem fs = FileSystem.get(inputPath.toUri(), conf);    SequenceFile.Writer writer = new SequenceFile.Writer(fs, conf, inputPath, Text.class, Text.class);    String[] docs = { "a b c", "a a a a a b", "a a a a a c" };    try {        for (int i = 0; i < docs.length; i++) {            writer.append(new Text("Document::ID::" + i), new Text(docs[i]));        }    } finally {        Closeables.close(writer, false);    }    Path outPath = runTest(false, false, false, 2, docs.length);    Path tfidfVectors = new Path(outPath, "tfidf-vectors");    int count = 0;    Vector[] res = new Vector[docs.length];    for (VectorWritable value : new SequenceFileDirValueIterable<VectorWritable>(tfidfVectors, PathType.LIST, PathFilters.partFilter(), null, true, conf)) {        Vector v = value.get();        System.out.println(v);        assertEquals(2, v.size());        res[count] = v;        count++;    }    assertEquals(docs.length, count);        assertEquals(2, res[0].getNumNondefaultElements());    assertEquals(1, res[1].getNumNondefaultElements());    assertEquals(1, res[2].getNumNondefaultElements());}
0
public void testPruningTF() throws Exception
{    conf = getConfiguration();    FileSystem fs = FileSystem.get(conf);    inputPath = getTestTempFilePath("documents/docs.file");    SequenceFile.Writer writer = new SequenceFile.Writer(fs, conf, inputPath, Text.class, Text.class);    String[] docs = { "a b c", "a a a a a b", "a a a a a c" };    try {        for (int i = 0; i < docs.length; i++) {            writer.append(new Text("Document::ID::" + i), new Text(docs[i]));        }    } finally {        Closeables.close(writer, false);    }    Path outPath = runTest(true, false, false, 2, docs.length);    Path tfVectors = new Path(outPath, "tf-vectors");    int count = 0;    Vector[] res = new Vector[docs.length];    for (VectorWritable value : new SequenceFileDirValueIterable<VectorWritable>(tfVectors, PathType.LIST, PathFilters.partFilter(), null, true, conf)) {        Vector v = value.get();        System.out.println(v);        assertEquals(2, v.size());        res[count] = v;        count++;    }    assertEquals(docs.length, count);        assertEquals(2, res[0].getNumNondefaultElements());    assertEquals(1, res[1].getNumNondefaultElements());    assertEquals(1, res[2].getNumNondefaultElements());}
0
private Path runTest(boolean tfWeighting, boolean sequential, boolean named, double maxDFSigma, int numDocs) throws Exception
{    Path outputPath = getTestTempFilePath("output");    List<String> argList = Lists.newLinkedList();    argList.add("-i");    argList.add(inputPath.toString());    argList.add("-o");    argList.add(outputPath.toString());    if (sequential) {        argList.add("-seq");    }    if (named) {        argList.add("-nv");    }    if (maxDFSigma >= 0) {        argList.add("--maxDFSigma");        argList.add(String.valueOf(maxDFSigma));    }    if (tfWeighting) {        argList.add("--weight");        argList.add("tf");    }    String[] args = argList.toArray(new String[argList.size()]);    ToolRunner.run(getConfiguration(), new SparseVectorsFromSequenceFiles(), args);    Path tfVectors = new Path(outputPath, "tf-vectors");    Path tfidfVectors = new Path(outputPath, "tfidf-vectors");    DictionaryVectorizerTest.validateVectors(conf, numDocs, tfVectors, sequential, named);    if (!tfWeighting) {        DictionaryVectorizerTest.validateVectors(conf, numDocs, tfidfVectors, sequential, named);    }    return outputPath;}
0
public List<RecommendedItem> recommend(long userID, int howMany) throws TasteException
{    return recommender.recommend(userID, howMany);}
0
public List<RecommendedItem> recommend(long userID, int howMany, boolean includeKnownItems) throws TasteException
{    return recommend(userID, howMany, null, includeKnownItems);}
0
public List<RecommendedItem> recommend(long userID, int howMany, IDRescorer rescorer) throws TasteException
{    return recommender.recommend(userID, howMany, rescorer, false);}
0
public List<RecommendedItem> recommend(long userID, int howMany, IDRescorer rescorer, boolean includeKnownItems) throws TasteException
{    return recommender.recommend(userID, howMany, rescorer, includeKnownItems);}
0
public float estimatePreference(long userID, long itemID) throws TasteException
{    return recommender.estimatePreference(userID, itemID);}
0
public void setPreference(long userID, long itemID, float value) throws TasteException
{    recommender.setPreference(userID, itemID, value);}
0
public void removePreference(long userID, long itemID) throws TasteException
{    recommender.removePreference(userID, itemID);}
0
public DataModel getDataModel()
{    return recommender.getDataModel();}
0
public void refresh(Collection<Refreshable> alreadyRefreshed)
{    recommender.refresh(alreadyRefreshed);}
0
public String toString()
{    return "BookCrossingBooleanRecommender[recommender:" + recommender + ']';}
0
public Recommender buildRecommender(DataModel dataModel) throws TasteException
{    return new BookCrossingBooleanRecommender(dataModel);}
0
public static void main(String... args) throws IOException, TasteException, OptionException
{    RecommenderIRStatsEvaluator evaluator = new GenericRecommenderIRStatsEvaluator();    File ratingsFile = TasteOptionParser.getRatings(args);    DataModel model = ratingsFile == null ? new BookCrossingDataModel(true) : new BookCrossingDataModel(ratingsFile, true);    IRStatistics evaluation = evaluator.evaluate(new BookCrossingBooleanRecommenderBuilder(), new BookCrossingDataModelBuilder(), model, null, 3, Double.NEGATIVE_INFINITY, 1.0);    }
1
private static File convertBCFile(File originalFile, boolean ignoreRatings) throws IOException
{    if (!originalFile.exists()) {        throw new FileNotFoundException(originalFile.toString());    }    File resultFile = new File(new File(System.getProperty("java.io.tmpdir")), "taste.bookcrossing.txt");    resultFile.delete();    Writer writer = null;    try {        writer = new OutputStreamWriter(new FileOutputStream(resultFile), Charsets.UTF_8);        for (String line : new FileLineIterable(originalFile, true)) {                        if (line.endsWith("\"0\"")) {                continue;            }                        String convertedLine = NON_DIGIT_SEMICOLON_PATTERN.matcher(line).replaceAll("").replace(';', ',');                        if (convertedLine.contains(",,")) {                continue;            }            if (ignoreRatings) {                                convertedLine = convertedLine.substring(0, convertedLine.lastIndexOf(','));            }            writer.write(convertedLine);            writer.write('\n');        }        writer.flush();    } catch (IOException ioe) {        resultFile.delete();        throw ioe;    } finally {        Closeables.close(writer, false);    }    return resultFile;}
0
public String toString()
{    return "BookCrossingDataModel";}
0
public DataModel buildDataModel(FastByIDMap<PreferenceArray> trainingData)
{    return new GenericBooleanPrefDataModel(GenericBooleanPrefDataModel.toDataMap(trainingData));}
0
public List<RecommendedItem> recommend(long userID, int howMany) throws TasteException
{    return recommender.recommend(userID, howMany);}
0
public List<RecommendedItem> recommend(long userID, int howMany, boolean includeKnownItems) throws TasteException
{    return recommend(userID, howMany, null, includeKnownItems);}
0
public List<RecommendedItem> recommend(long userID, int howMany, IDRescorer rescorer) throws TasteException
{    return recommender.recommend(userID, howMany, rescorer, false);}
0
public List<RecommendedItem> recommend(long userID, int howMany, IDRescorer rescorer, boolean includeKnownItems) throws TasteException
{    return recommender.recommend(userID, howMany, rescorer, false);}
0
public float estimatePreference(long userID, long itemID) throws TasteException
{    return recommender.estimatePreference(userID, itemID);}
0
public void setPreference(long userID, long itemID, float value) throws TasteException
{    recommender.setPreference(userID, itemID, value);}
0
public void removePreference(long userID, long itemID) throws TasteException
{    recommender.removePreference(userID, itemID);}
0
public DataModel getDataModel()
{    return recommender.getDataModel();}
0
public void refresh(Collection<Refreshable> alreadyRefreshed)
{    recommender.refresh(alreadyRefreshed);}
0
public String toString()
{    return "BookCrossingRecommender[recommender:" + recommender + ']';}
0
public Recommender buildRecommender(DataModel dataModel) throws TasteException
{    return new BookCrossingRecommender(dataModel);}
0
public static void main(String... args) throws IOException, TasteException, OptionException
{    RecommenderEvaluator evaluator = new AverageAbsoluteDifferenceRecommenderEvaluator();    File ratingsFile = TasteOptionParser.getRatings(args);    DataModel model = ratingsFile == null ? new BookCrossingDataModel(false) : new BookCrossingDataModel(ratingsFile, false);    double evaluation = evaluator.evaluate(new BookCrossingRecommenderBuilder(), null, model, 0.9, 0.3);    }
1
public static String cleanUpEmailAddress(CharSequence address)
{        return ADDRESS_CLEANUP.matcher(address).replaceAll("");}
0
public static void loadDictionaries(Configuration conf, String fromPrefix, OpenObjectIntHashMap<String> fromDictionary, String msgIdPrefix, OpenObjectIntHashMap<String> msgIdDictionary) throws IOException
{    Path[] localFiles = HadoopUtil.getCachedFiles(conf);    FileSystem fs = FileSystem.getLocal(conf);    for (Path dictionaryFile : localFiles) {                OpenObjectIntHashMap<String> dictionary = null;        if (dictionaryFile.getName().startsWith(fromPrefix)) {            dictionary = fromDictionary;        } else if (dictionaryFile.getName().startsWith(msgIdPrefix)) {            dictionary = msgIdDictionary;        }        if (dictionary != null) {            dictionaryFile = fs.makeQualified(dictionaryFile);            for (Pair<Writable, IntWritable> record : new SequenceFileIterable<Writable, IntWritable>(dictionaryFile, true, conf)) {                dictionary.put(record.getFirst().toString(), record.getSecond().get());            }        }    }}
0
public static String[] parseReferences(CharSequence rawRefs)
{    String[] splits;    if (rawRefs != null && rawRefs.length() > 0) {        splits = SPACE_OR_CLOSE_ANGLE.split(rawRefs);        for (int i = 0; i < splits.length; i++) {            splits[i] = ANGLE_BRACES.matcher(splits[i]).replaceAll("");        }    } else {        splits = EMPTY;    }    return splits;}
0
protected void setup(Context context) throws IOException, InterruptedException
{    super.setup(context);    separator = context.getConfiguration().get(EmailUtility.SEPARATOR);}
0
protected void map(Text key, Text value, Context context) throws IOException, InterruptedException
{        String valStr = value.toString();    int idx = valStr.indexOf(separator);    if (idx == -1) {        context.getCounter(EmailUtility.Counters.NO_FROM_ADDRESS).increment(1);    } else {        String full = valStr.substring(0, idx);                                full = EmailUtility.cleanUpEmailAddress(full);        if (EmailUtility.WHITESPACE.matcher(full).matches()) {            context.getCounter(EmailUtility.Counters.NO_FROM_ADDRESS).increment(1);        } else {            context.write(new Text(full), new VarIntWritable(1));        }    }}
0
protected void reduce(Text key, Iterable<VarIntWritable> values, Context context) throws IOException, InterruptedException
{    int sum = 0;    for (VarIntWritable value : values) {        sum += value.get();    }    context.write(new Text(key), new VarIntWritable(sum));}
0
public static void main(String[] args) throws Exception
{    ToolRunner.run(new Configuration(), new MailToPrefsDriver(), args);}
0
public int run(String[] args) throws Exception
{    addInputOption();    addOutputOption();    addOption(DefaultOptionCreator.overwriteOption().create());    addOption("chunkSize", "cs", "The size of chunks to write.  Default is 100 mb", "100");    addOption("separator", "sep", "The separator used in the input file to separate to, from, subject.  Default is \\n", "\n");    addOption("from", "f", "The position in the input text (value) where the from email is located, starting from " + "zero (0).", "0");    addOption("refs", "r", "The position in the input text (value) where the reference ids are located, " + "starting from zero (0).", "1");    addOption(buildOption("useCounts", "u", "If set, then use the number of times the user has interacted with a " + "thread as an indication of their preference.  Otherwise, use boolean preferences.", false, false, String.valueOf(true)));    Map<String, List<String>> parsedArgs = parseArguments(args);    Path input = getInputPath();    Path output = getOutputPath();    int chunkSize = Integer.parseInt(getOption("chunkSize"));    String separator = getOption("separator");    Configuration conf = getConf();    boolean useCounts = hasOption("useCounts");    AtomicInteger currentPhase = new AtomicInteger();    int[] msgDim = new int[1];        List<Path> msgIdChunks = null;    boolean overwrite = hasOption(DefaultOptionCreator.OVERWRITE_OPTION);        if (shouldRunNextPhase(parsedArgs, currentPhase)) {                        Path msgIdsPath = new Path(output, "msgIds");        if (overwrite) {            HadoopUtil.delete(conf, msgIdsPath);        }                Job createMsgIdDictionary = prepareJob(input, msgIdsPath, SequenceFileInputFormat.class, MsgIdToDictionaryMapper.class, Text.class, VarIntWritable.class, MailToDictionaryReducer.class, Text.class, VarIntWritable.class, SequenceFileOutputFormat.class);        boolean succeeded = createMsgIdDictionary.waitForCompletion(true);        if (!succeeded) {            return -1;        }                msgIdChunks = createDictionaryChunks(msgIdsPath, output, "msgIds-dictionary-", createMsgIdDictionary.getConfiguration(), chunkSize, msgDim);    }        List<Path> fromChunks = null;    if (shouldRunNextPhase(parsedArgs, currentPhase)) {        Path fromIdsPath = new Path(output, "fromIds");        if (overwrite) {            HadoopUtil.delete(conf, fromIdsPath);        }                Job createFromIdDictionary = prepareJob(input, fromIdsPath, SequenceFileInputFormat.class, FromEmailToDictionaryMapper.class, Text.class, VarIntWritable.class, MailToDictionaryReducer.class, Text.class, VarIntWritable.class, SequenceFileOutputFormat.class);        createFromIdDictionary.getConfiguration().set(EmailUtility.SEPARATOR, separator);        boolean succeeded = createFromIdDictionary.waitForCompletion(true);        if (!succeeded) {            return -1;        }                int[] fromDim = new int[1];        fromChunks = createDictionaryChunks(fromIdsPath, output, "fromIds-dictionary-", createFromIdDictionary.getConfiguration(), chunkSize, fromDim);    }        if (shouldRunNextPhase(parsedArgs, currentPhase) && fromChunks != null && msgIdChunks != null) {                                        Path vecPath = new Path(output, "recInput");        if (overwrite) {            HadoopUtil.delete(conf, vecPath);        }                conf.set(EmailUtility.MSG_ID_DIMENSION, String.valueOf(msgDim[0]));        conf.set(EmailUtility.FROM_PREFIX, "fromIds-dictionary-");        conf.set(EmailUtility.MSG_IDS_PREFIX, "msgIds-dictionary-");        conf.set(EmailUtility.FROM_INDEX, getOption("from"));        conf.set(EmailUtility.REFS_INDEX, getOption("refs"));        conf.set(EmailUtility.SEPARATOR, separator);        conf.set(MailToRecReducer.USE_COUNTS_PREFERENCE, String.valueOf(useCounts));        int j = 0;        int i = 0;        for (Path fromChunk : fromChunks) {            for (Path idChunk : msgIdChunks) {                Path out = new Path(vecPath, "tmp-" + i + '-' + j);                DistributedCache.setCacheFiles(new URI[] { fromChunk.toUri(), idChunk.toUri() }, conf);                Job createRecMatrix = prepareJob(input, out, SequenceFileInputFormat.class, MailToRecMapper.class, Text.class, LongWritable.class, MailToRecReducer.class, Text.class, NullWritable.class, TextOutputFormat.class);                createRecMatrix.getConfiguration().set("mapred.output.compress", "false");                boolean succeeded = createRecMatrix.waitForCompletion(true);                if (!succeeded) {                    return -1;                }                                                                FileStatus[] fs = HadoopUtil.getFileStatus(new Path(out, "*"), PathType.GLOB, PathFilters.partFilter(), null, conf);                for (int k = 0; k < fs.length; k++) {                    FileStatus f = fs[k];                    Path outPath = new Path(vecPath, "chunk-" + i + '-' + j + '-' + k);                    FileUtil.copy(f.getPath().getFileSystem(conf), f.getPath(), outPath.getFileSystem(conf), outPath, true, overwrite, conf);                }                HadoopUtil.delete(conf, out);                j++;            }            i++;        }        /*Path mergePath = new Path(output, "vectors.dat");      if (overwrite) {        HadoopUtil.delete(conf, mergePath);      }      */            }    return 0;}
1
private static List<Path> createDictionaryChunks(Path inputPath, Path dictionaryPathBase, String name, Configuration baseConf, int chunkSizeInMegabytes, int[] maxTermDimension) throws IOException
{    List<Path> chunkPaths = new ArrayList<>();    Configuration conf = new Configuration(baseConf);    FileSystem fs = FileSystem.get(inputPath.toUri(), conf);    long chunkSizeLimit = chunkSizeInMegabytes * 1024L * 1024L;    int chunkIndex = 0;    Path chunkPath = new Path(dictionaryPathBase, name + chunkIndex);    chunkPaths.add(chunkPath);    SequenceFile.Writer dictWriter = new SequenceFile.Writer(fs, conf, chunkPath, Text.class, IntWritable.class);    try {        long currentChunkSize = 0;        Path filesPattern = new Path(inputPath, OUTPUT_FILES_PATTERN);                int i = 1;        for (Pair<Writable, Writable> record : new SequenceFileDirIterable<>(filesPattern, PathType.GLOB, null, null, true, conf)) {            if (currentChunkSize > chunkSizeLimit) {                Closeables.close(dictWriter, false);                chunkIndex++;                chunkPath = new Path(dictionaryPathBase, name + chunkIndex);                chunkPaths.add(chunkPath);                dictWriter = new SequenceFile.Writer(fs, conf, chunkPath, Text.class, IntWritable.class);                currentChunkSize = 0;            }            Writable key = record.getFirst();            int fieldSize = DICTIONARY_BYTE_OVERHEAD + key.toString().length() * 2 + Integer.SIZE / 8;            currentChunkSize += fieldSize;            dictWriter.append(key, new IntWritable(i++));        }        maxTermDimension[0] = i;    } finally {        Closeables.close(dictWriter, false);    }    return chunkPaths;}
0
protected void setup(Context context) throws IOException, InterruptedException
{    super.setup(context);    Configuration conf = context.getConfiguration();    String fromPrefix = conf.get(EmailUtility.FROM_PREFIX);    String msgPrefix = conf.get(EmailUtility.MSG_IDS_PREFIX);    fromIdx = conf.getInt(EmailUtility.FROM_INDEX, 0);    refsIdx = conf.getInt(EmailUtility.REFS_INDEX, 1);    EmailUtility.loadDictionaries(conf, fromPrefix, fromDictionary, msgPrefix, msgIdDictionary);        separator = context.getConfiguration().get(EmailUtility.SEPARATOR);}
1
protected void map(Text key, Text value, Context context) throws IOException, InterruptedException
{    int msgIdKey = Integer.MIN_VALUE;    int fromKey = Integer.MIN_VALUE;    String valStr = value.toString();    String[] splits = StringUtils.splitByWholeSeparatorPreserveAllTokens(valStr, separator);    if (splits != null && splits.length > 0) {        if (splits.length > refsIdx) {            String from = EmailUtility.cleanUpEmailAddress(splits[fromIdx]);            fromKey = fromDictionary.get(from);        }                if (splits.length > refsIdx) {            String[] theRefs = EmailUtility.parseReferences(splits[refsIdx]);            if (theRefs != null && theRefs.length > 0) {                                msgIdKey = msgIdDictionary.get(theRefs[0]);                context.getCounter(Counters.REFERENCE).increment(1);            }        }    }        if (msgIdKey == Integer.MIN_VALUE) {                String keyStr = key.toString();        int idx = keyStr.lastIndexOf('/');        if (idx != -1) {            String msgId = keyStr.substring(idx + 1);            msgIdKey = msgIdDictionary.get(msgId);            context.getCounter(Counters.ORIGINAL).increment(1);        }    }    if (msgIdKey != Integer.MIN_VALUE && fromKey != Integer.MIN_VALUE) {        context.write(new Text(fromKey + "," + msgIdKey), new LongWritable(1));    }}
0
protected void setup(Context context) throws IOException, InterruptedException
{    useCounts = context.getConfiguration().getBoolean(USE_COUNTS_PREFERENCE, true);}
0
protected void reduce(Text key, Iterable<LongWritable> values, Context context) throws IOException, InterruptedException
{    if (useCounts) {        long sum = 0;        for (LongWritable value : values) {            sum++;        }        context.write(new Text(key.toString() + ',' + sum), null);    } else {        context.write(new Text(key.toString()), null);    }}
0
protected void map(Text key, Text value, Context context) throws IOException, InterruptedException
{        String keyStr = key.toString();        int idx = keyStr.lastIndexOf('@');    if (idx == -1) {        context.getCounter(EmailUtility.Counters.NO_MESSAGE_ID).increment(1);    } else {                idx = keyStr.lastIndexOf('/', idx);        String msgId = keyStr.substring(idx + 1);        if (EmailUtility.WHITESPACE.matcher(msgId).matches()) {            context.getCounter(EmailUtility.Counters.NO_MESSAGE_ID).increment(1);        } else {            context.write(new Text(msgId), new VarIntWritable(1));        }    }}
0
public Iterator<Pair<PreferenceArray, long[]>> iterator()
{    try {        return new DataFileIterator(dataFile);    } catch (IOException ioe) {        throw new IllegalStateException(ioe);    }}
0
protected Pair<PreferenceArray, long[]> computeNext()
{    if (!lineIterator.hasNext()) {        return endOfData();    }    String line = lineIterator.next();        String[] tokens = PIPE_PATTERN.split(line);    long userID = Long.parseLong(tokens[0]);    int ratingsLeftToRead = Integer.parseInt(tokens[1]);    int ratingsRead = 0;    PreferenceArray currentUserPrefs = new GenericUserPreferenceArray(ratingsLeftToRead);    long[] timestamps = new long[ratingsLeftToRead];    while (ratingsLeftToRead > 0) {        line = lineIterator.next();                                tokens = TAB_PATTERN.split(line);        boolean hasPref = tokens.length == 2 || tokens.length == 4;        boolean hasDate = tokens.length > 2;        long itemID = Long.parseLong(tokens[0]);        currentUserPrefs.setUserID(0, userID);        currentUserPrefs.setItemID(ratingsRead, itemID);        if (hasPref) {            float preference = Float.parseFloat(tokens[1]);            currentUserPrefs.setValue(ratingsRead, preference);        }        if (hasDate) {            long timestamp;            if (hasPref) {                timestamp = parseFakeTimestamp(tokens[2], tokens[3]);            } else {                timestamp = parseFakeTimestamp(tokens[1], tokens[2]);            }            timestamps[ratingsRead] = timestamp;        }        ratingsRead++;        ratingsLeftToRead--;    }    return new Pair<>(currentUserPrefs, timestamps);}
0
public void skip(int n)
{    for (int i = 0; i < n; i++) {        if (lineIterator.hasNext()) {            String line = lineIterator.next();                        String[] tokens = PIPE_PATTERN.split(line);            int linesToSKip = Integer.parseInt(tokens[1]);            lineIterator.skip(linesToSKip);        } else {            break;        }    }}
0
public void close()
{    endOfData();    try {        Closeables.close(lineIterator, true);    } catch (IOException e) {            }}
1
private static long parseFakeTimestamp(String dateString, CharSequence timeString)
{    int days = Integer.parseInt(dateString);    String[] timeTokens = COLON_PATTERN.split(timeString);    int hours = Integer.parseInt(timeTokens[0]);    int minutes = Integer.parseInt(timeTokens[1]);    int seconds = Integer.parseInt(timeTokens[2]);    return 86400L * days + 3600L + hours + 60L * minutes + seconds;}
0
public File getDataFileDirectory()
{    return dataFileDirectory;}
0
public static File getTrainingFile(File dataFileDirectory)
{    return getFile(dataFileDirectory, "trainIdx");}
0
public static File getValidationFile(File dataFileDirectory)
{    return getFile(dataFileDirectory, "validationIdx");}
0
public static File getTestFile(File dataFileDirectory)
{    return getFile(dataFileDirectory, "testIdx");}
0
public static File getTrackFile(File dataFileDirectory)
{    return getFile(dataFileDirectory, "trackData");}
0
private static File getFile(File dataFileDirectory, String prefix)
{        for (int set : new int[] { 1, 2 }) {                for (String firstLinesOrNot : new String[] { "", ".firstLines" }) {            for (String gzippedOrNot : new String[] { ".gz", "" }) {                File dataFile = new File(dataFileDirectory, prefix + set + firstLinesOrNot + ".txt" + gzippedOrNot);                if (dataFile.exists()) {                    return dataFile;                }            }        }    }    throw new IllegalArgumentException("Can't find " + prefix + " file in " + dataFileDirectory);}
0
public LongPrimitiveIterator getUserIDs() throws TasteException
{    return delegate.getUserIDs();}
0
public PreferenceArray getPreferencesFromUser(long userID) throws TasteException
{    return delegate.getPreferencesFromUser(userID);}
0
public FastIDSet getItemIDsFromUser(long userID) throws TasteException
{    return delegate.getItemIDsFromUser(userID);}
0
public LongPrimitiveIterator getItemIDs() throws TasteException
{    return delegate.getItemIDs();}
0
public PreferenceArray getPreferencesForItem(long itemID) throws TasteException
{    return delegate.getPreferencesForItem(itemID);}
0
public Float getPreferenceValue(long userID, long itemID) throws TasteException
{    return delegate.getPreferenceValue(userID, itemID);}
0
public Long getPreferenceTime(long userID, long itemID) throws TasteException
{    return delegate.getPreferenceTime(userID, itemID);}
0
public int getNumItems() throws TasteException
{    return delegate.getNumItems();}
0
public int getNumUsers() throws TasteException
{    return delegate.getNumUsers();}
0
public int getNumUsersWithPreferenceFor(long itemID) throws TasteException
{    return delegate.getNumUsersWithPreferenceFor(itemID);}
0
public int getNumUsersWithPreferenceFor(long itemID1, long itemID2) throws TasteException
{    return delegate.getNumUsersWithPreferenceFor(itemID1, itemID2);}
0
public void setPreference(long userID, long itemID, float value) throws TasteException
{    delegate.setPreference(userID, itemID, value);}
0
public void removePreference(long userID, long itemID) throws TasteException
{    delegate.removePreference(userID, itemID);}
0
public boolean hasPreferenceValues()
{    return delegate.hasPreferenceValues();}
0
public float getMaxPreference()
{    return 100.0f;}
0
public float getMinPreference()
{    return 0.0f;}
0
public void refresh(Collection<Refreshable> alreadyRefreshed)
{}
0
public static void main(String[] args) throws Exception
{    File inputFile = new File(args[0]);    File outputFile = new File(args[1]);    int columnsToOutput = 4;    if (args.length >= 3) {        columnsToOutput = Integer.parseInt(args[2]);    }    OutputStream outStream = new GZIPOutputStream(new FileOutputStream(outputFile));    try (Writer outWriter = new BufferedWriter(new OutputStreamWriter(outStream, Charsets.UTF_8))) {        for (Pair<PreferenceArray, long[]> user : new DataFileIterable(inputFile)) {            PreferenceArray prefs = user.getFirst();            long[] timestamps = user.getSecond();            for (int i = 0; i < prefs.length(); i++) {                outWriter.write(String.valueOf(prefs.getUserID(i)));                outWriter.write(',');                outWriter.write(String.valueOf(prefs.getItemID(i)));                if (columnsToOutput > 2) {                    outWriter.write(',');                    outWriter.write(String.valueOf(prefs.getValue(i)));                }                if (columnsToOutput > 3) {                    outWriter.write(',');                    outWriter.write(String.valueOf(timestamps[i]));                }                outWriter.write('\n');            }        }    }}
0
public static byte convert(double estimate, long userID, long itemID)
{    if (Double.isNaN(estimate)) {                return 0x7F;    } else {        int scaledEstimate = (int) (estimate * 2.55);        if (scaledEstimate > 255) {            scaledEstimate = 255;        } else if (scaledEstimate < 0) {            scaledEstimate = 0;        }        return (byte) scaledEstimate;    }}
1
public LongPrimitiveIterator getUserIDs()
{    return userIDs.iterator();}
0
public LongPrimitiveIterator getItemIDs()
{    return itemIDs.iterator();}
0
public Iterable<Preference> getPreferences()
{    return preferences;}
0
public float getMinPreference()
{    return minPreference;}
0
public float getMaxPreference()
{    return maxPreference;}
0
public int numUsers()
{    return userIDs.size();}
0
public int numItems()
{    return itemIDs.size();}
0
public int numPreferences()
{    return preferences.size();}
0
public LongPrimitiveIterator getUserIDs()
{    return new FixedSizeLongIterator(numUsers());}
0
public LongPrimitiveIterator getItemIDs()
{    return new FixedSizeLongIterator(numItems());}
0
public Iterable<Preference> getPreferences()
{    Iterable<Iterable<Preference>> prefIterators = Iterables.transform(new DataFileIterable(dataFile), new Function<Pair<PreferenceArray, long[]>, Iterable<Preference>>() {        @Override        public Iterable<Preference> apply(Pair<PreferenceArray, long[]> from) {            return from.getFirst();        }    });    return Iterables.concat(prefIterators);}
0
public Iterable<Preference> apply(Pair<PreferenceArray, long[]> from)
{    return from.getFirst();}
0
public float getMinPreference()
{    return 0;}
0
public float getMaxPreference()
{    return 100;}
0
public int numUsers()
{    return 1000990;}
0
public int numItems()
{    return 624961;}
0
public int numPreferences()
{    return 252800275;}
0
public long nextLong()
{    return currentValue++;}
0
public long peek()
{    return currentValue;}
0
public void skip(int n)
{    currentValue += n;}
0
public boolean hasNext()
{    return currentValue < maximum;}
0
public void remove()
{    throw new UnsupportedOperationException();}
0
public Factorization factorize() throws TasteException
{    for (int feature = 0; feature < numFeatures; feature++) {                shufflePreferences();                for (int currentIteration = 0; currentIteration < numIterations; currentIteration++) {            if (currentIteration == numIterations - 1) {                double rmse = trainingIterationWithRmse(feature);                            } else {                trainingIteration(feature);            }        }        if (feature < numFeatures - 1) {                        for (int index = 0; index < userIndexes.length; index++) {                cachedEstimates[index] = estimate(userIndexes[index], itemIndexes[index], feature, cachedEstimates[index], false);            }        }    }        return new Factorization(userIDMapping, itemIDMapping, userFeatures, itemFeatures);}
1
private void trainingIteration(int feature)
{    for (int index = 0; index < userIndexes.length; index++) {        train(userIndexes[index], itemIndexes[index], feature, values[index], cachedEstimates[index]);    }}
0
private double trainingIterationWithRmse(int feature)
{    double rmse = 0.0;    for (int index = 0; index < userIndexes.length; index++) {        double error = train(userIndexes[index], itemIndexes[index], feature, values[index], cachedEstimates[index]);        rmse += error * error;    }    return Math.sqrt(rmse / userIndexes.length);}
0
private double estimate(int userIndex, int itemIndex, int feature, double cachedEstimate, boolean trailing)
{    double sum = cachedEstimate;    sum += userFeatures[userIndex][feature] * itemFeatures[itemIndex][feature];    if (trailing) {        sum += (numFeatures - feature - 1) * (defaultValue + interval) * (defaultValue + interval);        if (sum > maxPreference) {            sum = maxPreference;        } else if (sum < minPreference) {            sum = minPreference;        }    }    return sum;}
0
public double train(int userIndex, int itemIndex, int feature, double original, double cachedEstimate)
{    double error = original - estimate(userIndex, itemIndex, feature, cachedEstimate, true);    double[] userVector = userFeatures[userIndex];    double[] itemVector = itemFeatures[itemIndex];    userVector[feature] += learningRate * (error * itemVector[feature] - preventOverfitting * userVector[feature]);    itemVector[feature] += learningRate * (error * userVector[feature] - preventOverfitting * itemVector[feature]);    return error;}
0
protected void shufflePreferences()
{    /* Durstenfeld shuffle */    for (int currentPos = userIndexes.length - 1; currentPos > 0; currentPos--) {        int swapPos = random.nextInt(currentPos + 1);        swapPreferences(currentPos, swapPos);    }}
0
private void swapPreferences(int posA, int posB)
{    int tmpUserIndex = userIndexes[posA];    int tmpItemIndex = itemIndexes[posA];    float tmpValue = values[posA];    double tmpEstimate = cachedEstimates[posA];    userIndexes[posA] = userIndexes[posB];    itemIndexes[posA] = itemIndexes[posB];    values[posA] = values[posB];    cachedEstimates[posA] = cachedEstimates[posB];    userIndexes[posB] = tmpUserIndex;    itemIndexes[posB] = tmpItemIndex;    values[posB] = tmpValue;    cachedEstimates[posB] = tmpEstimate;}
0
public void refresh(Collection<Refreshable> alreadyRefreshed)
{}
0
public static void main(String[] args) throws Exception
{    if (args.length != 2) {        System.err.println("Necessary arguments: <kddDataFileDirectory> <resultFile>");        return;    }    File dataFileDirectory = new File(args[0]);    if (!dataFileDirectory.exists() || !dataFileDirectory.isDirectory()) {        throw new IllegalArgumentException("Bad data file directory: " + dataFileDirectory);    }    File resultFile = new File(args[1]);    /* the knobs to turn */    int numFeatures = 20;    int numIterations = 5;    double learningRate = 0.0001;    double preventOverfitting = 0.002;    double randomNoise = 0.0001;    KDDCupFactorizablePreferences factorizablePreferences = new KDDCupFactorizablePreferences(KDDCupDataModel.getTrainingFile(dataFileDirectory));    Factorizer sgdFactorizer = new ParallelArraysSGDFactorizer(factorizablePreferences, numFeatures, numIterations, learningRate, preventOverfitting, randomNoise);    Factorization factorization = sgdFactorizer.factorize();        int prefsProcessed = 0;    RunningAverage average = new FullRunningAverage();    for (Pair<PreferenceArray, long[]> validationPair : new DataFileIterable(KDDCupDataModel.getValidationFile(dataFileDirectory))) {        for (Preference validationPref : validationPair.getFirst()) {            double estimate = estimatePreference(factorization, validationPref.getUserID(), validationPref.getItemID(), factorizablePreferences.getMinPreference(), factorizablePreferences.getMaxPreference());            double error = validationPref.getValue() - estimate;            average.addDatum(error * error);            prefsProcessed++;            if (prefsProcessed % 100000 == 0) {                            }        }    }        double rmse = Math.sqrt(average.getAverage());            OutputStream out = null;    try {        out = new BufferedOutputStream(new FileOutputStream(resultFile));        for (Pair<PreferenceArray, long[]> testPair : new DataFileIterable(KDDCupDataModel.getTestFile(dataFileDirectory))) {            for (Preference testPref : testPair.getFirst()) {                double estimate = estimatePreference(factorization, testPref.getUserID(), testPref.getItemID(), factorizablePreferences.getMinPreference(), factorizablePreferences.getMaxPreference());                byte result = EstimateConverter.convert(estimate, testPref.getUserID(), testPref.getItemID());                out.write(result);            }        }    } finally {        Closeables.close(out, false);    }    }
1
 static double estimatePreference(Factorization factorization, long userID, long itemID, float minPreference, float maxPreference) throws NoSuchUserException, NoSuchItemException
{    double[] userFeatures = factorization.getUserFeatures(userID);    double[] itemFeatures = factorization.getItemFeatures(itemID);    double estimate = 0;    for (int feature = 0; feature < userFeatures.length; feature++) {        estimate += userFeatures[feature] * itemFeatures[feature];    }    if (estimate < minPreference) {        estimate = minPreference;    } else if (estimate > maxPreference) {        estimate = maxPreference;    }    return estimate;}
0
public List<RecommendedItem> recommend(long userID, int howMany) throws TasteException
{    return recommender.recommend(userID, howMany);}
0
public List<RecommendedItem> recommend(long userID, int howMany, boolean includeKnownItems) throws TasteException
{    return recommend(userID, howMany, null, includeKnownItems);}
0
public List<RecommendedItem> recommend(long userID, int howMany, IDRescorer rescorer) throws TasteException
{    return recommender.recommend(userID, howMany, rescorer, false);}
0
public List<RecommendedItem> recommend(long userID, int howMany, IDRescorer rescorer, boolean includeKnownItems) throws TasteException
{    return recommender.recommend(userID, howMany, rescorer, includeKnownItems);}
0
public float estimatePreference(long userID, long itemID) throws TasteException
{    return recommender.estimatePreference(userID, itemID);}
0
public void setPreference(long userID, long itemID, float value) throws TasteException
{    recommender.setPreference(userID, itemID, value);}
0
public void removePreference(long userID, long itemID) throws TasteException
{    recommender.removePreference(userID, itemID);}
0
public DataModel getDataModel()
{    return recommender.getDataModel();}
0
public void refresh(Collection<Refreshable> alreadyRefreshed)
{    recommender.refresh(alreadyRefreshed);}
0
public String toString()
{    return "Track1Recommender[recommender:" + recommender + ']';}
0
public Recommender buildRecommender(DataModel dataModel) throws TasteException
{    return new Track1Recommender(dataModel);}
0
public double evaluate(RecommenderBuilder recommenderBuilder, DataModelBuilder dataModelBuilder, DataModel dataModel, double trainingPercentage, double evaluationPercentage) throws TasteException
{    Recommender recommender = recommenderBuilder.buildRecommender(dataModel);    Collection<Callable<Void>> estimateCallables = Lists.newArrayList();    AtomicInteger noEstimateCounter = new AtomicInteger();    for (Pair<PreferenceArray, long[]> userData : new DataFileIterable(KDDCupDataModel.getValidationFile(dataFileDirectory))) {        PreferenceArray validationPrefs = userData.getFirst();        long userID = validationPrefs.get(0).getUserID();        estimateCallables.add(new PreferenceEstimateCallable(recommender, userID, validationPrefs, noEstimateCounter));    }    RunningAverageAndStdDev timing = new FullRunningAverageAndStdDev();    execute(estimateCallables, noEstimateCounter, timing);    double result = computeFinalEvaluation();        return result;}
1
protected void reset()
{    average = new FullRunningAverage();}
0
protected void processOneEstimate(float estimatedPreference, Preference realPref)
{    double diff = realPref.getValue() - estimatedPreference;    average.addDatum(diff * diff);}
0
protected double computeFinalEvaluation()
{    return Math.sqrt(average.getAverage());}
0
public static void main(String... args) throws IOException, TasteException, OptionException
{    File dataFileDirectory = TasteOptionParser.getRatings(args);    if (dataFileDirectory == null) {        throw new IllegalArgumentException("No data directory");    }    if (!dataFileDirectory.exists() || !dataFileDirectory.isDirectory()) {        throw new IllegalArgumentException("Bad data file directory: " + dataFileDirectory);    }    Track1RecommenderEvaluator evaluator = new Track1RecommenderEvaluator(dataFileDirectory);    DataModel model = new KDDCupDataModel(KDDCupDataModel.getTrainingFile(dataFileDirectory));    double evaluation = evaluator.evaluate(new Track1RecommenderBuilder(), null, model, Float.NaN, Float.NaN);    }
1
public static void main(String[] args) throws Exception
{    File dataFileDirectory = new File(args[0]);    if (!dataFileDirectory.exists() || !dataFileDirectory.isDirectory()) {        throw new IllegalArgumentException("Bad data file directory: " + dataFileDirectory);    }    long start = System.currentTimeMillis();    KDDCupDataModel model = new KDDCupDataModel(KDDCupDataModel.getTrainingFile(dataFileDirectory));    Track1Recommender recommender = new Track1Recommender(model);    long end = System.currentTimeMillis();        start = end;    Collection<Track1Callable> callables = new ArrayList<>();    for (Pair<PreferenceArray, long[]> tests : new DataFileIterable(KDDCupDataModel.getTestFile(dataFileDirectory))) {        PreferenceArray userTest = tests.getFirst();        callables.add(new Track1Callable(recommender, userTest));    }    int cores = Runtime.getRuntime().availableProcessors();        ExecutorService executor = Executors.newFixedThreadPool(cores);    List<Future<byte[]>> results = executor.invokeAll(callables);    executor.shutdown();    end = System.currentTimeMillis();        start = end;    try (OutputStream out = new BufferedOutputStream(new FileOutputStream(new File(args[1])))) {        for (Future<byte[]> result : results) {            for (byte estimate : result.get()) {                out.write(estimate);            }        }    }    end = System.currentTimeMillis();    }
1
public double itemSimilarity(long itemID1, long itemID2) throws TasteException
{    return contentSimilarity.itemSimilarity(itemID1, itemID2) * cfSimilarity.itemSimilarity(itemID1, itemID2);}
0
public double[] itemSimilarities(long itemID1, long[] itemID2s) throws TasteException
{    double[] result = contentSimilarity.itemSimilarities(itemID1, itemID2s);    double[] multipliers = cfSimilarity.itemSimilarities(itemID1, itemID2s);    for (int i = 0; i < result.length; i++) {        result[i] *= multipliers[i];    }    return result;}
0
public void refresh(Collection<Refreshable> alreadyRefreshed)
{    cfSimilarity.refresh(alreadyRefreshed);}
0
public List<RecommendedItem> recommend(long userID, int howMany) throws TasteException
{    return recommender.recommend(userID, howMany);}
0
public List<RecommendedItem> recommend(long userID, int howMany, boolean includeKnownItems) throws TasteException
{    return recommend(userID, howMany, null, includeKnownItems);}
0
public List<RecommendedItem> recommend(long userID, int howMany, IDRescorer rescorer) throws TasteException
{    return recommender.recommend(userID, howMany, rescorer, false);}
0
public List<RecommendedItem> recommend(long userID, int howMany, IDRescorer rescorer, boolean includeKnownItems) throws TasteException
{    return recommender.recommend(userID, howMany, rescorer, includeKnownItems);}
0
public float estimatePreference(long userID, long itemID) throws TasteException
{    return recommender.estimatePreference(userID, itemID);}
0
public void setPreference(long userID, long itemID, float value) throws TasteException
{    recommender.setPreference(userID, itemID, value);}
0
public void removePreference(long userID, long itemID) throws TasteException
{    recommender.removePreference(userID, itemID);}
0
public DataModel getDataModel()
{    return recommender.getDataModel();}
0
public void refresh(Collection<Refreshable> alreadyRefreshed)
{    recommender.refresh(alreadyRefreshed);}
0
public String toString()
{    return "Track1Recommender[recommender:" + recommender + ']';}
0
public Recommender buildRecommender(DataModel dataModel) throws TasteException
{    return new Track2Recommender(dataModel, ((KDDCupDataModel) dataModel).getDataFileDirectory());}
0
public static void main(String[] args) throws Exception
{    File dataFileDirectory = new File(args[0]);    if (!dataFileDirectory.exists() || !dataFileDirectory.isDirectory()) {        throw new IllegalArgumentException("Bad data file directory: " + dataFileDirectory);    }    long start = System.currentTimeMillis();    KDDCupDataModel model = new KDDCupDataModel(KDDCupDataModel.getTrainingFile(dataFileDirectory));    Track2Recommender recommender = new Track2Recommender(model, dataFileDirectory);    long end = System.currentTimeMillis();        start = end;    Collection<Track2Callable> callables = new ArrayList<>();    for (Pair<PreferenceArray, long[]> tests : new DataFileIterable(KDDCupDataModel.getTestFile(dataFileDirectory))) {        PreferenceArray userTest = tests.getFirst();        callables.add(new Track2Callable(recommender, userTest));    }    int cores = Runtime.getRuntime().availableProcessors();        ExecutorService executor = Executors.newFixedThreadPool(cores);    List<Future<UserResult>> futures = executor.invokeAll(callables);    executor.shutdown();    end = System.currentTimeMillis();        start = end;    try (OutputStream out = new BufferedOutputStream(new FileOutputStream(new File(args[1])))) {        long lastUserID = Long.MIN_VALUE;        for (Future<UserResult> future : futures) {            UserResult result = future.get();            long userID = result.getUserID();            if (userID <= lastUserID) {                throw new IllegalStateException();            }            lastUserID = userID;            out.write(result.getResultBytes());        }    }    end = System.currentTimeMillis();    }
1
private static long parse(String value)
{    return NO_VALUE.equals(value) ? NO_VALUE_ID : Long.parseLong(value);}
0
public long getTrackID()
{    return trackID;}
0
public long getAlbumID()
{    return albumID;}
0
public long getArtistID()
{    return artistID;}
0
public FastIDSet getGenreIDs()
{    return genreIDs;}
0
public double itemSimilarity(long itemID1, long itemID2)
{    if (itemID1 == itemID2) {        return 1.0;    }    TrackData data1 = trackData.get(itemID1);    TrackData data2 = trackData.get(itemID2);    if (data1 == null || data2 == null) {        return 0.0;    }        if (data1.getAlbumID() != TrackData.NO_VALUE_ID && data1.getAlbumID() == data2.getAlbumID()) {        return 0.9;    }        if (data1.getArtistID() != TrackData.NO_VALUE_ID && data1.getArtistID() == data2.getArtistID()) {        return 0.7;    }        FastIDSet genres1 = data1.getGenreIDs();    FastIDSet genres2 = data2.getGenreIDs();    if (genres1 == null || genres2 == null) {        return 0.0;    }    int intersectionSize = genres1.intersectionSize(genres2);    if (intersectionSize == 0) {        return 0.0;    }    int unionSize = genres1.size() + genres2.size() - intersectionSize;    return intersectionSize / (4.0 * unionSize);}
0
public double[] itemSimilarities(long itemID1, long[] itemID2s)
{    int length = itemID2s.length;    double[] result = new double[length];    for (int i = 0; i < length; i++) {        result[i] = itemSimilarity(itemID1, itemID2s[i]);    }    return result;}
0
public long[] allSimilarItemIDs(long itemID)
{    FastIDSet allSimilarItemIDs = new FastIDSet();    LongPrimitiveIterator allItemIDs = trackData.keySetIterator();    while (allItemIDs.hasNext()) {        long possiblySimilarItemID = allItemIDs.nextLong();        if (!Double.isNaN(itemSimilarity(itemID, possiblySimilarItemID))) {            allSimilarItemIDs.add(possiblySimilarItemID);        }    }    return allSimilarItemIDs.toArray();}
0
public void refresh(Collection<Refreshable> alreadyRefreshed)
{}
0
public long getUserID()
{    return userID;}
0
public byte[] getResultBytes()
{    return resultBytes;}
0
public static File getRatings(String[] args) throws OptionException
{    DefaultOptionBuilder obuilder = new DefaultOptionBuilder();    ArgumentBuilder abuilder = new ArgumentBuilder();    GroupBuilder gbuilder = new GroupBuilder();    Option inputOpt = obuilder.withLongName("input").withRequired(false).withShortName("i").withArgument(abuilder.withName("input").withMinimum(1).withMaximum(1).create()).withDescription("The Path for input data directory.").create();    Option helpOpt = DefaultOptionCreator.helpOption();    Group group = gbuilder.withName("Options").withOption(inputOpt).withOption(helpOpt).create();    Parser parser = new Parser();    parser.setGroup(group);    CommandLine cmdLine = parser.parse(args);    if (cmdLine.hasOption(helpOpt)) {        CommandLineUtil.printHelp(group);        return null;    }    return cmdLine.hasOption(inputOpt) ? new File(cmdLine.getValue(inputOpt).toString()) : null;}
0
public static void main(String[] args) throws IOException
{    if (args.length != 4) {        System.err.println("Usage: NetflixDatasetConverter /path/to/training_set/ /path/to/qualifying.txt " + "/path/to/judging.txt /path/to/destination");        return;    }    String trainingDataDir = args[0];    String qualifyingTxt = args[1];    String judgingTxt = args[2];    Path outputPath = new Path(args[3]);    Configuration conf = new Configuration();    FileSystem fs = FileSystem.get(outputPath.toUri(), conf);    Preconditions.checkArgument(trainingDataDir != null, "Training Data location needs to be specified");        try (BufferedWriter writer = new BufferedWriter(new OutputStreamWriter(fs.create(new Path(outputPath, "trainingSet/ratings.tsv")), Charsets.UTF_8))) {        int ratingsProcessed = 0;        for (File movieRatings : new File(trainingDataDir).listFiles()) {            try (FileLineIterator lines = new FileLineIterator(movieRatings)) {                boolean firstLineRead = false;                String movieID = null;                while (lines.hasNext()) {                    String line = lines.next();                    if (firstLineRead) {                        String[] tokens = SEPARATOR.split(line);                        String userID = tokens[0];                        String rating = tokens[1];                        writer.write(userID + TAB + movieID + TAB + rating + NEWLINE);                        ratingsProcessed++;                        if (ratingsProcessed % 1000000 == 0) {                                                    }                    } else {                        movieID = line.replaceAll(MOVIE_DENOTER, "");                        firstLineRead = true;                    }                }            }        }            }        List<Preference> probes = new ArrayList<>(2817131);    long currentMovieID = -1;    for (String line : new FileLineIterable(new File(qualifyingTxt))) {        if (line.contains(MOVIE_DENOTER)) {            currentMovieID = Long.parseLong(line.replaceAll(MOVIE_DENOTER, ""));        } else {            long userID = Long.parseLong(SEPARATOR.split(line)[0]);            probes.add(new GenericPreference(userID, currentMovieID, 0));        }    }            try (BufferedWriter writer = new BufferedWriter(new OutputStreamWriter(fs.create(new Path(outputPath, "probeSet/ratings.tsv")), Charsets.UTF_8))) {        int ratingsProcessed = 0;        for (String line : new FileLineIterable(new File(judgingTxt))) {            if (line.contains(MOVIE_DENOTER)) {                currentMovieID = Long.parseLong(line.replaceAll(MOVIE_DENOTER, ""));            } else {                float rating = Float.parseFloat(SEPARATOR.split(line)[0]);                Preference pref = probes.get(ratingsProcessed);                Preconditions.checkState(pref.getItemID() == currentMovieID);                ratingsProcessed++;                writer.write(pref.getUserID() + TAB + pref.getItemID() + TAB + rating + NEWLINE);                if (ratingsProcessed % 1000000 == 0) {                                    }            }        }            }}
1
public static void main(String[] args) throws Exception
{    if (args.length != 1) {        System.err.println("Need path to ratings.dat of the movielens1M dataset as argument!");        System.exit(-1);    }    File resultFile = new File(System.getProperty("java.io.tmpdir"), "similarities.csv");    if (resultFile.exists()) {        resultFile.delete();    }    DataModel dataModel = new GroupLensDataModel(new File(args[0]));    ItemBasedRecommender recommender = new GenericItemBasedRecommender(dataModel, new LogLikelihoodSimilarity(dataModel));    BatchItemSimilarities batch = new MultithreadedBatchItemSimilarities(recommender, 5);    int numSimilarities = batch.computeItemSimilarities(Runtime.getRuntime().availableProcessors(), 1, new FileSimilarItemsWriter(resultFile));    System.out.println("Computed " + numSimilarities + " similarities for " + dataModel.getNumItems() + " items " + "and saved them to " + resultFile.getAbsolutePath());}
0
private static File convertGLFile(File originalFile) throws IOException
{        File resultFile = new File(new File(System.getProperty("java.io.tmpdir")), "ratings.txt");    if (resultFile.exists()) {        resultFile.delete();    }    try (Writer writer = new OutputStreamWriter(new FileOutputStream(resultFile), Charsets.UTF_8)) {        for (String line : new FileLineIterable(originalFile, false)) {            int lastDelimiterStart = line.lastIndexOf(COLON_DELIMTER);            if (lastDelimiterStart < 0) {                throw new IOException("Unexpected input format on line: " + line);            }            String subLine = line.substring(0, lastDelimiterStart);            String convertedLine = COLON_DELIMITER_PATTERN.matcher(subLine).replaceAll(",");            writer.write(convertedLine);            writer.write('\n');        }    } catch (IOException ioe) {        resultFile.delete();        throw ioe;    }    return resultFile;}
0
public static File readResourceToTempFile(String resourceName) throws IOException
{    InputSupplier<? extends InputStream> inSupplier;    try {        URL resourceURL = Resources.getResource(GroupLensDataModel.class, resourceName);        inSupplier = Resources.newInputStreamSupplier(resourceURL);    } catch (IllegalArgumentException iae) {        File resourceFile = new File("src/main/java" + resourceName);        inSupplier = Files.newInputStreamSupplier(resourceFile);    }    File tempFile = File.createTempFile("taste", null);    tempFile.deleteOnExit();    Files.copy(inSupplier, tempFile);    return tempFile;}
0
public String toString()
{    return "GroupLensDataModel";}
0
protected void setup(Context context) throws IOException, InterruptedException
{    useListName = Boolean.parseBoolean(context.getConfiguration().get(PrepEmailVectorsDriver.USE_LIST_NAME));}
0
protected void map(WritableComparable<?> key, VectorWritable value, Context context) throws IOException, InterruptedException
{    String input = key.toString();        String[] splits = SLASH.split(input);        if (splits.length >= 3) {        StringBuilder bldr = new StringBuilder();        bldr.append(escape(splits[1]));        if (useListName) {            bldr.append('_').append(escape(splits[2]));        }        context.write(new Text(bldr.toString()), value);    }}
0
private static String escape(CharSequence value)
{    return DASH_DOT.matcher(value).replaceAll("_").toLowerCase(Locale.ENGLISH);}
0
protected void setup(Context context) throws IOException, InterruptedException
{    maxItemsPerLabel = Long.parseLong(context.getConfiguration().get(PrepEmailVectorsDriver.ITEMS_PER_CLASS));}
0
protected void reduce(Text key, Iterable<VectorWritable> values, Context context) throws IOException, InterruptedException
{        long i = 0;    Iterator<VectorWritable> iterator = values.iterator();    while (i < maxItemsPerLabel && iterator.hasNext()) {        context.write(key, iterator.next());        i++;    }}
0
public static void main(String[] args) throws Exception
{    ToolRunner.run(new Configuration(), new PrepEmailVectorsDriver(), args);}
0
public int run(String[] args) throws Exception
{    addInputOption();    addOutputOption();    addOption(DefaultOptionCreator.overwriteOption().create());    addOption("maxItemsPerLabel", "mipl", "The maximum number of items per label.  Can be useful for making the " + "training sets the same size", String.valueOf(100000));    addOption(buildOption("useListName", "ul", "Use the name of the list as part of the label.  If not set, then " + "just use the project name", false, false, "false"));    Map<String, List<String>> parsedArgs = parseArguments(args);    if (parsedArgs == null) {        return -1;    }    Path input = getInputPath();    Path output = getOutputPath();    if (hasOption(DefaultOptionCreator.OVERWRITE_OPTION)) {        HadoopUtil.delete(getConf(), output);    }    Job convertJob = prepareJob(input, output, SequenceFileInputFormat.class, PrepEmailMapper.class, Text.class, VectorWritable.class, PrepEmailReducer.class, Text.class, VectorWritable.class, SequenceFileOutputFormat.class);    convertJob.getConfiguration().set(ITEMS_PER_CLASS, getOption("maxItemsPerLabel"));    convertJob.getConfiguration().set(USE_LIST_NAME, String.valueOf(hasOption("useListName")));    boolean succeeded = convertJob.waitForCompletion(true);    return succeeded ? 0 : -1;}
0
public FeatureVectorEncoder getEncoder()
{    return encoder;}
0
public FeatureVectorEncoder getBias()
{    return bias;}
0
public Random getRandom()
{    return rand;}
0
public Vector encodeFeatureVector(File file, int actual, int leakType, Multiset<String> overallCounts) throws IOException
{    long date = (long) (1000 * (DATE_REFERENCE + actual * MONTH + 1 * WEEK * rand.nextDouble()));    Multiset<String> words = ConcurrentHashMultiset.create();    try (BufferedReader reader = Files.newReader(file, Charsets.UTF_8)) {        String line = reader.readLine();        Reader dateString = new StringReader(DATE_FORMATS[leakType % 3].format(new Date(date)));        countWords(analyzer, words, dateString, overallCounts);        while (line != null && !line.isEmpty()) {            boolean countHeader = (line.startsWith("From:") || line.startsWith("Subject:") || line.startsWith("Keywords:") || line.startsWith("Summary:")) && leakType < 6;            do {                Reader in = new StringReader(line);                if (countHeader) {                    countWords(analyzer, words, in, overallCounts);                }                line = reader.readLine();            } while (line != null && line.startsWith(" "));        }        if (leakType < 3) {            countWords(analyzer, words, reader, overallCounts);        }    }    Vector v = new RandomAccessSparseVector(FEATURES);    bias.addToVector("", 1, v);    for (String word : words.elementSet()) {        encoder.addToVector(word, Math.log1p(words.count(word)), v);    }    return v;}
0
public static void countWords(Analyzer analyzer, Collection<String> words, Reader in, Multiset<String> overallCounts) throws IOException
{    TokenStream ts = analyzer.tokenStream("text", in);    ts.addAttribute(CharTermAttribute.class);    ts.reset();    while (ts.incrementToken()) {        String s = ts.getAttribute(CharTermAttribute.class).toString();        words.add(s);    }    overallCounts.addAll(words);    ts.end();    Closeables.close(ts, true);}
0
private static void readFromURL(String url, boolean assignIDs) throws IOException
{        hiddenSequences = new LinkedList<>();    observedSequences = new LinkedList<>();    readLines = 0;        List<Integer> observedSequence = new LinkedList<>();    List<Integer> hiddenSequence = new LinkedList<>();    for (String line : Resources.readLines(new URL(url), Charsets.UTF_8)) {        if (line.isEmpty()) {                        int[] observedSequenceArray = new int[observedSequence.size()];            int[] hiddenSequenceArray = new int[hiddenSequence.size()];            for (int i = 0; i < observedSequence.size(); ++i) {                observedSequenceArray[i] = observedSequence.get(i);                hiddenSequenceArray[i] = hiddenSequence.get(i);            }                        hiddenSequences.add(hiddenSequenceArray);            observedSequences.add(observedSequenceArray);                        observedSequence.clear();            hiddenSequence.clear();            continue;        }        readLines++;                String[] tags = SPACE.split(line);                if (assignIDs) {            if (!wordIDs.containsKey(tags[0])) {                wordIDs.put(tags[0], nextWordId++);            }            if (!tagIDs.containsKey(tags[1])) {                tagIDs.put(tags[1], nextTagId++);            }        }                Integer wordID = wordIDs.get(tags[0]);        Integer tagID = tagIDs.get(tags[1]);                if (wordID == null) {            observedSequence.add(0);        } else {            observedSequence.add(wordID);        }        if (tagID == null) {            hiddenSequence.add(0);        } else {            hiddenSequence.add(tagID);        }    }        if (!observedSequence.isEmpty()) {        int[] observedSequenceArray = new int[observedSequence.size()];        int[] hiddenSequenceArray = new int[hiddenSequence.size()];        for (int i = 0; i < observedSequence.size(); ++i) {            observedSequenceArray[i] = observedSequence.get(i);            hiddenSequenceArray[i] = hiddenSequence.get(i);        }                hiddenSequences.add(hiddenSequenceArray);        observedSequences.add(observedSequenceArray);    }}
0
private static void trainModel(String trainingURL) throws IOException
{        tagIDs = new HashMap<>(44);        wordIDs = new HashMap<>(19122);            long start = System.currentTimeMillis();    readFromURL(trainingURL, true);    long end = System.currentTimeMillis();    double duration = (end - start) / 1000.0;            start = System.currentTimeMillis();    taggingModel = HmmTrainer.trainSupervisedSequence(nextTagId, nextWordId, hiddenSequences, observedSequences, 0.05);                Matrix emissions = taggingModel.getEmissionMatrix();    for (int i = 0; i < taggingModel.getNrOfHiddenStates(); ++i) {        emissions.setQuick(i, 0, 0.1 / taggingModel.getNrOfHiddenStates());    }    int nnptag = tagIDs.get("NNP");    emissions.setQuick(nnptag, 0, 1 / (double) taggingModel.getNrOfHiddenStates());        HmmUtils.normalizeModel(taggingModel);        taggingModel.registerHiddenStateNames(tagIDs);    taggingModel.registerOutputStateNames(wordIDs);    end = System.currentTimeMillis();    duration = (end - start) / 1000.0;    }
1
private static void testModel(String testingURL) throws IOException
{        long start = System.currentTimeMillis();    readFromURL(testingURL, false);    long end = System.currentTimeMillis();    double duration = (end - start) / 1000.0;            start = System.currentTimeMillis();    int errorCount = 0;    int totalCount = 0;    for (int i = 0; i < observedSequences.size(); ++i) {                int[] posEstimate = HmmEvaluator.decode(taggingModel, observedSequences.get(i), false);                int[] posExpected = hiddenSequences.get(i);        for (int j = 0; j < posExpected.length; ++j) {            totalCount++;            if (posEstimate[j] != posExpected[j]) {                errorCount++;            }        }    }    end = System.currentTimeMillis();    duration = (end - start) / 1000.0;        double errorRate = (double) errorCount / totalCount;    }
1
private static List<String> tagSentence(String sentence)
{            sentence = sentence.replaceAll("[,.!?:;\"]", " $0 ");    sentence = sentence.replaceAll("''", " '' ");        String[] tokens = SPACES.split(sentence);        int[] observedSequence = HmmUtils.encodeStateSequence(taggingModel, Arrays.asList(tokens), true, 0);        int[] hiddenSequence = HmmEvaluator.decode(taggingModel, observedSequence, false);        return HmmUtils.decodeStateSequence(taggingModel, hiddenSequence, false, null);}
0
public static void main(String[] args) throws IOException
{        trainModel("http://www.jaist.ac.jp/~hieuxuan/flexcrfs/CoNLL2000-NP/train.txt");    testModel("http://www.jaist.ac.jp/~hieuxuan/flexcrfs/CoNLL2000-NP/test.txt");        String test = "McDonalds is a huge company with many employees .";    String[] testWords = SPACE.split(test);    List<String> posTags = tagSentence(test);    for (int i = 0; i < posTags.size(); ++i) {            }}
1
public AdaptiveLogisticRegression createAdaptiveLogisticRegression()
{    if (alr == null) {        alr = new AdaptiveLogisticRegression(getMaxTargetCategories(), getNumFeatures(), createPrior(prior, priorOption));        alr.setInterval(interval);        alr.setAveragingWindow(averageWindow);        alr.setThreadCount(threads);        alr.setAucEvaluator(createAUC(auc));    }    return alr;}
0
public void checkParameters()
{    if (prior != null) {        String priorUppercase = prior.toUpperCase(Locale.ENGLISH).trim();        if (("TP".equals(priorUppercase) || "EBP".equals(priorUppercase)) && Double.isNaN(priorOption)) {            throw new IllegalArgumentException("You must specify a double value for TPrior and ElasticBandPrior.");        }    }}
0
private static PriorFunction createPrior(String cmd, double priorOption)
{    if (cmd == null) {        return null;    }    if ("L1".equals(cmd.toUpperCase(Locale.ENGLISH).trim())) {        return new L1();    }    if ("L2".equals(cmd.toUpperCase(Locale.ENGLISH).trim())) {        return new L2();    }    if ("UP".equals(cmd.toUpperCase(Locale.ENGLISH).trim())) {        return new UniformPrior();    }    if ("TP".equals(cmd.toUpperCase(Locale.ENGLISH).trim())) {        return new TPrior(priorOption);    }    if ("EBP".equals(cmd.toUpperCase(Locale.ENGLISH).trim())) {        return new ElasticBandPrior(priorOption);    }    return null;}
0
private static OnlineAuc createAUC(String cmd)
{    if (cmd == null) {        return null;    }    if ("GLOBAL".equals(cmd.toUpperCase(Locale.ENGLISH).trim())) {        return new GlobalOnlineAuc();    }    if ("GROUPED".equals(cmd.toUpperCase(Locale.ENGLISH).trim())) {        return new GroupedOnlineAuc();    }    return null;}
0
public void saveTo(OutputStream out) throws IOException
{    if (alr != null) {        alr.close();    }    setTargetCategories(getCsvRecordFactory().getTargetCategories());    write(new DataOutputStream(out));}
0
public void write(DataOutput out) throws IOException
{    out.writeUTF(getTargetVariable());    out.writeInt(getTypeMap().size());    for (Map.Entry<String, String> entry : getTypeMap().entrySet()) {        out.writeUTF(entry.getKey());        out.writeUTF(entry.getValue());    }    out.writeInt(getNumFeatures());    out.writeInt(getMaxTargetCategories());    out.writeInt(getTargetCategories().size());    for (String category : getTargetCategories()) {        out.writeUTF(category);    }    out.writeInt(interval);    out.writeInt(averageWindow);    out.writeInt(threads);    out.writeUTF(prior);    out.writeDouble(priorOption);    out.writeUTF(auc);        alr.write(out);}
0
public void readFields(DataInput in) throws IOException
{    setTargetVariable(in.readUTF());    int typeMapSize = in.readInt();    Map<String, String> typeMap = new HashMap<>(typeMapSize);    for (int i = 0; i < typeMapSize; i++) {        String key = in.readUTF();        String value = in.readUTF();        typeMap.put(key, value);    }    setTypeMap(typeMap);    setNumFeatures(in.readInt());    setMaxTargetCategories(in.readInt());    int targetCategoriesSize = in.readInt();    List<String> targetCategories = new ArrayList<>(targetCategoriesSize);    for (int i = 0; i < targetCategoriesSize; i++) {        targetCategories.add(in.readUTF());    }    setTargetCategories(targetCategories);    interval = in.readInt();    averageWindow = in.readInt();    threads = in.readInt();    prior = in.readUTF();    priorOption = in.readDouble();    auc = in.readUTF();    alr = new AdaptiveLogisticRegression();    alr.readFields(in);}
0
private static AdaptiveLogisticModelParameters loadFromStream(InputStream in) throws IOException
{    AdaptiveLogisticModelParameters result = new AdaptiveLogisticModelParameters();    result.readFields(new DataInputStream(in));    return result;}
0
public static AdaptiveLogisticModelParameters loadFromFile(File in) throws IOException
{    try (InputStream input = new FileInputStream(in)) {        return loadFromStream(input);    }}
0
public int getInterval()
{    return interval;}
0
public void setInterval(int interval)
{    this.interval = interval;}
0
public int getAverageWindow()
{    return averageWindow;}
0
public void setAverageWindow(int averageWindow)
{    this.averageWindow = averageWindow;}
0
public int getThreads()
{    return threads;}
0
public void setThreads(int threads)
{    this.threads = threads;}
0
public String getPrior()
{    return prior;}
0
public void setPrior(String prior)
{    this.prior = prior;}
0
public String getAuc()
{    return auc;}
0
public void setAuc(String auc)
{    this.auc = auc;}
0
public double getPriorOption()
{    return priorOption;}
0
public void setPriorOption(double priorOption)
{    this.priorOption = priorOption;}
0
public static void main(String[] args) throws Exception
{    List<TelephoneCall> calls = Lists.newArrayList(new TelephoneCallParser("bank-full.csv"));    double heldOutPercentage = 0.10;    for (int run = 0; run < 20; run++) {        Collections.shuffle(calls);        int cutoff = (int) (heldOutPercentage * calls.size());        List<TelephoneCall> test = calls.subList(0, cutoff);        List<TelephoneCall> train = calls.subList(cutoff, calls.size());        OnlineLogisticRegression lr = new OnlineLogisticRegression(NUM_CATEGORIES, TelephoneCall.FEATURES, new L1()).learningRate(1).alpha(1).lambda(0.000001).stepOffset(10000).decayExponent(0.2);        for (int pass = 0; pass < 20; pass++) {            for (TelephoneCall observation : train) {                lr.train(observation.getTarget(), observation.asVector());            }            if (pass % 5 == 0) {                Auc eval = new Auc(0.5);                for (TelephoneCall testCall : test) {                    eval.add(testCall.getTarget(), lr.classifyScalar(testCall.asVector()));                }                System.out.printf("%d, %.4f, %.4f\n", pass, lr.currentLearningRate(), eval.auc());            }        }    }}
0
public Vector asVector()
{    return vector;}
0
public int getTarget()
{    return fields.get("y").equals("no") ? 0 : 1;}
0
public Iterator<TelephoneCall> iterator()
{    try {        return new AbstractIterator<TelephoneCall>() {            BufferedReader input = new BufferedReader(new InputStreamReader(Resources.getResource(resourceName).openStream()));            Iterable<String> fieldNames = onSemi.split(input.readLine());            @Override            protected TelephoneCall computeNext() {                try {                    String line = input.readLine();                    if (line == null) {                        return endOfData();                    }                    return new TelephoneCall(fieldNames, onSemi.split(line));                } catch (IOException e) {                    throw new RuntimeException("Error reading data", e);                }            }        };    } catch (IOException e) {        throw new RuntimeException("Error reading data", e);    }}
0
protected TelephoneCall computeNext()
{    try {        String line = input.readLine();        if (line == null) {            return endOfData();        }        return new TelephoneCall(fieldNames, onSemi.split(line));    } catch (IOException e) {        throw new RuntimeException("Error reading data", e);    }}
0
public CsvRecordFactory getCsvRecordFactory()
{    if (csv == null) {        csv = new CsvRecordFactory(getTargetVariable(), getTypeMap()).maxTargetValue(getMaxTargetCategories()).includeBiasTerm(useBias());        if (targetCategories != null) {            csv.defineTargetCategories(targetCategories);        }    }    return csv;}
0
public OnlineLogisticRegression createRegression()
{    if (lr == null) {        lr = new OnlineLogisticRegression(getMaxTargetCategories(), getNumFeatures(), new L1()).lambda(getLambda()).learningRate(getLearningRate()).alpha(1 - 1.0e-3);    }    return lr;}
0
public void saveTo(OutputStream out) throws IOException
{    Closeables.close(lr, false);    targetCategories = getCsvRecordFactory().getTargetCategories();    write(new DataOutputStream(out));}
0
public static LogisticModelParameters loadFrom(InputStream in) throws IOException
{    LogisticModelParameters result = new LogisticModelParameters();    result.readFields(new DataInputStream(in));    return result;}
0
public static LogisticModelParameters loadFrom(File in) throws IOException
{    try (InputStream input = new FileInputStream(in)) {        return loadFrom(input);    }}
0
public void write(DataOutput out) throws IOException
{    out.writeUTF(targetVariable);    out.writeInt(typeMap.size());    for (Map.Entry<String, String> entry : typeMap.entrySet()) {        out.writeUTF(entry.getKey());        out.writeUTF(entry.getValue());    }    out.writeInt(numFeatures);    out.writeBoolean(useBias);    out.writeInt(maxTargetCategories);    if (targetCategories == null) {        out.writeInt(0);    } else {        out.writeInt(targetCategories.size());        for (String category : targetCategories) {            out.writeUTF(category);        }    }    out.writeDouble(lambda);    out.writeDouble(learningRate);        lr.write(out);}
0
public void readFields(DataInput in) throws IOException
{    targetVariable = in.readUTF();    int typeMapSize = in.readInt();    typeMap = new HashMap<>(typeMapSize);    for (int i = 0; i < typeMapSize; i++) {        String key = in.readUTF();        String value = in.readUTF();        typeMap.put(key, value);    }    numFeatures = in.readInt();    useBias = in.readBoolean();    maxTargetCategories = in.readInt();    int targetCategoriesSize = in.readInt();    targetCategories = new ArrayList<>(targetCategoriesSize);    for (int i = 0; i < targetCategoriesSize; i++) {        targetCategories.add(in.readUTF());    }    lambda = in.readDouble();    learningRate = in.readDouble();    csv = null;    lr = new OnlineLogisticRegression();    lr.readFields(in);}
0
public void setTypeMap(Iterable<String> predictorList, List<String> typeList)
{    Preconditions.checkArgument(!typeList.isEmpty(), "Must have at least one type specifier");    typeMap = new HashMap<>();    Iterator<String> iTypes = typeList.iterator();    String lastType = null;    for (Object x : predictorList) {                if (iTypes.hasNext()) {            lastType = iTypes.next();        }        typeMap.put(x.toString(), lastType);    }}
0
public void setTargetVariable(String targetVariable)
{    this.targetVariable = targetVariable;}
0
public void setMaxTargetCategories(int maxTargetCategories)
{    this.maxTargetCategories = maxTargetCategories;}
0
public void setNumFeatures(int numFeatures)
{    this.numFeatures = numFeatures;}
0
public void setTargetCategories(List<String> targetCategories)
{    this.targetCategories = targetCategories;    maxTargetCategories = targetCategories.size();}
0
public List<String> getTargetCategories()
{    return this.targetCategories;}
0
public void setUseBias(boolean useBias)
{    this.useBias = useBias;}
0
public boolean useBias()
{    return useBias;}
0
public String getTargetVariable()
{    return targetVariable;}
0
public Map<String, String> getTypeMap()
{    return typeMap;}
0
public void setTypeMap(Map<String, String> map)
{    this.typeMap = map;}
0
public int getNumFeatures()
{    return numFeatures;}
0
public int getMaxTargetCategories()
{    return maxTargetCategories;}
0
public double getLambda()
{    return lambda;}
0
public void setLambda(double lambda)
{    this.lambda = lambda;}
0
public double getLearningRate()
{    return learningRate;}
0
public void setLearningRate(double learningRate)
{    this.learningRate = learningRate;}
0
public static void main(String[] args) throws Exception
{    Preconditions.checkArgument(args.length == 1, "Must have a single argument that names a file or resource.");    try (BufferedReader in = TrainLogistic.open(args[0])) {        String line;        while ((line = in.readLine()) != null) {            System.out.println(line);        }    }}
0
public static void main(String[] args) throws Exception
{    mainToOutput(args, new PrintWriter(new OutputStreamWriter(System.out, Charsets.UTF_8), true));}
0
 static void mainToOutput(String[] args, PrintWriter output) throws Exception
{    if (!parseArgs(args)) {        return;    }    AdaptiveLogisticModelParameters lmp = AdaptiveLogisticModelParameters.loadFromFile(new File(modelFile));    CsvRecordFactory csv = lmp.getCsvRecordFactory();    csv.setIdName(idColumn);    AdaptiveLogisticRegression lr = lmp.createAdaptiveLogisticRegression();    State<Wrapper, CrossFoldLearner> best = lr.getBest();    if (best == null) {        output.println("AdaptiveLogisticRegression has not be trained probably.");        return;    }    CrossFoldLearner learner = best.getPayload().getLearner();    BufferedReader in = TrainAdaptiveLogistic.open(inputFile);    int k = 0;    try (BufferedWriter out = new BufferedWriter(new OutputStreamWriter(new FileOutputStream(outputFile), Charsets.UTF_8))) {        out.write(idColumn + ",target,score");        out.newLine();        String line = in.readLine();        csv.firstLine(line);        line = in.readLine();        Map<String, Double> results = new HashMap<>();        while (line != null) {            Vector v = new SequentialAccessSparseVector(lmp.getNumFeatures());            csv.processLine(line, v, false);            Vector scores = learner.classifyFull(v);            results.clear();            if (maxScoreOnly) {                results.put(csv.getTargetLabel(scores.maxValueIndex()), scores.maxValue());            } else {                for (int i = 0; i < scores.size(); i++) {                    results.put(csv.getTargetLabel(i), scores.get(i));                }            }            for (Map.Entry<String, Double> entry : results.entrySet()) {                out.write(csv.getIdString(line) + ',' + entry.getKey() + ',' + entry.getValue());                out.newLine();            }            k++;            if (k % 100 == 0) {                output.println(k + " records processed");            }            line = in.readLine();        }        out.flush();    }    output.println(k + " records processed totally.");}
0
private static boolean parseArgs(String[] args)
{    DefaultOptionBuilder builder = new DefaultOptionBuilder();    Option help = builder.withLongName("help").withDescription("print this list").create();    Option quiet = builder.withLongName("quiet").withDescription("be extra quiet").create();    ArgumentBuilder argumentBuilder = new ArgumentBuilder();    Option inputFileOption = builder.withLongName("input").withRequired(true).withArgument(argumentBuilder.withName("input").withMaximum(1).create()).withDescription("where to get training data").create();    Option modelFileOption = builder.withLongName("model").withRequired(true).withArgument(argumentBuilder.withName("model").withMaximum(1).create()).withDescription("where to get the trained model").create();    Option outputFileOption = builder.withLongName("output").withRequired(true).withDescription("the file path to output scores").withArgument(argumentBuilder.withName("output").withMaximum(1).create()).create();    Option idColumnOption = builder.withLongName("idcolumn").withRequired(true).withDescription("the name of the id column for each record").withArgument(argumentBuilder.withName("idcolumn").withMaximum(1).create()).create();    Option maxScoreOnlyOption = builder.withLongName("maxscoreonly").withDescription("only output the target label with max scores").create();    Group normalArgs = new GroupBuilder().withOption(help).withOption(quiet).withOption(inputFileOption).withOption(modelFileOption).withOption(outputFileOption).withOption(idColumnOption).withOption(maxScoreOnlyOption).create();    Parser parser = new Parser();    parser.setHelpOption(help);    parser.setHelpTrigger("--help");    parser.setGroup(normalArgs);    parser.setHelpFormatter(new HelpFormatter(" ", "", " ", 130));    CommandLine cmdLine = parser.parseAndHelp(args);    if (cmdLine == null) {        return false;    }    inputFile = getStringArgument(cmdLine, inputFileOption);    modelFile = getStringArgument(cmdLine, modelFileOption);    outputFile = getStringArgument(cmdLine, outputFileOption);    idColumn = getStringArgument(cmdLine, idColumnOption);    maxScoreOnly = getBooleanArgument(cmdLine, maxScoreOnlyOption);    return true;}
0
private static boolean getBooleanArgument(CommandLine cmdLine, Option option)
{    return cmdLine.hasOption(option);}
0
private static String getStringArgument(CommandLine cmdLine, Option inputFile)
{    return (String) cmdLine.getValue(inputFile);}
0
public static void main(String[] args) throws Exception
{    mainToOutput(args, new PrintWriter(new OutputStreamWriter(System.out, Charsets.UTF_8), true));}
0
 static void mainToOutput(String[] args, PrintWriter output) throws Exception
{    if (parseArgs(args)) {        if (!showAuc && !showConfusion && !showScores) {            showAuc = true;            showConfusion = true;        }        Auc collector = new Auc();        LogisticModelParameters lmp = LogisticModelParameters.loadFrom(new File(modelFile));        CsvRecordFactory csv = lmp.getCsvRecordFactory();        OnlineLogisticRegression lr = lmp.createRegression();        BufferedReader in = TrainLogistic.open(inputFile);        String line = in.readLine();        csv.firstLine(line);        line = in.readLine();        if (showScores) {            output.println("\"target\",\"model-output\",\"log-likelihood\"");        }        while (line != null) {            Vector v = new SequentialAccessSparseVector(lmp.getNumFeatures());            int target = csv.processLine(line, v);            double score = lr.classifyScalar(v);            if (showScores) {                output.printf(Locale.ENGLISH, "%d,%.3f,%.6f%n", target, score, lr.logLikelihood(target, v));            }            collector.add(target, score);            line = in.readLine();        }        if (showAuc) {            output.printf(Locale.ENGLISH, "AUC = %.2f%n", collector.auc());        }        if (showConfusion) {            Matrix m = collector.confusion();            output.printf(Locale.ENGLISH, "confusion: [[%.1f, %.1f], [%.1f, %.1f]]%n", m.get(0, 0), m.get(1, 0), m.get(0, 1), m.get(1, 1));            m = collector.entropy();            output.printf(Locale.ENGLISH, "entropy: [[%.1f, %.1f], [%.1f, %.1f]]%n", m.get(0, 0), m.get(1, 0), m.get(0, 1), m.get(1, 1));        }    }}
0
private static boolean parseArgs(String[] args)
{    DefaultOptionBuilder builder = new DefaultOptionBuilder();    Option help = builder.withLongName("help").withDescription("print this list").create();    Option quiet = builder.withLongName("quiet").withDescription("be extra quiet").create();    Option auc = builder.withLongName("auc").withDescription("print AUC").create();    Option confusion = builder.withLongName("confusion").withDescription("print confusion matrix").create();    Option scores = builder.withLongName("scores").withDescription("print scores").create();    ArgumentBuilder argumentBuilder = new ArgumentBuilder();    Option inputFileOption = builder.withLongName("input").withRequired(true).withArgument(argumentBuilder.withName("input").withMaximum(1).create()).withDescription("where to get training data").create();    Option modelFileOption = builder.withLongName("model").withRequired(true).withArgument(argumentBuilder.withName("model").withMaximum(1).create()).withDescription("where to get a model").create();    Group normalArgs = new GroupBuilder().withOption(help).withOption(quiet).withOption(auc).withOption(scores).withOption(confusion).withOption(inputFileOption).withOption(modelFileOption).create();    Parser parser = new Parser();    parser.setHelpOption(help);    parser.setHelpTrigger("--help");    parser.setGroup(normalArgs);    parser.setHelpFormatter(new HelpFormatter(" ", "", " ", 130));    CommandLine cmdLine = parser.parseAndHelp(args);    if (cmdLine == null) {        return false;    }    inputFile = getStringArgument(cmdLine, inputFileOption);    modelFile = getStringArgument(cmdLine, modelFileOption);    showAuc = getBooleanArgument(cmdLine, auc);    showScores = getBooleanArgument(cmdLine, scores);    showConfusion = getBooleanArgument(cmdLine, confusion);    return true;}
0
private static boolean getBooleanArgument(CommandLine cmdLine, Option option)
{    return cmdLine.hasOption(option);}
0
private static String getStringArgument(CommandLine cmdLine, Option inputFile)
{    return (String) cmdLine.getValue(inputFile);}
0
public static void dissect(int leakType, Dictionary dictionary, AdaptiveLogisticRegression learningAlgorithm, Iterable<File> files, Multiset<String> overallCounts) throws IOException
{    CrossFoldLearner model = learningAlgorithm.getBest().getPayload().getLearner();    model.close();    Map<String, Set<Integer>> traceDictionary = new TreeMap<>();    ModelDissector md = new ModelDissector();    NewsgroupHelper helper = new NewsgroupHelper();    helper.getEncoder().setTraceDictionary(traceDictionary);    helper.getBias().setTraceDictionary(traceDictionary);    for (File file : permute(files, helper.getRandom()).subList(0, 500)) {        String ng = file.getParentFile().getName();        int actual = dictionary.intern(ng);        traceDictionary.clear();        Vector v = helper.encodeFeatureVector(file, actual, leakType, overallCounts);        md.update(v, traceDictionary, model);    }    List<String> ngNames = new ArrayList<>(dictionary.values());    List<ModelDissector.Weight> weights = md.summary(100);    System.out.println("============");    System.out.println("Model Dissection");    for (ModelDissector.Weight w : weights) {        System.out.printf("%s\t%.1f\t%s\t%.1f\t%s\t%.1f\t%s%n", w.getFeature(), w.getWeight(), ngNames.get(w.getMaxImpact() + 1), w.getCategory(1), w.getWeight(1), w.getCategory(2), w.getWeight(2));    }}
0
public static List<File> permute(Iterable<File> files, Random rand)
{    List<File> r = new ArrayList<>();    for (File file : files) {        int i = rand.nextInt(r.size() + 1);        if (i == r.size()) {            r.add(file);        } else {            r.add(r.get(i));            r.set(i, file);        }    }    return r;}
0
 static void analyzeState(SGDInfo info, int leakType, int k, State<AdaptiveLogisticRegression.Wrapper, CrossFoldLearner> best) throws IOException
{    int bump = info.getBumps()[(int) Math.floor(info.getStep()) % info.getBumps().length];    int scale = (int) Math.pow(10, Math.floor(info.getStep() / info.getBumps().length));    double maxBeta;    double nonZeros;    double positive;    double norm;    double lambda = 0;    double mu = 0;    if (best != null) {        CrossFoldLearner state = best.getPayload().getLearner();        info.setAverageCorrect(state.percentCorrect());        info.setAverageLL(state.logLikelihood());        OnlineLogisticRegression model = state.getModels().get(0);                model.close();        Matrix beta = model.getBeta();        maxBeta = beta.aggregate(Functions.MAX, Functions.ABS);        nonZeros = beta.aggregate(Functions.PLUS, new DoubleFunction() {            @Override            public double apply(double v) {                return Math.abs(v) > 1.0e-6 ? 1 : 0;            }        });        positive = beta.aggregate(Functions.PLUS, new DoubleFunction() {            @Override            public double apply(double v) {                return v > 0 ? 1 : 0;            }        });        norm = beta.aggregate(Functions.PLUS, Functions.ABS);        lambda = best.getMappedParams()[0];        mu = best.getMappedParams()[1];    } else {        maxBeta = 0;        nonZeros = 0;        positive = 0;        norm = 0;    }    if (k % (bump * scale) == 0) {        if (best != null) {            File modelFile = new File(System.getProperty("java.io.tmpdir"), "news-group-" + k + ".model");            ModelSerializer.writeBinary(modelFile.getAbsolutePath(), best.getPayload().getLearner().getModels().get(0));        }        info.setStep(info.getStep() + 0.25);        System.out.printf("%.2f\t%.2f\t%.2f\t%.2f\t%.8g\t%.8g\t", maxBeta, nonZeros, positive, norm, lambda, mu);        System.out.printf("%d\t%.3f\t%.2f\t%s%n", k, info.getAverageLL(), info.getAverageCorrect() * 100, LEAK_LABELS[leakType % 3]);    }}
0
public double apply(double v)
{    return Math.abs(v) > 1.0e-6 ? 1 : 0;}
0
public double apply(double v)
{    return v > 0 ? 1 : 0;}
0
 double getAverageLL()
{    return averageLL;}
0
 void setAverageLL(double averageLL)
{    this.averageLL = averageLL;}
0
 double getAverageCorrect()
{    return averageCorrect;}
0
 void setAverageCorrect(double averageCorrect)
{    this.averageCorrect = averageCorrect;}
0
 double getStep()
{    return step;}
0
 void setStep(double step)
{    this.step = step;}
0
 int[] getBumps()
{    return bumps;}
0
 void setBumps(int[] bumps)
{    this.bumps = bumps;}
0
public static void main(String[] args) throws IOException
{    FeatureVectorEncoder[] encoder = new FeatureVectorEncoder[FIELDS];    for (int i = 0; i < FIELDS; i++) {        encoder[i] = new ConstantValueEncoder("v" + 1);    }    OnlineSummarizer[] s = new OnlineSummarizer[FIELDS];    for (int i = 0; i < FIELDS; i++) {        s[i] = new OnlineSummarizer();    }    long t0 = System.currentTimeMillis();    Vector v = new DenseVector(1000);    if ("--generate".equals(args[0])) {        try (PrintWriter out = new PrintWriter(new OutputStreamWriter(new FileOutputStream(new File(args[2])), Charsets.UTF_8))) {            int n = Integer.parseInt(args[1]);            for (int i = 0; i < n; i++) {                Line x = Line.generate();                out.println(x);            }        }    } else if ("--parse".equals(args[0])) {        try (BufferedReader in = Files.newReader(new File(args[1]), Charsets.UTF_8)) {            String line = in.readLine();            while (line != null) {                v.assign(0);                Line x = new Line(line);                for (int i = 0; i < FIELDS; i++) {                    s[i].add(x.getDouble(i));                    encoder[i].addToVector(x.get(i), v);                }                line = in.readLine();            }        }        String separator = "";        for (int i = 0; i < FIELDS; i++) {            System.out.printf("%s%.3f", separator, s[i].getMean());            separator = ",";        }    } else if ("--fast".equals(args[0])) {        try (FastLineReader in = new FastLineReader(new FileInputStream(args[1]))) {            FastLine line = in.read();            while (line != null) {                v.assign(0);                for (int i = 0; i < FIELDS; i++) {                    double z = line.getDouble(i);                    s[i].add(z);                    encoder[i].addToVector((byte[]) null, z, v);                }                line = in.read();            }        }        String separator = "";        for (int i = 0; i < FIELDS; i++) {            System.out.printf("%s%.3f", separator, s[i].getMean());            separator = ",";        }    }    System.out.printf("\nElapsed time = %.3f%n", (System.currentTimeMillis() - t0) / 1000.0);}
0
public double getDouble(int field)
{    return Double.parseDouble(data.get(field));}
0
public static Line generate()
{    Line r = new Line();    for (int i = 0; i < FIELDS; i++) {        double mean = ((i + 1) * 257) % 50 + 1;        r.data.add(Integer.toString(randomValue(mean)));    }    return r;}
0
private static int randomValue(double mean)
{    return (int) (-mean * Math.log1p(-RAND.nextDouble()));}
0
public String toString()
{    return WITH_COMMAS.join(data);}
0
public String get(int field)
{    return data.get(field);}
0
public static FastLine read(ByteBuffer buf)
{    FastLine r = new FastLine(buf);    r.start.add(buf.position());    int offset = buf.position();    while (offset < buf.limit()) {        int ch = buf.get();        offset = buf.position();        switch(ch) {            case '\n':                r.length.add(offset - r.start.get(r.length.size()) - 1);                return r;            case SEPARATOR_CHAR:                r.length.add(offset - r.start.get(r.length.size()) - 1);                r.start.add(offset);                break;            default:        }    }    throw new IllegalArgumentException("Not enough bytes in buffer");}
0
public double getDouble(int field)
{    int offset = start.get(field);    int size = length.get(field);    switch(size) {        case 1:            return base.get(offset) - '0';        case 2:            return (base.get(offset) - '0') * 10 + base.get(offset + 1) - '0';        default:            double r = 0;            for (int i = 0; i < size; i++) {                r = 10 * r + base.get(offset + i) - '0';            }            return r;    }}
0
public FastLine read() throws IOException
{    fillBuffer();    if (buf.remaining() > 0) {        return FastLine.read(buf);    } else {        return null;    }}
0
private void fillBuffer() throws IOException
{    if (buf.remaining() < 10000) {        buf.compact();        int n = in.read(buf.array(), buf.position(), buf.remaining());        if (n == -1) {            buf.flip();        } else {            buf.limit(buf.position() + n);            buf.position(0);        }    }}
0
public void close()
{    try {        Closeables.close(in, true);    } catch (IOException e) {            }}
1
public static void main(String[] args) throws IOException
{    TestASFEmail runner = new TestASFEmail();    if (runner.parseArgs(args)) {        runner.run(new PrintWriter(new OutputStreamWriter(System.out, Charsets.UTF_8), true));    }}
0
public void run(PrintWriter output) throws IOException
{    File base = new File(inputFile);        OnlineLogisticRegression classifier = ModelSerializer.readBinary(new FileInputStream(modelFile), OnlineLogisticRegression.class);    Dictionary asfDictionary = new Dictionary();    Configuration conf = new Configuration();    PathFilter testFilter = new PathFilter() {        @Override        public boolean accept(Path path) {            return path.getName().contains("test");        }    };    SequenceFileDirIterator<Text, VectorWritable> iter = new SequenceFileDirIterator<>(new Path(base.toString()), PathType.LIST, testFilter, null, true, conf);    long numItems = 0;    while (iter.hasNext()) {        Pair<Text, VectorWritable> next = iter.next();        asfDictionary.intern(next.getFirst().toString());        numItems++;    }    System.out.println(numItems + " test files");    ResultAnalyzer ra = new ResultAnalyzer(asfDictionary.values(), "DEFAULT");    iter = new SequenceFileDirIterator<>(new Path(base.toString()), PathType.LIST, testFilter, null, true, conf);    while (iter.hasNext()) {        Pair<Text, VectorWritable> next = iter.next();        String ng = next.getFirst().toString();        int actual = asfDictionary.intern(ng);        Vector result = classifier.classifyFull(next.getSecond().get());        int cat = result.maxValueIndex();        double score = result.maxValue();        double ll = classifier.logLikelihood(actual, next.getSecond().get());        ClassifierResult cr = new ClassifierResult(asfDictionary.values().get(cat), score, ll);        ra.addInstance(asfDictionary.values().get(actual), cr);    }    output.println(ra);}
0
public boolean accept(Path path)
{    return path.getName().contains("test");}
0
 boolean parseArgs(String[] args)
{    DefaultOptionBuilder builder = new DefaultOptionBuilder();    Option help = builder.withLongName("help").withDescription("print this list").create();    ArgumentBuilder argumentBuilder = new ArgumentBuilder();    Option inputFileOption = builder.withLongName("input").withRequired(true).withArgument(argumentBuilder.withName("input").withMaximum(1).create()).withDescription("where to get training data").create();    Option modelFileOption = builder.withLongName("model").withRequired(true).withArgument(argumentBuilder.withName("model").withMaximum(1).create()).withDescription("where to get a model").create();    Group normalArgs = new GroupBuilder().withOption(help).withOption(inputFileOption).withOption(modelFileOption).create();    Parser parser = new Parser();    parser.setHelpOption(help);    parser.setHelpTrigger("--help");    parser.setGroup(normalArgs);    parser.setHelpFormatter(new HelpFormatter(" ", "", " ", 130));    CommandLine cmdLine = parser.parseAndHelp(args);    if (cmdLine == null) {        return false;    }    inputFile = (String) cmdLine.getValue(inputFileOption);    modelFile = (String) cmdLine.getValue(modelFileOption);    return true;}
0
public static void main(String[] args) throws IOException
{    TestNewsGroups runner = new TestNewsGroups();    if (runner.parseArgs(args)) {        runner.run(new PrintWriter(new OutputStreamWriter(System.out, Charsets.UTF_8), true));    }}
0
public void run(PrintWriter output) throws IOException
{    File base = new File(inputFile);        OnlineLogisticRegression classifier = ModelSerializer.readBinary(new FileInputStream(modelFile), OnlineLogisticRegression.class);    Dictionary newsGroups = new Dictionary();    Multiset<String> overallCounts = HashMultiset.create();    List<File> files = new ArrayList<>();    for (File newsgroup : base.listFiles()) {        if (newsgroup.isDirectory()) {            newsGroups.intern(newsgroup.getName());            files.addAll(Arrays.asList(newsgroup.listFiles()));        }    }    System.out.println(files.size() + " test files");    ResultAnalyzer ra = new ResultAnalyzer(newsGroups.values(), "DEFAULT");    for (File file : files) {        String ng = file.getParentFile().getName();        int actual = newsGroups.intern(ng);        NewsgroupHelper helper = new NewsgroupHelper();                Vector input = helper.encodeFeatureVector(file, actual, 0, overallCounts);        Vector result = classifier.classifyFull(input);        int cat = result.maxValueIndex();        double score = result.maxValue();        double ll = classifier.logLikelihood(actual, input);        ClassifierResult cr = new ClassifierResult(newsGroups.values().get(cat), score, ll);        ra.addInstance(newsGroups.values().get(actual), cr);    }    output.println(ra);}
0
 boolean parseArgs(String[] args)
{    DefaultOptionBuilder builder = new DefaultOptionBuilder();    Option help = builder.withLongName("help").withDescription("print this list").create();    ArgumentBuilder argumentBuilder = new ArgumentBuilder();    Option inputFileOption = builder.withLongName("input").withRequired(true).withArgument(argumentBuilder.withName("input").withMaximum(1).create()).withDescription("where to get training data").create();    Option modelFileOption = builder.withLongName("model").withRequired(true).withArgument(argumentBuilder.withName("model").withMaximum(1).create()).withDescription("where to get a model").create();    Group normalArgs = new GroupBuilder().withOption(help).withOption(inputFileOption).withOption(modelFileOption).create();    Parser parser = new Parser();    parser.setHelpOption(help);    parser.setHelpTrigger("--help");    parser.setGroup(normalArgs);    parser.setHelpFormatter(new HelpFormatter(" ", "", " ", 130));    CommandLine cmdLine = parser.parseAndHelp(args);    if (cmdLine == null) {        return false;    }    inputFile = (String) cmdLine.getValue(inputFileOption);    modelFile = (String) cmdLine.getValue(modelFileOption);    return true;}
0
public static void main(String[] args) throws Exception
{    mainToOutput(args, new PrintWriter(new OutputStreamWriter(System.out, Charsets.UTF_8), true));}
0
 static void mainToOutput(String[] args, PrintWriter output) throws Exception
{    if (parseArgs(args)) {        CsvRecordFactory csv = lmp.getCsvRecordFactory();        model = lmp.createAdaptiveLogisticRegression();        State<Wrapper, CrossFoldLearner> best;        CrossFoldLearner learner = null;        int k = 0;        for (int pass = 0; pass < passes; pass++) {            BufferedReader in = open(inputFile);                        csv.firstLine(in.readLine());            String line = in.readLine();            while (line != null) {                                Vector input = new RandomAccessSparseVector(lmp.getNumFeatures());                int targetValue = csv.processLine(line, input);                                model.train(targetValue, input);                k++;                if (showperf && (k % (skipperfnum + 1) == 0)) {                    best = model.getBest();                    if (best != null) {                        learner = best.getPayload().getLearner();                    }                    if (learner != null) {                        double averageCorrect = learner.percentCorrect();                        double averageLL = learner.logLikelihood();                        output.printf("%d\t%.3f\t%.2f%n", k, averageLL, averageCorrect * 100);                    } else {                        output.printf(Locale.ENGLISH, "%10d %2d %s%n", k, targetValue, "AdaptiveLogisticRegression has not found a good model ......");                    }                }                line = in.readLine();            }            in.close();        }        best = model.getBest();        if (best != null) {            learner = best.getPayload().getLearner();        }        if (learner == null) {            output.println("AdaptiveLogisticRegression has failed to train a model.");            return;        }        try (OutputStream modelOutput = new FileOutputStream(outputFile)) {            lmp.saveTo(modelOutput);        }        OnlineLogisticRegression lr = learner.getModels().get(0);        output.println(lmp.getNumFeatures());        output.println(lmp.getTargetVariable() + " ~ ");        String sep = "";        for (String v : csv.getTraceDictionary().keySet()) {            double weight = predictorWeight(lr, 0, csv, v);            if (weight != 0) {                output.printf(Locale.ENGLISH, "%s%.3f*%s", sep, weight, v);                sep = " + ";            }        }        output.printf("%n");        for (int row = 0; row < lr.getBeta().numRows(); row++) {            for (String key : csv.getTraceDictionary().keySet()) {                double weight = predictorWeight(lr, row, csv, key);                if (weight != 0) {                    output.printf(Locale.ENGLISH, "%20s %.5f%n", key, weight);                }            }            for (int column = 0; column < lr.getBeta().numCols(); column++) {                output.printf(Locale.ENGLISH, "%15.9f ", lr.getBeta().get(row, column));            }            output.println();        }    }}
0
private static double predictorWeight(OnlineLogisticRegression lr, int row, RecordFactory csv, String predictor)
{    double weight = 0;    for (Integer column : csv.getTraceDictionary().get(predictor)) {        weight += lr.getBeta().get(row, column);    }    return weight;}
0
private static boolean parseArgs(String[] args)
{    DefaultOptionBuilder builder = new DefaultOptionBuilder();    Option help = builder.withLongName("help").withDescription("print this list").create();    Option quiet = builder.withLongName("quiet").withDescription("be extra quiet").create();    ArgumentBuilder argumentBuilder = new ArgumentBuilder();    Option showperf = builder.withLongName("showperf").withDescription("output performance measures during training").create();    Option inputFile = builder.withLongName("input").withRequired(true).withArgument(argumentBuilder.withName("input").withMaximum(1).create()).withDescription("where to get training data").create();    Option outputFile = builder.withLongName("output").withRequired(true).withArgument(argumentBuilder.withName("output").withMaximum(1).create()).withDescription("where to write the model content").create();    Option threads = builder.withLongName("threads").withArgument(argumentBuilder.withName("threads").withDefault("4").create()).withDescription("the number of threads AdaptiveLogisticRegression uses").create();    Option predictors = builder.withLongName("predictors").withRequired(true).withArgument(argumentBuilder.withName("predictors").create()).withDescription("a list of predictor variables").create();    Option types = builder.withLongName("types").withRequired(true).withArgument(argumentBuilder.withName("types").create()).withDescription("a list of predictor variable types (numeric, word, or text)").create();    Option target = builder.withLongName("target").withDescription("the name of the target variable").withRequired(true).withArgument(argumentBuilder.withName("target").withMaximum(1).create()).create();    Option targetCategories = builder.withLongName("categories").withDescription("the number of target categories to be considered").withRequired(true).withArgument(argumentBuilder.withName("categories").withMaximum(1).create()).create();    Option features = builder.withLongName("features").withDescription("the number of internal hashed features to use").withArgument(argumentBuilder.withName("numFeatures").withDefault("1000").withMaximum(1).create()).create();    Option passes = builder.withLongName("passes").withDescription("the number of times to pass over the input data").withArgument(argumentBuilder.withName("passes").withDefault("2").withMaximum(1).create()).create();    Option interval = builder.withLongName("interval").withArgument(argumentBuilder.withName("interval").withDefault("500").create()).withDescription("the interval property of AdaptiveLogisticRegression").create();    Option window = builder.withLongName("window").withArgument(argumentBuilder.withName("window").withDefault("800").create()).withDescription("the average propery of AdaptiveLogisticRegression").create();    Option skipperfnum = builder.withLongName("skipperfnum").withArgument(argumentBuilder.withName("skipperfnum").withDefault("99").create()).withDescription("show performance measures every (skipperfnum + 1) rows").create();    Option prior = builder.withLongName("prior").withArgument(argumentBuilder.withName("prior").withDefault("L1").create()).withDescription("the prior algorithm to use: L1, L2, ebp, tp, up").create();    Option priorOption = builder.withLongName("prioroption").withArgument(argumentBuilder.withName("prioroption").create()).withDescription("constructor parameter for ElasticBandPrior and TPrior").create();    Option auc = builder.withLongName("auc").withArgument(argumentBuilder.withName("auc").withDefault("global").create()).withDescription("the auc to use: global or grouped").create();    Group normalArgs = new GroupBuilder().withOption(help).withOption(quiet).withOption(inputFile).withOption(outputFile).withOption(target).withOption(targetCategories).withOption(predictors).withOption(types).withOption(passes).withOption(interval).withOption(window).withOption(threads).withOption(prior).withOption(features).withOption(showperf).withOption(skipperfnum).withOption(priorOption).withOption(auc).create();    Parser parser = new Parser();    parser.setHelpOption(help);    parser.setHelpTrigger("--help");    parser.setGroup(normalArgs);    parser.setHelpFormatter(new HelpFormatter(" ", "", " ", 130));    CommandLine cmdLine = parser.parseAndHelp(args);    if (cmdLine == null) {        return false;    }    TrainAdaptiveLogistic.inputFile = getStringArgument(cmdLine, inputFile);    TrainAdaptiveLogistic.outputFile = getStringArgument(cmdLine, outputFile);    List<String> typeList = new ArrayList<>();    for (Object x : cmdLine.getValues(types)) {        typeList.add(x.toString());    }    List<String> predictorList = new ArrayList<>();    for (Object x : cmdLine.getValues(predictors)) {        predictorList.add(x.toString());    }    lmp = new AdaptiveLogisticModelParameters();    lmp.setTargetVariable(getStringArgument(cmdLine, target));    lmp.setMaxTargetCategories(getIntegerArgument(cmdLine, targetCategories));    lmp.setNumFeatures(getIntegerArgument(cmdLine, features));    lmp.setInterval(getIntegerArgument(cmdLine, interval));    lmp.setAverageWindow(getIntegerArgument(cmdLine, window));    lmp.setThreads(getIntegerArgument(cmdLine, threads));    lmp.setAuc(getStringArgument(cmdLine, auc));    lmp.setPrior(getStringArgument(cmdLine, prior));    if (cmdLine.getValue(priorOption) != null) {        lmp.setPriorOption(getDoubleArgument(cmdLine, priorOption));    }    lmp.setTypeMap(predictorList, typeList);    TrainAdaptiveLogistic.showperf = getBooleanArgument(cmdLine, showperf);    TrainAdaptiveLogistic.skipperfnum = getIntegerArgument(cmdLine, skipperfnum);    TrainAdaptiveLogistic.passes = getIntegerArgument(cmdLine, passes);    lmp.checkParameters();    return true;}
0
private static String getStringArgument(CommandLine cmdLine, Option inputFile)
{    return (String) cmdLine.getValue(inputFile);}
0
private static boolean getBooleanArgument(CommandLine cmdLine, Option option)
{    return cmdLine.hasOption(option);}
0
private static int getIntegerArgument(CommandLine cmdLine, Option features)
{    return Integer.parseInt((String) cmdLine.getValue(features));}
0
private static double getDoubleArgument(CommandLine cmdLine, Option op)
{    return Double.parseDouble((String) cmdLine.getValue(op));}
0
public static AdaptiveLogisticRegression getModel()
{    return model;}
0
public static LogisticModelParameters getParameters()
{    return lmp;}
0
 static BufferedReader open(String inputFile) throws IOException
{    InputStream in;    try {        in = Resources.getResource(inputFile).openStream();    } catch (IllegalArgumentException e) {        in = new FileInputStream(new File(inputFile));    }    return new BufferedReader(new InputStreamReader(in, Charsets.UTF_8));}
0
public int run(String[] args) throws Exception
{    addInputOption();    addOutputOption();    addOption("categories", "nc", "The number of categories to train on", true);    addOption("cardinality", "c", "The size of the vectors to use", "100000");    addOption("threads", "t", "The number of threads to use in the learner", "20");    addOption("poolSize", "p", "The number of CrossFoldLearners to use in the AdaptiveLogisticRegression. " + "Higher values require more memory.", "5");    if (parseArguments(args) == null) {        return -1;    }    File base = new File(getInputPath().toString());    Multiset<String> overallCounts = HashMultiset.create();    File output = new File(getOutputPath().toString());    output.mkdirs();    int numCats = Integer.parseInt(getOption("categories"));    int cardinality = Integer.parseInt(getOption("cardinality", "100000"));    int threadCount = Integer.parseInt(getOption("threads", "20"));    int poolSize = Integer.parseInt(getOption("poolSize", "5"));    Dictionary asfDictionary = new Dictionary();    AdaptiveLogisticRegression learningAlgorithm = new AdaptiveLogisticRegression(numCats, cardinality, new L1(), threadCount, poolSize);    learningAlgorithm.setInterval(800);    learningAlgorithm.setAveragingWindow(500);        Configuration conf = new Configuration();    PathFilter trainFilter = new PathFilter() {        @Override        public boolean accept(Path path) {            return path.getName().contains("training");        }    };    SequenceFileDirIterator<Text, VectorWritable> iter = new SequenceFileDirIterator<>(new Path(base.toString()), PathType.LIST, trainFilter, null, true, conf);    long numItems = 0;    while (iter.hasNext()) {        Pair<Text, VectorWritable> next = iter.next();        asfDictionary.intern(next.getFirst().toString());        numItems++;    }    System.out.println(numItems + " training files");    SGDInfo info = new SGDInfo();    iter = new SequenceFileDirIterator<>(new Path(base.toString()), PathType.LIST, trainFilter, null, true, conf);    int k = 0;    while (iter.hasNext()) {        Pair<Text, VectorWritable> next = iter.next();        String ng = next.getFirst().toString();        int actual = asfDictionary.intern(ng);                learningAlgorithm.train(actual, next.getSecond().get());        k++;        State<AdaptiveLogisticRegression.Wrapper, CrossFoldLearner> best = learningAlgorithm.getBest();        SGDHelper.analyzeState(info, 0, k, best);    }    learningAlgorithm.close();            System.out.println("exiting main, writing model to " + output);    ModelSerializer.writeBinary(output + "/asf.model", learningAlgorithm.getBest().getPayload().getLearner().getModels().get(0));    List<Integer> counts = new ArrayList<>();    System.out.println("Word counts");    for (String count : overallCounts.elementSet()) {        counts.add(overallCounts.count(count));    }    Collections.sort(counts, Ordering.natural().reverse());    k = 0;    for (Integer count : counts) {        System.out.println(k + "\t" + count);        k++;        if (k > 1000) {            break;        }    }    return 0;}
0
public boolean accept(Path path)
{    return path.getName().contains("training");}
0
public static void main(String[] args) throws Exception
{    TrainASFEmail trainer = new TrainASFEmail();    trainer.run(args);}
0
public static void main(String[] args) throws Exception
{    mainToOutput(args, new PrintWriter(new OutputStreamWriter(System.out, Charsets.UTF_8), true));}
0
 static void mainToOutput(String[] args, PrintWriter output) throws Exception
{    if (parseArgs(args)) {        double logPEstimate = 0;        int samples = 0;        CsvRecordFactory csv = lmp.getCsvRecordFactory();        OnlineLogisticRegression lr = lmp.createRegression();        for (int pass = 0; pass < passes; pass++) {            try (BufferedReader in = open(inputFile)) {                                csv.firstLine(in.readLine());                String line = in.readLine();                while (line != null) {                                        Vector input = new RandomAccessSparseVector(lmp.getNumFeatures());                    int targetValue = csv.processLine(line, input);                                        double logP = lr.logLikelihood(targetValue, input);                    if (!Double.isInfinite(logP)) {                        if (samples < 20) {                            logPEstimate = (samples * logPEstimate + logP) / (samples + 1);                        } else {                            logPEstimate = 0.95 * logPEstimate + 0.05 * logP;                        }                        samples++;                    }                    double p = lr.classifyScalar(input);                    if (scores) {                        output.printf(Locale.ENGLISH, "%10d %2d %10.2f %2.4f %10.4f %10.4f%n", samples, targetValue, lr.currentLearningRate(), p, logP, logPEstimate);                    }                                        lr.train(targetValue, input);                    line = in.readLine();                }            }        }        try (OutputStream modelOutput = new FileOutputStream(outputFile)) {            lmp.saveTo(modelOutput);        }        output.println(lmp.getNumFeatures());        output.println(lmp.getTargetVariable() + " ~ ");        String sep = "";        for (String v : csv.getTraceDictionary().keySet()) {            double weight = predictorWeight(lr, 0, csv, v);            if (weight != 0) {                output.printf(Locale.ENGLISH, "%s%.3f*%s", sep, weight, v);                sep = " + ";            }        }        output.printf("%n");        model = lr;        for (int row = 0; row < lr.getBeta().numRows(); row++) {            for (String key : csv.getTraceDictionary().keySet()) {                double weight = predictorWeight(lr, row, csv, key);                if (weight != 0) {                    output.printf(Locale.ENGLISH, "%20s %.5f%n", key, weight);                }            }            for (int column = 0; column < lr.getBeta().numCols(); column++) {                output.printf(Locale.ENGLISH, "%15.9f ", lr.getBeta().get(row, column));            }            output.println();        }    }}
0
private static double predictorWeight(OnlineLogisticRegression lr, int row, RecordFactory csv, String predictor)
{    double weight = 0;    for (Integer column : csv.getTraceDictionary().get(predictor)) {        weight += lr.getBeta().get(row, column);    }    return weight;}
0
private static boolean parseArgs(String[] args)
{    DefaultOptionBuilder builder = new DefaultOptionBuilder();    Option help = builder.withLongName("help").withDescription("print this list").create();    Option quiet = builder.withLongName("quiet").withDescription("be extra quiet").create();    Option scores = builder.withLongName("scores").withDescription("output score diagnostics during training").create();    ArgumentBuilder argumentBuilder = new ArgumentBuilder();    Option inputFile = builder.withLongName("input").withRequired(true).withArgument(argumentBuilder.withName("input").withMaximum(1).create()).withDescription("where to get training data").create();    Option outputFile = builder.withLongName("output").withRequired(true).withArgument(argumentBuilder.withName("output").withMaximum(1).create()).withDescription("where to get training data").create();    Option predictors = builder.withLongName("predictors").withRequired(true).withArgument(argumentBuilder.withName("p").create()).withDescription("a list of predictor variables").create();    Option types = builder.withLongName("types").withRequired(true).withArgument(argumentBuilder.withName("t").create()).withDescription("a list of predictor variable types (numeric, word, or text)").create();    Option target = builder.withLongName("target").withRequired(true).withArgument(argumentBuilder.withName("target").withMaximum(1).create()).withDescription("the name of the target variable").create();    Option features = builder.withLongName("features").withArgument(argumentBuilder.withName("numFeatures").withDefault("1000").withMaximum(1).create()).withDescription("the number of internal hashed features to use").create();    Option passes = builder.withLongName("passes").withArgument(argumentBuilder.withName("passes").withDefault("2").withMaximum(1).create()).withDescription("the number of times to pass over the input data").create();    Option lambda = builder.withLongName("lambda").withArgument(argumentBuilder.withName("lambda").withDefault("1e-4").withMaximum(1).create()).withDescription("the amount of coefficient decay to use").create();    Option rate = builder.withLongName("rate").withArgument(argumentBuilder.withName("learningRate").withDefault("1e-3").withMaximum(1).create()).withDescription("the learning rate").create();    Option noBias = builder.withLongName("noBias").withDescription("don't include a bias term").create();    Option targetCategories = builder.withLongName("categories").withRequired(true).withArgument(argumentBuilder.withName("number").withMaximum(1).create()).withDescription("the number of target categories to be considered").create();    Group normalArgs = new GroupBuilder().withOption(help).withOption(quiet).withOption(inputFile).withOption(outputFile).withOption(target).withOption(targetCategories).withOption(predictors).withOption(types).withOption(passes).withOption(lambda).withOption(rate).withOption(noBias).withOption(features).create();    Parser parser = new Parser();    parser.setHelpOption(help);    parser.setHelpTrigger("--help");    parser.setGroup(normalArgs);    parser.setHelpFormatter(new HelpFormatter(" ", "", " ", 130));    CommandLine cmdLine = parser.parseAndHelp(args);    if (cmdLine == null) {        return false;    }    TrainLogistic.inputFile = getStringArgument(cmdLine, inputFile);    TrainLogistic.outputFile = getStringArgument(cmdLine, outputFile);    List<String> typeList = new ArrayList<>();    for (Object x : cmdLine.getValues(types)) {        typeList.add(x.toString());    }    List<String> predictorList = new ArrayList<>();    for (Object x : cmdLine.getValues(predictors)) {        predictorList.add(x.toString());    }    lmp = new LogisticModelParameters();    lmp.setTargetVariable(getStringArgument(cmdLine, target));    lmp.setMaxTargetCategories(getIntegerArgument(cmdLine, targetCategories));    lmp.setNumFeatures(getIntegerArgument(cmdLine, features));    lmp.setUseBias(!getBooleanArgument(cmdLine, noBias));    lmp.setTypeMap(predictorList, typeList);    lmp.setLambda(getDoubleArgument(cmdLine, lambda));    lmp.setLearningRate(getDoubleArgument(cmdLine, rate));    TrainLogistic.scores = getBooleanArgument(cmdLine, scores);    TrainLogistic.passes = getIntegerArgument(cmdLine, passes);    return true;}
0
private static String getStringArgument(CommandLine cmdLine, Option inputFile)
{    return (String) cmdLine.getValue(inputFile);}
0
private static boolean getBooleanArgument(CommandLine cmdLine, Option option)
{    return cmdLine.hasOption(option);}
0
private static int getIntegerArgument(CommandLine cmdLine, Option features)
{    return Integer.parseInt((String) cmdLine.getValue(features));}
0
private static double getDoubleArgument(CommandLine cmdLine, Option op)
{    return Double.parseDouble((String) cmdLine.getValue(op));}
0
public static OnlineLogisticRegression getModel()
{    return model;}
0
public static LogisticModelParameters getParameters()
{    return lmp;}
0
 static BufferedReader open(String inputFile) throws IOException
{    InputStream in;    try {        in = Resources.getResource(inputFile).openStream();    } catch (IllegalArgumentException e) {        in = new FileInputStream(new File(inputFile));    }    return new BufferedReader(new InputStreamReader(in, Charsets.UTF_8));}
0
public static void main(String[] args) throws IOException
{    File base = new File(args[0]);    Multiset<String> overallCounts = HashMultiset.create();    int leakType = 0;    if (args.length > 1) {        leakType = Integer.parseInt(args[1]);    }    Dictionary newsGroups = new Dictionary();    NewsgroupHelper helper = new NewsgroupHelper();    helper.getEncoder().setProbes(2);    AdaptiveLogisticRegression learningAlgorithm = new AdaptiveLogisticRegression(20, NewsgroupHelper.FEATURES, new L1());    learningAlgorithm.setInterval(800);    learningAlgorithm.setAveragingWindow(500);    List<File> files = new ArrayList<>();    for (File newsgroup : base.listFiles()) {        if (newsgroup.isDirectory()) {            newsGroups.intern(newsgroup.getName());            files.addAll(Arrays.asList(newsgroup.listFiles()));        }    }    Collections.shuffle(files);    System.out.println(files.size() + " training files");    SGDInfo info = new SGDInfo();    int k = 0;    for (File file : files) {        String ng = file.getParentFile().getName();        int actual = newsGroups.intern(ng);        Vector v = helper.encodeFeatureVector(file, actual, leakType, overallCounts);        learningAlgorithm.train(actual, v);        k++;        State<AdaptiveLogisticRegression.Wrapper, CrossFoldLearner> best = learningAlgorithm.getBest();        SGDHelper.analyzeState(info, leakType, k, best);    }    learningAlgorithm.close();    SGDHelper.dissect(leakType, newsGroups, learningAlgorithm, files, overallCounts);    System.out.println("exiting main");    File modelFile = new File(System.getProperty("java.io.tmpdir"), "news-group.model");    ModelSerializer.writeBinary(modelFile.getAbsolutePath(), learningAlgorithm.getBest().getPayload().getLearner().getModels().get(0));    List<Integer> counts = new ArrayList<>();    System.out.println("Word counts");    for (String count : overallCounts.elementSet()) {        counts.add(overallCounts.count(count));    }    Collections.sort(counts, Ordering.natural().reverse());    k = 0;    for (Integer count : counts) {        System.out.println(k + "\t" + count);        k++;        if (k > 1000) {            break;        }    }}
0
public static void main(String[] args) throws IOException
{    mainToOutput(args, new PrintWriter(new OutputStreamWriter(System.out, Charsets.UTF_8), true));}
0
 static void mainToOutput(String[] args, PrintWriter output) throws IOException
{    if (parseArgs(args)) {        if (!showAuc && !showConfusion && !showScores) {            showAuc = true;            showConfusion = true;        }        Auc collector = null;        AdaptiveLogisticModelParameters lmp = AdaptiveLogisticModelParameters.loadFromFile(new File(modelFile));        CsvRecordFactory csv = lmp.getCsvRecordFactory();        AdaptiveLogisticRegression lr = lmp.createAdaptiveLogisticRegression();        if (lmp.getTargetCategories().size() <= 2) {            collector = new Auc();        }        OnlineSummarizer slh = new OnlineSummarizer();        ConfusionMatrix cm = new ConfusionMatrix(lmp.getTargetCategories(), defaultCategory);        State<Wrapper, CrossFoldLearner> best = lr.getBest();        if (best == null) {            output.println("AdaptiveLogisticRegression has not be trained probably.");            return;        }        CrossFoldLearner learner = best.getPayload().getLearner();        BufferedReader in = TrainLogistic.open(inputFile);        String line = in.readLine();        csv.firstLine(line);        line = in.readLine();        if (showScores) {            output.println("\"target\", \"model-output\", \"log-likelihood\", \"average-likelihood\"");        }        while (line != null) {            Vector v = new SequentialAccessSparseVector(lmp.getNumFeatures());                        int target = csv.processLine(line, v);            double likelihood = learner.logLikelihood(target, v);            double score = learner.classifyFull(v).maxValue();            slh.add(likelihood);            cm.addInstance(csv.getTargetString(line), csv.getTargetLabel(target));            if (showScores) {                output.printf(Locale.ENGLISH, "%8d, %.12f, %.13f, %.13f%n", target, score, learner.logLikelihood(target, v), slh.getMean());            }            if (collector != null) {                collector.add(target, score);            }            line = in.readLine();        }        output.printf(Locale.ENGLISH, "\nLog-likelihood:");        output.printf(Locale.ENGLISH, "Min=%.2f, Max=%.2f, Mean=%.2f, Median=%.2f%n", slh.getMin(), slh.getMax(), slh.getMean(), slh.getMedian());        if (collector != null) {            output.printf(Locale.ENGLISH, "%nAUC = %.2f%n", collector.auc());        }        if (showConfusion) {            output.printf(Locale.ENGLISH, "%n%s%n%n", cm.toString());            if (collector != null) {                Matrix m = collector.entropy();                output.printf(Locale.ENGLISH, "Entropy Matrix: [[%.1f, %.1f], [%.1f, %.1f]]%n", m.get(0, 0), m.get(1, 0), m.get(0, 1), m.get(1, 1));            }        }    }}
0
private static boolean parseArgs(String[] args)
{    DefaultOptionBuilder builder = new DefaultOptionBuilder();    Option help = builder.withLongName("help").withDescription("print this list").create();    Option quiet = builder.withLongName("quiet").withDescription("be extra quiet").create();    Option auc = builder.withLongName("auc").withDescription("print AUC").create();    Option confusion = builder.withLongName("confusion").withDescription("print confusion matrix").create();    Option scores = builder.withLongName("scores").withDescription("print scores").create();    ArgumentBuilder argumentBuilder = new ArgumentBuilder();    Option inputFileOption = builder.withLongName("input").withRequired(true).withArgument(argumentBuilder.withName("input").withMaximum(1).create()).withDescription("where to get validate data").create();    Option modelFileOption = builder.withLongName("model").withRequired(true).withArgument(argumentBuilder.withName("model").withMaximum(1).create()).withDescription("where to get the trained model").create();    Option defaultCagetoryOption = builder.withLongName("defaultCategory").withRequired(false).withArgument(argumentBuilder.withName("defaultCategory").withMaximum(1).withDefault("unknown").create()).withDescription("the default category value to use").create();    Group normalArgs = new GroupBuilder().withOption(help).withOption(quiet).withOption(auc).withOption(scores).withOption(confusion).withOption(inputFileOption).withOption(modelFileOption).withOption(defaultCagetoryOption).create();    Parser parser = new Parser();    parser.setHelpOption(help);    parser.setHelpTrigger("--help");    parser.setGroup(normalArgs);    parser.setHelpFormatter(new HelpFormatter(" ", "", " ", 130));    CommandLine cmdLine = parser.parseAndHelp(args);    if (cmdLine == null) {        return false;    }    inputFile = getStringArgument(cmdLine, inputFileOption);    modelFile = getStringArgument(cmdLine, modelFileOption);    defaultCategory = getStringArgument(cmdLine, defaultCagetoryOption);    showAuc = getBooleanArgument(cmdLine, auc);    showScores = getBooleanArgument(cmdLine, scores);    showConfusion = getBooleanArgument(cmdLine, confusion);    return true;}
0
private static boolean getBooleanArgument(CommandLine cmdLine, Option option)
{    return cmdLine.hasOption(option);}
0
private static String getStringArgument(CommandLine cmdLine, Option inputFile)
{    return (String) cmdLine.getValue(inputFile);}
0
public boolean accept(Path path)
{    String pathString = path.toString();    return pathString.contains("/clusters-");}
0
public void paint(Graphics g)
{    plotSampleData((Graphics2D) g);    plotClusters((Graphics2D) g);}
0
protected static void plotClusters(Graphics2D g2)
{    int cx = CLUSTERS.size() - 1;    for (List<Cluster> clusters : CLUSTERS) {        for (Cluster cluster : clusters) {            if (isSignificant(cluster)) {                g2.setStroke(new BasicStroke(1));                g2.setColor(Color.BLUE);                double[] t1 = { T1, T1 };                plotEllipse(g2, cluster.getCenter(), new DenseVector(t1));                double[] t2 = { T2, T2 };                plotEllipse(g2, cluster.getCenter(), new DenseVector(t2));                g2.setColor(COLORS[Math.min(DisplayClustering.COLORS.length - 1, cx)]);                g2.setStroke(new BasicStroke(cx == 0 ? 3 : 1));                plotEllipse(g2, cluster.getCenter(), cluster.getRadius().times(3));            }        }        cx--;    }}
0
public static void main(String[] args) throws Exception
{    Path samples = new Path("samples");    Path output = new Path("output");    Configuration conf = new Configuration();    HadoopUtil.delete(conf, samples);    HadoopUtil.delete(conf, output);    RandomUtils.useTestSeed();    generateSamples();    writeSampleData(samples);    CanopyDriver.buildClusters(conf, samples, output, new ManhattanDistanceMeasure(), T1, T2, 0, true);    loadClustersWritable(output);    new DisplayCanopy();}
0
public void initialize()
{        res = Toolkit.getDefaultToolkit().getScreenResolution();        this.setSize(SIZE * res, SIZE * res);    this.setVisible(true);    this.setTitle("Asymmetric Sample Data");        this.addWindowListener(new WindowAdapter() {        @Override        public void windowClosing(WindowEvent e) {            System.exit(0);        }    });}
0
public void windowClosing(WindowEvent e)
{    System.exit(0);}
0
public static void main(String[] args) throws Exception
{    RandomUtils.useTestSeed();    generateSamples();    new DisplayClustering();}
0
public void paint(Graphics g)
{    Graphics2D g2 = (Graphics2D) g;    plotSampleData(g2);    plotSampleParameters(g2);    plotClusters(g2);}
0
protected static void plotClusters(Graphics2D g2)
{    int cx = CLUSTERS.size() - 1;    for (List<Cluster> clusters : CLUSTERS) {        g2.setStroke(new BasicStroke(cx == 0 ? 3 : 1));        g2.setColor(COLORS[Math.min(COLORS.length - 1, cx--)]);        for (Cluster cluster : clusters) {            plotEllipse(g2, cluster.getCenter(), cluster.getRadius().times(3));        }    }}
0
protected static void plotSampleParameters(Graphics2D g2)
{    Vector v = new DenseVector(2);    Vector dv = new DenseVector(2);    g2.setColor(Color.RED);    for (Vector param : SAMPLE_PARAMS) {        v.set(0, param.get(0));        v.set(1, param.get(1));        dv.set(0, param.get(2) * 3);        dv.set(1, param.get(3) * 3);        plotEllipse(g2, v, dv);    }}
0
protected static void plotSampleData(Graphics2D g2)
{    double sx = (double) res / DS;    g2.setTransform(AffineTransform.getScaleInstance(sx, sx));        g2.setColor(Color.BLACK);    Vector dv = new DenseVector(2).assign(SIZE / 2.0);    plotRectangle(g2, new DenseVector(2).assign(2), dv);    plotRectangle(g2, new DenseVector(2).assign(-2), dv);        g2.setColor(Color.DARK_GRAY);    dv.assign(0.03);    for (VectorWritable v : SAMPLE_DATA) {        plotRectangle(g2, v.get(), dv);    }}
0
protected static void plotClusteredSampleData(Graphics2D g2, Path data)
{    double sx = (double) res / DS;    g2.setTransform(AffineTransform.getScaleInstance(sx, sx));    g2.setColor(Color.BLACK);    Vector dv = new DenseVector(2).assign(SIZE / 2.0);    plotRectangle(g2, new DenseVector(2).assign(2), dv);    plotRectangle(g2, new DenseVector(2).assign(-2), dv);        dv.assign(0.03);    Path clusteredPointsPath = new Path(data, "clusteredPoints");    Path inputPath = new Path(clusteredPointsPath, "part-m-00000");    Map<Integer, Color> colors = new HashMap<>();    int point = 0;    for (Pair<IntWritable, WeightedVectorWritable> record : new SequenceFileIterable<IntWritable, WeightedVectorWritable>(inputPath, new Configuration())) {        int clusterId = record.getFirst().get();        VectorWritable v = SAMPLE_DATA.get(point++);        Integer key = clusterId;        if (!colors.containsKey(key)) {            colors.put(key, COLORS[Math.min(COLORS.length - 1, colors.size())]);        }        plotClusteredRectangle(g2, v.get(), dv, colors.get(key));    }}
0
protected static void plotClusteredRectangle(Graphics2D g2, Vector v, Vector dv, Color color)
{    double[] flip = { 1, -1 };    Vector v2 = v.times(new DenseVector(flip));    v2 = v2.minus(dv.divide(2));    int h = SIZE / 2;    double x = v2.get(0) + h;    double y = v2.get(1) + h;    g2.setStroke(new BasicStroke(1));    g2.setColor(color);    g2.draw(new Rectangle2D.Double(x * DS, y * DS, dv.get(0) * DS, dv.get(1) * DS));}
0
protected static void plotRectangle(Graphics2D g2, Vector v, Vector dv)
{    double[] flip = { 1, -1 };    Vector v2 = v.times(new DenseVector(flip));    v2 = v2.minus(dv.divide(2));    int h = SIZE / 2;    double x = v2.get(0) + h;    double y = v2.get(1) + h;    g2.draw(new Rectangle2D.Double(x * DS, y * DS, dv.get(0) * DS, dv.get(1) * DS));}
0
protected static void plotEllipse(Graphics2D g2, Vector v, Vector dv)
{    double[] flip = { 1, -1 };    Vector v2 = v.times(new DenseVector(flip));    v2 = v2.minus(dv.divide(2));    int h = SIZE / 2;    double x = v2.get(0) + h;    double y = v2.get(1) + h;    g2.draw(new Ellipse2D.Double(x * DS, y * DS, dv.get(0) * DS, dv.get(1) * DS));}
0
protected static void generateSamples()
{    generateSamples(500, 1, 1, 3);    generateSamples(300, 1, 0, 0.5);    generateSamples(300, 0, 2, 0.1);}
0
protected static void generate2dSamples()
{    generate2dSamples(500, 1, 1, 3, 1);    generate2dSamples(300, 1, 0, 0.5, 1);    generate2dSamples(300, 0, 2, 0.1, 0.5);}
0
protected static void generateSamples(int num, double mx, double my, double sd)
{    double[] params = { mx, my, sd, sd };    SAMPLE_PARAMS.add(new DenseVector(params));        for (int i = 0; i < num; i++) {        SAMPLE_DATA.add(new VectorWritable(new DenseVector(new double[] { UncommonDistributions.rNorm(mx, sd), UncommonDistributions.rNorm(my, sd) })));    }}
1
protected static void writeSampleData(Path output) throws IOException
{    Configuration conf = new Configuration();    FileSystem fs = FileSystem.get(output.toUri(), conf);    try (SequenceFile.Writer writer = new SequenceFile.Writer(fs, conf, output, Text.class, VectorWritable.class)) {        int i = 0;        for (VectorWritable vw : SAMPLE_DATA) {            writer.append(new Text("sample_" + i++), vw);        }    }}
0
protected static List<Cluster> readClustersWritable(Path clustersIn)
{    List<Cluster> clusters = new ArrayList<>();    Configuration conf = new Configuration();    for (ClusterWritable value : new SequenceFileDirValueIterable<ClusterWritable>(clustersIn, PathType.LIST, PathFilters.logsCRCFilter(), conf)) {        Cluster cluster = value.getValue();                clusters.add(cluster);    }    return clusters;}
1
protected static void loadClustersWritable(Path output) throws IOException
{    Configuration conf = new Configuration();    FileSystem fs = FileSystem.get(output.toUri(), conf);    for (FileStatus s : fs.listStatus(output, new ClustersFilter())) {        List<Cluster> clusters = readClustersWritable(s.getPath());        CLUSTERS.add(clusters);    }}
0
protected static void generate2dSamples(int num, double mx, double my, double sdx, double sdy)
{    double[] params = { mx, my, sdx, sdy };    SAMPLE_PARAMS.add(new DenseVector(params));        for (int i = 0; i < num; i++) {        SAMPLE_DATA.add(new VectorWritable(new DenseVector(new double[] { UncommonDistributions.rNorm(mx, sdx), UncommonDistributions.rNorm(my, sdy) })));    }}
1
protected static boolean isSignificant(Cluster cluster)
{    return (double) cluster.getNumObservations() / SAMPLE_DATA.size() > significance;}
0
public void paint(Graphics g)
{    plotSampleData((Graphics2D) g);    plotClusters((Graphics2D) g);}
0
public static void main(String[] args) throws Exception
{    DistanceMeasure measure = new ManhattanDistanceMeasure();    Path samples = new Path("samples");    Path output = new Path("output");    Configuration conf = new Configuration();    HadoopUtil.delete(conf, output);    HadoopUtil.delete(conf, samples);    RandomUtils.useTestSeed();    DisplayClustering.generateSamples();    writeSampleData(samples);    boolean runClusterer = true;    int maxIterations = 10;    float threshold = 0.001F;    float m = 1.1F;    if (runClusterer) {        runSequentialFuzzyKClusterer(conf, samples, output, measure, maxIterations, m, threshold);    } else {        int numClusters = 3;        runSequentialFuzzyKClassifier(conf, samples, output, measure, numClusters, maxIterations, m, threshold);    }    new DisplayFuzzyKMeans();}
0
private static void runSequentialFuzzyKClassifier(Configuration conf, Path samples, Path output, DistanceMeasure measure, int numClusters, int maxIterations, float m, double threshold) throws IOException
{    Collection<Vector> points = Lists.newArrayList();    for (int i = 0; i < numClusters; i++) {        points.add(SAMPLE_DATA.get(i).get());    }    List<Cluster> initialClusters = Lists.newArrayList();    int id = 0;    for (Vector point : points) {        initialClusters.add(new SoftCluster(point, id++, measure));    }    ClusterClassifier prior = new ClusterClassifier(initialClusters, new FuzzyKMeansClusteringPolicy(m, threshold));    Path priorPath = new Path(output, "classifier-0");    prior.writeToSeqFiles(priorPath);    ClusterIterator.iterateSeq(conf, samples, priorPath, output, maxIterations);    loadClustersWritable(output);}
0
private static void runSequentialFuzzyKClusterer(Configuration conf, Path samples, Path output, DistanceMeasure measure, int maxIterations, float m, double threshold) throws IOException, ClassNotFoundException, InterruptedException
{    Path clustersIn = new Path(output, "random-seeds");    RandomSeedGenerator.buildRandom(conf, samples, clustersIn, 3, measure);    FuzzyKMeansDriver.run(samples, clustersIn, output, threshold, maxIterations, m, true, true, threshold, true);    loadClustersWritable(output);}
0
public static void main(String[] args) throws Exception
{    DistanceMeasure measure = new ManhattanDistanceMeasure();    Path samples = new Path("samples");    Path output = new Path("output");    Configuration conf = new Configuration();    HadoopUtil.delete(conf, samples);    HadoopUtil.delete(conf, output);    RandomUtils.useTestSeed();    generateSamples();    writeSampleData(samples);    boolean runClusterer = true;    double convergenceDelta = 0.001;    int numClusters = 3;    int maxIterations = 10;    if (runClusterer) {        runSequentialKMeansClusterer(conf, samples, output, measure, numClusters, maxIterations, convergenceDelta);    } else {        runSequentialKMeansClassifier(conf, samples, output, measure, numClusters, maxIterations, convergenceDelta);    }    new DisplayKMeans();}
0
private static void runSequentialKMeansClassifier(Configuration conf, Path samples, Path output, DistanceMeasure measure, int numClusters, int maxIterations, double convergenceDelta) throws IOException
{    Collection<Vector> points = Lists.newArrayList();    for (int i = 0; i < numClusters; i++) {        points.add(SAMPLE_DATA.get(i).get());    }    List<Cluster> initialClusters = Lists.newArrayList();    int id = 0;    for (Vector point : points) {        initialClusters.add(new org.apache.mahout.clustering.kmeans.Kluster(point, id++, measure));    }    ClusterClassifier prior = new ClusterClassifier(initialClusters, new KMeansClusteringPolicy(convergenceDelta));    Path priorPath = new Path(output, Cluster.INITIAL_CLUSTERS_DIR);    prior.writeToSeqFiles(priorPath);    ClusterIterator.iterateSeq(conf, samples, priorPath, output, maxIterations);    loadClustersWritable(output);}
0
private static void runSequentialKMeansClusterer(Configuration conf, Path samples, Path output, DistanceMeasure measure, int numClusters, int maxIterations, double convergenceDelta) throws IOException, InterruptedException, ClassNotFoundException
{    Path clustersIn = new Path(output, "random-seeds");    RandomSeedGenerator.buildRandom(conf, samples, clustersIn, numClusters, measure);    KMeansDriver.run(samples, clustersIn, output, convergenceDelta, maxIterations, true, 0.0, true);    loadClustersWritable(output);}
0
public void paint(Graphics g)
{    plotSampleData((Graphics2D) g);    plotClusters((Graphics2D) g);}
0
public static void main(String[] args) throws Exception
{    DistanceMeasure measure = new ManhattanDistanceMeasure();    Path samples = new Path(SAMPLES);    Path output = new Path(OUTPUT);    Path tempDir = new Path(TEMP);    Configuration conf = new Configuration();    HadoopUtil.delete(conf, samples);    HadoopUtil.delete(conf, output);    RandomUtils.useTestSeed();    DisplayClustering.generateSamples();    writeSampleData(samples);    Path affinities = new Path(output, AFFINITIES);    FileSystem fs = FileSystem.get(output.toUri(), conf);    if (!fs.exists(output)) {        fs.mkdirs(output);    }    try (Writer writer = new BufferedWriter(new FileWriter(affinities.toString()))) {        for (int i = 0; i < SAMPLE_DATA.size(); i++) {            for (int j = 0; j < SAMPLE_DATA.size(); j++) {                writer.write(i + "," + j + ',' + measure.distance(SAMPLE_DATA.get(i).get(), SAMPLE_DATA.get(j).get()) + '\n');            }        }    }    int maxIter = 10;    double convergenceDelta = 0.001;    SpectralKMeansDriver.run(new Configuration(), affinities, output, SAMPLE_DATA.size(), 3, measure, convergenceDelta, maxIter, tempDir);    new DisplaySpectralKMeans();}
0
public void paint(Graphics g)
{    plotClusteredSampleData((Graphics2D) g, new Path(new Path(OUTPUT), "kmeans_out"));}
0
public void printSummaries(List<OnlineSummarizer> summarizers, String type)
{    printSummaries(summarizers, type, fileOut);}
0
public static void printSummaries(List<OnlineSummarizer> summarizers, String type, PrintWriter fileOut)
{    double maxDistance = 0;    for (int i = 0; i < summarizers.size(); ++i) {        OnlineSummarizer summarizer = summarizers.get(i);        if (summarizer.getCount() > 1) {            maxDistance = Math.max(maxDistance, summarizer.getMax());            System.out.printf("Average distance in cluster %d [%d]: %f\n", i, summarizer.getCount(), summarizer.getMean());                        if (fileOut != null) {                fileOut.printf("%d,%f,%f,%f,%f,%f,%f,%f,%d,%s\n", i, summarizer.getMean(), summarizer.getSD(), summarizer.getQuartile(0), summarizer.getQuartile(1), summarizer.getQuartile(2), summarizer.getQuartile(3), summarizer.getQuartile(4), summarizer.getCount(), type);            }        } else {            System.out.printf("Cluster %d is has %d data point. Need atleast 2 data points in a cluster for" + " OnlineSummarizer.\n", i, summarizer.getCount());        }    }    System.out.printf("Num clusters: %d; maxDistance: %f\n", summarizers.size(), maxDistance);}
0
public int run(String[] args) throws IOException
{    if (!parseArgs(args)) {        return -1;    }    Configuration conf = new Configuration();    try {        fileOut = new PrintWriter(new FileOutputStream(outputFile));        fileOut.printf("cluster,distance.mean,distance.sd,distance.q0,distance.q1,distance.q2,distance.q3," + "distance.q4,count,is.train\n");                List<Centroid> centroids;        List<Centroid> centroidsCompare = null;        if (mahoutKMeansFormat) {            SequenceFileDirValueIterable<ClusterWritable> clusterIterable = new SequenceFileDirValueIterable<>(new Path(centroidFile), PathType.GLOB, conf);            centroids = Lists.newArrayList(IOUtils.getCentroidsFromClusterWritableIterable(clusterIterable));        } else {            SequenceFileDirValueIterable<CentroidWritable> centroidIterable = new SequenceFileDirValueIterable<>(new Path(centroidFile), PathType.GLOB, conf);            centroids = Lists.newArrayList(IOUtils.getCentroidsFromCentroidWritableIterable(centroidIterable));        }        if (centroidCompareFile != null) {            if (mahoutKMeansFormatCompare) {                SequenceFileDirValueIterable<ClusterWritable> clusterCompareIterable = new SequenceFileDirValueIterable<>(new Path(centroidCompareFile), PathType.GLOB, conf);                centroidsCompare = Lists.newArrayList(IOUtils.getCentroidsFromClusterWritableIterable(clusterCompareIterable));            } else {                SequenceFileDirValueIterable<CentroidWritable> centroidCompareIterable = new SequenceFileDirValueIterable<>(new Path(centroidCompareFile), PathType.GLOB, conf);                centroidsCompare = Lists.newArrayList(IOUtils.getCentroidsFromCentroidWritableIterable(centroidCompareIterable));            }        }                SequenceFileDirValueIterable<VectorWritable> trainIterable = new SequenceFileDirValueIterable<>(new Path(trainFile), PathType.GLOB, conf);        Iterable<Vector> trainDatapoints = IOUtils.getVectorsFromVectorWritableIterable(trainIterable);        Iterable<Vector> datapoints = trainDatapoints;        printSummaries(ClusteringUtils.summarizeClusterDistances(trainDatapoints, centroids, new SquaredEuclideanDistanceMeasure()), "train");                if (testFile != null) {            SequenceFileDirValueIterable<VectorWritable> testIterable = new SequenceFileDirValueIterable<>(new Path(testFile), PathType.GLOB, conf);            Iterable<Vector> testDatapoints = IOUtils.getVectorsFromVectorWritableIterable(testIterable);            printSummaries(ClusteringUtils.summarizeClusterDistances(testDatapoints, centroids, new SquaredEuclideanDistanceMeasure()), "test");            datapoints = Iterables.concat(trainDatapoints, testDatapoints);        }                List<OnlineSummarizer> summaries = ClusteringUtils.summarizeClusterDistances(datapoints, centroids, distanceMeasure);        List<OnlineSummarizer> compareSummaries = null;        if (centroidsCompare != null) {            compareSummaries = ClusteringUtils.summarizeClusterDistances(datapoints, centroidsCompare, distanceMeasure);        }        System.out.printf("[Dunn Index] First: %f", ClusteringUtils.dunnIndex(centroids, distanceMeasure, summaries));        if (compareSummaries != null) {            System.out.printf(" Second: %f\n", ClusteringUtils.dunnIndex(centroidsCompare, distanceMeasure, compareSummaries));        } else {            System.out.printf("\n");        }        System.out.printf("[Davies-Bouldin Index] First: %f", ClusteringUtils.daviesBouldinIndex(centroids, distanceMeasure, summaries));        if (compareSummaries != null) {            System.out.printf(" Second: %f\n", ClusteringUtils.daviesBouldinIndex(centroidsCompare, distanceMeasure, compareSummaries));        } else {            System.out.printf("\n");        }    } catch (IOException e) {        System.out.println(e.getMessage());    } finally {        Closeables.close(fileOut, false);    }    return 0;}
0
private boolean parseArgs(String[] args)
{    DefaultOptionBuilder builder = new DefaultOptionBuilder();    Option help = builder.withLongName("help").withDescription("print this list").create();    ArgumentBuilder argumentBuilder = new ArgumentBuilder();    Option inputFileOption = builder.withLongName("input").withShortName("i").withRequired(true).withArgument(argumentBuilder.withName("input").withMaximum(1).create()).withDescription("where to get seq files with the vectors (training set)").create();    Option testInputFileOption = builder.withLongName("testInput").withShortName("itest").withArgument(argumentBuilder.withName("testInput").withMaximum(1).create()).withDescription("where to get seq files with the vectors (test set)").create();    Option centroidsFileOption = builder.withLongName("centroids").withShortName("c").withRequired(true).withArgument(argumentBuilder.withName("centroids").withMaximum(1).create()).withDescription("where to get seq files with the centroids (from Mahout KMeans or StreamingKMeansDriver)").create();    Option centroidsCompareFileOption = builder.withLongName("centroidsCompare").withShortName("cc").withRequired(false).withArgument(argumentBuilder.withName("centroidsCompare").withMaximum(1).create()).withDescription("where to get seq files with the second set of centroids (from Mahout KMeans or " + "StreamingKMeansDriver)").create();    Option outputFileOption = builder.withLongName("output").withShortName("o").withRequired(true).withArgument(argumentBuilder.withName("output").withMaximum(1).create()).withDescription("where to dump the CSV file with the results").create();    Option mahoutKMeansFormatOption = builder.withLongName("mahoutkmeansformat").withShortName("mkm").withDescription("if set, read files as (IntWritable, ClusterWritable) pairs").withArgument(argumentBuilder.withName("numpoints").withMaximum(1).create()).create();    Option mahoutKMeansCompareFormatOption = builder.withLongName("mahoutkmeansformatCompare").withShortName("mkmc").withDescription("if set, read files as (IntWritable, ClusterWritable) pairs").withArgument(argumentBuilder.withName("numpoints").withMaximum(1).create()).create();    Group normalArgs = new GroupBuilder().withOption(help).withOption(inputFileOption).withOption(testInputFileOption).withOption(outputFileOption).withOption(centroidsFileOption).withOption(centroidsCompareFileOption).withOption(mahoutKMeansFormatOption).withOption(mahoutKMeansCompareFormatOption).create();    Parser parser = new Parser();    parser.setHelpOption(help);    parser.setHelpTrigger("--help");    parser.setGroup(normalArgs);    parser.setHelpFormatter(new HelpFormatter(" ", "", " ", 150));    CommandLine cmdLine = parser.parseAndHelp(args);    if (cmdLine == null) {        return false;    }    trainFile = (String) cmdLine.getValue(inputFileOption);    if (cmdLine.hasOption(testInputFileOption)) {        testFile = (String) cmdLine.getValue(testInputFileOption);    }    centroidFile = (String) cmdLine.getValue(centroidsFileOption);    if (cmdLine.hasOption(centroidsCompareFileOption)) {        centroidCompareFile = (String) cmdLine.getValue(centroidsCompareFileOption);    }    outputFile = (String) cmdLine.getValue(outputFileOption);    if (cmdLine.hasOption(mahoutKMeansFormatOption)) {        mahoutKMeansFormat = true;    }    if (cmdLine.hasOption(mahoutKMeansCompareFormatOption)) {        mahoutKMeansFormatCompare = true;    }    return true;}
0
public static void main(String[] args) throws IOException
{    new ClusterQualitySummarizer().run(args);}
0
public static Iterable<Centroid> getCentroidsFromCentroidWritableIterable(Iterable<CentroidWritable> dirIterable)
{    return Iterables.transform(dirIterable, new Function<CentroidWritable, Centroid>() {        @Override        public Centroid apply(CentroidWritable input) {            Preconditions.checkNotNull(input);            return input.getCentroid().clone();        }    });}
0
public Centroid apply(CentroidWritable input)
{    Preconditions.checkNotNull(input);    return input.getCentroid().clone();}
0
public static Iterable<Centroid> getCentroidsFromClusterWritableIterable(Iterable<ClusterWritable> dirIterable)
{    return Iterables.transform(dirIterable, new Function<ClusterWritable, Centroid>() {        int numClusters = 0;        @Override        public Centroid apply(ClusterWritable input) {            Preconditions.checkNotNull(input);            return new Centroid(numClusters++, input.getValue().getCenter().clone(), input.getValue().getTotalObservations());        }    });}
0
public Centroid apply(ClusterWritable input)
{    Preconditions.checkNotNull(input);    return new Centroid(numClusters++, input.getValue().getCenter().clone(), input.getValue().getTotalObservations());}
0
public static Iterable<Vector> getVectorsFromVectorWritableIterable(Iterable<VectorWritable> dirIterable)
{    return Iterables.transform(dirIterable, new Function<VectorWritable, Vector>() {        @Override        public Vector apply(VectorWritable input) {            Preconditions.checkNotNull(input);            return input.get().clone();        }    });}
0
public Vector apply(VectorWritable input)
{    Preconditions.checkNotNull(input);    return input.get().clone();}
0
public static void main(String[] args) throws Exception
{    if (args.length > 0) {                ToolRunner.run(new Configuration(), new Job(), args);    } else {                Path output = new Path("output");        HadoopUtil.delete(new Configuration(), output);        run(new Path("testdata"), output, new EuclideanDistanceMeasure(), 80, 55);    }}
1
private static void run(Path input, Path output, DistanceMeasure measure, double t1, double t2) throws Exception
{    Path directoryContainingConvertedInput = new Path(output, DIRECTORY_CONTAINING_CONVERTED_INPUT);    InputDriver.runJob(input, directoryContainingConvertedInput, "org.apache.mahout.math.RandomAccessSparseVector");    CanopyDriver.run(new Configuration(), directoryContainingConvertedInput, output, measure, t1, t2, true, 0.0, false);        ClusterDumper clusterDumper = new ClusterDumper(new Path(output, "clusters-0-final"), new Path(output, "clusteredPoints"));    clusterDumper.printClusters(null);}
0
public int run(String[] args) throws Exception
{    addInputOption();    addOutputOption();    addOption(DefaultOptionCreator.distanceMeasureOption().create());    addOption(DefaultOptionCreator.t1Option().create());    addOption(DefaultOptionCreator.t2Option().create());    addOption(DefaultOptionCreator.overwriteOption().create());    Map<String, List<String>> argMap = parseArguments(args);    if (argMap == null) {        return -1;    }    Path input = getInputPath();    Path output = getOutputPath();    if (hasOption(DefaultOptionCreator.OVERWRITE_OPTION)) {        HadoopUtil.delete(new Configuration(), output);    }    String measureClass = getOption(DefaultOptionCreator.DISTANCE_MEASURE_OPTION);    double t1 = Double.parseDouble(getOption(DefaultOptionCreator.T1_OPTION));    double t2 = Double.parseDouble(getOption(DefaultOptionCreator.T2_OPTION));    DistanceMeasure measure = ClassUtils.instantiateAs(measureClass, DistanceMeasure.class);    run(input, output, measure, t1, t2);    return 0;}
0
public static void main(String[] args) throws Exception
{    if (args.length > 0) {                ToolRunner.run(new Configuration(), new Job(), args);    } else {                Path output = new Path("output");        Configuration conf = new Configuration();        HadoopUtil.delete(conf, output);        run(conf, new Path("testdata"), output, new EuclideanDistanceMeasure(), 80, 55, 10, 2.0f, 0.5);    }}
1
public int run(String[] args) throws Exception
{    addInputOption();    addOutputOption();    addOption(DefaultOptionCreator.distanceMeasureOption().create());    addOption(DefaultOptionCreator.convergenceOption().create());    addOption(DefaultOptionCreator.maxIterationsOption().create());    addOption(DefaultOptionCreator.overwriteOption().create());    addOption(DefaultOptionCreator.t1Option().create());    addOption(DefaultOptionCreator.t2Option().create());    addOption(M_OPTION, M_OPTION, "coefficient normalization factor, must be greater than 1", true);    Map<String, List<String>> argMap = parseArguments(args);    if (argMap == null) {        return -1;    }    Path input = getInputPath();    Path output = getOutputPath();    String measureClass = getOption(DefaultOptionCreator.DISTANCE_MEASURE_OPTION);    if (measureClass == null) {        measureClass = SquaredEuclideanDistanceMeasure.class.getName();    }    double convergenceDelta = Double.parseDouble(getOption(DefaultOptionCreator.CONVERGENCE_DELTA_OPTION));    int maxIterations = Integer.parseInt(getOption(DefaultOptionCreator.MAX_ITERATIONS_OPTION));    float fuzziness = Float.parseFloat(getOption(M_OPTION));    addOption(new DefaultOptionBuilder().withLongName(M_OPTION).withRequired(true).withArgument(new ArgumentBuilder().withName(M_OPTION).withMinimum(1).withMaximum(1).create()).withDescription("coefficient normalization factor, must be greater than 1").withShortName(M_OPTION).create());    if (hasOption(DefaultOptionCreator.OVERWRITE_OPTION)) {        HadoopUtil.delete(getConf(), output);    }    DistanceMeasure measure = ClassUtils.instantiateAs(measureClass, DistanceMeasure.class);    double t1 = Double.parseDouble(getOption(DefaultOptionCreator.T1_OPTION));    double t2 = Double.parseDouble(getOption(DefaultOptionCreator.T2_OPTION));    run(getConf(), input, output, measure, t1, t2, maxIterations, fuzziness, convergenceDelta);    return 0;}
0
public static void run(Configuration conf, Path input, Path output, DistanceMeasure measure, double t1, double t2, int maxIterations, float fuzziness, double convergenceDelta) throws Exception
{    Path directoryContainingConvertedInput = new Path(output, DIRECTORY_CONTAINING_CONVERTED_INPUT);        InputDriver.runJob(input, directoryContainingConvertedInput, "org.apache.mahout.math.RandomAccessSparseVector");        Path canopyOutput = new Path(output, "canopies");    CanopyDriver.run(new Configuration(), directoryContainingConvertedInput, canopyOutput, measure, t1, t2, false, 0.0, false);        FuzzyKMeansDriver.run(directoryContainingConvertedInput, new Path(canopyOutput, "clusters-0-final"), output, convergenceDelta, maxIterations, fuzziness, true, true, 0.0, false);        ClusterDumper clusterDumper = new ClusterDumper(new Path(output, "clusters-*-final"), new Path(output, "clusteredPoints"));    clusterDumper.printClusters(null);}
1
public static void main(String[] args) throws Exception
{    if (args.length > 0) {                ToolRunner.run(new Configuration(), new Job(), args);    } else {                Path output = new Path("output");        Configuration conf = new Configuration();        HadoopUtil.delete(conf, output);        run(conf, new Path("testdata"), output, new EuclideanDistanceMeasure(), 6, 0.5, 10);    }}
1
public int run(String[] args) throws Exception
{    addInputOption();    addOutputOption();    addOption(DefaultOptionCreator.distanceMeasureOption().create());    addOption(DefaultOptionCreator.numClustersOption().create());    addOption(DefaultOptionCreator.t1Option().create());    addOption(DefaultOptionCreator.t2Option().create());    addOption(DefaultOptionCreator.convergenceOption().create());    addOption(DefaultOptionCreator.maxIterationsOption().create());    addOption(DefaultOptionCreator.overwriteOption().create());    Map<String, List<String>> argMap = parseArguments(args);    if (argMap == null) {        return -1;    }    Path input = getInputPath();    Path output = getOutputPath();    String measureClass = getOption(DefaultOptionCreator.DISTANCE_MEASURE_OPTION);    if (measureClass == null) {        measureClass = SquaredEuclideanDistanceMeasure.class.getName();    }    double convergenceDelta = Double.parseDouble(getOption(DefaultOptionCreator.CONVERGENCE_DELTA_OPTION));    int maxIterations = Integer.parseInt(getOption(DefaultOptionCreator.MAX_ITERATIONS_OPTION));    if (hasOption(DefaultOptionCreator.OVERWRITE_OPTION)) {        HadoopUtil.delete(getConf(), output);    }    DistanceMeasure measure = ClassUtils.instantiateAs(measureClass, DistanceMeasure.class);    if (hasOption(DefaultOptionCreator.NUM_CLUSTERS_OPTION)) {        int k = Integer.parseInt(getOption(DefaultOptionCreator.NUM_CLUSTERS_OPTION));        run(getConf(), input, output, measure, k, convergenceDelta, maxIterations);    } else {        double t1 = Double.parseDouble(getOption(DefaultOptionCreator.T1_OPTION));        double t2 = Double.parseDouble(getOption(DefaultOptionCreator.T2_OPTION));        run(getConf(), input, output, measure, t1, t2, convergenceDelta, maxIterations);    }    return 0;}
0
public static void run(Configuration conf, Path input, Path output, DistanceMeasure measure, int k, double convergenceDelta, int maxIterations) throws Exception
{    Path directoryContainingConvertedInput = new Path(output, DIRECTORY_CONTAINING_CONVERTED_INPUT);        InputDriver.runJob(input, directoryContainingConvertedInput, "org.apache.mahout.math.RandomAccessSparseVector");        Path clusters = new Path(output, "random-seeds");    clusters = RandomSeedGenerator.buildRandom(conf, directoryContainingConvertedInput, clusters, k, measure);        KMeansDriver.run(conf, directoryContainingConvertedInput, clusters, output, convergenceDelta, maxIterations, true, 0.0, false);        Path outGlob = new Path(output, "clusters-*-final");    Path clusteredPoints = new Path(output, "clusteredPoints");        ClusterDumper clusterDumper = new ClusterDumper(outGlob, clusteredPoints);    clusterDumper.printClusters(null);}
1
public static void run(Configuration conf, Path input, Path output, DistanceMeasure measure, double t1, double t2, double convergenceDelta, int maxIterations) throws Exception
{    Path directoryContainingConvertedInput = new Path(output, DIRECTORY_CONTAINING_CONVERTED_INPUT);        InputDriver.runJob(input, directoryContainingConvertedInput, "org.apache.mahout.math.RandomAccessSparseVector");        Path canopyOutput = new Path(output, "canopies");    CanopyDriver.run(new Configuration(), directoryContainingConvertedInput, canopyOutput, measure, t1, t2, false, 0.0, false);        KMeansDriver.run(conf, directoryContainingConvertedInput, new Path(canopyOutput, Cluster.INITIAL_CLUSTERS_DIR + "-final"), output, convergenceDelta, maxIterations, true, 0.0, false);        ClusterDumper clusterDumper = new ClusterDumper(new Path(output, "clusters-*-final"), new Path(output, "clusteredPoints"));    clusterDumper.printClusters(null);}
1
protected void reduce(Text key, Iterable<StringTuple> values, Context context) throws IOException, InterruptedException
{    Set<String> outputValues = new HashSet<>();    for (StringTuple value : values) {        outputValues.addAll(value.getEntries());    }    context.write(key, new StringTuple(outputValues));}
0
public static void startJob(Parameters params) throws IOException, InterruptedException, ClassNotFoundException
{    Configuration conf = new Configuration();    conf.set("job.parameters", params.toString());    conf.set("mapred.compress.map.output", "true");    conf.set("mapred.output.compression.type", "BLOCK");    conf.set("mapred.map.output.compression.codec", "org.apache.hadoop.io.compress.GzipCodec");    conf.set("io.serializations", "org.apache.hadoop.io.serializer.JavaSerialization," + "org.apache.hadoop.io.serializer.WritableSerialization");    String input = params.get("input");    Job job = new Job(conf, "Generating dataset based from input" + input);    job.setJarByClass(KeyBasedStringTupleGrouper.class);    job.setMapOutputKeyClass(Text.class);    job.setMapOutputValueClass(StringTuple.class);    job.setOutputKeyClass(Text.class);    job.setOutputValueClass(Text.class);    FileInputFormat.addInputPath(job, new Path(input));    Path outPath = new Path(params.get("output"));    FileOutputFormat.setOutputPath(job, outPath);    HadoopUtil.delete(conf, outPath);    job.setInputFormatClass(TextInputFormat.class);    job.setMapperClass(KeyBasedStringTupleMapper.class);    job.setCombinerClass(KeyBasedStringTupleCombiner.class);    job.setReducerClass(KeyBasedStringTupleReducer.class);    job.setOutputFormatClass(TextOutputFormat.class);    boolean succeeded = job.waitForCompletion(true);    if (!succeeded) {        throw new IllegalStateException("Job failed!");    }}
0
protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException
{    String[] fields = splitter.split(value.toString());    if (fields.length != 4) {                context.getCounter("Map", "ERROR").increment(1);        return;    }    Collection<String> oKey = new ArrayList<>();    for (int groupingField : groupingFields) {        oKey.add(fields[groupingField]);        context.setStatus(fields[groupingField]);    }    List<String> oValue = new ArrayList<>();    for (int selectedField : selectedFields) {        oValue.add(fields[selectedField]);    }    context.write(new Text(oKey.toString()), new StringTuple(oValue));}
1
protected void setup(Context context) throws IOException, InterruptedException
{    super.setup(context);    Parameters params = new Parameters(context.getConfiguration().get("job.parameters", ""));    splitter = Pattern.compile(params.get("splitPattern", "[ \t]*\t[ \t]*"));    int selectedFieldCount = Integer.valueOf(params.get("selectedFieldCount", "0"));    selectedFields = new int[selectedFieldCount];    for (int i = 0; i < selectedFieldCount; i++) {        selectedFields[i] = Integer.valueOf(params.get("field" + i, "0"));    }    int groupingFieldCount = Integer.valueOf(params.get("groupingFieldCount", "0"));    groupingFields = new int[groupingFieldCount];    for (int i = 0; i < groupingFieldCount; i++) {        groupingFields[i] = Integer.valueOf(params.get("gfield" + i, "0"));    }}
0
protected void reduce(Text key, Iterable<StringTuple> values, Context context) throws IOException, InterruptedException
{    Collection<String> items = new HashSet<>();    for (StringTuple value : values) {        for (String field : value.getEntries()) {            items.add(field);        }    }    if (items.size() > 1) {        int i = 0;        StringBuilder sb = new StringBuilder();        String sep = "";        for (String field : items) {            if (i % maxTransactionLength == 0) {                if (i != 0) {                    context.write(null, new Text(sb.toString()));                }                sb.replace(0, sb.length(), "");                sep = "";            }            sb.append(sep).append(field);            sep = "\t";            i++;        }        if (sb.length() > 0) {            context.write(null, new Text(sb.toString()));        }    }}
0
protected void setup(Context context) throws IOException, InterruptedException
{    super.setup(context);    Parameters params = new Parameters(context.getConfiguration().get("job.parameters", ""));    maxTransactionLength = Integer.valueOf(params.get("maxTransactionLength", "100"));}
0
public static void main(String[] args) throws IOException, InterruptedException, ClassNotFoundException
{    DefaultOptionBuilder obuilder = new DefaultOptionBuilder();    ArgumentBuilder abuilder = new ArgumentBuilder();    GroupBuilder gbuilder = new GroupBuilder();    Option inputDirOpt = DefaultOptionCreator.inputOption().create();    Option outputOpt = DefaultOptionCreator.outputOption().create();    Option helpOpt = DefaultOptionCreator.helpOption();    Option recordSplitterOpt = obuilder.withLongName("splitterPattern").withArgument(abuilder.withName("splitterPattern").withMinimum(1).withMaximum(1).create()).withDescription("Regular Expression pattern used to split given line into fields." + " Default value splits comma or tab separated fields." + " Default Value: \"[ ,\\t]*\\t[ ,\\t]*\" ").withShortName("regex").create();    Option encodingOpt = obuilder.withLongName("encoding").withArgument(abuilder.withName("encoding").withMinimum(1).withMaximum(1).create()).withDescription("(Optional) The file encoding.  Default value: UTF-8").withShortName("e").create();    Group group = gbuilder.withName("Options").withOption(inputDirOpt).withOption(outputOpt).withOption(helpOpt).withOption(recordSplitterOpt).withOption(encodingOpt).create();    try {        Parser parser = new Parser();        parser.setGroup(group);        CommandLine cmdLine = parser.parse(args);        if (cmdLine.hasOption(helpOpt)) {            CommandLineUtil.printHelp(group);            return;        }        Parameters params = new Parameters();        if (cmdLine.hasOption(recordSplitterOpt)) {            params.set("splitPattern", (String) cmdLine.getValue(recordSplitterOpt));        }        String encoding = "UTF-8";        if (cmdLine.hasOption(encodingOpt)) {            encoding = (String) cmdLine.getValue(encodingOpt);        }        params.set("encoding", encoding);        String inputDir = (String) cmdLine.getValue(inputDirOpt);        String outputDir = (String) cmdLine.getValue(outputOpt);        params.set("input", inputDir);        params.set("output", outputDir);        params.set("groupingFieldCount", "2");        params.set("gfield0", "1");        params.set("gfield1", "2");        params.set("selectedFieldCount", "1");        params.set("field0", "3");        params.set("maxTransactionLength", "100");        KeyBasedStringTupleGrouper.startJob(params);    } catch (OptionException ex) {        CommandLineUtil.printHelp(group);    }}
0
public void serializationWithoutCsv() throws IOException
{    LogisticModelParameters params = new LogisticModelParameters();    params.setTargetVariable("foo");    params.setTypeMap(Collections.<String, String>emptyMap());    params.setTargetCategories(Arrays.asList("foo", "bar"));    params.setNumFeatures(1);    params.createRegression();        params.saveTo(new ByteArrayOutputStream());}
0
public void testCategoryOrdering()
{    ModelDissector.Weight w = new ModelDissector.Weight("a", new DenseVector(new double[] { -2, -5, 5, 2, 4, 1, 0 }), 4);    assertEquals(1, w.getCategory(0), 0);    assertEquals(-5, w.getWeight(0), 0);    assertEquals(2, w.getCategory(1), 0);    assertEquals(5, w.getWeight(1), 0);    assertEquals(4, w.getCategory(2), 0);    assertEquals(4, w.getWeight(2), 0);    assertEquals(0, w.getCategory(3), 0);    assertEquals(-2, w.getWeight(3), 0);}
0
public void example131() throws Exception
{    String outputFile = getTestTempFile("model").getAbsolutePath();    StringWriter sw = new StringWriter();    PrintWriter pw = new PrintWriter(sw, true);    TrainLogistic.mainToOutput(new String[] { "--input", "donut.csv", "--output", outputFile, "--target", "color", "--categories", "2", "--predictors", "x", "y", "--types", "numeric", "--features", "20", "--passes", "100", "--rate", "50" }, pw);    String trainOut = sw.toString();    assertTrue(trainOut.contains("x -0.7"));    assertTrue(trainOut.contains("y -0.4"));    LogisticModelParameters lmp = TrainLogistic.getParameters();    assertEquals(1.0e-4, lmp.getLambda(), 1.0e-9);    assertEquals(20, lmp.getNumFeatures());    assertTrue(lmp.useBias());    assertEquals("color", lmp.getTargetVariable());    CsvRecordFactory csv = lmp.getCsvRecordFactory();    assertEquals("[1, 2]", new TreeSet<>(csv.getTargetCategories()).toString());    assertEquals("[Intercept Term, x, y]", Sets.newTreeSet(csv.getPredictors()).toString());        AbstractVectorClassifier model = TrainLogistic.getModel();    List<String> data = Resources.readLines(Resources.getResource("donut.csv"), Charsets.UTF_8);    Map<String, Double> expectedValues = ImmutableMap.of("x", -0.7, "y", -0.43, "Intercept Term", -0.15);    verifyModel(lmp, csv, data, model, expectedValues);        try (InputStream in = new FileInputStream(new File(outputFile))) {        LogisticModelParameters lmpOut = LogisticModelParameters.loadFrom(in);        CsvRecordFactory csvOut = lmpOut.getCsvRecordFactory();        csvOut.firstLine(data.get(0));        OnlineLogisticRegression lrOut = lmpOut.createRegression();        verifyModel(lmpOut, csvOut, data, lrOut, expectedValues);    }    sw = new StringWriter();    pw = new PrintWriter(sw, true);    RunLogistic.mainToOutput(new String[] { "--input", "donut.csv", "--model", outputFile, "--auc", "--confusion" }, pw);    trainOut = sw.toString();    assertTrue(trainOut.contains("AUC = 0.57"));    assertTrue(trainOut.contains("confusion: [[27.0, 13.0], [0.0, 0.0]]"));}
0
public void example132() throws Exception
{    String outputFile = getTestTempFile("model").getAbsolutePath();    StringWriter sw = new StringWriter();    PrintWriter pw = new PrintWriter(sw, true);    TrainLogistic.mainToOutput(new String[] { "--input", "donut.csv", "--output", outputFile, "--target", "color", "--categories", "2", "--predictors", "x", "y", "a", "b", "c", "--types", "numeric", "--features", "20", "--passes", "100", "--rate", "50" }, pw);    String trainOut = sw.toString();    assertTrue(trainOut.contains("a 0."));    assertTrue(trainOut.contains("b -1."));    assertTrue(trainOut.contains("c -25."));    sw = new StringWriter();    pw = new PrintWriter(sw, true);    RunLogistic.mainToOutput(new String[] { "--input", "donut.csv", "--model", outputFile, "--auc", "--confusion" }, pw);    trainOut = sw.toString();    assertTrue(trainOut.contains("AUC = 1.00"));    sw = new StringWriter();    pw = new PrintWriter(sw, true);    RunLogistic.mainToOutput(new String[] { "--input", "donut-test.csv", "--model", outputFile, "--auc", "--confusion" }, pw);    trainOut = sw.toString();    assertTrue(trainOut.contains("AUC = 0.9"));}
0
private static void verifyModel(LogisticModelParameters lmp, RecordFactory csv, List<String> data, AbstractVectorClassifier model, Map<String, Double> expectedValues)
{    ModelDissector md = new ModelDissector();    for (String line : data.subList(1, data.size())) {        Vector v = new DenseVector(lmp.getNumFeatures());        csv.getTraceDictionary().clear();        csv.processLine(line, v);        md.update(v, csv.getTraceDictionary(), model);    }        List<ModelDissector.Weight> weights = md.summary(10);    Set<String> expected = Sets.newHashSet(expectedValues.keySet());    for (ModelDissector.Weight weight : weights) {        assertTrue(expected.remove(weight.getFeature()));        assertEquals(expectedValues.get(weight.getFeature()), weight.getWeight(), 0.1);    }    assertEquals(0, expected.size());}
0
public void setUp() throws Exception
{    super.setUp();    configuration = getConfiguration();    output = getTestTempDirPath();}
0
public void testAcceptNotFinal() throws Exception
{    Path path0 = new Path(output, "clusters-0");    Path path1 = new Path(output, "clusters-1");    path0.getFileSystem(configuration).createNewFile(path0);    path1.getFileSystem(configuration).createNewFile(path1);    PathFilter clustersFilter = new ClustersFilter();    assertTrue(clustersFilter.accept(path0));    assertTrue(clustersFilter.accept(path1));}
0
public void testAcceptFinalPath() throws IOException
{    Path path0 = new Path(output, "clusters-0");    Path path1 = new Path(output, "clusters-1");    Path path2 = new Path(output, "clusters-2");    Path path3Final = new Path(output, "clusters-3-final");    path0.getFileSystem(configuration).createNewFile(path0);    path1.getFileSystem(configuration).createNewFile(path1);    path2.getFileSystem(configuration).createNewFile(path2);    path3Final.getFileSystem(configuration).createNewFile(path3Final);    PathFilter clustersFilter = new ClustersFilter();    assertTrue(clustersFilter.accept(path0));    assertTrue(clustersFilter.accept(path1));    assertTrue(clustersFilter.accept(path2));    assertTrue(clustersFilter.accept(path3Final));}
0
public static double binomial(double n, long k)
{    if (k < 0) {        return 0;    }    if (k == 0) {        return 1;    }    if (k == 1) {        return n;    }        double a = n - k + 1;    double b = 1;    double binomial = 1;    for (long i = k; i-- > 0; ) {        binomial *= (a++) / (b++);    }    return binomial;}
0
public static double binomial(long n, long k)
{    if (k < 0) {        return 0;    }    if (k == 0 || k == n) {        return 1;    }    if (k == 1 || k == n - 1) {        return n;    }        if (n > k) {        int max = LONG_FACTORIALS.length + DOUBLE_FACTORIALS.length;        if (n < max) {                        double n_fac = factorial((int) n);            double k_fac = factorial((int) k);            double n_minus_k_fac = factorial((int) (n - k));            double nk = n_minus_k_fac * k_fac;            if (nk != Double.POSITIVE_INFINITY) {                                return n_fac / nk;            }        }        if (k > n / 2) {            k = n - k;        }        }        long a = n - k + 1;    long b = 1;    double binomial = 1;    for (long i = k; i-- > 0; ) {        binomial *= (double) a++ / (b++);    }    return binomial;}
0
public static long ceil(double value)
{    return Math.round(Math.ceil(value));}
0
public static double chbevl(double x, double[] coef, int N)
{    int p = 0;    double b0 = coef[p++];    double b1 = 0.0;    int i = N - 1;    double b2;    do {        b2 = b1;        b1 = b0;        b0 = x * b1 - b2 + coef[p++];    } while (--i > 0);    return 0.5 * (b0 - b2);}
0
private static double factorial(int k)
{    if (k < 0) {        throw new IllegalArgumentException();    }    int length1 = LONG_FACTORIALS.length;    if (k < length1) {        return LONG_FACTORIALS[k];    }    int length2 = DOUBLE_FACTORIALS.length;    if (k < length1 + length2) {        return DOUBLE_FACTORIALS[k - length1];    } else {        return Double.POSITIVE_INFINITY;    }}
0
public static long floor(double value)
{    return Math.round(Math.floor(value));}
0
public static double log(double base, double value)
{    return Math.log(value) / Math.log(base);}
0
public static double log10(double value)
{        return Math.log(value) * 0.43429448190325176;}
0
public static double log2(double value)
{        return Math.log(value) * 1.4426950408889634;}
0
public static double logFactorial(int k)
{    if (k >= 30) {        double r = 1.0 / k;        double rr = r * r;        double C7 = -5.95238095238095238e-04;        double C5 = 7.93650793650793651e-04;        double C3 = -2.77777777777777778e-03;        double C1 = 8.33333333333333333e-02;        double C0 = 9.18938533204672742e-01;        return (k + 0.5) * Math.log(k) - k + C0 + r * (C1 + rr * (C3 + rr * (C5 + rr * C7)));    } else {        return LOG_FACTORIALS[k];    }}
0
public static long longFactorial(int k)
{    if (k < 0) {        throw new IllegalArgumentException("Negative k");    }    if (k < LONG_FACTORIALS.length) {        return LONG_FACTORIALS[k];    }    throw new IllegalArgumentException("Overflow");}
0
public static double stirlingCorrection(int k)
{    if (k > 30) {        double r = 1.0 / k;        double rr = r * r;                double C7 = -5.95238095238095238e-04;                double C5 = 7.93650793650793651e-04;                double C3 = -2.77777777777777778e-03;                double C1 = 8.33333333333333333e-02;        return r * (C1 + rr * (C3 + rr * (C5 + rr * C7)));    } else {        return STIRLING_CORRECTION[k];    }}
0
public static void useTestSeed()
{    testSeed = true;    synchronized (INSTANCES) {        for (RandomWrapper rng : INSTANCES.keySet()) {            rng.resetToTestSeed();        }    }}
0
public static RandomWrapper getRandom()
{    RandomWrapper random = new RandomWrapper();    if (testSeed) {        random.resetToTestSeed();    }    INSTANCES.put(random, Boolean.TRUE);    return random;}
0
public static Random getRandom(long seed)
{    RandomWrapper random = new RandomWrapper(seed);    INSTANCES.put(random, Boolean.TRUE);    return random;}
0
public static int hashDouble(double value)
{    return Longs.hashCode(Double.doubleToLongBits(value));}
0
public static int hashFloat(float value)
{    return Float.floatToIntBits(value);}
0
public static int nextTwinPrime(int n)
{    if (n > MAX_INT_SMALLER_TWIN_PRIME) {        throw new IllegalArgumentException();    }    if (n <= 3) {        return 5;    }    int next = Primes.nextPrime(n);    while (!Primes.isPrime(next + 2)) {        next = Primes.nextPrime(next + 4);    }    return next + 2;}
0
public void setSeed(long seed)
{        if (random != null) {        random.setSeed(seed);    }}
0
 void resetToTestSeed()
{    setSeed(STANDARD_SEED);}
0
public RandomGenerator getRandomGenerator()
{    return random;}
0
protected int next(int bits)
{        throw new UnsupportedOperationException();}
0
public void nextBytes(byte[] bytes)
{    random.nextBytes(bytes);}
0
public int nextInt()
{    return random.nextInt();}
0
public int nextInt(int n)
{    return random.nextInt(n);}
0
public long nextLong()
{    return random.nextLong();}
0
public boolean nextBoolean()
{    return random.nextBoolean();}
0
public float nextFloat()
{    return random.nextFloat();}
0
public double nextDouble()
{    return random.nextDouble();}
0
public double nextGaussian()
{    return random.nextGaussian();}
0
public int columnSize()
{    return columns;}
0
public int rowSize()
{    return rows;}
0
public Iterator<MatrixSlice> iterator()
{    return iterateAll();}
0
public Iterator<MatrixSlice> iterateAll()
{    return new AbstractIterator<MatrixSlice>() {        private int row;        @Override        protected MatrixSlice computeNext() {            if (row >= numRows()) {                return endOfData();            }            int i = row++;            return new MatrixSlice(viewRow(i), i);        }    };}
0
protected MatrixSlice computeNext()
{    if (row >= numRows()) {        return endOfData();    }    int i = row++;    return new MatrixSlice(viewRow(i), i);}
0
public Iterator<MatrixSlice> iterateNonEmpty()
{    return iterator();}
0
public int numSlices()
{    return numRows();}
0
public double get(String rowLabel, String columnLabel)
{    if (columnLabelBindings == null || rowLabelBindings == null) {        throw new IllegalStateException("Unbound label");    }    Integer row = rowLabelBindings.get(rowLabel);    Integer col = columnLabelBindings.get(columnLabel);    if (row == null || col == null) {        throw new IllegalStateException("Unbound label");    }    return get(row, col);}
0
public Map<String, Integer> getColumnLabelBindings()
{    return columnLabelBindings;}
0
public Map<String, Integer> getRowLabelBindings()
{    return rowLabelBindings;}
0
public void set(String rowLabel, double[] rowData)
{    if (columnLabelBindings == null) {        throw new IllegalStateException("Unbound label");    }    Integer row = rowLabelBindings.get(rowLabel);    if (row == null) {        throw new IllegalStateException("Unbound label");    }    set(row, rowData);}
0
public void set(String rowLabel, int row, double[] rowData)
{    if (rowLabelBindings == null) {        rowLabelBindings = new HashMap<>();    }    rowLabelBindings.put(rowLabel, row);    set(row, rowData);}
0
public void set(String rowLabel, String columnLabel, double value)
{    if (columnLabelBindings == null || rowLabelBindings == null) {        throw new IllegalStateException("Unbound label");    }    Integer row = rowLabelBindings.get(rowLabel);    Integer col = columnLabelBindings.get(columnLabel);    if (row == null || col == null) {        throw new IllegalStateException("Unbound label");    }    set(row, col, value);}
0
public void set(String rowLabel, String columnLabel, int row, int column, double value)
{    if (rowLabelBindings == null) {        rowLabelBindings = new HashMap<>();    }    rowLabelBindings.put(rowLabel, row);    if (columnLabelBindings == null) {        columnLabelBindings = new HashMap<>();    }    columnLabelBindings.put(columnLabel, column);    set(row, column, value);}
0
public void setColumnLabelBindings(Map<String, Integer> bindings)
{    columnLabelBindings = bindings;}
0
public void setRowLabelBindings(Map<String, Integer> bindings)
{    rowLabelBindings = bindings;}
0
public int numRows()
{    return rowSize();}
0
public int numCols()
{    return columnSize();}
0
public String asFormatString()
{    return toString();}
0
public Matrix assign(double value)
{    int rows = rowSize();    int columns = columnSize();    for (int row = 0; row < rows; row++) {        for (int col = 0; col < columns; col++) {            setQuick(row, col, value);        }    }    return this;}
0
public Matrix assign(double[][] values)
{    int rows = rowSize();    if (rows != values.length) {        throw new CardinalityException(rows, values.length);    }    int columns = columnSize();    for (int row = 0; row < rows; row++) {        if (columns == values[row].length) {            for (int col = 0; col < columns; col++) {                setQuick(row, col, values[row][col]);            }        } else {            throw new CardinalityException(columns, values[row].length);        }    }    return this;}
0
public Matrix assign(Matrix other, DoubleDoubleFunction function)
{    int rows = rowSize();    if (rows != other.rowSize()) {        throw new CardinalityException(rows, other.rowSize());    }    int columns = columnSize();    if (columns != other.columnSize()) {        throw new CardinalityException(columns, other.columnSize());    }    for (int row = 0; row < rows; row++) {        for (int col = 0; col < columns; col++) {            setQuick(row, col, function.apply(getQuick(row, col), other.getQuick(row, col)));        }    }    return this;}
0
public Matrix assign(Matrix other)
{    int rows = rowSize();    if (rows != other.rowSize()) {        throw new CardinalityException(rows, other.rowSize());    }    int columns = columnSize();    if (columns != other.columnSize()) {        throw new CardinalityException(columns, other.columnSize());    }    for (int row = 0; row < rows; row++) {        for (int col = 0; col < columns; col++) {            setQuick(row, col, other.getQuick(row, col));        }    }    return this;}
0
public Matrix assign(DoubleFunction function)
{    int rows = rowSize();    int columns = columnSize();    for (int row = 0; row < rows; row++) {        for (int col = 0; col < columns; col++) {            setQuick(row, col, function.apply(getQuick(row, col)));        }    }    return this;}
0
public Vector aggregateRows(VectorFunction f)
{    Vector r = new DenseVector(numRows());    int n = numRows();    for (int row = 0; row < n; row++) {        r.set(row, f.apply(viewRow(row)));    }    return r;}
0
public Vector viewRow(int row)
{    return new MatrixVectorView(this, row, 0, 0, 1);}
0
public Vector viewColumn(int column)
{    return new MatrixVectorView(this, 0, column, 1, 0);}
0
public Vector viewDiagonal()
{    return new MatrixVectorView(this, 0, 0, 1, 1);}
0
public double aggregate(final DoubleDoubleFunction combiner, final DoubleFunction mapper)
{    return aggregateRows(new VectorFunction() {        @Override        public double apply(Vector v) {            return v.aggregate(combiner, mapper);        }    }).aggregate(combiner, Functions.IDENTITY);}
0
public double apply(Vector v)
{    return v.aggregate(combiner, mapper);}
0
public Vector aggregateColumns(VectorFunction f)
{    Vector r = new DenseVector(numCols());    for (int col = 0; col < numCols(); col++) {        r.set(col, f.apply(viewColumn(col)));    }    return r;}
0
public double determinant()
{    int rows = rowSize();    int columns = columnSize();    if (rows != columns) {        throw new CardinalityException(rows, columns);    }    if (rows == 2) {        return getQuick(0, 0) * getQuick(1, 1) - getQuick(0, 1) * getQuick(1, 0);    } else {                        int sign = 1;        double ret = 0;        for (int i = 0; i < columns; i++) {            Matrix minor = new DenseMatrix(rows - 1, columns - 1);            for (int j = 1; j < rows; j++) {                boolean flag = false;                /* column offset flag */                for (int k = 0; k < columns; k++) {                    if (k == i) {                        flag = true;                        continue;                    }                    minor.set(j - 1, flag ? k - 1 : k, getQuick(j, k));                }            }            ret += getQuick(0, i) * sign * minor.determinant();            sign *= -1;        }        return ret;    }}
0
public Matrix clone()
{    AbstractMatrix clone;    try {        clone = (AbstractMatrix) super.clone();    } catch (CloneNotSupportedException cnse) {                throw new IllegalStateException(cnse);    }    if (rowLabelBindings != null) {        clone.rowLabelBindings = Maps.newHashMap(rowLabelBindings);    }    if (columnLabelBindings != null) {        clone.columnLabelBindings = Maps.newHashMap(columnLabelBindings);    }    return clone;}
0
public Matrix divide(double x)
{    Matrix result = like();    for (int row = 0; row < rowSize(); row++) {        for (int col = 0; col < columnSize(); col++) {            result.setQuick(row, col, getQuick(row, col) / x);        }    }    return result;}
0
public double get(int row, int column)
{    if (row < 0 || row >= rowSize()) {        throw new IndexException(row, rowSize());    }    if (column < 0 || column >= columnSize()) {        throw new IndexException(column, columnSize());    }    return getQuick(row, column);}
0
public Matrix minus(Matrix other)
{    int rows = rowSize();    if (rows != other.rowSize()) {        throw new CardinalityException(rows, other.rowSize());    }    int columns = columnSize();    if (columns != other.columnSize()) {        throw new CardinalityException(columns, other.columnSize());    }    Matrix result = like();    for (int row = 0; row < rows; row++) {        for (int col = 0; col < columns; col++) {            result.setQuick(row, col, getQuick(row, col) - other.getQuick(row, col));        }    }    return result;}
0
public Matrix plus(double x)
{    Matrix result = like();    int rows = rowSize();    int columns = columnSize();    for (int row = 0; row < rows; row++) {        for (int col = 0; col < columns; col++) {            result.setQuick(row, col, getQuick(row, col) + x);        }    }    return result;}
0
public Matrix plus(Matrix other)
{    int rows = rowSize();    if (rows != other.rowSize()) {        throw new CardinalityException(rows, other.rowSize());    }    int columns = columnSize();    if (columns != other.columnSize()) {        throw new CardinalityException(columns, other.columnSize());    }    Matrix result = like();    for (int row = 0; row < rows; row++) {        for (int col = 0; col < columns; col++) {            result.setQuick(row, col, getQuick(row, col) + other.getQuick(row, col));        }    }    return result;}
0
public void set(int row, int column, double value)
{    if (row < 0 || row >= rowSize()) {        throw new IndexException(row, rowSize());    }    if (column < 0 || column >= columnSize()) {        throw new IndexException(column, columnSize());    }    setQuick(row, column, value);}
0
public void set(int row, double[] data)
{    int columns = columnSize();    if (columns < data.length) {        throw new CardinalityException(columns, data.length);    }    int rows = rowSize();    if (row < 0 || row >= rows) {        throw new IndexException(row, rowSize());    }    for (int i = 0; i < columns; i++) {        setQuick(row, i, data[i]);    }}
0
public Matrix times(double x)
{    Matrix result = like();    int rows = rowSize();    int columns = columnSize();    for (int row = 0; row < rows; row++) {        for (int col = 0; col < columns; col++) {            result.setQuick(row, col, getQuick(row, col) * x);        }    }    return result;}
0
public Matrix times(Matrix other)
{    int columns = columnSize();    if (columns != other.rowSize()) {        throw new CardinalityException(columns, other.rowSize());    }    int rows = rowSize();    int otherColumns = other.columnSize();    Matrix result = like(rows, otherColumns);    for (int row = 0; row < rows; row++) {        for (int col = 0; col < otherColumns; col++) {            double sum = 0.0;            for (int k = 0; k < columns; k++) {                sum += getQuick(row, k) * other.getQuick(k, col);            }            result.setQuick(row, col, sum);        }    }    return result;}
0
public Vector times(Vector v)
{    int columns = columnSize();    if (columns != v.size()) {        throw new CardinalityException(columns, v.size());    }    int rows = rowSize();    Vector w = new DenseVector(rows);    for (int row = 0; row < rows; row++) {        w.setQuick(row, v.dot(viewRow(row)));    }    return w;}
0
public Vector timesSquared(Vector v)
{    int columns = columnSize();    if (columns != v.size()) {        throw new CardinalityException(columns, v.size());    }    int rows = rowSize();    Vector w = new DenseVector(columns);    for (int i = 0; i < rows; i++) {        Vector xi = viewRow(i);        double d = xi.dot(v);        if (d != 0.0) {            w.assign(xi, new PlusMult(d));        }    }    return w;}
0
public Matrix transpose()
{    int rows = rowSize();    int columns = columnSize();    Matrix result = like(columns, rows);    for (int row = 0; row < rows; row++) {        for (int col = 0; col < columns; col++) {            result.setQuick(col, row, getQuick(row, col));        }    }    return result;}
0
public Matrix viewPart(int rowOffset, int rowsRequested, int columnOffset, int columnsRequested)
{    return viewPart(new int[] { rowOffset, columnOffset }, new int[] { rowsRequested, columnsRequested });}
0
public Matrix viewPart(int[] offset, int[] size)
{    if (offset[ROW] < 0) {        throw new IndexException(offset[ROW], 0);    }    if (offset[ROW] + size[ROW] > rowSize()) {        throw new IndexException(offset[ROW] + size[ROW], rowSize());    }    if (offset[COL] < 0) {        throw new IndexException(offset[COL], 0);    }    if (offset[COL] + size[COL] > columnSize()) {        throw new IndexException(offset[COL] + size[COL], columnSize());    }    return new MatrixView(this, offset, size);}
0
public double zSum()
{    double result = 0;    for (int row = 0; row < rowSize(); row++) {        for (int col = 0; col < columnSize(); col++) {            result += getQuick(row, col);        }    }    return result;}
0
public int[] getNumNondefaultElements()
{    return new int[] { rowSize(), columnSize() };}
0
public Vector clone()
{    Vector v = new DenseVector(size());    v.assign(this, Functions.PLUS);    return v;}
0
public boolean isDense()
{    return true;}
0
public boolean isSequentialAccess()
{    return true;}
0
protected Matrix matrixLike(int rows, int columns)
{    return matrix.like(rows, columns);}
0
public Iterator<Element> iterator()
{    return new AbstractIterator<Element>() {        private int i;        @Override        protected Element computeNext() {            if (i >= size()) {                return endOfData();            }            return getElement(i++);        }    };}
0
protected Element computeNext()
{    if (i >= size()) {        return endOfData();    }    return getElement(i++);}
0
public Iterator<Element> iterateNonZero()
{    return iterator();}
0
public Element getElement(final int i)
{    return new Element() {        @Override        public double get() {            return getQuick(i);        }        @Override        public int index() {            return i;        }        @Override        public void set(double value) {            setQuick(i, value);        }    };}
0
public double get()
{    return getQuick(i);}
0
public int index()
{    return i;}
0
public void set(double value)
{    setQuick(i, value);}
0
public void mergeUpdates(OrderedIntDoubleMapping updates)
{    throw new UnsupportedOperationException("Cannot mutate TransposeViewVector");}
0
public double getQuick(int index)
{    Vector v = rowToColumn ? matrix.viewColumn(index) : matrix.viewRow(index);    return v == null ? 0.0 : v.getQuick(transposeOffset);}
0
public void setQuick(int index, double value)
{    Vector v = rowToColumn ? matrix.viewColumn(index) : matrix.viewRow(index);    if (v == null) {        v = newVector(numCols);        if (rowToColumn) {            matrix.assignColumn(index, v);        } else {            matrix.assignRow(index, v);        }    }    v.setQuick(transposeOffset, value);}
0
protected Vector newVector(int cardinality)
{    return new DenseVector(cardinality);}
0
public Vector like()
{    return new DenseVector(size());}
0
public Vector like(int cardinality)
{    return new DenseVector(cardinality);}
0
public int getNumNondefaultElements()
{    return size();}
0
public double getLookupCost()
{    return (rowToColumn ? matrix.viewColumn(0) : matrix.viewRow(0)).getLookupCost();}
0
public double getIteratorAdvanceCost()
{    return (rowToColumn ? matrix.viewColumn(0) : matrix.viewRow(0)).getIteratorAdvanceCost();}
0
public boolean isAddConstantTime()
{    return (rowToColumn ? matrix.viewColumn(0) : matrix.viewRow(0)).isAddConstantTime();}
0
public String toString()
{    int row = 0;    int maxRowsToDisplay = 10;    int maxColsToDisplay = 20;    int colsToDisplay = maxColsToDisplay;    if (maxColsToDisplay > columnSize()) {        colsToDisplay = columnSize();    }    StringBuilder s = new StringBuilder("{\n");    Iterator<MatrixSlice> it = iterator();    while ((it.hasNext()) && (row < maxRowsToDisplay)) {        MatrixSlice next = it.next();        s.append(" ").append(next.index()).append(" =>\t").append(new VectorView(next.vector(), 0, colsToDisplay)).append('\n');        row++;    }    String returnString = s.toString();    if (maxColsToDisplay <= columnSize()) {        returnString = returnString.replace("}", " ... } ");    }    if (maxRowsToDisplay <= rowSize())        return returnString + ("... }");    else {        return returnString + ("}");    }}
0
public MatrixFlavor getFlavor()
{    throw new UnsupportedOperationException("Flavor support not implemented for this matrix.");}
0
public Iterable<Element> all()
{    return new Iterable<Element>() {        @Override        public Iterator<Element> iterator() {            return AbstractVector.this.iterator();        }    };}
0
public Iterator<Element> iterator()
{    return AbstractVector.this.iterator();}
0
public Iterable<Element> nonZeroes()
{    return new Iterable<Element>() {        @Override        public Iterator<Element> iterator() {            return iterateNonZero();        }    };}
0
public Iterator<Element> iterator()
{    return iterateNonZero();}
0
public double aggregate(DoubleDoubleFunction aggregator, DoubleFunction map)
{    if (size == 0) {        return 0;    }        if (aggregator.isAssociativeAndCommutative() && aggregator.isLikeLeftMult() && size > getNumNondefaultElements() && !map.isDensifying()) {        return 0;    }    double result;    if (isSequentialAccess() || aggregator.isAssociativeAndCommutative()) {        Iterator<Element> iterator;                if (!map.isDensifying() && aggregator.isLikeRightPlus()) {            iterator = iterateNonZero();            if (!iterator.hasNext()) {                return 0;            }        } else {            iterator = iterator();        }        Element element = iterator.next();        result = map.apply(element.get());        while (iterator.hasNext()) {            element = iterator.next();            result = aggregator.apply(result, map.apply(element.get()));        }    } else {        result = map.apply(getQuick(0));        for (int i = 1; i < size; i++) {            result = aggregator.apply(result, map.apply(getQuick(i)));        }    }    return result;}
0
public double aggregate(Vector other, DoubleDoubleFunction aggregator, DoubleDoubleFunction combiner)
{    Preconditions.checkArgument(size == other.size(), "Vector sizes differ");    if (size == 0) {        return 0;    }    return VectorBinaryAggregate.aggregateBest(this, other, aggregator, combiner);}
0
public Vector viewPart(int offset, int length)
{    if (offset < 0) {        throw new IndexException(offset, size);    }    if (offset + length > size) {        throw new IndexException(offset + length, size);    }    return new VectorView(this, offset, length);}
0
public Vector clone()
{    try {        AbstractVector r = (AbstractVector) super.clone();        r.size = size;        r.lengthSquared = lengthSquared;        return r;    } catch (CloneNotSupportedException e) {        throw new IllegalStateException("Can't happen");    }}
0
public Vector divide(double x)
{    if (x == 1.0) {        return clone();    }    Vector result = createOptimizedCopy();    for (Element element : result.nonZeroes()) {        element.set(element.get() / x);    }    return result;}
0
public double dot(Vector x)
{    if (size != x.size()) {        throw new CardinalityException(size, x.size());    }    if (this == x) {        return getLengthSquared();    }    return aggregate(x, Functions.PLUS, Functions.MULT);}
0
protected double dotSelf()
{    return aggregate(Functions.PLUS, Functions.pow(2));}
0
public double get(int index)
{    if (index < 0 || index >= size) {        throw new IndexException(index, size);    }    return getQuick(index);}
0
public Element getElement(int index)
{    return new LocalElement(index);}
0
public Vector normalize()
{    return divide(Math.sqrt(getLengthSquared()));}
0
public Vector normalize(double power)
{    return divide(norm(power));}
0
public Vector logNormalize()
{    return logNormalize(2.0, Math.sqrt(getLengthSquared()));}
0
public Vector logNormalize(double power)
{    return logNormalize(power, norm(power));}
0
public Vector logNormalize(double power, double normLength)
{        if (Double.isInfinite(power) || power <= 1.0) {        throw new IllegalArgumentException("Power must be > 1 and < infinity");    } else {        double denominator = normLength * Math.log(power);        Vector result = createOptimizedCopy();        for (Element element : result.nonZeroes()) {            element.set(Math.log1p(element.get()) / denominator);        }        return result;    }}
0
public double norm(double power)
{    if (power < 0.0) {        throw new IllegalArgumentException("Power must be >= 0");    }        if (Double.isInfinite(power)) {        return aggregate(Functions.MAX, Functions.ABS);    } else if (power == 2.0) {        return Math.sqrt(getLengthSquared());    } else if (power == 1.0) {        double result = 0.0;        Iterator<Element> iterator = this.iterateNonZero();        while (iterator.hasNext()) {            result += Math.abs(iterator.next().get());        }        return result;            } else if (power == 0.0) {        return getNumNonZeroElements();    } else {        return Math.pow(aggregate(Functions.PLUS, Functions.pow(power)), 1.0 / power);    }}
0
public double getLengthSquared()
{    if (lengthSquared >= 0.0) {        return lengthSquared;    }    return lengthSquared = dotSelf();}
0
public void invalidateCachedLength()
{    lengthSquared = -1;}
0
public double getDistanceSquared(Vector that)
{    if (size != that.size()) {        throw new CardinalityException(size, that.size());    }    double thisLength = getLengthSquared();    double thatLength = that.getLengthSquared();    double dot = dot(that);    double distanceEstimate = thisLength + thatLength - 2 * dot;    if (distanceEstimate > 1.0e-3 * (thisLength + thatLength)) {                return Math.max(distanceEstimate, 0);    } else {        return aggregate(that, Functions.PLUS, Functions.MINUS_SQUARED);    }}
0
public double maxValue()
{    if (size == 0) {        return Double.NEGATIVE_INFINITY;    }    return aggregate(Functions.MAX, Functions.IDENTITY);}
0
public int maxValueIndex()
{    int result = -1;    double max = Double.NEGATIVE_INFINITY;    int nonZeroElements = 0;    Iterator<Element> iter = this.iterateNonZero();    while (iter.hasNext()) {        nonZeroElements++;        Element element = iter.next();        double tmp = element.get();        if (tmp > max) {            max = tmp;            result = element.index();        }    }        if (nonZeroElements < size && max < 0.0) {        for (Element element : all()) {            if (element.get() == 0.0) {                return element.index();            }        }    }    return result;}
0
public double minValue()
{    if (size == 0) {        return Double.POSITIVE_INFINITY;    }    return aggregate(Functions.MIN, Functions.IDENTITY);}
0
public int minValueIndex()
{    int result = -1;    double min = Double.POSITIVE_INFINITY;    int nonZeroElements = 0;    Iterator<Element> iter = this.iterateNonZero();    while (iter.hasNext()) {        nonZeroElements++;        Element element = iter.next();        double tmp = element.get();        if (tmp < min) {            min = tmp;            result = element.index();        }    }        if (nonZeroElements < size && min > 0.0) {        for (Element element : all()) {            if (element.get() == 0.0) {                return element.index();            }        }    }    return result;}
0
public Vector plus(double x)
{    Vector result = createOptimizedCopy();    if (x == 0.0) {        return result;    }    return result.assign(Functions.plus(x));}
0
public Vector plus(Vector that)
{    if (size != that.size()) {        throw new CardinalityException(size, that.size());    }    return createOptimizedCopy().assign(that, Functions.PLUS);}
0
public Vector minus(Vector that)
{    if (size != that.size()) {        throw new CardinalityException(size, that.size());    }    return createOptimizedCopy().assign(that, Functions.MINUS);}
0
public void set(int index, double value)
{    if (index < 0 || index >= size) {        throw new IndexException(index, size);    }    setQuick(index, value);}
0
public void incrementQuick(int index, double increment)
{    setQuick(index, getQuick(index) + increment);}
0
public Vector times(double x)
{    if (x == 0.0) {        return like();    }    return createOptimizedCopy().assign(Functions.mult(x));}
0
protected Vector createOptimizedCopy()
{    return createOptimizedCopy(this);}
0
private static Vector createOptimizedCopy(Vector vector)
{    Vector result;    if (vector.isDense()) {        result = vector.like().assign(vector, Functions.SECOND_LEFT_ZERO);    } else {        result = vector.clone();    }    return result;}
0
public Vector times(Vector that)
{    if (size != that.size()) {        throw new CardinalityException(size, that.size());    }    if (this.getNumNondefaultElements() <= that.getNumNondefaultElements()) {        return createOptimizedCopy(this).assign(that, Functions.MULT);    } else {        return createOptimizedCopy(that).assign(this, Functions.MULT);    }}
0
public double zSum()
{    return aggregate(Functions.PLUS, Functions.IDENTITY);}
0
public int getNumNonZeroElements()
{    int count = 0;    Iterator<Element> it = iterateNonZero();    while (it.hasNext()) {        if (it.next().get() != 0.0) {            count++;        }    }    return count;}
0
public Vector assign(double value)
{    Iterator<Element> it;    if (value == 0.0) {                it = iterateNonZero();        while (it.hasNext()) {            it.next().set(value);        }    } else {        if (isSequentialAccess() && !isAddConstantTime()) {                                    it = iterator();            OrderedIntDoubleMapping updates = new OrderedIntDoubleMapping();            while (it.hasNext()) {                Element element = it.next();                if (element.get() == 0.0) {                    updates.set(element.index(), value);                } else {                    element.set(value);                }            }            mergeUpdates(updates);        } else {            for (int i = 0; i < size; ++i) {                setQuick(i, value);            }        }    }    invalidateCachedLength();    return this;}
0
public Vector assign(double[] values)
{    if (size != values.length) {        throw new CardinalityException(size, values.length);    }    if (isSequentialAccess() && !isAddConstantTime()) {        OrderedIntDoubleMapping updates = new OrderedIntDoubleMapping();        Iterator<Element> it = iterator();        while (it.hasNext()) {            Element element = it.next();            int index = element.index();            if (element.get() == 0.0) {                updates.set(index, values[index]);            } else {                element.set(values[index]);            }        }        mergeUpdates(updates);    } else {        for (int i = 0; i < size; ++i) {            setQuick(i, values[i]);        }    }    invalidateCachedLength();    return this;}
0
public Vector assign(Vector other)
{    return assign(other, Functions.SECOND);}
0
public Vector assign(DoubleDoubleFunction f, double y)
{    Iterator<Element> iterator = f.apply(0, y) == 0 ? iterateNonZero() : iterator();    while (iterator.hasNext()) {        Element element = iterator.next();        element.set(f.apply(element.get(), y));    }    invalidateCachedLength();    return this;}
0
public Vector assign(DoubleFunction f)
{    Iterator<Element> iterator = !f.isDensifying() ? iterateNonZero() : iterator();    while (iterator.hasNext()) {        Element element = iterator.next();        element.set(f.apply(element.get()));    }    invalidateCachedLength();    return this;}
0
public Vector assign(Vector other, DoubleDoubleFunction function)
{    if (size != other.size()) {        throw new CardinalityException(size, other.size());    }    VectorBinaryAssign.assignBest(this, other, function);    invalidateCachedLength();    return this;}
0
public Matrix cross(Vector other)
{    Matrix result = matrixLike(size, other.size());    Iterator<Vector.Element> it = iterateNonZero();    while (it.hasNext()) {        Vector.Element e = it.next();        int row = e.index();        result.assignRow(row, other.times(getQuick(row)));    }    return result;}
0
public final int size()
{    return size;}
0
public String asFormatString()
{    return toString();}
0
public int hashCode()
{    int result = size;    Iterator<Element> iter = iterateNonZero();    while (iter.hasNext()) {        Element ele = iter.next();        result += ele.index() * RandomUtils.hashDouble(ele.get());    }    return result;}
0
public boolean equals(Object o)
{    if (this == o) {        return true;    }    if (!(o instanceof Vector)) {        return false;    }    Vector that = (Vector) o;    return size == that.size() && aggregate(that, Functions.PLUS, Functions.MINUS_ABS) == 0.0;}
0
public String toString()
{    return toString(null);}
0
public String toString(String[] dictionary)
{    StringBuilder result = new StringBuilder();    result.append('{');    for (int index = 0; index < size; index++) {        double value = getQuick(index);        if (value != 0.0) {            result.append(dictionary != null && dictionary.length > index ? dictionary[index] : index);            result.append(':');            result.append(value);            result.append(',');        }    }    if (result.length() > 1) {        result.setCharAt(result.length() - 1, '}');    } else {        result.append('}');    }    return result.toString();}
0
public String sparseVectorToString()
{    Iterator<Element> it = iterateNonZero();    if (!it.hasNext()) {        return "{}";    } else {        StringBuilder result = new StringBuilder();        result.append('{');        while (it.hasNext()) {            Vector.Element e = it.next();            result.append(e.index());            result.append(':');            result.append(e.get());            result.append(',');        }        result.setCharAt(result.length() - 1, '}');        return result.toString();    }}
0
public double get()
{    return getQuick(index);}
0
public int index()
{    return index;}
0
public void set(double value)
{    setQuick(index, value);}
0
public static Vector mult(Matrix m, Vector v)
{    if (m.numRows() != v.size()) {        throw new CardinalityException(m.numRows(), v.size());    }        Vector result = new DenseVector(m.numRows());    for (int i = 0; i < m.numRows(); i++) {        result.set(i, m.viewRow(i).dot(v));    }    return result;}
0
public static double hypot(double a, double b)
{    double r;    if (Math.abs(a) > Math.abs(b)) {        r = b / a;        r = Math.abs(a) * Math.sqrt(1 + r * r);    } else if (b != 0) {        r = a / b;        r = Math.abs(b) * Math.sqrt(1 + r * r);    } else {        r = 0.0;    }    return r;}
0
public static double getNorm(Matrix m)
{    double max = 0.0;    for (int i = 0; i < m.numRows(); i++) {        int sum = 0;        Vector cv = m.viewRow(i);        for (int j = 0; j < cv.size(); j++) {            sum += (int) Math.abs(cv.getQuick(j));        }        if (sum > max) {            max = sum;        }    }    return max;}
0
public static Vector solve(Iterable<Vector> featureVectors, Vector ratingVector, double lambda, int numFeatures)
{    Preconditions.checkNotNull(featureVectors, "Feature Vectors cannot be null");    Preconditions.checkArgument(!Iterables.isEmpty(featureVectors));    Preconditions.checkNotNull(ratingVector, "Rating Vector cannot be null");    Preconditions.checkArgument(ratingVector.getNumNondefaultElements() > 0, "Rating Vector cannot be empty");    Preconditions.checkArgument(Iterables.size(featureVectors) == ratingVector.getNumNondefaultElements());    int nui = ratingVector.getNumNondefaultElements();    Matrix MiIi = createMiIi(featureVectors, numFeatures);    Matrix RiIiMaybeTransposed = createRiIiMaybeTransposed(ratingVector);    /* compute Ai = MiIi * t(MiIi) + lambda * nui * E */    Matrix Ai = miTimesMiTransposePlusLambdaTimesNuiTimesE(MiIi, lambda, nui);    /* compute Vi = MiIi * t(R(i,Ii)) */    Matrix Vi = MiIi.times(RiIiMaybeTransposed);    /* compute Ai * ui = Vi */    return solve(Ai, Vi);}
0
private static Vector solve(Matrix Ai, Matrix Vi)
{    return new QRDecomposition(Ai).solve(Vi).viewColumn(0);}
0
 static Matrix addLambdaTimesNuiTimesE(Matrix matrix, double lambda, int nui)
{    Preconditions.checkArgument(matrix.numCols() == matrix.numRows(), "Must be a Square Matrix");    double lambdaTimesNui = lambda * nui;    int numCols = matrix.numCols();    for (int n = 0; n < numCols; n++) {        matrix.setQuick(n, n, matrix.getQuick(n, n) + lambdaTimesNui);    }    return matrix;}
0
private static Matrix miTimesMiTransposePlusLambdaTimesNuiTimesE(Matrix MiIi, double lambda, int nui)
{    double lambdaTimesNui = lambda * nui;    int rows = MiIi.numRows();    double[][] result = new double[rows][rows];    for (int i = 0; i < rows; i++) {        for (int j = i; j < rows; j++) {            double dot = MiIi.viewRow(i).dot(MiIi.viewRow(j));            if (i != j) {                result[i][j] = dot;                result[j][i] = dot;            } else {                result[i][i] = dot + lambdaTimesNui;            }        }    }    return new DenseMatrix(result, true);}
0
 static Matrix createMiIi(Iterable<Vector> featureVectors, int numFeatures)
{    double[][] MiIi = new double[numFeatures][Iterables.size(featureVectors)];    int n = 0;    for (Vector featureVector : featureVectors) {        for (int m = 0; m < numFeatures; m++) {            MiIi[m][n] = featureVector.getQuick(m);        }        n++;    }    return new DenseMatrix(MiIi, true);}
0
 static Matrix createRiIiMaybeTransposed(Vector ratingVector)
{    Preconditions.checkArgument(ratingVector.isSequentialAccess(), "Ratings should be iterable in Index or Sequential Order");    double[][] RiIiMaybeTransposed = new double[ratingVector.getNumNondefaultElements()][1];    int index = 0;    for (Vector.Element elem : ratingVector.nonZeroes()) {        RiIiMaybeTransposed[index++][0] = elem.get();    }    return new DenseMatrix(RiIiMaybeTransposed, true);}
0
public Vector solve(Vector ratings)
{    return solve(YtransposeY.plus(getYtransponseCuMinusIYPlusLambdaI(ratings)), getYtransponseCuPu(ratings));}
0
private static Vector solve(Matrix A, Matrix y)
{    return new QRDecomposition(A).solve(y).viewColumn(0);}
0
 double confidence(double rating)
{    return 1 + alpha * rating;}
0
public Matrix getYtransposeY(final OpenIntObjectHashMap<Vector> Y)
{    ExecutorService queue = Executors.newFixedThreadPool(numTrainingThreads);    if (log.isInfoEnabled()) {            }    long startTime = System.nanoTime();    final IntArrayList indexes = Y.keys();    final int numIndexes = indexes.size();    final double[][] YtY = new double[numFeatures][numFeatures];        for (int i = 0; i < numFeatures; i++) {        for (int j = i; j < numFeatures; j++) {            final int ii = i;            final int jj = j;            queue.execute(new Runnable() {                @Override                public void run() {                    double dot = 0;                    for (int k = 0; k < numIndexes; k++) {                        Vector row = Y.get(indexes.getQuick(k));                        dot += row.getQuick(ii) * row.getQuick(jj);                    }                    YtY[ii][jj] = dot;                    if (ii != jj) {                        YtY[jj][ii] = dot;                    }                }            });        }    }    queue.shutdown();    try {        queue.awaitTermination(1, TimeUnit.DAYS);    } catch (InterruptedException e) {                throw new RuntimeException("Error during Y'Y queue shutdown");    }    if (log.isInfoEnabled()) {            }    return new DenseMatrix(YtY, true);}
1
public void run()
{    double dot = 0;    for (int k = 0; k < numIndexes; k++) {        Vector row = Y.get(indexes.getQuick(k));        dot += row.getQuick(ii) * row.getQuick(jj);    }    YtY[ii][jj] = dot;    if (ii != jj) {        YtY[jj][ii] = dot;    }}
0
private Matrix getYtransponseCuMinusIYPlusLambdaI(Vector userRatings)
{    Preconditions.checkArgument(userRatings.isSequentialAccess(), "need sequential access to ratings!");    /* (Cu -I) Y */    OpenIntObjectHashMap<Vector> CuMinusIY = new OpenIntObjectHashMap<>(userRatings.getNumNondefaultElements());    for (Element e : userRatings.nonZeroes()) {        CuMinusIY.put(e.index(), Y.get(e.index()).times(confidence(e.get()) - 1));    }    Matrix YtransponseCuMinusIY = new DenseMatrix(numFeatures, numFeatures);    /* Y' (Cu -I) Y by outer products */    for (Element e : userRatings.nonZeroes()) {        for (Vector.Element feature : Y.get(e.index()).all()) {            Vector partial = CuMinusIY.get(e.index()).times(feature.get());            YtransponseCuMinusIY.viewRow(feature.index()).assign(partial, Functions.PLUS);        }    }    /* Y' (Cu - I) Y +  I  add lambda on the diagonal */    for (int feature = 0; feature < numFeatures; feature++) {        YtransponseCuMinusIY.setQuick(feature, feature, YtransponseCuMinusIY.getQuick(feature, feature) + lambda);    }    return YtransponseCuMinusIY;}
0
private Matrix getYtransponseCuPu(Vector userRatings)
{    Preconditions.checkArgument(userRatings.isSequentialAccess(), "need sequential access to ratings!");    Vector YtransponseCuPu = new DenseVector(numFeatures);    for (Element e : userRatings.nonZeroes()) {        YtransponseCuPu.assign(Y.get(e.index()).times(confidence(e.get())), Functions.PLUS);    }    return columnVectorAsMatrix(YtransponseCuPu);}
0
private Matrix columnVectorAsMatrix(Vector v)
{    double[][] matrix = new double[numFeatures][1];    for (Vector.Element e : v.all()) {        matrix[e.index()][0] = e.get();    }    return new DenseMatrix(matrix, true);}
0
public static byte[] ensureCapacity(byte[] array, int minCapacity)
{    int oldCapacity = array.length;    byte[] newArray;    if (minCapacity > oldCapacity) {        int newCapacity = (oldCapacity * 3) / 2 + 1;        if (newCapacity < minCapacity) {            newCapacity = minCapacity;        }        newArray = new byte[newCapacity];        System.arraycopy(array, 0, newArray, 0, oldCapacity);    } else {        newArray = array;    }    return newArray;}
0
public static char[] ensureCapacity(char[] array, int minCapacity)
{    int oldCapacity = array.length;    char[] newArray;    if (minCapacity > oldCapacity) {        int newCapacity = (oldCapacity * 3) / 2 + 1;        if (newCapacity < minCapacity) {            newCapacity = minCapacity;        }        newArray = new char[newCapacity];        System.arraycopy(array, 0, newArray, 0, oldCapacity);    } else {        newArray = array;    }    return newArray;}
0
public static double[] ensureCapacity(double[] array, int minCapacity)
{    int oldCapacity = array.length;    double[] newArray;    if (minCapacity > oldCapacity) {        int newCapacity = (oldCapacity * 3) / 2 + 1;        if (newCapacity < minCapacity) {            newCapacity = minCapacity;        }        newArray = new double[newCapacity];                System.arraycopy(array, 0, newArray, 0, oldCapacity);    } else {        newArray = array;    }    return newArray;}
0
public static float[] ensureCapacity(float[] array, int minCapacity)
{    int oldCapacity = array.length;    float[] newArray;    if (minCapacity > oldCapacity) {        int newCapacity = (oldCapacity * 3) / 2 + 1;        if (newCapacity < minCapacity) {            newCapacity = minCapacity;        }        newArray = new float[newCapacity];        System.arraycopy(array, 0, newArray, 0, oldCapacity);    } else {        newArray = array;    }    return newArray;}
0
public static int[] ensureCapacity(int[] array, int minCapacity)
{    int oldCapacity = array.length;    int[] newArray;    if (minCapacity > oldCapacity) {        int newCapacity = (oldCapacity * 3) / 2 + 1;        if (newCapacity < minCapacity) {            newCapacity = minCapacity;        }        newArray = new int[newCapacity];        System.arraycopy(array, 0, newArray, 0, oldCapacity);    } else {        newArray = array;    }    return newArray;}
0
public static long[] ensureCapacity(long[] array, int minCapacity)
{    int oldCapacity = array.length;    long[] newArray;    if (minCapacity > oldCapacity) {        int newCapacity = (oldCapacity * 3) / 2 + 1;        if (newCapacity < minCapacity) {            newCapacity = minCapacity;        }        newArray = new long[newCapacity];        System.arraycopy(array, 0, newArray, 0, oldCapacity);    } else {        newArray = array;    }    return newArray;}
0
public static Object[] ensureCapacity(Object[] array, int minCapacity)
{    int oldCapacity = array.length;    Object[] newArray;    if (minCapacity > oldCapacity) {        int newCapacity = (oldCapacity * 3) / 2 + 1;        if (newCapacity < minCapacity) {            newCapacity = minCapacity;        }        newArray = new Object[newCapacity];        System.arraycopy(array, 0, newArray, 0, oldCapacity);    } else {        newArray = array;    }    return newArray;}
0
public static short[] ensureCapacity(short[] array, int minCapacity)
{    int oldCapacity = array.length;    short[] newArray;    if (minCapacity > oldCapacity) {        int newCapacity = (oldCapacity * 3) / 2 + 1;        if (newCapacity < minCapacity) {            newCapacity = minCapacity;        }        newArray = new short[newCapacity];        System.arraycopy(array, 0, newArray, 0, oldCapacity);    } else {        newArray = array;    }    return newArray;}
0
public static boolean[] ensureCapacity(boolean[] array, int minCapacity)
{    int oldCapacity = array.length;    boolean[] newArray;    if (minCapacity > oldCapacity) {        int newCapacity = (oldCapacity * 3) / 2 + 1;        if (newCapacity < minCapacity) {            newCapacity = minCapacity;        }        newArray = new boolean[newCapacity];        System.arraycopy(array, 0, newArray, 0, oldCapacity);    } else {        newArray = array;    }    return newArray;}
0
public static String toString(byte[] array)
{    StringBuilder buf = new StringBuilder();    buf.append('[');    int maxIndex = array.length - 1;    for (int i = 0; i <= maxIndex; i++) {        buf.append(array[i]);        if (i < maxIndex) {            buf.append(", ");        }    }    buf.append(']');    return buf.toString();}
0
public static String toString(char[] array)
{    StringBuilder buf = new StringBuilder();    buf.append('[');    int maxIndex = array.length - 1;    for (int i = 0; i <= maxIndex; i++) {        buf.append(array[i]);        if (i < maxIndex) {            buf.append(", ");        }    }    buf.append(']');    return buf.toString();}
0
public static String toString(double[] array)
{    StringBuilder buf = new StringBuilder();    buf.append('[');    int maxIndex = array.length - 1;    for (int i = 0; i <= maxIndex; i++) {        buf.append(array[i]);        if (i < maxIndex) {            buf.append(", ");        }    }    buf.append(']');    return buf.toString();}
0
public static String toString(float[] array)
{    StringBuilder buf = new StringBuilder();    buf.append('[');    int maxIndex = array.length - 1;    for (int i = 0; i <= maxIndex; i++) {        buf.append(array[i]);        if (i < maxIndex) {            buf.append(", ");        }    }    buf.append(']');    return buf.toString();}
0
public static String toString(int[] array)
{    StringBuilder buf = new StringBuilder();    buf.append('[');    int maxIndex = array.length - 1;    for (int i = 0; i <= maxIndex; i++) {        buf.append(array[i]);        if (i < maxIndex) {            buf.append(", ");        }    }    buf.append(']');    return buf.toString();}
0
public static String toString(long[] array)
{    StringBuilder buf = new StringBuilder();    buf.append('[');    int maxIndex = array.length - 1;    for (int i = 0; i <= maxIndex; i++) {        buf.append(array[i]);        if (i < maxIndex) {            buf.append(", ");        }    }    buf.append(']');    return buf.toString();}
0
public static String toString(Object[] array)
{    StringBuilder buf = new StringBuilder();    buf.append('[');    int maxIndex = array.length - 1;    for (int i = 0; i <= maxIndex; i++) {        buf.append(array[i]);        if (i < maxIndex) {            buf.append(", ");        }    }    buf.append(']');    return buf.toString();}
0
public static String toString(short[] array)
{    StringBuilder buf = new StringBuilder();    buf.append('[');    int maxIndex = array.length - 1;    for (int i = 0; i <= maxIndex; i++) {        buf.append(array[i]);        if (i < maxIndex) {            buf.append(", ");        }    }    buf.append(']');    return buf.toString();}
0
public static String toString(boolean[] array)
{    StringBuilder buf = new StringBuilder();    buf.append('[');    int maxIndex = array.length - 1;    for (int i = 0; i <= maxIndex; i++) {        buf.append(array[i]);        if (i < maxIndex) {            buf.append(", ");        }    }    buf.append(']');    return buf.toString();}
0
public static byte[] trimToCapacity(byte[] array, int maxCapacity)
{    if (array.length > maxCapacity) {        byte[] oldArray = array;        array = new byte[maxCapacity];        System.arraycopy(oldArray, 0, array, 0, maxCapacity);    }    return array;}
0
public static char[] trimToCapacity(char[] array, int maxCapacity)
{    if (array.length > maxCapacity) {        char[] oldArray = array;        array = new char[maxCapacity];        System.arraycopy(oldArray, 0, array, 0, maxCapacity);    }    return array;}
0
public static double[] trimToCapacity(double[] array, int maxCapacity)
{    if (array.length > maxCapacity) {        double[] oldArray = array;        array = new double[maxCapacity];        System.arraycopy(oldArray, 0, array, 0, maxCapacity);    }    return array;}
0
public static float[] trimToCapacity(float[] array, int maxCapacity)
{    if (array.length > maxCapacity) {        float[] oldArray = array;        array = new float[maxCapacity];        System.arraycopy(oldArray, 0, array, 0, maxCapacity);    }    return array;}
0
public static int[] trimToCapacity(int[] array, int maxCapacity)
{    if (array.length > maxCapacity) {        int[] oldArray = array;        array = new int[maxCapacity];        System.arraycopy(oldArray, 0, array, 0, maxCapacity);    }    return array;}
0
public static long[] trimToCapacity(long[] array, int maxCapacity)
{    if (array.length > maxCapacity) {        long[] oldArray = array;        array = new long[maxCapacity];        System.arraycopy(oldArray, 0, array, 0, maxCapacity);    }    return array;}
0
public static Object[] trimToCapacity(Object[] array, int maxCapacity)
{    if (array.length > maxCapacity) {        Object[] oldArray = array;        array = new Object[maxCapacity];        System.arraycopy(oldArray, 0, array, 0, maxCapacity);    }    return array;}
0
public static short[] trimToCapacity(short[] array, int maxCapacity)
{    if (array.length > maxCapacity) {        short[] oldArray = array;        array = new short[maxCapacity];        System.arraycopy(oldArray, 0, array, 0, maxCapacity);    }    return array;}
0
public static boolean[] trimToCapacity(boolean[] array, int maxCapacity)
{    if (array.length > maxCapacity) {        boolean[] oldArray = array;        array = new boolean[maxCapacity];        System.arraycopy(oldArray, 0, array, 0, maxCapacity);    }    return array;}
0
public static byte[] copyOf(byte[] src, int length)
{    byte[] result = new byte[length];    System.arraycopy(src, 0, result, 0, Math.min(length, src.length));    return result;}
0
public static char[] copyOf(char[] src, int length)
{    char[] result = new char[length];    System.arraycopy(src, 0, result, 0, Math.min(length, src.length));    return result;}
0
public static short[] copyOf(short[] src, int length)
{    short[] result = new short[length];    System.arraycopy(src, 0, result, 0, Math.min(length, src.length));    return result;}
0
public static int[] copyOf(int[] src, int length)
{    int[] result = new int[length];    System.arraycopy(src, 0, result, 0, Math.min(length, src.length));    return result;}
0
public static float[] copyOf(float[] src, int length)
{    float[] result = new float[length];    System.arraycopy(src, 0, result, 0, Math.min(length, src.length));    return result;}
0
public static double[] copyOf(double[] src, int length)
{    double[] result = new double[length];    System.arraycopy(src, 0, result, 0, Math.min(length, src.length));    return result;}
0
public static long[] copyOf(long[] src, int length)
{    long[] result = new long[length];    System.arraycopy(src, 0, result, 0, Math.min(length, src.length));    return result;}
0
public static int binarySearchFromTo(byte[] array, byte value, int from, int to)
{    int mid = -1;    while (from <= to) {        mid = (from + to) >>> 1;        if (value > array[mid]) {            from = mid + 1;        } else if (value == array[mid]) {            return mid;        } else {            to = mid - 1;        }    }    if (mid < 0) {        return -1;    }    return -mid - (value < array[mid] ? 1 : 2);}
0
public static int binarySearchFromTo(char[] array, char value, int from, int to)
{    int mid = -1;    while (from <= to) {        mid = (from + to) >>> 1;        if (value > array[mid]) {            from = mid + 1;        } else if (value == array[mid]) {            return mid;        } else {            to = mid - 1;        }    }    if (mid < 0) {        return -1;    }    return -mid - (value < array[mid] ? 1 : 2);}
0
public static int binarySearchFromTo(double[] array, double value, int from, int to)
{    long longBits = Double.doubleToLongBits(value);    int mid = -1;    while (from <= to) {        mid = (from + to) >>> 1;        if (lessThan(array[mid], value)) {            from = mid + 1;        } else if (longBits == Double.doubleToLongBits(array[mid])) {            return mid;        } else {            to = mid - 1;        }    }    if (mid < 0) {        return -1;    }    return -mid - (lessThan(value, array[mid]) ? 1 : 2);}
0
public static int binarySearchFromTo(float[] array, float value, int from, int to)
{    int intBits = Float.floatToIntBits(value);    int mid = -1;    while (from <= to) {        mid = (from + to) >>> 1;        if (lessThan(array[mid], value)) {            from = mid + 1;        } else if (intBits == Float.floatToIntBits(array[mid])) {            return mid;        } else {            to = mid - 1;        }    }    if (mid < 0) {        return -1;    }    return -mid - (lessThan(value, array[mid]) ? 1 : 2);}
0
public static int binarySearchFromTo(int[] array, int value, int from, int to)
{    int mid = -1;    while (from <= to) {        mid = (from + to) >>> 1;        if (value > array[mid]) {            from = mid + 1;        } else if (value == array[mid]) {            return mid;        } else {            to = mid - 1;        }    }    if (mid < 0) {        return -1;    }    return -mid - (value < array[mid] ? 1 : 2);}
0
public static int binarySearchFromTo(long[] array, long value, int from, int to)
{    int mid = -1;    while (from <= to) {        mid = (from + to) >>> 1;        if (value > array[mid]) {            from = mid + 1;        } else if (value == array[mid]) {            return mid;        } else {            to = mid - 1;        }    }    if (mid < 0) {        return -1;    }    return -mid - (value < array[mid] ? 1 : 2);}
0
public static int binarySearchFromTo(T[] array, T object, int from, int to)
{    if (array.length == 0) {        return -1;    }    int mid = 0;    int result = 0;    while (from <= to) {        mid = (from + to) >>> 1;        if ((result = array[mid].compareTo(object)) < 0) {            from = mid + 1;        } else if (result == 0) {            return mid;        } else {            to = mid - 1;        }    }    return -mid - (result >= 0 ? 1 : 2);}
0
public static int binarySearchFromTo(T[] array, T object, int from, int to, Comparator<? super T> comparator)
{    int mid = 0;    int result = 0;    while (from <= to) {        mid = (from + to) >>> 1;        if ((result = comparator.compare(array[mid], object)) < 0) {            from = mid + 1;        } else if (result == 0) {            return mid;        } else {            to = mid - 1;        }    }    return -mid - (result >= 0 ? 1 : 2);}
0
public static int binarySearchFromTo(short[] array, short value, int from, int to)
{    int mid = -1;    while (from <= to) {        mid = (from + to) >>> 1;        if (value > array[mid]) {            from = mid + 1;        } else if (value == array[mid]) {            return mid;        } else {            to = mid - 1;        }    }    if (mid < 0) {        return -1;    }    return -mid - (value < array[mid] ? 1 : 2);}
0
private static boolean lessThan(double double1, double double2)
{        if (double1 < double2) {        return true;    }    if (double1 > double2) {        return false;    }    if (double1 == double2 && double1 != 0.0) {        return false;    }        if (Double.isNaN(double1)) {        return false;    }    if (Double.isNaN(double2)) {        return true;    }        long d1 = Double.doubleToRawLongBits(double1);    long d2 = Double.doubleToRawLongBits(double2);    return d1 < d2;}
0
private static boolean lessThan(float float1, float float2)
{        if (float1 < float2) {        return true;    }    if (float1 > float2) {        return false;    }    if (float1 == float2 && float1 != 0.0f) {        return false;    }        if (Float.isNaN(float1)) {        return false;    }    if (Float.isNaN(float2)) {        return true;    }        int f1 = Float.floatToRawIntBits(float1);    int f2 = Float.floatToRawIntBits(float2);    return f1 < f2;}
0
public static Centroid create(int key, Vector initialValue)
{    if (initialValue instanceof WeightedVector) {        return new Centroid(key, new DenseVector(initialValue), ((WeightedVector) initialValue).getWeight());    } else {        return new Centroid(key, new DenseVector(initialValue), 1);    }}
0
public void update(Vector v)
{    if (v instanceof Centroid) {        Centroid c = (Centroid) v;        update(c.delegate, c.getWeight());    } else {        update(v, 1);    }}
0
public void update(Vector other, final double wy)
{    final double wx = getWeight();    delegate.assign(other, Functions.reweigh(wx, wy));    setWeight(wx + wy);}
0
public Centroid like()
{    return new Centroid(getIndex(), getVector().like(), getWeight());}
0
public int getKey()
{    return getIndex();}
0
public void addWeight(double newWeight)
{    setWeight(getWeight() + newWeight);}
0
public String toString()
{    return String.format("key = %d, weight = %.2f, vector = %s", getIndex(), getWeight(), delegate);}
0
public Centroid clone()
{    return new Centroid(this);}
0
private void decomposeWithPivoting(Matrix a)
{    int n = a.rowSize();    L.assign(a);        double uberMax = L.viewDiagonal().aggregate(Functions.MAX, Functions.ABS);    for (int k = 0; k < n; k++) {        double max = 0;        int pivot = k;        for (int j = k; j < n; j++) {            if (L.get(j, j) > max) {                max = L.get(j, j);                pivot = j;                if (uberMax < Math.abs(max)) {                    uberMax = Math.abs(max);                }            }        }        L.swap(k, pivot);        double akk = L.get(k, k);        double epsilon = 1.0e-10 * Math.max(uberMax, L.viewColumn(k).aggregate(Functions.MAX, Functions.ABS));        if (akk < -epsilon) {                        throw new IllegalArgumentException("Matrix is not positive semi-definite");        } else if (akk <= epsilon) {                        L.viewColumn(k).assign(0);            isPositiveDefinite = false;                } else {                        akk = Math.sqrt(Math.max(0, akk));            L.viewColumn(k).viewPart(k, n - k).assign(Functions.div(akk));            L.viewColumn(k).viewPart(0, k).assign(0);                        for (int j = k + 1; j < n; j++) {                Vector columnJ = L.viewColumn(j).viewPart(k, n - k);                Vector columnK = L.viewColumn(k).viewPart(k, n - k);                columnJ.assign(columnK, Functions.minusMult(columnK.get(j - k)));            }        }    }}
0
private void decompose(Matrix a)
{    int n = a.rowSize();    L.assign(a);        for (int k = 0; k < n; k++) {        double akk = L.get(k, k);                L.viewColumn(k).viewPart(0, k).assign(0);        double epsilon = 1.0e-10 * L.viewColumn(k).aggregate(Functions.MAX, Functions.ABS);        if (akk <= epsilon) {                        L.viewColumn(k).viewPart(k, n - k).assign(0);            isPositiveDefinite = false;                } else {                        akk = Math.sqrt(Math.max(0, akk));            L.set(k, k, akk);            L.viewColumn(k).viewPart(k + 1, n - k - 1).assign(Functions.div(akk));                        for (int j = k + 1; j < n; j++) {                Vector columnJ = L.viewColumn(j).viewPart(j, n - j);                Vector columnK = L.viewColumn(k).viewPart(j, n - j);                columnJ.assign(columnK, Functions.minusMult(L.get(j, k)));            }        }    }}
0
public boolean isPositiveDefinite()
{    return isPositiveDefinite;}
0
public Matrix getL()
{    return L.getBase();}
0
public PivotedMatrix getPermutedL()
{    return L;}
0
public int[] getPivot()
{    return L.getRowPivot();}
0
public int[] getInversePivot()
{    return L.getInverseRowPivot();}
0
public Matrix solveLeft(Matrix z)
{    int n = L.columnSize();    int nx = z.columnSize();    Matrix X = new DenseMatrix(n, z.columnSize());    X.assign(z);        for (int internalK = 0; internalK < n; internalK++) {        int k = L.rowUnpivot(internalK);        for (int j = 0; j < nx; j++) {            for (int internalI = 0; internalI < internalK; internalI++) {                int i = L.rowUnpivot(internalI);                X.set(k, j, X.get(k, j) - X.get(i, j) * L.get(k, i));            }            if (L.get(k, k) != 0) {                X.set(k, j, X.get(k, j) / L.get(k, k));            } else {                X.set(k, j, 0);            }        }    }    return X;}
0
public Matrix solveRight(Matrix z)
{    int n = z.columnSize();    int nx = z.rowSize();    Matrix x = new DenseMatrix(z.rowSize(), z.columnSize());    x.assign(z);        for (int internalK = 0; internalK < n; internalK++) {        int k = L.rowUnpivot(internalK);        for (int j = 0; j < nx; j++) {            for (int internalI = 0; internalI < k; internalI++) {                int i = L.rowUnpivot(internalI);                x.set(j, k, x.get(j, k) - x.get(j, i) * L.get(k, i));                if (Double.isInfinite(x.get(j, k)) || Double.isNaN(x.get(j, k))) {                    throw new IllegalStateException(String.format("Invalid value found at %d,%d (should not be possible)", j, k));                }            }            if (L.get(k, k) != 0) {                x.set(j, k, x.get(j, k) / L.get(k, k));            } else {                x.set(j, k, 0);            }            if (Double.isInfinite(x.get(j, k)) || Double.isNaN(x.get(j, k))) {                throw new IllegalStateException(String.format("Invalid value found at %d,%d (should not be possible)", j, k));            }        }    }    return x;}
0
protected Matrix matrixLike(int rows, int columns)
{    return new DenseMatrix(rows, columns);}
0
public void mergeUpdates(OrderedIntDoubleMapping updates)
{    throw new UnsupportedOperationException("Cannot mutate a ConstantVector");}
0
public boolean isDense()
{    return true;}
0
public boolean isSequentialAccess()
{    return true;}
0
public Iterator<Element> iterator()
{    return new AbstractIterator<Element>() {        private int i = 0;        private final int n = size();        @Override        protected Element computeNext() {            if (i < n) {                return new LocalElement(i++);            } else {                return endOfData();            }        }    };}
0
protected Element computeNext()
{    if (i < n) {        return new LocalElement(i++);    } else {        return endOfData();    }}
0
public Iterator<Element> iterateNonZero()
{    return iterator();}
0
public double getQuick(int index)
{    return value;}
0
public Vector like()
{    return new DenseVector(size());}
0
public Vector like(int cardinality)
{    return new DenseVector(cardinality);}
0
public void setQuick(int index, double value)
{    throw new UnsupportedOperationException("Can't set a value in a constant matrix");}
0
public int getNumNondefaultElements()
{    return size();}
0
public double getLookupCost()
{    return 1;}
0
public double getIteratorAdvanceCost()
{    return 1;}
0
public boolean isAddConstantTime()
{    throw new UnsupportedOperationException("Cannot mutate a ConstantVector");}
0
public synchronized EigenStatus verify(VectorIterable corpus, Vector vector)
{    if (!finished && !started) {                status = new EigenStatus(-1, 0);        Vector vectorCopy = vector.clone();        threadPool.execute(new VerifierRunnable(corpus, vectorCopy));        started = true;    }    if (finished) {        finished = false;    }    return status;}
0
public void close()
{    this.threadPool.shutdownNow();}
0
protected EigenStatus innerVerify(VectorIterable corpus, Vector vector)
{    return super.verify(corpus, vector);}
0
public void run()
{    EigenStatus status = innerVerify(corpus, vector);    synchronized (AsyncEigenVerifier.this) {        AsyncEigenVerifier.this.status = status;        finished = true;        started = false;    }}
0
public double getCosAngle()
{    return cosAngle;}
0
public double getEigenValue()
{    return eigenValue;}
0
public boolean inProgress()
{    return inProgress;}
0
 void setInProgress(boolean status)
{    inProgress = status;}
0
public TrainingState solve(Matrix corpus, int desiredRank)
{    int cols = corpus.numCols();    Matrix eigens = new DenseMatrix(desiredRank, cols);    List<Double> eigenValues = new ArrayList<>();        /*     * The corpusProjections matrix is a running cache of the residual projection of each corpus vector against all     * of the previously found singular vectors.  Without this, if multiple passes over the data is made (per     * singular vector), recalculating these projections eventually dominates the computational complexity of the     * solver.     */    Matrix corpusProjections = new DenseMatrix(corpus.numRows(), desiredRank);    TrainingState state = new TrainingState(eigens, corpusProjections);    for (int i = 0; i < desiredRank; i++) {        Vector currentEigen = new DenseVector(cols);        Vector previousEigen = null;        while (hasNotConverged(currentEigen, corpus, state)) {            int randomStartingIndex = getRandomStartingIndex(corpus, eigens);            Vector initialTrainingVector = corpus.viewRow(randomStartingIndex);            state.setTrainingIndex(randomStartingIndex);            updater.update(currentEigen, initialTrainingVector, state);            for (int corpusRow = 0; corpusRow < corpus.numRows(); corpusRow++) {                state.setTrainingIndex(corpusRow);                if (corpusRow != randomStartingIndex) {                    updater.update(currentEigen, corpus.viewRow(corpusRow), state);                }            }            state.setFirstPass(false);            if (DEBUG) {                if (previousEigen == null) {                    previousEigen = currentEigen.clone();                } else {                    double dot = currentEigen.dot(previousEigen);                    if (dot > 0.0) {                        dot /= currentEigen.norm(2) * previousEigen.norm(2);                    }                                }            }        }                double eigenValue = state.getStatusProgress().get(state.getStatusProgress().size() - 1).getEigenValue();                        currentEigen.assign(new TimesFunction(), 1 / currentEigen.norm(2));        eigens.assignRow(i, currentEigen);        eigenValues.add(eigenValue);        state.setCurrentEigenValues(eigenValues);                /**         *  TODO: Persist intermediate output!         */        state.setFirstPass(true);        state.setNumEigensProcessed(state.getNumEigensProcessed() + 1);        state.setActivationDenominatorSquared(0);        state.setActivationNumerator(0);        state.getStatusProgress().clear();        numPasses = 0;    }    return state;}
1
private int getRandomStartingIndex(Matrix corpus, Matrix eigens)
{    int index;    Vector v;    do {        double r = rng.nextDouble();        index = (int) (r * corpus.numRows());        v = corpus.viewRow(index);    } while (v == null || v.norm(2) == 0 || v.getNumNondefaultElements() < 5);    return index;}
0
protected boolean hasNotConverged(Vector currentPseudoEigen, Matrix corpus, TrainingState state)
{    numPasses++;    if (state.isFirstPass()) {                return true;    }    Matrix previousEigens = state.getCurrentEigens();        /*     * Step 1: orthogonalize currentPseudoEigen by subtracting off eigen(i) * helper.get(i)     * Step 2: zero-out the helper vector because it has already helped.     */    for (int i = 0; i < state.getNumEigensProcessed(); i++) {        Vector previousEigen = previousEigens.viewRow(i);        currentPseudoEigen.assign(previousEigen, new PlusMult(-state.getHelperVector().get(i)));        state.getHelperVector().set(i, 0);    }    if (currentPseudoEigen.norm(2) > 0) {        for (int i = 0; i < state.getNumEigensProcessed(); i++) {            Vector previousEigen = previousEigens.viewRow(i);                    }    }    /*     * Step 3: verify how eigen-like the prospective eigen is.  This is potentially asynchronous.     */    EigenStatus status = verify(corpus, currentPseudoEigen);    if (status.inProgress()) {            } else {                state.getStatusProgress().add(status);    }    return state.getStatusProgress().size() <= maxPassesPerEigen && 1.0 - status.getCosAngle() > convergenceTarget;}
1
protected EigenStatus verify(Matrix corpus, Vector currentPseudoEigen)
{    return verifier.verify(corpus, currentPseudoEigen);}
0
public static void main(String[] args)
{    Properties props = new Properties();    String propertiesFile = args.length > 0 ? args[0] : "config/solver.properties";        String corpusDir = props.getProperty("solver.input.dir");    String outputDir = props.getProperty("solver.output.dir");    if (corpusDir == null || corpusDir.isEmpty() || outputDir == null || outputDir.isEmpty()) {                return;    }        int rank = Integer.parseInt(props.getProperty("solver.output.desiredRank"));    double convergence = Double.parseDouble(props.getProperty("solver.convergence"));    int maxPasses = Integer.parseInt(props.getProperty("solver.maxPasses"));        HebbianUpdater updater = new HebbianUpdater();    SingularVectorVerifier verifier = new AsyncEigenVerifier();    HebbianSolver solver = new HebbianSolver(updater, verifier, convergence, maxPasses);    Matrix corpus = null;    /*    if (numThreads <= 1) {          } else {          }     */    long now = System.currentTimeMillis();    TrainingState finalState = solver.solve(corpus, rank);    long time = (System.currentTimeMillis() - now) / 1000;    }
1
public void update(Vector pseudoEigen, Vector trainingVector, TrainingState currentState)
{    double trainingVectorNorm = trainingVector.norm(2);    int numPreviousEigens = currentState.getNumEigensProcessed();    if (numPreviousEigens > 0 && currentState.isFirstPass()) {        updateTrainingProjectionsVector(currentState, trainingVector, numPreviousEigens - 1);    }    if (currentState.getActivationDenominatorSquared() == 0 || trainingVectorNorm == 0) {        if (currentState.getActivationDenominatorSquared() == 0) {            pseudoEigen.assign(trainingVector, new PlusMult(1));            currentState.setHelperVector(currentState.currentTrainingProjection().clone());            double helperNorm = currentState.getHelperVector().norm(2);            currentState.setActivationDenominatorSquared(trainingVectorNorm * trainingVectorNorm - helperNorm * helperNorm);        }        return;    }    currentState.setActivationNumerator(pseudoEigen.dot(trainingVector));    currentState.setActivationNumerator(currentState.getActivationNumerator() - currentState.getHelperVector().dot(currentState.currentTrainingProjection()));    double activation = currentState.getActivationNumerator() / Math.sqrt(currentState.getActivationDenominatorSquared());    currentState.setActivationDenominatorSquared(currentState.getActivationDenominatorSquared() + 2 * activation * currentState.getActivationNumerator() + activation * activation * (trainingVector.getLengthSquared() - currentState.currentTrainingProjection().getLengthSquared()));    if (numPreviousEigens > 0) {        currentState.getHelperVector().assign(currentState.currentTrainingProjection(), new PlusMult(activation));    }    pseudoEigen.assign(trainingVector, new PlusMult(activation));}
0
private static void updateTrainingProjectionsVector(TrainingState state, Vector trainingVector, int previousEigenIndex)
{    Vector previousEigen = state.mostRecentEigen();    Vector currentTrainingVectorProjection = state.currentTrainingProjection();    double projection = previousEigen.dot(trainingVector);    currentTrainingVectorProjection.set(previousEigenIndex, projection);}
0
public Vector mostRecentEigen()
{    return currentEigens.viewRow(numEigensProcessed - 1);}
0
public Vector currentTrainingProjection()
{    if (trainingProjections.viewRow(trainingIndex) == null) {        trainingProjections.assignRow(trainingIndex, new DenseVector(currentEigens.numCols()));    }    return trainingProjections.viewRow(trainingIndex);}
0
public Matrix getCurrentEigens()
{    return currentEigens;}
0
public void setCurrentEigens(Matrix currentEigens)
{    this.currentEigens = currentEigens;}
0
public int getNumEigensProcessed()
{    return numEigensProcessed;}
0
public void setNumEigensProcessed(int numEigensProcessed)
{    this.numEigensProcessed = numEigensProcessed;}
0
public List<Double> getCurrentEigenValues()
{    return currentEigenValues;}
0
public void setCurrentEigenValues(List<Double> currentEigenValues)
{    this.currentEigenValues = currentEigenValues;}
0
public Matrix getTrainingProjections()
{    return trainingProjections;}
0
public void setTrainingProjections(Matrix trainingProjections)
{    this.trainingProjections = trainingProjections;}
0
public int getTrainingIndex()
{    return trainingIndex;}
0
public void setTrainingIndex(int trainingIndex)
{    this.trainingIndex = trainingIndex;}
0
public Vector getHelperVector()
{    return helperVector;}
0
public void setHelperVector(Vector helperVector)
{    this.helperVector = helperVector;}
0
public boolean isFirstPass()
{    return firstPass;}
0
public void setFirstPass(boolean firstPass)
{    this.firstPass = firstPass;}
0
public List<EigenStatus> getStatusProgress()
{    return statusProgress;}
0
public void setStatusProgress(List<EigenStatus> statusProgress)
{    this.statusProgress = statusProgress;}
0
public double getActivationNumerator()
{    return activationNumerator;}
0
public void setActivationNumerator(double activationNumerator)
{    this.activationNumerator = activationNumerator;}
0
public double getActivationDenominatorSquared()
{    return activationDenominatorSquared;}
0
public void setActivationDenominatorSquared(double activationDenominatorSquared)
{    this.activationDenominatorSquared = activationDenominatorSquared;}
0
public double apply(double arg1)
{    return arg1 * d;}
0
public void solve(LanczosState state, int desiredRank)
{    solve(state, desiredRank, false);}
0
public void solve(LanczosState state, int desiredRank, boolean isSymmetric)
{    VectorIterable corpus = state.getCorpus();        int i = state.getIterationNumber();    Vector currentVector = state.getBasisVector(i - 1);    Vector previousVector = state.getBasisVector(i - 2);    double beta = 0;    Matrix triDiag = state.getDiagonalMatrix();    while (i < desiredRank) {        startTime(TimingSection.ITERATE);        Vector nextVector = isSymmetric ? corpus.times(currentVector) : corpus.timesSquared(currentVector);                if (state.getScaleFactor() <= 0) {            state.setScaleFactor(calculateScaleFactor(nextVector));        }        nextVector.assign(new Scale(1.0 / state.getScaleFactor()));        if (previousVector != null) {            nextVector.assign(previousVector, new PlusMult(-beta));        }                double alpha = currentVector.dot(nextVector);        nextVector.assign(currentVector, new PlusMult(-alpha));        endTime(TimingSection.ITERATE);        startTime(TimingSection.ORTHOGANLIZE);        orthoganalizeAgainstAllButLast(nextVector, state);        endTime(TimingSection.ORTHOGANLIZE);                beta = nextVector.norm(2);        if (outOfRange(beta) || outOfRange(alpha)) {                        break;        }        nextVector.assign(new Scale(1 / beta));        state.setBasisVector(i, nextVector);        previousVector = currentVector;        currentVector = nextVector;                triDiag.set(i - 1, i - 1, alpha);        if (i < desiredRank - 1) {            triDiag.set(i - 1, i, beta);            triDiag.set(i, i - 1, beta);        }        state.setIterationNumber(++i);    }    startTime(TimingSection.TRIDIAG_DECOMP);            EigenDecomposition decomp = new EigenDecomposition(triDiag);    Matrix eigenVects = decomp.getV();    Vector eigenVals = decomp.getRealEigenvalues();    endTime(TimingSection.TRIDIAG_DECOMP);    startTime(TimingSection.FINAL_EIGEN_CREATE);    for (int row = 0; row < i; row++) {        Vector realEigen = null;        Vector ejCol = eigenVects.viewColumn(row);        int size = Math.min(ejCol.size(), state.getBasisSize());        for (int j = 0; j < size; j++) {            double d = ejCol.get(j);            Vector rowJ = state.getBasisVector(j);            if (realEigen == null) {                realEigen = rowJ.like();            }            realEigen.assign(rowJ, new PlusMult(d));        }        Preconditions.checkState(realEigen != null);        assert realEigen != null;        realEigen = realEigen.normalize();        state.setRightSingularVector(row, realEigen);        double e = eigenVals.get(row) * state.getScaleFactor();        if (!isSymmetric) {            e = Math.sqrt(e);        }                state.setSingularValue(row, e);    }        endTime(TimingSection.FINAL_EIGEN_CREATE);}
1
protected static double calculateScaleFactor(Vector nextVector)
{    return nextVector.norm(2);}
0
private static boolean outOfRange(double d)
{    return Double.isNaN(d) || d > SAFE_MAX || -d > SAFE_MAX;}
0
protected static void orthoganalizeAgainstAllButLast(Vector nextVector, LanczosState state)
{    for (int i = 0; i < state.getIterationNumber(); i++) {        Vector basisVector = state.getBasisVector(i);        double alpha;        if (basisVector == null || (alpha = nextVector.dot(basisVector)) == 0.0) {            continue;        }        nextVector.assign(basisVector, new PlusMult(-alpha));    }}
0
private void startTime(TimingSection section)
{    startTimes.put(section, System.nanoTime());}
0
private void endTime(TimingSection section)
{    if (!times.containsKey(section)) {        times.put(section, 0L);    }    times.put(section, times.get(section) + System.nanoTime() - startTimes.get(section));}
0
private void intitializeBasisAndSingularVectors()
{    basis = Maps.newHashMap();    singularVectors = Maps.newHashMap();}
0
public Matrix getDiagonalMatrix()
{    return diagonalMatrix;}
0
public int getIterationNumber()
{    return iterationNumber;}
0
public double getScaleFactor()
{    return scaleFactor;}
0
public VectorIterable getCorpus()
{    return corpus;}
0
public Vector getRightSingularVector(int i)
{    return singularVectors.get(i);}
0
public Double getSingularValue(int i)
{    return singularValues.get(i);}
0
public Vector getBasisVector(int i)
{    return basis.get(i);}
0
public int getBasisSize()
{    return basis.size();}
0
public void setBasisVector(int i, Vector basisVector)
{    basis.put(i, basisVector);}
0
public void setScaleFactor(double scale)
{    scaleFactor = scale;}
0
public void setIterationNumber(int i)
{    iterationNumber = i;}
0
public void setRightSingularVector(int i, Vector vector)
{    singularVectors.put(i, vector);}
0
public void setSingularValue(int i, double value)
{    singularValues.put(i, value);}
0
public EigenStatus verify(VectorIterable corpus, Vector vector)
{    Vector resultantVector = corpus.timesSquared(vector);    double newNorm = resultantVector.norm(2);    double oldNorm = vector.norm(2);    double eigenValue;    double cosAngle;    if (newNorm > 0 && oldNorm > 0) {        eigenValue = newNorm / oldNorm;        cosAngle = resultantVector.dot(vector) / newNorm * oldNorm;    } else {        eigenValue = 1.0;        cosAngle = 0.0;    }    return new EigenStatus(eigenValue, cosAngle, false);}
0
public Vector getVector()
{    return delegate;}
0
public double aggregate(DoubleDoubleFunction aggregator, DoubleFunction map)
{    return delegate.aggregate(aggregator, map);}
0
public double aggregate(Vector other, DoubleDoubleFunction aggregator, DoubleDoubleFunction combiner)
{    return delegate.aggregate(other, aggregator, combiner);}
0
public Vector viewPart(int offset, int length)
{    return delegate.viewPart(offset, length);}
0
public Vector clone()
{    DelegatingVector r;    try {        r = (DelegatingVector) super.clone();    } catch (CloneNotSupportedException e) {        throw new RuntimeException("Clone not supported for DelegatingVector, shouldn't be possible");    }        r.delegate = delegate.clone();    return r;}
0
public Iterable<Element> all()
{    return delegate.all();}
0
public Iterable<Element> nonZeroes()
{    return delegate.nonZeroes();}
0
public Vector divide(double x)
{    return delegate.divide(x);}
0
public double dot(Vector x)
{    return delegate.dot(x);}
0
public double get(int index)
{    return delegate.get(index);}
0
public Element getElement(int index)
{    return delegate.getElement(index);}
0
public void mergeUpdates(OrderedIntDoubleMapping updates)
{    delegate.mergeUpdates(updates);}
0
public Vector minus(Vector that)
{    return delegate.minus(that);}
0
public Vector normalize()
{    return delegate.normalize();}
0
public Vector normalize(double power)
{    return delegate.normalize(power);}
0
public Vector logNormalize()
{    return delegate.logNormalize();}
0
public Vector logNormalize(double power)
{    return delegate.logNormalize(power);}
0
public double norm(double power)
{    return delegate.norm(power);}
0
public double getLengthSquared()
{    return delegate.getLengthSquared();}
0
public void invalidateCachedLength()
{    if (delegate instanceof LengthCachingVector) {        ((LengthCachingVector) delegate).invalidateCachedLength();    }}
0
public double getDistanceSquared(Vector v)
{    return delegate.getDistanceSquared(v);}
0
public double getLookupCost()
{    return delegate.getLookupCost();}
0
public double getIteratorAdvanceCost()
{    return delegate.getIteratorAdvanceCost();}
0
public boolean isAddConstantTime()
{    return delegate.isAddConstantTime();}
0
public double maxValue()
{    return delegate.maxValue();}
0
public int maxValueIndex()
{    return delegate.maxValueIndex();}
0
public double minValue()
{    return delegate.minValue();}
0
public int minValueIndex()
{    return delegate.minValueIndex();}
0
public Vector plus(double x)
{    return delegate.plus(x);}
0
public Vector plus(Vector x)
{    return delegate.plus(x);}
0
public void set(int index, double value)
{    delegate.set(index, value);}
0
public Vector times(double x)
{    return delegate.times(x);}
0
public Vector times(Vector x)
{    return delegate.times(x);}
0
public double zSum()
{    return delegate.zSum();}
0
public Vector assign(double value)
{    delegate.assign(value);    return this;}
0
public Vector assign(double[] values)
{    delegate.assign(values);    return this;}
0
public Vector assign(Vector other)
{    delegate.assign(other);    return this;}
0
public Vector assign(DoubleDoubleFunction f, double y)
{    delegate.assign(f, y);    return this;}
0
public Vector assign(DoubleFunction function)
{    delegate.assign(function);    return this;}
0
public Vector assign(Vector other, DoubleDoubleFunction function)
{    delegate.assign(other, function);    return this;}
0
public Matrix cross(Vector other)
{    return delegate.cross(other);}
0
public int size()
{    return delegate.size();}
0
public String asFormatString()
{    return delegate.asFormatString();}
0
public int hashCode()
{    return delegate.hashCode();}
0
public boolean equals(Object o)
{    return delegate.equals(o);}
0
public String toString()
{    return delegate.toString();}
0
public boolean isDense()
{    return delegate.isDense();}
0
public boolean isSequentialAccess()
{    return delegate.isSequentialAccess();}
0
public double getQuick(int index)
{    return delegate.getQuick(index);}
0
public Vector like()
{    return new DelegatingVector(delegate.like());}
0
public Vector like(int cardinality)
{    return new DelegatingVector(delegate.like(cardinality));}
0
public void setQuick(int index, double value)
{    delegate.setQuick(index, value);}
0
public void incrementQuick(int index, double increment)
{    delegate.incrementQuick(index, increment);}
0
public int getNumNondefaultElements()
{    return delegate.getNumNondefaultElements();}
0
public int getNumNonZeroElements()
{    return delegate.getNumNonZeroElements();}
0
public double[][] getBackingStructure()
{    return this.values;}
0
public Matrix clone()
{    DenseMatrix clone = (DenseMatrix) super.clone();    clone.values = new double[values.length][];    for (int i = 0; i < values.length; i++) {        clone.values[i] = values[i].clone();    }    return clone;}
0
public double getQuick(int row, int column)
{    return values[row][column];}
0
public Matrix like()
{    return like(rowSize(), columnSize());}
0
public Matrix like(int rows, int columns)
{    return new DenseMatrix(rows, columns);}
0
public void setQuick(int row, int column, double value)
{    values[row][column] = value;}
0
public Matrix viewPart(int[] offset, int[] size)
{    int rowOffset = offset[ROW];    int rowsRequested = size[ROW];    int columnOffset = offset[COL];    int columnsRequested = size[COL];    return viewPart(rowOffset, rowsRequested, columnOffset, columnsRequested);}
0
public Matrix viewPart(int rowOffset, int rowsRequested, int columnOffset, int columnsRequested)
{    if (rowOffset < 0) {        throw new IndexException(rowOffset, rowSize());    }    if (rowOffset + rowsRequested > rowSize()) {        throw new IndexException(rowOffset + rowsRequested, rowSize());    }    if (columnOffset < 0) {        throw new IndexException(columnOffset, columnSize());    }    if (columnOffset + columnsRequested > columnSize()) {        throw new IndexException(columnOffset + columnsRequested, columnSize());    }    return new MatrixView(this, new int[] { rowOffset, columnOffset }, new int[] { rowsRequested, columnsRequested });}
0
public Matrix assign(double value)
{    for (int row = 0; row < rowSize(); row++) {        Arrays.fill(values[row], value);    }    return this;}
0
public Matrix assign(DenseMatrix matrix)
{        if (matrix.values[0].length != this.values[0].length || matrix.values.length != this.values.length) {        this.values = new double[matrix.values.length][matrix.values[0].length];    }        for (int i = 0; i < this.values.length; i++) {        System.arraycopy(matrix.values[i], 0, this.values[i], 0, this.values[0].length);    }    return this;}
0
public Matrix assignColumn(int column, Vector other)
{    if (rowSize() != other.size()) {        throw new CardinalityException(rowSize(), other.size());    }    if (column < 0 || column >= columnSize()) {        throw new IndexException(column, columnSize());    }    for (int row = 0; row < rowSize(); row++) {        values[row][column] = other.getQuick(row);    }    return this;}
0
public Matrix assignRow(int row, Vector other)
{    if (columnSize() != other.size()) {        throw new CardinalityException(columnSize(), other.size());    }    if (row < 0 || row >= rowSize()) {        throw new IndexException(row, rowSize());    }    for (int col = 0; col < columnSize(); col++) {        values[row][col] = other.getQuick(col);    }    return this;}
0
public Vector viewRow(int row)
{    if (row < 0 || row >= rowSize()) {        throw new IndexException(row, rowSize());    }    return new DenseVector(values[row], true);}
0
public MatrixFlavor getFlavor()
{    return MatrixFlavor.DENSELIKE;}
0
public double getQuick(int row, int column)
{    if (column < row) {        int swap = row;        row = column;        column = swap;    }    return super.getQuick(row, column);}
0
public void setQuick(int row, int column, double value)
{    if (column < row) {        int swap = row;        row = column;        column = swap;    }    super.setQuick(row, column, value);}
0
public double dot(Vector x)
{    if (!x.isDense()) {        return super.dot(x);    } else {        int size = x.size();        if (values.length != size) {            throw new CardinalityException(values.length, size);        }        double sum = 0;        for (int n = 0; n < size; n++) {            sum += values[n] * x.getQuick(n);        }        return sum;    }}
0
protected Matrix matrixLike(int rows, int columns)
{    return new DenseMatrix(rows, columns);}
0
public DenseVector clone()
{    return new DenseVector(values.clone());}
0
public boolean isDense()
{    return true;}
0
public boolean isSequentialAccess()
{    return true;}
0
protected double dotSelf()
{    double result = 0.0;    int max = size();    for (int i = 0; i < max; i++) {        result += values[i] * values[i];    }    return result;}
0
public double getQuick(int index)
{    return values[index];}
0
public DenseVector like()
{    return new DenseVector(size());}
0
public Vector like(int cardinality)
{    return new DenseVector(cardinality);}
0
public void setQuick(int index, double value)
{    invalidateCachedLength();    values[index] = value;}
0
public void incrementQuick(int index, double increment)
{    invalidateCachedLength();    values[index] += increment;}
0
public Vector assign(double value)
{    invalidateCachedLength();    Arrays.fill(values, value);    return this;}
0
public int getNumNondefaultElements()
{    return values.length;}
0
public int getNumNonZeroElements()
{    int numNonZeros = 0;    for (int index = 0; index < values.length; index++) {        if (values[index] != 0) {            numNonZeros++;        }    }    return numNonZeros;}
0
public Vector assign(DenseVector vector)
{        if (vector.values.length != this.values.length) {        this.values = new double[vector.values.length];    }        System.arraycopy(vector.values, 0, this.values, 0, this.values.length);    return this;}
0
public void mergeUpdates(OrderedIntDoubleMapping updates)
{    int numUpdates = updates.getNumMappings();    int[] indices = updates.getIndices();    double[] values = updates.getValues();    for (int i = 0; i < numUpdates; ++i) {        this.values[indices[i]] = values[i];    }}
0
public Vector viewPart(int offset, int length)
{    if (offset < 0) {        throw new IndexException(offset, size());    }    if (offset + length > size()) {        throw new IndexException(offset + length, size());    }    return new DenseVectorView(this, offset, length);}
0
public double getLookupCost()
{    return 1;}
0
public double getIteratorAdvanceCost()
{    return 1;}
0
public boolean isAddConstantTime()
{    return true;}
0
public Iterator<Element> iterateNonZero()
{    return new NonDefaultIterator();}
0
public Iterator<Element> iterator()
{    return new AllIterator();}
0
public boolean equals(Object o)
{    if (o instanceof DenseVector) {                return Arrays.equals(values, ((DenseVector) o).values);    }    return super.equals(o);}
0
public void addAll(Vector v)
{    if (size() != v.size()) {        throw new CardinalityException(size(), v.size());    }    for (Element element : v.nonZeroes()) {        values[element.index()] += element.get();    }}
0
public boolean hasNext()
{    if (lookAheadIndex == index) {                lookAhead();    }        return lookAheadIndex < size();}
0
private void lookAhead()
{    lookAheadIndex++;    while (lookAheadIndex < size() && values[lookAheadIndex] == 0.0) {        lookAheadIndex++;    }}
0
public Element next()
{    if (lookAheadIndex == index) {                lookAhead();    }    Preconditions.checkState(lookAheadIndex > index);    index = lookAheadIndex;    if (index >= size()) {                throw new NoSuchElementException();    }    element.index = index;    return element;}
0
public void remove()
{    throw new UnsupportedOperationException();}
0
public boolean hasNext()
{    return element.index + 1 < size();}
0
public Element next()
{    if (element.index + 1 >= size()) {                throw new NoSuchElementException();    }    element.index++;    return element;}
0
public void remove()
{    throw new UnsupportedOperationException();}
0
public double get()
{    return values[index];}
0
public int index()
{    return index;}
0
public void set(double value)
{    invalidateCachedLength();    values[index] = value;}
0
public double dot(Vector x)
{        if (x instanceof DenseVectorView) {        if (size() != x.size())            throw new IllegalArgumentException("Cardinality mismatch during dot(x,y).");        DenseVectorView xv = (DenseVectorView) x;        double[] thisValues = ((DenseVector) vector).values;        double[] thatValues = ((DenseVector) xv.vector).values;        int untilOffset = offset + size();        int i, j;        double sum = 0.0;                int until4 = offset + (size() & ~3);        for (i = offset, j = xv.offset; i < until4; i += 4, j += 4) {            sum += thisValues[i] * thatValues[j] + thisValues[i + 1] * thatValues[j + 1] + thisValues[i + 2] * thatValues[j + 2] + thisValues[i + 3] * thatValues[j + 3];        }                for (i = offset, j = xv.offset; i < untilOffset; ) {            sum += thisValues[i++] * thatValues[j++];        }        return sum;    } else if (x instanceof DenseVector) {        if (size() != x.size())            throw new IllegalArgumentException("Cardinality mismatch during dot(x,y).");        DenseVector xv = (DenseVector) x;        double[] thisValues = ((DenseVector) vector).values;        double[] thatValues = xv.values;        int untilOffset = offset + size();        int i, j;        double sum = 0.0;                int until4 = offset + (size() & ~3);        for (i = offset, j = 0; i < until4; i += 4, j += 4) {            sum += thisValues[i] * thatValues[j] + thisValues[i + 1] * thatValues[j + 1] + thisValues[i + 2] * thatValues[j + 2] + thisValues[i + 3] * thatValues[j + 3];        }                for (; i < untilOffset; ) {            sum += thisValues[i++] * thatValues[j++];        }        return sum;    } else {        return super.dot(x);    }}
0
public Vector viewPart(int offset, int length)
{    if (offset < 0) {        throw new IndexException(offset, size());    }    if (offset + length > size()) {        throw new IndexException(offset + length, size());    }    return new DenseVectorView(vector, offset + this.offset, length);}
0
public static DiagonalMatrix identity(int size)
{    return new DiagonalMatrix(1, size);}
0
public Matrix assignColumn(int column, Vector other)
{    throw new UnsupportedOperationException("Can't assign a column to a diagonal matrix");}
0
public Matrix assignRow(int row, Vector other)
{    throw new UnsupportedOperationException("Can't assign a row to a diagonal matrix");}
0
public Vector viewRow(int row)
{    return new SingleElementVector(row);}
0
public Vector viewColumn(int row)
{    return new SingleElementVector(row);}
0
public double getQuick(int index)
{    if (index == this.index) {        return diagonal.get(index);    } else {        return 0;    }}
0
public void set(int index, double value)
{    if (index == this.index) {        diagonal.set(index, value);    } else {        throw new IllegalArgumentException("Can't set off-diagonal element of diagonal matrix");    }}
0
protected Iterator<Element> iterateNonZero()
{    return new Iterator<Element>() {        boolean more = true;        @Override        public boolean hasNext() {            return more;        }        @Override        public Element next() {            if (more) {                more = false;                return new Element() {                    @Override                    public double get() {                        return diagonal.get(index);                    }                    @Override                    public int index() {                        return index;                    }                    @Override                    public void set(double value) {                        diagonal.set(index, value);                    }                };            } else {                throw new NoSuchElementException("Only one non-zero element in a row or column of a diagonal matrix");            }        }        @Override        public void remove() {            throw new UnsupportedOperationException("Can't remove from vector view");        }    };}
0
public boolean hasNext()
{    return more;}
0
public Element next()
{    if (more) {        more = false;        return new Element() {            @Override            public double get() {                return diagonal.get(index);            }            @Override            public int index() {                return index;            }            @Override            public void set(double value) {                diagonal.set(index, value);            }        };    } else {        throw new NoSuchElementException("Only one non-zero element in a row or column of a diagonal matrix");    }}
0
public double get()
{    return diagonal.get(index);}
0
public int index()
{    return index;}
0
public void set(double value)
{    diagonal.set(index, value);}
0
public void remove()
{    throw new UnsupportedOperationException("Can't remove from vector view");}
0
protected Iterator<Element> iterator()
{    return new Iterator<Element>() {        int i = 0;        Element r = new Element() {            @Override            public double get() {                if (i == index) {                    return diagonal.get(index);                } else {                    return 0;                }            }            @Override            public int index() {                return i;            }            @Override            public void set(double value) {                if (i == index) {                    diagonal.set(index, value);                } else {                    throw new IllegalArgumentException("Can't set any element but diagonal");                }            }        };        @Override        public boolean hasNext() {            return i < diagonal.size() - 1;        }        @Override        public Element next() {            if (i < SingleElementVector.this.size() - 1) {                i++;                return r;            } else {                throw new NoSuchElementException("Attempted to access passed last element of vector");            }        }        @Override        public void remove() {            throw new UnsupportedOperationException("Default operation");        }    };}
0
public double get()
{    if (i == index) {        return diagonal.get(index);    } else {        return 0;    }}
0
public int index()
{    return i;}
0
public void set(double value)
{    if (i == index) {        diagonal.set(index, value);    } else {        throw new IllegalArgumentException("Can't set any element but diagonal");    }}
0
public boolean hasNext()
{    return i < diagonal.size() - 1;}
0
public Element next()
{    if (i < SingleElementVector.this.size() - 1) {        i++;        return r;    } else {        throw new NoSuchElementException("Attempted to access passed last element of vector");    }}
0
public void remove()
{    throw new UnsupportedOperationException("Default operation");}
0
protected Matrix matrixLike(int rows, int columns)
{    return new DiagonalMatrix(rows, columns);}
0
public boolean isDense()
{    return false;}
0
public boolean isSequentialAccess()
{    return true;}
0
public void mergeUpdates(OrderedIntDoubleMapping updates)
{    throw new UnsupportedOperationException("Default operation");}
0
public Vector like()
{    return new DenseVector(size());}
0
public Vector like(int cardinality)
{    return new DenseVector(cardinality);}
0
public void setQuick(int index, double value)
{    if (index == this.index) {        diagonal.set(this.index, value);    } else {        throw new IllegalArgumentException("Can't set off-diagonal element of DiagonalMatrix");    }}
0
public int getNumNondefaultElements()
{    return 1;}
0
public double getLookupCost()
{    return 0;}
0
public double getIteratorAdvanceCost()
{    return 1;}
0
public boolean isAddConstantTime()
{    return false;}
0
public Vector viewDiagonal()
{    return this.diagonal;}
0
public double getQuick(int row, int column)
{    if (row == column) {        return diagonal.get(row);    } else {        return 0;    }}
0
public Matrix like()
{    return new SparseRowMatrix(rowSize(), columnSize());}
0
public Matrix like(int rows, int columns)
{    return new SparseRowMatrix(rows, columns);}
0
public void setQuick(int row, int column, double value)
{    if (row == column) {        diagonal.set(row, value);    } else {        throw new UnsupportedOperationException("Can't set off-diagonal element");    }}
0
public int[] getNumNondefaultElements()
{    throw new UnsupportedOperationException("Don't understand how to implement this");}
0
public Matrix viewPart(int[] offset, int[] size)
{    return new MatrixView(this, offset, size);}
0
public Matrix times(Matrix other)
{    return timesRight(other);}
0
public Matrix timesRight(Matrix that)
{    if (that.numRows() != diagonal.size()) {        throw new IllegalArgumentException("Incompatible number of rows in the right operand of matrix multiplication.");    }    Matrix m = that.like();    for (int row = 0; row < diagonal.size(); row++) {        m.assignRow(row, that.viewRow(row).times(diagonal.getQuick(row)));    }    return m;}
0
public Matrix timesLeft(Matrix that)
{    if (that.numCols() != diagonal.size()) {        throw new IllegalArgumentException("Incompatible number of rows in the left operand of matrix-matrix multiplication.");    }    Matrix m = that.like();    for (int col = 0; col < diagonal.size(); col++) {        m.assignColumn(col, that.viewColumn(col).times(diagonal.getQuick(col)));    }    return m;}
0
public MatrixFlavor getFlavor()
{    return MatrixFlavor.DIAGONALLIKE;}
0
private void addData(DoubleBuffer content)
{    this.content.add(content);}
0
public void setData(File f, boolean loadNow) throws IOException
{    Preconditions.checkArgument(f.length() == rows * columns * 8L, "File " + f + " is wrong length");    for (int i = 0; i < (rows + rowsPerBlock - 1) / rowsPerBlock; i++) {        long start = i * rowsPerBlock * columns * 8L;        long size = rowsPerBlock * columns * 8L;        MappedByteBuffer buf = new FileInputStream(f).getChannel().map(FileChannel.MapMode.READ_ONLY, start, Math.min(f.length() - start, size));        if (loadNow) {            buf.load();        }        addData(buf.asDoubleBuffer());    }}
0
public static void writeMatrix(File f, Matrix m) throws IOException
{    Preconditions.checkArgument(f.canWrite(), "Can't write to output file");    FileOutputStream fos = new FileOutputStream(f);    try {        ByteBuffer buf = ByteBuffer.allocate(m.columnSize() * 8);        for (MatrixSlice row : m) {            buf.clear();            for (Vector.Element element : row.vector().all()) {                buf.putDouble(element.get());            }            buf.flip();            fos.write(buf.array());        }    } finally {        fos.close();    }}
0
public Matrix assignColumn(int column, Vector other)
{    throw new UnsupportedOperationException("Default operation");}
0
public Matrix assignRow(int row, Vector other)
{    throw new UnsupportedOperationException("Default operation");}
0
public double getQuick(int row, int column)
{    int block = row / rowsPerBlock;    return content.get(block).get((row % rowsPerBlock) * columns + column);}
0
public Matrix like()
{    throw new UnsupportedOperationException("Default operation");}
0
public Matrix like(int rows, int columns)
{    return new DenseMatrix(rows, columns);}
0
public void setQuick(int row, int column, double value)
{    throw new UnsupportedOperationException("Default operation");}
0
public Matrix viewPart(int[] offset, int[] size)
{    throw new UnsupportedOperationException("Default operation");}
0
public void setData(File f) throws IOException
{    List<ByteBuffer> buffers = Lists.newArrayList();    FileChannel input = new FileInputStream(f).getChannel();    buffers.add(input.map(FileChannel.MapMode.READ_ONLY, 0, Math.min(Integer.MAX_VALUE, f.length())));    data.add(buffers.get(0).asIntBuffer());    Preconditions.checkArgument(buffers.get(0).getInt() == MAGIC_NUMBER_V0, "Wrong type of file");    int rows = buffers.get(0).getInt();    int cols = buffers.get(0).getInt();    Preconditions.checkArgument(rows == rowSize());    Preconditions.checkArgument(cols == columnSize());    rowOffset = new int[rows];    rowSize = new int[rows];    bufferIndex = new int[rows];    int offset = 12 + 4 * rows;    for (int i = 0; i < rows; i++) {        int size = buffers.get(0).getInt();        int buffer = 0;        while (buffer < buffers.size()) {            if (offset + size * 4 <= buffers.get(buffer).limit()) {                break;            } else {                offset -= buffers.get(buffer).capacity();            }        }        if (buffer == buffers.size()) {            buffers.add(input.map(FileChannel.MapMode.READ_ONLY, 0, Math.min(Integer.MAX_VALUE, f.length() - offset)));            data.add(buffers.get(buffer).asIntBuffer());        }        rowOffset[i] = offset / 4;        rowSize[i] = size;        bufferIndex[i] = buffer;                        offset += size * 4;    }}
0
public static void writeMatrix(File f, Matrix m) throws IOException
{    Preconditions.checkArgument(f.canWrite(), "Can't write to output file");    FileOutputStream fos = new FileOutputStream(f);        DataOutputStream out = new DataOutputStream(fos);    out.writeInt(MAGIC_NUMBER_V0);    out.writeInt(m.rowSize());    out.writeInt(m.columnSize());        for (MatrixSlice row : m) {        int nondefaultElements = row.vector().getNumNondefaultElements();        out.writeInt(nondefaultElements);    }        for (MatrixSlice row : m) {        List<Integer> columns = Lists.newArrayList(Iterables.transform(row.vector().nonZeroes(), new Function<Vector.Element, Integer>() {            @Override            public Integer apply(Vector.Element element) {                return element.index();            }        }));        Collections.sort(columns);        for (Integer column : columns) {            out.writeInt(column);        }    }    out.close();    fos.close();}
0
public Integer apply(Vector.Element element)
{    return element.index();}
0
public Matrix assignColumn(int column, Vector other)
{    throw new UnsupportedOperationException("Default operation");}
0
public Matrix assignRow(int row, Vector other)
{    throw new UnsupportedOperationException("Default operation");}
0
public double getQuick(int rowIndex, int columnIndex)
{    IntBuffer tmp = data.get(bufferIndex[rowIndex]).asReadOnlyBuffer();    tmp.position(rowOffset[rowIndex]);    tmp.limit(rowSize[rowIndex]);    tmp = tmp.slice();    return searchForIndex(tmp, columnIndex);}
0
private static double searchForIndex(IntBuffer row, int columnIndex)
{    int high = row.limit();    if (high == 0) {        return 0;    }    int low = 0;    while (high > low) {        int mid = (low + high) / 2;        if (row.get(mid) < columnIndex) {            low = mid + 1;        } else {            high = mid;        }    }    if (low >= row.limit()) {        return 0;    } else if (high == low && row.get(low) == columnIndex) {        return 1;    } else {        return 0;    }}
0
public Matrix like()
{    throw new UnsupportedOperationException("Default operation");}
0
public Matrix like(int rows, int columns)
{    return new DenseMatrix(rows, columns);}
0
public void setQuick(int row, int column, double value)
{    throw new UnsupportedOperationException("Default operation");}
0
public Matrix viewPart(int[] offset, int[] size)
{    throw new UnsupportedOperationException("Default operation");}
0
public Vector viewRow(int rowIndex)
{    IntBuffer tmp = data.get(bufferIndex[rowIndex]).asReadOnlyBuffer();    tmp.position(rowOffset[rowIndex]);    tmp.limit(rowOffset[rowIndex] + rowSize[rowIndex]);    tmp = tmp.slice();    return new SparseBinaryVector(tmp, columnSize());}
0
protected Matrix matrixLike(int rows, int columns)
{    throw new UnsupportedOperationException("Default operation");}
0
public void mergeUpdates(OrderedIntDoubleMapping updates)
{    throw new UnsupportedOperationException("Cannot mutate SparseBinaryVector");}
0
public boolean isDense()
{    return false;}
0
public boolean isSequentialAccess()
{    return true;}
0
public Iterator<Element> iterator()
{    return new AbstractIterator<Element>() {        int i = 0;        @Override        protected Element computeNext() {            if (i < maxIndex) {                return new Element() {                    int index = i++;                    /**                     * @return the value of this vector element.                     */                    @Override                    public double get() {                        return getQuick(index);                    }                    /**                     * @return the index of this vector element.                     */                    @Override                    public int index() {                        return index;                    }                    /**                     * @param value Set the current element to value.                     */                    @Override                    public void set(double value) {                        throw new UnsupportedOperationException("Default operation");                    }                };            } else {                return endOfData();            }        }    };}
0
protected Element computeNext()
{    if (i < maxIndex) {        return new Element() {            int index = i++;            /**             * @return the value of this vector element.             */            @Override            public double get() {                return getQuick(index);            }            /**             * @return the index of this vector element.             */            @Override            public int index() {                return index;            }            /**             * @param value Set the current element to value.             */            @Override            public void set(double value) {                throw new UnsupportedOperationException("Default operation");            }        };    } else {        return endOfData();    }}
0
public double get()
{    return getQuick(index);}
0
public int index()
{    return index;}
0
public void set(double value)
{    throw new UnsupportedOperationException("Default operation");}
0
public Iterator<Element> iterateNonZero()
{    return new AbstractIterator<Element>() {        int i = 0;        @Override        protected Element computeNext() {            if (i < buffer.limit()) {                return new BinaryReadOnlyElement(buffer.get(i++));            } else {                return endOfData();            }        }    };}
0
protected Element computeNext()
{    if (i < buffer.limit()) {        return new BinaryReadOnlyElement(buffer.get(i++));    } else {        return endOfData();    }}
0
public double getQuick(int index)
{    return searchForIndex(buffer, index);}
0
public Vector like()
{    return new RandomAccessSparseVector(size());}
0
public Vector like(int cardinality)
{    return new RandomAccessSparseVector(cardinality);}
0
protected Vector createOptimizedCopy()
{    return new RandomAccessSparseVector(size()).assign(this);}
0
public void setQuick(int index, double value)
{    throw new UnsupportedOperationException("Read-only view");}
0
public void incrementQuick(int index, double increment)
{    throw new UnsupportedOperationException("Read-only view");}
0
public int getNumNondefaultElements()
{    return buffer.limit();}
0
public double getLookupCost()
{    return 1;}
0
public double getIteratorAdvanceCost()
{    return 1;}
0
public boolean isAddConstantTime()
{    throw new UnsupportedOperationException("Can't add binary value");}
0
public double get()
{    return 1;}
0
public int index()
{    return index;}
0
public void set(double value)
{    throw new UnsupportedOperationException("Can't set binary value");}
0
public BackEnum getBacking()
{    return pBacking;}
0
public TraversingStructureEnum getStructure()
{    return pStructure;}
0
public boolean isDense()
{    return pDense;}
0
public boolean isLikeRightPlus()
{    return false;}
0
public boolean isLikeLeftMult()
{    return false;}
0
public boolean isLikeRightMult()
{    return false;}
0
public boolean isLikeMult()
{    return isLikeLeftMult() && isLikeRightMult();}
0
public boolean isCommutative()
{    return false;}
0
public boolean isAssociative()
{    return false;}
0
public boolean isAssociativeAndCommutative()
{    return isAssociative() && isCommutative();}
0
public boolean isDensifying()
{    return apply(0.0, 0.0) != 0.0;}
0
public boolean isDensifying()
{    return Math.abs(apply(0.0)) != 0.0;}
0
public double apply(double a)
{    return Math.abs(a);}
0
public double apply(double a)
{    return Math.acos(a);}
0
public double apply(double a)
{    return Math.asin(a);}
0
public double apply(double a)
{    return Math.atan(a);}
0
public double apply(double a)
{    return Math.ceil(a);}
0
public double apply(double a)
{    return Math.cos(a);}
0
public double apply(double a)
{    return Math.exp(a);}
0
public double apply(double a)
{    return Math.floor(a);}
0
public double apply(double a)
{    return a;}
0
public double apply(double a)
{    return 1.0 / a;}
0
public double apply(double a)
{    return Math.log(a);}
0
public double apply(double a)
{    return Math.log(a) * 1.4426950408889634;}
0
public double apply(double a)
{    return -a;}
0
public double apply(double a)
{    return Math.rint(a);}
0
public double apply(double a)
{    return a < 0 ? -1 : a > 0 ? 1 : 0;}
0
public double apply(double a)
{    return Math.sin(a);}
0
public double apply(double a)
{    return Math.sqrt(a);}
0
public double apply(double a)
{    return a * a;}
0
public double apply(double a)
{    return 1.0 / (1.0 + Math.exp(-a));}
0
public double apply(double a)
{    return a * (1.0 - a);}
0
public double apply(double a)
{    return Math.tan(a);}
0
public double apply(double a, double b)
{    return Math.atan2(a, b);}
0
public double apply(double a, double b)
{    return a < b ? -1 : a > b ? 1 : 0;}
0
public double apply(double a, double b)
{    return a / b;}
0
public boolean isLikeRightPlus()
{    return false;}
0
public boolean isLikeLeftMult()
{    return false;}
0
public boolean isLikeRightMult()
{    return false;}
0
public boolean isCommutative()
{    return false;}
0
public boolean isAssociative()
{    return false;}
0
public double apply(double a, double b)
{    return a == b ? 1 : 0;}
0
public boolean isCommutative()
{    return true;}
0
public double apply(double a, double b)
{    return a > b ? 1 : 0;}
0
public double apply(double a, double b)
{    return Math.IEEEremainder(a, b);}
0
public boolean apply(double a, double b)
{    return a == b;}
0
public boolean apply(double a, double b)
{    return a < b;}
0
public boolean apply(double a, double b)
{    return a > b;}
0
public double apply(double a, double b)
{    return a < b ? 1 : 0;}
0
public double apply(double a, double b)
{    return Math.log(a) / Math.log(b);}
0
public double apply(double a, double b)
{    return Math.max(a, b);}
0
public boolean isLikeRightPlus()
{    return false;}
0
public boolean isLikeLeftMult()
{    return false;}
0
public boolean isLikeRightMult()
{    return false;}
0
public boolean isAssociative()
{    return true;}
0
public boolean isCommutative()
{    return true;}
0
public double apply(double a, double b)
{    return Math.max(Math.abs(a), Math.abs(b));}
0
public boolean isLikeRightPlus()
{    return true;}
0
public boolean isLikeLeftMult()
{    return false;}
0
public boolean isLikeRightMult()
{    return false;}
0
public boolean isAssociative()
{    return true;}
0
public boolean isCommutative()
{    return true;}
0
public double apply(double a, double b)
{    return Math.min(a, b);}
0
public boolean isLikeRightPlus()
{    return false;}
0
public boolean isLikeLeftMult()
{    return false;}
0
public boolean isLikeRightMult()
{    return false;}
0
public boolean isAssociative()
{    return true;}
0
public boolean isCommutative()
{    return true;}
0
public double apply(double x, double y)
{    return (x - y) * (x - y);}
0
public boolean isLikeRightPlus()
{    return false;}
0
public boolean isLikeLeftMult()
{    return false;}
0
public boolean isLikeRightMult()
{    return false;}
0
public boolean isCommutative()
{    return true;}
0
public boolean isAssociative()
{    return false;}
0
public double apply(double a, double b)
{    return a % b;}
0
public double apply(double a, double b)
{    return Math.abs(a) + Math.abs(b);}
0
public boolean isLikeRightPlus()
{    return false;}
0
public boolean isLikeLeftMult()
{    return false;}
0
public boolean isLikeRightMult()
{    return false;}
0
public boolean isAssociative()
{    return true;}
0
public boolean isCommutative()
{    return true;}
0
public double apply(double x, double y)
{    return Math.abs(x - y);}
0
public boolean isLikeRightPlus()
{    return false;}
0
public boolean isLikeLeftMult()
{    return false;}
0
public boolean isLikeRightMult()
{    return false;}
0
public boolean isCommutative()
{    return true;}
0
public boolean isAssociative()
{    return false;}
0
public double apply(double a, double b)
{    return Math.pow(a, b);}
0
public boolean isLikeRightPlus()
{    return false;}
0
public boolean isLikeLeftMult()
{    return false;}
0
public boolean isLikeRightMult()
{    return false;}
0
public boolean isCommutative()
{    return false;}
0
public boolean isAssociative()
{    return false;}
0
public double apply(double x, double y)
{    return y;}
0
public boolean isLikeRightPlus()
{    return false;}
0
public boolean isLikeLeftMult()
{    return false;}
0
public boolean isLikeRightMult()
{    return true;}
0
public boolean isCommutative()
{    return false;}
0
public boolean isAssociative()
{    return true;}
0
public double apply(double x, double y)
{    Preconditions.checkArgument(x == 0, "This special version of SECOND needs x == 0");    return y;}
0
public boolean isLikeRightPlus()
{    return true;}
0
public boolean isLikeLeftMult()
{    return false;}
0
public boolean isLikeRightMult()
{    return true;}
0
public boolean isCommutative()
{    return false;}
0
public boolean isAssociative()
{    return true;}
0
public double apply(double x, double y)
{    return x * x * y;}
0
public boolean isLikeRightPlus()
{    return false;}
0
public boolean isLikeLeftMult()
{    return true;}
0
public boolean isLikeRightMult()
{    return true;}
0
public boolean isCommutative()
{    return false;}
0
public boolean isAssociative()
{    return false;}
0
public double apply(double x, double y)
{    return x * (y + 1);}
0
public boolean isLikeRightPlus()
{    return true;}
0
public boolean isLikeLeftMult()
{    return true;}
0
public boolean isLikeRightMult()
{    return false;}
0
public boolean isCommutative()
{    return false;}
0
public boolean isAssociative()
{    return false;}
0
public static DoubleDoubleFunction reweigh(final double wx, final double wy)
{    final double tw = wx + wy;    return new DoubleDoubleFunction() {        @Override        public double apply(double x, double y) {            return (wx * x + wy * y) / tw;        }        /**         * f(x, 0) = wx * x / tw = x iff wx = tw (practically, impossible, as tw = wx + wy and wy > 0)         * @return true iff f(x, 0) = x for any x         */        @Override        public boolean isLikeRightPlus() {            return wx == tw;        }        /**         * f(0, y) = wy * y / tw = 0 iff y = 0         * @return true iff f(0, y) = 0 for any y         */        @Override        public boolean isLikeLeftMult() {            return false;        }        /**         * f(x, 0) = wx * x / tw = 0 iff x = 0         * @return true iff f(x, 0) = 0 for any x         */        @Override        public boolean isLikeRightMult() {            return false;        }        /**         * wx * x + wy * y = wx * y + wy * x iff wx = wy         * @return true iff f(x, y) = f(y, x) for any x, y         */        @Override        public boolean isCommutative() {            return wx == wy;        }        /**         * @return true iff f(x, f(y, z)) = f(f(x, y), z) for any x, y, z         */        @Override        public boolean isAssociative() {            return false;        }    };}
0
public double apply(double x, double y)
{    return (wx * x + wy * y) / tw;}
0
public boolean isLikeRightPlus()
{    return wx == tw;}
0
public boolean isLikeLeftMult()
{    return false;}
0
public boolean isLikeRightMult()
{    return false;}
0
public boolean isCommutative()
{    return wx == wy;}
0
public boolean isAssociative()
{    return false;}
0
public static DoubleFunction between(final double from, final double to)
{    return new DoubleFunction() {        @Override        public double apply(double a) {            return from <= a && a <= to ? 1 : 0;        }    };}
0
public double apply(double a)
{    return from <= a && a <= to ? 1 : 0;}
0
public static DoubleFunction bindArg1(final DoubleDoubleFunction function, final double c)
{    return new DoubleFunction() {        @Override        public double apply(double var) {            return function.apply(c, var);        }    };}
0
public double apply(double var)
{    return function.apply(c, var);}
0
public static DoubleFunction bindArg2(final DoubleDoubleFunction function, final double c)
{    return new DoubleFunction() {        @Override        public double apply(double var) {            return function.apply(var, c);        }    };}
0
public double apply(double var)
{    return function.apply(var, c);}
0
public static DoubleDoubleFunction chain(final DoubleDoubleFunction f, final DoubleFunction g, final DoubleFunction h)
{    return new DoubleDoubleFunction() {        @Override        public double apply(double a, double b) {            return f.apply(g.apply(a), h.apply(b));        }        /**         * fx(c, 0) = f(g(x), h(0)) = f(g(x), 0) = g(x) = x if h(0) = 0 and f isLikeRightPlus and g(x) = x         * Impossible to check whether g(x) = x for any x, so we return false.         * @return true iff f(x, 0) = x for any x         */        @Override        public boolean isLikeRightPlus() {            return false;        }        /**         * fc(0, y) = f(g(0), h(y)) = f(0, h(y)) = 0 if g(0) = 0 and f isLikeLeftMult         * @return true iff f(0, y) = 0 for any y         */        @Override        public boolean isLikeLeftMult() {            return g.apply(0) == 0 && f.isLikeLeftMult();        }        /**         * fc(x, 0) = f(g(x), h(0)) = f(g(x), 0) = 0 if h(0) = 0 and f isLikeRightMult         * @return true iff f(x, 0) = 0 for any x         */        @Override        public boolean isLikeRightMult() {            return h.apply(0) == 0 && f.isLikeRightMult();        }        /**         * fc(x, y) = f(g(x), h(y)) = f(h(y), g(x))         * fc(y, x) = f(g(y), h(x)) = f(h(x), g(y))         * Either g(x) = g(y) for any x, y and h(x) = h(y) for any x, y or g = h and f isCommutative.         * Can only check if g = h (reference equality, assuming they're both the same static function in         * this file) and f isCommutative. There are however other scenarios when this might happen that are NOT         * covered by this definition.         * @return true iff f(x, y) = f(y, x) for any x, y         */        @Override        public boolean isCommutative() {            return g.equals(h) && f.isCommutative();        }        /**         * fc(x, fc(y, z)) = f(g(x), h(f(g(y), h(z))))         * fc(fc(x, y), z) = f(g(f(g(x), h(y))), h(z))         * Impossible to check.         * @return true iff f(x, f(y, z)) = f(f(x, y), z) for any x, y, z         */        @Override        public boolean isAssociative() {            return false;        }    };}
0
public double apply(double a, double b)
{    return f.apply(g.apply(a), h.apply(b));}
0
public boolean isLikeRightPlus()
{    return false;}
0
public boolean isLikeLeftMult()
{    return g.apply(0) == 0 && f.isLikeLeftMult();}
0
public boolean isLikeRightMult()
{    return h.apply(0) == 0 && f.isLikeRightMult();}
0
public boolean isCommutative()
{    return g.equals(h) && f.isCommutative();}
0
public boolean isAssociative()
{    return false;}
0
public static DoubleDoubleFunction chain(final DoubleFunction g, final DoubleDoubleFunction h)
{    return new DoubleDoubleFunction() {        @Override        public double apply(double a, double b) {            return g.apply(h.apply(a, b));        }        /**         * g(h(x, 0)) = g(x) = x for any x iff g(x) = x and h isLikeRightPlus         * Impossible to check.         * @return true iff f(x, 0) = x for any x         */        @Override        public boolean isLikeRightPlus() {            return false;        }        /**         * g(h(0, y)) = g(0) = 0 for any y iff g(0) = 0 and h isLikeLeftMult         * @return true iff f(0, y) = 0 for any y         */        @Override        public boolean isLikeLeftMult() {            return !g.isDensifying() && h.isLikeLeftMult();        }        /**         * g(h(x, 0)) = g(0) = 0 for any x iff g(0) = 0 and h isLikeRightMult         * @return true iff f(x, 0) = 0 for any x         */        @Override        public boolean isLikeRightMult() {            return !g.isDensifying() && h.isLikeRightMult();        }        /**         * fc(x, y) = g(h(x, y)) = g(h(y, x)) = fc(y, x) iff h isCommutative         * @return true iff f(x, y) = f(y, x) for any x, y         */        @Override        public boolean isCommutative() {            return h.isCommutative();        }        /**         * fc(x, fc(y, z)) = g(h(x, g(h(y, z)))         * fc(fc(x, y), z) = g(h(g(h(x, y)), z))         * Impossible to check.         * @return true iff f(x, f(y, z)) = f(f(x, y), z) for any x, y, z         */        @Override        public boolean isAssociative() {            return false;        }    };}
0
public double apply(double a, double b)
{    return g.apply(h.apply(a, b));}
0
public boolean isLikeRightPlus()
{    return false;}
0
public boolean isLikeLeftMult()
{    return !g.isDensifying() && h.isLikeLeftMult();}
0
public boolean isLikeRightMult()
{    return !g.isDensifying() && h.isLikeRightMult();}
0
public boolean isCommutative()
{    return h.isCommutative();}
0
public boolean isAssociative()
{    return false;}
0
public static DoubleFunction chain(final DoubleFunction g, final DoubleFunction h)
{    return new DoubleFunction() {        @Override        public double apply(double a) {            return g.apply(h.apply(a));        }    };}
0
public double apply(double a)
{    return g.apply(h.apply(a));}
0
public static IntIntFunction chain(final DoubleFunction g, final IntIntFunction h)
{    return new IntIntFunction() {        @Override        public double apply(int first, int second) {            return g.apply(h.apply(first, second));        }    };}
0
public double apply(int first, int second)
{    return g.apply(h.apply(first, second));}
0
public static DoubleFunction compare(final double b)
{    return new DoubleFunction() {        @Override        public double apply(double a) {            return a < b ? -1 : a > b ? 1 : 0;        }    };}
0
public double apply(double a)
{    return a < b ? -1 : a > b ? 1 : 0;}
0
public static DoubleFunction constant(final double c)
{    return new DoubleFunction() {        @Override        public double apply(double a) {            return c;        }    };}
0
public double apply(double a)
{    return c;}
0
public static DoubleFunction div(double b)
{    return mult(1 / b);}
0
public static DoubleFunction equals(final double b)
{    return new DoubleFunction() {        @Override        public double apply(double a) {            return a == b ? 1 : 0;        }    };}
0
public double apply(double a)
{    return a == b ? 1 : 0;}
0
public static DoubleFunction notEqual(final double b)
{    return new DoubleFunction() {        @Override        public double apply(double a) {            return a != b ? 1 : 0;        }    };}
0
public double apply(double a)
{    return a != b ? 1 : 0;}
0
public static DoubleFunction greater(final double b)
{    return new DoubleFunction() {        @Override        public double apply(double a) {            return a > b ? 1 : 0;        }    };}
0
public double apply(double a)
{    return a > b ? 1 : 0;}
0
public static DoubleFunction mathIEEEremainder(final double b)
{    return new DoubleFunction() {        @Override        public double apply(double a) {            return Math.IEEEremainder(a, b);        }    };}
0
public double apply(double a)
{    return Math.IEEEremainder(a, b);}
0
public static DoubleProcedure isBetween(final double from, final double to)
{    return new DoubleProcedure() {        @Override        public boolean apply(double a) {            return from <= a && a <= to;        }    };}
0
public boolean apply(double a)
{    return from <= a && a <= to;}
0
public static DoubleProcedure isEqual(final double b)
{    return new DoubleProcedure() {        @Override        public boolean apply(double a) {            return a == b;        }    };}
0
public boolean apply(double a)
{    return a == b;}
0
public static DoubleProcedure isGreater(final double b)
{    return new DoubleProcedure() {        @Override        public boolean apply(double a) {            return a > b;        }    };}
0
public boolean apply(double a)
{    return a > b;}
0
public static DoubleProcedure isLess(final double b)
{    return new DoubleProcedure() {        @Override        public boolean apply(double a) {            return a < b;        }    };}
0
public boolean apply(double a)
{    return a < b;}
0
public static DoubleFunction less(final double b)
{    return new DoubleFunction() {        @Override        public double apply(double a) {            return a < b ? 1 : 0;        }    };}
0
public double apply(double a)
{    return a < b ? 1 : 0;}
0
public static DoubleFunction lg(final double b)
{    return new DoubleFunction() {                private final double logInv = 1 / Math.log(b);        @Override        public double apply(double a) {            return Math.log(a) * logInv;        }    };}
0
public double apply(double a)
{    return Math.log(a) * logInv;}
0
public static DoubleFunction max(final double b)
{    return new DoubleFunction() {        @Override        public double apply(double a) {            return Math.max(a, b);        }    };}
0
public double apply(double a)
{    return Math.max(a, b);}
0
public static DoubleFunction min(final double b)
{    return new DoubleFunction() {        @Override        public double apply(double a) {            return Math.min(a, b);        }    };}
0
public double apply(double a)
{    return Math.min(a, b);}
0
public static DoubleFunction minus(double b)
{    return plus(-b);}
0
public static DoubleDoubleFunction minusMult(double constant)
{    return plusMult(-constant);}
0
public static DoubleFunction mod(final double b)
{    return new DoubleFunction() {        @Override        public double apply(double a) {            return a % b;        }    };}
0
public double apply(double a)
{    return a % b;}
0
public static DoubleFunction mult(double b)
{    return new Mult(b);/*    return new DoubleFunction() {      public final double apply(double a) { return a * b; }    };    */}
0
public static DoubleFunction plus(final double b)
{    return new DoubleFunction() {        @Override        public double apply(double a) {            return a + b;        }    };}
0
public double apply(double a)
{    return a + b;}
0
public static DoubleDoubleFunction plusMult(double constant)
{    return new PlusMult(constant);}
0
public static DoubleFunction pow(final double b)
{    return new DoubleFunction() {        @Override        public double apply(double a) {            if (b == 2) {                return a * a;            } else {                return Math.pow(a, b);            }        }    };}
0
public double apply(double a)
{    if (b == 2) {        return a * a;    } else {        return Math.pow(a, b);    }}
0
public static DoubleFunction random()
{    return new MersenneTwister(new Date());}
0
public static DoubleFunction round(final double precision)
{    return new DoubleFunction() {        @Override        public double apply(double a) {            return Math.rint(a / precision) * precision;        }    };}
0
public double apply(double a)
{    return Math.rint(a / precision) * precision;}
0
public static DoubleDoubleFunction swapArgs(final DoubleDoubleFunction function)
{    return new DoubleDoubleFunction() {        @Override        public double apply(double a, double b) {            return function.apply(b, a);        }    };}
0
public double apply(double a, double b)
{    return function.apply(b, a);}
0
public static DoubleDoubleFunction minusAbsPow(final double exponent)
{    return new DoubleDoubleFunction() {        @Override        public double apply(double x, double y) {            return Math.pow(Math.abs(x - y), exponent);        }        /**         * |x - 0|^p = |x|^p != x unless x > 0 and p = 1         * @return true iff f(x, 0) = x for any x         */        @Override        public boolean isLikeRightPlus() {            return false;        }        /**         * |0 - y|^p = |y|^p         * @return true iff f(0, y) = 0 for any y         */        @Override        public boolean isLikeLeftMult() {            return false;        }        /**         * |x - 0|^p = |x|^p         * @return true iff f(x, 0) = 0 for any x         */        @Override        public boolean isLikeRightMult() {            return false;        }        /**         * |x - y|^p = |y - x|^p         * @return true iff f(x, y) = f(y, x) for any x, y         */        @Override        public boolean isCommutative() {            return true;        }        /**         * |x - |y - z|^p|^p != ||x - y|^p - z|^p         * @return true iff f(x, f(y, z)) = f(f(x, y), z) for any x, y, z         */        @Override        public boolean isAssociative() {            return false;        }    };}
0
public double apply(double x, double y)
{    return Math.pow(Math.abs(x - y), exponent);}
0
public boolean isLikeRightPlus()
{    return false;}
0
public boolean isLikeLeftMult()
{    return false;}
0
public boolean isLikeRightMult()
{    return false;}
0
public boolean isCommutative()
{    return true;}
0
public boolean isAssociative()
{    return false;}
0
public double apply(double a)
{    return a * multiplicator;}
0
public static Mult div(double constant)
{    return mult(1 / constant);}
0
public static Mult mult(double constant)
{    return new Mult(constant);}
0
public double getMultiplicator()
{    return multiplicator;}
0
public void setMultiplicator(double multiplicator)
{    this.multiplicator = multiplicator;}
0
public double apply(double a, double b)
{    return a + b * multiplicator;}
0
public static PlusMult minusMult(double constant)
{    return new PlusMult(-constant);}
0
public static PlusMult plusMult(double constant)
{    return new PlusMult(constant);}
0
public double getMultiplicator()
{    return multiplicator;}
0
public boolean isLikeRightPlus()
{    return true;}
0
public boolean isLikeLeftMult()
{    return false;}
0
public boolean isLikeRightMult()
{    return false;}
0
public boolean isCommutative()
{    return Math.abs(multiplicator - 1.0) < Constants.EPSILON;}
0
public boolean isAssociative()
{    return Math.abs(multiplicator - 0.0) < Constants.EPSILON || Math.abs(multiplicator - 1.0) < Constants.EPSILON;}
0
public void setMultiplicator(double multiplicator)
{    this.multiplicator = multiplicator;}
0
public double apply(double arg1)
{    return Math.sqrt(arg1);}
0
public double apply(double x, double y)
{    return x * y;}
0
public boolean isLikeRightPlus()
{    return false;}
0
public boolean isLikeLeftMult()
{    return true;}
0
public boolean isLikeRightMult()
{    return true;}
0
public boolean isCommutative()
{    return true;}
0
public boolean isAssociative()
{    return true;}
0
public Matrix assignColumn(int column, Vector other)
{    throw new UnsupportedOperationException("Assignment to a matrix not supported");}
0
public Matrix assignRow(int row, Vector other)
{    throw new UnsupportedOperationException("Assignment to a matrix view not supported");}
0
public double getQuick(int row, int column)
{    return gf.apply(row, column);}
0
public Matrix like()
{    return like(rows, columns);}
0
public Matrix like(int rows, int columns)
{    if (denseLike)        return new DenseMatrix(rows, columns);    else        return new SparseMatrix(rows, columns);}
0
public void setQuick(int row, int column, double value)
{    throw new UnsupportedOperationException("Assignment to a matrix view not supported");}
0
public Vector viewRow(int row)
{    return new MatrixVectorView(this, row, 0, 0, 1, denseLike);}
0
public Vector viewColumn(int column)
{    return new MatrixVectorView(this, 0, column, 1, 0, denseLike);}
0
public MatrixFlavor getFlavor()
{    return flavor;}
0
public static double binomial(long n, long k)
{    if (k < 0) {        return 0;    }    if (k == 0 || k == n) {        return 1;    }    if (k == 1 || k == n - 1) {        return n;    }        if (n > k) {        int max = FACTORIAL_TABLE.length + LARGE_FACTORIAL_TABLE.length;        if (n < max) {                        double nFactorial = factorial((int) n);            double kFactorial = factorial((int) k);            double nMinusKFactorial = factorial((int) (n - k));            double nk = nMinusKFactorial * kFactorial;            if (nk != Double.POSITIVE_INFINITY) {                                return nFactorial / nk;            }        }        if (k > n / 2) {            k = n - k;        }        }        long a = n - k + 1;    long b = 1;    double binomial = 1;    for (long i = k; i-- > 0; ) {        binomial *= (double) a++ / b++;    }    return binomial;}
0
private static double factorial(int k)
{    if (k < 0) {        throw new IllegalArgumentException();    }    int length1 = FACTORIAL_TABLE.length;    if (k < length1) {        return FACTORIAL_TABLE[k];    }    int length2 = LARGE_FACTORIAL_TABLE.length;    if (k < length1 + length2) {        return LARGE_FACTORIAL_TABLE[k - length1];    } else {        return Double.POSITIVE_INFINITY;    }}
0
public static double logFactorial(int k)
{    if (k >= 30) {        double r = 1.0 / k;        double rr = r * r;        double c7 = -5.95238095238095238e-04;        double c5 = 7.93650793650793651e-04;        double c3 = -2.77777777777777778e-03;        double c1 = 8.33333333333333333e-02;        double c0 = 9.18938533204672742e-01;        return (k + 0.5) * Math.log(k) - k + c0 + r * (c1 + rr * (c3 + rr * (c5 + rr * c7)));    } else {        return LOG_FACTORIAL_TABLE[k];    }}
0
public static double p1evl(double x, double[] coef, int N)
{    double ans = x + coef[0];    for (int i = 1; i < N; i++) {        ans = ans * x + coef[i];    }    return ans;}
0
public static double polevl(double x, double[] coef, int N)
{    double ans = coef[0];    for (int i = 1; i <= N; i++) {        ans = ans * x + coef[i];    }    return ans;}
0
public double cdf(double x)
{    throw new UnsupportedOperationException("Can't compute pdf for " + this.getClass().getName());}
0
public double pdf(double x)
{    throw new UnsupportedOperationException("Can't compute pdf for " + this.getClass().getName());}
0
public int nextInt()
{    return (int) Math.round(nextDouble());}
0
public double nextDouble()
{    return nextInt();}
0
protected Random getRandomGenerator()
{    return randomGenerator;}
0
protected double randomDouble()
{    return randomGenerator.nextDouble();}
0
public double apply(double dummy)
{    return nextDouble();}
0
public int apply(int dummy)
{    return nextInt();}
0
public void setRandomGenerator(Random randomGenerator)
{    this.randomGenerator = randomGenerator;}
0
 void nextBlock()
{    int y;    int kk;    for (kk = 0; kk < N - M; kk++) {        y = (mt[kk] & UPPER_MASK) | (mt[kk + 1] & LOWER_MASK);        mt[kk] = mt[kk + M] ^ (y >>> 1) ^ ((y & 0x1) == 0 ? MAG0 : MAG1);    }    for (; kk < N - 1; kk++) {        y = (mt[kk] & UPPER_MASK) | (mt[kk + 1] & LOWER_MASK);        mt[kk] = mt[kk + (M - N)] ^ (y >>> 1) ^ ((y & 0x1) == 0 ? MAG0 : MAG1);    }    y = (mt[N - 1] & UPPER_MASK) | (mt[0] & LOWER_MASK);    mt[N - 1] = mt[M - 1] ^ (y >>> 1) ^ ((y & 0x1) == 0 ? MAG0 : MAG1);    this.mti = 0;}
0
public int nextInt()
{    /* Each single bit including the sign bit will be random */    if (mti == N) {        nextBlock();    }        int y = mt[mti++];        y ^= y >>> 11;        y ^= (y << 7) & TEMPERING_MASK_B;        y ^= (y << 15) & TEMPERING_MASK_C;            y ^= y >>> 18;    return y;}
0
 void setSeed(int seed)
{    mt[0] = seed;    for (int i = 1; i < N; i++) {        mt[i] = 1812433253 * (mt[i - 1] ^ (mt[i - 1] >> 30)) + i;    /* See Knuth TAOCP Vol2. 3rd Ed. P.106 for multiplier. */    /* In the previous versions, MSBs of the seed affect   */    /* only MSBs of the array mt[].                        */    /* 2002/01/09 modified by Makoto Matsumoto             */        /* for >32 bit machines */    }        mti = N;}
0
 void setReferenceSeed(int seed)
{    for (int i = 0; i < N; i++) {        mt[i] = seed & 0xffff0000;        seed = 69069 * seed + 1;        mt[i] |= (seed & 0xffff0000) >>> 16;        seed = 69069 * seed + 1;    }        mti = N;}
0
public double apply(double dummy)
{    return raw();}
0
public int apply(int dummy)
{    return nextInt();}
0
public double nextDouble()
{    double nextDouble;    do {                        nextDouble = (nextLong() - -9.223372036854776E18) * 5.421010862427522E-20;    } while (    !(nextDouble > 0.0 && nextDouble < 1.0));        return nextDouble;/*      nextLong == Long.MAX_VALUE         --> 1.0      nextLong == Long.MIN_VALUE         --> 0.0      nextLong == Long.MAX_VALUE-1       --> 1.0      nextLong == Long.MAX_VALUE-100000L --> 0.9999999999999946      nextLong == Long.MIN_VALUE+1       --> 0.0      nextLong == Long.MIN_VALUE-100000L --> 0.9999999999999946      nextLong == 1L                     --> 0.5      nextLong == -1L                    --> 0.5      nextLong == 2L                     --> 0.5      nextLong == -2L                    --> 0.5      nextLong == 2L+100000L             --> 0.5000000000000054      nextLong == -2L-100000L            --> 0.49999999999999456    */}
0
public float nextFloat()
{        float nextFloat;    do {        nextFloat = (float) raw();    } while (nextFloat >= 1.0f);        return nextFloat;}
0
public long nextLong()
{        return ((nextInt() & 0xFFFFFFFFL) << 32) | (nextInt() & 0xFFFFFFFFL);}
0
public double raw()
{    int nextInt;    do {                        nextInt = nextInt();    } while (nextInt == 0);        return (nextInt & 0xFFFFFFFFL) * 2.3283064365386963E-10;/*      nextInt == Integer.MAX_VALUE   --> 0.49999999976716936      nextInt == Integer.MIN_VALUE   --> 0.5      nextInt == Integer.MAX_VALUE-1 --> 0.4999999995343387      nextInt == Integer.MIN_VALUE+1 --> 0.5000000002328306      nextInt == 1                   --> 2.3283064365386963E-10      nextInt == -1                  --> 0.9999999997671694      nextInt == 2                   --> 4.6566128730773926E-10      nextInt == -2                  --> 0.9999999995343387    */}
0
public double cdf(double x)
{    if (x <= 0.0) {        return 0.0;    }    return 1.0 - Math.exp(-x * lambda);}
0
public double nextDouble()
{    return -Math.log1p(-randomDouble()) / lambda;}
0
public double pdf(double x)
{    if (x < 0.0) {        return 0.0;    }    return lambda * Math.exp(-x * lambda);}
0
public void setState(double lambda)
{    this.lambda = lambda;}
0
public String toString()
{    return String.format(Locale.ENGLISH, "%s(%.4f)", this.getClass().getName(), lambda);}
0
public double cdf(double x)
{    return Probability.gamma(alpha, rate, x);}
0
public double nextDouble()
{    return nextDouble(alpha, rate);}
0
public double nextDouble(double alpha, double rate)
{    if (alpha <= 0.0) {        throw new IllegalArgumentException();    }    if (rate <= 0.0) {        throw new IllegalArgumentException();    }    double gds;    double b = 0.0;    if (alpha < 1.0) {                        b = 1.0 + 0.36788794412 * alpha;        while (true) {            double p = b * randomDouble();            if (p <= 1.0) {                                gds = Math.exp(Math.log(p) / alpha);                if (Math.log(randomDouble()) <= -gds) {                    return gds / rate;                }            } else {                                gds = -Math.log((b - p) / alpha);                if (Math.log(randomDouble()) <= (alpha - 1.0) * Math.log(gds)) {                    return gds / rate;                }            }        }    } else {                double ss = 0.0;        double s = 0.0;        double d = 0.0;        if (alpha != -1.0) {                        ss = alpha - 0.5;            s = Math.sqrt(ss);            d = 5.656854249 - 12.0 * s;        }                double v12;        double v1;        do {            v1 = 2.0 * randomDouble() - 1.0;            double v2 = 2.0 * randomDouble() - 1.0;            v12 = v1 * v1 + v2 * v2;        } while (v12 > 1.0);        double t = v1 * Math.sqrt(-2.0 * Math.log(v12) / v12);        double x = s + 0.5 * t;        gds = x * x;        if (t >= 0.0) {            return gds / rate;        }                double u = randomDouble();        if (d * u <= t * t * t) {            return gds / rate;        }                double q0 = 0.0;        double si = 0.0;        double c = 0.0;        if (alpha != -1.0) {                        double r = 1.0 / alpha;            double q9 = 0.0001710320;            double q8 = -0.0004701849;            double q7 = 0.0006053049;            double q6 = 0.0003340332;            double q5 = -0.0003349403;            double q4 = 0.0015746717;            double q3 = 0.0079849875;            double q2 = 0.0208333723;            double q1 = 0.0416666664;            q0 = ((((((((q9 * r + q8) * r + q7) * r + q6) * r + q5) * r + q4) * r + q3) * r + q2) * r + q1) * r;            if (alpha > 3.686) {                if (alpha > 13.022) {                    b = 1.77;                    si = 0.75;                    c = 0.1515 / s;                } else {                    b = 1.654 + 0.0076 * ss;                    si = 1.68 / s + 0.275;                    c = 0.062 / s + 0.024;                }            } else {                b = 0.463 + s - 0.178 * ss;                si = 1.235;                c = 0.195 / s - 0.079 + 0.016 * s;            }        }        double v;        double q;        double a9 = 0.104089866;        double a8 = -0.112750886;        double a7 = 0.110368310;        double a6 = -0.124385581;        double a5 = 0.142873973;        double a4 = -0.166677482;        double a3 = 0.199999867;        double a2 = -0.249999949;        double a1 = 0.333333333;        if (x > 0.0) {                                    v = t / (s + s);            if (Math.abs(v) > 0.25) {                q = q0 - s * t + 0.25 * t * t + (ss + ss) * Math.log1p(v);            } else {                q = q0 + 0.5 * t * t * ((((((((a9 * v + a8) * v + a7) * v + a6) * v + a5) * v + a4) * v + a3) * v + a2) * v + a1) * v;            }                        if (Math.log1p(-u) <= q) {                return gds / rate;            }        }        double e7 = 0.000247453;        double e6 = 0.001353826;        double e5 = 0.008345522;        double e4 = 0.041664508;        double e3 = 0.166666848;        double e2 = 0.499999994;        double e1 = 1.000000000;        while (true) {                        double sign_u;            double e;            do {                e = -Math.log(randomDouble());                u = randomDouble();                u = u + u - 1.0;                sign_u = u > 0 ? 1.0 : -1.0;                t = b + e * si * sign_u;            } while (            t <= -0.71874483771719);                        v = t / (s + s);            if (Math.abs(v) > 0.25) {                q = q0 - s * t + 0.25 * t * t + (ss + ss) * Math.log1p(v);            } else {                q = q0 + 0.5 * t * t * ((((((((a9 * v + a8) * v + a7) * v + a6) * v + a5) * v + a4) * v + a3) * v + a2) * v + a1) * v;            }            if (q <= 0.0) {                continue;            }                        double w;            if (q > 0.5) {                w = Math.exp(q) - 1.0;            } else {                w = ((((((e7 * q + e6) * q + e5) * q + e4) * q + e3) * q + e2) * q + e1) * q;            }                        if (c * u * sign_u <= w * Math.exp(e - 0.5 * t * t)) {                x = s + 0.5 * t;                return x * x / rate;            }        }    }}
0
public double pdf(double x)
{    if (x < 0) {        throw new IllegalArgumentException();    }    if (x == 0) {        if (alpha == 1.0) {            return rate;        } else if (alpha < 1) {            return Double.POSITIVE_INFINITY;        } else {            return 0;        }    }    if (alpha == 1.0) {        return rate * Math.exp(-x * rate);    }    return rate * Math.exp((alpha - 1.0) * Math.log(x * rate) - x * rate - logGamma(alpha));}
0
public String toString()
{    return this.getClass().getName() + '(' + rate + ',' + alpha + ')';}
0
public static double logGamma(double x)
{    if (x <= 0.0) /* || x > 1.3e19 */    {        return -999;    }    double z;    for (z = 1.0; x < 11.0; x++) {        z *= x;    }    double r = 1.0 / (x * x);    double c6 = -1.9175269175269175e-03;    double c5 = 8.4175084175084175e-04;    double c4 = -5.9523809523809524e-04;    double c3 = 7.9365079365079365e-04;    double c2 = -2.7777777777777777e-03;    double c1 = 8.3333333333333333e-02;    double g = c1 + r * (c2 + r * (c3 + r * (c4 + r * (c5 + r + c6))));    double c0 = 9.1893853320467274e-01;    g = (x - 0.5) * Math.log(x) - x + c0 + g / x;    if (z == 1.0) {        return g;    }    return g - Math.log(z);}
0
public double cdf(int k)
{    return Probability.negativeBinomial(k, r, p);}
0
public double pdf(int k)
{    return Arithmetic.binomial(k + r - 1, r - 1) * Math.pow(p, r) * Math.pow(1.0 - p, k);}
0
public int nextInt()
{    return nextInt(r, p);}
0
public int nextInt(int r, double p)
{    return this.poisson.nextInt(gamma.nextDouble(r, p / (1.0 - p)));}
0
public String toString()
{    return this.getClass().getName() + '(' + r + ',' + p + ')';}
0
public double cdf(double x)
{    return Probability.normal(mean, variance, x);}
0
public double pdf(double x)
{    double diff = x - mean;    return normalizer * Math.exp(-(diff * diff) / (2.0 * variance));}
0
public double nextDouble()
{        if (cacheFilled) {        cacheFilled = false;        return cache;    }    double x;    double y;    double r;    do {        x = 2.0 * randomDouble() - 1.0;        y = 2.0 * randomDouble() - 1.0;        r = x * x + y * y;    } while (r >= 1.0);    double z = Math.sqrt(-2.0 * Math.log(r) / r);    cache = this.mean + this.standardDeviation * x * z;    cacheFilled = true;    return this.mean + this.standardDeviation * y * z;}
0
public final void setRandomGenerator(Random randomGenerator)
{    super.setRandomGenerator(randomGenerator);    this.cacheFilled = false;}
0
public final void setState(double mean, double standardDeviation)
{    if (mean != this.mean || standardDeviation != this.standardDeviation) {        this.mean = mean;        this.standardDeviation = standardDeviation;        this.variance = standardDeviation * standardDeviation;        this.cacheFilled = false;        this.normalizer = 1.0 / Math.sqrt(2.0 * Math.PI * variance);    }}
0
public String toString()
{    return String.format(Locale.ENGLISH, "%s(m=%f, sd=%f)", this.getClass().getName(), mean, standardDeviation);}
0
private static double f(int k, double lNu, double cPm)
{    return Math.exp(k * lNu - Arithmetic.logFactorial(k) - cPm);}
0
public int nextInt()
{    return nextInt(mean);}
0
public int nextInt(double theMean)
{    /**     * ***************************************************************     *                                                                 *     *  Poisson Distribution - Patchwork Rejection/Inversion           *     *                                                                 *     * *****************************************************************     *                                                                 *     *  For parameter  my < 10  Tabulated Inversion is applied.        *     *  For my >= 10  Patchwork Rejection is employed:                 *     *  The area below the histogram function f(x) is rearranged in    *     *  its body by certain point reflections. Within a large center   *     *  interval variates are sampled efficiently by rejection from    *     *  uniform hats. Rectangular immediate acceptance regions speed   *     *  up the generation. The remaining tails are covered by          *     *  exponential functions.                                         *     *                                                                 *     * ***************************************************************     */    Random gen = getRandomGenerator();                        int m;    if (theMean < SWITCH_MEAN) {                if (theMean != myOld) {            myOld = theMean;            llll = 0;            p = Math.exp(-theMean);            q = p;            p0 = p;                }        m = theMean > 1.0 ? (int) theMean : 1;        while (true) {            double u = gen.nextDouble();            int k = 0;            if (u <= p0) {                return k;            }            if (llll != 0) {                                int i = u > 0.458 ? Math.min(llll, m) : 1;                for (k = i; k <= llll; k++) {                    if (u <= pp[k]) {                        return k;                    }                }                if (llll == 35) {                    continue;                }            }            for (k = llll + 1; k <= 35; k++) {                                p *= theMean / k;                q += p;                pp[k] = q;                if (u <= q) {                    llll = k;                    return k;                }            }            llll = 35;        }        } else if (theMean < MEAN_MAX) {                                                m = (int) theMean;        if (theMean != myLast) {                        myLast = theMean;                        double Ds = Math.sqrt(theMean + 0.25);                                    k2 = (int) Math.ceil(theMean - 0.5 - Ds);            k4 = (int) (theMean - 0.5 + Ds);            k1 = k2 + k2 - m + 1;            k5 = k4 + k4 - m;                        dl = k2 - k1;            dr = k5 - k4;                        r1 = theMean / k1;            r2 = theMean / k2;            r4 = theMean / (k4 + 1);            r5 = theMean / (k5 + 1);                                    ll = Math.log(r1);                        lr = -Math.log(r5);                        lMy = Math.log(theMean);            cPm = m * lMy - Arithmetic.logFactorial(m);                        f2 = f(k2, lMy, cPm);            f4 = f(k4, lMy, cPm);            f1 = f(k1, lMy, cPm);            f5 = f(k5, lMy, cPm);                                                p1 = f2 * (dl + 1.0);                        p2 = f2 * dl + p1;                        p3 = f4 * (dr + 1.0) + p2;                        p4 = f4 * dr + p3;                        p5 = f1 / ll + p4;                        p6 = f5 / lr + p5;        }        while (true) {                                    double W;            double V;            double U;            int Y;            int X;            int Dk;            if ((U = gen.nextDouble() * p6) < p2) {                                if ((V = U - p1) < 0.0) {                    return k2 + (int) (U / f2);                }                                if ((W = V / dl) < f1) {                    return k1 + (int) (V / f1);                }                                                Dk = gen.nextInt((int) dl) + 1;                if (W <= f2 - Dk * (f2 - f2 / r2)) {                                        return k2 - Dk;                }                if ((V = f2 + f2 - W) < 1.0) {                                        Y = k2 + Dk;                    if (V <= f2 + Dk * (1.0 - f2) / (dl + 1.0)) {                                                return Y;                    }                    if (V <= f(Y, lMy, cPm)) {                        return Y;                    }                                }                X = k2 - Dk;            } else if (U < p4) {                                if ((V = U - p3) < 0.0) {                    return k4 - (int) ((U - p2) / f4);                }                                if ((W = V / dr) < f5) {                    return k5 - (int) (V / f5);                }                                                Dk = gen.nextInt((int) dr) + 1;                if (W <= f4 - Dk * (f4 - f4 * r4)) {                                        return k4 + Dk;                }                if ((V = f4 + f4 - W) < 1.0) {                                        Y = k4 - Dk;                    if (V <= f4 + Dk * (1.0 - f4) / dr) {                                                return Y;                    }                    if (V <= f(Y, lMy, cPm)) {                        return Y;                    }                                }                X = k4 + Dk;            } else {                W = gen.nextDouble();                if (U < p5) {                                        Dk = (int) (1.0 - Math.log(W) / ll);                    if ((X = k1 - Dk) < 0) {                        continue;                    }                                                            W *= (U - p4) * ll;                    if (W <= f1 - Dk * (f1 - f1 / r1)) {                        return X;                    }                                } else {                                        Dk = (int) (1.0 - Math.log(W) / lr);                                        X = k5 + Dk;                                        W *= (U - p5) * lr;                    if (W <= f5 - Dk * (f5 - f5 * r5)) {                        return X;                    }                                }            }                        if (Math.log(W) <= X * lMy - Arithmetic.logFactorial(X) - cPm) {                return X;            }        }    } else {                return (int) theMean;    }}
0
private static void rejectMethodD(long n, long N, int count, long low, long[] values, int fromIndex, Random randomGenerator)
{    /*  This algorithm is applicable if a large percentage (90%..100%) of N shall be sampled.      In such cases it is more efficient than sampleMethodA() and sampleMethodD().        The idea is that it is more efficient to express      sample(n,N,count) in terms of reject(N-n,N,count)       and then invert the result.      For example, sampling 99% turns into sampling 1% plus inversion.      This algorithm is the same as method sampleMethodD(...) with the exception that sampled elements are rejected,      and not sampled elements included in the result set.    */        n = N - n;        long chosen = -1 + low;                double nreal = n;    double ninv = 1.0 / nreal;    double Nreal = N;    double Vprime = Math.exp(Math.log(randomGenerator.nextDouble()) * ninv);    long qu1 = -n + 1 + N;    double qu1real = -nreal + 1.0 + Nreal;        long S;    while (n > 1 && count > 0) {                double nmin1inv = 1.0 / (-1.0 + nreal);        double negSreal;        while (true) {            double X;            while (true) {                                X = Nreal * (-Vprime + 1.0);                S = (long) X;                if (S < qu1) {                    break;                }                Vprime = Math.exp(Math.log(randomGenerator.nextDouble()) * ninv);            }            double U = randomGenerator.nextDouble();            negSreal = -S;                        double y1 = Math.exp(Math.log(U * Nreal / qu1real) * nmin1inv);            Vprime = y1 * (-X / Nreal + 1.0) * qu1real / (negSreal + qu1real);            if (Vprime <= 1.0) {                break;            }                                    double top = -1.0 + Nreal;            long limit;            double bottom;            if (n - 1 > S) {                bottom = -nreal + Nreal;                limit = -S + N;            } else {                bottom = -1.0 + negSreal + Nreal;                limit = qu1;            }            double y2 = 1.0;            for (long t = N - 1; t >= limit; t--) {                y2 *= top / bottom;                top--;                bottom--;            }            if (Nreal / (-X + Nreal) >= y1 * Math.exp(Math.log(y2) * nmin1inv)) {                                Vprime = Math.exp(Math.log(randomGenerator.nextDouble()) * nmin1inv);                                break;            }            Vprime = Math.exp(Math.log(randomGenerator.nextDouble()) * ninv);        }                        int iter = count;        if (S < iter) {            iter = (int) S;        }        count -= iter;        while (--iter >= 0) {            values[fromIndex++] = ++chosen;        }        chosen++;        N -= S + 1;        Nreal = negSreal - 1.0 + Nreal;        n--;        nreal--;        ninv = nmin1inv;        qu1 = -S + qu1;        qu1real = negSreal + qu1real;        }    if (count > 0) {                        S = (long) (N * Vprime);                int iter = count;        if (S < iter) {            iter = (int) S;        }        count -= iter;        while (--iter >= 0) {            values[fromIndex++] = ++chosen;        }        chosen++;                while (--count >= 0) {            values[fromIndex++] = ++chosen;        }    }}
0
public static void sample(long n, long N, int count, long low, long[] values, int fromIndex, Random randomGenerator)
{    if (n <= 0 || count <= 0) {        return;    }    if (count > n) {        throw new IllegalArgumentException("count must not be greater than n");    }    if (randomGenerator == null) {        randomGenerator = RandomUtils.getRandom();    }    if (count == N) {                long val = low;        int limit = fromIndex + count;        for (int i = fromIndex; i < limit; i++) {            values[i] = val++;        }        return;    }    if (n < N * 0.95) {                sampleMethodD(n, N, count, low, values, fromIndex, randomGenerator);    } else {                rejectMethodD(n, N, count, low, values, fromIndex, randomGenerator);    }}
0
private static void sampleMethodA(long n, long N, int count, long low, long[] values, int fromIndex, Random randomGenerator)
{    long chosen = -1 + low;    double top = N - n;    double Nreal = N;    long S;    while (n >= 2 && count > 0) {        double V = randomGenerator.nextDouble();        S = 0;        double quot = top / Nreal;        while (quot > V) {            S++;            top--;            Nreal--;            quot *= top / Nreal;        }        chosen += S + 1;        values[fromIndex++] = chosen;        count--;        Nreal--;        n--;    }    if (count > 0) {                S = (long) (Math.round(Nreal) * randomGenerator.nextDouble());        chosen += S + 1;        values[fromIndex] = chosen;    }}
0
private static void sampleMethodD(long n, long N, int count, long low, long[] values, int fromIndex, Random randomGenerator)
{    long chosen = -1 + low;    double nreal = n;    double ninv = 1.0 / nreal;    double Nreal = N;    double vprime = Math.exp(Math.log(randomGenerator.nextDouble()) * ninv);    long qu1 = -n + 1 + N;    double qu1real = -nreal + 1.0 + Nreal;    long negalphainv = -13;            long threshold = -negalphainv * n;    long S;    while (n > 1 && count > 0 && threshold < N) {        double nmin1inv = 1.0 / (-1.0 + nreal);        double negSreal;        while (true) {            double X;            while (true) {                                X = Nreal * (-vprime + 1.0);                S = (long) X;                if (S < qu1) {                    break;                }                vprime = Math.exp(Math.log(randomGenerator.nextDouble()) * ninv);            }            double U = randomGenerator.nextDouble();            negSreal = -S;                        double y1 = Math.exp(Math.log(U * Nreal / qu1real) * nmin1inv);            vprime = y1 * (-X / Nreal + 1.0) * qu1real / (negSreal + qu1real);            if (vprime <= 1.0) {                break;            }                                    double top = -1.0 + Nreal;            long limit;            double bottom;            if (n - 1 > S) {                bottom = -nreal + Nreal;                limit = -S + N;            } else {                bottom = -1.0 + negSreal + Nreal;                limit = qu1;            }            double y2 = 1.0;            for (long t = N - 1; t >= limit; t--) {                y2 *= top / bottom;                top--;                bottom--;            }            if (Nreal / (-X + Nreal) >= y1 * Math.exp(Math.log(y2) * nmin1inv)) {                                vprime = Math.exp(Math.log(randomGenerator.nextDouble()) * nmin1inv);                                break;            }            vprime = Math.exp(Math.log(randomGenerator.nextDouble()) * ninv);        }                chosen += S + 1;        values[fromIndex++] = chosen;        /*            for (int iter=0; iter<S && count > 0; iter++) {        values[fromIndex++] = ++chosen;        count--;      }      chosen++;      */        count--;        N -= S + 1;        Nreal = negSreal - 1.0 + Nreal;        n--;        nreal--;        ninv = nmin1inv;        qu1 = -S + qu1;        qu1real = negSreal + qu1real;        threshold += negalphainv;    }    if (count > 0) {        if (n > 1) {                        sampleMethodA(n, N, count, chosen + 1, values, fromIndex, randomGenerator);        } else {                        S = (long) (N * vprime);            chosen += S + 1;            values[fromIndex++] = chosen;        }    }}
0
public double cdf(double x)
{    if (x <= min) {        return 0.0;    }    if (x >= max) {        return 1.0;    }    return (x - min) / (max - min);}
0
public boolean nextBoolean()
{    return randomDouble() > 0.5;}
0
public double nextDouble()
{    return min + (max - min) * randomDouble();}
0
public double nextDoubleFromTo(double from, double to)
{    return from + (to - from) * randomDouble();}
0
public float nextFloatFromTo(float from, float to)
{    return (float) nextDoubleFromTo(from, to);}
0
public int nextIntFromTo(int from, int to)
{    return (int) (from + (long) ((1L + to - from) * randomDouble()));}
0
public long nextLongFromTo(long from, long to)
{        if (from >= 0 && to < Long.MAX_VALUE) {        return from + (long) nextDoubleFromTo(0.0, to - from + 1);    }            double diff = (double) to - (double) from + 1.0;    if (diff <= Long.MAX_VALUE) {        return from + (long) nextDoubleFromTo(0.0, diff);    }            long random;    if (from == Long.MIN_VALUE) {        if (to == Long.MAX_VALUE) {                        int i1 = nextIntFromTo(Integer.MIN_VALUE, Integer.MAX_VALUE);            int i2 = nextIntFromTo(Integer.MIN_VALUE, Integer.MAX_VALUE);            return ((i1 & 0xFFFFFFFFL) << 32) | (i2 & 0xFFFFFFFFL);        }        random = Math.round(nextDoubleFromTo(Long.MIN_VALUE, to + 1));        if (random > to) {            random = Long.MIN_VALUE;        }    } else {        random = Math.round(nextDoubleFromTo(from - 1, to));        if (random < from) {            random = to;        }    }    return random;}
0
public double pdf(double x)
{    if (x <= min || x >= max) {        return 0.0;    }    return 1.0 / (max - min);}
0
public void setState(double min, double max)
{    if (max < min) {        setState(max, min);        return;    }    this.min = min;    this.max = max;}
0
public String toString()
{    return this.getClass().getName() + '(' + min + ',' + max + ')';}
0
public static double beta(double alpha, double beta)
{    double y;    if (alpha < 40 && beta < 40) {        y = gamma(alpha + beta);        if (y == 0.0) {            return 1.0;        }        if (alpha > beta) {            y = gamma(alpha) / y;            y *= gamma(beta);        } else {            y = gamma(beta) / y;            y *= gamma(alpha);        }    } else {        y = Math.exp(logGamma(alpha) + logGamma(beta) - logGamma(alpha + beta));    }    return y;}
0
public static double gamma(double x)
{    double[] pCoefficient = { 1.60119522476751861407E-4, 1.19135147006586384913E-3, 1.04213797561761569935E-2, 4.76367800457137231464E-2, 2.07448227648435975150E-1, 4.94214826801497100753E-1, 9.99999999999999996796E-1 };    double[] qCoefficient = { -2.31581873324120129819E-5, 5.39605580493303397842E-4, -4.45641913851797240494E-3, 1.18139785222060435552E-2, 3.58236398605498653373E-2, -2.34591795718243348568E-1, 7.14304917030273074085E-2, 1.00000000000000000320E0 };            double p;    double z;    double q = Math.abs(x);    if (q > 33.0) {        if (x < 0.0) {            p = Math.floor(q);            if (p == q) {                throw new ArithmeticException("gamma: overflow");            }                        z = q - p;            if (z > 0.5) {                p += 1.0;                z = q - p;            }            z = q * Math.sin(Math.PI * z);            if (z == 0.0) {                throw new ArithmeticException("gamma: overflow");            }            z = Math.abs(z);            z = Math.PI / (z * stirlingFormula(q));            return -z;        } else {            return stirlingFormula(x);        }    }    z = 1.0;    while (x >= 3.0) {        x -= 1.0;        z *= x;    }    while (x < 0.0) {        if (x == 0.0) {            throw new ArithmeticException("gamma: singular");        }        if (x > -1.0e-9) {            return z / ((1.0 + 0.5772156649015329 * x) * x);        }        z /= x;        x += 1.0;    }    while (x < 2.0) {        if (x == 0.0) {            throw new ArithmeticException("gamma: singular");        }        if (x < 1.0e-9) {            return z / ((1.0 + 0.5772156649015329 * x) * x);        }        z /= x;        x += 1.0;    }    if ((x == 2.0) || (x == 3.0)) {        return z;    }    x -= 2.0;    p = Polynomial.polevl(x, pCoefficient, 6);    q = Polynomial.polevl(x, qCoefficient, 7);    return z * p / q;}
0
public static double incompleteBeta(double alpha, double beta, double xx)
{    if (alpha <= 0.0) {        throw new ArithmeticException("incompleteBeta: Domain error! alpha must be > 0, but was " + alpha);    }    if (beta <= 0.0) {        throw new ArithmeticException("incompleteBeta: Domain error! beta must be > 0, but was " + beta);    }    if (xx <= 0.0) {        return 0.0;    }    if (xx >= 1.0) {        return 1.0;    }    double t;    if ((beta * xx) <= 1.0 && xx <= 0.95) {        t = powerSeries(alpha, beta, xx);        return t;    }    double w = 1.0 - xx;    /* Reverse a and b if x is greater than the mean. */    double xc;    double x;    double b;    double a;    boolean flag = false;    if (xx > (alpha / (alpha + beta))) {        flag = true;        a = beta;        b = alpha;        xc = xx;        x = w;    } else {        a = alpha;        b = beta;        xc = w;        x = xx;    }    if (flag && (b * x) <= 1.0 && x <= 0.95) {        t = powerSeries(a, b, x);        t = t <= Constants.MACHEP ? 1.0 - Constants.MACHEP : 1.0 - t;        return t;    }    /* Choose expansion for better convergence. */    double y = x * (a + b - 2.0) - (a - 1.0);    w = y < 0.0 ? incompleteBetaFraction1(a, b, x) : incompleteBetaFraction2(a, b, x) / xc;    /* Multiply w by the factor       a      b   _             _     _      x  (1-x)   | (a+b) / ( a | (a) | (b) ) .   */    y = a * Math.log(x);    t = b * Math.log(xc);    if ((a + b) < Constants.MAXGAM && Math.abs(y) < Constants.MAXLOG && Math.abs(t) < Constants.MAXLOG) {        t = Math.pow(xc, b);        t *= Math.pow(x, a);        t /= a;        t *= w;        t *= gamma(a + b) / (gamma(a) * gamma(b));        if (flag) {            t = t <= Constants.MACHEP ? 1.0 - Constants.MACHEP : 1.0 - t;        }        return t;    }    /* Resort to logarithms.  */    y += t + logGamma(a + b) - logGamma(a) - logGamma(b);    y += Math.log(w / a);    t = y < Constants.MINLOG ? 0.0 : Math.exp(y);    if (flag) {        t = t <= Constants.MACHEP ? 1.0 - Constants.MACHEP : 1.0 - t;    }    return t;}
0
 static double incompleteBetaFraction1(double a, double b, double x)
{    double k1 = a;    double k2 = a + b;    double k3 = a;    double k4 = a + 1.0;    double k5 = 1.0;    double k6 = b - 1.0;    double k7 = k4;    double k8 = a + 2.0;    double pkm2 = 0.0;    double qkm2 = 1.0;    double pkm1 = 1.0;    double qkm1 = 1.0;    double ans = 1.0;    double r = 1.0;    int n = 0;    double thresh = 3.0 * Constants.MACHEP;    do {        double xk = -(x * k1 * k2) / (k3 * k4);        double pk = pkm1 + pkm2 * xk;        double qk = qkm1 + qkm2 * xk;        pkm2 = pkm1;        pkm1 = pk;        qkm2 = qkm1;        qkm1 = qk;        xk = (x * k5 * k6) / (k7 * k8);        pk = pkm1 + pkm2 * xk;        qk = qkm1 + qkm2 * xk;        pkm2 = pkm1;        pkm1 = pk;        qkm2 = qkm1;        qkm1 = qk;        if (qk != 0) {            r = pk / qk;        }        double t;        if (r != 0) {            t = Math.abs((ans - r) / r);            ans = r;        } else {            t = 1.0;        }        if (t < thresh) {            return ans;        }        k1 += 1.0;        k2 += 1.0;        k3 += 2.0;        k4 += 2.0;        k5 += 1.0;        k6 -= 1.0;        k7 += 2.0;        k8 += 2.0;        if ((Math.abs(qk) + Math.abs(pk)) > Constants.BIG) {            pkm2 *= Constants.BIG_INVERSE;            pkm1 *= Constants.BIG_INVERSE;            qkm2 *= Constants.BIG_INVERSE;            qkm1 *= Constants.BIG_INVERSE;        }        if ((Math.abs(qk) < Constants.BIG_INVERSE) || (Math.abs(pk) < Constants.BIG_INVERSE)) {            pkm2 *= Constants.BIG;            pkm1 *= Constants.BIG;            qkm2 *= Constants.BIG;            qkm1 *= Constants.BIG;        }    } while (++n < 300);    return ans;}
0
 static double incompleteBetaFraction2(double a, double b, double x)
{    double k1 = a;    double k2 = b - 1.0;    double k3 = a;    double k4 = a + 1.0;    double k5 = 1.0;    double k6 = a + b;    double k7 = a + 1.0;    double k8 = a + 2.0;    double pkm2 = 0.0;    double qkm2 = 1.0;    double pkm1 = 1.0;    double qkm1 = 1.0;    double z = x / (1.0 - x);    double ans = 1.0;    double r = 1.0;    int n = 0;    double thresh = 3.0 * Constants.MACHEP;    do {        double xk = -(z * k1 * k2) / (k3 * k4);        double pk = pkm1 + pkm2 * xk;        double qk = qkm1 + qkm2 * xk;        pkm2 = pkm1;        pkm1 = pk;        qkm2 = qkm1;        qkm1 = qk;        xk = (z * k5 * k6) / (k7 * k8);        pk = pkm1 + pkm2 * xk;        qk = qkm1 + qkm2 * xk;        pkm2 = pkm1;        pkm1 = pk;        qkm2 = qkm1;        qkm1 = qk;        if (qk != 0) {            r = pk / qk;        }        double t;        if (r != 0) {            t = Math.abs((ans - r) / r);            ans = r;        } else {            t = 1.0;        }        if (t < thresh) {            return ans;        }        k1 += 1.0;        k2 -= 1.0;        k3 += 2.0;        k4 += 2.0;        k5 += 1.0;        k6 += 1.0;        k7 += 2.0;        k8 += 2.0;        if ((Math.abs(qk) + Math.abs(pk)) > Constants.BIG) {            pkm2 *= Constants.BIG_INVERSE;            pkm1 *= Constants.BIG_INVERSE;            qkm2 *= Constants.BIG_INVERSE;            qkm1 *= Constants.BIG_INVERSE;        }        if ((Math.abs(qk) < Constants.BIG_INVERSE) || (Math.abs(pk) < Constants.BIG_INVERSE)) {            pkm2 *= Constants.BIG;            pkm1 *= Constants.BIG;            qkm2 *= Constants.BIG;            qkm1 *= Constants.BIG;        }    } while (++n < 300);    return ans;}
0
public static double incompleteGamma(double alpha, double x)
{    if (x <= 0 || alpha <= 0) {        return 0.0;    }    if (x > 1.0 && x > alpha) {        return 1.0 - incompleteGammaComplement(alpha, x);    }    /* Compute  x**a * exp(-x) / gamma(a)  */    double ax = alpha * Math.log(x) - x - logGamma(alpha);    if (ax < -Constants.MAXLOG) {        return 0.0;    }    ax = Math.exp(ax);    /* power series */    double r = alpha;    double c = 1.0;    double ans = 1.0;    do {        r += 1.0;        c *= x / r;        ans += c;    } while (c / ans > Constants.MACHEP);    return ans * ax / alpha;}
0
public static double incompleteGammaComplement(double alpha, double x)
{    if (x <= 0 || alpha <= 0) {        return 1.0;    }    if (x < 1.0 || x < alpha) {        return 1.0 - incompleteGamma(alpha, x);    }    double ax = alpha * Math.log(x) - x - logGamma(alpha);    if (ax < -Constants.MAXLOG) {        return 0.0;    }    ax = Math.exp(ax);    /* continued fraction */    double y = 1.0 - alpha;    double z = x + y + 1.0;    double c = 0.0;    double pkm2 = 1.0;    double qkm2 = x;    double pkm1 = x + 1.0;    double qkm1 = z * x;    double ans = pkm1 / qkm1;    double t;    do {        c += 1.0;        y += 1.0;        z += 2.0;        double yc = y * c;        double pk = pkm1 * z - pkm2 * yc;        double qk = qkm1 * z - qkm2 * yc;        if (qk != 0) {            double r = pk / qk;            t = Math.abs((ans - r) / r);            ans = r;        } else {            t = 1.0;        }        pkm2 = pkm1;        pkm1 = pk;        qkm2 = qkm1;        qkm1 = qk;        if (Math.abs(pk) > Constants.BIG) {            pkm2 *= Constants.BIG_INVERSE;            pkm1 *= Constants.BIG_INVERSE;            qkm2 *= Constants.BIG_INVERSE;            qkm1 *= Constants.BIG_INVERSE;        }    } while (t > Constants.MACHEP);    return ans * ax;}
0
public static double logGamma(double x)
{    double p;    double q;    double z;    double[] aCoefficient = { 8.11614167470508450300E-4, -5.95061904284301438324E-4, 7.93650340457716943945E-4, -2.77777777730099687205E-3, 8.33333333333331927722E-2 };    double[] bCoefficient = { -1.37825152569120859100E3, -3.88016315134637840924E4, -3.31612992738871184744E5, -1.16237097492762307383E6, -1.72173700820839662146E6, -8.53555664245765465627E5 };    double[] cCoefficient = { /* 1.00000000000000000000E0, */    -3.51815701436523470549E2, -1.70642106651881159223E4, -2.20528590553854454839E5, -1.13933444367982507207E6, -2.53252307177582951285E6, -2.01889141433532773231E6 };    if (x < -34.0) {        q = -x;        double w = logGamma(q);        p = Math.floor(q);        if (p == q) {            throw new ArithmeticException("lgam: Overflow");        }        z = q - p;        if (z > 0.5) {            p += 1.0;            z = p - q;        }        z = q * Math.sin(Math.PI * z);        if (z == 0.0) {            throw new ArithmeticException("lgamma: Overflow");        }        z = Constants.LOGPI - Math.log(z) - w;        return z;    }    if (x < 13.0) {        z = 1.0;        while (x >= 3.0) {            x -= 1.0;            z *= x;        }        while (x < 2.0) {            if (x == 0.0) {                throw new ArithmeticException("lgamma: Overflow");            }            z /= x;            x += 1.0;        }        if (z < 0.0) {            z = -z;        }        if (x == 2.0) {            return Math.log(z);        }        x -= 2.0;        p = x * Polynomial.polevl(x, bCoefficient, 5) / Polynomial.p1evl(x, cCoefficient, 6);        return Math.log(z) + p;    }    if (x > 2.556348e305) {        throw new ArithmeticException("lgamma: Overflow");    }    q = (x - 0.5) * Math.log(x) - x + 0.91893853320467274178;        if (x > 1.0e8) {        return q;    }    p = 1.0 / (x * x);    if (x >= 1000.0) {        q += ((7.9365079365079365079365e-4 * p - 2.7777777777777777777778e-3) * p + 0.0833333333333333333333) / x;    } else {        q += Polynomial.polevl(p, aCoefficient, 4) / x;    }    return q;}
0
private static double powerSeries(double a, double b, double x)
{    double ai = 1.0 / a;    double u = (1.0 - b) * x;    double v = u / (a + 1.0);    double t1 = v;    double t = u;    double n = 2.0;    double s = 0.0;    double z = Constants.MACHEP * ai;    while (Math.abs(v) > z) {        u = (n - b) * x / n;        t *= u;        v = t / (a + n);        s += v;        n += 1.0;    }    s += t1;    s += ai;    u = a * Math.log(x);    if ((a + b) < Constants.MAXGAM && Math.abs(u) < Constants.MAXLOG) {        t = gamma(a + b) / (gamma(a) * gamma(b));        s *= t * Math.pow(x, a);    } else {        t = logGamma(a + b) - logGamma(a) - logGamma(b) + u + Math.log(s);        s = t < Constants.MINLOG ? 0.0 : Math.exp(t);    }    return s;}
0
 static double stirlingFormula(double x)
{    double[] coefficients = { 7.87311395793093628397E-4, -2.29549961613378126380E-4, -2.68132617805781232825E-3, 3.47222221605458667310E-3, 8.33333333333482257126E-2 };    double w = 1.0 / x;    double y = Math.exp(x);    w = 1.0 + w * Polynomial.polevl(w, coefficients, 4);    if (x > MAXSTIR) {        /* Avoid overflow in Math.pow() */        double v = Math.pow(x, 0.5 * x - 0.25);        y = v * (v / y);    } else {        y = Math.pow(x, x - 0.5) / y;    }    y = Constants.SQTPI * y * w;    return y;}
0
public static double beta(double a, double b, double x)
{    return Gamma.incompleteBeta(a, b, x);}
0
public static double gamma(double alpha, double beta, double x)
{    if (x < 0.0) {        return 0.0;    }    return Gamma.incompleteGamma(alpha, beta * x);}
0
public static double negativeBinomial(int k, int n, double p)
{    if (p < 0.0 || p > 1.0) {        throw new IllegalArgumentException();    }    if (k < 0) {        return 0.0;    }    return Gamma.incompleteBeta(n, k + 1, p);}
0
public static double normal(double a)
{    if (a < 0) {        return 1 - normal(-a);    }    double b0 = 0.2316419;    double b1 = 0.319381530;    double b2 = -0.356563782;    double b3 = 1.781477937;    double b4 = -1.821255978;    double b5 = 1.330274429;    double t = 1 / (1 + b0 * a);    return 1 - UNIT_NORMAL.pdf(a) * t * (b1 + t * (b2 + t * (b3 + t * (b4 + t * b5))));}
0
public static double normal(double mean, double variance, double x)
{    return normal((x - mean) / Math.sqrt(variance));}
0
public static double poisson(int k, double mean)
{    if (mean < 0) {        throw new IllegalArgumentException();    }    if (k < 0) {        return 0.0;    }    return Gamma.incompleteGammaComplement(k + 1, mean);}
0
public boolean isEmpty()
{    return size() == 0;}
0
protected static void checkRange(int index, int theSize)
{    if (index >= theSize || index < 0) {        throw new IndexOutOfBoundsException("Index: " + index + ", Size: " + theSize);    }}
0
protected static void checkRangeFromTo(int from, int to, int theSize)
{    if (to == from - 1) {        return;    }    if (from < 0 || from > to || to >= theSize) {        throw new IndexOutOfBoundsException("from: " + from + ", to: " + to + ", size=" + theSize);    }}
0
public void clear()
{    removeFromTo(0, size() - 1);}
0
public final void mergeSort()
{    mergeSortFromTo(0, size() - 1);}
0
public final void quickSort()
{    quickSortFromTo(0, size() - 1);}
0
public void remove(int index)
{    removeFromTo(index, index);}
0
public void setSize(int newSize)
{    if (newSize < 0) {        throw new IndexOutOfBoundsException("newSize:" + newSize);    }    int currentSize = size();    if (newSize != currentSize) {        if (newSize > currentSize) {            beforeInsertDummies(currentSize, newSize - currentSize);        } else if (newSize < currentSize) {            removeFromTo(newSize, currentSize - 1);        }    }}
0
public final void sort()
{    sortFromTo(0, size() - 1);}
0
public void sortFromTo(int from, int to)
{    quickSortFromTo(from, to);}
0
public void trimToSize()
{}
0
public void addAllOf(Collection<T> collection)
{    this.beforeInsertAllOf(size(), collection);}
0
public void beforeInsertAllOf(int index, Collection<T> collection)
{    this.beforeInsertDummies(index, collection.size());    this.replaceFromWith(index, collection);}
0
public void add(T element)
{        if (size == elements.length) {        ensureCapacity(size + 1);    }    elements[size++] = element;}
0
public void beforeInsert(int index, T element)
{        if (size == index) {        add(element);        return;    }    if (index > size || index < 0) {        throw new IndexOutOfBoundsException("Index: " + index + ", Size: " + size);    }    ensureCapacity(size + 1);    System.arraycopy(elements, index, elements, index + 1, size - index);    elements[index] = element;    size++;}
0
public Object clone()
{        return new ObjectArrayList<>((T[]) elements.clone());}
0
public ObjectArrayList<T> copy()
{    return (ObjectArrayList<T>) clone();}
0
public Q[] elements()
{    return (Q[]) elements;}
0
public void elements(T[] elements)
{    this.elements = elements;    this.size = elements.length;}
0
public void ensureCapacity(int minCapacity)
{    elements = org.apache.mahout.math.Arrays.ensureCapacity(elements, minCapacity);}
0
public boolean equals(Object otherObj)
{        if (!(otherObj instanceof ObjectArrayList)) {        return super.equals(otherObj);    }    if (this == otherObj) {        return true;    }    if (otherObj == null) {        return false;    }    ObjectArrayList<?> other = (ObjectArrayList<?>) otherObj;    if (size() != other.size()) {        return false;    }    Object[] theElements = elements();    Object[] otherElements = other.elements();    for (int i = size(); --i >= 0; ) {        if (theElements[i] != otherElements[i]) {            return false;        }    }    return true;}
0
public boolean forEach(ObjectProcedure<T> procedure)
{    T[] theElements = (T[]) elements;    int theSize = size;    for (int i = 0; i < theSize; ) {        if (!procedure.apply(theElements[i++])) {            return false;        }    }    return true;}
0
public T get(int index)
{        if (index >= size || index < 0) {        throw new IndexOutOfBoundsException("Index: " + index + ", Size: " + size);    }    return (T) elements[index];}
0
public T getQuick(int index)
{    return (T) elements[index];}
0
public int indexOfFromTo(T element, int from, int to)
{        if (size == 0) {        return -1;    }    checkRangeFromTo(from, to, size);    Object[] theElements = elements;    for (int i = from; i <= to; i++) {        if (element == theElements[i]) {            return i;        }        }        return -1;}
0
public int lastIndexOfFromTo(T element, int from, int to)
{        if (size == 0) {        return -1;    }    checkRangeFromTo(from, to, size);    Object[] theElements = elements;    for (int i = to; i >= from; i--) {        if (element == theElements[i]) {            return i;        }        }        return -1;}
0
public AbstractObjectList<T> partFromTo(int from, int to)
{    if (size == 0) {        return new ObjectArrayList<>(0);    }    checkRangeFromTo(from, to, size);    Object[] part = new Object[to - from + 1];    System.arraycopy(elements, from, part, 0, to - from + 1);    return new ObjectArrayList<>((T[]) part);}
0
public void reverse()
{        int limit = size / 2;    int j = size - 1;    Object[] theElements = elements;    for (int i = 0; i < limit; ) {                Object tmp = theElements[i];        theElements[i++] = theElements[j];        theElements[j--] = tmp;    }}
0
public void set(int index, T element)
{        if (index >= size || index < 0) {        throw new IndexOutOfBoundsException("Index: " + index + ", Size: " + size);    }    elements[index] = element;}
0
public void setQuick(int index, T element)
{    elements[index] = element;}
0
public void trimToSize()
{    elements = org.apache.mahout.math.Arrays.trimToCapacity(elements, size());}
0
public void removeFromTo(int fromIndex, int toIndex)
{    throw new UnsupportedOperationException();}
0
public void replaceFromWith(int from, Collection<T> other)
{    throw new UnsupportedOperationException();}
0
protected void beforeInsertDummies(int index, int length)
{    throw new UnsupportedOperationException();}
0
public void mergeSortFromTo(int from, int to)
{    throw new UnsupportedOperationException();}
0
public void quickSortFromTo(int from, int to)
{    throw new UnsupportedOperationException();}
0
public int size()
{    return size;}
0
public void ensureCapacity(int minCapacity)
{    elements = org.apache.mahout.math.Arrays.ensureCapacity(elements, minCapacity);}
0
protected long getQuick(int index)
{    return elements[index];}
0
protected void setQuick(int index, long element)
{    elements[index] = element;}
0
public void trimToSize()
{    elements = org.apache.mahout.math.Arrays.trimToCapacity(elements, size());}
0
public static int hash(char value)
{    return value;}
0
public static int hash(double value)
{    long bits = Double.doubleToLongBits(value);    return (int) (bits ^ (bits >>> 32));}
0
public static int hash(float value)
{    return Float.floatToIntBits(value * 663608941.737f);}
0
public static int hash(int value)
{    int h = value;    h ^= h >>> 16;    h *= 0x85ebca6b;    h ^= h >>> 13;    h *= 0xc2b2ae35;    h ^= h >>> 16;    return h;}
0
public static int hash(long value)
{    return (int) (value ^ (value >> 32));/*    value &= 0x7FFFFFFFFFFFFFFFL;     int hashCode = 0;    do hashCode = 31*hashCode + (int) (value%10);    while ((value /= 10) > 0);    return 28629151*hashCode;     */}
0
public static int hash(Object object)
{    return object == null ? 0 : object.hashCode();}
0
public static int hash(short value)
{    return value;}
0
public static int hash(boolean value)
{    return value ? 1231 : 1237;}
0
public void clear()
{    Arrays.fill(this.state, FREE);    distinct = 0;        freeEntries = table.length;    trimToSize();}
0
public Object clone()
{    OpenHashMap<K, V> copy = (OpenHashMap<K, V>) super.clone();    copy.table = copy.table.clone();    copy.values = copy.values.clone();    copy.state = copy.state.clone();    return copy;}
0
public boolean containsKey(Object key)
{    return indexOfKey((K) key) >= 0;}
0
public boolean containsValue(Object value)
{    return indexOfValue((V) value) >= 0;}
0
public void ensureCapacity(int minCapacity)
{    if (table.length < minCapacity) {        int newCapacity = nextPrime(minCapacity);        rehash(newCapacity);    }}
0
public boolean forEachKey(ObjectProcedure<K> procedure)
{    for (int i = table.length; i-- > 0; ) {        if (state[i] == FULL && !procedure.apply((K) table[i])) {            return false;        }    }    return true;}
0
public boolean forEachPair(ObjectObjectProcedure<K, V> procedure)
{    for (int i = table.length; i-- > 0; ) {        if (state[i] == FULL && !procedure.apply((K) table[i], (V) values[i])) {            return false;        }    }    return true;}
0
public V get(Object key)
{    int i = indexOfKey((K) key);    if (i < 0) {        return null;    }        return (V) values[i];}
0
protected int indexOfInsertion(K key)
{    Object[] tab = table;    byte[] stat = state;    int length = tab.length;    int hash = key.hashCode() & 0x7FFFFFFF;    int i = hash % length;        int decrement = hash % (length - 2);        if (decrement == 0) {        decrement = 1;    }        while (stat[i] == FULL && !equalsMindTheNull(key, tab[i])) {        i -= decrement;                if (i < 0) {            i += length;        }    }    if (stat[i] == REMOVED) {                                int j = i;        while (stat[i] != FREE && (stat[i] == REMOVED || tab[i] != key)) {            i -= decrement;                        if (i < 0) {                i += length;            }        }        if (stat[i] == FREE) {            i = j;        }    }    if (stat[i] == FULL) {                return -i - 1;    }        return i;}
0
protected int indexOfKey(K key)
{    Object[] tab = table;    byte[] stat = state;    int length = tab.length;    int hash = key.hashCode() & 0x7FFFFFFF;    int i = hash % length;        int decrement = hash % (length - 2);        if (decrement == 0) {        decrement = 1;    }        while (stat[i] != FREE && (stat[i] == REMOVED || !equalsMindTheNull(key, tab[i]))) {        i -= decrement;                if (i < 0) {            i += length;        }    }    if (stat[i] == FREE) {        return -1;    }        return i;}
0
protected int indexOfValue(V value)
{    Object[] val = values;    byte[] stat = state;    for (int i = stat.length; --i >= 0; ) {        if (stat[i] == FULL && equalsMindTheNull(val[i], value)) {            return i;        }    }        return -1;}
0
public void keys(List<K> list)
{    list.clear();    Object[] tab = table;    byte[] stat = state;    for (int i = tab.length; i-- > 0; ) {        if (stat[i] == FULL) {            list.add((K) tab[i]);        }    }}
0
public V put(K key, V value)
{    int i = indexOfInsertion(key);    if (i < 0) {                i = -i - 1;        V previous = (V) this.values[i];        this.values[i] = value;        return previous;    }    if (this.distinct > this.highWaterMark) {        int newCapacity = chooseGrowCapacity(this.distinct + 1, this.minLoadFactor, this.maxLoadFactor);        rehash(newCapacity);        return put(key, value);    }    this.table[i] = key;    this.values[i] = value;    if (this.state[i] == FREE) {        this.freeEntries--;    }    this.state[i] = FULL;    this.distinct++;    if (this.freeEntries < 1) {                int newCapacity = chooseGrowCapacity(this.distinct + 1, this.minLoadFactor, this.maxLoadFactor);        rehash(newCapacity);    }    return null;}
0
protected void rehash(int newCapacity)
{    int oldCapacity = table.length;        Object[] oldTable = table;    Object[] oldValues = values;    byte[] oldState = state;    Object[] newTable = new Object[newCapacity];    Object[] newValues = new Object[newCapacity];    byte[] newState = new byte[newCapacity];    this.lowWaterMark = chooseLowWaterMark(newCapacity, this.minLoadFactor);    this.highWaterMark = chooseHighWaterMark(newCapacity, this.maxLoadFactor);    this.table = newTable;    this.values = newValues;    this.state = newState;        this.freeEntries = newCapacity - this.distinct;    for (int i = oldCapacity; i-- > 0; ) {        if (oldState[i] == FULL) {            Object element = oldTable[i];            int index = indexOfInsertion((K) element);            newTable[index] = element;            newValues[index] = oldValues[i];            newState[index] = FULL;        }    }}
0
public V remove(Object key)
{    int i = indexOfKey((K) key);    if (i < 0) {        return null;    }        V removed = (V) values[i];    this.state[i] = REMOVED;        this.distinct--;    if (this.distinct < this.lowWaterMark) {        int newCapacity = chooseShrinkCapacity(this.distinct, this.minLoadFactor, this.maxLoadFactor);        rehash(newCapacity);    }    return removed;}
0
protected void setUp(int initialCapacity, double minLoadFactor, double maxLoadFactor)
{    int capacity = initialCapacity;    super.setUp(capacity, minLoadFactor, maxLoadFactor);    capacity = nextPrime(capacity);    if (capacity == 0) {        capacity = 1;    }        this.table = new Object[capacity];    this.values = new Object[capacity];    this.state = new byte[capacity];        this.minLoadFactor = minLoadFactor;    if (capacity == PrimeFinder.LARGEST_PRIME) {        this.maxLoadFactor = 1.0;    } else {        this.maxLoadFactor = maxLoadFactor;    }    this.distinct = 0;        this.freeEntries = capacity;                    this.lowWaterMark = 0;    this.highWaterMark = chooseHighWaterMark(capacity, this.maxLoadFactor);}
0
public void trimToSize()
{            int newCapacity = nextPrime((int) (1 + 1.2 * size()));    if (table.length > newCapacity) {        rehash(newCapacity);    }}
0
 void getInternalFactors(int[] capacity, double[] minLoadFactor, double[] maxLoadFactor)
{    capacity[0] = table.length;    minLoadFactor[0] = this.minLoadFactor;    maxLoadFactor[0] = this.maxLoadFactor;}
0
public K getKey()
{    return key;}
0
public V getValue()
{    return value;}
0
public V setValue(V value)
{    throw new UnsupportedOperationException("Map.Entry.setValue not supported for OpenHashMap");}
0
public Set<java.util.Map.Entry<K, V>> entrySet()
{    final Set<Entry<K, V>> entries = new OpenHashSet<>();    forEachPair(new ObjectObjectProcedure<K, V>() {        @Override        public boolean apply(K key, V value) {            entries.add(new MapEntry(key, value));            return true;        }    });    return entries;}
0
public boolean apply(K key, V value)
{    entries.add(new MapEntry(key, value));    return true;}
0
public Set<K> keySet()
{    final Set<K> keys = new OpenHashSet<>();    forEachKey(new ObjectProcedure<K>() {        @Override        public boolean apply(K element) {            keys.add(element);            return true;        }    });    return keys;}
0
public boolean apply(K element)
{    keys.add(element);    return true;}
0
public void putAll(Map<? extends K, ? extends V> m)
{    for (Map.Entry<? extends K, ? extends V> e : m.entrySet()) {        put(e.getKey(), e.getValue());    }}
0
public Collection<V> values()
{    final List<V> valueList = new ArrayList<>();    forEachPair(new ObjectObjectProcedure<K, V>() {        @Override        public boolean apply(K key, V value) {            valueList.add(value);            return true;        }    });    return valueList;}
0
public boolean apply(K key, V value)
{    valueList.add(value);    return true;}
0
public boolean equals(Object obj)
{    if (!(obj instanceof OpenHashMap)) {        return false;    }    final OpenHashMap<K, V> o = (OpenHashMap<K, V>) obj;    if (o.size() != size()) {        return false;    }    final boolean[] equal = new boolean[1];    equal[0] = true;    forEachPair(new ObjectObjectProcedure<K, V>() {        @Override        public boolean apply(K key, V value) {            Object ov = o.get(key);            if (!value.equals(ov)) {                equal[0] = false;                return false;            }            return true;        }    });    return equal[0];}
0
public boolean apply(K key, V value)
{    Object ov = o.get(key);    if (!value.equals(ov)) {        equal[0] = false;        return false;    }    return true;}
0
public String toString()
{    final StringBuilder sb = new StringBuilder();    sb.append('{');    forEachPair(new ObjectObjectProcedure<K, V>() {        @Override        public boolean apply(K key, V value) {            sb.append('[');            sb.append(key);            sb.append(" -> ");            sb.append(value);            sb.append("] ");            return true;        }    });    sb.append('}');    return sb.toString();}
0
public boolean apply(K key, V value)
{    sb.append('[');    sb.append(key);    sb.append(" -> ");    sb.append(value);    sb.append("] ");    return true;}
0
public static int nextPrime(int desiredCapacity)
{    int i = java.util.Arrays.binarySearch(PRIME_CAPACITIES, desiredCapacity);    if (i < 0) {                        i = -i - 1;    }    return PRIME_CAPACITIES[i];}
0
public boolean put(int key, int value)
{    /*       This is open addressing with double hashing, using "Brent's variation".       Brent's variation slows insertions a bit down (not much) but reduces probes (collisions) for successful searches,       in particular for large load factors.       (It does not improve unsuccessful searches.)       See D. Knuth, Searching and Sorting, 3rd ed., p.533-545       h1(key) = hash % M       h2(key) = decrement = Max(1, hash/M % M)       M is prime = capacity = table.length       probing positions are table[(h1-j*h2) % M] for j=0,1,...       (M and h2 could also be chosen differently, but h2 is required to be relative prime to M.)    */    int[] tab = table;    byte[] stat = state;    int length = tab.length;    int hash = HashFunctions.hash(key) & 0x7FFFFFFF;    int i = hash % length;    int decrement = (hash / length) % length;    if (decrement == 0) {        decrement = 1;    }                    int t = 0;        int p0 = i;    while (stat[i] == FULL && tab[i] != key) {        t++;        i -= decrement;                if (i < 0) {            i += length;        }    }    if (stat[i] == FULL) {                this.values[i] = value;        return false;    }    if (this.distinct > this.highWaterMark) {        int newCapacity = chooseGrowCapacity(this.distinct + 1, this.minLoadFactor, this.maxLoadFactor);        rehash(newCapacity);        return put(key, value);    }    /*    Brent's variation does a local reorganization to reduce probes. It essentially means:    We test whether it is possible to move the association we probed first (table[p0]) out of the way.    If this is possible, it will reduce probes for the key to be inserted, since it takes its place;    it gets hit earlier.    However, future probes for the key that we move out of the way will increase.    Thus we only move it out of the way, if we have a net gain, that is, if we save more probes than we loose.    For the first probe we safe more than we loose if the number of probes we needed was >=2 (t>=2).    If the first probe cannot be moved out of the way, we try the next probe (p1).    Now we safe more than we loose if t>=3.    We repeat this until we find that we cannot gain or that we can indeed move p(x) out of the way.    Note: Under the great majority of insertions t<=1, so the loop is entered very infrequently.    */    while (t > 1) {        int key0 = tab[p0];        hash = HashFunctions.hash(key0) & 0x7FFFFFFF;        decrement = (hash / length) % length;        if (decrement == 0) {            decrement = 1;        }                int pc = p0 - decrement;        if (pc < 0) {            pc += length;        }        if (stat[pc] != FREE) {                        p0 = pc;            t--;        } else {                        tab[pc] = key0;            stat[pc] = FULL;            values[pc] = values[p0];                        i = p0;                        t = 0;        }    }    this.table[i] = key;    this.values[i] = value;    if (this.state[i] == FREE) {        this.freeEntries--;    }    this.state[i] = FULL;    this.distinct++;    if (this.freeEntries < 1) {                int newCapacity = chooseGrowCapacity(this.distinct + 1, this.minLoadFactor, this.maxLoadFactor);        rehash(newCapacity);    }    return true;}
0
protected void rehash(int newCapacity)
{    int oldCapacity = table.length;        int[] oldTable = table;    int[] oldValues = values;    byte[] oldState = state;    int[] newTable = new int[newCapacity];    int[] newValues = new int[newCapacity];    byte[] newState = new byte[newCapacity];    this.lowWaterMark = chooseLowWaterMark(newCapacity, this.minLoadFactor);    this.highWaterMark = chooseHighWaterMark(newCapacity, this.maxLoadFactor);    this.table = newTable;    this.values = newValues;    this.state = newState;        this.freeEntries = newCapacity - this.distinct;    int tmp = this.distinct;        this.distinct = Integer.MIN_VALUE;    for (int i = oldCapacity; i-- > 0; ) {        if (oldState[i] == FULL) {            put(oldTable[i], oldValues[i]);        /*        int element = oldTable[i];        int index = indexOfInsertion(element);        newTable[index]=element;        newValues[index]=oldValues[i];        newState[index]=FULL;        */        }    }    this.distinct = tmp;}
0
public static Matrix functionalMatrixView(final int rows, final int columns, final IntIntFunction gf, final boolean denseLike)
{    return new FunctionalMatrixView(rows, columns, gf, denseLike);}
0
public static Matrix functionalMatrixView(final int rows, final int columns, final IntIntFunction gf)
{    return new FunctionalMatrixView(rows, columns, gf);}
0
public static Matrix transposedView(final Matrix m)
{    Preconditions.checkArgument(!(m instanceof SparseColumnMatrix));    if (m instanceof TransposedMatrixView) {        return ((TransposedMatrixView) m).getDelegate();    } else {        return new TransposedMatrixView(m);    }}
0
public static Matrix gaussianView(final int rows, final int columns, long seed)
{    return functionalMatrixView(rows, columns, gaussianGenerator(seed), true);}
0
public static Matrix symmetricUniformView(final int rows, final int columns, int seed)
{    return functionalMatrixView(rows, columns, uniformSymmetricGenerator(seed), true);}
0
public static Matrix uniformView(final int rows, final int columns, int seed)
{    return functionalMatrixView(rows, columns, uniformGenerator(seed), true);}
0
public static IntIntFunction gaussianGenerator(final long seed)
{    final Random rnd = RandomUtils.getRandom(seed);    return new IntIntFunction() {        @Override        public double apply(int first, int second) {            rnd.setSeed(seed ^ (((long) first << 32) | (second & 0xffffffffL)));            return rnd.nextGaussian();        }    };}
0
public double apply(int first, int second)
{    rnd.setSeed(seed ^ (((long) first << 32) | (second & 0xffffffffL)));    return rnd.nextGaussian();}
0
public static IntIntFunction uniformSymmetricGenerator(final int seed)
{    return new IntIntFunction() {        private byte[] data = new byte[8];        @Override        public double apply(int row, int column) {            long d = ((long) row << Integer.SIZE) | (column & 0xffffffffL);            for (int i = 0; i < 8; i++, d >>>= 8) data[i] = (byte) d;            long hash = MurmurHash.hash64A(data, seed);            return hash / UNIFORM_DIVISOR;        }    };}
0
public double apply(int row, int column)
{    long d = ((long) row << Integer.SIZE) | (column & 0xffffffffL);    for (int i = 0; i < 8; i++, d >>>= 8) data[i] = (byte) d;    long hash = MurmurHash.hash64A(data, seed);    return hash / UNIFORM_DIVISOR;}
0
public static IntIntFunction uniformGenerator(final int seed)
{    return Functions.chain(new DoubleFunction() {        @Override        public double apply(double x) {            return (x + 1.0) / 2.0;        }    }, uniformSymmetricGenerator(seed));}
0
public double apply(double x)
{    return (x + 1.0) / 2.0;}
0
public Vector vector()
{    return getVector();}
0
public int index()
{    return index;}
0
private static int viewSize(Matrix matrix, int row, int column, int rowStride, int columnStride)
{    if (rowStride != 0 && columnStride != 0) {        int n1 = (matrix.numRows() - row) / rowStride;        int n2 = (matrix.numCols() - column) / columnStride;        return Math.min(n1, n2);    } else if (rowStride > 0) {        return (matrix.numRows() - row) / rowStride;    } else {        return (matrix.numCols() - column) / columnStride;    }}
0
public boolean isDense()
{    return isDense;}
0
public boolean isSequentialAccess()
{    return true;}
0
public Iterator<Element> iterator()
{    final LocalElement r = new LocalElement(0);    return new Iterator<Element>() {        private int i;        @Override        public boolean hasNext() {            return i < size();        }        @Override        public Element next() {            if (i >= size()) {                throw new NoSuchElementException();            }            r.index = i++;            return r;        }        @Override        public void remove() {            throw new UnsupportedOperationException("Can't remove from a view");        }    };}
0
public boolean hasNext()
{    return i < size();}
0
public Element next()
{    if (i >= size()) {        throw new NoSuchElementException();    }    r.index = i++;    return r;}
0
public void remove()
{    throw new UnsupportedOperationException("Can't remove from a view");}
0
public Iterator<Element> iterateNonZero()
{    return new Iterator<Element>() {        class NonZeroElement implements Element {            int index;            @Override            public double get() {                return getQuick(index);            }            @Override            public int index() {                return index;            }            @Override            public void set(double value) {                invalidateCachedLength();                setQuick(index, value);            }        }        private final NonZeroElement element = new NonZeroElement();        private int index = -1;        private int lookAheadIndex = -1;        @Override        public boolean hasNext() {            if (lookAheadIndex == index) {                                lookAhead();            }                        return lookAheadIndex < size();        }        private void lookAhead() {            lookAheadIndex++;            while (lookAheadIndex < size() && getQuick(lookAheadIndex) == 0.0) {                lookAheadIndex++;            }        }        @Override        public Element next() {            if (lookAheadIndex == index) {                                lookAhead();            }            index = lookAheadIndex;            if (index >= size()) {                                throw new NoSuchElementException();            }            element.index = index;            return element;        }        @Override        public void remove() {            throw new UnsupportedOperationException();        }    };}
0
public double get()
{    return getQuick(index);}
0
public int index()
{    return index;}
0
public void set(double value)
{    invalidateCachedLength();    setQuick(index, value);}
0
public boolean hasNext()
{    if (lookAheadIndex == index) {                lookAhead();    }        return lookAheadIndex < size();}
0
private void lookAhead()
{    lookAheadIndex++;    while (lookAheadIndex < size() && getQuick(lookAheadIndex) == 0.0) {        lookAheadIndex++;    }}
0
public Element next()
{    if (lookAheadIndex == index) {                lookAhead();    }    index = lookAheadIndex;    if (index >= size()) {                throw new NoSuchElementException();    }    element.index = index;    return element;}
0
public void remove()
{    throw new UnsupportedOperationException();}
0
public double getQuick(int index)
{    return matrix.getQuick(row + rowStride * index, column + columnStride * index);}
0
public Vector like()
{    return matrix.like(size(), 1).viewColumn(0);}
0
public Vector like(int cardinality)
{    return matrix.like(cardinality, 1).viewColumn(0);}
0
public void setQuick(int index, double value)
{    matrix.setQuick(row + rowStride * index, column + columnStride * index, value);}
0
public int getNumNondefaultElements()
{    return size();}
0
public double getLookupCost()
{        return 1;}
0
public double getIteratorAdvanceCost()
{        return 1;}
0
public boolean isAddConstantTime()
{        return true;}
0
protected Matrix matrixLike(int rows, int columns)
{    return matrix.like(rows, columns);}
0
public Vector clone()
{    MatrixVectorView r = (MatrixVectorView) super.clone();    r.matrix = matrix.clone();    r.row = row;    r.column = column;    r.rowStride = rowStride;    r.columnStride = columnStride;    return r;}
0
public void mergeUpdates(OrderedIntDoubleMapping updates)
{    int[] indices = updates.getIndices();    double[] values = updates.getValues();    for (int i = 0; i < updates.getNumMappings(); ++i) {        matrix.setQuick(row + rowStride * indices[i], column + columnStride * indices[i], values[i]);    }}
0
public Matrix clone()
{    MatrixView clone = (MatrixView) super.clone();    clone.matrix = matrix.clone();    clone.offset = offset.clone();    return clone;}
0
public double getQuick(int row, int column)
{    return matrix.getQuick(offset[ROW] + row, offset[COL] + column);}
0
public Matrix like()
{    return matrix.like(rowSize(), columnSize());}
0
public Matrix like(int rows, int columns)
{    return matrix.like(rows, columns);}
0
public void setQuick(int row, int column, double value)
{    matrix.setQuick(offset[ROW] + row, offset[COL] + column, value);}
0
public int[] getNumNondefaultElements()
{    return new int[] { rowSize(), columnSize() };}
0
public Matrix viewPart(int[] offset, int[] size)
{    if (offset[ROW] < 0) {        throw new IndexException(offset[ROW], 0);    }    if (offset[ROW] + size[ROW] > rowSize()) {        throw new IndexException(offset[ROW] + size[ROW], rowSize());    }    if (offset[COL] < 0) {        throw new IndexException(offset[COL], 0);    }    if (offset[COL] + size[COL] > columnSize()) {        throw new IndexException(offset[COL] + size[COL], columnSize());    }    int[] origin = this.offset.clone();    origin[ROW] += offset[ROW];    origin[COL] += offset[COL];    return new MatrixView(matrix, origin, size);}
0
public Matrix assignColumn(int column, Vector other)
{    if (rowSize() != other.size()) {        throw new CardinalityException(rowSize(), other.size());    }    for (int row = 0; row < rowSize(); row++) {        matrix.setQuick(row + offset[ROW], column + offset[COL], other.getQuick(row));    }    return this;}
0
public Matrix assignRow(int row, Vector other)
{    if (columnSize() != other.size()) {        throw new CardinalityException(columnSize(), other.size());    }    for (int col = 0; col < columnSize(); col++) {        matrix.setQuick(row + offset[ROW], col + offset[COL], other.getQuick(col));    }    return this;}
0
public Vector viewColumn(int column)
{    if (column < 0 || column >= columnSize()) {        throw new IndexException(column, columnSize());    }    return matrix.viewColumn(column + offset[COL]).viewPart(offset[ROW], rowSize());}
0
public Vector viewRow(int row)
{    if (row < 0 || row >= rowSize()) {        throw new IndexException(row, rowSize());    }    return matrix.viewRow(row + offset[ROW]).viewPart(offset[COL], columnSize());}
0
public MatrixFlavor getFlavor()
{    return matrix.getFlavor();}
0
public static int hash(int data, int seed)
{    return hash(ByteBuffer.wrap(Ints.toByteArray(data)), seed);}
0
public static int hash(byte[] data, int seed)
{    return hash(ByteBuffer.wrap(data), seed);}
0
public static int hash(byte[] data, int offset, int length, int seed)
{    return hash(ByteBuffer.wrap(data, offset, length), seed);}
0
public static int hash(ByteBuffer buf, int seed)
{        ByteOrder byteOrder = buf.order();    buf.order(ByteOrder.LITTLE_ENDIAN);    int m = 0x5bd1e995;    int r = 24;    int h = seed ^ buf.remaining();    while (buf.remaining() >= 4) {        int k = buf.getInt();        k *= m;        k ^= k >>> r;        k *= m;        h *= m;        h ^= k;    }    if (buf.remaining() > 0) {        ByteBuffer finish = ByteBuffer.allocate(4).order(ByteOrder.LITTLE_ENDIAN);                        finish.put(buf).rewind();        h ^= finish.getInt();        h *= m;    }    h ^= h >>> 13;    h *= m;    h ^= h >>> 15;    buf.order(byteOrder);    return h;}
0
public static long hash64A(byte[] data, int seed)
{    return hash64A(ByteBuffer.wrap(data), seed);}
0
public static long hash64A(byte[] data, int offset, int length, int seed)
{    return hash64A(ByteBuffer.wrap(data, offset, length), seed);}
0
public static long hash64A(ByteBuffer buf, int seed)
{    ByteOrder byteOrder = buf.order();    buf.order(ByteOrder.LITTLE_ENDIAN);    long m = 0xc6a4a7935bd1e995L;    int r = 47;    long h = seed ^ (buf.remaining() * m);    while (buf.remaining() >= 8) {        long k = buf.getLong();        k *= m;        k ^= k >>> r;        k *= m;        h ^= k;        h *= m;    }    if (buf.remaining() > 0) {        ByteBuffer finish = ByteBuffer.allocate(8).order(ByteOrder.LITTLE_ENDIAN);                        finish.put(buf).rewind();        h ^= finish.getLong();        h *= m;    }    h ^= h >>> r;    h *= m;    h ^= h >>> r;    buf.order(byteOrder);    return h;}
0
public static int murmurhash3x8632(byte[] data, int offset, int len, int seed)
{    int c1 = 0xcc9e2d51;    int c2 = 0x1b873593;    int h1 = seed;        int roundedEnd = offset + (len & 0xfffffffc);    for (int i = offset; i < roundedEnd; i += 4) {                int k1 = (data[i] & 0xff) | ((data[i + 1] & 0xff) << 8) | ((data[i + 2] & 0xff) << 16) | (data[i + 3] << 24);        k1 *= c1;                k1 = (k1 << 15) | (k1 >>> 17);        k1 *= c2;        h1 ^= k1;                h1 = (h1 << 13) | (h1 >>> 19);        h1 = h1 * 5 + 0xe6546b64;    }        int k1 = 0;    switch(len & 0x03) {        case 3:            k1 = (data[roundedEnd + 2] & 0xff) << 16;                case 2:            k1 |= (data[roundedEnd + 1] & 0xff) << 8;                case 1:            k1 |= data[roundedEnd] & 0xff;            k1 *= c1;                        k1 = (k1 << 15) | (k1 >>> 17);            k1 *= c2;            h1 ^= k1;        default:    }        h1 ^= len;        h1 ^= h1 >>> 16;    h1 *= 0x85ebca6b;    h1 ^= h1 >>> 13;    h1 *= 0xc2b2ae35;    h1 ^= h1 >>> 16;    return h1;}
0
public String getName()
{    return name;}
0
public Vector getDelegate()
{    return delegate;}
0
public int hashCode()
{    return delegate.hashCode();}
0
public boolean equals(Object other)
{    return delegate.equals(other);}
0
public NamedVector clone()
{    return new NamedVector(delegate.clone(), name);}
0
public Iterable<Element> all()
{    return delegate.all();}
0
public Iterable<Element> nonZeroes()
{    return delegate.nonZeroes();}
0
public String asFormatString()
{    return toString();}
0
public String toString()
{    StringBuilder bldr = new StringBuilder();    bldr.append(name).append(':').append(delegate.toString());    return bldr.toString();}
0
public Vector assign(double value)
{    return delegate.assign(value);}
0
public Vector assign(double[] values)
{    return delegate.assign(values);}
0
public Vector assign(Vector other)
{    return delegate.assign(other);}
0
public Vector assign(DoubleFunction function)
{    return delegate.assign(function);}
0
public Vector assign(Vector other, DoubleDoubleFunction function)
{    return delegate.assign(other, function);}
0
public Vector assign(DoubleDoubleFunction f, double y)
{    return delegate.assign(f, y);}
0
public int size()
{    return delegate.size();}
0
public boolean isDense()
{    return delegate.isDense();}
0
public boolean isSequentialAccess()
{    return delegate.isSequentialAccess();}
0
public Element getElement(int index)
{    return delegate.getElement(index);}
0
public void mergeUpdates(OrderedIntDoubleMapping updates)
{    delegate.mergeUpdates(updates);}
0
public Vector divide(double x)
{    return delegate.divide(x);}
0
public double dot(Vector x)
{    return delegate.dot(x);}
0
public double get(int index)
{    return delegate.get(index);}
0
public double getQuick(int index)
{    return delegate.getQuick(index);}
0
public NamedVector like()
{    return new NamedVector(delegate.like(), name);}
0
public Vector like(int cardinality)
{    return new NamedVector(delegate.like(cardinality), name);}
0
public Vector minus(Vector x)
{    return delegate.minus(x);}
0
public Vector normalize()
{    return delegate.normalize();}
0
public Vector normalize(double power)
{    return delegate.normalize(power);}
0
public Vector logNormalize()
{    return delegate.logNormalize();}
0
public Vector logNormalize(double power)
{    return delegate.logNormalize(power);}
0
public double norm(double power)
{    return delegate.norm(power);}
0
public double maxValue()
{    return delegate.maxValue();}
0
public int maxValueIndex()
{    return delegate.maxValueIndex();}
0
public double minValue()
{    return delegate.minValue();}
0
public int minValueIndex()
{    return delegate.minValueIndex();}
0
public Vector plus(double x)
{    return delegate.plus(x);}
0
public Vector plus(Vector x)
{    return delegate.plus(x);}
0
public void set(int index, double value)
{    delegate.set(index, value);}
0
public void setQuick(int index, double value)
{    delegate.setQuick(index, value);}
0
public void incrementQuick(int index, double increment)
{    delegate.incrementQuick(index, increment);}
0
public int getNumNonZeroElements()
{    return delegate.getNumNonZeroElements();}
0
public int getNumNondefaultElements()
{    return delegate.getNumNondefaultElements();}
0
public Vector times(double x)
{    return delegate.times(x);}
0
public Vector times(Vector x)
{    return delegate.times(x);}
0
public Vector viewPart(int offset, int length)
{    return delegate.viewPart(offset, length);}
0
public double zSum()
{    return delegate.zSum();}
0
public Matrix cross(Vector other)
{    return delegate.cross(other);}
0
public double aggregate(DoubleDoubleFunction aggregator, DoubleFunction map)
{    return delegate.aggregate(aggregator, map);}
0
public double aggregate(Vector other, DoubleDoubleFunction aggregator, DoubleDoubleFunction combiner)
{    return delegate.aggregate(other, aggregator, combiner);}
0
public double getLengthSquared()
{    return delegate.getLengthSquared();}
0
public double getDistanceSquared(Vector v)
{    return delegate.getDistanceSquared(v);}
0
public double getLookupCost()
{    return delegate.getLookupCost();}
0
public double getIteratorAdvanceCost()
{    return delegate.getIteratorAdvanceCost();}
0
public boolean isAddConstantTime()
{    return delegate.isAddConstantTime();}
0
public Matrix getQ()
{    int columns = Math.min(originalColumns, originalRows);    Matrix q = qr.like(originalRows, columns);    for (int k = columns - 1; k >= 0; k--) {        Vector QRcolk = qr.viewColumn(k).viewPart(k, originalRows - k);        q.set(k, k, 1);        for (int j = k; j < columns; j++) {            if (qr.get(k, k) != 0) {                Vector Qcolj = q.viewColumn(j).viewPart(k, originalRows - k);                double s = -QRcolk.dot(Qcolj) / qr.get(k, k);                Qcolj.assign(QRcolk, Functions.plusMult(s));            }        }    }    return q;}
0
public Matrix getR()
{    int rows = Math.min(originalRows, originalColumns);    Matrix r = qr.like(rows, originalColumns);    for (int i = 0; i < rows; i++) {        for (int j = 0; j < originalColumns; j++) {            if (i < j) {                r.setQuick(i, j, qr.getQuick(i, j));            } else if (i == j) {                r.setQuick(i, j, rDiag.getQuick(i));            } else {                r.setQuick(i, j, 0);            }        }    }    return r;}
0
public boolean hasFullRank()
{    for (int j = 0; j < originalColumns; j++) {        if (rDiag.getQuick(j) == 0) {            return false;        }    }    return true;}
0
public Matrix solve(Matrix B)
{    if (B.numRows() != originalRows) {        throw new IllegalArgumentException("Matrix row dimensions must agree.");    }    int columns = B.numCols();    Matrix x = B.like(originalColumns, columns);                Matrix qt = getQ().transpose();    Matrix y = qt.times(B);    Matrix r = getR();    for (int k = Math.min(originalColumns, originalRows) - 1; k >= 0; k--) {                x.viewRow(k).assign(y.viewRow(k), Functions.plusMult(1 / r.get(k, k)));                Vector rColumn = r.viewColumn(k).viewPart(0, k);        for (int c = 0; c < columns; c++) {            y.viewColumn(c).viewPart(0, k).assign(rColumn, Functions.plusMult(-x.get(k, c)));        }    }    return x;}
0
public String toString()
{    return String.format(Locale.ENGLISH, "QR(%d,%d,fullRank=%s)", originalColumns, originalRows, hasFullRank());}
0
public int[] getIndices()
{    return indices;}
0
public int indexAt(int offset)
{    return indices[offset];}
0
public void setIndexAt(int offset, int index)
{    indices[offset] = index;}
0
public double[] getValues()
{    return values;}
0
public void setValueAt(int offset, double value)
{    values[offset] = value;}
0
public int getNumMappings()
{    return numMappings;}
0
private void growTo(int newCapacity)
{    if (newCapacity > indices.length) {        int[] newIndices = new int[newCapacity];        System.arraycopy(indices, 0, newIndices, 0, numMappings);        indices = newIndices;        double[] newValues = new double[newCapacity];        System.arraycopy(values, 0, newValues, 0, numMappings);        values = newValues;    }}
0
private int find(int index)
{    int low = 0;    int high = numMappings - 1;    while (low <= high) {        int mid = low + (high - low >>> 1);        int midVal = indices[mid];        if (midVal < index) {            low = mid + 1;        } else if (midVal > index) {            high = mid - 1;        } else {            return mid;        }    }    return -(low + 1);}
0
public double get(int index)
{    int offset = find(index);    return offset >= 0 ? values[offset] : DEFAULT_VALUE;}
0
public void set(int index, double value)
{    if (numMappings == 0 || index > indices[numMappings - 1]) {        if (!noDefault || value != DEFAULT_VALUE) {            if (numMappings >= indices.length) {                growTo(Math.max((int) (1.2 * numMappings), numMappings + 1));            }            indices[numMappings] = index;            values[numMappings] = value;            ++numMappings;        }    } else {        int offset = find(index);        if (offset >= 0) {            insertOrUpdateValueIfPresent(offset, value);        } else {            insertValueIfNotDefault(index, offset, value);        }    }}
0
public void merge(OrderedIntDoubleMapping updates)
{    int[] updateIndices = updates.getIndices();    double[] updateValues = updates.getValues();    int newNumMappings = numMappings + updates.getNumMappings();    int newCapacity = Math.max((int) (1.2 * newNumMappings), newNumMappings + 1);    int[] newIndices = new int[newCapacity];    double[] newValues = new double[newCapacity];    int k = 0;    int i = 0, j = 0;    for (; i < numMappings && j < updates.getNumMappings(); ++k) {        if (indices[i] < updateIndices[j]) {            newIndices[k] = indices[i];            newValues[k] = values[i];            ++i;        } else if (indices[i] > updateIndices[j]) {            newIndices[k] = updateIndices[j];            newValues[k] = updateValues[j];            ++j;        } else {            newIndices[k] = updateIndices[j];            newValues[k] = updateValues[j];            ++i;            ++j;        }    }    for (; i < numMappings; ++i, ++k) {        newIndices[k] = indices[i];        newValues[k] = values[i];    }    for (; j < updates.getNumMappings(); ++j, ++k) {        newIndices[k] = updateIndices[j];        newValues[k] = updateValues[j];    }    indices = newIndices;    values = newValues;    numMappings = k;}
0
public int hashCode()
{    int result = 0;    for (int i = 0; i < numMappings; i++) {        result = 31 * result + indices[i];        result = 31 * result + (int) Double.doubleToRawLongBits(values[i]);    }    return result;}
0
public boolean equals(Object o)
{    if (o instanceof OrderedIntDoubleMapping) {        OrderedIntDoubleMapping other = (OrderedIntDoubleMapping) o;        if (numMappings == other.numMappings) {            for (int i = 0; i < numMappings; i++) {                if (indices[i] != other.indices[i] || values[i] != other.values[i]) {                    return false;                }            }            return true;        }    }    return false;}
0
public String toString()
{    StringBuilder result = new StringBuilder(10 * numMappings);    for (int i = 0; i < numMappings; i++) {        result.append('(');        result.append(indices[i]);        result.append(',');        result.append(values[i]);        result.append(')');    }    return result.toString();}
0
public OrderedIntDoubleMapping clone()
{    return new OrderedIntDoubleMapping(indices.clone(), values.clone(), numMappings);}
0
public void increment(int index, double increment)
{    int offset = find(index);    if (offset >= 0) {        double newValue = values[offset] + increment;        insertOrUpdateValueIfPresent(offset, newValue);    } else {        insertValueIfNotDefault(index, offset, increment);    }}
0
private void insertValueIfNotDefault(int index, int offset, double value)
{    if (!noDefault || value != DEFAULT_VALUE) {        if (numMappings >= indices.length) {            growTo(Math.max((int) (1.2 * numMappings), numMappings + 1));        }        int at = -offset - 1;        if (numMappings > at) {            for (int i = numMappings - 1, j = numMappings; i >= at; i--, j--) {                indices[j] = indices[i];                values[j] = values[i];            }        }        indices[at] = index;        values[at] = value;        numMappings++;    }}
0
private void insertOrUpdateValueIfPresent(int offset, double newValue)
{    if (noDefault && newValue == DEFAULT_VALUE) {        for (int i = offset + 1, j = offset; i < numMappings; i++, j++) {            indices[j] = indices[i];            values[j] = values[i];        }        numMappings--;    } else {        values[offset] = newValue;    }}
0
public static VectorIterable pairwiseInnerProducts(Iterable<MatrixSlice> basis)
{    DenseMatrix out = null;    for (MatrixSlice slice1 : basis) {        List<Double> dots = Lists.newArrayList();        for (MatrixSlice slice2 : basis) {            dots.add(slice1.vector().dot(slice2.vector()));        }        if (out == null) {            out = new DenseMatrix(dots.size(), dots.size());        }        for (int i = 0; i < dots.size(); i++) {            out.set(slice1.index(), i, dots.get(i));        }    }    return out;}
0
private static int[] reversePivotPermutation(int[] pivot)
{    int[] unpivot1 = new int[pivot.length];    for (int i = 0; i < pivot.length; i++) {        unpivot1[pivot[i]] = i;    }    return unpivot1;}
0
protected Matrix matrixLike(int rows, int columns)
{    if (vector.isDense()) {        return new DenseMatrix(rows, columns);    } else {        return new SparseRowMatrix(rows, columns);    }}
0
public void mergeUpdates(OrderedIntDoubleMapping updates)
{    for (int i = 0; i < updates.getNumMappings(); ++i) {        updates.setIndexAt(i, pivot[updates.indexAt(i)]);    }    vector.mergeUpdates(updates);}
0
public boolean isDense()
{    return vector.isDense();}
0
public boolean isSequentialAccess()
{    return false;}
0
public Iterator<Element> iterator()
{    return new AbstractIterator<Element>() {        private final Iterator<Element> i = vector.all().iterator();        @Override        protected Vector.Element computeNext() {            if (i.hasNext()) {                final Element x = i.next();                return new Element() {                    private final int index = unpivot[x.index()];                    @Override                    public double get() {                        return x.get();                    }                    @Override                    public int index() {                        return index;                    }                    @Override                    public void set(double value) {                        x.set(value);                    }                };            } else {                return endOfData();            }        }    };}
0
protected Vector.Element computeNext()
{    if (i.hasNext()) {        final Element x = i.next();        return new Element() {            private final int index = unpivot[x.index()];            @Override            public double get() {                return x.get();            }            @Override            public int index() {                return index;            }            @Override            public void set(double value) {                x.set(value);            }        };    } else {        return endOfData();    }}
0
public double get()
{    return x.get();}
0
public int index()
{    return index;}
0
public void set(double value)
{    x.set(value);}
0
public Iterator<Element> iterateNonZero()
{    return new AbstractIterator<Element>() {        private final Iterator<Element> i = vector.nonZeroes().iterator();        @Override        protected Vector.Element computeNext() {            if (i.hasNext()) {                final Element x = i.next();                return new Element() {                    private final int index = unpivot[x.index()];                    @Override                    public double get() {                        return x.get();                    }                    @Override                    public int index() {                        return index;                    }                    @Override                    public void set(double value) {                        x.set(value);                    }                };            } else {                return endOfData();            }        }    };}
0
protected Vector.Element computeNext()
{    if (i.hasNext()) {        final Element x = i.next();        return new Element() {            private final int index = unpivot[x.index()];            @Override            public double get() {                return x.get();            }            @Override            public int index() {                return index;            }            @Override            public void set(double value) {                x.set(value);            }        };    } else {        return endOfData();    }}
0
public double get()
{    return x.get();}
0
public int index()
{    return index;}
0
public void set(double value)
{    x.set(value);}
0
public double getQuick(int index)
{    return vector.getQuick(pivot[index]);}
0
public Vector like()
{    return vector.like();}
0
public Vector like(int cardinality)
{    return vector.like(cardinality);}
0
public void setQuick(int index, double value)
{    vector.setQuick(pivot[index], value);}
0
public int getNumNondefaultElements()
{    return vector.getNumNondefaultElements();}
0
public int getNumNonZeroElements()
{        return vector.getNumNonZeroElements();}
0
public double getLookupCost()
{    return vector.getLookupCost();}
0
public double getIteratorAdvanceCost()
{    return vector.getIteratorAdvanceCost();}
0
public boolean isAddConstantTime()
{    return vector.isAddConstantTime();}
0
public Object clone()
{    try {        return super.clone();    } catch (CloneNotSupportedException exc) {                throw new InternalError();    }}
0
public void swap(int i, int j)
{    swapRows(i, j);    swapColumns(i, j);}
0
public void swapRows(int i, int j)
{    swap(rowPivot, rowUnpivot, i, j);}
0
public void swapColumns(int i, int j)
{    swap(columnPivot, columnUnpivot, i, j);}
0
private static void swap(int[] pivot, int[] unpivot, int i, int j)
{    Preconditions.checkPositionIndex(i, pivot.length);    Preconditions.checkPositionIndex(j, pivot.length);    if (i != j) {        int tmp = pivot[i];        pivot[i] = pivot[j];        pivot[j] = tmp;        unpivot[pivot[i]] = i;        unpivot[pivot[j]] = j;    }}
0
public Matrix assignColumn(int column, Vector other)
{        return base.assignColumn(columnPivot[column], new PermutedVectorView(other, rowUnpivot, rowPivot));}
0
public Matrix assignRow(int row, Vector other)
{        return base.assignRow(rowPivot[row], new PermutedVectorView(other, columnUnpivot, columnPivot));}
0
public Vector viewColumn(int column)
{    if (column < 0 || column >= columnSize()) {        throw new IndexException(column, columnSize());    }    return new PermutedVectorView(base.viewColumn(columnPivot[column]), rowPivot, rowUnpivot);}
0
public Vector viewRow(int row)
{    if (row < 0 || row >= rowSize()) {        throw new IndexException(row, rowSize());    }    return new PermutedVectorView(base.viewRow(rowPivot[row]), columnPivot, columnUnpivot);}
0
public double getQuick(int row, int column)
{    return base.getQuick(rowPivot[row], columnPivot[column]);}
0
public Matrix like()
{    return new PivotedMatrix(base.like());}
0
public Matrix clone()
{    PivotedMatrix clone = (PivotedMatrix) super.clone();    base = base.clone();    rowPivot = rowPivot.clone();    rowUnpivot = rowUnpivot.clone();    columnPivot = columnPivot.clone();    columnUnpivot = columnUnpivot.clone();    return clone;}
0
public Matrix like(int rows, int columns)
{    return new PivotedMatrix(base.like(rows, columns));}
0
public void setQuick(int row, int column, double value)
{    base.setQuick(rowPivot[row], columnPivot[column], value);}
0
public int[] getNumNondefaultElements()
{    return base.getNumNondefaultElements();}
0
public Matrix viewPart(int[] offset, int[] size)
{    return new MatrixView(this, offset, size);}
0
public int rowUnpivot(int k)
{    return rowUnpivot[k];}
0
public int columnUnpivot(int k)
{    return columnUnpivot[k];}
0
public int[] getRowPivot()
{    return rowPivot;}
0
public int[] getInverseRowPivot()
{    return rowUnpivot;}
0
public int[] getColumnPivot()
{    return columnPivot;}
0
public int[] getInverseColumnPivot()
{    return columnUnpivot;}
0
public Matrix getBase()
{    return base;}
0
private static int[] identityPivot(int n)
{    int[] pivot = new int[n];    for (int i = 0; i < n; i++) {        pivot[i] = i;    }    return pivot;}
0
private static int[] invert(int[] pivot)
{    int[] x = new int[pivot.length];    for (int i = 0; i < pivot.length; i++) {        x[pivot[i]] = i;    }    return x;}
0
public Matrix getQ()
{    return q;}
0
public Matrix getR()
{    return r;}
0
public boolean hasFullRank()
{    return fullRank;}
0
public Matrix solve(Matrix B)
{    if (B.numRows() != rows) {        throw new IllegalArgumentException("Matrix row dimensions must agree.");    }    int cols = B.numCols();    Matrix x = mType.like(columns, cols);                Matrix qt = getQ().transpose();    Matrix y = qt.times(B);    Matrix r = getR();    for (int k = Math.min(columns, rows) - 1; k >= 0; k--) {                x.viewRow(k).assign(y.viewRow(k), Functions.plusMult(1 / r.get(k, k)));                Vector rColumn = r.viewColumn(k).viewPart(0, k);        for (int c = 0; c < cols; c++) {            y.viewColumn(c).viewPart(0, k).assign(rColumn, Functions.plusMult(-x.get(k, c)));        }    }    return x;}
0
public String toString()
{    return String.format(Locale.ENGLISH, "QR(%d x %d,fullRank=%s)", rows, columns, hasFullRank());}
0
public double apply(double ignored)
{    return sample();}
0
public Integer sample()
{    double u = rand.nextDouble() * (alpha + weight);    for (int j = 0; j < weights.size(); j++) {                if (u < weights.get(j) - discount) {            weights.set(j, weights.get(j) + 1);            weight++;            return j;        } else {            u -= weights.get(j) - discount;        }    }            weights.add(1);    weight++;    return weights.size() - 1;}
0
public int size()
{    return weights.size();}
0
public int count()
{    return (int) weight;}
0
public int count(int j)
{    Preconditions.checkArgument(j >= 0);    if (j < weights.size()) {        return (int) weights.get(j);    } else {        return 0;    }}
0
public Double sample()
{    return sample(gen.nextDouble());}
0
public double sample(double u)
{    if (exceedMinimum && u < x[0]) {                if (u == 0) {            u = 1.0e-16;        }        return y[0] + Math.log(u / x[0]) * x[0] * (y[1] - y[0]) / (x[1] - x[0]);    } else if (exceedMaximum && u > x[n - 1]) {        if (u == 1) {            u = 1 - 1.0e-16;        }                double dy = y[n - 1] - y[n - 2];        double dx = x[n - 1] - x[n - 2];        return y[n - 1] - Math.log((1 - u) / (1 - x[n - 1])) * (1 - x[n - 1]) * dy / dx;    } else {                for (int i = 1; i < n; i++) {            if (x[i] > u) {                double dy = y[i] - y[i - 1];                double dx = x[i] - x[i - 1];                return y[i - 1] + (u - x[i - 1]) * dy / dx;            }        }        throw new RuntimeException(String.format("Can't happen (%.3f is not in [%.3f,%.3f]", u, x[0], x[n - 1]));    }}
0
public static IndianBuffet<Integer> createIntegerDocumentSampler(double alpha)
{    return new IndianBuffet<>(alpha, new IdentityConverter());}
0
public static IndianBuffet<String> createTextDocumentSampler(double alpha)
{    return new IndianBuffet<>(alpha, new WordConverter());}
0
public List<T> sample()
{    List<T> r = Lists.newArrayList();    if (documents == 0) {        double n = new PoissonSampler(alpha).sample();        for (int i = 0; i < n; i++) {            r.add(converter.convert(i));            count.add(1);        }        documents++;    } else {        documents++;        int i = 0;        for (double cnt : count) {            if (gen.nextDouble() < cnt / documents) {                r.add(converter.convert(i));                count.set(i, count.get(i) + 1);            }            i++;        }        int newItems = new PoissonSampler(alpha / documents).sample().intValue();        for (int j = 0; j < newItems; j++) {            r.add(converter.convert(i + j));            count.add(1);        }    }    return r;}
0
public Integer convert(int i)
{    return i;}
0
public String convert(int i)
{    return String.valueOf(i);}
0
public boolean processLine(String line)
{    Iterables.addAll(theWords, onSpace.split(line));    return true;}
0
public List<String> getResult()
{    return theWords;}
0
public String convert(int i)
{    if (i < words.size()) {        return words.get(i);    } else {        return "w_" + i;    }}
0
public T sample()
{    if (gen.nextDouble() >= p) {        return delegate.sample();    } else {        return missingMarker;    }}
0
public void add(T value, double w)
{    Preconditions.checkNotNull(value);    Preconditions.checkArgument(!items.containsKey(value));    int n = this.weight.size();    if (n == 1) {        weight.add(w);        values.add(value);        items.put(value, 1);    } else {                weight.add(weight.get(n / 2));        values.add(values.get(n / 2));        items.put(values.get(n / 2), n);        n++;                items.put(value, n);        this.weight.add(w);        values.add(value);                while (n > 1) {            n /= 2;            this.weight.set(n, this.weight.get(n) + w);        }    }}
0
public double getWeight(T value)
{    if (items.containsKey(value)) {        return weight.get(items.get(value));    } else {        return 0;    }}
0
public double getProbability(T value)
{    if (items.containsKey(value)) {        return weight.get(items.get(value)) / weight.get(1);    } else {        return 0;    }}
0
public double getWeight()
{    if (weight.size() > 1) {        return weight.get(1);    } else {        return 0;    }}
0
public void delete(T value)
{    set(value, 0);}
0
public void set(T value, double newP)
{    Preconditions.checkArgument(items.containsKey(value));    int n = items.get(value);    if (newP <= 0) {                        items.remove(value);    }    double oldP = weight.get(n);    while (n > 0) {        weight.set(n, weight.get(n) - oldP + newP);        n /= 2;    }}
0
public T sample()
{    Preconditions.checkArgument(!weight.isEmpty());    return sample(rand.nextDouble());}
0
public T sample(double u)
{    u *= weight.get(1);    int n = 1;    while (2 * n < weight.size()) {                double left = weight.get(2 * n);        if (u <= left) {            n = 2 * n;        } else {            u -= left;            n = 2 * n + 1;        }    }    return values.get(n);}
0
 List<Double> getWeights()
{    List<Double> r = Lists.newArrayList();    int i = Integer.highestOneBit(weight.size());    while (i < weight.size()) {        r.add(weight.get(i));        i++;    }    i /= 2;    while (i < Integer.highestOneBit(weight.size())) {        r.add(weight.get(i));        i++;    }    return r;}
0
public Iterator<T> iterator()
{    return new AbstractIterator<T>() {        Iterator<T> valuesIterator = Iterables.skip(values, 1).iterator();        @Override        protected T computeNext() {            while (valuesIterator.hasNext()) {                T next = valuesIterator.next();                if (items.containsKey(next)) {                    return next;                }            }            return endOfData();        }    };}
0
protected T computeNext()
{    while (valuesIterator.hasNext()) {        T next = valuesIterator.next();        if (items.containsKey(next)) {            return next;        }    }    return endOfData();}
0
public Vector sample()
{    Vector v = new DenseVector(dimension).assign(new DoubleFunction() {        @Override        public double apply(double ignored) {            return gen.nextGaussian();        }    });    if (mean != null) {        if (scale != null) {            return scale.times(v).plus(mean);        } else {            return v.plus(mean);        }    } else {        if (scale != null) {            return scale.times(v);        } else {            return v;        }    }}
0
public double apply(double ignored)
{    return gen.nextGaussian();}
0
public Vector getScale()
{    return mean;}
0
public Double sample()
{    return rand.nextGaussian() * sd + mean;}
0
public Double sample()
{    return sample(gen.nextDouble());}
0
 double sample(double u)
{    if (u < limit) {        List<WeightedThing<Integer>> steps = Lists.newArrayList();        limit = 1;        int i = 0;        while (u / 20 < limit) {            double pdf = pd.probability(i);            limit -= pdf;            steps.add(new WeightedThing<>(i, pdf));            i++;        }        steps.add(new WeightedThing<>(steps.size(), limit));        partial = new Multinomial<>(steps);    }    return partial.sample(u);}
0
public T getValue()
{    return value;}
0
public double getWeight()
{    return weight;}
0
public void setWeight(double weight)
{    this.weight = weight;}
0
public int compareTo(WeightedThing<T> other)
{    return Double.compare(this.weight, other.weight);}
0
public boolean equals(Object o)
{    if (o instanceof WeightedThing) {        @SuppressWarnings("unchecked")        WeightedThing<T> other = (WeightedThing<T>) o;        return weight == other.weight && value.equals(other.value);    }    return false;}
0
public int hashCode()
{    return 31 * RandomUtils.hashDouble(weight) + value.hashCode();}
0
protected Matrix matrixLike(int rows, int columns)
{    return new SparseMatrix(rows, columns);}
0
public RandomAccessSparseVector clone()
{    return new RandomAccessSparseVector(size(), values.clone());}
0
public String toString()
{    return sparseVectorToString();}
0
public Vector assign(Vector other)
{    if (size() != other.size()) {        throw new CardinalityException(size(), other.size());    }    values.clear();    for (Element e : other.nonZeroes()) {        setQuick(e.index(), e.get());    }    return this;}
0
public void mergeUpdates(OrderedIntDoubleMapping updates)
{    for (int i = 0; i < updates.getNumMappings(); ++i) {        values.put(updates.getIndices()[i], updates.getValues()[i]);    }}
0
public boolean isDense()
{    return false;}
0
public boolean isSequentialAccess()
{    return false;}
0
public double getQuick(int index)
{    return values.get(index);}
0
public void setQuick(int index, double value)
{    invalidateCachedLength();    if (value == 0.0) {        values.remove(index);    } else {        values.put(index, value);    }}
0
public void incrementQuick(int index, double increment)
{    invalidateCachedLength();    values.addTo(index, increment);}
0
public RandomAccessSparseVector like()
{    return new RandomAccessSparseVector(size(), values.size());}
0
public Vector like(int cardinality)
{    return new RandomAccessSparseVector(cardinality, values.size());}
0
public int getNumNondefaultElements()
{    return values.size();}
0
public int getNumNonZeroElements()
{    final DoubleIterator iterator = values.values().iterator();    int numNonZeros = 0;    for (int i = values.size(); i-- != 0; ) if (iterator.nextDouble() != 0)        numNonZeros++;    return numNonZeros;}
0
public double getLookupCost()
{    return 1;}
0
public double getIteratorAdvanceCost()
{    return 1 + (AbstractSet.DEFAULT_MAX_LOAD_FACTOR + AbstractSet.DEFAULT_MIN_LOAD_FACTOR) / 2;}
0
public boolean isAddConstantTime()
{    return true;}
0
public boolean hasNext()
{    return fastIterator.hasNext();}
0
public Element next()
{    if (!hasNext())        throw new NoSuchElementException();    element.entry = fastIterator.next();    return element;}
0
public void remove()
{    throw new UnsupportedOperationException();}
0
public double get()
{    return entry.getDoubleValue();}
0
public int index()
{    return entry.getIntKey();}
0
public void set(double value)
{    invalidateCachedLength();    if (value == 0.0)        fastIterator.remove();    else        entry.setValue(value);}
0
public Iterator<Element> iterateNonZero()
{    return new NonZeroIterator();}
0
public Iterator<Element> iterator()
{    return new AllIterator();}
0
public double get()
{    return value;}
0
public int index()
{    return index;}
0
public void set(double value)
{    invalidateCachedLength();    if (value == 0.0)        values.remove(index);    else        values.put(index, value);}
0
public boolean hasNext()
{    return element.index + 1 < size();}
0
public Element next()
{    if (!hasNext()) {        throw new NoSuchElementException();    }    element.value = values.get(++element.index);    return element;}
0
public void remove()
{    throw new UnsupportedOperationException();}
0
public Matrix assignColumn(int column, Vector other)
{    throw new UnsupportedOperationException("Can't assign to read-only matrix");}
0
public Matrix assignRow(int row, Vector other)
{    throw new UnsupportedOperationException("Can't assign to read-only matrix");}
0
public double getQuick(int row, int column)
{    if (highQuality) {        ByteBuffer buf = ByteBuffer.allocate(8);        buf.putInt(row);        buf.putInt(column);        buf.flip();        return (MurmurHash.hash64A(buf, seed) & (SCALE - 1)) / (double) SCALE;    } else {                return ((((row * PRIME1) + column * PRIME2 + row * column * PRIME3) & 8) * 0.25) - 1;    }}
0
public Matrix like()
{    return new DenseMatrix(rowSize(), columnSize());}
0
public Matrix like(int rows, int columns)
{    return new DenseMatrix(rows, columns);}
0
public void setQuick(int row, int column, double value)
{    throw new UnsupportedOperationException("Can't assign to read-only matrix");}
0
public int[] getNumNondefaultElements()
{    throw new UnsupportedOperationException("Can't assign to read-only matrix");}
0
public Matrix viewPart(int[] offset, int[] size)
{    return new MatrixView(this, offset, size);}
0
private int copySortedRandomAccessSparseVector(Vector other)
{    int elementCount = other.getNumNondefaultElements();    OrderedElement[] sortableElements = new OrderedElement[elementCount];    int s = 0;    for (Element e : other.nonZeroes()) {        sortableElements[s++] = new OrderedElement(e.index(), e.get());    }    Arrays.sort(sortableElements);    for (int i = 0; i < sortableElements.length; i++) {        values.setIndexAt(i, sortableElements[i].index);        values.setValueAt(i, sortableElements[i].value);    }    values = new OrderedIntDoubleMapping(values.getIndices(), values.getValues(), elementCount);    return elementCount;}
0
protected Matrix matrixLike(int rows, int columns)
{        return new SparseMatrix(rows, columns);}
0
public SequentialAccessSparseVector clone()
{    return new SequentialAccessSparseVector(size(), values.clone());}
0
public void mergeUpdates(OrderedIntDoubleMapping updates)
{    values.merge(updates);}
0
public String toString()
{    return sparseVectorToString();}
0
public boolean isDense()
{    return false;}
0
public boolean isSequentialAccess()
{    return true;}
0
public double getQuick(int index)
{    return values.get(index);}
0
public void setQuick(int index, double value)
{    invalidateCachedLength();    values.set(index, value);}
0
public void incrementQuick(int index, double increment)
{    invalidateCachedLength();    values.increment(index, increment);}
0
public SequentialAccessSparseVector like()
{    return new SequentialAccessSparseVector(size(), values.getNumMappings());}
0
public Vector like(int cardinality)
{    return new SequentialAccessSparseVector(cardinality);}
0
public int getNumNondefaultElements()
{    return values.getNumMappings();}
0
public int getNumNonZeroElements()
{    double[] elementValues = values.getValues();    int numMappedElements = values.getNumMappings();    int numNonZeros = 0;    for (int index = 0; index < numMappedElements; index++) {        if (elementValues[index] != 0) {            numNonZeros++;        }    }    return numNonZeros;}
0
public double getLookupCost()
{    return Math.max(1, Math.round(Functions.LOG2.apply(getNumNondefaultElements())));}
0
public double getIteratorAdvanceCost()
{    return 1;}
0
public boolean isAddConstantTime()
{    return false;}
0
public Iterator<Element> iterateNonZero()
{        return new NonDefaultIterator();}
0
public Iterator<Element> iterator()
{    return new AllIterator();}
0
public boolean hasNext()
{    return element.getNextOffset() < values.getNumMappings();}
0
public Element next()
{    if (!hasNext()) {        throw new NoSuchElementException();    }    element.advanceOffset();    return element;}
0
public void remove()
{    throw new UnsupportedOperationException();}
0
public boolean hasNext()
{    return element.getNextIndex() < SequentialAccessSparseVector.this.size();}
0
public Element next()
{    if (!hasNext()) {        throw new NoSuchElementException();    }    element.advanceIndex();    return element;}
0
public void remove()
{    throw new UnsupportedOperationException();}
0
 void advanceOffset()
{    offset++;}
0
 int getNextOffset()
{    return offset + 1;}
0
public double get()
{    return values.getValues()[offset];}
0
public int index()
{    return values.getIndices()[offset];}
0
public void set(double value)
{    invalidateCachedLength();    values.setValueAt(offset, value);}
0
 void advanceIndex()
{    index++;    if (nextOffset < values.getNumMappings() && index > values.getIndices()[nextOffset]) {        nextOffset++;    }}
0
 int getNextIndex()
{    return index + 1;}
0
public double get()
{    if (nextOffset < values.getNumMappings() && index == values.getIndices()[nextOffset]) {        return values.getValues()[nextOffset];    } else {        return OrderedIntDoubleMapping.DEFAULT_VALUE;    }}
0
public int index()
{    return index;}
0
public void set(double value)
{    invalidateCachedLength();    if (nextOffset < values.getNumMappings() && index == values.indexAt(nextOffset)) {        values.setValueAt(nextOffset, value);    } else {                values.set(index, value);    }}
0
public int compareTo(OrderedElement that)
{        return this.index - that.index;}
0
public int hashCode()
{    return index ^ Doubles.hashCode(value);}
0
public boolean equals(Object o)
{    if (!(o instanceof OrderedElement)) {        return false;    }    OrderedElement other = (OrderedElement) o;    return index == other.index && value == other.value;}
0
protected int chooseGrowCapacity(int size, double minLoad, double maxLoad)
{    return nextPrime(Math.max(size + 1, (int) ((4 * size / (3 * minLoad + maxLoad)))));}
0
protected int chooseHighWaterMark(int capacity, double maxLoad)
{        return Math.min(capacity - 2, (int) (capacity * maxLoad));}
0
protected int chooseLowWaterMark(int capacity, double minLoad)
{    return (int) (capacity * minLoad);}
0
protected int chooseMeanCapacity(int size, double minLoad, double maxLoad)
{    return nextPrime(Math.max(size + 1, (int) ((2 * size / (minLoad + maxLoad)))));}
0
protected int chooseShrinkCapacity(int size, double minLoad, double maxLoad)
{    return nextPrime(Math.max(size + 1, (int) ((4 * size / (minLoad + 3 * maxLoad)))));}
0
public void ensureCapacity(int minCapacity)
{}
0
public boolean isEmpty()
{    return distinct == 0;}
0
protected int nextPrime(int desiredCapacity)
{    return PrimeFinder.nextPrime(desiredCapacity);}
0
protected void setUp(int initialCapacity, double minLoadFactor, double maxLoadFactor)
{    if (initialCapacity < 0) {        throw new IllegalArgumentException("Initial Capacity must not be less than zero: " + initialCapacity);    }    if (minLoadFactor < 0.0 || minLoadFactor >= 1.0) {        throw new IllegalArgumentException("Illegal minLoadFactor: " + minLoadFactor);    }    if (maxLoadFactor <= 0.0 || maxLoadFactor >= 1.0) {        throw new IllegalArgumentException("Illegal maxLoadFactor: " + maxLoadFactor);    }    if (minLoadFactor >= maxLoadFactor) {        throw new IllegalArgumentException("Illegal minLoadFactor: " + minLoadFactor + " and maxLoadFactor: " + maxLoadFactor);    }}
0
public int size()
{    return distinct;}
0
public void trimToSize()
{}
0
protected static boolean equalsMindTheNull(Object a, Object b)
{    if (a == null && b == null) {        return true;    }    if (a == null || b == null) {        return false;    }    return a.equals(b);}
0
public static int hash(byte x)
{    return x;}
0
public static int hash(short x)
{    return x;}
0
public static int hash(char x)
{    return x;}
0
public static int hash(int x)
{    return x;}
0
public static int hash(float x)
{    return Float.floatToIntBits(x) >>> 3 + Float.floatToIntBits((float) (Math.PI * x));}
0
public static int hash(double x)
{    return hash(17 * Double.doubleToLongBits(x));}
0
public static int hash(long x)
{    return (int) ((x * 11) >>> 32 ^ x);}
0
public void clear()
{    Arrays.fill(this.state, 0, state.length - 1, FREE);    distinct = 0;        freeEntries = table.length;    trimToSize();}
0
public Object clone()
{    OpenHashSet<T> copy = (OpenHashSet<T>) super.clone();    copy.table = copy.table.clone();    copy.state = copy.state.clone();    return copy;}
0
public boolean contains(Object key)
{    return indexOfKey((T) key) >= 0;}
0
public void ensureCapacity(int minCapacity)
{    if (table.length < minCapacity) {        int newCapacity = nextPrime(minCapacity);        rehash(newCapacity);    }}
0
public boolean forEachKey(ObjectProcedure<T> procedure)
{    for (int i = table.length; i-- > 0; ) {        if (state[i] == FULL) {            if (!procedure.apply((T) table[i])) {                return false;            }        }    }    return true;}
0
protected int indexOfInsertion(T key)
{    Object[] tab = table;    byte[] stat = state;    int length = tab.length;    int hash = key.hashCode() & 0x7FFFFFFF;    int i = hash % length;        int decrement = hash % (length - 2);        if (decrement == 0) {        decrement = 1;    }        while (stat[i] == FULL && tab[i] != key) {        i -= decrement;                if (i < 0) {            i += length;        }    }    if (stat[i] == REMOVED) {                                int j = i;        while (stat[i] != FREE && (stat[i] == REMOVED || tab[i] != key)) {            i -= decrement;                        if (i < 0) {                i += length;            }        }        if (stat[i] == FREE) {            i = j;        }    }    if (stat[i] == FULL) {                return -i - 1;    }        return i;}
0
protected int indexOfKey(T key)
{    Object[] tab = table;    byte[] stat = state;    int length = tab.length;    int hash = key.hashCode() & 0x7FFFFFFF;    int i = hash % length;        int decrement = hash % (length - 2);        if (decrement == 0) {        decrement = 1;    }        while (stat[i] != FREE && (stat[i] == REMOVED || (!key.equals(tab[i])))) {        i -= decrement;                if (i < 0) {            i += length;        }    }    if (stat[i] == FREE) {        return -1;    }        return i;}
0
public void keys(List<T> list)
{    list.clear();    Object[] tab = table;    byte[] stat = state;    for (int i = tab.length; i-- > 0; ) {        if (stat[i] == FULL) {            list.add((T) tab[i]);        }    }}
0
public boolean add(Object key)
{    int i = indexOfInsertion((T) key);    if (i < 0) {                return false;    }    if (this.distinct > this.highWaterMark) {        int newCapacity = chooseGrowCapacity(this.distinct + 1, this.minLoadFactor, this.maxLoadFactor);        rehash(newCapacity);        return add(key);    }    this.table[i] = key;    if (this.state[i] == FREE) {        this.freeEntries--;    }    this.state[i] = FULL;    this.distinct++;    if (this.freeEntries < 1) {                int newCapacity = chooseGrowCapacity(this.distinct + 1, this.minLoadFactor, this.maxLoadFactor);        rehash(newCapacity);        return add(key);    }    return true;}
0
protected void rehash(int newCapacity)
{    int oldCapacity = table.length;        Object[] oldTable = table;    byte[] oldState = state;    Object[] newTable = new Object[newCapacity];    byte[] newState = new byte[newCapacity];    this.lowWaterMark = chooseLowWaterMark(newCapacity, this.minLoadFactor);    this.highWaterMark = chooseHighWaterMark(newCapacity, this.maxLoadFactor);    this.table = newTable;    this.state = newState;        this.freeEntries = newCapacity - this.distinct;    for (int i = oldCapacity; i-- > 0; ) {        if (oldState[i] == FULL) {            Object element = oldTable[i];            int index = indexOfInsertion((T) element);            newTable[index] = element;            newState[index] = FULL;        }    }}
0
public boolean remove(Object key)
{    int i = indexOfKey((T) key);    if (i < 0) {        return false;    }        this.state[i] = REMOVED;    this.distinct--;    if (this.distinct < this.lowWaterMark) {        int newCapacity = chooseShrinkCapacity(this.distinct, this.minLoadFactor, this.maxLoadFactor);        rehash(newCapacity);    }    return true;}
0
protected final void setUp(int initialCapacity, double minLoadFactor, double maxLoadFactor)
{    int capacity = initialCapacity;    super.setUp(capacity, minLoadFactor, maxLoadFactor);    capacity = nextPrime(capacity);    if (capacity == 0) {        capacity = 1;    }        this.table = new Object[capacity];    this.state = new byte[capacity];        this.minLoadFactor = minLoadFactor;    if (capacity == PrimeFinder.LARGEST_PRIME) {        this.maxLoadFactor = 1.0;    } else {        this.maxLoadFactor = maxLoadFactor;    }    this.distinct = 0;        this.freeEntries = capacity;                    this.lowWaterMark = 0;    this.highWaterMark = chooseHighWaterMark(capacity, this.maxLoadFactor);}
0
public void trimToSize()
{            int newCapacity = nextPrime((int) (1 + 1.2 * size()));    if (table.length > newCapacity) {        rehash(newCapacity);    }}
0
 void getInternalFactors(int[] capacity, double[] minLoadFactor, double[] maxLoadFactor)
{    capacity[0] = table.length;    minLoadFactor[0] = this.minLoadFactor;    maxLoadFactor[0] = this.maxLoadFactor;}
0
public boolean isEmpty()
{    return size() == 0;}
0
public boolean equals(Object obj)
{    if (obj == this) {        return true;    }    if (!(obj instanceof OpenHashSet)) {        return false;    }    final OpenHashSet<T> other = (OpenHashSet<T>) obj;    if (other.size() != size()) {        return false;    }    return forEachKey(new ObjectProcedure<T>() {        @Override        public boolean apply(T key) {            return other.contains(key);        }    });}
0
public boolean apply(T key)
{    return other.contains(key);}
0
public int hashCode()
{    ByteBuffer buf = ByteBuffer.allocate(size());    for (int i = 0; i < table.length; i++) {        Object v = table[i];        if (state[i] == FULL) {            buf.putInt(v.hashCode());        }    }    return MurmurHash.hash(buf, this.getClass().getName().hashCode());}
0
public Iterator<T> iterator()
{    List<T> keyList = new ArrayList<>();    keys(keyList);    return keyList.iterator();}
0
public Object[] toArray()
{    List<T> keyList = new ArrayList<>();    keys(keyList);    return keyList.toArray();}
0
public boolean addAll(Collection<? extends T> c)
{    boolean anyAdded = false;    for (T o : c) {        boolean added = add(o);        anyAdded |= added;    }    return anyAdded;}
0
public boolean containsAll(Collection<?> c)
{    for (Object o : c) {        if (!contains(o)) {            return false;        }    }    return true;}
0
public boolean removeAll(Collection<?> c)
{    boolean anyRemoved = false;    for (Object o : c) {        boolean removed = remove(o);        anyRemoved |= removed;    }    return anyRemoved;}
0
public boolean retainAll(Collection<?> c)
{    final Collection<?> finalCollection = c;    final boolean[] modified = new boolean[1];    modified[0] = false;    forEachKey(new ObjectProcedure<T>() {        @Override        public boolean apply(T element) {            if (!finalCollection.contains(element)) {                remove(element);                modified[0] = true;            }            return true;        }    });    return modified[0];}
0
public boolean apply(T element)
{    if (!finalCollection.contains(element)) {        remove(element);        modified[0] = true;    }    return true;}
0
public T1[] toArray(T1[] a)
{    return keys().toArray(a);}
0
public List<T> keys()
{    List<T> keys = new ArrayList<>();    keys(keys);    return keys;}
0
public double cond()
{    return s[0] / s[Math.min(m, n) - 1];}
0
public Matrix getS()
{    double[][] s = new double[n][n];    for (int i = 0; i < n; i++) {        for (int j = 0; j < n; j++) {            s[i][j] = 0.0;        }        s[i][i] = this.s[i];    }    return new DenseMatrix(s);}
0
public double[] getSingularValues()
{    return s;}
0
public Matrix getU()
{    if (transpositionNeeded) {                return new DenseMatrix(v);    } else {        int numCols = Math.min(m + 1, n);        Matrix r = new DenseMatrix(m, numCols);        for (int i = 0; i < m; i++) {            for (int j = 0; j < numCols; j++) {                r.set(i, j, u[i][j]);            }        }        return r;    }}
0
public Matrix getV()
{    if (transpositionNeeded) {                int numCols = Math.min(m + 1, n);        Matrix r = new DenseMatrix(m, numCols);        for (int i = 0; i < m; i++) {            for (int j = 0; j < numCols; j++) {                r.set(i, j, u[i][j]);            }        }        return r;    } else {        return new DenseMatrix(v);    }}
0
public double norm2()
{    return s[0];}
0
public int rank()
{    double eps = Math.pow(2.0, -52.0);    double tol = Math.max(m, n) * s[0] * eps;    int r = 0;    for (double value : s) {        if (value > tol) {            r++;        }    }    return r;}
0
 Matrix getCovariance(double minSingularValue)
{    Matrix j = new DenseMatrix(s.length, s.length);    Matrix vMat = new DenseMatrix(this.v);    for (int i = 0; i < s.length; i++) {        j.set(i, i, s[i] >= minSingularValue ? 1 / (s[i] * s[i]) : 0.0);    }    return vMat.times(j).times(vMat.transpose());}
0
public String toString()
{    StringBuilder buf = new StringBuilder();    buf.append("---------------------------------------------------------------------\n");    buf.append("SingularValueDecomposition(A) --> cond(A), rank(A), norm2(A), U, S, V\n");    buf.append("---------------------------------------------------------------------\n");    buf.append("cond = ");    String unknown = "Illegal operation or error: ";    try {        buf.append(String.valueOf(this.cond()));    } catch (IllegalArgumentException exc) {        buf.append(unknown).append(exc.getMessage());    }    buf.append("\nrank = ");    try {        buf.append(String.valueOf(this.rank()));    } catch (IllegalArgumentException exc) {        buf.append(unknown).append(exc.getMessage());    }    buf.append("\nnorm2 = ");    try {        buf.append(String.valueOf(this.norm2()));    } catch (IllegalArgumentException exc) {        buf.append(unknown).append(exc.getMessage());    }    buf.append("\n\nU = ");    try {        buf.append(String.valueOf(this.getU()));    } catch (IllegalArgumentException exc) {        buf.append(unknown).append(exc.getMessage());    }    buf.append("\n\nS = ");    try {        buf.append(String.valueOf(this.getS()));    } catch (IllegalArgumentException exc) {        buf.append(unknown).append(exc.getMessage());    }    buf.append("\n\nV = ");    try {        buf.append(String.valueOf(this.getV()));    } catch (IllegalArgumentException exc) {        buf.append(unknown).append(exc.getMessage());    }    return buf.toString();}
0
public Vector solve(VectorIterable a, Vector b)
{    return solve(a, b, null, b.size() + 2, DEFAULT_MAX_ERROR);}
0
public Vector solve(VectorIterable a, Vector b, Preconditioner precond)
{    return solve(a, b, precond, b.size() + 2, DEFAULT_MAX_ERROR);}
0
public Vector solve(VectorIterable a, Vector b, Preconditioner preconditioner, int maxIterations, double maxError)
{    if (a.numRows() != a.numCols()) {        throw new IllegalArgumentException("Matrix must be square, symmetric and positive definite.");    }    if (a.numCols() != b.size()) {        throw new CardinalityException(a.numCols(), b.size());    }    if (maxIterations <= 0) {        throw new IllegalArgumentException("Max iterations must be positive.");    }    if (maxError < 0.0) {        throw new IllegalArgumentException("Max error must be non-negative.");    }    Vector x = new DenseVector(b.size());    iterations = 0;    Vector residual = b.minus(a.times(x));    residualNormSquared = residual.dot(residual);        double previousConditionedNormSqr = 0.0;    Vector updateDirection = null;    while (Math.sqrt(residualNormSquared) > maxError && iterations < maxIterations) {        Vector conditionedResidual;        double conditionedNormSqr;        if (preconditioner == null) {            conditionedResidual = residual;            conditionedNormSqr = residualNormSquared;        } else {            conditionedResidual = preconditioner.precondition(residual);            conditionedNormSqr = residual.dot(conditionedResidual);        }        ++iterations;        if (iterations == 1) {            updateDirection = new DenseVector(conditionedResidual);        } else {            double beta = conditionedNormSqr / previousConditionedNormSqr;                        updateDirection.assign(Functions.MULT, beta);            updateDirection.assign(conditionedResidual, Functions.PLUS);        }        Vector aTimesUpdate = a.times(updateDirection);        double alpha = conditionedNormSqr / updateDirection.dot(aTimesUpdate);                PLUS_MULT.setMultiplicator(alpha);        x.assign(updateDirection, PLUS_MULT);                PLUS_MULT.setMultiplicator(-alpha);        residual.assign(aTimesUpdate, PLUS_MULT);        previousConditionedNormSqr = conditionedNormSqr;        residualNormSquared = residual.dot(residual);            }    return x;}
1
public int getIterations()
{    return iterations;}
0
public double getResidualNorm()
{    return Math.sqrt(residualNormSquared);}
0
public Matrix getV()
{    return v.like().assign(v);}
0
public Vector getRealEigenvalues()
{    return d;}
0
public Vector getImagEigenvalues()
{    return e;}
0
public Matrix getD()
{    Matrix x = new DenseMatrix(n, n);    x.assign(0);    x.viewDiagonal().assign(d);    for (int i = 0; i < n; i++) {        double v = e.getQuick(i);        if (v > 0) {            x.setQuick(i, i + 1, v);        } else if (v < 0) {            x.setQuick(i, i - 1, v);        }    }    return x;}
0
private void tred2()
{                    d.assign(v.viewColumn(n - 1));    for (int i = n - 1; i > 0; i--) {                double scale = d.viewPart(0, i).norm(1);        double h = 0.0;        if (scale == 0.0) {            e.setQuick(i, d.getQuick(i - 1));            for (int j = 0; j < i; j++) {                d.setQuick(j, v.getQuick(i - 1, j));                v.setQuick(i, j, 0.0);                v.setQuick(j, i, 0.0);            }        } else {            for (int k = 0; k < i; k++) {                d.setQuick(k, d.getQuick(k) / scale);                h += d.getQuick(k) * d.getQuick(k);            }            double f = d.getQuick(i - 1);            double g = Math.sqrt(h);            if (f > 0) {                g = -g;            }            e.setQuick(i, scale * g);            h -= f * g;            d.setQuick(i - 1, f - g);            for (int j = 0; j < i; j++) {                e.setQuick(j, 0.0);            }            for (int j = 0; j < i; j++) {                f = d.getQuick(j);                v.setQuick(j, i, f);                g = e.getQuick(j) + v.getQuick(j, j) * f;                for (int k = j + 1; k <= i - 1; k++) {                    g += v.getQuick(k, j) * d.getQuick(k);                    e.setQuick(k, e.getQuick(k) + v.getQuick(k, j) * f);                }                e.setQuick(j, g);            }            f = 0.0;            for (int j = 0; j < i; j++) {                e.setQuick(j, e.getQuick(j) / h);                f += e.getQuick(j) * d.getQuick(j);            }            double hh = f / (h + h);            for (int j = 0; j < i; j++) {                e.setQuick(j, e.getQuick(j) - hh * d.getQuick(j));            }            for (int j = 0; j < i; j++) {                f = d.getQuick(j);                g = e.getQuick(j);                for (int k = j; k <= i - 1; k++) {                    v.setQuick(k, j, v.getQuick(k, j) - (f * e.getQuick(k) + g * d.getQuick(k)));                }                d.setQuick(j, v.getQuick(i - 1, j));                v.setQuick(i, j, 0.0);            }        }        d.setQuick(i, h);    }    for (int i = 0; i < n - 1; i++) {        v.setQuick(n - 1, i, v.getQuick(i, i));        v.setQuick(i, i, 1.0);        double h = d.getQuick(i + 1);        if (h != 0.0) {            for (int k = 0; k <= i; k++) {                d.setQuick(k, v.getQuick(k, i + 1) / h);            }            for (int j = 0; j <= i; j++) {                double g = 0.0;                for (int k = 0; k <= i; k++) {                    g += v.getQuick(k, i + 1) * v.getQuick(k, j);                }                for (int k = 0; k <= i; k++) {                    v.setQuick(k, j, v.getQuick(k, j) - g * d.getQuick(k));                }            }        }        for (int k = 0; k <= i; k++) {            v.setQuick(k, i + 1, 0.0);        }    }    d.assign(v.viewRow(n - 1));    v.viewRow(n - 1).assign(0);    v.setQuick(n - 1, n - 1, 1.0);    e.setQuick(0, 0.0);}
0
private void tql2()
{                    e.viewPart(0, n - 1).assign(e.viewPart(1, n - 1));    e.setQuick(n - 1, 0.0);    double f = 0.0;    double tst1 = 0.0;    double eps = Math.pow(2.0, -52.0);    for (int l = 0; l < n; l++) {                tst1 = Math.max(tst1, Math.abs(d.getQuick(l)) + Math.abs(e.getQuick(l)));        int m = l;        while (m < n) {            if (Math.abs(e.getQuick(m)) <= eps * tst1) {                break;            }            m++;        }        if (m > l) {            do {                                double g = d.getQuick(l);                double p = (d.getQuick(l + 1) - g) / (2.0 * e.getQuick(l));                double r = Math.hypot(p, 1.0);                if (p < 0) {                    r = -r;                }                d.setQuick(l, e.getQuick(l) / (p + r));                d.setQuick(l + 1, e.getQuick(l) * (p + r));                double dl1 = d.getQuick(l + 1);                double h = g - d.getQuick(l);                for (int i = l + 2; i < n; i++) {                    d.setQuick(i, d.getQuick(i) - h);                }                f += h;                                p = d.getQuick(m);                double c = 1.0;                double c2 = c;                double c3 = c;                double el1 = e.getQuick(l + 1);                double s = 0.0;                double s2 = 0.0;                for (int i = m - 1; i >= l; i--) {                    c3 = c2;                    c2 = c;                    s2 = s;                    g = c * e.getQuick(i);                    h = c * p;                    r = Math.hypot(p, e.getQuick(i));                    e.setQuick(i + 1, s * r);                    s = e.getQuick(i) / r;                    c = p / r;                    p = c * d.getQuick(i) - s * g;                    d.setQuick(i + 1, h + s * (c * g + s * d.getQuick(i)));                    for (int k = 0; k < n; k++) {                        h = v.getQuick(k, i + 1);                        v.setQuick(k, i + 1, s * v.getQuick(k, i) + c * h);                        v.setQuick(k, i, c * v.getQuick(k, i) - s * h);                    }                }                p = -s * s2 * c3 * el1 * e.getQuick(l) / dl1;                e.setQuick(l, s * p);                d.setQuick(l, c * p);                        } while (Math.abs(e.getQuick(l)) > eps * tst1);        }        d.setQuick(l, d.getQuick(l) + f);        e.setQuick(l, 0.0);    }    for (int i = 0; i < n - 1; i++) {        int k = i;        double p = d.getQuick(i);        for (int j = i + 1; j < n; j++) {            if (d.getQuick(j) > p) {                k = j;                p = d.getQuick(j);            }        }        if (k != i) {            d.setQuick(k, d.getQuick(i));            d.setQuick(i, p);            for (int j = 0; j < n; j++) {                p = v.getQuick(j, i);                v.setQuick(j, i, v.getQuick(j, k));                v.setQuick(j, k, p);            }        }    }}
0
private Matrix orthes(Matrix x)
{        Vector ort = new DenseVector(n);    Matrix hessenBerg = new DenseMatrix(n, n).assign(x);                    int low = 0;    int high = n - 1;    for (int m = low + 1; m <= high - 1; m++) {                Vector hColumn = hessenBerg.viewColumn(m - 1).viewPart(m, high - m + 1);        double scale = hColumn.norm(1);        if (scale != 0.0) {                        ort.viewPart(m, high - m + 1).assign(hColumn, Functions.plusMult(1 / scale));            double h = ort.viewPart(m, high - m + 1).getLengthSquared();            double g = Math.sqrt(h);            if (ort.getQuick(m) > 0) {                g = -g;            }            h -= ort.getQuick(m) * g;            ort.setQuick(m, ort.getQuick(m) - g);                                    Vector ortPiece = ort.viewPart(m, high - m + 1);            for (int j = m; j < n; j++) {                double f = ortPiece.dot(hessenBerg.viewColumn(j).viewPart(m, high - m + 1)) / h;                hessenBerg.viewColumn(j).viewPart(m, high - m + 1).assign(ortPiece, Functions.plusMult(-f));            }            for (int i = 0; i <= high; i++) {                double f = ortPiece.dot(hessenBerg.viewRow(i).viewPart(m, high - m + 1)) / h;                hessenBerg.viewRow(i).viewPart(m, high - m + 1).assign(ortPiece, Functions.plusMult(-f));            }            ort.setQuick(m, scale * ort.getQuick(m));            hessenBerg.setQuick(m, m - 1, scale * g);        }    }        v.assign(0);    v.viewDiagonal().assign(1);    for (int m = high - 1; m >= low + 1; m--) {        if (hessenBerg.getQuick(m, m - 1) != 0.0) {            ort.viewPart(m + 1, high - m).assign(hessenBerg.viewColumn(m - 1).viewPart(m + 1, high - m));            for (int j = m; j <= high; j++) {                double g = ort.viewPart(m, high - m + 1).dot(v.viewColumn(j).viewPart(m, high - m + 1));                                g = g / ort.getQuick(m) / hessenBerg.getQuick(m, m - 1);                v.viewColumn(j).viewPart(m, high - m + 1).assign(ort.viewPart(m, high - m + 1), Functions.plusMult(g));            }        }    }    return hessenBerg;}
0
private void cdiv(double xr, double xi, double yr, double yi)
{    double r;    double d;    if (Math.abs(yr) > Math.abs(yi)) {        r = yi / yr;        d = yr + r * yi;        cdivr = (xr + r * xi) / d;        cdivi = (xi - r * xr) / d;    } else {        r = yr / yi;        d = yi + r * yr;        cdivr = (r * xr + xi) / d;        cdivi = (r * xi - xr) / d;    }}
0
private void hqr2(Matrix h)
{                        int nn = this.n;    int n = nn - 1;    int low = 0;    int high = nn - 1;    double eps = Math.pow(2.0, -52.0);    double exshift = 0.0;    double p = 0;    double q = 0;    double r = 0;    double s = 0;    double z = 0;    double w;    double x;    double y;        double norm = h.aggregate(Functions.PLUS, Functions.ABS);        int iter = 0;    while (n >= low) {                int l = n;        while (l > low) {            s = Math.abs(h.getQuick(l - 1, l - 1)) + Math.abs(h.getQuick(l, l));            if (s == 0.0) {                s = norm;            }            if (Math.abs(h.getQuick(l, l - 1)) < eps * s) {                break;            }            l--;        }        if (l == n) {                        h.setQuick(n, n, h.getQuick(n, n) + exshift);            d.setQuick(n, h.getQuick(n, n));            e.setQuick(n, 0.0);            n--;            iter = 0;        } else if (l == n - 1) {                        w = h.getQuick(n, n - 1) * h.getQuick(n - 1, n);            p = (h.getQuick(n - 1, n - 1) - h.getQuick(n, n)) / 2.0;            q = p * p + w;            z = Math.sqrt(Math.abs(q));            h.setQuick(n, n, h.getQuick(n, n) + exshift);            h.setQuick(n - 1, n - 1, h.getQuick(n - 1, n - 1) + exshift);            x = h.getQuick(n, n);                        if (q >= 0) {                if (p >= 0) {                    z = p + z;                } else {                    z = p - z;                }                d.setQuick(n - 1, x + z);                d.setQuick(n, d.getQuick(n - 1));                if (z != 0.0) {                    d.setQuick(n, x - w / z);                }                e.setQuick(n - 1, 0.0);                e.setQuick(n, 0.0);                x = h.getQuick(n, n - 1);                s = Math.abs(x) + Math.abs(z);                p = x / s;                q = z / s;                r = Math.sqrt(p * p + q * q);                p /= r;                q /= r;                for (int j = n - 1; j < nn; j++) {                    z = h.getQuick(n - 1, j);                    h.setQuick(n - 1, j, q * z + p * h.getQuick(n, j));                    h.setQuick(n, j, q * h.getQuick(n, j) - p * z);                }                for (int i = 0; i <= n; i++) {                    z = h.getQuick(i, n - 1);                    h.setQuick(i, n - 1, q * z + p * h.getQuick(i, n));                    h.setQuick(i, n, q * h.getQuick(i, n) - p * z);                }                for (int i = low; i <= high; i++) {                    z = v.getQuick(i, n - 1);                    v.setQuick(i, n - 1, q * z + p * v.getQuick(i, n));                    v.setQuick(i, n, q * v.getQuick(i, n) - p * z);                }                        } else {                d.setQuick(n - 1, x + p);                d.setQuick(n, x + p);                e.setQuick(n - 1, z);                e.setQuick(n, -z);            }            n -= 2;            iter = 0;                } else {                        x = h.getQuick(n, n);            y = 0.0;            w = 0.0;            if (l < n) {                y = h.getQuick(n - 1, n - 1);                w = h.getQuick(n, n - 1) * h.getQuick(n - 1, n);            }            if (iter == 10) {                exshift += x;                for (int i = low; i <= n; i++) {                    h.setQuick(i, i, x);                }                s = Math.abs(h.getQuick(n, n - 1)) + Math.abs(h.getQuick(n - 1, n - 2));                x = y = 0.75 * s;                w = -0.4375 * s * s;            }            if (iter == 30) {                s = (y - x) / 2.0;                s = s * s + w;                if (s > 0) {                    s = Math.sqrt(s);                    if (y < x) {                        s = -s;                    }                    s = x - w / ((y - x) / 2.0 + s);                    for (int i = low; i <= n; i++) {                        h.setQuick(i, i, h.getQuick(i, i) - s);                    }                    exshift += s;                    x = y = w = 0.964;                }            }                        iter++;                        int m = n - 2;            while (m >= l) {                z = h.getQuick(m, m);                r = x - z;                s = y - z;                p = (r * s - w) / h.getQuick(m + 1, m) + h.getQuick(m, m + 1);                q = h.getQuick(m + 1, m + 1) - z - r - s;                r = h.getQuick(m + 2, m + 1);                s = Math.abs(p) + Math.abs(q) + Math.abs(r);                p /= s;                q /= s;                r /= s;                if (m == l) {                    break;                }                double hmag = Math.abs(h.getQuick(m - 1, m - 1)) + Math.abs(h.getQuick(m + 1, m + 1));                double threshold = eps * Math.abs(p) * (Math.abs(z) + hmag);                if (Math.abs(h.getQuick(m, m - 1)) * (Math.abs(q) + Math.abs(r)) < threshold) {                    break;                }                m--;            }            for (int i = m + 2; i <= n; i++) {                h.setQuick(i, i - 2, 0.0);                if (i > m + 2) {                    h.setQuick(i, i - 3, 0.0);                }            }            for (int k = m; k <= n - 1; k++) {                boolean notlast = k != n - 1;                if (k != m) {                    p = h.getQuick(k, k - 1);                    q = h.getQuick(k + 1, k - 1);                    r = notlast ? h.getQuick(k + 2, k - 1) : 0.0;                    x = Math.abs(p) + Math.abs(q) + Math.abs(r);                    if (x != 0.0) {                        p /= x;                        q /= x;                        r /= x;                    }                }                if (x == 0.0) {                    break;                }                s = Math.sqrt(p * p + q * q + r * r);                if (p < 0) {                    s = -s;                }                if (s != 0) {                    if (k != m) {                        h.setQuick(k, k - 1, -s * x);                    } else if (l != m) {                        h.setQuick(k, k - 1, -h.getQuick(k, k - 1));                    }                    p += s;                    x = p / s;                    y = q / s;                    z = r / s;                    q /= p;                    r /= p;                    for (int j = k; j < nn; j++) {                        p = h.getQuick(k, j) + q * h.getQuick(k + 1, j);                        if (notlast) {                            p += r * h.getQuick(k + 2, j);                            h.setQuick(k + 2, j, h.getQuick(k + 2, j) - p * z);                        }                        h.setQuick(k, j, h.getQuick(k, j) - p * x);                        h.setQuick(k + 1, j, h.getQuick(k + 1, j) - p * y);                    }                    for (int i = 0; i <= Math.min(n, k + 3); i++) {                        p = x * h.getQuick(i, k) + y * h.getQuick(i, k + 1);                        if (notlast) {                            p += z * h.getQuick(i, k + 2);                            h.setQuick(i, k + 2, h.getQuick(i, k + 2) - p * r);                        }                        h.setQuick(i, k, h.getQuick(i, k) - p);                        h.setQuick(i, k + 1, h.getQuick(i, k + 1) - p * q);                    }                    for (int i = low; i <= high; i++) {                        p = x * v.getQuick(i, k) + y * v.getQuick(i, k + 1);                        if (notlast) {                            p += z * v.getQuick(i, k + 2);                            v.setQuick(i, k + 2, v.getQuick(i, k + 2) - p * r);                        }                        v.setQuick(i, k, v.getQuick(i, k) - p);                        v.setQuick(i, k + 1, v.getQuick(i, k + 1) - p * q);                    }                }                        }                }        }    if (norm == 0.0) {        return;    }    for (n = nn - 1; n >= 0; n--) {        p = d.getQuick(n);        q = e.getQuick(n);                double t;        if (q == 0) {            int l = n;            h.setQuick(n, n, 1.0);            for (int i = n - 1; i >= 0; i--) {                w = h.getQuick(i, i) - p;                r = 0.0;                for (int j = l; j <= n; j++) {                    r += h.getQuick(i, j) * h.getQuick(j, n);                }                if (e.getQuick(i) < 0.0) {                    z = w;                    s = r;                } else {                    l = i;                    if (e.getQuick(i) == 0.0) {                        if (w == 0.0) {                            h.setQuick(i, n, -r / (eps * norm));                        } else {                            h.setQuick(i, n, -r / w);                        }                                        } else {                        x = h.getQuick(i, i + 1);                        y = h.getQuick(i + 1, i);                        q = (d.getQuick(i) - p) * (d.getQuick(i) - p) + e.getQuick(i) * e.getQuick(i);                        t = (x * s - z * r) / q;                        h.setQuick(i, n, t);                        if (Math.abs(x) > Math.abs(z)) {                            h.setQuick(i + 1, n, (-r - w * t) / x);                        } else {                            h.setQuick(i + 1, n, (-s - y * t) / z);                        }                    }                                        t = Math.abs(h.getQuick(i, n));                    if (eps * t * t > 1) {                        for (int j = i; j <= n; j++) {                            h.setQuick(j, n, h.getQuick(j, n) / t);                        }                    }                }            }                } else if (q < 0) {            int l = n - 1;            if (Math.abs(h.getQuick(n, n - 1)) > Math.abs(h.getQuick(n - 1, n))) {                h.setQuick(n - 1, n - 1, q / h.getQuick(n, n - 1));                h.setQuick(n - 1, n, -(h.getQuick(n, n) - p) / h.getQuick(n, n - 1));            } else {                cdiv(0.0, -h.getQuick(n - 1, n), h.getQuick(n - 1, n - 1) - p, q);                h.setQuick(n - 1, n - 1, cdivr);                h.setQuick(n - 1, n, cdivi);            }            h.setQuick(n, n - 1, 0.0);            h.setQuick(n, n, 1.0);            for (int i = n - 2; i >= 0; i--) {                double ra = 0.0;                double sa = 0.0;                for (int j = l; j <= n; j++) {                    ra += h.getQuick(i, j) * h.getQuick(j, n - 1);                    sa += h.getQuick(i, j) * h.getQuick(j, n);                }                w = h.getQuick(i, i) - p;                if (e.getQuick(i) < 0.0) {                    z = w;                    r = ra;                    s = sa;                } else {                    l = i;                    if (e.getQuick(i) == 0) {                        cdiv(-ra, -sa, w, q);                        h.setQuick(i, n - 1, cdivr);                        h.setQuick(i, n, cdivi);                    } else {                                                x = h.getQuick(i, i + 1);                        y = h.getQuick(i + 1, i);                        double vr = (d.getQuick(i) - p) * (d.getQuick(i) - p) + e.getQuick(i) * e.getQuick(i) - q * q;                        double vi = (d.getQuick(i) - p) * 2.0 * q;                        if (vr == 0.0 && vi == 0.0) {                            double hmag = Math.abs(x) + Math.abs(y);                            vr = eps * norm * (Math.abs(w) + Math.abs(q) + hmag + Math.abs(z));                        }                        cdiv(x * r - z * ra + q * sa, x * s - z * sa - q * ra, vr, vi);                        h.setQuick(i, n - 1, cdivr);                        h.setQuick(i, n, cdivi);                        if (Math.abs(x) > (Math.abs(z) + Math.abs(q))) {                            h.setQuick(i + 1, n - 1, (-ra - w * h.getQuick(i, n - 1) + q * h.getQuick(i, n)) / x);                            h.setQuick(i + 1, n, (-sa - w * h.getQuick(i, n) - q * h.getQuick(i, n - 1)) / x);                        } else {                            cdiv(-r - y * h.getQuick(i, n - 1), -s - y * h.getQuick(i, n), z, q);                            h.setQuick(i + 1, n - 1, cdivr);                            h.setQuick(i + 1, n, cdivi);                        }                    }                                        t = Math.max(Math.abs(h.getQuick(i, n - 1)), Math.abs(h.getQuick(i, n)));                    if (eps * t * t > 1) {                        for (int j = i; j <= n; j++) {                            h.setQuick(j, n - 1, h.getQuick(j, n - 1) / t);                            h.setQuick(j, n, h.getQuick(j, n) / t);                        }                    }                }            }        }    }    for (int i = 0; i < nn; i++) {        if (i < low || i > high) {            for (int j = i; j < nn; j++) {                v.setQuick(i, j, h.getQuick(i, j));            }        }    }    for (int j = nn - 1; j >= low; j--) {        for (int i = low; i <= high; i++) {            z = 0.0;            for (int k = low; k <= Math.min(j, high); k++) {                z += v.getQuick(i, k) * h.getQuick(k, j);            }            v.setQuick(i, j, z);        }    }}
0
private static boolean isSymmetric(Matrix a)
{    /*    Symmetry flag.    */    int n = a.columnSize();    boolean isSymmetric = true;    for (int j = 0; (j < n) && isSymmetric; j++) {        for (int i = 0; (i < n) && isSymmetric; i++) {            isSymmetric = a.getQuick(i, j) == a.getQuick(j, i);        }    }    return isSymmetric;}
0
public Vector precondition(Vector v)
{    return v.times(inverseDiagonal);}
0
public int getIterationCount()
{    return iteration;}
0
public double getResidualNorm()
{    return residualNorm;}
0
public double getNormalEquationResidual()
{    return normalEquationResidual;}
0
public double getANorm()
{    return normA;}
0
public double getCondition()
{    return condA;}
0
public double getXNorm()
{    return xNorm;}
0
public Vector solve(Matrix A, Vector b)
{    /*        % Initialize.        hdg1 = '   itn      x(1)       norm r    norm A''r';        hdg2 = ' compatible   LS      norm A   cond A';        pfreq  = 20;   % print frequency (for repeating the heading)        pcount = 0;    % print counter        % Determine dimensions m and n, and        % form the first vectors u and v.        % These satisfy  beta*u = b,  alpha*v = A'u.    */            Matrix transposedA = A.transpose();    Vector u = b;    double beta = u.norm(2);    if (beta > 0) {        u = u.divide(beta);    }    Vector v = transposedA.times(u);    int m = A.numRows();    int n = A.numCols();    int minDim = Math.min(m, n);    if (iterationLimit == -1) {        iterationLimit = minDim;    }    if (log.isDebugEnabled()) {                    }    double alpha = v.norm(2);    if (alpha > 0) {        v.assign(Functions.div(alpha));    }        localPointer = 0;                    localV = new Vector[Math.min(localSize, minDim)];    boolean localOrtho = false;    if (localSize > 0) {        localOrtho = true;        localV[0] = v;    }        iteration = 0;    double zetabar = alpha * beta;    double alphabar = alpha;    Vector h = v;    Vector hbar = zeros(n);    Vector x = zeros(n);        double betadd = beta;        double aNorm = alpha * alpha;        double normb = beta;    double ctol = 0;    if (conditionLimit > 0) {        ctol = 1 / conditionLimit;    }    residualNorm = beta;        normalEquationResidual = alpha * beta;    if (normalEquationResidual == 0) {        return x;    }    if (log.isDebugEnabled()) {        double test2 = alpha / beta;                                double test1 = 1;            }                double rho = 1;    double rhobar = 1;    double cbar = 1;    double sbar = 0;    double betad = 0;    double rhodold = 1;    double tautildeold = 0;    double thetatilde = 0;    double zeta = 0;    double d = 0;    double maxrbar = 0;    double minrbar = 1.0e+100;    StopCode stop = StopCode.CONTINUE;    while (iteration <= iterationLimit && stop == StopCode.CONTINUE) {        iteration++;                                        u = A.times(v).minus(u.times(alpha));        beta = u.norm(2);        if (beta > 0) {            u.assign(Functions.div(beta));                        if (localOrtho) {                localVEnqueue(v);            }            v = transposedA.times(u).minus(v.times(beta));                        if (localOrtho) {                v = localVOrtho(v);            }            alpha = v.norm(2);            if (alpha > 0) {                v.assign(Functions.div(alpha));            }        }                        double alphahat = Math.hypot(alphabar, lambda);        double chat = alphabar / alphahat;        double shat = lambda / alphahat;                double rhoold = rho;        rho = Math.hypot(alphahat, beta);        double c = alphahat / rho;        double s = beta / rho;        double thetanew = s * alpha;        alphabar = c * alpha;                double rhobarold = rhobar;        double zetaold = zeta;        double thetabar = sbar * rho;        double rhotemp = cbar * rho;        rhobar = Math.hypot(cbar * rho, thetanew);        cbar = cbar * rho / rhobar;        sbar = thetanew / rhobar;        zeta = cbar * zetabar;        zetabar = -sbar * zetabar;                hbar = h.minus(hbar.times(thetabar * rho / (rhoold * rhobarold)));        x.assign(hbar.times(zeta / (rho * rhobar)), Functions.PLUS);        h = v.minus(h.times(thetanew / rho));                        double betaacute = chat * betadd;        double betacheck = -shat * betadd;                double betahat = c * betaacute;        betadd = -s * betaacute;                        double thetatildeold = thetatilde;        double rhotildeold = Math.hypot(rhodold, thetabar);        double ctildeold = rhodold / rhotildeold;        double stildeold = thetabar / rhotildeold;        thetatilde = stildeold * rhobar;        rhodold = ctildeold * rhobar;        betad = -stildeold * betad + ctildeold * betahat;                        tautildeold = (zetaold - thetatildeold * tautildeold) / rhotildeold;        double taud = (zeta - thetatilde * tautildeold) / rhodold;        d += betacheck * betacheck;        residualNorm = Math.sqrt(d + (betad - taud) * (betad - taud) + betadd * betadd);                aNorm += beta * beta;        normA = Math.sqrt(aNorm);        aNorm += alpha * alpha;                maxrbar = Math.max(maxrbar, rhobarold);        if (iteration > 1) {            minrbar = Math.min(minrbar, rhobarold);        }        condA = Math.max(maxrbar, rhotemp) / Math.min(minrbar, rhotemp);                        normalEquationResidual = Math.abs(zetabar);        xNorm = x.norm(2);                        double test1 = residualNorm / normb;        double test2 = normalEquationResidual / (normA * residualNorm);        double test3 = 1 / condA;        double t1 = test1 / (1 + normA * xNorm / normb);        double rtol = bTolerance + aTolerance * normA * xNorm / normb;        if (iteration > iterationLimit) {            stop = StopCode.ITERATION_LIMIT;        }        if (1 + test3 <= 1) {            stop = StopCode.CONDITION_MACHINE_TOLERANCE;        }        if (1 + test2 <= 1) {            stop = StopCode.LEAST_SQUARE_CONVERGED_MACHINE_TOLERANCE;        }        if (1 + t1 <= 1) {            stop = StopCode.CONVERGED_MACHINE_TOLERANCE;        }        if (test3 <= ctol) {            stop = StopCode.CONDITION;        }        if (test2 <= aTolerance) {            stop = StopCode.CONVERGED;        }        if (test1 <= rtol) {            stop = StopCode.TRIVIAL;        }                if (log.isDebugEnabled()) {            if ((n <= 40) || (iteration <= 10) || (iteration >= iterationLimit - 10) || ((iteration % 10) == 0) || (test3 <= 1.1 * ctol) || (test2 <= 1.1 * aTolerance) || (test1 <= 1.1 * rtol) || (stop != StopCode.CONTINUE)) {                statusDump(x, normA, condA, test1, test2);            }        }    }                return x;/*    if show      fprintf('\n\nLSMR finished')      fprintf('\n%s', msg(istop+1,:))      fprintf('\nistop =%8g    normr =%8.1e'     , istop, normr )      fprintf('    normA =%8.1e    normAr =%8.1e', normA, normAr)      fprintf('\nitn   =%8g    condA =%8.1e'     , itn  , condA )      fprintf('    normx =%8.1e\n', normx)    end    */}
1
private void statusDump(Vector x, double normA, double condA, double test1, double test2)
{                }
1
private static Vector zeros(int n)
{    return new DenseVector(n);}
0
private void localVEnqueue(Vector v)
{    if (localV.length > 0) {        localV[localPointer] = v;        localPointer = (localPointer + 1) % localV.length;    }}
0
private Vector localVOrtho(Vector v)
{    for (Vector old : localV) {        if (old != null) {            double x = v.dot(old);            v = v.minus(old.times(x));        }    }    return v;}
0
public String getMessage()
{    return message;}
0
public void setAtolerance(double aTolerance)
{    this.aTolerance = aTolerance;}
0
public void setBtolerance(double bTolerance)
{    this.bTolerance = bTolerance;}
0
public void setConditionLimit(double conditionLimit)
{    this.conditionLimit = conditionLimit;}
0
public void setIterationLimit(int iterationLimit)
{    this.iterationLimit = iterationLimit;}
0
public void setLocalSize(int localSize)
{    this.localSize = localSize;}
0
public double getLambda()
{    return lambda;}
0
public double getAtolerance()
{    return aTolerance;}
0
public double getBtolerance()
{    return bTolerance;}
0
private static int med3(T[] array, int a, int b, int c, Comparator<T> comp)
{    T x = array[a];    T y = array[b];    T z = array[c];    int comparisonxy = comp.compare(x, y);    int comparisonxz = comp.compare(x, z);    int comparisonyz = comp.compare(y, z);    return comparisonxy < 0 ? (comparisonyz < 0 ? b : (comparisonxz < 0 ? c : a)) : (comparisonyz > 0 ? b : (comparisonxz > 0 ? c : a));}
0
private static int med3(byte[] array, int a, int b, int c, ByteComparator comp)
{    byte x = array[a];    byte y = array[b];    byte z = array[c];    int comparisonxy = comp.compare(x, y);    int comparisonxz = comp.compare(x, z);    int comparisonyz = comp.compare(y, z);    return comparisonxy < 0 ? (comparisonyz < 0 ? b : (comparisonxz < 0 ? c : a)) : (comparisonyz > 0 ? b : (comparisonxz > 0 ? c : a));}
0
private static int med3(char[] array, int a, int b, int c, CharComparator comp)
{    char x = array[a];    char y = array[b];    char z = array[c];    int comparisonxy = comp.compare(x, y);    int comparisonxz = comp.compare(x, z);    int comparisonyz = comp.compare(y, z);    return comparisonxy < 0 ? (comparisonyz < 0 ? b : (comparisonxz < 0 ? c : a)) : (comparisonyz > 0 ? b : (comparisonxz > 0 ? c : a));}
0
private static int med3(double[] array, int a, int b, int c, DoubleComparator comp)
{    double x = array[a];    double y = array[b];    double z = array[c];    int comparisonxy = comp.compare(x, y);    int comparisonxz = comp.compare(x, z);    int comparisonyz = comp.compare(y, z);    return comparisonxy < 0 ? (comparisonyz < 0 ? b : (comparisonxz < 0 ? c : a)) : (comparisonyz > 0 ? b : (comparisonxz > 0 ? c : a));}
0
private static int med3(float[] array, int a, int b, int c, FloatComparator comp)
{    float x = array[a];    float y = array[b];    float z = array[c];    int comparisonxy = comp.compare(x, y);    int comparisonxz = comp.compare(x, z);    int comparisonyz = comp.compare(y, z);    return comparisonxy < 0 ? (comparisonyz < 0 ? b : (comparisonxz < 0 ? c : a)) : (comparisonyz > 0 ? b : (comparisonxz > 0 ? c : a));}
0
private static int med3(int[] array, int a, int b, int c, IntComparator comp)
{    int x = array[a];    int y = array[b];    int z = array[c];    int comparisonxy = comp.compare(x, y);    int comparisonxz = comp.compare(x, z);    int comparisonyz = comp.compare(y, z);    return comparisonxy < 0 ? (comparisonyz < 0 ? b : (comparisonxz < 0 ? c : a)) : (comparisonyz > 0 ? b : (comparisonxz > 0 ? c : a));}
0
private static int med3(int a, int b, int c, IntComparator comp)
{    int comparisonab = comp.compare(a, b);    int comparisonac = comp.compare(a, c);    int comparisonbc = comp.compare(b, c);    return comparisonab < 0 ? (comparisonbc < 0 ? b : (comparisonac < 0 ? c : a)) : (comparisonbc > 0 ? b : (comparisonac > 0 ? c : a));}
0
private static int med3(long[] array, int a, int b, int c, LongComparator comp)
{    long x = array[a];    long y = array[b];    long z = array[c];    int comparisonxy = comp.compare(x, y);    int comparisonxz = comp.compare(x, z);    int comparisonyz = comp.compare(y, z);    return comparisonxy < 0 ? (comparisonyz < 0 ? b : (comparisonxz < 0 ? c : a)) : (comparisonyz > 0 ? b : (comparisonxz > 0 ? c : a));}
0
private static int med3(short[] array, int a, int b, int c, ShortComparator comp)
{    short x = array[a];    short y = array[b];    short z = array[c];    int comparisonxy = comp.compare(x, y);    int comparisonxz = comp.compare(x, z);    int comparisonyz = comp.compare(y, z);    return comparisonxy < 0 ? (comparisonyz < 0 ? b : (comparisonxz < 0 ? c : a)) : (comparisonyz > 0 ? b : (comparisonxz > 0 ? c : a));}
0
public static void quickSort(byte[] array, int start, int end, ByteComparator comp)
{    Preconditions.checkNotNull(array);    checkBounds(array.length, start, end);    quickSort0(start, end, array, comp);}
0
private static void checkBounds(int arrLength, int start, int end)
{    if (start > end) {                throw new IllegalArgumentException("Start index " + start + " is greater than end index " + end);    }    if (start < 0) {        throw new ArrayIndexOutOfBoundsException("Array index out of range " + start);    }    if (end > arrLength) {        throw new ArrayIndexOutOfBoundsException("Array index out of range " + end);    }}
0
private static void quickSort0(int start, int end, byte[] array, ByteComparator comp)
{    byte temp;    int length = end - start;    if (length < 7) {        for (int i = start + 1; i < end; i++) {            for (int j = i; j > start && comp.compare(array[j - 1], array[j]) > 0; j--) {                temp = array[j];                array[j] = array[j - 1];                array[j - 1] = temp;            }        }        return;    }    int middle = (start + end) / 2;    if (length > 7) {        int bottom = start;        int top = end - 1;        if (length > 40) {            length /= 8;            bottom = med3(array, bottom, bottom + length, bottom + (2 * length), comp);            middle = med3(array, middle - length, middle, middle + length, comp);            top = med3(array, top - (2 * length), top - length, top, comp);        }        middle = med3(array, bottom, middle, top, comp);    }    byte partionValue = array[middle];    int a = start;    int b = a;    int c = end - 1;    int d = c;    while (true) {        int comparison;        while (b <= c && (comparison = comp.compare(array[b], partionValue)) <= 0) {            if (comparison == 0) {                temp = array[a];                array[a++] = array[b];                array[b] = temp;            }            b++;        }        while (c >= b && (comparison = comp.compare(array[c], partionValue)) >= 0) {            if (comparison == 0) {                temp = array[c];                array[c] = array[d];                array[d--] = temp;            }            c--;        }        if (b > c) {            break;        }        temp = array[b];        array[b++] = array[c];        array[c--] = temp;    }    length = a - start < b - a ? a - start : b - a;    int l = start;    int h = b - length;    while (length-- > 0) {        temp = array[l];        array[l++] = array[h];        array[h++] = temp;    }    length = d - c < end - 1 - d ? d - c : end - 1 - d;    l = b;    h = end - length;    while (length-- > 0) {        temp = array[l];        array[l++] = array[h];        array[h++] = temp;    }    if ((length = b - a) > 0) {        quickSort0(start, start + length, array, comp);    }    if ((length = d - c) > 0) {        quickSort0(end - length, end, array, comp);    }}
0
public static void quickSort(int start, int end, IntComparator comp, Swapper swap)
{    checkBounds(end + 1, start, end);    quickSort0(start, end, comp, swap);}
0
private static void quickSort0(int start, int end, IntComparator comp, Swapper swap)
{    int length = end - start;    if (length < 7) {        insertionSort(start, end, comp, swap);        return;    }    int middle = (start + end) / 2;    if (length > 7) {        int bottom = start;        int top = end - 1;        if (length > 40) {                        int skosh = length / 8;            bottom = med3(bottom, bottom + skosh, bottom + (2 * skosh), comp);            middle = med3(middle - skosh, middle, middle + skosh, comp);            top = med3(top - (2 * skosh), top - skosh, top, comp);        }        middle = med3(bottom, middle, top, comp);    }        int partitionIndex = middle;        int a = start;    int b = a;    int c = end - 1;    int d = c;    while (b <= c) {                        int comparison;        while (b <= c && (comparison = comp.compare(b, partitionIndex)) <= 0) {            if (comparison == 0) {                if (a == partitionIndex) {                    partitionIndex = b;                } else if (b == partitionIndex) {                    partitionIndex = a;                }                swap.swap(a, b);                a++;            }            b++;        }        while (c >= b && (comparison = comp.compare(c, partitionIndex)) >= 0) {            if (comparison == 0) {                if (c == partitionIndex) {                    partitionIndex = d;                } else if (d == partitionIndex) {                    partitionIndex = c;                }                swap.swap(c, d);                d--;            }            c--;        }        if (b <= c) {                        if (c == partitionIndex) {                partitionIndex = b;            } else if (b == partitionIndex) {                partitionIndex = d;            }            swap.swap(b, c);            b++;            c--;        }    }                            length = Math.min(a - start, b - a);    int l = start;    int h = b - length;    while (length-- > 0) {        swap.swap(l, h);        l++;        h++;    }        length = Math.min(d - c, end - 1 - d);    l = b;    h = end - length;    while (length-- > 0) {        swap.swap(l, h);        l++;        h++;    }        length = b - a;    if (length > 0) {        quickSort0(start, start + length, comp, swap);    }    length = d - c;    if (length > 0) {        quickSort0(end - length, end, comp, swap);    }}
0
private static void insertionSort(int start, int end, IntComparator comp, Swapper swap)
{    for (int i = start + 1; i < end; i++) {        for (int j = i; j > start && comp.compare(j - 1, j) > 0; j--) {            swap.swap(j - 1, j);        }    }}
0
public static void quickSort(char[] array, int start, int end, CharComparator comp)
{    Preconditions.checkNotNull(array);    checkBounds(array.length, start, end);    quickSort0(start, end, array, comp);}
0
private static void quickSort0(int start, int end, char[] array, CharComparator comp)
{    char temp;    int length = end - start;    if (length < 7) {        for (int i = start + 1; i < end; i++) {            for (int j = i; j > start && comp.compare(array[j - 1], array[j]) > 0; j--) {                temp = array[j];                array[j] = array[j - 1];                array[j - 1] = temp;            }        }        return;    }    int middle = (start + end) / 2;    if (length > 7) {        int bottom = start;        int top = end - 1;        if (length > 40) {            length /= 8;            bottom = med3(array, bottom, bottom + length, bottom + (2 * length), comp);            middle = med3(array, middle - length, middle, middle + length, comp);            top = med3(array, top - (2 * length), top - length, top, comp);        }        middle = med3(array, bottom, middle, top, comp);    }    char partionValue = array[middle];    int a = start;    int b = a;    int c = end - 1;    int d = c;    while (true) {        int comparison;        while (b <= c && (comparison = comp.compare(array[b], partionValue)) <= 0) {            if (comparison == 0) {                temp = array[a];                array[a++] = array[b];                array[b] = temp;            }            b++;        }        while (c >= b && (comparison = comp.compare(array[c], partionValue)) >= 0) {            if (comparison == 0) {                temp = array[c];                array[c] = array[d];                array[d--] = temp;            }            c--;        }        if (b > c) {            break;        }        temp = array[b];        array[b++] = array[c];        array[c--] = temp;    }    length = a - start < b - a ? a - start : b - a;    int l = start;    int h = b - length;    while (length-- > 0) {        temp = array[l];        array[l++] = array[h];        array[h++] = temp;    }    length = d - c < end - 1 - d ? d - c : end - 1 - d;    l = b;    h = end - length;    while (length-- > 0) {        temp = array[l];        array[l++] = array[h];        array[h++] = temp;    }    if ((length = b - a) > 0) {        quickSort0(start, start + length, array, comp);    }    if ((length = d - c) > 0) {        quickSort0(end - length, end, array, comp);    }}
0
public static void quickSort(double[] array, int start, int end, DoubleComparator comp)
{    Preconditions.checkNotNull(array);    checkBounds(array.length, start, end);    quickSort0(start, end, array, comp);}
0
private static void quickSort0(int start, int end, double[] array, DoubleComparator comp)
{    double temp;    int length = end - start;    if (length < 7) {        for (int i = start + 1; i < end; i++) {            for (int j = i; j > start && comp.compare(array[j], array[j - 1]) < 0; j--) {                temp = array[j];                array[j] = array[j - 1];                array[j - 1] = temp;            }        }        return;    }    int middle = (start + end) / 2;    if (length > 7) {        int bottom = start;        int top = end - 1;        if (length > 40) {            length /= 8;            bottom = med3(array, bottom, bottom + length, bottom + (2 * length), comp);            middle = med3(array, middle - length, middle, middle + length, comp);            top = med3(array, top - (2 * length), top - length, top, comp);        }        middle = med3(array, bottom, middle, top, comp);    }    double partionValue = array[middle];    int a = start;    int b = a;    int c = end - 1;    int d = c;    while (true) {        int comparison;        while (b <= c && (comparison = comp.compare(partionValue, array[b])) >= 0) {            if (comparison == 0) {                temp = array[a];                array[a++] = array[b];                array[b] = temp;            }            b++;        }        while (c >= b && (comparison = comp.compare(array[c], partionValue)) >= 0) {            if (comparison == 0) {                temp = array[c];                array[c] = array[d];                array[d--] = temp;            }            c--;        }        if (b > c) {            break;        }        temp = array[b];        array[b++] = array[c];        array[c--] = temp;    }    length = a - start < b - a ? a - start : b - a;    int l = start;    int h = b - length;    while (length-- > 0) {        temp = array[l];        array[l++] = array[h];        array[h++] = temp;    }    length = d - c < end - 1 - d ? d - c : end - 1 - d;    l = b;    h = end - length;    while (length-- > 0) {        temp = array[l];        array[l++] = array[h];        array[h++] = temp;    }    if ((length = b - a) > 0) {        quickSort0(start, start + length, array, comp);    }    if ((length = d - c) > 0) {        quickSort0(end - length, end, array, comp);    }}
0
public static void quickSort(float[] array, int start, int end, FloatComparator comp)
{    Preconditions.checkNotNull(array);    checkBounds(array.length, start, end);    quickSort0(start, end, array, comp);}
0
private static void quickSort0(int start, int end, float[] array, FloatComparator comp)
{    float temp;    int length = end - start;    if (length < 7) {        for (int i = start + 1; i < end; i++) {            for (int j = i; j > start && comp.compare(array[j], array[j - 1]) < 0; j--) {                temp = array[j];                array[j] = array[j - 1];                array[j - 1] = temp;            }        }        return;    }    int middle = (start + end) / 2;    if (length > 7) {        int bottom = start;        int top = end - 1;        if (length > 40) {            length /= 8;            bottom = med3(array, bottom, bottom + length, bottom + (2 * length), comp);            middle = med3(array, middle - length, middle, middle + length, comp);            top = med3(array, top - (2 * length), top - length, top, comp);        }        middle = med3(array, bottom, middle, top, comp);    }    float partionValue = array[middle];    int a = start;    int b = a;    int c = end - 1;    int d = c;    while (true) {        int comparison;        while (b <= c && (comparison = comp.compare(partionValue, array[b])) >= 0) {            if (comparison == 0) {                temp = array[a];                array[a++] = array[b];                array[b] = temp;            }            b++;        }        while (c >= b && (comparison = comp.compare(array[c], partionValue)) >= 0) {            if (comparison == 0) {                temp = array[c];                array[c] = array[d];                array[d--] = temp;            }            c--;        }        if (b > c) {            break;        }        temp = array[b];        array[b++] = array[c];        array[c--] = temp;    }    length = a - start < b - a ? a - start : b - a;    int l = start;    int h = b - length;    while (length-- > 0) {        temp = array[l];        array[l++] = array[h];        array[h++] = temp;    }    length = d - c < end - 1 - d ? d - c : end - 1 - d;    l = b;    h = end - length;    while (length-- > 0) {        temp = array[l];        array[l++] = array[h];        array[h++] = temp;    }    if ((length = b - a) > 0) {        quickSort0(start, start + length, array, comp);    }    if ((length = d - c) > 0) {        quickSort0(end - length, end, array, comp);    }}
0
public static void quickSort(int[] array, int start, int end, IntComparator comp)
{    Preconditions.checkNotNull(array);    checkBounds(array.length, start, end);    quickSort0(start, end, array, comp);}
0
private static void quickSort0(int start, int end, int[] array, IntComparator comp)
{    int temp;    int length = end - start;    if (length < 7) {        for (int i = start + 1; i < end; i++) {            for (int j = i; j > start && comp.compare(array[j - 1], array[j]) > 0; j--) {                temp = array[j];                array[j] = array[j - 1];                array[j - 1] = temp;            }        }        return;    }    int middle = (start + end) / 2;    if (length > 7) {        int bottom = start;        int top = end - 1;        if (length > 40) {            length /= 8;            bottom = med3(array, bottom, bottom + length, bottom + (2 * length), comp);            middle = med3(array, middle - length, middle, middle + length, comp);            top = med3(array, top - (2 * length), top - length, top, comp);        }        middle = med3(array, bottom, middle, top, comp);    }    int partionValue = array[middle];    int a = start;    int b = a;    int c = end - 1;    int d = c;    while (true) {        int comparison;        while (b <= c && (comparison = comp.compare(array[b], partionValue)) <= 0) {            if (comparison == 0) {                temp = array[a];                array[a++] = array[b];                array[b] = temp;            }            b++;        }        while (c >= b && (comparison = comp.compare(array[c], partionValue)) >= 0) {            if (comparison == 0) {                temp = array[c];                array[c] = array[d];                array[d--] = temp;            }            c--;        }        if (b > c) {            break;        }        temp = array[b];        array[b++] = array[c];        array[c--] = temp;    }    length = a - start < b - a ? a - start : b - a;    int l = start;    int h = b - length;    while (length-- > 0) {        temp = array[l];        array[l++] = array[h];        array[h++] = temp;    }    length = d - c < end - 1 - d ? d - c : end - 1 - d;    l = b;    h = end - length;    while (length-- > 0) {        temp = array[l];        array[l++] = array[h];        array[h++] = temp;    }    if ((length = b - a) > 0) {        quickSort0(start, start + length, array, comp);    }    if ((length = d - c) > 0) {        quickSort0(end - length, end, array, comp);    }}
0
public static void quickSort(long[] array, int start, int end, LongComparator comp)
{    Preconditions.checkNotNull(array);    checkBounds(array.length, start, end);    quickSort0(start, end, array, comp);}
0
private static void quickSort0(int start, int end, long[] array, LongComparator comp)
{    long temp;    int length = end - start;    if (length < 7) {        for (int i = start + 1; i < end; i++) {            for (int j = i; j > start && comp.compare(array[j - 1], array[j]) > 0; j--) {                temp = array[j];                array[j] = array[j - 1];                array[j - 1] = temp;            }        }        return;    }    int middle = (start + end) / 2;    if (length > 7) {        int bottom = start;        int top = end - 1;        if (length > 40) {            length /= 8;            bottom = med3(array, bottom, bottom + length, bottom + (2 * length), comp);            middle = med3(array, middle - length, middle, middle + length, comp);            top = med3(array, top - (2 * length), top - length, top, comp);        }        middle = med3(array, bottom, middle, top, comp);    }    long partionValue = array[middle];    int a = start;    int b = a;    int c = end - 1;    int d = c;    while (true) {        int comparison;        while (b <= c && (comparison = comp.compare(array[b], partionValue)) <= 0) {            if (comparison == 0) {                temp = array[a];                array[a++] = array[b];                array[b] = temp;            }            b++;        }        while (c >= b && (comparison = comp.compare(array[c], partionValue)) >= 0) {            if (comparison == 0) {                temp = array[c];                array[c] = array[d];                array[d--] = temp;            }            c--;        }        if (b > c) {            break;        }        temp = array[b];        array[b++] = array[c];        array[c--] = temp;    }    length = a - start < b - a ? a - start : b - a;    int l = start;    int h = b - length;    while (length-- > 0) {        temp = array[l];        array[l++] = array[h];        array[h++] = temp;    }    length = d - c < end - 1 - d ? d - c : end - 1 - d;    l = b;    h = end - length;    while (length-- > 0) {        temp = array[l];        array[l++] = array[h];        array[h++] = temp;    }    if ((length = b - a) > 0) {        quickSort0(start, start + length, array, comp);    }    if ((length = d - c) > 0) {        quickSort0(end - length, end, array, comp);    }}
0
public static void quickSort(T[] array, int start, int end, Comparator<T> comp)
{    Preconditions.checkNotNull(array);    checkBounds(array.length, start, end);    quickSort0(start, end, array, comp);}
0
public int compare(T o1, T o2)
{    return o1.compareTo(o2);}
0
public static void quickSort(T[] array, int start, int end)
{    quickSort(array, start, end, new ComparableAdaptor<T>());}
0
private static void quickSort0(int start, int end, T[] array, Comparator<T> comp)
{    T temp;    int length = end - start;    if (length < 7) {        for (int i = start + 1; i < end; i++) {            for (int j = i; j > start && comp.compare(array[j - 1], array[j]) > 0; j--) {                temp = array[j];                array[j] = array[j - 1];                array[j - 1] = temp;            }        }        return;    }    int middle = (start + end) / 2;    if (length > 7) {        int bottom = start;        int top = end - 1;        if (length > 40) {            length /= 8;            bottom = med3(array, bottom, bottom + length, bottom + (2 * length), comp);            middle = med3(array, middle - length, middle, middle + length, comp);            top = med3(array, top - (2 * length), top - length, top, comp);        }        middle = med3(array, bottom, middle, top, comp);    }    T partionValue = array[middle];    int a = start;    int b = a;    int c = end - 1;    int d = c;    while (true) {        int comparison;        while (b <= c && (comparison = comp.compare(array[b], partionValue)) <= 0) {            if (comparison == 0) {                temp = array[a];                array[a++] = array[b];                array[b] = temp;            }            b++;        }        while (c >= b && (comparison = comp.compare(array[c], partionValue)) >= 0) {            if (comparison == 0) {                temp = array[c];                array[c] = array[d];                array[d--] = temp;            }            c--;        }        if (b > c) {            break;        }        temp = array[b];        array[b++] = array[c];        array[c--] = temp;    }    length = a - start < b - a ? a - start : b - a;    int l = start;    int h = b - length;    while (length-- > 0) {        temp = array[l];        array[l++] = array[h];        array[h++] = temp;    }    length = d - c < end - 1 - d ? d - c : end - 1 - d;    l = b;    h = end - length;    while (length-- > 0) {        temp = array[l];        array[l++] = array[h];        array[h++] = temp;    }    if ((length = b - a) > 0) {        quickSort0(start, start + length, array, comp);    }    if ((length = d - c) > 0) {        quickSort0(end - length, end, array, comp);    }}
0
public static void quickSort(short[] array, int start, int end, ShortComparator comp)
{    Preconditions.checkNotNull(array);    checkBounds(array.length, start, end);    quickSort0(start, end, array, comp);}
0
private static void quickSort0(int start, int end, short[] array, ShortComparator comp)
{    short temp;    int length = end - start;    if (length < 7) {        for (int i = start + 1; i < end; i++) {            for (int j = i; j > start && comp.compare(array[j - 1], array[j]) > 0; j--) {                temp = array[j];                array[j] = array[j - 1];                array[j - 1] = temp;            }        }        return;    }    int middle = (start + end) / 2;    if (length > 7) {        int bottom = start;        int top = end - 1;        if (length > 40) {            length /= 8;            bottom = med3(array, bottom, bottom + length, bottom + (2 * length), comp);            middle = med3(array, middle - length, middle, middle + length, comp);            top = med3(array, top - (2 * length), top - length, top, comp);        }        middle = med3(array, bottom, middle, top, comp);    }    short partionValue = array[middle];    int a = start;    int b = a;    int c = end - 1;    int d = c;    while (true) {        int comparison;        while (b <= c && (comparison = comp.compare(array[b], partionValue)) < 0) {            if (comparison == 0) {                temp = array[a];                array[a++] = array[b];                array[b] = temp;            }            b++;        }        while (c >= b && (comparison = comp.compare(array[c], partionValue)) > 0) {            if (comparison == 0) {                temp = array[c];                array[c] = array[d];                array[d--] = temp;            }            c--;        }        if (b > c) {            break;        }        temp = array[b];        array[b++] = array[c];        array[c--] = temp;    }    length = a - start < b - a ? a - start : b - a;    int l = start;    int h = b - length;    while (length-- > 0) {        temp = array[l];        array[l++] = array[h];        array[h++] = temp;    }    length = d - c < end - 1 - d ? d - c : end - 1 - d;    l = b;    h = end - length;    while (length-- > 0) {        temp = array[l];        array[l++] = array[h];        array[h++] = temp;    }    if ((length = b - a) > 0) {        quickSort0(start, start + length, array, comp);    }    if ((length = d - c) > 0) {        quickSort0(end - length, end, array, comp);    }}
0
public static void mergeSort(T[] array, int start, int end, Comparator<T> comp)
{    checkBounds(array.length, start, end);    int length = end - start;    if (length <= 0) {        return;    }    T[] out = (T[]) new Object[array.length];    System.arraycopy(array, start, out, start, length);    mergeSort(out, array, start, end, comp);}
0
public static void mergeSort(T[] array, int start, int end)
{    mergeSort(array, start, end, new ComparableAdaptor<T>());}
0
private static void mergeSort(T[] in, T[] out, int start, int end, Comparator<T> c)
{    int len = end - start;        if (len <= SIMPLE_LENGTH) {        for (int i = start + 1; i < end; i++) {            T current = out[i];            T prev = out[i - 1];            if (c.compare(prev, current) > 0) {                int j = i;                do {                    out[j--] = prev;                } while (j > start && (c.compare(prev = out[j - 1], current) > 0));                out[j] = current;            }        }        return;    }    int med = (end + start) >>> 1;    mergeSort(out, in, start, med, c);    mergeSort(out, in, med, end, c);        if (c.compare(in[med - 1], in[med]) <= 0) {        System.arraycopy(in, start, out, start, len);        return;    }    int r = med;    int i = start;        do {        T fromVal = in[start];        T rVal = in[r];        if (c.compare(fromVal, rVal) <= 0) {            int l_1 = find(in, rVal, -1, start + 1, med - 1, c);            int toCopy = l_1 - start + 1;            System.arraycopy(in, start, out, i, toCopy);            i += toCopy;            out[i++] = rVal;            r++;            start = l_1 + 1;        } else {            int r_1 = find(in, fromVal, 0, r + 1, end - 1, c);            int toCopy = r_1 - r + 1;            System.arraycopy(in, r, out, i, toCopy);            i += toCopy;            out[i++] = fromVal;            start++;            r = r_1 + 1;        }    } while ((end - r) > 0 && (med - start) > 0);        if ((end - r) <= 0) {        System.arraycopy(in, start, out, i, med - start);    } else {        System.arraycopy(in, r, out, i, end - r);    }}
0
private static int find(T[] arr, T val, int bnd, int l, int r, Comparator<T> c)
{    int m = l;    int d = 1;    while (m <= r) {        if (c.compare(val, arr[m]) > bnd) {            l = m + 1;        } else {            r = m - 1;            break;        }        m += d;        d <<= 1;    }    while (l <= r) {        m = (l + r) >>> 1;        if (c.compare(val, arr[m]) > bnd) {            l = m + 1;        } else {            r = m - 1;        }    }    return l - 1;}
0
public int compare(byte o1, byte o2)
{    return o1 - o2;}
0
public static void mergeSort(byte[] array, int start, int end)
{    mergeSort(array, start, end, NATURAL_BYTE_COMPARISON);}
0
public static void mergeSort(byte[] array, int start, int end, ByteComparator comp)
{    checkBounds(array.length, start, end);    byte[] out = Arrays.copyOf(array, array.length);    mergeSort(out, array, start, end, comp);}
0
private static void mergeSort(byte[] in, byte[] out, int start, int end, ByteComparator c)
{    int len = end - start;        if (len <= SIMPLE_LENGTH) {        for (int i = start + 1; i < end; i++) {            byte current = out[i];            byte prev = out[i - 1];            if (c.compare(prev, current) > 0) {                int j = i;                do {                    out[j--] = prev;                } while (j > start && (c.compare(prev = out[j - 1], current) > 0));                out[j] = current;            }        }        return;    }    int med = (end + start) >>> 1;    mergeSort(out, in, start, med, c);    mergeSort(out, in, med, end, c);        if (c.compare(in[med - 1], in[med]) <= 0) {        System.arraycopy(in, start, out, start, len);        return;    }    int r = med;    int i = start;        do {        byte fromVal = in[start];        byte rVal = in[r];        if (c.compare(fromVal, rVal) <= 0) {            int l_1 = find(in, rVal, -1, start + 1, med - 1, c);            int toCopy = l_1 - start + 1;            System.arraycopy(in, start, out, i, toCopy);            i += toCopy;            out[i++] = rVal;            r++;            start = l_1 + 1;        } else {            int r_1 = find(in, fromVal, 0, r + 1, end - 1, c);            int toCopy = r_1 - r + 1;            System.arraycopy(in, r, out, i, toCopy);            i += toCopy;            out[i++] = fromVal;            start++;            r = r_1 + 1;        }    } while ((end - r) > 0 && (med - start) > 0);        if ((end - r) <= 0) {        System.arraycopy(in, start, out, i, med - start);    } else {        System.arraycopy(in, r, out, i, end - r);    }}
0
private static int find(byte[] arr, byte val, int bnd, int l, int r, ByteComparator c)
{    int m = l;    int d = 1;    while (m <= r) {        if (c.compare(val, arr[m]) > bnd) {            l = m + 1;        } else {            r = m - 1;            break;        }        m += d;        d <<= 1;    }    while (l <= r) {        m = (l + r) >>> 1;        if (c.compare(val, arr[m]) > bnd) {            l = m + 1;        } else {            r = m - 1;        }    }    return l - 1;}
0
public int compare(char o1, char o2)
{    return o1 - o2;}
0
public static void mergeSort(char[] array, int start, int end)
{    mergeSort(array, start, end, NATURAL_CHAR_COMPARISON);}
0
public static void mergeSort(char[] array, int start, int end, CharComparator comp)
{    checkBounds(array.length, start, end);    char[] out = Arrays.copyOf(array, array.length);    mergeSort(out, array, start, end, comp);}
0
private static void mergeSort(char[] in, char[] out, int start, int end, CharComparator c)
{    int len = end - start;        if (len <= SIMPLE_LENGTH) {        for (int i = start + 1; i < end; i++) {            char current = out[i];            char prev = out[i - 1];            if (c.compare(prev, current) > 0) {                int j = i;                do {                    out[j--] = prev;                } while (j > start && (c.compare(prev = out[j - 1], current) > 0));                out[j] = current;            }        }        return;    }    int med = (end + start) >>> 1;    mergeSort(out, in, start, med, c);    mergeSort(out, in, med, end, c);        if (c.compare(in[med - 1], in[med]) <= 0) {        System.arraycopy(in, start, out, start, len);        return;    }    int r = med;    int i = start;        do {        char fromVal = in[start];        char rVal = in[r];        if (c.compare(fromVal, rVal) <= 0) {            int l_1 = find(in, rVal, -1, start + 1, med - 1, c);            int toCopy = l_1 - start + 1;            System.arraycopy(in, start, out, i, toCopy);            i += toCopy;            out[i++] = rVal;            r++;            start = l_1 + 1;        } else {            int r_1 = find(in, fromVal, 0, r + 1, end - 1, c);            int toCopy = r_1 - r + 1;            System.arraycopy(in, r, out, i, toCopy);            i += toCopy;            out[i++] = fromVal;            start++;            r = r_1 + 1;        }    } while ((end - r) > 0 && (med - start) > 0);        if ((end - r) <= 0) {        System.arraycopy(in, start, out, i, med - start);    } else {        System.arraycopy(in, r, out, i, end - r);    }}
0
private static int find(char[] arr, char val, int bnd, int l, int r, CharComparator c)
{    int m = l;    int d = 1;    while (m <= r) {        if (c.compare(val, arr[m]) > bnd) {            l = m + 1;        } else {            r = m - 1;            break;        }        m += d;        d <<= 1;    }    while (l <= r) {        m = (l + r) >>> 1;        if (c.compare(val, arr[m]) > bnd) {            l = m + 1;        } else {            r = m - 1;        }    }    return l - 1;}
0
public int compare(short o1, short o2)
{    return o1 - o2;}
0
public static void mergeSort(short[] array, int start, int end)
{    mergeSort(array, start, end, NATURAL_SHORT_COMPARISON);}
0
public static void mergeSort(short[] array, int start, int end, ShortComparator comp)
{    checkBounds(array.length, start, end);    short[] out = Arrays.copyOf(array, array.length);    mergeSort(out, array, start, end, comp);}
0
private static void mergeSort(short[] in, short[] out, int start, int end, ShortComparator c)
{    int len = end - start;        if (len <= SIMPLE_LENGTH) {        for (int i = start + 1; i < end; i++) {            short current = out[i];            short prev = out[i - 1];            if (c.compare(prev, current) > 0) {                int j = i;                do {                    out[j--] = prev;                } while (j > start && (c.compare(prev = out[j - 1], current) > 0));                out[j] = current;            }        }        return;    }    int med = (end + start) >>> 1;    mergeSort(out, in, start, med, c);    mergeSort(out, in, med, end, c);        if (c.compare(in[med - 1], in[med]) <= 0) {        System.arraycopy(in, start, out, start, len);        return;    }    int r = med;    int i = start;        do {        short fromVal = in[start];        short rVal = in[r];        if (c.compare(fromVal, rVal) <= 0) {            int l_1 = find(in, rVal, -1, start + 1, med - 1, c);            int toCopy = l_1 - start + 1;            System.arraycopy(in, start, out, i, toCopy);            i += toCopy;            out[i++] = rVal;            r++;            start = l_1 + 1;        } else {            int r_1 = find(in, fromVal, 0, r + 1, end - 1, c);            int toCopy = r_1 - r + 1;            System.arraycopy(in, r, out, i, toCopy);            i += toCopy;            out[i++] = fromVal;            start++;            r = r_1 + 1;        }    } while ((end - r) > 0 && (med - start) > 0);        if ((end - r) <= 0) {        System.arraycopy(in, start, out, i, med - start);    } else {        System.arraycopy(in, r, out, i, end - r);    }}
0
private static int find(short[] arr, short val, int bnd, int l, int r, ShortComparator c)
{    int m = l;    int d = 1;    while (m <= r) {        if (c.compare(val, arr[m]) > bnd) {            l = m + 1;        } else {            r = m - 1;            break;        }        m += d;        d <<= 1;    }    while (l <= r) {        m = (l + r) >>> 1;        if (c.compare(val, arr[m]) > bnd) {            l = m + 1;        } else {            r = m - 1;        }    }    return l - 1;}
0
public int compare(int o1, int o2)
{    return o1 < o2 ? -1 : o1 > o2 ? 1 : 0;}
0
public static void mergeSort(int[] array, int start, int end)
{    mergeSort(array, start, end, NATURAL_INT_COMPARISON);}
0
public static void mergeSort(int[] array, int start, int end, IntComparator comp)
{    checkBounds(array.length, start, end);    int[] out = Arrays.copyOf(array, array.length);    mergeSort(out, array, start, end, comp);}
0
private static void mergeSort(int[] in, int[] out, int start, int end, IntComparator c)
{    int len = end - start;        if (len <= SIMPLE_LENGTH) {        for (int i = start + 1; i < end; i++) {            int current = out[i];            int prev = out[i - 1];            if (c.compare(prev, current) > 0) {                int j = i;                do {                    out[j--] = prev;                } while (j > start && (c.compare(prev = out[j - 1], current) > 0));                out[j] = current;            }        }        return;    }    int med = (end + start) >>> 1;    mergeSort(out, in, start, med, c);    mergeSort(out, in, med, end, c);        if (c.compare(in[med - 1], in[med]) <= 0) {        System.arraycopy(in, start, out, start, len);        return;    }    int r = med;    int i = start;        do {        int fromVal = in[start];        int rVal = in[r];        if (c.compare(fromVal, rVal) <= 0) {            int l_1 = find(in, rVal, -1, start + 1, med - 1, c);            int toCopy = l_1 - start + 1;            System.arraycopy(in, start, out, i, toCopy);            i += toCopy;            out[i++] = rVal;            r++;            start = l_1 + 1;        } else {            int r_1 = find(in, fromVal, 0, r + 1, end - 1, c);            int toCopy = r_1 - r + 1;            System.arraycopy(in, r, out, i, toCopy);            i += toCopy;            out[i++] = fromVal;            start++;            r = r_1 + 1;        }    } while ((end - r) > 0 && (med - start) > 0);        if ((end - r) <= 0) {        System.arraycopy(in, start, out, i, med - start);    } else {        System.arraycopy(in, r, out, i, end - r);    }}
0
private static int find(int[] arr, int val, int bnd, int l, int r, IntComparator c)
{    int m = l;    int d = 1;    while (m <= r) {        if (c.compare(val, arr[m]) > bnd) {            l = m + 1;        } else {            r = m - 1;            break;        }        m += d;        d <<= 1;    }    while (l <= r) {        m = (l + r) >>> 1;        if (c.compare(val, arr[m]) > bnd) {            l = m + 1;        } else {            r = m - 1;        }    }    return l - 1;}
0
public int compare(long o1, long o2)
{    return o1 < o2 ? -1 : o1 > o2 ? 1 : 0;}
0
public static void mergeSort(long[] array, int start, int end)
{    mergeSort(array, start, end, NATURAL_LONG_COMPARISON);}
0
public static void mergeSort(long[] array, int start, int end, LongComparator comp)
{    checkBounds(array.length, start, end);    long[] out = Arrays.copyOf(array, array.length);    mergeSort(out, array, start, end, comp);}
0
private static void mergeSort(long[] in, long[] out, int start, int end, LongComparator c)
{    int len = end - start;        if (len <= SIMPLE_LENGTH) {        for (int i = start + 1; i < end; i++) {            long current = out[i];            long prev = out[i - 1];            if (c.compare(prev, current) > 0) {                int j = i;                do {                    out[j--] = prev;                } while (j > start && (c.compare(prev = out[j - 1], current) > 0));                out[j] = current;            }        }        return;    }    int med = (end + start) >>> 1;    mergeSort(out, in, start, med, c);    mergeSort(out, in, med, end, c);        if (c.compare(in[med - 1], in[med]) <= 0) {        System.arraycopy(in, start, out, start, len);        return;    }    int r = med;    int i = start;        do {        long fromVal = in[start];        long rVal = in[r];        if (c.compare(fromVal, rVal) <= 0) {            int l_1 = find(in, rVal, -1, start + 1, med - 1, c);            int toCopy = l_1 - start + 1;            System.arraycopy(in, start, out, i, toCopy);            i += toCopy;            out[i++] = rVal;            r++;            start = l_1 + 1;        } else {            int r_1 = find(in, fromVal, 0, r + 1, end - 1, c);            int toCopy = r_1 - r + 1;            System.arraycopy(in, r, out, i, toCopy);            i += toCopy;            out[i++] = fromVal;            start++;            r = r_1 + 1;        }    } while ((end - r) > 0 && (med - start) > 0);        if ((end - r) <= 0) {        System.arraycopy(in, start, out, i, med - start);    } else {        System.arraycopy(in, r, out, i, end - r);    }}
0
private static int find(long[] arr, long val, int bnd, int l, int r, LongComparator c)
{    int m = l;    int d = 1;    while (m <= r) {        if (c.compare(val, arr[m]) > bnd) {            l = m + 1;        } else {            r = m - 1;            break;        }        m += d;        d <<= 1;    }    while (l <= r) {        m = (l + r) >>> 1;        if (c.compare(val, arr[m]) > bnd) {            l = m + 1;        } else {            r = m - 1;        }    }    return l - 1;}
0
public int compare(float o1, float o2)
{    return Float.compare(o1, o2);}
0
public static void mergeSort(float[] array, int start, int end)
{    mergeSort(array, start, end, NATURAL_FLOAT_COMPARISON);}
0
public static void mergeSort(float[] array, int start, int end, FloatComparator comp)
{    checkBounds(array.length, start, end);    float[] out = Arrays.copyOf(array, array.length);    mergeSort(out, array, start, end, comp);}
0
private static void mergeSort(float[] in, float[] out, int start, int end, FloatComparator c)
{    int len = end - start;        if (len <= SIMPLE_LENGTH) {        for (int i = start + 1; i < end; i++) {            float current = out[i];            float prev = out[i - 1];            if (c.compare(prev, current) > 0) {                int j = i;                do {                    out[j--] = prev;                } while (j > start && (c.compare(prev = out[j - 1], current) > 0));                out[j] = current;            }        }        return;    }    int med = (end + start) >>> 1;    mergeSort(out, in, start, med, c);    mergeSort(out, in, med, end, c);        if (c.compare(in[med - 1], in[med]) <= 0) {        System.arraycopy(in, start, out, start, len);        return;    }    int r = med;    int i = start;        do {        float fromVal = in[start];        float rVal = in[r];        if (c.compare(fromVal, rVal) <= 0) {            int l_1 = find(in, rVal, -1, start + 1, med - 1, c);            int toCopy = l_1 - start + 1;            System.arraycopy(in, start, out, i, toCopy);            i += toCopy;            out[i++] = rVal;            r++;            start = l_1 + 1;        } else {            int r_1 = find(in, fromVal, 0, r + 1, end - 1, c);            int toCopy = r_1 - r + 1;            System.arraycopy(in, r, out, i, toCopy);            i += toCopy;            out[i++] = fromVal;            start++;            r = r_1 + 1;        }    } while ((end - r) > 0 && (med - start) > 0);        if ((end - r) <= 0) {        System.arraycopy(in, start, out, i, med - start);    } else {        System.arraycopy(in, r, out, i, end - r);    }}
0
private static int find(float[] arr, float val, int bnd, int l, int r, FloatComparator c)
{    int m = l;    int d = 1;    while (m <= r) {        if (c.compare(val, arr[m]) > bnd) {            l = m + 1;        } else {            r = m - 1;            break;        }        m += d;        d <<= 1;    }    while (l <= r) {        m = (l + r) >>> 1;        if (c.compare(val, arr[m]) > bnd) {            l = m + 1;        } else {            r = m - 1;        }    }    return l - 1;}
0
public int compare(double o1, double o2)
{    return Double.compare(o1, o2);}
0
public static void mergeSort(double[] array, int start, int end)
{    mergeSort(array, start, end, NATURAL_DOUBLE_COMPARISON);}
0
public static void mergeSort(double[] array, int start, int end, DoubleComparator comp)
{    checkBounds(array.length, start, end);    double[] out = Arrays.copyOf(array, array.length);    mergeSort(out, array, start, end, comp);}
0
private static void mergeSort(double[] in, double[] out, int start, int end, DoubleComparator c)
{    int len = end - start;        if (len <= SIMPLE_LENGTH) {        for (int i = start + 1; i < end; i++) {            double current = out[i];            double prev = out[i - 1];            if (c.compare(prev, current) > 0) {                int j = i;                do {                    out[j--] = prev;                } while (j > start && (c.compare(prev = out[j - 1], current) > 0));                out[j] = current;            }        }        return;    }    int med = (end + start) >>> 1;    mergeSort(out, in, start, med, c);    mergeSort(out, in, med, end, c);        if (c.compare(in[med - 1], in[med]) <= 0) {        System.arraycopy(in, start, out, start, len);        return;    }    int r = med;    int i = start;        do {        double fromVal = in[start];        double rVal = in[r];        if (c.compare(fromVal, rVal) <= 0) {            int l_1 = find(in, rVal, -1, start + 1, med - 1, c);            int toCopy = l_1 - start + 1;            System.arraycopy(in, start, out, i, toCopy);            i += toCopy;            out[i++] = rVal;            r++;            start = l_1 + 1;        } else {            int r_1 = find(in, fromVal, 0, r + 1, end - 1, c);            int toCopy = r_1 - r + 1;            System.arraycopy(in, r, out, i, toCopy);            i += toCopy;            out[i++] = fromVal;            start++;            r = r_1 + 1;        }    } while ((end - r) > 0 && (med - start) > 0);        if ((end - r) <= 0) {        System.arraycopy(in, start, out, i, med - start);    } else {        System.arraycopy(in, r, out, i, end - r);    }}
0
private static int find(double[] arr, double val, int bnd, int l, int r, DoubleComparator c)
{    int m = l;    int d = 1;    while (m <= r) {        if (c.compare(val, arr[m]) > bnd) {            l = m + 1;        } else {            r = m - 1;            break;        }        m += d;        d <<= 1;    }    while (l <= r) {        m = (l + r) >>> 1;        if (c.compare(val, arr[m]) > bnd) {            l = m + 1;        } else {            r = m - 1;        }    }    return l - 1;}
0
 static void inplaceMerge(int first, int middle, int last, IntComparator comp, Swapper swapper)
{    if (first >= middle || middle >= last) {        return;    }    if (last - first == 2) {        if (comp.compare(middle, first) < 0) {            swapper.swap(first, middle);        }        return;    }    int firstCut;    int secondCut;    if (middle - first > last - middle) {        firstCut = first + (middle - first) / 2;        secondCut = lowerBound(middle, last, firstCut, comp);    } else {        secondCut = middle + (last - middle) / 2;        firstCut = upperBound(first, middle, secondCut, comp);    }                        int first2 = firstCut;    int middle2 = middle;    int last2 = secondCut;    if (middle2 != first2 && middle2 != last2) {        int first1 = first2;        int last1 = middle2;        while (first1 < --last1) {            swapper.swap(first1++, last1);        }        first1 = middle2;        last1 = last2;        while (first1 < --last1) {            swapper.swap(first1++, last1);        }        first1 = first2;        last1 = last2;        while (first1 < --last1) {            swapper.swap(first1++, last1);        }    }        middle = firstCut + (secondCut - middle);    inplaceMerge(first, firstCut, middle, comp, swapper);    inplaceMerge(middle, secondCut, last, comp, swapper);}
0
 static int lowerBound(int first, int last, int x, IntComparator comp)
{    int len = last - first;    while (len > 0) {        int half = len / 2;        int middle = first + half;        if (comp.compare(middle, x) < 0) {            first = middle + 1;            len -= half + 1;        } else {            len = half;        }    }    return first;}
0
public static void mergeSort(int fromIndex, int toIndex, IntComparator c, Swapper swapper)
{    /*      We retain the same method signature as quickSort.      Given only a comparator and swapper we do not know how to copy and move elements from/to temporary arrays.      Hence, in contrast to the JDK mergesorts this is an "in-place" mergesort, i.e. does not allocate any temporary      arrays.      A non-inplace mergesort would perhaps be faster in most cases, but would require non-intuitive delegate objects...    */    int length = toIndex - fromIndex;        if (length < SMALL) {        for (int i = fromIndex; i < toIndex; i++) {            for (int j = i; j > fromIndex && (c.compare(j - 1, j) > 0); j--) {                swapper.swap(j, j - 1);            }        }        return;    }        int mid = (fromIndex + toIndex) / 2;    mergeSort(fromIndex, mid, c, swapper);    mergeSort(mid, toIndex, c, swapper);        if (c.compare(mid - 1, mid) <= 0) {        return;    }        inplaceMerge(fromIndex, mid, toIndex, c, swapper);}
0
 static int upperBound(int first, int last, int x, IntComparator comp)
{    int len = last - first;    while (len > 0) {        int half = len / 2;        int middle = first + half;        if (comp.compare(x, middle) < 0) {            len = half;        } else {            first = middle + 1;            len -= half + 1;        }    }    return first;}
0
public Matrix clone()
{    SparseColumnMatrix clone = (SparseColumnMatrix) super.clone();    clone.columnVectors = new Vector[columnVectors.length];    for (int i = 0; i < columnVectors.length; i++) {        clone.columnVectors[i] = columnVectors[i].clone();    }    return clone;}
0
public int numSlices()
{    return numCols();}
0
public double getQuick(int row, int column)
{    return columnVectors[column] == null ? 0.0 : columnVectors[column].getQuick(row);}
0
public Matrix like()
{    return new SparseColumnMatrix(rowSize(), columnSize());}
0
public Matrix like(int rows, int columns)
{    return new SparseColumnMatrix(rows, columns);}
0
public void setQuick(int row, int column, double value)
{    if (columnVectors[column] == null) {        columnVectors[column] = new RandomAccessSparseVector(rowSize());    }    columnVectors[column].setQuick(row, value);}
0
public int[] getNumNondefaultElements()
{    int[] result = new int[2];    result[COL] = columnVectors.length;    for (int col = 0; col < columnSize(); col++) {        result[ROW] = Math.max(result[ROW], columnVectors[col].getNumNondefaultElements());    }    return result;}
0
public Matrix viewPart(int[] offset, int[] size)
{    if (offset[ROW] < 0) {        throw new IndexException(offset[ROW], columnVectors[COL].size());    }    if (offset[ROW] + size[ROW] > columnVectors[COL].size()) {        throw new IndexException(offset[ROW] + size[ROW], columnVectors[COL].size());    }    if (offset[COL] < 0) {        throw new IndexException(offset[COL], columnVectors.length);    }    if (offset[COL] + size[COL] > columnVectors.length) {        throw new IndexException(offset[COL] + size[COL], columnVectors.length);    }    return new MatrixView(this, offset, size);}
0
public Matrix assignColumn(int column, Vector other)
{    if (rowSize() != other.size()) {        throw new CardinalityException(rowSize(), other.size());    }    if (column < 0 || column >= columnSize()) {        throw new IndexException(column, columnSize());    }    columnVectors[column].assign(other);    return this;}
0
public Matrix assignRow(int row, Vector other)
{    if (columnSize() != other.size()) {        throw new CardinalityException(columnSize(), other.size());    }    if (row < 0 || row >= rowSize()) {        throw new IndexException(row, rowSize());    }    for (int col = 0; col < columnSize(); col++) {        columnVectors[col].setQuick(row, other.getQuick(col));    }    return this;}
0
public Vector viewColumn(int column)
{    if (column < 0 || column >= columnSize()) {        throw new IndexException(column, columnSize());    }    return columnVectors[column];}
0
public Matrix transpose()
{    SparseRowMatrix srm = new SparseRowMatrix(columns, rows);    for (int i = 0; i < columns; i++) {        Vector col = columnVectors[i];        if (col.getNumNonZeroElements() > 0)                        srm.assignRow(i, col);    }    return srm;}
0
public String toString()
{    int row = 0;    int maxRowsToDisplay = 10;    int maxColsToDisplay = 20;    int colsToDisplay = maxColsToDisplay;    if (maxColsToDisplay > columnSize()) {        colsToDisplay = columnSize();    }    StringBuilder s = new StringBuilder("{\n");    for (MatrixSlice next : this.transpose()) {        if (row < maxRowsToDisplay) {            s.append(" ").append(next.index()).append(" =>\t").append(new VectorView(next.vector(), 0, colsToDisplay)).append('\n');            row++;        }    }    String returnString = s.toString();    if (maxColsToDisplay <= columnSize()) {        returnString = returnString.replace("}", " ... }");    }    if (maxRowsToDisplay <= rowSize()) {        return returnString + "... }";    } else {        return returnString + "}";    }}
0
public Matrix clone()
{    SparseMatrix clone = new SparseMatrix(numRows(), numCols());    for (MatrixSlice slice : this) {        clone.rowVectors.put(slice.index(), slice.clone());    }    return clone;}
0
public int numSlices()
{    return rowVectors.size();}
0
public Iterator<MatrixSlice> iterateNonEmpty()
{    final int[] keys = rowVectors.keySet().toIntArray();    return new AbstractIterator<MatrixSlice>() {        private int slice;        @Override        protected MatrixSlice computeNext() {            if (slice >= rowVectors.size()) {                return endOfData();            }            int i = keys[slice];            Vector row = rowVectors.get(i);            slice++;            return new MatrixSlice(row, i);        }    };}
0
protected MatrixSlice computeNext()
{    if (slice >= rowVectors.size()) {        return endOfData();    }    int i = keys[slice];    Vector row = rowVectors.get(i);    slice++;    return new MatrixSlice(row, i);}
0
public double getQuick(int row, int column)
{    Vector r = rowVectors.get(row);    return r == null ? 0.0 : r.getQuick(column);}
0
public Matrix like()
{    return new SparseMatrix(rowSize(), columnSize());}
0
public Matrix like(int rows, int columns)
{    return new SparseMatrix(rows, columns);}
0
public void setQuick(int row, int column, double value)
{    Vector r = rowVectors.get(row);    if (r == null) {        r = new RandomAccessSparseVector(columnSize());        rowVectors.put(row, r);    }    r.setQuick(column, value);}
0
public int[] getNumNondefaultElements()
{    int[] result = new int[2];    result[ROW] = rowVectors.size();    for (Vector row : rowVectors.values()) {        result[COL] = Math.max(result[COL], row.getNumNondefaultElements());    }    return result;}
0
public Matrix viewPart(int[] offset, int[] size)
{    if (offset[ROW] < 0) {        throw new IndexException(offset[ROW], rowSize());    }    if (offset[ROW] + size[ROW] > rowSize()) {        throw new IndexException(offset[ROW] + size[ROW], rowSize());    }    if (offset[COL] < 0) {        throw new IndexException(offset[COL], columnSize());    }    if (offset[COL] + size[COL] > columnSize()) {        throw new IndexException(offset[COL] + size[COL], columnSize());    }    return new MatrixView(this, offset, size);}
0
public Matrix assign(Matrix other, DoubleDoubleFunction function)
{        if (Functions.PLUS.equals(function) && other instanceof SparseMatrix) {        int rows = rowSize();        if (rows != other.rowSize()) {            throw new CardinalityException(rows, other.rowSize());        }        int columns = columnSize();        if (columns != other.columnSize()) {            throw new CardinalityException(columns, other.columnSize());        }        SparseMatrix otherSparse = (SparseMatrix) other;        for (ObjectIterator<Entry<Vector>> fastIterator = otherSparse.rowVectors.int2ObjectEntrySet().fastIterator(); fastIterator.hasNext(); ) {            final Entry<Vector> entry = fastIterator.next();            final int rowIndex = entry.getIntKey();            Vector row = rowVectors.get(rowIndex);            if (row == null) {                rowVectors.put(rowIndex, entry.getValue().clone());            } else {                row.assign(entry.getValue(), Functions.PLUS);            }        }        return this;    } else {        return super.assign(other, function);    }}
0
public Matrix assignColumn(int column, Vector other)
{    if (rowSize() != other.size()) {        throw new CardinalityException(rowSize(), other.size());    }    if (column < 0 || column >= columnSize()) {        throw new IndexException(column, columnSize());    }    for (int row = 0; row < rowSize(); row++) {        double val = other.getQuick(row);        if (val != 0.0) {            Vector r = rowVectors.get(row);            if (r == null) {                r = new RandomAccessSparseVector(columnSize());                rowVectors.put(row, r);            }            r.setQuick(column, val);        }    }    return this;}
0
public Matrix assignRow(int row, Vector other)
{    if (columnSize() != other.size()) {        throw new CardinalityException(columnSize(), other.size());    }    if (row < 0 || row >= rowSize()) {        throw new IndexException(row, rowSize());    }    rowVectors.put(row, other);    return this;}
0
public Vector viewRow(int row)
{    if (row < 0 || row >= rowSize()) {        throw new IndexException(row, rowSize());    }    Vector res = rowVectors.get(row);    if (res == null) {        res = new RandomAccessSparseVector(columnSize());        rowVectors.put(row, res);    }    return res;}
0
public IntArrayList nonZeroRowIndices()
{    return new IntArrayList(rowVectors.keySet().toIntArray());}
0
public MatrixFlavor getFlavor()
{    return MatrixFlavor.SPARSEROWLIKE;}
0
public Matrix clone()
{    SparseRowMatrix clone = (SparseRowMatrix) super.clone();    clone.rowVectors = new Vector[rowVectors.length];    for (int i = 0; i < rowVectors.length; i++) {        clone.rowVectors[i] = rowVectors[i].clone();    }    return clone;}
0
public double getQuick(int row, int column)
{    return rowVectors[row] == null ? 0.0 : rowVectors[row].getQuick(column);}
0
public Matrix like()
{    return new SparseRowMatrix(rowSize(), columnSize(), randomAccessRows);}
0
public Matrix like(int rows, int columns)
{    return new SparseRowMatrix(rows, columns, randomAccessRows);}
0
public void setQuick(int row, int column, double value)
{    rowVectors[row].setQuick(column, value);}
0
public int[] getNumNondefaultElements()
{    int[] result = new int[2];    result[ROW] = rowVectors.length;    for (int row = 0; row < rowSize(); row++) {        result[COL] = Math.max(result[COL], rowVectors[row].getNumNondefaultElements());    }    return result;}
0
public Matrix viewPart(int[] offset, int[] size)
{    if (offset[ROW] < 0) {        throw new IndexException(offset[ROW], rowVectors.length);    }    if (offset[ROW] + size[ROW] > rowVectors.length) {        throw new IndexException(offset[ROW] + size[ROW], rowVectors.length);    }    if (offset[COL] < 0) {        throw new IndexException(offset[COL], rowVectors[ROW].size());    }    if (offset[COL] + size[COL] > rowVectors[ROW].size()) {        throw new IndexException(offset[COL] + size[COL], rowVectors[ROW].size());    }    return new MatrixView(this, offset, size);}
0
public Matrix assign(Matrix other, DoubleDoubleFunction function)
{    int rows = rowSize();    if (rows != other.rowSize()) {        throw new CardinalityException(rows, other.rowSize());    }    int columns = columnSize();    if (columns != other.columnSize()) {        throw new CardinalityException(columns, other.columnSize());    }    for (int row = 0; row < rows; row++) {        try {            Iterator<Vector.Element> sparseRowIterator = ((SequentialAccessSparseVector) this.rowVectors[row]).iterateNonZero();            if (function.isLikeMult()) {                                while (sparseRowIterator.hasNext()) {                    Vector.Element element = sparseRowIterator.next();                    int col = element.index();                    setQuick(row, col, function.apply(element.get(), other.getQuick(row, col)));                }            } else {                for (int col = 0; col < columns; col++) {                    setQuick(row, col, function.apply(getQuick(row, col), other.getQuick(row, col)));                }            }        } catch (ClassCastException e) {                                    for (int col = 0; col < columns; col++) {                setQuick(row, col, function.apply(getQuick(row, col), other.getQuick(row, col)));            }        }    }    return this;}
1
public Matrix assignColumn(int column, Vector other)
{    if (rowSize() != other.size()) {        throw new CardinalityException(rowSize(), other.size());    }    if (column < 0 || column >= columnSize()) {        throw new IndexException(column, columnSize());    }    for (int row = 0; row < rowSize(); row++) {        rowVectors[row].setQuick(column, other.getQuick(row));    }    return this;}
0
public Matrix assignRow(int row, Vector other)
{    if (columnSize() != other.size()) {        throw new CardinalityException(columnSize(), other.size());    }    if (row < 0 || row >= rowSize()) {        throw new IndexException(row, rowSize());    }    rowVectors[row].assign(other);    return this;}
0
public Vector viewRow(int row)
{    if (row < 0 || row >= rowSize()) {        throw new IndexException(row, rowSize());    }    return rowVectors[row];}
0
public Matrix transpose()
{    SparseColumnMatrix scm = new SparseColumnMatrix(columns, rows);    for (int i = 0; i < rows; i++) {        Vector row = rowVectors[i];        if (row.getNumNonZeroElements() > 0) {            scm.assignColumn(i, row);        }    }    return scm;}
0
public Matrix times(Matrix other)
{    if (columnSize() != other.rowSize()) {        throw new CardinalityException(columnSize(), other.rowSize());    }    if (other instanceof SparseRowMatrix) {        SparseRowMatrix y = (SparseRowMatrix) other;        SparseRowMatrix result = (SparseRowMatrix) like(rowSize(), other.columnSize());        for (int i = 0; i < rows; i++) {            Vector row = rowVectors[i];            for (Vector.Element element : row.nonZeroes()) {                result.rowVectors[i].assign(y.rowVectors[element.index()], Functions.plusMult(element.get()));            }        }        return result;    } else {        if (other.viewRow(0).isDense()) {                        Matrix result = other.like(rowSize(), other.columnSize());            for (int i = 0; i < rows; i++) {                Vector row = rowVectors[i];                Vector r = new DenseVector(other.columnSize());                for (Vector.Element element : row.nonZeroes()) {                    r.assign(other.viewRow(element.index()), Functions.plusMult(element.get()));                }                result.viewRow(i).assign(r);            }            return result;        } else {                        SparseRowMatrix result = (SparseRowMatrix) like(rowSize(), other.columnSize());            for (int i = 0; i < rows; i++) {                Vector row = rowVectors[i];                for (Vector.Element element : row.nonZeroes()) {                    result.rowVectors[i].assign(other.viewRow(element.index()), Functions.plusMult(element.get()));                }            }            return result;        }    }}
0
public MatrixFlavor getFlavor()
{    return MatrixFlavor.SPARSELIKE;}
0
public Vector getSingularValues()
{    return new DenseVector(svd.getSingularValues());}
0
public Matrix getU()
{        return cd1.solveRight(y).times(svd.getU());}
0
public Matrix getV()
{        return cd2.solveRight(b.transpose()).times(svd.getV());}
0
public static double entropy(long... elements)
{    long sum = 0;    double result = 0.0;    for (long element : elements) {        Preconditions.checkArgument(element >= 0);        result += xLogX(element);        sum += element;    }    return xLogX(sum) - result;}
0
private static double xLogX(long x)
{    return x == 0 ? 0.0 : x * Math.log(x);}
0
private static double entropy(long a, long b)
{    return xLogX(a + b) - xLogX(a) - xLogX(b);}
0
private static double entropy(long a, long b, long c, long d)
{    return xLogX(a + b + c + d) - xLogX(a) - xLogX(b) - xLogX(c) - xLogX(d);}
0
public static double logLikelihoodRatio(long k11, long k12, long k21, long k22)
{    Preconditions.checkArgument(k11 >= 0 && k12 >= 0 && k21 >= 0 && k22 >= 0);        double rowEntropy = entropy(k11 + k12, k21 + k22);    double columnEntropy = entropy(k11 + k21, k12 + k22);    double matrixEntropy = entropy(k11, k12, k21, k22);    if (rowEntropy + columnEntropy < matrixEntropy) {                return 0.0;    }    return 2.0 * (rowEntropy + columnEntropy - matrixEntropy);}
0
public static double rootLogLikelihoodRatio(long k11, long k12, long k21, long k22)
{    double llr = logLikelihoodRatio(k11, k12, k21, k22);    double sqrt = Math.sqrt(llr);    if ((double) k11 / (k11 + k12) < (double) k21 / (k21 + k22)) {        sqrt = -sqrt;    }    return sqrt;}
0
public static List<ScoredItem<T>> compareFrequencies(Multiset<T> a, Multiset<T> b, int maxReturn, double threshold)
{    int totalA = a.size();    int totalB = b.size();    Ordering<ScoredItem<T>> byScoreAscending = new Ordering<ScoredItem<T>>() {        @Override        public int compare(ScoredItem<T> tScoredItem, ScoredItem<T> tScoredItem1) {            return Double.compare(tScoredItem.score, tScoredItem1.score);        }    };    Queue<ScoredItem<T>> best = new PriorityQueue<>(maxReturn + 1, byScoreAscending);    for (T t : a.elementSet()) {        compareAndAdd(a, b, maxReturn, threshold, totalA, totalB, best, t);    }        if (threshold < 0) {        for (T t : b.elementSet()) {                        if (a.count(t) == 0) {                compareAndAdd(a, b, maxReturn, threshold, totalA, totalB, best, t);            }        }    }    List<ScoredItem<T>> r = new ArrayList<>(best);    Collections.sort(r, byScoreAscending.reverse());    return r;}
0
public int compare(ScoredItem<T> tScoredItem, ScoredItem<T> tScoredItem1)
{    return Double.compare(tScoredItem.score, tScoredItem1.score);}
0
private static void compareAndAdd(Multiset<T> a, Multiset<T> b, int maxReturn, double threshold, int totalA, int totalB, Queue<ScoredItem<T>> best, T t)
{    int kA = a.count(t);    int kB = b.count(t);    double score = rootLogLikelihoodRatio(kA, totalA - kA, kB, totalB - kB);    if (score >= threshold) {        ScoredItem<T> x = new ScoredItem<>(t, score);        best.add(x);        while (best.size() > maxReturn) {            best.poll();        }    }}
0
public double getScore()
{    return score;}
0
public T getItem()
{    return item;}
0
public void add(double t, double x)
{    double pi = Math.exp(-(t - lastT) / alpha);    s = x + pi * s;    w = 1.0 + pi * w;    this.t = t - lastT + pi * this.t;    lastT = t;}
0
public double mean()
{    return s / w;}
0
public double meanRate()
{    return s / t;}
0
public void add(double sample)
{    n++;    double oldMean = mean;    mean += (sample - mean) / n;    double diff = (sample - mean) * (sample - oldMean);    variance += (diff - variance) / n;}
0
public int getCount()
{    return n;}
0
public double getMean()
{    return mean;}
0
public double getSD()
{    return Math.sqrt(variance);}
0
public Matrix assignColumn(int column, Vector other)
{    m.assignRow(column, other);    return this;}
0
public Matrix assignRow(int row, Vector other)
{    m.assignColumn(row, other);    return this;}
0
public double getQuick(int row, int column)
{    return m.getQuick(column, row);}
0
public Matrix like()
{    return m.like(rows, columns);}
0
public Matrix like(int rows, int columns)
{    return m.like(rows, columns);}
0
public void setQuick(int row, int column, double value)
{    m.setQuick(column, row, value);}
0
public Vector viewRow(int row)
{    return m.viewColumn(row);}
0
public Vector viewColumn(int column)
{    return m.viewRow(column);}
0
public Matrix assign(double value)
{    return m.assign(value);}
0
public Matrix assign(Matrix other, DoubleDoubleFunction function)
{    if (other instanceof TransposedMatrixView) {        m.assign(((TransposedMatrixView) other).m, function);    } else {        m.assign(new TransposedMatrixView(other), function);    }    return this;}
0
public Matrix assign(Matrix other)
{    if (other instanceof TransposedMatrixView) {        return m.assign(((TransposedMatrixView) other).m);    } else {        return m.assign(new TransposedMatrixView(other));    }}
0
public Matrix assign(DoubleFunction function)
{    return m.assign(function);}
0
public MatrixFlavor getFlavor()
{    return flavor;}
0
public BackEnum getBacking()
{    return m.getFlavor().getBacking();}
0
public TraversingStructureEnum getStructure()
{    TraversingStructureEnum flavor = m.getFlavor().getStructure();    switch(flavor) {        case COLWISE:            return TraversingStructureEnum.ROWWISE;        case SPARSECOLWISE:            return TraversingStructureEnum.SPARSEROWWISE;        case ROWWISE:            return TraversingStructureEnum.COLWISE;        case SPARSEROWWISE:            return TraversingStructureEnum.SPARSECOLWISE;        default:            return flavor;    }}
0
public boolean isDense()
{    return m.getFlavor().isDense();}
0
 Matrix getDelegate()
{    return m;}
0
private static int elementsToMatrixSize(int dataSize)
{    return (int) Math.round((-1 + Math.sqrt(1 + 8 * dataSize)) / 2);}
0
public Matrix assignColumn(int column, Vector other)
{    if (columnSize() != other.size()) {        throw new IndexException(columnSize(), other.size());    }    if (other.viewPart(column + 1, other.size() - column - 1).norm(1) > 1.0e-14) {        throw new IllegalArgumentException("Cannot set lower portion of triangular matrix to non-zero");    }    for (Vector.Element element : other.viewPart(0, column).all()) {        setQuick(element.index(), column, element.get());    }    return this;}
0
public Matrix assignRow(int row, Vector other)
{    if (columnSize() != other.size()) {        throw new IndexException(numCols(), other.size());    }    for (int i = 0; i < row; i++) {        if (Math.abs(other.getQuick(i)) > EPSILON) {            throw new IllegalArgumentException("non-triangular source");        }    }    for (int i = row; i < rows; i++) {        setQuick(row, i, other.get(i));    }    return this;}
0
public Matrix assignNonZeroElementsInRow(int row, double[] other)
{    System.arraycopy(other, row, values, getL(row, row), rows - row);    return this;}
0
public double getQuick(int row, int column)
{    if (row > column) {        return 0;    }    int i = getL(row, column);    return values[i];}
0
private int getL(int row, int col)
{    /*     * each row starts with some zero elements that we don't store. this     * accumulates an offset of (row+1)*row/2     */    return col + row * numCols() - (row + 1) * row / 2;}
0
public Matrix like()
{    return like(rowSize(), columnSize());}
0
public Matrix like(int rows, int columns)
{    return new DenseMatrix(rows, columns);}
0
public void setQuick(int row, int column, double value)
{    values[getL(row, column)] = value;}
0
public int[] getNumNondefaultElements()
{    throw new UnsupportedOperationException();}
0
public Matrix viewPart(int[] offset, int[] size)
{    return new MatrixView(this, offset, size);}
0
public double[] getData()
{    return values;}
0
public MatrixFlavor getFlavor()
{        return new MatrixFlavor.FlavorImpl(BackEnum.JVMMEM, TraversingStructureEnum.VECTORBACKED, true);}
0
public static VectorBinaryAggregate getBestOperation(Vector x, Vector y, DoubleDoubleFunction fa, DoubleDoubleFunction fc)
{    int bestOperationIndex = -1;    double bestCost = Double.POSITIVE_INFINITY;    for (int i = 0; i < OPERATIONS.length; ++i) {        if (OPERATIONS[i].isValid(x, y, fa, fc)) {            double cost = OPERATIONS[i].estimateCost(x, y, fa, fc);            if (cost < bestCost) {                bestCost = cost;                bestOperationIndex = i;            }        }    }    return OPERATIONS[bestOperationIndex];}
0
public static double aggregateBest(Vector x, Vector y, DoubleDoubleFunction fa, DoubleDoubleFunction fc)
{    return getBestOperation(x, y, fa, fc).aggregate(x, y, fa, fc);}
0
public boolean isValid(Vector x, Vector y, DoubleDoubleFunction fa, DoubleDoubleFunction fc)
{    return fa.isLikeRightPlus() && (fa.isAssociativeAndCommutative() || x.isSequentialAccess()) && fc.isLikeLeftMult();}
0
public double estimateCost(Vector x, Vector y, DoubleDoubleFunction fa, DoubleDoubleFunction fc)
{    return x.getNumNondefaultElements() * x.getIteratorAdvanceCost() * y.getLookupCost();}
0
public double aggregate(Vector x, Vector y, DoubleDoubleFunction fa, DoubleDoubleFunction fc)
{    Iterator<Vector.Element> xi = x.nonZeroes().iterator();    if (!xi.hasNext()) {        return 0;    }    Vector.Element xe = xi.next();    double result = fc.apply(xe.get(), y.getQuick(xe.index()));    while (xi.hasNext()) {        xe = xi.next();        result = fa.apply(result, fc.apply(xe.get(), y.getQuick(xe.index())));    }    return result;}
0
public boolean isValid(Vector x, Vector y, DoubleDoubleFunction fa, DoubleDoubleFunction fc)
{    return fa.isLikeRightPlus() && (fa.isAssociativeAndCommutative() || y.isSequentialAccess()) && fc.isLikeRightMult();}
0
public double estimateCost(Vector x, Vector y, DoubleDoubleFunction fa, DoubleDoubleFunction fc)
{    return y.getNumNondefaultElements() * y.getIteratorAdvanceCost() * x.getLookupCost() * x.getLookupCost();}
0
public double aggregate(Vector x, Vector y, DoubleDoubleFunction fa, DoubleDoubleFunction fc)
{    Iterator<Vector.Element> yi = y.nonZeroes().iterator();    if (!yi.hasNext()) {        return 0;    }    Vector.Element ye = yi.next();    double result = fc.apply(x.getQuick(ye.index()), ye.get());    while (yi.hasNext()) {        ye = yi.next();        result = fa.apply(result, fc.apply(x.getQuick(ye.index()), ye.get()));    }    return result;}
0
public boolean isValid(Vector x, Vector y, DoubleDoubleFunction fa, DoubleDoubleFunction fc)
{    return fa.isLikeRightPlus() && fc.isLikeMult() && x.isSequentialAccess() && y.isSequentialAccess();}
0
public double estimateCost(Vector x, Vector y, DoubleDoubleFunction fa, DoubleDoubleFunction fc)
{    return Math.min(x.getNumNondefaultElements() * x.getIteratorAdvanceCost(), y.getNumNondefaultElements() * y.getIteratorAdvanceCost());}
0
public double aggregate(Vector x, Vector y, DoubleDoubleFunction fa, DoubleDoubleFunction fc)
{    Iterator<Vector.Element> xi = x.nonZeroes().iterator();    Iterator<Vector.Element> yi = y.nonZeroes().iterator();    Vector.Element xe = null;    Vector.Element ye = null;    boolean advanceThis = true;    boolean advanceThat = true;    boolean validResult = false;    double result = 0;    while (true) {        if (advanceThis) {            if (xi.hasNext()) {                xe = xi.next();            } else {                break;            }        }        if (advanceThat) {            if (yi.hasNext()) {                ye = yi.next();            } else {                break;            }        }        if (xe.index() == ye.index()) {            double thisResult = fc.apply(xe.get(), ye.get());            if (validResult) {                result = fa.apply(result, thisResult);            } else {                result = thisResult;                validResult = true;            }            advanceThis = true;            advanceThat = true;        } else {            if (xe.index() < ye.index()) {                                advanceThis = true;                advanceThat = false;            } else {                                advanceThis = false;                advanceThat = true;            }        }    }    return result;}
0
public boolean isValid(Vector x, Vector y, DoubleDoubleFunction fa, DoubleDoubleFunction fc)
{    return fa.isLikeRightPlus() && !fc.isDensifying() && x.isSequentialAccess() && y.isSequentialAccess();}
0
public double estimateCost(Vector x, Vector y, DoubleDoubleFunction fa, DoubleDoubleFunction fc)
{    return Math.max(x.getNumNondefaultElements() * x.getIteratorAdvanceCost(), y.getNumNondefaultElements() * y.getIteratorAdvanceCost());}
0
public double aggregate(Vector x, Vector y, DoubleDoubleFunction fa, DoubleDoubleFunction fc)
{    Iterator<Vector.Element> xi = x.nonZeroes().iterator();    Iterator<Vector.Element> yi = y.nonZeroes().iterator();    Vector.Element xe = null;    Vector.Element ye = null;    boolean advanceThis = true;    boolean advanceThat = true;    boolean validResult = false;    double result = 0;    while (true) {        if (advanceThis) {            if (xi.hasNext()) {                xe = xi.next();            } else {                xe = null;            }        }        if (advanceThat) {            if (yi.hasNext()) {                ye = yi.next();            } else {                ye = null;            }        }        double thisResult;        if (xe != null && ye != null) {                        if (xe.index() == ye.index()) {                thisResult = fc.apply(xe.get(), ye.get());                advanceThis = true;                advanceThat = true;            } else {                if (xe.index() < ye.index()) {                                        thisResult = fc.apply(xe.get(), 0);                    advanceThis = true;                    advanceThat = false;                } else {                    thisResult = fc.apply(0, ye.get());                    advanceThis = false;                    advanceThat = true;                }            }        } else if (xe != null) {                        thisResult = fc.apply(xe.get(), 0);            advanceThis = true;            advanceThat = false;        } else if (ye != null) {                        thisResult = fc.apply(0, ye.get());            advanceThis = false;            advanceThat = true;        } else {                        break;        }        if (validResult) {            result = fa.apply(result, thisResult);        } else {            result = thisResult;            validResult = true;        }    }    return result;}
0
public boolean isValid(Vector x, Vector y, DoubleDoubleFunction fa, DoubleDoubleFunction fc)
{    return fa.isLikeRightPlus() && !fc.isDensifying() && (fa.isAssociativeAndCommutative() || (x.isSequentialAccess() && y.isSequentialAccess()));}
0
public double estimateCost(Vector x, Vector y, DoubleDoubleFunction fa, DoubleDoubleFunction fc)
{    return Math.max(x.getNumNondefaultElements() * x.getIteratorAdvanceCost() * y.getLookupCost(), y.getNumNondefaultElements() * y.getIteratorAdvanceCost() * x.getLookupCost());}
0
public double aggregate(Vector x, Vector y, DoubleDoubleFunction fa, DoubleDoubleFunction fc)
{    OpenIntHashSet visited = new OpenIntHashSet();    Iterator<Vector.Element> xi = x.nonZeroes().iterator();    boolean validResult = false;    double result = 0;    double thisResult;    while (xi.hasNext()) {        Vector.Element xe = xi.next();        thisResult = fc.apply(xe.get(), y.getQuick(xe.index()));        if (validResult) {            result = fa.apply(result, thisResult);        } else {            result = thisResult;            validResult = true;        }        visited.add(xe.index());    }    Iterator<Vector.Element> yi = y.nonZeroes().iterator();    while (yi.hasNext()) {        Vector.Element ye = yi.next();        if (!visited.contains(ye.index())) {            thisResult = fc.apply(x.getQuick(ye.index()), ye.get());            if (validResult) {                result = fa.apply(result, thisResult);            } else {                result = thisResult;                validResult = true;            }        }    }    return result;}
0
public boolean isValid(Vector x, Vector y, DoubleDoubleFunction fa, DoubleDoubleFunction fc)
{    return x.isSequentialAccess() && y.isSequentialAccess() && !x.isDense() && !y.isDense();}
0
public double estimateCost(Vector x, Vector y, DoubleDoubleFunction fa, DoubleDoubleFunction fc)
{    return Math.max(x.size() * x.getIteratorAdvanceCost(), y.size() * y.getIteratorAdvanceCost());}
0
public double aggregate(Vector x, Vector y, DoubleDoubleFunction fa, DoubleDoubleFunction fc)
{    Iterator<Vector.Element> xi = x.all().iterator();    Iterator<Vector.Element> yi = y.all().iterator();    boolean validResult = false;    double result = 0;    while (xi.hasNext() && yi.hasNext()) {        Vector.Element xe = xi.next();        double thisResult = fc.apply(xe.get(), yi.next().get());        if (validResult) {            result = fa.apply(result, thisResult);        } else {            result = thisResult;            validResult = true;        }    }    return result;}
0
public boolean isValid(Vector x, Vector y, DoubleDoubleFunction fa, DoubleDoubleFunction fc)
{    return (fa.isAssociativeAndCommutative() || x.isSequentialAccess()) && !x.isDense();}
0
public double estimateCost(Vector x, Vector y, DoubleDoubleFunction fa, DoubleDoubleFunction fc)
{    return x.size() * x.getIteratorAdvanceCost() * y.getLookupCost();}
0
public double aggregate(Vector x, Vector y, DoubleDoubleFunction fa, DoubleDoubleFunction fc)
{    Iterator<Vector.Element> xi = x.all().iterator();    boolean validResult = false;    double result = 0;    while (xi.hasNext()) {        Vector.Element xe = xi.next();        double thisResult = fc.apply(xe.get(), y.getQuick(xe.index()));        if (validResult) {            result = fa.apply(result, thisResult);        } else {            result = thisResult;            validResult = true;        }    }    return result;}
0
public boolean isValid(Vector x, Vector y, DoubleDoubleFunction fa, DoubleDoubleFunction fc)
{    return (fa.isAssociativeAndCommutative() || y.isSequentialAccess()) && !y.isDense();}
0
public double estimateCost(Vector x, Vector y, DoubleDoubleFunction fa, DoubleDoubleFunction fc)
{    return y.size() * y.getIteratorAdvanceCost() * x.getLookupCost();}
0
public double aggregate(Vector x, Vector y, DoubleDoubleFunction fa, DoubleDoubleFunction fc)
{    Iterator<Vector.Element> yi = y.all().iterator();    boolean validResult = false;    double result = 0;    while (yi.hasNext()) {        Vector.Element ye = yi.next();        double thisResult = fc.apply(x.getQuick(ye.index()), ye.get());        if (validResult) {            result = fa.apply(result, thisResult);        } else {            result = thisResult;            validResult = true;        }    }    return result;}
0
public boolean isValid(Vector x, Vector y, DoubleDoubleFunction fa, DoubleDoubleFunction fc)
{    return true;}
0
public double estimateCost(Vector x, Vector y, DoubleDoubleFunction fa, DoubleDoubleFunction fc)
{    return x.size() * x.getLookupCost() * y.getLookupCost();}
0
public double aggregate(Vector x, Vector y, DoubleDoubleFunction fa, DoubleDoubleFunction fc)
{    double result = fc.apply(x.getQuick(0), y.getQuick(0));    int s = x.size();    for (int i = 1; i < s; ++i) {        result = fa.apply(result, fc.apply(x.getQuick(i), y.getQuick(i)));    }    return result;}
0
public static VectorBinaryAssign getBestOperation(Vector x, Vector y, DoubleDoubleFunction f)
{    int bestOperationIndex = -1;    double bestCost = Double.POSITIVE_INFINITY;    for (int i = 0; i < OPERATIONS.length; ++i) {        if (OPERATIONS[i].isValid(x, y, f)) {            double cost = OPERATIONS[i].estimateCost(x, y, f);            if (cost < bestCost) {                bestCost = cost;                bestOperationIndex = i;            }        }    }    return OPERATIONS[bestOperationIndex];}
0
public static Vector assignBest(Vector x, Vector y, DoubleDoubleFunction f)
{    return getBestOperation(x, y, f).assign(x, y, f);}
0
public boolean isValid(Vector x, Vector y, DoubleDoubleFunction f)
{    return f.isLikeLeftMult();}
0
public double estimateCost(Vector x, Vector y, DoubleDoubleFunction f)
{    return x.getNumNondefaultElements() * x.getIteratorAdvanceCost() * y.getLookupCost();}
0
public Vector assign(Vector x, Vector y, DoubleDoubleFunction f)
{    for (Element xe : x.nonZeroes()) {        xe.set(f.apply(xe.get(), y.getQuick(xe.index())));    }    return x;}
0
public boolean isValid(Vector x, Vector y, DoubleDoubleFunction f)
{    return f.isLikeRightPlus();}
0
public double estimateCost(Vector x, Vector y, DoubleDoubleFunction f)
{    return y.getNumNondefaultElements() * y.getIteratorAdvanceCost() * x.getLookupCost() * x.getLookupCost();}
0
public Vector assign(Vector x, Vector y, DoubleDoubleFunction f)
{    for (Element ye : y.nonZeroes()) {        x.setQuick(ye.index(), f.apply(x.getQuick(ye.index()), ye.get()));    }    return x;}
0
public boolean isValid(Vector x, Vector y, DoubleDoubleFunction f)
{    return f.isLikeRightPlus() && y.isSequentialAccess() && !x.isAddConstantTime();}
0
public double estimateCost(Vector x, Vector y, DoubleDoubleFunction f)
{    return y.getNumNondefaultElements() * y.getIteratorAdvanceCost() * y.getLookupCost();}
0
public Vector assign(Vector x, Vector y, DoubleDoubleFunction f)
{    OrderedIntDoubleMapping updates = new OrderedIntDoubleMapping(false);    for (Element ye : y.nonZeroes()) {        updates.set(ye.index(), f.apply(x.getQuick(ye.index()), ye.get()));    }    x.mergeUpdates(updates);    return x;}
0
public boolean isValid(Vector x, Vector y, DoubleDoubleFunction f)
{    return f.isLikeLeftMult() && f.isLikeRightPlus() && x.isSequentialAccess() && y.isSequentialAccess();}
0
public double estimateCost(Vector x, Vector y, DoubleDoubleFunction f)
{    return Math.min(x.getNumNondefaultElements() * x.getIteratorAdvanceCost(), y.getNumNondefaultElements() * y.getIteratorAdvanceCost());}
0
public Vector assign(Vector x, Vector y, DoubleDoubleFunction f)
{    Iterator<Vector.Element> xi = x.nonZeroes().iterator();    Iterator<Vector.Element> yi = y.nonZeroes().iterator();    Vector.Element xe = null;    Vector.Element ye = null;    boolean advanceThis = true;    boolean advanceThat = true;    while (true) {        if (advanceThis) {            if (xi.hasNext()) {                xe = xi.next();            } else {                break;            }        }        if (advanceThat) {            if (yi.hasNext()) {                ye = yi.next();            } else {                break;            }        }        if (xe.index() == ye.index()) {            xe.set(f.apply(xe.get(), ye.get()));            advanceThis = true;            advanceThat = true;        } else {            if (xe.index() < ye.index()) {                                advanceThis = true;                advanceThat = false;            } else {                                advanceThis = false;                advanceThat = true;            }        }    }    return x;}
0
public boolean isValid(Vector x, Vector y, DoubleDoubleFunction f)
{    return !f.isDensifying() && x.isSequentialAccess() && y.isSequentialAccess() && !x.isAddConstantTime();}
0
public double estimateCost(Vector x, Vector y, DoubleDoubleFunction f)
{    return Math.max(x.getNumNondefaultElements() * x.getIteratorAdvanceCost(), y.getNumNondefaultElements() * y.getIteratorAdvanceCost());}
0
public Vector assign(Vector x, Vector y, DoubleDoubleFunction f)
{    Iterator<Vector.Element> xi = x.nonZeroes().iterator();    Iterator<Vector.Element> yi = y.nonZeroes().iterator();    Vector.Element xe = null;    Vector.Element ye = null;    boolean advanceThis = true;    boolean advanceThat = true;    OrderedIntDoubleMapping updates = new OrderedIntDoubleMapping(false);    while (true) {        if (advanceThis) {            if (xi.hasNext()) {                xe = xi.next();            } else {                xe = null;            }        }        if (advanceThat) {            if (yi.hasNext()) {                ye = yi.next();            } else {                ye = null;            }        }        if (xe != null && ye != null) {                        if (xe.index() == ye.index()) {                xe.set(f.apply(xe.get(), ye.get()));                advanceThis = true;                advanceThat = true;            } else {                if (xe.index() < ye.index()) {                                        xe.set(f.apply(xe.get(), 0));                    advanceThis = true;                    advanceThat = false;                } else {                    updates.set(ye.index(), f.apply(0, ye.get()));                    advanceThis = false;                    advanceThat = true;                }            }        } else if (xe != null) {                        xe.set(f.apply(xe.get(), 0));            advanceThis = true;            advanceThat = false;        } else if (ye != null) {                        updates.set(ye.index(), f.apply(0, ye.get()));            advanceThis = false;            advanceThat = true;        } else {                        break;        }    }    x.mergeUpdates(updates);    return x;}
0
public boolean isValid(Vector x, Vector y, DoubleDoubleFunction f)
{    return !f.isDensifying() && x.isSequentialAccess() && y.isSequentialAccess() && x.isAddConstantTime();}
0
public double estimateCost(Vector x, Vector y, DoubleDoubleFunction f)
{    return Math.max(x.getNumNondefaultElements() * x.getIteratorAdvanceCost(), y.getNumNondefaultElements() * y.getIteratorAdvanceCost());}
0
public Vector assign(Vector x, Vector y, DoubleDoubleFunction f)
{    Iterator<Vector.Element> xi = x.nonZeroes().iterator();    Iterator<Vector.Element> yi = y.nonZeroes().iterator();    Vector.Element xe = null;    Vector.Element ye = null;    boolean advanceThis = true;    boolean advanceThat = true;    while (true) {        if (advanceThis) {            if (xi.hasNext()) {                xe = xi.next();            } else {                xe = null;            }        }        if (advanceThat) {            if (yi.hasNext()) {                ye = yi.next();            } else {                ye = null;            }        }        if (xe != null && ye != null) {                        if (xe.index() == ye.index()) {                xe.set(f.apply(xe.get(), ye.get()));                advanceThis = true;                advanceThat = true;            } else {                if (xe.index() < ye.index()) {                                        xe.set(f.apply(xe.get(), 0));                    advanceThis = true;                    advanceThat = false;                } else {                    x.setQuick(ye.index(), f.apply(0, ye.get()));                    advanceThis = false;                    advanceThat = true;                }            }        } else if (xe != null) {                        xe.set(f.apply(xe.get(), 0));            advanceThis = true;            advanceThat = false;        } else if (ye != null) {                        x.setQuick(ye.index(), f.apply(0, ye.get()));            advanceThis = false;            advanceThat = true;        } else {                        break;        }    }    return x;}
0
public boolean isValid(Vector x, Vector y, DoubleDoubleFunction f)
{    return !f.isDensifying() && !x.isAddConstantTime() && y.isSequentialAccess();}
0
public double estimateCost(Vector x, Vector y, DoubleDoubleFunction f)
{    return Math.max(x.getNumNondefaultElements() * x.getIteratorAdvanceCost() * y.getLookupCost(), y.getNumNondefaultElements() * y.getIteratorAdvanceCost() * x.getLookupCost());}
0
public Vector assign(Vector x, Vector y, DoubleDoubleFunction f)
{    OpenIntHashSet visited = new OpenIntHashSet();    for (Element xe : x.nonZeroes()) {        xe.set(f.apply(xe.get(), y.getQuick(xe.index())));        visited.add(xe.index());    }    OrderedIntDoubleMapping updates = new OrderedIntDoubleMapping(false);    for (Element ye : y.nonZeroes()) {        if (!visited.contains(ye.index())) {            updates.set(ye.index(), f.apply(x.getQuick(ye.index()), ye.get()));        }    }    x.mergeUpdates(updates);    return x;}
0
public boolean isValid(Vector x, Vector y, DoubleDoubleFunction f)
{    return !f.isDensifying() && x.isAddConstantTime();}
0
public double estimateCost(Vector x, Vector y, DoubleDoubleFunction f)
{    return Math.max(x.getNumNondefaultElements() * x.getIteratorAdvanceCost() * y.getLookupCost(), y.getNumNondefaultElements() * y.getIteratorAdvanceCost() * x.getLookupCost());}
0
public Vector assign(Vector x, Vector y, DoubleDoubleFunction f)
{    OpenIntHashSet visited = new OpenIntHashSet();    for (Element xe : x.nonZeroes()) {        xe.set(f.apply(xe.get(), y.getQuick(xe.index())));        visited.add(xe.index());    }    for (Element ye : y.nonZeroes()) {        if (!visited.contains(ye.index())) {            x.setQuick(ye.index(), f.apply(x.getQuick(ye.index()), ye.get()));        }    }    return x;}
0
public boolean isValid(Vector x, Vector y, DoubleDoubleFunction f)
{    return x.isSequentialAccess() && y.isSequentialAccess() && !x.isAddConstantTime() && !x.isDense() && !y.isDense();}
0
public double estimateCost(Vector x, Vector y, DoubleDoubleFunction f)
{    return Math.max(x.size() * x.getIteratorAdvanceCost(), y.size() * y.getIteratorAdvanceCost());}
0
public Vector assign(Vector x, Vector y, DoubleDoubleFunction f)
{    Iterator<Vector.Element> xi = x.all().iterator();    Iterator<Vector.Element> yi = y.all().iterator();    OrderedIntDoubleMapping updates = new OrderedIntDoubleMapping(false);    while (xi.hasNext() && yi.hasNext()) {        Element xe = xi.next();        updates.set(xe.index(), f.apply(xe.get(), yi.next().get()));    }    x.mergeUpdates(updates);    return x;}
0
public boolean isValid(Vector x, Vector y, DoubleDoubleFunction f)
{    return x.isSequentialAccess() && y.isSequentialAccess() && x.isAddConstantTime() && !x.isDense() && !y.isDense();}
0
public double estimateCost(Vector x, Vector y, DoubleDoubleFunction f)
{    return Math.max(x.size() * x.getIteratorAdvanceCost(), y.size() * y.getIteratorAdvanceCost());}
0
public Vector assign(Vector x, Vector y, DoubleDoubleFunction f)
{    Iterator<Vector.Element> xi = x.all().iterator();    Iterator<Vector.Element> yi = y.all().iterator();    while (xi.hasNext() && yi.hasNext()) {        Element xe = xi.next();        x.setQuick(xe.index(), f.apply(xe.get(), yi.next().get()));    }    return x;}
0
public boolean isValid(Vector x, Vector y, DoubleDoubleFunction f)
{    return !x.isAddConstantTime() && !x.isDense();}
0
public double estimateCost(Vector x, Vector y, DoubleDoubleFunction f)
{    return x.size() * x.getIteratorAdvanceCost() * y.getLookupCost();}
0
public Vector assign(Vector x, Vector y, DoubleDoubleFunction f)
{    OrderedIntDoubleMapping updates = new OrderedIntDoubleMapping(false);    for (Element xe : x.all()) {        updates.set(xe.index(), f.apply(xe.get(), y.getQuick(xe.index())));    }    x.mergeUpdates(updates);    return x;}
0
public boolean isValid(Vector x, Vector y, DoubleDoubleFunction f)
{    return x.isAddConstantTime() && !x.isDense();}
0
public double estimateCost(Vector x, Vector y, DoubleDoubleFunction f)
{    return x.size() * x.getIteratorAdvanceCost() * y.getLookupCost();}
0
public Vector assign(Vector x, Vector y, DoubleDoubleFunction f)
{    for (Element xe : x.all()) {        x.setQuick(xe.index(), f.apply(xe.get(), y.getQuick(xe.index())));    }    return x;}
0
public boolean isValid(Vector x, Vector y, DoubleDoubleFunction f)
{    return !x.isAddConstantTime() && !y.isDense();}
0
public double estimateCost(Vector x, Vector y, DoubleDoubleFunction f)
{    return y.size() * y.getIteratorAdvanceCost() * x.getLookupCost();}
0
public Vector assign(Vector x, Vector y, DoubleDoubleFunction f)
{    OrderedIntDoubleMapping updates = new OrderedIntDoubleMapping(false);    for (Element ye : y.all()) {        updates.set(ye.index(), f.apply(x.getQuick(ye.index()), ye.get()));    }    x.mergeUpdates(updates);    return x;}
0
public boolean isValid(Vector x, Vector y, DoubleDoubleFunction f)
{    return x.isAddConstantTime() && !y.isDense();}
0
public double estimateCost(Vector x, Vector y, DoubleDoubleFunction f)
{    return y.size() * y.getIteratorAdvanceCost() * x.getLookupCost();}
0
public Vector assign(Vector x, Vector y, DoubleDoubleFunction f)
{    for (Element ye : y.all()) {        x.setQuick(ye.index(), f.apply(x.getQuick(ye.index()), ye.get()));    }    return x;}
0
public boolean isValid(Vector x, Vector y, DoubleDoubleFunction f)
{    return !x.isAddConstantTime();}
0
public double estimateCost(Vector x, Vector y, DoubleDoubleFunction f)
{    return x.size() * x.getLookupCost() * y.getLookupCost();}
0
public Vector assign(Vector x, Vector y, DoubleDoubleFunction f)
{    OrderedIntDoubleMapping updates = new OrderedIntDoubleMapping(false);    for (int i = 0; i < x.size(); ++i) {        updates.set(i, f.apply(x.getQuick(i), y.getQuick(i)));    }    x.mergeUpdates(updates);    return x;}
0
public boolean isValid(Vector x, Vector y, DoubleDoubleFunction f)
{    return x.isAddConstantTime();}
0
public double estimateCost(Vector x, Vector y, DoubleDoubleFunction f)
{    return x.size() * x.getLookupCost() * y.getLookupCost();}
0
public Vector assign(Vector x, Vector y, DoubleDoubleFunction f)
{    for (int i = 0; i < x.size(); ++i) {        x.setQuick(i, f.apply(x.getQuick(i), y.getQuick(i)));    }    return x;}
0
protected Matrix matrixLike(int rows, int columns)
{    return ((AbstractVector) vector).matrixLike(rows, columns);}
0
public Vector clone()
{    VectorView r = (VectorView) super.clone();    r.vector = vector.clone();    r.offset = offset;    return r;}
0
public boolean isDense()
{    return vector.isDense();}
0
public boolean isSequentialAccess()
{    return vector.isSequentialAccess();}
0
public VectorView like()
{    return new VectorView(vector.like(), offset, size());}
0
public Vector like(int cardinality)
{    return vector.like(cardinality);}
0
public double getQuick(int index)
{    return vector.getQuick(offset + index);}
0
public void setQuick(int index, double value)
{    vector.setQuick(offset + index, value);}
0
public int getNumNondefaultElements()
{    return size();}
0
public Vector viewPart(int offset, int length)
{    if (offset < 0) {        throw new IndexException(offset, size());    }    if (offset + length > size()) {        throw new IndexException(offset + length, size());    }    return new VectorView(vector, offset + this.offset, length);}
0
private boolean isInView(int index)
{    return index >= offset && index < offset + size();}
0
public Iterator<Element> iterateNonZero()
{    return new NonZeroIterator();}
0
public Iterator<Element> iterator()
{    return new AllIterator();}
0
protected Element computeNext()
{    while (it.hasNext()) {        Element el = it.next();        if (isInView(el.index()) && el.get() != 0) {            Element decorated = el;            /* vector.getElement(el.index()); */            return new DecoratorElement(decorated);        }    }    return endOfData();}
0
protected Element computeNext()
{    while (it.hasNext()) {        Element el = it.next();        if (isInView(el.index())) {            Element decorated = vector.getElement(el.index());            return new DecoratorElement(decorated);        }    }        return endOfData();}
0
public double get()
{    return decorated.get();}
0
public int index()
{    return decorated.index() - offset;}
0
public void set(double value)
{    decorated.set(value);}
0
public double getLengthSquared()
{    double result = 0.0;    int size = size();    for (int i = 0; i < size; i++) {        double value = getQuick(i);        result += value * value;    }    return result;}
0
public double getDistanceSquared(Vector v)
{    double result = 0.0;    int size = size();    for (int i = 0; i < size; i++) {        double delta = getQuick(i) - v.getQuick(i);        result += delta * delta;    }    return result;}
0
public double getLookupCost()
{    return vector.getLookupCost();}
0
public double getIteratorAdvanceCost()
{        return 2 * vector.getIteratorAdvanceCost();}
0
public boolean isAddConstantTime()
{    return vector.isAddConstantTime();}
0
public void mergeUpdates(OrderedIntDoubleMapping updates)
{    for (int i = 0; i < updates.getNumMappings(); ++i) {        updates.setIndexAt(i, updates.indexAt(i) + offset);    }    vector.mergeUpdates(updates);}
0
public static WeightedVector project(Vector v, Vector projection)
{    return project(v, projection, INVALID_INDEX);}
0
public static WeightedVector project(Vector v, Vector projection, int index)
{    return new WeightedVector(v, projection, index);}
0
public double getWeight()
{    return weight;}
0
public int getIndex()
{    return index;}
0
public void setWeight(double newWeight)
{    this.weight = newWeight;}
0
public void setIndex(int index)
{    this.index = index;}
0
public Vector like()
{    return new WeightedVector(getVector().like(), weight, index);}
0
public String toString()
{    return String.format("index=%d, weight=%.2f, v=%s", index, weight, getVector());}
0
public WeightedVector clone()
{    WeightedVector v = (WeightedVector) super.clone();    v.weight = weight;    v.index = index;    return v;}
0
public int compare(WeightedVector a, WeightedVector b)
{    if (a == b) {        return 0;    }    double aWeight = a.getWeight();    double bWeight = b.getWeight();    int r = Double.compare(aWeight, bWeight);    if (r != 0 && Math.abs(aWeight - bWeight) >= DOUBLE_EQUALITY_ERROR) {        return r;    }    double diff = a.minus(b).norm(1);    if (diff < 1.0e-12) {        return 0;    }    for (Vector.Element element : a.all()) {        r = Double.compare(element.get(), b.get(element.index()));        if (r != 0) {            return r;        }    }    return 0;}
0
public void testHashDouble()
{    assertEquals(new Double(0.0).hashCode(), RandomUtils.hashDouble(0.0));    assertEquals(new Double(1.0).hashCode(), RandomUtils.hashDouble(1.0));    assertEquals(new Double(Double.POSITIVE_INFINITY).hashCode(), RandomUtils.hashDouble(Double.POSITIVE_INFINITY));    assertEquals(new Double(Double.NaN).hashCode(), RandomUtils.hashDouble(Double.NaN));}
0
public void testHashFloat()
{    assertEquals(new Float(0.0f).hashCode(), RandomUtils.hashFloat(0.0f));    assertEquals(new Float(1.0f).hashCode(), RandomUtils.hashFloat(1.0f));    assertEquals(new Float(Float.POSITIVE_INFINITY).hashCode(), RandomUtils.hashFloat(Float.POSITIVE_INFINITY));    assertEquals(new Float(Float.NaN).hashCode(), RandomUtils.hashFloat(Float.NaN));}
0
public void testNextTwinPrime()
{    assertEquals(5, RandomUtils.nextTwinPrime(-1));    assertEquals(5, RandomUtils.nextTwinPrime(1));    assertEquals(5, RandomUtils.nextTwinPrime(2));    assertEquals(5, RandomUtils.nextTwinPrime(3));    assertEquals(7, RandomUtils.nextTwinPrime(4));    assertEquals(7, RandomUtils.nextTwinPrime(5));    assertEquals(13, RandomUtils.nextTwinPrime(6));    assertEquals(RandomUtils.MAX_INT_SMALLER_TWIN_PRIME + 2, RandomUtils.nextTwinPrime(RandomUtils.MAX_INT_SMALLER_TWIN_PRIME));    try {        RandomUtils.nextTwinPrime(RandomUtils.MAX_INT_SMALLER_TWIN_PRIME + 1);        fail();    } catch (IllegalArgumentException iae) {        }}
0
public void testSetSeed()
{    Random rTest0 = RandomUtils.getRandom();    Random rTest1 = RandomUtils.getRandom();    Random r0 = RandomUtils.getRandom(0);    Random r1 = RandomUtils.getRandom(1);    long lTest0 = rTest0.nextLong();    long lTest1 = rTest1.nextLong();    long l0 = r0.nextLong();    long l1 = r1.nextLong();    assertEquals("getRandom() must match getRandom() in unit tests", lTest0, lTest1);    assertTrue("getRandom() must differ from getRandom(0)", lTest0 != l1);    assertTrue("getRandom(0) must differ from getRandom(1)", l0 != l1);}
0
private static void checkIterator(Iterator<Vector.Element> nzIter, double[] values)
{    while (nzIter.hasNext()) {        Vector.Element elt = nzIter.next();        assertEquals(elt.index() + " Value: " + values[elt.index()] + " does not equal: " + elt.get(), values[elt.index()], elt.get(), 0.0);    }}
0
public void testSimpleOps()
{    T v0 = vectorToTest(20);    Random gen = RandomUtils.getRandom();    Vector v1 = v0.assign(new Normal(0, 1, gen));        assertEquals(v0.get(12), v1.get(12), 0);    v0.set(12, gen.nextDouble());    assertEquals(v0.get(12), v1.get(12), 0);    assertSame(v0, v1);    Vector v2 = vectorToTest(20).assign(new Normal(0, 1, gen));    Vector dv1 = new DenseVector(v1);    Vector dv2 = new DenseVector(v2);    Vector sv1 = new RandomAccessSparseVector(v1);    Vector sv2 = new RandomAccessSparseVector(v2);    assertEquals(0, dv1.plus(dv2).getDistanceSquared(v1.plus(v2)), FUZZ);    assertEquals(0, dv1.plus(dv2).getDistanceSquared(v1.plus(dv2)), FUZZ);    assertEquals(0, dv1.plus(dv2).getDistanceSquared(v1.plus(sv2)), FUZZ);    assertEquals(0, dv1.plus(dv2).getDistanceSquared(sv1.plus(v2)), FUZZ);    assertEquals(0, dv1.times(dv2).getDistanceSquared(v1.times(v2)), FUZZ);    assertEquals(0, dv1.times(dv2).getDistanceSquared(v1.times(dv2)), FUZZ);    assertEquals(0, dv1.times(dv2).getDistanceSquared(v1.times(sv2)), FUZZ);    assertEquals(0, dv1.times(dv2).getDistanceSquared(sv1.times(v2)), FUZZ);    assertEquals(0, dv1.minus(dv2).getDistanceSquared(v1.minus(v2)), FUZZ);    assertEquals(0, dv1.minus(dv2).getDistanceSquared(v1.minus(dv2)), FUZZ);    assertEquals(0, dv1.minus(dv2).getDistanceSquared(v1.minus(sv2)), FUZZ);    assertEquals(0, dv1.minus(dv2).getDistanceSquared(sv1.minus(v2)), FUZZ);    double z = gen.nextDouble();    assertEquals(0, dv1.divide(z).getDistanceSquared(v1.divide(z)), 1.0e-12);    assertEquals(0, dv1.times(z).getDistanceSquared(v1.times(z)), 1.0e-12);    assertEquals(0, dv1.plus(z).getDistanceSquared(v1.plus(z)), 1.0e-12);    assertEquals(dv1.dot(dv2), v1.dot(v2), FUZZ);    assertEquals(dv1.dot(dv2), v1.dot(dv2), FUZZ);    assertEquals(dv1.dot(dv2), v1.dot(sv2), FUZZ);    assertEquals(dv1.dot(dv2), sv1.dot(v2), FUZZ);    assertEquals(dv1.dot(dv2), dv1.dot(v2), FUZZ);        assertEquals(dv1.getDistanceSquared(dv2), v1.getDistanceSquared(v2), FUZZ);    assertEquals(dv1.getDistanceSquared(dv2), dv1.getDistanceSquared(v2), FUZZ);    assertEquals(dv1.getDistanceSquared(dv2), sv1.getDistanceSquared(v2), FUZZ);    assertEquals(dv1.getDistanceSquared(dv2), v1.getDistanceSquared(dv2), FUZZ);    assertEquals(dv1.getDistanceSquared(dv2), v1.getDistanceSquared(sv2), FUZZ);        assertEquals(dv1.getLengthSquared(), v1.getLengthSquared(), FUZZ);    assertEquals(dv1.getDistanceSquared(dv2), v1.getDistanceSquared(v2), FUZZ);    assertEquals(dv1.getDistanceSquared(dv2), dv1.getDistanceSquared(v2), FUZZ);    assertEquals(dv1.getDistanceSquared(dv2), sv1.getDistanceSquared(v2), FUZZ);    assertEquals(dv1.getDistanceSquared(dv2), v1.getDistanceSquared(dv2), FUZZ);    assertEquals(dv1.getDistanceSquared(dv2), v1.getDistanceSquared(sv2), FUZZ);    assertEquals(dv1.minValue(), v1.minValue(), FUZZ);    assertEquals(dv1.minValueIndex(), v1.minValueIndex());    assertEquals(dv1.maxValue(), v1.maxValue(), FUZZ);    assertEquals(dv1.maxValueIndex(), v1.maxValueIndex());    Vector nv1 = v1.normalize();    assertEquals(0, dv1.getDistanceSquared(v1), FUZZ);    assertEquals(1, nv1.norm(2), FUZZ);    assertEquals(0, dv1.normalize().getDistanceSquared(nv1), FUZZ);    nv1 = v1.normalize(1);    assertEquals(0, dv1.getDistanceSquared(v1), FUZZ);    assertEquals(1, nv1.norm(1), FUZZ);    assertEquals(0, dv1.normalize(1).getDistanceSquared(nv1), FUZZ);    assertEquals(dv1.norm(0), v1.norm(0), FUZZ);    assertEquals(dv1.norm(1), v1.norm(1), FUZZ);    assertEquals(dv1.norm(1.5), v1.norm(1.5), FUZZ);    assertEquals(dv1.norm(2), v1.norm(2), FUZZ);    assertEquals(dv1.zSum(), v1.zSum(), FUZZ);    assertEquals(3.1 * v1.size(), v1.assign(3.1).zSum(), FUZZ);    assertEquals(0, v1.plus(-3.1).norm(1), FUZZ);    v1.assign(dv1);    assertEquals(0, v1.getDistanceSquared(dv1), FUZZ);    assertEquals(dv1.zSum() - dv1.size() * 3.4, v1.assign(Functions.minus(3.4)).zSum(), FUZZ);    assertEquals(dv1.zSum() - dv1.size() * 4.5, v1.assign(Functions.MINUS, 1.1).zSum(), FUZZ);    v1.assign(dv1);    assertEquals(0, dv1.minus(dv2).getDistanceSquared(v1.assign(v2, Functions.MINUS)), FUZZ);    v1.assign(dv1);    assertEquals(dv1.norm(2), Math.sqrt(v1.aggregate(Functions.PLUS, Functions.pow(2))), FUZZ);    assertEquals(dv1.dot(dv2), v1.aggregate(v2, Functions.PLUS, Functions.MULT), FUZZ);    assertEquals(dv1.viewPart(5, 10).zSum(), v1.viewPart(5, 10).zSum(), FUZZ);    Vector v3 = v1.clone();        assertTrue(v0.getClass().isAssignableFrom(v3.getClass()));    assertTrue(v3.getClass().isAssignableFrom(v0.getClass()));    assertEquals(0, v1.getDistanceSquared(v3), FUZZ);    assertNotSame(v1, v3);    v3.assign(0);    assertEquals(0, dv1.getDistanceSquared(v1), FUZZ);    assertEquals(0, v3.getLengthSquared(), FUZZ);    dv1.assign(Functions.ABS);    v1.assign(Functions.ABS);    assertEquals(0, dv1.logNormalize().getDistanceSquared(v1.logNormalize()), FUZZ);    assertEquals(0, dv1.logNormalize(1.5).getDistanceSquared(v1.logNormalize(1.5)), FUZZ);    for (Vector.Element element : v1.all()) {        assertEquals(dv1.get(element.index()), element.get(), 0);        assertEquals(dv1.get(element.index()), v1.get(element.index()), 0);        assertEquals(dv1.get(element.index()), v1.getQuick(element.index()), 0);    }}
0
 Vector getTestVector()
{    return test;}
0
public void setUp() throws Exception
{    super.setUp();    test = generateTestVector(2 * values.length + 1);    for (int i = 0; i < values.length; i++) {        test.set(2 * i + 1, values[i]);    }}
0
public void testCardinality()
{    assertEquals("size", 7, test.size());}
0
public void testIterator()
{    Iterator<Vector.Element> iterator = test.nonZeroes().iterator();    checkIterator(iterator, gold);    iterator = test.all().iterator();    checkIterator(iterator, gold);    double[] doubles = { 0.0, 5.0, 0, 3.0 };    RandomAccessSparseVector zeros = new RandomAccessSparseVector(doubles.length);    for (int i = 0; i < doubles.length; i++) {        zeros.setQuick(i, doubles[i]);    }    iterator = zeros.iterateNonZero();    checkIterator(iterator, doubles);    iterator = zeros.iterator();    checkIterator(iterator, doubles);    doubles = new double[] { 0.0, 0.0, 0, 0.0 };    zeros = new RandomAccessSparseVector(doubles.length);    for (int i = 0; i < doubles.length; i++) {        zeros.setQuick(i, doubles[i]);    }    iterator = zeros.iterateNonZero();    checkIterator(iterator, doubles);    iterator = zeros.iterator();    checkIterator(iterator, doubles);}
0
public void testIteratorSet()
{    Vector clone = test.clone();    for (Element e : clone.nonZeroes()) {        e.set(e.get() * 2.0);    }    for (Element e : clone.nonZeroes()) {        assertEquals(test.get(e.index()) * 2.0, e.get(), EPSILON);    }    clone = test.clone();    for (Element e : clone.all()) {        e.set(e.get() * 2.0);    }    for (Element e : clone.all()) {        assertEquals(test.get(e.index()) * 2.0, e.get(), EPSILON);    }}
0
public void testCopy()
{    Vector copy = test.clone();    for (int i = 0; i < test.size(); i++) {        assertEquals("copy [" + i + ']', test.get(i), copy.get(i), EPSILON);    }}
0
public void testGet()
{    for (int i = 0; i < test.size(); i++) {        if (i % 2 == 0) {            assertEquals("get [" + i + ']', 0.0, test.get(i), EPSILON);        } else {            assertEquals("get [" + i + ']', values[i / 2], test.get(i), EPSILON);        }    }}
0
public void testGetOver()
{    test.get(test.size());}
0
public void testGetUnder()
{    test.get(-1);}
0
public void testSet()
{    test.set(3, 4.5);    for (int i = 0; i < test.size(); i++) {        if (i % 2 == 0) {            assertEquals("get [" + i + ']', 0.0, test.get(i), EPSILON);        } else if (i == 3) {            assertEquals("set [" + i + ']', 4.5, test.get(i), EPSILON);        } else {            assertEquals("set [" + i + ']', values[i / 2], test.get(i), EPSILON);        }    }}
0
public void testSize()
{    assertEquals("size", 3, test.getNumNondefaultElements());}
0
public void testViewPart()
{    Vector part = test.viewPart(1, 2);    assertEquals("part size", 2, part.getNumNondefaultElements());    for (int i = 0; i < part.size(); i++) {        assertEquals("part[" + i + ']', test.get(i + 1), part.get(i), EPSILON);    }}
0
public void testViewPartUnder()
{    test.viewPart(-1, values.length);}
0
public void testViewPartOver()
{    test.viewPart(2, 7);}
0
public void testViewPartCardinality()
{    test.viewPart(1, 8);}
0
public void testSparseDoubleVectorInt()
{    Vector val = new RandomAccessSparseVector(4);    assertEquals("size", 4, val.size());    for (int i = 0; i < 4; i++) {        assertEquals("get [" + i + ']', 0.0, val.get(i), EPSILON);    }}
0
public void testDot()
{    double res = test.dot(test);    double expected = 3.3 * 3.3 + 2.2 * 2.2 + 1.1 * 1.1;    assertEquals("dot", expected, res, EPSILON);}
0
public void testDot2()
{    Vector test2 = test.clone();    test2.set(1, 0.0);    test2.set(3, 0.0);    assertEquals(3.3 * 3.3, test2.dot(test), EPSILON);}
0
public void testDotCardinality()
{    test.dot(new DenseVector(test.size() + 1));}
0
public void testNormalize()
{    Vector val = test.normalize();    double mag = Math.sqrt(1.1 * 1.1 + 2.2 * 2.2 + 3.3 * 3.3);    for (int i = 0; i < test.size(); i++) {        if (i % 2 == 0) {            assertEquals("get [" + i + ']', 0.0, val.get(i), EPSILON);        } else {            assertEquals("dot", values[i / 2] / mag, val.get(i), EPSILON);        }    }}
0
public void testMinus()
{    Vector val = test.minus(test);    assertEquals("size", test.size(), val.size());    for (int i = 0; i < test.size(); i++) {        assertEquals("get [" + i + ']', 0.0, val.get(i), EPSILON);    }    val = test.minus(test).minus(test);    assertEquals("cardinality", test.size(), val.size());    for (int i = 0; i < test.size(); i++) {        assertEquals("get [" + i + ']', 0.0, val.get(i) + test.get(i), EPSILON);    }    Vector val1 = test.plus(1);    val = val1.minus(test);    for (int i = 0; i < test.size(); i++) {        assertEquals("get [" + i + ']', 1.0, val.get(i), EPSILON);    }    val1 = test.plus(-1);    val = val1.minus(test);    for (int i = 0; i < test.size(); i++) {        assertEquals("get [" + i + ']', -1.0, val.get(i), EPSILON);    }}
0
public void testPlusDouble()
{    Vector val = test.plus(1);    assertEquals("size", test.size(), val.size());    for (int i = 0; i < test.size(); i++) {        if (i % 2 == 0) {            assertEquals("get [" + i + ']', 1.0, val.get(i), EPSILON);        } else {            assertEquals("get [" + i + ']', values[i / 2] + 1.0, val.get(i), EPSILON);        }    }}
0
public void testPlusVector()
{    Vector val = test.plus(test);    assertEquals("size", test.size(), val.size());    for (int i = 0; i < test.size(); i++) {        if (i % 2 == 0) {            assertEquals("get [" + i + ']', 0.0, val.get(i), EPSILON);        } else {            assertEquals("get [" + i + ']', values[i / 2] * 2.0, val.get(i), EPSILON);        }    }}
0
public void testPlusVectorCardinality()
{    test.plus(new DenseVector(test.size() + 1));}
0
public void testTimesDouble()
{    Vector val = test.times(3);    assertEquals("size", test.size(), val.size());    for (int i = 0; i < test.size(); i++) {        if (i % 2 == 0) {            assertEquals("get [" + i + ']', 0.0, val.get(i), EPSILON);        } else {            assertEquals("get [" + i + ']', values[i / 2] * 3.0, val.get(i), EPSILON);        }    }}
0
public void testDivideDouble()
{    Vector val = test.divide(3);    assertEquals("size", test.size(), val.size());    for (int i = 0; i < test.size(); i++) {        if (i % 2 == 0) {            assertEquals("get [" + i + ']', 0.0, val.get(i), EPSILON);        } else {            assertEquals("get [" + i + ']', values[i / 2] / 3.0, val.get(i), EPSILON);        }    }}
0
public void testTimesVector()
{    Vector val = test.times(test);    assertEquals("size", test.size(), val.size());    for (int i = 0; i < test.size(); i++) {        if (i % 2 == 0) {            assertEquals("get [" + i + ']', 0.0, val.get(i), EPSILON);        } else {            assertEquals("get [" + i + ']', values[i / 2] * values[i / 2], val.get(i), EPSILON);        }    }}
0
public void testTimesVectorCardinality()
{    test.times(new DenseVector(test.size() + 1));}
0
public void testZSum()
{    double expected = 0;    for (double value : values) {        expected += value;    }    assertEquals("wrong zSum", expected, test.zSum(), EPSILON);}
0
public void testGetDistanceSquared()
{    Vector other = new RandomAccessSparseVector(test.size());    other.set(1, -2);    other.set(2, -5);    other.set(3, -9);    other.set(4, 1);    double expected = test.minus(other).getLengthSquared();    assertTrue("a.getDistanceSquared(b) != a.minus(b).getLengthSquared", Math.abs(expected - test.getDistanceSquared(other)) < 10.0E-7);}
0
public void testAssignDouble()
{    test.assign(0);    for (int i = 0; i < values.length; i++) {        assertEquals("value[" + i + ']', 0.0, test.getQuick(i), EPSILON);    }}
0
public void testAssignDoubleArray()
{    double[] array = new double[test.size()];    test.assign(array);    for (int i = 0; i < values.length; i++) {        assertEquals("value[" + i + ']', 0.0, test.getQuick(i), EPSILON);    }}
0
public void testAssignDoubleArrayCardinality()
{    double[] array = new double[test.size() + 1];    test.assign(array);}
0
public void testAssignVector()
{    Vector other = new DenseVector(test.size());    test.assign(other);    for (int i = 0; i < values.length; i++) {        assertEquals("value[" + i + ']', 0.0, test.getQuick(i), EPSILON);    }}
0
public void testAssignVectorCardinality()
{    Vector other = new DenseVector(test.size() - 1);    test.assign(other);}
0
public void testAssignUnaryFunction()
{    test.assign(Functions.NEGATE);    for (int i = 1; i < values.length; i += 2) {        assertEquals("value[" + i + ']', -values[i], test.getQuick(i + 2), EPSILON);    }}
0
public void testAssignBinaryFunction()
{    test.assign(test, Functions.PLUS);    for (int i = 0; i < values.length; i++) {        if (i % 2 == 0) {            assertEquals("get [" + i + ']', 0.0, test.get(i), EPSILON);        } else {            assertEquals("value[" + i + ']', 2 * values[i - 1], test.getQuick(i), EPSILON);        }    }}
0
public void testAssignBinaryFunction2()
{    test.assign(Functions.plus(4));    for (int i = 0; i < values.length; i++) {        if (i % 2 == 0) {            assertEquals("get [" + i + ']', 4.0, test.get(i), EPSILON);        } else {            assertEquals("value[" + i + ']', values[i - 1] + 4, test.getQuick(i), EPSILON);        }    }}
0
public void testAssignBinaryFunction3()
{    test.assign(Functions.mult(4));    for (int i = 0; i < values.length; i++) {        if (i % 2 == 0) {            assertEquals("get [" + i + ']', 0.0, test.get(i), EPSILON);        } else {            assertEquals("value[" + i + ']', values[i - 1] * 4, test.getQuick(i), EPSILON);        }    }}
0
public void testLike()
{    Vector other = test.like();    assertTrue("not like", test.getClass().isAssignableFrom(other.getClass()));    assertEquals("size", test.size(), other.size());}
0
public void testCrossProduct()
{    Matrix result = test.cross(test);    assertEquals("row size", test.size(), result.rowSize());    assertEquals("col size", test.size(), result.columnSize());    for (int row = 0; row < result.rowSize(); row++) {        for (int col = 0; col < result.columnSize(); col++) {            assertEquals("cross[" + row + "][" + col + ']', test.getQuick(row) * test.getQuick(col), result.getQuick(row, col), EPSILON);        }    }}
0
public void testIterators()
{    final T v0 = vectorToTest(20);    double sum = 0;    int elements = 0;    int nonZero = 0;    for (Element element : v0.all()) {        elements++;        sum += element.get();        if (element.get() != 0) {            nonZero++;        }    }    int nonZeroIterated = Iterables.size(v0.nonZeroes());    assertEquals(20, elements);    assertEquals(v0.size(), elements);    assertEquals(nonZeroIterated, nonZero);    assertEquals(v0.zSum(), sum, 0);}
0
public void testSmallDistances()
{    for (double fuzz : new double[] { 1.0e-5, 1.0e-6, 1.0e-7, 1.0e-8, 1.0e-9, 1.0e-10 }) {        MultiNormal x = new MultiNormal(fuzz, new ConstantVector(0, 20));        for (int i = 0; i < 10000; i++) {            final T v1 = vectorToTest(20);            Vector v2 = v1.plus(x.sample());            if (1 + fuzz * fuzz > 1) {                String msg = String.format("fuzz = %.1g, >", fuzz);                assertTrue(msg, v1.getDistanceSquared(v2) > 0);                assertTrue(msg, v2.getDistanceSquared(v1) > 0);            } else {                String msg = String.format("fuzz = %.1g, >=", fuzz);                assertTrue(msg, v1.getDistanceSquared(v2) >= 0);                assertTrue(msg, v2.getDistanceSquared(v1) >= 0);            }        }    }}
0
public void testToString()
{    Vector w;    w = generateTestVector(20);    w.set(0, 1.1);    w.set(13, 100500.);    w.set(19, 3.141592);    assertEquals("{0:1.1,13:100500.0,19:3.141592}", w.toString());    w = generateTestVector(12);    w.set(10, 0.1);    assertEquals("{10:0.1}", w.toString());    w = generateTestVector(12);    assertEquals("{}", w.toString());}
0
public void testYtY()
{    double[][] testMatrix = new double[][] { new double[] { 1, 2, 3, 4, 5 }, new double[] { 1, 2, 3, 4, 5 }, new double[] { 1, 2, 3, 4, 5 }, new double[] { 1, 2, 3, 4, 5 }, new double[] { 1, 2, 3, 4, 5 } };    double[][] testMatrix2 = new double[][] { new double[] { 1, 2, 3, 4, 5, 6 }, new double[] { 5, 4, 3, 2, 1, 7 }, new double[] { 1, 2, 3, 4, 5, 8 }, new double[] { 1, 2, 3, 4, 5, 8 }, new double[] { 11, 12, 13, 20, 27, 8 } };    double[][][] testData = new double[][][] { testMatrix, testMatrix2 };    for (int i = 0; i < testData.length; i++) {        Matrix matrixToTest = new DenseMatrix(testData[i]);                for (int j = 0; j < 100; j++) {            validateYtY(matrixToTest, 4);        }                validateYtY(matrixToTest, 1);    }}
0
private void validateYtY(Matrix matrixToTest, int numThreads)
{    OpenIntObjectHashMap<Vector> matrixToTestAsRowVectors = asRowVectors(matrixToTest);    ImplicitFeedbackAlternatingLeastSquaresSolver solver = new ImplicitFeedbackAlternatingLeastSquaresSolver(matrixToTest.columnSize(), 1, 1, matrixToTestAsRowVectors, numThreads);    Matrix yTy = matrixToTest.transpose().times(matrixToTest);    Matrix shouldMatchyTy = solver.getYtransposeY(matrixToTestAsRowVectors);    for (int row = 0; row < yTy.rowSize(); row++) {        for (int column = 0; column < yTy.columnSize(); column++) {            assertEquals(yTy.getQuick(row, column), shouldMatchyTy.getQuick(row, column), 0);        }    }}
0
private OpenIntObjectHashMap<Vector> asRowVectors(Matrix matrix)
{    OpenIntObjectHashMap<Vector> rows = new OpenIntObjectHashMap<>();    for (int row = 0; row < matrix.numRows(); row++) {        rows.put(row, matrix.viewRow(row).clone());    }    return rows;}
0
public void addLambdaTimesNuiTimesE()
{    int nui = 5;    double lambda = 0.2;    Matrix matrix = new SparseMatrix(5, 5);    AlternatingLeastSquaresSolver.addLambdaTimesNuiTimesE(matrix, lambda, nui);    for (int n = 0; n < 5; n++) {        assertEquals(1.0, matrix.getQuick(n, n), EPSILON);    }}
0
public void createMiIi()
{    Vector f1 = new DenseVector(new double[] { 1, 2, 3 });    Vector f2 = new DenseVector(new double[] { 4, 5, 6 });    Matrix miIi = AlternatingLeastSquaresSolver.createMiIi(Arrays.asList(f1, f2), 3);    assertEquals(1.0, miIi.getQuick(0, 0), EPSILON);    assertEquals(2.0, miIi.getQuick(1, 0), EPSILON);    assertEquals(3.0, miIi.getQuick(2, 0), EPSILON);    assertEquals(4.0, miIi.getQuick(0, 1), EPSILON);    assertEquals(5.0, miIi.getQuick(1, 1), EPSILON);    assertEquals(6.0, miIi.getQuick(2, 1), EPSILON);}
0
public void createRiIiMaybeTransposed()
{    Vector ratings = new SequentialAccessSparseVector(3);    ratings.setQuick(1, 1.0);    ratings.setQuick(3, 3.0);    ratings.setQuick(5, 5.0);    Matrix riIiMaybeTransposed = AlternatingLeastSquaresSolver.createRiIiMaybeTransposed(ratings);    assertEquals(1, riIiMaybeTransposed.numCols(), 1);    assertEquals(3, riIiMaybeTransposed.numRows(), 3);    assertEquals(1.0, riIiMaybeTransposed.getQuick(0, 0), EPSILON);    assertEquals(3.0, riIiMaybeTransposed.getQuick(1, 0), EPSILON);    assertEquals(5.0, riIiMaybeTransposed.getQuick(2, 0), EPSILON);}
0
public void createRiIiMaybeTransposedExceptionOnNonSequentialVector()
{    Vector ratings = new RandomAccessSparseVector(3);    ratings.setQuick(1, 1.0);    ratings.setQuick(3, 3.0);    ratings.setQuick(5, 5.0);    try {        AlternatingLeastSquaresSolver.createRiIiMaybeTransposed(ratings);        fail();    } catch (IllegalArgumentException e) {    }}
0
public void testUpdate()
{    MultiNormal f = new MultiNormal(20);    Vector a = f.sample();    Vector b = f.sample();    Vector c = f.sample();    DenseVector x = new DenseVector(a);    Centroid x1 = new Centroid(1, x);    x1.update(new Centroid(2, new DenseVector(b)));    Centroid x2 = new Centroid(x1);    x1.update(c);        Vector mean = a.plus(b).plus(c).assign(Functions.div(3));    assertEquals(0, x1.getVector().minus(mean).norm(1), 1.0e-8);    assertEquals(3, x1.getWeight(), 0);    assertEquals(0, x2.minus(a.plus(b).divide(2)).norm(1), 1.0e-8);    assertEquals(2, x2.getWeight(), 0);    assertEquals(0, new Centroid(x1.getIndex(), x1, x1.getWeight()).minus(x1).norm(1), 1.0e-8);        assertEquals(0, x.minus(x1).norm(1), 0);    assertEquals(3, x1.getWeight(), 1.0e-8);    assertEquals(1, x1.getIndex());}
0
public Centroid vectorToTest(int size)
{    return new Centroid(new WeightedVector(new DenseVector(size), 3.15, 51));}
0
public void testSize()
{    assertEquals("size", 3, getTestVector().getNumNonZeroElements());}
0
 Vector generateTestVector(int cardinality)
{    return new Centroid(new WeightedVector(new DenseVector(cardinality), 3.14, 53));}
0
public void rank1()
{    Matrix x = new DenseMatrix(3, 3);    x.viewRow(0).assign(new double[] { 1, 2, 3 });    x.viewRow(1).assign(new double[] { 2, 4, 6 });    x.viewRow(2).assign(new double[] { 3, 6, 9 });    CholeskyDecomposition rr = new CholeskyDecomposition(x.transpose().times(x), false);    assertEquals(0, new DenseVector(new double[] { 3.741657, 7.483315, 11.22497 }).aggregate(rr.getL().transpose().viewRow(0), Functions.PLUS, new DoubleDoubleFunction() {        @Override        public double apply(double arg1, double arg2) {            return Math.abs(arg1) - Math.abs(arg2);        }    }), 1.0e-5);    assertEquals(0, rr.getL().viewPart(0, 3, 1, 2).aggregate(Functions.PLUS, Functions.ABS), 1.0e-9);}
0
public double apply(double arg1, double arg2)
{    return Math.abs(arg1) - Math.abs(arg2);}
0
public void test1()
{    final Random rand = RandomUtils.getRandom();    Matrix z = new DenseMatrix(100, 100);    z.assign(new DoubleFunction() {        @Override        public double apply(double arg1) {            return rand.nextDouble();        }    });    Matrix A = z.times(z.transpose());    for (boolean type = false; !type; type = true) {        CholeskyDecomposition cd = new CholeskyDecomposition(A, type);        Matrix L = cd.getL();                Matrix Abar = L.times(L.transpose());        double error = A.minus(Abar).aggregate(Functions.MAX, Functions.ABS);        Assert.assertEquals("type = " + type, 0, error, 1.0e-10);                Matrix q = cd.solveLeft(z);        Matrix id = q.times(q.transpose());        for (int i = 0; i < id.columnSize(); i++) {            Assert.assertEquals("type = " + type, 1, id.get(i, i), 1.0e-9);            Assert.assertEquals("type = " + type, 1, id.viewRow(i).norm(1), 1.0e-9);        }                q = cd.solveRight(z.transpose());        id = q.transpose().times(q);        for (int i = 0; i < id.columnSize(); i++) {            Assert.assertEquals("type = " + type, 1, id.get(i, i), 1.0e-9);            Assert.assertEquals("type = " + type, 1, id.viewRow(i).norm(1), 1.0e-9);        }    }}
0
public double apply(double arg1)
{    return rand.nextDouble();}
0
public void test2()
{        double[][] values = new double[3][];    values[0] = new double[] { 1, -1, 1 };    values[1] = new double[] { -1, 1, -1 };    values[2] = new double[] { 1, -1, 2 };    Matrix A = new DenseMatrix(values);        CholeskyDecomposition cd = new CholeskyDecomposition(A, false);    assertEquals(0, cd.getL().times(cd.getL().transpose()).minus(A).aggregate(Functions.PLUS, Functions.ABS), 1.0e-10);        cd = new CholeskyDecomposition(A);    assertEquals(0, cd.getL().times(cd.getL().transpose()).minus(A).aggregate(Functions.PLUS, Functions.ABS), 1.0e-10);}
0
public void testRankDeficient()
{    Matrix A = rank4Matrix();    CholeskyDecomposition cd = new CholeskyDecomposition(A);    PivotedMatrix Ax = new PivotedMatrix(A, cd.getPivot());    CholeskyDecomposition cd2 = new CholeskyDecomposition(Ax, false);    assertEquals(0, cd2.getL().times(cd2.getL().transpose()).minus(Ax).aggregate(Functions.PLUS, Functions.ABS), 1.0e-10);    assertEquals(0, cd.getL().times(cd.getL().transpose()).minus(A).aggregate(Functions.PLUS, Functions.ABS), 1.0e-10);    Assert.assertFalse(cd.isPositiveDefinite());    Matrix L = cd.getL();    Matrix Abar = L.times(L.transpose());    double error = A.minus(Abar).aggregate(Functions.MAX, Functions.ABS);    Assert.assertEquals(0, error, 1.0e-10);}
0
private static Matrix rank4Matrix()
{    final Random rand = RandomUtils.getRandom();    Matrix u = new DenseMatrix(10, 4);    u.assign(new DoubleFunction() {        @Override        public double apply(double arg1) {            return rand.nextDouble();        }    });    Matrix v = new DenseMatrix(10, 4);    v.assign(new DoubleFunction() {        @Override        public double apply(double arg1) {            return rand.nextDouble();        }    });    Matrix z = u.times(v.transpose());    return z.times(z.transpose());}
0
public double apply(double arg1)
{    return rand.nextDouble();}
0
public double apply(double arg1)
{    return rand.nextDouble();}
0
public static long timeSolver(Matrix corpus, double convergence, int maxNumPasses, TrainingState state)
{    return timeSolver(corpus, convergence, maxNumPasses, 10, state);}
0
public static long timeSolver(Matrix corpus, double convergence, int maxNumPasses, int desiredRank, TrainingState state)
{    HebbianUpdater updater = new HebbianUpdater();    AsyncEigenVerifier verifier = new AsyncEigenVerifier();    HebbianSolver solver = new HebbianSolver(updater, verifier, convergence, maxNumPasses);    long start = System.nanoTime();    TrainingState finalState = solver.solve(corpus, desiredRank);    assertNotNull(finalState);    state.setCurrentEigens(finalState.getCurrentEigens());    state.setCurrentEigenValues(finalState.getCurrentEigenValues());    long time = 0L;    time += System.nanoTime() - start;    verifier.close();    assertEquals(state.getCurrentEigens().numRows(), desiredRank);    return time / 1000000L;}
0
public static long timeSolver(Matrix corpus, TrainingState state)
{    return timeSolver(corpus, state, 10);}
0
public static long timeSolver(Matrix corpus, TrainingState state, int rank)
{    return timeSolver(corpus, 0.01, 20, rank, state);}
0
public void testHebbianSolver()
{    int numColumns = 800;    Matrix corpus = randomSequentialAccessSparseMatrix(1000, 900, numColumns, 30, 1.0);    int rank = 50;    Matrix eigens = new DenseMatrix(rank, numColumns);    TrainingState state = new TrainingState(eigens, null);    long optimizedTime = timeSolver(corpus, 0.00001, 5, rank, state);    eigens = state.getCurrentEigens();    assertEigen(eigens, corpus, 0.05, false);    assertOrthonormal(eigens, 1.0e-6);    System.out.println("Avg solving (Hebbian) time in ms: " + optimizedTime);}
0
public void testEigenvalueCheck() throws Exception
{    int size = 100;    Matrix m = randomHierarchicalSymmetricMatrix(size);    Vector initialVector = new DenseVector(size);    initialVector.assign(1.0 / Math.sqrt(size));    LanczosSolver solver = new LanczosSolver();    int desiredRank = 80;    LanczosState state = new LanczosState(m, desiredRank, initialVector);        solver.solve(state, desiredRank, true);    EigenDecomposition decomposition = new EigenDecomposition(m);    Vector eigenvalues = decomposition.getRealEigenvalues();    float fractionOfEigensExpectedGood = 0.6f;    for (int i = 0; i < fractionOfEigensExpectedGood * desiredRank; i++) {        double s = state.getSingularValue(i);        double e = eigenvalues.get(i);                assertTrue("Singular value differs from eigenvalue", Math.abs((s - e) / e) < ERROR_TOLERANCE);        Vector v = state.getRightSingularVector(i);        Vector v2 = decomposition.getV().viewColumn(i);        double error = 1 - Math.abs(v.dot(v2) / (v.norm(2) * v2.norm(2)));                assertTrue(i + ": 1 - cosAngle = " + error, error < ERROR_TOLERANCE);    }}
1
public void testLanczosSolver() throws Exception
{    int numRows = 800;    int numColumns = 500;    Matrix corpus = randomHierarchicalMatrix(numRows, numColumns, false);    Vector initialVector = new DenseVector(numColumns);    initialVector.assign(1.0 / Math.sqrt(numColumns));    int rank = 50;    LanczosState state = new LanczosState(corpus, rank, initialVector);    LanczosSolver solver = new LanczosSolver();    solver.solve(state, rank, false);    assertOrthonormal(state);    for (int i = 0; i < rank / 2; i++) {        assertEigen(i, state.getRightSingularVector(i), corpus, ERROR_TOLERANCE, false);    }}
0
public void testLanczosSolverSymmetric() throws Exception
{    int numCols = 500;    Matrix corpus = randomHierarchicalSymmetricMatrix(numCols);    Vector initialVector = new DenseVector(numCols);    initialVector.assign(1.0 / Math.sqrt(numCols));    int rank = 30;    LanczosState state = new LanczosState(corpus, rank, initialVector);    LanczosSolver solver = new LanczosSolver();    solver.solve(state, rank, true);}
0
public static void assertOrthonormal(Matrix eigens)
{    assertOrthonormal(eigens, 1.0e-6);}
0
public static void assertOrthonormal(Matrix currentEigens, double errorMargin)
{    List<String> nonOrthogonals = Lists.newArrayList();    for (int i = 0; i < currentEigens.numRows(); i++) {        Vector ei = currentEigens.viewRow(i);        for (int j = 0; j <= i; j++) {            Vector ej = currentEigens.viewRow(j);            if (ei.norm(2) == 0 || ej.norm(2) == 0) {                continue;            }            double dot = ei.dot(ej);            if (i == j) {                assertTrue("not norm 1 : " + dot + " (eigen #" + i + ')', Math.abs(1.0 - dot) < errorMargin);            } else {                if (Math.abs(dot) > errorMargin) {                                        nonOrthogonals.add("(" + i + ',' + j + ')');                }            }        }            }}
1
public static void assertOrthonormal(LanczosState state)
{    double errorMargin = 1.0e-5;    List<String> nonOrthogonals = Lists.newArrayList();    for (int i = 0; i < state.getIterationNumber(); i++) {        Vector ei = state.getRightSingularVector(i);        for (int j = 0; j <= i; j++) {            Vector ej = state.getRightSingularVector(j);            if (ei.norm(2) == 0 || ej.norm(2) == 0) {                continue;            }            double dot = ei.dot(ej);            if (i == j) {                assertTrue("not norm 1 : " + dot + " (eigen #" + i + ')', Math.abs(1.0 - dot) < errorMargin);            } else {                if (Math.abs(dot) > errorMargin) {                                        nonOrthogonals.add("(" + i + ',' + j + ')');                }            }        }        if (!nonOrthogonals.isEmpty()) {                    }    }}
1
public static void assertEigen(Matrix eigens, VectorIterable corpus, double errorMargin, boolean isSymmetric)
{    assertEigen(eigens, corpus, eigens.numRows(), errorMargin, isSymmetric);}
0
public static void assertEigen(Matrix eigens, VectorIterable corpus, int numEigensToCheck, double errorMargin, boolean isSymmetric)
{    for (int i = 0; i < numEigensToCheck; i++) {        Vector e = eigens.viewRow(i);        assertEigen(i, e, corpus, errorMargin, isSymmetric);    }}
0
public static void assertEigen(int i, Vector e, VectorIterable corpus, double errorMargin, boolean isSymmetric)
{    if (e.getLengthSquared() == 0) {        return;    }    Vector afterMultiply = isSymmetric ? corpus.times(e) : corpus.timesSquared(e);    double dot = afterMultiply.dot(e);    double afterNorm = afterMultiply.getLengthSquared();    double error = 1 - Math.abs(dot / Math.sqrt(afterNorm * e.getLengthSquared()));        assertTrue("Error: {" + error + " too high! (for eigen " + i + ')', Math.abs(error) < errorMargin);}
1
public static Matrix randomSequentialAccessSparseMatrix(int numRows, int nonNullRows, int numCols, int entriesPerRow, double entryMean)
{    Matrix m = new SparseRowMatrix(numRows, numCols);        Random r = RandomUtils.getRandom();    for (int i = 0; i < nonNullRows; i++) {        Vector v = new SequentialAccessSparseVector(numCols);        for (int j = 0; j < entriesPerRow; j++) {            int col = r.nextInt(numCols);            double val = r.nextGaussian();            v.set(col, val * entryMean);        }        int c = r.nextInt(numRows);        if (r.nextBoolean() || numRows == nonNullRows) {            m.assignRow(numRows == nonNullRows ? i : c, v);        } else {            Vector other = m.viewRow(r.nextInt(numRows));            if (other != null && other.getLengthSquared() > 0) {                m.assignRow(c, other.clone());            }        }        }    return m;}
0
public static Matrix randomHierarchicalMatrix(int numRows, int numCols, boolean symmetric)
{    Matrix matrix = new DenseMatrix(numRows, numCols);        Random r = new Random(1234L);    for (int row = 0; row < numRows; row++) {        Vector v = new DenseVector(numCols);        for (int col = 0; col < numCols; col++) {            double val = r.nextGaussian();            v.set(col, val);        }        v.assign(Functions.MULT, 1 / ((row + 1) * v.norm(2)));        matrix.assignRow(row, v);    }    if (symmetric) {        return matrix.times(matrix.transpose());    }    return matrix;}
0
public static Matrix randomHierarchicalSymmetricMatrix(int size)
{    return randomHierarchicalMatrix(size, size, true);}
0
public void testBasics()
{    Matrix a = new DenseSymmetricMatrix(new double[] { 1, 2, 3, 4, 5, 6, 7, 8, 9, 10 }, false);    System.out.println(a.toString());    assertEquals(0, a.viewDiagonal().minus(new DenseVector(new double[] { 1, 5, 8, 10 })).norm(1), 1.0e-10);    assertEquals(0, a.viewPart(0, 3, 1, 3).viewDiagonal().minus(new DenseVector(new double[] { 2, 6, 9 })).norm(1), 1.0e-10);    assertEquals(4, a.get(0, 3), 1.0e-10);    System.out.println(a);    Matrix m = new DenseMatrix(4, 4).assign(a);    assertEquals(0, m.minus(a).aggregate(Functions.PLUS, Functions.ABS), 1.0e-10);    System.out.println(m);    assertEquals(0, m.transpose().times(m).minus(a.transpose().times(a)).aggregate(Functions.PLUS, Functions.ABS), 1.0e-10);    System.out.println(a.plus(a));    assertEquals(0, m.plus(m).minus(a.plus(a)).aggregate(Functions.PLUS, Functions.ABS), 1.0e-10);}
0
public void testEigen()
{    Matrix a = new DenseSymmetricMatrix(new double[] { 1, 2, 3, 4, 5, 6, 7, 8, 9, 10 }, false);    Matrix b = new DenseMatrix(a.numRows(), a.numCols());    b.assign(a);    assertEquals(0, a.minus(b).aggregate(Functions.PLUS, Functions.ABS), 1.0e-10);    EigenDecomposition edA = new EigenDecomposition(a);    EigenDecomposition edB = new EigenDecomposition(b);    System.out.println(edA.getV());    assertEquals(0, edA.getV().minus(edB.getV()).aggregate(Functions.PLUS, Functions.ABS), 1.0e-10);    assertEquals(0, edA.getRealEigenvalues().minus(edA.getRealEigenvalues()).aggregate(Functions.PLUS, Functions.ABS), 1.0e-10);}
0
public void testBasics()
{    DiagonalMatrix a = new DiagonalMatrix(new double[] { 1, 2, 3, 4 });    assertEquals(0, a.viewDiagonal().minus(new DenseVector(new double[] { 1, 2, 3, 4 })).norm(1), 1.0e-10);    assertEquals(0, a.viewPart(0, 3, 0, 3).viewDiagonal().minus(new DenseVector(new double[] { 1, 2, 3 })).norm(1), 1.0e-10);    assertEquals(4, a.get(3, 3), 1.0e-10);    Matrix m = new DenseMatrix(4, 4);    m.assign(a);    assertEquals(0, m.minus(a).aggregate(Functions.PLUS, Functions.ABS), 1.0e-10);    assertEquals(0, m.transpose().times(m).minus(a.transpose().times(a)).aggregate(Functions.PLUS, Functions.ABS), 1.0e-10);    assertEquals(0, m.plus(m).minus(a.plus(a)).aggregate(Functions.PLUS, Functions.ABS), 1.0e-10);    m = new DenseMatrix(new double[][] { { 1, 2, 3, 4 }, { 5, 6, 7, 8 } });    assertEquals(100, a.timesLeft(m).aggregate(Functions.PLUS, Functions.ABS), 1.0e-10);    assertEquals(100, a.times(m.transpose()).aggregate(Functions.PLUS, Functions.ABS), 1.0e-10);}
0
public void testSparsity()
{    Vector d = new DenseVector(10);    for (int i = 0; i < 10; i++) {        d.set(i, i * i);    }    DiagonalMatrix m = new DiagonalMatrix(d);    Assert.assertFalse(m.viewRow(0).isDense());    Assert.assertFalse(m.viewColumn(0).isDense());    for (int i = 0; i < 10; i++) {        assertEquals(i * i, m.viewRow(i).zSum(), 0);        assertEquals(i * i, m.viewRow(i).get(i), 0);        assertEquals(i * i, m.viewColumn(i).zSum(), 0);        assertEquals(i * i, m.viewColumn(i).get(i), 0);    }    Iterator<Vector.Element> ix = m.viewRow(7).nonZeroes().iterator();    assertTrue(ix.hasNext());    Vector.Element r = ix.next();    assertEquals(7, r.index());    assertEquals(49, r.get(), 0);    assertFalse(ix.hasNext());    assertEquals(0, m.viewRow(5).get(3), 0);    assertEquals(0, m.viewColumn(8).get(3), 0);    m.viewRow(3).set(3, 1);    assertEquals(1, m.get(3, 3), 0);    for (Vector.Element element : m.viewRow(6).all()) {        if (element.index() == 6) {            assertEquals(36, element.get(), 0);        } else {            assertEquals(0, element.get(), 0);        }    }}
0
public void testBigMatrix() throws IOException
{        Assume.assumeNotNull(System.getProperty("runSlowTests"));    Matrix m0 = new SparseRowMatrix(ROWS, COLUMNS);    Random gen = RandomUtils.getRandom();    for (int i = 0; i < 1000; i++) {        m0.set(gen.nextInt(ROWS), gen.nextInt(COLUMNS), matrixValue(i));    }    File f = File.createTempFile("foo", ".m", getTestTempDir());    f.deleteOnExit();    System.out.printf("Starting to write to %s\n", f.getAbsolutePath());    FileBasedMatrix.writeMatrix(f, m0);    System.out.printf("done\n");    System.out.printf("File is %.1f MB\n", f.length() / 1.0e6);    FileBasedMatrix m1 = new FileBasedMatrix(ROWS, COLUMNS);    System.out.printf("Starting read\n");    m1.setData(f, false);    gen = RandomUtils.getRandom();    for (int i = 0; i < 1000; i++) {        assertEquals(matrixValue(i), m1.get(gen.nextInt(ROWS), gen.nextInt(COLUMNS)), 0.0);    }    System.out.printf("done\n");}
0
private static int matrixValue(int i)
{    return (i * 88513) % 10000;}
0
public void testSetData() throws IOException
{    File f = File.createTempFile("matrix", ".m", getTestTempDir());    f.deleteOnExit();    Matrix m0 = new DenseMatrix(100000, 30);    MultiNormal gen = new MultiNormal(30);    for (MatrixSlice row : m0) {        row.vector().assign(gen.sample());    }    FileBasedMatrix.writeMatrix(f, m0);    FileBasedMatrix m = new FileBasedMatrix(100000, 30);    m.setData(f, true);    assertEquals(0, m0.minus(m).aggregate(Functions.MAX, Functions.ABS), 1.0e-8);    int i = 0;    for (MatrixSlice row : m) {        assertEquals(0, row.vector().minus(m0.viewRow(i++)).norm(1), 1.0e-8);    }}
0
public void testSetData() throws IOException
{    File f = File.createTempFile("matrix", ".m", getTestTempDir());    f.deleteOnExit();    Random gen = RandomUtils.getRandom();    Matrix m0 = new SparseRowMatrix(10, 21);    for (MatrixSlice row : m0) {        int len = (int) Math.ceil(-15 * Math.log(1 - gen.nextDouble()));        for (int i = 0; i < len; i++) {            row.vector().set(gen.nextInt(21), 1);        }    }    FileBasedSparseBinaryMatrix.writeMatrix(f, m0);    FileBasedSparseBinaryMatrix m = new FileBasedSparseBinaryMatrix(10, 21);    m.setData(f);    for (MatrixSlice row : m) {        Vector diff = row.vector().minus(m0.viewRow(row.index()));        double error = diff.norm(1);        if (error > 1.0e-14) {            System.out.printf("%s\n", diff);        }        assertEquals(0, error, 1.0e-14);    }}
0
public static Collection<Object[]> generateData()
{    List<Object[]> data = Lists.newArrayList();    for (Field field : Functions.class.getDeclaredFields()) {        if (field.getType().isAssignableFrom(DoubleDoubleFunction.class) && Modifier.isStatic(field.getModifiers()) && !field.getName().equals("SECOND_LEFT_ZERO")) {            try {                data.add(new Object[] { field.get(null), field.getName() });            } catch (IllegalAccessException e) {                System.out.printf("Couldn't access Functions field %s\n", field.getName());            }        }    }    return data;}
0
public void testIsLikeRightPlus()
{    if (!function.isLikeRightPlus()) {        return;    }    for (int i = 0; i < NUM_POINTS; ++i) {        double x = random.nextDouble();        assertEquals(functionName, x, function.apply(x, 0), 0);    }}
0
public void testIsLikeLeftMult()
{    if (!function.isLikeLeftMult()) {        return;    }    for (int i = 0; i < NUM_POINTS; ++i) {        double y = random.nextDouble();        assertEquals(functionName, 0, function.apply(0, y), 0);    }}
0
public void testIsLikeRightMult()
{    if (!function.isLikeRightMult()) {        return;    }    for (int i = 0; i < NUM_POINTS; ++i) {        double x = random.nextDouble();        assertEquals(functionName, 0, function.apply(x, 0), 0);    }}
0
public void testIsCommutative()
{    if (!function.isCommutative()) {        return;    }    for (int i = 0; i < NUM_POINTS; ++i) {        double x = random.nextDouble();        double y = random.nextDouble();        assertEquals(functionName, function.apply(x, y), function.apply(y, x), Constants.EPSILON);    }}
0
public void testIsAssociative()
{    if (!function.isAssociative()) {        return;    }    for (int i = 0; i < NUM_POINTS; ++i) {        double x = random.nextDouble();        double y = random.nextDouble();        double z = random.nextDouble();        assertEquals(functionName, function.apply(x, function.apply(y, z)), function.apply(function.apply(x, y), z), Constants.EPSILON);    }}
0
public void testIsDensifying()
{    if (!function.isDensifying()) {        assertEquals(functionName, 0, function.apply(0, 0), 0);    }}
0
public static void checkDistribution(final AbstractContinousDistribution dist, double[] x, double offset, double scale, int n)
{    double[] xs = Arrays.copyOf(x, x.length);    for (int i = 0; i < xs.length; i++) {        xs[i] = xs[i] * scale + offset;    }    Arrays.sort(xs);        double[] y = new double[n];    for (int i = 0; i < n; i++) {        y[i] = dist.nextDouble();    }    Arrays.sort(y);        double[] p = new double[xs.length + 1];    double lastP = 0;    for (int i = 0; i < xs.length; i++) {        double thisP = dist.cdf(xs[i]);        p[i] = thisP - lastP;        lastP = thisP;    }    p[p.length - 1] = 1 - lastP;        int[] k = new int[xs.length + 1];    int lastJ = 0;    for (int i = 0; i < k.length - 1; i++) {        int j = 0;        while (j < n && y[j] < xs[i]) {            j++;        }        k[i] = j - lastJ;        lastJ = j;    }    k[k.length - 1] = n - lastJ;        UnivariateIntegrator integrator = new RombergIntegrator();    for (int i = 0; i < xs.length - 1; i++) {        double delta = integrator.integrate(1000000, new UnivariateFunction() {            @Override            public double value(double v) {                return dist.pdf(v);            }        }, xs[i], xs[i + 1]);        Assert.assertEquals(delta, p[i + 1], 1.0e-6);    }        double sum = 0;    for (int i = 0; i < k.length; i++) {        if (k[i] != 0) {            sum += k[i] * Math.log(k[i] / p[i] / n);        }    }    sum *= 2;        int dof = k.length - 1;        double z = Math.sqrt(2 * sum) - Math.sqrt(2 * dof - 1);    Assert.assertTrue(String.format("offset=%.3f scale=%.3f Z = %.1f", offset, scale, z), Math.abs(z) < 3);}
0
public double value(double v)
{    return dist.pdf(v);}
0
 static void checkCdf(double offset, double scale, AbstractContinousDistribution dist, double[] breaks, double[] quantiles)
{    int i = 0;    for (double x : breaks) {        Assert.assertEquals(String.format("m=%.3f sd=%.3f x=%.3f", offset, scale, x), quantiles[i], dist.cdf(x * scale + offset), 1.0e-6);        i++;    }}
0
private static double toDouble(long y)
{    return (y & 0xffffffffL) * 2.3283064365386963e-10;}
0
public void test10001()
{    MersenneTwister r = new MersenneTwister();    r.setReferenceSeed(4357);        int i = 0;    for (long x : reference1) {        int y = r.nextInt();        assertEquals("t-ref-int-" + i, x, y);        i++;    }    r.setReferenceSeed(4357);    i = 0;    for (Double x : ref1) {        assertEquals("t-ref-double-" + i, x, toDouble(r.nextInt()), 1.0e-7);        i++;    }}
0
public void testRegression()
{    RandomEngine r = new MersenneTwister(42);    int i = 0;    for (double x : reference3) {        assertEquals("t-regression-" + i, x, r.nextDouble(), 1.0e-7);        i++;    }}
0
public void testDateConstructor()
{    RandomEngine r1 = new MersenneTwister(1275264362);    RandomEngine r2 = new MersenneTwister(new Date(1275264362));    for (int i = 0; i < 100; i++) {        assertEquals("date-" + i, r1.nextInt(), r2.nextInt());    }}
0
public void consistency()
{    Exponential dist = new Exponential(1, RandomUtils.getRandom());        double[] breaks = { 0.1053605, 0.2231436, 0.3566749, 0.5108256, 0.6931472, 0.9162907, 1.2039728, 1.6094379, 2.3025851 };    for (double lambda : new double[] { 0.01, 0.1, 1, 2, 5, 100 }) {        dist.setState(lambda);        DistributionChecks.checkDistribution(dist, breaks, 0, 1 / lambda, 10000);    }}
0
public void testCdf()
{    Exponential dist = new Exponential(5.0, RandomUtils.getRandom());    for (int i = 0; i < 1000; i++) {        double x = i / 50.0;        assertEquals(1 - Math.exp(-x * 5.0), dist.cdf(x), 1.0e-9);    }}
0
public void testPdf()
{    checkPdf(new Exponential(13.0, null), 13.0);}
0
private static void checkPdf(Exponential dist, double lambda)
{    assertEquals(0, dist.pdf(-1), 0);    double sum = 0;    double dx = 0.001 / lambda;    for (double x = 0; x < 20 / lambda; x += dx) {        sum += x * dist.pdf(x) * dx;        assertEquals(Math.exp(-x * lambda) * lambda, dist.pdf(x), 1.0e-9);    }    assertEquals(1 / lambda, sum, 1.0e-6 / lambda);}
0
public void testSetState()
{    Exponential dist = new Exponential(13.0, null);    for (double lambda = 0.1; lambda < 1000; lambda *= 1.3) {        dist.setState(lambda);        checkPdf(dist, lambda);    }}
0
public void testNextDouble() throws Exception
{    double[] x = { -0.01, 0.1053605, 0.2231436, 0.3566749, 0.5108256, 0.6931472, 0.9162907, 1.2039728, 1.6094379, 2.3025851 };    Exponential dist = new Exponential(1, RandomUtils.getRandom());    for (double lambda : new double[] { 13.0, 0.02, 1.6 }) {        dist.setState(lambda);        checkEmpiricalDistribution(dist, 10000, lambda);        DistributionChecks.checkDistribution(dist, x, 0, 1 / lambda, 10000);    }}
0
private static void checkEmpiricalDistribution(Exponential dist, int n, double lambda)
{    double[] x = new double[n];    for (int i = 0; i < n; i++) {        x[i] = dist.nextDouble();    }    Arrays.sort(x);    for (int i = 0; i < n; i++) {        double cumulative = (double) i / (n - 1);        assertEquals(String.format("lambda = %.3f", lambda), cumulative, dist.cdf(x[i]), 0.02);    }}
0
public void testToString()
{    assertEquals("org.apache.mahout.math.jet.random.Exponential(3.1000)", new Exponential(3.1, null).toString());    assertEquals("org.apache.mahout.math.jet.random.Exponential(3.1000)", new Exponential(3.1, null).toString());}
0
public void testNextDouble()
{    double[] z = new double[100000];    Random gen = RandomUtils.getRandom();    for (double alpha : new double[] { 1, 2, 10, 0.1, 0.01, 100 }) {        Gamma g = new Gamma(alpha, 1, gen);        for (int i = 0; i < z.length; i++) {            z[i] = g.nextDouble();        }        Arrays.sort(z);                for (double q : seq(0.01, 1, 0.01)) {            double p = z[(int) (q * z.length)];            assertEquals(q, g.cdf(p), 0.01);        }    }}
0
public void testCdf()
{    Random gen = RandomUtils.getRandom();        for (double beta : new double[] { 1, 0.1, 2, 100 }) {        Gamma g1 = new Gamma(1, beta, gen);        Gamma g2 = new Gamma(1, 1, gen);        for (double x : seq(0, 0.99, 0.1)) {            assertEquals(String.format(Locale.ENGLISH, "Rate invariance: x = %.4f, alpha = 1, beta = %.1f", x, beta), 1 - Math.exp(-x * beta), g1.cdf(x), 1.0e-9);            assertEquals(String.format(Locale.ENGLISH, "Rate invariance: x = %.4f, alpha = 1, beta = %.1f", x, beta), g2.cdf(beta * x), g1.cdf(x), 1.0e-9);        }    }        for (double alpha : new double[] { 0.01, 0.1, 1, 2, 10, 100, 1000 }) {        Gamma g = new Gamma(alpha, 1, gen);        for (double beta : new double[] { 0.1, 1, 2, 100 }) {            Gamma g1 = new Gamma(alpha, beta, gen);            for (double x : seq(0, 0.9999, 0.001)) {                assertEquals(String.format(Locale.ENGLISH, "Rate invariance: x = %.4f, alpha = %.2f, beta = %.1f", x, alpha, beta), g.cdf(x * beta), g1.cdf(x), 0);            }        }    }        checkGammaCdf(0.01, 1, 0.0000000, 0.9450896, 0.9516444, 0.9554919, 0.9582258, 0.9603474, 0.9620810, 0.9635462, 0.9648148, 0.9659329, 0.9669321);    checkGammaCdf(0.1, 1, 0.0000000, 0.7095387, 0.7591012, 0.7891072, 0.8107067, 0.8275518, 0.8413180, 0.8529198, 0.8629131, 0.8716623, 0.8794196);    checkGammaCdf(1, 1, 0.0000000, 0.1812692, 0.3296800, 0.4511884, 0.5506710, 0.6321206, 0.6988058, 0.7534030, 0.7981035, 0.8347011, 0.8646647);    checkGammaCdf(10, 1, 0.000000e+00, 4.649808e-05, 8.132243e-03, 8.392402e-02, 2.833757e-01, 5.420703e-01, 7.576078e-01, 8.906006e-01, 9.567017e-01, 9.846189e-01, 9.950046e-01);    checkGammaCdf(100, 1, 0.000000e+00, 3.488879e-37, 1.206254e-15, 1.481528e-06, 1.710831e-02, 5.132988e-01, 9.721363e-01, 9.998389e-01, 9.999999e-01, 1.000000e+00, 1.000000e+00);}
0
private static void checkGammaCdf(double alpha, double beta, double... values)
{    Gamma g = new Gamma(alpha, beta, RandomUtils.getRandom());    int i = 0;    for (double x : seq(0, 2 * alpha, 2 * alpha / 10)) {        assertEquals(String.format(Locale.ENGLISH, "alpha=%.2f, i=%d, x=%.2f", alpha, i, x), values[i], g.cdf(x), 1.0e-7);        i++;    }}
0
private static double[] seq(double from, double to, double by)
{    double[] r = new double[(int) Math.ceil(0.999999 * (to - from) / by)];    int i = 0;    for (double x = from; x < to - (to - from) * 1.0e-6; x += by) {        r[i++] = x;    }    return r;}
0
public void testPdf()
{    Random gen = RandomUtils.getRandom();    for (double alpha : new double[] { 0.01, 0.1, 1, 2, 10, 100 }) {        for (double beta : new double[] { 0.1, 1, 2, 100 }) {            Gamma g1 = new Gamma(alpha, beta, gen);            for (double x : seq(0, 0.99, 0.1)) {                double p = Math.pow(beta, alpha) * Math.pow(x, alpha - 1) * Math.exp(-beta * x - org.apache.mahout.math.jet.stat.Gamma.logGamma(alpha));                assertEquals(String.format(Locale.ENGLISH, "alpha=%.2f, beta=%.2f, x=%.2f\n", alpha, beta, x), p, g1.pdf(x), 1.0e-9);            }        }    }}
0
public void testDistributionFunctions() throws Exception
{    InputSupplier<InputStreamReader> input = Resources.newReaderSupplier(Resources.getResource("negative-binomial-test-data.csv"), Charsets.UTF_8);    boolean header = true;    for (String line : CharStreams.readLines(input)) {        if (header) {                        header = false;        } else {            Iterable<String> values = onComma.split(line);            int k = Integer.parseInt(Iterables.get(values, 0));            double p = Double.parseDouble(Iterables.get(values, 1));            int r = Integer.parseInt(Iterables.get(values, 2));            double density = Double.parseDouble(Iterables.get(values, 3));            double cume = Double.parseDouble(Iterables.get(values, 4));            NegativeBinomial nb = new NegativeBinomial(r, p, RandomUtils.getRandom());            assertEquals("cumulative " + k + ',' + p + ',' + r, cume, nb.cdf(k), cume * 1.0e-5);            assertEquals("density " + k + ',' + p + ',' + r, density, nb.pdf(k), density * 1.0e-5);        }    }}
0
public void testCdf()
{    Random gen = RandomUtils.getRandom();    double offset = 0;    double scale = 1;    for (int k = 0; k < 20; k++) {        Normal dist = new Normal(offset, scale, null);        DistributionChecks.checkCdf(offset, scale, dist, breaks, quantiles);        offset = gen.nextGaussian();        scale = Math.exp(3 * gen.nextGaussian());    }}
0
public void consistency()
{    Random gen = RandomUtils.getRandom();    double offset = 0;    double scale = 1;    Normal dist = new Normal(offset, scale, RandomUtils.getRandom());    for (int k = 0; k < 20; k++) {        dist.setState(offset, scale);        DistributionChecks.checkDistribution(dist, breaks, offset, scale, 10000);        offset = gen.nextGaussian();        scale = Math.exp(3 * gen.nextGaussian());    }}
0
public void testSetState() throws Exception
{    Normal dist = new Normal(0, 1, RandomUtils.getRandom());    dist.setState(1.3, 5.9);    DistributionChecks.checkDistribution(dist, breaks, 1.3, 5.9, 10000);}
0
public void testToString()
{    assertEquals("org.apache.mahout.math.jet.random.Normal(m=1.300000, sd=5.900000)", new Normal(1.3, 5.9, null).toString());}
0
public void testGamma()
{    double[] x = { 1, 2, 5, 10, 20, 50, 100 };    double[] expected = { 1.000000e+00, 1.000000e+00, 2.400000e+01, 3.628800e+05, 1.216451e+17, 6.082819e+62, 9.332622e+155 };    for (int i = 0; i < x.length; i++) {        assertEquals(expected[i], Gamma.gamma(x[i]), expected[i] * 1.0e-5);        assertEquals(gammaInteger(x[i]), Gamma.gamma(x[i]), expected[i] * 1.0e-5);        assertEquals(gammaInteger(x[i]), Math.exp(Gamma.logGamma(x[i])), expected[i] * 1.0e-5);    }}
0
public void testNegativeArgForGamma()
{    double[] x = { -30.3, -20.7, -10.5, -1.1, 0.5, 0.99, -0.999 };    double[] expected = { -5.243216e-33, -1.904051e-19, -2.640122e-07, 9.714806e+00, 1.772454e+00, 1.005872e+00, -1.000424e+03 };    for (int i = 0; i < x.length; i++) {        assertEquals(expected[i], Gamma.gamma(x[i]), Math.abs(expected[i] * 1.0e-5));        assertEquals(Math.abs(expected[i]), Math.abs(Math.exp(Gamma.logGamma(x[i]))), Math.abs(expected[i] * 1.0e-5));    }}
0
private static double gammaInteger(double x)
{    double r = 1;    for (int i = 2; i < x; i++) {        r *= i;    }    return r;}
0
public void testBigX()
{    assertEquals(factorial(4), 4 * 3 * 2, 0);    assertEquals(factorial(4), Gamma.gamma(5), 0);    assertEquals(factorial(14), Gamma.gamma(15), 0);    assertEquals(factorial(34), Gamma.gamma(35), 1.0e-15 * factorial(34));    assertEquals(factorial(44), Gamma.gamma(45), 1.0e-15 * factorial(44));    assertEquals(-6.884137e-40 + 3.508309e-47, Gamma.gamma(-35.1), 1.0e-52);    assertEquals(-3.915646e-41 - 3.526813e-48 - 1.172516e-55, Gamma.gamma(-35.9), 1.0e-52);    assertEquals(-2000000000.577215, Gamma.gamma(-0.5e-9), 1.0e-15 * 2000000000.577215);    assertEquals(1999999999.422784, Gamma.gamma(0.5e-9), 1.0e-15 * 1999999999.422784);    assertEquals(1.324296658017984e+252, Gamma.gamma(146.1), 1.0e-10 * 1.324296658017984e+252);    for (double x : new double[] { 5, 15, 35, 45, -35.1, -35.9, -0.5e-9, 0.5e-9, 146.1 }) {        double ref = Math.log(Math.abs(Gamma.gamma(x)));        double actual = Gamma.logGamma(x);        double diff = Math.abs(ref - actual) / ref;        assertEquals("gamma versus logGamma at " + x + " (diff = " + diff + ')', 0, (ref - actual) / ref, 1.0e-8);    }}
0
private static double factorial(int n)
{    double r = 1;    for (int i = 2; i <= n; i++) {        r *= i;    }    return r;}
0
public void beta()
{    Random r = RandomUtils.getRandom();    for (int i = 0; i < 200; i++) {        double alpha = -50 * Math.log1p(-r.nextDouble());        double beta = -50 * Math.log1p(-r.nextDouble());        double ref = Math.exp(Gamma.logGamma(alpha) + Gamma.logGamma(beta) - Gamma.logGamma(alpha + beta));        double actual = Gamma.beta(alpha, beta);        double err = (ref - actual) / ref;        assertEquals("beta at (" + alpha + ", " + beta + ") relative error = " + err, 0, err, 1.0e-10);    }}
0
public void incompleteBeta() throws IOException
{    Splitter onComma = Splitter.on(",").trimResults();    InputSupplier<InputStreamReader> input = Resources.newReaderSupplier(Resources.getResource("beta-test-data.csv"), Charsets.UTF_8);    boolean header = true;    for (String line : CharStreams.readLines(input)) {        if (header) {                        header = false;        } else {            Iterable<String> values = onComma.split(line);            double alpha = Double.parseDouble(Iterables.get(values, 0));            double beta = Double.parseDouble(Iterables.get(values, 1));            double x = Double.parseDouble(Iterables.get(values, 2));            double ref = Double.parseDouble(Iterables.get(values, 3));            double actual = Gamma.incompleteBeta(alpha, beta, x);            assertEquals(alpha + "," + beta + ',' + x, ref, actual, ref * 1.0e-5);        }    }}
0
public void testNormalCdf()
{            double[] ref = { 2.866516e-07, 4.816530e-07, 8.013697e-07, 1.320248e-06, 2.153811e-06, 3.479323e-06, 5.565743e-06, 8.816559e-06, 1.383023e-05, 2.148428e-05, 3.305072e-05, 5.035210e-05, 7.596947e-05, 1.135152e-04, 1.679855e-04, 2.462079e-04, 3.574003e-04, 5.138562e-04, 7.317683e-04, 1.032198e-03, 1.442193e-03, 1.996034e-03, 2.736602e-03, 3.716808e-03, 5.001037e-03, 6.666521e-03, 8.804535e-03, 1.152131e-02, 1.493850e-02, 1.919309e-02, 2.443656e-02, 3.083320e-02, 3.855748e-02, 4.779035e-02, 5.871452e-02, 7.150870e-02, 8.634102e-02, 1.033618e-01, 1.226957e-01, 1.444345e-01, 1.686293e-01, 1.952845e-01, 2.243525e-01, 2.557301e-01, 2.892574e-01, 3.247181e-01, 3.618436e-01, 4.003175e-01, 4.397847e-01, 4.798600e-01, 5.201400e-01, 5.602153e-01, 5.996825e-01, 6.381564e-01, 6.752819e-01, 7.107426e-01, 7.442699e-01, 7.756475e-01, 8.047155e-01, 8.313707e-01, 8.555655e-01, 8.773043e-01, 8.966382e-01, 9.136590e-01, 9.284913e-01, 9.412855e-01, 9.522096e-01, 9.614425e-01, 9.691668e-01, 9.755634e-01, 9.808069e-01, 9.850615e-01, 9.884787e-01, 9.911955e-01, 9.933335e-01, 9.949990e-01, 9.962832e-01, 9.972634e-01, 9.980040e-01, 9.985578e-01, 9.989678e-01, 9.992682e-01, 9.994861e-01, 9.996426e-01, 9.997538e-01, 9.998320e-01, 9.998865e-01, 9.999240e-01, 9.999496e-01, 9.999669e-01, 9.999785e-01, 9.999862e-01, 9.999912e-01, 9.999944e-01, 9.999965e-01, 9.999978e-01, 9.999987e-01, 9.999992e-01, 9.999995e-01, 9.999997e-01 };    assertEquals(0.682689492137 / 2 + 0.5, Probability.normal(1), 1.0e-7);    int i = 0;    for (double x = -5; x <= 5.005; x += 10.0 / 99) {        assertEquals("Test 1 cdf function at " + x, ref[i], Probability.normal(x), 1.0e-6);        assertEquals("Test 2 cdf function at " + x, ref[i], Probability.normal(12, 1, x + 12), 1.0e-6);        assertEquals("Test 3 cdf function at " + x, ref[i], Probability.normal(12, 0.25, x / 2.0 + 12), 1.0e-6);        i++;    }}
0
public void testBetaCdf()
{                            double[][] ref = new double[5][];    ref[0] = new double[] { 0.00000000, 0.01010101, 0.02020202, 0.03030303, 0.04040404, 0.05050505, 0.06060606, 0.07070707, 0.08080808, 0.09090909, 0.10101010, 0.11111111, 0.12121212, 0.13131313, 0.14141414, 0.15151515, 0.16161616, 0.17171717, 0.18181818, 0.19191919, 0.20202020, 0.21212121, 0.22222222, 0.23232323, 0.24242424, 0.25252525, 0.26262626, 0.27272727, 0.28282828, 0.29292929, 0.30303030, 0.31313131, 0.32323232, 0.33333333, 0.34343434, 0.35353535, 0.36363636, 0.37373737, 0.38383838, 0.39393939, 0.40404040, 0.41414141, 0.42424242, 0.43434343, 0.44444444, 0.45454545, 0.46464646, 0.47474747, 0.48484848, 0.49494949, 0.50505051, 0.51515152, 0.52525253, 0.53535354, 0.54545455, 0.55555556, 0.56565657, 0.57575758, 0.58585859, 0.59595960, 0.60606061, 0.61616162, 0.62626263, 0.63636364, 0.64646465, 0.65656566, 0.66666667, 0.67676768, 0.68686869, 0.69696970, 0.70707071, 0.71717172, 0.72727273, 0.73737374, 0.74747475, 0.75757576, 0.76767677, 0.77777778, 0.78787879, 0.79797980, 0.80808081, 0.81818182, 0.82828283, 0.83838384, 0.84848485, 0.85858586, 0.86868687, 0.87878788, 0.88888889, 0.89898990, 0.90909091, 0.91919192, 0.92929293, 0.93939394, 0.94949495, 0.95959596, 0.96969697, 0.97979798, 0.98989899, 1.00000000 };    ref[1] = new double[] { 0.0000000000, 0.0001020304, 0.0004081216, 0.0009182736, 0.0016324865, 0.0025507601, 0.0036730946, 0.0049994898, 0.0065299459, 0.0082644628, 0.0102030405, 0.0123456790, 0.0146923783, 0.0172431385, 0.0199979594, 0.0229568411, 0.0261197837, 0.0294867871, 0.0330578512, 0.0368329762, 0.0408121620, 0.0449954086, 0.0493827160, 0.0539740843, 0.0587695133, 0.0637690032, 0.0689725538, 0.0743801653, 0.0799918376, 0.0858075707, 0.0918273646, 0.0980512193, 0.1044791348, 0.1111111111, 0.1179471483, 0.1249872462, 0.1322314050, 0.1396796245, 0.1473319049, 0.1551882461, 0.1632486481, 0.1715131109, 0.1799816345, 0.1886542190, 0.1975308642, 0.2066115702, 0.2158963371, 0.2253851648, 0.2350780533, 0.2449750026, 0.2550760127, 0.2653810836, 0.2758902153, 0.2866034078, 0.2975206612, 0.3086419753, 0.3199673503, 0.3314967860, 0.3432302826, 0.3551678400, 0.3673094582, 0.3796551372, 0.3922048771, 0.4049586777, 0.4179165391, 0.4310784614, 0.4444444444, 0.4580144883, 0.4717885930, 0.4857667585, 0.4999489848, 0.5143352719, 0.5289256198, 0.5437200286, 0.5587184981, 0.5739210285, 0.5893276196, 0.6049382716, 0.6207529844, 0.6367717580, 0.6529945924, 0.6694214876, 0.6860524436, 0.7028874605, 0.7199265381, 0.7371696766, 0.7546168758, 0.7722681359, 0.7901234568, 0.8081828385, 0.8264462810, 0.8449137843, 0.8635853484, 0.8824609734, 0.9015406591, 0.9208244057, 0.9403122130, 0.9600040812, 0.9799000102, 1.0000000000 };    ref[2] = new double[] { 0.000000000, 0.001489698, 0.005799444, 0.012698382, 0.021966298, 0.033393335, 0.046779694, 0.061935356, 0.078679798, 0.096841712, 0.116258735, 0.136777178, 0.158251755, 0.180545326, 0.203528637, 0.227080061, 0.251085352, 0.275437393, 0.300035957, 0.324787463, 0.349604743, 0.374406809, 0.399118623, 0.423670875, 0.447999763, 0.472046772, 0.495758466, 0.519086275, 0.541986291, 0.564419069, 0.586349424, 0.607746242, 0.628582288, 0.648834019, 0.668481403, 0.687507740, 0.705899486, 0.723646086, 0.740739801, 0.757175549, 0.772950746, 0.788065147, 0.802520695, 0.816321377, 0.829473074, 0.841983426, 0.853861691, 0.865118615, 0.875766302, 0.885818092, 0.895288433, 0.904192771, 0.912547431, 0.920369513, 0.927676778, 0.934487554, 0.940820632, 0.946695177, 0.952130629, 0.957146627, 0.961762916, 0.965999275, 0.969875437, 0.973411020, 0.976625460, 0.979537944, 0.982167353, 0.984532203, 0.986650598, 0.988540173, 0.990218056, 0.991700827, 0.993004475, 0.994144371, 0.995135237, 0.995991117, 0.996725360, 0.997350600, 0.997878739, 0.998320942, 0.998687627, 0.998988463, 0.999232371, 0.999427531, 0.999581387, 0.999700663, 0.999791377, 0.999858864, 0.999907798, 0.999942219, 0.999965567, 0.999980718, 0.999990021, 0.999995342, 0.999998111, 0.999999376, 0.999999851, 0.999999980, 0.999999999, 1.000000000 };    ref[3] = new double[] { 0.0000000, 0.5858072, 0.6684658, 0.7201859, 0.7578936, 0.7873991, 0.8114552, 0.8316029, 0.8487998, 0.8636849, 0.8767081, 0.8881993, 0.8984080, 0.9075280, 0.9157131, 0.9230876, 0.9297536, 0.9357958, 0.9412856, 0.9462835, 0.9508414, 0.9550044, 0.9588113, 0.9622963, 0.9654896, 0.9684178, 0.9711044, 0.9735707, 0.9758356, 0.9779161, 0.9798276, 0.9815839, 0.9831977, 0.9846805, 0.9860426, 0.9872936, 0.9884422, 0.9894965, 0.9904638, 0.9913509, 0.9921638, 0.9929085, 0.9935900, 0.9942134, 0.9947832, 0.9953034, 0.9957779, 0.9962104, 0.9966041, 0.9969621, 0.9972872, 0.9975821, 0.9978492, 0.9980907, 0.9983088, 0.9985055, 0.9986824, 0.9988414, 0.9989839, 0.9991113, 0.9992251, 0.9993265, 0.9994165, 0.9994963, 0.9995668, 0.9996288, 0.9996834, 0.9997311, 0.9997727, 0.9998089, 0.9998401, 0.9998671, 0.9998901, 0.9999098, 0.9999265, 0.9999406, 0.9999524, 0.9999622, 0.9999703, 0.9999769, 0.9999823, 0.9999866, 0.9999900, 0.9999927, 0.9999947, 0.9999963, 0.9999975, 0.9999983, 0.9999989, 0.9999993, 0.9999996, 0.9999998, 0.9999999, 0.9999999, 1.0000000, 1.0000000, 1.0000000, 1.0000000, 1.0000000, 1.0000000 };    ref[4] = new double[] { 0.00000000, 0.01908202, 0.02195656, 0.02385194, 0.02530810, 0.02650923, 0.02754205, 0.02845484, 0.02927741, 0.03002959, 0.03072522, 0.03137444, 0.03198487, 0.03256240, 0.03311171, 0.03363655, 0.03414001, 0.03462464, 0.03509259, 0.03554568, 0.03598550, 0.03641339, 0.03683054, 0.03723799, 0.03763667, 0.03802739, 0.03841091, 0.03878787, 0.03915890, 0.03952453, 0.03988529, 0.04024162, 0.04059396, 0.04094272, 0.04128827, 0.04163096, 0.04197113, 0.04230909, 0.04264515, 0.04297958, 0.04331268, 0.04364471, 0.04397592, 0.04430658, 0.04463693, 0.04496722, 0.04529770, 0.04562860, 0.04596017, 0.04629265, 0.04662629, 0.04696134, 0.04729804, 0.04763666, 0.04797747, 0.04832073, 0.04866673, 0.04901578, 0.04936816, 0.04972422, 0.05008428, 0.05044871, 0.05081789, 0.05119222, 0.05157213, 0.05195809, 0.05235059, 0.05275018, 0.05315743, 0.05357298, 0.05399753, 0.05443184, 0.05487673, 0.05533315, 0.05580212, 0.05628480, 0.05678247, 0.05729660, 0.05782885, 0.05838111, 0.05895557, 0.05955475, 0.06018161, 0.06083965, 0.06153300, 0.06226670, 0.06304685, 0.06388102, 0.06477877, 0.06575235, 0.06681788, 0.06799717, 0.06932077, 0.07083331, 0.07260394, 0.07474824, 0.07748243, 0.08129056, 0.08771055, 1.00000000 };    double[] alpha = { 1.0, 2.0, 2.0, 0.2, 0.2 };    double[] beta = { 1.0, 1.0, 5.0, 5.0, 0.01 };    for (int j = 0; j < 4; j++) {        for (int i = 0; i < 100; i++) {            double x = i / 99.0;            String p = String.format(Locale.ENGLISH, "pbeta(q=%6.4f, shape1=%5.3f shape2=%5.3f) = %.8f", x, alpha[j], beta[j], ref[j][i]);            assertEquals(p, ref[j][i], Probability.beta(alpha[j], beta[j], x), 1.0e-7);        }    }}
0
public void testLogGamma()
{    double[] xValues = { 1.1, 2.1, 3.1, 4.1, 5.1, 20.1, 100.1, -0.9 };    double[] ref = { -0.04987244, 0.04543774, 0.78737508, 1.91877719, 3.32976417, 39.63719250, 359.59427179, 2.35807317 };    for (int i = 0; i < xValues.length; i++) {        double x = xValues[i];        assertEquals(ref[i], Gamma.logGamma(x), 1.0e-7);    }}
0
public void emptyOnCreation()
{    ObjectArrayList<String> list = new ObjectArrayList<>();    assertTrue(list.isEmpty());    assertEquals(0, list.size());    list.add("1");    list.add("2");    list.add("3");    assertEquals(3, list.size());}
0
public void correctSizeAfterInstantiation()
{    ObjectArrayList<String> list = new ObjectArrayList<>(100);    assertTrue(list.isEmpty());    assertEquals(0, list.size());}
0
public void correctSizeAfterInstantiationWithElements()
{    ObjectArrayList<String> list = new ObjectArrayList<>(new String[] { "1", "2", "3" });    assertFalse(list.isEmpty());    assertEquals(3, list.size());}
0
public void setUp() throws Exception
{    testTempDir = null;    RandomUtils.useTestSeed();}
0
public void tearDown() throws Exception
{    if (testTempDir != null) {        new DeletingVisitor().accept(testTempDir);    }}
0
protected final File getTestTempDir() throws IOException
{    if (testTempDir == null) {        String systemTmpDir = System.getProperty("mahout.test.directory");        if (systemTmpDir == null) {            systemTmpDir = "target/";            systemTmpDir += "test-data";        }        long simpleRandomLong = (long) (Long.MAX_VALUE * Math.random());        testTempDir = new File(systemTmpDir, "mahout-" + getClass().getSimpleName() + '-' + simpleRandomLong);        if (!testTempDir.mkdirs()) {            throw new IOException("Could not create " + testTempDir);        }        testTempDir.deleteOnExit();    }    return testTempDir;}
0
protected final File getTestTempFile(String name) throws IOException
{    return getTestTempFileOrDir(name, false);}
0
protected final File getTestTempDir(String name) throws IOException
{    return getTestTempFileOrDir(name, true);}
0
private File getTestTempFileOrDir(String name, boolean dir) throws IOException
{    File f = new File(getTestTempDir(), name);    f.deleteOnExit();    if (dir && !f.mkdirs()) {        throw new IOException("Could not make directory " + f);    }    return f;}
0
public boolean accept(File f)
{    if (!f.isFile()) {        f.listFiles(this);    }    f.delete();    return false;}
0
public void testFunctionalView()
{    Matrix m = Matrices.functionalMatrixView(5, 6, new IntIntFunction() {        @Override        public double apply(int row, int col) {            assertTrue(row < 5);            assertTrue(col < 6);            return row + col;        }    });            assertEquals(135, m.aggregate(Functions.PLUS, Functions.IDENTITY), 1e-10);}
0
public double apply(int row, int col)
{    assertTrue(row < 5);    assertTrue(col < 6);    return row + col;}
0
public void testTransposeView()
{    Matrix m = Matrices.gaussianView(5, 6, 1234L);    Matrix controlM = new DenseMatrix(5, 6).assign(m);    System.out.printf("M=\n%s\n", m);    System.out.printf("controlM=\n%s\n", controlM);    Matrix mtm = Matrices.transposedView(m).times(m);    Matrix controlMtm = controlM.transpose().times(controlM);    System.out.printf("M'M=\n%s\n", mtm);    Matrix diff = mtm.minus(controlMtm);    assertEquals(0, diff.aggregate(Functions.PLUS, Functions.ABS), 1e-10);}
0
public void testViewDenseSparseReporting()
{    Matrix m = new SparseMatrix(1000, 1000);    m.set(1, 1, 33.0);    Matrix mt = Matrices.transposedView(m);    assertTrue(mt.viewColumn(0).isDense() == m.viewRow(0).isDense());    assertTrue(mt.viewRow(0).isDense() == m.viewColumn(0).isDense());    m = new DenseMatrix(10, 10);    m.set(1, 1, 33.0);    mt = Matrices.transposedView(m);    assertTrue(mt.viewColumn(0).isDense());    assertTrue(mt.viewRow(0).isDense());}
0
public void testUniformView()
{    Matrix m1 = Matrices.uniformView(5, 6, 1234);    Matrix m2 = Matrices.uniformView(5, 6, 1234);    for (int row = 0; row < m1.numRows(); row++) {        for (int col = 0; col < m1.numCols(); col++) {            assertTrue(m1.getQuick(row, col) >= 0.0);            assertTrue(m1.getQuick(row, col) < 1.0);        }    }    Matrix diff = m1.minus(m2);    assertEquals(0, diff.aggregate(Functions.PLUS, Functions.ABS), 1e-10);}
0
public void testSymmetricUniformView()
{    Matrix m1 = Matrices.symmetricUniformView(5, 6, 1234);    Matrix m2 = Matrices.symmetricUniformView(5, 6, 1234);    for (int row = 0; row < m1.numRows(); row++) {        for (int col = 0; col < m1.numCols(); col++) {            assertTrue(m1.getQuick(row, col) >= -1.0);            assertTrue(m1.getQuick(row, col) < 1.0);        }    }    Matrix diff = m1.minus(m2);    assertEquals(0, diff.aggregate(Functions.PLUS, Functions.ABS), 1e-10);}
0
public void testGaussianView()
{    Matrix m1 = Matrices.gaussianView(5, 6, 1234);    Matrix m2 = Matrices.gaussianView(5, 6, 1234);    Matrix diff = m1.minus(m2);    assertEquals(0, diff.aggregate(Functions.PLUS, Functions.ABS), 1e-10);}
0
public void setUp() throws Exception
{    super.setUp();    test = matrixFactory(values);}
0
public void testCardinality()
{    assertEquals("row cardinality", values.length, test.rowSize());    assertEquals("col cardinality", values[0].length, test.columnSize());}
0
public void testCopy()
{    Matrix copy = test.clone();    assertSame("wrong class", copy.getClass(), test.getClass());    for (int row = 0; row < test.rowSize(); row++) {        for (int col = 0; col < test.columnSize(); col++) {            assertEquals("value[" + row + "][" + col + ']', test.getQuick(row, col), copy.getQuick(row, col), EPSILON);        }    }}
0
public void testClone()
{    double oldValue = 1.23;    double newValue = 2.34;    double[][] values = { { oldValue, 3 }, { 3, 5 }, { 7, 9 } };    Matrix matrix = matrixFactory(values);    Matrix clone = matrix.clone();    clone.set(0, 0, newValue);        assertEquals("Matrix clone is not independent of the original", oldValue, matrix.get(0, 0), EPSILON);}
0
public void testIterate()
{    Iterator<MatrixSlice> it = test.iterator();    MatrixSlice m;    while (it.hasNext() && (m = it.next()) != null) {        Vector v = m.vector();        Vector w = test instanceof SparseColumnMatrix ? test.viewColumn(m.index()) : test.viewRow(m.index());        assertEquals("iterator: " + v + ", randomAccess: " + w, v, w);    }}
0
public void testGetQuick()
{    for (int row = 0; row < test.rowSize(); row++) {        for (int col = 0; col < test.columnSize(); col++) {            assertEquals("value[" + row + "][" + col + ']', values[row][col], test.getQuick(row, col), EPSILON);        }    }}
0
public void testLike()
{    Matrix like = test.like();    assertSame("type", like.getClass(), test.getClass());    assertEquals("rows", test.rowSize(), like.rowSize());    assertEquals("columns", test.columnSize(), like.columnSize());}
0
public void testLikeIntInt()
{    Matrix like = test.like(4, 4);    assertSame("type", like.getClass(), test.getClass());    assertEquals("rows", 4, like.rowSize());    assertEquals("columns", 4, like.columnSize());}
0
public void testSetQuick()
{    for (int row = 0; row < test.rowSize(); row++) {        for (int col = 0; col < test.columnSize(); col++) {            test.setQuick(row, col, 1.23);            assertEquals("value[" + row + "][" + col + ']', 1.23, test.getQuick(row, col), EPSILON);        }    }}
0
public void testSize()
{    int[] c = test.getNumNondefaultElements();    assertEquals("row size", values.length, c[ROW]);    assertEquals("col size", values[0].length, c[COL]);}
0
public void testViewPart()
{    int[] offset = { 1, 1 };    int[] size = { 2, 1 };    Matrix view = test.viewPart(offset, size);    assertEquals(2, view.rowSize());    assertEquals(1, view.columnSize());    for (int row = 0; row < view.rowSize(); row++) {        for (int col = 0; col < view.columnSize(); col++) {            assertEquals("value[" + row + "][" + col + ']', values[row + 1][col + 1], view.get(row, col), EPSILON);        }    }}
0
public void testViewPartCardinality()
{    int[] offset = { 1, 1 };    int[] size = { 3, 3 };    test.viewPart(offset, size);}
0
public void testViewPartIndexOver()
{    int[] offset = { 1, 1 };    int[] size = { 2, 2 };    test.viewPart(offset, size);}
0
public void testViewPartIndexUnder()
{    int[] offset = { -1, -1 };    int[] size = { 2, 2 };    test.viewPart(offset, size);}
0
public void testAssignDouble()
{    test.assign(4.53);    for (int row = 0; row < test.rowSize(); row++) {        for (int col = 0; col < test.columnSize(); col++) {            assertEquals("value[" + row + "][" + col + ']', 4.53, test.getQuick(row, col), EPSILON);        }    }}
0
public void testAssignDoubleArrayArray()
{    test.assign(new double[3][2]);    for (int row = 0; row < test.rowSize(); row++) {        for (int col = 0; col < test.columnSize(); col++) {            assertEquals("value[" + row + "][" + col + ']', 0.0, test.getQuick(row, col), EPSILON);        }    }}
0
public void testAssignDoubleArrayArrayCardinality()
{    test.assign(new double[test.rowSize() + 1][test.columnSize()]);}
0
public void testMatrixViewBug()
{    Matrix m = test.viewPart(0, 3, 0, 2);        m = m.viewPart(2, 1, 0, 1);    assertEquals(5.5, m.zSum(), 0);}
0
public void testAssignMatrixBinaryFunction()
{    test.assign(test, Functions.PLUS);    for (int row = 0; row < test.rowSize(); row++) {        for (int col = 0; col < test.columnSize(); col++) {            assertEquals("value[" + row + "][" + col + ']', 2 * values[row][col], test.getQuick(row, col), EPSILON);        }    }}
0
public void testAssignMatrixBinaryFunctionCardinality()
{    test.assign(test.transpose(), Functions.PLUS);}
0
public void testAssignMatrix()
{    Matrix value = test.like();    value.assign(test);    for (int row = 0; row < test.rowSize(); row++) {        for (int col = 0; col < test.columnSize(); col++) {            assertEquals("value[" + row + "][" + col + ']', test.getQuick(row, col), value.getQuick(row, col), EPSILON);        }    }}
0
public void testAssignMatrixCardinality()
{    test.assign(test.transpose());}
0
public void testAssignUnaryFunction()
{    test.assign(Functions.mult(-1));    for (int row = 0; row < test.rowSize(); row++) {        for (int col = 0; col < test.columnSize(); col++) {            assertEquals("value[" + row + "][" + col + ']', -values[row][col], test.getQuick(row, col), EPSILON);        }    }}
0
public void testRowView()
{    assertEquals(test.columnSize(), test.viewRow(1).size());    assertEquals(test.columnSize(), test.viewRow(2).size());    Random gen = RandomUtils.getRandom();    for (int row = 0; row < test.rowSize(); row++) {        int j = gen.nextInt(test.columnSize());        double old = test.get(row, j);        double v = gen.nextGaussian();        test.viewRow(row).set(j, v);        assertEquals(v, test.get(row, j), 0);        assertEquals(v, test.viewRow(row).get(j), 0);        test.set(row, j, old);        assertEquals(old, test.get(row, j), 0);        assertEquals(old, test.viewRow(row).get(j), 0);    }}
0
public void testColumnView()
{    assertEquals(test.rowSize(), test.viewColumn(0).size());    assertEquals(test.rowSize(), test.viewColumn(1).size());    Random gen = RandomUtils.getRandom();    for (int col = 0; col < test.columnSize(); col++) {        int j = gen.nextInt(test.columnSize());        double old = test.get(col, j);        double v = gen.nextGaussian();        test.viewColumn(col).set(j, v);        assertEquals(v, test.get(j, col), 0);        assertEquals(v, test.viewColumn(col).get(j), 0);        test.set(j, col, old);        assertEquals(old, test.get(j, col), 0);        assertEquals(old, test.viewColumn(col).get(j), 0);    }}
0
public void testAggregateRows()
{    Vector v = test.aggregateRows(new VectorFunction() {        @Override        public double apply(Vector v) {            return v.zSum();        }    });    for (int i = 0; i < test.numRows(); i++) {        assertEquals(test.viewRow(i).zSum(), v.get(i), EPSILON);    }}
0
public double apply(Vector v)
{    return v.zSum();}
0
public void testAggregateCols()
{    Vector v = test.aggregateColumns(new VectorFunction() {        @Override        public double apply(Vector v) {            return v.zSum();        }    });    for (int i = 0; i < test.numCols(); i++) {        assertEquals(test.viewColumn(i).zSum(), v.get(i), EPSILON);    }}
0
public double apply(Vector v)
{    return v.zSum();}
0
public void testAggregate()
{    double total = test.aggregate(Functions.PLUS, Functions.IDENTITY);    assertEquals(test.aggregateRows(new VectorFunction() {        @Override        public double apply(Vector v) {            return v.zSum();        }    }).zSum(), total, EPSILON);}
0
public double apply(Vector v)
{    return v.zSum();}
0
public void testDivide()
{    Matrix value = test.divide(4.53);    for (int row = 0; row < test.rowSize(); row++) {        for (int col = 0; col < test.columnSize(); col++) {            assertEquals("value[" + row + "][" + col + ']', values[row][col] / 4.53, value.getQuick(row, col), EPSILON);        }    }}
0
public void testGet()
{    for (int row = 0; row < test.rowSize(); row++) {        for (int col = 0; col < test.columnSize(); col++) {            assertEquals("value[" + row + "][" + col + ']', values[row][col], test.get(row, col), EPSILON);        }    }}
0
public void testGetIndexUnder()
{    for (int row = -1; row < test.rowSize(); row++) {        for (int col = 0; col < test.columnSize(); col++) {            test.get(row, col);        }    }}
0
public void testGetIndexOver()
{    for (int row = 0; row < test.rowSize() + 1; row++) {        for (int col = 0; col < test.columnSize(); col++) {            test.get(row, col);        }    }}
0
public void testMinus()
{    Matrix value = test.minus(test);    for (int row = 0; row < test.rowSize(); row++) {        for (int col = 0; col < test.columnSize(); col++) {            assertEquals("value[" + row + "][" + col + ']', 0.0, value.getQuick(row, col), EPSILON);        }    }}
0
public void testMinusCardinality()
{    test.minus(test.transpose());}
0
public void testPlusDouble()
{    Matrix value = test.plus(4.53);    for (int row = 0; row < test.rowSize(); row++) {        for (int col = 0; col < test.columnSize(); col++) {            assertEquals("value[" + row + "][" + col + ']', values[row][col] + 4.53, value.getQuick(row, col), EPSILON);        }    }}
0
public void testPlusMatrix()
{    Matrix value = test.plus(test);    for (int row = 0; row < test.rowSize(); row++) {        for (int col = 0; col < test.columnSize(); col++) {            assertEquals("value[" + row + "][" + col + ']', values[row][col] * 2, value.getQuick(row, col), EPSILON);        }    }}
0
public void testPlusMatrixCardinality()
{    test.plus(test.transpose());}
0
public void testSetUnder()
{    for (int row = -1; row < test.rowSize(); row++) {        for (int col = 0; col < test.columnSize(); col++) {            test.set(row, col, 1.23);        }    }}
0
public void testSetOver()
{    for (int row = 0; row < test.rowSize() + 1; row++) {        for (int col = 0; col < test.columnSize(); col++) {            test.set(row, col, 1.23);        }    }}
0
public void testTimesDouble()
{    Matrix value = test.times(4.53);    for (int row = 0; row < test.rowSize(); row++) {        for (int col = 0; col < test.columnSize(); col++) {            assertEquals("value[" + row + "][" + col + ']', values[row][col] * 4.53, value.getQuick(row, col), EPSILON);        }    }}
0
public void testTimesMatrix()
{    Matrix transpose = test.transpose();    Matrix value = test.times(transpose);    assertEquals("rows", test.rowSize(), value.rowSize());    assertEquals("cols", test.rowSize(), value.columnSize());    Matrix expected = new DenseMatrix(new double[][] { { 5.0, 11.0, 17.0 }, { 11.0, 25.0, 39.0 }, { 17.0, 39.0, 61.0 } }).times(1.21);    for (int i = 0; i < expected.numCols(); i++) {        for (int j = 0; j < expected.numRows(); j++) {            assertTrue("Matrix times transpose not correct: " + i + ", " + j + "\nexpected:\n\t" + expected + "\nactual:\n\t" + value, Math.abs(expected.get(i, j) - value.get(i, j)) < 1.0e-12);        }    }    Matrix timestest = new DenseMatrix(10, 1);    /* will throw ArrayIndexOutOfBoundsException exception without MAHOUT-26 */    timestest.transpose().times(timestest);}
0
public void testTimesVector()
{    Vector vectorA = new DenseVector(vectorAValues);    Vector testTimesVectorA = test.times(vectorA);    Vector expected = new DenseVector(new double[] { 5.0, 11.0, 17.0 });    assertTrue("Matrix times vector not equals: " + vectorA + " != " + testTimesVectorA, expected.minus(testTimesVectorA).norm(2) < 1.0e-12);    test.times(testTimesVectorA);}
0
public void testTimesSquaredTimesVector()
{    Vector vectorA = new DenseVector(vectorAValues);    Vector ttA = test.timesSquared(vectorA);    Vector ttASlow = test.transpose().times(test.times(vectorA));    assertTrue("M'Mv != M.timesSquared(v): " + ttA + " != " + ttASlow, ttASlow.minus(ttA).norm(2) < 1.0e-12);}
0
public void testTimesMatrixCardinality()
{    Matrix other = test.like(5, 8);    test.times(other);}
0
public void testTranspose()
{    Matrix transpose = test.transpose();    assertEquals("rows", test.columnSize(), transpose.rowSize());    assertEquals("cols", test.rowSize(), transpose.columnSize());    for (int row = 0; row < test.rowSize(); row++) {        for (int col = 0; col < test.columnSize(); col++) {            assertEquals("value[" + row + "][" + col + ']', test.getQuick(row, col), transpose.getQuick(col, row), EPSILON);        }    }}
0
public void testZSum()
{    double sum = test.zSum();    assertEquals("zsum", 23.1, sum, EPSILON);}
0
public void testAssignRow()
{    double[] data = { 2.1, 3.2 };    test.assignRow(1, new DenseVector(data));    assertEquals("test[1][0]", 2.1, test.getQuick(1, 0), EPSILON);    assertEquals("test[1][1]", 3.2, test.getQuick(1, 1), EPSILON);}
0
public void testAssignRowCardinality()
{    double[] data = { 2.1, 3.2, 4.3 };    test.assignRow(1, new DenseVector(data));}
0
public void testAssignColumn()
{    double[] data = { 2.1, 3.2, 4.3 };    test.assignColumn(1, new DenseVector(data));    assertEquals("test[0][1]", 2.1, test.getQuick(0, 1), EPSILON);    assertEquals("test[1][1]", 3.2, test.getQuick(1, 1), EPSILON);    assertEquals("test[2][1]", 4.3, test.getQuick(2, 1), EPSILON);}
0
public void testAssignColumnCardinality()
{    double[] data = { 2.1, 3.2 };    test.assignColumn(1, new DenseVector(data));}
0
public void testViewRow()
{    Vector row = test.viewRow(1);    assertEquals("row size", 2, row.getNumNondefaultElements());        Matrix matrix = new SparseMatrix(1, 1);    Vector view = matrix.viewRow(0);    final double value = 1.23;    view.assign(value);        assertEquals("Matrix value", view.getQuick(0), matrix.getQuick(0, 0), EPSILON);}
0
public void testViewRowIndexUnder()
{    test.viewRow(-1);}
0
public void testViewRowIndexOver()
{    test.viewRow(5);}
0
public void testViewColumn()
{    Vector column = test.viewColumn(1);    assertEquals("row size", 3, column.getNumNondefaultElements());}
0
public void testViewColumnIndexUnder()
{    test.viewColumn(-1);}
0
public void testViewColumnIndexOver()
{    test.viewColumn(5);}
0
public void testDeterminant()
{    Matrix m = matrixFactory(new double[][] { { 1, 3, 4 }, { 5, 2, 3 }, { 1, 4, 2 } });    assertEquals("determinant", 43.0, m.determinant(), EPSILON);}
0
public void testLabelBindings()
{    Matrix m = matrixFactory(new double[][] { { 1, 3, 4 }, { 5, 2, 3 }, { 1, 4, 2 } });    assertNull("row bindings", m.getRowLabelBindings());    assertNull("col bindings", m.getColumnLabelBindings());    Map<String, Integer> rowBindings = new HashMap<>();    rowBindings.put("Fee", 0);    rowBindings.put("Fie", 1);    rowBindings.put("Foe", 2);    m.setRowLabelBindings(rowBindings);    assertEquals("row", rowBindings, m.getRowLabelBindings());    Map<String, Integer> colBindings = new HashMap<>();    colBindings.put("Foo", 0);    colBindings.put("Bar", 1);    colBindings.put("Baz", 2);    m.setColumnLabelBindings(colBindings);    assertEquals("row", rowBindings, m.getRowLabelBindings());    assertEquals("Fee", m.get(0, 1), m.get("Fee", "Bar"), EPSILON);    double[] newrow = { 9, 8, 7 };    m.set("Foe", newrow);    assertEquals("FeeBaz", m.get(0, 2), m.get("Fee", "Baz"), EPSILON);}
0
public void testSettingLabelBindings()
{    Matrix m = matrixFactory(new double[][] { { 1, 3, 4 }, { 5, 2, 3 }, { 1, 4, 2 } });    assertNull("row bindings", m.getRowLabelBindings());    assertNull("col bindings", m.getColumnLabelBindings());    m.set("Fee", "Foo", 1, 2, 9);    assertNotNull("row", m.getRowLabelBindings());    assertNotNull("row", m.getRowLabelBindings());    assertEquals("Fee", 1, m.getRowLabelBindings().get("Fee").intValue());    assertEquals("Fee", 2, m.getColumnLabelBindings().get("Foo").intValue());    assertEquals("FeeFoo", m.get(1, 2), m.get("Fee", "Foo"), EPSILON);    m.get("Fie", "Foe");}
0
public void testLabelBindingSerialization()
{    Matrix m = matrixFactory(new double[][] { { 1, 3, 4 }, { 5, 2, 3 }, { 1, 4, 2 } });    assertNull("row bindings", m.getRowLabelBindings());    assertNull("col bindings", m.getColumnLabelBindings());    Map<String, Integer> rowBindings = new HashMap<>();    rowBindings.put("Fee", 0);    rowBindings.put("Fie", 1);    rowBindings.put("Foe", 2);    m.setRowLabelBindings(rowBindings);    assertEquals("row", rowBindings, m.getRowLabelBindings());    Map<String, Integer> colBindings = new HashMap<>();    colBindings.put("Foo", 0);    colBindings.put("Bar", 1);    colBindings.put("Baz", 2);    m.setColumnLabelBindings(colBindings);    assertEquals("col", colBindings, m.getColumnLabelBindings());}
0
public void testColumnView()
{    Matrix matrix = new DenseMatrix(5, 3);    Vector column2 = matrix.viewColumn(2);    Matrix outerProduct = column2.cross(column2);    assertEquals(matrix.numRows(), outerProduct.numRows());    assertEquals(matrix.numRows(), outerProduct.numCols());}
0
public void testIndexRange()
{    Matrix m = new DenseMatrix(20, 30).assign(Functions.random());    try {        m.viewColumn(30);        fail("Should have thrown exception");    } catch (IllegalArgumentException e) {        assertTrue(e.getMessage().startsWith("Index 30 is outside allowable"));    }    try {        m.viewRow(20);        fail("Should have thrown exception");    } catch (IllegalArgumentException e) {        assertTrue(e.getMessage().startsWith("Index 20 is outside allowable"));    }}
0
public void testCorrectValues() throws Exception
{    byte[] bytes = "Now is the time for all good men to come to the aid of their country".getBytes("UTF-8");    int hash = 0;    for (int i = 0; i < bytes.length; i++) {        hash = hash * 31 + (bytes[i] & 0xff);        bytes[i] = (byte) hash;    }        for (int offset = 0; offset < 10; offset++) {        byte[] arr = new byte[bytes.length + offset];        System.arraycopy(bytes, 0, arr, offset, bytes.length);        for (int len = 0; len < bytes.length; len++) {            int h = MurmurHash3.murmurhash3x8632(arr, offset, len, len);            assertEquals(ANSWERS[len], h);        }    }}
0
public void testForLotsOfChange64() throws UnsupportedEncodingException
{    long h1 = MurmurHash.hash64A("abc".getBytes(Charsets.UTF_8), 0);    long h2 = MurmurHash.hash64A("abc ".getBytes(Charsets.UTF_8), 0);    int flipCount = Long.bitCount(h1 ^ h2);    Assert.assertTrue("Small changes should result in lots of bit flips, only found " + flipCount, flipCount > 25);}
0
public void testHash64()
{        Assert.assertEquals(0x9cc9c33498a95efbL, MurmurHash.hash64A("abc".getBytes(Charsets.UTF_8), 0));    Assert.assertEquals(0xd2c8c9b470122bddL, MurmurHash.hash64A("abc def ghi jkl ".getBytes(Charsets.UTF_8), 0));    Assert.assertEquals(0xcd37895736a81cbcL, MurmurHash.hash64A("abc def ghi jkl moreGoo".getBytes(Charsets.UTF_8), 0));}
0
public void testForLotsOfChange32() throws UnsupportedEncodingException
{    int h1 = MurmurHash.hash("abc".getBytes(Charsets.UTF_8), 0);    int h2 = MurmurHash.hash("abc ".getBytes(Charsets.UTF_8), 0);    int flipCount = Integer.bitCount(h1 ^ h2);    Assert.assertTrue("Small changes should result in lots of bit flips, only found " + flipCount, flipCount > 14);}
0
public void testChangingSeed()
{        byte[] key = { 0x4E, (byte) 0xE3, (byte) 0x91, 0x00, 0x10, (byte) 0x8F, (byte) 0xFF };    int[] expected = { 0xeef8be32, 0x8109dec6, 0x9aaf4192, 0xc1bcaf1c, 0x821d2ce4, 0xd45ed1df, 0x6c0357a7, 0x21d4e845, 0xfa97db50, 0x2f1985c8, 0x5d69782a, 0x0d6e4b85, 0xe7d9cf6b, 0x337e6b49, 0xe1606944, 0xccc18ae8 };    for (int i = 0; i < expected.length; i++) {        int expectedHash = expected[i];        int hash = MurmurHash.hash(key, i);        Assert.assertEquals("i = " + i, expectedHash, hash);    }}
0
public void testChangingKey()
{    byte[] key = new byte[133];    int[] expected = { 0xd743ae0b, 0xf1b461c6, 0xa45a6ceb, 0xdb15e003, 0x877721a4, 0xc30465f1, 0xfb658ba4, 0x1adf93b2, 0xe40a7931, 0x3da52db0, 0xbf523511, 0x1efaf273, 0xe628c1dd, 0x9a0344df, 0x901c99fc, 0x5ae1aa44 };    for (int i = 0; i < 16; i++) {                setKey(key, i);        int expectedHash = expected[i];        int hash = MurmurHash.hash(key, 0x1234ABCD);        Assert.assertEquals("i = " + i, expectedHash, hash);    }}
0
public void testChangingKeyLength()
{    int[] expected = { 0xa0c72f8e, 0x29c2f97e, 0x00ca8bba, 0x88387876, 0xe203ce49, 0x58d75952, 0xab84febe, 0x98153c65, 0xcbb38375, 0x6ea1a28b, 0x9afa8f55, 0xfb890eb6, 0x9516cc49, 0x6408a8eb, 0xbb12d3e6, 0x00fb7519 };        for (int i = 0; i < 16; i++) {        byte[] key = new byte[i];        setKey(key, i);        int expectedHash = expected[i];        int hash = MurmurHash.hash(key, 0x7870AAFF);        Assert.assertEquals("i = " + i, expectedHash, hash);    }}
0
private static void setKey(byte[] key, int start)
{    for (int i = 0; i < key.length; i++) {        key[i] = (byte) ((start + i) & 0xFF);    }}
0
public void rank1()
{    Matrix x = new DenseMatrix(3, 3);    x.viewRow(0).assign(new double[] { 1, 2, 3 });    x.viewRow(1).assign(new double[] { 2, 4, 6 });    x.viewRow(2).assign(new double[] { 3, 6, 9 });    OldQRDecomposition qr = new OldQRDecomposition(x);    assertFalse(qr.hasFullRank());    assertEquals(0, new DenseVector(new double[] { 3.741657, 7.483315, 11.22497 }).aggregate(qr.getR().viewRow(0), Functions.PLUS, new DoubleDoubleFunction() {        @Override        public double apply(double arg1, double arg2) {            return Math.abs(arg1) - Math.abs(arg2);        }    }), 1.0e-5);}
0
public double apply(double arg1, double arg2)
{    return Math.abs(arg1) - Math.abs(arg2);}
0
public void fullRankTall()
{    Matrix x = matrix();    OldQRDecomposition qr = new OldQRDecomposition(x);    assertTrue(qr.hasFullRank());    Matrix rRef = reshape(new double[] { -2.99129686445138, 0, 0, 0, 0, -0.0282260628674372, -2.38850244769059, 0, 0, 0, 0.733739310355871, 1.48042000631646, 2.29051263117895, 0, 0, -0.0394082168269326, 0.282829484207801, -0.00438521041803086, -2.90823198084203, 0, 0.923669647838536, 1.76679276072492, 0.637690104222683, -0.225890909498753, -1.35732293800944 }, 5, 5);    Matrix r = qr.getR();    assertEquals(rRef, r, 1.0e-8);    Matrix qRef = reshape(new double[] { -0.165178287646573, 0.0510035857637869, 0.13985915987379, -0.120173729496501, -0.453198314345324, 0.644400679630493, -0.503117990820608, 0.24968739845381, 0.323968339146224, -0.465266080134262, 0.276508948773268, -0.687909700644343, 0.0544048888907195, -0.0166677718378263, 0.171309755790717, 0.310339001630029, 0.674790532821663, 0.0058166082200493, -0.381707516461884, 0.300504956413142, -0.105751091334003, 0.410450870871096, 0.31113446615821, 0.179338172684956, 0.361951807617901, 0.763921725548796, 0.380327892605634, -0.287274944594054, 0.0311604042556675, 0.0386096858143961, 0.0387156960650472, -0.232975755728917, 0.0358178276684149, 0.173105775703199, 0.327321867815603, 0.328671945345279, -0.36015879836344, -0.444261660176044, 0.09438499563253, 0.646216148583769 }, 8, 5);    printMatrix("qRef", qRef);    Matrix q = qr.getQ();    printMatrix("q", q);    assertEquals(qRef, q, 1.0e-8);    Matrix x1 = qr.solve(reshape(new double[] { -0.0178247686747641, 0.68631714634098, -0.335464858468858, 1.50249941751569, -0.669901640772149, -0.977025038942455, -1.18857546169856, -1.24792900492054 }, 8, 1));    Matrix xref = reshape(new double[] { -0.0127440093664874, 0.655825940180799, -0.100755415991702, -0.0349559562697406, -0.190744297762028 }, 5, 1);    printMatrix("x1", x1);    printMatrix("xref", xref);    assertEquals(xref, x1, 1.0e-8);}
0
public void fullRankWide()
{    Matrix x = matrix().transpose();    OldQRDecomposition qr = new OldQRDecomposition(x);    assertFalse(qr.hasFullRank());    Matrix rActual = qr.getR();    Matrix rRef = reshape(new double[] { -2.42812464965842, 0, 0, 0, 0, 0.303587286111356, -2.91663643494775, 0, 0, 0, -0.201812474153156, -0.765485720168378, 1.09989373598954, 0, 0, 1.47980701097885, -0.637545820524326, -1.55519859337935, 0.844655127991726, 0, 0.0248883129453161, 0.00115010570270549, -0.236340588891252, -0.092924118200147, 1.42910099545547, -1.1678472412429, 0.531245845248056, 0.351978196071514, -1.03241474816555, -2.20223861735426, -0.887809959067632, 0.189731251982918, -0.504321849233586, 0.490484123999836, 1.21266692336743, -0.633888169775463, 1.04738559065986, 0.284041239547031, 0.578183510077156, -0.942314870832456 }, 5, 8);    printMatrix("rRef", rRef);    printMatrix("rActual", rActual);    assertEquals(rRef, rActual, 1.0e-8);    Matrix qRef = reshape(new double[] { -0.203489262374627, 0.316761677948356, -0.784155643293468, 0.394321494579, -0.29641971170211, 0.0311283614803723, -0.34755265020736, 0.137138511478328, 0.848579887681972, 0.373287266507375, -0.39603700561249, -0.787812566647329, -0.377864833067864, -0.275080943427399, 0.0636764674878229, 0.0763976893309043, -0.318551137554327, 0.286407036668598, 0.206004127289883, -0.876482672226889, 0.89159476695423, -0.238213616975551, -0.376141107880836, -0.0794701657055114, 0.0227025098210165 }, 5, 5);    Matrix q = qr.getQ();    printMatrix("qRef", qRef);    printMatrix("q", q);    assertEquals(qRef, q, 1.0e-8);    Matrix x1 = qr.solve(b());    Matrix xRef = reshape(new double[] { -0.182580239668147, -0.437233627652114, 0.138787653097464, 0.672934739896228, -0.131420217069083, 0, 0, 0 }, 8, 1);    printMatrix("xRef", xRef);    printMatrix("x", x1);    assertEquals(xRef, x1, 1.0e-8);}
0
private static void assertEquals(Matrix ref, Matrix actual, double epsilon)
{    assertEquals(0, ref.minus(actual).aggregate(Functions.MAX, Functions.ABS), epsilon);}
0
private static void printMatrix(String name, Matrix m)
{    int rows = m.numRows();    int columns = m.numCols();    System.out.printf("%s - %d x %d\n", name, rows, columns);    for (int i = 0; i < rows; i++) {        for (int j = 0; j < columns; j++) {            System.out.printf("%10.5f", m.get(i, j));        }        System.out.printf("\n");    }    System.out.printf("\n");    System.out.printf("\n");}
0
private static Matrix matrix()
{    double[] values = { 0.494097293912641, -0.152566866170993, -0.418360266395271, 0.359475300232312, 1.35565069667582, -1.92759373242903, 1.50497526839076, -0.746889132087904, -0.769136838293565, 1.10984954080986, -0.664389974392489, 1.6464660350229, -0.11715420616969, 0.0216221197371269, -0.394972730980765, -0.748293157213142, 1.90402764664962, -0.638042862848559, -0.362336344669668, -0.418261074380526, -0.494211543128429, 1.38828971158414, 0.597110366867923, 1.05341387608687, -0.957461740877418, -2.35528802598249, -1.03171458944128, 0.644319090271635, -0.0569108993041965, -0.14419465550881, -0.0456801828174936, 0.754694392571835, 0.719744008628535, -1.17873249802301, -0.155887528905918, -1.5159868405466, 0.0918931582603128, 1.42179027361583, -0.100495054250176, 0.0687986548485584 };    return reshape(values, 8, 5);}
0
private static Matrix reshape(double[] values, int rows, int columns)
{    Matrix m = new DenseMatrix(rows, columns);    int i = 0;    for (double v : values) {        m.set(i % rows, i / rows, v);        i++;    }    return m;}
0
private static Matrix b()
{    return reshape(new double[] { -0.0178247686747641, 0.68631714634098, -0.335464858468858, 1.50249941751569, -0.669901640772149 }, 5, 1);}
0
public void testViewBasics()
{    Vector v = randomVector();    int[] pivot = pivot();    Vector pvv = new PermutedVectorView(v, pivot);        for (int i = 0; i < 20; i++) {        assertEquals("Element " + i, v.get(pivot[i]), pvv.get(i), 0);    }        pvv.set(6, 321);    v.set(9, 512);        for (int i = 0; i < 20; i++) {        assertEquals("Element " + i, v.get(pivot[i]), pvv.get(i), 0);    }}
0
public void testIterators()
{    int[] pivot = pivot();    int[] unpivot = unpivot();    Vector v = randomVector();    PermutedVectorView pvv = new PermutedVectorView(v, pivot);        assertEquals(v.zSum(), pvv.zSum(), 0);    assertEquals(v.getNumNondefaultElements(), pvv.getNumNondefaultElements());    v.set(11, 0);    assertEquals(v.getNumNondefaultElements(), pvv.getNumNondefaultElements());    Iterator<Vector.Element> vi = pvv.iterator();    int i = 0;    while (vi.hasNext()) {        Vector.Element e = vi.next();        assertEquals("Index " + i, i, pivot[e.index()]);        assertEquals("Reverse Index " + i, unpivot[i], e.index());        assertEquals("Self-value " + i, e.get(), pvv.get(e.index()), 0);                assertEquals("Value " + i, v.get(i), e.get(), 0);        i++;    }}
0
private static int[] pivot()
{    return new int[] { 11, 7, 10, 9, 8, 3, 17, 0, 19, 13, 12, 1, 5, 6, 16, 2, 4, 14, 18, 15 };}
0
private static int[] unpivot()
{    int[] pivot = pivot();    int[] unpivot = new int[20];    for (int i = 0; i < 20; i++) {        unpivot[pivot[i]] = i;    }    return unpivot;}
0
private static Vector randomVector()
{    Vector v = new DenseVector(20);    v.assign(new DoubleFunction() {        private final Random gen = RandomUtils.getRandom();        @Override        public double apply(double arg1) {            return gen.nextDouble();        }    });    return v;}
0
public double apply(double arg1)
{    return gen.nextDouble();}
0
public Matrix matrixFactory(double[][] values)
{    Matrix base = new DenseMatrix(values);                PivotedMatrix pm = new PivotedMatrix(base.like());    pm.swap(0, 1);    pm.swapRows(1, 2);    pm.assign(base);    return pm;}
0
public void testSwap()
{    Matrix m = new DenseMatrix(10, 10);    for (int i = 0; i < 10; i++) {        for (int j = 0; j < 10; j++) {            m.set(i, j, 10 * i + j);        }    }    PivotedMatrix pm = new PivotedMatrix(m);    pm.swap(3, 5);    assertEquals(0, pm.viewDiagonal().minus(new DenseVector(new double[] { 0, 11, 22, 55, 44, 33, 66, 77, 88, 99 })).norm(1), 1.0e-10);    pm.swap(2, 7);    assertEquals(0, pm.viewDiagonal().minus(new DenseVector(new double[] { 0, 11, 77, 55, 44, 33, 66, 22, 88, 99 })).norm(1), 1.0e-10);    pm.swap(5, 8);    assertEquals(0, pm.viewColumn(4).minus(new DenseVector(new double[] { 4.0, 14.0, 74.0, 54.0, 44.0, 84.0, 64.0, 24.0, 34.0, 94.0 })).norm(1), 1.0e-10);    assertEquals(0, pm.viewDiagonal().minus(new DenseVector(new double[] { 0, 11, 77, 55, 44, 88, 66, 22, 33, 99 })).norm(1), 1.0e-10);}
0
public void randomMatrix()
{    Matrix a = new DenseMatrix(60, 60).assign(Functions.random());    QRDecomposition qr = new QRDecomposition(a);        double maxIdent = qr.getQ().transpose().times(qr.getQ()).viewDiagonal().assign(Functions.plus(-1)).norm(1);    assertEquals(0, maxIdent, 1.0e-13);        Matrix z = qr.getQ().times(qr.getR()).minus(a);    double maxError = z.aggregate(Functions.MIN, Functions.ABS);    assertEquals(0, maxError, 1.0e-13);}
0
public void rank1()
{    Matrix x = new DenseMatrix(3, 3);    x.viewRow(0).assign(new double[] { 1, 2, 3 });    x.viewRow(1).assign(new double[] { 2, 4, 6 });    x.viewRow(2).assign(new double[] { 3, 6, 9 });    QRDecomposition qr = new QRDecomposition(x);    assertFalse(qr.hasFullRank());    assertEquals(0, new DenseVector(new double[] { 3.741657, 7.483315, 11.22497 }).aggregate(qr.getR().viewRow(0), Functions.PLUS, new DoubleDoubleFunction() {        @Override        public double apply(double arg1, double arg2) {            return Math.abs(arg1) - Math.abs(arg2);        }    }), 1.0e-5);}
0
public double apply(double arg1, double arg2)
{    return Math.abs(arg1) - Math.abs(arg2);}
0
public void fullRankTall()
{    Matrix x = matrix();    QRDecomposition qr = new QRDecomposition(x);    assertTrue(qr.hasFullRank());    Matrix rRef = reshape(new double[] { -2.99129686445138, 0, 0, 0, 0, -0.0282260628674372, -2.38850244769059, 0, 0, 0, 0.733739310355871, 1.48042000631646, 2.29051263117895, 0, 0, -0.0394082168269326, 0.282829484207801, -0.00438521041803086, -2.90823198084203, 0, 0.923669647838536, 1.76679276072492, 0.637690104222683, -0.225890909498753, -1.35732293800944 }, 5, 5);    Matrix r = qr.getR();        assertEquals(0, r.clone().assign(Functions.ABS).minus(rRef.clone().assign(Functions.ABS)).aggregate(Functions.PLUS, Functions.IDENTITY), 1.0e-12);    Matrix qRef = reshape(new double[] { -0.165178287646573, 0.0510035857637869, 0.13985915987379, -0.120173729496501, -0.453198314345324, 0.644400679630493, -0.503117990820608, 0.24968739845381, 0.323968339146224, -0.465266080134262, 0.276508948773268, -0.687909700644343, 0.0544048888907195, -0.0166677718378263, 0.171309755790717, 0.310339001630029, 0.674790532821663, 0.0058166082200493, -0.381707516461884, 0.300504956413142, -0.105751091334003, 0.410450870871096, 0.31113446615821, 0.179338172684956, 0.361951807617901, 0.763921725548796, 0.380327892605634, -0.287274944594054, 0.0311604042556675, 0.0386096858143961, 0.0387156960650472, -0.232975755728917, 0.0358178276684149, 0.173105775703199, 0.327321867815603, 0.328671945345279, -0.36015879836344, -0.444261660176044, 0.09438499563253, 0.646216148583769 }, 8, 5);    printMatrix("qRef", qRef);    Matrix q = qr.getQ();    printMatrix("q", q);    assertEquals(0, q.clone().assign(Functions.ABS).minus(qRef.clone().assign(Functions.ABS)).aggregate(Functions.PLUS, Functions.IDENTITY), 1.0e-12);    Matrix x1 = qr.solve(reshape(new double[] { -0.0178247686747641, 0.68631714634098, -0.335464858468858, 1.50249941751569, -0.669901640772149, -0.977025038942455, -1.18857546169856, -1.24792900492054 }, 8, 1));    Matrix xref = reshape(new double[] { -0.0127440093664874, 0.655825940180799, -0.100755415991702, -0.0349559562697406, -0.190744297762028 }, 5, 1);    printMatrix("x1", x1);    printMatrix("xref", xref);    assertEquals(xref, x1, 1.0e-8);}
0
public void fullRankWide()
{    Matrix x = matrix().transpose();    QRDecomposition qr = new QRDecomposition(x);    assertTrue(qr.hasFullRank());    Matrix rActual = qr.getR();    Matrix rRef = reshape(new double[] { -2.42812464965842, 0, 0, 0, 0, 0.303587286111356, -2.91663643494775, 0, 0, 0, -0.201812474153156, -0.765485720168378, 1.09989373598954, 0, 0, 1.47980701097885, -0.637545820524326, -1.55519859337935, 0.844655127991726, 0, 0.0248883129453161, 0.00115010570270549, -0.236340588891252, -0.092924118200147, 1.42910099545547, -1.1678472412429, 0.531245845248056, 0.351978196071514, -1.03241474816555, -2.20223861735426, -0.887809959067632, 0.189731251982918, -0.504321849233586, 0.490484123999836, 1.21266692336743, -0.633888169775463, 1.04738559065986, 0.284041239547031, 0.578183510077156, -0.942314870832456 }, 5, 8);    printMatrix("rRef", rRef);    printMatrix("rActual", rActual);    assertEquals(0, rActual.clone().assign(Functions.ABS).minus(rRef.clone().assign(Functions.ABS)).aggregate(Functions.PLUS, Functions.IDENTITY), 1.0e-12);        Matrix qRef = reshape(new double[] { -0.203489262374627, 0.316761677948356, -0.784155643293468, 0.394321494579, -0.29641971170211, 0.0311283614803723, -0.34755265020736, 0.137138511478328, 0.848579887681972, 0.373287266507375, -0.39603700561249, -0.787812566647329, -0.377864833067864, -0.275080943427399, 0.0636764674878229, 0.0763976893309043, -0.318551137554327, 0.286407036668598, 0.206004127289883, -0.876482672226889, 0.89159476695423, -0.238213616975551, -0.376141107880836, -0.0794701657055114, 0.0227025098210165 }, 5, 5);    Matrix q = qr.getQ();    printMatrix("qRef", qRef);    printMatrix("q", q);    assertEquals(0, q.clone().assign(Functions.ABS).minus(qRef.clone().assign(Functions.ABS)).aggregate(Functions.PLUS, Functions.IDENTITY), 1.0e-12);        Matrix x1 = qr.solve(b());    Matrix xRef = reshape(new double[] { -0.182580239668147, -0.437233627652114, 0.138787653097464, 0.672934739896228, -0.131420217069083, 0, 0, 0 }, 8, 1);    printMatrix("xRef", xRef);    printMatrix("x", x1);    assertEquals(xRef, x1, 1.0e-8);    assertEquals(x, qr.getQ().times(qr.getR()), 1.0e-15);}
0
public void fasterThanBefore()
{    OnlineSummarizer s1 = new OnlineSummarizer();    OnlineSummarizer s2 = new OnlineSummarizer();    Matrix a = new DenseMatrix(60, 60).assign(Functions.random());    decompositionSpeedCheck(new Decomposer() {        @Override        public QR decompose(Matrix a) {            return new QRDecomposition(a);        }    }, s1, a, "new");    decompositionSpeedCheck(new Decomposer() {        @Override        public QR decompose(Matrix a) {            return new OldQRDecomposition(a);        }    }, s2, a, "old");        System.out.printf("Speedup is about %.1f times\n", s2.getMean() / s1.getMean());    assertTrue(s1.getMean() < 0.5 * s2.getMean());}
0
public QR decompose(Matrix a)
{    return new QRDecomposition(a);}
0
public QR decompose(Matrix a)
{    return new OldQRDecomposition(a);}
0
private static void decompositionSpeedCheck(Decomposer qrf, OnlineSummarizer s1, Matrix a, String label)
{    int n = 0;    List<Integer> counts = Lists.newArrayList(10, 20, 50, 100, 200, 500);    for (int k : counts) {        double warmup = 0;        double other = 0;        n += k;        for (int i = 0; i < k; i++) {            QR qr = qrf.decompose(a);            warmup = Math.max(warmup, qr.getQ().transpose().times(qr.getQ()).viewDiagonal().assign(Functions.plus(-1)).norm(1));            Matrix z = qr.getQ().times(qr.getR()).minus(a);            other = Math.max(other, z.aggregate(Functions.MIN, Functions.ABS));        }        double maxIdent = 0;        double maxError = 0;        long t0 = System.nanoTime();        for (int i = 0; i < n; i++) {            QR qr = qrf.decompose(a);            maxIdent = Math.max(maxIdent, qr.getQ().transpose().times(qr.getQ()).viewDiagonal().assign(Functions.plus(-1)).norm(1));            Matrix z = qr.getQ().times(qr.getR()).minus(a);            maxError = Math.max(maxError, z.aggregate(Functions.MIN, Functions.ABS));        }        long t1 = System.nanoTime();        if (k > 100) {            s1.add(t1 - t0);        }        System.out.printf("%s %d\t%.1f\t%g\t%g\t%g\n", label, n, (t1 - t0) / 1.0e3 / n, maxIdent, maxError, warmup);    }}
0
private static void assertEquals(Matrix ref, Matrix actual, double epsilon)
{    assertEquals(0, ref.minus(actual).aggregate(Functions.MAX, Functions.ABS), epsilon);}
0
private static void printMatrix(String name, Matrix m)
{    int rows = m.numRows();    int columns = m.numCols();    System.out.printf("%s - %d x %d\n", name, rows, columns);    for (int i = 0; i < rows; i++) {        for (int j = 0; j < columns; j++) {            System.out.printf("%10.5f", m.get(i, j));        }        System.out.printf("\n");    }    System.out.printf("\n");    System.out.printf("\n");}
0
private static Matrix matrix()
{    double[] values = { 0.494097293912641, -0.152566866170993, -0.418360266395271, 0.359475300232312, 1.35565069667582, -1.92759373242903, 1.50497526839076, -0.746889132087904, -0.769136838293565, 1.10984954080986, -0.664389974392489, 1.6464660350229, -0.11715420616969, 0.0216221197371269, -0.394972730980765, -0.748293157213142, 1.90402764664962, -0.638042862848559, -0.362336344669668, -0.418261074380526, -0.494211543128429, 1.38828971158414, 0.597110366867923, 1.05341387608687, -0.957461740877418, -2.35528802598249, -1.03171458944128, 0.644319090271635, -0.0569108993041965, -0.14419465550881, -0.0456801828174936, 0.754694392571835, 0.719744008628535, -1.17873249802301, -0.155887528905918, -1.5159868405466, 0.0918931582603128, 1.42179027361583, -0.100495054250176, 0.0687986548485584 };    return reshape(values, 8, 5);}
0
private static Matrix reshape(double[] values, int rows, int columns)
{    Matrix m = new DenseMatrix(rows, columns);    int i = 0;    for (double v : values) {        m.set(i % rows, i / rows, v);        i++;    }    return m;}
0
private static Matrix b()
{    return reshape(new double[] { -0.0178247686747641, 0.68631714634098, -0.335464858468858, 1.50249941751569, -0.669901640772149 }, 5, 1);}
0
public void testDepth()
{    List<Integer> totals = Lists.newArrayList();    for (int i = 0; i < 1000; i++) {        ChineseRestaurant x = new ChineseRestaurant(10);        Multiset<Integer> counts = HashMultiset.create();        for (int j = 0; j < 100; j++) {            counts.add(x.sample());        }        List<Integer> tmp = Lists.newArrayList();        for (Integer k : counts.elementSet()) {            tmp.add(counts.count(k));        }        Collections.sort(tmp, Collections.reverseOrder());        while (totals.size() < tmp.size()) {            totals.add(0);        }        int j = 0;        for (Integer k : tmp) {            totals.set(j, totals.get(j) + k);            j++;        }    }        assertEquals(25000.0, (double) totals.get(0), 1000);    assertEquals(24000.0, (double) totals.get(1), 1000);    assertEquals(8000.0, (double) totals.get(2), 200);    assertEquals(1000.0, (double) totals.get(15), 50);    assertEquals(1000.0, (double) totals.get(20), 40);}
0
public void testExtremeDiscount()
{    ChineseRestaurant x = new ChineseRestaurant(100, 1);    Multiset<Integer> counts = HashMultiset.create();    for (int i = 0; i < 10000; i++) {        counts.add(x.sample());    }    assertEquals(10000, x.size());    for (int i = 0; i < 10000; i++) {        assertEquals(1, x.count(i));    }}
0
public void testGrowth()
{    ChineseRestaurant s0 = new ChineseRestaurant(10, 0.0);    ChineseRestaurant s5 = new ChineseRestaurant(10, 0.5);    ChineseRestaurant s9 = new ChineseRestaurant(10, 0.9);    Set<Double> splits = ImmutableSet.of(1.0, 1.5, 2.0, 3.0, 5.0, 8.0);    double offset0 = 0;    int k = 0;    int i = 0;    Matrix m5 = new DenseMatrix(20, 3);    Matrix m9 = new DenseMatrix(20, 3);    while (i <= 200000) {        double n = i / Math.pow(10, Math.floor(Math.log10(i)));        if (splits.contains(n)) {                        if (i > 900) {                double predict5 = predictSize(m5.viewPart(0, k, 0, 3), i, 0.5);                assertEquals(predict5, Math.log(s5.size()), 1);                double predict9 = predictSize(m9.viewPart(0, k, 0, 3), i, 0.9);                assertEquals(predict9, Math.log(s9.size()), 1);                        } else if (i > 50) {                double x = 10.5 * Math.log(i) - s0.size();                m5.viewRow(k).assign(new double[] { Math.log(s5.size()), Math.log(i), 1 });                m9.viewRow(k).assign(new double[] { Math.log(s9.size()), Math.log(i), 1 });                k++;                offset0 += (x - offset0) / k;            }            if (i > 10000) {                assertEquals(0.0, (double) hapaxCount(s0) / s0.size(), 0.25);                assertEquals(0.5, (double) hapaxCount(s5) / s5.size(), 0.1);                assertEquals(0.9, (double) hapaxCount(s9) / s9.size(), 0.05);            }        }        s0.sample();        s5.sample();        s9.sample();        i++;    }}
0
private static double predictSize(Matrix m, int currentIndex, double expectedCoefficient)
{    int rows = m.rowSize();    Matrix a = m.viewPart(0, rows, 1, 2);    Matrix b = m.viewPart(0, rows, 0, 1);    Matrix ata = a.transpose().times(a);    Matrix atb = a.transpose().times(b);    QRDecomposition s = new QRDecomposition(ata);    Matrix r = s.solve(atb).transpose();    assertEquals(expectedCoefficient, r.get(0, 0), 0.2);    return r.times(new DenseVector(new double[] { Math.log(currentIndex), 1 })).get(0);}
0
private static int hapaxCount(ChineseRestaurant s)
{    int r = 0;    for (int i = 0; i < s.size(); i++) {        if (s.count(i) == 1) {            r++;        }    }    return r;}
0
public void testSimpleDist()
{    RandomUtils.useTestSeed();    Empirical z = new Empirical(true, true, 3, 0, 1, 0.5, 2, 1, 3.0);    List<Double> r = Lists.newArrayList();    for (int i = 0; i < 10001; i++) {        r.add(z.sample());    }    Collections.sort(r);    assertEquals(2.0, r.get(5000), 0.15);}
0
public void testZeros()
{    Empirical z = new Empirical(true, true, 3, 0, 1, 0.5, 2, 1, 3.0);    assertEquals(-16.52, z.sample(0), 1.0e-2);    assertEquals(20.47, z.sample(1), 1.0e-2);}
0
public void testBadArguments()
{    try {        new Empirical(true, false, 20, 0, 1, 0.5, 2, 0.9, 9, 0.99, 10.0);        Assert.fail("Should have caught that");    } catch (IllegalArgumentException e) {    }    try {        new Empirical(false, true, 20, 0.1, 1, 0.5, 2, 0.9, 9, 1, 10.0);        Assert.fail("Should have caught that");    } catch (IllegalArgumentException e) {    }    try {        new Empirical(true, true, 20, -0.1, 1, 0.5, 2, 0.9, 9, 1, 10.0);        Assert.fail("Should have caught that");    } catch (IllegalArgumentException e) {    }    try {        new Empirical(true, true, 20, 0, 1, 0.5, 2, 0.9, 9, 1.2, 10.0);        Assert.fail("Should have caught that");    } catch (IllegalArgumentException e) {    }    try {        new Empirical(true, true, 20, 0, 1, 0.5, 2, 0.4, 9, 1, 10.0);        Assert.fail("Should have caught that");    } catch (IllegalArgumentException e) {    }}
0
public void testBasicText()
{    RandomUtils.useTestSeed();    IndianBuffet<String> sampler = IndianBuffet.createTextDocumentSampler(30);    Multiset<String> counts = HashMultiset.create();    int[] lengths = new int[100];    for (int i = 0; i < 30; i++) {        final List<String> doc = sampler.sample();        lengths[doc.size()]++;        for (String w : doc) {            counts.add(w);        }        System.out.printf("%s\n", doc);    }}
0
public void setUp()
{    RandomUtils.useTestSeed();}
0
public void testNoValues()
{    Multiset<String> emptySet = HashMultiset.create();    new Multinomial<>(emptySet);}
0
public void testSingleton()
{    Multiset<String> oneThing = HashMultiset.create();    oneThing.add("one");    Multinomial<String> s = new Multinomial<>(oneThing);    assertEquals("one", s.sample(0));    assertEquals("one", s.sample(0.1));    assertEquals("one", s.sample(1));}
0
public void testEvenSplit()
{    Multiset<String> stuff = HashMultiset.create();    for (int i = 0; i < 5; i++) {        stuff.add(String.valueOf(i));    }    Multinomial<String> s = new Multinomial<>(stuff);    double EPSILON = 1.0e-15;    Multiset<String> cnt = HashMultiset.create();    for (int i = 0; i < 5; i++) {        cnt.add(s.sample(i * 0.2));        cnt.add(s.sample(i * 0.2 + EPSILON));        cnt.add(s.sample((i + 1) * 0.2 - EPSILON));    }    assertEquals(5, cnt.elementSet().size());    for (String v : cnt.elementSet()) {        assertEquals(3, cnt.count(v), 1.01);    }    assertTrue(cnt.contains(s.sample(1)));    assertEquals(s.sample(1 - EPSILON), s.sample(1));}
0
public void testPrime()
{    List<String> data = Lists.newArrayList();    for (int i = 0; i < 17; i++) {        String s = "0";        if ((i & 1) != 0) {            s = "1";        }        if ((i & 2) != 0) {            s = "2";        }        if ((i & 4) != 0) {            s = "3";        }        if ((i & 8) != 0) {            s = "4";        }        data.add(s);    }    Multiset<String> stuff = HashMultiset.create();    for (String x : data) {        stuff.add(x);    }    Multinomial<String> s0 = new Multinomial<>(stuff);    Multinomial<String> s1 = new Multinomial<>(stuff);    Multinomial<String> s2 = new Multinomial<>(stuff);    double EPSILON = 1.0e-15;    Multiset<String> cnt = HashMultiset.create();    for (int i = 0; i < 50; i++) {        double p0 = i * 0.02;        double p1 = (i + 1) * 0.02;        cnt.add(s0.sample(p0));        cnt.add(s0.sample(p0 + EPSILON));        cnt.add(s0.sample(p1 - EPSILON));        assertEquals(s0.sample(p0), s1.sample(p0));        assertEquals(s0.sample(p0 + EPSILON), s1.sample(p0 + EPSILON));        assertEquals(s0.sample(p1 - EPSILON), s1.sample(p1 - EPSILON));        assertEquals(s0.sample(p0), s2.sample(p0));        assertEquals(s0.sample(p0 + EPSILON), s2.sample(p0 + EPSILON));        assertEquals(s0.sample(p1 - EPSILON), s2.sample(p1 - EPSILON));    }    assertEquals(s0.sample(0), s1.sample(0));    assertEquals(s0.sample(0 + EPSILON), s1.sample(0 + EPSILON));    assertEquals(s0.sample(1 - EPSILON), s1.sample(1 - EPSILON));    assertEquals(s0.sample(1), s1.sample(1));    assertEquals(s0.sample(0), s2.sample(0));    assertEquals(s0.sample(0 + EPSILON), s2.sample(0 + EPSILON));    assertEquals(s0.sample(1 - EPSILON), s2.sample(1 - EPSILON));    assertEquals(s0.sample(1), s2.sample(1));    assertEquals(5, cnt.elementSet().size());            Map<String, Integer> ref = ImmutableMap.of("3", 35, "2", 18, "1", 9, "0", 16, "4", 72);    for (String v : cnt.elementSet()) {        assertTrue(Math.abs(ref.get(v) - cnt.count(v)) <= 2);    }    assertTrue(cnt.contains(s0.sample(1)));    assertEquals(s0.sample(1 - EPSILON), s0.sample(1));}
0
public void testInsert()
{    Random rand = RandomUtils.getRandom();    Multinomial<Integer> table = new Multinomial<>();    double[] p = new double[10];    for (int i = 0; i < 10; i++) {        p[i] = rand.nextDouble();        table.add(i, p[i]);    }    checkSelfConsistent(table);    for (int i = 0; i < 10; i++) {        assertEquals(p[i], table.getWeight(i), 0);    }}
0
public void testSetZeroWhileIterating()
{    Multinomial<Integer> table = new Multinomial<>();    for (int i = 0; i < 10000; ++i) {        table.add(i, i);    }        for (Integer sample : table) {        table.set(sample, 0);    }}
0
public void testNoNullValuesAllowed()
{    Multinomial<Integer> table = new Multinomial<>();        table.add(null, 1);}
0
public void testDeleteAndUpdate()
{    Random rand = RandomUtils.getRandom();    Multinomial<Integer> table = new Multinomial<>();    assertEquals(0, table.getWeight(), 1.0e-9);    double total = 0;    double[] p = new double[10];    for (int i = 0; i < 10; i++) {        p[i] = rand.nextDouble();        table.add(i, p[i]);        total += p[i];        assertEquals(total, table.getWeight(), 1.0e-9);    }    assertEquals(total, table.getWeight(), 1.0e-9);    checkSelfConsistent(table);    double delta = p[7] + p[8];    table.delete(7);    p[7] = 0;    table.set(8, 0);    p[8] = 0;    total -= delta;    checkSelfConsistent(table);    assertEquals(total, table.getWeight(), 1.0e-9);    for (int i = 0; i < 10; i++) {        assertEquals(p[i], table.getWeight(i), 0);        assertEquals(p[i] / total, table.getProbability(i), 1.0e-10);    }    table.set(9, 5.1);    total -= p[9];    p[9] = 5.1;    total += 5.1;    assertEquals(total, table.getWeight(), 1.0e-9);    for (int i = 0; i < 10; i++) {        assertEquals(p[i], table.getWeight(i), 0);        assertEquals(p[i] / total, table.getProbability(i), 1.0e-10);    }    checkSelfConsistent(table);    for (int i = 0; i < 10; i++) {        assertEquals(p[i], table.getWeight(i), 0);    }}
0
private static void checkSelfConsistent(Multinomial<Integer> table)
{    List<Double> weights = table.getWeights();    double totalWeight = table.getWeight();    double p = 0;    int[] k = new int[weights.size()];    for (double weight : weights) {        if (weight > 0) {            if (p > 0) {                k[table.sample(p - 1.0e-9)]++;            }            k[table.sample(p + 1.0e-9)]++;        }        p += weight / totalWeight;    }    k[table.sample(p - 1.0e-9)]++;    assertEquals(1, p, 1.0e-9);    for (int i = 0; i < weights.size(); i++) {        if (table.getWeight(i) > 0) {            assertEquals(2, k[i]);        } else {            assertEquals(0, k[i]);        }    }}
0
public void setUp()
{    RandomUtils.useTestSeed();}
0
public void testDiagonal()
{    DenseVector offset = new DenseVector(new double[] { 6, 3, 0 });    MultiNormal n = new MultiNormal(new DenseVector(new double[] { 1, 2, 5 }), offset);    OnlineSummarizer[] s = { new OnlineSummarizer(), new OnlineSummarizer(), new OnlineSummarizer() };    OnlineSummarizer[] cross = { new OnlineSummarizer(), new OnlineSummarizer(), new OnlineSummarizer() };    for (int i = 0; i < 10000; i++) {        Vector v = n.sample();        for (int j = 0; j < 3; j++) {            s[j].add(v.get(j) - offset.get(j));            int k1 = j % 2;            int k2 = (j + 1) / 2 + 1;            cross[j].add((v.get(k1) - offset.get(k1)) * (v.get(k2) - offset.get(k2)));        }    }    for (int j = 0; j < 3; j++) {        assertEquals(0, s[j].getMean() / s[j].getSD(), 0.04);        assertEquals(0, cross[j].getMean() / cross[j].getSD(), 0.04);    }}
0
public void testRadius()
{    MultiNormal gen = new MultiNormal(0.1, new DenseVector(10));    OnlineSummarizer s = new OnlineSummarizer();    for (int i = 0; i < 10000; i++) {        double x = gen.sample().norm(2) / Math.sqrt(10);        s.add(x);    }    assertEquals(0.1, s.getMean(), 0.01);}
0
public void setUp()
{    RandomUtils.useTestSeed();}
0
public void testOffset()
{    OnlineSummarizer s = new OnlineSummarizer();    Sampler<Double> sampler = new Normal(2, 5);    for (int i = 0; i < 10001; i++) {        s.add(sampler.sample());    }    assertEquals(String.format("m = %.3f, sd = %.3f", s.getMean(), s.getSD()), 2, s.getMean(), 0.04 * s.getSD());    assertEquals(5, s.getSD(), 0.12);}
0
public void testSample() throws Exception
{    double[] data = new double[10001];    Sampler<Double> sampler = new Normal();    for (int i = 0; i < data.length; i++) {        data[i] = sampler.sample();    }    Arrays.sort(data);    NormalDistribution reference = new NormalDistribution(RandomUtils.getRandom().getRandomGenerator(), 0, 1, NormalDistribution.DEFAULT_INVERSE_ABSOLUTE_ACCURACY);    assertEquals("Median", reference.inverseCumulativeProbability(0.5), data[5000], 0.04);}
0
public void setUp()
{    RandomUtils.useTestSeed();}
0
public void testBasics()
{    for (double alpha : new double[] { 0.1, 1, 10, 100 }) {        checkDistribution(new PoissonSampler(alpha), alpha);    }}
0
private static void checkDistribution(Sampler<Double> pd, double alpha)
{    int[] count = new int[(int) Math.max(10, 5 * alpha)];    for (int i = 0; i < 10000; i++) {        count[pd.sample().intValue()]++;    }    IntegerDistribution ref = new PoissonDistribution(RandomUtils.getRandom().getRandomGenerator(), alpha, PoissonDistribution.DEFAULT_EPSILON, PoissonDistribution.DEFAULT_MAX_ITERATIONS);    for (int i = 0; i < count.length; i++) {        assertEquals(ref.probability(i), count[i] / 10000.0, 2.0e-2);    }}
0
public void testAgainstReferenceOpenObjectIntHashMap()
{    OpenObjectIntHashMap<Integer> base = new OpenObjectIntHashMap<>();    Map<Integer, Integer> reference = new HashMap<>();    List<Operation> ops = Lists.newArrayList();    addOp(ops, Operation.ADD, 60);    addOp(ops, Operation.REMOVE, 30);    addOp(ops, Operation.INDEXOF, 30);    addOp(ops, Operation.CLEAR, 5);    addOp(ops, Operation.ISEMPTY, 2);    addOp(ops, Operation.SIZE, 2);    int max = randomIntBetween(1000, 20000);    for (int reps = 0; reps < max; reps++) {                int k = randomIntBetween(0, max / 4);        int v = randomInt();        switch(randomFrom(ops)) {            case ADD:                assertEquals(reference.put(k, v) == null, base.put(k, v));                break;            case REMOVE:                assertEquals(reference.remove(k) != null, base.removeKey(k));                break;            case INDEXOF:                assertEquals(reference.containsKey(k), base.containsKey(k));                break;            case CLEAR:                reference.clear();                base.clear();                break;            case ISEMPTY:                assertEquals(reference.isEmpty(), base.isEmpty());                break;            case SIZE:                assertEquals(reference.size(), base.size());                break;            default:                throw new RuntimeException();        }    }}
0
public void testAgainstReferenceOpenIntObjectHashMap()
{    OpenIntObjectHashMap<Integer> base = new OpenIntObjectHashMap<>();    Map<Integer, Integer> reference = new HashMap<>();    List<Operation> ops = Lists.newArrayList();    addOp(ops, Operation.ADD, 60);    addOp(ops, Operation.REMOVE, 30);    addOp(ops, Operation.INDEXOF, 30);    addOp(ops, Operation.CLEAR, 5);    addOp(ops, Operation.ISEMPTY, 2);    addOp(ops, Operation.SIZE, 2);    int max = randomIntBetween(1000, 20000);    for (int reps = 0; reps < max; reps++) {                int k = randomIntBetween(0, max / 4);        int v = randomInt();        switch(randomFrom(ops)) {            case ADD:                assertEquals(reference.put(k, v) == null, base.put(k, v));                break;            case REMOVE:                assertEquals(reference.remove(k) != null, base.removeKey(k));                break;            case INDEXOF:                assertEquals(reference.containsKey(k), base.containsKey(k));                break;            case CLEAR:                reference.clear();                base.clear();                break;            case ISEMPTY:                assertEquals(reference.isEmpty(), base.isEmpty());                break;            case SIZE:                assertEquals(reference.size(), base.size());                break;            default:                throw new RuntimeException();        }    }}
0
public void testAgainstReferenceOpenIntIntHashMap()
{    OpenIntIntHashMap base = new OpenIntIntHashMap();    HashMap<Integer, Integer> reference = new HashMap<>();    List<Operation> ops = Lists.newArrayList();    addOp(ops, Operation.ADD, 60);    addOp(ops, Operation.REMOVE, 30);    addOp(ops, Operation.INDEXOF, 30);    addOp(ops, Operation.CLEAR, 5);    addOp(ops, Operation.ISEMPTY, 2);    addOp(ops, Operation.SIZE, 2);    int max = randomIntBetween(1000, 20000);    for (int reps = 0; reps < max; reps++) {                int k = randomIntBetween(0, max / 4);        int v = randomInt();        switch(randomFrom(ops)) {            case ADD:                Integer prevValue = reference.put(k, v);                if (prevValue == null) {                    assertEquals(true, base.put(k, v));                } else {                    assertEquals(prevValue.intValue(), base.get(k));                    assertEquals(false, base.put(k, v));                }                break;            case REMOVE:                assertEquals(reference.containsKey(k), base.containsKey(k));                Integer removed = reference.remove(k);                if (removed == null) {                    assertEquals(false, base.removeKey(k));                } else {                    assertEquals(removed.intValue(), base.get(k));                    assertEquals(true, base.removeKey(k));                }                break;            case INDEXOF:                assertEquals(reference.containsKey(k), base.containsKey(k));                break;            case CLEAR:                reference.clear();                base.clear();                break;            case ISEMPTY:                assertEquals(reference.isEmpty(), base.isEmpty());                break;            case SIZE:                assertEquals(reference.size(), base.size());                break;            default:                throw new RuntimeException();        }    }}
0
public void testAgainstReferenceOpenIntHashSet()
{    AbstractIntSet base = new OpenIntHashSet();    HashSet<Integer> reference = Sets.newHashSet();    List<Operation> ops = Lists.newArrayList();    addOp(ops, Operation.ADD, 60);    addOp(ops, Operation.REMOVE, 30);    addOp(ops, Operation.INDEXOF, 30);    addOp(ops, Operation.CLEAR, 5);    addOp(ops, Operation.ISEMPTY, 2);    addOp(ops, Operation.SIZE, 2);    int max = randomIntBetween(1000, 20000);    for (int reps = 0; reps < max; reps++) {                int k = randomIntBetween(0, max / 4);        switch(randomFrom(ops)) {            case ADD:                assertEquals(reference.add(k), base.add(k));                break;            case REMOVE:                assertEquals(reference.remove(k), base.remove(k));                break;            case INDEXOF:                assertEquals(reference.contains(k), base.contains(k));                break;            case CLEAR:                reference.clear();                base.clear();                break;            case ISEMPTY:                assertEquals(reference.isEmpty(), base.isEmpty());                break;            case SIZE:                assertEquals(reference.size(), base.size());                break;            default:                throw new RuntimeException();        }    }}
0
public void testAgainstReferenceOpenHashSet()
{    Set<Integer> base = new OpenHashSet<>();    Set<Integer> reference = Sets.newHashSet();    List<Operation> ops = Lists.newArrayList();    addOp(ops, Operation.ADD, 60);    addOp(ops, Operation.REMOVE, 30);    addOp(ops, Operation.INDEXOF, 30);    addOp(ops, Operation.CLEAR, 5);    addOp(ops, Operation.ISEMPTY, 2);    addOp(ops, Operation.SIZE, 2);    int max = randomIntBetween(1000, 20000);    for (int reps = 0; reps < max; reps++) {                int k = randomIntBetween(0, max / 4);        switch(randomFrom(ops)) {            case ADD:                assertEquals(reference.contains(k), base.contains(k));                break;            case REMOVE:                assertEquals(reference.remove(k), base.remove(k));                break;            case INDEXOF:                assertEquals(reference.contains(k), base.contains(k));                break;            case CLEAR:                reference.clear();                base.clear();                break;            case ISEMPTY:                assertEquals(reference.isEmpty(), base.isEmpty());                break;            case SIZE:                assertEquals(reference.size(), base.size());                break;            default:                throw new RuntimeException();        }    }}
0
public void testMahout1225()
{    AbstractIntSet s = new OpenIntHashSet();    s.clear();    s.add(23);    s.add(46);    s.clear();    s.add(70);    s.add(93);    s.contains(100);}
0
public void testClearTable() throws Exception
{    OpenObjectIntHashMap<Integer> m = new OpenObjectIntHashMap<>();        m.clear();    m.put(1, 2);        m.clear();    Field tableField = m.getClass().getDeclaredField("table");    tableField.setAccessible(true);    Object[] table = (Object[]) tableField.get(m);    assertEquals(Sets.newHashSet(Arrays.asList(new Object[] { null })), Sets.newHashSet(Arrays.asList(table)));}
0
private static void addOp(List<Operation> ops, Operation op, int reps)
{    for (int i = 0; i < reps; i++) {        ops.add(op);    }}
0
public void testHashFloat()
{    Multiset<Integer> violations = HashMultiset.create();    for (int k = 0; k < 1000; k++) {        List<Float> original = Lists.newArrayList();        Random gen = RandomUtils.getRandom();        for (int i = 0; i < 10000; i++) {            float x = (float) gen.nextDouble();            original.add(x);        }        violations.add(checkCounts(original) <= 12 ? 0 : 1);    }            assertTrue(violations.count(0) >= 985);}
0
public void testHashDouble()
{    List<Double> original = Lists.newArrayList();    for (int k = 0; k < 10; k++) {        Random gen = RandomUtils.getRandom();        for (int i = 0; i < 10000; i++) {            double x = gen.nextDouble();            original.add(x);        }        checkCounts(original);    }}
0
public void testHashLong()
{    List<Long> original = Lists.newArrayList();    for (int k = 0; k < 10; k++) {        Random gen = RandomUtils.getRandom();        for (int i = 0; i < 10000; i++) {            long x = gen.nextLong();            original.add(x);        }        checkCounts(original);    }}
0
private static int checkCounts(Collection<T> original)
{    Multiset<T> hashCounts = HashMultiset.create();    for (T v : original) {        hashCounts.add(v);    }    Multiset<Integer> countCounts = HashMultiset.create();    for (T hash : hashCounts) {        countCounts.add(hashCounts.count(hash));    }    return original.size() - countCounts.count(1);}
0
public void testDegenerateMatrix()
{    double[][] m = { new double[] { 0.641284, 0.767303, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000 }, new double[] { 0.767303, 3.050159, 2.561342, 0.000000, 0.000000, 0.000000, 0.000000 }, new double[] { 0.000000, 2.561342, 5.000609, 0.810507, 0.000000, 0.000000, 0.000000 }, new double[] { 0.000000, 0.000000, 0.810507, 0.550477, 0.142853, 0.000000, 0.000000 }, new double[] { 0.000000, 0.000000, 0.000000, 0.142853, 0.254566, 0.000000, 0.000000 }, new double[] { 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 0.256073, 0.000000 }, new double[] { 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000 } };    Matrix x = new DenseMatrix(m);    EigenDecomposition eig = new EigenDecomposition(x, true);    Matrix d = eig.getD();    Matrix v = eig.getV();    check("EigenvalueDecomposition (evil)...", x.times(v), v.times(d));}
0
public void testDeficientRank()
{    Matrix a = new DenseMatrix(10, 3).assign(new DoubleFunction() {        private final Random gen = RandomUtils.getRandom();        @Override        public double apply(double arg1) {            return gen.nextGaussian();        }    });    a = a.transpose().times(a);    EigenDecomposition eig = new EigenDecomposition(a);    Matrix d = eig.getD();    Matrix v = eig.getV();    check("EigenvalueDecomposition (rank deficient)...", a.times(v), v.times(d));    Assert.assertEquals(0, eig.getImagEigenvalues().norm(1), 1.0e-10);    Assert.assertEquals(3, eig.getRealEigenvalues().norm(0), 1.0e-10);}
0
public double apply(double arg1)
{    return gen.nextGaussian();}
0
public void testEigen()
{    double[] evals = { 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 2.0e-7, 0.0, 0.0, -2.0e-7, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0 };    int i = 0;    Matrix a = new DenseMatrix(4, 4);    for (MatrixSlice row : a) {        for (Vector.Element element : row.vector().all()) {            element.set(evals[i++]);        }    }    EigenDecomposition eig = new EigenDecomposition(a);    Matrix d = eig.getD();    Matrix v = eig.getV();    check("EigenvalueDecomposition (nonsymmetric)...", a.times(v), v.times(d));}
0
public void testSequential()
{    int validld = 3;    Matrix A = new DenseMatrix(validld, validld);    double[] columnwise = { 1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0, 11.0, 12.0 };    int i = 0;    for (MatrixSlice row : A) {        for (Vector.Element element : row.vector().all()) {            element.set(columnwise[i++]);        }    }    EigenDecomposition Eig = new EigenDecomposition(A);    Matrix D = Eig.getD();    Matrix V = Eig.getV();    check("EigenvalueDecomposition (nonsymmetric)...", A.times(V), V.times(D));    A = A.transpose().times(A);    Eig = new EigenDecomposition(A);    D = Eig.getD();    V = Eig.getV();    check("EigenvalueDecomposition (symmetric)...", A.times(V), V.times(D));}
0
private static void check(String msg, Matrix a, Matrix b)
{    Assert.assertEquals(msg, 0, a.minus(b).aggregate(Functions.PLUS, Functions.ABS), 1.0e-10);}
0
public void basics()
{    Matrix m = hilbert(5);        assertEquals(1, m.get(0, 0), 0);    assertEquals(0.5, m.get(0, 1), 0);    assertEquals(1 / 6.0, m.get(2, 3), 1.0e-9);    Vector x = new DenseVector(new double[] { 5, -120, 630, -1120, 630 });    Vector b = new DenseVector(5);    b.assign(1);    assertEquals(0, m.times(x).minus(b).norm(2), 1.0e-9);    LSMR r = new LSMR();    Vector x1 = r.solve(m, b);                            assertEquals(0, m.times(x1).minus(b).norm(2), 1.0e-2);    assertEquals(0, m.transpose().times(m).times(x1).minus(m.transpose().times(b)).norm(2), 1.0e-7);        assertEquals(m.times(x1).minus(b).norm(2), r.getResidualNorm(), 1.0e-5);    assertEquals(m.transpose().times(m).times(x1).minus(m.transpose().times(b)).norm(2), r.getNormalEquationResidual(), 1.0e-9);}
0
public void random()
{    Matrix m = new DenseMatrix(200, 30).assign(Functions.random());    Vector b = new DenseVector(200).assign(1);    LSMR r = new LSMR();    Vector x1 = r.solve(m, b);        double norm = new SingularValueDecomposition(m).getS().viewDiagonal().norm(2);    double actual = m.transpose().times(m).times(x1).minus(m.transpose().times(b)).norm(2);    System.out.printf("%.4f\n", actual / norm * 1.0e6);    assertEquals(0, actual, norm * 1.0e-5);        assertEquals(m.times(x1).minus(b).norm(2), r.getResidualNorm(), 1.0e-5);    assertEquals(actual, r.getNormalEquationResidual(), 1.0e-9);}
0
private static Matrix hilbert(int n)
{    Matrix r = new DenseMatrix(n, n);    for (int i = 0; i < n; i++) {        for (int j = 0; j < n; j++) {            r.set(i, j, 1.0 / (i + j + 1));        }    }    return r;}
0
public void testConjugateGradientSolver()
{    Matrix a = getA();    Vector b = getB();    ConjugateGradientSolver solver = new ConjugateGradientSolver();    Vector x = solver.solve(a, b);    assertEquals(0.0, Math.sqrt(a.times(x).getDistanceSquared(b)), EPSILON);    assertEquals(0.0, solver.getResidualNorm(), ConjugateGradientSolver.DEFAULT_MAX_ERROR);    assertEquals(10, solver.getIterations());}
0
public void testConditionedConjugateGradientSolver()
{    Matrix a = getIllConditionedMatrix();    Vector b = getB();    Preconditioner conditioner = new JacobiConditioner(a);    ConjugateGradientSolver solver = new ConjugateGradientSolver();    Vector x = solver.solve(a, b, null, 100, ConjugateGradientSolver.DEFAULT_MAX_ERROR);    double distance = Math.sqrt(a.times(x).getDistanceSquared(b));    assertEquals(0.0, distance, EPSILON);    assertEquals(0.0, solver.getResidualNorm(), ConjugateGradientSolver.DEFAULT_MAX_ERROR);    assertEquals(16, solver.getIterations());    Vector x2 = solver.solve(a, b, conditioner, 100, ConjugateGradientSolver.DEFAULT_MAX_ERROR);        distance = Math.sqrt(a.times(x2).getDistanceSquared(b));    assertEquals(0.0, distance, EPSILON);    assertEquals(0.0, solver.getResidualNorm(), ConjugateGradientSolver.DEFAULT_MAX_ERROR);    assertEquals(15, solver.getIterations());}
0
public void testEarlyStop()
{    Matrix a = getA();    Vector b = getB();    ConjugateGradientSolver solver = new ConjugateGradientSolver();        Vector x = solver.solve(a, b, null, 10, 0.1);    double distance = Math.sqrt(a.times(x).getDistanceSquared(b));    assertTrue(distance > EPSILON);        assertEquals(0.0, distance, 0.1);        assertEquals(7, solver.getIterations());        x = solver.solve(a, b, null, 7, ConjugateGradientSolver.DEFAULT_MAX_ERROR);    distance = Math.sqrt(a.times(x).getDistanceSquared(b));    assertTrue(distance > EPSILON);    assertEquals(0.0, distance, 0.1);    assertEquals(7, solver.getIterations());}
0
private static Matrix getA()
{    return reshape(new double[] { 11.7155649822793997, -0.7125253363083646, 4.6473613961860183, 1.6020939468348456, -4.6789817799137134, -0.8140416763434970, -4.5995617505618345, -1.1749070042775340, -1.6747995811678336, 3.1922255171058342, -0.7125253363083646, 12.3400579683994867, -2.6498099427000645, 0.5264507222630669, 0.3783428369189767, -2.1170186159188811, 2.3695134252190528, 3.8182131490333013, 6.5285942298270347, 2.8564814419366353, 4.6473613961860183, -2.6498099427000645, 16.1317933921668484, -0.0409475448061225, 1.4805687075608227, -2.9958076484628950, -2.5288893025027264, -0.9614557539842487, -2.2974738351519077, -1.5516184284572598, 1.6020939468348456, 0.5264507222630669, -0.0409475448061225, 4.1946802122694482, -2.5210038046912198, 0.6634899962909317, 0.4036187419205338, -0.2829211393003727, -0.2283091172980954, 1.1253516563552464, -4.6789817799137134, 0.3783428369189767, 1.4805687075608227, -2.5210038046912198, 19.4307361862733430, -2.5200132222091787, 2.3748511971444510, 11.6426598443305522, -0.1508136510863874, 4.3471343888063512, -0.8140416763434970, -2.1170186159188811, -2.9958076484628950, 0.6634899962909317, -2.5200132222091787, 7.6712334419700747, -3.8687773629502851, -3.0453418711591529, -0.1155580876143619, -2.4025459467422121, -4.5995617505618345, 2.3695134252190528, -2.5288893025027264, 0.4036187419205338, 2.3748511971444510, -3.8687773629502851, 10.4681666057470082, 1.6527180866171229, 2.9341795819365384, -2.1708176372763099, -1.1749070042775340, 3.8182131490333013, -0.9614557539842487, -0.2829211393003727, 11.6426598443305522, -3.0453418711591529, 1.6527180866171229, 16.0050616934176233, 1.1689747208793086, 1.6665090945954870, -1.6747995811678336, 6.5285942298270347, -2.2974738351519077, -0.2283091172980954, -0.1508136510863874, -0.1155580876143619, 2.9341795819365384, 1.1689747208793086, 6.4794329751637481, -1.9197339981871877, 3.1922255171058342, 2.8564814419366353, -1.5516184284572598, 1.1253516563552464, 4.3471343888063512, -2.4025459467422121, -2.1708176372763099, 1.6665090945954870, -1.9197339981871877, 18.9149021356344598 }, 10, 10);}
0
private static Vector getB()
{    return new DenseVector(new double[] { -0.552252, 0.038430, 0.058392, -1.234496, 1.240369, 0.373649, 0.505113, 0.503723, 1.215340, -0.391908 });}
0
private static Matrix getIllConditionedMatrix()
{    return reshape(new double[] { 0.00695278043678842, 0.09911830022078683, 0.01309584636255063, 0.00652917453032394, 0.04337631487735064, 0.14232165273321387, 0.05808722912361313, -0.06591965049732287, 0.06055771542862332, 0.00577423310349649, 0.09911830022078683, 1.50071402418061428, 0.14988743575884242, 0.07195514527480981, 0.63747362341752722, 1.30711819020414688, 0.82151609385115953, -0.72616125524587938, 1.03490136002022948, 0.12800239664439328, 0.01309584636255063, 0.14988743575884242, 0.04068462583124965, 0.02147022047006482, 0.07388113580146650, 0.58070223915076002, 0.11280336266257514, -0.21690068430020618, 0.04065087561300068, -0.00876895259593769, 0.00652917453032394, 0.07195514527480981, 0.02147022047006482, 0.01140105250542524, 0.03624164348693958, 0.31291554581393255, 0.05648457235205666, -0.11507583016077780, 0.01475756130709823, -0.00584453679519805, 0.04337631487735064, 0.63747362341752722, 0.07388113580146649, 0.03624164348693959, 0.27491543200760571, 0.73410543168748121, 0.36120630002843257, -0.36583546331208316, 0.41472509341940017, 0.04581458758255480, 0.14232165273321387, 1.30711819020414666, 0.58070223915076002, 0.31291554581393255, 0.73410543168748121, 9.02536073121807014, 1.25426385582883104, -3.16186335125594642, -0.19740140818905436, -0.26613760880058035, 0.05808722912361314, 0.82151609385115953, 0.11280336266257514, 0.05648457235205667, 0.36120630002843257, 1.25426385582883126, 0.48661058451606820, -0.57030511336562195, 0.49151280464818098, 0.04428280690189127, -0.06591965049732286, -0.72616125524587938, -0.21690068430020618, -0.11507583016077781, -0.36583546331208316, -3.16186335125594642, -0.57030511336562195, 1.16270815038078945, -0.14837898963724327, 0.05917203395002889, 0.06055771542862331, 1.03490136002022926, 0.04065087561300068, 0.01475756130709823, 0.41472509341940023, -0.19740140818905436, 0.49151280464818103, -0.14837898963724327, 0.86693820682049716, 0.14089688752570340, 0.00577423310349649, 0.12800239664439328, -0.00876895259593769, -0.00584453679519805, 0.04581458758255480, -0.26613760880058035, 0.04428280690189126, 0.05917203395002889, 0.14089688752570340, 0.02901858439788401 }, 10, 10);}
0
private static Matrix reshape(double[] values, int rows, int columns)
{    Matrix m = new DenseMatrix(rows, columns);    int i = 0;    for (double v : values) {        m.set(i % rows, i / rows, v);        i++;    }    return m;}
0
public void testSingularValues()
{    Matrix A = lowRankMatrix();    SequentialBigSvd s = new SequentialBigSvd(A, 8);    SingularValueDecomposition svd = new SingularValueDecomposition(A);    Vector reference = new DenseVector(svd.getSingularValues()).viewPart(0, 8);    assertEquals(reference, s.getSingularValues());    assertEquals(A, s.getU().times(new DiagonalMatrix(s.getSingularValues())).times(s.getV().transpose()));}
0
public void testLeftVectors()
{    Matrix A = lowRankMatrix();    SequentialBigSvd s = new SequentialBigSvd(A, 8);    SingularValueDecomposition svd = new SingularValueDecomposition(A);            Matrix u1 = svd.getU().viewPart(0, 20, 0, 4).assign(Functions.ABS);    Matrix u2 = s.getU().viewPart(0, 20, 0, 4).assign(Functions.ABS);    assertEquals(0, u1.minus(u2).aggregate(Functions.PLUS, Functions.ABS), 1.0e-9);}
0
private static void assertEquals(Matrix u1, Matrix u2)
{    assertEquals(0, u1.minus(u2).aggregate(Functions.MAX, Functions.ABS), 1.0e-10);}
0
private static void assertEquals(Vector u1, Vector u2)
{    assertEquals(0, u1.minus(u2).aggregate(Functions.MAX, Functions.ABS), 1.0e-10);}
0
public void testRightVectors()
{    Matrix A = lowRankMatrix();    SequentialBigSvd s = new SequentialBigSvd(A, 6);    SingularValueDecomposition svd = new SingularValueDecomposition(A);    Matrix v1 = svd.getV().viewPart(0, 20, 0, 3).assign(Functions.ABS);    Matrix v2 = s.getV().viewPart(0, 20, 0, 3).assign(Functions.ABS);    assertEquals(v1, v2);}
0
private static Matrix lowRankMatrix()
{    Matrix u = new RandomTrinaryMatrix(1, 20, 4, false);    Matrix d = new DiagonalMatrix(new double[] { 5, 3, 1, 0.5 });    Matrix v = new RandomTrinaryMatrix(2, 23, 4, false);    return u.times(d).times(v.transpose());}
0
public void testEntropy() throws Exception
{    assertEquals(1.386294, LogLikelihood.entropy(1, 1), 0.0001);    assertEquals(0.0, LogLikelihood.entropy(1), 0.0);        try {                LogLikelihood.entropy(-1, -1);        fail();    } catch (IllegalArgumentException e) {    }}
0
public void testLogLikelihood() throws Exception
{        assertEquals(2.772589, LogLikelihood.logLikelihoodRatio(1, 0, 0, 1), 0.000001);    assertEquals(27.72589, LogLikelihood.logLikelihoodRatio(10, 0, 0, 10), 0.00001);    assertEquals(39.33052, LogLikelihood.logLikelihoodRatio(5, 1995, 0, 100000), 0.00001);    assertEquals(4730.737, LogLikelihood.logLikelihoodRatio(1000, 1995, 1000, 100000), 0.001);    assertEquals(5734.343, LogLikelihood.logLikelihoodRatio(1000, 1000, 1000, 100000), 0.001);    assertEquals(5714.932, LogLikelihood.logLikelihoodRatio(1000, 1000, 1000, 99000), 0.001);}
0
public void testRootLogLikelihood()
{        assertTrue(LogLikelihood.rootLogLikelihoodRatio(904, 21060, 1144, 283012) > 0.0);        assertTrue(LogLikelihood.rootLogLikelihoodRatio(36, 21928, 60280, 623876) < 0.0);    assertEquals(Math.sqrt(2.772589), LogLikelihood.rootLogLikelihoodRatio(1, 0, 0, 1), 0.000001);    assertEquals(-Math.sqrt(2.772589), LogLikelihood.rootLogLikelihoodRatio(0, 1, 1, 0), 0.000001);    assertEquals(Math.sqrt(27.72589), LogLikelihood.rootLogLikelihoodRatio(10, 0, 0, 10), 0.00001);    assertEquals(Math.sqrt(39.33052), LogLikelihood.rootLogLikelihoodRatio(5, 1995, 0, 100000), 0.00001);    assertEquals(-Math.sqrt(39.33052), LogLikelihood.rootLogLikelihoodRatio(0, 100000, 5, 1995), 0.00001);    assertEquals(Math.sqrt(4730.737), LogLikelihood.rootLogLikelihoodRatio(1000, 1995, 1000, 100000), 0.001);    assertEquals(-Math.sqrt(4730.737), LogLikelihood.rootLogLikelihoodRatio(1000, 100000, 1000, 1995), 0.001);    assertEquals(Math.sqrt(5734.343), LogLikelihood.rootLogLikelihoodRatio(1000, 1000, 1000, 100000), 0.001);    assertEquals(Math.sqrt(5714.932), LogLikelihood.rootLogLikelihoodRatio(1000, 1000, 1000, 99000), 0.001);}
0
public void testRootNegativeLLR()
{    assertTrue(LogLikelihood.rootLogLikelihoodRatio(6, 7567, 1924, 2426487) >= 0.0);}
0
public void testFrequencyComparison()
{    final Random rand = RandomUtils.getRandom();            Vector p1 = new DenseVector(25).assign(new DoubleFunction() {        @Override        public double apply(double arg1) {            return -Math.log1p(-rand.nextDouble());        }    });        Vector p2 = p1.like().assign(p1);        p1.viewPart(0, 5).assign(0);        p1.viewPart(5, 3).assign(Functions.mult(4));        p1.assign(Functions.div(p1.norm(1)));        p2.assign(Functions.div(p2.norm(1)));        Multiset<Integer> w1 = HashMultiset.create();    for (int i = 0; i < 100; i++) {        w1.add(sample(p1, rand));    }        Multiset<Integer> w2 = HashMultiset.create();    for (int i = 0; i < 1000; i++) {        w2.add(sample(p2, rand));    }        List<LogLikelihood.ScoredItem<Integer>> r = LogLikelihood.compareFrequencies(w1, w2, 8, 0);    assertTrue(r.size() <= 8);    assertFalse(r.isEmpty());    for (LogLikelihood.ScoredItem<Integer> item : r) {        assertTrue(item.getScore() >= 0);    }        assertEquals(7, (int) r.get(0).getItem());        double lastScore = r.get(0).getScore();    for (LogLikelihood.ScoredItem<Integer> item : r) {        assertTrue(item.getScore() <= lastScore);        lastScore = item.getScore();    }        r = LogLikelihood.compareFrequencies(w1, w2, 40, 1);        assertEquals(2, r.size());    assertEquals(7, (int) r.get(0).getItem());    assertEquals(6, (int) r.get(1).getItem());    r = LogLikelihood.compareFrequencies(w1, w2, 1000, -100);    Multiset<Integer> k = HashMultiset.create();    for (LogLikelihood.ScoredItem<Integer> item : r) {        k.add(item.getItem());    }    for (int i = 0; i < 25; i++) {        assertTrue("i = " + i, k.count(i) == 1 || w2.count(i) == 0);    }        assertEquals(w2.elementSet().size(), r.size());    assertEquals(7, (int) r.get(0).getItem());    assertEquals(6, (int) r.get(1).getItem());        assertTrue(r.get(r.size() - 1).getScore() < 0);        lastScore = r.get(0).getScore();    for (LogLikelihood.ScoredItem<Integer> item : r) {        assertTrue(item.getScore() <= lastScore);        lastScore = item.getScore();    }}
0
public double apply(double arg1)
{    return -Math.log1p(-rand.nextDouble());}
0
private static int sample(Vector p, Random rand)
{    double u = rand.nextDouble();        for (int i = 0; i < p.size(); i++) {        if (u <= p.get(i)) {            return i;        }        u -= p.get(i);    }    return p.size() - 1;}
0
public void testAverage()
{    double[] t = { 11.35718, 21.54637, 28.91061, 33.03586, 39.57767 };    double[] x = { 1.5992071, -1.3577032, -0.3405638, 0.7048632, 0.3020558 };    double[] m = { 1.5992071, -1.0168100, -0.4797436, 0.2836447, 0.2966159 };    OnlineExponentialAverage averager = new OnlineExponentialAverage(5);    for (int i = 0; i < t.length; i++) {        averager.add(t[i], x[i]);        assertEquals("Step " + i, m[i], averager.mean(), 1.0e-6);    }}
0
public void testRate()
{    Random gen = RandomUtils.getRandom();    Poisson p = new Poisson(5, gen);    double lastT = 0;    double[] k = new double[1000];    double[] t = new double[1000];    for (int i = 1; i < 1000; i++) {                double dt = gen.nextDouble() * 10 + 5;        t[i] = lastT + dt;                k[i] = p.nextInt(dt * 0.2);        lastT = t[i];    }    OnlineExponentialAverage averager = new OnlineExponentialAverage(2000);    for (int i = 1; i < 1000; i++) {        averager.add(t[i], k[i]);    }    assertEquals("Expected rate", 0.2, averager.meanRate(), 0.01);}
0
public void testStats()
{    /**     *     the reference limits here were derived using a numerical simulation where I took     *     10,000 samples from the distribution in question and computed the stats from that     *     sample to get min, 25%-ile, median and so on. I did this 1000 times to get 5% and     *     95% confidence limits for those values.     */        System.out.printf("normal\n");    check(normal(10000));        System.out.printf("exp\n");    check(exp(10000));        System.out.printf("gamma\n");    check(gamma(10000, 0.1));}
0
private static void check(double[] samples)
{    OnlineSummarizer s = new OnlineSummarizer();    double mean = 0;    double sd = 0;    int n = 1;    for (double x : samples) {        s.add(x);        double old = mean;        mean += (x - mean) / n;        sd += (x - old) * (x - mean);        n++;    }    sd = Math.sqrt(sd / samples.length);    Arrays.sort(samples);                        assertEquals("mean", s.getMean(), mean, 0);    assertEquals("sd", s.getSD(), sd, 1e-8);}
0
private static double[] normal(int n)
{    double[] r = new double[n];    Random gen = RandomUtils.getRandom(1L);    for (int i = 0; i < n; i++) {        r[i] = gen.nextGaussian();    }    return r;}
0
private static double[] exp(int n)
{    double[] r = new double[n];    Random gen = RandomUtils.getRandom(1L);    for (int i = 0; i < n; i++) {        r[i] = -Math.log1p(-gen.nextDouble());    }    return r;}
0
private static double[] gamma(int n, double shape)
{    double[] r = new double[n];    Random gen = RandomUtils.getRandom();    AbstractContinousDistribution gamma = new Gamma(shape, shape, gen);    for (int i = 0; i < n; i++) {        r[i] = gamma.nextDouble();    }    return r;}
0
public Matrix matrixFactory(double[][] values)
{    return new DenseMatrix(values);}
0
public void testGetValues()
{    DenseMatrix m = new DenseMatrix(10, 10);    for (int i = 0; i < 10; i++) {        for (int j = 0; j < 10; j++) {            m.set(i, j, 10 * i + j);        }    }    double[][] values = m.getBackingStructure();    Assert.assertEquals(values.length, 10);    Assert.assertEquals(values[0].length, 10);    Assert.assertEquals(values[9][9], 99.0, 0.0);}
0
 Vector generateTestVector(int cardinality)
{    return new DenseVector(cardinality);}
0
public void testSize()
{    assertEquals("size", 3, getTestVector().getNumNonZeroElements());}
0
public DenseVector vectorToTest(int size)
{    DenseVector r = new DenseVector(size);    r.assign(Functions.random());    return r;}
0
public void testToString()
{    super.testToString();}
0
public void setUp() throws Exception
{    super.setUp();    int[] offset = { 1, 1 };    int[] card = { 3, 2 };    test = new MatrixView(new DenseMatrix(values), offset, card);}
0
public void testCardinality()
{    assertEquals("row cardinality", values.length - 2, test.rowSize());    assertEquals("col cardinality", values[0].length - 1, test.columnSize());}
0
public void testCopy()
{    Matrix copy = test.clone();    assertTrue("wrong class", copy instanceof MatrixView);    for (int row = 0; row < test.rowSize(); row++) {        for (int col = 0; col < test.columnSize(); col++) {            assertEquals("value[" + row + "][" + col + ']', test.getQuick(row, col), copy.getQuick(row, col), EPSILON);        }    }}
0
public void testGetQuick()
{    for (int row = 0; row < test.rowSize(); row++) {        for (int col = 0; col < test.columnSize(); col++) {            assertEquals("value[" + row + "][" + col + ']', values[row + 1][col + 1], test.getQuick(row, col), EPSILON);        }    }}
0
public void testLike()
{    Matrix like = test.like();    assertTrue("type", like instanceof DenseMatrix);    assertEquals("rows", test.rowSize(), like.rowSize());    assertEquals("columns", test.columnSize(), like.columnSize());}
0
public void testLikeIntInt()
{    Matrix like = test.like(4, 4);    assertTrue("type", like instanceof DenseMatrix);    assertEquals("rows", 4, like.rowSize());    assertEquals("columns", 4, like.columnSize());}
0
public void testSetQuick()
{    for (int row = 0; row < test.rowSize(); row++) {        for (int col = 0; col < test.columnSize(); col++) {            test.setQuick(row, col, 1.23);            assertEquals("value[" + row + "][" + col + ']', 1.23, test.getQuick(row, col), EPSILON);        }    }}
0
public void testSize()
{    assertEquals("row size", values.length - 2, test.rowSize());    assertEquals("col size", values[0].length - 1, test.columnSize());}
0
public void testViewPart()
{    int[] offset = { 1, 1 };    int[] size = { 2, 1 };    Matrix view = test.viewPart(offset, size);    for (int row = 0; row < view.rowSize(); row++) {        for (int col = 0; col < view.columnSize(); col++) {            assertEquals("value[" + row + "][" + col + ']', values[row + 2][col + 2], view.getQuick(row, col), EPSILON);        }    }}
0
public void testViewPartCardinality()
{    int[] offset = { 1, 1 };    int[] size = { 3, 3 };    test.viewPart(offset, size);}
0
public void testViewPartIndexOver()
{    int[] offset = { 1, 1 };    int[] size = { 2, 2 };    test.viewPart(offset, size);}
0
public void testViewPartIndexUnder()
{    int[] offset = { -1, -1 };    int[] size = { 2, 2 };    test.viewPart(offset, size);}
0
public void testAssignDouble()
{    test.assign(4.53);    for (int row = 0; row < test.rowSize(); row++) {        for (int col = 0; col < test.columnSize(); col++) {            assertEquals("value[" + row + "][" + col + ']', 4.53, test.getQuick(row, col), EPSILON);        }    }}
0
public void testAssignDoubleArrayArray()
{    test.assign(new double[3][2]);    for (int row = 0; row < test.rowSize(); row++) {        for (int col = 0; col < test.columnSize(); col++) {            assertEquals("value[" + row + "][" + col + ']', 0.0, test.getQuick(row, col), EPSILON);        }    }}
0
public void testAssignDoubleArrayArrayCardinality()
{    test.assign(new double[test.rowSize() + 1][test.columnSize()]);}
0
public void testAssignMatrixBinaryFunction()
{    test.assign(test, Functions.PLUS);    for (int row = 0; row < test.rowSize(); row++) {        for (int col = 0; col < test.columnSize(); col++) {            assertEquals("value[" + row + "][" + col + ']', 2 * values[row + 1][col + 1], test.getQuick(row, col), EPSILON);        }    }}
0
public void testAssignMatrixBinaryFunctionCardinality()
{    test.assign(test.transpose(), Functions.PLUS);}
0
public void testAssignMatrix()
{    Matrix value = test.like();    value.assign(test);    for (int row = 0; row < test.rowSize(); row++) {        for (int col = 0; col < test.columnSize(); col++) {            assertEquals("value[" + row + "][" + col + ']', test.getQuick(row, col), value.getQuick(row, col), EPSILON);        }    }}
0
public void testAssignMatrixCardinality()
{    test.assign(test.transpose());}
0
public void testAssignUnaryFunction()
{    test.assign(Functions.NEGATE);    for (int row = 0; row < test.rowSize(); row++) {        for (int col = 0; col < test.columnSize(); col++) {            assertEquals("value[" + row + "][" + col + ']', -values[row + 1][col + 1], test.getQuick(row, col), EPSILON);        }    }}
0
public void testDivide()
{    Matrix value = test.divide(4.53);    for (int row = 0; row < test.rowSize(); row++) {        for (int col = 0; col < test.columnSize(); col++) {            assertEquals("value[" + row + "][" + col + ']', values[row + 1][col + 1] / 4.53, value.getQuick(row, col), EPSILON);        }    }}
0
public void testGet()
{    for (int row = 0; row < test.rowSize(); row++) {        for (int col = 0; col < test.columnSize(); col++) {            assertEquals("value[" + row + "][" + col + ']', values[row + 1][col + 1], test.get(row, col), EPSILON);        }    }}
0
public void testGetIndexUnder()
{    for (int row = -1; row < test.rowSize(); row++) {        for (int col = 0; col < test.columnSize(); col++) {            test.get(row, col);        }    }}
0
public void testGetIndexOver()
{    for (int row = 0; row < test.rowSize() + 1; row++) {        for (int col = 0; col < test.columnSize(); col++) {            test.get(row, col);        }    }}
0
public void testMinus()
{    Matrix value = test.minus(test);    for (int row = 0; row < test.rowSize(); row++) {        for (int col = 0; col < test.columnSize(); col++) {            assertEquals("value[" + row + "][" + col + ']', 0.0, value.getQuick(row, col), EPSILON);        }    }}
0
public void testMinusCardinality()
{    test.minus(test.transpose());}
0
public void testPlusDouble()
{    Matrix value = test.plus(4.53);    for (int row = 0; row < test.rowSize(); row++) {        for (int col = 0; col < test.columnSize(); col++) {            assertEquals("value[" + row + "][" + col + ']', values[row + 1][col + 1] + 4.53, value.getQuick(row, col), EPSILON);        }    }}
0
public void testPlusMatrix()
{    Matrix value = test.plus(test);    for (int row = 0; row < test.rowSize(); row++) {        for (int col = 0; col < test.columnSize(); col++) {            assertEquals("value[" + row + "][" + col + ']', values[row + 1][col + 1] * 2, value.getQuick(row, col), EPSILON);        }    }}
0
public void testPlusMatrixCardinality()
{    test.plus(test.transpose());}
0
public void testSetUnder()
{    for (int row = -1; row < test.rowSize(); row++) {        for (int col = 0; col < test.columnSize(); col++) {            test.set(row, col, 1.23);        }    }}
0
public void testSetOver()
{    for (int row = 0; row < test.rowSize() + 1; row++) {        for (int col = 0; col < test.columnSize(); col++) {            test.set(row, col, 1.23);        }    }}
0
public void testTimesDouble()
{    Matrix value = test.times(4.53);    for (int row = 0; row < test.rowSize(); row++) {        for (int col = 0; col < test.columnSize(); col++) {            assertEquals("value[" + row + "][" + col + ']', values[row + 1][col + 1] * 4.53, value.getQuick(row, col), EPSILON);        }    }}
0
public void testTimesMatrix()
{    Matrix transpose = test.transpose();    Matrix value = test.times(transpose);    assertEquals("rows", test.rowSize(), value.rowSize());    assertEquals("cols", test.rowSize(), value.columnSize());}
0
public void testTimesMatrixCardinality()
{    Matrix other = test.like(5, 8);    test.times(other);}
0
public void testTranspose()
{    Matrix transpose = test.transpose();    assertEquals("rows", test.columnSize(), transpose.rowSize());    assertEquals("cols", test.rowSize(), transpose.columnSize());    for (int row = 0; row < test.rowSize(); row++) {        for (int col = 0; col < test.columnSize(); col++) {            assertEquals("value[" + row + "][" + col + ']', test.getQuick(row, col), transpose.getQuick(col, row), EPSILON);        }    }}
0
public void testZSum()
{    double sum = test.zSum();    assertEquals("zsum", 29.7, sum, EPSILON);}
0
public void testAssignRow()
{    double[] data = { 2.1, 3.2 };    test.assignRow(1, new DenseVector(data));    assertEquals("test[1][0]", 2.1, test.getQuick(1, 0), EPSILON);    assertEquals("test[1][1]", 3.2, test.getQuick(1, 1), EPSILON);}
0
public void testAssignRowCardinality()
{    double[] data = { 2.1, 3.2, 4.3 };    test.assignRow(1, new DenseVector(data));}
0
public void testAssignColumn()
{    double[] data = { 2.1, 3.2, 4.3 };    test.assignColumn(1, new DenseVector(data));    assertEquals("test[0][1]", 2.1, test.getQuick(0, 1), EPSILON);    assertEquals("test[1][1]", 3.2, test.getQuick(1, 1), EPSILON);    assertEquals("test[2][1]", 4.3, test.getQuick(2, 1), EPSILON);}
0
public void testAssignColumnCardinality()
{    double[] data = { 2.1, 3.2 };    test.assignColumn(1, new DenseVector(data));}
0
public void testViewRow()
{    Vector row = test.viewRow(1);    assertEquals("row size", 2, row.getNumNondefaultElements());}
0
public void testViewRowIndexUnder()
{    test.viewRow(-1);}
0
public void testViewRowIndexOver()
{    test.viewRow(5);}
0
public void testViewColumn()
{    Vector column = test.viewColumn(1);    assertEquals("row size", 3, column.getNumNondefaultElements());    int i = 0;    for (double x : new double[] { 3.3, 5.5, 7.7 }) {        assertEquals(x, column.get(i++), 0);    }}
0
public void testViewColumnIndexUnder()
{    test.viewColumn(-1);}
0
public void testViewColumnIndexOver()
{    test.viewColumn(5);}
0
public void testLabelBindings()
{    assertNull("row bindings", test.getRowLabelBindings());    assertNull("col bindings", test.getColumnLabelBindings());    Map<String, Integer> rowBindings = Maps.newHashMap();    rowBindings.put("Fee", 0);    rowBindings.put("Fie", 1);    test.setRowLabelBindings(rowBindings);    assertEquals("row", rowBindings, test.getRowLabelBindings());    Map<String, Integer> colBindings = Maps.newHashMap();    colBindings.put("Foo", 0);    colBindings.put("Bar", 1);    test.setColumnLabelBindings(colBindings);    assertEquals("row", rowBindings, test.getRowLabelBindings());    assertEquals("Fee", test.get(0, 1), test.get("Fee", "Bar"), EPSILON);    double[] newrow = { 9, 8 };    test.set("Fie", newrow);    assertEquals("FeeBar", test.get(0, 1), test.get("Fee", "Bar"), EPSILON);}
0
public void testSettingLabelBindings()
{    assertNull("row bindings", test.getRowLabelBindings());    assertNull("col bindings", test.getColumnLabelBindings());    test.set("Fee", "Foo", 1, 1, 9);    assertNotNull("row", test.getRowLabelBindings());    assertNotNull("row", test.getRowLabelBindings());    assertEquals("Fee", 1, test.getRowLabelBindings().get("Fee").intValue());    assertEquals("Foo", 1, test.getColumnLabelBindings().get("Foo").intValue());    assertEquals("FeeFoo", test.get(1, 1), test.get("Fee", "Foo"), EPSILON);    test.get("Fie", "Foe");}
0
public void testLabelBindingSerialization()
{    assertNull("row bindings", test.getRowLabelBindings());    assertNull("col bindings", test.getColumnLabelBindings());    Map<String, Integer> rowBindings = Maps.newHashMap();    rowBindings.put("Fee", 0);    rowBindings.put("Fie", 1);    rowBindings.put("Foe", 2);    test.setRowLabelBindings(rowBindings);    assertEquals("row", rowBindings, test.getRowLabelBindings());    Map<String, Integer> colBindings = Maps.newHashMap();    colBindings.put("Foo", 0);    colBindings.put("Bar", 1);    colBindings.put("Baz", 2);    test.setColumnLabelBindings(colBindings);    assertEquals("col", colBindings, test.getColumnLabelBindings());}
0
public void testGetSet()
{    OrderedIntDoubleMapping mapping = new OrderedIntDoubleMapping(1);    assertEquals(0, mapping.getNumMappings());    assertEquals(0.0, mapping.get(0), EPSILON);    assertEquals(0.0, mapping.get(1), EPSILON);    mapping.set(0, 1.1);    assertEquals(1, mapping.getNumMappings());    assertEquals(1.1, mapping.get(0), EPSILON);    assertEquals(0.0, mapping.get(1), EPSILON);    mapping.set(5, 6.6);    assertEquals(2, mapping.getNumMappings());    assertEquals(1.1, mapping.get(0), EPSILON);    assertEquals(0.0, mapping.get(1), EPSILON);    assertEquals(6.6, mapping.get(5), EPSILON);    assertEquals(0.0, mapping.get(6), EPSILON);    mapping.set(0, 0.0);    assertEquals(1, mapping.getNumMappings());    assertEquals(0.0, mapping.get(0), EPSILON);    assertEquals(0.0, mapping.get(1), EPSILON);    assertEquals(6.6, mapping.get(5), EPSILON);    mapping.set(5, 0.0);    assertEquals(0, mapping.getNumMappings());    assertEquals(0.0, mapping.get(0), EPSILON);    assertEquals(0.0, mapping.get(1), EPSILON);    assertEquals(0.0, mapping.get(5), EPSILON);}
0
public void testClone() throws Exception
{    OrderedIntDoubleMapping mapping = new OrderedIntDoubleMapping(1);    mapping.set(0, 1.1);    mapping.set(5, 6.6);    OrderedIntDoubleMapping clone = mapping.clone();    assertEquals(2, clone.getNumMappings());    assertEquals(1.1, clone.get(0), EPSILON);    assertEquals(0.0, clone.get(1), EPSILON);    assertEquals(6.6, clone.get(5), EPSILON);    assertEquals(0.0, clone.get(6), EPSILON);}
0
public void testAddDefaultElements()
{    OrderedIntDoubleMapping mapping = new OrderedIntDoubleMapping(false);    mapping.set(1, 1.1);    assertEquals(1, mapping.getNumMappings());    mapping.set(2, 0);    assertEquals(2, mapping.getNumMappings());    mapping.set(0, 0);    assertEquals(3, mapping.getNumMappings());}
0
public void testMerge()
{    OrderedIntDoubleMapping mappingOne = new OrderedIntDoubleMapping(false);    mappingOne.set(0, 0);    mappingOne.set(2, 2);    mappingOne.set(4, 4);    mappingOne.set(10, 10);    OrderedIntDoubleMapping mappingTwo = new OrderedIntDoubleMapping();    mappingTwo.set(1, 1);    mappingTwo.set(3, 3);    mappingTwo.set(5, 5);    mappingTwo.set(10, 20);    mappingOne.merge(mappingTwo);    assertEquals(7, mappingOne.getNumMappings());    for (int i = 0; i < 6; ++i) {        assertEquals(i, mappingOne.get(i), i);    }    assertEquals(20, mappingOne.get(10), 0);}
0
 Vector generateTestVector(int cardinality)
{    return new RandomAccessSparseVector(cardinality);}
0
public RandomAccessSparseVector vectorToTest(int size)
{    RandomAccessSparseVector r = new RandomAccessSparseVector(size);    Random gen = RandomUtils.getRandom();    for (int i = 0; i < 3; i++) {        r.set(gen.nextInt(r.size()), gen.nextGaussian());    }    return r;}
0
public void testToString()
{    Vector w;    w = generateTestVector(20);    w.set(0, 1.1);    w.set(13, 100500.);    w.set(19, 3.141592);    for (String token : Splitter.on(',').split(w.toString().substring(1, w.toString().length() - 1))) {        String[] tokens = token.split(":");        assertEquals(Double.parseDouble(tokens[1]), w.get(Integer.parseInt(tokens[0])), 0.0);    }    w = generateTestVector(12);    w.set(10, 0.1);    assertEquals("{10:0.1}", w.toString());    w = generateTestVector(12);    assertEquals("{}", w.toString());}
0
 Vector generateTestVector(int cardinality)
{    return new SequentialAccessSparseVector(cardinality);}
0
public void testDotSuperBig()
{    Vector w = new SequentialAccessSparseVector(Integer.MAX_VALUE, 12);    w.set(1, 0.4);    w.set(2, 0.4);    w.set(3, -0.666666667);    Vector v = new SequentialAccessSparseVector(Integer.MAX_VALUE, 12);    v.set(3, 1);    assertEquals("super-big", -0.666666667, v.dot(w), EPSILON);}
0
public SequentialAccessSparseVector vectorToTest(int size)
{    SequentialAccessSparseVector r = new SequentialAccessSparseVector(size);    Random gen = RandomUtils.getRandom();    for (int i = 0; i < 3; i++) {        r.set(gen.nextInt(r.size()), gen.nextGaussian());    }    return r;}
0
public void testToString()
{    super.testToString();}
0
public void testMoreRows()
{    double[] singularValues = { 123.456, 2.3, 1.001, 0.999 };    int rows = singularValues.length + 2;    int columns = singularValues.length;    Random r = RandomUtils.getRandom();    SingularValueDecomposition svd = new SingularValueDecomposition(createTestMatrix(r, rows, columns, singularValues));    double[] computedSV = svd.getSingularValues();    assertEquals(singularValues.length, computedSV.length);    for (int i = 0; i < singularValues.length; ++i) {        assertEquals(singularValues[i], computedSV[i], 1.0e-10);    }}
0
public void testMoreColumns()
{    double[] singularValues = { 123.456, 2.3, 1.001, 0.999 };    int rows = singularValues.length;    int columns = singularValues.length + 2;    Random r = RandomUtils.getRandom();    SingularValueDecomposition svd = new SingularValueDecomposition(createTestMatrix(r, rows, columns, singularValues));    double[] computedSV = svd.getSingularValues();    assertEquals(singularValues.length, computedSV.length);    for (int i = 0; i < singularValues.length; ++i) {        assertEquals(singularValues[i], computedSV[i], 1.0e-10);    }}
0
public void testDimensions()
{    Matrix matrix = new DenseMatrix(testSquare);    int m = matrix.numRows();    int n = matrix.numCols();    SingularValueDecomposition svd = new SingularValueDecomposition(matrix);    assertEquals(m, svd.getU().numRows());    assertEquals(m, svd.getU().numCols());    assertEquals(m, svd.getS().numCols());    assertEquals(n, svd.getS().numCols());    assertEquals(n, svd.getV().numRows());    assertEquals(n, svd.getV().numCols());}
0
public void testHadamard()
{    Matrix matrix = new DenseMatrix(new double[][] { { 15.0 / 2.0, 5.0 / 2.0, 9.0 / 2.0, 3.0 / 2.0 }, { 5.0 / 2.0, 15.0 / 2.0, 3.0 / 2.0, 9.0 / 2.0 }, { 9.0 / 2.0, 3.0 / 2.0, 15.0 / 2.0, 5.0 / 2.0 }, { 3.0 / 2.0, 9.0 / 2.0, 5.0 / 2.0, 15.0 / 2.0 } });    SingularValueDecomposition svd = new SingularValueDecomposition(matrix);    assertEquals(16.0, svd.getSingularValues()[0], 1.0e-14);    assertEquals(8.0, svd.getSingularValues()[1], 1.0e-14);    assertEquals(4.0, svd.getSingularValues()[2], 1.0e-14);    assertEquals(2.0, svd.getSingularValues()[3], 1.0e-14);    Matrix fullCovariance = new DenseMatrix(new double[][] { { 85.0 / 1024, -51.0 / 1024, -75.0 / 1024, 45.0 / 1024 }, { -51.0 / 1024, 85.0 / 1024, 45.0 / 1024, -75.0 / 1024 }, { -75.0 / 1024, 45.0 / 1024, 85.0 / 1024, -51.0 / 1024 }, { 45.0 / 1024, -75.0 / 1024, -51.0 / 1024, 85.0 / 1024 } });    assertEquals(0.0, Algebra.getNorm(fullCovariance.minus(svd.getCovariance(0.0))), 1.0e-14);    Matrix halfCovariance = new DenseMatrix(new double[][] { { 5.0 / 1024, -3.0 / 1024, 5.0 / 1024, -3.0 / 1024 }, { -3.0 / 1024, 5.0 / 1024, -3.0 / 1024, 5.0 / 1024 }, { 5.0 / 1024, -3.0 / 1024, 5.0 / 1024, -3.0 / 1024 }, { -3.0 / 1024, 5.0 / 1024, -3.0 / 1024, 5.0 / 1024 } });    assertEquals(0.0, Algebra.getNorm(halfCovariance.minus(svd.getCovariance(6.0))), 1.0e-14);}
0
public void testAEqualUSVt()
{    checkAEqualUSVt(new DenseMatrix(testSquare));    checkAEqualUSVt(new DenseMatrix(testNonSquare));    checkAEqualUSVt(new DenseMatrix(testNonSquare).transpose());}
0
public static void checkAEqualUSVt(Matrix matrix)
{    SingularValueDecomposition svd = new SingularValueDecomposition(matrix);    Matrix u = svd.getU();    Matrix s = svd.getS();    Matrix v = svd.getV();        if (s.numRows() < matrix.numRows()) {        Matrix sp = new DenseMatrix(s.numRows() + 1, s.numCols());        Matrix up = new DenseMatrix(u.numRows(), u.numCols() + 1);        for (int i = 0; i < u.numRows(); i++) {            for (int j = 0; j < u.numCols(); j++) {                up.set(i, j, u.get(i, j));            }        }        for (int i = 0; i < s.numRows(); i++) {            for (int j = 0; j < s.numCols(); j++) {                sp.set(i, j, s.get(i, j));            }        }        u = up;        s = sp;    }    double norm = Algebra.getNorm(u.times(s).times(v.transpose()).minus(matrix));    assertEquals(0, norm, NORM_TOLERANCE);}
0
public void testUOrthogonal()
{    checkOrthogonal(new SingularValueDecomposition(new DenseMatrix(testSquare)).getU());    checkOrthogonal(new SingularValueDecomposition(new DenseMatrix(testNonSquare)).getU());    checkOrthogonal(new SingularValueDecomposition(new DenseMatrix(testNonSquare).transpose()).getU());}
0
public void testVOrthogonal()
{    checkOrthogonal(new SingularValueDecomposition(new DenseMatrix(testSquare)).getV());    checkOrthogonal(new SingularValueDecomposition(new DenseMatrix(testNonSquare)).getV());    checkOrthogonal(new SingularValueDecomposition(new DenseMatrix(testNonSquare).transpose()).getV());}
0
public static void checkOrthogonal(Matrix m)
{    Matrix mTm = m.transpose().times(m);    Matrix id = new DenseMatrix(mTm.numRows(), mTm.numRows());    for (int i = 0; i < mTm.numRows(); i++) {        id.set(i, i, 1);    }    assertEquals(0, Algebra.getNorm(mTm.minus(id)), NORM_TOLERANCE);}
0
public void testMatricesValues1()
{    SingularValueDecomposition svd = new SingularValueDecomposition(new DenseMatrix(testSquare));    Matrix uRef = new DenseMatrix(new double[][] { { 3.0 / 5.0, 4.0 / 5.0 }, { 4.0 / 5.0, -3.0 / 5.0 } });    Matrix sRef = new DenseMatrix(new double[][] { { 3.0, 0.0 }, { 0.0, 1.0 } });    Matrix vRef = new DenseMatrix(new double[][] { { 4.0 / 5.0, -3.0 / 5.0 }, { 3.0 / 5.0, 4.0 / 5.0 } });        Matrix u = svd.getU();    assertEquals(0, Algebra.getNorm(u.minus(uRef)), NORM_TOLERANCE);    Matrix s = svd.getS();    assertEquals(0, Algebra.getNorm(s.minus(sRef)), NORM_TOLERANCE);    Matrix v = svd.getV();    assertEquals(0, Algebra.getNorm(v.minus(vRef)), NORM_TOLERANCE);}
0
public void testConditionNumber()
{    SingularValueDecomposition svd = new SingularValueDecomposition(new DenseMatrix(testSquare));        assertEquals(3.0, svd.cond(), 1.5e-15);}
0
public void testSvdHang() throws IOException, InterruptedException, ExecutionException, TimeoutException
{    System.out.printf("starting hanging-svd\n");    final Matrix m = readTsv("hanging-svd.tsv");    SingularValueDecomposition svd = new SingularValueDecomposition(m);    assertEquals(0, m.minus(svd.getU().times(svd.getS()).times(svd.getV().transpose())).aggregate(Functions.PLUS, Functions.ABS), 1e-10);    System.out.printf("No hang\n");}
0
 Matrix readTsv(String name) throws IOException
{    Splitter onTab = Splitter.on("\t");    List<String> lines = Resources.readLines((Resources.getResource(name)), Charsets.UTF_8);    int rows = lines.size();    int columns = Iterables.size(onTab.split(lines.get(0)));    Matrix r = new DenseMatrix(rows, columns);    int row = 0;    for (String line : lines) {        Iterable<String> values = onTab.split(line);        int column = 0;        for (String value : values) {            r.set(row, column, Double.parseDouble(value));            column++;        }        row++;    }    return r;}
0
private static Matrix createTestMatrix(Random r, int rows, int columns, double[] singularValues)
{    Matrix u = createOrthogonalMatrix(r, rows);    Matrix d = createDiagonalMatrix(singularValues, rows, columns);    Matrix v = createOrthogonalMatrix(r, columns);    return u.times(d).times(v);}
0
public static Matrix createOrthogonalMatrix(Random r, int size)
{    double[][] data = new double[size][size];    for (int i = 0; i < size; ++i) {        double[] dataI = data[i];        double norm2;        do {                        for (int j = 0; j < size; ++j) {                dataI[j] = 2 * r.nextDouble() - 1;            }                        for (int k = 0; k < i; ++k) {                double[] dataK = data[k];                double dotProduct = 0;                for (int j = 0; j < size; ++j) {                    dotProduct += dataI[j] * dataK[j];                }                for (int j = 0; j < size; ++j) {                    dataI[j] -= dotProduct * dataK[j];                }            }                        norm2 = 0;            for (double dataIJ : dataI) {                norm2 += dataIJ * dataIJ;            }            double inv = 1.0 / Math.sqrt(norm2);            for (int j = 0; j < size; ++j) {                dataI[j] *= inv;            }        } while (norm2 * size < 0.01);    }    return new DenseMatrix(data);}
0
public static Matrix createDiagonalMatrix(double[] diagonal, int rows, int columns)
{    double[][] dData = new double[rows][columns];    for (int i = 0; i < Math.min(rows, columns); ++i) {        dData[i][i] = diagonal[i];    }    return new DenseMatrix(dData);}
0
public Matrix matrixFactory(double[][] values)
{    Matrix matrix = new SparseColumnMatrix(values.length, values[0].length);    for (int row = 0; row < matrix.rowSize(); row++) {        for (int col = 0; col < matrix.columnSize(); col++) {            matrix.setQuick(row, col, values[row][col]);        }    }    return matrix;}
0
public void testIterate()
{}
0
public Matrix matrixFactory(double[][] values)
{    Matrix matrix = new SparseMatrix(values.length, values[0].length);    for (int row = 0; row < matrix.rowSize(); row++) {        for (int col = 0; col < matrix.columnSize(); col++) {            matrix.setQuick(row, col, values[row][col]);        }    }    return matrix;}
0
public void add()
{    Matrix a = new SparseMatrix(3, 3);    a.set(0, 0, 1);    a.set(0, 2, 3);    a.set(2, 0, 1);    a.set(2, 1, 2);    Matrix b = new SparseMatrix(3, 3);    b.set(0, 0, 3);    b.set(0, 2, 1);    b.set(1, 1, 5);    b.set(2, 2, 2);    a.assign(b, Functions.PLUS);    assertEquals(4, a.getQuick(0, 0), 0.0);    assertEquals(0, a.getQuick(0, 1), 0.0);    assertEquals(4, a.getQuick(0, 2), 0.0);    assertEquals(0, a.getQuick(1, 0), 0.0);    assertEquals(5, a.getQuick(1, 1), 0.0);    assertEquals(0, a.getQuick(1, 2), 0.0);    assertEquals(1, a.getQuick(2, 0), 0.0);    assertEquals(2, a.getQuick(2, 1), 0.0);    assertEquals(2, a.getQuick(2, 2), 0.0);}
0
public void testSparseCopy()
{    SparseMatrix matrix = createSparseMatrixWithEmptyRow();    Matrix copy = matrix.clone();    assertSame("wrong class", copy.getClass(), matrix.getClass());    SparseMatrix castedCopy = (SparseMatrix) copy;    Iterator<MatrixSlice> originalSlices = matrix.iterator();    Iterator<MatrixSlice> copySlices = castedCopy.iterator();    while (originalSlices.hasNext() && copySlices.hasNext()) {        MatrixSlice originalSlice = originalSlices.next();        MatrixSlice copySlice = copySlices.next();        assertEquals("Wrong row indices.", originalSlice.index(), copySlice.index());        assertEquals("Slices are not equal.", originalSlice, copySlice);    }    assertSame("Number of rows of original and copy are not equal.", originalSlices.hasNext(), copySlices.hasNext());}
0
private SparseMatrix createSparseMatrixWithEmptyRow()
{    SparseMatrix result = new SparseMatrix(3, 3);    result.setQuick(0, 0, 1);    result.setQuick(1, 1, 1);    result.setQuick(1, 2, 1);    return result;}
0
public Matrix matrixFactory(double[][] values)
{    Matrix matrix = new SparseRowMatrix(values.length, values[0].length);    for (int row = 0; row < matrix.rowSize(); row++) {        for (int col = 0; col < matrix.columnSize(); col++) {            matrix.setQuick(row, col, values[row][col]);        }    }    return matrix;}
0
public void testTimesSparseEfficiency()
{    Random raw = RandomUtils.getRandom();    Gamma gen = new Gamma(0.1, 0.1, raw);        Matrix x = new SparseRowMatrix(1000, 2000, false);    for (int i = 0; i < 1000; i++) {        int[] values = new int[1000];        for (int k = 0; k < 1000; k++) {            int j = (int) Math.min(1000, gen.nextDouble());            values[j]++;        }        for (int j = 0; j < 1000; j++) {            if (values[j] > 0) {                x.set(i, j, values[j]);            }        }    }    Matrix y = new SparseRowMatrix(2000, 1000, false);    for (int i = 0; i < 2000; i++) {        int[] values = new int[1000];        for (int k = 0; k < 1000; k++) {            int j = (int) Math.min(1000, gen.nextDouble());            values[j]++;        }        for (int j = 0; j < 1000; j++) {            if (values[j] > 0) {                y.set(i, j, values[j]);            }        }    }    long t0 = System.nanoTime();    Matrix z = x.times(y);    double elapsedTime = (System.nanoTime() - t0) * 1e-6;    System.out.printf("done in %.1f ms\n", elapsedTime);    for (int k = 0; k < 1000; k++) {        int i = (int) (-10 * Math.log(raw.nextDouble()));        int j = (int) (-10 * Math.log(raw.nextDouble()));        Assert.assertEquals(x.viewRow(i).dot(y.viewColumn(j)), z.get(i, j), 1e-12);    }}
0
public void testTimesDenseEfficiency()
{    Random raw = RandomUtils.getRandom();    Gamma gen = new Gamma(0.1, 0.1, raw);        Matrix x = new SparseRowMatrix(1000, 2000, false);    for (int i = 0; i < 1000; i++) {        int[] values = new int[1000];        for (int k = 0; k < 1000; k++) {            int j = (int) Math.min(1000, gen.nextDouble());            values[j]++;        }        for (int j = 0; j < 1000; j++) {            if (values[j] > 0) {                x.set(i, j, values[j]);            }        }    }    Matrix y = new DenseMatrix(2000, 20);    for (int i = 0; i < 2000; i++) {        for (int j = 0; j < 20; j++) {            y.set(i, j, raw.nextDouble());        }    }    long t0 = System.nanoTime();    Matrix z = x.times(y);    double elapsedTime = (System.nanoTime() - t0) * 1e-6;    System.out.printf("done in %.1f ms\n", elapsedTime);    for (int i = 0; i < 1000; i++) {        for (int j = 0; j < 20; j++) {            Assert.assertEquals(x.viewRow(i).dot(y.viewColumn(j)), z.get(i, j), 1e-12);        }    }}
0
public void testTimesOtherSparseEfficiency()
{    Random raw = RandomUtils.getRandom();    Gamma gen = new Gamma(0.1, 0.1, raw);        Matrix x = new SparseRowMatrix(1000, 2000, false);    for (int i = 0; i < 1000; i++) {        int[] values = new int[1000];        for (int k = 0; k < 1000; k++) {            int j = (int) Math.min(1000, gen.nextDouble());            values[j]++;        }        for (int j = 0; j < 1000; j++) {            if (values[j] > 0) {                x.set(i, j, values[j]);            }        }    }    Vector d = new DenseVector(2000).assign(Functions.random());    Matrix y = new DiagonalMatrix(d);    long t0 = System.nanoTime();    Matrix z = x.times(y);    double elapsedTime = (System.nanoTime() - t0) * 1e-6;    System.out.printf("done in %.1f ms\n", elapsedTime);    for (MatrixSlice row : z) {        for (Vector.Element element : row.nonZeroes()) {            assertEquals(x.get(row.index(), element.index()) * d.get(element.index()), element.get(), 1e-12);        }    }}
0
public void testTimesCorrect()
{    Random raw = RandomUtils.getRandom();        Matrix x = new SparseRowMatrix(100, 2000, false).assign(Functions.random());    Matrix y = new SparseRowMatrix(2000, 100, false).assign(Functions.random());    Matrix xd = new DenseMatrix(100, 2000).assign(x);    Matrix yd = new DenseMatrix(2000, 100).assign(y);    assertEquals(0, xd.times(yd).minus(x.times(y)).aggregate(Functions.PLUS, Functions.ABS), 1e-15);    assertEquals(0, x.times(yd).minus(x.times(y)).aggregate(Functions.PLUS, Functions.ABS), 1e-15);    assertEquals(0, xd.times(y).minus(x.times(y)).aggregate(Functions.PLUS, Functions.ABS), 1e-15);}
0
public void testCardinality()
{    assertEquals("size", 3, test.size());}
0
public void testCopy() throws Exception
{    Vector copy = test.clone();    for (int i = 0; i < test.size(); i++) {        assertEquals("copy [" + i + ']', test.get(i), copy.get(i), EPSILON);    }}
0
public void testGet() throws Exception
{    for (int i = 0; i < test.size(); i++) {        assertEquals("get [" + i + ']', values[i + OFFSET], test.get(i), EPSILON);    }}
0
public void testGetOver()
{    test.get(test.size());}
0
public void testIterator() throws Exception
{    VectorView view = new VectorView(new DenseVector(values), OFFSET, CARDINALITY);    double[] gold = { 1.1, 2.2, 3.3 };    Iterator<Vector.Element> iter = view.iterator();    checkIterator(iter, gold);    iter = view.iterateNonZero();    checkIterator(iter, gold);    view = new VectorView(new DenseVector(values), 0, CARDINALITY);    gold = new double[] { 0.0, 1.1, 2.2 };    iter = view.iterator();    checkIterator(iter, gold);    gold = new double[] { 1.1, 2.2 };    iter = view.iterateNonZero();    checkIterator(iter, gold);}
0
private static void checkIterator(Iterator<Vector.Element> iter, double[] gold)
{    int i = 0;    while (iter.hasNext()) {        Vector.Element elt = iter.next();        assertEquals(elt.index() + " Value: " + gold[i] + " does not equal: " + elt.get(), gold[i], elt.get(), 0.0);        i++;    }}
0
public void testGetUnder()
{    test.get(-1);}
0
public void testSet() throws Exception
{    test.set(2, 4.5);    for (int i = 0; i < test.size(); i++) {        assertEquals("set [" + i + ']', i == 2 ? 4.5 : values[OFFSET + i], test.get(i), EPSILON);    }}
0
public void testSize() throws Exception
{    assertEquals("size", 3, test.getNumNondefaultElements());}
0
public void testViewPart() throws Exception
{    Vector part = test.viewPart(1, 2);    assertEquals("part size", 2, part.getNumNondefaultElements());    for (int i = 0; i < part.size(); i++) {        assertEquals("part[" + i + ']', values[OFFSET + i + 1], part.get(i), EPSILON);    }}
0
public void testViewPartUnder()
{    test.viewPart(-1, CARDINALITY);}
0
public void testViewPartOver()
{    test.viewPart(2, CARDINALITY);}
0
public void testViewPartCardinality()
{    test.viewPart(1, values.length + 1);}
0
public void testDot() throws Exception
{    double res = test.dot(test);    assertEquals("dot", 1.1 * 1.1 + 2.2 * 2.2 + 3.3 * 3.3, res, EPSILON);}
0
public void testDotCardinality()
{    test.dot(new DenseVector(test.size() + 1));}
0
public void testNormalize() throws Exception
{    Vector res = test.normalize();    double mag = Math.sqrt(1.1 * 1.1 + 2.2 * 2.2 + 3.3 * 3.3);    for (int i = 0; i < test.size(); i++) {        assertEquals("dot", values[OFFSET + i] / mag, res.get(i), EPSILON);    }}
0
public void testMinus() throws Exception
{    Vector val = test.minus(test);    assertEquals("size", 3, val.size());    for (int i = 0; i < test.size(); i++) {        assertEquals("get [" + i + ']', 0.0, val.get(i), EPSILON);    }}
0
public void testPlusDouble() throws Exception
{    Vector val = test.plus(1);    assertEquals("size", 3, val.size());    for (int i = 0; i < test.size(); i++) {        assertEquals("get [" + i + ']', values[OFFSET + i] + 1, val.get(i), EPSILON);    }}
0
public void testPlusVector() throws Exception
{    Vector val = test.plus(test);    assertEquals("size", 3, val.size());    for (int i = 0; i < test.size(); i++) {        assertEquals("get [" + i + ']', values[OFFSET + i] * 2, val.get(i), EPSILON);    }}
0
public void testPlusVectorCardinality()
{    test.plus(new DenseVector(test.size() + 1));}
0
public void testTimesDouble() throws Exception
{    Vector val = test.times(3);    assertEquals("size", 3, val.size());    for (int i = 0; i < test.size(); i++) {        assertEquals("get [" + i + ']', values[OFFSET + i] * 3, val.get(i), EPSILON);    }}
0
public void testDivideDouble() throws Exception
{    Vector val = test.divide(3);    assertEquals("size", 3, val.size());    for (int i = 0; i < test.size(); i++) {        assertEquals("get [" + i + ']', values[OFFSET + i] / 3, val.get(i), EPSILON);    }}
0
public void testTimesVector() throws Exception
{    Vector val = test.times(test);    assertEquals("size", 3, val.size());    for (int i = 0; i < test.size(); i++) {        assertEquals("get [" + i + ']', values[OFFSET + i] * values[OFFSET + i], val.get(i), EPSILON);    }}
0
public void testTimesVectorCardinality()
{    test.times(new DenseVector(test.size() + 1));}
0
public void testZSum()
{    double expected = 0;    for (int i = OFFSET; i < OFFSET + CARDINALITY; i++) {        expected += values[i];    }    assertEquals("wrong zSum", expected, test.zSum(), EPSILON);}
0
public void testAssignDouble()
{    test.assign(0);    for (int i = 0; i < test.size(); i++) {        assertEquals("value[" + i + ']', 0.0, test.getQuick(i), EPSILON);    }}
0
public void testAssignDoubleArray() throws Exception
{    double[] array = new double[test.size()];    test.assign(array);    for (int i = 0; i < test.size(); i++) {        assertEquals("value[" + i + ']', 0.0, test.getQuick(i), EPSILON);    }}
0
public void testAssignDoubleArrayCardinality()
{    double[] array = new double[test.size() + 1];    test.assign(array);}
0
public void testAssignVector() throws Exception
{    Vector other = new DenseVector(test.size());    test.assign(other);    for (int i = 0; i < test.size(); i++) {        assertEquals("value[" + i + ']', 0.0, test.getQuick(i), EPSILON);    }}
0
public void testAssignVectorCardinality()
{    Vector other = new DenseVector(test.size() - 1);    test.assign(other);}
0
public void testAssignUnaryFunction()
{    test.assign(Functions.NEGATE);    for (int i = 0; i < test.size(); i++) {        assertEquals("value[" + i + ']', -values[i + 1], test.getQuick(i), EPSILON);    }}
0
public void testAssignBinaryFunction() throws Exception
{    test.assign(test, Functions.PLUS);    for (int i = 0; i < test.size(); i++) {        assertEquals("value[" + i + ']', 2 * values[i + 1], test.getQuick(i), EPSILON);    }}
0
public void testAssignBinaryFunction2() throws Exception
{    test.assign(Functions.PLUS, 4);    for (int i = 0; i < test.size(); i++) {        assertEquals("value[" + i + ']', values[i + 1] + 4, test.getQuick(i), EPSILON);    }}
0
public void testAssignBinaryFunction3() throws Exception
{    test.assign(new TimesFunction(), 4);    for (int i = 0; i < test.size(); i++) {        assertEquals("value[" + i + ']', values[i + 1] * 4, test.getQuick(i), EPSILON);    }}
0
public void testLike()
{    assertTrue("not like", test.like() instanceof VectorView);}
0
public void testCrossProduct()
{    Matrix result = test.cross(test);    assertEquals("row size", test.size(), result.rowSize());    assertEquals("col size", test.size(), result.columnSize());    for (int row = 0; row < result.rowSize(); row++) {        for (int col = 0; col < result.columnSize(); col++) {            assertEquals("cross[" + row + "][" + col + ']', test.getQuick(row) * test.getQuick(col), result.getQuick(row, col), EPSILON);        }    }}
0
public void testBasics()
{    Matrix a = new UpperTriangular(new double[] { 1, 2, 3, 4, 5, 6, 7, 8, 9, 10 }, false);    assertEquals(0, a.viewDiagonal().minus(new DenseVector(new double[] { 1, 5, 8, 10 })).norm(1), 1.0e-10);    assertEquals(0, a.viewPart(0, 3, 1, 3).viewDiagonal().minus(new DenseVector(new double[] { 2, 6, 9 })).norm(1), 1.0e-10);    assertEquals(4, a.get(0, 3), 1.0e-10);    print(a);    Matrix m = new DenseMatrix(4, 4).assign(a);    assertEquals(0, m.minus(a).aggregate(Functions.PLUS, Functions.ABS), 1.0e-10);    print(m);    assertEquals(0, m.transpose().times(m).minus(a.transpose().times(a)).aggregate(Functions.PLUS, Functions.ABS), 1.0e-10);    assertEquals(0, m.plus(m).minus(a.plus(a)).aggregate(Functions.PLUS, Functions.ABS), 1.0e-10);}
0
private static void print(Matrix m)
{    for (int i = 0; i < m.rowSize(); i++) {        for (int j = 0; j < m.columnSize(); j++) {            if (Math.abs(m.get(i, j)) > 1.0e-10) {                System.out.printf("%10.3f ", m.get(i, j));            } else {                System.out.printf("%10s ", (i + j) % 3 == 0 ? "." : "");            }        }        System.out.printf("\n");    }    System.out.printf("\n");}
0
private static void createStubs(Vector v, Vector realV)
{    expect(v.getLookupCost()).andStubReturn(realV instanceof SequentialAccessSparseVector ? Math.round(Math.log(1000)) : realV.getLookupCost());    expect(v.getIteratorAdvanceCost()).andStubReturn(realV.getIteratorAdvanceCost());    expect(v.isAddConstantTime()).andStubReturn(realV.isAddConstantTime());    expect(v.isSequentialAccess()).andStubReturn(realV.isSequentialAccess());    expect(v.isDense()).andStubReturn(realV.isDense());    expect(v.getNumNondefaultElements()).andStubReturn(realV.isDense() ? realV.size() : 1000);    expect(v.size()).andStubReturn(realV.size());}
0
public void setUpStubs()
{    createStubs(dense, realDense);    createStubs(sasv, realSasv);    createStubs(rasv, realRasv);}
0
public void denseInteractions()
{    replayAll();        assertEquals(VectorBinaryAggregate.AggregateNonzerosIterateThisLookupThat.class, VectorBinaryAggregate.getBestOperation(dense, dense, Functions.PLUS, Functions.MULT).getClass());        assertEquals(VectorBinaryAggregate.AggregateIterateUnionSequential.class, VectorBinaryAggregate.getBestOperation(dense, dense, Functions.MAX_ABS, Functions.MINUS).getClass());        assertEquals(VectorBinaryAggregate.AggregateIterateUnionSequential.class, VectorBinaryAggregate.getBestOperation(dense, dense, Functions.PLUS, Functions.MINUS_SQUARED).getClass());        assertEquals(VectorBinaryAggregate.AggregateIterateUnionSequential.class, VectorBinaryAggregate.getBestOperation(dense, dense, Functions.PLUS, Functions.MINUS_ABS).getClass());        assertEquals(VectorBinaryAggregate.AggregateIterateUnionSequential.class, VectorBinaryAggregate.getBestOperation(dense, dense, Functions.PLUS, Functions.minusAbsPow(1.2)).getClass());        assertEquals(VectorBinaryAggregate.AggregateNonzerosIterateThisLookupThat.class, VectorBinaryAggregate.getBestOperation(dense, dense, Functions.PLUS, Functions.MULT_SQUARE_LEFT).getClass());}
0
public void sasvInteractions()
{    replayAll();        assertEquals(VectorBinaryAggregate.AggregateIterateIntersection.class, VectorBinaryAggregate.getBestOperation(sasv, sasv, Functions.PLUS, Functions.MULT).getClass());        assertEquals(VectorBinaryAggregate.AggregateIterateUnionSequential.class, VectorBinaryAggregate.getBestOperation(sasv, sasv, Functions.MAX_ABS, Functions.MINUS).getClass());        assertEquals(VectorBinaryAggregate.AggregateIterateUnionSequential.class, VectorBinaryAggregate.getBestOperation(sasv, sasv, Functions.PLUS, Functions.MINUS_SQUARED).getClass());        assertEquals(VectorBinaryAggregate.AggregateIterateUnionSequential.class, VectorBinaryAggregate.getBestOperation(sasv, sasv, Functions.PLUS, Functions.MINUS_ABS).getClass());        assertEquals(VectorBinaryAggregate.AggregateIterateUnionSequential.class, VectorBinaryAggregate.getBestOperation(sasv, sasv, Functions.PLUS, Functions.minusAbsPow(1.2)).getClass());        assertEquals(VectorBinaryAggregate.AggregateIterateIntersection.class, VectorBinaryAggregate.getBestOperation(sasv, sasv, Functions.PLUS, Functions.MULT_SQUARE_LEFT).getClass());}
0
public void rasvInteractions()
{    replayAll();        assertEquals(VectorBinaryAggregate.AggregateNonzerosIterateThisLookupThat.class, VectorBinaryAggregate.getBestOperation(rasv, rasv, Functions.PLUS, Functions.MULT).getClass());        assertEquals(VectorBinaryAggregate.AggregateIterateUnionRandom.class, VectorBinaryAggregate.getBestOperation(rasv, rasv, Functions.MAX_ABS, Functions.MINUS).getClass());        assertEquals(VectorBinaryAggregate.AggregateIterateUnionRandom.class, VectorBinaryAggregate.getBestOperation(rasv, rasv, Functions.PLUS, Functions.MINUS_SQUARED).getClass());        assertEquals(VectorBinaryAggregate.AggregateIterateUnionRandom.class, VectorBinaryAggregate.getBestOperation(rasv, rasv, Functions.PLUS, Functions.MINUS_ABS).getClass());        assertEquals(VectorBinaryAggregate.AggregateIterateUnionRandom.class, VectorBinaryAggregate.getBestOperation(rasv, rasv, Functions.PLUS, Functions.minusAbsPow(1.2)).getClass());        assertEquals(VectorBinaryAggregate.AggregateNonzerosIterateThisLookupThat.class, VectorBinaryAggregate.getBestOperation(rasv, rasv, Functions.PLUS, Functions.MULT_SQUARE_LEFT).getClass());}
0
public void sasvDenseInteractions()
{    replayAll();        assertEquals(VectorBinaryAggregate.AggregateNonzerosIterateThisLookupThat.class, VectorBinaryAggregate.getBestOperation(sasv, dense, Functions.PLUS, Functions.MULT).getClass());        assertEquals(VectorBinaryAggregate.AggregateIterateUnionSequential.class, VectorBinaryAggregate.getBestOperation(sasv, dense, Functions.MAX_ABS, Functions.MINUS).getClass());        assertEquals(VectorBinaryAggregate.AggregateIterateUnionSequential.class, VectorBinaryAggregate.getBestOperation(sasv, dense, Functions.PLUS, Functions.MINUS_SQUARED).getClass());        assertEquals(VectorBinaryAggregate.AggregateIterateUnionSequential.class, VectorBinaryAggregate.getBestOperation(sasv, dense, Functions.PLUS, Functions.MINUS_ABS).getClass());        assertEquals(VectorBinaryAggregate.AggregateIterateUnionSequential.class, VectorBinaryAggregate.getBestOperation(sasv, dense, Functions.PLUS, Functions.minusAbsPow(1.2)).getClass());        assertEquals(VectorBinaryAggregate.AggregateNonzerosIterateThisLookupThat.class, VectorBinaryAggregate.getBestOperation(sasv, dense, Functions.PLUS, Functions.MULT_SQUARE_LEFT).getClass());}
0
public void denseSasvInteractions()
{    replayAll();        assertEquals(VectorBinaryAggregate.AggregateNonzerosIterateThatLookupThis.class, VectorBinaryAggregate.getBestOperation(dense, sasv, Functions.PLUS, Functions.MULT).getClass());        assertEquals(VectorBinaryAggregate.AggregateIterateUnionSequential.class, VectorBinaryAggregate.getBestOperation(dense, sasv, Functions.MAX_ABS, Functions.MINUS).getClass());        assertEquals(VectorBinaryAggregate.AggregateIterateUnionSequential.class, VectorBinaryAggregate.getBestOperation(dense, sasv, Functions.PLUS, Functions.MINUS_SQUARED).getClass());        assertEquals(VectorBinaryAggregate.AggregateIterateUnionSequential.class, VectorBinaryAggregate.getBestOperation(dense, sasv, Functions.PLUS, Functions.MINUS_ABS).getClass());        assertEquals(VectorBinaryAggregate.AggregateIterateUnionSequential.class, VectorBinaryAggregate.getBestOperation(dense, sasv, Functions.PLUS, Functions.minusAbsPow(1.2)).getClass());        assertEquals(VectorBinaryAggregate.AggregateNonzerosIterateThatLookupThis.class, VectorBinaryAggregate.getBestOperation(dense, sasv, Functions.PLUS, Functions.MULT_SQUARE_LEFT).getClass());}
0
public void denseRasvInteractions()
{    replayAll();        assertEquals(VectorBinaryAggregate.AggregateNonzerosIterateThatLookupThis.class, VectorBinaryAggregate.getBestOperation(dense, rasv, Functions.PLUS, Functions.MULT).getClass());        assertEquals(VectorBinaryAggregate.AggregateIterateUnionRandom.class, VectorBinaryAggregate.getBestOperation(dense, rasv, Functions.MAX_ABS, Functions.MINUS).getClass());        assertEquals(VectorBinaryAggregate.AggregateIterateUnionRandom.class, VectorBinaryAggregate.getBestOperation(dense, rasv, Functions.PLUS, Functions.MINUS_SQUARED).getClass());        assertEquals(VectorBinaryAggregate.AggregateIterateUnionRandom.class, VectorBinaryAggregate.getBestOperation(dense, rasv, Functions.PLUS, Functions.MINUS_ABS).getClass());        assertEquals(VectorBinaryAggregate.AggregateIterateUnionRandom.class, VectorBinaryAggregate.getBestOperation(dense, rasv, Functions.PLUS, Functions.minusAbsPow(1.2)).getClass());        assertEquals(VectorBinaryAggregate.AggregateNonzerosIterateThatLookupThis.class, VectorBinaryAggregate.getBestOperation(dense, rasv, Functions.PLUS, Functions.MULT_SQUARE_LEFT).getClass());}
0
public void rasvDenseInteractions()
{    replayAll();        assertEquals(VectorBinaryAggregate.AggregateNonzerosIterateThisLookupThat.class, VectorBinaryAggregate.getBestOperation(rasv, dense, Functions.PLUS, Functions.MULT).getClass());        assertEquals(VectorBinaryAggregate.AggregateIterateUnionRandom.class, VectorBinaryAggregate.getBestOperation(rasv, dense, Functions.MAX_ABS, Functions.MINUS).getClass());        assertEquals(VectorBinaryAggregate.AggregateIterateUnionRandom.class, VectorBinaryAggregate.getBestOperation(rasv, dense, Functions.PLUS, Functions.MINUS_SQUARED).getClass());        assertEquals(VectorBinaryAggregate.AggregateIterateUnionRandom.class, VectorBinaryAggregate.getBestOperation(rasv, dense, Functions.PLUS, Functions.MINUS_ABS).getClass());        assertEquals(VectorBinaryAggregate.AggregateIterateUnionRandom.class, VectorBinaryAggregate.getBestOperation(rasv, dense, Functions.PLUS, Functions.minusAbsPow(1.2)).getClass());        assertEquals(VectorBinaryAggregate.AggregateNonzerosIterateThisLookupThat.class, VectorBinaryAggregate.getBestOperation(rasv, dense, Functions.PLUS, Functions.MULT_SQUARE_LEFT).getClass());}
0
public void sasvRasvInteractions()
{    replayAll();        assertEquals(VectorBinaryAggregate.AggregateNonzerosIterateThisLookupThat.class, VectorBinaryAggregate.getBestOperation(sasv, rasv, Functions.PLUS, Functions.MULT).getClass());        assertEquals(VectorBinaryAggregate.AggregateIterateUnionRandom.class, VectorBinaryAggregate.getBestOperation(sasv, rasv, Functions.MAX_ABS, Functions.MINUS).getClass());        assertEquals(VectorBinaryAggregate.AggregateIterateUnionRandom.class, VectorBinaryAggregate.getBestOperation(sasv, rasv, Functions.PLUS, Functions.MINUS_SQUARED).getClass());        assertEquals(VectorBinaryAggregate.AggregateIterateUnionRandom.class, VectorBinaryAggregate.getBestOperation(sasv, rasv, Functions.PLUS, Functions.MINUS_ABS).getClass());        assertEquals(VectorBinaryAggregate.AggregateIterateUnionRandom.class, VectorBinaryAggregate.getBestOperation(sasv, rasv, Functions.PLUS, Functions.minusAbsPow(1.2)).getClass());        assertEquals(VectorBinaryAggregate.AggregateNonzerosIterateThisLookupThat.class, VectorBinaryAggregate.getBestOperation(sasv, rasv, Functions.PLUS, Functions.MULT_SQUARE_LEFT).getClass());}
0
public void rasvSasvInteractions()
{    replayAll();        assertEquals(VectorBinaryAggregate.AggregateNonzerosIterateThatLookupThis.class, VectorBinaryAggregate.getBestOperation(rasv, sasv, Functions.PLUS, Functions.MULT).getClass());        assertEquals(VectorBinaryAggregate.AggregateIterateUnionRandom.class, VectorBinaryAggregate.getBestOperation(rasv, sasv, Functions.MAX_ABS, Functions.MINUS).getClass());        assertEquals(VectorBinaryAggregate.AggregateIterateUnionRandom.class, VectorBinaryAggregate.getBestOperation(rasv, sasv, Functions.PLUS, Functions.MINUS_SQUARED).getClass());        assertEquals(VectorBinaryAggregate.AggregateIterateUnionRandom.class, VectorBinaryAggregate.getBestOperation(rasv, sasv, Functions.PLUS, Functions.MINUS_ABS).getClass());        assertEquals(VectorBinaryAggregate.AggregateIterateUnionRandom.class, VectorBinaryAggregate.getBestOperation(rasv, sasv, Functions.PLUS, Functions.minusAbsPow(1.2)).getClass());        assertEquals(VectorBinaryAggregate.AggregateNonzerosIterateThatLookupThis.class, VectorBinaryAggregate.getBestOperation(rasv, sasv, Functions.PLUS, Functions.MULT_SQUARE_LEFT).getClass());}
0
private void replayAll()
{    replay(dense, sasv, rasv);}
0
public static Collection<Object[]> generateData()
{    List<Object[]> data = Lists.newArrayList();    for (List<?> entry : Sets.cartesianProduct(Lists.newArrayList(ImmutableSet.of(Functions.PLUS, Functions.PLUS_ABS, Functions.MAX), ImmutableSet.of(Functions.PLUS, Functions.PLUS_ABS, Functions.MULT, Functions.MULT_RIGHT_PLUS1, Functions.MINUS), ImmutableSet.copyOf(VectorBinaryAggregate.OPERATIONS), ImmutableSet.of(new SequentialAccessSparseVector(CARDINALITY), new RandomAccessSparseVector(CARDINALITY), new DenseVector(CARDINALITY)), ImmutableSet.of(new SequentialAccessSparseVector(CARDINALITY), new RandomAccessSparseVector(CARDINALITY), new DenseVector(CARDINALITY))))) {        data.add(entry.toArray());    }    return data;}
0
public void testSelf()
{    Vector x = first.like();    Vector xBase = new DenseVector(CARDINALITY);    List<Double> items = Lists.newArrayList();    for (int i = 0; i < x.size(); ++i) {        items.add(r.nextDouble());    }    for (int i = 1; i < x.size(); ++i) {        x.setQuick(i, items.get(i));        xBase.setQuick(i, items.get(i));    }    Vector y = second.like().assign(x);    Vector yBase = new DenseVector(x);    System.out.printf("aggregator %s; combiner %s; operation %s\n", aggregator, combiner, operation);    double expectedResult = combiner.apply(0, 0);    for (int i = 1; i < x.size(); ++i) {        expectedResult = aggregator.apply(expectedResult, combiner.apply(items.get(i), items.get(i)));    }    double result = operation.aggregate(x, y, aggregator, combiner);    double resultBase = operation.aggregate(xBase, yBase, aggregator, combiner);    assertEquals(expectedResult, result, 0.0);    assertEquals(resultBase, result, 0.0);}
0
public void testSeparate()
{    List<Double> items1 = Lists.newArrayList();    List<Double> items2 = Lists.newArrayList();    for (int i = 0; i < CARDINALITY; ++i) {        items1.add(r.nextDouble());        items2.add(r.nextDouble());    }    Vector x = first.like();    Vector xBase = new DenseVector(CARDINALITY);    for (int i = 0; i < x.size(); ++i) {        x.setQuick(i, items1.get(i));        xBase.setQuick(i, items1.get(i));    }    Vector y = second.like();    Vector yBase = new DenseVector(CARDINALITY);    for (int i = 0; i < y.size(); ++i) {        y.setQuick(i, items2.get(i));        yBase.setQuick(i, items2.get(i));    }    System.out.printf("aggregator %s; combiner %s; operation %s\n", aggregator, combiner, operation);    double expectedResult = combiner.apply(items1.get(0), items2.get(0));    for (int i = 1; i < x.size(); ++i) {        expectedResult = aggregator.apply(expectedResult, combiner.apply(items1.get(i), items2.get(i)));    }    double result = operation.aggregate(x, y, aggregator, combiner);    double resultBase = operation.aggregate(xBase, yBase, aggregator, combiner);    assertEquals(expectedResult, result, 0.0);    assertEquals(resultBase, result, 0.0);}
0
private static void createStubs(Vector v, Vector realV)
{    expect(v.getLookupCost()).andStubReturn(realV instanceof SequentialAccessSparseVector ? Math.round(Math.log(1000)) : realV.getLookupCost());    expect(v.getIteratorAdvanceCost()).andStubReturn(realV.getIteratorAdvanceCost());    expect(v.isAddConstantTime()).andStubReturn(realV.isAddConstantTime());    expect(v.isSequentialAccess()).andStubReturn(realV.isSequentialAccess());    expect(v.isDense()).andStubReturn(realV.isDense());    expect(v.getNumNondefaultElements()).andStubReturn(realV.isDense() ? realV.size() : 1000);    expect(v.size()).andStubReturn(realV.size());}
0
public void setUpStubs()
{    createStubs(dense, realDense);    createStubs(sasv, realSasv);    createStubs(rasv, realRasv);}
0
public void denseInteractions()
{    replayAll();    assertEquals(VectorBinaryAssign.AssignNonzerosIterateThatLookupThisInplaceUpdates.class, VectorBinaryAssign.getBestOperation(dense, dense, Functions.PLUS).getClass());    assertEquals(VectorBinaryAssign.AssignNonzerosIterateThatLookupThisInplaceUpdates.class, VectorBinaryAssign.getBestOperation(dense, dense, Functions.MINUS).getClass());    assertEquals(VectorBinaryAssign.AssignNonzerosIterateThisLookupThat.class, VectorBinaryAssign.getBestOperation(dense, dense, Functions.MULT).getClass());    assertEquals(VectorBinaryAssign.AssignAllLoopInplaceUpdates.class, VectorBinaryAssign.getBestOperation(dense, dense, Functions.DIV).getClass());    assertEquals(VectorBinaryAssign.AssignNonzerosIterateThatLookupThisInplaceUpdates.class, VectorBinaryAssign.getBestOperation(dense, dense, Functions.SECOND_LEFT_ZERO).getClass());}
0
public void sasvInteractions()
{    replayAll();    assertEquals(VectorBinaryAssign.AssignIterateUnionSequentialMergeUpdates.class, VectorBinaryAssign.getBestOperation(sasv, sasv, Functions.PLUS).getClass());    assertEquals(VectorBinaryAssign.AssignIterateUnionSequentialMergeUpdates.class, VectorBinaryAssign.getBestOperation(sasv, sasv, Functions.MINUS).getClass());    assertEquals(VectorBinaryAssign.AssignIterateUnionSequentialMergeUpdates.class, VectorBinaryAssign.getBestOperation(sasv, sasv, Functions.MULT).getClass());    assertEquals(VectorBinaryAssign.AssignAllIterateSequentialMergeUpdates.class, VectorBinaryAssign.getBestOperation(sasv, sasv, Functions.DIV).getClass());    assertEquals(VectorBinaryAssign.AssignIterateUnionSequentialMergeUpdates.class, VectorBinaryAssign.getBestOperation(sasv, sasv, Functions.SECOND_LEFT_ZERO).getClass());}
0
public void rasvInteractions()
{    replayAll();    assertEquals(VectorBinaryAssign.AssignNonzerosIterateThatLookupThisInplaceUpdates.class, VectorBinaryAssign.getBestOperation(rasv, rasv, Functions.PLUS).getClass());    assertEquals(VectorBinaryAssign.AssignNonzerosIterateThatLookupThisInplaceUpdates.class, VectorBinaryAssign.getBestOperation(rasv, rasv, Functions.MINUS).getClass());    assertEquals(VectorBinaryAssign.AssignNonzerosIterateThisLookupThat.class, VectorBinaryAssign.getBestOperation(rasv, rasv, Functions.MULT).getClass());    assertEquals(VectorBinaryAssign.AssignAllLoopInplaceUpdates.class, VectorBinaryAssign.getBestOperation(rasv, rasv, Functions.DIV).getClass());    assertEquals(VectorBinaryAssign.AssignNonzerosIterateThatLookupThisInplaceUpdates.class, VectorBinaryAssign.getBestOperation(rasv, rasv, Functions.SECOND_LEFT_ZERO).getClass());}
0
public void sasvDenseInteractions()
{    replayAll();    assertEquals(VectorBinaryAssign.AssignNonzerosIterateThatLookupThisMergeUpdates.class, VectorBinaryAssign.getBestOperation(sasv, dense, Functions.PLUS).getClass());    assertEquals(VectorBinaryAssign.AssignNonzerosIterateThatLookupThisMergeUpdates.class, VectorBinaryAssign.getBestOperation(sasv, dense, Functions.MINUS).getClass());    assertEquals(VectorBinaryAssign.AssignNonzerosIterateThisLookupThat.class, VectorBinaryAssign.getBestOperation(sasv, dense, Functions.MULT).getClass());    assertEquals(VectorBinaryAssign.AssignAllIterateThisLookupThatMergeUpdates.class, VectorBinaryAssign.getBestOperation(sasv, dense, Functions.DIV).getClass());    assertEquals(VectorBinaryAssign.AssignNonzerosIterateThatLookupThisMergeUpdates.class, VectorBinaryAssign.getBestOperation(sasv, dense, Functions.SECOND_LEFT_ZERO).getClass());}
0
public void denseSasvInteractions()
{    replayAll();    assertEquals(VectorBinaryAssign.AssignNonzerosIterateThatLookupThisInplaceUpdates.class, VectorBinaryAssign.getBestOperation(dense, sasv, Functions.PLUS).getClass());    assertEquals(VectorBinaryAssign.AssignNonzerosIterateThatLookupThisInplaceUpdates.class, VectorBinaryAssign.getBestOperation(dense, sasv, Functions.MINUS).getClass());    assertEquals(VectorBinaryAssign.AssignIterateUnionSequentialInplaceUpdates.class, VectorBinaryAssign.getBestOperation(dense, sasv, Functions.MULT).getClass());    assertEquals(VectorBinaryAssign.AssignAllIterateThatLookupThisInplaceUpdates.class, VectorBinaryAssign.getBestOperation(dense, sasv, Functions.DIV).getClass());    assertEquals(VectorBinaryAssign.AssignNonzerosIterateThatLookupThisInplaceUpdates.class, VectorBinaryAssign.getBestOperation(dense, sasv, Functions.SECOND_LEFT_ZERO).getClass());}
0
public void denseRasvInteractions()
{    replayAll();    assertEquals(VectorBinaryAssign.AssignNonzerosIterateThatLookupThisInplaceUpdates.class, VectorBinaryAssign.getBestOperation(dense, rasv, Functions.PLUS).getClass());    assertEquals(VectorBinaryAssign.AssignNonzerosIterateThatLookupThisInplaceUpdates.class, VectorBinaryAssign.getBestOperation(dense, rasv, Functions.MINUS).getClass());    assertEquals(VectorBinaryAssign.AssignNonzerosIterateThisLookupThat.class, VectorBinaryAssign.getBestOperation(dense, rasv, Functions.MULT).getClass());    assertEquals(VectorBinaryAssign.AssignAllLoopInplaceUpdates.class, VectorBinaryAssign.getBestOperation(dense, rasv, Functions.DIV).getClass());    assertEquals(VectorBinaryAssign.AssignNonzerosIterateThatLookupThisInplaceUpdates.class, VectorBinaryAssign.getBestOperation(dense, rasv, Functions.SECOND_LEFT_ZERO).getClass());}
0
public void rasvDenseInteractions()
{    replayAll();    assertEquals(VectorBinaryAssign.AssignNonzerosIterateThatLookupThisInplaceUpdates.class, VectorBinaryAssign.getBestOperation(rasv, dense, Functions.PLUS).getClass());    assertEquals(VectorBinaryAssign.AssignNonzerosIterateThatLookupThisInplaceUpdates.class, VectorBinaryAssign.getBestOperation(rasv, dense, Functions.MINUS).getClass());    assertEquals(VectorBinaryAssign.AssignNonzerosIterateThisLookupThat.class, VectorBinaryAssign.getBestOperation(rasv, dense, Functions.MULT).getClass());    assertEquals(VectorBinaryAssign.AssignAllLoopInplaceUpdates.class, VectorBinaryAssign.getBestOperation(rasv, dense, Functions.DIV).getClass());    assertEquals(VectorBinaryAssign.AssignNonzerosIterateThatLookupThisInplaceUpdates.class, VectorBinaryAssign.getBestOperation(rasv, dense, Functions.SECOND_LEFT_ZERO).getClass());}
0
public void sasvRasvInteractions()
{    replayAll();    assertEquals(VectorBinaryAssign.AssignNonzerosIterateThatLookupThisInplaceUpdates.class, VectorBinaryAssign.getBestOperation(sasv, rasv, Functions.PLUS).getClass());    assertEquals(VectorBinaryAssign.AssignNonzerosIterateThatLookupThisInplaceUpdates.class, VectorBinaryAssign.getBestOperation(sasv, rasv, Functions.MINUS).getClass());    assertEquals(VectorBinaryAssign.AssignNonzerosIterateThisLookupThat.class, VectorBinaryAssign.getBestOperation(sasv, rasv, Functions.MULT).getClass());    assertEquals(VectorBinaryAssign.AssignAllIterateThisLookupThatMergeUpdates.class, VectorBinaryAssign.getBestOperation(sasv, rasv, Functions.DIV).getClass());    assertEquals(VectorBinaryAssign.AssignNonzerosIterateThatLookupThisInplaceUpdates.class, VectorBinaryAssign.getBestOperation(sasv, rasv, Functions.SECOND_LEFT_ZERO).getClass());}
0
public void rasvSasvInteractions()
{    replayAll();    assertEquals(VectorBinaryAssign.AssignNonzerosIterateThatLookupThisInplaceUpdates.class, VectorBinaryAssign.getBestOperation(rasv, sasv, Functions.PLUS).getClass());    assertEquals(VectorBinaryAssign.AssignNonzerosIterateThatLookupThisInplaceUpdates.class, VectorBinaryAssign.getBestOperation(rasv, sasv, Functions.MINUS).getClass());    assertEquals(VectorBinaryAssign.AssignNonzerosIterateThisLookupThat.class, VectorBinaryAssign.getBestOperation(rasv, sasv, Functions.MULT).getClass());    assertEquals(VectorBinaryAssign.AssignAllIterateThatLookupThisInplaceUpdates.class, VectorBinaryAssign.getBestOperation(rasv, sasv, Functions.DIV).getClass());    assertEquals(VectorBinaryAssign.AssignNonzerosIterateThatLookupThisInplaceUpdates.class, VectorBinaryAssign.getBestOperation(rasv, sasv, Functions.SECOND_LEFT_ZERO).getClass());}
0
private void replayAll()
{    replay(dense, sasv, rasv);}
0
public static Collection<Object[]> generateData()
{    List<Object[]> data = Lists.newArrayList();    for (List entry : Sets.cartesianProduct(Lists.newArrayList(ImmutableSet.of(Functions.PLUS, Functions.PLUS_ABS, Functions.MULT, Functions.MULT_RIGHT_PLUS1, Functions.MINUS), ImmutableSet.copyOf(VectorBinaryAssign.OPERATIONS)))) {        data.add(entry.toArray());    }    return data;}
0
public void testAll()
{    SequentialAccessSparseVector x = new SequentialAccessSparseVector(CARDINALITY);    for (int i = 0; i < x.size(); ++i) {        x.setQuick(i, i);    }    SequentialAccessSparseVector y = new SequentialAccessSparseVector(x);    System.out.printf("function %s; operation %s\n", function, operation);    operation.assign(x, y, function);    for (int i = 0; i < x.size(); ++i) {        assertEquals(x.getQuick(i), function.apply(i, i), 0.0);    }}
0
public void testSparseVector()
{    Vector vec1 = new RandomAccessSparseVector(3);    Vector vec2 = new RandomAccessSparseVector(3);    doTestVectors(vec1, vec2);}
0
public void testSparseVectorFullIteration()
{    int[] index = { 0, 1, 2, 3, 4, 5 };    double[] values = { 1, 2, 3, 4, 5, 6 };    assertEquals(index.length, values.length);    int n = index.length;    Vector vector = new SequentialAccessSparseVector(n);    for (int i = 0; i < n; i++) {        vector.set(index[i], values[i]);    }    for (int i = 0; i < n; i++) {        assertEquals(vector.get(i), values[i], EPSILON);    }    int elements = 0;    for (Element ignore : vector.all()) {        elements++;    }    assertEquals(n, elements);    assertFalse(new SequentialAccessSparseVector(0).iterator().hasNext());}
0
public void testSparseVectorSparseIteration()
{    int[] index = { 0, 1, 2, 3, 4, 5 };    double[] values = { 1, 2, 3, 4, 5, 6 };    assertEquals(index.length, values.length);    int n = index.length;    Vector vector = new SequentialAccessSparseVector(n);    for (int i = 0; i < n; i++) {        vector.set(index[i], values[i]);    }    for (int i = 0; i < n; i++) {        assertEquals(vector.get(i), values[i], EPSILON);    }    int elements = 0;    for (Element ignored : vector.nonZeroes()) {        elements++;    }    assertEquals(n, elements);    Vector empty = new SequentialAccessSparseVector(0);    assertFalse(empty.nonZeroes().iterator().hasNext());}
0
public void testEquivalent()
{        RandomAccessSparseVector randomAccessLeft = new RandomAccessSparseVector(3);    Vector sequentialAccessLeft = new SequentialAccessSparseVector(3);    Vector right = new DenseVector(3);    randomAccessLeft.setQuick(0, 1);    randomAccessLeft.setQuick(1, 2);    randomAccessLeft.setQuick(2, 3);    sequentialAccessLeft.setQuick(0, 1);    sequentialAccessLeft.setQuick(1, 2);    sequentialAccessLeft.setQuick(2, 3);    right.setQuick(0, 1);    right.setQuick(1, 2);    right.setQuick(2, 3);    assertEquals(randomAccessLeft, right);    assertEquals(sequentialAccessLeft, right);    assertEquals(sequentialAccessLeft, randomAccessLeft);    Vector leftBar = new DenseVector(3);    leftBar.setQuick(0, 1);    leftBar.setQuick(1, 2);    leftBar.setQuick(2, 3);    assertEquals(leftBar, right);    assertEquals(randomAccessLeft, right);    assertEquals(sequentialAccessLeft, right);    Vector rightBar = new RandomAccessSparseVector(3);    rightBar.setQuick(0, 1);    rightBar.setQuick(1, 2);    rightBar.setQuick(2, 3);    assertEquals(randomAccessLeft, rightBar);    right.setQuick(2, 4);    assertFalse(randomAccessLeft.equals(right));    right = new DenseVector(4);    right.setQuick(0, 1);    right.setQuick(1, 2);    right.setQuick(2, 3);    right.setQuick(3, 3);    assertFalse(randomAccessLeft.equals(right));    randomAccessLeft = new RandomAccessSparseVector(2);    randomAccessLeft.setQuick(0, 1);    randomAccessLeft.setQuick(1, 2);    assertFalse(randomAccessLeft.equals(right));    Vector dense = new DenseVector(3);    right = new DenseVector(3);    right.setQuick(0, 1);    right.setQuick(1, 2);    right.setQuick(2, 3);    dense.setQuick(0, 1);    dense.setQuick(1, 2);    dense.setQuick(2, 3);    assertEquals(dense, right);    RandomAccessSparseVector sparse = new RandomAccessSparseVector(3);    randomAccessLeft = new RandomAccessSparseVector(3);    sparse.setQuick(0, 1);    sparse.setQuick(1, 2);    sparse.setQuick(2, 3);    randomAccessLeft.setQuick(0, 1);    randomAccessLeft.setQuick(1, 2);    randomAccessLeft.setQuick(2, 3);    assertEquals(randomAccessLeft, sparse);    Vector v1 = new VectorView(randomAccessLeft, 0, 2);    Vector v2 = new VectorView(right, 0, 2);    assertEquals(v1, v2);    sparse = new RandomAccessSparseVector(2);    sparse.setQuick(0, 1);    sparse.setQuick(1, 2);    assertEquals(v1, sparse);}
0
private static void doTestVectors(Vector left, Vector right)
{    left.setQuick(0, 1);    left.setQuick(1, 2);    left.setQuick(2, 3);    right.setQuick(0, 4);    right.setQuick(1, 5);    right.setQuick(2, 6);    double result = left.dot(right);    assertEquals(32.0, result, EPSILON);}
0
public void testGetDistanceSquared()
{    Vector v = new DenseVector(5);    Vector w = new DenseVector(5);    setUpV(v);    setUpW(w);    doTestGetDistanceSquared(v, w);    v = new RandomAccessSparseVector(5);    w = new RandomAccessSparseVector(5);    setUpV(v);    setUpW(w);    doTestGetDistanceSquared(v, w);    v = new SequentialAccessSparseVector(5);    w = new SequentialAccessSparseVector(5);    setUpV(v);    setUpW(w);    doTestGetDistanceSquared(v, w);}
0
public void testAddTo() throws Exception
{    Vector v = new DenseVector(4);    Vector w = new DenseVector(4);    v.setQuick(0, 1);    v.setQuick(1, 2);    v.setQuick(2, 0);    v.setQuick(3, 4);    w.setQuick(0, 1);    w.setQuick(1, 1);    w.setQuick(2, 1);    w.setQuick(3, 1);    w.assign(v, Functions.PLUS);    Vector gold = new DenseVector(new double[] { 2, 3, 1, 5 });    assertEquals(w, gold);    assertFalse(v.equals(gold));}
0
private static void setUpV(Vector v)
{    v.setQuick(1, 2);    v.setQuick(2, -4);    v.setQuick(3, -9);}
0
private static void setUpW(Vector w)
{    w.setQuick(0, -5);    w.setQuick(1, -1);    w.setQuick(2, 9);    w.setQuick(3, 0.1);    w.setQuick(4, 2.1);}
0
private static void doTestGetDistanceSquared(Vector v, Vector w)
{    double expected = v.minus(w).getLengthSquared();    assertEquals(expected, v.getDistanceSquared(w), 1.0e-6);}
0
public void testGetLengthSquared()
{    Vector v = new DenseVector(5);    setUpV(v);    doTestGetLengthSquared(v);    v = new RandomAccessSparseVector(5);    setUpV(v);    doTestGetLengthSquared(v);    v = new SequentialAccessSparseVector(5);    setUpV(v);    doTestGetLengthSquared(v);}
0
public static double lengthSquaredSlowly(Vector v)
{    double d = 0.0;    for (int i = 0; i < v.size(); i++) {        double value = v.get(i);        d += value * value;    }    return d;}
0
private static void doTestGetLengthSquared(Vector v)
{    double expected = lengthSquaredSlowly(v);    assertEquals("v.getLengthSquared() != sum_of_squared_elements(v)", expected, v.getLengthSquared(), 0.0);    v.set(v.size() / 2, v.get(v.size() / 2) + 1.0);    expected = lengthSquaredSlowly(v);    assertEquals("mutation via set() fails to change lengthSquared", expected, v.getLengthSquared(), EPSILON);    v.setQuick(v.size() / 5, v.get(v.size() / 5) + 1.0);    expected = lengthSquaredSlowly(v);    assertEquals("mutation via setQuick() fails to change lengthSquared", expected, v.getLengthSquared(), EPSILON);    for (Element e : v.nonZeroes()) {        if (e.index() == v.size() - 2) {            e.set(e.get() - 5.0);        }    }    expected = lengthSquaredSlowly(v);    assertEquals("mutation via dense iterator.set fails to change lengthSquared", expected, v.getLengthSquared(), EPSILON);    int i = 0;    for (Element e : v.nonZeroes()) {        i++;        if (i == v.getNumNondefaultElements() - 1) {            e.set(e.get() - 5.0);        }    }    expected = lengthSquaredSlowly(v);    assertEquals("mutation via sparse iterator.set fails to change lengthSquared", expected, v.getLengthSquared(), EPSILON);    v.assign(3.0);    expected = lengthSquaredSlowly(v);    assertEquals("mutation via assign(double) fails to change lengthSquared", expected, v.getLengthSquared(), EPSILON);    v.assign(Functions.SQUARE);    expected = lengthSquaredSlowly(v);    assertEquals("mutation via assign(square) fails to change lengthSquared", expected, v.getLengthSquared(), EPSILON);    v.assign(new double[v.size()]);    expected = lengthSquaredSlowly(v);    assertEquals("mutation via assign(double[]) fails to change lengthSquared", expected, v.getLengthSquared(), EPSILON);    v.getElement(v.size() / 2).set(2.5);    expected = lengthSquaredSlowly(v);    assertEquals("mutation via v.getElement().set() fails to change lengthSquared", expected, v.getLengthSquared(), EPSILON);    v.normalize();    expected = lengthSquaredSlowly(v);    assertEquals("mutation via normalize() fails to change lengthSquared", expected, v.getLengthSquared(), EPSILON);    v.set(0, 1.5);    v.normalize(1.0);    expected = lengthSquaredSlowly(v);    assertEquals("mutation via normalize(double) fails to change lengthSquared", expected, v.getLengthSquared(), EPSILON);    v.times(2.0);    expected = lengthSquaredSlowly(v);    assertEquals("mutation via times(double) fails to change lengthSquared", expected, v.getLengthSquared(), EPSILON);    v.times(v);    expected = lengthSquaredSlowly(v);    assertEquals("mutation via times(vector) fails to change lengthSquared", expected, v.getLengthSquared(), EPSILON);    v.assign(Functions.POW, 3.0);    expected = lengthSquaredSlowly(v);    assertEquals("mutation via assign(pow, 3.0) fails to change lengthSquared", expected, v.getLengthSquared(), EPSILON);    v.assign(v, Functions.PLUS);    expected = lengthSquaredSlowly(v);    assertEquals("mutation via assign(v,plus) fails to change lengthSquared", expected, v.getLengthSquared(), EPSILON);}
0
public void testIterator()
{    Collection<Integer> expectedIndices = Sets.newHashSet();    int i = 1;    while (i <= 20) {        expectedIndices.add(i * (i + 1) / 2);        i++;    }    Vector denseVector = new DenseVector(i * i);    for (int index : expectedIndices) {        denseVector.set(index, (double) 2 * index);    }    doTestIterators(denseVector, expectedIndices);    Vector randomAccessVector = new RandomAccessSparseVector(i * i);    for (int index : expectedIndices) {        randomAccessVector.set(index, (double) 2 * index);    }    doTestIterators(randomAccessVector, expectedIndices);    Vector sequentialVector = new SequentialAccessSparseVector(i * i);    for (int index : expectedIndices) {        sequentialVector.set(index, (double) 2 * index);    }    doTestIterators(sequentialVector, expectedIndices);}
0
private static void doTestIterators(Vector vector, Collection<Integer> expectedIndices)
{    expectedIndices = Sets.newHashSet(expectedIndices);    Iterator<Element> allIterator = vector.all().iterator();    int index = 0;    while (allIterator.hasNext()) {        Element element = allIterator.next();        assertEquals(index, element.index());        if (expectedIndices.contains(index)) {            assertEquals((double) index * 2, element.get(), EPSILON);        } else {            assertEquals(0.0, element.get(), EPSILON);        }        index++;    }    for (Element element : vector.nonZeroes()) {        index = element.index();        assertTrue(expectedIndices.contains(index));        assertEquals((double) index * 2, element.get(), EPSILON);        expectedIndices.remove(index);    }    assertTrue(expectedIndices.isEmpty());}
0
public void testNormalize()
{    Vector vec1 = new RandomAccessSparseVector(3);    vec1.setQuick(0, 1);    vec1.setQuick(1, 2);    vec1.setQuick(2, 3);    Vector norm = vec1.normalize();    assertNotNull("norm1 is null and it shouldn't be", norm);    Vector vec2 = new SequentialAccessSparseVector(3);    vec2.setQuick(0, 1);    vec2.setQuick(1, 2);    vec2.setQuick(2, 3);    Vector norm2 = vec2.normalize();    assertNotNull("norm1 is null and it shouldn't be", norm2);    Vector expected = new RandomAccessSparseVector(3);    expected.setQuick(0, 0.2672612419124244);    expected.setQuick(1, 0.5345224838248488);    expected.setQuick(2, 0.8017837257372732);    assertEquals(expected, norm);    norm = vec1.normalize(2);    assertEquals(expected, norm);    norm2 = vec2.normalize(2);    assertEquals(expected, norm2);    norm = vec1.normalize(1);    norm2 = vec2.normalize(1);    expected.setQuick(0, 1.0 / 6);    expected.setQuick(1, 2.0 / 6);    expected.setQuick(2, 3.0 / 6);    assertEquals(expected, norm);    assertEquals(expected, norm2);    norm = vec1.normalize(3);                double cube = Math.pow(36, 1.0 / 3);    expected = vec1.divide(cube);    assertEquals(norm, expected);    norm = vec1.normalize(Double.POSITIVE_INFINITY);    norm2 = vec2.normalize(Double.POSITIVE_INFINITY);        expected.setQuick(0, 1.0 / 3);    expected.setQuick(1, 2.0 / 3);    expected.setQuick(2, 3.0 / 3);    assertEquals(norm, expected);    assertEquals(norm2, expected);    norm = vec1.normalize(0);        expected.setQuick(0, 1.0 / 3);    expected.setQuick(1, 2.0 / 3);    expected.setQuick(2, 3.0 / 3);    assertEquals(norm, expected);    try {        vec1.normalize(-1);        fail();    } catch (IllegalArgumentException e) {        }    try {        vec2.normalize(-1);        fail();    } catch (IllegalArgumentException e) {        }}
0
public void testLogNormalize()
{    Vector vec1 = new RandomAccessSparseVector(3);    vec1.setQuick(0, 1);    vec1.setQuick(1, 2);    vec1.setQuick(2, 3);    Vector norm = vec1.logNormalize();    assertNotNull("norm1 is null and it shouldn't be", norm);    Vector vec2 = new SequentialAccessSparseVector(3);    vec2.setQuick(0, 1);    vec2.setQuick(1, 2);    vec2.setQuick(2, 3);    Vector norm2 = vec2.logNormalize();    assertNotNull("norm1 is null and it shouldn't be", norm2);    Vector expected = new DenseVector(new double[] { 0.2672612419124244, 0.4235990463273581, 0.5345224838248488 });    assertVectorEquals(expected, norm, 1.0e-15);    assertVectorEquals(expected, norm2, 1.0e-15);    norm = vec1.logNormalize(2);    assertVectorEquals(expected, norm, 1.0e-15);    norm2 = vec2.logNormalize(2);    assertVectorEquals(expected, norm2, 1.0e-15);    try {        vec1.logNormalize(1);        fail("Should fail with power == 1");    } catch (IllegalArgumentException e) {        }    try {        vec1.logNormalize(-1);        fail("Should fail with negative power");    } catch (IllegalArgumentException e) {        }    try {        vec2.logNormalize(Double.POSITIVE_INFINITY);        fail("Should fail with positive infinity norm");    } catch (IllegalArgumentException e) {        }}
0
private static void assertVectorEquals(Vector expected, Vector actual, double epsilon)
{    assertEquals(expected.size(), actual.size());    for (Element x : expected.all()) {        assertEquals(x.get(), actual.get(x.index()), epsilon);    }}
0
public void testMax()
{    Vector vec1 = new RandomAccessSparseVector(3);    vec1.setQuick(0, -1);    vec1.setQuick(1, -3);    vec1.setQuick(2, -2);    double max = vec1.maxValue();    assertEquals(-1.0, max, 0.0);    int idx = vec1.maxValueIndex();    assertEquals(0, idx);    vec1 = new RandomAccessSparseVector(3);    vec1.setQuick(0, -1);    vec1.setQuick(2, -2);    max = vec1.maxValue();    assertEquals(0.0, max, 0.0);    idx = vec1.maxValueIndex();    assertEquals(1, idx);    vec1 = new SequentialAccessSparseVector(3);    vec1.setQuick(0, -1);    vec1.setQuick(2, -2);    max = vec1.maxValue();    assertEquals(0.0, max, 0.0);    idx = vec1.maxValueIndex();    assertEquals(1, idx);    vec1 = new DenseVector(3);    vec1.setQuick(0, -1);    vec1.setQuick(2, -2);    max = vec1.maxValue();    assertEquals(0.0, max, 0.0);    idx = vec1.maxValueIndex();    assertEquals(1, idx);    vec1 = new RandomAccessSparseVector(3);    max = vec1.maxValue();    assertEquals(0.0, max, EPSILON);    vec1 = new DenseVector(3);    max = vec1.maxValue();    assertEquals(0.0, max, EPSILON);    vec1 = new SequentialAccessSparseVector(3);    max = vec1.maxValue();    assertEquals(0.0, max, EPSILON);    vec1 = new RandomAccessSparseVector(0);    max = vec1.maxValue();    assertEquals(Double.NEGATIVE_INFINITY, max, EPSILON);    vec1 = new DenseVector(0);    max = vec1.maxValue();    assertEquals(Double.NEGATIVE_INFINITY, max, EPSILON);    vec1 = new SequentialAccessSparseVector(0);    max = vec1.maxValue();    assertEquals(Double.NEGATIVE_INFINITY, max, EPSILON);}
0
public void testMin()
{    Vector vec1 = new RandomAccessSparseVector(3);    vec1.setQuick(0, 1);    vec1.setQuick(1, 3);    vec1.setQuick(2, 2);    double max = vec1.minValue();    assertEquals(1.0, max, 0.0);    int idx = vec1.maxValueIndex();    assertEquals(1, idx);    vec1 = new RandomAccessSparseVector(3);    vec1.setQuick(0, -1);    vec1.setQuick(2, -2);    max = vec1.maxValue();    assertEquals(0.0, max, 0.0);    idx = vec1.maxValueIndex();    assertEquals(1, idx);    vec1 = new SequentialAccessSparseVector(3);    vec1.setQuick(0, -1);    vec1.setQuick(2, -2);    max = vec1.maxValue();    assertEquals(0.0, max, 0.0);    idx = vec1.maxValueIndex();    assertEquals(1, idx);    vec1 = new DenseVector(3);    vec1.setQuick(0, -1);    vec1.setQuick(2, -2);    max = vec1.maxValue();    assertEquals(0.0, max, 0.0);    idx = vec1.maxValueIndex();    assertEquals(1, idx);    vec1 = new RandomAccessSparseVector(3);    max = vec1.maxValue();    assertEquals(0.0, max, EPSILON);    vec1 = new DenseVector(3);    max = vec1.maxValue();    assertEquals(0.0, max, EPSILON);    vec1 = new SequentialAccessSparseVector(3);    max = vec1.maxValue();    assertEquals(0.0, max, EPSILON);    vec1 = new RandomAccessSparseVector(0);    max = vec1.maxValue();    assertEquals(Double.NEGATIVE_INFINITY, max, EPSILON);    vec1 = new DenseVector(0);    max = vec1.maxValue();    assertEquals(Double.NEGATIVE_INFINITY, max, EPSILON);    vec1 = new SequentialAccessSparseVector(0);    max = vec1.maxValue();    assertEquals(Double.NEGATIVE_INFINITY, max, EPSILON);}
0
public void testDenseVector()
{    Vector vec1 = new DenseVector(3);    Vector vec2 = new DenseVector(3);    doTestVectors(vec1, vec2);}
0
public void testVectorView()
{    RandomAccessSparseVector vec1 = new RandomAccessSparseVector(3);    RandomAccessSparseVector vec2 = new RandomAccessSparseVector(6);    SequentialAccessSparseVector vec3 = new SequentialAccessSparseVector(3);    SequentialAccessSparseVector vec4 = new SequentialAccessSparseVector(6);    Vector vecV1 = new VectorView(vec1, 0, 3);    Vector vecV2 = new VectorView(vec2, 2, 3);    Vector vecV3 = new VectorView(vec3, 0, 3);    Vector vecV4 = new VectorView(vec4, 2, 3);    doTestVectors(vecV1, vecV2);    doTestVectors(vecV3, vecV4);}
0
private static void doTestEnumeration(double[] apriori, Vector vector)
{    double[] test = new double[apriori.length];    for (Element e : vector.all()) {        test[e.index()] = e.get();    }    for (int i = 0; i < test.length; i++) {        assertEquals(apriori[i], test[i], EPSILON);    }}
0
public void testEnumeration()
{    double[] apriori = { 0, 1, 2, 3, 4 };    doTestEnumeration(apriori, new VectorView(new DenseVector(new double[] { -2, -1, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9 }), 2, 5));    doTestEnumeration(apriori, new DenseVector(new double[] { 0, 1, 2, 3, 4 }));    Vector sparse = new RandomAccessSparseVector(5);    sparse.set(0, 0);    sparse.set(1, 1);    sparse.set(2, 2);    sparse.set(3, 3);    sparse.set(4, 4);    doTestEnumeration(apriori, sparse);    sparse = new SequentialAccessSparseVector(5);    sparse.set(0, 0);    sparse.set(1, 1);    sparse.set(2, 2);    sparse.set(3, 3);    sparse.set(4, 4);    doTestEnumeration(apriori, sparse);}
0
public void testAggregation()
{    Vector v = new DenseVector(5);    Vector w = new DenseVector(5);    setUpFirstVector(v);    setUpSecondVector(w);    doTestAggregation(v, w);    v = new RandomAccessSparseVector(5);    w = new RandomAccessSparseVector(5);    setUpFirstVector(v);    doTestAggregation(v, w);    setUpSecondVector(w);    doTestAggregation(w, v);    v = new SequentialAccessSparseVector(5);    w = new SequentialAccessSparseVector(5);    setUpFirstVector(v);    doTestAggregation(v, w);    setUpSecondVector(w);    doTestAggregation(w, v);}
0
private static void doTestAggregation(Vector v, Vector w)
{    assertEquals("aggregate(plus, pow(2)) not equal to " + v.getLengthSquared(), v.getLengthSquared(), v.aggregate(Functions.PLUS, Functions.pow(2)), EPSILON);    assertEquals("aggregate(plus, abs) not equal to " + v.norm(1), v.norm(1), v.aggregate(Functions.PLUS, Functions.ABS), EPSILON);    assertEquals("aggregate(max, abs) not equal to " + v.norm(Double.POSITIVE_INFINITY), v.norm(Double.POSITIVE_INFINITY), v.aggregate(Functions.MAX, Functions.ABS), EPSILON);    assertEquals("v.dot(w) != v.aggregate(w, plus, mult)", v.dot(w), v.aggregate(w, Functions.PLUS, Functions.MULT), EPSILON);    assertEquals("|(v-w)|^2 != v.aggregate(w, plus, chain(pow(2), minus))", v.minus(w).dot(v.minus(w)), v.aggregate(w, Functions.PLUS, Functions.chain(Functions.pow(2), Functions.MINUS)), EPSILON);}
0
public void testEmptyAggregate1()
{    assertEquals(1.0, new DenseVector(new double[] { 1 }).aggregate(Functions.MIN, Functions.IDENTITY), EPSILON);    assertEquals(1.0, new DenseVector(new double[] { 2, 1 }).aggregate(Functions.MIN, Functions.IDENTITY), EPSILON);    assertEquals(0, new DenseVector(new double[0]).aggregate(Functions.MIN, Functions.IDENTITY), 0);}
0
public void testEmptyAggregate2()
{    assertEquals(3.0, new DenseVector(new double[] { 1 }).aggregate(new DenseVector(new double[] { 2 }), Functions.MIN, Functions.PLUS), EPSILON);    assertEquals(0, new DenseVector(new double[0]).aggregate(new DenseVector(new double[0]), Functions.MIN, Functions.PLUS), 0);}
0
private static void setUpFirstVector(Vector v)
{    v.setQuick(1, 2);    v.setQuick(2, 0.5);    v.setQuick(3, -5);}
0
private static void setUpSecondVector(Vector v)
{    v.setQuick(0, 3);    v.setQuick(1, -1.5);    v.setQuick(2, -5);    v.setQuick(3, 2);}
0
public void testHashCodeEquivalence()
{        Vector sparseLeft = new RandomAccessSparseVector(3);    Vector denseRight = new DenseVector(3);    sparseLeft.setQuick(0, 1);    sparseLeft.setQuick(1, 2);    sparseLeft.setQuick(2, 3);    denseRight.setQuick(0, 1);    denseRight.setQuick(1, 2);    denseRight.setQuick(2, 3);    assertEquals(sparseLeft, denseRight);    assertEquals(sparseLeft.hashCode(), denseRight.hashCode());    sparseLeft = new SequentialAccessSparseVector(3);    sparseLeft.setQuick(0, 1);    sparseLeft.setQuick(1, 2);    sparseLeft.setQuick(2, 3);    assertEquals(sparseLeft, denseRight);    assertEquals(sparseLeft.hashCode(), denseRight.hashCode());    Vector denseLeft = new DenseVector(3);    denseLeft.setQuick(0, 1);    denseLeft.setQuick(1, 2);    denseLeft.setQuick(2, 3);    assertEquals(denseLeft, denseRight);    assertEquals(denseLeft.hashCode(), denseRight.hashCode());    Vector sparseRight = new SequentialAccessSparseVector(3);    sparseRight.setQuick(0, 1);    sparseRight.setQuick(1, 2);    sparseRight.setQuick(2, 3);    assertEquals(sparseLeft, sparseRight);    assertEquals(sparseLeft.hashCode(), sparseRight.hashCode());    DenseVector emptyLeft = new DenseVector(0);    Vector emptyRight = new SequentialAccessSparseVector(0);    assertEquals(emptyLeft, emptyRight);    assertEquals(emptyLeft.hashCode(), emptyRight.hashCode());    emptyRight = new RandomAccessSparseVector(0);    assertEquals(emptyLeft, emptyRight);    assertEquals(emptyLeft.hashCode(), emptyRight.hashCode());}
0
public void testHashCode()
{        Vector left = new SequentialAccessSparseVector(3);    Vector right = new SequentialAccessSparseVector(3);    left.setQuick(0, 1);    left.setQuick(2, 2);    right.setQuick(0, 1);    right.setQuick(1, 2);    assertFalse(left.equals(right));    assertFalse(left.hashCode() == right.hashCode());    left = new RandomAccessSparseVector(3);    right = new RandomAccessSparseVector(3);    left.setQuick(0, 1);    left.setQuick(2, 2);    right.setQuick(0, 1);    right.setQuick(1, 2);    assertFalse(left.equals(right));    assertFalse(left.hashCode() == right.hashCode());        right = new SequentialAccessSparseVector(5);    right.setQuick(0, 1);    right.setQuick(2, 2);    assertFalse(left.equals(right));    assertFalse(left.hashCode() == right.hashCode());    right = new RandomAccessSparseVector(5);    right.setQuick(0, 1);    right.setQuick(2, 2);    assertFalse(left.equals(right));    assertFalse(left.hashCode() == right.hashCode());}
0
public void testIteratorRasv()
{    testIterator(new RandomAccessSparseVector(99));    testEmptyAllIterator(new RandomAccessSparseVector(0));    testExample1NonZeroIterator(new RandomAccessSparseVector(13));}
0
public void testIteratorSasv()
{    testIterator(new SequentialAccessSparseVector(99));    testEmptyAllIterator(new SequentialAccessSparseVector(0));    testExample1NonZeroIterator(new SequentialAccessSparseVector(13));}
0
public void testIteratorDense()
{    testIterator(new DenseVector(99));    testEmptyAllIterator(new DenseVector(0));    testExample1NonZeroIterator(new DenseVector(13));}
0
private static void testIterator(Vector vector)
{    testSkips(vector.like());    testSkipsLast(vector.like());    testEmptyNonZeroIterator(vector.like());    testSingleNonZeroIterator(vector.like());}
0
private static void testSkips(Vector vector)
{    vector.set(0, 1);    vector.set(2, 2);    vector.set(4, 3);    vector.set(6, 4);        Iterator<Element> it = vector.nonZeroes().iterator();    Element element = null;    int i = 0;    HashSet<Integer> indexes = new HashSet<>();    while (it.hasNext()) {                if (i % 2 == 0) {            element = it.next();            indexes.add(element.index());        }                assertEquals(element.get(), vector.get(element.index()), 0);        ++i;    }        assertEquals(7, i);    assertEquals(4, indexes.size());    assertTrue(indexes.contains(0));    assertTrue(indexes.contains(2));    assertTrue(indexes.contains(4));    assertTrue(indexes.contains(6));        it = vector.all().iterator();    element = null;    i = 0;    while (it.hasNext()) {                if (i % 2 == 0) {            element = it.next();        }                assertEquals(element.index(), i / 2);        assertEquals(element.get(), vector.get(i / 2), 0);        ++i;    }        assertEquals(197, i);}
0
private static void testSkipsLast(Vector vector)
{    vector.set(1, 6);    vector.set(98, 6);        Iterator<Element> it = vector.nonZeroes().iterator();    int i = 0;    while (it.hasNext()) {                it.next();        ++i;    }        assertEquals(2, i);        it = vector.all().iterator();    i = 0;    while (it.hasNext()) {                Element element = it.next();        assertEquals(i, element.index());        ++i;    }    assertFalse(it.hasNext());        assertEquals(99, i);}
0
private static void testEmptyNonZeroIterator(Vector vector)
{        Iterator<Element> it = vector.nonZeroes().iterator();    int i = 0;    while (it.hasNext()) {        ++i;    }    assertEquals(0, i);    it = vector.nonZeroes().iterator();    assertFalse(it.hasNext());    try {        it.next();        fail();    } catch (NoSuchElementException e) {        }}
0
private static void testEmptyAllIterator(Vector vector)
{        Iterator<Element> it = vector.all().iterator();    int i = 0;    while (it.hasNext()) {        ++i;    }    assertEquals(0, i);    it = vector.nonZeroes().iterator();    assertFalse(it.hasNext());    try {        it.next();        fail();    } catch (NoSuchElementException e) {        }    it = vector.all().iterator();    assertFalse(it.hasNext());    try {        it.next();        fail();    } catch (NoSuchElementException e) {        }}
0
public void testNumNonZerosDense()
{    DenseVector vector = new DenseVector(10);    vector.assign(1);    vector.setQuick(3, 0);    vector.set(5, 0);    assertEquals(8, vector.getNumNonZeroElements());}
0
public void testNumNonZerosRandomAccessSparse()
{    RandomAccessSparseVector vector = new RandomAccessSparseVector(10);    vector.setQuick(3, 1);    vector.set(5, 1);    vector.setQuick(7, 0);    vector.set(9, 0);    assertEquals(2, vector.getNumNonZeroElements());}
0
public void testNumNonZerosSequentialAccessSparse()
{    SequentialAccessSparseVector vector = new SequentialAccessSparseVector(10);    vector.setQuick(3, 1);    vector.set(5, 1);    vector.setQuick(7, 0);    vector.set(9, 0);    assertEquals(2, vector.getNumNonZeroElements());}
0
private static void testSingleNonZeroIterator(Vector vector)
{    vector.set(1, 6);        Iterator<Element> it = vector.nonZeroes().iterator();    for (int i = 0; i < 10; ++i) {        assertTrue(it.hasNext());    }    it = vector.nonZeroes().iterator();    it.next();    for (int i = 0; i < 10; ++i) {        assertFalse(it.hasNext());    }    try {        it.next();        fail();    } catch (NoSuchElementException e) {        }}
0
private static void testExample1NonZeroIterator(Vector vector)
{    double[] val = { 0, 2, 0, 0, 8, 3, 0, 6, 0, 1, 1, 2, 1 };    for (int i = 0; i < val.length; ++i) {        vector.set(i, val[i]);    }    Set<Integer> expected = Sets.newHashSet(1, 4, 5, 7, 9, 10, 11, 12);    Set<Double> expectedValue = Sets.newHashSet(2.0, 8.0, 3.0, 6.0, 1.0);        Iterator<Element> it = vector.nonZeroes().iterator();    int i = 0;    while (it.hasNext()) {        Element e = it.next();        assertTrue(expected.contains(e.index()));        assertTrue(expectedValue.contains(e.get()));        ++i;    }    assertEquals(8, i);        assertEquals(8, vector.getNumNonZeroElements());        it = vector.nonZeroes().iterator();    i = 0;    while (it.hasNext()) {        Element e = it.next();        if (e.index() == 5) {            e.set(0.0);        }        ++i;    }    assertEquals(8, i);    assertEquals(7, vector.getNumNonZeroElements());        it = vector.nonZeroes().iterator();    i = 0;    while (it.hasNext()) {        Element e = it.next();        if (e.index() == 5) {            vector.set(5, 0.0);        }        ++i;    }        assertEquals(7, i);        assertEquals(7, vector.getNumNonZeroElements());}
0
public void testLength()
{    Vector v = new DenseVector(new double[] { 0.9921337470551008, 1.0031004325833064, 0.9963963182745947 });    Centroid c = new Centroid(3, new DenseVector(v), 2);    assertEquals(c.getVector().getLengthSquared(), c.getLengthSquared(), 1.0e-17);        c.set(0, -1);    System.out.printf("c = %.9f\nv = %.9f\n", c.getLengthSquared(), c.getVector().getLengthSquared());    assertEquals(c.getVector().getLengthSquared(), c.getLengthSquared(), 1.0e-17);}
0
public Vector vectorToTest(int size)
{    return new WeightedVector(new DenseVector(size), 4.52, 345);}
0
public void testOrdering()
{    WeightedVector v1 = new WeightedVector(new DenseVector(new double[] { 1, 2, 3 }), 5.41, 31);    WeightedVector v2 = new WeightedVector(new DenseVector(new double[] { 1, 2, 3 }), 5.00, 31);    WeightedVector v3 = new WeightedVector(new DenseVector(new double[] { 1, 3, 3 }), 5.00, 31);    WeightedVector v4 = v1.clone();    WeightedVectorComparator comparator = new WeightedVectorComparator();    assertTrue(comparator.compare(v1, v2) > 0);    assertTrue(comparator.compare(v3, v1) < 0);    assertTrue(comparator.compare(v3, v2) > 0);    assertEquals(0, comparator.compare(v4, v1));    assertEquals(0, comparator.compare(v1, v1));}
0
public void testProjection()
{    Vector v1 = new DenseVector(10).assign(Functions.random());    WeightedVector v2 = new WeightedVector(v1, v1, 31);    assertEquals(v1.dot(v1), v2.getWeight(), 1.0e-13);    assertEquals(31, v2.getIndex());    Matrix y = new DenseMatrix(10, 4).assign(Functions.random());    Matrix q = new QRDecomposition(y.viewPart(0, 10, 0, 3)).getQ();    Vector nullSpace = y.viewColumn(3).minus(q.times(q.transpose().times(y.viewColumn(3))));    WeightedVector v3 = new WeightedVector(q.viewColumn(0).plus(q.viewColumn(1)), nullSpace, 1);    assertEquals(0, v3.getWeight(), 1.0e-13);    Vector qx = q.viewColumn(0).plus(q.viewColumn(1)).normalize();    WeightedVector v4 = new WeightedVector(qx, q.viewColumn(0), 2);    assertEquals(Math.sqrt(0.5), v4.getWeight(), 1.0e-13);    WeightedVector v5 = WeightedVector.project(q.viewColumn(0), qx);    assertEquals(Math.sqrt(0.5), v5.getWeight(), 1.0e-13);}
0
public void testSize()
{    assertEquals("size", 3, getTestVector().getNumNonZeroElements());}
0
 Vector generateTestVector(int cardinality)
{    return new WeightedVector(new DenseVector(cardinality), 3.14, 53);}
0
public static void quietClose(ResultSet resultSet, Statement statement, Connection connection)
{    quietClose(resultSet);    quietClose(statement);    quietClose(connection);}
0
public static void close(Collection<? extends Closeable> closeables) throws IOException
{    Throwable lastThr = null;    for (Closeable closeable : closeables) {        try {            closeable.close();        } catch (Throwable thr) {                        lastThr = thr;        }    }            closeables.clear();    if (lastThr != null) {        if (lastThr instanceof IOException) {            throw (IOException) lastThr;        } else if (lastThr instanceof RuntimeException) {            throw (RuntimeException) lastThr;        } else {            throw (Error) lastThr;        }    }}
1
public void close() throws IOException
{    if (file.isFile()) {        file.delete();    }}
0
public void close() throws IOException
{    if (mo != null) {        mo.close();    }}
0
public Matrix get()
{    return matrix;}
0
public void set(Matrix matrix)
{    this.matrix = matrix;}
0
public void write(DataOutput out) throws IOException
{    writeMatrix(out, matrix);}
0
public void readFields(DataInput in) throws IOException
{    matrix = readMatrix(in);}
0
public static void readLabels(DataInput in, Map<String, Integer> columnLabelBindings, Map<String, Integer> rowLabelBindings) throws IOException
{    int colSize = in.readInt();    if (colSize > 0) {        for (int i = 0; i < colSize; i++) {            columnLabelBindings.put(in.readUTF(), in.readInt());        }    }    int rowSize = in.readInt();    if (rowSize > 0) {        for (int i = 0; i < rowSize; i++) {            rowLabelBindings.put(in.readUTF(), in.readInt());        }    }}
0
public static void writeLabelBindings(DataOutput out, Map<String, Integer> columnLabelBindings, Map<String, Integer> rowLabelBindings) throws IOException
{    if (columnLabelBindings == null) {        out.writeInt(0);    } else {        out.writeInt(columnLabelBindings.size());        for (Map.Entry<String, Integer> stringIntegerEntry : columnLabelBindings.entrySet()) {            out.writeUTF(stringIntegerEntry.getKey());            out.writeInt(stringIntegerEntry.getValue());        }    }    if (rowLabelBindings == null) {        out.writeInt(0);    } else {        out.writeInt(rowLabelBindings.size());        for (Map.Entry<String, Integer> stringIntegerEntry : rowLabelBindings.entrySet()) {            out.writeUTF(stringIntegerEntry.getKey());            out.writeInt(stringIntegerEntry.getValue());        }    }}
0
public static Matrix readMatrix(DataInput in) throws IOException
{    int flags = in.readInt();    Preconditions.checkArgument(flags >> NUM_FLAGS == 0, "Unknown flags set: %d", Integer.toString(flags, 2));    boolean dense = (flags & FLAG_DENSE) != 0;    boolean sequential = (flags & FLAG_SEQUENTIAL) != 0;    boolean hasLabels = (flags & FLAG_LABELS) != 0;    boolean isSparseRowMatrix = (flags & FLAG_SPARSE_ROW) != 0;    int rows = in.readInt();    int columns = in.readInt();    byte vectorFlags = in.readByte();    Matrix matrix;    if (dense) {        matrix = new DenseMatrix(rows, columns);        for (int row = 0; row < rows; row++) {            matrix.assignRow(row, VectorWritable.readVector(in, vectorFlags, columns));        }    } else if (isSparseRowMatrix) {        Vector[] rowVectors = new Vector[rows];        for (int row = 0; row < rows; row++) {            rowVectors[row] = VectorWritable.readVector(in, vectorFlags, columns);        }        matrix = new SparseRowMatrix(rows, columns, rowVectors, true, !sequential);    } else {        matrix = new SparseMatrix(rows, columns);        int numNonZeroRows = in.readInt();        int rowsRead = 0;        while (rowsRead++ < numNonZeroRows) {            int rowIndex = in.readInt();            matrix.assignRow(rowIndex, VectorWritable.readVector(in, vectorFlags, columns));        }    }    if (hasLabels) {        Map<String, Integer> columnLabelBindings = new HashMap<>();        Map<String, Integer> rowLabelBindings = new HashMap<>();        readLabels(in, columnLabelBindings, rowLabelBindings);        if (!columnLabelBindings.isEmpty()) {            matrix.setColumnLabelBindings(columnLabelBindings);        }        if (!rowLabelBindings.isEmpty()) {            matrix.setRowLabelBindings(rowLabelBindings);        }    }    return matrix;}
0
public static void writeMatrix(final DataOutput out, Matrix matrix) throws IOException
{    int flags = 0;    Vector row = matrix.viewRow(0);    boolean isDense = row.isDense();    if (isDense) {        flags |= FLAG_DENSE;    }    if (row.isSequentialAccess()) {        flags |= FLAG_SEQUENTIAL;    }    if (matrix.getRowLabelBindings() != null || matrix.getColumnLabelBindings() != null) {        flags |= FLAG_LABELS;    }    boolean isSparseRowMatrix = matrix instanceof SparseRowMatrix;    if (isSparseRowMatrix) {        flags |= FLAG_SPARSE_ROW;    }    out.writeInt(flags);    out.writeInt(matrix.rowSize());    out.writeInt(matrix.columnSize());        byte vectorFlags = VectorWritable.flags(matrix.viewRow(0), false);    out.writeByte(vectorFlags);    if (isDense || isSparseRowMatrix) {        for (int i = 0; i < matrix.rowSize(); i++) {            VectorWritable.writeVectorContents(out, matrix.viewRow(i), vectorFlags);        }    } else {        IntArrayList rowIndices = ((SparseMatrix) matrix).nonZeroRowIndices();        int numNonZeroRows = rowIndices.size();        out.writeInt(numNonZeroRows);        for (int i = 0; i < numNonZeroRows; i++) {            int rowIndex = rowIndices.getQuick(i);            out.writeInt(rowIndex);            VectorWritable.writeVectorContents(out, matrix.viewRow(rowIndex), vectorFlags);        }    }    if ((flags & FLAG_LABELS) != 0) {        writeLabelBindings(out, matrix.getColumnLabelBindings(), matrix.getRowLabelBindings());    }}
0
public static void writeSignedVarLong(long value, DataOutput out) throws IOException
{        writeUnsignedVarLong((value << 1) ^ (value >> 63), out);}
0
public static void writeUnsignedVarLong(long value, DataOutput out) throws IOException
{    while ((value & 0xFFFFFFFFFFFFFF80L) != 0L) {        out.writeByte(((int) value & 0x7F) | 0x80);        value >>>= 7;    }    out.writeByte((int) value & 0x7F);}
0
public static void writeSignedVarInt(int value, DataOutput out) throws IOException
{        writeUnsignedVarInt((value << 1) ^ (value >> 31), out);}
0
public static void writeUnsignedVarInt(int value, DataOutput out) throws IOException
{    while ((value & 0xFFFFFF80) != 0L) {        out.writeByte((value & 0x7F) | 0x80);        value >>>= 7;    }    out.writeByte(value & 0x7F);}
0
public static long readSignedVarLong(DataInput in) throws IOException
{    long raw = readUnsignedVarLong(in);        long temp = (((raw << 63) >> 63) ^ raw) >> 1;        return temp ^ (raw & (1L << 63));}
0
public static long readUnsignedVarLong(DataInput in) throws IOException
{    long value = 0L;    int i = 0;    long b;    while (((b = in.readByte()) & 0x80L) != 0) {        value |= (b & 0x7F) << i;        i += 7;        Preconditions.checkArgument(i <= 63, "Variable length quantity is too long (must be <= 63)");    }    return value | (b << i);}
0
public static int readSignedVarInt(DataInput in) throws IOException
{    int raw = readUnsignedVarInt(in);        int temp = (((raw << 31) >> 31) ^ raw) >> 1;        return temp ^ (raw & (1 << 31));}
0
public static int readUnsignedVarInt(DataInput in) throws IOException
{    int value = 0;    int i = 0;    int b;    while (((b = in.readByte()) & 0x80) != 0) {        value |= (b & 0x7F) << i;        i += 7;        Preconditions.checkArgument(i <= 35, "Variable length quantity is too long (must be <= 35)");    }    return value | (b << i);}
0
public int get()
{    return value;}
0
public void set(int value)
{    this.value = value;}
0
public boolean equals(Object other)
{    return other instanceof VarIntWritable && ((VarIntWritable) other).value == value;}
0
public int hashCode()
{    return value;}
0
public String toString()
{    return String.valueOf(value);}
0
public VarIntWritable clone()
{    return new VarIntWritable(value);}
0
public int compareTo(VarIntWritable other)
{    if (value < other.value) {        return -1;    }    if (value > other.value) {        return 1;    }    return 0;}
0
public void write(DataOutput out) throws IOException
{    Varint.writeSignedVarInt(value, out);}
0
public void readFields(DataInput in) throws IOException
{    value = Varint.readSignedVarInt(in);}
0
public long get()
{    return value;}
0
public void set(long value)
{    this.value = value;}
0
public boolean equals(Object other)
{    return other != null && getClass().equals(other.getClass()) && ((VarLongWritable) other).value == value;}
0
public int hashCode()
{    return Longs.hashCode(value);}
0
public String toString()
{    return String.valueOf(value);}
0
public int compareTo(VarLongWritable other)
{    if (value >= other.value) {        if (value > other.value) {            return 1;        }    } else {        return -1;    }    return 0;}
0
public void write(DataOutput out) throws IOException
{    Varint.writeSignedVarLong(value, out);}
0
public void readFields(DataInput in) throws IOException
{    value = Varint.readSignedVarLong(in);}
0
public Vector get()
{    return vector;}
0
public void set(Vector vector)
{    this.vector = vector;}
0
public boolean isWritesLaxPrecision()
{    return writesLaxPrecision;}
0
public void setWritesLaxPrecision(boolean writesLaxPrecision)
{    this.writesLaxPrecision = writesLaxPrecision;}
0
public void write(DataOutput out) throws IOException
{    writeVector(out, this.vector, this.writesLaxPrecision);}
0
public void readFields(DataInput in) throws IOException
{    int flags = in.readByte();    int size = Varint.readUnsignedVarInt(in);    readFields(in, (byte) flags, size);}
0
private void readFields(DataInput in, byte flags, int size) throws IOException
{    Preconditions.checkArgument(flags >> NUM_FLAGS == 0, "Unknown flags set: %d", Integer.toString(flags, 2));    boolean dense = (flags & FLAG_DENSE) != 0;    boolean sequential = (flags & FLAG_SEQUENTIAL) != 0;    boolean named = (flags & FLAG_NAMED) != 0;    boolean laxPrecision = (flags & FLAG_LAX_PRECISION) != 0;    Vector v;    if (dense) {        double[] values = new double[size];        for (int i = 0; i < size; i++) {            values[i] = laxPrecision ? in.readFloat() : in.readDouble();        }        v = new DenseVector(values);    } else {        int numNonDefaultElements = Varint.readUnsignedVarInt(in);        v = sequential ? new SequentialAccessSparseVector(size, numNonDefaultElements) : new RandomAccessSparseVector(size, numNonDefaultElements);        if (sequential) {            int lastIndex = 0;            for (int i = 0; i < numNonDefaultElements; i++) {                int delta = Varint.readUnsignedVarInt(in);                int index = lastIndex + delta;                lastIndex = index;                double value = laxPrecision ? in.readFloat() : in.readDouble();                v.setQuick(index, value);            }        } else {            for (int i = 0; i < numNonDefaultElements; i++) {                int index = Varint.readUnsignedVarInt(in);                double value = laxPrecision ? in.readFloat() : in.readDouble();                v.setQuick(index, value);            }        }    }    if (named) {        String name = in.readUTF();        v = new NamedVector(v, name);    }    vector = v;}
0
public static void writeVector(DataOutput out, Vector vector) throws IOException
{    writeVector(out, vector, false);}
0
public static byte flags(Vector vector, boolean laxPrecision)
{    boolean dense = vector.isDense();    boolean sequential = vector.isSequentialAccess();    boolean named = vector instanceof NamedVector;    return (byte) ((dense ? FLAG_DENSE : 0) | (sequential ? FLAG_SEQUENTIAL : 0) | (named ? FLAG_NAMED : 0) | (laxPrecision ? FLAG_LAX_PRECISION : 0));}
0
public static void writeVectorFlagsAndSize(DataOutput out, byte flags, int size) throws IOException
{    out.writeByte(flags);    Varint.writeUnsignedVarInt(size, out);}
0
public static void writeVector(DataOutput out, Vector vector, boolean laxPrecision) throws IOException
{    byte flags = flags(vector, laxPrecision);    writeVectorFlagsAndSize(out, flags, vector.size());    writeVectorContents(out, vector, flags);}
0
public static void writeVectorContents(DataOutput out, Vector vector, byte flags) throws IOException
{    boolean dense = (flags & FLAG_DENSE) != 0;    boolean sequential = (flags & FLAG_SEQUENTIAL) != 0;    boolean named = (flags & FLAG_NAMED) != 0;    boolean laxPrecision = (flags & FLAG_LAX_PRECISION) != 0;    if (dense) {        for (Element element : vector.all()) {            if (laxPrecision) {                out.writeFloat((float) element.get());            } else {                out.writeDouble(element.get());            }        }    } else {        Varint.writeUnsignedVarInt(vector.getNumNonZeroElements(), out);        Iterator<Element> iter = vector.nonZeroes().iterator();        if (sequential) {            int lastIndex = 0;            while (iter.hasNext()) {                Element element = iter.next();                if (element.get() == 0) {                    continue;                }                int thisIndex = element.index();                                Varint.writeUnsignedVarInt(thisIndex - lastIndex, out);                lastIndex = thisIndex;                if (laxPrecision) {                    out.writeFloat((float) element.get());                } else {                    out.writeDouble(element.get());                }            }        } else {            while (iter.hasNext()) {                Element element = iter.next();                if (element.get() == 0) {                                        continue;                }                Varint.writeUnsignedVarInt(element.index(), out);                if (laxPrecision) {                    out.writeFloat((float) element.get());                } else {                    out.writeDouble(element.get());                }            }        }    }    if (named) {        String name = ((NamedVector) vector).getName();        out.writeUTF(name == null ? "" : name);    }}
0
public static Vector readVector(DataInput in) throws IOException
{    VectorWritable v = new VectorWritable();    v.readFields(in);    return v.get();}
0
public static Vector readVector(DataInput in, byte vectorFlags, int size) throws IOException
{    VectorWritable v = new VectorWritable();    v.readFields(in, vectorFlags, size);    return v.get();}
0
public static VectorWritable merge(Iterator<VectorWritable> vectors)
{    return new VectorWritable(mergeToVector(vectors));}
0
public static Vector mergeToVector(Iterator<VectorWritable> vectors)
{    Vector accumulator = vectors.next().get();    while (vectors.hasNext()) {        VectorWritable v = vectors.next();        if (v != null) {            for (Element nonZeroElement : v.get().nonZeroes()) {                accumulator.setQuick(nonZeroElement.index(), nonZeroElement.get());            }        }    }    return accumulator;}
0
public boolean equals(Object o)
{    return o instanceof VectorWritable && vector.equals(((VectorWritable) o).get());}
0
public int hashCode()
{    return vector.hashCode();}
0
public String toString()
{    return vector.toString();}
0
public void testSparseMatrixWritable() throws Exception
{    Matrix m = new SparseMatrix(5, 5);    m.set(1, 2, 3.0);    m.set(3, 4, 5.0);    Map<String, Integer> bindings = new HashMap<>();    bindings.put("A", 0);    bindings.put("B", 1);    bindings.put("C", 2);    bindings.put("D", 3);    bindings.put("default", 4);    m.setRowLabelBindings(bindings);    m.setColumnLabelBindings(bindings);    doTestMatrixWritableEquals(m);}
0
public void testSparseRowMatrixWritable() throws Exception
{    Matrix m = new SparseRowMatrix(5, 5);    m.set(1, 2, 3.0);    m.set(3, 4, 5.0);    Map<String, Integer> bindings = new HashMap<>();    bindings.put("A", 0);    bindings.put("B", 1);    bindings.put("C", 2);    bindings.put("D", 3);    bindings.put("default", 4);    m.setRowLabelBindings(bindings);    m.setColumnLabelBindings(bindings);    doTestMatrixWritableEquals(m);}
0
public void testDenseMatrixWritable() throws Exception
{    Matrix m = new DenseMatrix(5, 5);    m.set(1, 2, 3.0);    m.set(3, 4, 5.0);    Map<String, Integer> bindings = new HashMap<>();    bindings.put("A", 0);    bindings.put("B", 1);    bindings.put("C", 2);    bindings.put("D", 3);    bindings.put("default", 4);    m.setRowLabelBindings(bindings);    m.setColumnLabelBindings(bindings);    doTestMatrixWritableEquals(m);}
0
private static void doTestMatrixWritableEquals(Matrix m) throws IOException
{    Writable matrixWritable = new MatrixWritable(m);    MatrixWritable matrixWritable2 = new MatrixWritable();    writeAndRead(matrixWritable, matrixWritable2);    Matrix m2 = matrixWritable2.get();    compareMatrices(m, m2);    doCheckBindings(m2.getRowLabelBindings());    doCheckBindings(m2.getColumnLabelBindings());}
0
private static void compareMatrices(Matrix m, Matrix m2)
{    assertEquals(m.numRows(), m2.numRows());    assertEquals(m.numCols(), m2.numCols());    for (int r = 0; r < m.numRows(); r++) {        for (int c = 0; c < m.numCols(); c++) {            assertEquals(m.get(r, c), m2.get(r, c), EPSILON);        }    }    Map<String, Integer> bindings = m.getRowLabelBindings();    Map<String, Integer> bindings2 = m2.getRowLabelBindings();    assertEquals(bindings == null, bindings2 == null);    if (bindings != null) {        assertEquals(bindings.size(), m.numRows());        assertEquals(bindings.size(), bindings2.size());        for (Map.Entry<String, Integer> entry : bindings.entrySet()) {            assertEquals(entry.getValue(), bindings2.get(entry.getKey()));        }    }    bindings = m.getColumnLabelBindings();    bindings2 = m2.getColumnLabelBindings();    assertEquals(bindings == null, bindings2 == null);    if (bindings != null) {        assertEquals(bindings.size(), bindings2.size());        for (Map.Entry<String, Integer> entry : bindings.entrySet()) {            assertEquals(entry.getValue(), bindings2.get(entry.getKey()));        }    }}
0
private static void doCheckBindings(Map<String, Integer> labels)
{    assertTrue("Missing label", labels.keySet().contains("A"));    assertTrue("Missing label", labels.keySet().contains("B"));    assertTrue("Missing label", labels.keySet().contains("C"));    assertTrue("Missing label", labels.keySet().contains("D"));    assertTrue("Missing label", labels.keySet().contains("default"));}
0
private static void writeAndRead(Writable toWrite, Writable toRead) throws IOException
{    ByteArrayOutputStream baos = new ByteArrayOutputStream();    try (DataOutputStream dos = new DataOutputStream(baos)) {        toWrite.write(dos);    }    ByteArrayInputStream bais = new ByteArrayInputStream(baos.toByteArray());    try (DataInputStream dis = new DataInputStream(bais)) {        toRead.readFields(dis);    }}
0
public void testUnsignedLong() throws Exception
{    ByteArrayOutputStream baos = new ByteArrayOutputStream();    DataOutput out = new DataOutputStream(baos);    Varint.writeUnsignedVarLong(0L, out);    for (long i = 1L; i > 0L && i <= (1L << 62); i <<= 1) {        Varint.writeUnsignedVarLong(i - 1, out);        Varint.writeUnsignedVarLong(i, out);    }    Varint.writeUnsignedVarLong(Long.MAX_VALUE, out);    DataInput in = new DataInputStream(new ByteArrayInputStream(baos.toByteArray()));    assertEquals(0L, Varint.readUnsignedVarLong(in));    for (long i = 1L; i > 0L && i <= (1L << 62); i <<= 1) {        assertEquals(i - 1, Varint.readUnsignedVarLong(in));        assertEquals(i, Varint.readUnsignedVarLong(in));    }    assertEquals(Long.MAX_VALUE, Varint.readUnsignedVarLong(in));}
0
public void testSignedPositiveLong() throws Exception
{    ByteArrayOutputStream baos = new ByteArrayOutputStream();    DataOutput out = new DataOutputStream(baos);    Varint.writeSignedVarLong(0L, out);    for (long i = 1L; i <= (1L << 61); i <<= 1) {        Varint.writeSignedVarLong(i - 1, out);        Varint.writeSignedVarLong(i, out);    }    Varint.writeSignedVarLong((1L << 62) - 1, out);    Varint.writeSignedVarLong((1L << 62), out);    Varint.writeSignedVarLong(Long.MAX_VALUE, out);    DataInput in = new DataInputStream(new ByteArrayInputStream(baos.toByteArray()));    assertEquals(0L, Varint.readSignedVarLong(in));    for (long i = 1L; i <= (1L << 61); i <<= 1) {        assertEquals(i - 1, Varint.readSignedVarLong(in));        assertEquals(i, Varint.readSignedVarLong(in));    }    assertEquals((1L << 62) - 1, Varint.readSignedVarLong(in));    assertEquals((1L << 62), Varint.readSignedVarLong(in));    assertEquals(Long.MAX_VALUE, Varint.readSignedVarLong(in));}
0
public void testSignedNegativeLong() throws Exception
{    ByteArrayOutputStream baos = new ByteArrayOutputStream();    DataOutput out = new DataOutputStream(baos);    for (long i = -1L; i >= -(1L << 62); i <<= 1) {        Varint.writeSignedVarLong(i, out);        Varint.writeSignedVarLong(i + 1, out);    }    Varint.writeSignedVarLong(Long.MIN_VALUE, out);    Varint.writeSignedVarLong(Long.MIN_VALUE + 1, out);    DataInput in = new DataInputStream(new ByteArrayInputStream(baos.toByteArray()));    for (long i = -1L; i >= -(1L << 62); i <<= 1) {        assertEquals(i, Varint.readSignedVarLong(in));        assertEquals(i + 1, Varint.readSignedVarLong(in));    }    assertEquals(Long.MIN_VALUE, Varint.readSignedVarLong(in));    assertEquals(Long.MIN_VALUE + 1, Varint.readSignedVarLong(in));}
0
public void testUnsignedInt() throws Exception
{    ByteArrayOutputStream baos = new ByteArrayOutputStream();    DataOutput out = new DataOutputStream(baos);    Varint.writeUnsignedVarInt(0, out);    for (int i = 1; i > 0 && i <= (1 << 30); i <<= 1) {        Varint.writeUnsignedVarLong(i - 1, out);        Varint.writeUnsignedVarLong(i, out);    }    Varint.writeUnsignedVarLong(Integer.MAX_VALUE, out);    DataInput in = new DataInputStream(new ByteArrayInputStream(baos.toByteArray()));    assertEquals(0, Varint.readUnsignedVarInt(in));    for (int i = 1; i > 0 && i <= (1 << 30); i <<= 1) {        assertEquals(i - 1, Varint.readUnsignedVarInt(in));        assertEquals(i, Varint.readUnsignedVarInt(in));    }    assertEquals(Integer.MAX_VALUE, Varint.readUnsignedVarInt(in));}
0
public void testSignedPositiveInt() throws Exception
{    ByteArrayOutputStream baos = new ByteArrayOutputStream();    DataOutput out = new DataOutputStream(baos);    Varint.writeSignedVarInt(0, out);    for (int i = 1; i <= (1 << 29); i <<= 1) {        Varint.writeSignedVarLong(i - 1, out);        Varint.writeSignedVarLong(i, out);    }    Varint.writeSignedVarInt((1 << 30) - 1, out);    Varint.writeSignedVarInt((1 << 30), out);    Varint.writeSignedVarInt(Integer.MAX_VALUE, out);    DataInput in = new DataInputStream(new ByteArrayInputStream(baos.toByteArray()));    assertEquals(0, Varint.readSignedVarInt(in));    for (int i = 1; i <= (1 << 29); i <<= 1) {        assertEquals(i - 1, Varint.readSignedVarInt(in));        assertEquals(i, Varint.readSignedVarInt(in));    }    assertEquals((1L << 30) - 1, Varint.readSignedVarInt(in));    assertEquals((1L << 30), Varint.readSignedVarInt(in));    assertEquals(Integer.MAX_VALUE, Varint.readSignedVarInt(in));}
0
public void testSignedNegativeInt() throws Exception
{    ByteArrayOutputStream baos = new ByteArrayOutputStream();    DataOutput out = new DataOutputStream(baos);    for (int i = -1; i >= -(1 << 30); i <<= 1) {        Varint.writeSignedVarInt(i, out);        Varint.writeSignedVarInt(i + 1, out);    }    Varint.writeSignedVarInt(Integer.MIN_VALUE, out);    Varint.writeSignedVarInt(Integer.MIN_VALUE + 1, out);    DataInput in = new DataInputStream(new ByteArrayInputStream(baos.toByteArray()));    for (int i = -1; i >= -(1 << 30); i <<= 1) {        assertEquals(i, Varint.readSignedVarInt(in));        assertEquals(i + 1, Varint.readSignedVarInt(in));    }    assertEquals(Integer.MIN_VALUE, Varint.readSignedVarInt(in));    assertEquals(Integer.MIN_VALUE + 1, Varint.readSignedVarInt(in));}
0
public void testUnsignedSize() throws Exception
{    ByteArrayOutputStream baos = new ByteArrayOutputStream();    DataOutput out = new DataOutputStream(baos);    int expectedSize = 0;    for (int exponent = 0; exponent <= 62; exponent++) {        Varint.writeUnsignedVarLong(1L << exponent, out);        expectedSize += 1 + exponent / 7;        assertEquals(expectedSize, baos.size());    }}
0
public void testSignedSize() throws Exception
{    ByteArrayOutputStream baos = new ByteArrayOutputStream();    DataOutput out = new DataOutputStream(baos);    int expectedSize = 0;    for (int exponent = 0; exponent <= 61; exponent++) {        Varint.writeSignedVarLong(1L << exponent, out);        expectedSize += 1 + ((exponent + 1) / 7);        assertEquals(expectedSize, baos.size());    }    for (int exponent = 0; exponent <= 61; exponent++) {        Varint.writeSignedVarLong(-(1L << exponent) - 1, out);        expectedSize += 1 + ((exponent + 1) / 7);        assertEquals(expectedSize, baos.size());    }}
0
public void createRandom(Vector v)
{    int size = randomInt(v.size() - 1);    for (int i = 0; i < size; ++i) {        v.set(randomInt(v.size() - 1), randomDouble());    }    int zeros = Math.max(2, size / 4);    for (Element e : v.nonZeroes()) {        if (e.index() % zeros == 0) {            e.set(0.0);        }    }}
0
public void testViewSequentialAccessSparseVectorWritable() throws Exception
{    Vector v = new SequentialAccessSparseVector(MAX_VECTOR_SIZE);    createRandom(v);    Vector view = new VectorView(v, 0, v.size());    doTestVectorWritableEquals(view);}
0
public void testSequentialAccessSparseVectorWritable() throws Exception
{    Vector v = new SequentialAccessSparseVector(MAX_VECTOR_SIZE);    createRandom(v);    doTestVectorWritableEquals(v);}
0
public void testRandomAccessSparseVectorWritable() throws Exception
{    Vector v = new RandomAccessSparseVector(MAX_VECTOR_SIZE);    createRandom(v);    doTestVectorWritableEquals(v);}
0
public void testDenseVectorWritable() throws Exception
{    Vector v = new DenseVector(MAX_VECTOR_SIZE);    createRandom(v);    doTestVectorWritableEquals(v);}
0
public void testNamedVectorWritable() throws Exception
{    Vector v = new DenseVector(MAX_VECTOR_SIZE);    v = new NamedVector(v, "Victor");    createRandom(v);    doTestVectorWritableEquals(v);}
0
private static void doTestVectorWritableEquals(Vector v) throws IOException
{    Writable vectorWritable = new VectorWritable(v);    VectorWritable vectorWritable2 = new VectorWritable();    writeAndRead(vectorWritable, vectorWritable2);    Vector v2 = vectorWritable2.get();    if (v instanceof NamedVector) {        assertTrue(v2 instanceof NamedVector);        NamedVector nv = (NamedVector) v;        NamedVector nv2 = (NamedVector) v2;        assertEquals(nv.getName(), nv2.getName());        assertEquals("Victor", nv.getName());    }    assertEquals(v, v2);}
0
private static void writeAndRead(Writable toWrite, Writable toRead) throws IOException
{    ByteArrayOutputStream baos = new ByteArrayOutputStream();    try (DataOutputStream dos = new DataOutputStream(baos)) {        toWrite.write(dos);    }    ByteArrayInputStream bais = new ByteArrayInputStream(baos.toByteArray());    try (DataInputStream dis = new DataInputStream(bais)) {        toRead.readFields(dis);    }}
0
