public void setConf(Configuration conf)
{    super.setConf(conf);    if (conf == null)        return;    defaultInterval = conf.getInt("db.fetch.interval.default", 0);    maxInterval = conf.getInt("db.fetch.interval.max", 0);        }
1
public CrawlDatum initializeSchedule(Text url, CrawlDatum datum)
{    datum.setFetchTime(System.currentTimeMillis());    datum.setFetchInterval(defaultInterval);    datum.setRetriesSinceFetch(0);    return datum;}
0
public CrawlDatum setFetchSchedule(Text url, CrawlDatum datum, long prevFetchTime, long prevModifiedTime, long fetchTime, long modifiedTime, int state)
{    datum.setRetriesSinceFetch(0);    return datum;}
0
public CrawlDatum setPageGoneSchedule(Text url, CrawlDatum datum, long prevFetchTime, long prevModifiedTime, long fetchTime)
{        if ((datum.getFetchInterval() * 1.5f) < maxInterval)        datum.setFetchInterval(datum.getFetchInterval() * 1.5f);    else        datum.setFetchInterval(maxInterval * 0.9f);    datum.setFetchTime(fetchTime + (long) datum.getFetchInterval() * 1000);    return datum;}
0
public CrawlDatum setPageRetrySchedule(Text url, CrawlDatum datum, long prevFetchTime, long prevModifiedTime, long fetchTime)
{    datum.setFetchTime(fetchTime + (long) SECONDS_PER_DAY * 1000);    datum.setRetriesSinceFetch(datum.getRetriesSinceFetch() + 1);    return datum;}
0
public long calculateLastFetchTime(CrawlDatum datum)
{    if (datum.getStatus() == CrawlDatum.STATUS_DB_UNFETCHED) {        return 0L;    } else {        return datum.getFetchTime() - (long) datum.getFetchInterval() * 1000;    }}
0
public boolean shouldFetch(Text url, CrawlDatum datum, long curTime)
{        if (datum.getFetchTime() - curTime > (long) maxInterval * 1000) {        if (datum.getFetchInterval() > maxInterval) {            datum.setFetchInterval(maxInterval * 0.9f);        }        datum.setFetchTime(curTime);    }    if (datum.getFetchTime() > curTime) {                return false;    }    return true;}
0
public CrawlDatum forceRefetch(Text url, CrawlDatum datum, boolean asap)
{        if (datum.getFetchInterval() > maxInterval)        datum.setFetchInterval(maxInterval * 0.9f);    datum.setStatus(CrawlDatum.STATUS_DB_UNFETCHED);    datum.setRetriesSinceFetch(0);    datum.setSignature(null);    datum.setModifiedTime(0L);    if (asap)        datum.setFetchTime(System.currentTimeMillis());    return datum;}
0
public void setConf(Configuration conf)
{    super.setConf(conf);    if (conf == null)        return;    INC_RATE = conf.getFloat("db.fetch.schedule.adaptive.inc_rate", 0.2f);    DEC_RATE = conf.getFloat("db.fetch.schedule.adaptive.dec_rate", 0.2f);    MIN_INTERVAL = conf.getFloat("db.fetch.schedule.adaptive.min_interval", (float) 60.0);    MAX_INTERVAL = conf.getFloat("db.fetch.schedule.adaptive.max_interval",     (float) SECONDS_PER_DAY * 365);    SYNC_DELTA = conf.getBoolean("db.fetch.schedule.adaptive.sync_delta", true);    SYNC_DELTA_RATE = conf.getFloat("db.fetch.schedule.adaptive.sync_delta_rate", 0.2f);}
0
public CrawlDatum setFetchSchedule(Text url, CrawlDatum datum, long prevFetchTime, long prevModifiedTime, long fetchTime, long modifiedTime, int state)
{    super.setFetchSchedule(url, datum, prevFetchTime, prevModifiedTime, fetchTime, modifiedTime, state);    float interval = datum.getFetchInterval();    long refTime = fetchTime;        interval = (interval == 0) ? defaultInterval : interval;    if (datum.getMetaData().containsKey(Nutch.WRITABLE_FIXED_INTERVAL_KEY)) {                FloatWritable customIntervalWritable = (FloatWritable) (datum.getMetaData().get(Nutch.WRITABLE_FIXED_INTERVAL_KEY));        interval = customIntervalWritable.get();    } else {        if (modifiedTime <= 0)            modifiedTime = fetchTime;        switch(state) {            case FetchSchedule.STATUS_MODIFIED:                interval *= (1.0f - DEC_RATE);                modifiedTime = fetchTime;                break;            case FetchSchedule.STATUS_NOTMODIFIED:                interval *= (1.0f + INC_RATE);                break;            case FetchSchedule.STATUS_UNKNOWN:                break;        }        if (SYNC_DELTA) {                        long delta = (fetchTime - modifiedTime) / 1000L;            if (delta > interval)                interval = delta;            refTime = fetchTime - Math.round(delta * SYNC_DELTA_RATE * 1000);        }        if (interval < MIN_INTERVAL) {            interval = MIN_INTERVAL;        } else if (interval > MAX_INTERVAL) {            interval = MAX_INTERVAL;        }    }    datum.setFetchInterval(interval);    datum.setFetchTime(refTime + Math.round(interval * 1000.0));    datum.setModifiedTime(modifiedTime);    return datum;}
0
public static void main(String[] args) throws Exception
{    FetchSchedule fs = new AdaptiveFetchSchedule();    fs.setConf(NutchConfiguration.create());        long curTime = 0;        long delta = 1000L * 3600L * 24L;            long update = 1000L * 3600L * 24L * 30L;    boolean changed = true;    long lastModified = 0;    int miss = 0;    int totalMiss = 0;    int maxMiss = 0;    int fetchCnt = 0;    int changeCnt = 0;        CrawlDatum p = new CrawlDatum(1, 3600 * 24 * 30, 1.0f);    p.setFetchTime(0);            for (int i = 0; i < 10000; i++) {        if (lastModified + update < curTime) {                                    changed = true;            changeCnt++;            lastModified = curTime;        }                if (p.getFetchTime() <= curTime) {            fetchCnt++;            fs.setFetchSchedule(new Text("http://www.example.com"), p, p.getFetchTime(), p.getModifiedTime(), curTime, lastModified, changed ? FetchSchedule.STATUS_MODIFIED : FetchSchedule.STATUS_NOTMODIFIED);                        if (!changed)                miss++;            if (miss > maxMiss)                maxMiss = miss;            changed = false;            totalMiss += miss;            miss = 0;        }        if (changed)            miss++;        curTime += delta;    }        }
1
public static boolean hasDbStatus(CrawlDatum datum)
{    if (datum.status <= STATUS_DB_MAX)        return true;    return false;}
0
public static boolean hasFetchStatus(CrawlDatum datum)
{    if (datum.status > STATUS_DB_MAX && datum.status <= STATUS_FETCH_MAX)        return true;    return false;}
0
public byte getStatus()
{    return status;}
0
public static String getStatusName(byte value)
{    String res = statNames.get(value);    if (res == null)        res = "unknown";    return res;}
0
public static byte getStatusByName(String name)
{    for (Entry<Byte, String> status : statNames.entrySet()) {        if (name.equalsIgnoreCase(status.getValue())) {            return status.getKey();        }    }    return -1;}
0
public void setStatus(int status)
{    this.status = (byte) status;}
0
public long getFetchTime()
{    return fetchTime;}
0
public void setFetchTime(long fetchTime)
{    this.fetchTime = fetchTime;}
0
public long getModifiedTime()
{    return modifiedTime;}
0
public void setModifiedTime(long modifiedTime)
{    this.modifiedTime = modifiedTime;}
0
public byte getRetriesSinceFetch()
{    return retries;}
0
public void setRetriesSinceFetch(int retries)
{    this.retries = (byte) retries;}
0
public int getFetchInterval()
{    return fetchInterval;}
0
public void setFetchInterval(int fetchInterval)
{    this.fetchInterval = fetchInterval;}
0
public void setFetchInterval(float fetchInterval)
{    this.fetchInterval = Math.round(fetchInterval);}
0
public float getScore()
{    return score;}
0
public void setScore(float score)
{    this.score = score;}
0
public byte[] getSignature()
{    return signature;}
0
public void setSignature(byte[] signature)
{    if (signature != null && signature.length > 256)        throw new RuntimeException("Max signature length (256) exceeded: " + signature.length);    this.signature = signature;}
0
public void setMetaData(org.apache.hadoop.io.MapWritable mapWritable)
{    this.metaData = new org.apache.hadoop.io.MapWritable(mapWritable);}
0
public void putAllMetaData(CrawlDatum other)
{    for (Entry<Writable, Writable> e : other.getMetaData().entrySet()) {        getMetaData().put(e.getKey(), e.getValue());    }}
0
public org.apache.hadoop.io.MapWritable getMetaData()
{    if (this.metaData == null)        this.metaData = new org.apache.hadoop.io.MapWritable();    return this.metaData;}
0
public static CrawlDatum read(DataInput in) throws IOException
{    CrawlDatum result = new CrawlDatum();    result.readFields(in);    return result;}
0
public void readFields(DataInput in) throws IOException
{        byte version = in.readByte();    if (    version > CUR_VERSION)        throw new VersionMismatchException(CUR_VERSION, version);    status = in.readByte();    fetchTime = in.readLong();    retries = in.readByte();    if (version > 5) {        fetchInterval = in.readInt();    } else        fetchInterval = Math.round(in.readFloat());    score = in.readFloat();    if (version > 2) {        modifiedTime = in.readLong();        int cnt = in.readByte();        if (cnt > 0) {            signature = new byte[cnt];            in.readFully(signature);        } else            signature = null;    }    if (version > 3) {        boolean hasMetadata = false;        if (version < 7) {            org.apache.hadoop.io.MapWritable oldMetaData = new org.apache.hadoop.io.MapWritable();            if (in.readBoolean()) {                hasMetadata = true;                metaData = new org.apache.hadoop.io.MapWritable();                oldMetaData.readFields(in);            }            for (Writable key : oldMetaData.keySet()) {                metaData.put(key, oldMetaData.get(key));            }        } else {            if (in.readBoolean()) {                hasMetadata = true;                metaData = new org.apache.hadoop.io.MapWritable();                metaData.readFields(in);            }        }        if (hasMetadata == false)            metaData = null;    }        if (version < 5) {        if (oldToNew.containsKey(status))            status = oldToNew.get(status);        else            status = STATUS_DB_UNFETCHED;    }}
0
public void write(DataOutput out) throws IOException
{        out.writeByte(CUR_VERSION);    out.writeByte(status);    out.writeLong(fetchTime);    out.writeByte(retries);    out.writeInt(fetchInterval);    out.writeFloat(score);    out.writeLong(modifiedTime);    if (signature == null) {        out.writeByte(0);    } else {        out.writeByte(signature.length);        out.write(signature);    }    if (metaData != null && metaData.size() > 0) {        out.writeBoolean(true);        metaData.write(out);    } else {        out.writeBoolean(false);    }}
0
public void set(CrawlDatum that)
{    this.status = that.status;    this.fetchTime = that.fetchTime;    this.retries = that.retries;    this.fetchInterval = that.fetchInterval;    this.score = that.score;    this.modifiedTime = that.modifiedTime;    this.signature = that.signature;    if (that.metaData != null) {                this.metaData = new org.apache.hadoop.io.MapWritable(that.metaData);    } else {        this.metaData = null;    }}
0
public int compareTo(CrawlDatum that)
{    if (that.score != this.score)        return (that.score - this.score) > 0 ? 1 : -1;    if (that.status != this.status)        return this.status - that.status;    if (that.fetchTime != this.fetchTime)        return (that.fetchTime - this.fetchTime) > 0 ? 1 : -1;    if (that.retries != this.retries)        return that.retries - this.retries;    if (that.fetchInterval != this.fetchInterval)        return (that.fetchInterval - this.fetchInterval) > 0 ? 1 : -1;    if (that.modifiedTime != this.modifiedTime)        return (that.modifiedTime - this.modifiedTime) > 0 ? 1 : -1;    return SignatureComparator._compare(this, that);}
0
public int compare(byte[] b1, int s1, int l1, byte[] b2, int s2, int l2)
{    float score1 = readFloat(b1, s1 + SCORE_OFFSET);    float score2 = readFloat(b2, s2 + SCORE_OFFSET);    if (score2 != score1) {        return (score2 - score1) > 0 ? 1 : -1;    }    int status1 = b1[s1 + 1];    int status2 = b2[s2 + 1];    if (status2 != status1)        return status1 - status2;    long fetchTime1 = readLong(b1, s1 + 2);    long fetchTime2 = readLong(b2, s2 + 2);    if (fetchTime2 != fetchTime1)        return (fetchTime2 - fetchTime1) > 0 ? 1 : -1;    int retries1 = b1[s1 + 10];    int retries2 = b2[s2 + 10];    if (retries2 != retries1)        return retries2 - retries1;    int fetchInterval1 = readInt(b1, s1 + 11);    int fetchInterval2 = readInt(b2, s2 + 11);    if (fetchInterval2 != fetchInterval1)        return (fetchInterval2 - fetchInterval1) > 0 ? 1 : -1;    long modifiedTime1 = readLong(b1, s1 + SCORE_OFFSET + 4);    long modifiedTime2 = readLong(b2, s2 + SCORE_OFFSET + 4);    if (modifiedTime2 != modifiedTime1)        return (modifiedTime2 - modifiedTime1) > 0 ? 1 : -1;    int sigl1 = b1[s1 + SIG_OFFSET];    int sigl2 = b2[s2 + SIG_OFFSET];    return SignatureComparator._compare(b1, SIG_OFFSET, sigl1, b2, SIG_OFFSET, sigl2);}
0
public String toString()
{    StringBuilder buf = new StringBuilder();    buf.append("Version: " + CUR_VERSION + "\n");    buf.append("Status: " + getStatus() + " (" + getStatusName(getStatus()) + ")\n");    buf.append("Fetch time: " + new Date(getFetchTime()) + "\n");    buf.append("Modified time: " + new Date(getModifiedTime()) + "\n");    buf.append("Retries since fetch: " + getRetriesSinceFetch() + "\n");    buf.append("Retry interval: " + getFetchInterval() + " seconds (" + (getFetchInterval() / FetchSchedule.SECONDS_PER_DAY) + " days)\n");    buf.append("Score: " + getScore() + "\n");    buf.append("Signature: " + StringUtil.toHexString(getSignature()) + "\n");    buf.append("Metadata: \n ");    if (metaData != null) {        for (Entry<Writable, Writable> e : metaData.entrySet()) {            buf.append("\t");            buf.append(e.getKey());            buf.append("=");            buf.append(e.getValue());            buf.append("\n");        }    }    return buf.toString();}
0
private boolean metadataEquals(org.apache.hadoop.io.MapWritable otherMetaData)
{    if (metaData == null || metaData.size() == 0) {        return otherMetaData == null || otherMetaData.size() == 0;    }    if (otherMetaData == null) {                return false;    }    HashSet<Entry<Writable, Writable>> set1 = new HashSet<>(metaData.entrySet());    HashSet<Entry<Writable, Writable>> set2 = new HashSet<>(otherMetaData.entrySet());    return set1.equals(set2);}
0
public boolean equals(Object o)
{    if (!(o instanceof CrawlDatum))        return false;    CrawlDatum other = (CrawlDatum) o;    boolean res = (this.status == other.status) && (this.fetchTime == other.fetchTime) && (this.modifiedTime == other.modifiedTime) && (this.retries == other.retries) && (this.fetchInterval == other.fetchInterval) && (SignatureComparator._compare(this.signature, other.signature) == 0) && (this.score == other.score);    if (!res)        return res;    return metadataEquals(other.metaData);}
0
public int hashCode()
{    int res = 0;    if (signature != null) {        for (int i = 0; i < signature.length / 4; i += 4) {            res ^= (signature[i] << 24 + signature[i + 1] << 16 + signature[i + 2] << 8 + signature[i + 3]);        }    }    if (metaData != null) {        res ^= metaData.entrySet().hashCode();    }    return res ^ status ^ ((int) fetchTime) ^ ((int) modifiedTime) ^ retries ^ fetchInterval ^ Float.floatToIntBits(score);}
0
public Object clone()
{    try {        return super.clone();    } catch (CloneNotSupportedException e) {        throw new RuntimeException(e);    }}
0
public boolean evaluate(Expression expr, String url)
{    if (expr != null && url != null) {                JexlContext jcontext = new MapContext();                jcontext.set("url", url);        jcontext.set("status", getStatusName(getStatus()));        jcontext.set("fetchTime", (long) (getFetchTime()));        jcontext.set("modifiedTime", (long) (getModifiedTime()));        jcontext.set("retries", getRetriesSinceFetch());        jcontext.set("interval", Integer.valueOf(getFetchInterval()));        jcontext.set("score", getScore());        jcontext.set("signature", StringUtil.toHexString(getSignature()));                for (Map.Entry<Writable, Writable> entry : getMetaData().entrySet()) {            Object value = entry.getValue();            Text tkey = (Text) entry.getKey();            if (value instanceof FloatWritable) {                FloatWritable fvalue = (FloatWritable) value;                jcontext.set(tkey.toString(), fvalue.get());            }            if (value instanceof IntWritable) {                IntWritable ivalue = (IntWritable) value;                jcontext.set(tkey.toString(), ivalue.get());            }            if (value instanceof Text) {                Text tvalue = (Text) value;                jcontext.set(tkey.toString().replace("-", "_"), tvalue.toString());            }            if (value instanceof ProtocolStatus) {                ProtocolStatus pvalue = (ProtocolStatus) value;                jcontext.set(tkey.toString().replace("-", "_"), pvalue.toString());            }        }        try {            if (Boolean.TRUE.equals(expr.evaluate(jcontext))) {                return true;            }        } catch (Exception e) {                }    }    return false;}
0
public void update(Path crawlDb, Path[] segments, boolean normalize, boolean filter) throws IOException, InterruptedException, ClassNotFoundException
{    boolean additionsAllowed = getConf().getBoolean(CRAWLDB_ADDITIONS_ALLOWED, true);    update(crawlDb, segments, normalize, filter, additionsAllowed, false);}
0
public void update(Path crawlDb, Path[] segments, boolean normalize, boolean filter, boolean additionsAllowed, boolean force) throws IOException, InterruptedException, ClassNotFoundException
{    Path lock = lock(getConf(), crawlDb, force);    SimpleDateFormat sdf = new SimpleDateFormat("yyyy-MM-dd HH:mm:ss");    long start = System.currentTimeMillis();    Job job = CrawlDb.createJob(getConf(), crawlDb);    Configuration conf = job.getConfiguration();    conf.setBoolean(CRAWLDB_ADDITIONS_ALLOWED, additionsAllowed);    conf.setBoolean(CrawlDbFilter.URL_FILTERING, filter);    conf.setBoolean(CrawlDbFilter.URL_NORMALIZING, normalize);    boolean url404Purging = conf.getBoolean(CRAWLDB_PURGE_404, false);    if (LOG.isInfoEnabled()) {                                                            }    for (int i = 0; i < segments.length; i++) {        FileSystem sfs = segments[i].getFileSystem(getConf());        Path fetch = new Path(segments[i], CrawlDatum.FETCH_DIR_NAME);        Path parse = new Path(segments[i], CrawlDatum.PARSE_DIR_NAME);        if (sfs.exists(fetch)) {            FileInputFormat.addInputPath(job, fetch);            if (sfs.exists(parse)) {                FileInputFormat.addInputPath(job, parse);            } else {                            }        } else {                    }    }    if (LOG.isInfoEnabled()) {            }    FileSystem fs = crawlDb.getFileSystem(getConf());    Path outPath = FileOutputFormat.getOutputPath(job);    try {        boolean success = job.waitForCompletion(true);        if (!success) {            String message = "CrawlDb update job did not succeed, job status:" + job.getStatus().getState() + ", reason: " + job.getStatus().getFailureInfo();                        NutchJob.cleanupAfterFailure(outPath, lock, fs);            throw new RuntimeException(message);        }    } catch (IOException | InterruptedException | ClassNotFoundException e) {                NutchJob.cleanupAfterFailure(outPath, lock, fs);        throw e;    }    CrawlDb.install(job, crawlDb);    if (filter) {        long urlsFiltered = job.getCounters().findCounter("CrawlDB filter", "URLs filtered").getValue();            }    long end = System.currentTimeMillis();    }
1
public static Job createJob(Configuration config, Path crawlDb) throws IOException
{    Path newCrawlDb = new Path(crawlDb, Integer.toString(new Random().nextInt(Integer.MAX_VALUE)));    Job job = NutchJob.getInstance(config);    job.setJobName("crawldb " + crawlDb);    Path current = new Path(crawlDb, CURRENT_NAME);    if (current.getFileSystem(job.getConfiguration()).exists(current)) {        FileInputFormat.addInputPath(job, current);    }    job.setInputFormatClass(SequenceFileInputFormat.class);    job.setMapperClass(CrawlDbFilter.class);    job.setReducerClass(CrawlDbReducer.class);    job.setJarByClass(CrawlDb.class);    FileOutputFormat.setOutputPath(job, newCrawlDb);    job.setOutputFormatClass(MapFileOutputFormat.class);    job.setOutputKeyClass(Text.class);    job.setOutputValueClass(CrawlDatum.class);        job.getConfiguration().setBoolean("mapreduce.fileoutputcommitter.marksuccessfuljobs", false);    return job;}
0
public static Path lock(Configuration job, Path crawlDb, boolean force) throws IOException
{    Path lock = new Path(crawlDb, LOCK_NAME);    LockUtil.createLockFile(job, lock, force);    return lock;}
0
private static void install(Configuration conf, Path crawlDb, Path tempCrawlDb) throws IOException
{    boolean preserveBackup = conf.getBoolean("db.preserve.backup", true);    FileSystem fs = crawlDb.getFileSystem(conf);    Path old = new Path(crawlDb, "old");    Path current = new Path(crawlDb, CURRENT_NAME);    if (fs.exists(current)) {        FSUtils.replace(fs, old, current, true);    }    FSUtils.replace(fs, current, tempCrawlDb, true);    Path lock = new Path(crawlDb, LOCK_NAME);    LockUtil.removeLockFile(fs, lock);    if (!preserveBackup && fs.exists(old)) {        fs.delete(old, true);    }}
0
public static void install(Job job, Path crawlDb) throws IOException
{    Configuration conf = job.getConfiguration();    Path tempCrawlDb = org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.getOutputPath(job);    install(conf, crawlDb, tempCrawlDb);}
0
public static void main(String[] args) throws Exception
{    int res = ToolRunner.run(NutchConfiguration.create(), new CrawlDb(), args);    System.exit(res);}
0
public int run(String[] args) throws Exception
{    if (args.length < 1) {        System.err.println("Usage: CrawlDb <crawldb> (-dir <segments> | <seg1> <seg2> ...) [-force] [-normalize] [-filter] [-noAdditions]");        System.err.println("\tcrawldb\tCrawlDb to update");        System.err.println("\t-dir segments\tparent directory containing all segments to update from");        System.err.println("\tseg1 seg2 ...\tlist of segment names to update from");        System.err.println("\t-force\tforce update even if CrawlDb appears to be locked (CAUTION advised)");        System.err.println("\t-normalize\tuse URLNormalizer on urls in CrawlDb and segment (usually not needed)");        System.err.println("\t-filter\tuse URLFilters on urls in CrawlDb and segment");        System.err.println("\t-noAdditions\tonly update already existing URLs, don't add any newly discovered URLs");        return -1;    }    boolean normalize = getConf().getBoolean(CrawlDbFilter.URL_NORMALIZING, false);    boolean filter = getConf().getBoolean(CrawlDbFilter.URL_FILTERING, false);    boolean additionsAllowed = getConf().getBoolean(CRAWLDB_ADDITIONS_ALLOWED, true);    boolean force = false;    HashSet<Path> dirs = new HashSet<>();    for (int i = 1; i < args.length; i++) {        if (args[i].equals("-normalize")) {            normalize = true;        } else if (args[i].equals("-filter")) {            filter = true;        } else if (args[i].equals("-force")) {            force = true;        } else if (args[i].equals("-noAdditions")) {            additionsAllowed = false;        } else if (args[i].equals("-dir")) {            Path dirPath = new Path(args[++i]);            FileSystem fs = dirPath.getFileSystem(getConf());            FileStatus[] paths = fs.listStatus(dirPath, HadoopFSUtil.getPassDirectoriesFilter(fs));            dirs.addAll(Arrays.asList(HadoopFSUtil.getPaths(paths)));        } else {            dirs.add(new Path(args[i]));        }    }    try {        update(new Path(args[0]), dirs.toArray(new Path[dirs.size()]), normalize, filter, additionsAllowed, force);        return 0;    } catch (Exception e) {                return -1;    }}
1
public Map<String, Object> run(Map<String, Object> args, String crawlId) throws Exception
{    Map<String, Object> results = new HashMap<>();    boolean normalize = getConf().getBoolean(CrawlDbFilter.URL_NORMALIZING, false);    boolean filter = getConf().getBoolean(CrawlDbFilter.URL_FILTERING, false);    boolean additionsAllowed = getConf().getBoolean(CRAWLDB_ADDITIONS_ALLOWED, true);    boolean force = false;    HashSet<Path> dirs = new HashSet<>();    if (args.containsKey("normalize")) {        normalize = true;    }    if (args.containsKey("filter")) {        filter = true;    }    if (args.containsKey("force")) {        force = true;    }    if (args.containsKey("noAdditions")) {        additionsAllowed = false;    }    Path crawlDb;    if (args.containsKey(Nutch.ARG_CRAWLDB)) {        Object crawldbPath = args.get(Nutch.ARG_CRAWLDB);        if (crawldbPath instanceof Path) {            crawlDb = (Path) crawldbPath;        } else {            crawlDb = new Path(crawldbPath.toString());        }    } else {        crawlDb = new Path(crawlId + "/crawldb");    }    Path segmentsDir;    if (args.containsKey(Nutch.ARG_SEGMENTDIR)) {        Object segDir = args.get(Nutch.ARG_SEGMENTDIR);        if (segDir instanceof Path) {            segmentsDir = (Path) segDir;        } else {            segmentsDir = new Path(segDir.toString());        }        FileSystem fs = segmentsDir.getFileSystem(getConf());        FileStatus[] paths = fs.listStatus(segmentsDir, HadoopFSUtil.getPassDirectoriesFilter(fs));        dirs.addAll(Arrays.asList(HadoopFSUtil.getPaths(paths)));    } else if (args.containsKey(Nutch.ARG_SEGMENTS)) {        Object segments = args.get(Nutch.ARG_SEGMENTS);        ArrayList<String> segmentList = new ArrayList<>();        if (segments instanceof ArrayList) {            segmentList = (ArrayList<String>) segments;        } else if (segments instanceof Path) {            segmentList.add(segments.toString());        }        for (String segment : segmentList) {            dirs.add(new Path(segment));        }    } else {        String segmentDir = crawlId + "/segments";        File dir = new File(segmentDir);        File[] segmentsList = dir.listFiles();        Arrays.sort(segmentsList, (f1, f2) -> {            if (f1.lastModified() > f2.lastModified())                return -1;            else                return 0;        });        dirs.add(new Path(segmentsList[0].getPath()));    }    try {        update(crawlDb, dirs.toArray(new Path[dirs.size()]), normalize, filter, additionsAllowed, force);        results.put(Nutch.VAL_RESULT, Integer.toString(0));        return results;    } catch (Exception e) {                results.put(Nutch.VAL_RESULT, Integer.toString(-1));        return results;    }}
1
public void setup(Mapper<Text, CrawlDatum, Text, CrawlDatum>.Context context)
{    Configuration conf = context.getConfiguration();    urlFiltering = conf.getBoolean(URL_FILTERING, false);    urlNormalizers = conf.getBoolean(URL_NORMALIZING, false);    url404Purging = conf.getBoolean(CrawlDb.CRAWLDB_PURGE_404, false);    purgeOrphans = conf.getBoolean(CrawlDb.CRAWLDB_PURGE_ORPHANS, false);    if (urlFiltering) {        filters = new URLFilters(conf);    }    if (urlNormalizers) {        scope = conf.get(URL_NORMALIZING_SCOPE, URLNormalizers.SCOPE_CRAWLDB);        normalizers = new URLNormalizers(conf, scope);    }}
0
public void close()
{}
0
public void map(Text key, CrawlDatum value, Context context) throws IOException, InterruptedException
{    String url = key.toString();        if (url404Purging && CrawlDatum.STATUS_DB_GONE == value.getStatus()) {        context.getCounter("CrawlDB filter", "Gone records removed").increment(1);        return;    }        if (purgeOrphans && CrawlDatum.STATUS_DB_ORPHAN == value.getStatus()) {        context.getCounter("CrawlDB filter", "Orphan records removed").increment(1);        return;    }    if (url != null && urlNormalizers) {        try {                        url = normalizers.normalize(url, scope);        } catch (Exception e) {                        url = null;        }    }    if (url != null && urlFiltering) {        try {                        url = filters.filter(url);        } catch (Exception e) {                        url = null;        }    }    if (url == null) {        context.getCounter("CrawlDB filter", "URLs filtered").increment(1);    } else {                        newKey.set(url);        context.write(newKey, value);    }}
1
public void close() throws IOException
{}
0
public void setup(Reducer<Text, CrawlDatum, Text, CrawlDatum>.Context context)
{    Configuration conf = context.getConfiguration();    schedule = FetchScheduleFactory.getFetchSchedule(conf);}
0
public void reduce(Text key, Iterable<CrawlDatum> values, Context context) throws IOException, InterruptedException
{    CrawlDatum res = new CrawlDatum();        res.setFetchTime(-1);    MapWritable meta = new MapWritable();    for (CrawlDatum val : values) {        if (isNewer(res, val)) {                        meta = mergeMeta(val.getMetaData(), meta);            res.set(val);        } else {                        meta = mergeMeta(meta, val.getMetaData());        }    }    res.setMetaData(meta);    context.write(key, res);}
0
private boolean isNewer(CrawlDatum cd1, CrawlDatum cd2)
{    return schedule.calculateLastFetchTime(cd2) > schedule.calculateLastFetchTime(cd1) || schedule.calculateLastFetchTime(cd2) == schedule.calculateLastFetchTime(cd1) && cd2.getFetchTime() > cd1.getFetchTime();}
0
private MapWritable mergeMeta(MapWritable from, MapWritable to)
{    for (Entry<Writable, Writable> e : from.entrySet()) {        to.put(e.getKey(), e.getValue());    }    return to;}
0
public void merge(Path output, Path[] dbs, boolean normalize, boolean filter) throws Exception
{    Path lock = CrawlDb.lock(getConf(), output, false);    SimpleDateFormat sdf = new SimpleDateFormat("yyyy-MM-dd HH:mm:ss");    long start = System.currentTimeMillis();        Job job = createMergeJob(getConf(), output, normalize, filter);    for (int i = 0; i < dbs.length; i++) {        if (LOG.isInfoEnabled()) {                    }        FileInputFormat.addInputPath(job, new Path(dbs[i], CrawlDb.CURRENT_NAME));    }    Path outPath = FileOutputFormat.getOutputPath(job);    FileSystem fs = outPath.getFileSystem(getConf());    try {        boolean success = job.waitForCompletion(true);        if (!success) {            String message = "CrawlDbMerger job did not succeed, job status:" + job.getStatus().getState() + ", reason: " + job.getStatus().getFailureInfo();                        NutchJob.cleanupAfterFailure(outPath, lock, fs);            throw new RuntimeException(message);        }        CrawlDb.install(job, output);    } catch (IOException | InterruptedException | ClassNotFoundException e) {                NutchJob.cleanupAfterFailure(outPath, lock, fs);        throw e;    }    long end = System.currentTimeMillis();    }
1
public static Job createMergeJob(Configuration conf, Path output, boolean normalize, boolean filter) throws IOException
{    Path newCrawlDb = new Path(output, "merge-" + Integer.toString(new Random().nextInt(Integer.MAX_VALUE)));    Job job = NutchJob.getInstance(conf);    conf = job.getConfiguration();    job.setJobName("crawldb merge " + output);    job.setInputFormatClass(SequenceFileInputFormat.class);    job.setJarByClass(CrawlDbMerger.class);    job.setMapperClass(CrawlDbFilter.class);    conf.setBoolean(CrawlDbFilter.URL_FILTERING, filter);    conf.setBoolean(CrawlDbFilter.URL_NORMALIZING, normalize);    job.setReducerClass(Merger.class);    FileOutputFormat.setOutputPath(job, newCrawlDb);    job.setOutputFormatClass(MapFileOutputFormat.class);    job.setOutputKeyClass(Text.class);    job.setOutputValueClass(CrawlDatum.class);    return job;}
0
public static void main(String[] args) throws Exception
{    int res = ToolRunner.run(NutchConfiguration.create(), new CrawlDbMerger(), args);    System.exit(res);}
0
public int run(String[] args) throws Exception
{    if (args.length < 2) {        System.err.println("Usage: CrawlDbMerger <output_crawldb> <crawldb1> [<crawldb2> <crawldb3> ...] [-normalize] [-filter]");        System.err.println("\toutput_crawldb\toutput CrawlDb");        System.err.println("\tcrawldb1 ...\tinput CrawlDb-s (single input CrawlDb is ok)");        System.err.println("\t-normalize\tuse URLNormalizer on urls in the crawldb(s) (usually not needed)");        System.err.println("\t-filter\tuse URLFilters on urls in the crawldb(s)");        return -1;    }    Path output = new Path(args[0]);    ArrayList<Path> dbs = new ArrayList<>();    boolean filter = false;    boolean normalize = false;    for (int i = 1; i < args.length; i++) {        if ("-filter".equals(args[i])) {            filter = true;            continue;        } else if ("-normalize".equals(args[i])) {            normalize = true;            continue;        }        final Path dbPath = new Path(args[i]);        FileSystem fs = dbPath.getFileSystem(getConf());        if (fs.exists(dbPath))            dbs.add(dbPath);    }    try {        merge(output, dbs.toArray(new Path[dbs.size()]), normalize, filter);        return 0;    } catch (Exception e) {                return -1;    }}
1
private void openReaders(String crawlDb, Configuration config) throws IOException
{    if (readers != null)        return;    Path crawlDbPath = new Path(crawlDb, CrawlDb.CURRENT_NAME);    readers = MapFileOutputFormat.getReaders(crawlDbPath, config);}
0
private void closeReaders()
{    if (readers == null)        return;    for (int i = 0; i < readers.length; i++) {        try {            readers[i].close();        } catch (Exception e) {        }    }    readers = null;}
0
public synchronized void write(Text key, CrawlDatum value) throws IOException
{    out.writeByte('"');    out.writeBytes(key.toString());    out.writeByte('"');    out.writeByte(',');    out.writeBytes(Integer.toString(value.getStatus()));    out.writeByte(',');    out.writeByte('"');    out.writeBytes(CrawlDatum.getStatusName(value.getStatus()));    out.writeByte('"');    out.writeByte(',');    out.writeBytes(new Date(value.getFetchTime()).toString());    out.writeByte(',');    out.writeBytes(new Date(value.getModifiedTime()).toString());    out.writeByte(',');    out.writeBytes(Integer.toString(value.getRetriesSinceFetch()));    out.writeByte(',');    out.writeBytes(Float.toString(value.getFetchInterval()));    out.writeByte(',');    out.writeBytes(Float.toString((value.getFetchInterval() / FetchSchedule.SECONDS_PER_DAY)));    out.writeByte(',');    out.writeBytes(Float.toString(value.getScore()));    out.writeByte(',');    out.writeByte('"');    out.writeBytes(value.getSignature() != null ? StringUtil.toHexString(value.getSignature()) : "null");    out.writeByte('"');    out.writeByte(',');    out.writeByte('"');    if (value.getMetaData() != null) {        for (Entry<Writable, Writable> e : value.getMetaData().entrySet()) {            out.writeBytes(e.getKey().toString());            out.writeByte(':');            out.writeBytes(e.getValue().toString());            out.writeBytes("|||");        }    }    out.writeByte('"');    out.writeByte('\n');}
0
public synchronized void close(TaskAttemptContext context) throws IOException
{    out.close();}
0
public RecordWriter<Text, CrawlDatum> getRecordWriter(TaskAttemptContext context) throws IOException
{    String name = getUniqueFile(context, "part", "");    Path dir = FileOutputFormat.getOutputPath(context);    FileSystem fs = dir.getFileSystem(context.getConfiguration());    DataOutputStream fileOut = fs.create(new Path(dir, name), context);    return new LineRecordWriter(fileOut);}
0
public void setup(Mapper<Text, CrawlDatum, Text, NutchWritable>.Context context)
{    Configuration conf = context.getConfiguration();    sort = conf.getBoolean("db.reader.stats.sort", false);}
0
public void map(Text key, CrawlDatum value, Context context) throws IOException, InterruptedException
{    context.write(new Text("T"), COUNT_1);    context.write(new Text("status " + value.getStatus()), COUNT_1);    context.write(new Text("retry " + value.getRetriesSinceFetch()), COUNT_1);    if (Float.isNaN(value.getScore())) {        context.write(new Text("scNaN"), COUNT_1);    } else {        NutchWritable score = new NutchWritable(new FloatWritable(value.getScore()));        context.write(new Text("sc"), score);        context.write(new Text("sct"), score);        context.write(new Text("scd"), score);    }        NutchWritable fetchTime = new NutchWritable(new LongWritable(value.getFetchTime() / (1000 * 60)));    context.write(new Text("ft"), fetchTime);    context.write(new Text("ftt"), fetchTime);        NutchWritable fetchInterval = new NutchWritable(new LongWritable(value.getFetchInterval()));    context.write(new Text("fi"), fetchInterval);    context.write(new Text("fit"), fetchInterval);    if (sort) {        URL u = new URL(key.toString());        String host = u.getHost();        context.write(new Text("status " + value.getStatus() + " " + host), COUNT_1);    }}
0
public void setup(Reducer<Text, NutchWritable, Text, NutchWritable>.Context context)
{}
0
public void reduce(Text key, Iterable<NutchWritable> values, Context context) throws IOException, InterruptedException
{    String k = key.toString();    if (k.equals("T") || k.startsWith("status") || k.startsWith("retry") || k.equals("ftt") || k.equals("fit")) {                long sum = 0;        for (NutchWritable value : values) {            sum += ((LongWritable) value.get()).get();        }                context.write(key, new NutchWritable(new LongWritable(sum)));    } else if (k.equals("sc")) {        float min = Float.MAX_VALUE;        float max = Float.MIN_VALUE;        for (NutchWritable nvalue : values) {            float value = ((FloatWritable) nvalue.get()).get();            if (max < value) {                max = value;            }            if (min > value) {                min = value;            }        }        context.write(key, new NutchWritable(new FloatWritable(min)));        context.write(key, new NutchWritable(new FloatWritable(max)));    } else if (k.equals("ft") || k.equals("fi")) {        long min = Long.MAX_VALUE;        long max = Long.MIN_VALUE;        for (NutchWritable nvalue : values) {            long value = ((LongWritable) nvalue.get()).get();            if (max < value) {                max = value;            }            if (min > value) {                min = value;            }        }        context.write(key, new NutchWritable(new LongWritable(min)));        context.write(key, new NutchWritable(new LongWritable(max)));    } else if (k.equals("sct")) {        float cnt = 0.0f;        for (NutchWritable nvalue : values) {            float value = ((FloatWritable) nvalue.get()).get();            cnt += value;        }        context.write(key, new NutchWritable(new FloatWritable(cnt)));    } else if (k.equals("scd")) {        MergingDigest tdigest = null;        for (NutchWritable nvalue : values) {            Writable value = nvalue.get();            if (value instanceof BytesWritable) {                byte[] bytes = ((BytesWritable) value).getBytes();                MergingDigest tdig = MergingDigest.fromBytes(ByteBuffer.wrap(bytes));                if (tdigest == null) {                    tdigest = tdig;                } else {                    tdigest.add(tdig);                }            } else if (value instanceof FloatWritable) {                float val = ((FloatWritable) value).get();                if (!Float.isNaN(val)) {                    if (tdigest == null) {                        tdigest = (MergingDigest) TDigest.createMergingDigest(100.0);                    }                    tdigest.add(val);                }            }        }        ByteBuffer tdigestBytes = ByteBuffer.allocate(tdigest.smallByteSize());        tdigest.asSmallBytes(tdigestBytes);        context.write(key, new NutchWritable(new BytesWritable(tdigestBytes.array())));    }}
0
public void setup(Mapper<Text, CrawlDatum, FloatWritable, Text>.Context context)
{    Configuration conf = context.getConfiguration();    min = conf.getFloat("db.reader.topn.min", 0.0f);}
0
public void map(Text key, CrawlDatum value, Context context) throws IOException, InterruptedException
{    if (value.getScore() < min)                return;        fw.set(-value.getScore());        context.write(fw, key);}
0
public void reduce(FloatWritable key, Iterable<Text> values, Context context) throws IOException, InterruptedException
{    for (Text value : values) {        if (count < topN) {            key.set(-key.get());            context.write(key, value);            count++;        }    }}
0
public void setup(Reducer<FloatWritable, Text, FloatWritable, Text>.Context context)
{    Configuration conf = context.getConfiguration();    topN = conf.getLong("db.reader.topn", 100) / Integer.parseInt(conf.get("mapreduce.job.reduces"));}
0
public void close()
{    closeReaders();}
0
private TreeMap<String, Writable> processStatJobHelper(String crawlDb, Configuration config, boolean sort) throws IOException, InterruptedException, ClassNotFoundException
{    Path tmpFolder = new Path(crawlDb, "stat_tmp" + System.currentTimeMillis());    Job job = NutchJob.getInstance(config);    config = job.getConfiguration();    job.setJobName("stats " + crawlDb);    config.setBoolean("db.reader.stats.sort", sort);    FileInputFormat.addInputPath(job, new Path(crawlDb, CrawlDb.CURRENT_NAME));    job.setInputFormatClass(SequenceFileInputFormat.class);    job.setJarByClass(CrawlDbReader.class);    job.setMapperClass(CrawlDbStatMapper.class);    job.setCombinerClass(CrawlDbStatReducer.class);    job.setReducerClass(CrawlDbStatReducer.class);    FileOutputFormat.setOutputPath(job, tmpFolder);    job.setOutputFormatClass(SequenceFileOutputFormat.class);    job.setOutputKeyClass(Text.class);    job.setOutputValueClass(NutchWritable.class);        config.setBoolean("mapreduce.fileoutputcommitter.marksuccessfuljobs", false);    FileSystem fileSystem = tmpFolder.getFileSystem(config);    try {        boolean success = job.waitForCompletion(true);        if (!success) {            String message = "CrawlDbReader job did not succeed, job status:" + job.getStatus().getState() + ", reason: " + job.getStatus().getFailureInfo();                        fileSystem.delete(tmpFolder, true);            throw new RuntimeException(message);        }    } catch (IOException | InterruptedException | ClassNotFoundException e) {                fileSystem.delete(tmpFolder, true);        throw e;    }        SequenceFile.Reader[] readers = SegmentReaderUtil.getReaders(tmpFolder, config);    Text key = new Text();    NutchWritable value = new NutchWritable();    TreeMap<String, Writable> stats = new TreeMap<>();    for (int i = 0; i < readers.length; i++) {        SequenceFile.Reader reader = readers[i];        while (reader.next(key, value)) {            String k = key.toString();            Writable val = stats.get(k);            if (val == null) {                stats.put(k, value.get());                continue;            }            if (k.equals("sc")) {                float min = Float.MAX_VALUE;                float max = Float.MIN_VALUE;                if (stats.containsKey("scn")) {                    min = ((FloatWritable) stats.get("scn")).get();                } else {                    min = ((FloatWritable) stats.get("sc")).get();                }                if (stats.containsKey("scx")) {                    max = ((FloatWritable) stats.get("scx")).get();                } else {                    max = ((FloatWritable) stats.get("sc")).get();                }                float fvalue = ((FloatWritable) value.get()).get();                if (min > fvalue) {                    min = fvalue;                }                if (max < fvalue) {                    max = fvalue;                }                stats.put("scn", new FloatWritable(min));                stats.put("scx", new FloatWritable(max));            } else if (k.equals("ft") || k.equals("fi")) {                long min = Long.MAX_VALUE;                long max = Long.MIN_VALUE;                String minKey = k + "n";                String maxKey = k + "x";                if (stats.containsKey(minKey)) {                    min = ((LongWritable) stats.get(minKey)).get();                } else if (stats.containsKey(k)) {                    min = ((LongWritable) stats.get(k)).get();                }                if (stats.containsKey(maxKey)) {                    max = ((LongWritable) stats.get(maxKey)).get();                } else if (stats.containsKey(k)) {                    max = ((LongWritable) stats.get(k)).get();                }                long lvalue = ((LongWritable) value.get()).get();                if (min > lvalue) {                    min = lvalue;                }                if (max < lvalue) {                    max = lvalue;                }                stats.put(k + "n", new LongWritable(min));                stats.put(k + "x", new LongWritable(max));            } else if (k.equals("sct")) {                FloatWritable fvalue = (FloatWritable) value.get();                ((FloatWritable) val).set(((FloatWritable) val).get() + fvalue.get());            } else if (k.equals("scd")) {                MergingDigest tdigest = null;                MergingDigest tdig = MergingDigest.fromBytes(ByteBuffer.wrap(((BytesWritable) value.get()).getBytes()));                if (val instanceof BytesWritable) {                    tdigest = MergingDigest.fromBytes(ByteBuffer.wrap(((BytesWritable) val).getBytes()));                    tdigest.add(tdig);                } else {                    tdigest = tdig;                }                ByteBuffer tdigestBytes = ByteBuffer.allocate(tdigest.smallByteSize());                tdigest.asSmallBytes(tdigestBytes);                stats.put(k, new BytesWritable(tdigestBytes.array()));            } else {                LongWritable lvalue = (LongWritable) value.get();                ((LongWritable) val).set(((LongWritable) val).get() + lvalue.get());            }        }        reader.close();    }            stats.remove("sc");    stats.remove("fi");    stats.remove("ft");        fileSystem.delete(tmpFolder, true);    return stats;}
1
public CrawlDatum get(String crawlDb, String url, Configuration config) throws IOException
{    Text key = new Text(url);    CrawlDatum val = new CrawlDatum();    openReaders(crawlDb, config);    CrawlDatum res = (CrawlDatum) MapFileOutputFormat.getEntry(readers, new HashPartitioner<>(), key, val);    return res;}
0
protected int process(String line, StringBuilder output) throws Exception
{    Job job = NutchJob.getInstance(getConf());    Configuration config = job.getConfiguration();        closeReaders();    readUrl(this.crawlDb, line, config, output);    return 0;}
0
public void readUrl(String crawlDb, String url, Configuration config, StringBuilder output) throws IOException
{    CrawlDatum res = get(crawlDb, url, config);    output.append("URL: " + url + "\n");    if (res != null) {        output.append(res);    } else {        output.append("not found");    }    output.append("\n");}
0
public void processDumpJob(String crawlDb, String output, Configuration config, String format, String regex, String status, Integer retry, String expr, Float sample) throws IOException, ClassNotFoundException, InterruptedException
{    if (LOG.isInfoEnabled()) {                    }    Path outFolder = new Path(output);    Job job = NutchJob.getInstance(config);    job.setJobName("dump " + crawlDb);    Configuration jobConf = job.getConfiguration();    FileInputFormat.addInputPath(job, new Path(crawlDb, CrawlDb.CURRENT_NAME));    job.setInputFormatClass(SequenceFileInputFormat.class);    FileOutputFormat.setOutputPath(job, outFolder);    if (format.equals("csv")) {        job.setOutputFormatClass(CrawlDatumCsvOutputFormat.class);    } else if (format.equals("crawldb")) {        job.setOutputFormatClass(MapFileOutputFormat.class);    } else {        job.setOutputFormatClass(TextOutputFormat.class);    }    if (status != null)        jobConf.set("status", status);    if (regex != null)        jobConf.set("regex", regex);    if (retry != null)        jobConf.setInt("retry", retry);    if (expr != null) {        jobConf.set("expr", expr);            }    if (sample != null) {        jobConf.setFloat("sample", sample);    }    job.setMapperClass(CrawlDbDumpMapper.class);    job.setOutputKeyClass(Text.class);    job.setOutputValueClass(CrawlDatum.class);    job.setJarByClass(CrawlDbReader.class);    try {        boolean success = job.waitForCompletion(true);        if (!success) {            String message = "CrawlDbReader job did not succeed, job status:" + job.getStatus().getState() + ", reason: " + job.getStatus().getFailureInfo();                        throw new RuntimeException(message);        }    } catch (IOException | InterruptedException | ClassNotFoundException e) {                throw e;    }    if (LOG.isInfoEnabled()) {            }}
1
public void setup(Mapper<Text, CrawlDatum, Text, CrawlDatum>.Context context)
{    Configuration config = context.getConfiguration();    if (config.get("regex", null) != null) {        pattern = Pattern.compile(config.get("regex"));    }    status = config.get("status", null);    retry = config.getInt("retry", -1);    if (config.get("expr", null) != null) {        expr = JexlUtil.parseExpression(config.get("expr", null));    }    sample = config.getFloat("sample", 1);}
0
public void map(Text key, CrawlDatum value, Context context) throws IOException, InterruptedException
{        if (sample < 1 && Math.random() > sample) {        return;    }        if (retry != -1) {        if (value.getRetriesSinceFetch() < retry) {            return;        }    }        if (status != null && !status.equalsIgnoreCase(CrawlDatum.getStatusName(value.getStatus())))        return;        if (pattern != null) {        matcher = pattern.matcher(key.toString());        if (!matcher.matches()) {            return;        }    }        if (expr != null) {        if (!value.evaluate(expr, key.toString())) {            return;        }    }    context.write(key, value);}
0
public void processTopNJob(String crawlDb, long topN, float min, String output, Configuration config) throws IOException, ClassNotFoundException, InterruptedException
{    if (LOG.isInfoEnabled()) {                    }    Path outFolder = new Path(output);    Path tempDir = new Path(config.get("mapreduce.cluster.temp.dir", ".") + "/readdb-topN-temp-" + Integer.toString(new Random().nextInt(Integer.MAX_VALUE)));    Job job = NutchJob.getInstance(config);    job.setJobName("topN prepare " + crawlDb);    FileInputFormat.addInputPath(job, new Path(crawlDb, CrawlDb.CURRENT_NAME));    job.setInputFormatClass(SequenceFileInputFormat.class);    job.setJarByClass(CrawlDbReader.class);    job.setMapperClass(CrawlDbTopNMapper.class);    job.setReducerClass(Reducer.class);    FileOutputFormat.setOutputPath(job, tempDir);    job.setOutputFormatClass(SequenceFileOutputFormat.class);    job.setOutputKeyClass(FloatWritable.class);    job.setOutputValueClass(Text.class);    job.getConfiguration().setFloat("db.reader.topn.min", min);    FileSystem fs = tempDir.getFileSystem(config);    try {        boolean success = job.waitForCompletion(true);        if (!success) {            String message = "CrawlDbReader job did not succeed, job status:" + job.getStatus().getState() + ", reason: " + job.getStatus().getFailureInfo();                        fs.delete(tempDir, true);            throw new RuntimeException(message);        }    } catch (IOException | InterruptedException | ClassNotFoundException e) {                fs.delete(tempDir, true);        throw e;    }    if (LOG.isInfoEnabled()) {            }    job = NutchJob.getInstance(config);    job.setJobName("topN collect " + crawlDb);    job.getConfiguration().setLong("db.reader.topn", topN);    FileInputFormat.addInputPath(job, tempDir);    job.setInputFormatClass(SequenceFileInputFormat.class);    job.setMapperClass(Mapper.class);    job.setReducerClass(CrawlDbTopNReducer.class);    job.setJarByClass(CrawlDbReader.class);    FileOutputFormat.setOutputPath(job, outFolder);    job.setOutputFormatClass(TextOutputFormat.class);    job.setOutputKeyClass(FloatWritable.class);    job.setOutputValueClass(Text.class);        job.setNumReduceTasks(1);    try {        boolean success = job.waitForCompletion(true);        if (!success) {            String message = "CrawlDbReader job did not succeed, job status:" + job.getStatus().getState() + ", reason: " + job.getStatus().getFailureInfo();                        fs.delete(tempDir, true);            throw new RuntimeException(message);        }    } catch (IOException | InterruptedException | ClassNotFoundException e) {                fs.delete(tempDir, true);        throw e;    }    fs.delete(tempDir, true);    if (LOG.isInfoEnabled()) {            }}
1
public int run(String[] args) throws IOException, InterruptedException, ClassNotFoundException, Exception
{    @SuppressWarnings("resource")    CrawlDbReader dbr = new CrawlDbReader();    if (args.length < 2) {        System.err.println("Usage: CrawlDbReader <crawldb> (-stats | -dump <out_dir> | -topN <nnnn> <out_dir> [<min>] | -url <url>)");        System.err.println("\t<crawldb>\tdirectory name where crawldb is located");        System.err.println("\t-stats [-sort] \tprint overall statistics to System.out");        System.err.println("\t\t[-sort]\tlist status sorted by host");        System.err.println("\t-dump <out_dir> [-format normal|csv|crawldb]\tdump the whole db to a text file in <out_dir>");        System.err.println("\t\t[-format csv]\tdump in Csv format");        System.err.println("\t\t[-format normal]\tdump in standard format (default option)");        System.err.println("\t\t[-format crawldb]\tdump as CrawlDB");        System.err.println("\t\t[-regex <expr>]\tfilter records with expression");        System.err.println("\t\t[-retry <num>]\tminimum retry count");        System.err.println("\t\t[-status <status>]\tfilter records by CrawlDatum status");        System.err.println("\t\t[-expr <expr>]\tJexl expression to evaluate for this record");        System.err.println("\t\t[-sample <fraction>]\tOnly process a random sample with this ratio");        System.err.println("\t-url <url>\tprint information on <url> to System.out");        System.err.println("\t-topN <nnnn> <out_dir> [<min>]\tdump top <nnnn> urls sorted by score to <out_dir>");        System.err.println("\t\t[<min>]\tskip records with scores below this value.");        System.err.println("\t\t\tThis can significantly improve performance.");        return -1;    }    String param = null;    String crawlDb = args[0];    this.crawlDb = crawlDb;    int numConsumed = 0;    Configuration config = getConf();    for (int i = 1; i < args.length; i++) {        if (args[i].equals("-stats")) {            boolean toSort = false;            if (i < args.length - 1 && "-sort".equals(args[i + 1])) {                toSort = true;                i++;            }            dbr.processStatJob(crawlDb, config, toSort);        } else if (args[i].equals("-dump")) {            param = args[++i];            String format = "normal";            String regex = null;            Integer retry = null;            String status = null;            String expr = null;            Float sample = null;            for (int j = i + 1; j < args.length; j++) {                if (args[j].equals("-format")) {                    format = args[++j];                    i = i + 2;                }                if (args[j].equals("-regex")) {                    regex = args[++j];                    i = i + 2;                }                if (args[j].equals("-retry")) {                    retry = Integer.parseInt(args[++j]);                    i = i + 2;                }                if (args[j].equals("-status")) {                    status = args[++j];                    i = i + 2;                }                if (args[j].equals("-expr")) {                    expr = args[++j];                    i = i + 2;                }                if (args[j].equals("-sample")) {                    sample = Float.parseFloat(args[++j]);                    i = i + 2;                }            }            dbr.processDumpJob(crawlDb, param, config, format, regex, status, retry, expr, sample);        } else if (args[i].equals("-url")) {            param = args[++i];            StringBuilder output = new StringBuilder();            dbr.readUrl(crawlDb, param, config, output);            System.out.print(output);        } else if (args[i].equals("-topN")) {            param = args[++i];            long topN = Long.parseLong(param);            param = args[++i];            float min = 0.0f;            if (i < args.length - 1) {                min = Float.parseFloat(args[++i]);            }            dbr.processTopNJob(crawlDb, topN, min, param, config);        } else if ((numConsumed = super.parseArgs(args, i)) > 0) {            i += numConsumed - 1;        } else {            System.err.println("\nError: wrong argument " + args[i]);            return -1;        }    }    if (numConsumed > 0) {                return super.run();    }    return 0;}
0
public static void main(String[] args) throws Exception
{    int result = ToolRunner.run(NutchConfiguration.create(), new CrawlDbReader(), args);    System.exit(result);}
0
public Object query(Map<String, String> args, Configuration conf, String type, String crawlId) throws Exception
{    Map<String, Object> results = new HashMap<>();    String crawlDb = crawlId + "/crawldb";    if (type.equalsIgnoreCase("stats")) {        boolean sort = false;        if (args.containsKey("sort")) {            if (args.get("sort").equalsIgnoreCase("true"))                sort = true;        }        TreeMap<String, Writable> stats = processStatJobHelper(crawlDb, NutchConfiguration.create(), sort);        LongWritable totalCnt = (LongWritable) stats.get("T");        stats.remove("T");        results.put("totalUrls", String.valueOf(totalCnt.get()));        Map<String, Object> statusMap = new HashMap<>();        for (Map.Entry<String, Writable> entry : stats.entrySet()) {            String k = entry.getKey();            long val = 0L;            double fval = 0.0;            if (entry.getValue() instanceof LongWritable) {                val = ((LongWritable) entry.getValue()).get();            } else if (entry.getValue() instanceof FloatWritable) {                fval = ((FloatWritable) entry.getValue()).get();            } else if (entry.getValue() instanceof BytesWritable) {                continue;            }            if (k.equals("scn")) {                results.put("minScore", String.valueOf(fval));            } else if (k.equals("scx")) {                results.put("maxScore", String.valueOf(fval));            } else if (k.equals("sct")) {                results.put("avgScore", String.valueOf((fval / totalCnt.get())));            } else if (k.startsWith("status")) {                String[] st = k.split(" ");                int code = Integer.parseInt(st[1]);                if (st.length > 2) {                    @SuppressWarnings("unchecked")                    Map<String, Object> individualStatusInfo = (Map<String, Object>) statusMap.get(String.valueOf(code));                    Map<String, String> hostValues;                    if (individualStatusInfo.containsKey("hostValues")) {                        hostValues = (Map<String, String>) individualStatusInfo.get("hostValues");                    } else {                        hostValues = new HashMap<>();                        individualStatusInfo.put("hostValues", hostValues);                    }                    hostValues.put(st[2], String.valueOf(val));                } else {                    Map<String, Object> individualStatusInfo = new HashMap<>();                    individualStatusInfo.put("statusValue", CrawlDatum.getStatusName((byte) code));                    individualStatusInfo.put("count", String.valueOf(val));                    statusMap.put(String.valueOf(code), individualStatusInfo);                }            } else {                results.put(k, String.valueOf(val));            }        }        results.put("status", statusMap);        return results;    }    if (type.equalsIgnoreCase("dump")) {        String output = args.get("out_dir");        String format = "normal";        String regex = null;        Integer retry = null;        String status = null;        String expr = null;        Float sample = null;        if (args.containsKey("format")) {            format = args.get("format");        }        if (args.containsKey("regex")) {            regex = args.get("regex");        }        if (args.containsKey("retry")) {            retry = Integer.parseInt(args.get("retry"));        }        if (args.containsKey("status")) {            status = args.get("status");        }        if (args.containsKey("expr")) {            expr = args.get("expr");        }        if (args.containsKey("sample")) {            sample = Float.parseFloat(args.get("sample"));        }        processDumpJob(crawlDb, output, conf, format, regex, status, retry, expr, sample);        File dumpFile = new File(output + "/part-00000");        return dumpFile;    }    if (type.equalsIgnoreCase("topN")) {        String output = args.get("out_dir");        long topN = Long.parseLong(args.get("nnn"));        float min = 0.0f;        if (args.containsKey("min")) {            min = Float.parseFloat(args.get("min"));        }        processTopNJob(crawlDb, topN, min, output, conf);        File dumpFile = new File(output + "/part-00000");        return dumpFile;    }    if (type.equalsIgnoreCase("url")) {        String url = args.get("url");        CrawlDatum res = get(crawlDb, url, conf);        results.put("status", res.getStatus());        results.put("fetchTime", new Date(res.getFetchTime()));        results.put("modifiedTime", new Date(res.getModifiedTime()));        results.put("retriesSinceFetch", res.getRetriesSinceFetch());        results.put("retryInterval", res.getFetchInterval());        results.put("score", res.getScore());        results.put("signature", StringUtil.toHexString(res.getSignature()));        Map<String, String> metadata = new HashMap<>();        if (res.getMetaData() != null) {            for (Entry<Writable, Writable> e : res.getMetaData().entrySet()) {                metadata.put(String.valueOf(e.getKey()), String.valueOf(e.getValue()));            }        }        results.put("metadata", metadata);        return results;    }    return results;}
0
public void setup(Reducer<Text, CrawlDatum, Text, CrawlDatum>.Context context)
{    Configuration conf = context.getConfiguration();    retryMax = conf.getInt("db.fetch.retry.max", 3);    scfilters = new ScoringFilters(conf);    additionsAllowed = conf.getBoolean(CrawlDb.CRAWLDB_ADDITIONS_ALLOWED, true);    maxInterval = conf.getInt("db.fetch.interval.max", 0);    schedule = FetchScheduleFactory.getFetchSchedule(conf);    int maxLinks = conf.getInt("db.update.max.inlinks", 10000);    linked = new InlinkPriorityQueue(maxLinks);}
0
public void close()
{}
0
public void reduce(Text key, Iterable<CrawlDatum> values, Context context) throws IOException, InterruptedException
{    CrawlDatum fetch = new CrawlDatum();    CrawlDatum old = new CrawlDatum();    boolean fetchSet = false;    boolean oldSet = false;    byte[] signature = null;        boolean multiple = false;    linked.clear();    org.apache.hadoop.io.MapWritable metaFromParse = null;    for (CrawlDatum datum : values) {        if (!multiple)            multiple = true;        if (CrawlDatum.hasDbStatus(datum)) {            if (!oldSet) {                if (multiple) {                    old.set(datum);                } else {                                        old = datum;                }                oldSet = true;            } else {                                if (old.getFetchTime() < datum.getFetchTime())                    old.set(datum);            }            continue;        }        if (CrawlDatum.hasFetchStatus(datum)) {            if (!fetchSet) {                if (multiple) {                    fetch.set(datum);                } else {                    fetch = datum;                }                fetchSet = true;            } else {                                if (fetch.getFetchTime() < datum.getFetchTime())                    fetch.set(datum);            }            continue;        }        switch(        datum.getStatus()) {            case CrawlDatum.STATUS_LINKED:                CrawlDatum link;                if (multiple) {                    link = new CrawlDatum();                    link.set(datum);                } else {                    link = datum;                }                linked.insert(link);                break;            case CrawlDatum.STATUS_SIGNATURE:                signature = datum.getSignature();                break;            case CrawlDatum.STATUS_PARSE_META:                metaFromParse = datum.getMetaData();                break;            default:                        }    }            int numLinks = linked.size();    List<CrawlDatum> linkList = new ArrayList<>(numLinks);    for (int i = numLinks - 1; i >= 0; i--) {        linkList.add(linked.pop());    }        if (!oldSet && !additionsAllowed)        return;        if (!fetchSet && linkList.size() > 0) {        fetch = linkList.get(0);        fetchSet = true;    }        if (!fetchSet) {        if (oldSet) {                        try {                scfilters.orphanedScore(key, old);            } catch (ScoringFilterException e) {                if (LOG.isWarnEnabled()) {                                    }            }            context.write(key, old);            context.getCounter("CrawlDB status", CrawlDatum.getStatusName(old.getStatus())).increment(1);        } else {                    }        return;    }    if (signature == null)        signature = fetch.getSignature();    long prevModifiedTime = oldSet ? old.getModifiedTime() : 0L;    long prevFetchTime = oldSet ? old.getFetchTime() : 0L;        result.set(fetch);    if (oldSet) {                if (old.getMetaData().size() > 0) {            result.putAllMetaData(old);                        if (fetch.getMetaData().size() > 0)                result.putAllMetaData(fetch);        }                if (old.getModifiedTime() > 0 && fetch.getModifiedTime() == 0) {            result.setModifiedTime(old.getModifiedTime());        }    }    switch(    fetch.getStatus()) {        case         CrawlDatum.STATUS_LINKED:            if (oldSet) {                                                result.set(old);            } else {                result = schedule.initializeSchedule(key, result);                result.setStatus(CrawlDatum.STATUS_DB_UNFETCHED);                try {                    scfilters.initialScore(key, result);                } catch (ScoringFilterException e) {                    if (LOG.isWarnEnabled()) {                                            }                    result.setScore(0.0f);                }            }            break;                case CrawlDatum.STATUS_FETCH_SUCCESS:                case CrawlDatum.STATUS_FETCH_REDIR_TEMP:        case CrawlDatum.STATUS_FETCH_REDIR_PERM:        case         CrawlDatum.STATUS_FETCH_NOTMODIFIED:                        if (metaFromParse != null) {                for (Entry<Writable, Writable> e : metaFromParse.entrySet()) {                    result.getMetaData().put(e.getKey(), e.getValue());                }            }                        int modified = FetchSchedule.STATUS_UNKNOWN;            if (fetch.getStatus() == CrawlDatum.STATUS_FETCH_NOTMODIFIED) {                modified = FetchSchedule.STATUS_NOTMODIFIED;            } else if (fetch.getStatus() == CrawlDatum.STATUS_FETCH_SUCCESS) {                                if (oldSet && old.getSignature() != null && signature != null) {                    if (SignatureComparator._compare(old.getSignature(), signature) != 0) {                        modified = FetchSchedule.STATUS_MODIFIED;                    } else {                        modified = FetchSchedule.STATUS_NOTMODIFIED;                    }                }            }                        result = schedule.setFetchSchedule(key, result, prevFetchTime, prevModifiedTime, fetch.getFetchTime(), fetch.getModifiedTime(), modified);                        if (modified == FetchSchedule.STATUS_NOTMODIFIED) {                result.setStatus(CrawlDatum.STATUS_DB_NOTMODIFIED);                                                result.setModifiedTime(prevModifiedTime);                if (oldSet)                    result.setSignature(old.getSignature());            } else {                switch(fetch.getStatus()) {                    case CrawlDatum.STATUS_FETCH_SUCCESS:                        result.setStatus(CrawlDatum.STATUS_DB_FETCHED);                        break;                    case CrawlDatum.STATUS_FETCH_REDIR_PERM:                        result.setStatus(CrawlDatum.STATUS_DB_REDIR_PERM);                        break;                    case CrawlDatum.STATUS_FETCH_REDIR_TEMP:                        result.setStatus(CrawlDatum.STATUS_DB_REDIR_TEMP);                        break;                    default:                                                if (oldSet)                            result.setStatus(old.getStatus());                        else                            result.setStatus(CrawlDatum.STATUS_DB_UNFETCHED);                }                result.setSignature(signature);            }                        if (maxInterval < result.getFetchInterval())                result = schedule.forceRefetch(key, result, false);            break;        case CrawlDatum.STATUS_SIGNATURE:            if (LOG.isWarnEnabled()) {                            }            return;        case         CrawlDatum.STATUS_FETCH_RETRY:            if (oldSet) {                                result.setSignature(old.getSignature());            }            result = schedule.setPageRetrySchedule(key, result, prevFetchTime, prevModifiedTime, fetch.getFetchTime());            if (result.getRetriesSinceFetch() < retryMax) {                result.setStatus(CrawlDatum.STATUS_DB_UNFETCHED);            } else {                result.setStatus(CrawlDatum.STATUS_DB_GONE);                result = schedule.setPageGoneSchedule(key, result, prevFetchTime, prevModifiedTime, fetch.getFetchTime());            }            break;        case         CrawlDatum.STATUS_FETCH_GONE:            if (oldSet)                                result.setSignature(old.getSignature());            result.setStatus(CrawlDatum.STATUS_DB_GONE);            result = schedule.setPageGoneSchedule(key, result, prevFetchTime, prevModifiedTime, fetch.getFetchTime());            break;        default:            throw new RuntimeException("Unknown status: " + fetch.getStatus() + " " + key);    }    try {        scfilters.updateDbScore(key, oldSet ? old : null, result, linkList);    } catch (Exception e) {        if (LOG.isWarnEnabled()) {                    }    }        result.getMetaData().remove(Nutch.WRITABLE_GENERATE_TIME_KEY);    context.write(key, result);    context.getCounter("CrawlDB status", CrawlDatum.getStatusName(result.getStatus())).increment(1);}
1
protected boolean lessThan(Object arg0, Object arg1)
{    CrawlDatum candidate = (CrawlDatum) arg0;    CrawlDatum least = (CrawlDatum) arg1;    return candidate.getScore() > least.getScore();}
0
public void setup(Mapper<Text, CrawlDatum, BytesWritable, CrawlDatum>.Context context)
{    Configuration arg0 = context.getConfiguration();    groupMode = arg0.get(DEDUPLICATION_GROUP_MODE);}
0
public void map(Text key, CrawlDatum value, Context context) throws IOException, InterruptedException
{    if (value.getStatus() == CrawlDatum.STATUS_DB_FETCHED || value.getStatus() == CrawlDatum.STATUS_DB_NOTMODIFIED) {                byte[] signature = value.getSignature();        if (signature == null)            return;        String url = key.toString();        BytesWritable sig = null;        byte[] data;        switch(groupMode) {            case "none":                sig = new BytesWritable(signature);                break;            case "host":                byte[] host = URLUtil.getHost(url).getBytes();                data = new byte[signature.length + host.length];                System.arraycopy(signature, 0, data, 0, signature.length);                System.arraycopy(host, 0, data, signature.length, host.length);                sig = new BytesWritable(data);                break;            case "domain":                byte[] domain = URLUtil.getDomainName(url).getBytes();                data = new byte[signature.length + domain.length];                System.arraycopy(signature, 0, data, 0, signature.length);                System.arraycopy(domain, 0, data, signature.length, domain.length);                sig = new BytesWritable(data);                break;        }                value.getMetaData().put(urlKey, key);                context.write(sig, value);    }}
0
public void setup(Reducer<BytesWritable, CrawlDatum, Text, CrawlDatum>.Context context)
{    Configuration conf = context.getConfiguration();    compareOrder = conf.get(DEDUPLICATION_COMPARE_ORDER).split(",");}
0
private void writeOutAsDuplicate(CrawlDatum datum, Context context) throws IOException, InterruptedException
{    datum.setStatus(CrawlDatum.STATUS_DB_DUPLICATE);    Text key = (Text) datum.getMetaData().remove(urlKey);    context.getCounter("DeduplicationJobStatus", "Documents marked as duplicate").increment(1);    context.write(key, datum);}
0
public void reduce(BytesWritable key, Iterable<CrawlDatum> values, Context context) throws IOException, InterruptedException
{    CrawlDatum existingDoc = null;    for (CrawlDatum newDoc : values) {        if (existingDoc == null) {            existingDoc = new CrawlDatum();            existingDoc.set(newDoc);            continue;        }        CrawlDatum duplicate = getDuplicate(existingDoc, newDoc);        if (duplicate != null) {            writeOutAsDuplicate(duplicate, context);            if (duplicate == existingDoc) {                                existingDoc.set(newDoc);            }        }    }}
0
private CrawlDatum getDuplicate(CrawlDatum existingDoc, CrawlDatum newDoc) throws IOException
{    for (int i = 0; i < compareOrder.length; i++) {        switch(compareOrder[i]) {            case "score":                                if (existingDoc.getScore() < newDoc.getScore()) {                    return existingDoc;                } else if (existingDoc.getScore() > newDoc.getScore()) {                                        return newDoc;                }                break;            case "fetchTime":                                if (existingDoc.getFetchTime() > newDoc.getFetchTime()) {                                        return newDoc;                } else if (existingDoc.getFetchTime() < newDoc.getFetchTime()) {                                        return existingDoc;                }                break;            case "httpsOverHttp":                                                String url1 = existingDoc.getMetaData().get(urlKey).toString();                String url2 = newDoc.getMetaData().get(urlKey).toString();                if (url1.startsWith("https://") && url2.startsWith("http://") && url1.substring(8).equals(url2.substring(7))) {                                        return newDoc;                } else if (url2.startsWith("https://") && url1.startsWith("http://") && url2.substring(8).equals(url1.substring(7))) {                                        return existingDoc;                }                break;            case "urlLength":                                String urlExisting;                String urlnewDoc;                try {                    urlExisting = URLDecoder.decode(existingDoc.getMetaData().get(urlKey).toString(), "UTF8");                    urlnewDoc = URLDecoder.decode(newDoc.getMetaData().get(urlKey).toString(), "UTF8");                } catch (UnsupportedEncodingException e) {                                        throw new IOException("UnsupportedEncodingException for " + urlKey);                }                if (urlExisting.length() < urlnewDoc.length()) {                                        return newDoc;                } else if (urlExisting.length() > urlnewDoc.length()) {                                        return existingDoc;                }                break;        }    }        return null;}
1
public void setup(Reducer<Text, CrawlDatum, Text, CrawlDatum>.Context context)
{}
0
public void reduce(Text key, Iterable<CrawlDatum> values, Context context) throws IOException, InterruptedException
{    boolean duplicateSet = false;    for (CrawlDatum val : values) {        if (val.getStatus() == CrawlDatum.STATUS_DB_DUPLICATE) {            duplicate.set(val);            duplicateSet = true;        } else {            old.set(val);        }    }        if (duplicateSet) {        context.write(key, duplicate);        return;    }        context.write(key, old);}
0
public int run(String[] args) throws IOException
{    if (args.length < 1) {        System.err.println("Usage: DeduplicationJob <crawldb> [-group <none|host|domain>] [-compareOrder <score>,<fetchTime>,<httpsOverHttp>,<urlLength>]");        return 1;    }    String group = "none";    Path crawlDb = new Path(args[0]);    String compareOrder = "score,fetchTime,urlLength";    for (int i = 1; i < args.length; i++) {        if (args[i].equals("-group"))            group = args[++i];        if (args[i].equals("-compareOrder")) {            compareOrder = args[++i];            if (compareOrder.indexOf("score") == -1 || compareOrder.indexOf("fetchTime") == -1 || compareOrder.indexOf("urlLength") == -1) {                System.err.println("DeduplicationJob: compareOrder must contain score, fetchTime and urlLength.");                return 1;            }        }    }    SimpleDateFormat sdf = new SimpleDateFormat("yyyy-MM-dd HH:mm:ss");    long start = System.currentTimeMillis();        Path tempDir = new Path(crawlDb, "dedup-temp-" + Integer.toString(new Random().nextInt(Integer.MAX_VALUE)));    Job job = NutchJob.getInstance(getConf());    Configuration conf = job.getConfiguration();    job.setJobName("Deduplication on " + crawlDb);    conf.set(DEDUPLICATION_GROUP_MODE, group);    conf.set(DEDUPLICATION_COMPARE_ORDER, compareOrder);    job.setJarByClass(DeduplicationJob.class);    FileInputFormat.addInputPath(job, new Path(crawlDb, CrawlDb.CURRENT_NAME));    job.setInputFormatClass(SequenceFileInputFormat.class);    FileOutputFormat.setOutputPath(job, tempDir);    job.setOutputFormatClass(SequenceFileOutputFormat.class);    job.setMapOutputKeyClass(BytesWritable.class);    job.setMapOutputValueClass(CrawlDatum.class);    job.setOutputKeyClass(Text.class);    job.setOutputValueClass(CrawlDatum.class);    job.setMapperClass(DBFilter.class);    job.setReducerClass(DedupReducer.class);    FileSystem fs = tempDir.getFileSystem(getConf());    try {        boolean success = job.waitForCompletion(true);        if (!success) {            String message = "Crawl job did not succeed, job status:" + job.getStatus().getState() + ", reason: " + job.getStatus().getFailureInfo();                        fs.delete(tempDir, true);            throw new RuntimeException(message);        }        CounterGroup g = job.getCounters().getGroup("DeduplicationJobStatus");        if (g != null) {            Counter counter = g.findCounter("Documents marked as duplicate");            long dups = counter.getValue();                    }    } catch (IOException | InterruptedException | ClassNotFoundException e) {                fs.delete(tempDir, true);        return -1;    }        if (LOG.isInfoEnabled()) {            }    Job mergeJob = CrawlDb.createJob(getConf(), crawlDb);    FileInputFormat.addInputPath(mergeJob, tempDir);    mergeJob.setReducerClass(StatusUpdateReducer.class);    mergeJob.setJarByClass(DeduplicationJob.class);    fs = crawlDb.getFileSystem(getConf());    Path outPath = FileOutputFormat.getOutputPath(job);    Path lock = CrawlDb.lock(getConf(), crawlDb, false);    try {        boolean success = mergeJob.waitForCompletion(true);        if (!success) {            String message = "Crawl job did not succeed, job status:" + mergeJob.getStatus().getState() + ", reason: " + mergeJob.getStatus().getFailureInfo();                        fs.delete(tempDir, true);            NutchJob.cleanupAfterFailure(outPath, lock, fs);            throw new RuntimeException(message);        }    } catch (IOException | InterruptedException | ClassNotFoundException e) {                fs.delete(tempDir, true);        NutchJob.cleanupAfterFailure(outPath, lock, fs);        return -1;    }    CrawlDb.install(mergeJob, crawlDb);        fs.delete(tempDir, true);    long end = System.currentTimeMillis();        return 0;}
1
public static void main(String[] args) throws Exception
{    int result = ToolRunner.run(NutchConfiguration.create(), new DeduplicationJob(), args);    System.exit(result);}
0
public Map<String, Object> run(Map<String, Object> args, String crawlId) throws Exception
{    Map<String, Object> results = new HashMap<>();    String[] arg = new String[1];    String crawldb;    if (args.containsKey(Nutch.ARG_CRAWLDB)) {        crawldb = (String) args.get(Nutch.ARG_CRAWLDB);    } else {        crawldb = crawlId + "/crawldb";    }    arg[0] = crawldb;    int res = run(arg);    results.put(Nutch.VAL_RESULT, Integer.toString(res));    return results;}
0
public CrawlDatum setFetchSchedule(Text url, CrawlDatum datum, long prevFetchTime, long prevModifiedTime, long fetchTime, long modifiedTime, int state)
{    datum = super.setFetchSchedule(url, datum, prevFetchTime, prevModifiedTime, fetchTime, modifiedTime, state);    if (datum.getFetchInterval() == 0) {        datum.setFetchInterval(defaultInterval);    }    datum.setFetchTime(fetchTime + (long) datum.getFetchInterval() * 1000);    if (modifiedTime <= 0 || state == FetchSchedule.STATUS_MODIFIED) {                modifiedTime = fetchTime;    }    datum.setModifiedTime(modifiedTime);    return datum;}
0
public static synchronized FetchSchedule getFetchSchedule(Configuration conf)
{    String clazz = conf.get("db.fetch.schedule.class", DefaultFetchSchedule.class.getName());    ObjectCache objectCache = ObjectCache.get(conf);    FetchSchedule impl = (FetchSchedule) objectCache.getObject(clazz);    if (impl == null) {        try {                        Class<?> implClass = Class.forName(clazz);            impl = (FetchSchedule) implClass.getConstructor().newInstance();            impl.setConf(conf);            objectCache.setObject(clazz, impl);        } catch (Exception e) {            throw new RuntimeException("Couldn't create " + clazz, e);        }    }    return impl;}
1
public void readFields(DataInput in) throws IOException
{    url.readFields(in);    datum.readFields(in);    segnum.readFields(in);}
0
public void write(DataOutput out) throws IOException
{    url.write(out);    datum.write(out);    segnum.write(out);}
0
public String toString()
{    return "url=" + url.toString() + ", datum=" + datum.toString() + ", segnum=" + segnum.toString();}
0
public int getPartition(FloatWritable key, Writable value, int numReduceTasks)
{    return partitioner.getPartition(((SelectorEntry) value).url, key, numReduceTasks);}
0
public Configuration getConf()
{    return partitioner.getConf();}
0
public void setConf(Configuration conf)
{    partitioner.setConf(conf);}
0
public void setup(Mapper<Text, CrawlDatum, FloatWritable, SelectorEntry>.Context context) throws IOException
{    conf = context.getConfiguration();    curTime = conf.getLong(GENERATOR_CUR_TIME, System.currentTimeMillis());    filters = new URLFilters(conf);    scfilters = new ScoringFilters(conf);    filter = conf.getBoolean(GENERATOR_FILTER, true);    /* CrawlDb items are unblocked after 7 days as default */    genDelay = conf.getLong(GENERATOR_DELAY, 604800000L);    long time = conf.getLong(Nutch.GENERATE_TIME_KEY, 0L);    if (time > 0)        genTime.set(time);    schedule = FetchScheduleFactory.getFetchSchedule(conf);    scoreThreshold = conf.getFloat(GENERATOR_MIN_SCORE, Float.NaN);    intervalThreshold = conf.getInt(GENERATOR_MIN_INTERVAL, -1);    String restrictStatusString = conf.getTrimmed(GENERATOR_RESTRICT_STATUS, "");    if (!restrictStatusString.isEmpty()) {        restrictStatus = CrawlDatum.getStatusByName(restrictStatusString);    }    expr = JexlUtil.parseExpression(conf.get(GENERATOR_EXPR, null));}
0
public void map(Text key, CrawlDatum value, Context context) throws IOException, InterruptedException
{    Text url = key;    if (filter) {                try {            if (filters.filter(url.toString()) == null)                return;        } catch (URLFilterException e) {                    }    }    CrawlDatum crawlDatum = value;        if (!schedule.shouldFetch(url, crawlDatum, curTime)) {                context.getCounter("Generator", "SCHEDULE_REJECTED").increment(1);        return;    }    LongWritable oldGenTime = (LongWritable) crawlDatum.getMetaData().get(Nutch.WRITABLE_GENERATE_TIME_KEY);    if (oldGenTime != null) {                if (        oldGenTime.get() + genDelay > curTime)                        context.getCounter("Generator", "WAIT_FOR_UPDATE").increment(1);        return;    }    float sort = 1.0f;    try {        sort = scfilters.generatorSortValue(key, crawlDatum, sort);    } catch (ScoringFilterException sfe) {        if (LOG.isWarnEnabled()) {                    }    }        if (expr != null) {        if (!crawlDatum.evaluate(expr, key.toString())) {            context.getCounter("Generator", "EXPR_REJECTED").increment(1);            return;        }    }    if (restrictStatus != -1 && restrictStatus != crawlDatum.getStatus()) {        context.getCounter("Generator", "STATUS_REJECTED").increment(1);        return;    }        if (!Float.isNaN(scoreThreshold) && sort < scoreThreshold) {        context.getCounter("Generator", "SCORE_TOO_LOW").increment(1);        return;    }        if (intervalThreshold != -1 && crawlDatum.getFetchInterval() > intervalThreshold) {        context.getCounter("Generator", "INTERVAL_REJECTED").increment(1);        return;    }        sortValue.set(sort);        crawlDatum.getMetaData().put(Nutch.WRITABLE_GENERATE_TIME_KEY, genTime);    entry.datum = crawlDatum;    entry.url = key;        context.write(sortValue, entry);}
1
public void open()
{    if (conf.get(GENERATOR_HOSTDB) != null) {        try {            Path path = new Path(conf.get(GENERATOR_HOSTDB), "current");            hostdbReaders = SegmentReaderUtil.getReaders(path, conf);        } catch (IOException e) {                    }    }}
1
public void close()
{    if (hostdbReaders != null) {        try {            for (int i = 0; i < hostdbReaders.length; i++) {                hostdbReaders[i].close();            }        } catch (IOException e) {                    }    }}
1
private JexlContext createContext(HostDatum datum)
{    JexlContext context = new MapContext();    context.set("dnsFailures", datum.getDnsFailures());    context.set("connectionFailures", datum.getConnectionFailures());    context.set("unfetched", datum.getUnfetched());    context.set("fetched", datum.getFetched());    context.set("notModified", datum.getNotModified());    context.set("redirTemp", datum.getRedirTemp());    context.set("redirPerm", datum.getRedirPerm());    context.set("gone", datum.getGone());    context.set("conf", conf);        for (Map.Entry<Writable, Writable> entry : datum.getMetaData().entrySet()) {        Object value = entry.getValue();        if (value instanceof FloatWritable) {            FloatWritable fvalue = (FloatWritable) value;            Text tkey = (Text) entry.getKey();            context.set(tkey.toString(), fvalue.get());        }        if (value instanceof IntWritable) {            IntWritable ivalue = (IntWritable) value;            Text tkey = (Text) entry.getKey();            context.set(tkey.toString(), ivalue.get());        }        if (value instanceof Text) {            Text tvalue = (Text) value;            Text tkey = (Text) entry.getKey();            context.set(tkey.toString().replace("-", "_"), tvalue.toString());        }    }    return context;}
0
public void setup(Context context) throws IOException
{    conf = context.getConfiguration();    mos = new MultipleOutputs<FloatWritable, SelectorEntry>(context);    Job job = Job.getInstance(conf);    limit = conf.getLong(GENERATOR_TOP_N, Long.MAX_VALUE) / job.getNumReduceTasks();    maxNumSegments = conf.getInt(GENERATOR_MAX_NUM_SEGMENTS, 1);    segCounts = new int[maxNumSegments];    maxCount = conf.getInt(GENERATOR_MAX_COUNT, -1);    if (maxCount == -1) {        byDomain = false;    }    if (GENERATOR_COUNT_VALUE_DOMAIN.equals(conf.get(GENERATOR_COUNT_MODE)))        byDomain = true;    normalise = conf.getBoolean(GENERATOR_NORMALISE, true);    if (normalise)        normalizers = new URLNormalizers(conf, URLNormalizers.SCOPE_GENERATE_HOST_COUNT);    if (conf.get(GENERATOR_HOSTDB) != null) {        maxCountExpr = JexlUtil.parseExpression(conf.get(GENERATOR_MAX_COUNT_EXPR, null));        fetchDelayExpr = JexlUtil.parseExpression(conf.get(GENERATOR_FETCH_DELAY_EXPR, null));    }}
0
public void cleanup(Context context) throws IOException, InterruptedException
{    mos.close();}
0
public void reduce(FloatWritable key, Iterable<SelectorEntry> values, Context context) throws IOException, InterruptedException
{    String hostname = null;    HostDatum host = null;        LongWritable variableFetchDelayWritable = null;    Text variableFetchDelayKey = new Text("_variableFetchDelay_");        int maxCount = this.maxCount;    for (SelectorEntry entry : values) {        Text url = entry.url;        String urlString = url.toString();        URL u = null;                if (host == null) {            try {                hostname = URLUtil.getHost(urlString);                host = getHostDatum(hostname);            } catch (Exception e) {            }                        if (host == null) {                                host = new HostDatum();            } else {                if (maxCountExpr != null) {                    long variableMaxCount = Math.round((double) maxCountExpr.evaluate(createContext(host)));                                        maxCount = (int) variableMaxCount;                }                if (fetchDelayExpr != null) {                    long variableFetchDelay = Math.round((double) fetchDelayExpr.evaluate(createContext(host)));                                        variableFetchDelayWritable = new LongWritable(variableFetchDelay);                }            }        }                if (variableFetchDelayWritable != null) {            entry.datum.getMetaData().put(variableFetchDelayKey, variableFetchDelayWritable);        }        if (count == limit) {                        if (currentsegmentnum < maxNumSegments) {                count = 0;                currentsegmentnum++;            } else                break;        }        String hostordomain = null;        try {            if (normalise && normalizers != null) {                urlString = normalizers.normalize(urlString, URLNormalizers.SCOPE_GENERATE_HOST_COUNT);            }            u = new URL(urlString);            if (byDomain) {                hostordomain = URLUtil.getDomainName(u);            } else {                hostordomain = u.getHost();            }        } catch (MalformedURLException e) {                        context.getCounter("Generator", "MALFORMED_URL").increment(1);            continue;        }        hostordomain = hostordomain.toLowerCase();                if (maxCount > 0) {            int[] hostCount = hostCounts.get(hostordomain);            if (hostCount == null) {                hostCount = new int[] { 1, 0 };                hostCounts.put(hostordomain, hostCount);            }                        hostCount[1]++;                        while (segCounts[hostCount[0] - 1] >= limit && hostCount[0] < maxNumSegments) {                hostCount[0]++;                hostCount[1] = 0;            }                        if (hostCount[1] > maxCount) {                if (hostCount[0] < maxNumSegments) {                    hostCount[0]++;                    hostCount[1] = 1;                } else {                    if (hostCount[1] == (maxCount + 1)) {                        context.getCounter("Generator", "HOSTS_AFFECTED_PER_HOST_OVERFLOW").increment(1);                                            }                                        context.getCounter("Generator", "URLS_SKIPPED_PER_HOST_OVERFLOW").increment(1);                    continue;                }            }            entry.segnum = new IntWritable(hostCount[0]);            segCounts[hostCount[0] - 1]++;        } else {            entry.segnum = new IntWritable(currentsegmentnum);            segCounts[currentsegmentnum - 1]++;        }        outputFile = generateFileName(entry);        mos.write("sequenceFiles", key, entry, outputFile);                        count++;    }}
1
private String generateFileName(SelectorEntry entry)
{    return "fetchlist-" + entry.segnum.toString() + "/part";}
0
private HostDatum getHostDatum(String host) throws Exception
{    Text key = new Text();    HostDatum value = new HostDatum();    open();    for (int i = 0; i < hostdbReaders.length; i++) {        while (hostdbReaders[i].next(key, value)) {            if (host.equals(key.toString())) {                close();                return value;            }        }    }    close();    return null;}
0
public int compare(byte[] b1, int s1, int l1, byte[] b2, int s2, int l2)
{    return super.compare(b2, s2, l2, b1, s1, l1);}
0
public void map(FloatWritable key, SelectorEntry value, Context context) throws IOException, InterruptedException
{    SelectorEntry entry = value;    context.write(entry.url, entry);}
0
public void reduce(Text key, Iterable<SelectorEntry> values, Context context) throws IOException, InterruptedException
{        for (SelectorEntry entry : values) {        context.write(entry.url, entry.datum);    }}
0
public int compare(WritableComparable a, WritableComparable b)
{    Text url1 = (Text) a;    Text url2 = (Text) b;    int hash1 = hash(url1.getBytes(), 0, url1.getLength());    int hash2 = hash(url2.getBytes(), 0, url2.getLength());    return (hash1 < hash2 ? -1 : (hash1 == hash2 ? 0 : 1));}
0
public int compare(byte[] b1, int s1, int l1, byte[] b2, int s2, int l2)
{    int hash1 = hash(b1, s1, l1);    int hash2 = hash(b2, s2, l2);    return (hash1 < hash2 ? -1 : (hash1 == hash2 ? 0 : 1));}
0
private static int hash(byte[] bytes, int start, int length)
{    int hash = 1;        for (int i = length - 1; i >= 0; i--) hash = (31 * hash) + (int) bytes[start + i];    return hash;}
0
public void map(Text key, CrawlDatum value, Context context) throws IOException, InterruptedException
{    context.write(key, value);}
0
public void setup(Reducer<Text, CrawlDatum, Text, CrawlDatum>.Context context)
{    Configuration conf = context.getConfiguration();    generateTime = conf.getLong(Nutch.GENERATE_TIME_KEY, 0L);}
0
public void reduce(Text key, Iterable<CrawlDatum> values, Context context) throws IOException, InterruptedException
{    genTime.set(0L);    for (CrawlDatum val : values) {        if (val.getMetaData().containsKey(Nutch.WRITABLE_GENERATE_TIME_KEY)) {            LongWritable gt = (LongWritable) val.getMetaData().get(Nutch.WRITABLE_GENERATE_TIME_KEY);            genTime.set(gt.get());            if (genTime.get() != generateTime) {                orig.set(val);                genTime.set(0L);                continue;            }        } else {            orig.set(val);        }    }    if (genTime.get() != 0L) {        orig.getMetaData().put(Nutch.WRITABLE_GENERATE_TIME_KEY, genTime);    }    context.write(key, orig);}
0
public Path[] generate(Path dbDir, Path segments, int numLists, long topN, long curTime) throws IOException, InterruptedException, ClassNotFoundException
{    Job job = NutchJob.getInstance(getConf());    Configuration conf = job.getConfiguration();    boolean filter = conf.getBoolean(GENERATOR_FILTER, true);    boolean normalise = conf.getBoolean(GENERATOR_NORMALISE, true);    return generate(dbDir, segments, numLists, topN, curTime, filter, normalise, false, 1, null);}
0
public Path[] generate(Path dbDir, Path segments, int numLists, long topN, long curTime, boolean filter, boolean force) throws IOException, InterruptedException, ClassNotFoundException
{    return generate(dbDir, segments, numLists, topN, curTime, filter, true, force, 1, null);}
0
public Path[] generate(Path dbDir, Path segments, int numLists, long topN, long curTime, boolean filter, boolean norm, boolean force, int maxNumSegments, String expr) throws IOException, InterruptedException, ClassNotFoundException
{    return generate(dbDir, segments, numLists, topN, curTime, filter, true, force, 1, expr, null);}
0
public Path[] generate(Path dbDir, Path segments, int numLists, long topN, long curTime, boolean filter, boolean norm, boolean force, int maxNumSegments, String expr, String hostdb) throws IOException, InterruptedException, ClassNotFoundException
{    Path tempDir = new Path(getConf().get("mapreduce.cluster.temp.dir", ".") + "/generate-temp-" + java.util.UUID.randomUUID().toString());    FileSystem fs = tempDir.getFileSystem(getConf());    Path lock = CrawlDb.lock(getConf(), dbDir, force);    SimpleDateFormat sdf = new SimpleDateFormat("yyyy-MM-dd HH:mm:ss");    long start = System.currentTimeMillis();                    if (topN != Long.MAX_VALUE) {            }    if (expr != null) {            }    if (hostdb != null) {            }        Job job = NutchJob.getInstance(getConf());    job.setJobName("generate: select from " + dbDir);    Configuration conf = job.getConfiguration();    if (numLists == -1) {        /* for politeness create exactly one partition per fetch task */        numLists = Integer.parseInt(conf.get("mapreduce.job.maps"));    }    if ("local".equals(conf.get("mapreduce.framework.name")) && numLists != 1) {                        numLists = 1;    }    conf.setLong(GENERATOR_CUR_TIME, curTime);        long generateTime = System.currentTimeMillis();    conf.setLong(Nutch.GENERATE_TIME_KEY, generateTime);    conf.setLong(GENERATOR_TOP_N, topN);    conf.setBoolean(GENERATOR_FILTER, filter);    conf.setBoolean(GENERATOR_NORMALISE, norm);    conf.setInt(GENERATOR_MAX_NUM_SEGMENTS, maxNumSegments);    if (expr != null) {        conf.set(GENERATOR_EXPR, expr);    }    if (hostdb != null) {        conf.set(GENERATOR_HOSTDB, hostdb);    }    FileInputFormat.addInputPath(job, new Path(dbDir, CrawlDb.CURRENT_NAME));    job.setInputFormatClass(SequenceFileInputFormat.class);    job.setJarByClass(Selector.class);    job.setMapperClass(SelectorMapper.class);    job.setPartitionerClass(Selector.class);    job.setReducerClass(SelectorReducer.class);    FileOutputFormat.setOutputPath(job, tempDir);    job.setOutputKeyClass(FloatWritable.class);    job.setSortComparatorClass(DecreasingFloatComparator.class);    job.setOutputValueClass(SelectorEntry.class);    MultipleOutputs.addNamedOutput(job, "sequenceFiles", SequenceFileOutputFormat.class, FloatWritable.class, SelectorEntry.class);    try {        boolean success = job.waitForCompletion(true);        if (!success) {            String message = "Generator job did not succeed, job status:" + job.getStatus().getState() + ", reason: " + job.getStatus().getFailureInfo();                        NutchJob.cleanupAfterFailure(tempDir, lock, fs);            throw new RuntimeException(message);        }    } catch (IOException | InterruptedException | ClassNotFoundException e) {                NutchJob.cleanupAfterFailure(tempDir, lock, fs);        throw e;    }        for (Counter counter : job.getCounters().getGroup("Generator")) {            }            List<Path> generatedSegments = new ArrayList<>();    FileStatus[] status = fs.listStatus(tempDir);    try {        for (FileStatus stat : status) {            Path subfetchlist = stat.getPath();            if (!subfetchlist.getName().startsWith("fetchlist-"))                continue;                        Path newSeg = partitionSegment(segments, subfetchlist, numLists);            generatedSegments.add(newSeg);        }    } catch (Exception e) {                LockUtil.removeLockFile(getConf(), lock);        fs.delete(tempDir, true);        return null;    }    if (generatedSegments.size() == 0) {                LockUtil.removeLockFile(getConf(), lock);        fs.delete(tempDir, true);        return null;    }    if (getConf().getBoolean(GENERATE_UPDATE_CRAWLDB, false)) {                Path tempDir2 = new Path(dbDir, "generate-temp-" + java.util.UUID.randomUUID().toString());        job = NutchJob.getInstance(getConf());        job.setJobName("generate: updatedb " + dbDir);        job.getConfiguration().setLong(Nutch.GENERATE_TIME_KEY, generateTime);        for (Path segmpaths : generatedSegments) {            Path subGenDir = new Path(segmpaths, CrawlDatum.GENERATE_DIR_NAME);            FileInputFormat.addInputPath(job, subGenDir);        }        FileInputFormat.addInputPath(job, new Path(dbDir, CrawlDb.CURRENT_NAME));        job.setInputFormatClass(SequenceFileInputFormat.class);        job.setMapperClass(CrawlDbUpdater.CrawlDbUpdateMapper.class);        job.setReducerClass(CrawlDbUpdater.CrawlDbUpdateReducer.class);        job.setJarByClass(CrawlDbUpdater.class);        job.setOutputFormatClass(MapFileOutputFormat.class);        job.setOutputKeyClass(Text.class);        job.setOutputValueClass(CrawlDatum.class);        FileOutputFormat.setOutputPath(job, tempDir2);        try {            boolean success = job.waitForCompletion(true);            if (!success) {                String message = "Generator job did not succeed, job status:" + job.getStatus().getState() + ", reason: " + job.getStatus().getFailureInfo();                                NutchJob.cleanupAfterFailure(tempDir, lock, fs);                NutchJob.cleanupAfterFailure(tempDir2, lock, fs);                throw new RuntimeException(message);            }            CrawlDb.install(job, dbDir);        } catch (IOException | InterruptedException | ClassNotFoundException e) {                        NutchJob.cleanupAfterFailure(tempDir, lock, fs);            NutchJob.cleanupAfterFailure(tempDir2, lock, fs);            throw e;        }        fs.delete(tempDir2, true);    }    LockUtil.removeLockFile(getConf(), lock);    fs.delete(tempDir, true);    long end = System.currentTimeMillis();        Path[] patharray = new Path[generatedSegments.size()];    return generatedSegments.toArray(patharray);}
1
private Path partitionSegment(Path segmentsDir, Path inputDir, int numLists) throws IOException, ClassNotFoundException, InterruptedException
{        if (LOG.isInfoEnabled()) {            }    Path segment = new Path(segmentsDir, generateSegmentName());    Path output = new Path(segment, CrawlDatum.GENERATE_DIR_NAME);        Job job = NutchJob.getInstance(getConf());    job.setJobName("generate: partition " + segment);    Configuration conf = job.getConfiguration();    conf.setInt("partition.url.seed", new Random().nextInt());    FileInputFormat.addInputPath(job, inputDir);    job.setInputFormatClass(SequenceFileInputFormat.class);    job.setJarByClass(Generator.class);    job.setMapperClass(SelectorInverseMapper.class);    job.setMapOutputKeyClass(Text.class);    job.setMapOutputValueClass(SelectorEntry.class);    job.setPartitionerClass(URLPartitioner.class);    job.setReducerClass(PartitionReducer.class);    job.setNumReduceTasks(numLists);    FileOutputFormat.setOutputPath(job, output);    job.setOutputFormatClass(SequenceFileOutputFormat.class);    job.setOutputKeyClass(Text.class);    job.setOutputValueClass(CrawlDatum.class);    job.setSortComparatorClass(HashComparator.class);    try {        boolean success = job.waitForCompletion(true);        if (!success) {            String message = "Generator job did not succeed, job status:" + job.getStatus().getState() + ", reason: " + job.getStatus().getFailureInfo();                        throw new RuntimeException(message);        }    } catch (IOException | InterruptedException | ClassNotFoundException e) {                throw e;    }    return segment;}
1
public static synchronized String generateSegmentName()
{    try {        Thread.sleep(1000);    } catch (Throwable t) {    }    ;    return sdf.format(new Date(System.currentTimeMillis()));}
0
public static void main(String[] args) throws Exception
{    int res = ToolRunner.run(NutchConfiguration.create(), new Generator(), args);    System.exit(res);}
0
public int run(String[] args) throws Exception
{    if (args.length < 2) {        System.out.println("Usage: Generator <crawldb> <segments_dir> [-force] [-topN N] [-numFetchers numFetchers] [-expr <expr>] [-adddays <numDays>] [-noFilter] [-noNorm] [-maxNumSegments <num>]");        return -1;    }    Path dbDir = new Path(args[0]);    Path segmentsDir = new Path(args[1]);    String hostdb = null;    long curTime = System.currentTimeMillis();    long topN = Long.MAX_VALUE;    int numFetchers = -1;    boolean filter = true;    boolean norm = true;    boolean force = false;    String expr = null;    int maxNumSegments = 1;    for (int i = 2; i < args.length; i++) {        if ("-topN".equals(args[i])) {            topN = Long.parseLong(args[i + 1]);            i++;        } else if ("-numFetchers".equals(args[i])) {            numFetchers = Integer.parseInt(args[i + 1]);            i++;        } else if ("-hostdb".equals(args[i])) {            hostdb = args[i + 1];            i++;        } else if ("-adddays".equals(args[i])) {            long numDays = Integer.parseInt(args[i + 1]);            curTime += numDays * 1000L * 60 * 60 * 24;        } else if ("-noFilter".equals(args[i])) {            filter = false;        } else if ("-noNorm".equals(args[i])) {            norm = false;        } else if ("-force".equals(args[i])) {            force = true;        } else if ("-maxNumSegments".equals(args[i])) {            maxNumSegments = Integer.parseInt(args[i + 1]);        } else if ("-expr".equals(args[i])) {            expr = args[i + 1];        }    }    try {        Path[] segs = generate(dbDir, segmentsDir, numFetchers, topN, curTime, filter, norm, force, maxNumSegments, expr, hostdb);        if (segs == null)            return 1;    } catch (Exception e) {                return -1;    }    return 0;}
1
public Map<String, Object> run(Map<String, Object> args, String crawlId) throws Exception
{    Map<String, Object> results = new HashMap<>();    long curTime = System.currentTimeMillis();    long topN = Long.MAX_VALUE;    int numFetchers = -1;    boolean filter = true;    boolean norm = true;    boolean force = false;    int maxNumSegments = 1;    String expr = null;    String hostdb = null;    Path crawlDb;    if (args.containsKey(Nutch.ARG_CRAWLDB)) {        Object crawldbPath = args.get(Nutch.ARG_CRAWLDB);        if (crawldbPath instanceof Path) {            crawlDb = (Path) crawldbPath;        } else {            crawlDb = new Path(crawldbPath.toString());        }    } else {        crawlDb = new Path(crawlId + "/crawldb");    }    Path segmentsDir;    if (args.containsKey(Nutch.ARG_SEGMENTDIR)) {        Object segDir = args.get(Nutch.ARG_SEGMENTDIR);        if (segDir instanceof Path) {            segmentsDir = (Path) segDir;        } else {            segmentsDir = new Path(segDir.toString());        }    } else {        segmentsDir = new Path(crawlId + "/segments");    }    if (args.containsKey(Nutch.ARG_HOSTDB)) {        hostdb = (String) args.get(Nutch.ARG_HOSTDB);    }    if (args.containsKey("expr")) {        expr = (String) args.get("expr");    }    if (args.containsKey("topN")) {        topN = Long.parseLong((String) args.get("topN"));    }    if (args.containsKey("numFetchers")) {        numFetchers = Integer.parseInt((String) args.get("numFetchers"));    }    if (args.containsKey("adddays")) {        long numDays = Integer.parseInt((String) args.get("adddays"));        curTime += numDays * 1000L * 60 * 60 * 24;    }    if (args.containsKey("noFilter")) {        filter = false;    }    if (args.containsKey("noNorm")) {        norm = false;    }    if (args.containsKey("force")) {        force = true;    }    if (args.containsKey("maxNumSegments")) {        maxNumSegments = Integer.parseInt((String) args.get("maxNumSegments"));    }    try {        Path[] segs = generate(crawlDb, segmentsDir, numFetchers, topN, curTime, filter, norm, force, maxNumSegments, expr, hostdb);        if (segs == null) {            results.put(Nutch.VAL_RESULT, Integer.toString(1));            return results;        }    } catch (Exception e) {                results.put(Nutch.VAL_RESULT, Integer.toString(-1));        return results;    }    results.put(Nutch.VAL_RESULT, Integer.toString(0));    return results;}
1
public void setup(Context context)
{    Configuration conf = context.getConfiguration();    boolean normalize = conf.getBoolean(CrawlDbFilter.URL_NORMALIZING, true);    boolean filter = conf.getBoolean(CrawlDbFilter.URL_FILTERING, true);    filterNormalizeAll = conf.getBoolean(URL_FILTER_NORMALIZE_ALL, false);    if (normalize) {        scope = conf.get(URL_NORMALIZING_SCOPE, URLNormalizers.SCOPE_INJECT);        urlNormalizers = new URLNormalizers(conf, scope);    }    interval = conf.getInt("db.fetch.interval.default", 2592000);    if (filter) {        filters = new URLFilters(conf);    }    scfilters = new ScoringFilters(conf);    scoreInjected = conf.getFloat("db.score.injected", 1.0f);    curTime = conf.getLong("injector.current.time", System.currentTimeMillis());    url404Purging = conf.getBoolean(CrawlDb.CRAWLDB_PURGE_404, false);}
0
private String filterNormalize(String url)
{    if (url != null) {        try {            if (urlNormalizers != null)                                url = urlNormalizers.normalize(url, scope);            if (filters != null)                                url = filters.filter(url);        } catch (Exception e) {                        url = null;        }    }    return url;}
1
private void processMetaData(String metadata, CrawlDatum datum, String url)
{    String[] splits = metadata.split(TAB_CHARACTER);    for (String split : splits) {                int indexEquals = split.indexOf(EQUAL_CHARACTER);        if (        indexEquals == -1)            continue;        String metaname = split.substring(0, indexEquals);        String metavalue = split.substring(indexEquals + 1);        try {            if (metaname.equals(nutchScoreMDName)) {                datum.setScore(Float.parseFloat(metavalue));            } else if (metaname.equals(nutchFetchIntervalMDName)) {                datum.setFetchInterval(Integer.parseInt(metavalue));            } else if (metaname.equals(nutchFixedFetchIntervalMDName)) {                int fixedInterval = Integer.parseInt(metavalue);                if (fixedInterval > -1) {                                                            datum.getMetaData().put(Nutch.WRITABLE_FIXED_INTERVAL_KEY, new FloatWritable(fixedInterval));                    datum.setFetchInterval(fixedInterval);                }            } else {                datum.getMetaData().put(new Text(metaname), new Text(metavalue));            }        } catch (NumberFormatException nfe) {                    }    }}
1
public void map(Text key, Writable value, Context context) throws IOException, InterruptedException
{    if (value instanceof Text) {                String url = key.toString().trim();                if (url.length() == 0 || url.startsWith("#"))            return;        url = filterNormalize(url);        if (url == null) {            context.getCounter("injector", "urls_filtered").increment(1);        } else {            CrawlDatum datum = new CrawlDatum();            datum.setStatus(CrawlDatum.STATUS_INJECTED);            datum.setFetchTime(curTime);            datum.setScore(scoreInjected);            datum.setFetchInterval(interval);            String metadata = value.toString().trim();            if (metadata.length() > 0)                processMetaData(metadata, datum, url);            try {                key.set(url);                scfilters.injectedScore(key, datum);            } catch (ScoringFilterException e) {                if (LOG.isWarnEnabled()) {                                    }            }            context.getCounter("injector", "urls_injected").increment(1);            context.write(key, datum);        }    } else if (value instanceof CrawlDatum) {                        CrawlDatum datum = (CrawlDatum) value;                if (url404Purging && CrawlDatum.STATUS_DB_GONE == datum.getStatus()) {            context.getCounter("injector", "urls_purged_404").increment(1);            return;        }        if (filterNormalizeAll) {            String url = filterNormalize(key.toString());            if (url == null) {                context.getCounter("injector", "urls_purged_filter").increment(1);            } else {                key.set(url);                context.write(key, datum);            }        } else {            context.write(key, datum);        }    }}
1
public void setup(Context context)
{    Configuration conf = context.getConfiguration();    interval = conf.getInt("db.fetch.interval.default", 2592000);    scoreInjected = conf.getFloat("db.score.injected", 1.0f);    overwrite = conf.getBoolean("db.injector.overwrite", false);    update = conf.getBoolean("db.injector.update", false);        }
1
public void reduce(Text key, Iterable<CrawlDatum> values, Context context) throws IOException, InterruptedException
{    boolean oldSet = false;    boolean injectedSet = false;        for (CrawlDatum val : values) {        if (val.getStatus() == CrawlDatum.STATUS_INJECTED) {            injected.set(val);            injected.setStatus(CrawlDatum.STATUS_DB_UNFETCHED);            injectedSet = true;        } else {            old.set(val);            oldSet = true;        }    }    CrawlDatum result;    if (injectedSet && (!oldSet || overwrite)) {                result = injected;    } else {                result = old;        if (injectedSet && update) {                        old.putAllMetaData(injected);            old.setScore(injected.getScore() != scoreInjected ? injected.getScore() : old.getScore());            old.setFetchInterval(injected.getFetchInterval() != interval ? injected.getFetchInterval() : old.getFetchInterval());        }    }    if (injectedSet && oldSet) {        context.getCounter("injector", "urls_merged").increment(1);    }    context.write(key, result);}
0
public void inject(Path crawlDb, Path urlDir) throws IOException, ClassNotFoundException, InterruptedException
{    inject(crawlDb, urlDir, false, false);}
0
public void inject(Path crawlDb, Path urlDir, boolean overwrite, boolean update) throws IOException, ClassNotFoundException, InterruptedException
{    inject(crawlDb, urlDir, overwrite, update, true, true, false);}
0
public void inject(Path crawlDb, Path urlDir, boolean overwrite, boolean update, boolean normalize, boolean filter, boolean filterNormalizeAll) throws IOException, ClassNotFoundException, InterruptedException
{    SimpleDateFormat sdf = new SimpleDateFormat("yyyy-MM-dd HH:mm:ss");    long start = System.currentTimeMillis();    if (LOG.isInfoEnabled()) {                                    }        Configuration conf = getConf();    conf.setLong("injector.current.time", System.currentTimeMillis());    conf.setBoolean("db.injector.overwrite", overwrite);    conf.setBoolean("db.injector.update", update);    conf.setBoolean(CrawlDbFilter.URL_NORMALIZING, normalize);    conf.setBoolean(CrawlDbFilter.URL_FILTERING, filter);    conf.setBoolean(URL_FILTER_NORMALIZE_ALL, filterNormalizeAll);    conf.setBoolean("mapreduce.fileoutputcommitter.marksuccessfuljobs", false);        FileSystem fs = crawlDb.getFileSystem(conf);    Path current = new Path(crawlDb, CrawlDb.CURRENT_NAME);    if (!fs.exists(current))        fs.mkdirs(current);    Path tempCrawlDb = new Path(crawlDb, "crawldb-" + Integer.toString(new Random().nextInt(Integer.MAX_VALUE)));        Path lock = CrawlDb.lock(conf, crawlDb, false);        Job job = Job.getInstance(conf, "inject " + urlDir);    job.setJarByClass(Injector.class);    job.setMapperClass(InjectMapper.class);    job.setReducerClass(InjectReducer.class);    job.setOutputFormatClass(MapFileOutputFormat.class);    job.setOutputKeyClass(Text.class);    job.setOutputValueClass(CrawlDatum.class);    job.setSpeculativeExecution(false);        MultipleInputs.addInputPath(job, current, SequenceFileInputFormat.class);    FileStatus[] seedFiles = urlDir.getFileSystem(getConf()).listStatus(urlDir);    int numSeedFiles = 0;    for (FileStatus seedFile : seedFiles) {        if (seedFile.isFile()) {            MultipleInputs.addInputPath(job, seedFile.getPath(), KeyValueTextInputFormat.class);            numSeedFiles++;                    } else {                    }    }    if (numSeedFiles == 0) {                LockUtil.removeLockFile(fs, lock);        return;    }    FileOutputFormat.setOutputPath(job, tempCrawlDb);    try {                boolean success = job.waitForCompletion(true);        if (!success) {            String message = "Injector job did not succeed, job status: " + job.getStatus().getState() + ", reason: " + job.getStatus().getFailureInfo();                        NutchJob.cleanupAfterFailure(tempCrawlDb, lock, fs);                        throw new RuntimeException(message);        }                CrawlDb.install(job, crawlDb);        if (LOG.isInfoEnabled()) {            long urlsInjected = job.getCounters().findCounter("injector", "urls_injected").getValue();            long urlsFiltered = job.getCounters().findCounter("injector", "urls_filtered").getValue();            long urlsMerged = job.getCounters().findCounter("injector", "urls_merged").getValue();            long urlsPurged404 = job.getCounters().findCounter("injector", "urls_purged_404").getValue();            long urlsPurgedFilter = job.getCounters().findCounter("injector", "urls_purged_filter").getValue();                                                            if (filterNormalizeAll) {                            }            if (conf.getBoolean(CrawlDb.CRAWLDB_PURGE_404, false)) {                            }            long end = System.currentTimeMillis();                    }    } catch (IOException | InterruptedException | ClassNotFoundException | NullPointerException e) {                NutchJob.cleanupAfterFailure(tempCrawlDb, lock, fs);        throw e;    }}
1
public void usage()
{    System.err.println("Usage: Injector [-D...] <crawldb> <url_dir> [-overwrite|-update] [-noFilter] [-noNormalize] [-filterNormalizeAll]\n");    System.err.println("  <crawldb>\tPath to a crawldb directory. If not present, a new one would be created.");    System.err.println("  <url_dir>\tPath to URL file or directory with URL file(s) containing URLs to be injected.");    System.err.println("           \tA URL file should have one URL per line, optionally followed by custom metadata.");    System.err.println("           \tBlank lines or lines starting with a '#' would be ignored. Custom metadata must");    System.err.println("           \tbe of form 'key=value' and separated by tabs.");    System.err.println("           \tBelow are reserved metadata keys:\n");    System.err.println("           \t\tnutch.score: A custom score for a url");    System.err.println("           \t\tnutch.fetchInterval: A custom fetch interval for a url");    System.err.println("           \t\tnutch.fetchInterval.fixed: A custom fetch interval for a url that is not " + "changed by AdaptiveFetchSchedule\n");    System.err.println("           \tExample:");    System.err.println("           \t http://www.apache.org/");    System.err.println("           \t http://www.nutch.org/ \\t nutch.score=10 \\t nutch.fetchInterval=2592000 \\t userType=open_source\n");    System.err.println(" -overwrite\tOverwite existing crawldb records by the injected records. Has precedence over 'update'");    System.err.println(" -update   \tUpdate existing crawldb records with the injected records. Old metadata is preserved");    System.err.println();    System.err.println(" -nonormalize\tDo not normalize URLs before injecting");    System.err.println(" -nofilter \tDo not apply URL filters to injected URLs");    System.err.println(" -filterNormalizeAll\n" + "           \tNormalize and filter all URLs including the URLs of existing CrawlDb records");    System.err.println();    System.err.println(" -D...     \tset or overwrite configuration property (property=value)");    System.err.println(" -Ddb.update.purge.404=true\n" + "           \tremove URLs with status gone (404) from CrawlDb");}
0
public static void main(String[] args) throws Exception
{    int res = ToolRunner.run(NutchConfiguration.create(), new Injector(), args);    System.exit(res);}
0
public int run(String[] args) throws Exception
{    if (args.length < 2) {        usage();        return -1;    }    boolean overwrite = false;    boolean update = false;    boolean normalize = true;    boolean filter = true;    boolean filterNormalizeAll = false;    for (int i = 2; i < args.length; i++) {        if (args[i].equals("-overwrite")) {            overwrite = true;        } else if (args[i].equals("-update")) {            update = true;        } else if (args[i].equals("-noNormalize")) {            normalize = false;        } else if (args[i].equals("-noFilter")) {            filter = false;        } else if (args[i].equals("-filterNormalizeAll")) {            filterNormalizeAll = true;        } else {                        usage();            return -1;        }    }    try {        inject(new Path(args[0]), new Path(args[1]), overwrite, update, normalize, filter, filterNormalizeAll);        return 0;    } catch (Exception e) {                return -1;    }}
1
public Map<String, Object> run(Map<String, Object> args, String crawlId) throws Exception
{    if (args.size() < 1) {        throw new IllegalArgumentException("Required arguments <url_dir> or <seedName>");    }    Path input;    Object path = null;    if (args.containsKey(Nutch.ARG_SEEDDIR)) {        path = args.get(Nutch.ARG_SEEDDIR);    } else if (args.containsKey(Nutch.ARG_SEEDNAME)) {        path = NutchServer.getInstance().getSeedManager().getSeedList((String) args.get(Nutch.ARG_SEEDNAME)).getSeedFilePath();    } else {        throw new IllegalArgumentException("Required arguments <url_dir> or <seedName>");    }    if (path instanceof Path) {        input = (Path) path;    } else {        input = new Path(path.toString());    }    Map<String, Object> results = new HashMap<>();    Path crawlDb;    if (args.containsKey(Nutch.ARG_CRAWLDB)) {        Object crawldbPath = args.get(Nutch.ARG_CRAWLDB);        if (crawldbPath instanceof Path) {            crawlDb = (Path) crawldbPath;        } else {            crawlDb = new Path(crawldbPath.toString());        }    } else {        crawlDb = new Path(crawlId + "/crawldb");    }    inject(crawlDb, input);    results.put(Nutch.VAL_RESULT, Integer.toString(0));    return results;}
0
public void readFields(DataInput in) throws IOException
{    fromUrl = Text.readString(in);    anchor = Text.readString(in);}
0
public static void skip(DataInput in) throws IOException
{        Text.skip(in);        Text.skip(in);}
0
public void write(DataOutput out) throws IOException
{    Text.writeString(out, fromUrl);    Text.writeString(out, anchor);}
0
public static Inlink read(DataInput in) throws IOException
{    Inlink inlink = new Inlink();    inlink.readFields(in);    return inlink;}
0
public String getFromUrl()
{    return fromUrl;}
0
public String getAnchor()
{    return anchor;}
0
public boolean equals(Object o)
{    if (!(o instanceof Inlink))        return false;    Inlink other = (Inlink) o;    return this.fromUrl.equals(other.fromUrl) && this.anchor.equals(other.anchor);}
0
public int hashCode()
{    return fromUrl.hashCode() ^ anchor.hashCode();}
0
public String toString()
{    return "fromUrl: " + fromUrl + " anchor: " + anchor;}
0
public void add(Inlink inlink)
{    inlinks.add(inlink);}
0
public void add(Inlinks inlinks)
{    this.inlinks.addAll(inlinks.inlinks);}
0
public Iterator<Inlink> iterator()
{    return this.inlinks.iterator();}
0
public int size()
{    return inlinks.size();}
0
public void clear()
{    inlinks.clear();}
0
public void readFields(DataInput in) throws IOException
{    int length = in.readInt();    inlinks.clear();    for (int i = 0; i < length; i++) {        add(Inlink.read(in));    }}
0
public void write(DataOutput out) throws IOException
{    out.writeInt(inlinks.size());    Iterator<Inlink> it = inlinks.iterator();    while (it.hasNext()) {        it.next().write(out);    }}
0
public String toString()
{    StringBuilder buffer = new StringBuilder();    buffer.append("Inlinks:\n");    Iterator<Inlink> it = inlinks.iterator();    while (it.hasNext()) {        buffer.append(" ");        buffer.append(it.next());        buffer.append("\n");    }    return buffer.toString();}
0
public String[] getAnchors()
{    HashMap<String, Set<String>> domainToAnchors = new HashMap<>();    ArrayList<String> results = new ArrayList<>();    Iterator<Inlink> it = inlinks.iterator();    while (it.hasNext()) {        Inlink inlink = it.next();        String anchor = inlink.getAnchor();        if (        anchor.length() == 0)            continue;                String domain = null;        try {            domain = new URL(inlink.getFromUrl()).getHost();        } catch (MalformedURLException e) {        }        Set<String> domainAnchors = domainToAnchors.get(domain);        if (domainAnchors == null) {            domainAnchors = new HashSet<>();            domainToAnchors.put(domain, domainAnchors);        }        if (domainAnchors.add(anchor)) {                                    results.add(anchor);        }    }    return results.toArray(new String[results.size()]);}
0
public void setup(Mapper<Text, ParseData, Text, Inlinks>.Context context)
{    Configuration conf = context.getConfiguration();    maxAnchorLength = conf.getInt("linkdb.max.anchor.length", 100);    ignoreInternalLinks = conf.getBoolean(IGNORE_INTERNAL_LINKS, true);    ignoreExternalLinks = conf.getBoolean(IGNORE_EXTERNAL_LINKS, false);    if (conf.getBoolean(LinkDbFilter.URL_FILTERING, false)) {        urlFilters = new URLFilters(conf);    }    if (conf.getBoolean(LinkDbFilter.URL_NORMALIZING, false)) {        urlNormalizers = new URLNormalizers(conf, URLNormalizers.SCOPE_LINKDB);    }}
0
public void map(Text key, ParseData parseData, Context context) throws IOException, InterruptedException
{    String fromUrl = key.toString();    String fromHost = getHost(fromUrl);    if (urlNormalizers != null) {        try {            fromUrl = urlNormalizers.normalize(fromUrl,             URLNormalizers.SCOPE_LINKDB);        } catch (Exception e) {                        fromUrl = null;        }    }    if (fromUrl != null && urlFilters != null) {        try {                        fromUrl = urlFilters.filter(fromUrl);        } catch (Exception e) {                        fromUrl = null;        }    }    if (fromUrl == null)                return;    Outlink[] outlinks = parseData.getOutlinks();    Inlinks inlinks = new Inlinks();    for (int i = 0; i < outlinks.length; i++) {        Outlink outlink = outlinks[i];        String toUrl = outlink.getToUrl();        if (ignoreInternalLinks) {            String toHost = getHost(toUrl);            if (toHost == null || toHost.equals(fromHost)) {                                continue;            }        } else if (ignoreExternalLinks) {            String toHost = getHost(toUrl);            if (toHost == null || !toHost.equals(fromHost)) {                                continue;            }        }        if (urlNormalizers != null) {            try {                                toUrl = urlNormalizers.normalize(toUrl, URLNormalizers.SCOPE_LINKDB);            } catch (Exception e) {                                toUrl = null;            }        }        if (toUrl != null && urlFilters != null) {            try {                                toUrl = urlFilters.filter(toUrl);            } catch (Exception e) {                                toUrl = null;            }        }        if (toUrl == null)            continue;        inlinks.clear();                String anchor = outlink.getAnchor();        if (anchor.length() > maxAnchorLength) {            anchor = anchor.substring(0, maxAnchorLength);        }                inlinks.add(new Inlink(fromUrl, anchor));        context.write(new Text(toUrl), inlinks);    }}
1
private static String getHost(String url)
{    try {        return new URL(url).getHost().toLowerCase();    } catch (MalformedURLException e) {        return null;    }}
0
public void invert(Path linkDb, final Path segmentsDir, boolean normalize, boolean filter, boolean force) throws IOException, InterruptedException, ClassNotFoundException
{    FileSystem fs = segmentsDir.getFileSystem(getConf());    FileStatus[] files = fs.listStatus(segmentsDir, HadoopFSUtil.getPassDirectoriesFilter(fs));    invert(linkDb, HadoopFSUtil.getPaths(files), normalize, filter, force);}
0
public void invert(Path linkDb, Path[] segments, boolean normalize, boolean filter, boolean force) throws IOException, InterruptedException, ClassNotFoundException
{    Job job = LinkDb.createJob(getConf(), linkDb, normalize, filter);    Path lock = new Path(linkDb, LOCK_NAME);    FileSystem fs = linkDb.getFileSystem(getConf());    LockUtil.createLockFile(fs, lock, force);    Path currentLinkDb = new Path(linkDb, CURRENT_NAME);    Configuration conf = job.getConfiguration();    SimpleDateFormat sdf = new SimpleDateFormat("yyyy-MM-dd HH:mm:ss");    long start = System.currentTimeMillis();    if (LOG.isInfoEnabled()) {                                        if (conf.getBoolean(IGNORE_INTERNAL_LINKS, true)) {                    }        if (conf.getBoolean(IGNORE_EXTERNAL_LINKS, false)) {                    }    }    if (conf.getBoolean(IGNORE_INTERNAL_LINKS, true) && conf.getBoolean(IGNORE_EXTERNAL_LINKS, false)) {                LockUtil.removeLockFile(fs, lock);        return;    }    for (int i = 0; i < segments.length; i++) {        if (LOG.isInfoEnabled()) {                    }        FileInputFormat.addInputPath(job, new Path(segments[i], ParseData.DIR_NAME));    }    try {        boolean success = job.waitForCompletion(true);        if (!success) {            String message = "LinkDb job did not succeed, job status:" + job.getStatus().getState() + ", reason: " + job.getStatus().getFailureInfo();                        LockUtil.removeLockFile(fs, lock);            throw new RuntimeException(message);        }    } catch (IOException | InterruptedException | ClassNotFoundException e) {                LockUtil.removeLockFile(fs, lock);        throw e;    }    if (fs.exists(currentLinkDb)) {        if (LOG.isInfoEnabled()) {                    }                Path newLinkDb = FileOutputFormat.getOutputPath(job);        job = LinkDbMerger.createMergeJob(getConf(), linkDb, normalize, filter);        FileInputFormat.addInputPath(job, currentLinkDb);        FileInputFormat.addInputPath(job, newLinkDb);        try {            boolean success = job.waitForCompletion(true);            if (!success) {                String message = "LinkDb job did not succeed, job status:" + job.getStatus().getState() + ", reason: " + job.getStatus().getFailureInfo();                                NutchJob.cleanupAfterFailure(newLinkDb, lock, fs);                throw new RuntimeException(message);            }        } catch (IOException | InterruptedException | ClassNotFoundException e) {                        NutchJob.cleanupAfterFailure(newLinkDb, lock, fs);            throw e;        }        fs.delete(newLinkDb, true);    }    LinkDb.install(job, linkDb);    long end = System.currentTimeMillis();    }
1
private static Job createJob(Configuration config, Path linkDb, boolean normalize, boolean filter) throws IOException
{    Path newLinkDb = new Path(linkDb, Integer.toString(new Random().nextInt(Integer.MAX_VALUE)));    Job job = NutchJob.getInstance(config);    Configuration conf = job.getConfiguration();    job.setJobName("linkdb " + linkDb);    job.setInputFormatClass(SequenceFileInputFormat.class);    job.setJarByClass(LinkDb.class);    job.setMapperClass(LinkDb.LinkDbMapper.class);    job.setJarByClass(LinkDbMerger.class);    job.setCombinerClass(LinkDbMerger.LinkDbMergeReducer.class);        if (normalize || filter) {        try {            FileSystem fs = linkDb.getFileSystem(config);            if (!fs.exists(linkDb)) {                conf.setBoolean(LinkDbFilter.URL_FILTERING, filter);                conf.setBoolean(LinkDbFilter.URL_NORMALIZING, normalize);            }        } catch (Exception e) {                    }    }    job.setReducerClass(LinkDbMerger.LinkDbMergeReducer.class);    FileOutputFormat.setOutputPath(job, newLinkDb);    job.setOutputFormatClass(MapFileOutputFormat.class);    conf.setBoolean("mapreduce.output.fileoutputformat.compress", true);    job.setOutputKeyClass(Text.class);    job.setOutputValueClass(Inlinks.class);    return job;}
1
public static void install(Job job, Path linkDb) throws IOException
{    Configuration conf = job.getConfiguration();    Path newLinkDb = FileOutputFormat.getOutputPath(job);    FileSystem fs = linkDb.getFileSystem(conf);    Path old = new Path(linkDb, "old");    Path current = new Path(linkDb, CURRENT_NAME);    if (fs.exists(current)) {        if (fs.exists(old))            fs.delete(old, true);        fs.rename(current, old);    }    fs.mkdirs(linkDb);    fs.rename(newLinkDb, current);    if (fs.exists(old))        fs.delete(old, true);    LockUtil.removeLockFile(fs, new Path(linkDb, LOCK_NAME));}
0
public static void main(String[] args) throws Exception
{    int res = ToolRunner.run(NutchConfiguration.create(), new LinkDb(), args);    System.exit(res);}
0
public int run(String[] args) throws Exception
{    if (args.length < 2) {        System.err.println("Usage: LinkDb <linkdb> (-dir <segmentsDir> | <seg1> <seg2> ...) [-force] [-noNormalize] [-noFilter]");        System.err.println("\tlinkdb\toutput LinkDb to create or update");        System.err.println("\t-dir segmentsDir\tparent directory of several segments, OR");        System.err.println("\tseg1 seg2 ...\t list of segment directories");        System.err.println("\t-force\tforce update even if LinkDb appears to be locked (CAUTION advised)");        System.err.println("\t-noNormalize\tdon't normalize link URLs");        System.err.println("\t-noFilter\tdon't apply URLFilters to link URLs");        return -1;    }    Path db = new Path(args[0]);    ArrayList<Path> segs = new ArrayList<>();    boolean filter = true;    boolean normalize = true;    boolean force = false;    for (int i = 1; i < args.length; i++) {        if ("-dir".equals(args[i])) {            Path segDir = new Path(args[++i]);            FileSystem fs = segDir.getFileSystem(getConf());            FileStatus[] paths = fs.listStatus(segDir, HadoopFSUtil.getPassDirectoriesFilter(fs));            segs.addAll(Arrays.asList(HadoopFSUtil.getPaths(paths)));        } else if ("-noNormalize".equalsIgnoreCase(args[i])) {            normalize = false;        } else if ("-noFilter".equalsIgnoreCase(args[i])) {            filter = false;        } else if ("-force".equalsIgnoreCase(args[i])) {            force = true;        } else            segs.add(new Path(args[i]));    }    try {        invert(db, segs.toArray(new Path[segs.size()]), normalize, filter, force);        return 0;    } catch (Exception e) {                return -1;    }}
1
public Map<String, Object> run(Map<String, Object> args, String crawlId) throws Exception
{    Map<String, Object> results = new HashMap<>();    Path linkdb;    if (args.containsKey(Nutch.ARG_LINKDB)) {        Object path = args.get(Nutch.ARG_LINKDB);        if (path instanceof Path) {            linkdb = (Path) path;        } else {            linkdb = new Path(path.toString());        }    } else {        linkdb = new Path(crawlId + "/linkdb");    }    ArrayList<Path> segs = new ArrayList<>();    boolean filter = true;    boolean normalize = true;    boolean force = false;    if (args.containsKey("noNormalize")) {        normalize = false;    }    if (args.containsKey("noFilter")) {        filter = false;    }    if (args.containsKey("force")) {        force = true;    }    Path segmentsDir;    if (args.containsKey(Nutch.ARG_SEGMENTDIR)) {        Object segDir = args.get(Nutch.ARG_SEGMENTDIR);        if (segDir instanceof Path) {            segmentsDir = (Path) segDir;        } else {            segmentsDir = new Path(segDir.toString());        }        FileSystem fs = segmentsDir.getFileSystem(getConf());        FileStatus[] paths = fs.listStatus(segmentsDir, HadoopFSUtil.getPassDirectoriesFilter(fs));        segs.addAll(Arrays.asList(HadoopFSUtil.getPaths(paths)));    } else if (args.containsKey(Nutch.ARG_SEGMENTS)) {        Object segments = args.get(Nutch.ARG_SEGMENTS);        ArrayList<String> segmentList = new ArrayList<>();        if (segments instanceof ArrayList) {            segmentList = (ArrayList<String>) segments;        } else if (segments instanceof Path) {            segmentList.add(segments.toString());        }        for (String segment : segmentList) {            segs.add(new Path(segment));        }    } else {        String segmentDir = crawlId + "/segments";        File dir = new File(segmentDir);        File[] segmentsList = dir.listFiles();        Arrays.sort(segmentsList, (f1, f2) -> {            if (f1.lastModified() > f2.lastModified())                return -1;            else                return 0;        });        segs.add(new Path(segmentsList[0].getPath()));    }    try {        invert(linkdb, segs.toArray(new Path[segs.size()]), normalize, filter, force);        results.put(Nutch.VAL_RESULT, Integer.toString(0));        return results;    } catch (Exception e) {                results.put(Nutch.VAL_RESULT, Integer.toString(-1));        return results;    }}
1
public void setup(Mapper<Text, Inlinks, Text, Inlinks>.Context context)
{    Configuration conf = context.getConfiguration();    filter = conf.getBoolean(URL_FILTERING, false);    normalize = conf.getBoolean(URL_NORMALIZING, false);    if (filter) {        filters = new URLFilters(conf);    }    if (normalize) {        scope = conf.get(URL_NORMALIZING_SCOPE, URLNormalizers.SCOPE_LINKDB);        normalizers = new URLNormalizers(conf, scope);    }}
0
public void close()
{}
0
public void map(Text key, Inlinks value, Context context) throws IOException, InterruptedException
{    String url = key.toString();    Inlinks result = new Inlinks();    if (normalize) {        try {                        url = normalizers.normalize(url, scope);        } catch (Exception e) {                        url = null;        }    }    if (url != null && filter) {        try {                        url = filters.filter(url);        } catch (Exception e) {                        url = null;        }    }    if (url == null)                return;    Iterator<Inlink> it = value.iterator();    String fromUrl = null;    while (it.hasNext()) {        Inlink inlink = it.next();        fromUrl = inlink.getFromUrl();        if (normalize) {            try {                                fromUrl = normalizers.normalize(fromUrl, scope);            } catch (Exception e) {                                fromUrl = null;            }        }        if (fromUrl != null && filter) {            try {                                fromUrl = filters.filter(fromUrl);            } catch (Exception e) {                                fromUrl = null;            }        }        if (fromUrl != null) {            result.add(new Inlink(fromUrl, inlink.getAnchor()));        }    }    if (result.size() > 0) {                newKey.set(url);        context.write(newKey, result);    }}
1
public void setup(Reducer<Text, Inlinks, Text, Inlinks>.Context context)
{    Configuration conf = context.getConfiguration();    maxInlinks = conf.getInt("linkdb.max.inlinks", 10000);}
0
public void reduce(Text key, Iterable<Inlinks> values, Context context) throws IOException, InterruptedException
{    Inlinks result = new Inlinks();    for (Inlinks inlinks : values) {        int end = Math.min(maxInlinks - result.size(), inlinks.size());        Iterator<Inlink> it = inlinks.iterator();        int i = 0;        while (it.hasNext() && i++ < end) {            result.add(it.next());        }    }    if (result.size() == 0)        return;    context.write(key, result);}
0
public void close() throws IOException
{}
0
public void merge(Path output, Path[] dbs, boolean normalize, boolean filter) throws Exception
{    SimpleDateFormat sdf = new SimpleDateFormat("yyyy-MM-dd HH:mm:ss");    long start = System.currentTimeMillis();        Job job = createMergeJob(getConf(), output, normalize, filter);    for (int i = 0; i < dbs.length; i++) {        FileInputFormat.addInputPath(job, new Path(dbs[i], LinkDb.CURRENT_NAME));    }    try {        boolean success = job.waitForCompletion(true);        if (!success) {            String message = "LinkDbMerge job did not succeed, job status:" + job.getStatus().getState() + ", reason: " + job.getStatus().getFailureInfo();                        throw new RuntimeException(message);        }    } catch (IOException | InterruptedException | ClassNotFoundException e) {                throw e;    }    FileSystem fs = output.getFileSystem(getConf());    fs.mkdirs(output);    fs.rename(FileOutputFormat.getOutputPath(job), new Path(output, LinkDb.CURRENT_NAME));    long end = System.currentTimeMillis();    }
1
public static Job createMergeJob(Configuration config, Path linkDb, boolean normalize, boolean filter) throws IOException
{    Path newLinkDb = new Path(linkDb, "merge-" + Integer.toString(new Random().nextInt(Integer.MAX_VALUE)));    Job job = NutchJob.getInstance(config);    job.setJobName("linkdb merge " + linkDb);    Configuration conf = job.getConfiguration();    job.setInputFormatClass(SequenceFileInputFormat.class);    job.setMapperClass(LinkDbFilter.class);    conf.setBoolean(LinkDbFilter.URL_NORMALIZING, normalize);    conf.setBoolean(LinkDbFilter.URL_FILTERING, filter);    job.setJarByClass(LinkDbMerger.class);    job.setReducerClass(LinkDbMergeReducer.class);    FileOutputFormat.setOutputPath(job, newLinkDb);    job.setOutputFormatClass(MapFileOutputFormat.class);    conf.setBoolean("mapreduce.output.fileoutputformat.compress", true);    job.setOutputKeyClass(Text.class);    job.setOutputValueClass(Inlinks.class);        conf.setBoolean("mapreduce.fileoutputcommitter.marksuccessfuljobs", false);    return job;}
0
public static void main(String[] args) throws Exception
{    int res = ToolRunner.run(NutchConfiguration.create(), new LinkDbMerger(), args);    System.exit(res);}
0
public int run(String[] args) throws Exception
{    if (args.length < 2) {        System.err.println("Usage: LinkDbMerger <output_linkdb> <linkdb1> [<linkdb2> <linkdb3> ...] [-normalize] [-filter]");        System.err.println("\toutput_linkdb\toutput LinkDb");        System.err.println("\tlinkdb1 ...\tinput LinkDb-s (single input LinkDb is ok)");        System.err.println("\t-normalize\tuse URLNormalizer on both fromUrls and toUrls in linkdb(s) (usually not needed)");        System.err.println("\t-filter\tuse URLFilters on both fromUrls and toUrls in linkdb(s)");        return -1;    }    Path output = new Path(args[0]);    ArrayList<Path> dbs = new ArrayList<>();    boolean normalize = false;    boolean filter = false;    for (int i = 1; i < args.length; i++) {        if (args[i].equals("-filter")) {            filter = true;        } else if (args[i].equals("-normalize")) {            normalize = true;        } else            dbs.add(new Path(args[i]));    }    try {        merge(output, dbs.toArray(new Path[dbs.size()]), normalize, filter);        return 0;    } catch (Exception e) {                return -1;    }}
1
public void init(Path directory) throws Exception
{    this.directory = directory;}
0
public String[] getAnchors(Text url) throws IOException
{    Inlinks inlinks = getInlinks(url);    if (inlinks == null)        return null;    return inlinks.getAnchors();}
0
public Inlinks getInlinks(Text url) throws IOException
{    if (readers == null) {        synchronized (this) {            readers = MapFileOutputFormat.getReaders(new Path(directory, LinkDb.CURRENT_NAME), getConf());        }    }    return (Inlinks) MapFileOutputFormat.getEntry(readers, PARTITIONER, url, new Inlinks());}
0
public void close() throws IOException
{    if (readers != null) {        for (int i = 0; i < readers.length; i++) {            readers[i].close();        }    }}
0
public void setup(Mapper<Text, Inlinks, Text, Inlinks>.Context context)
{    Configuration conf = context.getConfiguration();    if (conf.get("linkdb.regex", null) != null) {        pattern = Pattern.compile(conf.get("linkdb.regex"));    }}
0
public void map(Text key, Inlinks value, Context context) throws IOException, InterruptedException
{    if (pattern != null) {        matcher = pattern.matcher(key.toString());        if (!matcher.matches()) {            return;        }    }    context.write(key, value);}
0
public void processDumpJob(String linkdb, String output, String regex) throws IOException, InterruptedException, ClassNotFoundException
{    SimpleDateFormat sdf = new SimpleDateFormat("yyyy-MM-dd HH:mm:ss");    long start = System.currentTimeMillis();    if (LOG.isInfoEnabled()) {                    }    Path outFolder = new Path(output);    Job job = NutchJob.getInstance(getConf());    job.setJobName("read " + linkdb);    job.setJarByClass(LinkDbReader.class);    Configuration conf = job.getConfiguration();    if (regex != null) {        conf.set("linkdb.regex", regex);        job.setMapperClass(LinkDBDumpMapper.class);    }    FileInputFormat.addInputPath(job, new Path(linkdb, LinkDb.CURRENT_NAME));    job.setInputFormatClass(SequenceFileInputFormat.class);    FileOutputFormat.setOutputPath(job, outFolder);    job.setOutputFormatClass(TextOutputFormat.class);    job.setOutputKeyClass(Text.class);    job.setOutputValueClass(Inlinks.class);    try {        boolean success = job.waitForCompletion(true);        if (!success) {            String message = "LinkDbRead job did not succeed, job status:" + job.getStatus().getState() + ", reason: " + job.getStatus().getFailureInfo();                        throw new RuntimeException(message);        }    } catch (IOException | InterruptedException | ClassNotFoundException e) {                throw e;    }    long end = System.currentTimeMillis();    }
1
protected int process(String line, StringBuilder output) throws Exception
{    Inlinks links = getInlinks(new Text(line));    if (links == null) {        output.append(" - no link information.");    } else {        Iterator<Inlink> it = links.iterator();        while (it.hasNext()) {            output.append(it.next().toString());        }    }    output.append("\n");    return 0;}
0
public static void main(String[] args) throws Exception
{    int res = ToolRunner.run(NutchConfiguration.create(), new LinkDbReader(), args);    System.exit(res);}
0
public int run(String[] args) throws Exception
{    if (args.length < 2) {        System.err.println("Usage: LinkDbReader <linkdb> (-dump <out_dir> [-regex <regex>]) | -url <url>");        System.err.println("\t-dump <out_dir>\tdump whole link db to a text file in <out_dir>");        System.err.println("\t\t-regex <regex>\trestrict to url's matching expression");        System.err.println("\t-url <url>\tprint information about <url> to System.out");        return -1;    }    int numConsumed = 0;    try {        for (int i = 1; i < args.length; i++) {            if (args[i].equals("-dump")) {                String regex = null;                for (int j = i + 1; j < args.length; j++) {                    if (args[i].equals("-regex")) {                        regex = args[++j];                    }                }                processDumpJob(args[0], args[i + 1], regex);                return 0;            } else if (args[i].equals("-url")) {                init(new Path(args[0]));                return processSingle(args[++i]);            } else if ((numConsumed = super.parseArgs(args, i)) > 0) {                init(new Path(args[0]));                i += numConsumed - 1;            } else {                System.err.println("Error: wrong argument " + args[1]);                return -1;            }        }    } catch (Exception e) {                return -1;    }    if (numConsumed > 0) {                return super.run();    }    return 0;}
1
public byte[] calculate(Content content, Parse parse)
{    byte[] data = content.getContent();    if (data == null || (data.length == 0))        data = content.getUrl().getBytes();    return MD5Hash.digest(data).getDigest();}
0
public void setConf(Configuration conf)
{    super.setConf(conf);    if (conf == null)        return;            defaultIncRate = conf.getFloat(SCHEDULE_INC_RATE, 0.2f);    defaultDecRate = conf.getFloat(SCHEDULE_DEC_RATE, 0.2f);        Reader mimeFile = conf.getConfResourceAsReader(conf.get(SCHEDULE_MIME_FILE, "adaptive-mimetypes.txt"));    try {        readMimeFile(mimeFile);    } catch (IOException e) {            }}
1
public CrawlDatum setFetchSchedule(Text url, CrawlDatum datum, long prevFetchTime, long prevModifiedTime, long fetchTime, long modifiedTime, int state)
{        INC_RATE = defaultIncRate;    DEC_RATE = defaultDecRate;        if (datum.getMetaData().containsKey(HttpHeaders.WRITABLE_CONTENT_TYPE)) {                String currentMime = MimeUtil.cleanMimeType(datum.getMetaData().get(HttpHeaders.WRITABLE_CONTENT_TYPE).toString());                if (mimeMap.containsKey(currentMime)) {                        INC_RATE = mimeMap.get(currentMime).inc;            DEC_RATE = mimeMap.get(currentMime).dec;        }    }    return super.setFetchSchedule(url, datum, prevFetchTime, prevModifiedTime, fetchTime, modifiedTime, state);}
0
private void readMimeFile(Reader mimeFile) throws IOException
{        mimeMap = new HashMap<>();        BufferedReader reader = new BufferedReader(mimeFile);    String line = null;    String[] splits = null;        while ((line = reader.readLine()) != null) {                if (StringUtils.isNotBlank(line) && !line.startsWith("#")) {                        splits = line.split("\t");                        if (splits.length == 3) {                                mimeMap.put(StringUtils.lowerCase(splits[0]), new AdaptiveRate(Float.valueOf(splits[1]), Float.valueOf(splits[2])));            } else {                            }        }    }}
1
public static void main(String[] args) throws Exception
{    FetchSchedule fs = new MimeAdaptiveFetchSchedule();    fs.setConf(NutchConfiguration.create());        long curTime = 0;        long delta = 1000L * 3600L * 24L;            long update = 1000L * 3600L * 24L * 30L;    boolean changed = true;    long lastModified = 0;    int miss = 0;    int totalMiss = 0;    int maxMiss = 0;    int fetchCnt = 0;    int changeCnt = 0;        CrawlDatum p = new CrawlDatum(1, 3600 * 24 * 30, 1.0f);        org.apache.hadoop.io.MapWritable x = new org.apache.hadoop.io.MapWritable();    x.put(HttpHeaders.WRITABLE_CONTENT_TYPE, new Text("text/html; charset=utf-8"));    p.setMetaData(x);    p.setFetchTime(0);            for (int i = 0; i < 10000; i++) {        if (lastModified + update < curTime) {                                    changed = true;            changeCnt++;            lastModified = curTime;        }                if (p.getFetchTime() <= curTime) {            fetchCnt++;            fs.setFetchSchedule(new Text("http://www.example.com"), p, p.getFetchTime(), p.getModifiedTime(), curTime, lastModified, changed ? FetchSchedule.STATUS_MODIFIED : FetchSchedule.STATUS_NOTMODIFIED);                        if (!changed)                miss++;            if (miss > maxMiss)                maxMiss = miss;            changed = false;            totalMiss += miss;            miss = 0;        }        if (changed)            miss++;        curTime += delta;    }        }
1
protected Class<? extends Writable>[] getTypes()
{    return CLASSES;}
0
public Configuration getConf()
{    return conf;}
0
public void setConf(Configuration conf)
{    this.conf = conf;}
0
public int compare(Object o1, Object o2)
{    return _compare(o1, o2);}
0
public static int _compare(Object o1, Object o2)
{    if (o1 == null && o2 == null)        return 0;    if (o1 == null)        return -1;    if (o2 == null)        return 1;    if (!(o1 instanceof byte[]))        return -1;    if (!(o2 instanceof byte[]))        return 1;    byte[] data1 = (byte[]) o1;    byte[] data2 = (byte[]) o2;    return _compare(data1, 0, data1.length, data2, 0, data2.length);}
0
public static int _compare(byte[] data1, int s1, int l1, byte[] data2, int s2, int l2)
{    if (l2 > l1)        return -1;    if (l2 < l1)        return 1;    int res = 0;    for (int i = 0; i < l1; i++) {        res = (data1[s1 + i] - data2[s2 + i]);        if (res != 0)            return res;    }    return 0;}
0
public static synchronized Signature getSignature(Configuration conf)
{    String clazz = conf.get("db.signature.class", MD5Signature.class.getName());    ObjectCache objectCache = ObjectCache.get(conf);    Signature impl = (Signature) objectCache.getObject(clazz);    if (impl == null) {        try {            if (LOG.isInfoEnabled()) {                            }            Class<?> implClass = Class.forName(clazz);            impl = (Signature) implClass.getConstructor().newInstance();            impl.setConf(conf);            objectCache.setObject(clazz, impl);        } catch (Exception e) {            throw new RuntimeException("Couldn't create " + clazz, e);        }    }    return impl;}
1
public byte[] calculate(Content content, Parse parse)
{    String text = parse.getText();    if (text == null || text.length() == 0) {        return fallback.calculate(content, parse);    }    return MD5Hash.digest(text).getDigest();}
0
public void setConf(Configuration conf)
{    super.setConf(conf);    MIN_TOKEN_LEN = conf.getInt("db.signature.text_profile.min_token_len", 2);    QUANT_RATE = conf.getFloat("db.signature.text_profile.quant_rate", 0.01f);    secondaryLexicographicSorting = conf.getBoolean("db.signature.text_profile.sec_sort_lex", true);}
0
public byte[] calculate(Content content, Parse parse)
{    HashMap<String, Token> tokens = new HashMap<>();    String text = null;    if (parse != null)        text = parse.getText();    if (text == null || text.length() == 0)        return fallback.calculate(content, parse);    StringBuffer curToken = new StringBuffer();    int maxFreq = 0;    for (int i = 0; i < text.length(); i++) {        char c = text.charAt(i);        if (Character.isLetterOrDigit(c)) {            curToken.append(Character.toLowerCase(c));        } else {            if (curToken.length() > 0) {                if (curToken.length() > MIN_TOKEN_LEN) {                                        String s = curToken.toString();                    Token tok = tokens.get(s);                    if (tok == null) {                        tok = new Token(0, s);                        tokens.put(s, tok);                    }                    tok.cnt++;                    if (tok.cnt > maxFreq)                        maxFreq = tok.cnt;                }                curToken.setLength(0);            }        }    }        if (curToken.length() > MIN_TOKEN_LEN) {                String s = curToken.toString();        Token tok = tokens.get(s);        if (tok == null) {            tok = new Token(0, s);            tokens.put(s, tok);        }        tok.cnt++;        if (tok.cnt > maxFreq)            maxFreq = tok.cnt;    }    Iterator<Token> it = tokens.values().iterator();    ArrayList<Token> profile = new ArrayList<>();        int QUANT = Math.round(maxFreq * QUANT_RATE);    if (QUANT < 2) {        if (maxFreq > 1)            QUANT = 2;        else            QUANT = 1;    }    while (it.hasNext()) {        Token t = it.next();                t.cnt = (t.cnt / QUANT) * QUANT;                if (t.cnt < QUANT) {            continue;        }        profile.add(t);    }    Collections.sort(profile, new TokenComparator());    StringBuffer newText = new StringBuffer();    it = profile.iterator();    while (it.hasNext()) {        Token t = it.next();        if (newText.length() > 0)            newText.append("\n");        newText.append(t.toString());    }    return MD5Hash.digest(newText.toString()).getDigest();}
0
public String toString()
{    return val + " " + cnt;}
0
public int compare(Token t1, Token t2)
{    int diffCnt = t2.cnt - t1.cnt;    if (diffCnt == 0 && secondaryLexicographicSorting) {        return t1.val.compareTo(t2.val);    }    return diffCnt;}
0
public static void main(String[] args) throws Exception
{    TextProfileSignature sig = new TextProfileSignature();    sig.setConf(NutchConfiguration.create());    HashMap<String, byte[]> res = new HashMap<>();    File[] files = new File(args[0]).listFiles();    for (int i = 0; i < files.length; i++) {        FileInputStream fis = new FileInputStream(files[i]);        BufferedReader br = new BufferedReader(new InputStreamReader(fis, "UTF-8"));        StringBuffer text = new StringBuffer();        String line = null;        while ((line = br.readLine()) != null) {            if (text.length() > 0)                text.append("\n");            text.append(line);        }        br.close();        byte[] signature = sig.calculate(null, new ParseImpl(text.toString(), null));        res.put(files[i].toString(), signature);    }    Iterator<String> it = res.keySet().iterator();    while (it.hasNext()) {        String name = it.next();        byte[] signature = res.get(name);        System.out.println(name + "\t" + StringUtil.toHexString(signature));    }}
0
public void setConf(Configuration conf)
{    this.conf = conf;    seed = conf.getInt("partition.url.seed", 0);    mode = conf.get(PARTITION_MODE_KEY, PARTITION_MODE_HOST);        if (!mode.equals(PARTITION_MODE_IP) && !mode.equals(PARTITION_MODE_DOMAIN) && !mode.equals(PARTITION_MODE_HOST)) {                mode = PARTITION_MODE_HOST;    }    normalizers = new URLNormalizers(conf, URLNormalizers.SCOPE_PARTITION);}
1
public Configuration getConf()
{    return conf;}
0
public void close()
{}
0
public static ExchangeConfig getInstance(Element element)
{    String id = element.getAttribute("id");    String clazz = element.getAttribute("class");        NodeList writerList = element.getElementsByTagName("writer");    String[] writers = new String[writerList.getLength()];    for (int i = 0; i < writerList.getLength(); i++) {        writers[i] = ((Element) writerList.item(i)).getAttribute("id");    }        NodeList paramList = element.getElementsByTagName("param");    Map<String, String> paramsMap = new HashMap<>();    for (int i = 0; i < paramList.getLength(); i++) {        Element param = (Element) paramList.item(i);        paramsMap.put(param.getAttribute("name"), param.getAttribute("value"));    }    return new ExchangeConfig(id, clazz, writers, paramsMap);}
0
public String getId()
{    return id;}
0
public String getClazz()
{    return clazz;}
0
 String[] getWritersIDs()
{    return writersIDs;}
0
public Map<String, String> getParameters()
{    return parameters;}
0
public boolean areAvailableExchanges()
{    return availableExchanges;}
0
private ExchangeConfig[] loadConfigurations(Configuration conf)
{    String filename = conf.get("exchanges.exchanges.file", "exchanges.xml");    InputSource inputSource = new InputSource(conf.getConfResourceAsInputStream(filename));    final List<ExchangeConfig> configList = new LinkedList<>();    try {        DocumentBuilderFactory factory = DocumentBuilderFactory.newInstance();        DocumentBuilder builder = factory.newDocumentBuilder();        Element rootElement = builder.parse(inputSource).getDocumentElement();        NodeList exchangeList = rootElement.getElementsByTagName("exchange");        for (int i = 0; i < exchangeList.getLength(); i++) {            Element element = (Element) exchangeList.item(i);            ExchangeConfig exchangeConfig = ExchangeConfig.getInstance(element);            if ("default".equals(exchangeConfig.getClazz())) {                this.defaultExchangeConfig = exchangeConfig;                continue;            }            configList.add(exchangeConfig);        }    } catch (SAXException | IOException | ParserConfigurationException e) {            }    return configList.toArray(new ExchangeConfig[0]);}
1
public void open()
{    exchanges.forEach((id, value) -> value.exchange.open(value.config.getParameters()));}
0
public String[] indexWriters(final NutchDocument nutchDocument)
{    final Set<String> writersIDs = new HashSet<>();    exchanges.forEach((id, value) -> {        if (value.exchange.match(nutchDocument)) {            writersIDs.addAll(Arrays.asList(value.config.getWritersIDs()));        }    });        if (defaultExchangeConfig != null && writersIDs.isEmpty()) {        return defaultExchangeConfig.getWritersIDs();    }    return writersIDs.toArray(new String[0]);}
0
public List<InputSplit> getSplits(JobContext job) throws IOException
{    List<FileStatus> files = listStatus(job);    List<InputSplit> splits = new ArrayList<>();    for (FileStatus cur : files) {        splits.add(new FileSplit(cur.getPath(), 0, cur.getLen(), (String[]) null));    }    return splits;}
0
public static boolean isParsing(Configuration conf)
{    return conf.getBoolean("fetcher.parse", true);}
0
public static boolean isStoringContent(Configuration conf)
{    return conf.getBoolean("fetcher.store.content", true);}
0
private AtomicInteger getActiveThreads()
{    return activeThreads;}
0
private void reportStatus(Context context, FetchItemQueues fetchQueues, int pagesLastSec, int bytesLastSec) throws IOException
{    StringBuilder status = new StringBuilder();    Long elapsed = Long.valueOf((System.currentTimeMillis() - start) / 1000);    float avgPagesSec = (float) pages.get() / elapsed.floatValue();    long avgBytesSec = (bytes.get() / 128l) / elapsed.longValue();    status.append(activeThreads).append(" threads (").append(spinWaiting.get()).append(" waiting), ");    status.append(fetchQueues.getQueueCount()).append(" queues, ");    status.append(fetchQueues.getTotalSize()).append(" URLs queued, ");    status.append(pages).append(" pages, ").append(errors).append(" errors, ");    status.append(String.format("%.2f", avgPagesSec)).append(" pages/s (");    status.append(pagesLastSec).append(" last sec), ");    status.append(avgBytesSec).append(" kbits/s (").append((bytesLastSec / 128)).append(" last sec)");    context.setStatus(status.toString());}
0
public void setup(Mapper<Text, CrawlDatum, Text, NutchWritable>.Context context)
{    Configuration conf = context.getConfiguration();    segmentName = conf.get(Nutch.SEGMENT_NAME_KEY);    storingContent = isStoringContent(conf);    parsing = isParsing(conf);}
0
public void run(Context innerContext) throws IOException
{    setup(innerContext);    Configuration conf = innerContext.getConfiguration();    LinkedList<FetcherThread> fetcherThreads = new LinkedList<>();    FetchItemQueues fetchQueues = new FetchItemQueues(conf);    QueueFeeder feeder;    int threadCount = conf.getInt("fetcher.threads.fetch", 10);    if (LOG.isInfoEnabled()) {            }    int timeoutDivisor = conf.getInt("fetcher.threads.timeout.divisor", 2);    if (LOG.isInfoEnabled()) {            }    int queueDepthMuliplier = conf.getInt("fetcher.queue.depth.multiplier", 50);    feeder = new QueueFeeder(innerContext, fetchQueues, threadCount * queueDepthMuliplier);            long timelimit = conf.getLong("fetcher.timelimit", -1);    if (timelimit != -1)        feeder.setTimeLimit(timelimit);    feeder.start();    for (int i = 0; i < threadCount; i++) {                FetcherThread t = new FetcherThread(conf, getActiveThreads(), fetchQueues, feeder, spinWaiting, lastRequestStart, innerContext, errors, segmentName, parsing, storingContent, pages, bytes);        fetcherThreads.add(t);        t.start();    }        long timeout = conf.getInt("mapreduce.task.timeout", 10 * 60 * 1000) / timeoutDivisor;            int pagesLastSec;    int bytesLastSec;    int throughputThresholdNumRetries = 0;    int throughputThresholdPages = conf.getInt("fetcher.throughput.threshold.pages", -1);    if (LOG.isInfoEnabled()) {            }    int throughputThresholdMaxRetries = conf.getInt("fetcher.throughput.threshold.retries", 5);    if (LOG.isInfoEnabled()) {            }    long throughputThresholdTimeLimit = conf.getLong("fetcher.throughput.threshold.check.after", -1);    int targetBandwidth = conf.getInt("fetcher.bandwidth.target", -1) * 1000;    int maxNumThreads = conf.getInt("fetcher.maxNum.threads", threadCount);    if (maxNumThreads < threadCount) {                maxNumThreads = threadCount;    }    int bandwidthTargetCheckEveryNSecs = conf.getInt("fetcher.bandwidth.target.check.everyNSecs", 30);    if (bandwidthTargetCheckEveryNSecs < 1) {                bandwidthTargetCheckEveryNSecs = 1;    }    int maxThreadsPerQueue = conf.getInt("fetcher.threads.per.queue", 1);    int bandwidthTargetCheckCounter = 0;    long bytesAtLastBWTCheck = 0l;    do {                pagesLastSec = pages.get();        bytesLastSec = (int) bytes.get();        try {            Thread.sleep(1000);        } catch (InterruptedException e) {        }        pagesLastSec = pages.get() - pagesLastSec;        bytesLastSec = (int) bytes.get() - bytesLastSec;        innerContext.getCounter("FetcherStatus", "bytes_downloaded").increment(bytesLastSec);        reportStatus(innerContext, fetchQueues, pagesLastSec, bytesLastSec);                if (!feeder.isAlive() && fetchQueues.getTotalSize() < 5) {            fetchQueues.dump();        }                if (throughputThresholdTimeLimit < System.currentTimeMillis() && throughputThresholdPages != -1) {                        if (pagesLastSec < throughputThresholdPages) {                throughputThresholdNumRetries++;                                                if (throughputThresholdNumRetries == throughputThresholdMaxRetries) {                                                            throughputThresholdPages = -1;                                                            int hitByThrougputThreshold = fetchQueues.emptyQueues();                    if (hitByThrougputThreshold != 0)                        innerContext.getCounter("FetcherStatus", "hitByThrougputThreshold").increment(hitByThrougputThreshold);                }            }        }                if (targetBandwidth > 0) {            if (bandwidthTargetCheckCounter < bandwidthTargetCheckEveryNSecs)                bandwidthTargetCheckCounter++;            else if (bandwidthTargetCheckCounter == bandwidthTargetCheckEveryNSecs) {                long bpsSinceLastCheck = ((bytes.get() - bytesAtLastBWTCheck) * 8) / bandwidthTargetCheckEveryNSecs;                bytesAtLastBWTCheck = bytes.get();                bandwidthTargetCheckCounter = 0;                int averageBdwPerThread = 0;                if (activeThreads.get() > 0)                    averageBdwPerThread = Math.round(bpsSinceLastCheck / activeThreads.get());                                if (bpsSinceLastCheck < targetBandwidth && averageBdwPerThread > 0) {                    if ((fetchQueues.getQueueCount() * maxThreadsPerQueue) > activeThreads.get()) {                        long remainingBdw = targetBandwidth - bpsSinceLastCheck;                        int additionalThreads = Math.round(remainingBdw / averageBdwPerThread);                        int availableThreads = maxNumThreads - activeThreads.get();                                                                        additionalThreads = (availableThreads < additionalThreads ? availableThreads : additionalThreads);                                                                        for (int i = 0; i < additionalThreads; i++) {                            FetcherThread thread = new FetcherThread(conf, getActiveThreads(), fetchQueues, feeder, spinWaiting, lastRequestStart, innerContext, errors, segmentName, parsing, storingContent, pages, bytes);                            fetcherThreads.add(thread);                            thread.start();                        }                    }                } else if (bpsSinceLastCheck > targetBandwidth && averageBdwPerThread > 0) {                                                            long excessBdw = bpsSinceLastCheck - targetBandwidth;                    int excessThreads = Math.round(excessBdw / averageBdwPerThread);                                                            if (excessThreads >= fetcherThreads.size())                        excessThreads = 0;                                        for (int i = 0; i < excessThreads; i++) {                        FetcherThread thread = fetcherThreads.removeLast();                        thread.setHalted(true);                    }                }            }        }                if (!feeder.isAlive()) {            int hitByTimeLimit = fetchQueues.checkTimelimit();            if (hitByTimeLimit != 0)                innerContext.getCounter("FetcherStatus", "hitByTimeLimit").increment(hitByTimeLimit);        }                if ((System.currentTimeMillis() - lastRequestStart.get()) > timeout) {            if (LOG.isWarnEnabled()) {                                for (int i = 0; i < fetcherThreads.size(); i++) {                    FetcherThread thread = fetcherThreads.get(i);                    if (thread.isAlive()) {                                                if (LOG.isDebugEnabled()) {                            StackTraceElement[] stack = thread.getStackTrace();                            StringBuilder sb = new StringBuilder();                            sb.append("Stack of thread #").append(i).append(":\n");                            for (StackTraceElement s : stack) {                                sb.append(s.toString()).append('\n');                            }                                                    }                    }                }            }            return;        }    } while (activeThreads.get() > 0);    }
1
public void fetch(Path segment, int threads) throws IOException, InterruptedException, ClassNotFoundException
{    checkConfiguration();    long start = System.currentTimeMillis();    if (LOG.isInfoEnabled()) {                    }                long timelimit = getConf().getLong("fetcher.timelimit.mins", -1);    if (timelimit != -1) {        timelimit = System.currentTimeMillis() + (timelimit * 60 * 1000);                getConf().setLong("fetcher.timelimit", timelimit);    }            timelimit = getConf().getLong("fetcher.throughput.threshold.check.after", 10);    timelimit = System.currentTimeMillis() + (timelimit * 60 * 1000);    getConf().setLong("fetcher.throughput.threshold.check.after", timelimit);    int maxOutlinkDepth = getConf().getInt("fetcher.follow.outlinks.depth", -1);    if (maxOutlinkDepth > 0) {                int maxOutlinkDepthNumLinks = getConf().getInt("fetcher.follow.outlinks.num.links", 4);        int outlinksDepthDivisor = getConf().getInt("fetcher.follow.outlinks.depth.divisor", 2);        int totalOutlinksToFollow = 0;        for (int i = 0; i < maxOutlinkDepth; i++) {            totalOutlinksToFollow += (int) Math.floor(outlinksDepthDivisor / (i + 1) * maxOutlinkDepthNumLinks);        }            }    Job job = NutchJob.getInstance(getConf());    job.setJobName("FetchData");    Configuration conf = job.getConfiguration();    conf.setInt("fetcher.threads.fetch", threads);    conf.set(Nutch.SEGMENT_NAME_KEY, segment.getName());        conf.set("mapreduce.map.speculative", "false");    FileInputFormat.addInputPath(job, new Path(segment, CrawlDatum.GENERATE_DIR_NAME));    job.setInputFormatClass(InputFormat.class);    job.setJarByClass(Fetcher.class);    job.setMapperClass(Fetcher.FetcherRun.class);    FileOutputFormat.setOutputPath(job, segment);    job.setOutputFormatClass(FetcherOutputFormat.class);    job.setOutputKeyClass(Text.class);    job.setOutputValueClass(NutchWritable.class);    try {        boolean success = job.waitForCompletion(true);        if (!success) {            String message = "Fetcher job did not succeed, job status:" + job.getStatus().getState() + ", reason: " + job.getStatus().getFailureInfo();                        throw new RuntimeException(message);        }    } catch (InterruptedException | ClassNotFoundException e) {                throw e;    }    long end = System.currentTimeMillis();    }
1
public static void main(String[] args) throws Exception
{    int res = ToolRunner.run(NutchConfiguration.create(), new Fetcher(), args);    System.exit(res);}
0
public int run(String[] args) throws Exception
{    String usage = "Usage: Fetcher <segment> [-threads n]";    if (args.length < 1) {        System.err.println(usage);        return -1;    }    Path segment = new Path(args[0]);    int threads = getConf().getInt("fetcher.threads.fetch", 10);    for (int i = 1; i < args.length; i++) {                if (args[i].equals("-threads")) {                        threads = Integer.parseInt(args[++i]);        }    }    getConf().setInt("fetcher.threads.fetch", threads);    try {        fetch(segment, threads);        return 0;    } catch (Exception e) {                return -1;    }}
1
private void checkConfiguration()
{        String agentName = getConf().get("http.agent.name");    if (agentName == null || agentName.trim().length() == 0) {        String message = "Fetcher: No agents listed in 'http.agent.name'" + " property.";        if (LOG.isErrorEnabled()) {                    }        throw new IllegalArgumentException(message);    }}
1
public Map<String, Object> run(Map<String, Object> args, String crawlId) throws Exception
{    Map<String, Object> results = new HashMap<>();    Path segment = null;    if (args.containsKey(Nutch.ARG_SEGMENTS)) {        Object seg = args.get(Nutch.ARG_SEGMENTS);        if (seg instanceof Path) {            segment = (Path) seg;        } else if (seg instanceof String) {            segment = new Path(seg.toString());        } else if (seg instanceof ArrayList) {            String[] segmentsArray = (String[]) seg;            segment = new Path(segmentsArray[0].toString());            if (segmentsArray.length > 1) {                            }        }    } else {        String segmentDir = crawlId + "/segments";        File segmentsDir = new File(segmentDir);        File[] segmentsList = segmentsDir.listFiles();        Arrays.sort(segmentsList, (f1, f2) -> {            if (f1.lastModified() > f2.lastModified())                return -1;            else                return 0;        });        segment = new Path(segmentsList[0].getPath());    }    int threads = getConf().getInt("fetcher.threads.fetch", 10);        if (args.containsKey("threads")) {                threads = Integer.parseInt((String) args.get("threads"));    }    getConf().setInt("fetcher.threads.fetch", threads);    try {        fetch(segment, threads);        results.put(Nutch.VAL_RESULT, Integer.toString(0));        return results;    } catch (Exception e) {                results.put(Nutch.VAL_RESULT, Integer.toString(-1));        return results;    }}
1
public void checkOutputSpecs(JobContext job) throws IOException
{    Configuration conf = job.getConfiguration();    Path out = FileOutputFormat.getOutputPath(job);    if ((out == null) && (job.getNumReduceTasks() != 0)) {        throw new InvalidJobConfException("Output directory not set in conf.");    }    FileSystem fs = out.getFileSystem(conf);    if (fs.exists(new Path(out, CrawlDatum.FETCH_DIR_NAME))) {        throw new IOException("Segment already fetched!");    }}
0
public RecordWriter<Text, NutchWritable> getRecordWriter(TaskAttemptContext context) throws IOException
{    Configuration conf = context.getConfiguration();    String name = getUniqueFile(context, "part", "");    Path out = FileOutputFormat.getOutputPath(context);    final Path fetch = new Path(new Path(out, CrawlDatum.FETCH_DIR_NAME), name);    final Path content = new Path(new Path(out, Content.DIR_NAME), name);    final CompressionType compType = SequenceFileOutputFormat.getOutputCompressionType(context);    Option fKeyClassOpt = MapFile.Writer.keyClass(Text.class);    org.apache.hadoop.io.SequenceFile.Writer.Option fValClassOpt = SequenceFile.Writer.valueClass(CrawlDatum.class);    org.apache.hadoop.io.SequenceFile.Writer.Option fProgressOpt = SequenceFile.Writer.progressable((Progressable) context);    org.apache.hadoop.io.SequenceFile.Writer.Option fCompOpt = SequenceFile.Writer.compression(compType);    final MapFile.Writer fetchOut = new MapFile.Writer(conf, fetch, fKeyClassOpt, fValClassOpt, fCompOpt, fProgressOpt);    return new RecordWriter<Text, NutchWritable>() {        private MapFile.Writer contentOut;        private RecordWriter<Text, Parse> parseOut;        {            if (Fetcher.isStoringContent(conf)) {                Option cKeyClassOpt = MapFile.Writer.keyClass(Text.class);                org.apache.hadoop.io.SequenceFile.Writer.Option cValClassOpt = SequenceFile.Writer.valueClass(Content.class);                org.apache.hadoop.io.SequenceFile.Writer.Option cProgressOpt = SequenceFile.Writer.progressable((Progressable) context);                org.apache.hadoop.io.SequenceFile.Writer.Option cCompOpt = SequenceFile.Writer.compression(compType);                contentOut = new MapFile.Writer(conf, content, cKeyClassOpt, cValClassOpt, cCompOpt, cProgressOpt);            }            if (Fetcher.isParsing(conf)) {                parseOut = new ParseOutputFormat().getRecordWriter(context);            }        }        public void write(Text key, NutchWritable value) throws IOException, InterruptedException {            Writable w = value.get();            if (w instanceof CrawlDatum)                fetchOut.append(key, w);            else if (w instanceof Content && contentOut != null)                contentOut.append(key, w);            else if (w instanceof Parse && parseOut != null)                parseOut.write(key, (Parse) w);        }        public void close(TaskAttemptContext context) throws IOException, InterruptedException {            fetchOut.close();            if (contentOut != null) {                contentOut.close();            }            if (parseOut != null) {                parseOut.close(context);            }        }    };}
0
public void write(Text key, NutchWritable value) throws IOException, InterruptedException
{    Writable w = value.get();    if (w instanceof CrawlDatum)        fetchOut.append(key, w);    else if (w instanceof Content && contentOut != null)        contentOut.append(key, w);    else if (w instanceof Parse && parseOut != null)        parseOut.write(key, (Parse) w);}
0
public void close(TaskAttemptContext context) throws IOException, InterruptedException
{    fetchOut.close();    if (contentOut != null) {        contentOut.close();    }    if (parseOut != null) {        parseOut.close(context);    }}
0
public void run()
{        activeThreads.incrementAndGet();    FetchItem fit = null;    try {                if (parsing && NutchServer.getInstance().isRunning())            reportToNutchServer = true;        while (true) {                        if (reportToNutchServer)                this.fetchNode = new FetchNode();            else                this.fetchNode = null;                        if (isHalted()) {                                fit = null;                return;            }            fit = ((FetchItemQueues) fetchQueues).getFetchItem();            if (fit == null) {                if (feeder.isAlive() || ((FetchItemQueues) fetchQueues).getTotalSize() > 0) {                                                            ((AtomicInteger) spinWaiting).incrementAndGet();                    try {                        Thread.sleep(500);                    } catch (Exception e) {                    }                    ((AtomicInteger) spinWaiting).decrementAndGet();                    continue;                } else {                                                            return;                }            }            lastRequestStart.set(System.currentTimeMillis());            Text reprUrlWritable = (Text) fit.datum.getMetaData().get(Nutch.WRITABLE_REPR_URL_KEY);            if (reprUrlWritable == null) {                setReprUrl(fit.url.toString());            } else {                setReprUrl(reprUrlWritable.toString());            }            try {                                redirecting = false;                redirectCount = 0;                                if (activatePublisher) {                    FetcherThreadEvent startEvent = new FetcherThreadEvent(PublishEventType.START, fit.getUrl().toString());                    publisher.publish(startEvent, conf);                }                do {                    if (LOG.isInfoEnabled()) {                                            }                    if (LOG.isDebugEnabled()) {                                            }                    redirecting = false;                    Protocol protocol = this.protocolFactory.getProtocol(fit.u);                    BaseRobotRules rules = protocol.getRobotRules(fit.url, fit.datum, robotsTxtContent);                    if (robotsTxtContent != null) {                        outputRobotsTxt(robotsTxtContent);                        robotsTxtContent.clear();                    }                    if (!rules.isAllowed(fit.url.toString())) {                                                ((FetchItemQueues) fetchQueues).finishFetchItem(fit, true);                                                output(fit.url, fit.datum, null, ProtocolStatus.STATUS_ROBOTS_DENIED, CrawlDatum.STATUS_FETCH_GONE);                        context.getCounter("FetcherStatus", "robots_denied").increment(1);                        continue;                    }                    if (rules.getCrawlDelay() > 0) {                        if (rules.getCrawlDelay() > maxCrawlDelay && maxCrawlDelay >= 0) {                                                        ((FetchItemQueues) fetchQueues).finishFetchItem(fit, true);                                                        output(fit.url, fit.datum, null, ProtocolStatus.STATUS_ROBOTS_DENIED, CrawlDatum.STATUS_FETCH_GONE);                            context.getCounter("FetcherStatus", "robots_denied_maxcrawldelay").increment(1);                            continue;                        } else {                            FetchItemQueue fiq = ((FetchItemQueues) fetchQueues).getFetchItemQueue(fit.queueID);                            fiq.crawlDelay = rules.getCrawlDelay();                            if (LOG.isDebugEnabled()) {                                                            }                        }                    }                    ProtocolOutput output = protocol.getProtocolOutput(fit.url, fit.datum);                    ProtocolStatus status = output.getStatus();                    Content content = output.getContent();                    ParseStatus pstatus = null;                                        ((FetchItemQueues) fetchQueues).finishFetchItem(fit);                                        if (fetchNode != null) {                        fetchNode.setStatus(status.getCode());                        fetchNode.setFetchTime(System.currentTimeMillis());                        fetchNode.setUrl(fit.url);                    }                                        if (activatePublisher) {                        FetcherThreadEvent endEvent = new FetcherThreadEvent(PublishEventType.END, fit.getUrl().toString());                        endEvent.addEventData("status", status.getName());                        publisher.publish(endEvent, conf);                    }                    context.getCounter("FetcherStatus", status.getName()).increment(1);                    switch(status.getCode()) {                        case ProtocolStatus.WOULDBLOCK:                                                        ((FetchItemQueues) fetchQueues).addFetchItem(fit);                            break;                        case                         ProtocolStatus.SUCCESS:                            pstatus = output(fit.url, fit.datum, content, status, CrawlDatum.STATUS_FETCH_SUCCESS, fit.outlinkDepth);                            updateStatus(content.getContent().length);                            if (pstatus != null && pstatus.isSuccess() && pstatus.getMinorCode() == ParseStatus.SUCCESS_REDIRECT) {                                String newUrl = pstatus.getMessage();                                int refreshTime = Integer.valueOf(pstatus.getArgs()[1]);                                Text redirUrl = handleRedirect(fit, newUrl, refreshTime < Fetcher.PERM_REFRESH_TIME, Fetcher.CONTENT_REDIR);                                if (redirUrl != null) {                                    fit = queueRedirect(redirUrl, fit);                                }                            }                            break;                                                case ProtocolStatus.MOVED:                        case ProtocolStatus.TEMP_MOVED:                            int code;                            boolean temp;                            if (status.getCode() == ProtocolStatus.MOVED) {                                code = CrawlDatum.STATUS_FETCH_REDIR_PERM;                                temp = false;                            } else {                                code = CrawlDatum.STATUS_FETCH_REDIR_TEMP;                                temp = true;                            }                            output(fit.url, fit.datum, content, status, code);                            String newUrl = status.getMessage();                            Text redirUrl = handleRedirect(fit, newUrl, temp, Fetcher.PROTOCOL_REDIR);                            if (redirUrl != null) {                                fit = queueRedirect(redirUrl, fit);                            } else {                                                                redirecting = false;                            }                            break;                        case ProtocolStatus.EXCEPTION:                            logError(fit.url, status.getMessage());                            int killedURLs = ((FetchItemQueues) fetchQueues).checkExceptionThreshold(fit.getQueueID());                            if (killedURLs != 0)                                context.getCounter("FetcherStatus", "AboveExceptionThresholdInQueue").increment(killedURLs);                                                case ProtocolStatus.RETRY:                        case ProtocolStatus.BLOCKED:                            output(fit.url, fit.datum, null, status, CrawlDatum.STATUS_FETCH_RETRY);                            break;                                                case ProtocolStatus.GONE:                        case ProtocolStatus.NOTFOUND:                        case ProtocolStatus.ACCESS_DENIED:                        case ProtocolStatus.ROBOTS_DENIED:                            output(fit.url, fit.datum, null, status, CrawlDatum.STATUS_FETCH_GONE);                            break;                        case ProtocolStatus.NOTMODIFIED:                            output(fit.url, fit.datum, null, status, CrawlDatum.STATUS_FETCH_NOTMODIFIED);                            break;                        default:                            if (LOG.isWarnEnabled()) {                                                            }                            output(fit.url, fit.datum, null, status, CrawlDatum.STATUS_FETCH_RETRY);                    }                    if (redirecting && redirectCount > maxRedirect) {                        ((FetchItemQueues) fetchQueues).finishFetchItem(fit);                        if (LOG.isInfoEnabled()) {                                                    }                        output(fit.url, fit.datum, null, ProtocolStatus.STATUS_REDIR_EXCEEDED, CrawlDatum.STATUS_FETCH_GONE);                    }                } while (redirecting && (redirectCount <= maxRedirect));            } catch (Throwable t) {                                                ((FetchItemQueues) fetchQueues).finishFetchItem(fit);                String message;                if (LOG.isDebugEnabled()) {                    message = StringUtils.stringifyException(t);                } else if (logUtil.logShort(t)) {                    message = t.getClass().getName();                } else {                    message = StringUtils.stringifyException(t);                }                logError(fit.url, message);                output(fit.url, fit.datum, null, ProtocolStatus.STATUS_FAILED, CrawlDatum.STATUS_FETCH_RETRY);            }        }    } catch (Throwable e) {        if (LOG.isErrorEnabled()) {                    }    } finally {        if (fit != null)            ((FetchItemQueues) fetchQueues).finishFetchItem(fit);                activeThreads.decrementAndGet();            }}
1
private Text handleRedirect(FetchItem fit, String newUrl, boolean temp, String redirType) throws MalformedURLException, URLFilterException, InterruptedException
{    if (newUrl.length() > maxOutlinkLength) {        return null;    }    newUrl = normalizers.normalize(newUrl, URLNormalizers.SCOPE_FETCHER);    newUrl = urlFilters.filter(newUrl);    String urlString = fit.url.toString();    if (newUrl == null || newUrl.equals(urlString)) {                return null;    }    if (ignoreAlsoRedirects && (ignoreExternalLinks || ignoreInternalLinks)) {        try {            URL origUrl = fit.u;            URL redirUrl = new URL(newUrl);            if (ignoreExternalLinks) {                String origHostOrDomain, newHostOrDomain;                if ("bydomain".equalsIgnoreCase(ignoreExternalLinksMode)) {                    origHostOrDomain = URLUtil.getDomainName(origUrl).toLowerCase();                    newHostOrDomain = URLUtil.getDomainName(redirUrl).toLowerCase();                } else {                                        origHostOrDomain = origUrl.getHost().toLowerCase();                    newHostOrDomain = redirUrl.getHost().toLowerCase();                }                if (!origHostOrDomain.equals(newHostOrDomain)) {                                        return null;                }            }            if (ignoreInternalLinks) {                String origHost = origUrl.getHost().toLowerCase();                String newHost = redirUrl.getHost().toLowerCase();                if (origHost.equals(newHost)) {                                        return null;                }            }        } catch (MalformedURLException e) {            return null;        }    }    reprUrl = URLUtil.chooseRepr(reprUrl, newUrl, temp);    Text url = new Text(newUrl);    if (maxRedirect > 0) {        redirecting = true;        redirectCount++;                return url;    } else {        CrawlDatum newDatum = new CrawlDatum(CrawlDatum.STATUS_LINKED, fit.datum.getFetchInterval(), fit.datum.getScore());                newDatum.getMetaData().putAll(fit.datum.getMetaData());        try {            scfilters.initialScore(url, newDatum);        } catch (ScoringFilterException e) {            e.printStackTrace();        }        if (reprUrl != null) {            newDatum.getMetaData().put(Nutch.WRITABLE_REPR_URL_KEY, new Text(reprUrl));        }        output(url, newDatum, null, null, CrawlDatum.STATUS_LINKED);                return null;    }}
1
private FetchItem queueRedirect(Text redirUrl, FetchItem fit) throws ScoringFilterException
{    CrawlDatum newDatum = new CrawlDatum(CrawlDatum.STATUS_DB_UNFETCHED, fit.datum.getFetchInterval(), fit.datum.getScore());        newDatum.getMetaData().putAll(fit.datum.getMetaData());    scfilters.initialScore(redirUrl, newDatum);    if (reprUrl != null) {        newDatum.getMetaData().put(Nutch.WRITABLE_REPR_URL_KEY, new Text(reprUrl));    }    fit = FetchItem.create(redirUrl, newDatum, queueMode);    if (fit != null) {        FetchItemQueue fiq = ((FetchItemQueues) fetchQueues).getFetchItemQueue(fit.queueID);        fiq.addInProgressFetchItem(fit);    } else {                redirecting = false;        context.getCounter("FetcherStatus", "FetchItem.notCreated.redirect").increment(1);    }    return fit;}
0
private void logError(Text url, String message)
{    if (LOG.isInfoEnabled()) {            }    errors.incrementAndGet();}
1
private ParseStatus output(Text key, CrawlDatum datum, Content content, ProtocolStatus pstatus, int status) throws InterruptedException
{    return output(key, datum, content, pstatus, status, 0);}
0
private ParseStatus output(Text key, CrawlDatum datum, Content content, ProtocolStatus pstatus, int status, int outlinkDepth) throws InterruptedException
{    datum.setStatus(status);    datum.setFetchTime(System.currentTimeMillis());    if (pstatus != null)        datum.getMetaData().put(Nutch.WRITABLE_PROTO_STATUS_KEY, pstatus);    ParseResult parseResult = null;    if (content != null) {        Metadata metadata = content.getMetadata();                if (content.getContentType() != null)            datum.getMetaData().put(new Text(Metadata.CONTENT_TYPE), new Text(content.getContentType()));                metadata.set(Nutch.SEGMENT_NAME_KEY, segmentName);                try {            scfilters.passScoreBeforeParsing(key, datum, content);        } catch (Exception e) {            if (LOG.isWarnEnabled()) {                            }        }        if (status == CrawlDatum.STATUS_FETCH_SUCCESS) {            if (parsing && !(skipTruncated && ParseSegment.isTruncated(content))) {                try {                    parseResult = this.parseUtil.parse(content);                } catch (Exception e) {                                    }            }            if (parseResult == null && (parsing || signatureWithoutParsing)) {                byte[] signature = SignatureFactory.getSignature(conf).calculate(content, new ParseStatus().getEmptyParse(conf));                datum.setSignature(signature);            }        }        /*       * Store status code in content So we can read this value during parsing       * (as a separate job) and decide to parse or not.       */        content.getMetadata().add(Nutch.FETCH_STATUS_KEY, Integer.toString(status));    }    try {        context.write(key, new NutchWritable(datum));        if (content != null && storingContent)            context.write(key, new NutchWritable(content));        if (parseResult != null) {            for (Entry<Text, Parse> entry : parseResult) {                Text url = entry.getKey();                Parse parse = entry.getValue();                ParseStatus parseStatus = parse.getData().getStatus();                ParseData parseData = parse.getData();                if (!parseStatus.isSuccess()) {                                        parse = parseStatus.getEmptyParse(conf);                }                                                byte[] signature = SignatureFactory.getSignature(conf).calculate(content, parse);                                parseData.getContentMeta().set(Nutch.SEGMENT_NAME_KEY, segmentName);                parseData.getContentMeta().set(Nutch.SIGNATURE_KEY, StringUtil.toHexString(signature));                                parseData.getContentMeta().set(Nutch.FETCH_TIME_KEY, Long.toString(datum.getFetchTime()));                if (url.equals(key))                    datum.setSignature(signature);                try {                    scfilters.passScoreAfterParsing(url, content, parse);                } catch (Exception e) {                    if (LOG.isWarnEnabled()) {                                            }                }                String origin = null;                                Outlink[] links = parseData.getOutlinks();                int outlinksToStore = Math.min(maxOutlinks, links.length);                if (ignoreExternalLinks || ignoreInternalLinks) {                    URL originURL = new URL(url.toString());                                        if ("bydomain".equalsIgnoreCase(ignoreExternalLinksMode)) {                        origin = URLUtil.getDomainName(originURL).toLowerCase();                    } else                     {                        origin = originURL.getHost().toLowerCase();                    }                }                                if (fetchNode != null) {                    fetchNode.setOutlinks(links);                    fetchNode.setTitle(parseData.getTitle());                    FetchNodeDb.getInstance().put(fetchNode.getUrl().toString(), fetchNode);                }                int validCount = 0;                                List<Outlink> outlinkList = new ArrayList<>(outlinksToStore);                HashSet<String> outlinks = new HashSet<>(outlinksToStore);                for (int i = 0; i < links.length && validCount < outlinksToStore; i++) {                    String toUrl = links[i].getToUrl();                    if (toUrl.length() > maxOutlinkLength) {                        continue;                    }                    toUrl = ParseOutputFormat.filterNormalize(url.toString(), toUrl, origin, ignoreInternalLinks, ignoreExternalLinks, ignoreExternalLinksMode, urlFiltersForOutlinks, urlExemptionFilters, normalizersForOutlinks);                    if (toUrl == null) {                        continue;                    }                    validCount++;                    links[i].setUrl(toUrl);                    outlinkList.add(links[i]);                    outlinks.add(toUrl);                }                                if (activatePublisher) {                    FetcherThreadEvent reportEvent = new FetcherThreadEvent(PublishEventType.REPORT, url.toString());                    reportEvent.addOutlinksToEventData(outlinkList);                    reportEvent.addEventData(Nutch.FETCH_EVENT_TITLE, parseData.getTitle());                    reportEvent.addEventData(Nutch.FETCH_EVENT_CONTENTTYPE, parseData.getContentMeta().get("content-type"));                    reportEvent.addEventData(Nutch.FETCH_EVENT_SCORE, datum.getScore());                    reportEvent.addEventData(Nutch.FETCH_EVENT_FETCHTIME, datum.getFetchTime());                    reportEvent.addEventData(Nutch.FETCH_EVENT_CONTENTLANG, parseData.getContentMeta().get("content-language"));                    publisher.publish(reportEvent, conf);                }                                if (maxOutlinkDepth > 0 && outlinkDepth < maxOutlinkDepth) {                    FetchItem ft = FetchItem.create(url, null, queueMode);                    FetchItemQueue queue = ((FetchItemQueues) fetchQueues).getFetchItemQueue(ft.queueID);                    queue.alreadyFetched.add(url.toString().hashCode());                    context.getCounter("FetcherOutlinks", "outlinks_detected").increment(outlinks.size());                                        int outlinkCounter = 0;                    String followUrl;                                        Iterator<String> iter = outlinks.iterator();                    while (iter.hasNext() && outlinkCounter < maxOutlinkDepthNumLinks) {                        followUrl = iter.next();                                                if (outlinksIgnoreExternal) {                            if (!URLUtil.getHost(url.toString()).equals(URLUtil.getHost(followUrl))) {                                continue;                            }                        }                                                int urlHashCode = followUrl.hashCode();                        if (queue.alreadyFetched.contains(urlHashCode)) {                            continue;                        }                        queue.alreadyFetched.add(urlHashCode);                                                FetchItem fit = FetchItem.create(new Text(followUrl), new CrawlDatum(CrawlDatum.STATUS_LINKED, interval), queueMode, outlinkDepth + 1);                        context.getCounter("FetcherOutlinks", "outlinks_following").increment(1);                        ((FetchItemQueues) fetchQueues).addFetchItem(fit);                        outlinkCounter++;                    }                }                                                parseData.setOutlinks(outlinkList.toArray(new Outlink[outlinkList.size()]));                context.write(url, new NutchWritable(new ParseImpl(new ParseText(parse.getText()), parseData, parse.isCanonical())));            }        }    } catch (IOException e) {        if (LOG.isErrorEnabled()) {                    }    }        if (parseResult != null && !parseResult.isEmpty()) {        Parse p = parseResult.get(content.getUrl());        if (p != null) {            context.getCounter("ParserStatus", ParseStatus.majorCodes[p.getData().getStatus().getMajorCode()]).increment(1);            return p.getData().getStatus();        }    }    return null;}
1
private void outputRobotsTxt(List<Content> robotsTxtContent) throws InterruptedException
{    for (Content robotsTxt : robotsTxtContent) {                try {            context.write(new Text(robotsTxt.getUrl()), new NutchWritable(robotsTxt));        } catch (IOException e) {                    }    }}
1
private void updateStatus(int bytesInPage) throws IOException
{    pages.incrementAndGet();    bytes.addAndGet(bytesInPage);}
0
public synchronized void setHalted(boolean halted)
{    this.halted = halted;}
0
public synchronized boolean isHalted()
{    return halted;}
0
public String getReprUrl()
{    return reprUrl;}
0
private void setReprUrl(String urlString)
{    this.reprUrl = urlString;}
0
public PublishEventType getEventType()
{    return eventType;}
0
public void setEventType(PublishEventType eventType)
{    this.eventType = eventType;}
0
public Map<String, Object> getEventData()
{    return eventData;}
0
public void setEventData(Map<String, Object> eventData)
{    this.eventData = eventData;}
0
public String getUrl()
{    return url;}
0
public void setUrl(String url)
{    this.url = url;}
0
public void addEventData(String key, Object value)
{    if (eventData == null) {        eventData = new HashMap<>();    }    eventData.put(key, value);}
0
public void addOutlinksToEventData(Collection<Outlink> links)
{    ArrayList<Map<String, String>> outlinkList = new ArrayList<>();    for (Outlink link : links) {        Map<String, String> outlink = new HashMap<>();        outlink.put("url", link.getToUrl());        outlink.put("anchor", link.getAnchor());        outlinkList.add(outlink);    }    this.addEventData("outlinks", outlinkList);}
0
public Long getTimestamp()
{    return timestamp;}
0
public void setTimestamp(Long timestamp)
{    this.timestamp = timestamp;}
0
public void publish(FetcherThreadEvent event, Configuration conf)
{    if (publisher != null) {        publisher.publish(event, conf);    } else {            }}
1
public static FetchItem create(Text url, CrawlDatum datum, String queueMode)
{    return create(url, datum, queueMode, 0);}
0
public static FetchItem create(Text url, CrawlDatum datum, String queueMode, int outlinkDepth)
{    URL u = null;    try {        u = new URL(url.toString());    } catch (Exception e) {                return null;    }    String key;    if (FetchItemQueues.QUEUE_MODE_IP.equalsIgnoreCase(queueMode)) {        try {            final InetAddress addr = InetAddress.getByName(u.getHost());            key = addr.getHostAddress();        } catch (final UnknownHostException e) {                                    return null;        }    } else if (FetchItemQueues.QUEUE_MODE_DOMAIN.equalsIgnoreCase(queueMode)) {        key = URLUtil.getDomainName(u).toLowerCase(Locale.ROOT);        if (key == null) {                        key = u.toExternalForm();        }    } else {        key = u.getHost().toLowerCase(Locale.ROOT);        if (key == null) {                        key = u.toExternalForm();        }    }    return new FetchItem(url, u, datum, key, outlinkDepth);}
1
public CrawlDatum getDatum()
{    return datum;}
0
public String getQueueID()
{    return queueID;}
0
public Text getUrl()
{    return url;}
0
public URL getURL2()
{    return u;}
0
public synchronized int emptyQueue()
{    int presize = queue.size();    queue.clear();    return presize;}
0
public int getQueueSize()
{    return queue.size();}
0
public int getInProgressSize()
{    return inProgress.get();}
0
public int incrementExceptionCounter()
{    return exceptionCounter.incrementAndGet();}
0
public void finishFetchItem(FetchItem it, boolean asap)
{    if (it != null) {        inProgress.decrementAndGet();        setEndTime(System.currentTimeMillis(), asap);    }}
0
public void addFetchItem(FetchItem it)
{    if (it == null)        return;        if (it.datum.getMetaData().containsKey(variableFetchDelayKey)) {        if (!variableFetchDelaySet) {            variableFetchDelaySet = true;            crawlDelay = ((LongWritable) (it.datum.getMetaData().get(variableFetchDelayKey))).get();            minCrawlDelay = ((LongWritable) (it.datum.getMetaData().get(variableFetchDelayKey))).get();            setEndTime(System.currentTimeMillis() - crawlDelay);        }                it.datum.getMetaData().remove(variableFetchDelayKey);    }    queue.add(it);}
0
public void addInProgressFetchItem(FetchItem it)
{    if (it == null)        return;    inProgress.incrementAndGet();}
0
public FetchItem getFetchItem()
{    if (inProgress.get() >= maxThreads)        return null;    long now = System.currentTimeMillis();    if (nextFetchTime.get() > now)        return null;    FetchItem it = null;    if (queue.size() == 0)        return null;    try {        it = queue.remove(0);        inProgress.incrementAndGet();    } catch (Exception e) {            }    return it;}
1
public void setCookie(Text cookie)
{    this.cookie = cookie;}
0
public Text getCookie()
{    return cookie;}
0
public synchronized void dump()
{                            for (int i = 0; i < queue.size(); i++) {        FetchItem it = queue.get(i);            }}
1
private void setEndTime(long endTime)
{    setEndTime(endTime, false);}
0
private void setEndTime(long endTime, boolean asap)
{    if (!asap)        nextFetchTime.set(endTime + (maxThreads > 1 ? minCrawlDelay : crawlDelay));    else        nextFetchTime.set(endTime);}
0
protected static String checkQueueMode(String queueMode)
{        if (!queueMode.equals(QUEUE_MODE_IP) && !queueMode.equals(QUEUE_MODE_DOMAIN) && !queueMode.equals(QUEUE_MODE_HOST)) {                queueMode = QUEUE_MODE_HOST;    }    return queueMode;}
1
public int getTotalSize()
{    return totalSize.get();}
0
public int getQueueCount()
{    return queues.size();}
0
public void addFetchItem(Text url, CrawlDatum datum)
{    FetchItem it = FetchItem.create(url, datum, queueMode);    if (it != null)        addFetchItem(it);}
0
public synchronized void addFetchItem(FetchItem it)
{    FetchItemQueue fiq = getFetchItemQueue(it.queueID);    fiq.addFetchItem(it);    totalSize.incrementAndGet();}
0
public void finishFetchItem(FetchItem it)
{    finishFetchItem(it, false);}
0
public void finishFetchItem(FetchItem it, boolean asap)
{    FetchItemQueue fiq = queues.get(it.queueID);    if (fiq == null) {                return;    }    fiq.finishFetchItem(it, asap);}
1
public synchronized FetchItemQueue getFetchItemQueue(String id)
{    FetchItemQueue fiq = queues.get(id);    if (fiq == null) {                fiq = new FetchItemQueue(conf, maxThreads, crawlDelay, minCrawlDelay);        queues.put(id, fiq);    }    return fiq;}
0
public synchronized FetchItem getFetchItem()
{    Iterator<Map.Entry<String, FetchItemQueue>> it = queues.entrySet().iterator();    while (it.hasNext()) {        FetchItemQueue fiq = it.next().getValue();                if (fiq.getQueueSize() == 0 && fiq.getInProgressSize() == 0) {            it.remove();            continue;        }        FetchItem fit = fiq.getFetchItem();        if (fit != null) {            totalSize.decrementAndGet();            return fit;        }    }    return null;}
0
public synchronized int checkTimelimit()
{    int count = 0;    if (System.currentTimeMillis() >= timelimit && timelimit != -1) {                count = emptyQueues();                if (totalSize.get() != 0 && queues.size() == 0)            totalSize.set(0);    }    return count;}
0
public synchronized int emptyQueues()
{    int count = 0;    for (String id : queues.keySet()) {        FetchItemQueue fiq = queues.get(id);        if (fiq.getQueueSize() == 0)            continue;                int deleted = fiq.emptyQueue();        for (int i = 0; i < deleted; i++) {            totalSize.decrementAndGet();        }        count += deleted;    }    return count;}
1
public synchronized int checkExceptionThreshold(String queueid)
{    FetchItemQueue fiq = queues.get(queueid);    if (fiq == null) {        return 0;    }    if (fiq.getQueueSize() == 0) {        return 0;    }    int excCount = fiq.incrementExceptionCounter();    if (maxExceptionsPerQueue != -1 && excCount >= maxExceptionsPerQueue) {                int deleted = fiq.emptyQueue();                for (int i = 0; i < deleted; i++) {            totalSize.decrementAndGet();        }        return deleted;    }    return 0;}
1
public synchronized void dump()
{    for (String id : queues.keySet()) {        FetchItemQueue fiq = queues.get(id);        if (fiq.getQueueSize() == 0)            continue;                fiq.dump();    }}
1
public Text getUrl()
{    return url;}
0
public void setUrl(Text url)
{    this.url = url;}
0
public Outlink[] getOutlinks()
{    return outlinks;}
0
public void setOutlinks(Outlink[] links)
{    this.outlinks = links;}
0
public int getStatus()
{    return status;}
0
public void setStatus(int status)
{    this.status = status;}
0
public String getTitle()
{    return title;}
0
public void setTitle(String title)
{    this.title = title;}
0
public long getFetchTime()
{    return fetchTime;}
0
public void setFetchTime(long fetchTime)
{    this.fetchTime = fetchTime;}
0
public static FetchNodeDb getInstance()
{    if (fetchNodeDbInstance == null) {        fetchNodeDbInstance = new FetchNodeDb();    }    return fetchNodeDbInstance;}
0
public void put(String url, FetchNode fetchNode)
{    System.out.println("FetchNodeDb : putting node - " + fetchNode.hashCode());    fetchNodeDbMap.put(index++, fetchNode);}
0
public Map<Integer, FetchNode> getFetchNodeDb()
{    return fetchNodeDbMap;}
0
public void setTimeLimit(long tl)
{    timelimit = tl;}
0
private String filterNormalize(String url)
{    if (url != null) {        try {            if (urlNormalizers != null)                                url = urlNormalizers.normalize(url, urlNormalizerScope);            if (urlFilters != null)                url = urlFilters.filter(url);        } catch (MalformedURLException | URLFilterException e) {                        url = null;        }    }    return url;}
1
public void run()
{    boolean hasMore = true;    int cnt = 0;    int timelimitcount = 0;    while (hasMore) {        if (System.currentTimeMillis() >= timelimit && timelimit != -1) {                        try {                hasMore = context.nextKeyValue();                timelimitcount++;            } catch (IOException e) {                                return;            } catch (InterruptedException e) {                                return;            }            continue;        }        int feed = size - queues.getTotalSize();        if (feed <= 0) {                        try {                Thread.sleep(1000);            } catch (InterruptedException e) {            }            continue;        }                while (feed > 0 && hasMore) {            try {                hasMore = context.nextKeyValue();                if (hasMore) {                    Text url = context.getCurrentKey();                    if (urlFilters != null || urlNormalizers != null) {                        String u = filterNormalize(url.toString());                        if (u == null) {                                                        context.getCounter("FetcherStatus", "filtered").increment(1);                            continue;                        }                        url = new Text(u);                    } else /*             * Need to copy key and value objects because MapReduce will reuse             * the original objects while the objects are stored in the queue.             */                    {                        url = new Text(url);                    }                    CrawlDatum datum = new CrawlDatum();                    datum.set((CrawlDatum) context.getCurrentValue());                    queues.addFetchItem(url, datum);                    cnt++;                    feed--;                }            } catch (IOException e) {                                return;            } catch (InterruptedException e) {                            }        }    }    }
1
public void resetFailures()
{    setDnsFailures(0l);    setConnectionFailures(0l);}
0
public void setDnsFailures(Long dnsFailures)
{    this.dnsFailures = dnsFailures;}
0
public void setConnectionFailures(Long connectionFailures)
{    this.connectionFailures = connectionFailures;}
0
public void incDnsFailures()
{    this.dnsFailures++;}
0
public void incConnectionFailures()
{    this.connectionFailures++;}
0
public Long numFailures()
{    return getDnsFailures() + getConnectionFailures();}
0
public Long getDnsFailures()
{    return dnsFailures;}
0
public Long getConnectionFailures()
{    return connectionFailures;}
0
public void setScore(float score)
{    this.score = score;}
0
public void setLastCheck()
{    setLastCheck(new Date());}
0
public void setLastCheck(Date date)
{    lastCheck = date;}
0
public boolean isEmpty()
{    return (lastCheck.getTime() == 0) ? true : false;}
0
public float getScore()
{    return score;}
0
public Long numRecords()
{    return unfetched + fetched + gone + redirPerm + redirTemp + notModified;}
0
public Date getLastCheck()
{    return lastCheck;}
0
public boolean hasHomepageUrl()
{    return homepageUrl.length() > 0;}
0
public String getHomepageUrl()
{    return homepageUrl;}
0
public void setHomepageUrl(String homepageUrl)
{    this.homepageUrl = homepageUrl;}
0
public void setUnfetched(long val)
{    unfetched = val;}
0
public long getUnfetched()
{    return unfetched;}
0
public void setFetched(long val)
{    fetched = val;}
0
public long getFetched()
{    return fetched;}
0
public void setNotModified(long val)
{    notModified = val;}
0
public long getNotModified()
{    return notModified;}
0
public void setRedirTemp(long val)
{    redirTemp = val;}
0
public long getRedirTemp()
{    return redirTemp;}
0
public void setRedirPerm(long val)
{    redirPerm = val;}
0
public long getRedirPerm()
{    return redirPerm;}
0
public void setGone(long val)
{    gone = val;}
0
public long getGone()
{    return gone;}
0
public void resetStatistics()
{    setUnfetched(0);    setFetched(0);    setGone(0);    setRedirTemp(0);    setRedirPerm(0);    setNotModified(0);}
0
public void setMetaData(org.apache.hadoop.io.MapWritable mapWritable)
{    this.metaData = new org.apache.hadoop.io.MapWritable(mapWritable);}
0
public void putAllMetaData(HostDatum other)
{    for (Entry<Writable, Writable> e : other.getMetaData().entrySet()) {        getMetaData().put(e.getKey(), e.getValue());    }}
0
public org.apache.hadoop.io.MapWritable getMetaData()
{    if (this.metaData == null)        this.metaData = new org.apache.hadoop.io.MapWritable();    return this.metaData;}
0
public Object clone() throws CloneNotSupportedException
{    HostDatum result = (HostDatum) super.clone();    result.score = score;    result.lastCheck = lastCheck;    result.homepageUrl = homepageUrl;    result.dnsFailures = dnsFailures;    result.connectionFailures = connectionFailures;    result.unfetched = unfetched;    result.fetched = fetched;    result.notModified = notModified;    result.redirTemp = redirTemp;    result.redirPerm = redirPerm;    result.gone = gone;    result.metaData = metaData;    return result;}
0
public void readFields(DataInput in) throws IOException
{    score = in.readFloat();    lastCheck = new Date(in.readLong());    homepageUrl = Text.readString(in);    dnsFailures = in.readLong();    connectionFailures = in.readLong();    unfetched = in.readLong();    fetched = in.readLong();    notModified = in.readLong();    redirTemp = in.readLong();    redirPerm = in.readLong();    gone = in.readLong();    metaData = new org.apache.hadoop.io.MapWritable();    metaData.readFields(in);}
0
public void write(DataOutput out) throws IOException
{    out.writeFloat(score);    out.writeLong(lastCheck.getTime());    Text.writeString(out, homepageUrl);    out.writeLong(dnsFailures);    out.writeLong(connectionFailures);    out.writeLong(unfetched);    out.writeLong(fetched);    out.writeLong(notModified);    out.writeLong(redirTemp);    out.writeLong(redirPerm);    out.writeLong(gone);    metaData.write(out);}
0
public String toString()
{    StringBuilder buf = new StringBuilder();    buf.append(Long.toString(getUnfetched()));    buf.append("\t");    buf.append(Long.toString(getFetched()));    buf.append("\t");    buf.append(Long.toString(getGone()));    buf.append("\t");    buf.append(Long.toString(getRedirTemp()));    buf.append("\t");    buf.append(Long.toString(getRedirPerm()));    buf.append("\t");    buf.append(Long.toString(getNotModified()));    buf.append("\t");    buf.append(Long.toString(numRecords()));    buf.append("\t");    buf.append(Long.toString(getDnsFailures()));    buf.append("\t");    buf.append(Long.toString(getConnectionFailures()));    buf.append("\t");    buf.append(Long.toString(numFailures()));    buf.append("\t");    buf.append(Float.toString(score));    buf.append("\t");    buf.append(new SimpleDateFormat("yyyy-MM-dd HH:mm:ss").format(lastCheck));    buf.append("\t");    buf.append(homepageUrl);    buf.append("\t");    for (Entry<Writable, Writable> e : getMetaData().entrySet()) {        buf.append(e.getKey().toString());        buf.append(':');        buf.append(e.getValue().toString());        buf.append("|||");    }    return buf.toString();}
0
public void setup(Context context)
{    dumpHomepages = context.getConfiguration().getBoolean(HOSTDB_DUMP_HOMEPAGES, false);    dumpHostnames = context.getConfiguration().getBoolean(HOSTDB_DUMP_HOSTNAMES, false);    fieldHeader = context.getConfiguration().getBoolean(HOSTDB_DUMP_HEADER, true);    String expr = context.getConfiguration().get(HOSTDB_FILTER_EXPRESSION);    if (expr != null) {                JexlEngine jexl = new JexlEngine();                jexl.setSilent(true);        jexl.setStrict(true);                this.expr = jexl.createExpression(expr);    }}
0
public void map(Text key, HostDatum datum, Context context) throws IOException, InterruptedException
{    if (fieldHeader && !dumpHomepages && !dumpHostnames) {        context.write(new Text("hostname"), new Text("unfetched\tfetched\tgone\tredirTemp\tredirPerm\tnotModified\tnumRecords\tdnsFail\tcnxFail\tsumFail\tscore\tlastCheck\thomepage\tmetadata"));        fieldHeader = false;    }    if (expr != null) {                JexlContext jcontext = new MapContext();                jcontext.set("unfetched", datum.getUnfetched());        jcontext.set("fetched", datum.getFetched());        jcontext.set("gone", datum.getGone());        jcontext.set("redirTemp", datum.getRedirTemp());        jcontext.set("redirPerm", datum.getRedirPerm());        jcontext.set("redirs", datum.getRedirPerm() + datum.getRedirTemp());        jcontext.set("notModified", datum.getNotModified());        jcontext.set("ok", datum.getFetched() + datum.getNotModified());        jcontext.set("numRecords", datum.numRecords());        jcontext.set("dnsFailures", datum.getDnsFailures());        jcontext.set("connectionFailures", datum.getConnectionFailures());                for (Map.Entry<Writable, Writable> entry : datum.getMetaData().entrySet()) {            Object value = entry.getValue();            if (value instanceof FloatWritable) {                FloatWritable fvalue = (FloatWritable) value;                Text tkey = (Text) entry.getKey();                jcontext.set(tkey.toString(), fvalue.get());            }            if (value instanceof IntWritable) {                IntWritable ivalue = (IntWritable) value;                Text tkey = (Text) entry.getKey();                jcontext.set(tkey.toString(), ivalue.get());            }        }                try {            if (!Boolean.TRUE.equals(expr.evaluate(jcontext))) {                return;            }        } catch (Exception e) {                    }    }    if (dumpHomepages) {        if (datum.hasHomepageUrl()) {            context.write(new Text(datum.getHomepageUrl()), emptyText);        }        return;    }    if (dumpHostnames) {        context.write(key, emptyText);        return;    }        context.write(key, new Text(datum.toString()));}
1
private void readHostDb(Path hostDb, Path output, boolean dumpHomepages, boolean dumpHostnames, String expr) throws Exception
{    SimpleDateFormat sdf = new SimpleDateFormat("yyyy-MM-dd HH:mm:ss");    long start = System.currentTimeMillis();        Configuration conf = getConf();    conf.setBoolean(HOSTDB_DUMP_HOMEPAGES, dumpHomepages);    conf.setBoolean(HOSTDB_DUMP_HOSTNAMES, dumpHostnames);    if (expr != null) {        conf.set(HOSTDB_FILTER_EXPRESSION, expr);    }    conf.setBoolean("mapreduce.fileoutputcommitter.marksuccessfuljobs", false);    conf.set("mapreduce.output.textoutputformat.separator", "\t");    Job job = Job.getInstance(conf);    job.setJobName("ReadHostDb");    job.setJarByClass(ReadHostDb.class);    FileInputFormat.addInputPath(job, new Path(hostDb, "current"));    FileOutputFormat.setOutputPath(job, output);    job.setMapperClass(ReadHostDbMapper.class);    job.setInputFormatClass(SequenceFileInputFormat.class);    job.setOutputFormatClass(TextOutputFormat.class);    job.setMapOutputKeyClass(Text.class);    job.setMapOutputValueClass(Text.class);    job.setOutputKeyClass(Text.class);    job.setOutputValueClass(Text.class);    job.setNumReduceTasks(0);    try {        boolean success = job.waitForCompletion(true);        if (!success) {            String message = "ReadHostDb job did not succeed, job status: " + job.getStatus().getState() + ", reason: " + job.getStatus().getFailureInfo();                                    throw new RuntimeException(message);        }    } catch (IOException | InterruptedException | ClassNotFoundException e) {                throw e;    }    long end = System.currentTimeMillis();    }
1
private void getHostDbRecord(Path hostDb, String host) throws Exception
{    Configuration conf = getConf();    SequenceFile.Reader[] readers = SegmentReaderUtil.getReaders(hostDb, conf);    Class<?> keyClass = readers[0].getKeyClass();    Class<?> valueClass = readers[0].getValueClass();    if (!keyClass.getName().equals("org.apache.hadoop.io.Text"))        throw new IOException("Incompatible key (" + keyClass.getName() + ")");    Text key = (Text) keyClass.getConstructor().newInstance();    HostDatum value = (HostDatum) valueClass.getConstructor().newInstance();    for (int i = 0; i < readers.length; i++) {        while (readers[i].next(key, value)) {            if (host.equals(key.toString())) {                System.out.println(value.toString());            }        }        readers[i].close();    }}
0
public static void main(String[] args) throws Exception
{    int res = ToolRunner.run(NutchConfiguration.create(), new ReadHostDb(), args);    System.exit(res);}
0
public int run(String[] args) throws Exception
{    if (args.length < 2) {        System.err.println("Usage: ReadHostDb <hostdb> [-get <url>] [<output> [-dumpHomepages | -dumpHostnames | -expr <expr.>]]");        return -1;    }    boolean dumpHomepages = false;    boolean dumpHostnames = false;    String expr = null;    String get = null;    for (int i = 0; i < args.length; i++) {        if (args[i].equals("-dumpHomepages")) {                        dumpHomepages = true;        }        if (args[i].equals("-dumpHostnames")) {                        dumpHostnames = true;        }        if (args[i].equals("-get")) {            get = args[i + 1];                        i++;        }        if (args[i].equals("-expr")) {            expr = args[i + 1];                        i++;        }    }    try {        if (get != null) {            getHostDbRecord(new Path(args[0], "current"), get);        } else {            readHostDb(new Path(args[0]), new Path(args[1]), dumpHomepages, dumpHostnames, expr);        }        return 0;    } catch (Exception e) {                return -1;    }}
1
public void run()
{        try {                @SuppressWarnings("unused")        InetAddress inetAddr = InetAddress.getByName(host);        if (datum.isEmpty()) {            context.getCounter("UpdateHostDb", "new_known_host").increment(1);            datum.setLastCheck();                    } else if (datum.getDnsFailures() > 0) {            context.getCounter("UpdateHostDb", "rediscovered_host").increment(1);            datum.setLastCheck();            datum.setDnsFailures(0l);                    } else {            context.getCounter("UpdateHostDb", "existing_known_host").increment(1);            datum.setLastCheck();                    }                context.write(hostText, datum);    } catch (UnknownHostException e) {        try {                        if (datum.isEmpty()) {                datum.setLastCheck();                datum.setDnsFailures(1l);                context.write(hostText, datum);                context.getCounter("UpdateHostDb", "new_unknown_host").increment(1);                            } else {                datum.setLastCheck();                datum.incDnsFailures();                                if (purgeFailedHostsThreshold == -1 || purgeFailedHostsThreshold < datum.getDnsFailures()) {                    context.write(hostText, datum);                    context.getCounter("UpdateHostDb", "existing_unknown_host").increment(1);                                    } else {                    context.getCounter("UpdateHostDb", "purged_unknown_host").increment(1);                                    }            }            context.getCounter("UpdateHostDb", Long.toString(datum.numFailures()) + "_times_failed").increment(1);        } catch (Exception ioe) {                    }    } catch (Exception e) {            }    context.getCounter("UpdateHostDb", "checked_hosts").increment(1);}
1
private void updateHostDb(Path hostDb, Path crawlDb, Path topHosts, boolean checkFailed, boolean checkNew, boolean checkKnown, boolean force, boolean filter, boolean normalize) throws Exception
{    SimpleDateFormat sdf = new SimpleDateFormat("yyyy-MM-dd HH:mm:ss");    long start = System.currentTimeMillis();        Job job = NutchJob.getInstance(getConf());    Configuration conf = job.getConfiguration();    boolean preserveBackup = conf.getBoolean("db.preserve.backup", true);    job.setJarByClass(UpdateHostDb.class);    job.setJobName("UpdateHostDb");    FileSystem fs = hostDb.getFileSystem(conf);    Path old = new Path(hostDb, "old");    Path current = new Path(hostDb, "current");    Path tempHostDb = new Path(hostDb, "hostdb-" + Integer.toString(new Random().nextInt(Integer.MAX_VALUE)));        Path lock = new Path(hostDb, LOCK_NAME);    if (!fs.exists(current)) {        fs.mkdirs(current);    }    LockUtil.createLockFile(fs, lock, false);    MultipleInputs.addInputPath(job, current, SequenceFileInputFormat.class);    if (topHosts != null) {        MultipleInputs.addInputPath(job, topHosts, KeyValueTextInputFormat.class);    }    if (crawlDb != null) {                conf.setBoolean("hostdb.reading.crawldb", true);        MultipleInputs.addInputPath(job, new Path(crawlDb, CrawlDb.CURRENT_NAME), SequenceFileInputFormat.class);    }    FileOutputFormat.setOutputPath(job, tempHostDb);    job.setOutputFormatClass(SequenceFileOutputFormat.class);    job.setMapOutputKeyClass(Text.class);    job.setMapOutputValueClass(NutchWritable.class);    job.setOutputKeyClass(Text.class);    job.setOutputValueClass(HostDatum.class);    job.setMapperClass(UpdateHostDbMapper.class);    job.setReducerClass(UpdateHostDbReducer.class);    job.setSpeculativeExecution(false);    conf.setBoolean("mapreduce.fileoutputcommitter.marksuccessfuljobs", false);    conf.setBoolean(HOSTDB_CHECK_FAILED, checkFailed);    conf.setBoolean(HOSTDB_CHECK_NEW, checkNew);    conf.setBoolean(HOSTDB_CHECK_KNOWN, checkKnown);    conf.setBoolean(HOSTDB_FORCE_CHECK, force);    conf.setBoolean(HOSTDB_URL_FILTERING, filter);    conf.setBoolean(HOSTDB_URL_NORMALIZING, normalize);    conf.setClassLoader(Thread.currentThread().getContextClassLoader());    try {        boolean success = job.waitForCompletion(true);        if (!success) {            String message = "UpdateHostDb job did not succeed, job status:" + job.getStatus().getState() + ", reason: " + job.getStatus().getFailureInfo();                        NutchJob.cleanupAfterFailure(tempHostDb, lock, fs);            throw new RuntimeException(message);        }        FSUtils.replace(fs, old, current, true);        FSUtils.replace(fs, current, tempHostDb, true);        if (!preserveBackup && fs.exists(old))            fs.delete(old, true);    } catch (Exception e) {                NutchJob.cleanupAfterFailure(tempHostDb, lock, fs);        throw e;    }    LockUtil.removeLockFile(fs, lock);    long end = System.currentTimeMillis();    }
1
public static void main(String[] args) throws Exception
{    int res = ToolRunner.run(NutchConfiguration.create(), new UpdateHostDb(), args);    System.exit(res);}
0
public int run(String[] args) throws Exception
{    if (args.length < 2) {        System.err.println("Usage: UpdateHostDb -hostdb <hostdb> " + "[-tophosts <tophosts>] [-crawldb <crawldb>] [-checkAll] [-checkFailed]" + " [-checkNew] [-checkKnown] [-force] [-filter] [-normalize]");        return -1;    }    Path hostDb = null;    Path crawlDb = null;    Path topHosts = null;    boolean checkFailed = false;    boolean checkNew = false;    boolean checkKnown = false;    boolean force = false;    boolean filter = false;    boolean normalize = false;    for (int i = 0; i < args.length; i++) {        if (args[i].equals("-hostdb")) {            hostDb = new Path(args[i + 1]);                        i++;        }        if (args[i].equals("-crawldb")) {            crawlDb = new Path(args[i + 1]);                        i++;        }        if (args[i].equals("-tophosts")) {            topHosts = new Path(args[i + 1]);                        i++;        }        if (args[i].equals("-checkFailed")) {                        checkFailed = true;        }        if (args[i].equals("-checkNew")) {                        checkNew = true;        }        if (args[i].equals("-checkKnown")) {                        checkKnown = true;        }        if (args[i].equals("-checkAll")) {                        checkFailed = true;            checkNew = true;            checkKnown = true;        }        if (args[i].equals("-force")) {                        force = true;        }        if (args[i].equals("-filter")) {                        filter = true;        }        if (args[i].equals("-normalize")) {                        normalize = true;        }    }    if (hostDb == null) {        System.err.println("hostDb is mandatory");        return -1;    }    try {        updateHostDb(hostDb, crawlDb, topHosts, checkFailed, checkNew, checkKnown, force, filter, normalize);        return 0;    } catch (Exception e) {                return -1;    }}
1
public void setup(Mapper<Text, Writable, Text, NutchWritable>.Context context)
{    Configuration conf = context.getConfiguration();    readingCrawlDb = conf.getBoolean("hostdb.reading.crawldb", false);    filter = conf.getBoolean(UpdateHostDb.HOSTDB_URL_FILTERING, false);    normalize = conf.getBoolean(UpdateHostDb.HOSTDB_URL_NORMALIZING, false);    if (filter)        filters = new URLFilters(conf);    if (normalize)        normalizers = new URLNormalizers(conf, URLNormalizers.SCOPE_DEFAULT);}
0
protected String filterNormalize(String url)
{                url = "http://" + url + "/";    try {        if (normalize)            url = normalizers.normalize(url, URLNormalizers.SCOPE_DEFAULT);        if (filter)            url = filters.filter(url);        if (url == null)            return null;    } catch (Exception e) {        return null;    }        return URLUtil.getHost(url);}
0
public void map(Text key, Writable value, Context context) throws IOException, InterruptedException
{        String keyStr = key.toString();        if (key instanceof Text && value instanceof CrawlDatum) {                buffer = filterNormalize(URLUtil.getHost(keyStr));                if (buffer == null) {            context.getCounter("UpdateHostDb", "filtered_records").increment(1);                        return;        }                host.set(buffer);        crawlDatum = (CrawlDatum) value;        hostDatum = new HostDatum();                if (crawlDatum.getStatus() != CrawlDatum.STATUS_DB_UNFETCHED) {                        String protocol = URLUtil.getProtocol(keyStr);                        String homepage = protocol + "://" + buffer + "/";                        if (keyStr.equals(homepage)) {                                if (crawlDatum.getStatus() == CrawlDatum.STATUS_DB_REDIR_PERM || crawlDatum.getStatus() == CrawlDatum.STATUS_DB_REDIR_TEMP) {                                        ProtocolStatus z = (ProtocolStatus) crawlDatum.getMetaData().get(Nutch.WRITABLE_PROTO_STATUS_KEY);                                        args = z.getArgs();                                        reprUrl = args[0];                                        if (reprUrl != null) {                                                context.write(host, new NutchWritable(hostDatum));                        hostDatum.setHomepageUrl(reprUrl);                    } else {                                            }                } else {                    hostDatum.setHomepageUrl(homepage);                    context.write(host, new NutchWritable(hostDatum));                                    }            }        }                context.write(host, new NutchWritable(crawlDatum));    }        if (key instanceof Text && value instanceof HostDatum) {        buffer = filterNormalize(keyStr);                if (buffer == null) {            context.getCounter("UpdateHostDb", "filtered_records").increment(1);                        return;        }                hostDatum = (HostDatum) value;        key.set(buffer);                if (readingCrawlDb) {            hostDatum.resetStatistics();        }        context.write(key, new NutchWritable(hostDatum));    }        if (key instanceof Text && value instanceof Text) {        buffer = filterNormalize(keyStr);                if (buffer == null) {            context.getCounter("UpdateHostDb", "filtered_records").increment(1);                        return;        }        key.set(buffer);        context.write(key, new NutchWritable(new FloatWritable(Float.parseFloat(value.toString()))));    }}
1
public void setup(Reducer<Text, NutchWritable, Text, HostDatum>.Context context)
{    Configuration conf = context.getConfiguration();    purgeFailedHostsThreshold = conf.getInt(UpdateHostDb.HOSTDB_PURGE_FAILED_HOSTS_THRESHOLD, -1);    numResolverThreads = conf.getInt(UpdateHostDb.HOSTDB_NUM_RESOLVER_THREADS, 10);    recheckInterval = conf.getInt(UpdateHostDb.HOSTDB_RECHECK_INTERVAL, 86400) * 1000;    checkFailed = conf.getBoolean(UpdateHostDb.HOSTDB_CHECK_FAILED, false);    checkNew = conf.getBoolean(UpdateHostDb.HOSTDB_CHECK_NEW, false);    checkKnown = conf.getBoolean(UpdateHostDb.HOSTDB_CHECK_KNOWN, false);    force = conf.getBoolean(UpdateHostDb.HOSTDB_FORCE_CHECK, false);    numericFields = conf.getStrings(UpdateHostDb.HOSTDB_NUMERIC_FIELDS);    stringFields = conf.getStrings(UpdateHostDb.HOSTDB_STRING_FIELDS);    percentiles = conf.getInts(UpdateHostDb.HOSTDB_PERCENTILES);        if (numericFields != null) {        numericFieldWritables = new Text[numericFields.length];        for (int i = 0; i < numericFields.length; i++) {            numericFieldWritables[i] = new Text(numericFields[i]);        }    }    if (stringFields != null) {        stringFieldWritables = new Text[stringFields.length];        for (int i = 0; i < stringFields.length; i++) {            stringFieldWritables[i] = new Text(stringFields[i]);        }    }        executor = new ThreadPoolExecutor(numResolverThreads, numResolverThreads, 5, TimeUnit.SECONDS, queue);        executor.prestartAllCoreThreads();}
0
public void reduce(Text key, Iterable<NutchWritable> values, Context context) throws IOException, InterruptedException
{    Map<String, Map<String, Long>> stringCounts = new HashMap<>();    Map<String, Float> maximums = new HashMap<>();        Map<String, Float> sums = new HashMap<>();        Map<String, Long> counts = new HashMap<>();    Map<String, Float> minimums = new HashMap<>();    Map<String, TDigest> tdigests = new HashMap<String, TDigest>();    HostDatum hostDatum = new HostDatum();    float score = 0;    if (stringFields != null) {        for (int i = 0; i < stringFields.length; i++) {            stringCounts.put(stringFields[i], new HashMap<>());        }    }        for (NutchWritable val : values) {                final Writable value = val.get();                if (value instanceof CrawlDatum) {            CrawlDatum buffer = (CrawlDatum) value;                        switch(buffer.getStatus()) {                case CrawlDatum.STATUS_DB_UNFETCHED:                    hostDatum.setUnfetched(hostDatum.getUnfetched() + 1l);                    break;                case CrawlDatum.STATUS_DB_FETCHED:                    hostDatum.setFetched(hostDatum.getFetched() + 1l);                    break;                case CrawlDatum.STATUS_DB_GONE:                    hostDatum.setGone(hostDatum.getGone() + 1l);                    break;                case CrawlDatum.STATUS_DB_REDIR_TEMP:                    hostDatum.setRedirTemp(hostDatum.getRedirTemp() + 1l);                    break;                case CrawlDatum.STATUS_DB_REDIR_PERM:                    hostDatum.setRedirPerm(hostDatum.getRedirPerm() + 1l);                    break;                case CrawlDatum.STATUS_DB_NOTMODIFIED:                    hostDatum.setNotModified(hostDatum.getNotModified() + 1l);                    break;            }                        if (buffer.getRetriesSinceFetch() != 0) {                hostDatum.incConnectionFailures();            }                        if (buffer.getStatus() == CrawlDatum.STATUS_DB_FETCHED || buffer.getStatus() == CrawlDatum.STATUS_DB_NOTMODIFIED) {                                if (stringFields != null) {                    for (int i = 0; i < stringFields.length; i++) {                                                if (buffer.getMetaData().get(stringFieldWritables[i]) != null) {                                                        String metadataValue = null;                            try {                                metadataValue = buffer.getMetaData().get(stringFieldWritables[i]).toString();                            } catch (Exception e) {                                                            }                                                        if (stringCounts.get(stringFields[i]).containsKey(metadataValue)) {                                                                stringCounts.get(stringFields[i]).put(metadataValue, stringCounts.get(stringFields[i]).get(metadataValue) + 1l);                            } else {                                                                stringCounts.get(stringFields[i]).put(metadataValue, 1l);                            }                        }                    }                }                                if (numericFields != null) {                    for (int i = 0; i < numericFields.length; i++) {                                                if (buffer.getMetaData().get(numericFieldWritables[i]) != null) {                            try {                                                                Float metadataValue = Float.parseFloat(buffer.getMetaData().get(numericFieldWritables[i]).toString());                                                                if (tdigests.containsKey(numericFields[i])) {                                    tdigests.get(numericFields[i]).add(metadataValue);                                } else {                                                                        TDigest tdigest = TDigest.createDigest(100);                                    tdigest.add((double) metadataValue);                                    tdigests.put(numericFields[i], tdigest);                                }                                                                if (minimums.containsKey(numericFields[i])) {                                                                        if (metadataValue < minimums.get(numericFields[i])) {                                        minimums.put(numericFields[i], metadataValue);                                    }                                } else {                                                                        minimums.put(numericFields[i], metadataValue);                                }                                                                if (maximums.containsKey(numericFields[i])) {                                                                        if (metadataValue > maximums.get(numericFields[i])) {                                        maximums.put(numericFields[i], metadataValue);                                    }                                } else {                                                                        maximums.put(numericFields[i], metadataValue);                                }                                                                if (sums.containsKey(numericFields[i])) {                                                                        sums.put(numericFields[i], sums.get(numericFields[i]) + metadataValue);                                    counts.put(numericFields[i], counts.get(numericFields[i]) + 1l);                                } else {                                                                        sums.put(numericFields[i], metadataValue);                                    counts.put(numericFields[i], 1l);                                }                            } catch (Exception e) {                                                            }                        }                    }                }            }        } else         if (value instanceof HostDatum) {            HostDatum buffer = (HostDatum) value;                        if (buffer.hasHomepageUrl()) {                hostDatum.setHomepageUrl(buffer.getHomepageUrl());            }                        if (!buffer.isEmpty()) {                hostDatum.setLastCheck(buffer.getLastCheck());            }                        if (buffer.getDnsFailures() > 0) {                hostDatum.setDnsFailures(buffer.getDnsFailures());            }                        if (buffer.getConnectionFailures() > 0) {                hostDatum.setConnectionFailures(buffer.getConnectionFailures());            }                        if (!buffer.getMetaData().isEmpty()) {                hostDatum.setMetaData(buffer.getMetaData());            }                        if (buffer.getScore() > 0) {                hostDatum.setScore(buffer.getScore());            }        } else         if (value instanceof FloatWritable) {            FloatWritable buffer = (FloatWritable) value;            score = buffer.get();        } else {                    }    }        if (score > 0) {        hostDatum.setScore(score);    }        for (Map.Entry<String, Map<String, Long>> entry : stringCounts.entrySet()) {        for (Map.Entry<String, Long> subEntry : entry.getValue().entrySet()) {            hostDatum.getMetaData().put(new Text(entry.getKey() + "." + subEntry.getKey()), new LongWritable(subEntry.getValue()));        }    }    for (Map.Entry<String, Float> entry : maximums.entrySet()) {        hostDatum.getMetaData().put(new Text("max." + entry.getKey()), new FloatWritable(entry.getValue()));    }    for (Map.Entry<String, Float> entry : sums.entrySet()) {        hostDatum.getMetaData().put(new Text("avg." + entry.getKey()), new FloatWritable(entry.getValue() / counts.get(entry.getKey())));    }    for (Map.Entry<String, TDigest> entry : tdigests.entrySet()) {                for (int i = 0; i < percentiles.length; i++) {            hostDatum.getMetaData().put(new Text("pct" + Long.toString(percentiles[i]) + "." + entry.getKey()), new FloatWritable((float) entry.getValue().quantile(0.5)));        }    }    for (Map.Entry<String, Float> entry : minimums.entrySet()) {        hostDatum.getMetaData().put(new Text("min." + entry.getKey()), new FloatWritable(entry.getValue()));    }    context.getCounter("UpdateHostDb", "total_hosts").increment(1);        if (shouldCheck(hostDatum)) {                resolverThread = new ResolverThread(key.toString(), hostDatum, context, purgeFailedHostsThreshold);                try {            queue.put(resolverThread);        } catch (InterruptedException e) {                    }                return;    } else {        context.getCounter("UpdateHostDb", "skipped_not_eligible").increment(1);            }        context.write(key, hostDatum);}
1
protected boolean shouldCheck(HostDatum datum)
{        if (checkNew && datum.isEmpty()) {        return true;    }        if (checkKnown && !datum.isEmpty() && datum.getDnsFailures() == 0) {        return isEligibleForCheck(datum);    }        if (checkFailed && datum.getDnsFailures() > 0) {        return isEligibleForCheck(datum);    }        return false;}
0
protected boolean isEligibleForCheck(HostDatum datum)
{        if (force || datum.getLastCheck().getTime() + (recheckInterval * datum.getDnsFailures() + 1) > now) {        return true;    }    return false;}
0
public void cleanup(Context context)
{            executor.shutdown();    boolean finished = false;        while (!finished) {        try {                        if (!executor.isTerminated()) {                                Thread.sleep(1000);            } else {                                finished = true;            }        } catch (InterruptedException e) {                                }    }}
1
public Configuration getConf()
{    return conf;}
0
public void setConf(Configuration conf)
{    this.conf = conf;}
0
public void map(Text key, CrawlDatum value, Context context) throws IOException, InterruptedException
{    if (value.getStatus() == CrawlDatum.STATUS_DB_GONE || value.getStatus() == CrawlDatum.STATUS_DB_DUPLICATE) {        context.write(OUT, key);    }}
0
public void setup(Reducer<ByteWritable, Text, Text, ByteWritable>.Context context)
{    Configuration conf = context.getConfiguration();    writers = IndexWriters.get(conf);    try {        writers.open(conf, "Deletion");    } catch (IOException e) {        throw new RuntimeException(e);    }    noCommit = conf.getBoolean("noCommit", false);}
0
public void cleanup(Context context) throws IOException
{    if (totalDeleted > 0 && !noCommit) {        writers.commit();    }    writers.close();    }
1
public void reduce(ByteWritable key, Iterable<Text> values, Context context) throws IOException
{    for (Text document : values) {        writers.delete(document.toString());        totalDeleted++;        context.getCounter("CleaningJobStatus", "Deleted documents").increment(1);                                        }}
0
public void delete(String crawldb, boolean noCommit) throws IOException, InterruptedException, ClassNotFoundException
{    SimpleDateFormat sdf = new SimpleDateFormat("yyyy-MM-dd HH:mm:ss");    long start = System.currentTimeMillis();        Job job = NutchJob.getInstance(getConf());    Configuration conf = job.getConfiguration();    FileInputFormat.addInputPath(job, new Path(crawldb, CrawlDb.CURRENT_NAME));    conf.setBoolean("noCommit", noCommit);    job.setInputFormatClass(SequenceFileInputFormat.class);    job.setOutputFormatClass(NullOutputFormat.class);    job.setMapOutputKeyClass(ByteWritable.class);    job.setMapOutputValueClass(Text.class);    job.setMapperClass(DBFilter.class);    job.setReducerClass(DeleterReducer.class);    job.setJarByClass(CleaningJob.class);    job.setJobName("CleaningJob");        conf.setBoolean(IndexerMapReduce.INDEXER_DELETE, true);    try {        boolean success = job.waitForCompletion(true);        if (!success) {            String message = "CleaningJob did not succeed, job status:" + job.getStatus().getState() + ", reason: " + job.getStatus().getFailureInfo();                        throw new RuntimeException(message);        }    } catch (InterruptedException | ClassNotFoundException e) {                throw e;    }    long end = System.currentTimeMillis();    }
1
public int run(String[] args) throws IOException
{    if (args.length < 1) {        String usage = "Usage: CleaningJob <crawldb> [-noCommit]";                System.err.println(usage);        return 1;    }    boolean noCommit = false;    if (args.length == 2 && args[1].equals("-noCommit")) {        noCommit = true;    }    try {        delete(args[0], noCommit);    } catch (final Exception e) {                System.err.println("ERROR CleaningJob: " + StringUtils.stringifyException(e));        return -1;    }    return 0;}
1
public static void main(String[] args) throws Exception
{    int result = ToolRunner.run(NutchConfiguration.create(), new CleaningJob(), args);    System.exit(result);}
0
private static String normalizeUrl(String url, boolean normalize, URLNormalizers urlNormalizers)
{    if (!normalize) {        return url;    }    String normalized = null;    if (urlNormalizers != null) {        try {                        normalized = urlNormalizers.normalize(url, URLNormalizers.SCOPE_INDEXER);            normalized = normalized.trim();        } catch (Exception e) {                        normalized = null;        }    }    return normalized;}
1
private static String filterUrl(String url, boolean filter, URLFilters urlFilters)
{    if (!filter) {        return url;    }    try {        url = urlFilters.filter(url);    } catch (Exception e) {        url = null;    }    return url;}
0
public void setup(Mapper<Text, Writable, Text, NutchWritable>.Context context)
{    Configuration conf = context.getConfiguration();    normalize = conf.getBoolean(URL_NORMALIZING, false);    filter = conf.getBoolean(URL_FILTERING, false);    if (normalize) {        urlNormalizers = new URLNormalizers(conf, URLNormalizers.SCOPE_INDEXER);    }    if (filter) {        urlFilters = new URLFilters(conf);    }}
0
public void map(Text key, Writable value, Context context) throws IOException, InterruptedException
{    String urlString = filterUrl(normalizeUrl(key.toString(), normalize, urlNormalizers), filter, urlFilters);    if (urlString == null) {        return;    } else {        key.set(urlString);    }    context.write(key, new NutchWritable(value));}
0
public void setup(Reducer<Text, NutchWritable, Text, NutchIndexAction>.Context context)
{    Configuration conf = context.getConfiguration();    filters = new IndexingFilters(conf);    scfilters = new ScoringFilters(conf);    delete = conf.getBoolean(INDEXER_DELETE, false);    deleteRobotsNoIndex = conf.getBoolean(INDEXER_DELETE_ROBOTS_NOINDEX, false);    deleteSkippedByIndexingFilter = conf.getBoolean(INDEXER_DELETE_SKIPPED, false);    skip = conf.getBoolean(INDEXER_SKIP_NOTMODIFIED, false);    base64 = conf.getBoolean(INDEXER_BINARY_AS_BASE64, false);    normalize = conf.getBoolean(URL_NORMALIZING, false);    filter = conf.getBoolean(URL_FILTERING, false);    if (normalize) {        urlNormalizers = new URLNormalizers(conf, URLNormalizers.SCOPE_INDEXER);    }    if (filter) {        urlFilters = new URLFilters(conf);    }}
0
public void reduce(Text key, Iterable<NutchWritable> values, Context context) throws IOException, InterruptedException
{    Inlinks inlinks = null;    CrawlDatum dbDatum = null;    CrawlDatum fetchDatum = null;    Content content = null;    ParseData parseData = null;    ParseText parseText = null;    for (NutchWritable val : values) {                final Writable value = val.get();        if (value instanceof Inlinks) {            inlinks = (Inlinks) value;        } else if (value instanceof CrawlDatum) {            final CrawlDatum datum = (CrawlDatum) value;            if (CrawlDatum.hasDbStatus(datum)) {                dbDatum = datum;            } else if (CrawlDatum.hasFetchStatus(datum)) {                                if (datum.getStatus() != CrawlDatum.STATUS_FETCH_NOTMODIFIED) {                    fetchDatum = datum;                }            } else if (CrawlDatum.STATUS_LINKED == datum.getStatus() || CrawlDatum.STATUS_SIGNATURE == datum.getStatus() || CrawlDatum.STATUS_PARSE_META == datum.getStatus()) {                continue;            } else {                throw new RuntimeException("Unexpected status: " + datum.getStatus());            }        } else if (value instanceof ParseData) {            parseData = (ParseData) value;                        if (deleteRobotsNoIndex) {                                String robotsMeta = parseData.getMeta("robots");                                if (robotsMeta != null && robotsMeta.toLowerCase().indexOf("noindex") != -1) {                                        context.write(key, DELETE_ACTION);                    context.getCounter("IndexerStatus", "deleted (robots=noindex)").increment(1);                    return;                }            }        } else if (value instanceof ParseText) {            parseText = (ParseText) value;        } else if (value instanceof Content) {            content = (Content) value;        } else if (LOG.isWarnEnabled()) {                    }    }        if (delete && fetchDatum != null) {        if (fetchDatum.getStatus() == CrawlDatum.STATUS_FETCH_GONE || dbDatum != null && dbDatum.getStatus() == CrawlDatum.STATUS_DB_GONE) {            context.getCounter("IndexerStatus", "deleted (gone)").increment(1);            context.write(key, DELETE_ACTION);            return;        }        if (fetchDatum.getStatus() == CrawlDatum.STATUS_FETCH_REDIR_PERM || fetchDatum.getStatus() == CrawlDatum.STATUS_FETCH_REDIR_TEMP || dbDatum != null && dbDatum.getStatus() == CrawlDatum.STATUS_DB_REDIR_PERM || dbDatum != null && dbDatum.getStatus() == CrawlDatum.STATUS_DB_REDIR_TEMP) {            context.getCounter("IndexerStatus", "deleted (redirects)").increment(1);            context.write(key, DELETE_ACTION);            return;        }    }    if (fetchDatum == null || parseText == null || parseData == null) {                return;    }        if (delete && dbDatum != null && dbDatum.getStatus() == CrawlDatum.STATUS_DB_DUPLICATE) {        context.getCounter("IndexerStatus", "deleted (duplicates)").increment(1);        context.write(key, DELETE_ACTION);        return;    }        if (skip && dbDatum != null && dbDatum.getStatus() == CrawlDatum.STATUS_DB_NOTMODIFIED) {        context.getCounter("IndexerStatus", "skipped (not modified)").increment(1);        return;    }    if (!parseData.getStatus().isSuccess() || fetchDatum.getStatus() != CrawlDatum.STATUS_FETCH_SUCCESS) {        return;    }    NutchDocument doc = new NutchDocument();    doc.add("id", key.toString());    final Metadata metadata = parseData.getContentMeta();        doc.add("segment", metadata.get(Nutch.SEGMENT_NAME_KEY));        doc.add("digest", metadata.get(Nutch.SIGNATURE_KEY));    final Parse parse = new ParseImpl(parseText, parseData);    float boost = 1.0f;        try {        boost = scfilters.indexerScore(key, doc, dbDatum, fetchDatum, parse, inlinks, boost);    } catch (final ScoringFilterException e) {        context.getCounter("IndexerStatus", "errors (ScoringFilter)").increment(1);        if (LOG.isWarnEnabled()) {                    }        return;    }        doc.setWeight(boost);        doc.add("boost", Float.toString(boost));    try {        if (dbDatum != null) {                        fetchDatum.setSignature(dbDatum.getSignature());                                    final Text url = (Text) dbDatum.getMetaData().get(Nutch.WRITABLE_REPR_URL_KEY);            if (url != null) {                                                                                String urlString = filterUrl(normalizeUrl(key.toString(), normalize, urlNormalizers), filter, urlFilters);                if (urlString != null) {                    url.set(urlString);                    fetchDatum.getMetaData().put(Nutch.WRITABLE_REPR_URL_KEY, url);                }            }        }                doc = filters.filter(doc, parse, key, fetchDatum, inlinks);    } catch (final IndexingException e) {        if (LOG.isWarnEnabled()) {                    }        context.getCounter("IndexerStatus", "errors (IndexingFilter)").increment(1);        return;    }        if (doc == null) {                if (deleteSkippedByIndexingFilter) {            NutchIndexAction action = new NutchIndexAction(null, NutchIndexAction.DELETE);            context.write(key, action);            context.getCounter("IndexerStatus", "deleted (IndexingFilter)").increment(1);        } else {            context.getCounter("IndexerStatus", "skipped (IndexingFilter)").increment(1);        }        return;    }    if (content != null) {                String binary;        if (base64) {                                                            binary = StringUtils.newStringUtf8(Base64.encodeBase64(content.getContent(), false, false));        } else {            binary = new String(content.getContent());        }        doc.add("binaryContent", binary);    }    context.getCounter("IndexerStatus", "indexed (add/update)").increment(1);    NutchIndexAction action = new NutchIndexAction(doc, NutchIndexAction.ADD);    context.write(key, action);}
1
public void close() throws IOException
{}
0
public RecordWriter<Text, NutchIndexAction> getRecordWriter(TaskAttemptContext context) throws IOException
{    Configuration conf = context.getConfiguration();    final IndexWriters writers = IndexWriters.get(conf);    String name = getUniqueFile(context, "part", "");    writers.open(conf, name);        return new RecordWriter<Text, NutchIndexAction>() {        public void close(TaskAttemptContext context) throws IOException {                        boolean noCommit = conf.getBoolean(IndexerMapReduce.INDEXER_NO_COMMIT, false);            if (!noCommit) {                writers.commit();            }            writers.close();        }        public void write(Text key, NutchIndexAction indexAction) throws IOException {            if (indexAction.action == NutchIndexAction.ADD) {                writers.write(indexAction.doc);            } else if (indexAction.action == NutchIndexAction.DELETE) {                writers.delete(key.toString());            }        }    };}
1
public void close(TaskAttemptContext context) throws IOException
{        boolean noCommit = conf.getBoolean(IndexerMapReduce.INDEXER_NO_COMMIT, false);    if (!noCommit) {        writers.commit();    }    writers.close();}
0
public void write(Text key, NutchIndexAction indexAction) throws IOException
{    if (indexAction.action == NutchIndexAction.ADD) {        writers.write(indexAction.doc);    } else if (indexAction.action == NutchIndexAction.DELETE) {        writers.delete(key.toString());    }}
0
public NutchDocument filter(NutchDocument doc, Parse parse, Text url, CrawlDatum datum, Inlinks inlinks) throws IndexingException
{    for (int i = 0; i < this.indexingFilters.length; i++) {        doc = this.indexingFilters[i].filter(doc, parse, url, datum, inlinks);                if (doc == null)            return null;    }    return doc;}
0
public int run(String[] args) throws Exception
{    String url = null;    String usage =     "Usage:\n" +     "  IndexingFiltersChecker [OPTIONS] <url>\n" +     "    Fetch single URL and index it\n" +     "  IndexingFiltersChecker [OPTIONS] -stdin\n" +     "    Read URLs to be indexed from stdin\n" +     "  IndexingFiltersChecker [OPTIONS] -listen <port> [-keepClientCnxOpen]\n" +     "    Listen on <port> for URLs to be indexed\n" +     "Options:\n" +     "  -D<property>=<value>\tset/overwrite Nutch/Hadoop properties\n" +     "                  \t(a generic Hadoop option to be passed\n" + "                  \t before other command-specific options)\n" +     "  -normalize      \tnormalize URLs\n" +     "  -followRedirects\tfollow redirects when fetching URL\n" +     "  -dumpText       \tshow the entire plain-text content,\n" +     "                  \tnot only the first 100 characters\n" +     "  -doIndex        \tpass document to configured index writers\n" +     "                  \tand let them index it\n" + "  -md <key>=<value>\tmetadata added to CrawlDatum before parsing\n";        if (args.length < 1) {        System.err.println(usage);        System.exit(-1);    }        doIndex = getConf().getBoolean("doIndex", false);    int numConsumed;    for (int i = 0; i < args.length; i++) {        if (args[i].equals("-normalize")) {            normalizers = new URLNormalizers(getConf(), URLNormalizers.SCOPE_DEFAULT);        } else if (args[i].equals("-followRedirects")) {            followRedirects = true;        } else if (args[i].equals("-dumpText")) {            dumpText = true;        } else if (args[i].equals("-doIndex")) {            doIndex = true;        } else if (args[i].equals("-md")) {            String k = null, v = null;            String nextOne = args[++i];            int firstEquals = nextOne.indexOf("=");            if (firstEquals != -1) {                k = nextOne.substring(0, firstEquals);                v = nextOne.substring(firstEquals + 1);            } else                k = nextOne;            metadata.put(k, v);        } else if ((numConsumed = super.parseArgs(args, i)) > 0) {            i += numConsumed - 1;        } else if (i != args.length - 1) {            System.err.println("ERR: Not a recognized argument: " + args[i]);            System.err.println(usage);            System.exit(-1);        } else {            url = args[i];        }    }    if (url != null) {        return super.processSingle(url);    } else {                return super.run();    }}
0
protected int process(String url, StringBuilder output) throws Exception
{    if (normalizers != null) {        url = normalizers.normalize(url, URLNormalizers.SCOPE_DEFAULT);    }        CrawlDatum datum = new CrawlDatum();    Iterator<String> iter = metadata.keySet().iterator();    while (iter.hasNext()) {        String key = iter.next();        String value = metadata.get(key);        if (value == null)            value = "";        datum.getMetaData().put(new Text(key), new Text(value));    }    int maxRedirects = getConf().getInt("http.redirect.max", 3);    if (followRedirects) {        if (maxRedirects == 0) {                        maxRedirects = 3;        } else {                    }    }    ProtocolOutput protocolOutput = getProtocolOutput(url, datum);    Text turl = new Text(url);        int numRedirects = 0;    while (!protocolOutput.getStatus().isSuccess() && followRedirects && protocolOutput.getStatus().isRedirect() && maxRedirects >= numRedirects) {        String[] stuff = protocolOutput.getStatus().getArgs();        url = stuff[0];                if (normalizers != null) {            url = normalizers.normalize(url, URLNormalizers.SCOPE_DEFAULT);        }        turl.set(url);                protocolOutput = getProtocolOutput(url, datum);        numRedirects++;    }    if (!protocolOutput.getStatus().isSuccess()) {        System.err.println("Fetch failed with protocol status: " + protocolOutput.getStatus());        if (protocolOutput.getStatus().isRedirect()) {            System.err.println("Redirect(s) not handled due to configuration.");            System.err.println("Max Redirects to handle per config: " + maxRedirects);            System.err.println("Number of Redirects handled: " + numRedirects);        }        return -1;    }    Content content = protocolOutput.getContent();    if (content == null) {        output.append("No content for " + url + "\n");        return 0;    }    String contentType = content.getContentType();    if (contentType == null) {                return -1;    }        datum.getMetaData().put(new Text(Metadata.CONTENT_TYPE), new Text(contentType));    if (ParseSegment.isTruncated(content)) {            }    ScoringFilters scfilters = new ScoringFilters(getConf());        try {        scfilters.passScoreBeforeParsing(turl, datum, content);    } catch (Exception e) {            }            ParseResult parseResult = new ParseUtil(getConf()).parse(content);    NutchDocument doc = new NutchDocument();    doc.add("id", url);    Text urlText = new Text(url);    Inlinks inlinks = null;    Parse parse = parseResult.get(urlText);    if (parse == null) {                        for (Map.Entry<Text, Parse> entry : parseResult) {                    }                        return -1;    }    byte[] signature = SignatureFactory.getSignature(getConf()).calculate(content, parse);    parse.getData().getContentMeta().set(Nutch.SIGNATURE_KEY, StringUtil.toHexString(signature));    String digest = parse.getData().getContentMeta().get(Nutch.SIGNATURE_KEY);    doc.add("digest", digest);    datum.setSignature(signature);        try {        scfilters.passScoreAfterParsing(turl, content, parseResult.get(turl));    } catch (Exception e) {            }    IndexingFilters indexers = new IndexingFilters(getConf());    try {        doc = indexers.filter(doc, parse, urlText, datum, inlinks);    } catch (IndexingException e) {        e.printStackTrace();    }    if (doc == null) {        output.append("Document discarded by indexing filter\n");        return 0;    }    for (String fname : doc.getFieldNames()) {        List<Object> values = doc.getField(fname).getValues();        if (values != null) {            for (Object value : values) {                String str = value.toString();                int minText = dumpText ? str.length() : Math.min(100, str.length());                output.append(fname + " :\t" + str.substring(0, minText) + "\n");            }        }    }        output.append("\n");    if (doIndex) {        IndexWriters writers = IndexWriters.get(getConf());        writers.open(getConf(), "IndexingFilterChecker");        writers.write(doc);        writers.close();    }    return 0;}
1
public static void main(String[] args) throws Exception
{    final int res = ToolRunner.run(NutchConfiguration.create(), new IndexingFiltersChecker(), args);    System.exit(res);}
0
public void index(Path crawlDb, Path linkDb, List<Path> segments, boolean noCommit) throws IOException, InterruptedException, ClassNotFoundException
{    index(crawlDb, linkDb, segments, noCommit, false, null);}
0
public void index(Path crawlDb, Path linkDb, List<Path> segments, boolean noCommit, boolean deleteGone) throws IOException, InterruptedException, ClassNotFoundException
{    index(crawlDb, linkDb, segments, noCommit, deleteGone, null);}
0
public void index(Path crawlDb, Path linkDb, List<Path> segments, boolean noCommit, boolean deleteGone, String params) throws IOException, InterruptedException, ClassNotFoundException
{    index(crawlDb, linkDb, segments, noCommit, deleteGone, params, false, false);}
0
public void index(Path crawlDb, Path linkDb, List<Path> segments, boolean noCommit, boolean deleteGone, String params, boolean filter, boolean normalize) throws IOException, InterruptedException, ClassNotFoundException
{    index(crawlDb, linkDb, segments, noCommit, deleteGone, params, false, false, false);}
0
public void index(Path crawlDb, Path linkDb, List<Path> segments, boolean noCommit, boolean deleteGone, String params, boolean filter, boolean normalize, boolean addBinaryContent) throws IOException, InterruptedException, ClassNotFoundException
{    index(crawlDb, linkDb, segments, noCommit, deleteGone, params, false, false, false, false);}
0
public void index(Path crawlDb, Path linkDb, List<Path> segments, boolean noCommit, boolean deleteGone, String params, boolean filter, boolean normalize, boolean addBinaryContent, boolean base64) throws IOException, InterruptedException, ClassNotFoundException
{    SimpleDateFormat sdf = new SimpleDateFormat("yyyy-MM-dd HH:mm:ss");    long start = System.currentTimeMillis();        final Job job = NutchJob.getInstance(getConf());    job.setJobName("Indexer");    Configuration conf = job.getConfiguration();                if (addBinaryContent) {        if (base64) {                    } else {                    }    }    IndexerMapReduce.initMRJob(crawlDb, linkDb, segments, job, addBinaryContent);    conf.setBoolean(IndexerMapReduce.INDEXER_DELETE, deleteGone);    conf.setBoolean(IndexerMapReduce.URL_FILTERING, filter);    conf.setBoolean(IndexerMapReduce.URL_NORMALIZING, normalize);    conf.setBoolean(IndexerMapReduce.INDEXER_BINARY_AS_BASE64, base64);    conf.setBoolean(IndexerMapReduce.INDEXER_NO_COMMIT, noCommit);    if (params != null) {        conf.set(IndexerMapReduce.INDEXER_PARAMS, params);    }    job.setReduceSpeculativeExecution(false);    final Path tmp = new Path("tmp_" + System.currentTimeMillis() + "-" + new Random().nextInt());    FileOutputFormat.setOutputPath(job, tmp);    try {        try {            boolean success = job.waitForCompletion(true);            if (!success) {                String message = "Indexing job did not succeed, job status:" + job.getStatus().getState() + ", reason: " + job.getStatus().getFailureInfo();                                throw new RuntimeException(message);            }        } catch (IOException | InterruptedException | ClassNotFoundException e) {                        throw e;        }                for (Counter counter : job.getCounters().getGroup("IndexerStatus")) {                    }        long end = System.currentTimeMillis();            } finally {        tmp.getFileSystem(conf).delete(tmp, true);    }}
1
public int run(String[] args) throws Exception
{    if (args.length < 2) {        System.err.println("Usage: Indexer <crawldb> [-linkdb <linkdb>] [-params k1=v1&k2=v2...] (<segment> ... | -dir <segments>) [-noCommit] [-deleteGone] [-filter] [-normalize] [-addBinaryContent] [-base64]");        return -1;    }    final Path crawlDb = new Path(args[0]);    Path linkDb = null;    final List<Path> segments = new ArrayList<>();    String params = null;    boolean noCommit = false;    boolean deleteGone = false;    boolean filter = false;    boolean normalize = false;    boolean addBinaryContent = false;    boolean base64 = false;    for (int i = 1; i < args.length; i++) {        FileSystem fs = null;        Path dir = null;        if (args[i].equals("-linkdb")) {            linkDb = new Path(args[++i]);        } else if (args[i].equals("-dir")) {            dir = new Path(args[++i]);            fs = dir.getFileSystem(getConf());            FileStatus[] fstats = fs.listStatus(dir, HadoopFSUtil.getPassDirectoriesFilter(fs));            Path[] files = HadoopFSUtil.getPaths(fstats);            for (Path p : files) {                if (SegmentChecker.isIndexable(p, fs)) {                    segments.add(p);                }            }        } else if (args[i].equals("-noCommit")) {            noCommit = true;        } else if (args[i].equals("-deleteGone")) {            deleteGone = true;        } else if (args[i].equals("-filter")) {            filter = true;        } else if (args[i].equals("-normalize")) {            normalize = true;        } else if (args[i].equals("-addBinaryContent")) {            addBinaryContent = true;        } else if (args[i].equals("-base64")) {            base64 = true;        } else if (args[i].equals("-params")) {            params = args[++i];        } else {            dir = new Path(args[i]);            fs = dir.getFileSystem(getConf());            if (SegmentChecker.isIndexable(dir, fs)) {                segments.add(dir);            }        }    }    try {        index(crawlDb, linkDb, segments, noCommit, deleteGone, params, filter, normalize, addBinaryContent, base64);        return 0;    } catch (final Exception e) {                return -1;    }}
1
public static void main(String[] args) throws Exception
{    final int res = ToolRunner.run(NutchConfiguration.create(), new IndexingJob(), args);    System.exit(res);}
0
public Map<String, Object> run(Map<String, Object> args, String crawlId) throws Exception
{    boolean noCommit = false;    boolean deleteGone = false;    boolean filter = false;    boolean normalize = false;    boolean isSegment = false;    String params = null;    Configuration conf = getConf();    Path crawlDb;    if (args.containsKey(Nutch.ARG_CRAWLDB)) {        Object crawldbPath = args.get(Nutch.ARG_CRAWLDB);        if (crawldbPath instanceof Path) {            crawlDb = (Path) crawldbPath;        } else {            crawlDb = new Path(crawldbPath.toString());        }    } else {        crawlDb = new Path(crawlId + "/crawldb");    }    Path linkdb = null;    List<Path> segments = new ArrayList<>();    if (args.containsKey(Nutch.ARG_LINKDB)) {        Object path = args.get(Nutch.ARG_LINKDB);        if (path instanceof Path) {            linkdb = (Path) path;        } else {            linkdb = new Path(path.toString());        }    } else {        linkdb = new Path(crawlId + "/linkdb");    }    if (args.containsKey(Nutch.ARG_SEGMENTDIR)) {        isSegment = true;        Path segmentsDir;        Object segDir = args.get(Nutch.ARG_SEGMENTDIR);        if (segDir instanceof Path) {            segmentsDir = (Path) segDir;        } else {            segmentsDir = new Path(segDir.toString());        }        FileSystem fs = segmentsDir.getFileSystem(getConf());        FileStatus[] fstats = fs.listStatus(segmentsDir, HadoopFSUtil.getPassDirectoriesFilter(fs));        Path[] files = HadoopFSUtil.getPaths(fstats);        for (Path p : files) {            if (SegmentChecker.isIndexable(p, fs)) {                segments.add(p);            }        }    }    if (args.containsKey(Nutch.ARG_SEGMENTS)) {        Object segmentsFromArg = args.get(Nutch.ARG_SEGMENTS);        ArrayList<String> segmentList = new ArrayList<String>();        if (segmentsFromArg instanceof ArrayList) {            segmentList = (ArrayList<String>) segmentsFromArg;        } else if (segmentsFromArg instanceof Path) {            segmentList.add(segmentsFromArg.toString());        }        for (String segment : segmentList) {            segments.add(new Path(segment));        }    }    if (!isSegment) {        String segment_dir = crawlId + "/segments";        File segmentsDir = new File(segment_dir);        File[] segmentsList = segmentsDir.listFiles();        Arrays.sort(segmentsList, (f1, f2) -> {            if (f1.lastModified() > f2.lastModified())                return -1;            else                return 0;        });        Path segment = new Path(segmentsList[0].getPath());        segments.add(segment);    }    if (args.containsKey("noCommit")) {        noCommit = true;    }    if (args.containsKey("deleteGone")) {        deleteGone = true;    }    if (args.containsKey("normalize")) {        normalize = true;    }    if (args.containsKey("filter")) {        filter = true;    }    if (args.containsKey("params")) {        params = (String) args.get("params");    }    setConf(conf);    index(crawlDb, linkdb, segments, noCommit, deleteGone, params, filter, normalize);    Map<String, Object> results = new HashMap<>();    results.put(Nutch.VAL_RESULT, 0);    return results;}
0
 static IndexWriterConfig getInstanceFromElement(Element rootElement)
{    String id = rootElement.getAttribute("id");    String clazz = rootElement.getAttribute("class");    NodeList parametersList = rootElement.getElementsByTagName("param");    Map<String, String> parameters = new HashMap<>();    for (int i = 0; i < parametersList.getLength(); i++) {        Element parameterNode = (Element) parametersList.item(i);        parameters.put(parameterNode.getAttribute("name"), parameterNode.getAttribute("value"));    }    return new IndexWriterConfig(id, clazz, parameters, MappingReader.parseMapping((Element) rootElement.getElementsByTagName("mapping").item(0)));}
0
 String getId()
{    return id;}
0
 String getClazz()
{    return clazz;}
0
 IndexWriterParams getParams()
{    return params;}
0
 Map<MappingReader.Actions, Map<String, List<String>>> getMapping()
{    return mapping;}
0
public String toString()
{    StringBuilder sb = new StringBuilder();    sb.append("ID: ");    sb.append(id);    sb.append("\n");    sb.append("Class: ");    sb.append(clazz);    sb.append("\n");    sb.append("Params {\n");    for (Map.Entry<String, String> entry : params.entrySet()) {        sb.append("\t");        sb.append(entry.getKey());        sb.append(":\t");        sb.append(entry.getValue());        sb.append("\n");    }    sb.append("}\n");    sb.append("Mapping {\n");    for (Map.Entry<MappingReader.Actions, Map<String, List<String>>> entry : mapping.entrySet()) {        sb.append("\t");        sb.append(entry.getKey());        sb.append(" {\n");        for (Map.Entry<String, List<String>> entry1 : entry.getValue().entrySet()) {            sb.append("\t\t");            sb.append(entry1.getKey());            sb.append(":\t");            sb.append(String.join(",", entry1.getValue()));            sb.append("\n");        }        sb.append("\t}\n");    }    sb.append("}\n");    return sb.toString();}
0
public String get(String name, String defaultValue)
{    return this.getOrDefault(name, defaultValue);}
0
public boolean getBoolean(String name, boolean defaultValue)
{    String value;    if ((value = this.get(name)) != null && !"".equals(value)) {        return Boolean.parseBoolean(value);    }    return defaultValue;}
0
public long getLong(String name, long defaultValue)
{    String value;    if ((value = this.get(name)) != null && !"".equals(value)) {        return Long.parseLong(value);    }    return defaultValue;}
0
public int getInt(String name, int defaultValue)
{    String value;    if ((value = this.get(name)) != null && !"".equals(value)) {        return Integer.parseInt(value);    }    return defaultValue;}
0
public String[] getStrings(String name)
{    String value = this.get(name);    return StringUtils.getStrings(value);}
0
public String[] getStrings(String name, String... defaultValue)
{    String value;    if ((value = this.get(name)) != null && !"".equals(value)) {        return StringUtils.getStrings(value);    }    return defaultValue;}
0
public static synchronized IndexWriters get(Configuration conf)
{    String uuid = NutchConfiguration.getUUID(conf);    if (uuid == null) {                uuid = "nonNutchConf@" + conf.hashCode();    }    return CACHE.computeIfAbsent(uuid, k -> new IndexWriters(conf));}
0
private IndexWriterConfig[] loadWritersConfiguration(Configuration conf)
{    String filename = conf.get("indexer.indexwriters.file", "index-writers.xml");    InputStream ssInputStream = conf.getConfResourceAsInputStream(filename);    InputSource inputSource = new InputSource(ssInputStream);    try {        DocumentBuilderFactory factory = DocumentBuilderFactory.newInstance();        DocumentBuilder builder = factory.newDocumentBuilder();        Document document = builder.parse(inputSource);        Element rootElement = document.getDocumentElement();        NodeList writerList = rootElement.getElementsByTagName("writer");        IndexWriterConfig[] indexWriterConfigs = new IndexWriterConfig[writerList.getLength()];        for (int i = 0; i < writerList.getLength(); i++) {            indexWriterConfigs[i] = IndexWriterConfig.getInstanceFromElement((Element) writerList.item(i));        }        return indexWriterConfigs;    } catch (SAXException | IOException | ParserConfigurationException e) {                return new IndexWriterConfig[0];    }}
1
private NutchDocument mapDocument(final NutchDocument document, final Map<MappingReader.Actions, Map<String, List<String>>> mapping)
{    try {        NutchDocument mappedDocument = document.clone();        mapping.get(MappingReader.Actions.COPY).forEach((key, value) -> {                        if (mappedDocument.getField(key) != null) {                for (String field : value) {                                        if (!key.equals(field)) {                        for (Object val : mappedDocument.getField(key).getValues()) {                            mappedDocument.add(field, val);                        }                    }                }            }        });        mapping.get(MappingReader.Actions.RENAME).forEach((key, value) -> {                        if (mappedDocument.getField(key) != null) {                NutchField field = mappedDocument.removeField(key);                mappedDocument.add(value.get(0), field.getValues());                mappedDocument.getField(value.get(0)).setWeight(field.getWeight());            }        });        mapping.get(MappingReader.Actions.REMOVE).forEach((key, value) -> mappedDocument.removeField(key));        return mappedDocument;    } catch (CloneNotSupportedException e) {                return document;    }}
1
private Collection<String> getIndexWriters(NutchDocument doc)
{    if (this.exchanges.areAvailableExchanges()) {        return Arrays.asList(this.exchanges.indexWriters(doc));    }    return this.indexWriters.keySet();}
0
public void open(Configuration conf, String name) throws IOException
{    for (Map.Entry<String, IndexWriterWrapper> entry : this.indexWriters.entrySet()) {        entry.getValue().getIndexWriter().open(conf, name);        entry.getValue().getIndexWriter().open(entry.getValue().getIndexWriterConfig().getParams());    }}
0
public void write(NutchDocument doc) throws IOException
{    for (String indexWriterId : getIndexWriters(doc)) {        if (!this.indexWriters.containsKey(indexWriterId)) {                        continue;        }        NutchDocument mappedDocument = mapDocument(doc, this.indexWriters.get(indexWriterId).getIndexWriterConfig().getMapping());        this.indexWriters.get(indexWriterId).getIndexWriter().write(mappedDocument);    }}
1
public void update(NutchDocument doc) throws IOException
{    for (String indexWriterId : getIndexWriters(doc)) {        if (!this.indexWriters.containsKey(indexWriterId)) {                        continue;        }        NutchDocument mappedDocument = mapDocument(doc, this.indexWriters.get(indexWriterId).getIndexWriterConfig().getMapping());        this.indexWriters.get(indexWriterId).getIndexWriter().update(mappedDocument);    }}
1
public void delete(String key) throws IOException
{    for (IndexWriterWrapper iww : indexWriters.values()) {        iww.getIndexWriter().delete(key);    }}
0
public void close() throws IOException
{    for (Map.Entry<String, IndexWriterWrapper> entry : this.indexWriters.entrySet()) {        entry.getValue().getIndexWriter().close();    }}
0
public void commit() throws IOException
{    for (Map.Entry<String, IndexWriterWrapper> entry : this.indexWriters.entrySet()) {        entry.getValue().getIndexWriter().commit();    }}
0
public String describe()
{    StringBuilder builder = new StringBuilder();    if (this.indexWriters.size() == 0)        builder.append("No IndexWriters activated - check your configuration\n");    else        builder.append("Active IndexWriters :\n");    for (IndexWriterWrapper indexWriterWrapper : this.indexWriters.values()) {                builder.append(indexWriterWrapper.getIndexWriter().getClass().getSimpleName()).append(":\n");                AsciiTable at = new AsciiTable();        at.getRenderer().setCWC((rows, colNumbers, tableWidth) -> {            int maxLengthFirstColumn = 0;            int maxLengthLastColumn = 0;            for (AT_Row row : rows) {                if (row.getType() == TableRowType.CONTENT) {                                        int lengthFirstColumn = row.getCells().get(0).toString().length();                    if (lengthFirstColumn > maxLengthFirstColumn) {                        maxLengthFirstColumn = lengthFirstColumn;                    }                                        int lengthLastColumn = row.getCells().get(2).toString().length();                    if (lengthLastColumn > maxLengthLastColumn) {                        maxLengthLastColumn = lengthLastColumn;                    }                }            }            return new int[] { maxLengthFirstColumn, tableWidth - maxLengthFirstColumn - maxLengthLastColumn, maxLengthLastColumn };        });                Map<String, Map.Entry<String, Object>> properties = indexWriterWrapper.getIndexWriter().describe();                properties.forEach((key, value) -> {            at.addRule();            at.addRow(key, value.getKey(), value.getValue() != null ? value.getValue() : "");        });                at.addRule();                builder.append(at.render(150)).append("\n\n");    }    return builder.toString();}
0
 IndexWriterConfig getIndexWriterConfig()
{    return indexWriterConfig;}
0
 void setIndexWriterConfig(IndexWriterConfig indexWriterConfig)
{    this.indexWriterConfig = indexWriterConfig;}
0
 IndexWriter getIndexWriter()
{    return indexWriter;}
0
 void setIndexWriter(IndexWriter indexWriter)
{    this.indexWriter = indexWriter;}
0
 static Map<Actions, Map<String, List<String>>> parseMapping(Element mappingElement)
{    Map<Actions, Map<String, List<String>>> parsedMapping = new HashMap<>();        Node node = mappingElement.getElementsByTagName("rename").item(0);    if (node != null) {        NodeList fieldList = ((Element) node).getElementsByTagName("field");        Map<String, List<String>> fieldsMap = new HashMap<>();        for (int j = 0; j < fieldList.getLength(); j++) {            Element field = (Element) fieldList.item(j);            fieldsMap.put(field.getAttribute("source"), Collections.singletonList(field.getAttribute("dest")));        }        parsedMapping.put(Actions.RENAME, fieldsMap);    }        node = mappingElement.getElementsByTagName("copy").item(0);    if (node != null) {        NodeList fieldList = ((Element) node).getElementsByTagName("field");        Map<String, List<String>> fieldsMap = new HashMap<>();        for (int j = 0; j < fieldList.getLength(); j++) {            Element field = (Element) fieldList.item(j);            fieldsMap.put(field.getAttribute("source"), Arrays.asList(field.getAttribute("dest").split(",")));        }        parsedMapping.put(Actions.COPY, fieldsMap);    }        node = mappingElement.getElementsByTagName("remove").item(0);    if (node != null) {        NodeList fieldList = ((Element) node).getElementsByTagName("field");        Map<String, List<String>> fieldsMap = new HashMap<>();        for (int j = 0; j < fieldList.getLength(); j++) {            Element field = (Element) fieldList.item(j);            fieldsMap.put(field.getAttribute("source"), null);        }        parsedMapping.put(Actions.REMOVE, fieldsMap);    }    return parsedMapping;}
0
public void add(String name, Object value)
{    NutchField field = fields.get(name);    if (field == null) {        field = new NutchField(value);        fields.put(name, field);    } else {        field.add(value);    }}
0
public Object getFieldValue(String name)
{    NutchField field = fields.get(name);    if (field == null) {        return null;    }    if (field.getValues().size() == 0) {        return null;    }    return field.getValues().get(0);}
0
public NutchField getField(String name)
{    return fields.get(name);}
0
public NutchField removeField(String name)
{    return fields.remove(name);}
0
public Collection<String> getFieldNames()
{    return fields.keySet();}
0
public Iterator<Entry<String, NutchField>> iterator()
{    return fields.entrySet().iterator();}
0
public float getWeight()
{    return weight;}
0
public void setWeight(float weight)
{    this.weight = weight;}
0
public Metadata getDocumentMeta()
{    return documentMeta;}
0
public void readFields(DataInput in) throws IOException
{    fields.clear();    byte version = in.readByte();    if (version != VERSION) {        throw new VersionMismatchException(VERSION, version);    }    int size = WritableUtils.readVInt(in);    for (int i = 0; i < size; i++) {        String name = Text.readString(in);        NutchField field = new NutchField();        field.readFields(in);        fields.put(name, field);    }    weight = in.readFloat();    documentMeta.readFields(in);}
0
public void write(DataOutput out) throws IOException
{    out.writeByte(VERSION);    WritableUtils.writeVInt(out, fields.size());    for (Map.Entry<String, NutchField> entry : fields.entrySet()) {        Text.writeString(out, entry.getKey());        NutchField field = entry.getValue();        field.write(out);    }    out.writeFloat(weight);    documentMeta.write(out);}
0
public String toString()
{    StringBuilder sb = new StringBuilder();    sb.append("doc {\n");    for (Map.Entry<String, NutchField> entry : fields.entrySet()) {        sb.append("\t");        sb.append(entry.getKey());        sb.append(":\t");        sb.append(entry.getValue());        sb.append("\n");    }    sb.append("}\n");    return sb.toString();}
0
public NutchDocument clone() throws CloneNotSupportedException
{    NutchDocument clonedDocument = (NutchDocument) super.clone();    clonedDocument.fields = new HashMap<>();    for (Entry<String, NutchField> field : this.fields.entrySet()) {        clonedDocument.fields.put(field.getKey(), field.getValue().clone());    }    return clonedDocument;}
0
public void add(Object value)
{    values.add(value);}
0
public float getWeight()
{    return weight;}
0
public void setWeight(float weight)
{    this.weight = weight;}
0
public List<Object> getValues()
{    return values;}
0
public void reset()
{    weight = 1.0f;    values.clear();}
0
public NutchField clone() throws CloneNotSupportedException
{    NutchField result = (NutchField) super.clone();    result.weight = weight;    result.values = (ArrayList<Object>) values.clone();    return result;}
0
public void readFields(DataInput in) throws IOException
{    weight = in.readFloat();    int count = in.readInt();    values = new ArrayList<>();    for (int i = 0; i < count; i++) {        String type = Text.readString(in);        if ("java.lang.String".equals(type)) {            values.add(Text.readString(in));        } else if ("java.lang.Boolean".equals(type)) {            values.add(in.readBoolean());        } else if ("java.lang.Integer".equals(type)) {            values.add(in.readInt());        } else if ("java.lang.Float".equals(type)) {            values.add(in.readFloat());        } else if ("java.lang.Long".equals(type)) {            values.add(in.readLong());        } else if ("java.util.Date".equals(type)) {            values.add(new Date(in.readLong()));        }    }}
0
public void write(DataOutput out) throws IOException
{    out.writeFloat(weight);    out.writeInt(values.size());    for (Object value : values) {        Text.writeString(out, value.getClass().getName());        if (value instanceof Boolean) {            out.writeBoolean((Boolean) value);        } else if (value instanceof Integer) {            out.writeInt((Integer) value);        } else if (value instanceof Long) {            out.writeLong((Long) value);        } else if (value instanceof Float) {            out.writeFloat((Float) value);        } else if (value instanceof String) {            Text.writeString(out, (String) value);        } else if (value instanceof Date) {            Date date = (Date) value;            out.writeLong(date.getTime());        }    }}
0
public String toString()
{    return values.toString();}
0
public void readFields(DataInput in) throws IOException
{    action = in.readByte();    doc = new NutchDocument();    doc.readFields(in);}
0
public void write(DataOutput out) throws IOException
{    out.write(action);    doc.write(out);}
0
public boolean isMultiValued(final String name)
{    return metadata.get(name) != null && metadata.get(name).length > 1;}
0
public String[] names()
{    return metadata.keySet().toArray(new String[metadata.keySet().size()]);}
0
public String get(final String name)
{    String[] values = metadata.get(name);    if (values == null) {        return null;    } else {        return values[0];    }}
0
public String[] getValues(final String name)
{    return _getValues(name);}
0
private String[] _getValues(final String name)
{    String[] values = metadata.get(name);    if (values == null) {        values = new String[0];    }    return values;}
0
public void add(final String name, final String value)
{    String[] values = metadata.get(name);    if (values == null) {        set(name, value);    } else {        String[] newValues = new String[values.length + 1];        System.arraycopy(values, 0, newValues, 0, values.length);        newValues[newValues.length - 1] = value;        metadata.put(name, newValues);    }}
0
public void addAll(Metadata metadata)
{    for (String name : metadata.names()) {        String[] addValues = metadata.getValues(name);        if (addValues == null)            continue;        String[] oldValues = this.metadata.get(name);        if (oldValues == null) {            this.metadata.put(name, addValues);        } else {            String[] newValues = new String[oldValues.length + addValues.length];            System.arraycopy(oldValues, 0, newValues, 0, oldValues.length);            System.arraycopy(addValues, 0, newValues, oldValues.length, addValues.length);            this.metadata.put(name, newValues);        }    }}
0
public void setAll(Properties properties)
{    Enumeration<?> names = properties.propertyNames();    while (names.hasMoreElements()) {        String name = (String) names.nextElement();        metadata.put(name, new String[] { properties.getProperty(name) });    }}
0
public void set(String name, String value)
{    metadata.put(name, new String[] { value });}
0
public void remove(String name)
{    metadata.remove(name);}
0
public int size()
{    return metadata.size();}
0
public void clear()
{    metadata.clear();}
0
public boolean equals(Object o)
{    if (o == null) {        return false;    }    Metadata other = null;    try {        other = (Metadata) o;    } catch (ClassCastException cce) {        return false;    }    if (other.size() != size()) {        return false;    }    String[] names = names();    for (int i = 0; i < names.length; i++) {        String[] otherValues = other._getValues(names[i]);        String[] thisValues = _getValues(names[i]);        if (otherValues.length != thisValues.length) {            return false;        }        for (int j = 0; j < otherValues.length; j++) {            if (!otherValues[j].equals(thisValues[j])) {                return false;            }        }    }    return true;}
0
public String toString()
{    StringBuffer buf = new StringBuffer();    String[] names = names();    for (int i = 0; i < names.length; i++) {        String[] values = _getValues(names[i]);        for (int j = 0; j < values.length; j++) {            buf.append(names[i]).append("=").append(values[j]).append(" ");        }    }    return buf.toString();}
0
public final void write(DataOutput out) throws IOException
{    out.writeInt(size());    String[] values = null;    String[] names = names();    for (int i = 0; i < names.length; i++) {        Text.writeString(out, names[i]);        values = _getValues(names[i]);        int cnt = 0;        for (int j = 0; j < values.length; j++) {            if (values[j] != null)                cnt++;        }        out.writeInt(cnt);        for (int j = 0; j < values.length; j++) {            if (values[j] != null) {                Text.writeString(out, values[j]);            }        }    }}
0
public final void readFields(DataInput in) throws IOException
{    int keySize = in.readInt();    String key;    for (int i = 0; i < keySize; i++) {        key = Text.readString(in);        int valueSize = in.readInt();        for (int j = 0; j < valueSize; j++) {            add(key, Text.readString(in));        }    }}
0
public Metadata getMetadata()
{    return metadata;}
0
public void addMeta(String name, String value)
{    metadata.add(name, value);}
0
public void setMeta(String name, String value)
{    metadata.set(name, value);}
0
public String getMeta(String name)
{    return metadata.get(name);}
0
public String[] getMetaValues(String name)
{    return metadata.getValues(name);}
0
public void readFields(DataInput in) throws IOException
{    super.readFields(in);    metadata = new Metadata();    metadata.readFields(in);}
0
public void write(DataOutput out) throws IOException
{    super.write(out);    metadata.write(out);}
0
private static String normalize(final String str)
{    char c;    StringBuffer buf = new StringBuffer();    for (int i = 0; i < str.length(); i++) {        c = str.charAt(i);        if (Character.isLetter(c)) {            buf.append(Character.toLowerCase(c));        }    }    return buf.toString();}
0
public static String getNormalizedName(final String name)
{    String searched = normalize(name);    String value = NAMES_IDX.get(searched);    if ((value == null) && (normalized != null)) {        int threshold = Math.min(3, searched.length() / TRESHOLD_DIVIDER);        for (int i = 0; i < normalized.length && value == null; i++) {            if (StringUtils.getLevenshteinDistance(searched, normalized[i]) < threshold) {                value = NAMES_IDX.get(normalized[i]);            }        }    }    return (value != null) ? value : name;}
0
public void remove(final String name)
{    super.remove(getNormalizedName(name));}
0
public void add(final String name, final String value)
{    super.add(getNormalizedName(name), value);}
0
public String[] getValues(final String name)
{    return super.getValues(getNormalizedName(name));}
0
public String get(final String name)
{    return super.get(getNormalizedName(name));}
0
public void set(final String name, final String value)
{    super.set(getNormalizedName(name), value);}
0
public static String toString(Date date)
{    String string;    synchronized (format) {        string = format.format(date);    }    return string;}
0
public static String toString(Calendar cal)
{    String string;    synchronized (format) {        string = format.format(cal.getTime());    }    return string;}
0
public static String toString(long time)
{    String string;    synchronized (format) {        string = format.format(new Date(time));    }    return string;}
0
public static Date toDate(String dateString) throws ParseException
{    Date date;    synchronized (format) {        date = format.parse(dateString);    }    return date;}
0
public static long toLong(String dateString) throws ParseException
{    long time;    synchronized (format) {        time = format.parse(dateString).getTime();    }    return time;}
0
public static void main(String[] args) throws Exception
{    Date now = new Date(System.currentTimeMillis());    String string = HttpDateFormat.toString(now);    long time = HttpDateFormat.toLong(string);    System.out.println(string);    System.out.println(HttpDateFormat.toString(time));}
0
public Configuration getConf()
{    return config;}
0
public void setConf(Configuration conf)
{    config = conf;    for (String exceptClassName : conf.getTrimmedStrings(HTTP_LOG_SUPPRESSION, "java.net.UnknownHostException", "java.net.NoRouteToHostException")) {        Class<?> clazz = conf.getClassByNameOrNull(exceptClassName);        if (clazz == null) {                        continue;        }        if (!Throwable.class.isAssignableFrom(clazz)) {                        continue;        }        exceptionsLogShort.add(clazz.asSubclass(Throwable.class));    }}
1
public boolean logShort(Throwable t)
{    if (exceptionsLogShort.contains(t.getClass())) {        return true;    }    return false;}
0
public boolean isExempted(String fromUrl, String toUrl)
{    if (filters.length < 1) {                return false;    }        boolean exempted = fromUrl != null && toUrl != null;        for (int i = 0; i < this.filters.length && exempted; i++) {        exempted = this.filters[i].filter(fromUrl, toUrl);    }    return exempted;}
0
public int run(String[] args) throws Exception
{    usage = "Usage: URLFilterChecker [-Dproperty=value]... [-filterName filterName] (-stdin | -listen <port> [-keepClientCnxOpen]) \n" + "\n  -filterName\tURL filter plugin name (eg. urlfilter-regex) to check," + "\n             \t(if not given all configured URL filters are applied)" + "\n  -stdin     \ttool reads a list of URLs from stdin, one URL per line" + "\n  -listen <port>\trun tool as Telnet server listening on <port>\n";        if (args.length < 1) {        System.err.println(usage);        System.exit(-1);    }    int numConsumed;    for (int i = 0; i < args.length; i++) {        if (args[i].equals("-filterName")) {            getConf().set("plugin.includes", args[++i]);        } else if ((numConsumed = super.parseArgs(args, i)) > 0) {            i += numConsumed - 1;        } else {            System.err.println("ERROR: Not a recognized argument: " + args[i]);            System.err.println(usage);            System.exit(-1);        }    }        filters = new URLFilters(getConf());    System.out.print("Checking combination of these URLFilters: ");    for (URLFilter filter : filters.getFilters()) {        System.out.print(filter.getClass().getSimpleName() + " ");    }    System.out.println("");        return super.run();}
0
protected int process(String line, StringBuilder output) throws Exception
{    String out = filters.filter(line);    if (out != null) {        output.append("+");        output.append(out);    } else {        output.append("-");        output.append(line);    }    return 0;}
0
public static void main(String[] args) throws Exception
{    final int res = ToolRunner.run(NutchConfiguration.create(), new URLFilterChecker(), args);    System.exit(res);}
0
public URLFilter[] getFilters()
{    return this.filters;}
0
public String filter(String urlString) throws URLFilterException
{    for (int i = 0; i < this.filters.length; i++) {        if (urlString == null)            return null;        urlString = this.filters[i].filter(urlString);    }    return urlString;}
0
public int run(String[] args) throws Exception
{    usage = "Usage: URLNormalizerChecker [-Dproperty=value]... [-normalizer <normalizerName>] [-scope <scope>] (-stdin | -listen <port> [-keepClientCnxOpen])\n" + "\n  -normalizer\tURL normalizer plugin (eg. urlnormalizer-basic) to check," + "\n             \t(if not given all configured URL normalizers are applied)" + "\n  -scope     \tone of: default,partition,generate_host_count,fetcher,crawldb,linkdb,inject,outlink" + "\n  -stdin     \ttool reads a list of URLs from stdin, one URL per line" + "\n  -listen <port>\trun tool as Telnet server listening on <port>" + "\n\nAn empty line is added to the output if a URL fails to normalize (MalformedURLException or null returned).\n";        if (args.length < 1) {        System.err.println(usage);        System.exit(-1);    }    int numConsumed;    for (int i = 0; i < args.length; i++) {        if (args[i].equals("-normalizer")) {            getConf().set("plugin.includes", args[++i]);        } else if (args[i].equals("-scope")) {            scope = args[++i];        } else if ((numConsumed = super.parseArgs(args, i)) > 0) {            i += numConsumed - 1;        } else {            System.err.println("ERROR: Not a recognized argument: " + args[i]);            System.err.println(usage);            System.exit(-1);        }    }        normalizers = new URLNormalizers(getConf(), scope);    System.out.print("Checking combination of these URLNormalizers: ");    for (URLNormalizer normalizer : normalizers.getURLNormalizers(scope)) {        System.out.print(normalizer.getClass().getSimpleName() + " ");    }    System.out.println("");        return super.run();}
0
protected int process(String line, StringBuilder output) throws Exception
{    try {        String norm = normalizers.normalize(line, scope);        if (norm == null) {            output.append("");        } else {            output.append(norm);        }    } catch (MalformedURLException e) {        output.append("");    }    return 0;}
0
public static void main(String[] args) throws Exception
{    final int res = ToolRunner.run(NutchConfiguration.create(), new URLNormalizerChecker(), args);    System.exit(res);}
0
 URLNormalizer[] getURLNormalizers(String scope)
{    List<Extension> extensions = getExtensions(scope);    ObjectCache objectCache = ObjectCache.get(conf);    if (extensions == EMPTY_EXTENSION_LIST) {        return EMPTY_NORMALIZERS;    }    List<URLNormalizer> normalizers = new Vector<>(extensions.size());    Iterator<Extension> it = extensions.iterator();    while (it.hasNext()) {        Extension ext = it.next();        URLNormalizer normalizer = null;        try {                        normalizer = (URLNormalizer) objectCache.getObject(ext.getId());            if (normalizer == null) {                                normalizer = (URLNormalizer) ext.getExtensionInstance();                objectCache.setObject(ext.getId(), normalizer);            }            normalizers.add(normalizer);        } catch (PluginRuntimeException e) {            e.printStackTrace();                    }    }    return normalizers.toArray(new URLNormalizer[normalizers.size()]);}
1
private List<Extension> getExtensions(String scope)
{    ObjectCache objectCache = ObjectCache.get(conf);    List<Extension> extensions = (List<Extension>) objectCache.getObject(URLNormalizer.X_POINT_ID + "_x_" + scope);        if (extensions == EMPTY_EXTENSION_LIST) {        return EMPTY_EXTENSION_LIST;    }    if (extensions == null) {        extensions = findExtensions(scope);        if (extensions != null) {            objectCache.setObject(URLNormalizer.X_POINT_ID + "_x_" + scope, extensions);        } else {                                    objectCache.setObject(URLNormalizer.X_POINT_ID + "_x_" + scope, EMPTY_EXTENSION_LIST);            extensions = EMPTY_EXTENSION_LIST;        }    }    return extensions;}
0
private List<Extension> findExtensions(String scope)
{    String[] orders = null;    String orderlist = conf.get("urlnormalizer.order." + scope);    if (orderlist == null)        orderlist = conf.get("urlnormalizer.order");    if (orderlist != null && !orderlist.trim().equals("")) {        orders = orderlist.trim().split("\\s+");    }    String scopelist = conf.get("urlnormalizer.scope." + scope);    Set<String> impls = null;    if (scopelist != null && !scopelist.trim().equals("")) {        String[] names = scopelist.split("\\s+");        impls = new HashSet<>(Arrays.asList(names));    }    Extension[] extensions = this.extensionPoint.getExtensions();    HashMap<String, Extension> normalizerExtensions = new HashMap<>();    for (int i = 0; i < extensions.length; i++) {        Extension extension = extensions[i];        if (impls != null && !impls.contains(extension.getClazz()))            continue;        normalizerExtensions.put(extension.getClazz(), extension);    }    List<Extension> res = new ArrayList<>();    if (orders == null) {        res.addAll(normalizerExtensions.values());    } else {                for (int i = 0; i < orders.length; i++) {            Extension e = normalizerExtensions.get(orders[i]);            if (e != null) {                res.add(e);                normalizerExtensions.remove(orders[i]);            }        }                res.addAll(normalizerExtensions.values());    }    return res;}
0
public String normalize(String urlString, String scope) throws MalformedURLException
{        String initialString = urlString;    for (int k = 0; k < loopCount; k++) {        for (int i = 0; i < this.normalizers.length; i++) {            if (urlString == null)                return null;            urlString = this.normalizers[i].normalize(urlString, scope);        }        if (initialString.equals(urlString))            break;        initialString = urlString;    }    return urlString;}
0
public void reset()
{    noIndex = false;    noFollow = false;    noCache = false;    refresh = false;    refreshTime = 0;    baseHref = null;    refreshHref = null;    generalTags.clear();    httpEquivTags.clear();}
0
public void setNoFollow()
{    noFollow = true;}
0
public void setNoIndex()
{    noIndex = true;}
0
public void setNoCache()
{    noCache = true;}
0
public void setRefresh(boolean refresh)
{    this.refresh = refresh;}
0
public void setBaseHref(URL baseHref)
{    this.baseHref = baseHref;}
0
public void setRefreshHref(URL refreshHref)
{    this.refreshHref = refreshHref;}
0
public void setRefreshTime(int refreshTime)
{    this.refreshTime = refreshTime;}
0
public boolean getNoIndex()
{    return noIndex;}
0
public boolean getNoFollow()
{    return noFollow;}
0
public boolean getNoCache()
{    return noCache;}
0
public boolean getRefresh()
{    return refresh;}
0
public URL getBaseHref()
{    return baseHref;}
0
public URL getRefreshHref()
{    return refreshHref;}
0
public int getRefreshTime()
{    return refreshTime;}
0
public Metadata getGeneralTags()
{    return generalTags;}
0
public Properties getHttpEquivTags()
{    return httpEquivTags;}
0
public String toString()
{    StringBuffer sb = new StringBuffer();    sb.append("base=" + baseHref + ", noCache=" + noCache + ", noFollow=" + noFollow + ", noIndex=" + noIndex + ", refresh=" + refresh + ", refreshHref=" + refreshHref + "\n");    sb.append(" * general tags:\n");    String[] names = generalTags.names();    for (String name : names) {        String key = name;        sb.append("   - " + key + "\t=\t" + generalTags.get(key) + "\n");    }    sb.append(" * http-equiv tags:\n");    Iterator<Object> it = httpEquivTags.keySet().iterator();    it = httpEquivTags.keySet().iterator();    while (it.hasNext()) {        String key = (String) it.next();        sb.append("   - " + key + "\t=\t" + httpEquivTags.get(key) + "\n");    }    return sb.toString();}
0
public ParseResult filter(Content content, ParseResult parseResult, HTMLMetaTags metaTags, DocumentFragment doc)
{        for (int i = 0; i < this.htmlParseFilters.length; i++) {                parseResult = htmlParseFilters[i].filter(content, parseResult, metaTags, doc);                if (!parseResult.isSuccess()) {                                                parseResult.filter();            return parseResult;        }    }    return parseResult;}
0
public void readFields(DataInput in) throws IOException
{    toUrl = Text.readString(in);    anchor = Text.readString(in);    boolean hasMD = in.readBoolean();    if (hasMD) {        md = new org.apache.hadoop.io.MapWritable();        md.readFields(in);    } else        md = null;}
0
public static void skip(DataInput in) throws IOException
{        Text.skip(in);        Text.skip(in);    boolean hasMD = in.readBoolean();    if (hasMD) {        MapWritable metadata = new org.apache.hadoop.io.MapWritable();        metadata.readFields(in);        ;    }}
0
public void write(DataOutput out) throws IOException
{    Text.writeString(out, toUrl);    Text.writeString(out, anchor);    if (md != null && md.size() > 0) {        out.writeBoolean(true);        md.write(out);    } else {        out.writeBoolean(false);    }}
0
public static Outlink read(DataInput in) throws IOException
{    Outlink outlink = new Outlink();    outlink.readFields(in);    return outlink;}
0
public String getToUrl()
{    return toUrl;}
0
public void setUrl(String toUrl)
{    this.toUrl = toUrl;}
0
public String getAnchor()
{    return anchor;}
0
public MapWritable getMetadata()
{    return md;}
0
public void setMetadata(MapWritable md)
{    this.md = md;}
0
public boolean equals(Object o)
{    if (!(o instanceof Outlink))        return false;    Outlink other = (Outlink) o;    return this.toUrl.equals(other.toUrl) && this.anchor.equals(other.anchor);}
0
public String toString()
{    StringBuffer repr = new StringBuffer("toUrl: ");    repr.append(toUrl);    repr.append(" anchor: ");    repr.append(anchor);    if (md != null && !md.isEmpty()) {        for (Entry<Writable, Writable> e : md.entrySet()) {            repr.append(" ");            repr.append(e.getKey());            repr.append(": ");            repr.append(e.getValue());        }    }    return repr.toString();}
0
public int hashCode()
{    return toUrl.hashCode() ^ anchor.hashCode();}
0
public static Outlink[] getOutlinks(final String plainText, Configuration conf)
{    return OutlinkExtractor.getOutlinks(plainText, "", conf);}
0
public static Outlink[] getOutlinks(final String plainText, String anchor, Configuration conf)
{    if (plainText == null) {        return new Outlink[0];    }    long start = System.currentTimeMillis();    final List<Outlink> outlinks = new ArrayList<>();    try {        Matcher matcher = URL_PATTERN.matcher(plainText);        String url;                while (matcher.find()) {                        if (System.currentTimeMillis() - start >= 60000L) {                if (LOG.isWarnEnabled()) {                                    }                break;            }            url = matcher.group().trim();            try {                outlinks.add(new Outlink(url, anchor));            } catch (MalformedURLException mue) {                            }        }    } catch (Exception ex) {                if (LOG.isErrorEnabled()) {                    }    }    final Outlink[] retval;        if (outlinks.size() > 0) {        retval = outlinks.toArray(new Outlink[0]);    } else {        retval = new Outlink[0];    }    return retval;}
1
public ParseResult call() throws Exception
{    return p.getParse(content);}
0
public ParseStatus getStatus()
{    return status;}
0
public String getTitle()
{    return title;}
0
public Outlink[] getOutlinks()
{    return outlinks;}
0
public Metadata getContentMeta()
{    return contentMeta;}
0
public Metadata getParseMeta()
{    return parseMeta;}
0
public void setParseMeta(Metadata parseMeta)
{    this.parseMeta = parseMeta;}
0
public void setOutlinks(Outlink[] outlinks)
{    this.outlinks = outlinks;}
0
public String getMeta(String name)
{    String value = parseMeta.get(name);    if (value == null) {        value = contentMeta.get(name);    }    return value;}
0
public byte getVersion()
{    return version;}
0
public final void readFields(DataInput in) throws IOException
{    version = in.readByte();        if (version != VERSION)        throw new VersionMismatchException(VERSION, version);    status = ParseStatus.read(in);        title = Text.readString(in);    int numOutlinks = in.readInt();    outlinks = new Outlink[numOutlinks];    for (int i = 0; i < numOutlinks; i++) {        outlinks[i] = Outlink.read(in);    }    contentMeta.clear();    contentMeta.readFields(in);    parseMeta.clear();    parseMeta.readFields(in);}
0
public final void write(DataOutput out) throws IOException
{        out.writeByte(VERSION);        status.write(out);        Text.writeString(out, title);        out.writeInt(outlinks.length);    for (int i = 0; i < outlinks.length; i++) {        outlinks[i].write(out);    }        contentMeta.write(out);    parseMeta.write(out);}
0
public static ParseData read(DataInput in) throws IOException
{    ParseData parseText = new ParseData();    parseText.readFields(in);    return parseText;}
0
public boolean equals(Object o)
{    if (!(o instanceof ParseData))        return false;    ParseData other = (ParseData) o;    return this.status.equals(other.status) && this.title.equals(other.title) && Arrays.equals(this.outlinks, other.outlinks) && this.contentMeta.equals(other.contentMeta) && this.parseMeta.equals(other.parseMeta);}
0
public String toString()
{    StringBuffer buffer = new StringBuffer();    buffer.append("Version: " + version + "\n");    buffer.append("Status: " + status + "\n");    buffer.append("Title: " + title + "\n");    if (outlinks != null) {        buffer.append("Outlinks: " + outlinks.length + "\n");        for (int i = 0; i < outlinks.length; i++) {            buffer.append("  outlink: " + outlinks[i] + "\n");        }    }    buffer.append("Content Metadata: " + contentMeta + "\n");    buffer.append("Parse Metadata: " + parseMeta + "\n");    return buffer.toString();}
0
public static void main(String[] argv) throws Exception
{    String usage = "ParseData (-local | -dfs <namenode:port>) recno segment";    if (argv.length < 3) {        System.out.println("usage:" + usage);        return;    }    Options opts = new Options();    Configuration conf = NutchConfiguration.create();    GenericOptionsParser parser = new GenericOptionsParser(conf, opts, argv);    String[] remainingArgs = parser.getRemainingArgs();    try (FileSystem fs = FileSystem.get(conf)) {        int recno = Integer.parseInt(remainingArgs[0]);        String segment = remainingArgs[1];        Path file = new Path(segment, DIR_NAME);        System.out.println("Reading from file: " + file);        ArrayFile.Reader parses = new ArrayFile.Reader(fs, file.toString(), conf);        ParseData parseDatum = new ParseData();        parses.get(recno, parseDatum);        System.out.println("Retrieved " + recno + " from file " + file);        System.out.println(parseDatum);        parses.close();    }}
0
public String getText()
{    return text.getText();}
0
public ParseData getData()
{    return data;}
0
public boolean isCanonical()
{    return isCanonical;}
0
public final void write(DataOutput out) throws IOException
{    out.writeBoolean(isCanonical);    text.write(out);    data.write(out);}
0
public void readFields(DataInput in) throws IOException
{    isCanonical = in.readBoolean();    text = new ParseText();    text.readFields(in);    data = new ParseData();    data.readFields(in);}
0
public static ParseImpl read(DataInput in) throws IOException
{    ParseImpl parseImpl = new ParseImpl();    parseImpl.readFields(in);    return parseImpl;}
0
public Text getKey()
{    return key;}
0
public CrawlDatum getValue()
{    return value;}
0
public CrawlDatum setValue(CrawlDatum value)
{    this.value = value;    return this.value;}
0
public OutputCommitter getOutputCommitter(TaskAttemptContext context) throws IOException
{    Path path = FileOutputFormat.getOutputPath(context);    return new FileOutputCommitter(path, context);}
0
public void checkOutputSpecs(JobContext context) throws IOException
{    Configuration conf = context.getConfiguration();    Path out = FileOutputFormat.getOutputPath(context);    FileSystem fs = out.getFileSystem(context.getConfiguration());    if ((out == null) && (context.getNumReduceTasks() != 0)) {        throw new IOException("Output directory not set in JobContext.");    }    if (fs == null) {        fs = out.getFileSystem(conf);    }    if (fs.exists(new Path(out, CrawlDatum.PARSE_DIR_NAME))) {        throw new IOException("Segment already parsed!");    }}
0
public String getUniqueFile(TaskAttemptContext context, String name)
{    TaskID taskId = context.getTaskAttemptID().getTaskID();    int partition = taskId.getId();    StringBuilder result = new StringBuilder();    result.append(name);    result.append('-');    result.append(TaskID.getRepresentingCharacter(taskId.getTaskType()));    result.append('-');    result.append(NUMBER_FORMAT.format(partition));    return result.toString();}
0
public RecordWriter<Text, Parse> getRecordWriter(TaskAttemptContext context) throws IOException
{    Configuration conf = context.getConfiguration();    String name = getUniqueFile(context, "part");    Path dir = FileOutputFormat.getOutputPath(context);    FileSystem fs = dir.getFileSystem(context.getConfiguration());    if (conf.getBoolean("parse.filter.urls", true)) {        filters = new URLFilters(conf);        exemptionFilters = new URLExemptionFilters(conf);    }    if (conf.getBoolean("parse.normalize.urls", true)) {        normalizers = new URLNormalizers(conf, URLNormalizers.SCOPE_OUTLINK);    }    this.scfilters = new ScoringFilters(conf);    final int interval = conf.getInt("db.fetch.interval.default", 2592000);    final boolean ignoreInternalLinks = conf.getBoolean("db.ignore.internal.links", false);    final boolean ignoreExternalLinks = conf.getBoolean("db.ignore.external.links", false);    final String ignoreExternalLinksMode = conf.get("db.ignore.external.links.mode", "byHost");            final boolean storeText = conf.getBoolean("parser.store.text", true);    int maxOutlinksPerPage = conf.getInt("db.max.outlinks.per.page", 100);    final int maxOutlinks = (maxOutlinksPerPage < 0) ? Integer.MAX_VALUE : maxOutlinksPerPage;    int maxOutlinkL = conf.getInt("db.max.outlink.length", 4096);    final int maxOutlinkLength = (maxOutlinkL < 0) ? Integer.MAX_VALUE : maxOutlinkL;    final boolean isParsing = conf.getBoolean("fetcher.parse", true);    final CompressionType compType = SequenceFileOutputFormat.getOutputCompressionType(context);    Path out = FileOutputFormat.getOutputPath(context);    Path text = new Path(new Path(out, ParseText.DIR_NAME), name);    Path data = new Path(new Path(out, ParseData.DIR_NAME), name);    Path crawl = new Path(new Path(out, CrawlDatum.PARSE_DIR_NAME), name);    final String[] parseMDtoCrawlDB = conf.get("db.parsemeta.to.crawldb", "").split(" *, *");        final MapFile.Writer textOut;    if (storeText) {        Option tKeyClassOpt = (Option) MapFile.Writer.keyClass(Text.class);        org.apache.hadoop.io.SequenceFile.Writer.Option tValClassOpt = SequenceFile.Writer.valueClass(ParseText.class);        org.apache.hadoop.io.SequenceFile.Writer.Option tProgressOpt = SequenceFile.Writer.progressable((Progressable) context);        org.apache.hadoop.io.SequenceFile.Writer.Option tCompOpt = SequenceFile.Writer.compression(CompressionType.RECORD);        textOut = new MapFile.Writer(conf, text, tKeyClassOpt, tValClassOpt, tCompOpt, tProgressOpt);    } else {        textOut = null;    }        Option dKeyClassOpt = (Option) MapFile.Writer.keyClass(Text.class);    org.apache.hadoop.io.SequenceFile.Writer.Option dValClassOpt = SequenceFile.Writer.valueClass(ParseData.class);    org.apache.hadoop.io.SequenceFile.Writer.Option dProgressOpt = SequenceFile.Writer.progressable((Progressable) context);    org.apache.hadoop.io.SequenceFile.Writer.Option dCompOpt = SequenceFile.Writer.compression(compType);    final MapFile.Writer dataOut = new MapFile.Writer(conf, data, dKeyClassOpt, dValClassOpt, dCompOpt, dProgressOpt);    final SequenceFile.Writer crawlOut = SequenceFile.createWriter(conf, SequenceFile.Writer.file(crawl), SequenceFile.Writer.keyClass(Text.class), SequenceFile.Writer.valueClass(CrawlDatum.class), SequenceFile.Writer.bufferSize(fs.getConf().getInt("io.file.buffer.size", 4096)), SequenceFile.Writer.replication(fs.getDefaultReplication(crawl)), SequenceFile.Writer.blockSize(1073741824), SequenceFile.Writer.compression(compType, new DefaultCodec()), SequenceFile.Writer.progressable((Progressable) context), SequenceFile.Writer.metadata(new Metadata()));    return new RecordWriter<Text, Parse>() {        public void write(Text key, Parse parse) throws IOException {            String fromUrl = key.toString();                        String origin = null;            if (textOut != null) {                textOut.append(key, new ParseText(parse.getText()));            }            ParseData parseData = parse.getData();                        String sig = parseData.getContentMeta().get(Nutch.SIGNATURE_KEY);            if (sig != null) {                byte[] signature = StringUtil.fromHexString(sig);                if (signature != null) {                                        CrawlDatum d = new CrawlDatum(CrawlDatum.STATUS_SIGNATURE, 0);                    d.setSignature(signature);                    crawlOut.append(key, d);                }            }                                    CrawlDatum parseMDCrawlDatum = null;            for (String mdname : parseMDtoCrawlDB) {                String mdvalue = parse.getData().getParseMeta().get(mdname);                if (mdvalue != null) {                    if (parseMDCrawlDatum == null)                        parseMDCrawlDatum = new CrawlDatum(CrawlDatum.STATUS_PARSE_META, 0);                    parseMDCrawlDatum.getMetaData().put(new Text(mdname), new Text(mdvalue));                }            }            if (parseMDCrawlDatum != null)                crawlOut.append(key, parseMDCrawlDatum);                        if (ignoreExternalLinks || ignoreInternalLinks) {                URL originURL = new URL(fromUrl.toString());                                if ("bydomain".equalsIgnoreCase(ignoreExternalLinksMode)) {                    origin = URLUtil.getDomainName(originURL).toLowerCase();                } else                 {                    origin = originURL.getHost().toLowerCase();                }            }            ParseStatus pstatus = parseData.getStatus();            if (pstatus != null && pstatus.isSuccess() && pstatus.getMinorCode() == ParseStatus.SUCCESS_REDIRECT) {                String newUrl = pstatus.getMessage();                int refreshTime = Integer.valueOf(pstatus.getArgs()[1]);                newUrl = filterNormalize(fromUrl, newUrl, origin, ignoreInternalLinks, ignoreExternalLinks, ignoreExternalLinksMode, filters, exemptionFilters, normalizers, URLNormalizers.SCOPE_FETCHER);                if (newUrl != null) {                    String reprUrl = URLUtil.chooseRepr(fromUrl, newUrl, refreshTime < Fetcher.PERM_REFRESH_TIME);                    CrawlDatum newDatum = new CrawlDatum();                    newDatum.setStatus(CrawlDatum.STATUS_LINKED);                    if (reprUrl != null && !reprUrl.equals(newUrl)) {                        newDatum.getMetaData().put(Nutch.WRITABLE_REPR_URL_KEY, new Text(reprUrl));                    }                    crawlOut.append(new Text(newUrl), newDatum);                }            }                        Outlink[] links = parseData.getOutlinks();            int outlinksToStore = Math.min(maxOutlinks, links.length);            int validCount = 0;            CrawlDatum adjust = null;            List<Entry<Text, CrawlDatum>> targets = new ArrayList<>(outlinksToStore);            List<Outlink> outlinkList = new ArrayList<>(outlinksToStore);            for (int i = 0; i < links.length && validCount < outlinksToStore; i++) {                String toUrl = links[i].getToUrl();                                if (!isParsing) {                    if (toUrl.length() > maxOutlinkLength) {                        continue;                    }                    toUrl = ParseOutputFormat.filterNormalize(fromUrl, toUrl, origin, ignoreInternalLinks, ignoreExternalLinks, ignoreExternalLinksMode, filters, exemptionFilters, normalizers);                    if (toUrl == null) {                        continue;                    }                }                CrawlDatum target = new CrawlDatum(CrawlDatum.STATUS_LINKED, interval);                Text targetUrl = new Text(toUrl);                                                                MapWritable outlinkMD = links[i].getMetadata();                if (outlinkMD != null) {                    target.getMetaData().putAll(outlinkMD);                }                try {                    scfilters.initialScore(targetUrl, target);                } catch (ScoringFilterException e) {                                        target.setScore(0.0f);                }                targets.add(new SimpleEntry(targetUrl, target));                                links[i].setUrl(toUrl);                outlinkList.add(links[i]);                validCount++;            }            try {                                adjust = scfilters.distributeScoreToOutlinks(key, parseData, targets, null, links.length);            } catch (ScoringFilterException e) {                            }            for (Entry<Text, CrawlDatum> target : targets) {                crawlOut.append(target.getKey(), target.getValue());            }            if (adjust != null)                crawlOut.append(key, adjust);            Outlink[] filteredLinks = outlinkList.toArray(new Outlink[outlinkList.size()]);            parseData = new ParseData(parseData.getStatus(), parseData.getTitle(), filteredLinks, parseData.getContentMeta(), parseData.getParseMeta());            dataOut.append(key, parseData);            if (!parse.isCanonical()) {                CrawlDatum datum = new CrawlDatum();                datum.setStatus(CrawlDatum.STATUS_FETCH_SUCCESS);                String timeString = parse.getData().getContentMeta().get(Nutch.FETCH_TIME_KEY);                try {                    datum.setFetchTime(Long.parseLong(timeString));                } catch (Exception e) {                                        datum.setFetchTime(System.currentTimeMillis());                }                crawlOut.append(key, datum);            }        }        public void close(TaskAttemptContext context) throws IOException {            if (textOut != null)                textOut.close();            dataOut.close();            crawlOut.close();        }    };}
1
public void write(Text key, Parse parse) throws IOException
{    String fromUrl = key.toString();        String origin = null;    if (textOut != null) {        textOut.append(key, new ParseText(parse.getText()));    }    ParseData parseData = parse.getData();        String sig = parseData.getContentMeta().get(Nutch.SIGNATURE_KEY);    if (sig != null) {        byte[] signature = StringUtil.fromHexString(sig);        if (signature != null) {                        CrawlDatum d = new CrawlDatum(CrawlDatum.STATUS_SIGNATURE, 0);            d.setSignature(signature);            crawlOut.append(key, d);        }    }            CrawlDatum parseMDCrawlDatum = null;    for (String mdname : parseMDtoCrawlDB) {        String mdvalue = parse.getData().getParseMeta().get(mdname);        if (mdvalue != null) {            if (parseMDCrawlDatum == null)                parseMDCrawlDatum = new CrawlDatum(CrawlDatum.STATUS_PARSE_META, 0);            parseMDCrawlDatum.getMetaData().put(new Text(mdname), new Text(mdvalue));        }    }    if (parseMDCrawlDatum != null)        crawlOut.append(key, parseMDCrawlDatum);        if (ignoreExternalLinks || ignoreInternalLinks) {        URL originURL = new URL(fromUrl.toString());                if ("bydomain".equalsIgnoreCase(ignoreExternalLinksMode)) {            origin = URLUtil.getDomainName(originURL).toLowerCase();        } else         {            origin = originURL.getHost().toLowerCase();        }    }    ParseStatus pstatus = parseData.getStatus();    if (pstatus != null && pstatus.isSuccess() && pstatus.getMinorCode() == ParseStatus.SUCCESS_REDIRECT) {        String newUrl = pstatus.getMessage();        int refreshTime = Integer.valueOf(pstatus.getArgs()[1]);        newUrl = filterNormalize(fromUrl, newUrl, origin, ignoreInternalLinks, ignoreExternalLinks, ignoreExternalLinksMode, filters, exemptionFilters, normalizers, URLNormalizers.SCOPE_FETCHER);        if (newUrl != null) {            String reprUrl = URLUtil.chooseRepr(fromUrl, newUrl, refreshTime < Fetcher.PERM_REFRESH_TIME);            CrawlDatum newDatum = new CrawlDatum();            newDatum.setStatus(CrawlDatum.STATUS_LINKED);            if (reprUrl != null && !reprUrl.equals(newUrl)) {                newDatum.getMetaData().put(Nutch.WRITABLE_REPR_URL_KEY, new Text(reprUrl));            }            crawlOut.append(new Text(newUrl), newDatum);        }    }        Outlink[] links = parseData.getOutlinks();    int outlinksToStore = Math.min(maxOutlinks, links.length);    int validCount = 0;    CrawlDatum adjust = null;    List<Entry<Text, CrawlDatum>> targets = new ArrayList<>(outlinksToStore);    List<Outlink> outlinkList = new ArrayList<>(outlinksToStore);    for (int i = 0; i < links.length && validCount < outlinksToStore; i++) {        String toUrl = links[i].getToUrl();                if (!isParsing) {            if (toUrl.length() > maxOutlinkLength) {                continue;            }            toUrl = ParseOutputFormat.filterNormalize(fromUrl, toUrl, origin, ignoreInternalLinks, ignoreExternalLinks, ignoreExternalLinksMode, filters, exemptionFilters, normalizers);            if (toUrl == null) {                continue;            }        }        CrawlDatum target = new CrawlDatum(CrawlDatum.STATUS_LINKED, interval);        Text targetUrl = new Text(toUrl);                                MapWritable outlinkMD = links[i].getMetadata();        if (outlinkMD != null) {            target.getMetaData().putAll(outlinkMD);        }        try {            scfilters.initialScore(targetUrl, target);        } catch (ScoringFilterException e) {                        target.setScore(0.0f);        }        targets.add(new SimpleEntry(targetUrl, target));                links[i].setUrl(toUrl);        outlinkList.add(links[i]);        validCount++;    }    try {                adjust = scfilters.distributeScoreToOutlinks(key, parseData, targets, null, links.length);    } catch (ScoringFilterException e) {            }    for (Entry<Text, CrawlDatum> target : targets) {        crawlOut.append(target.getKey(), target.getValue());    }    if (adjust != null)        crawlOut.append(key, adjust);    Outlink[] filteredLinks = outlinkList.toArray(new Outlink[outlinkList.size()]);    parseData = new ParseData(parseData.getStatus(), parseData.getTitle(), filteredLinks, parseData.getContentMeta(), parseData.getParseMeta());    dataOut.append(key, parseData);    if (!parse.isCanonical()) {        CrawlDatum datum = new CrawlDatum();        datum.setStatus(CrawlDatum.STATUS_FETCH_SUCCESS);        String timeString = parse.getData().getContentMeta().get(Nutch.FETCH_TIME_KEY);        try {            datum.setFetchTime(Long.parseLong(timeString));        } catch (Exception e) {                        datum.setFetchTime(System.currentTimeMillis());        }        crawlOut.append(key, datum);    }}
1
public void close(TaskAttemptContext context) throws IOException
{    if (textOut != null)        textOut.close();    dataOut.close();    crawlOut.close();}
0
public static String filterNormalize(String fromUrl, String toUrl, String fromHost, boolean ignoreInternalLinks, boolean ignoreExternalLinks, String ignoreExternalLinksMode, URLFilters filters, URLExemptionFilters exemptionFilters, URLNormalizers normalizers)
{    return filterNormalize(fromUrl, toUrl, fromHost, ignoreInternalLinks, ignoreExternalLinks, ignoreExternalLinksMode, filters, exemptionFilters, normalizers, URLNormalizers.SCOPE_OUTLINK);}
0
public static String filterNormalize(String fromUrl, String toUrl, String origin, boolean ignoreInternalLinks, boolean ignoreExternalLinks, String ignoreExternalLinksMode, URLFilters filters, URLExemptionFilters exemptionFilters, URLNormalizers normalizers, String urlNormalizerScope)
{        if (fromUrl.equals(toUrl)) {        return null;    }    if (ignoreExternalLinks || ignoreInternalLinks) {        URL targetURL = null;        try {            targetURL = new URL(toUrl);        } catch (MalformedURLException e1) {                        return null;        }        if (ignoreExternalLinks) {            if ("bydomain".equalsIgnoreCase(ignoreExternalLinksMode)) {                String toDomain = URLUtil.getDomainName(targetURL).toLowerCase();                                if (toDomain == null || !toDomain.equals(origin)) {                                        return null;                }            } else {                String toHost = targetURL.getHost().toLowerCase();                if (!toHost.equals(origin)) {                                        if (                    exemptionFilters == null || !exemptionFilters.isExempted(fromUrl, toUrl)) {                                                return null;                    }                }            }        }        if (ignoreInternalLinks) {            if ("bydomain".equalsIgnoreCase(ignoreExternalLinksMode)) {                String toDomain = URLUtil.getDomainName(targetURL).toLowerCase();                                if (toDomain == null || toDomain.equals(origin)) {                                        return null;                }            } else {                String toHost = targetURL.getHost().toLowerCase();                                if (toHost == null || toHost.equals(origin)) {                                        return null;                }            }        }    }    try {        if (normalizers != null) {                        toUrl = normalizers.normalize(toUrl, urlNormalizerScope);                }        if (filters != null) {                        toUrl = filters.filter(toUrl);        }        if (toUrl == null) {            return null;        }    } catch (Exception e) {        return null;    }    return toUrl;}
0
 List<String> getPluginList(String mimeType)
{    return fMimeTypeToPluginMap.get(mimeType);}
0
 void setAliases(Map<String, String> aliases)
{    this.aliases = aliases;}
0
 Map<String, String> getAliases()
{    return aliases;}
0
 void setPluginList(String mimeType, List<String> l)
{    fMimeTypeToPluginMap.put(mimeType, l);}
0
 List<String> getSupportedMimeTypes()
{    return Arrays.asList(fMimeTypeToPluginMap.keySet().toArray(new String[] {}));}
0
public ParsePluginList parse(Configuration conf)
{    ParsePluginList pList = new ParsePluginList();        DocumentBuilderFactory factory = null;    DocumentBuilder parser = null;    Document document = null;    InputSource inputSource = null;    InputStream ppInputStream = null;    if (fParsePluginsFile != null) {        URL parsePluginUrl = null;        try {            parsePluginUrl = new URL(fParsePluginsFile);            ppInputStream = parsePluginUrl.openStream();        } catch (Exception e) {            if (LOG.isWarnEnabled()) {                            }            return pList;        }    } else {        ppInputStream = conf.getConfResourceAsInputStream(conf.get(PP_FILE_PROP));    }    inputSource = new InputSource(ppInputStream);    try {        factory = DocumentBuilderFactory.newInstance();        parser = factory.newDocumentBuilder();        document = parser.parse(inputSource);    } catch (Exception e) {        if (LOG.isWarnEnabled()) {                    }        return null;    }    Element parsePlugins = document.getDocumentElement();        Map<String, String> aliases = getAliases(parsePlugins);        pList.setAliases(aliases);        NodeList mimeTypes = parsePlugins.getElementsByTagName("mimeType");        for (int i = 0; i < mimeTypes.getLength(); i++) {        Element mimeType = (Element) mimeTypes.item(i);        String mimeTypeStr = mimeType.getAttribute("name");                NodeList pluginList = mimeType.getElementsByTagName("plugin");                if (pluginList != null && pluginList.getLength() > 0) {            List<String> plugList = new ArrayList<>(pluginList.getLength());            for (int j = 0; j < pluginList.getLength(); j++) {                Element plugin = (Element) pluginList.item(j);                String pluginId = plugin.getAttribute("id");                String extId = aliases.get(pluginId);                if (extId == null) {                                        extId = pluginId;                }                String orderStr = plugin.getAttribute("order");                int order = -1;                try {                    order = Integer.parseInt(orderStr);                } catch (NumberFormatException ignore) {                }                if (order != -1) {                    plugList.add(order - 1, extId);                } else {                    plugList.add(extId);                }            }                        pList.setPluginList(mimeTypeStr, plugList);        } else if (LOG.isWarnEnabled()) {                    }    }    return pList;}
1
public static void main(String[] args) throws Exception
{    String parsePluginFile = null;    String usage = "ParsePluginsReader [--file <parse plugin file location>]";    if ((args.length != 0 && args.length != 2) || (args.length == 2 && !"--file".equals(args[0]))) {        System.err.println(usage);        System.exit(1);    }    for (int i = 0; i < args.length; i++) {        if (args[i].equals("--file")) {            parsePluginFile = args[++i];        }    }    ParsePluginsReader reader = new ParsePluginsReader();    if (parsePluginFile != null) {        reader.setFParsePluginsFile(parsePluginFile);    }    ParsePluginList prefs = reader.parse(NutchConfiguration.create());    for (String mimeType : prefs.getSupportedMimeTypes()) {        System.out.println("MIMETYPE: " + mimeType);        List<String> plugList = prefs.getPluginList(mimeType);        System.out.println("EXTENSION IDs:");        for (String j : plugList) {            System.out.println(j);        }    }}
0
public String getFParsePluginsFile()
{    return fParsePluginsFile;}
0
public void setFParsePluginsFile(String parsePluginsFile)
{    fParsePluginsFile = parsePluginsFile;}
0
private Map<String, String> getAliases(Element parsePluginsRoot)
{    Map<String, String> aliases = new HashMap<>();    NodeList aliasRoot = parsePluginsRoot.getElementsByTagName("aliases");    if (aliasRoot == null || aliasRoot.getLength() == 0) {        if (LOG.isWarnEnabled()) {                    }        return aliases;    }    if (aliasRoot.getLength() > 1) {                if (LOG.isWarnEnabled()) {                    }    }    Element aliasRootElem = (Element) aliasRoot.item(0);    NodeList aliasElements = aliasRootElem.getElementsByTagName("alias");    if (aliasElements != null && aliasElements.getLength() > 0) {        for (int i = 0; i < aliasElements.getLength(); i++) {            Element aliasElem = (Element) aliasElements.item(i);            String parsePluginId = aliasElem.getAttribute("name");            String extensionId = aliasElem.getAttribute("extension-id");            if (LOG.isTraceEnabled()) {                LOG.trace("Found alias: plugin-id: " + parsePluginId + ", extension-id: " + extensionId);            }            if (parsePluginId != null && extensionId != null) {                aliases.put(parsePluginId, extensionId);            }        }    }    return aliases;}
1
public int run(String[] args) throws Exception
{    String url = null;    String usage =     "Usage:\n" +     "  ParserChecker [OPTIONS] <url>\n" +     "    Fetch single URL and parse it\n" +     "  ParserChecker [OPTIONS] -stdin\n" +     "    Read URLs to be parsed from stdin\n" +     "  ParserChecker [OPTIONS] -listen <port> [-keepClientCnxOpen]\n" +     "    Listen on <port> for URLs to be parsed\n" +     "Options:\n" +     "  -D<property>=<value>\tset/overwrite Nutch/Hadoop properties\n" +     "                  \t(a generic Hadoop option to be passed\n" + "                  \t before other command-specific options)\n" +     "  -normalize      \tnormalize URLs\n" +     "  -followRedirects\tfollow redirects when fetching URL\n" +     "  -dumpText       \talso show the plain-text extracted by parsers\n" +     "  -forceAs <mimeType>\tforce parsing as <mimeType>\n" + "  -md <key>=<value>\tmetadata added to CrawlDatum before parsing\n";        if (args.length < 1) {        System.err.println(usage);        System.exit(-1);    }    int numConsumed;    for (int i = 0; i < args.length; i++) {        if (args[i].equals("-normalize")) {            normalizers = new URLNormalizers(getConf(), URLNormalizers.SCOPE_DEFAULT);        } else if (args[i].equals("-followRedirects")) {            followRedirects = true;        } else if (args[i].equals("-forceAs")) {            forceAsContentType = args[++i];        } else if (args[i].equals("-dumpText")) {            dumpText = true;        } else if (args[i].equals("-md")) {            String k = null, v = null;            String nextOne = args[++i];            int firstEquals = nextOne.indexOf("=");            if (firstEquals != -1) {                k = nextOne.substring(0, firstEquals);                v = nextOne.substring(firstEquals + 1);            } else                k = nextOne;            metadata.put(k, v);        } else if ((numConsumed = super.parseArgs(args, i)) > 0) {            i += numConsumed - 1;        } else if (i != args.length - 1) {            System.err.println("ERR: Not a recognized argument: " + args[i]);            System.err.println(usage);            System.exit(-1);        } else {            url = args[i];        }    }    scfilters = new ScoringFilters(getConf());    if (url != null) {        return super.processSingle(url);    } else {                return super.run();    }}
0
protected int process(String url, StringBuilder output) throws Exception
{    if (normalizers != null) {        url = normalizers.normalize(url, URLNormalizers.SCOPE_DEFAULT);    }        CrawlDatum datum = new CrawlDatum();    Iterator<String> iter = metadata.keySet().iterator();    while (iter.hasNext()) {        String key = iter.next();        String value = metadata.get(key);        if (value == null)            value = "";        datum.getMetaData().put(new Text(key), new Text(value));    }    int maxRedirects = getConf().getInt("http.redirect.max", 3);    if (followRedirects) {        if (maxRedirects == 0) {                        maxRedirects = 3;        } else {                    }    }    ProtocolOutput protocolOutput = getProtocolOutput(url, datum);    Text turl = new Text(url);        int numRedirects = 0;    while (!protocolOutput.getStatus().isSuccess() && followRedirects && protocolOutput.getStatus().isRedirect() && maxRedirects >= numRedirects) {        String[] stuff = protocolOutput.getStatus().getArgs();        url = stuff[0];                if (normalizers != null) {            url = normalizers.normalize(url, URLNormalizers.SCOPE_DEFAULT);        }        turl.set(url);                protocolOutput = getProtocolOutput(url, datum);        numRedirects++;    }    if (!protocolOutput.getStatus().isSuccess()) {        System.err.println("Fetch failed with protocol status: " + protocolOutput.getStatus());        if (protocolOutput.getStatus().isRedirect()) {            System.err.println("Redirect(s) not handled due to configuration.");            System.err.println("Max Redirects to handle per config: " + maxRedirects);            System.err.println("Number of Redirects handled: " + numRedirects);        }        return -1;    }    Content content = protocolOutput.getContent();    if (content == null) {        output.append("No content for " + url + "\n");        return 0;    }    String contentType;    if (forceAsContentType != null) {        content.setContentType(forceAsContentType);        contentType = forceAsContentType;    } else {        contentType = content.getContentType();    }    if (contentType == null) {                return -1;    }        datum.getMetaData().put(new Text(Metadata.CONTENT_TYPE), new Text(contentType));    if (ParseSegment.isTruncated(content)) {            }        try {        scfilters.passScoreBeforeParsing(turl, datum, content);    } catch (Exception e) {        if (LOG.isWarnEnabled()) {                                }    }    ParseResult parseResult = new ParseUtil(getConf()).parse(content);    if (parseResult == null) {                return (-1);    }        byte[] signature = SignatureFactory.getSignature(getConf()).calculate(content, parseResult.get(new Text(url)));    if (LOG.isInfoEnabled()) {                            }    for (Map.Entry<Text, Parse> entry : parseResult) {        turl = entry.getKey();        Parse parse = entry.getValue();                try {            scfilters.passScoreAfterParsing(turl, content, parse);        } catch (Exception e) {            if (LOG.isWarnEnabled()) {                                            }        }        output.append(turl + "\n");        output.append(parse.getData() + "\n");        if (dumpText) {            output.append(parse.getText());        }    }    return 0;}
1
public static void main(String[] args) throws Exception
{    int res = ToolRunner.run(NutchConfiguration.create(), new ParserChecker(), args);    System.exit(res);}
0
public static ParseResult createParseResult(String url, Parse parse)
{    ParseResult parseResult = new ParseResult(url);    parseResult.put(new Text(url), new ParseText(parse.getText()), parse.getData());    return parseResult;}
0
public boolean isEmpty()
{    return parseMap.isEmpty();}
0
public int size()
{    return parseMap.size();}
0
public Parse get(String key)
{    return get(new Text(key));}
0
public Parse get(Text key)
{    return parseMap.get(key);}
0
public void put(Text key, ParseText text, ParseData data)
{    put(key.toString(), text, data);}
0
public void put(String key, ParseText text, ParseData data)
{    parseMap.put(new Text(key), new ParseImpl(text, data, key.equals(originalUrl)));}
0
public Iterator<Entry<Text, Parse>> iterator()
{    return parseMap.entrySet().iterator();}
0
public void filter()
{    for (Iterator<Entry<Text, Parse>> i = iterator(); i.hasNext(); ) {        Entry<Text, Parse> entry = i.next();        if (!entry.getValue().getData().getStatus().isSuccess()) {                        i.remove();        }    }}
1
public boolean isSuccess()
{    for (Iterator<Entry<Text, Parse>> i = iterator(); i.hasNext(); ) {        Entry<Text, Parse> entry = i.next();        if (!entry.getValue().getData().getStatus().isSuccess()) {            return false;        }    }    return true;}
0
public boolean isAnySuccess()
{    for (Iterator<Entry<Text, Parse>> i = iterator(); i.hasNext(); ) {        Entry<Text, Parse> entry = i.next();        if (entry.getValue().getData().getStatus().isSuccess()) {            return true;        }    }    return false;}
0
public Parser[] getParsers(String contentType, String url) throws ParserNotFound
{    List<Parser> parsers = null;    List<Extension> parserExts = null;    ObjectCache objectCache = ObjectCache.get(conf);                                parserExts = getExtensions(contentType);    if (parserExts == null) {        throw new ParserNotFound(url, contentType);    }    parsers = new Vector<>(parserExts.size());    for (Iterator<Extension> i = parserExts.iterator(); i.hasNext(); ) {        Extension ext = i.next();        Parser p = null;        try {                        p = (Parser) objectCache.getObject(ext.getId());            if (p == null) {                                p = (Parser) ext.getExtensionInstance();                objectCache.setObject(ext.getId(), p);            }            parsers.add(p);        } catch (PluginRuntimeException e) {            if (LOG.isWarnEnabled()) {                            }        }    }    return parsers.toArray(new Parser[] {});}
1
public Parser getParserById(String id) throws ParserNotFound
{    Extension[] extensions = this.extensionPoint.getExtensions();    Extension parserExt = null;    ObjectCache objectCache = ObjectCache.get(conf);    if (id != null) {        parserExt = getExtension(extensions, id);    }    if (parserExt == null) {        parserExt = getExtensionFromAlias(extensions, id);    }    if (parserExt == null) {        throw new ParserNotFound("No Parser Found for id [" + id + "]");    }        if (objectCache.getObject(parserExt.getId()) != null) {        return (Parser) objectCache.getObject(parserExt.getId());        } else {        try {            Parser p = (Parser) parserExt.getExtensionInstance();            objectCache.setObject(parserExt.getId(), p);            return p;        } catch (PluginRuntimeException e) {            if (LOG.isWarnEnabled()) {                            }            throw new ParserNotFound("Cannot init parser for id [" + id + "]");        }    }}
1
protected List<Extension> getExtensions(String contentType)
{    ObjectCache objectCache = ObjectCache.get(conf);        String type = null;    type = MimeUtil.cleanMimeType(contentType);    List<Extension> extensions = (List<Extension>) objectCache.getObject(type);        if (extensions == EMPTY_EXTENSION_LIST) {        return null;    }    if (extensions == null) {        extensions = findExtensions(type);        if (extensions != null) {            objectCache.setObject(type, extensions);        } else {                                    objectCache.setObject(type, EMPTY_EXTENSION_LIST);        }    }    return extensions;}
0
private List<Extension> findExtensions(String contentType)
{    Extension[] extensions = this.extensionPoint.getExtensions();        List<String> parsePluginList = this.parsePluginList.getPluginList(contentType);    List<Extension> extensionList = matchExtensions(parsePluginList, extensions, contentType);    if (extensionList != null) {        return extensionList;    }        parsePluginList = this.parsePluginList.getPluginList(DEFAULT_PLUGIN);    return matchExtensions(parsePluginList, extensions, DEFAULT_PLUGIN);}
0
private List<Extension> matchExtensions(List<String> plugins, Extension[] extensions, String contentType)
{    List<Extension> extList = new ArrayList<>();    if (plugins != null) {        for (String parsePluginId : plugins) {            Extension ext = getExtension(extensions, parsePluginId, contentType);            if (ext == null) {                                ext = getExtension(extensions, parsePluginId);                if (LOG.isWarnEnabled()) {                    if (ext != null) {                                                                                                                    } else {                                                                    }                }            }            if (ext != null) {                                extList.add(ext);            }        }    } else {        for (int i = 0; i < extensions.length; i++) {            if ("*".equals(extensions[i].getAttribute("contentType"))) {                extList.add(0, extensions[i]);            } else if (extensions[i].getAttribute("contentType") != null && contentType.matches(escapeContentType(extensions[i].getAttribute("contentType")))) {                extList.add(extensions[i]);            }        }        if (extList.size() > 0) {            if (LOG.isInfoEnabled()) {                StringBuffer extensionsIDs = new StringBuffer("[");                boolean isFirst = true;                for (Extension ext : extList) {                    if (!isFirst)                        extensionsIDs.append(" - ");                    else                        isFirst = false;                    extensionsIDs.append(ext.getId());                }                extensionsIDs.append("]");                            }        } else if (LOG.isDebugEnabled()) {                    }    }    return (extList.size() > 0) ? extList : null;}
1
private String escapeContentType(String contentType)
{        return contentType.replace("+", "\\+").replace(".", "\\.");}
0
private boolean match(Extension extension, String id, String type)
{    return ((id.equals(extension.getId())) && (extension.getAttribute("contentType").equals("*") || type.matches(escapeContentType(extension.getAttribute("contentType"))) || type.equals(DEFAULT_PLUGIN)));}
0
private Extension getExtension(Extension[] list, String id, String type)
{    for (int i = 0; i < list.length; i++) {        if (match(list[i], id, type)) {            return list[i];        }    }    return null;}
0
private Extension getExtension(Extension[] list, String id)
{    for (int i = 0; i < list.length; i++) {        if (id.equals(list[i].getId())) {            return list[i];        }    }    return null;}
0
private Extension getExtensionFromAlias(Extension[] list, String id)
{    return getExtension(list, parsePluginList.getAliases().get(id));}
0
public String getUrl()
{    return url;}
0
public String getContentType()
{    return contentType;}
0
public void setup(Mapper<WritableComparable<?>, Content, Text, ParseImpl>.Context context)
{    Configuration conf = context.getConfiguration();    scfilters = new ScoringFilters(conf);    skipTruncated = conf.getBoolean(SKIP_TRUNCATED, true);}
0
public void cleanup(Context context)
{}
0
public void map(WritableComparable<?> key, Content content, Context context) throws IOException, InterruptedException
{        if (key instanceof Text) {        newKey.set(key.toString());        key = newKey;    }    String fetchStatus = content.getMetadata().get(Nutch.FETCH_STATUS_KEY);    if (fetchStatus == null) {                        return;    } else if (Integer.parseInt(fetchStatus) != CrawlDatum.STATUS_FETCH_SUCCESS) {                        return;    }    if (skipTruncated && isTruncated(content)) {        return;    }    long start = System.currentTimeMillis();    ParseResult parseResult = null;    try {        if (parseUtil == null)            parseUtil = new ParseUtil(context.getConfiguration());        parseResult = parseUtil.parse(content);    } catch (Exception e) {                return;    }    for (Entry<Text, Parse> entry : parseResult) {        Text url = entry.getKey();        Parse parse = entry.getValue();        ParseStatus parseStatus = parse.getData().getStatus();        context.getCounter("ParserStatus", ParseStatus.majorCodes[parseStatus.getMajorCode()]).increment(1);        if (!parseStatus.isSuccess()) {                        parse = parseStatus.getEmptyParse(context.getConfiguration());        }                parse.getData().getContentMeta().set(Nutch.SEGMENT_NAME_KEY, context.getConfiguration().get(Nutch.SEGMENT_NAME_KEY));                byte[] signature = SignatureFactory.getSignature(context.getConfiguration()).calculate(content, parse);        parse.getData().getContentMeta().set(Nutch.SIGNATURE_KEY, StringUtil.toHexString(signature));        try {            scfilters.passScoreAfterParsing(url, content, parse);        } catch (ScoringFilterException e) {            if (LOG.isWarnEnabled()) {                            }        }        long end = System.currentTimeMillis();                context.write(url, new ParseImpl(new ParseText(parse.getText()), parse.getData(), parse.isCanonical()));    }}
1
public static boolean isTruncated(Content content)
{    byte[] contentBytes = content.getContent();    if (contentBytes == null)        return false;    Metadata metadata = content.getMetadata();    if (metadata == null)        return false;    String lengthStr = metadata.get(Response.CONTENT_LENGTH);    if (lengthStr != null)        lengthStr = lengthStr.trim();    if (StringUtil.isEmpty(lengthStr)) {        return false;    }    int inHeaderSize;    String url = content.getUrl();    try {        inHeaderSize = Integer.parseInt(lengthStr);    } catch (NumberFormatException e) {                return false;    }    int actualSize = contentBytes.length;    if (inHeaderSize > actualSize) {                return true;    }    if (LOG.isDebugEnabled()) {            }    return false;}
1
public void reduce(Text key, Iterable<Writable> values, Context context) throws IOException, InterruptedException
{    Iterator<Writable> valuesIter = values.iterator();        context.write(key, valuesIter.next());}
0
public void parse(Path segment) throws IOException, InterruptedException, ClassNotFoundException
{    if (SegmentChecker.isParsed(segment, segment.getFileSystem(getConf()))) {                return;    }    SimpleDateFormat sdf = new SimpleDateFormat("yyyy-MM-dd HH:mm:ss");    long start = System.currentTimeMillis();    if (LOG.isInfoEnabled()) {                    }    Job job = NutchJob.getInstance(getConf());    job.setJobName("parse " + segment);    Configuration conf = job.getConfiguration();    FileInputFormat.addInputPath(job, new Path(segment, Content.DIR_NAME));    conf.set(Nutch.SEGMENT_NAME_KEY, segment.getName());    job.setInputFormatClass(SequenceFileInputFormat.class);    job.setJarByClass(ParseSegment.class);    job.setMapperClass(ParseSegment.ParseSegmentMapper.class);    job.setReducerClass(ParseSegment.ParseSegmentReducer.class);    FileOutputFormat.setOutputPath(job, segment);    job.setOutputFormatClass(ParseOutputFormat.class);    job.setOutputKeyClass(Text.class);    job.setOutputValueClass(ParseImpl.class);    try {        boolean success = job.waitForCompletion(true);        if (!success) {            String message = "Parse job did not succeed, job status:" + job.getStatus().getState() + ", reason: " + job.getStatus().getFailureInfo();                        throw new RuntimeException(message);        }    } catch (IOException | InterruptedException | ClassNotFoundException e) {                throw e;    }    long end = System.currentTimeMillis();    }
1
public static void main(String[] args) throws Exception
{    int res = ToolRunner.run(NutchConfiguration.create(), new ParseSegment(), args);    System.exit(res);}
0
public int run(String[] args) throws Exception
{    Path segment;    String usage = "Usage: ParseSegment segment [-noFilter] [-noNormalize]";    if (args.length == 0) {        System.err.println(usage);        System.exit(-1);    }    if (args.length > 1) {        for (int i = 1; i < args.length; i++) {            String param = args[i];            if ("-nofilter".equalsIgnoreCase(param)) {                getConf().setBoolean("parse.filter.urls", false);            } else if ("-nonormalize".equalsIgnoreCase(param)) {                getConf().setBoolean("parse.normalize.urls", false);            }        }    }    segment = new Path(args[0]);    parse(segment);    return 0;}
0
public Map<String, Object> run(Map<String, Object> args, String crawlId) throws Exception
{    Map<String, Object> results = new HashMap<>();    Path segment = null;    if (args.containsKey(Nutch.ARG_SEGMENTS)) {        Object seg = args.get(Nutch.ARG_SEGMENTS);        if (seg instanceof Path) {            segment = (Path) seg;        } else if (seg instanceof String) {            segment = new Path(seg.toString());        } else if (seg instanceof ArrayList) {            String[] segmentsArray = (String[]) seg;            segment = new Path(segmentsArray[0].toString());            if (segmentsArray.length > 1) {                            }        }    } else {        String segment_dir = crawlId + "/segments";        File segmentsDir = new File(segment_dir);        File[] segmentsList = segmentsDir.listFiles();        Arrays.sort(segmentsList, (f1, f2) -> {            if (f1.lastModified() > f2.lastModified())                return -1;            else                return 0;        });        segment = new Path(segmentsList[0].getPath());    }    if (args.containsKey("nofilter")) {        getConf().setBoolean("parse.filter.urls", false);    }    if (args.containsKey("nonormalize")) {        getConf().setBoolean("parse.normalize.urls", false);    }    parse(segment);    results.put(Nutch.VAL_RESULT, Integer.toString(0));    return results;}
1
public byte getVersion()
{    return VERSION;}
0
public static ParseStatus read(DataInput in) throws IOException
{    ParseStatus res = new ParseStatus();    res.readFields(in);    return res;}
0
public void readFields(DataInput in) throws IOException
{    byte version = in.readByte();    switch(version) {        case 1:            majorCode = in.readByte();            minorCode = in.readShort();            args = WritableUtils.readCompressedStringArray(in);            break;        case 2:            majorCode = in.readByte();            minorCode = in.readShort();            args = WritableUtils.readStringArray(in);            break;        default:            throw new VersionMismatchException(VERSION, version);    }}
0
public void write(DataOutput out) throws IOException
{    out.writeByte(VERSION);    out.writeByte(majorCode);    out.writeShort(minorCode);    if (args == null) {        out.writeInt(-1);    } else {        WritableUtils.writeStringArray(out, args);    }}
0
public boolean isSuccess()
{    return majorCode == SUCCESS;}
0
public String getMessage()
{    if (args != null && args.length > 0 && args[0] != null)        return args[0];    return null;}
0
public String[] getArgs()
{    return args;}
0
public int getMajorCode()
{    return majorCode;}
0
public int getMinorCode()
{    return minorCode;}
0
public Parse getEmptyParse(Configuration conf)
{    return new EmptyParseImpl(this, conf);}
0
public ParseResult getEmptyParseResult(String url, Configuration conf)
{    return ParseResult.createParseResult(url, getEmptyParse(conf));}
0
public String toString()
{    StringBuffer res = new StringBuffer();    String name = null;    if (majorCode >= 0 && majorCode < majorCodes.length)        name = majorCodes[majorCode];    else        name = "UNKNOWN!";    res.append(name + "(" + majorCode + "," + minorCode + ")");    if (args != null) {        if (args.length == 1) {            res.append(": " + String.valueOf(args[0]));        } else {            for (int i = 0; i < args.length; i++) {                if (args[i] != null)                    res.append(", args[" + i + "]=" + String.valueOf(args[i]));            }        }    }    return res.toString();}
0
public void setArgs(String[] args)
{    this.args = args;}
0
public void setMessage(String msg)
{    if (args == null || args.length == 0) {        args = new String[1];    }    args[0] = msg;}
0
public void setMajorCode(byte majorCode)
{    this.majorCode = majorCode;}
0
public void setMinorCode(short minorCode)
{    this.minorCode = minorCode;}
0
public boolean equals(Object o)
{    if (o == null)        return false;    if (!(o instanceof ParseStatus))        return false;    boolean res = true;    ParseStatus other = (ParseStatus) o;    res = res && (this.majorCode == other.majorCode) && (this.minorCode == other.minorCode);    if (!res)        return res;    if (this.args == null) {        if (other.args == null)            return true;        else            return false;    } else {        if (other.args == null)            return false;        if (other.args.length != this.args.length)            return false;        for (int i = 0; i < this.args.length; i++) {            if (!this.args[i].equals(other.args[i]))                return false;        }    }    return true;}
0
public ParseData getData()
{    return data;}
0
public String getText()
{    return "";}
0
public boolean isCanonical()
{    return true;}
0
public void readFields(DataInput in) throws IOException
{    byte version = in.readByte();    switch(version) {        case 1:            text = WritableUtils.readCompressedString(in);            break;        case VERSION:            text = Text.readString(in);            break;        default:            throw new VersionMismatchException(VERSION, version);    }}
0
public final void write(DataOutput out) throws IOException
{    out.write(VERSION);    Text.writeString(out, text);}
0
public static final ParseText read(DataInput in) throws IOException
{    ParseText parseText = new ParseText();    parseText.readFields(in);    return parseText;}
0
public String getText()
{    return text;}
0
public boolean equals(Object o)
{    if (!(o instanceof ParseText))        return false;    ParseText other = (ParseText) o;    return this.text.equals(other.text);}
0
public String toString()
{    return text;}
0
public static void main(String[] argv) throws Exception
{    String usage = "ParseText (-local | -dfs <namenode:port>) recno segment";    if (argv.length < 3) {        System.out.println("usage:" + usage);        return;    }    Options opts = new Options();    Configuration conf = NutchConfiguration.create();    GenericOptionsParser parser = new GenericOptionsParser(conf, opts, argv);    String[] remainingArgs = parser.getRemainingArgs();    try (FileSystem fs = FileSystem.get(conf)) {        int recno = Integer.parseInt(remainingArgs[0]);        String segment = remainingArgs[1];        String filename = new Path(segment, ParseText.DIR_NAME).toString();        ParseText parseText = new ParseText();        ArrayFile.Reader parseTexts = new ArrayFile.Reader(fs, filename, conf);        parseTexts.get(recno, parseText);        System.out.println("Retrieved " + recno + " from file " + filename);        System.out.println(parseText);        parseTexts.close();    }}
0
public ParseResult parse(Content content) throws ParseException
{    Parser[] parsers = null;    try {        parsers = this.parserFactory.getParsers(content.getContentType(), content.getUrl() != null ? content.getUrl() : "");    } catch (ParserNotFound e) {        if (LOG.isWarnEnabled()) {                    }        throw new ParseException(e.getMessage());    }    ParseResult parseResult = null;    for (int i = 0; i < parsers.length; i++) {        if (LOG.isDebugEnabled()) {                    }        if (maxParseTime != -1) {            parseResult = runParser(parsers[i], content);        } else {            try {                parseResult = parsers[i].getParse(content);            } catch (Throwable e) {                            }        }        if (parseResult != null && parseResult.isAnySuccess()) {            return parseResult;        }        }        if (parseResult != null && !parseResult.isEmpty()) {        return parseResult;    }    if (LOG.isWarnEnabled()) {            }    return new ParseStatus(new ParseException("Unable to successfully parse content")).getEmptyParseResult(content.getUrl(), null);}
1
public ParseResult parseByExtensionId(String extId, Content content) throws ParseException
{    Parser p = null;    try {        p = this.parserFactory.getParserById(extId);    } catch (ParserNotFound e) {        if (LOG.isWarnEnabled()) {                    }        throw new ParseException(e.getMessage());    }    ParseResult parseResult = null;    if (maxParseTime != -1) {        parseResult = runParser(p, content);    } else {        try {            parseResult = p.getParse(content);        } catch (Throwable e) {                    }    }    if (parseResult != null && !parseResult.isEmpty()) {        return parseResult;    } else {        if (LOG.isWarnEnabled()) {                    }        return new ParseStatus(new ParseException("Unable to successfully parse content")).getEmptyParseResult(content.getUrl(), null);    }}
1
private ParseResult runParser(Parser p, Content content)
{    ParseCallable pc = new ParseCallable(p, content);    Future<ParseResult> task = executorService.submit(pc);    ParseResult res = null;    try {        res = task.get(maxParseTime, TimeUnit.SECONDS);    } catch (Exception e) {                task.cancel(true);    } finally {        pc = null;    }    return res;}
1
private void setExtensionPoint(String point)
{    fTargetPoint = point;}
0
public String getAttribute(String pKey)
{    return fAttributes.get(pKey);}
0
public String getClazz()
{    return fClazz;}
0
public String getId()
{    return fId;}
0
public void addAttribute(String pKey, String pValue)
{    fAttributes.put(pKey, pValue);}
0
public void setClazz(String extensionClazz)
{    fClazz = extensionClazz;}
0
public void setId(String extensionID)
{    fId = extensionID;}
0
public String getTargetPoint()
{    return fTargetPoint;}
0
public Object getExtensionInstance() throws PluginRuntimeException
{        synchronized (getId()) {        try {            PluginRepository pluginRepository = PluginRepository.get(conf);            Class<?> extensionClazz = pluginRepository.getCachedClass(fDescriptor, getClazz());                                    pluginRepository.getPluginInstance(getDescriptor());            Object object = null;            try {                object = extensionClazz.getConstructor().newInstance();            } catch (IllegalArgumentException | InvocationTargetException | NoSuchMethodException | SecurityException e) {                e.printStackTrace();            }            if (object != null && object instanceof Configurable) {                ((Configurable) object).setConf(this.conf);            }            return object;        } catch (ClassNotFoundException e) {            throw new PluginRuntimeException(e);        } catch (InstantiationException e) {            throw new PluginRuntimeException(e);        } catch (IllegalAccessException e) {            throw new PluginRuntimeException(e);        }    }}
0
public PluginDescriptor getDescriptor()
{    return fDescriptor;}
0
public void setDescriptor(PluginDescriptor pDescriptor)
{    fDescriptor = pDescriptor;}
0
public String toString()
{    return getId() + ", " + getClazz() + ", " + getTargetPoint();}
0
public String getId()
{    return ftId;}
0
public String getName()
{    return fName;}
0
public String getSchema()
{    return fSchema;}
0
private void setId(String pId)
{    ftId = pId;}
0
private void setName(String pName)
{    fName = pName;}
0
private void setSchema(String pSchema)
{    fSchema = pSchema;}
0
public void addExtension(Extension extension)
{    fExtensions.add(extension);}
0
public Extension[] getExtensions()
{    return fExtensions.toArray(new Extension[fExtensions.size()]);}
0
public void startUp() throws PluginRuntimeException
{}
0
public void shutDown() throws PluginRuntimeException
{}
0
public PluginDescriptor getDescriptor()
{    return fDescriptor;}
0
private void setDescriptor(PluginDescriptor descriptor)
{    fDescriptor = descriptor;}
0
protected void finalize() throws Throwable
{    super.finalize();    shutDown();}
0
protected synchronized Class<?> loadClass(String name, boolean resolve) throws ClassNotFoundException
{        Class<?> c = findLoadedClass(name);    if (c == null) {        try {                        c = findClass(name);        } catch (ClassNotFoundException | SecurityException e) {            c = loadClassFromParent(name, resolve);        }    }    if (resolve) {        resolveClass(c);    }    return c;}
0
private Class<?> loadClassFromParent(String name, boolean resolve) throws ClassNotFoundException
{                Class<?> c;    try {        c = super.loadClass(name, resolve);    } catch (ClassNotFoundException e) {        c = loadClassFromSystem(name);    } catch (SecurityException e) {        c = loadClassFromSystem(name);    }    return c;}
0
private Class<?> loadClassFromSystem(String name) throws ClassNotFoundException
{    Class<?> c = null;    if (system != null) {                c = system.loadClass(name);    }    return c;}
0
public URL getResource(String name)
{    URL url = findResource(name);    if (url == null)        url = super.getResource(name);    if (url == null && system != null)        url = system.getResource(name);    return url;}
0
public Enumeration<URL> getResources(String name) throws IOException
{    /**     * Similar to super, but local resources are enumerated before parent     * resources     */    Enumeration<URL> systemUrls = null;    if (system != null) {        systemUrls = system.getResources(name);    }    Enumeration<URL> localUrls = findResources(name);    Enumeration<URL> parentUrls = null;    if (getParent() != null) {        parentUrls = getParent().getResources(name);    }    final List<URL> urls = new ArrayList<URL>();    if (localUrls != null) {        while (localUrls.hasMoreElements()) {            URL local = localUrls.nextElement();            urls.add(local);        }    }    if (systemUrls != null) {        while (systemUrls.hasMoreElements()) {            urls.add(systemUrls.nextElement());        }    }    if (parentUrls != null) {        while (parentUrls.hasMoreElements()) {            urls.add(parentUrls.nextElement());        }    }    return new Enumeration<URL>() {        Iterator<URL> iter = urls.iterator();        public boolean hasMoreElements() {            return iter.hasNext();        }        public URL nextElement() {            return iter.next();        }    };}
0
public boolean hasMoreElements()
{    return iter.hasNext();}
0
public URL nextElement()
{    return iter.next();}
0
public InputStream getResourceAsStream(String name)
{    URL url = getResource(name);    try {        return url != null ? url.openStream() : null;    } catch (IOException e) {    }    return null;}
0
public int hashCode()
{    final int PRIME = 31;    int result = 1;    result = PRIME * result + ((parent == null) ? 0 : parent.hashCode());    result = PRIME * result + Arrays.hashCode(urls);    return result;}
0
public boolean equals(Object obj)
{    if (this == obj)        return true;    if (obj == null)        return false;    if (getClass() != obj.getClass())        return false;    final PluginClassLoader other = (PluginClassLoader) obj;    if (parent == null) {        if (other.parent != null)            return false;    } else if (!parent.equals(other.parent))        return false;    if (!Arrays.equals(urls, other.urls))        return false;    return true;}
0
private void setPath(String pPath)
{    fPluginPath = pPath;}
0
public String getName()
{    return fName;}
0
private void setProvidername(String providerName)
{    fProviderName = providerName;}
0
private void setName(String name)
{    fName = name;}
0
private void setVersion(String version)
{    fVersion = version;}
0
public String getPluginClass()
{    return fPluginClass;}
0
public String getPluginId()
{    return fPluginId;}
0
public Extension[] getExtensions()
{    return fExtensions.toArray(new Extension[fExtensions.size()]);}
0
public void addExtension(Extension pExtension)
{    fExtensions.add(pExtension);}
0
private void setPluginClass(String pluginClass)
{    fPluginClass = pluginClass;}
0
private void setPluginId(String pluginId)
{    fPluginId = pluginId;}
0
public void addExtensionPoint(ExtensionPoint extensionPoint)
{    fExtensionPoints.add(extensionPoint);}
0
public ExtensionPoint[] getExtenstionPoints()
{    return fExtensionPoints.toArray(new ExtensionPoint[fExtensionPoints.size()]);}
0
public String[] getDependencies()
{    return fDependencies.toArray(new String[fDependencies.size()]);}
0
public void addDependency(String pId)
{    fDependencies.add(pId);}
0
public void addExportedLibRelative(String pLibPath) throws MalformedURLException
{    URI uri = new File(getPluginPath() + File.separator + pLibPath).toURI();    URL url = uri.toURL();    fExportedLibs.add(url);}
0
public String getPluginPath()
{    return fPluginPath;}
0
public URL[] getExportedLibUrls()
{    return fExportedLibs.toArray(new URL[0]);}
0
public void addNotExportedLibRelative(String pLibPath) throws MalformedURLException
{    URI uri = new File(getPluginPath() + File.separator + pLibPath).toURI();    URL url = uri.toURL();    fNotExportedLibs.add(url);}
0
public URL[] getNotExportedLibUrls()
{    return fNotExportedLibs.toArray(new URL[fNotExportedLibs.size()]);}
0
public PluginClassLoader getClassLoader()
{    if (fClassLoader != null)        return fClassLoader;    ArrayList<URL> arrayList = new ArrayList<>();    arrayList.addAll(fExportedLibs);    arrayList.addAll(fNotExportedLibs);    arrayList.addAll(getDependencyLibs());    File file = new File(getPluginPath());    try {        for (File file2 : file.listFiles()) {            if (file2.getAbsolutePath().endsWith("properties"))                arrayList.add(file2.getParentFile().toURI().toURL());        }    } catch (MalformedURLException e) {            }    URL[] urls = arrayList.toArray(new URL[arrayList.size()]);    fClassLoader = new PluginClassLoader(urls, PluginDescriptor.class.getClassLoader());    return fClassLoader;}
1
private ArrayList<URL> getDependencyLibs()
{    ArrayList<URL> list = new ArrayList<>();    collectLibs(list, this);    return list;}
0
private void collectLibs(ArrayList<URL> pLibs, PluginDescriptor pDescriptor)
{    for (String id : pDescriptor.getDependencies()) {        PluginDescriptor descriptor = PluginRepository.get(fConf).getPluginDescriptor(id);        for (URL url : descriptor.getExportedLibUrls()) {            pLibs.add(url);        }        collectLibs(pLibs, descriptor);    }}
0
public String getResourceString(String pKey, Locale pLocale) throws IOException
{    if (fMessages.containsKey(pLocale.toString())) {        ResourceBundle bundle = fMessages.get(pLocale.toString());        try {            return bundle.getString(pKey);        } catch (MissingResourceException e) {            return '!' + pKey + '!';        }    }    try {        ResourceBundle res = ResourceBundle.getBundle("messages", pLocale, getClassLoader());        return res.getString(pKey);    } catch (MissingResourceException x) {        return '!' + pKey + '!';    }}
0
public String getProviderName()
{    return fProviderName;}
0
public String getVersion()
{    return fVersion;}
0
public Map<String, PluginDescriptor> parsePluginFolder(String[] pluginFolders)
{    Map<String, PluginDescriptor> map = new HashMap<>();    if (pluginFolders == null) {        throw new IllegalArgumentException("plugin.folders is not defined");    }    for (String name : pluginFolders) {        File directory = getPluginFolder(name);        if (directory == null) {            continue;        }                for (File oneSubFolder : directory.listFiles()) {            if (oneSubFolder.isDirectory()) {                String manifestPath = oneSubFolder.getAbsolutePath() + File.separator + "plugin.xml";                try {                                        PluginDescriptor p = parseManifestFile(manifestPath);                    map.put(p.getPluginId(), p);                } catch (Exception e) {                                    }            }        }    }    return map;}
1
public File getPluginFolder(String name)
{    File directory = new File(name);    if (!directory.isAbsolute()) {        URL url = PluginManifestParser.class.getClassLoader().getResource(name);        if (url == null && directory.exists() && directory.isDirectory() && directory.listFiles().length > 0) {                        return directory;        } else if (url == null) {                        return null;        } else if (!"file".equals(url.getProtocol())) {                        return null;        }        String path = url.getPath();        if (        WINDOWS && path.startsWith("/"))            path = path.substring(1);        try {                        path = URLDecoder.decode(path, "UTF-8");        } catch (UnsupportedEncodingException e) {        }        directory = new File(path);    } else if (!directory.exists()) {                return null;    }    return directory;}
1
private PluginDescriptor parseManifestFile(String pManifestPath) throws MalformedURLException, SAXException, IOException, ParserConfigurationException
{    Document document = parseXML(new File(pManifestPath).toURI().toURL());    String pPath = new File(pManifestPath).getParent();    return parsePlugin(document, pPath);}
0
private Document parseXML(URL url) throws SAXException, IOException, ParserConfigurationException
{    DocumentBuilderFactory factory = DocumentBuilderFactory.newInstance();    DocumentBuilder builder = factory.newDocumentBuilder();    return builder.parse(url.openStream());}
0
private PluginDescriptor parsePlugin(Document pDocument, String pPath) throws MalformedURLException
{    Element rootElement = pDocument.getDocumentElement();    String id = rootElement.getAttribute(ATTR_ID);    String name = rootElement.getAttribute(ATTR_NAME);    String version = rootElement.getAttribute("version");    String providerName = rootElement.getAttribute("provider-name");    String pluginClazz = null;    if (rootElement.getAttribute(ATTR_CLASS).trim().length() > 0) {        pluginClazz = rootElement.getAttribute(ATTR_CLASS);    }    PluginDescriptor pluginDescriptor = new PluginDescriptor(id, version, name, providerName, pluginClazz, pPath, this.conf);        parseExtension(rootElement, pluginDescriptor);    parseExtensionPoints(rootElement, pluginDescriptor);    parseLibraries(rootElement, pluginDescriptor);    parseRequires(rootElement, pluginDescriptor);    return pluginDescriptor;}
1
private void parseRequires(Element pRootElement, PluginDescriptor pDescriptor) throws MalformedURLException
{    NodeList nodelist = pRootElement.getElementsByTagName("requires");    if (nodelist.getLength() > 0) {        Element requires = (Element) nodelist.item(0);        NodeList imports = requires.getElementsByTagName("import");        for (int i = 0; i < imports.getLength(); i++) {            Element anImport = (Element) imports.item(i);            String plugin = anImport.getAttribute("plugin");            if (plugin != null) {                pDescriptor.addDependency(plugin);            }        }    }}
0
private void parseLibraries(Element pRootElement, PluginDescriptor pDescriptor) throws MalformedURLException
{    NodeList nodelist = pRootElement.getElementsByTagName("runtime");    if (nodelist.getLength() > 0) {        Element runtime = (Element) nodelist.item(0);        NodeList libraries = runtime.getElementsByTagName("library");        for (int i = 0; i < libraries.getLength(); i++) {            Element library = (Element) libraries.item(i);            String libName = library.getAttribute(ATTR_NAME);            NodeList list = library.getElementsByTagName("export");            Element exportElement = (Element) list.item(0);            if (exportElement != null)                pDescriptor.addExportedLibRelative(libName);            else                pDescriptor.addNotExportedLibRelative(libName);        }    }}
0
private void parseExtensionPoints(Element pRootElement, PluginDescriptor pPluginDescriptor)
{    NodeList list = pRootElement.getElementsByTagName("extension-point");    if (list != null) {        for (int i = 0; i < list.getLength(); i++) {            Element oneExtensionPoint = (Element) list.item(i);            String id = oneExtensionPoint.getAttribute(ATTR_ID);            String name = oneExtensionPoint.getAttribute(ATTR_NAME);            String schema = oneExtensionPoint.getAttribute("schema");            ExtensionPoint extensionPoint = new ExtensionPoint(id, name, schema);            pPluginDescriptor.addExtensionPoint(extensionPoint);        }    }}
0
private void parseExtension(Element pRootElement, PluginDescriptor pPluginDescriptor)
{    NodeList extensions = pRootElement.getElementsByTagName("extension");    if (extensions != null) {        for (int i = 0; i < extensions.getLength(); i++) {            Element oneExtension = (Element) extensions.item(i);            String pointId = oneExtension.getAttribute("point");            NodeList extensionImplementations = oneExtension.getChildNodes();            if (extensionImplementations != null) {                for (int j = 0; j < extensionImplementations.getLength(); j++) {                    Node node = extensionImplementations.item(j);                    if (!node.getNodeName().equals("implementation")) {                        continue;                    }                    Element oneImplementation = (Element) node;                    String id = oneImplementation.getAttribute(ATTR_ID);                    String extensionClass = oneImplementation.getAttribute(ATTR_CLASS);                                        Extension extension = new Extension(pPluginDescriptor, pointId, id, extensionClass, this.conf, this.pluginRepository);                    NodeList parameters = oneImplementation.getElementsByTagName("parameter");                    if (parameters != null) {                        for (int k = 0; k < parameters.getLength(); k++) {                            Element param = (Element) parameters.item(k);                            extension.addAttribute(param.getAttribute(ATTR_NAME), param.getAttribute("value"));                        }                    }                    pPluginDescriptor.addExtension(extension);                }            }        }    }}
1
public static synchronized PluginRepository get(Configuration conf)
{    String uuid = NutchConfiguration.getUUID(conf);    if (uuid == null) {                uuid = "nonNutchConf@" + conf.hashCode();    }    PluginRepository result = CACHE.get(uuid);    if (result == null) {        result = new PluginRepository(conf);        CACHE.put(uuid, result);    }    return result;}
0
private void installExtensionPoints(List<PluginDescriptor> plugins)
{    if (plugins == null) {        return;    }    for (PluginDescriptor plugin : plugins) {        for (ExtensionPoint point : plugin.getExtenstionPoints()) {            String xpId = point.getId();                        fExtensionPoints.put(xpId, point);        }    }}
1
private void installExtensions(List<PluginDescriptor> pRegisteredPlugins) throws PluginRuntimeException
{    for (PluginDescriptor descriptor : pRegisteredPlugins) {        for (Extension extension : descriptor.getExtensions()) {            String xpId = extension.getTargetPoint();            ExtensionPoint point = getExtensionPoint(xpId);            if (point == null) {                throw new PluginRuntimeException("Plugin (" + descriptor.getPluginId() + "), " + "extension point: " + xpId + " does not exist.");            }            point.addExtension(extension);        }    }}
0
private void getPluginCheckedDependencies(PluginDescriptor plugin, Map<String, PluginDescriptor> plugins, Map<String, PluginDescriptor> dependencies, Map<String, PluginDescriptor> branch) throws MissingDependencyException, CircularDependencyException
{    if (dependencies == null) {        dependencies = new HashMap<>();    }    if (branch == null) {        branch = new HashMap<>();    }    branch.put(plugin.getPluginId(), plugin);        for (String id : plugin.getDependencies()) {        PluginDescriptor dependency = plugins.get(id);        if (dependency == null) {            throw new MissingDependencyException("Missing dependency " + id + " for plugin " + plugin.getPluginId());        }        if (branch.containsKey(id)) {            throw new CircularDependencyException("Circular dependency detected " + id + " for plugin " + plugin.getPluginId());        }        dependencies.put(id, dependency);        getPluginCheckedDependencies(plugins.get(id), plugins, dependencies, branch);    }    branch.remove(plugin.getPluginId());}
0
private Map<String, PluginDescriptor> getPluginCheckedDependencies(PluginDescriptor plugin, Map<String, PluginDescriptor> plugins) throws MissingDependencyException, CircularDependencyException
{    Map<String, PluginDescriptor> dependencies = new HashMap<>();    Map<String, PluginDescriptor> branch = new HashMap<>();    getPluginCheckedDependencies(plugin, plugins, dependencies, branch);    return dependencies;}
0
private List<PluginDescriptor> getDependencyCheckedPlugins(Map<String, PluginDescriptor> filtered, Map<String, PluginDescriptor> all)
{    if (filtered == null) {        return null;    }    Map<String, PluginDescriptor> checked = new HashMap<>();    for (PluginDescriptor plugin : filtered.values()) {        try {            checked.putAll(getPluginCheckedDependencies(plugin, all));            checked.put(plugin.getPluginId(), plugin);        } catch (MissingDependencyException mde) {                                } catch (CircularDependencyException cde) {                                }    }    return new ArrayList<>(checked.values());}
1
public PluginDescriptor[] getPluginDescriptors()
{    return fRegisteredPlugins.toArray(new PluginDescriptor[fRegisteredPlugins.size()]);}
0
public PluginDescriptor getPluginDescriptor(String pPluginId)
{    for (PluginDescriptor descriptor : fRegisteredPlugins) {        if (descriptor.getPluginId().equals(pPluginId))            return descriptor;    }    return null;}
0
public ExtensionPoint getExtensionPoint(String pXpId)
{    return this.fExtensionPoints.get(pXpId);}
0
public Plugin getPluginInstance(PluginDescriptor pDescriptor) throws PluginRuntimeException
{    if (fActivatedPlugins.containsKey(pDescriptor.getPluginId()))        return fActivatedPlugins.get(pDescriptor.getPluginId());    try {                synchronized (pDescriptor) {            Class<?> pluginClass = getCachedClass(pDescriptor, pDescriptor.getPluginClass());            Constructor<?> constructor = pluginClass.getConstructor(new Class<?>[] { PluginDescriptor.class, Configuration.class });            Plugin plugin = (Plugin) constructor.newInstance(new Object[] { pDescriptor, this.conf });            plugin.startUp();            fActivatedPlugins.put(pDescriptor.getPluginId(), plugin);            return plugin;        }    } catch (ClassNotFoundException e) {        throw new PluginRuntimeException(e);    } catch (InstantiationException e) {        throw new PluginRuntimeException(e);    } catch (IllegalAccessException e) {        throw new PluginRuntimeException(e);    } catch (NoSuchMethodException e) {        throw new PluginRuntimeException(e);    } catch (InvocationTargetException e) {        throw new PluginRuntimeException(e);    }}
0
public void finalize() throws Throwable
{    shutDownActivatedPlugins();}
0
private void shutDownActivatedPlugins() throws PluginRuntimeException
{    for (Plugin plugin : fActivatedPlugins.values()) {        plugin.shutDown();    }}
0
public Class getCachedClass(PluginDescriptor pDescriptor, String className) throws ClassNotFoundException
{    Map<PluginClassLoader, Class> descMap = CLASS_CACHE.get(className);    if (descMap == null) {        descMap = new HashMap<>();        CLASS_CACHE.put(className, descMap);    }    PluginClassLoader loader = pDescriptor.getClassLoader();    Class clazz = descMap.get(loader);    if (clazz == null) {        clazz = loader.loadClass(className);        descMap.put(loader, clazz);    }    return clazz;}
0
private void displayStatus()
{            if ((fRegisteredPlugins == null) || (fRegisteredPlugins.size() == 0)) {            } else {        for (PluginDescriptor plugin : fRegisteredPlugins) {                    }    }        if ((fExtensionPoints == null) || (fExtensionPoints.size() == 0)) {            } else {        for (ExtensionPoint ep : fExtensionPoints.values()) {                    }    }}
1
private Map<String, PluginDescriptor> filter(Pattern excludes, Pattern includes, Map<String, PluginDescriptor> plugins)
{    Map<String, PluginDescriptor> map = new HashMap<>();    if (plugins == null) {        return map;    }    for (PluginDescriptor plugin : plugins.values()) {        if (plugin == null) {            continue;        }        String id = plugin.getPluginId();        if (id == null) {            continue;        }        if (!includes.matcher(id).matches()) {                        continue;        }        if (excludes.matcher(id).matches()) {                        continue;        }        map.put(plugin.getPluginId(), plugin);    }    return map;}
1
public synchronized Object[] getOrderedPlugins(Class<?> clazz, String xPointId, String orderProperty)
{    Object[] filters;    ObjectCache objectCache = ObjectCache.get(conf);    filters = (Object[]) objectCache.getObject(clazz.getName());    if (filters == null) {        String order = conf.get(orderProperty);        List<String> orderOfFilters = new ArrayList<>();        boolean userDefinedOrder = false;        if (order != null && !order.trim().isEmpty()) {            orderOfFilters = Arrays.asList(order.trim().split("\\s+"));            userDefinedOrder = true;        }        try {            ExtensionPoint point = PluginRepository.get(conf).getExtensionPoint(xPointId);            if (point == null)                throw new RuntimeException(xPointId + " not found.");            Extension[] extensions = point.getExtensions();            HashMap<String, Object> filterMap = new HashMap<>();            for (int i = 0; i < extensions.length; i++) {                Extension extension = extensions[i];                Object filter = extension.getExtensionInstance();                if (!filterMap.containsKey(filter.getClass().getName())) {                    filterMap.put(filter.getClass().getName(), filter);                    if (!userDefinedOrder)                        orderOfFilters.add(filter.getClass().getName());                }            }            List<Object> sorted = new ArrayList<>();            for (String orderedFilter : orderOfFilters) {                Object f = filterMap.get(orderedFilter);                if (f == null) {                                        continue;                }                sorted.add(f);            }            Object[] filter = (Object[]) Array.newInstance(clazz, sorted.size());            for (int i = 0; i < sorted.size(); i++) {                filter[i] = sorted.get(i);                if (LOG.isTraceEnabled()) {                    LOG.trace(clazz.getSimpleName() + " : filters[" + i + "] = " + filter[i].getClass());                }            }            objectCache.setObject(clazz.getName(), filter);        } catch (PluginRuntimeException e) {            throw new RuntimeException(e);        }        filters = (Object[]) objectCache.getObject(clazz.getName());    }    return filters;}
1
public static void main(String[] args) throws Exception
{    if (args.length < 2) {        System.err.println("Usage: PluginRepository pluginId className [arg1 arg2 ...]");        return;    }    Configuration conf = NutchConfiguration.create();    PluginRepository repo = new PluginRepository(conf);        PluginDescriptor d = repo.getPluginDescriptor(args[0]);    if (d == null) {        System.err.println("Plugin '" + args[0] + "' not present or inactive.");        return;    }    ClassLoader cl = d.getClassLoader();        Class<?> clazz = null;    try {        clazz = Class.forName(args[1], true, cl);    } catch (Exception e) {        System.err.println("Could not load the class '" + args[1] + ": " + e.getMessage());        return;    }    Method m = null;    try {        m = clazz.getMethod("main", new Class<?>[] { args.getClass() });    } catch (Exception e) {        System.err.println("Could not find the 'main(String[])' method in class " + args[1] + ": " + e.getMessage());        return;    }    String[] subargs = new String[args.length - 2];    System.arraycopy(args, 2, subargs, 0, subargs.length);    m.invoke(null, new Object[] { subargs });}
0
private final void readFieldsCompressed(DataInput in) throws IOException
{    byte oldVersion = in.readByte();    switch(oldVersion) {        case 0:        case 1:                        url = Text.readString(in);                        base = Text.readString(in);                        content = new byte[in.readInt()];            in.readFully(content);                        contentType = Text.readString(in);                        int keySize = in.readInt();            String key;            for (int i = 0; i < keySize; i++) {                key = Text.readString(in);                int valueSize = in.readInt();                for (int j = 0; j < valueSize; j++) {                    metadata.add(key, Text.readString(in));                }            }            break;        case 2:                        url = Text.readString(in);                        base = Text.readString(in);                        content = new byte[in.readInt()];            in.readFully(content);                        contentType = Text.readString(in);                        metadata.readFields(in);            break;        default:            throw new VersionMismatchException((byte) 2, oldVersion);    }}
0
public final void readFields(DataInput in) throws IOException
{    metadata.clear();    int sizeOrVersion = in.readInt();    if (sizeOrVersion < 0) {                version = sizeOrVersion;        switch(version) {            case VERSION:                url = Text.readString(in);                base = Text.readString(in);                content = new byte[in.readInt()];                in.readFully(content);                contentType = Text.readString(in);                metadata.readFields(in);                break;            default:                throw new VersionMismatchException((byte) VERSION, (byte) version);        }    } else {                byte[] compressed = new byte[sizeOrVersion];        in.readFully(compressed, 0, compressed.length);        ByteArrayInputStream deflated = new ByteArrayInputStream(compressed);        DataInput inflater = new DataInputStream(new InflaterInputStream(deflated));        readFieldsCompressed(inflater);    }}
0
public final void write(DataOutput out) throws IOException
{    out.writeInt(VERSION);        Text.writeString(out, url);        Text.writeString(out, base);        out.writeInt(content.length);    out.write(content);        Text.writeString(out, contentType);        metadata.write(out);}
0
public static Content read(DataInput in) throws IOException
{    Content content = new Content();    content.readFields(in);    return content;}
0
public String getUrl()
{    return url;}
0
public String getBaseUrl()
{    return base;}
0
public byte[] getContent()
{    return content;}
0
public void setContent(byte[] content)
{    this.content = content;}
0
public String getContentType()
{    return contentType;}
0
public void setContentType(String contentType)
{    this.contentType = contentType;}
0
public Metadata getMetadata()
{    return metadata;}
0
public void setMetadata(Metadata metadata)
{    this.metadata = metadata;}
0
public boolean equals(Object o)
{    if (!(o instanceof Content)) {        return false;    }    Content that = (Content) o;    return this.url.equals(that.url) && this.base.equals(that.base) && Arrays.equals(this.getContent(), that.getContent()) && this.contentType.equals(that.contentType) && this.metadata.equals(that.metadata);}
0
public String toString()
{    StringBuffer buffer = new StringBuffer();    buffer.append("Version: " + version + "\n");    buffer.append("url: " + url + "\n");    buffer.append("base: " + base + "\n");    buffer.append("contentType: " + contentType + "\n");    buffer.append("metadata: " + metadata + "\n");    buffer.append("Content:\n");        buffer.append(new String(content));    return buffer.toString();}
0
public static void main(String[] argv) throws Exception
{    String usage = "Content (-local | -dfs <namenode:port>) recno segment";    if (argv.length < 3) {        System.out.println("usage:" + usage);        return;    }    Options opts = new Options();    Configuration conf = NutchConfiguration.create();    GenericOptionsParser parser = new GenericOptionsParser(conf, opts, argv);    String[] remainingArgs = parser.getRemainingArgs();    try (FileSystem fs = FileSystem.get(conf)) {        int recno = Integer.parseInt(remainingArgs[0]);        String segment = remainingArgs[1];        Path file = new Path(segment, DIR_NAME);        System.out.println("Reading from file: " + file);        ArrayFile.Reader contents = new ArrayFile.Reader(fs, file.toString(), conf);        Content content = new Content();        contents.get(recno, content);        System.out.println("Retrieved " + recno + " from file " + file);        System.out.println(content);        contents.close();    }}
0
private String getContentType(String typeName, String url, byte[] data)
{    return this.mimeTypes.autoResolveContentType(typeName, url, data);}
0
public Protocol getProtocol(String urlString) throws ProtocolNotFound
{    try {        URL url = new URL(urlString);        return getProtocol(url);    } catch (MalformedURLException e) {        throw new ProtocolNotFound(urlString, e.toString());    }}
0
public Protocol getProtocol(URL url) throws ProtocolNotFound
{    try {        Protocol protocol = null;                String host = url.getHost();        if (hostProtocolMapping.containsKey(host)) {            Extension extension = getExtensionById(hostProtocolMapping.get(host));            if (extension != null) {                protocol = getProtocolInstanceByExtension(extension);            }        }                if (protocol == null) {                        if (defaultProtocolImplMapping.containsKey(url.getProtocol())) {                Extension extension = getExtensionById(defaultProtocolImplMapping.get(url.getProtocol()));                if (extension != null) {                    protocol = getProtocolInstanceByExtension(extension);                }            }        }                if (protocol == null) {            Extension extension = findExtension(url.getProtocol(), "protocolName");            if (extension != null) {                protocol = getProtocolInstanceByExtension(extension);            }        }                if (protocol != null) {            return protocol;        }                throw new ProtocolNotFound(url.toString());    } catch (PluginRuntimeException e) {        throw new ProtocolNotFound(url.toString(), e.toString());    }}
0
private Protocol getProtocolInstanceByExtension(Extension extension) throws PluginRuntimeException
{    Protocol protocol = null;    String cacheId = extension.getId();    ObjectCache objectCache = ObjectCache.get(conf);    synchronized (objectCache) {        if (!objectCache.hasObject(cacheId)) {            protocol = (Protocol) extension.getExtensionInstance();            objectCache.setObject(cacheId, protocol);        }        protocol = (Protocol) objectCache.getObject(cacheId);    }    return protocol;}
0
private Extension getExtensionById(String id)
{    Extension[] extensions = this.extensionPoint.getExtensions();    for (int i = 0; i < extensions.length; i++) {        if (id.equals(extensions[i].getId())) {            return extensions[i];        }    }    return null;}
0
private Extension findExtension(String name, String attribute) throws PluginRuntimeException
{    for (int i = 0; i < this.extensionPoint.getExtensions().length; i++) {        Extension extension = this.extensionPoint.getExtensions()[i];        if (contains(name, extension.getAttribute(attribute)))            return extension;    }    return null;}
0
 boolean contains(String what, String where)
{    if (where != null) {        String[] parts = where.split("[, ]");        for (int i = 0; i < parts.length; i++) {            if (parts[i].equals(what))                return true;        }    }    return false;}
0
public Protocol getProtocolById(String id) throws PluginRuntimeException
{    Extension ext = getExtensionById(id);    if (ext == null) {        throw new PluginRuntimeException("ID " + id + " not found");    }    return getProtocolInstanceByExtension(ext);}
0
public String getUrl()
{    return url;}
0
public Content getContent()
{    return content;}
0
public void setContent(Content content)
{    this.content = content;}
0
public ProtocolStatus getStatus()
{    return status;}
0
public void setStatus(ProtocolStatus status)
{    this.status = status;}
0
public static ProtocolStatus read(DataInput in) throws IOException
{    ProtocolStatus res = new ProtocolStatus();    res.readFields(in);    return res;}
0
public void readFields(DataInput in) throws IOException
{    byte version = in.readByte();    switch(version) {        case 1:            code = in.readByte();            lastModified = in.readLong();            args = WritableUtils.readCompressedStringArray(in);            break;        case VERSION:            code = in.readByte();            lastModified = in.readLong();            args = WritableUtils.readStringArray(in);            break;        default:            throw new VersionMismatchException(VERSION, version);    }}
0
public void write(DataOutput out) throws IOException
{    out.writeByte(VERSION);    out.writeByte((byte) code);    out.writeLong(lastModified);    if (args == null) {        out.writeInt(-1);    } else {        WritableUtils.writeStringArray(out, args);    }}
0
public void setArgs(String[] args)
{    this.args = args;}
0
public String[] getArgs()
{    return args;}
0
public int getCode()
{    return code;}
0
public String getName()
{    return codeToName.get(this.code);}
0
public void setCode(int code)
{    this.code = code;}
0
public boolean isSuccess()
{    return code == SUCCESS;}
0
public boolean isTransientFailure()
{    return code == ACCESS_DENIED || code == EXCEPTION || code == REDIR_EXCEEDED || code == RETRY || code == TEMP_MOVED || code == WOULDBLOCK || code == PROTO_NOT_FOUND;}
0
public boolean isPermanentFailure()
{    return code == FAILED || code == GONE || code == MOVED || code == NOTFOUND || code == ROBOTS_DENIED;}
0
public boolean isRedirect()
{    return code == MOVED || code == TEMP_MOVED;}
0
public String getMessage()
{    if (args != null && args.length > 0)        return args[0];    return null;}
0
public void setMessage(String msg)
{    if (args != null && args.length > 0)        args[0] = msg;    else        args = new String[] { msg };}
0
public long getLastModified()
{    return lastModified;}
0
public void setLastModified(long lastModified)
{    this.lastModified = lastModified;}
0
public boolean equals(Object o)
{    if (o == null)        return false;    if (!(o instanceof ProtocolStatus))        return false;    ProtocolStatus other = (ProtocolStatus) o;    if (this.code != other.code || this.lastModified != other.lastModified)        return false;    if (this.args == null) {        if (other.args == null)            return true;        else            return false;    } else {        if (other.args == null)            return false;        if (other.args.length != this.args.length)            return false;        for (int i = 0; i < this.args.length; i++) {            if (!this.args[i].equals(other.args[i]))                return false;        }    }    return true;}
0
public String toString()
{    StringBuffer res = new StringBuffer();    res.append(codeToName.get(Integer.valueOf(code)) + "(" + code + "), lastModified=" + lastModified);    if (args != null) {        if (args.length == 1) {            res.append(": " + String.valueOf(args[0]));        } else {            for (int i = 0; i < args.length; i++) {                if (args[i] != null)                    res.append(", args[" + i + "]=" + String.valueOf(args[i]));            }        }    }    return res.toString();}
0
public void setConf(Configuration conf)
{    this.conf = conf;        String agentName = conf.get("http.agent.name");    if (agentName == null || (agentName = agentName.trim()).isEmpty()) {        throw new RuntimeException("Agent name not configured!");    }    agentNames = agentName;            String otherAgents = conf.get("http.robots.agents");    if (otherAgents != null && !otherAgents.trim().isEmpty()) {        StringTokenizer tok = new StringTokenizer(otherAgents, ",");        StringBuilder sb = new StringBuilder(agentNames);        while (tok.hasMoreTokens()) {            String str = tok.nextToken().trim();            if (str.equals("*") || str.equals(agentName)) {                                                } else {                sb.append(",").append(str);            }        }        agentNames = sb.toString();    }    String[] confWhiteList = conf.getStrings("http.robot.rules.whitelist");    if (confWhiteList == null) {            } else {        for (int i = 0; i < confWhiteList.length; i++) {            if (confWhiteList[i].isEmpty()) {                                continue;            }            whiteList.add(confWhiteList[i]);        }        if (whiteList.size() > 0) {            matcher = new SuffixStringMatcher(whiteList);                    }    }}
1
public Configuration getConf()
{    return conf;}
0
public boolean isWhiteListed(URL url)
{    boolean match = false;    String urlString = url.getHost();    if (matcher != null) {        match = matcher.matches(urlString);    }    return match;}
0
public BaseRobotRules parseRules(String url, byte[] content, String contentType, String robotName)
{    return robotParser.parseContent(url, content, contentType, robotName);}
0
public BaseRobotRules getRobotRulesSet(Protocol protocol, Text url, List<Content> robotsTxtContent)
{    URL u = null;    try {        u = new URL(url.toString());    } catch (Exception e) {        return EMPTY_RULES;    }    return getRobotRulesSet(protocol, u, robotsTxtContent);}
0
public int run(String[] args)
{    if (args.length < 2) {        String[] help = { "Usage: RobotRulesParser [ -Dproperty=... ] <robots-file-or-url> <url-file> [<agent-names>]", "", "<robots-file-or-url>\tlocal file or URL parsed as robots.txt file", "\tIf <robots-file-or-url> starts with a protocol specification", "\t(`http', `https', `ftp' or `file'), robots.txt it is fetched", "\tusing the specified protocol. Otherwise, a local file is assumed.", "", "<url-file>\tlocal file with URLs (one per line), for every URL", "\tthe path part (including the query) is checked whether", "\tit is allowed by the robots.txt rules.  Other parts of the URLs", "\t(mainly the host) are ignored.", "", "<agent-names>\tcomma-separated list of agent names", "\tused to select rules from the robots.txt file.", "\tIf no agent name is given the property http.agent.name is used.", "\tIf http.agent.name is empty, robots.txt is checked for rules", "\tassigned to the user agent `*' (meaning any other).", "", "Important properties:", " -D fetcher.store.robotstxt=true", "\toutput content and HTTP meta data of fetched robots.txt (if not a local file)", " -D http.agent.name=...\tsame as argument <agent-names>", " -D http.robots.agents=...\tadditional agent names", " -D http.robot.rules.whitelist=..." };        for (String s : help) {            System.err.println(s);        }        return -1;    }    Protocol protocol = null;    URL robotsTxtUrl = null;    if (args[0].matches("^(?:https?|ftp|file)://?.*")) {        try {            robotsTxtUrl = new URL(args[0]);        } catch (MalformedURLException e) {                    }        ProtocolFactory factory = new ProtocolFactory(conf);        try {            protocol = factory.getProtocol(robotsTxtUrl);        } catch (ProtocolNotFound e) {                        return -1;        }    }    if (robotsTxtUrl == null) {                File robotsFile = new File(args[0]);        if (!robotsFile.exists()) {                        return -1;        } else {            try {                robotsTxtUrl = robotsFile.toURI().toURL();            } catch (MalformedURLException e) {            }        }    }    File urlFile = new File(args[1]);    if (args.length > 2) {                String agents = args[2];        conf.set("http.agent.name", agents);        setConf(conf);    }    List<Content> robotsTxtContent = null;    if (getConf().getBoolean("fetcher.store.robotstxt", false)) {        robotsTxtContent = new LinkedList<>();    }    try {        BaseRobotRules rules = getRobotRulesSet(protocol, robotsTxtUrl, robotsTxtContent);        if (robotsTxtContent != null) {            for (Content robotsTxt : robotsTxtContent) {                                            }        }        System.out.println("Testing robots.txt for agent names: " + agentNames);        LineNumberReader testsIn = new LineNumberReader(new FileReader(urlFile));        String testPath;        testPath = testsIn.readLine();        while (testPath != null) {            testPath = testPath.trim();            try {                                URL url = new URL(testPath);                String status;                if (isWhiteListed(url)) {                    status = "whitelisted";                } else if (rules.isAllowed(testPath)) {                    status = "allowed";                } else {                    status = "not allowed";                }                System.out.println(status + ":\t" + testPath);            } catch (MalformedURLException e) {                            }            testPath = testsIn.readLine();        }        testsIn.close();    } catch (IOException e) {                return -1;    }    return 0;}
1
public BaseRobotRules getRobotRulesSet(Protocol protocol, URL url, List<Content> robotsTxtContent)
{    BaseRobotRules rules;    if (protocol != null) {        rules = protocol.getRobotRules(new Text(url.toString()), null, robotsTxtContent);    } else {        try {            int contentLength = url.openConnection().getContentLength();            byte[] robotsBytes = new byte[contentLength];            InputStream openStream = url.openStream();            openStream.read(robotsBytes);            openStream.close();            rules = robotParser.parseContent(url.toString(), robotsBytes, "text/plain", this.conf.get("http.agent.name"));        } catch (IOException e) {                        rules = EMPTY_RULES;        }    }    return rules;}
1
public static void main(String[] args) throws Exception
{    Configuration conf = NutchConfiguration.create();    int res = ToolRunner.run(conf, new TestRobotRulesParser(conf), args);    System.exit(res);}
0
public void publish(Object event, Configuration conf)
{    for (int i = 0; i < this.publishers.length; i++) {        try {            this.publishers[i].publish(event, conf);        } catch (Exception e) {                    }    }}
1
public Configuration getConf()
{    return conf;}
0
public void setConf(Configuration arg0)
{}
0
public Configuration getConf()
{    return conf;}
0
public void setConf(Configuration conf)
{    this.conf = conf;}
0
public void injectedScore(Text url, CrawlDatum datum) throws ScoringFilterException
{}
0
public void initialScore(Text url, CrawlDatum datum) throws ScoringFilterException
{}
0
public float generatorSortValue(Text url, CrawlDatum datum, float initSort) throws ScoringFilterException
{    return initSort;}
0
public void passScoreBeforeParsing(Text url, CrawlDatum datum, Content content) throws ScoringFilterException
{}
0
public void passScoreAfterParsing(Text url, Content content, Parse parse) throws ScoringFilterException
{}
0
public CrawlDatum distributeScoreToOutlinks(Text fromUrl, ParseData parseData, Collection<Entry<Text, CrawlDatum>> targets, CrawlDatum adjust, int allCount) throws ScoringFilterException
{    return adjust;}
0
public void updateDbScore(Text url, CrawlDatum old, CrawlDatum datum, List<CrawlDatum> inlinked) throws ScoringFilterException
{}
0
public float indexerScore(Text url, NutchDocument doc, CrawlDatum dbDatum, CrawlDatum fetchDatum, Parse parse, Inlinks inlinks, float initScore) throws ScoringFilterException
{    return initScore;}
0
public void orphanedScore(Text url, CrawlDatum datum) throws ScoringFilterException
{}
0
public float generatorSortValue(Text url, CrawlDatum datum, float initSort) throws ScoringFilterException
{    for (int i = 0; i < this.filters.length; i++) {        initSort = this.filters[i].generatorSortValue(url, datum, initSort);    }    return initSort;}
0
public void initialScore(Text url, CrawlDatum datum) throws ScoringFilterException
{    for (int i = 0; i < this.filters.length; i++) {        this.filters[i].initialScore(url, datum);    }}
0
public void injectedScore(Text url, CrawlDatum datum) throws ScoringFilterException
{    for (int i = 0; i < this.filters.length; i++) {        this.filters[i].injectedScore(url, datum);    }}
0
public void updateDbScore(Text url, CrawlDatum old, CrawlDatum datum, List<CrawlDatum> inlinked) throws ScoringFilterException
{    for (int i = 0; i < this.filters.length; i++) {        this.filters[i].updateDbScore(url, old, datum, inlinked);    }}
0
public void orphanedScore(Text url, CrawlDatum datum) throws ScoringFilterException
{    for (int i = 0; i < this.filters.length; i++) {        this.filters[i].orphanedScore(url, datum);    }}
0
public void passScoreBeforeParsing(Text url, CrawlDatum datum, Content content) throws ScoringFilterException
{    for (int i = 0; i < this.filters.length; i++) {        this.filters[i].passScoreBeforeParsing(url, datum, content);    }}
0
public void passScoreAfterParsing(Text url, Content content, Parse parse) throws ScoringFilterException
{    for (int i = 0; i < this.filters.length; i++) {        this.filters[i].passScoreAfterParsing(url, content, parse);    }}
0
public CrawlDatum distributeScoreToOutlinks(Text fromUrl, ParseData parseData, Collection<Entry<Text, CrawlDatum>> targets, CrawlDatum adjust, int allCount) throws ScoringFilterException
{    for (int i = 0; i < this.filters.length; i++) {        adjust = this.filters[i].distributeScoreToOutlinks(fromUrl, parseData, targets, adjust, allCount);    }    return adjust;}
0
public float indexerScore(Text url, NutchDocument doc, CrawlDatum dbDatum, CrawlDatum fetchDatum, Parse parse, Inlinks inlinks, float initScore) throws ScoringFilterException
{    for (int i = 0; i < this.filters.length; i++) {        initScore = this.filters[i].indexerScore(url, doc, dbDatum, fetchDatum, parse, inlinks, initScore);    }    return initScore;}
0
public String getUrl()
{    return url;}
0
public String getAnchor()
{    return anchor;}
0
public void setAnchor(String anchor)
{    this.anchor = anchor;}
0
public float getScore()
{    return score;}
0
public void setScore(float score)
{    this.score = score;}
0
public void setUrl(String url)
{    this.url = url;}
0
public long getTimestamp()
{    return timestamp;}
0
public void setTimestamp(long timestamp)
{    this.timestamp = timestamp;}
0
public byte getLinkType()
{    return linkType;}
0
public void setLinkType(byte linkType)
{    this.linkType = linkType;}
0
public void readFields(DataInput in) throws IOException
{    url = Text.readString(in);    anchor = Text.readString(in);    score = in.readFloat();    timestamp = in.readLong();    linkType = in.readByte();}
0
public void write(DataOutput out) throws IOException
{    Text.writeString(out, url);    Text.writeString(out, anchor != null ? anchor : "");    out.writeFloat(score);    out.writeLong(timestamp);    out.writeByte(linkType);}
0
public String toString()
{    String type = (linkType == INLINK ? "inlink" : (linkType == OUTLINK) ? "outlink" : "unknown");    return "url: " + url + ", anchor: " + anchor + ", score: " + score + ", timestamp: " + timestamp + ", link type: " + type;}
0
public static void main(String[] args) throws Exception
{    if (args == null || args.length < 2) {        System.out.println("LinkDumper$Reader usage: <webgraphdb> <url>");        return;    }        Configuration conf = NutchConfiguration.create();    Path webGraphDb = new Path(args[0]);    String url = args[1];    MapFile.Reader[] readers = MapFileOutputFormat.getReaders(new Path(webGraphDb, DUMP_DIR), conf);        Text key = new Text(url);    LinkNodes nodes = new LinkNodes();    MapFileOutputFormat.getEntry(readers, new HashPartitioner<>(), key, nodes);        LinkNode[] linkNodesAr = nodes.getLinks();    System.out.println(url + ":");    for (LinkNode node : linkNodesAr) {        System.out.println("  " + node.getUrl() + " - " + node.getNode().toString());    }        FSUtils.closeReaders(readers);}
0
public String getUrl()
{    return url;}
0
public void setUrl(String url)
{    this.url = url;}
0
public Node getNode()
{    return node;}
0
public void setNode(Node node)
{    this.node = node;}
0
public void readFields(DataInput in) throws IOException
{    url = in.readUTF();    node = new Node();    node.readFields(in);}
0
public void write(DataOutput out) throws IOException
{    out.writeUTF(url);    node.write(out);}
0
public LinkNode[] getLinks()
{    return links;}
0
public void setLinks(LinkNode[] links)
{    this.links = links;}
0
public void readFields(DataInput in) throws IOException
{    int numLinks = in.readInt();    if (numLinks > 0) {        links = new LinkNode[numLinks];        for (int i = 0; i < numLinks; i++) {            LinkNode node = new LinkNode();            node.readFields(in);            links[i] = node;        }    }}
0
public void write(DataOutput out) throws IOException
{    if (links != null && links.length > 0) {        int numLinks = links.length;        out.writeInt(numLinks);        for (int i = 0; i < numLinks; i++) {            links[i].write(out);        }    }}
0
public void map(Text key, Writable value, Context context) throws IOException, InterruptedException
{    ObjectWritable objWrite = new ObjectWritable();    objWrite.set(value);    context.write(key, objWrite);}
0
public void setup(Reducer<Text, ObjectWritable, Text, LinkNode>.Context context)
{    conf = context.getConfiguration();}
0
public void reduce(Text key, Iterable<ObjectWritable> values, Context context) throws IOException, InterruptedException
{    String fromUrl = key.toString();    List<LinkDatum> outlinks = new ArrayList<>();    Node node = null;        for (ObjectWritable write : values) {        Object obj = write.get();        if (obj instanceof Node) {            node = (Node) obj;        } else if (obj instanceof LinkDatum) {            outlinks.add(WritableUtils.clone((LinkDatum) obj, conf));        }    }        int numOutlinks = node.getNumOutlinks();    if (numOutlinks > 0) {        for (int i = 0; i < outlinks.size(); i++) {            LinkDatum outlink = outlinks.get(i);            String toUrl = outlink.getUrl();                        context.write(new Text(toUrl), new LinkNode(fromUrl, node));        }    }}
0
public void reduce(Text key, Iterable<LinkNode> values, Context context) throws IOException, InterruptedException
{    List<LinkNode> nodeList = new ArrayList<>();    int numNodes = 0;    for (LinkNode cur : values) {        if (numNodes < maxInlinks) {            nodeList.add(WritableUtils.clone(cur, conf));            numNodes++;        } else {            break;        }    }    LinkNode[] linkNodesAr = nodeList.toArray(new LinkNode[nodeList.size()]);    LinkNodes linkNodes = new LinkNodes(linkNodesAr);    context.write(key, linkNodes);}
0
public void dumpLinks(Path webGraphDb) throws IOException, InterruptedException, ClassNotFoundException
{    SimpleDateFormat sdf = new SimpleDateFormat("yyyy-MM-dd HH:mm:ss");    long start = System.currentTimeMillis();        Configuration conf = getConf();    FileSystem fs = webGraphDb.getFileSystem(conf);    Path linkdump = new Path(webGraphDb, DUMP_DIR);    Path nodeDb = new Path(webGraphDb, WebGraph.NODE_DIR);    Path outlinkDb = new Path(webGraphDb, WebGraph.OUTLINK_DIR);        Path tempInverted = new Path(webGraphDb, "inverted-" + Integer.toString(new Random().nextInt(Integer.MAX_VALUE)));    Job inverter = NutchJob.getInstance(conf);    inverter.setJobName("LinkDumper: inverter");    FileInputFormat.addInputPath(inverter, nodeDb);    FileInputFormat.addInputPath(inverter, outlinkDb);    inverter.setInputFormatClass(SequenceFileInputFormat.class);    inverter.setJarByClass(Inverter.class);    inverter.setMapperClass(Inverter.InvertMapper.class);    inverter.setReducerClass(Inverter.InvertReducer.class);    inverter.setMapOutputKeyClass(Text.class);    inverter.setMapOutputValueClass(ObjectWritable.class);    inverter.setOutputKeyClass(Text.class);    inverter.setOutputValueClass(LinkNode.class);    FileOutputFormat.setOutputPath(inverter, tempInverted);    inverter.setOutputFormatClass(SequenceFileOutputFormat.class);    try {                boolean success = inverter.waitForCompletion(true);        if (!success) {            String message = "LinkDumper inverter job did not succeed, job status:" + inverter.getStatus().getState() + ", reason: " + inverter.getStatus().getFailureInfo();                        throw new RuntimeException(message);        }            } catch (IOException | InterruptedException | ClassNotFoundException e) {                throw e;    }        Job merger = NutchJob.getInstance(conf);    merger.setJobName("LinkDumper: merger");    FileInputFormat.addInputPath(merger, tempInverted);    merger.setJarByClass(Merger.class);    merger.setInputFormatClass(SequenceFileInputFormat.class);    merger.setReducerClass(Merger.class);    merger.setMapOutputKeyClass(Text.class);    merger.setMapOutputValueClass(LinkNode.class);    merger.setOutputKeyClass(Text.class);    merger.setOutputValueClass(LinkNodes.class);    FileOutputFormat.setOutputPath(merger, linkdump);    merger.setOutputFormatClass(MapFileOutputFormat.class);    try {                boolean success = merger.waitForCompletion(true);        if (!success) {            String message = "LinkDumper merger job did not succeed, job status:" + merger.getStatus().getState() + ", reason: " + merger.getStatus().getFailureInfo();                        throw new RuntimeException(message);        }            } catch (IOException e) {                throw e;    }    fs.delete(tempInverted, true);    long end = System.currentTimeMillis();    }
1
public static void main(String[] args) throws Exception
{    int res = ToolRunner.run(NutchConfiguration.create(), new LinkDumper(), args);    System.exit(res);}
0
public int run(String[] args) throws Exception
{    Options options = new Options();    OptionBuilder.withArgName("help");    OptionBuilder.withDescription("show this help message");    Option helpOpts = OptionBuilder.create("help");    options.addOption(helpOpts);    OptionBuilder.withArgName("webgraphdb");    OptionBuilder.hasArg();    OptionBuilder.withDescription("the web graph database to use");    Option webGraphDbOpts = OptionBuilder.create("webgraphdb");    options.addOption(webGraphDbOpts);    CommandLineParser parser = new GnuParser();    try {        CommandLine line = parser.parse(options, args);        if (line.hasOption("help") || !line.hasOption("webgraphdb")) {            HelpFormatter formatter = new HelpFormatter();            formatter.printHelp("LinkDumper", options);            return -1;        }        String webGraphDb = line.getOptionValue("webgraphdb");        dumpLinks(new Path(webGraphDb));        return 0;    } catch (Exception e) {                return -2;    }}
1
private int runCounter(FileSystem fs, Path webGraphDb) throws IOException, ClassNotFoundException, InterruptedException
{        Path numLinksPath = new Path(webGraphDb, NUM_NODES);    Path nodeDb = new Path(webGraphDb, WebGraph.NODE_DIR);    Job counter = NutchJob.getInstance(getConf());    Configuration conf = counter.getConfiguration();    counter.setJobName("LinkRank Counter");    FileInputFormat.addInputPath(counter, nodeDb);    FileOutputFormat.setOutputPath(counter, numLinksPath);    counter.setInputFormatClass(SequenceFileInputFormat.class);    counter.setJarByClass(Counter.class);    counter.setMapperClass(Counter.CountMapper.class);    counter.setCombinerClass(Counter.CountReducer.class);    counter.setReducerClass(Counter.CountReducer.class);    counter.setMapOutputKeyClass(Text.class);    counter.setMapOutputValueClass(LongWritable.class);    counter.setOutputKeyClass(Text.class);    counter.setOutputValueClass(LongWritable.class);    counter.setNumReduceTasks(1);    counter.setOutputFormatClass(TextOutputFormat.class);    conf.setBoolean("mapreduce.fileoutputcommitter.marksuccessfuljobs", false);            try {        boolean success = counter.waitForCompletion(true);        if (!success) {            String message = "Link counter job did not succeed, job status:" + counter.getStatus().getState() + ", reason: " + counter.getStatus().getFailureInfo();                        throw new RuntimeException(message);        }    } catch (IOException | InterruptedException | ClassNotFoundException e) {                throw e;    }                FileStatus[] numLinksFiles = fs.listStatus(numLinksPath);    if (numLinksFiles.length == 0) {        throw new IOException("Failed to read numlinks temp file: " + " no file found in " + numLinksPath);    } else if (numLinksFiles.length > 1) {        throw new IOException("Failed to read numlinks temp file: " + " expected only one file but found " + numLinksFiles.length + " files in folder " + numLinksPath);    }    Path numLinksFile = numLinksFiles[0].getPath();        FSDataInputStream readLinks = fs.open(numLinksFile);    CompressionCodecFactory cf = new CompressionCodecFactory(conf);    CompressionCodec codec = cf.getCodec(numLinksFiles[0].getPath());    InputStream streamLinks;    if (codec == null) {                streamLinks = readLinks;    } else {                readLinks.seek(0);        streamLinks = codec.createInputStream(readLinks);    }    BufferedReader buffer = new BufferedReader(new InputStreamReader(streamLinks));    String numLinksLine = buffer.readLine();    readLinks.close();        if (numLinksLine == null || numLinksLine.length() == 0) {                fs.delete(numLinksPath, true);        throw new IOException("No links to process, is the webgraph empty?");    }            fs.delete(numLinksPath, true);    String numLinks = numLinksLine.split("\\s+")[1];    return Integer.parseInt(numLinks);}
1
private void runInitializer(Path nodeDb, Path output) throws IOException, InterruptedException, ClassNotFoundException
{        Job initializer = NutchJob.getInstance(getConf());    Configuration conf = initializer.getConfiguration();    initializer.setJobName("LinkAnalysis Initializer");    FileInputFormat.addInputPath(initializer, nodeDb);    FileOutputFormat.setOutputPath(initializer, output);    initializer.setJarByClass(Initializer.class);    initializer.setInputFormatClass(SequenceFileInputFormat.class);    initializer.setMapperClass(Initializer.class);    initializer.setMapOutputKeyClass(Text.class);    initializer.setMapOutputValueClass(Node.class);    initializer.setOutputKeyClass(Text.class);    initializer.setOutputValueClass(Node.class);    initializer.setOutputFormatClass(MapFileOutputFormat.class);    conf.setBoolean("mapreduce.fileoutputcommitter.marksuccessfuljobs", false);            try {        boolean success = initializer.waitForCompletion(true);        if (!success) {            String message = "Initialization job did not succeed, job status:" + initializer.getStatus().getState() + ", reason: " + initializer.getStatus().getFailureInfo();                        throw new RuntimeException(message);        }    } catch (IOException | InterruptedException | ClassNotFoundException e) {                throw e;    }    }
1
private void runInverter(Path nodeDb, Path outlinkDb, Path output) throws IOException, InterruptedException, ClassNotFoundException
{        Job inverter = NutchJob.getInstance(getConf());    Configuration conf = inverter.getConfiguration();    inverter.setJobName("LinkAnalysis Inverter");    FileInputFormat.addInputPath(inverter, nodeDb);    FileInputFormat.addInputPath(inverter, outlinkDb);    FileOutputFormat.setOutputPath(inverter, output);    inverter.setInputFormatClass(SequenceFileInputFormat.class);    inverter.setJarByClass(Inverter.class);    inverter.setMapperClass(Inverter.InvertMapper.class);    inverter.setReducerClass(Inverter.InvertReducer.class);    inverter.setMapOutputKeyClass(Text.class);    inverter.setMapOutputValueClass(ObjectWritable.class);    inverter.setOutputKeyClass(Text.class);    inverter.setOutputValueClass(LinkDatum.class);    inverter.setOutputFormatClass(SequenceFileOutputFormat.class);    conf.setBoolean("mapreduce.fileoutputcommitter.marksuccessfuljobs", false);            try {        boolean success = inverter.waitForCompletion(true);        if (!success) {            String message = "Inverter job did not succeed, job status:" + inverter.getStatus().getState() + ", reason: " + inverter.getStatus().getFailureInfo();                        throw new RuntimeException(message);        }    } catch (IOException | InterruptedException | ClassNotFoundException e) {                throw e;    }    }
1
private void runAnalysis(Path nodeDb, Path inverted, Path output, int iteration, int numIterations, float rankOne) throws IOException, InterruptedException, ClassNotFoundException
{    Job analyzer = NutchJob.getInstance(getConf());    Configuration conf = analyzer.getConfiguration();    conf.set("link.analyze.iteration", String.valueOf(iteration + 1));    analyzer.setJobName("LinkAnalysis Analyzer, iteration " + (iteration + 1) + " of " + numIterations);    FileInputFormat.addInputPath(analyzer, nodeDb);    FileInputFormat.addInputPath(analyzer, inverted);    FileOutputFormat.setOutputPath(analyzer, output);    conf.set("link.analyze.rank.one", String.valueOf(rankOne));    analyzer.setMapOutputKeyClass(Text.class);    analyzer.setMapOutputValueClass(ObjectWritable.class);    analyzer.setInputFormatClass(SequenceFileInputFormat.class);    analyzer.setJarByClass(Analyzer.class);    analyzer.setMapperClass(Analyzer.AnalyzerMapper.class);    analyzer.setReducerClass(Analyzer.AnalyzerReducer.class);    analyzer.setOutputKeyClass(Text.class);    analyzer.setOutputValueClass(Node.class);    analyzer.setOutputFormatClass(MapFileOutputFormat.class);    conf.setBoolean("mapreduce.fileoutputcommitter.marksuccessfuljobs", false);        try {        boolean success = analyzer.waitForCompletion(true);        if (!success) {            String message = "Analysis job did not succeed, job status:" + analyzer.getStatus().getState() + ", reason: " + analyzer.getStatus().getFailureInfo();                        throw new RuntimeException(message);        }    } catch (IOException | InterruptedException | ClassNotFoundException e) {                throw e;    }    }
1
public void setup(Mapper<Text, Node, Text, LongWritable>.Context context)
{}
0
public void map(Text key, Node value, Context context) throws IOException, InterruptedException
{    context.write(numNodes, one);}
0
public void setup(Reducer<Text, LongWritable, Text, LongWritable>.Context context)
{}
0
public void reduce(Text key, Iterable<LongWritable> values, Context context) throws IOException, InterruptedException
{    long total = 0;    for (LongWritable val : values) {        total += val.get();    }    context.write(numNodes, new LongWritable(total));}
0
public void setup(Mapper<Text, Node, Text, Node>.Context context)
{    conf = context.getConfiguration();    initialScore = conf.getFloat("link.analyze.initial.score", 1.0f);}
0
public void map(Text key, Node node, Context context) throws IOException, InterruptedException
{    String url = key.toString();    Node outNode = WritableUtils.clone(node, conf);    outNode.setInlinkScore(initialScore);    context.write(new Text(url), outNode);}
0
public void setup(Mapper<Text, Writable, Text, ObjectWritable>.Context context)
{}
0
public void map(Text key, Writable value, Context context) throws IOException, InterruptedException
{    ObjectWritable objWrite = new ObjectWritable();    objWrite.set(value);    context.write(key, objWrite);}
0
public void setup(Reducer<Text, ObjectWritable, Text, LinkDatum>.Context context)
{    conf = context.getConfiguration();}
0
public void reduce(Text key, Iterable<ObjectWritable> values, Context context) throws IOException, InterruptedException
{    String fromUrl = key.toString();    List<LinkDatum> outlinks = new ArrayList<>();    Node node = null;        for (ObjectWritable write : values) {        Object obj = write.get();        if (obj instanceof Node) {            node = (Node) obj;        } else if (obj instanceof LinkDatum) {            outlinks.add(WritableUtils.clone((LinkDatum) obj, conf));        }    }            int numOutlinks = node.getNumOutlinks();    float inlinkScore = node.getInlinkScore();    float outlinkScore = node.getOutlinkScore();            if (numOutlinks > 0) {        for (int i = 0; i < outlinks.size(); i++) {            LinkDatum outlink = outlinks.get(i);            String toUrl = outlink.getUrl();            outlink.setUrl(fromUrl);            outlink.setScore(outlinkScore);                        context.write(new Text(toUrl), outlink);                    }    }}
1
public void setup(Mapper<Text, Writable, Text, ObjectWritable>.Context context)
{    conf = context.getConfiguration();}
0
public void map(Text key, Writable value, Context context) throws IOException, InterruptedException
{    ObjectWritable objWrite = new ObjectWritable();    objWrite.set(WritableUtils.clone(value, conf));    context.write(key, objWrite);}
0
public void setup(Reducer<Text, ObjectWritable, Text, Node>.Context context)
{    conf = context.getConfiguration();    dampingFactor = conf.getFloat("link.analyze.damping.factor", 0.85f);    rankOne = conf.getFloat("link.analyze.rank.one", 0.0f);    itNum = conf.getInt("link.analyze.iteration", 0);    limitPages = conf.getBoolean("link.ignore.limit.page", true);    limitDomains = conf.getBoolean("link.ignore.limit.domain", true);}
0
public void reduce(Text key, Iterable<ObjectWritable> values, Context context) throws IOException, InterruptedException
{    String url = key.toString();    Set<String> domains = new HashSet<>();    Set<String> pages = new HashSet<>();    Node node = null;        int numInlinks = 0;    float totalInlinkScore = rankOne;    for (ObjectWritable next : values) {        Object value = next.get();        if (value instanceof Node) {            node = (Node) value;        } else if (value instanceof LinkDatum) {            LinkDatum linkDatum = (LinkDatum) value;            float scoreFromInlink = linkDatum.getScore();            String inlinkUrl = linkDatum.getUrl();            String inLinkDomain = URLUtil.getDomainName(inlinkUrl);            String inLinkPage = URLUtil.getPage(inlinkUrl);                        if ((limitPages && pages.contains(inLinkPage)) || (limitDomains && domains.contains(inLinkDomain))) {                                continue;            }                        numInlinks++;            totalInlinkScore += scoreFromInlink;            domains.add(inLinkDomain);            pages.add(inLinkPage);                    }    }        float linkRankScore = (1 - dampingFactor) + (dampingFactor * totalInlinkScore);            Node outNode = WritableUtils.clone(node, conf);    outNode.setInlinkScore(linkRankScore);    context.write(key, outNode);}
1
public void close()
{}
0
public void analyze(Path webGraphDb) throws IOException, ClassNotFoundException, InterruptedException
{    SimpleDateFormat sdf = new SimpleDateFormat("yyyy-MM-dd HH:mm:ss");    long start = System.currentTimeMillis();                Path linkRank = new Path(webGraphDb, "linkrank");    Configuration conf = getConf();    FileSystem fs = linkRank.getFileSystem(conf);        if (!fs.exists(linkRank)) {        fs.mkdirs(linkRank);    }        Path wgOutlinkDb = new Path(webGraphDb, WebGraph.OUTLINK_DIR);    Path wgNodeDb = new Path(webGraphDb, WebGraph.NODE_DIR);    Path nodeDb = new Path(linkRank, WebGraph.NODE_DIR);            int numLinks = runCounter(fs, webGraphDb);    runInitializer(wgNodeDb, nodeDb);    float rankOneScore = (1f / (float) numLinks);    if (LOG.isInfoEnabled()) {                    }            int numIterations = conf.getInt("link.analyze.num.iterations", 10);    for (int i = 0; i < numIterations; i++) {                        Path tempRank = new Path(linkRank + "-" + Integer.toString(new Random().nextInt(Integer.MAX_VALUE)));        fs.mkdirs(tempRank);        Path tempInverted = new Path(tempRank, "inverted");        Path tempNodeDb = new Path(tempRank, WebGraph.NODE_DIR);                runInverter(nodeDb, wgOutlinkDb, tempInverted);        runAnalysis(nodeDb, tempInverted, tempNodeDb, i, numIterations, rankOneScore);                        FSUtils.replace(fs, linkRank, tempRank, true);            }            FSUtils.replace(fs, wgNodeDb, nodeDb, true);        fs.delete(linkRank, true);    long end = System.currentTimeMillis();    }
1
public static void main(String[] args) throws Exception
{    int res = ToolRunner.run(NutchConfiguration.create(), new LinkRank(), args);    System.exit(res);}
0
public int run(String[] args) throws Exception
{    Options options = new Options();    OptionBuilder.withArgName("help");    OptionBuilder.withDescription("show this help message");    Option helpOpts = OptionBuilder.create("help");    options.addOption(helpOpts);    OptionBuilder.withArgName("webgraphdb");    OptionBuilder.hasArg();    OptionBuilder.withDescription("the web graph db to use");    Option webgraphOpts = OptionBuilder.create("webgraphdb");    options.addOption(webgraphOpts);    CommandLineParser parser = new GnuParser();    try {        CommandLine line = parser.parse(options, args);        if (line.hasOption("help") || !line.hasOption("webgraphdb")) {            HelpFormatter formatter = new HelpFormatter();            formatter.printHelp("LinkRank", options);            return -1;        }        String webGraphDb = line.getOptionValue("webgraphdb");        analyze(new Path(webGraphDb));        return 0;    } catch (Exception e) {                return -2;    }}
1
public int getNumInlinks()
{    return numInlinks;}
0
public void setNumInlinks(int numInlinks)
{    this.numInlinks = numInlinks;}
0
public int getNumOutlinks()
{    return numOutlinks;}
0
public void setNumOutlinks(int numOutlinks)
{    this.numOutlinks = numOutlinks;}
0
public float getInlinkScore()
{    return inlinkScore;}
0
public void setInlinkScore(float inlinkScore)
{    this.inlinkScore = inlinkScore;}
0
public float getOutlinkScore()
{    return (numOutlinks > 0) ? inlinkScore / numOutlinks : inlinkScore;}
0
public Metadata getMetadata()
{    return metadata;}
0
public void setMetadata(Metadata metadata)
{    this.metadata = metadata;}
0
public void readFields(DataInput in) throws IOException
{    numInlinks = in.readInt();    numOutlinks = in.readInt();    inlinkScore = in.readFloat();    metadata.clear();    metadata.readFields(in);}
0
public void write(DataOutput out) throws IOException
{    out.writeInt(numInlinks);    out.writeInt(numOutlinks);    out.writeFloat(inlinkScore);    metadata.write(out);}
0
public String toString()
{    return "num inlinks: " + numInlinks + ", num outlinks: " + numOutlinks + ", inlink score: " + inlinkScore + ", outlink score: " + getOutlinkScore() + ", metadata: " + metadata.toString();}
0
public void setup(Mapper<Text, Node, FloatWritable, Text>.Context context)
{    conf = context.getConfiguration();    inlinks = conf.getBoolean("inlinks", false);    outlinks = conf.getBoolean("outlinks", false);}
0
public void map(Text key, Node node, Context context) throws IOException, InterruptedException
{    float number = 0;    if (inlinks) {        number = node.getNumInlinks();    } else if (outlinks) {        number = node.getNumOutlinks();    } else {        number = node.getInlinkScore();    }        context.write(new FloatWritable(-number), key);}
0
public void setup(Reducer<FloatWritable, Text, Text, FloatWritable>.Context context)
{    conf = context.getConfiguration();    topn = conf.getLong("topn", Long.MAX_VALUE);}
0
public void reduce(FloatWritable key, Iterable<Text> values, Context context) throws IOException, InterruptedException
{            float val = key.get();    FloatWritable number = new FloatWritable(val == 0 ? 0 : -val);    long numCollected = 0;        for (Text value : values) {        if (numCollected < topn) {            Text url = WritableUtils.clone(value, conf);            context.write(url, number);            numCollected++;        }    }}
0
public void setup(Mapper<Text, Node, Text, FloatWritable>.Context context)
{    conf = context.getConfiguration();    inlinks = conf.getBoolean("inlinks", false);    outlinks = conf.getBoolean("outlinks", false);    host = conf.getBoolean("host", false);}
0
public void map(Text key, Node node, Context context) throws IOException, InterruptedException
{    float number = 0;    if (inlinks) {        number = node.getNumInlinks();    } else if (outlinks) {        number = node.getNumOutlinks();    } else {        number = node.getInlinkScore();    }    if (host) {        key.set(URLUtil.getHost(key.toString()));    } else {        key.set(URLUtil.getDomainName(key.toString()));    }    context.write(key, new FloatWritable(number));}
0
public void reduce(Text key, Iterable<FloatWritable> values, Context context) throws IOException, InterruptedException
{    long numCollected = 0;    float sumOrMax = 0;    float val = 0;        for (FloatWritable value : values) {        if (numCollected < topn) {            val = value.get();            if (sum) {                sumOrMax += val;            } else {                if (sumOrMax < val) {                    sumOrMax = val;                }            }            numCollected++;        } else {            break;        }    }    context.write(key, new FloatWritable(sumOrMax));}
0
public void setup(Reducer<Text, FloatWritable, Text, FloatWritable>.Context context)
{    conf = context.getConfiguration();    topn = conf.getLong("topn", Long.MAX_VALUE);    sum = conf.getBoolean("sum", false);}
0
public void dumpNodes(Path webGraphDb, DumpType type, long topN, Path output, boolean asEff, NameType nameType, AggrType aggrType, boolean asSequenceFile) throws Exception
{    SimpleDateFormat sdf = new SimpleDateFormat("yyyy-MM-dd HH:mm:ss");    long start = System.currentTimeMillis();        Path nodeDb = new Path(webGraphDb, WebGraph.NODE_DIR);    Job dumper = NutchJob.getInstance(getConf());    Configuration conf = dumper.getConfiguration();    dumper.setJobName("NodeDumper: " + webGraphDb);    FileInputFormat.addInputPath(dumper, nodeDb);    dumper.setInputFormatClass(SequenceFileInputFormat.class);    if (nameType == null) {        dumper.setJarByClass(Sorter.class);        dumper.setMapperClass(Sorter.SorterMapper.class);        dumper.setReducerClass(Sorter.SorterReducer.class);        dumper.setMapOutputKeyClass(FloatWritable.class);        dumper.setMapOutputValueClass(Text.class);    } else {        dumper.setJarByClass(Dumper.class);        dumper.setMapperClass(Dumper.DumperMapper.class);        dumper.setReducerClass(Dumper.DumperReducer.class);        dumper.setMapOutputKeyClass(Text.class);        dumper.setMapOutputValueClass(FloatWritable.class);    }    dumper.setOutputKeyClass(Text.class);    dumper.setOutputValueClass(FloatWritable.class);    FileOutputFormat.setOutputPath(dumper, output);    if (asSequenceFile) {        dumper.setOutputFormatClass(SequenceFileOutputFormat.class);    } else {        dumper.setOutputFormatClass(TextOutputFormat.class);    }    dumper.setNumReduceTasks(1);    conf.setBoolean("inlinks", type == DumpType.INLINKS);    conf.setBoolean("outlinks", type == DumpType.OUTLINKS);    conf.setBoolean("scores", type == DumpType.SCORES);    conf.setBoolean("host", nameType == NameType.HOST);    conf.setBoolean("domain", nameType == NameType.DOMAIN);    conf.setBoolean("sum", aggrType == AggrType.SUM);    conf.setBoolean("max", aggrType == AggrType.MAX);    conf.setLong("topn", topN);        if (asEff) {        conf.set("mapreduce.output.textoutputformat.separator", "=");    }    try {                boolean success = dumper.waitForCompletion(true);        if (!success) {            String message = "NodeDumper job did not succeed, job status:" + dumper.getStatus().getState() + ", reason: " + dumper.getStatus().getFailureInfo();                        throw new RuntimeException(message);        }    } catch (IOException e) {                throw e;    }    long end = System.currentTimeMillis();    }
1
public static void main(String[] args) throws Exception
{    int res = ToolRunner.run(NutchConfiguration.create(), new NodeDumper(), args);    System.exit(res);}
0
public int run(String[] args) throws Exception
{    Options options = new Options();    OptionBuilder.withArgName("help");    OptionBuilder.withDescription("show this help message");    Option helpOpts = OptionBuilder.create("help");    options.addOption(helpOpts);    OptionBuilder.withArgName("webgraphdb");    OptionBuilder.hasArg();    OptionBuilder.withDescription("the web graph database to use");    Option webGraphDbOpts = OptionBuilder.create("webgraphdb");    options.addOption(webGraphDbOpts);    OptionBuilder.withArgName("inlinks");    OptionBuilder.withDescription("show highest inlinks");    Option inlinkOpts = OptionBuilder.create("inlinks");    options.addOption(inlinkOpts);    OptionBuilder.withArgName("outlinks");    OptionBuilder.withDescription("show highest outlinks");    Option outlinkOpts = OptionBuilder.create("outlinks");    options.addOption(outlinkOpts);    OptionBuilder.withArgName("scores");    OptionBuilder.withDescription("show highest scores");    Option scoreOpts = OptionBuilder.create("scores");    options.addOption(scoreOpts);    OptionBuilder.withArgName("topn");    OptionBuilder.hasOptionalArg();    OptionBuilder.withDescription("show topN scores");    Option topNOpts = OptionBuilder.create("topn");    options.addOption(topNOpts);    OptionBuilder.withArgName("output");    OptionBuilder.hasArg();    OptionBuilder.withDescription("the output directory to use");    Option outputOpts = OptionBuilder.create("output");    options.addOption(outputOpts);    OptionBuilder.withArgName("asEff");    OptionBuilder.withDescription("Solr ExternalFileField compatible output format");    Option effOpts = OptionBuilder.create("asEff");    options.addOption(effOpts);    OptionBuilder.hasArgs(2);    OptionBuilder.withDescription("group <host|domain> <sum|max>");    Option groupOpts = OptionBuilder.create("group");    options.addOption(groupOpts);    OptionBuilder.withArgName("asSequenceFile");    OptionBuilder.withDescription("whether to output as a sequencefile");    Option sequenceFileOpts = OptionBuilder.create("asSequenceFile");    options.addOption(sequenceFileOpts);    CommandLineParser parser = new GnuParser();    try {        CommandLine line = parser.parse(options, args);        if (line.hasOption("help") || !line.hasOption("webgraphdb")) {            HelpFormatter formatter = new HelpFormatter();            formatter.printHelp("NodeDumper", options);            return -1;        }        String webGraphDb = line.getOptionValue("webgraphdb");        boolean inlinks = line.hasOption("inlinks");        boolean outlinks = line.hasOption("outlinks");        long topN = (line.hasOption("topn") ? Long.parseLong(line.getOptionValue("topn")) : Long.MAX_VALUE);                String output = line.getOptionValue("output");        DumpType type = (inlinks ? DumpType.INLINKS : outlinks ? DumpType.OUTLINKS : DumpType.SCORES);        NameType nameType = null;        AggrType aggrType = null;        String[] group = line.getOptionValues("group");        if (group != null && group.length == 2) {            nameType = (group[0].equals("host") ? NameType.HOST : group[0].equals("domain") ? NameType.DOMAIN : null);            aggrType = (group[1].equals("sum") ? AggrType.SUM : group[1].equals("sum") ? AggrType.MAX : null);        }                boolean asEff = line.hasOption("asEff");        boolean asSequenceFile = line.hasOption("asSequenceFile");        dumpNodes(new Path(webGraphDb), type, topN, new Path(output), asEff, nameType, aggrType, asSequenceFile);        return 0;    } catch (Exception e) {                return -2;    }}
1
public void dumpUrl(Path webGraphDb, String url) throws IOException
{    nodeReaders = MapFileOutputFormat.getReaders(new Path(webGraphDb, WebGraph.NODE_DIR), getConf());        Text key = new Text(url);    Node node = new Node();    MapFileOutputFormat.getEntry(nodeReaders, new HashPartitioner<>(), key, node);    System.out.println(url + ":");    System.out.println("  inlink score: " + node.getInlinkScore());    System.out.println("  outlink score: " + node.getOutlinkScore());    System.out.println("  num inlinks: " + node.getNumInlinks());    System.out.println("  num outlinks: " + node.getNumOutlinks());    FSUtils.closeReaders(nodeReaders);}
0
public static void main(String[] args) throws Exception
{    Options options = new Options();    OptionBuilder.withArgName("help");    OptionBuilder.withDescription("show this help message");    Option helpOpts = OptionBuilder.create("help");    options.addOption(helpOpts);    OptionBuilder.withArgName("webgraphdb");    OptionBuilder.hasArg();    OptionBuilder.withDescription("the webgraphdb to use");    Option webGraphOpts = OptionBuilder.create("webgraphdb");    options.addOption(webGraphOpts);    OptionBuilder.withArgName("url");    OptionBuilder.hasOptionalArg();    OptionBuilder.withDescription("the url to dump");    Option urlOpts = OptionBuilder.create("url");    options.addOption(urlOpts);    CommandLineParser parser = new GnuParser();    try {                CommandLine line = parser.parse(options, args);        if (line.hasOption("help") || !line.hasOption("webgraphdb") || !line.hasOption("url")) {            HelpFormatter formatter = new HelpFormatter();            formatter.printHelp("WebGraphReader", options);            return;        }                String webGraphDb = line.getOptionValue("webgraphdb");        String url = line.getOptionValue("url");        NodeReader reader = new NodeReader(NutchConfiguration.create());        reader.dumpUrl(new Path(webGraphDb), url);        return;    } catch (Exception e) {        e.printStackTrace();        return;    }}
0
public void map(Text key, Writable value, Context context) throws IOException, InterruptedException
{    ObjectWritable objWrite = new ObjectWritable();    objWrite.set(value);    context.write(key, objWrite);}
0
public void setup(Reducer<Text, ObjectWritable, Text, CrawlDatum>.Context context)
{    Configuration conf = context.getConfiguration();    clearScore = conf.getFloat("link.score.updater.clear.score", 0.0f);}
0
public void reduce(Text key, Iterable<ObjectWritable> values, Context context) throws IOException, InterruptedException
{    String url = key.toString();    Node node = null;    CrawlDatum datum = null;        for (ObjectWritable next : values) {        Object value = next.get();        if (value instanceof Node) {            node = (Node) value;        } else if (value instanceof CrawlDatum) {            datum = (CrawlDatum) value;        }    }        if (datum != null) {        if (node != null) {                        float inlinkScore = node.getInlinkScore();            datum.setScore(inlinkScore);                    } else {                        datum.setScore(clearScore);                    }        context.write(key, datum);    } else {            }}
1
public void update(Path crawlDb, Path webGraphDb) throws IOException, ClassNotFoundException, InterruptedException
{    SimpleDateFormat sdf = new SimpleDateFormat("yyyy-MM-dd HH:mm:ss");    long start = System.currentTimeMillis();        Configuration conf = getConf();            Path nodeDb = new Path(webGraphDb, WebGraph.NODE_DIR);    Path crawlDbCurrent = new Path(crawlDb, CrawlDb.CURRENT_NAME);    Path newCrawlDb = new Path(crawlDb, Integer.toString(new Random().nextInt(Integer.MAX_VALUE)));        Job updater = NutchJob.getInstance(conf);    updater.setJobName("Update CrawlDb from WebGraph");    FileInputFormat.addInputPath(updater, crawlDbCurrent);    FileInputFormat.addInputPath(updater, nodeDb);    FileOutputFormat.setOutputPath(updater, newCrawlDb);    updater.setInputFormatClass(SequenceFileInputFormat.class);    updater.setJarByClass(ScoreUpdater.class);    updater.setMapperClass(ScoreUpdater.ScoreUpdaterMapper.class);    updater.setReducerClass(ScoreUpdater.ScoreUpdaterReducer.class);    updater.setMapOutputKeyClass(Text.class);    updater.setMapOutputValueClass(ObjectWritable.class);    updater.setOutputKeyClass(Text.class);    updater.setOutputValueClass(CrawlDatum.class);    updater.setOutputFormatClass(MapFileOutputFormat.class);    try {        boolean success = updater.waitForCompletion(true);        if (!success) {            String message = "Update CrawlDb from WebGraph job did not succeed, job status:" + updater.getStatus().getState() + ", reason: " + updater.getStatus().getFailureInfo();                                    FileSystem fs = newCrawlDb.getFileSystem(conf);            if (fs.exists(newCrawlDb)) {                fs.delete(newCrawlDb, true);            }            throw new RuntimeException(message);        }    } catch (IOException | ClassNotFoundException | InterruptedException e) {                        FileSystem fs = newCrawlDb.getFileSystem(conf);        if (fs.exists(newCrawlDb)) {            fs.delete(newCrawlDb, true);        }        throw e;    }            CrawlDb.install(updater, crawlDb);    long end = System.currentTimeMillis();    }
1
public static void main(String[] args) throws Exception
{    int res = ToolRunner.run(NutchConfiguration.create(), new ScoreUpdater(), args);    System.exit(res);}
0
public int run(String[] args) throws Exception
{    Options options = new Options();    OptionBuilder.withArgName("help");    OptionBuilder.withDescription("show this help message");    Option helpOpts = OptionBuilder.create("help");    options.addOption(helpOpts);    OptionBuilder.withArgName("crawldb");    OptionBuilder.hasArg();    OptionBuilder.withDescription("the crawldb to use");    Option crawlDbOpts = OptionBuilder.create("crawldb");    options.addOption(crawlDbOpts);    OptionBuilder.withArgName("webgraphdb");    OptionBuilder.hasArg();    OptionBuilder.withDescription("the webgraphdb to use");    Option webGraphOpts = OptionBuilder.create("webgraphdb");    options.addOption(webGraphOpts);    CommandLineParser parser = new GnuParser();    try {        CommandLine line = parser.parse(options, args);        if (line.hasOption("help") || !line.hasOption("webgraphdb") || !line.hasOption("crawldb")) {            HelpFormatter formatter = new HelpFormatter();            formatter.printHelp("ScoreUpdater", options);            return -1;        }        String crawlDb = line.getOptionValue("crawldb");        String webGraphDb = line.getOptionValue("webgraphdb");        update(new Path(crawlDb), new Path(webGraphDb));        return 0;    } catch (Exception e) {                return -1;    }}
1
private static long getFetchTime(ParseData data)
{        long fetchTime = System.currentTimeMillis();    String fetchTimeStr = data.getContentMeta().get(Nutch.FETCH_TIME_KEY);    try {                fetchTime = Long.parseLong(fetchTimeStr);    } catch (Exception e) {        fetchTime = System.currentTimeMillis();    }    return fetchTime;}
0
private String normalizeUrl(String url)
{    if (!normalize) {        return url;    }    String normalized = null;    if (urlNormalizers != null) {        try {                        normalized = urlNormalizers.normalize(url, URLNormalizers.SCOPE_DEFAULT);            normalized = normalized.trim();        } catch (Exception e) {                        normalized = null;        }    }    return normalized;}
1
private String filterUrl(String url)
{    if (!filter) {        return url;    }    try {        url = filters.filter(url);    } catch (Exception e) {        url = null;    }    return url;}
0
public void setup(Mapper<Text, Writable, Text, NutchWritable>.Context context)
{    Configuration config = context.getConfiguration();    conf = config;    normalize = conf.getBoolean(URL_NORMALIZING, false);    filter = conf.getBoolean(URL_FILTERING, false);    if (normalize) {        urlNormalizers = new URLNormalizers(conf, URLNormalizers.SCOPE_DEFAULT);    }    if (filter) {        filters = new URLFilters(conf);    }}
0
public void map(Text key, Writable value, Context context) throws IOException, InterruptedException
{        String url = normalizeUrl(key.toString());    if (url == null) {        return;    }        if (filterUrl(url) == null) {        return;    }        key.set(url);    if (value instanceof CrawlDatum) {        CrawlDatum datum = (CrawlDatum) value;        if (datum.getStatus() == CrawlDatum.STATUS_FETCH_REDIR_TEMP || datum.getStatus() == CrawlDatum.STATUS_FETCH_REDIR_PERM || datum.getStatus() == CrawlDatum.STATUS_FETCH_GONE) {                        context.write(key, new NutchWritable(new BooleanWritable(true)));        }    } else if (value instanceof ParseData) {                        ParseData data = (ParseData) value;        long fetchTime = getFetchTime(data);        Outlink[] outlinkAr = data.getOutlinks();        Map<String, String> outlinkMap = new LinkedHashMap<>();                if (outlinkAr != null && outlinkAr.length > 0) {            for (int i = 0; i < outlinkAr.length; i++) {                Outlink outlink = outlinkAr[i];                String toUrl = normalizeUrl(outlink.getToUrl());                if (filterUrl(toUrl) == null) {                    continue;                }                                                                boolean existingUrl = outlinkMap.containsKey(toUrl);                if (toUrl != null && (!existingUrl || (existingUrl && outlinkMap.get(toUrl) == null))) {                    outlinkMap.put(toUrl, outlink.getAnchor());                }            }        }                for (String outlinkUrl : outlinkMap.keySet()) {            String anchor = outlinkMap.get(outlinkUrl);            LinkDatum datum = new LinkDatum(outlinkUrl, anchor, fetchTime);            context.write(key, new NutchWritable(datum));        }    } else if (value instanceof LinkDatum) {        LinkDatum datum = (LinkDatum) value;        String linkDatumUrl = normalizeUrl(datum.getUrl());        if (filterUrl(linkDatumUrl) != null) {            datum.setUrl(linkDatumUrl);                        context.write(key, new NutchWritable(datum));        }    }}
0
public void setup(Reducer<Text, NutchWritable, Text, LinkDatum>.Context context)
{    Configuration config = context.getConfiguration();    conf = config;    ignoreHost = conf.getBoolean("link.ignore.internal.host", true);    ignoreDomain = conf.getBoolean("link.ignore.internal.domain", true);    limitPages = conf.getBoolean("link.ignore.limit.page", true);    limitDomains = conf.getBoolean("link.ignore.limit.domain", true);}
0
public void reduce(Text key, Iterable<NutchWritable> values, Context context) throws IOException, InterruptedException
{            long mostRecent = 0L;    List<LinkDatum> outlinkList = new ArrayList<>();    for (NutchWritable val : values) {        final Writable value = val.get();        if (value instanceof LinkDatum) {                        LinkDatum next = (LinkDatum) value;            long timestamp = next.getTimestamp();            if (mostRecent == 0L || mostRecent < timestamp) {                mostRecent = timestamp;            }            outlinkList.add(WritableUtils.clone(next, conf));            context.getCounter("WebGraph.outlinks", "added links").increment(1);        } else if (value instanceof BooleanWritable) {            BooleanWritable delete = (BooleanWritable) value;                        if (delete.get() == true) {                                context.getCounter("WebGraph.outlinks", "removed links").increment(1);                return;            }        }    }        String url = key.toString();    String domain = URLUtil.getDomainName(url);    String host = URLUtil.getHost(url);        Set<String> domains = new HashSet<>();    Set<String> pages = new HashSet<>();        for (LinkDatum datum : outlinkList) {                String toUrl = datum.getUrl();        String toDomain = URLUtil.getDomainName(toUrl);        String toHost = URLUtil.getHost(toUrl);        String toPage = URLUtil.getPage(toUrl);        datum.setLinkType(LinkDatum.OUTLINK);                if (datum.getTimestamp() == mostRecent && (!limitPages || (limitPages && !pages.contains(toPage))) && (!limitDomains || (limitDomains && !domains.contains(toDomain))) && (!ignoreHost || (ignoreHost && !toHost.equalsIgnoreCase(host))) && (!ignoreDomain || (ignoreDomain && !toDomain.equalsIgnoreCase(domain)))) {            context.write(key, datum);            pages.add(toPage);            domains.add(toDomain);        }    }}
0
public void close()
{}
0
public void setup(Mapper<Text, LinkDatum, Text, LinkDatum>.Context context)
{    timestamp = System.currentTimeMillis();}
0
public void map(Text key, LinkDatum datum, Context context) throws IOException, InterruptedException
{        String fromUrl = key.toString();    String toUrl = datum.getUrl();    String anchor = datum.getAnchor();        LinkDatum inlink = new LinkDatum(fromUrl, anchor, timestamp);    inlink.setLinkType(LinkDatum.INLINK);    context.write(new Text(toUrl), inlink);}
0
public void setup(Reducer<Text, LinkDatum, Text, Node>.Context context)
{}
0
public void reduce(Text key, Iterable<LinkDatum> values, Context context) throws IOException, InterruptedException
{    Node node = new Node();    int numInlinks = 0;    int numOutlinks = 0;        for (LinkDatum next : values) {        if (next.getLinkType() == LinkDatum.INLINK) {            numInlinks++;        } else if (next.getLinkType() == LinkDatum.OUTLINK) {            numOutlinks++;        }    }        node.setNumInlinks(numInlinks);    node.setNumOutlinks(numOutlinks);    node.setInlinkScore(0.0f);    context.write(key, node);}
0
public void createWebGraph(Path webGraphDb, Path[] segments, boolean normalize, boolean filter) throws IOException, InterruptedException, ClassNotFoundException
{    SimpleDateFormat sdf = new SimpleDateFormat("yyyy-MM-dd HH:mm:ss");    long start = System.currentTimeMillis();    if (LOG.isInfoEnabled()) {                                    }    FileSystem fs = webGraphDb.getFileSystem(getConf());        Path lock = new Path(webGraphDb, LOCK_NAME);    if (!fs.exists(webGraphDb)) {        fs.mkdirs(webGraphDb);    }    LockUtil.createLockFile(fs, lock, false);        Path outlinkDb = new Path(webGraphDb, OUTLINK_DIR);    Path oldOutlinkDb = new Path(webGraphDb, OLD_OUTLINK_DIR);    if (!fs.exists(outlinkDb)) {        fs.mkdirs(outlinkDb);    }    Path tempOutlinkDb = new Path(outlinkDb + "-" + Integer.toString(new Random().nextInt(Integer.MAX_VALUE)));    Job outlinkJob = NutchJob.getInstance(getConf());    Configuration outlinkJobConf = outlinkJob.getConfiguration();    outlinkJob.setJobName("Outlinkdb: " + outlinkDb);    boolean deleteGone = outlinkJobConf.getBoolean("link.delete.gone", false);    boolean preserveBackup = outlinkJobConf.getBoolean("db.preserve.backup", true);    if (deleteGone) {            }        if (segments != null) {        for (int i = 0; i < segments.length; i++) {            FileSystem sfs = segments[i].getFileSystem(outlinkJobConf);            Path parseData = new Path(segments[i], ParseData.DIR_NAME);            if (sfs.exists(parseData)) {                                FileInputFormat.addInputPath(outlinkJob, parseData);            }            if (deleteGone) {                Path crawlFetch = new Path(segments[i], CrawlDatum.FETCH_DIR_NAME);                if (sfs.exists(crawlFetch)) {                                        FileInputFormat.addInputPath(outlinkJob, crawlFetch);                }            }        }    }            FileInputFormat.addInputPath(outlinkJob, outlinkDb);    outlinkJobConf.setBoolean(OutlinkDb.URL_NORMALIZING, normalize);    outlinkJobConf.setBoolean(OutlinkDb.URL_FILTERING, filter);    outlinkJob.setInputFormatClass(SequenceFileInputFormat.class);    outlinkJob.setJarByClass(OutlinkDb.class);    outlinkJob.setMapperClass(OutlinkDb.OutlinkDbMapper.class);    outlinkJob.setReducerClass(OutlinkDb.OutlinkDbReducer.class);    outlinkJob.setMapOutputKeyClass(Text.class);    outlinkJob.setMapOutputValueClass(NutchWritable.class);    outlinkJob.setOutputKeyClass(Text.class);    outlinkJob.setOutputValueClass(LinkDatum.class);    FileOutputFormat.setOutputPath(outlinkJob, tempOutlinkDb);    outlinkJob.setOutputFormatClass(MapFileOutputFormat.class);    outlinkJobConf.setBoolean("mapreduce.fileoutputcommitter.marksuccessfuljobs", false);        try {                boolean success = outlinkJob.waitForCompletion(true);        if (!success) {            String message = "OutlinkDb job did not succeed, job status:" + outlinkJob.getStatus().getState() + ", reason: " + outlinkJob.getStatus().getFailureInfo();                        NutchJob.cleanupAfterFailure(tempOutlinkDb, lock, fs);            throw new RuntimeException(message);        }                FSUtils.replace(fs, oldOutlinkDb, outlinkDb, true);        FSUtils.replace(fs, outlinkDb, tempOutlinkDb, true);        if (!preserveBackup && fs.exists(oldOutlinkDb))            fs.delete(oldOutlinkDb, true);            } catch (IOException | InterruptedException | ClassNotFoundException e) {                        NutchJob.cleanupAfterFailure(tempOutlinkDb, lock, fs);        throw e;    }        Path inlinkDb = new Path(webGraphDb, INLINK_DIR);    Path tempInlinkDb = new Path(inlinkDb + "-" + Integer.toString(new Random().nextInt(Integer.MAX_VALUE)));    Job inlinkJob = NutchJob.getInstance(getConf());    Configuration inlinkJobConf = inlinkJob.getConfiguration();    inlinkJob.setJobName("Inlinkdb " + inlinkDb);        FileInputFormat.addInputPath(inlinkJob, outlinkDb);    inlinkJob.setInputFormatClass(SequenceFileInputFormat.class);    inlinkJob.setJarByClass(InlinkDb.class);    inlinkJob.setMapperClass(InlinkDb.InlinkDbMapper.class);    inlinkJob.setMapOutputKeyClass(Text.class);    inlinkJob.setMapOutputValueClass(LinkDatum.class);    inlinkJob.setOutputKeyClass(Text.class);    inlinkJob.setOutputValueClass(LinkDatum.class);    FileOutputFormat.setOutputPath(inlinkJob, tempInlinkDb);    inlinkJob.setOutputFormatClass(MapFileOutputFormat.class);    inlinkJobConf.setBoolean("mapreduce.fileoutputcommitter.marksuccessfuljobs", false);    try {                        boolean success = inlinkJob.waitForCompletion(true);        if (!success) {            String message = "InlinkDb job did not succeed, job status:" + inlinkJob.getStatus().getState() + ", reason: " + inlinkJob.getStatus().getFailureInfo();                        NutchJob.cleanupAfterFailure(tempInlinkDb, lock, fs);            throw new RuntimeException(message);        }                FSUtils.replace(fs, inlinkDb, tempInlinkDb, true);            } catch (IOException | InterruptedException | ClassNotFoundException e) {                        NutchJob.cleanupAfterFailure(tempInlinkDb, lock, fs);        throw e;    }        Path nodeDb = new Path(webGraphDb, NODE_DIR);    Path tempNodeDb = new Path(nodeDb + "-" + Integer.toString(new Random().nextInt(Integer.MAX_VALUE)));    Job nodeJob = NutchJob.getInstance(getConf());    Configuration nodeJobConf = nodeJob.getConfiguration();    nodeJob.setJobName("NodeDb " + nodeDb);            FileInputFormat.addInputPath(nodeJob, outlinkDb);    FileInputFormat.addInputPath(nodeJob, inlinkDb);    nodeJob.setInputFormatClass(SequenceFileInputFormat.class);    nodeJob.setJarByClass(NodeDb.class);    nodeJob.setReducerClass(NodeDb.NodeDbReducer.class);    nodeJob.setMapOutputKeyClass(Text.class);    nodeJob.setMapOutputValueClass(LinkDatum.class);    nodeJob.setOutputKeyClass(Text.class);    nodeJob.setOutputValueClass(Node.class);    FileOutputFormat.setOutputPath(nodeJob, tempNodeDb);    nodeJob.setOutputFormatClass(MapFileOutputFormat.class);    nodeJobConf.setBoolean("mapreduce.fileoutputcommitter.marksuccessfuljobs", false);    try {                        boolean success = nodeJob.waitForCompletion(true);        if (!success) {            String message = "NodeDb job did not succeed, job status:" + nodeJob.getStatus().getState() + ", reason: " + nodeJob.getStatus().getFailureInfo();                                    NutchJob.cleanupAfterFailure(tempNodeDb, lock, fs);            throw new RuntimeException(message);        }                FSUtils.replace(fs, nodeDb, tempNodeDb, true);            } catch (IOException | InterruptedException | ClassNotFoundException e) {                        NutchJob.cleanupAfterFailure(tempNodeDb, lock, fs);        throw e;    }        LockUtil.removeLockFile(fs, lock);    long end = System.currentTimeMillis();    }
1
public static void main(String[] args) throws Exception
{    int res = ToolRunner.run(NutchConfiguration.create(), new WebGraph(), args);    System.exit(res);}
0
public int run(String[] args) throws Exception
{        Option helpOpt = new Option("h", "help", false, "show this help message");    Option normOpt = new Option("n", "normalize", false, "whether to use URLNormalizers on the URL's in the segment");    Option filtOpt = new Option("f", "filter", false, "whether to use URLFilters on the URL's in the segment");        @SuppressWarnings("static-access")    Option graphOpt = OptionBuilder.withArgName("webgraphdb").hasArg().withDescription("the web graph database to create (if none exists) or use if one does").create("webgraphdb");    @SuppressWarnings("static-access")    Option segOpt = OptionBuilder.withArgName("segment").hasArgs().withDescription("the segment(s) to use").create("segment");    @SuppressWarnings("static-access")    Option segDirOpt = OptionBuilder.withArgName("segmentDir").hasArgs().withDescription("the segment directory to use").create("segmentDir");        Options options = new Options();    options.addOption(helpOpt);    options.addOption(normOpt);    options.addOption(filtOpt);    options.addOption(graphOpt);    options.addOption(segOpt);    options.addOption(segDirOpt);    CommandLineParser parser = new GnuParser();    try {        CommandLine line = parser.parse(options, args);        if (line.hasOption("help") || !line.hasOption("webgraphdb") || (!line.hasOption("segment") && !line.hasOption("segmentDir"))) {            HelpFormatter formatter = new HelpFormatter();            formatter.printHelp("WebGraph", options, true);            return -1;        }        String webGraphDb = line.getOptionValue("webgraphdb");        Path[] segPaths = null;                if (line.hasOption("segment")) {            String[] segments = line.getOptionValues("segment");            segPaths = new Path[segments.length];            for (int i = 0; i < segments.length; i++) {                segPaths[i] = new Path(segments[i]);            }        }                if (line.hasOption("segmentDir")) {            Path dir = new Path(line.getOptionValue("segmentDir"));            FileSystem fs = dir.getFileSystem(getConf());            FileStatus[] fstats = fs.listStatus(dir, HadoopFSUtil.getPassDirectoriesFilter(fs));            segPaths = HadoopFSUtil.getPaths(fstats);        }        boolean normalize = false;        if (line.hasOption("normalize")) {            normalize = true;        }        boolean filter = false;        if (line.hasOption("filter")) {            filter = true;        }        createWebGraph(new Path(webGraphDb), segPaths, normalize, filter);        return 0;    } catch (Exception e) {                return -2;    }}
1
public Text getCurrentValue()
{    return new Text();}
0
public Text getCurrentKey()
{    return new Text();}
0
public boolean nextKeyValue()
{    return false;}
0
public void initialize(InputSplit split, TaskAttemptContext context)
{}
0
public synchronized boolean next(Text key, Text value) throws IOException, InterruptedException
{        Text tKey = key;    if (!sequenceFileRecordReader.nextKeyValue()) {        return false;    }    tKey.set(innerKey.toString());    String contentAsStr = new String(innerValue.getContent());        contentAsStr = contentAsStr.replaceAll("\n", " ");    value.set(contentAsStr);    return true;}
0
public float getProgress() throws IOException
{    return sequenceFileRecordReader.getProgress();}
0
public synchronized void close() throws IOException
{    sequenceFileRecordReader.close();}
0
public RecordReader<Text, Text> getRecordReader(InputSplit split, Job job, Context context) throws IOException
{    context.setStatus(split.toString());    Configuration conf = job.getConfiguration();    return new ContentAsTextRecordReader(conf, (FileSplit) split);}
0
public static boolean isIndexable(Path segmentPath, FileSystem fs) throws IOException
{    if (segmentPath == null || fs == null) {                return false;    }    boolean checkResult = true;    checkResult &= checkSegmentDir(segmentPath, fs);    if (checkResult) {        return true;    } else {        return false;    }}
1
public static boolean checkSegmentDir(Path segmentPath, FileSystem fs) throws IOException
{    if (segmentPath.getName().length() != 14) {                return false;    }    FileStatus[] fstats_segment = fs.listStatus(segmentPath, HadoopFSUtil.getPassDirectoriesFilter(fs));    Path[] segment_files = HadoopFSUtil.getPaths(fstats_segment);    boolean crawlFetchExists = false;    boolean crawlParseExists = false;    boolean parseDataExists = false;    boolean parseTextExists = false;    for (Path path : segment_files) {        String pathName = path.getName();        crawlFetchExists |= pathName.equals(CrawlDatum.FETCH_DIR_NAME);        crawlParseExists |= pathName.equals(CrawlDatum.PARSE_DIR_NAME);        parseDataExists |= pathName.equals(ParseData.DIR_NAME);        parseTextExists |= pathName.equals(ParseText.DIR_NAME);    }    if (parseTextExists && crawlParseExists && crawlFetchExists && parseDataExists) {                        return true;    } else {                StringBuilder missingDir = new StringBuilder("");        if (parseDataExists == false) {            missingDir.append(ParseData.DIR_NAME + ", ");        }        if (parseTextExists == false) {            missingDir.append(ParseText.DIR_NAME + ", ");        }        if (crawlParseExists == false) {            missingDir.append(CrawlDatum.PARSE_DIR_NAME + ", ");        }        if (crawlFetchExists == false) {            missingDir.append(CrawlDatum.FETCH_DIR_NAME + ", ");        }        String missingDirString = missingDir.toString();                return false;    }}
1
public static boolean isParsed(Path segment, FileSystem fs) throws IOException
{    if (fs.exists(new Path(segment, CrawlDatum.PARSE_DIR_NAME))) {        return true;    }    return false;}
0
public boolean filter(Text key, CrawlDatum generateData, CrawlDatum fetchData, CrawlDatum sigData, Content content, ParseData parseData, ParseText parseText, Collection<CrawlDatum> linked)
{    for (SegmentMergeFilter filter : filters) {        if (!filter.filter(key, generateData, fetchData, sigData, content, parseData, parseText, linked)) {            if (LOG.isTraceEnabled())                LOG.trace("Key " + key + " dropped by " + filter.getClass().getName());            return false;        }    }    if (LOG.isTraceEnabled())        LOG.trace("Key " + key + " accepted for merge.");    return true;}
0
public RecordReader<Text, MetaWrapper> createRecordReader(final InputSplit split, TaskAttemptContext context) throws IOException
{    context.setStatus(split.toString());        SegmentPart segmentPart;    final String spString;    final FileSplit fSplit = (FileSplit) split;    try {        segmentPart = SegmentPart.get(fSplit);        spString = segmentPart.toString();    } catch (IOException e) {        throw new RuntimeException("Cannot identify segment:", e);    }    final SequenceFileRecordReader<Text, Writable> splitReader = new SequenceFileRecordReader<>();    return new SequenceFileRecordReader<Text, MetaWrapper>() {        private Text key;        private MetaWrapper wrapper;        private Writable w;        @Override        public void initialize(InputSplit split, TaskAttemptContext context) throws IOException, InterruptedException {            splitReader.initialize(split, context);        }        @Override        public synchronized boolean nextKeyValue() throws IOException, InterruptedException {            try {                boolean res = splitReader.nextKeyValue();                if (res == false) {                    return res;                }                key = splitReader.getCurrentKey();                w = splitReader.getCurrentValue();                wrapper = new MetaWrapper();                wrapper.set(w);                wrapper.setMeta(SEGMENT_PART_KEY, spString);                return res;            } catch (InterruptedException e) {                                throw e;            }        }        @Override        public Text getCurrentKey() {            return key;        }        @Override        public MetaWrapper getCurrentValue() {            return wrapper;        }        @Override        public synchronized void close() throws IOException {            splitReader.close();        }    };}
1
public void initialize(InputSplit split, TaskAttemptContext context) throws IOException, InterruptedException
{    splitReader.initialize(split, context);}
0
public synchronized boolean nextKeyValue() throws IOException, InterruptedException
{    try {        boolean res = splitReader.nextKeyValue();        if (res == false) {            return res;        }        key = splitReader.getCurrentKey();        w = splitReader.getCurrentValue();        wrapper = new MetaWrapper();        wrapper.set(w);        wrapper.setMeta(SEGMENT_PART_KEY, spString);        return res;    } catch (InterruptedException e) {                throw e;    }}
1
public Text getCurrentKey()
{    return key;}
0
public MetaWrapper getCurrentValue()
{    return wrapper;}
0
public synchronized void close() throws IOException
{    splitReader.close();}
0
public RecordWriter<Text, MetaWrapper> getRecordWriter(TaskAttemptContext context) throws IOException
{    Configuration conf = context.getConfiguration();    String name = getUniqueFile(context, "part", "");    Path dir = FileOutputFormat.getOutputPath(context);    FileSystem fs = dir.getFileSystem(context.getConfiguration());    return new RecordWriter<Text, MetaWrapper>() {        MapFile.Writer cOut = null;        MapFile.Writer fOut = null;        MapFile.Writer pdOut = null;        MapFile.Writer ptOut = null;        SequenceFile.Writer gOut = null;        SequenceFile.Writer pOut = null;        HashMap<String, Closeable> sliceWriters = new HashMap<>();        String segmentName = conf.get("segment.merger.segmentName");        public void write(Text key, MetaWrapper wrapper) throws IOException {                        SegmentPart sp = SegmentPart.parse(wrapper.getMeta(SEGMENT_PART_KEY));            Writable o = wrapper.get();            String slice = wrapper.getMeta(SEGMENT_SLICE_KEY);            if (o instanceof CrawlDatum) {                if (sp.partName.equals(CrawlDatum.GENERATE_DIR_NAME)) {                    gOut = ensureSequenceFile(slice, CrawlDatum.GENERATE_DIR_NAME);                    gOut.append(key, o);                } else if (sp.partName.equals(CrawlDatum.FETCH_DIR_NAME)) {                    fOut = ensureMapFile(slice, CrawlDatum.FETCH_DIR_NAME, CrawlDatum.class);                    fOut.append(key, o);                } else if (sp.partName.equals(CrawlDatum.PARSE_DIR_NAME)) {                    pOut = ensureSequenceFile(slice, CrawlDatum.PARSE_DIR_NAME);                    pOut.append(key, o);                } else {                    throw new IOException("Cannot determine segment part: " + sp.partName);                }            } else if (o instanceof Content) {                cOut = ensureMapFile(slice, Content.DIR_NAME, Content.class);                cOut.append(key, o);            } else if (o instanceof ParseData) {                                if (slice == null) {                    ((ParseData) o).getContentMeta().set(Nutch.SEGMENT_NAME_KEY, segmentName);                } else {                    ((ParseData) o).getContentMeta().set(Nutch.SEGMENT_NAME_KEY, segmentName + "-" + slice);                }                pdOut = ensureMapFile(slice, ParseData.DIR_NAME, ParseData.class);                pdOut.append(key, o);            } else if (o instanceof ParseText) {                ptOut = ensureMapFile(slice, ParseText.DIR_NAME, ParseText.class);                ptOut.append(key, o);            }        }                private SequenceFile.Writer ensureSequenceFile(String slice, String dirName) throws IOException {            if (slice == null)                slice = DEFAULT_SLICE;            SequenceFile.Writer res = (SequenceFile.Writer) sliceWriters.get(slice + dirName);            if (res != null)                return res;            Path wname;            Path out = FileOutputFormat.getOutputPath(context);            if (slice == DEFAULT_SLICE) {                wname = new Path(new Path(new Path(out, segmentName), dirName), name);            } else {                wname = new Path(new Path(new Path(out, segmentName + "-" + slice), dirName), name);            }            res = SequenceFile.createWriter(conf, SequenceFile.Writer.file(wname), SequenceFile.Writer.keyClass(Text.class), SequenceFile.Writer.valueClass(CrawlDatum.class), SequenceFile.Writer.bufferSize(fs.getConf().getInt("io.file.buffer.size", 4096)), SequenceFile.Writer.replication(fs.getDefaultReplication(wname)), SequenceFile.Writer.blockSize(1073741824), SequenceFile.Writer.compression(SequenceFileOutputFormat.getOutputCompressionType(context), new DefaultCodec()), SequenceFile.Writer.progressable((Progressable) context), SequenceFile.Writer.metadata(new Metadata()));            sliceWriters.put(slice + dirName, res);            return res;        }                private MapFile.Writer ensureMapFile(String slice, String dirName, Class<? extends Writable> clazz) throws IOException {            if (slice == null)                slice = DEFAULT_SLICE;            MapFile.Writer res = (MapFile.Writer) sliceWriters.get(slice + dirName);            if (res != null)                return res;            Path wname;            Path out = FileOutputFormat.getOutputPath(context);            if (slice == DEFAULT_SLICE) {                wname = new Path(new Path(new Path(out, segmentName), dirName), name);            } else {                wname = new Path(new Path(new Path(out, segmentName + "-" + slice), dirName), name);            }            CompressionType compType = SequenceFileOutputFormat.getOutputCompressionType(context);            if (clazz.isAssignableFrom(ParseText.class)) {                compType = CompressionType.RECORD;            }            Option rKeyClassOpt = MapFile.Writer.keyClass(Text.class);            org.apache.hadoop.io.SequenceFile.Writer.Option rValClassOpt = SequenceFile.Writer.valueClass(clazz);            org.apache.hadoop.io.SequenceFile.Writer.Option rProgressOpt = SequenceFile.Writer.progressable((Progressable) context);            org.apache.hadoop.io.SequenceFile.Writer.Option rCompOpt = SequenceFile.Writer.compression(compType);            res = new MapFile.Writer(conf, wname, rKeyClassOpt, rValClassOpt, rCompOpt, rProgressOpt);            sliceWriters.put(slice + dirName, res);            return res;        }        @Override        public void close(TaskAttemptContext context) throws IOException {            Iterator<Closeable> it = sliceWriters.values().iterator();            while (it.hasNext()) {                Object o = it.next();                if (o instanceof SequenceFile.Writer) {                    ((SequenceFile.Writer) o).close();                } else {                    ((MapFile.Writer) o).close();                }            }        }    };}
0
public void write(Text key, MetaWrapper wrapper) throws IOException
{        SegmentPart sp = SegmentPart.parse(wrapper.getMeta(SEGMENT_PART_KEY));    Writable o = wrapper.get();    String slice = wrapper.getMeta(SEGMENT_SLICE_KEY);    if (o instanceof CrawlDatum) {        if (sp.partName.equals(CrawlDatum.GENERATE_DIR_NAME)) {            gOut = ensureSequenceFile(slice, CrawlDatum.GENERATE_DIR_NAME);            gOut.append(key, o);        } else if (sp.partName.equals(CrawlDatum.FETCH_DIR_NAME)) {            fOut = ensureMapFile(slice, CrawlDatum.FETCH_DIR_NAME, CrawlDatum.class);            fOut.append(key, o);        } else if (sp.partName.equals(CrawlDatum.PARSE_DIR_NAME)) {            pOut = ensureSequenceFile(slice, CrawlDatum.PARSE_DIR_NAME);            pOut.append(key, o);        } else {            throw new IOException("Cannot determine segment part: " + sp.partName);        }    } else if (o instanceof Content) {        cOut = ensureMapFile(slice, Content.DIR_NAME, Content.class);        cOut.append(key, o);    } else if (o instanceof ParseData) {                if (slice == null) {            ((ParseData) o).getContentMeta().set(Nutch.SEGMENT_NAME_KEY, segmentName);        } else {            ((ParseData) o).getContentMeta().set(Nutch.SEGMENT_NAME_KEY, segmentName + "-" + slice);        }        pdOut = ensureMapFile(slice, ParseData.DIR_NAME, ParseData.class);        pdOut.append(key, o);    } else if (o instanceof ParseText) {        ptOut = ensureMapFile(slice, ParseText.DIR_NAME, ParseText.class);        ptOut.append(key, o);    }}
0
private SequenceFile.Writer ensureSequenceFile(String slice, String dirName) throws IOException
{    if (slice == null)        slice = DEFAULT_SLICE;    SequenceFile.Writer res = (SequenceFile.Writer) sliceWriters.get(slice + dirName);    if (res != null)        return res;    Path wname;    Path out = FileOutputFormat.getOutputPath(context);    if (slice == DEFAULT_SLICE) {        wname = new Path(new Path(new Path(out, segmentName), dirName), name);    } else {        wname = new Path(new Path(new Path(out, segmentName + "-" + slice), dirName), name);    }    res = SequenceFile.createWriter(conf, SequenceFile.Writer.file(wname), SequenceFile.Writer.keyClass(Text.class), SequenceFile.Writer.valueClass(CrawlDatum.class), SequenceFile.Writer.bufferSize(fs.getConf().getInt("io.file.buffer.size", 4096)), SequenceFile.Writer.replication(fs.getDefaultReplication(wname)), SequenceFile.Writer.blockSize(1073741824), SequenceFile.Writer.compression(SequenceFileOutputFormat.getOutputCompressionType(context), new DefaultCodec()), SequenceFile.Writer.progressable((Progressable) context), SequenceFile.Writer.metadata(new Metadata()));    sliceWriters.put(slice + dirName, res);    return res;}
0
private MapFile.Writer ensureMapFile(String slice, String dirName, Class<? extends Writable> clazz) throws IOException
{    if (slice == null)        slice = DEFAULT_SLICE;    MapFile.Writer res = (MapFile.Writer) sliceWriters.get(slice + dirName);    if (res != null)        return res;    Path wname;    Path out = FileOutputFormat.getOutputPath(context);    if (slice == DEFAULT_SLICE) {        wname = new Path(new Path(new Path(out, segmentName), dirName), name);    } else {        wname = new Path(new Path(new Path(out, segmentName + "-" + slice), dirName), name);    }    CompressionType compType = SequenceFileOutputFormat.getOutputCompressionType(context);    if (clazz.isAssignableFrom(ParseText.class)) {        compType = CompressionType.RECORD;    }    Option rKeyClassOpt = MapFile.Writer.keyClass(Text.class);    org.apache.hadoop.io.SequenceFile.Writer.Option rValClassOpt = SequenceFile.Writer.valueClass(clazz);    org.apache.hadoop.io.SequenceFile.Writer.Option rProgressOpt = SequenceFile.Writer.progressable((Progressable) context);    org.apache.hadoop.io.SequenceFile.Writer.Option rCompOpt = SequenceFile.Writer.compression(compType);    res = new MapFile.Writer(conf, wname, rKeyClassOpt, rValClassOpt, rCompOpt, rProgressOpt);    sliceWriters.put(slice + dirName, res);    return res;}
0
public void close(TaskAttemptContext context) throws IOException
{    Iterator<Closeable> it = sliceWriters.values().iterator();    while (it.hasNext()) {        Object o = it.next();        if (o instanceof SequenceFile.Writer) {            ((SequenceFile.Writer) o).close();        } else {            ((MapFile.Writer) o).close();        }    }}
0
public void setConf(Configuration conf)
{    super.setConf(conf);}
0
public void close() throws IOException
{}
0
public void setup(Mapper<Text, MetaWrapper, Text, MetaWrapper>.Context context)
{    Configuration conf = context.getConfiguration();    if (conf.getBoolean("segment.merger.filter", false)) {        filters = new URLFilters(conf);    }    if (conf.getBoolean("segment.merger.normalizer", false))        normalizers = new URLNormalizers(conf, URLNormalizers.SCOPE_DEFAULT);}
0
public void map(Text key, MetaWrapper value, Context context) throws IOException, InterruptedException
{    Text newKey = new Text();    String url = key.toString();    if (normalizers != null) {        try {                        url = normalizers.normalize(url, URLNormalizers.SCOPE_DEFAULT);        } catch (Exception e) {                        url = null;        }    }    if (url != null && filters != null) {        try {            url = filters.filter(url);        } catch (Exception e) {                        url = null;        }    }    if (url != null) {        newKey.set(url);        context.write(newKey, value);    }}
1
public void setup(Reducer<Text, MetaWrapper, Text, MetaWrapper>.Context context)
{    Configuration conf = context.getConfiguration();    if (conf.getBoolean("segment.merger.filter", false)) {        mergeFilters = new SegmentMergeFilters(conf);    }    sliceSize = conf.getLong("segment.merger.slice", -1);    if ((sliceSize > 0) && (LOG.isInfoEnabled())) {            }    if (sliceSize > 0) {        sliceSize = sliceSize / Integer.parseInt(conf.get("mapreduce.job.reduces"));    }}
1
public void reduce(Text key, Iterable<MetaWrapper> values, Context context) throws IOException, InterruptedException
{    CrawlDatum lastG = null;    CrawlDatum lastF = null;    CrawlDatum lastSig = null;    Content lastC = null;    ParseData lastPD = null;    ParseText lastPT = null;    String lastGname = null;    String lastFname = null;    String lastSigname = null;    String lastCname = null;    String lastPDname = null;    String lastPTname = null;    TreeMap<String, ArrayList<CrawlDatum>> linked = new TreeMap<>();    for (MetaWrapper wrapper : values) {        Object o = wrapper.get();        String spString = wrapper.getMeta(SEGMENT_PART_KEY);        if (spString == null) {            throw new IOException("Null segment part, key=" + key);        }        SegmentPart sp = SegmentPart.parse(spString);        if (o instanceof CrawlDatum) {            CrawlDatum val = (CrawlDatum) o;                        if (sp.partName.equals(CrawlDatum.GENERATE_DIR_NAME)) {                if (lastG == null) {                    lastG = val;                    lastGname = sp.segmentName;                } else {                                        if (lastGname.compareTo(sp.segmentName) < 0) {                        lastG = val;                        lastGname = sp.segmentName;                    }                }            } else if (sp.partName.equals(CrawlDatum.FETCH_DIR_NAME)) {                                if (CrawlDatum.hasFetchStatus(val) && val.getStatus() != CrawlDatum.STATUS_FETCH_RETRY && val.getStatus() != CrawlDatum.STATUS_FETCH_NOTMODIFIED) {                    if (lastF == null) {                        lastF = val;                        lastFname = sp.segmentName;                    } else {                        if (lastFname.compareTo(sp.segmentName) < 0) {                            lastF = val;                            lastFname = sp.segmentName;                        }                    }                }            } else if (sp.partName.equals(CrawlDatum.PARSE_DIR_NAME)) {                if (val.getStatus() == CrawlDatum.STATUS_SIGNATURE) {                    if (lastSig == null) {                        lastSig = val;                        lastSigname = sp.segmentName;                    } else {                                                if (lastSigname.compareTo(sp.segmentName) < 0) {                            lastSig = val;                            lastSigname = sp.segmentName;                        }                    }                    continue;                }                                ArrayList<CrawlDatum> segLinked = linked.get(sp.segmentName);                if (segLinked == null) {                    segLinked = new ArrayList<>();                    linked.put(sp.segmentName, segLinked);                }                segLinked.add(val);            } else {                throw new IOException("Cannot determine segment part: " + sp.partName);            }        } else if (o instanceof Content) {            if (lastC == null) {                lastC = (Content) o;                lastCname = sp.segmentName;            } else {                if (lastCname.compareTo(sp.segmentName) < 0) {                    lastC = (Content) o;                    lastCname = sp.segmentName;                }            }        } else if (o instanceof ParseData) {            if (lastPD == null) {                lastPD = (ParseData) o;                lastPDname = sp.segmentName;            } else {                if (lastPDname.compareTo(sp.segmentName) < 0) {                    lastPD = (ParseData) o;                    lastPDname = sp.segmentName;                }            }        } else if (o instanceof ParseText) {            if (lastPT == null) {                lastPT = (ParseText) o;                lastPTname = sp.segmentName;            } else {                if (lastPTname.compareTo(sp.segmentName) < 0) {                    lastPT = (ParseText) o;                    lastPTname = sp.segmentName;                }            }        }    }        if (mergeFilters != null && !mergeFilters.filter(key, lastG, lastF, lastSig, lastC, lastPD, lastPT, linked.isEmpty() ? null : linked.lastEntry().getValue())) {        return;    }    curCount++;    String sliceName;    MetaWrapper wrapper = new MetaWrapper();    if (sliceSize > 0) {        sliceName = String.valueOf(curCount / sliceSize);        wrapper.setMeta(SEGMENT_SLICE_KEY, sliceName);    }    SegmentPart sp = new SegmentPart();        if (lastG != null) {        wrapper.set(lastG);        sp.partName = CrawlDatum.GENERATE_DIR_NAME;        sp.segmentName = lastGname;        wrapper.setMeta(SEGMENT_PART_KEY, sp.toString());        context.write(key, wrapper);    }    if (lastF != null) {        wrapper.set(lastF);        sp.partName = CrawlDatum.FETCH_DIR_NAME;        sp.segmentName = lastFname;        wrapper.setMeta(SEGMENT_PART_KEY, sp.toString());        context.write(key, wrapper);    }    if (lastSig != null) {        wrapper.set(lastSig);        sp.partName = CrawlDatum.PARSE_DIR_NAME;        sp.segmentName = lastSigname;        wrapper.setMeta(SEGMENT_PART_KEY, sp.toString());        context.write(key, wrapper);    }    if (lastC != null) {        wrapper.set(lastC);        sp.partName = Content.DIR_NAME;        sp.segmentName = lastCname;        wrapper.setMeta(SEGMENT_PART_KEY, sp.toString());        context.write(key, wrapper);    }    if (lastPD != null) {        wrapper.set(lastPD);        sp.partName = ParseData.DIR_NAME;        sp.segmentName = lastPDname;        wrapper.setMeta(SEGMENT_PART_KEY, sp.toString());        context.write(key, wrapper);    }    if (lastPT != null) {        wrapper.set(lastPT);        sp.partName = ParseText.DIR_NAME;        sp.segmentName = lastPTname;        wrapper.setMeta(SEGMENT_PART_KEY, sp.toString());        context.write(key, wrapper);    }    if (linked.size() > 0) {        String name = linked.lastKey();        sp.partName = CrawlDatum.PARSE_DIR_NAME;        sp.segmentName = name;        wrapper.setMeta(SEGMENT_PART_KEY, sp.toString());        ArrayList<CrawlDatum> segLinked = linked.get(name);        for (int i = 0; i < segLinked.size(); i++) {            CrawlDatum link = segLinked.get(i);            wrapper.set(link);            context.write(key, wrapper);        }    }}
0
public void merge(Path out, Path[] segs, boolean filter, boolean normalize, long slice) throws IOException, ClassNotFoundException, InterruptedException
{    String segmentName = Generator.generateSegmentName();    if (LOG.isInfoEnabled()) {            }    Job job = NutchJob.getInstance(getConf());    Configuration conf = job.getConfiguration();    job.setJobName("mergesegs " + out + "/" + segmentName);    conf.setBoolean("segment.merger.filter", filter);    conf.setBoolean("segment.merger.normalizer", normalize);    conf.setLong("segment.merger.slice", slice);    conf.set("segment.merger.segmentName", segmentName);        boolean g = true;    boolean f = true;    boolean p = true;    boolean c = true;    boolean pd = true;    boolean pt = true;        boolean pg = true;    boolean pf = true;    boolean pp = true;    boolean pc = true;    boolean ppd = true;    boolean ppt = true;    for (int i = 0; i < segs.length; i++) {        FileSystem fs = segs[i].getFileSystem(conf);        if (!fs.exists(segs[i])) {            if (LOG.isWarnEnabled()) {                            }            segs[i] = null;            continue;        }        if (LOG.isInfoEnabled()) {                    }        Path cDir = new Path(segs[i], Content.DIR_NAME);        Path gDir = new Path(segs[i], CrawlDatum.GENERATE_DIR_NAME);        Path fDir = new Path(segs[i], CrawlDatum.FETCH_DIR_NAME);        Path pDir = new Path(segs[i], CrawlDatum.PARSE_DIR_NAME);        Path pdDir = new Path(segs[i], ParseData.DIR_NAME);        Path ptDir = new Path(segs[i], ParseText.DIR_NAME);        c = c && fs.exists(cDir);        g = g && fs.exists(gDir);        f = f && fs.exists(fDir);        p = p && fs.exists(pDir);        pd = pd && fs.exists(pdDir);        pt = pt && fs.exists(ptDir);                if (g != pg || f != pf || p != pp || c != pc || pd != ppd || pt != ppt) {                    }        pg = g;        pf = f;        pp = p;        pc = c;        ppd = pd;        ppt = pt;    }    StringBuilder sb = new StringBuilder();    if (c)        sb.append(" " + Content.DIR_NAME);    if (g)        sb.append(" " + CrawlDatum.GENERATE_DIR_NAME);    if (f)        sb.append(" " + CrawlDatum.FETCH_DIR_NAME);    if (p)        sb.append(" " + CrawlDatum.PARSE_DIR_NAME);    if (pd)        sb.append(" " + ParseData.DIR_NAME);    if (pt)        sb.append(" " + ParseText.DIR_NAME);    if (LOG.isInfoEnabled()) {            }    for (int i = 0; i < segs.length; i++) {        if (segs[i] == null)            continue;        if (g) {            Path gDir = new Path(segs[i], CrawlDatum.GENERATE_DIR_NAME);            FileInputFormat.addInputPath(job, gDir);        }        if (c) {            Path cDir = new Path(segs[i], Content.DIR_NAME);            FileInputFormat.addInputPath(job, cDir);        }        if (f) {            Path fDir = new Path(segs[i], CrawlDatum.FETCH_DIR_NAME);            FileInputFormat.addInputPath(job, fDir);        }        if (p) {            Path pDir = new Path(segs[i], CrawlDatum.PARSE_DIR_NAME);            FileInputFormat.addInputPath(job, pDir);        }        if (pd) {            Path pdDir = new Path(segs[i], ParseData.DIR_NAME);            FileInputFormat.addInputPath(job, pdDir);        }        if (pt) {            Path ptDir = new Path(segs[i], ParseText.DIR_NAME);            FileInputFormat.addInputPath(job, ptDir);        }    }    job.setInputFormatClass(ObjectInputFormat.class);    job.setJarByClass(SegmentMerger.class);    job.setMapperClass(SegmentMerger.SegmentMergerMapper.class);    job.setReducerClass(SegmentMerger.SegmentMergerReducer.class);    FileOutputFormat.setOutputPath(job, out);    job.setOutputKeyClass(Text.class);    job.setOutputValueClass(MetaWrapper.class);    job.setOutputFormatClass(SegmentOutputFormat.class);    setConf(conf);    try {        boolean success = job.waitForCompletion(true);        if (!success) {            String message = "SegmentMerger job did not succeed, job status:" + job.getStatus().getState() + ", reason: " + job.getStatus().getFailureInfo();                        throw new RuntimeException(message);        }    } catch (IOException | InterruptedException | ClassNotFoundException e) {                throw e;    }}
1
public int run(String[] args) throws Exception
{    if (args.length < 2) {        System.err.println("SegmentMerger output_dir (-dir segments | seg1 seg2 ...) [-filter] [-slice NNNN]");        System.err.println("\toutput_dir\tname of the parent dir for output segment slice(s)");        System.err.println("\t-dir segments\tparent dir containing several segments");        System.err.println("\tseg1 seg2 ...\tlist of segment dirs");        System.err.println("\t-filter\t\tfilter out URL-s prohibited by current URLFilters");        System.err.println("\t-normalize\t\tnormalize URL via current URLNormalizers");        System.err.println("\t-slice NNNN\tcreate many output segments, each containing NNNN URLs");        return -1;    }    Configuration conf = NutchConfiguration.create();    Path out = new Path(args[0]);    ArrayList<Path> segs = new ArrayList<>();    long sliceSize = 0;    boolean filter = false;    boolean normalize = false;    for (int i = 1; i < args.length; i++) {        if ("-dir".equals(args[i])) {            Path dirPath = new Path(args[++i]);            FileSystem fs = dirPath.getFileSystem(conf);            FileStatus[] fstats = fs.listStatus(dirPath, HadoopFSUtil.getPassDirectoriesFilter(fs));            Path[] files = HadoopFSUtil.getPaths(fstats);            for (int j = 0; j < files.length; j++) segs.add(files[j]);        } else if ("-filter".equals(args[i])) {            filter = true;        } else if ("-normalize".equals(args[i])) {            normalize = true;        } else if ("-slice".equals(args[i])) {            sliceSize = Long.parseLong(args[++i]);        } else {            segs.add(new Path(args[i]));        }    }    if (segs.isEmpty()) {        System.err.println("ERROR: No input segments.");        return -1;    }    merge(out, segs.toArray(new Path[segs.size()]), filter, normalize, sliceSize);    return 0;}
0
public static void main(String[] args) throws Exception
{    int result = ToolRunner.run(NutchConfiguration.create(), new SegmentMerger(), args);    System.exit(result);}
0
public String toString()
{    return segmentName + "/" + partName;}
0
public static SegmentPart get(FileSplit split) throws IOException
{    return get(split.getPath().toString());}
0
public static SegmentPart get(String path) throws IOException
{        String dir = path.replace('\\', '/');    int idx = dir.lastIndexOf("/part-");    if (idx == -1) {        throw new IOException("Cannot determine segment part: " + dir);    }    dir = dir.substring(0, idx);    idx = dir.lastIndexOf('/');    if (idx == -1) {        throw new IOException("Cannot determine segment part: " + dir);    }    String part = dir.substring(idx + 1);        dir = dir.substring(0, idx);    idx = dir.lastIndexOf('/');    if (idx == -1) {        throw new IOException("Cannot determine segment name: " + dir);    }    String segment = dir.substring(idx + 1);    return new SegmentPart(segment, part);}
0
public static SegmentPart parse(String string) throws IOException
{    int idx = string.indexOf('/');    if (idx == -1) {        throw new IOException("Invalid SegmentPart: '" + string + "'");    }    String segment = string.substring(0, idx);    String part = string.substring(idx + 1);    return new SegmentPart(segment, part);}
0
public void map(WritableComparable<?> key, Writable value, Context context) throws IOException, InterruptedException
{        if (key instanceof Text) {        newKey.set(key.toString());        key = newKey;    }    context.write((Text) key, new NutchWritable(value));}
0
public RecordWriter<WritableComparable<?>, Writable> getRecordWriter(TaskAttemptContext context) throws IOException, InterruptedException
{    String name = getUniqueFile(context, "part", "");    Path dir = FileOutputFormat.getOutputPath(context);    FileSystem fs = dir.getFileSystem(context.getConfiguration());    final Path segmentDumpFile = new Path(FileOutputFormat.getOutputPath(context), name);        if (fs.exists(segmentDumpFile))        fs.delete(segmentDumpFile, true);    final PrintStream printStream = new PrintStream(fs.create(segmentDumpFile), false, StandardCharsets.UTF_8.name());    return new RecordWriter<WritableComparable<?>, Writable>() {        public synchronized void write(WritableComparable<?> key, Writable value) throws IOException {            printStream.println(value);        }        public synchronized void close(TaskAttemptContext context) throws IOException {            printStream.close();        }    };}
0
public synchronized void write(WritableComparable<?> key, Writable value) throws IOException
{    printStream.println(value);}
0
public synchronized void close(TaskAttemptContext context) throws IOException
{    printStream.close();}
0
public void setup(Job job)
{    Configuration conf = job.getConfiguration();    this.co = conf.getBoolean("segment.reader.co", true);    this.fe = conf.getBoolean("segment.reader.fe", true);    this.ge = conf.getBoolean("segment.reader.ge", true);    this.pa = conf.getBoolean("segment.reader.pa", true);    this.pd = conf.getBoolean("segment.reader.pd", true);    this.pt = conf.getBoolean("segment.reader.pt", true);}
0
public void close()
{}
0
public void reduce(Text key, Iterable<NutchWritable> values, Context context) throws IOException, InterruptedException
{    StringBuffer dump = new StringBuffer();    dump.append("\nRecno:: ").append(recNo++).append("\n");    dump.append("URL:: " + key.toString() + "\n");    for (NutchWritable val : values) {                Writable value = val.get();        if (value instanceof CrawlDatum) {            dump.append("\nCrawlDatum::\n").append(((CrawlDatum) value).toString());        } else if (value instanceof Content) {            dump.append("\nContent::\n").append(((Content) value).toString());        } else if (value instanceof ParseData) {            dump.append("\nParseData::\n").append(((ParseData) value).toString());        } else if (value instanceof ParseText) {            dump.append("\nParseText::\n").append(((ParseText) value).toString());        } else if (LOG.isWarnEnabled()) {                    }    }    context.write(key, new Text(dump.toString()));}
1
public void dump(Path segment, Path output) throws IOException, InterruptedException, ClassNotFoundException
{    if (LOG.isInfoEnabled()) {            }    Job job = Job.getInstance();    job.setJobName("read " + segment);    Configuration conf = job.getConfiguration();    if (ge)        FileInputFormat.addInputPath(job, new Path(segment, CrawlDatum.GENERATE_DIR_NAME));    if (fe)        FileInputFormat.addInputPath(job, new Path(segment, CrawlDatum.FETCH_DIR_NAME));    if (pa)        FileInputFormat.addInputPath(job, new Path(segment, CrawlDatum.PARSE_DIR_NAME));    if (co)        FileInputFormat.addInputPath(job, new Path(segment, Content.DIR_NAME));    if (pd)        FileInputFormat.addInputPath(job, new Path(segment, ParseData.DIR_NAME));    if (pt)        FileInputFormat.addInputPath(job, new Path(segment, ParseText.DIR_NAME));    job.setInputFormatClass(SequenceFileInputFormat.class);    job.setMapperClass(InputCompatMapper.class);    job.setReducerClass(InputCompatReducer.class);    job.setJarByClass(SegmentReader.class);    Path tempDir = new Path(conf.get("hadoop.tmp.dir", "/tmp") + "/segread-" + new java.util.Random().nextInt());    FileSystem fs = tempDir.getFileSystem(conf);    fs.delete(tempDir, true);    FileOutputFormat.setOutputPath(job, tempDir);    job.setOutputFormatClass(TextOutputFormat.class);    job.setOutputKeyClass(Text.class);    job.setOutputValueClass(NutchWritable.class);    try {        boolean success = job.waitForCompletion(true);        if (!success) {            String message = "SegmentReader job did not succeed, job status:" + job.getStatus().getState() + ", reason: " + job.getStatus().getFailureInfo();                        throw new RuntimeException(message);        }    } catch (IOException | InterruptedException | ClassNotFoundException e) {                throw e;    }        Path dumpFile = new Path(output, conf.get("segment.dump.dir", "dump"));    FileSystem outFs = dumpFile.getFileSystem(conf);        outFs.delete(dumpFile, true);    FileStatus[] fstats = fs.listStatus(tempDir, HadoopFSUtil.getPassAllFilter());    Path[] files = HadoopFSUtil.getPaths(fstats);    int currentRecordNumber = 0;    if (files.length > 0) {        try (PrintWriter writer = new PrintWriter(new BufferedWriter(new OutputStreamWriter(outFs.create(dumpFile), StandardCharsets.UTF_8)))) {            for (int i = 0; i < files.length; i++) {                Path partFile = files[i];                try {                    currentRecordNumber = append(fs, conf, partFile, writer, currentRecordNumber);                } catch (IOException exception) {                    if (LOG.isWarnEnabled()) {                                                                    }                }            }        }    }    fs.delete(tempDir, true);    if (LOG.isInfoEnabled()) {            }}
1
private int append(FileSystem fs, Configuration conf, Path src, PrintWriter writer, int currentRecordNumber) throws IOException
{    try (BufferedReader reader = new BufferedReader(new InputStreamReader(fs.open(src), StandardCharsets.UTF_8))) {        String line = reader.readLine();        while (line != null) {            if (line.startsWith("Recno:: ")) {                line = "Recno:: " + currentRecordNumber++;            }            writer.println(line);            line = reader.readLine();        }        return currentRecordNumber;    }}
0
public void get(final Path segment, final Text key, Writer writer, final Map<String, List<Writable>> results) throws Exception
{        ArrayList<Thread> threads = new ArrayList<>();    if (co)        threads.add(new Thread() {            public void run() {                try {                    List<Writable> res = getMapRecords(new Path(segment, Content.DIR_NAME), key);                    results.put("co", res);                } catch (Exception e) {                                    }            }        });    if (fe)        threads.add(new Thread() {            public void run() {                try {                    List<Writable> res = getMapRecords(new Path(segment, CrawlDatum.FETCH_DIR_NAME), key);                    results.put("fe", res);                } catch (Exception e) {                                    }            }        });    if (ge)        threads.add(new Thread() {            public void run() {                try {                    List<Writable> res = getSeqRecords(new Path(segment, CrawlDatum.GENERATE_DIR_NAME), key);                    results.put("ge", res);                } catch (Exception e) {                                    }            }        });    if (pa)        threads.add(new Thread() {            public void run() {                try {                    List<Writable> res = getSeqRecords(new Path(segment, CrawlDatum.PARSE_DIR_NAME), key);                    results.put("pa", res);                } catch (Exception e) {                                    }            }        });    if (pd)        threads.add(new Thread() {            public void run() {                try {                    List<Writable> res = getMapRecords(new Path(segment, ParseData.DIR_NAME), key);                    results.put("pd", res);                } catch (Exception e) {                                    }            }        });    if (pt)        threads.add(new Thread() {            public void run() {                try {                    List<Writable> res = getMapRecords(new Path(segment, ParseText.DIR_NAME), key);                    results.put("pt", res);                } catch (Exception e) {                                    }            }        });    Iterator<Thread> it = threads.iterator();    while (it.hasNext()) it.next().start();    int cnt;    do {        cnt = 0;        try {            Thread.sleep(5000);        } catch (Exception e) {        }        ;        it = threads.iterator();        while (it.hasNext()) {            if (it.next().isAlive())                cnt++;        }        if ((cnt > 0) && (LOG.isDebugEnabled())) {                    }    } while (cnt > 0);    for (int i = 0; i < keys.length; i++) {        List<Writable> res = results.get(keys[i][0]);        if (res != null && res.size() > 0) {            for (int k = 0; k < res.size(); k++) {                writer.write(keys[i][1]);                writer.write(res.get(k) + "\n");            }        }        writer.flush();    }}
1
public void run()
{    try {        List<Writable> res = getMapRecords(new Path(segment, Content.DIR_NAME), key);        results.put("co", res);    } catch (Exception e) {            }}
1
public void run()
{    try {        List<Writable> res = getMapRecords(new Path(segment, CrawlDatum.FETCH_DIR_NAME), key);        results.put("fe", res);    } catch (Exception e) {            }}
1
public void run()
{    try {        List<Writable> res = getSeqRecords(new Path(segment, CrawlDatum.GENERATE_DIR_NAME), key);        results.put("ge", res);    } catch (Exception e) {            }}
1
public void run()
{    try {        List<Writable> res = getSeqRecords(new Path(segment, CrawlDatum.PARSE_DIR_NAME), key);        results.put("pa", res);    } catch (Exception e) {            }}
1
public void run()
{    try {        List<Writable> res = getMapRecords(new Path(segment, ParseData.DIR_NAME), key);        results.put("pd", res);    } catch (Exception e) {            }}
1
public void run()
{    try {        List<Writable> res = getMapRecords(new Path(segment, ParseText.DIR_NAME), key);        results.put("pt", res);    } catch (Exception e) {            }}
1
private List<Writable> getMapRecords(Path dir, Text key) throws Exception
{    MapFile.Reader[] readers = MapFileOutputFormat.getReaders(dir, getConf());    ArrayList<Writable> res = new ArrayList<>();    Class<?> keyClass = readers[0].getKeyClass();    Class<?> valueClass = readers[0].getValueClass();    if (!keyClass.getName().equals("org.apache.hadoop.io.Text"))        throw new IOException("Incompatible key (" + keyClass.getName() + ")");    Writable value = (Writable) valueClass.getConstructor().newInstance();        for (int i = 0; i < readers.length; i++) {        if (readers[i].get(key, value) != null) {            res.add(value);            value = (Writable) valueClass.getConstructor().newInstance();            Text aKey = (Text) keyClass.getConstructor().newInstance();            while (readers[i].next(aKey, value) && aKey.equals(key)) {                res.add(value);                value = (Writable) valueClass.getConstructor().newInstance();            }        }        readers[i].close();    }    return res;}
0
private List<Writable> getSeqRecords(Path dir, Text key) throws Exception
{    SequenceFile.Reader[] readers = org.apache.hadoop.mapred.SequenceFileOutputFormat.getReaders(getConf(), dir);    ArrayList<Writable> res = new ArrayList<>();    Class<?> keyClass = readers[0].getKeyClass();    Class<?> valueClass = readers[0].getValueClass();    if (!keyClass.getName().equals("org.apache.hadoop.io.Text"))        throw new IOException("Incompatible key (" + keyClass.getName() + ")");    WritableComparable<?> aKey = (WritableComparable<?>) keyClass.getConstructor().newInstance();    Writable value = (Writable) valueClass.getConstructor().newInstance();    for (int i = 0; i < readers.length; i++) {        while (readers[i].next(aKey, value)) {            if (aKey.equals(key)) {                res.add(value);                value = (Writable) valueClass.getConstructor().newInstance();            }        }        readers[i].close();    }    return res;}
0
public void list(List<Path> dirs, Writer writer) throws Exception
{    writer.write("NAME\t\tGENERATED\tFETCHER START\t\tFETCHER END\t\tFETCHED\tPARSED\n");    for (int i = 0; i < dirs.size(); i++) {        Path dir = dirs.get(i);        SegmentReaderStats stats = new SegmentReaderStats();        getStats(dir, stats);        writer.write(dir.getName() + "\t");        if (stats.generated == -1)            writer.write("?");        else            writer.write(stats.generated + "");        writer.write("\t\t");        if (stats.start == -1)            writer.write("?\t");        else            writer.write(sdf.format(new Date(stats.start)));        writer.write("\t");        if (stats.end == -1)            writer.write("?");        else            writer.write(sdf.format(new Date(stats.end)));        writer.write("\t");        if (stats.fetched == -1)            writer.write("?");        else            writer.write(stats.fetched + "");        writer.write("\t");        if (stats.parsed == -1)            writer.write("?");        else            writer.write(stats.parsed + "");        writer.write("\n");        writer.flush();    }}
0
public void getStats(Path segment, final SegmentReaderStats stats) throws Exception
{    long cnt = 0L;    Text key = new Text();    CrawlDatum val = new CrawlDatum();    FileSystem fs = segment.getFileSystem(getConf());    if (ge) {        SequenceFile.Reader[] readers = SegmentReaderUtil.getReaders(new Path(segment, CrawlDatum.GENERATE_DIR_NAME), getConf());        for (int i = 0; i < readers.length; i++) {            while (readers[i].next(key, val)) cnt++;            readers[i].close();        }        stats.generated = cnt;    }    if (fe) {        Path fetchDir = new Path(segment, CrawlDatum.FETCH_DIR_NAME);        if (fs.exists(fetchDir) && fs.getFileStatus(fetchDir).isDirectory()) {            cnt = 0L;            long start = Long.MAX_VALUE;            long end = Long.MIN_VALUE;            CrawlDatum value = new CrawlDatum();            MapFile.Reader[] mreaders = MapFileOutputFormat.getReaders(fetchDir, getConf());            for (int i = 0; i < mreaders.length; i++) {                while (mreaders[i].next(key, value)) {                    cnt++;                    if (value.getFetchTime() < start)                        start = value.getFetchTime();                    if (value.getFetchTime() > end)                        end = value.getFetchTime();                }                mreaders[i].close();            }            stats.start = start;            stats.end = end;            stats.fetched = cnt;        }    }    if (pd) {        Path parseDir = new Path(segment, ParseData.DIR_NAME);        if (fs.exists(parseDir) && fs.getFileStatus(parseDir).isDirectory()) {            cnt = 0L;            long errors = 0L;            ParseData value = new ParseData();            MapFile.Reader[] mreaders = MapFileOutputFormat.getReaders(parseDir, getConf());            for (int i = 0; i < mreaders.length; i++) {                while (mreaders[i].next(key, value)) {                    cnt++;                    if (!value.getStatus().isSuccess())                        errors++;                }                mreaders[i].close();            }            stats.parsed = cnt;            stats.parseErrors = errors;        }    }}
0
public int run(String[] args) throws Exception
{    if (args.length < 2) {        usage();        return -1;    }    int mode = -1;    if (args[0].equals("-dump"))        mode = MODE_DUMP;    else if (args[0].equals("-list"))        mode = MODE_LIST;    else if (args[0].equals("-get"))        mode = MODE_GET;    boolean co = true;    boolean fe = true;    boolean ge = true;    boolean pa = true;    boolean pd = true;    boolean pt = true;        for (int i = 1; i < args.length; i++) {        if (args[i].equals("-nocontent")) {            co = false;            args[i] = null;        } else if (args[i].equals("-nofetch")) {            fe = false;            args[i] = null;        } else if (args[i].equals("-nogenerate")) {            ge = false;            args[i] = null;        } else if (args[i].equals("-noparse")) {            pa = false;            args[i] = null;        } else if (args[i].equals("-noparsedata")) {            pd = false;            args[i] = null;        } else if (args[i].equals("-noparsetext")) {            pt = false;            args[i] = null;        }    }    Configuration conf = NutchConfiguration.create();    SegmentReader segmentReader = new SegmentReader(conf, co, fe, ge, pa, pd, pt);        switch(mode) {        case MODE_DUMP:            this.co = co;            this.fe = fe;            this.ge = ge;            this.pa = pa;            this.pd = pd;            this.pt = pt;            String input = args[1];            if (input == null) {                System.err.println("Missing required argument: <segment_dir>");                usage();                return -1;            }            String output = args.length > 2 ? args[2] : null;            if (output == null) {                System.err.println("Missing required argument: <output>");                usage();                return -1;            }            dump(new Path(input), new Path(output));            return 0;        case MODE_LIST:            ArrayList<Path> dirs = new ArrayList<>();            for (int i = 1; i < args.length; i++) {                if (args[i] == null)                    continue;                if (args[i].equals("-dir")) {                    Path dir = new Path(args[++i]);                    FileSystem fs = dir.getFileSystem(conf);                    FileStatus[] fstats = fs.listStatus(dir, HadoopFSUtil.getPassDirectoriesFilter(fs));                    Path[] files = HadoopFSUtil.getPaths(fstats);                    if (files != null && files.length > 0) {                        dirs.addAll(Arrays.asList(files));                    }                } else                    dirs.add(new Path(args[i]));            }            segmentReader.list(dirs, new OutputStreamWriter(System.out, StandardCharsets.UTF_8));            return 0;        case MODE_GET:            input = args[1];            if (input == null) {                System.err.println("Missing required argument: <segment_dir>");                usage();                return -1;            }            String key = args.length > 2 ? args[2] : null;            if (key == null) {                System.err.println("Missing required argument: <keyValue>");                usage();                return -1;            }            segmentReader.get(new Path(input), new Text(key), new OutputStreamWriter(System.out, StandardCharsets.UTF_8), new HashMap<>());            return 0;        default:            System.err.println("Invalid operation: " + args[0]);            usage();            return -1;    }}
0
private static void usage()
{    System.err.println("Usage: SegmentReader (-dump ... | -list ... | -get ...) [general options]\n");    System.err.println("* General options:");    System.err.println("\t-nocontent\tignore content directory");    System.err.println("\t-nofetch\tignore crawl_fetch directory");    System.err.println("\t-nogenerate\tignore crawl_generate directory");    System.err.println("\t-noparse\tignore crawl_parse directory");    System.err.println("\t-noparsedata\tignore parse_data directory");    System.err.println("\t-noparsetext\tignore parse_text directory");    System.err.println();    System.err.println("* SegmentReader -dump <segment_dir> <output> [general options]");    System.err.println("  Dumps content of a <segment_dir> as a text file to <output>.\n");    System.err.println("\t<segment_dir>\tname of the segment directory.");    System.err.println("\t<output>\tname of the (non-existent) output directory.");    System.err.println();    System.err.println("* SegmentReader -list (<segment_dir1> ... | -dir <segments>) [general options]");    System.err.println("  List a synopsis of segments in specified directories, or all segments in");    System.err.println("  a directory <segments>, and print it on System.out\n");    System.err.println("\t<segment_dir1> ...\tlist of segment directories to process");    System.err.println("\t-dir <segments>\t\tdirectory that contains multiple segments");    System.err.println();    System.err.println("* SegmentReader -get <segment_dir> <keyValue> [general options]");    System.err.println("  Get a specified record from a segment, and print it on System.out.\n");    System.err.println("\t<segment_dir>\tname of the segment directory.");    System.err.println("\t<keyValue>\tvalue of the key (url).");    System.err.println("\t\tNote: put double-quotes around strings with spaces.");}
0
public static void main(String[] args) throws Exception
{    int result = ToolRunner.run(NutchConfiguration.create(), new SegmentReader(), args);    System.exit(result);}
0
public Configuration get(String confId)
{    if (confId == null) {        return configurations.get(ConfigResource.DEFAULT);    }    return configurations.get(confId);}
0
public Map<String, String> getAsMap(String confId)
{    Configuration configuration = configurations.get(confId);    if (configuration == null) {        return Collections.emptyMap();    }    Iterator<Entry<String, String>> iterator = configuration.iterator();    Map<String, String> configMap = Maps.newTreeMap();    while (iterator.hasNext()) {        Entry<String, String> entry = iterator.next();        configMap.put(entry.getKey(), entry.getValue());    }    return configMap;}
0
public void setProperty(String confId, String propName, String propValue)
{    if (!configurations.containsKey(confId)) {        throw new IllegalArgumentException("Unknown configId '" + confId + "'");    }    Configuration conf = configurations.get(confId);    conf.set(propName, propValue);}
0
public Set<String> list()
{    return configurations.keySet();}
0
public String create(NutchConfig nutchConfig)
{    if (StringUtils.isBlank(nutchConfig.getConfigId())) {        nutchConfig.setConfigId(String.valueOf(newConfigId.incrementAndGet()));    }    if (!canCreate(nutchConfig)) {        throw new IllegalArgumentException("Config already exists.");    }    createHadoopConfig(nutchConfig);    return nutchConfig.getConfigId();}
0
public void delete(String confId)
{    configurations.remove(confId);}
0
private boolean canCreate(NutchConfig nutchConfig)
{    if (nutchConfig.isForce()) {        return true;    }    if (!configurations.containsKey(nutchConfig.getConfigId())) {        return true;    }    return false;}
0
private void createHadoopConfig(NutchConfig nutchConfig)
{    Configuration conf = NutchConfiguration.create();    configurations.put(nutchConfig.getConfigId(), conf);    if (MapUtils.isEmpty(nutchConfig.getParams())) {        return;    }    for (Entry<String, String> e : nutchConfig.getParams().entrySet()) {        conf.set(e.getKey(), e.getValue());    }}
0
public NutchTool createToolByType(JobType type, Configuration conf)
{    if (!typeToClass.containsKey(type)) {        return null;    }    Class<? extends NutchTool> clz = typeToClass.get(type);    return createTool(clz, conf);}
0
public NutchTool createToolByClassName(String className, Configuration conf)
{    try {        Class clz = Class.forName(className);        return createTool(clz, conf);    } catch (ClassNotFoundException e) {        throw new IllegalStateException(e);    }}
0
private NutchTool createTool(Class<? extends NutchTool> clz, Configuration conf)
{    return ReflectionUtils.newInstance(clz, conf);}
0
public JobInfo create(JobConfig jobConfig)
{    if (jobConfig.getArgs() == null) {        throw new IllegalArgumentException("Arguments cannot be null!");    }    Configuration conf = cloneConfiguration(jobConfig.getConfId());    NutchTool tool = createTool(jobConfig, conf);    JobWorker worker = new JobWorker(jobConfig, conf, tool);    executor.execute(worker);    executor.purge();    return worker.getInfo();}
0
private Configuration cloneConfiguration(String confId)
{    Configuration conf = configManager.get(confId);    if (conf == null) {        throw new IllegalArgumentException("Unknown confId " + confId);    }    return new Configuration(conf);}
0
public Collection<JobInfo> list(String crawlId, State state)
{    if (state == null || state == State.ANY) {        return executor.getAllJobs();    }    if (state == State.RUNNING || state == State.IDLE) {        return executor.getJobRunning();    }    return executor.getJobHistory();}
0
public JobInfo get(String crawlId, String jobId)
{    return executor.getInfo(jobId);}
0
public boolean abort(String crawlId, String id)
{    return executor.findWorker(id).killJob();}
0
public boolean stop(String crawlId, String id)
{    return executor.findWorker(id).stopJob();}
0
private NutchTool createTool(JobConfig jobConfig, Configuration conf)
{    if (StringUtils.isNotBlank(jobConfig.getJobClassName())) {        return jobFactory.createToolByClassName(jobConfig.getJobClassName(), conf);    }    return jobFactory.createToolByType(jobConfig.getType(), conf);}
0
private String generateId()
{    if (jobConfig.getCrawlId() == null) {        return MessageFormat.format("{0}-{1}-{2}", jobConfig.getConfId(), jobConfig.getType(), String.valueOf(hashCode()));    }    return MessageFormat.format("{0}-{1}-{2}-{3}", jobConfig.getCrawlId(), jobConfig.getConfId(), jobConfig.getType(), String.valueOf(hashCode()));}
0
public void run()
{    try {        getInfo().setState(State.RUNNING);        getInfo().setMsg("OK");        getInfo().setResult(tool.run(getInfo().getArgs(), getInfo().getCrawlId()));        getInfo().setState(State.FINISHED);    } catch (Exception e) {                getInfo().setMsg("ERROR: " + e.toString());        getInfo().setState(State.FAILED);    }}
1
public JobInfo getInfo()
{    return jobInfo;}
0
public boolean stopJob()
{    getInfo().setState(State.STOPPING);    try {        return tool.stopJob();    } catch (Exception e) {        throw new RuntimeException("Cannot stop job with id " + getInfo().getId(), e);    }}
0
public boolean killJob()
{    getInfo().setState(State.KILLING);    try {        boolean result = tool.killJob();        getInfo().setState(State.KILLED);        return result;    } catch (Exception e) {        throw new RuntimeException("Cannot kill job with id " + getInfo().getId(), e);    }}
0
public void setInfo(JobInfo jobInfo)
{    this.jobInfo = jobInfo;}
0
public List read(String path) throws FileNotFoundException
{    List<HashMap> rows = new ArrayList<>();    Path file = new Path(path);    SequenceFile.Reader reader;    try {        reader = new SequenceFile.Reader(conf, Reader.file(file));        Writable key = (Writable) ReflectionUtils.newInstance(reader.getKeyClass(), conf);        LinkDatum value = new LinkDatum();        while (reader.next(key, value)) {            try {                HashMap<String, String> t_row = getLinksRow(key, value);                rows.add(t_row);            } catch (Exception e) {            }        }        reader.close();    } catch (FileNotFoundException fne) {        throw new FileNotFoundException();    } catch (IOException e) {        e.printStackTrace();                throw new WebApplicationException();    }    return rows;}
1
public List head(String path, int nrows) throws FileNotFoundException
{    List<HashMap> rows = new ArrayList<>();    Path file = new Path(path);    SequenceFile.Reader reader;    try {        reader = new SequenceFile.Reader(conf, Reader.file(file));        Writable key = (Writable) ReflectionUtils.newInstance(reader.getKeyClass(), conf);        LinkDatum value = new LinkDatum();        int i = 0;        while (reader.next(key, value) && i < nrows) {            HashMap<String, String> t_row = getLinksRow(key, value);            rows.add(t_row);            i++;        }        reader.close();    } catch (FileNotFoundException fne) {        throw new FileNotFoundException();    } catch (IOException e) {        e.printStackTrace();                throw new WebApplicationException();    }    return rows;}
1
public List slice(String path, int start, int end) throws FileNotFoundException
{    List<HashMap> rows = new ArrayList<>();    Path file = new Path(path);    SequenceFile.Reader reader;    try {        reader = new SequenceFile.Reader(conf, Reader.file(file));        Writable key = (Writable) ReflectionUtils.newInstance(reader.getKeyClass(), conf);        LinkDatum value = new LinkDatum();        int i = 0;                for (; i < start && reader.next(key, value); i++) {        }        while (reader.next(key, value) && i < end) {            HashMap<String, String> t_row = getLinksRow(key, value);            rows.add(t_row);            i++;        }        reader.close();    } catch (FileNotFoundException fne) {        throw new FileNotFoundException();    } catch (IOException e) {        e.printStackTrace();                throw new WebApplicationException();    }    return rows;}
1
public int count(String path) throws FileNotFoundException
{    Path file = new Path(path);    SequenceFile.Reader reader;    int i = 0;    try {        reader = new SequenceFile.Reader(conf, Reader.file(file));        Writable key = (Writable) ReflectionUtils.newInstance(reader.getKeyClass(), conf);        Writable value = (Writable) ReflectionUtils.newInstance(reader.getValueClass(), conf);        while (reader.next(key, value)) {            i++;        }        reader.close();    } catch (FileNotFoundException fne) {        throw new FileNotFoundException();    } catch (IOException e) {                throw new WebApplicationException();    }    return i;}
1
private HashMap<String, String> getLinksRow(Writable key, LinkDatum value)
{    HashMap<String, String> tRow = new HashMap<>();    tRow.put("key_url", key.toString());    tRow.put("url", value.getUrl());    tRow.put("anchor", value.getAnchor());    tRow.put("score", String.valueOf(value.getScore()));    tRow.put("timestamp", String.valueOf(value.getTimestamp()));    tRow.put("linktype", String.valueOf(value.getLinkType()));    return tRow;}
0
public List read(String path) throws FileNotFoundException
{    List<HashMap> rows = new ArrayList<>();    Path file = new Path(path);    SequenceFile.Reader reader;    try {        reader = new SequenceFile.Reader(conf, Reader.file(file));        Writable key = (Writable) ReflectionUtils.newInstance(reader.getKeyClass(), conf);        Node value = new Node();        while (reader.next(key, value)) {            try {                HashMap<String, String> t_row = getNodeRow(key, value);                rows.add(t_row);            } catch (Exception e) {            }        }        reader.close();    } catch (FileNotFoundException fne) {        throw new FileNotFoundException();    } catch (IOException e) {        e.printStackTrace();                throw new WebApplicationException();    }    return rows;}
1
public List head(String path, int nrows) throws FileNotFoundException
{    List<HashMap> rows = new ArrayList<>();    Path file = new Path(path);    SequenceFile.Reader reader;    try {        reader = new SequenceFile.Reader(conf, Reader.file(file));        Writable key = (Writable) ReflectionUtils.newInstance(reader.getKeyClass(), conf);        Node value = new Node();        int i = 0;        while (reader.next(key, value) && i < nrows) {            HashMap<String, String> t_row = getNodeRow(key, value);            rows.add(t_row);            i++;        }        reader.close();    } catch (FileNotFoundException fne) {        throw new FileNotFoundException();    } catch (IOException e) {        e.printStackTrace();                throw new WebApplicationException();    }    return rows;}
1
public List slice(String path, int start, int end) throws FileNotFoundException
{    List<HashMap> rows = new ArrayList<>();    Path file = new Path(path);    SequenceFile.Reader reader;    try {        reader = new SequenceFile.Reader(conf, Reader.file(file));        Writable key = (Writable) ReflectionUtils.newInstance(reader.getKeyClass(), conf);        Node value = new Node();        int i = 0;                for (; i < start && reader.next(key, value); i++) {        }        while (reader.next(key, value) && i < end) {            HashMap<String, String> t_row = getNodeRow(key, value);            rows.add(t_row);            i++;        }        reader.close();    } catch (FileNotFoundException fne) {        throw new FileNotFoundException();    } catch (IOException e) {        e.printStackTrace();                throw new WebApplicationException();    }    return rows;}
1
public int count(String path) throws FileNotFoundException
{    Path file = new Path(path);    SequenceFile.Reader reader;    int i = 0;    try {        reader = new SequenceFile.Reader(conf, Reader.file(file));        Writable key = (Writable) ReflectionUtils.newInstance(reader.getKeyClass(), conf);        Node value = new Node();        while (reader.next(key, value)) {            i++;        }        reader.close();    } catch (FileNotFoundException fne) {        throw new FileNotFoundException();    } catch (IOException e) {        e.printStackTrace();                throw new WebApplicationException();    }    return i;}
1
private HashMap<String, String> getNodeRow(Writable key, Node value)
{    HashMap<String, String> tRow = new HashMap<>();    tRow.put("key_url", key.toString());    tRow.put("num_inlinks", String.valueOf(value.getNumInlinks()));    tRow.put("num_outlinks", String.valueOf(value.getNumOutlinks()));    tRow.put("inlink_score", String.valueOf(value.getInlinkScore()));    tRow.put("outlink_score", String.valueOf(value.getOutlinkScore()));    tRow.put("metadata", value.getMetadata().toString());    return tRow;}
0
protected void beforeExecute(Thread thread, Runnable runnable)
{    super.beforeExecute(thread, runnable);    synchronized (runningWorkers) {        runningWorkers.offer(((JobWorker) runnable));    }}
0
protected void afterExecute(Runnable runnable, Throwable throwable)
{    super.afterExecute(runnable, throwable);    synchronized (runningWorkers) {        runningWorkers.remove((JobWorker) runnable);    }    JobWorker worker = ((JobWorker) runnable);    addStatusToHistory(worker);}
0
private void addStatusToHistory(JobWorker worker)
{    synchronized (workersHistory) {        if (!workersHistory.offer(worker)) {            workersHistory.poll();            workersHistory.add(worker);        }    }}
0
public JobWorker findWorker(String jobId)
{    synchronized (runningWorkers) {        for (JobWorker worker : runningWorkers) {            if (StringUtils.equals(worker.getInfo().getId(), jobId)) {                return worker;            }        }    }    return null;}
0
public Collection<JobInfo> getJobHistory()
{    return getJobsInfo(workersHistory);}
0
public Collection<JobInfo> getJobRunning()
{    return getJobsInfo(runningWorkers);}
0
public Collection<JobInfo> getAllJobs()
{    return CollectionUtils.union(getJobRunning(), getJobHistory());}
0
private Collection<JobInfo> getJobsInfo(Collection<JobWorker> workers)
{    List<JobInfo> jobsInfo = Lists.newLinkedList();    for (JobWorker worker : workers) {        jobsInfo.add(worker.getInfo());    }    return jobsInfo;}
0
public JobInfo getInfo(String jobId)
{    for (JobInfo jobInfo : getAllJobs()) {        if (StringUtils.equals(jobId, jobInfo.getId())) {            return jobInfo;        }    }    return null;}
0
public SeedList getSeedList(String seedName)
{    if (seeds.containsKey(seedName)) {        return seeds.get(seedName);    } else        return null;}
0
public void setSeedList(String seedName, SeedList seedList)
{    seeds.put(seedName, seedList);}
0
public Map<String, SeedList> getSeeds()
{    return seeds;}
0
public boolean deleteSeedList(String seedName)
{    if (seeds.containsKey(seedName)) {        seeds.remove(seedName);        return true;    } else        return false;}
0
public List<List<String>> read(String path) throws FileNotFoundException
{        List<List<String>> rows = new ArrayList<>();    Path file = new Path(path);    SequenceFile.Reader reader;    try {        reader = new SequenceFile.Reader(conf, Reader.file(file));        Writable key = (Writable) ReflectionUtils.newInstance(reader.getKeyClass(), conf);        Writable value = (Writable) ReflectionUtils.newInstance(reader.getValueClass(), conf);        while (reader.next(key, value)) {            List<String> row = new ArrayList<>();            row.add(key.toString());            row.add(value.toString());            rows.add(row);        }        reader.close();    } catch (FileNotFoundException fne) {        throw new FileNotFoundException();    } catch (IOException e) {                e.printStackTrace();                throw new WebApplicationException();    }    return rows;}
1
public List<List<String>> head(String path, int nrows) throws FileNotFoundException
{        List<List<String>> rows = new ArrayList<>();    Path file = new Path(path);    SequenceFile.Reader reader;    try {        reader = new SequenceFile.Reader(conf, Reader.file(file));        Writable key = (Writable) ReflectionUtils.newInstance(reader.getKeyClass(), conf);        Writable value = (Writable) ReflectionUtils.newInstance(reader.getValueClass(), conf);        int i = 0;        while (reader.next(key, value) && i < nrows) {            List<String> row = new ArrayList<>();            row.add(key.toString());            row.add(value.toString());            rows.add(row);            i++;        }        reader.close();    } catch (FileNotFoundException fne) {        throw new FileNotFoundException();    } catch (IOException e) {                        throw new WebApplicationException();    }    return rows;}
1
public List<List<String>> slice(String path, int start, int end) throws FileNotFoundException
{    List<List<String>> rows = new ArrayList<>();    Path file = new Path(path);    SequenceFile.Reader reader;    try {        reader = new SequenceFile.Reader(conf, Reader.file(file));        Writable key = (Writable) ReflectionUtils.newInstance(reader.getKeyClass(), conf);        Writable value = (Writable) ReflectionUtils.newInstance(reader.getValueClass(), conf);        int i = 0;                for (; i < start && reader.next(key, value); i++) {        }        while (reader.next(key, value) && i < end) {            List<String> row = new ArrayList<>();            row.add(key.toString());            row.add(value.toString());            rows.add(row);            i++;        }        reader.close();    } catch (FileNotFoundException fne) {        throw new FileNotFoundException();    } catch (IOException e) {                        throw new WebApplicationException();    }    return rows;}
1
public int count(String path) throws FileNotFoundException
{    Path file = new Path(path);    SequenceFile.Reader reader;    int i = 0;    try {        reader = new SequenceFile.Reader(conf, Reader.file(file));        Writable key = (Writable) ReflectionUtils.newInstance(reader.getKeyClass(), conf);        Writable value = (Writable) ReflectionUtils.newInstance(reader.getValueClass(), conf);        while (reader.next(key, value)) {            i++;        }        reader.close();    } catch (FileNotFoundException fne) {        throw new FileNotFoundException();    } catch (IOException e) {                        throw new WebApplicationException();    }    return i;}
1
public void run()
{    try {        tool.run(serviceConfig.getArgs(), serviceConfig.getCrawlId());    } catch (Exception e) {                    }}
1
public String getConfId()
{    return confId;}
0
public void setConfId(String confId)
{    this.confId = confId;}
0
public Map<String, String> getArgs()
{    return args;}
0
public void setArgs(Map<String, String> args)
{    this.args = args;}
0
public String getType()
{    return type;}
0
public void setType(String type)
{    this.type = type;}
0
public String getCrawlId()
{    return crawlId;}
0
public void setCrawlId(String crawlId)
{    this.crawlId = crawlId;}
0
public String getCrawlId()
{    return crawlId;}
0
public void setCrawlId(String crawlId)
{    this.crawlId = crawlId;}
0
public JobType getType()
{    return type;}
0
public void setType(JobType type)
{    this.type = type;}
0
public String getConfId()
{    return confId;}
0
public void setConfId(String confId)
{    this.confId = confId;}
0
public Map<String, Object> getArgs()
{    return args;}
0
public void setArgs(Map<String, Object> args)
{    this.args = args;}
0
public String getJobClassName()
{    return jobClassName;}
0
public void setJobClassName(String jobClass)
{    this.jobClassName = jobClass;}
0
public Map<String, String> getParams()
{    return params;}
0
public void setParams(Map<String, String> params)
{    this.params = params;}
0
public String getConfigId()
{    return configId;}
0
public void setConfigId(String configId)
{    this.configId = configId;}
0
public boolean isForce()
{    return force;}
0
public void setForce(boolean force)
{    this.force = force;}
0
public String getPath()
{    return path;}
0
public void setPath(String path)
{    this.path = path;}
0
public Long getId()
{    return id;}
0
public void setId(Long id)
{    this.id = id;}
0
public Collection<SeedUrl> getSeedUrls()
{    return seedUrls;}
0
public void setSeedUrls(Collection<SeedUrl> seedUrls)
{    this.seedUrls = seedUrls;}
0
public String getName()
{    return name;}
0
public void setName(String name)
{    this.name = name;}
0
public String getSeedFilePath()
{    return seedFilePath;}
0
public void setSeedFilePath(String seedFilePath)
{    this.seedFilePath = seedFilePath;}
0
public int getSeedUrlsCount()
{    if (CollectionUtils.isEmpty(seedUrls)) {        return 0;    }    return seedUrls.size();}
0
public int hashCode()
{    final int prime = 31;    int result = 1;    result = prime * result + ((id == null) ? 0 : id.hashCode());    return result;}
0
public boolean equals(Object obj)
{    if (this == obj)        return true;    if (obj == null)        return false;    if (getClass() != obj.getClass())        return false;    SeedList other = (SeedList) obj;    if (id == null) {        if (other.id != null)            return false;    } else if (!id.equals(other.id))        return false;    return true;}
0
public Long getId()
{    return id;}
0
public void setId(Long id)
{    this.id = id;}
0
public String getUrl()
{    return url;}
0
public void setUrl(String url)
{    this.url = url;}
0
public SeedList getSeedList()
{    return seedList;}
0
public void setSeedList(SeedList seedList)
{    this.seedList = seedList;}
0
public int hashCode()
{    final int prime = 31;    int result = 1;    result = prime * result + ((id == null) ? 0 : id.hashCode());    return result;}
0
public boolean equals(Object obj)
{    if (this == obj)        return true;    if (obj == null)        return false;    if (getClass() != obj.getClass())        return false;    SeedUrl other = (SeedUrl) obj;    if (id == null) {        if (other.id != null)            return false;    } else if (!id.equals(other.id))        return false;    return true;}
0
public String getCrawlId()
{    return crawlId;}
0
public void setCrawlId(String crawlId)
{    this.crawlId = crawlId;}
0
public String getConfId()
{    return confId;}
0
public void setConfId(String confId)
{    this.confId = confId;}
0
public Map<String, Object> getArgs()
{    return args;}
0
public void setArgs(Map<String, Object> args)
{    this.args = args;}
0
public String getUrl()
{    return url;}
0
public void setUrl(String url)
{    this.url = url;}
0
public int getStatus()
{    return status;}
0
public void setStatus(int status)
{    this.status = status;}
0
public int getNumOfOutlinks()
{    return numOfOutlinks;}
0
public void setNumOfOutlinks(int numOfOutlinks)
{    this.numOfOutlinks = numOfOutlinks;}
0
public void setChildNodes(Outlink[] links)
{    ChildNode childNode;    for (Outlink outlink : links) {        childNode = new ChildNode(outlink.getToUrl(), outlink.getAnchor());        children.add(childNode);    }}
0
public String getAnchorText()
{    return anchorText;}
0
public void setAnchorText(String anchorText)
{    this.anchorText = anchorText;}
0
public String getChildUrl()
{    return childUrl;}
0
public void setChildUrl(String childUrl)
{    this.childUrl = childUrl;}
0
public List<ChildNode> getChildren()
{    return children;}
0
public void setChildren(List<ChildNode> children)
{    this.children = children;}
0
public String getId()
{    return id;}
0
public void setId(String id)
{    this.id = id;}
0
public JobType getType()
{    return type;}
0
public void setType(JobType type)
{    this.type = type;}
0
public String getConfId()
{    return confId;}
0
public void setConfId(String confId)
{    this.confId = confId;}
0
public Map<String, Object> getArgs()
{    return args;}
0
public void setArgs(Map<String, Object> args)
{    this.args = args;}
0
public Map<String, Object> getResult()
{    return result;}
0
public void setResult(Map<String, Object> result)
{    this.result = result;}
0
public State getState()
{    return state;}
0
public void setState(State state)
{    this.state = state;}
0
public String getMsg()
{    return msg;}
0
public void setMsg(String msg)
{    this.msg = msg;}
0
public String getCrawlId()
{    return crawlId;}
0
public void setCrawlId(String crawlId)
{    this.crawlId = crawlId;}
0
public Date getStartDate()
{    return startDate;}
0
public void setStartDate(Date startDate)
{    this.startDate = startDate;}
0
public Set<String> getConfiguration()
{    return configuration;}
0
public void setConfiguration(Set<String> configuration)
{    this.configuration = configuration;}
0
public Collection<JobInfo> getJobs()
{    return jobs;}
0
public void setJobs(Collection<JobInfo> jobs)
{    this.jobs = jobs;}
0
public Collection<JobInfo> getRunningJobs()
{    return runningJobs;}
0
public void setRunningJobs(Collection<JobInfo> runningJobs)
{    this.runningJobs = runningJobs;}
0
public List<String> getDumpPaths()
{    return dumpPaths;}
0
public void setDumpPaths(List<String> dumpPaths)
{    this.dumpPaths = dumpPaths;}
0
public static NutchServer getInstance()
{    return server;}
0
protected static void startServer()
{    server.start();}
0
private void start()
{        try {        String address = "http://" + host + ":" + port;        sf.setAddress(address);        sf.create();    } catch (Exception e) {        throw new IllegalStateException("Server could not be started", e);    }    started = System.currentTimeMillis();    running = true;    }
1
private List<Class<?>> getClasses()
{    List<Class<?>> resources = new ArrayList<>();    resources.add(JobResource.class);    resources.add(ConfigResource.class);    resources.add(DbResource.class);    resources.add(AdminResource.class);    resources.add(SeedResource.class);    resources.add(ReaderResouce.class);    resources.add(ServicesResource.class);    return resources;}
0
private List<ResourceProvider> getResourceProviders()
{    List<ResourceProvider> resourceProviders = new ArrayList<>();    resourceProviders.add(new SingletonResourceProvider(getConfManager()));    return resourceProviders;}
0
public ConfManager getConfManager()
{    return configManager;}
0
public JobManager getJobManager()
{    return jobManager;}
0
public SeedManager getSeedManager()
{    return seedManager;}
0
public FetchNodeDb getFetchNodeDb()
{    return fetchNodeDb;}
0
public boolean isRunning()
{    return running;}
0
public long getStarted()
{    return started;}
0
public static void main(String[] args) throws ParseException
{    CommandLineParser parser = new PosixParser();    Options options = createOptions();    CommandLine commandLine = parser.parse(options, args);    if (commandLine.hasOption(CMD_HELP)) {        HelpFormatter formatter = new HelpFormatter();        formatter.printHelp("NutchServer", options, true);        return;    }    if (commandLine.hasOption(CMD_PORT)) {        port = Integer.parseInt(commandLine.getOptionValue(CMD_PORT));    }    if (commandLine.hasOption(CMD_HOST)) {        host = commandLine.getOptionValue(CMD_HOST);    }    startServer();}
0
private static Options createOptions()
{    Options options = new Options();    OptionBuilder.withDescription("Show this help");    options.addOption(OptionBuilder.create(CMD_HELP));    OptionBuilder.withArgName("port");    OptionBuilder.hasOptionalArg();    OptionBuilder.withDescription("The port to run the Nutch Server. Default port 8081");    options.addOption(OptionBuilder.create(CMD_PORT));    OptionBuilder.withArgName("host");    OptionBuilder.hasOptionalArg();    OptionBuilder.withDescription("The host to bind the Nutch Server to. Default is localhost.");    options.addOption(OptionBuilder.create(CMD_HOST));    return options;}
0
public boolean canStop(boolean force)
{    if (force)        return true;    Collection<JobInfo> jobs = getJobManager().list(null, State.RUNNING);    return jobs.isEmpty();}
0
protected static void setPort(int port)
{    NutchServer.port = port;}
0
public int getPort()
{    return port;}
0
public void stop()
{    System.exit(0);}
0
protected void throwBadRequestException(String message)
{    throw new WebApplicationException(Response.status(Status.BAD_REQUEST).entity(message).build());}
0
public NutchServerInfo getServerStatus()
{    NutchServerInfo serverInfo = new NutchServerInfo();    serverInfo.setConfiguration(configManager.list());    serverInfo.setStartDate(new Date(server.getStarted()));    serverInfo.setJobs(jobManager.list(null, State.ANY));    serverInfo.setRunningJobs(jobManager.list(null, State.RUNNING));    return serverInfo;}
0
public String stopServer(@QueryParam("force") boolean force)
{    if (!server.canStop(force)) {        return "Jobs still running -- Cannot stop server now";    }    scheduleServerStop();    return "Stopping in server on port " + server.getPort();}
0
private void scheduleServerStop()
{        Thread thread = new Thread() {        public void run() {            try {                Thread.sleep(DELAY_SEC * 1000);            } catch (InterruptedException e) {                Thread.currentThread().interrupt();            }            server.stop();                    }    };    thread.setDaemon(true);    thread.start();    }
1
public void run()
{    try {        Thread.sleep(DELAY_SEC * 1000);    } catch (InterruptedException e) {        Thread.currentThread().interrupt();    }    server.stop();    }
1
public Set<String> getConfigs()
{    return configManager.list();}
0
public Map<String, String> getConfig(@PathParam("configId") String configId)
{    return configManager.getAsMap(configId);}
0
public String getProperty(@PathParam("configId") String configId, @PathParam("propertyId") String propertyId)
{    return configManager.getAsMap(configId).get(propertyId);}
0
public void deleteConfig(@PathParam("configId") String configId)
{    configManager.delete(configId);}
0
public Response createConfig(NutchConfig newConfig)
{    if (newConfig == null) {        return Response.status(400).entity("Nutch configuration cannot be empty!").build();    }    try {        configManager.create(newConfig);    } catch (Exception e) {        return Response.status(400).entity(e.getMessage()).build();    }    return Response.ok(newConfig.getConfigId()).build();}
0
public Response updateProperty(@PathParam("configId") String confId, @PathParam("propertyId") String propertyKey, String value)
{    try {        configManager.setProperty(confId, propertyKey, value);    } catch (Exception e) {        return Response.status(400).entity(e.getMessage()).build();    }    return Response.ok().build();}
0
public Response readdb(DbQuery dbQuery)
{    if (dbQuery == null)        return Response.status(Status.BAD_REQUEST).build();    Configuration conf = configManager.get(dbQuery.getConfId());    if (conf == null) {        conf = configManager.get(ConfigResource.DEFAULT);    }    if (dbQuery.getCrawlId() == null || dbQuery.getType() == null) {        return Response.status(Status.BAD_REQUEST).build();    }    String type = dbQuery.getType();    if (type.equalsIgnoreCase("stats")) {        return crawlDbStats(conf, dbQuery.getArgs(), dbQuery.getCrawlId());    }    if (type.equalsIgnoreCase("dump")) {        return crawlDbDump(conf, dbQuery.getArgs(), dbQuery.getCrawlId());    }    if (type.equalsIgnoreCase("topN")) {        return crawlDbTopN(conf, dbQuery.getArgs(), dbQuery.getCrawlId());    }    if (type.equalsIgnoreCase("url")) {        return crawlDbUrl(conf, dbQuery.getArgs(), dbQuery.getCrawlId());    }    return null;}
0
public List<FetchNodeDbInfo> fetchDb(@DefaultValue("0") @QueryParam("to") int to, @DefaultValue("0") @QueryParam("from") int from)
{    List<FetchNodeDbInfo> listOfFetchedNodes = new ArrayList<>();    Map<Integer, FetchNode> fetchNodedbMap = FetchNodeDb.getInstance().getFetchNodeDb();    if (to == 0 || to > fetchNodedbMap.size()) {        to = fetchNodedbMap.size();    }    for (int i = from; i <= to; i++) {        if (!fetchNodedbMap.containsKey(i)) {            continue;        }        FetchNode node = fetchNodedbMap.get(i);        FetchNodeDbInfo fdbInfo = new FetchNodeDbInfo();        fdbInfo.setUrl(node.getUrl().toString());        fdbInfo.setStatus(node.getStatus());        fdbInfo.setNumOfOutlinks(node.getOutlinks().length);        fdbInfo.setChildNodes(node.getOutlinks());        listOfFetchedNodes.add(fdbInfo);    }    return listOfFetchedNodes;}
0
private Response crawlDbStats(Configuration conf, Map<String, String> args, String crawlId)
{    CrawlDbReader dbr = new CrawlDbReader();    try {        return Response.ok(dbr.query(args, conf, "stats", crawlId)).build();    } catch (Exception e) {        e.printStackTrace();        return Response.serverError().entity(e.getMessage()).type(MediaType.TEXT_PLAIN).build();    }}
0
private Response crawlDbDump(Configuration conf, Map<String, String> args, String crawlId)
{    @SuppressWarnings("resource")    CrawlDbReader dbr = new CrawlDbReader();    try {        return Response.ok(dbr.query(args, conf, "dump", crawlId), MediaType.APPLICATION_OCTET_STREAM).build();    } catch (Exception e) {        e.printStackTrace();        return Response.serverError().entity(e.getMessage()).type(MediaType.TEXT_PLAIN).build();    }}
0
private Response crawlDbTopN(Configuration conf, Map<String, String> args, String crawlId)
{    @SuppressWarnings("resource")    CrawlDbReader dbr = new CrawlDbReader();    try {        return Response.ok(dbr.query(args, conf, "topN", crawlId), MediaType.APPLICATION_OCTET_STREAM).build();    } catch (Exception e) {        e.printStackTrace();        return Response.serverError().entity(e.getMessage()).type(MediaType.TEXT_PLAIN).build();    }}
0
private Response crawlDbUrl(Configuration conf, Map<String, String> args, String crawlId)
{    @SuppressWarnings("resource")    CrawlDbReader dbr = new CrawlDbReader();    try {        return Response.ok(dbr.query(args, conf, "url", crawlId)).build();    } catch (Exception e) {        e.printStackTrace();        return Response.serverError().entity(e.getMessage()).type(MediaType.TEXT_PLAIN).build();    }}
0
public Collection<JobInfo> getJobs(@QueryParam("crawlId") String crawlId)
{    return jobManager.list(crawlId, State.ANY);}
0
public JobInfo getInfo(@PathParam("id") String id, @QueryParam("crawlId") String crawlId)
{    return jobManager.get(crawlId, id);}
0
public boolean stop(@PathParam("id") String id, @QueryParam("crawlId") String crawlId)
{    return jobManager.stop(crawlId, id);}
0
public boolean abort(@PathParam("id") String id, @QueryParam("crawlId") String crawlId)
{    return jobManager.abort(crawlId, id);}
0
public JobInfo create(JobConfig config)
{    if (config == null) {        throwBadRequestException("Job configuration is required!");    }    return jobManager.create(config);}
0
public Response seqRead(ReaderConfig readerConf, @DefaultValue("-1") @QueryParam("nrows") int nrows, @DefaultValue("-1") @QueryParam("start") int start, @QueryParam("end") int end, @QueryParam("count") boolean count)
{    NutchReader reader = new SequenceReader();    String path = readerConf.getPath();    return performRead(reader, path, nrows, start, end, count);}
0
public Response linkRead()
{    HashMap<String, String> schema = new HashMap<>();    schema.put("key_url", "string");    schema.put("timestamp", "int");    schema.put("score", "float");    schema.put("anchor", "string");    schema.put("linktype", "string");    schema.put("url", "string");    return Response.ok(schema).type(MediaType.APPLICATION_JSON).build();}
0
public Response linkRead(ReaderConfig readerConf, @DefaultValue("-1") @QueryParam("nrows") int nrows, @DefaultValue("-1") @QueryParam("start") int start, @QueryParam("end") int end, @QueryParam("count") boolean count)
{    NutchReader reader = new LinkReader();    String path = readerConf.getPath();    return performRead(reader, path, nrows, start, end, count);}
0
public Response nodeRead()
{    HashMap<String, String> schema = new HashMap<>();    schema.put("key_url", "string");    schema.put("num_inlinks", "int");    schema.put("num_outlinks", "int");    schema.put("inlink_score", "float");    schema.put("outlink_score", "float");    schema.put("metadata", "string");    return Response.ok(schema).type(MediaType.APPLICATION_JSON).build();}
0
public Response nodeRead(ReaderConfig readerConf, @DefaultValue("-1") @QueryParam("nrows") int nrows, @DefaultValue("-1") @QueryParam("start") int start, @QueryParam("end") int end, @QueryParam("count") boolean count)
{    NutchReader reader = new NodeReader();    String path = readerConf.getPath();    return performRead(reader, path, nrows, start, end, count);}
0
private Response performRead(NutchReader reader, String path, int nrows, int start, int end, boolean count)
{    Object result;    try {        if (count) {            result = reader.count(path);            return Response.ok(result).type(MediaType.TEXT_PLAIN).build();        } else if (start > -1 && end > 0) {            result = reader.slice(path, start, end);        } else if (nrows > -1) {            result = reader.head(path, nrows);        } else {            result = reader.read(path);        }        return Response.ok(result).type(MediaType.APPLICATION_JSON).build();    } catch (Exception e) {        return Response.status(Status.BAD_REQUEST).entity("File not found").build();    }}
0
public Response getSeedLists()
{    Map<String, SeedList> seeds = NutchServer.getInstance().getSeedManager().getSeeds();    if (seeds != null) {        return Response.ok(seeds).build();    } else {        return Response.ok().build();    }}
0
public Response createSeedFile(SeedList seedList)
{    try {        if (seedList == null) {            return Response.status(Status.BAD_REQUEST).entity("Seed list cannot be empty!").build();        }        Collection<SeedUrl> seedUrls = seedList.getSeedUrls();        String seedFilePath = writeToSeedFile(seedUrls);        seedList.setSeedFilePath(seedFilePath);        NutchServer.getInstance().getSeedManager().setSeedList(seedList.getName(), seedList);        return Response.ok().entity(seedFilePath).build();    } catch (Exception e) {            }    return Response.serverError().build();}
1
private String writeToSeedFile(Collection<SeedUrl> seedUrls) throws Exception
{    String seedFilePath = "seedFiles/seed-" + System.currentTimeMillis();    org.apache.hadoop.fs.Path seedFolder = new org.apache.hadoop.fs.Path(seedFilePath);    FileSystem fs = FileSystem.get(new Configuration());    if (!fs.exists(seedFolder)) {        if (!fs.mkdirs(seedFolder)) {            throw new Exception("Could not create seed folder at : " + seedFolder);        }    }    String filename = seedFilePath + System.getProperty("file.separator") + "urls";    org.apache.hadoop.fs.Path seedPath = new org.apache.hadoop.fs.Path(filename);    OutputStream os = fs.create(seedPath);    if (CollectionUtils.isNotEmpty(seedUrls)) {        for (SeedUrl seedUrl : seedUrls) {            os.write(seedUrl.getUrl().getBytes());            os.write("\n".getBytes());        }    }    os.close();    return seedPath.getParent().toString();}
0
public Response listDumpPaths(@PathParam("crawlId") String crawlId)
{    File dumpFilePath = new File(crawlId + File.separator + "dump/");    File[] dumpFileList = dumpFilePath.listFiles();    List<String> fileNames = new ArrayList<>();    if (dumpFileList != null) {        for (File f : dumpFileList) {            fileNames.add(f.getPath());        }    }    ServiceInfo info = new ServiceInfo();    info.setDumpPaths(fileNames);    return Response.ok().entity(info).type(MediaType.APPLICATION_JSON).build();}
0
public Response commoncrawlDump(ServiceConfig serviceConfig)
{    String crawlId = serviceConfig.getCrawlId();    String outputDir = crawlId + File.separator + "dump" + File.separator + "commoncrawl-" + sdf.format(System.currentTimeMillis());    Map<String, Object> args = serviceConfig.getArgs();    args.put("outputDir", outputDir);    if (!args.containsKey(Nutch.ARG_SEGMENTDIR)) {        args.put("segment", crawlId + File.separator + "segments");    }    serviceConfig.setArgs(args);    ServiceWorker worker = new ServiceWorker(serviceConfig, new CommonCrawlDataDumper());    worker.run();    return Response.ok(outputDir).type(MediaType.TEXT_PLAIN).build();}
0
public String getJsonData(String url, Content content, Metadata metadata) throws IOException
{    this.url = url;    this.content = content;    this.metadata = metadata;    return this.getJsonData();}
0
public String getJsonData(String url, Content content, Metadata metadata, ParseData parseData) throws IOException
{        throw new NotImplementedException();}
0
public String getJsonData() throws IOException
{    try {        startObject(null);                writeKeyValue("url", getUrl());                writeKeyValue("timestamp", getTimestamp());                startObject("request");        writeKeyValue("method", getMethod());        startObject("client");        writeKeyValue("hostname", getRequestHostName());        writeKeyValue("address", getRequestHostAddress());        writeKeyValue("software", getRequestSoftware());        writeKeyValue("robots", getRequestRobots());        startObject("contact");        writeKeyValue("name", getRequestContactName());        writeKeyValue("email", getRequestContactEmail());        closeObject("contact");        closeObject("client");                startHeaders("headers", false, true);        writeKeyValueWrapper("Accept", getRequestAccept());        writeKeyValueWrapper("Accept-Encoding", getRequestAcceptEncoding());        writeKeyValueWrapper("Accept-Language", getRequestAcceptLanguage());        writeKeyValueWrapper("User-Agent", getRequestUserAgent());                closeHeaders("headers", false, true);        writeKeyNull("body");        closeObject("request");                startObject("response");        writeKeyValue("status", getResponseStatus());        startObject("server");        writeKeyValue("hostname", getResponseHostName());        writeKeyValue("address", getResponseAddress());        closeObject("server");                startHeaders("headers", false, true);        writeKeyValueWrapper("Content-Encoding", getResponseContentEncoding());        writeKeyValueWrapper("Content-Type", getResponseContentType());        writeKeyValueWrapper("Date", getResponseDate());        writeKeyValueWrapper("Server", getResponseServer());        for (String name : metadata.names()) {            if (name.equalsIgnoreCase("Content-Encoding") || name.equalsIgnoreCase("Content-Type") || name.equalsIgnoreCase("Date") || name.equalsIgnoreCase("Server")) {                continue;            }            writeKeyValueWrapper(name, metadata.get(name));        }        closeHeaders("headers", false, true);        writeKeyValue("body", getResponseContent());        closeObject("response");                if (!this.keyPrefix.isEmpty()) {            this.keyPrefix += "-";        }        writeKeyValue("key", this.keyPrefix + getKey());                writeKeyValue("imported", getImported());        if (getInLinks() != null) {            startArray("inlinks", false, true);            for (String link : getInLinks()) {                writeArrayValue(link);            }            closeArray("inlinks", false, true);        }        closeObject(null);        return generateJson();    } catch (IOException ioe) {                throw new IOException("Error in generating JSON:" + ioe.getMessage());    }}
1
protected String getUrl()
{    try {        return URIUtil.encodePath(url);    } catch (URIException e) {            }    return url;}
1
protected String getTimestamp()
{    if (this.simpleDateFormat) {        String timestamp = null;        try {            long epoch = new SimpleDateFormat("EEE, d MMM yyyy HH:mm:ss z").parse(ifNullString(metadata.get(Metadata.LAST_MODIFIED))).getTime();            timestamp = String.valueOf(epoch);        } catch (ParseException pe) {                    }        return timestamp;    } else {        return ifNullString(metadata.get(Metadata.LAST_MODIFIED));    }}
1
protected String getMethod()
{    return new String("GET");}
0
protected String getRequestHostName()
{    String hostName = "";    try {        hostName = InetAddress.getLocalHost().getHostName();    } catch (UnknownHostException uhe) {    }    return hostName;}
0
protected String getRequestHostAddress()
{    String hostAddress = "";    try {        hostAddress = InetAddress.getLocalHost().getHostAddress();    } catch (UnknownHostException uhe) {    }    return hostAddress;}
0
protected String getRequestSoftware()
{    return conf.get("http.agent.version", "");}
0
protected String getRequestRobots()
{    return new String("CLASSIC");}
0
protected String getRequestContactName()
{    return conf.get("http.agent.name", "");}
0
protected String getRequestContactEmail()
{    return conf.get("http.agent.email", "");}
0
protected String getRequestAccept()
{    return conf.get("http.accept", "");}
0
protected String getRequestAcceptEncoding()
{        return new String("");}
0
protected String getRequestAcceptLanguage()
{    return conf.get("http.accept.language", "");}
0
protected String getRequestUserAgent()
{    return conf.get("http.robots.agents", "");}
0
protected String getResponseStatus()
{    return ifNullString(metadata.get("status"));}
0
protected String getResponseHostName()
{    return URLUtil.getHost(url);}
0
protected String getResponseAddress()
{    return ifNullString(metadata.get("_ip_"));}
0
protected String getResponseContentEncoding()
{    return ifNullString(metadata.get("Content-Encoding"));}
0
protected String getResponseContentType()
{    return ifNullString(metadata.get("Content-Type"));}
0
public List<String> getInLinks()
{    return inLinks;}
0
public void setInLinks(List<String> inLinks)
{    this.inLinks = inLinks;}
0
protected String getResponseDate()
{    if (this.simpleDateFormat) {        String timestamp = null;        try {            long epoch = new SimpleDateFormat("EEE, dd MMM yyyy HH:mm:ss z").parse(ifNullString(metadata.get("Date"))).getTime();            timestamp = String.valueOf(epoch);        } catch (ParseException pe) {                    }        return timestamp;    } else {        return ifNullString(metadata.get("Date"));    }}
1
protected String getResponseServer()
{    return ifNullString(metadata.get("Server"));}
0
protected String getResponseContent()
{    return new String(content.getContent());}
0
protected String getKey()
{    if (this.reverseKey) {        return this.reverseKeyValue;    } else {        return url;    }}
0
protected String getImported()
{    if (this.simpleDateFormat) {        String timestamp = null;        try {            long epoch = new SimpleDateFormat("EEE, d MMM yyyy HH:mm:ss z").parse(ifNullString(metadata.get("Date"))).getTime();            timestamp = String.valueOf(epoch);        } catch (ParseException pe) {                    }        return timestamp;    } else {        return ifNullString(metadata.get("Date"));    }}
1
private static String ifNullString(String value)
{    return (value != null) ? value : "";}
0
private void startHeaders(String key, boolean nested, boolean newline) throws IOException
{    if (this.jsonArray) {        startArray(key, nested, newline);    } else {        startObject(key);    }}
0
private void closeHeaders(String key, boolean nested, boolean newline) throws IOException
{    if (this.jsonArray) {        closeArray(key, nested, newline);    } else {        closeObject(key);    }}
0
private void writeKeyValueWrapper(String key, String value) throws IOException
{    if (this.jsonArray) {        startArray(null, true, false);        writeArrayValue(key);        writeArrayValue(value);        closeArray(null, true, false);    } else {        writeKeyValue(key, value);    }}
0
public void close()
{}
0
public RecordReader<Text, BytesWritable> createRecordReader(InputSplit split, TaskAttemptContext context)
{    return new SequenceFileRecordReader<Text, BytesWritable>();}
0
public RecordReader<Text, BytesWritable> getRecordReader(InputSplit split, Job job, Context context) throws IOException
{    context.setStatus(split.toString());    Configuration conf = job.getConfiguration();    return new ArcRecordReader(conf, (FileSplit) split);}
0
public static boolean isMagic(byte[] input)
{        if (input == null || input.length != MAGIC.length) {        return false;    }        for (int i = 0; i < MAGIC.length; i++) {        if (MAGIC[i] != input[i]) {            return false;        }    }        return true;}
0
public void close() throws IOException
{    this.in.close();}
0
public Text createKey()
{    return ReflectionUtils.newInstance(Text.class, conf);}
0
public BytesWritable createValue()
{    return ReflectionUtils.newInstance(BytesWritable.class, conf);}
0
public long getPos() throws IOException
{    return in.getPos();}
0
public float getProgress() throws IOException
{        if (splitEnd == splitStart) {        return 0.0f;    } else {                return Math.min(1.0f, (getPos() - splitStart) / (float) splitLen);    }}
0
public BytesWritable getCurrentValue()
{    return new BytesWritable();}
0
public Text getCurrentKey()
{    return new Text();}
0
public boolean nextKeyValue()
{    return false;}
0
public void initialize(InputSplit split, TaskAttemptContext context)
{}
0
public boolean next(Text key, BytesWritable value) throws IOException
{    try {                long startRead = in.getPos();        byte[] magicBuffer = null;                while (true) {                        if (startRead >= splitEnd) {                return false;            }                        boolean foundStart = false;            while (!foundStart) {                                                startRead = in.getPos();                magicBuffer = new byte[1024];                int read = in.read(magicBuffer);                if (read < 0) {                    break;                }                                for (int i = 0; i < read - 1; i++) {                    byte[] testMagic = new byte[2];                    System.arraycopy(magicBuffer, i, testMagic, 0, 2);                    if (isMagic(testMagic)) {                                                startRead += i;                        foundStart = true;                        break;                    }                }            }                        in.seek(startRead);            ByteArrayOutputStream baos = null;            int totalRead = 0;            try {                                byte[] buffer = new byte[4096];                GZIPInputStream zin = new GZIPInputStream(in);                int gzipRead = -1;                baos = new ByteArrayOutputStream();                while ((gzipRead = zin.read(buffer, 0, buffer.length)) != -1) {                    baos.write(buffer, 0, gzipRead);                    totalRead += gzipRead;                }            } catch (Exception e) {                                                                System.out.println("Ignoring position: " + (startRead));                if (startRead + 1 < fileLen) {                    in.seek(startRead + 1);                }                continue;            }                        byte[] content = baos.toByteArray();                        int eol = 0;            for (int i = 0; i < content.length; i++) {                if (i > 0 && content[i] == '\n') {                    eol = i;                    break;                }            }                        String header = new String(content, 0, eol).trim();            byte[] raw = new byte[(content.length - eol) - 1];            System.arraycopy(content, eol + 1, raw, 0, raw.length);                        Text keyText = key;            keyText.set(header);            BytesWritable valueBytes = value;            valueBytes.set(raw, 0, raw.length);                        if (startRead + 1 < fileLen) {                in.seek(startRead + 1);            }                        return true;        }    } catch (Exception e) {            }        return false;}
1
public static synchronized String generateSegmentName()
{    try {        Thread.sleep(1000);    } catch (Throwable t) {    }    return sdf.format(new Date(System.currentTimeMillis()));}
0
public void close()
{}
0
private static void logError(Text url, Throwable t)
{    if (LOG.isInfoEnabled()) {            }}
1
private ParseStatus output(Context context, String segmentName, Text key, CrawlDatum datum, Content content, ProtocolStatus pstatus, int status) throws InterruptedException
{        datum.setStatus(status);    datum.setFetchTime(System.currentTimeMillis());    if (pstatus != null)        datum.getMetaData().put(Nutch.WRITABLE_PROTO_STATUS_KEY, pstatus);    ParseResult parseResult = null;    if (content != null) {        Metadata metadata = content.getMetadata();                metadata.set(Nutch.SEGMENT_NAME_KEY, segmentName);                try {            scfilters.passScoreBeforeParsing(key, datum, content);        } catch (Exception e) {            if (LOG.isWarnEnabled()) {                            }        }        try {                        parseResult = parseUtil.parse(content);        } catch (Exception e) {                    }                if (parseResult == null) {            byte[] signature = SignatureFactory.getSignature(conf).calculate(content, new ParseStatus().getEmptyParse(conf));            datum.setSignature(signature);        }        if (parseResult == null) {            byte[] signature = SignatureFactory.getSignature(conf).calculate(content, new ParseStatus().getEmptyParse(conf));            datum.setSignature(signature);        }        try {            context.write(key, new NutchWritable(datum));            context.write(key, new NutchWritable(content));            if (parseResult != null) {                for (Entry<Text, Parse> entry : parseResult) {                    Text url = entry.getKey();                    Parse parse = entry.getValue();                    ParseStatus parseStatus = parse.getData().getStatus();                    if (!parseStatus.isSuccess()) {                                                parse = parseStatus.getEmptyParse(conf);                    }                                        byte[] signature = SignatureFactory.getSignature(conf).calculate(content, parse);                                        parse.getData().getContentMeta().set(Nutch.SEGMENT_NAME_KEY, segmentName);                    parse.getData().getContentMeta().set(Nutch.SIGNATURE_KEY, StringUtil.toHexString(signature));                                        parse.getData().getContentMeta().set(Nutch.FETCH_TIME_KEY, Long.toString(datum.getFetchTime()));                    if (url.equals(key))                        datum.setSignature(signature);                    try {                        scfilters.passScoreAfterParsing(url, content, parse);                    } catch (Exception e) {                        if (LOG.isWarnEnabled()) {                                                    }                    }                    context.write(url, new NutchWritable(new ParseImpl(new ParseText(parse.getText()), parse.getData(), parse.isCanonical())));                }            }        } catch (IOException e) {            if (LOG.isErrorEnabled()) {                            }        }        if (parseResult != null && !parseResult.isEmpty()) {            Parse p = parseResult.get(content.getUrl());            if (p != null) {                return p.getData().getStatus();            }        }    }    return null;}
1
public void setup(Mapper<Text, BytesWritable, Text, NutchWritable>.Context context)
{            conf = context.getConfiguration();    urlFilters = new URLFilters(conf);    scfilters = new ScoringFilters(conf);    parseUtil = new ParseUtil(conf);    normalizers = new URLNormalizers(conf, URLNormalizers.SCOPE_FETCHER);    interval = conf.getInt("db.fetch.interval.default", 2592000);}
0
public void map(Text key, BytesWritable bytes, Context context) throws IOException, InterruptedException
{    String[] headers = key.toString().split("\\s+");    String urlStr = headers[0];    String version = headers[2];    String contentType = headers[3];        if (urlStr.startsWith("filedesc://")) {                return;    }            Text url = new Text();    CrawlDatum datum = new CrawlDatum(CrawlDatum.STATUS_DB_FETCHED, interval, 1.0f);    String segmentName = conf.get(Nutch.SEGMENT_NAME_KEY);        try {        urlStr = normalizers.normalize(urlStr, URLNormalizers.SCOPE_FETCHER);                urlStr = urlFilters.filter(urlStr);    } catch (Exception e) {        if (LOG.isWarnEnabled()) {                    }        urlStr = null;    }        if (urlStr != null) {        url.set(urlStr);        try {                                                            ProtocolStatus status = ProtocolStatus.STATUS_SUCCESS;            Content content = new Content(urlStr, urlStr, bytes.getBytes(), contentType, new Metadata(), conf);                        content.getMetadata().set(URL_VERSION, version);            @SuppressWarnings("unused")            ParseStatus pstatus = null;            pstatus = output(context, segmentName, url, datum, content, status, CrawlDatum.STATUS_FETCH_SUCCESS);            context.progress();        } catch (Throwable t) {                        logError(url, t);            output(context, segmentName, url, datum, null, null, CrawlDatum.STATUS_FETCH_RETRY);        }    }}
1
public void createSegments(Path arcFiles, Path segmentsOutDir) throws IOException, InterruptedException, ClassNotFoundException
{    SimpleDateFormat sdf = new SimpleDateFormat("yyyy-MM-dd HH:mm:ss");    long start = System.currentTimeMillis();    if (LOG.isInfoEnabled()) {                    }    Job job = NutchJob.getInstance(getConf());    Configuration conf = job.getConfiguration();    job.setJobName("ArcSegmentCreator " + arcFiles);    String segName = generateSegmentName();    conf.set(Nutch.SEGMENT_NAME_KEY, segName);    FileInputFormat.addInputPath(job, arcFiles);    job.setInputFormatClass(ArcInputFormat.class);    job.setJarByClass(ArcSegmentCreator.class);    job.setMapperClass(ArcSegmentCreator.ArcSegmentCreatorMapper.class);    FileOutputFormat.setOutputPath(job, new Path(segmentsOutDir, segName));    job.setOutputFormatClass(FetcherOutputFormat.class);    job.setOutputKeyClass(Text.class);    job.setOutputValueClass(NutchWritable.class);    try {        boolean success = job.waitForCompletion(true);        if (!success) {            String message = "ArcSegmentCreator job did not succeed, job status:" + job.getStatus().getState() + ", reason: " + job.getStatus().getFailureInfo();                        throw new RuntimeException(message);        }    } catch (IOException | InterruptedException | ClassNotFoundException e) {                throw e;    }    long end = System.currentTimeMillis();    }
1
public static void main(String[] args) throws Exception
{    int res = ToolRunner.run(NutchConfiguration.create(), new ArcSegmentCreator(), args);    System.exit(res);}
0
public int run(String[] args) throws Exception
{    String usage = "Usage: ArcSegmentCreator <arcFiles> <segmentsOutDir>";    if (args.length < 2) {        System.err.println(usage);        return -1;    }        Path arcFiles = new Path(args[0]);    Path segmentsOutDir = new Path(args[1]);    try {                createSegments(arcFiles, segmentsOutDir);        return 0;    } catch (Exception e) {                return -1;    }}
1
public static void main(String[] args) throws Exception
{    Configuration conf = NutchConfiguration.create();    int res = ToolRunner.run(conf, new Benchmark(), args);    System.exit(res);}
0
private static String getDate()
{    return new SimpleDateFormat("yyyyMMddHHmmss").format(new Date(System.currentTimeMillis()));}
0
private void createSeeds(FileSystem fs, Path seedsDir, int count) throws Exception
{    OutputStream os = fs.create(new Path(seedsDir, "seeds"));    for (int i = 0; i < count; i++) {        String url = "http://www.test-" + i + ".com/\r\n";        os.write(url.getBytes());    }    os.flush();    os.close();}
0
public void addTiming(String stage, String run, long timing)
{    if (!runs.contains(run)) {        runs.add(run);    }    if (!stages.contains(stage)) {        stages.add(stage);    }    Map<String, Long> t = timings.get(stage);    if (t == null) {        t = new HashMap<>();        timings.put(stage, t);    }    t.put(run, timing);}
0
public String toString()
{    StringBuilder sb = new StringBuilder();    sb.append("* Plugins:\t" + plugins + "\n");    sb.append("* Seeds:\t" + seeds + "\n");    sb.append("* Depth:\t" + depth + "\n");    sb.append("* Threads:\t" + threads + "\n");    sb.append("* TopN:\t" + topN + "\n");    sb.append("* Delete:\t" + delete + "\n");    sb.append("* TOTAL ELAPSED:\t" + elapsed + "\n");    for (String stage : stages) {        Map<String, Long> timing = timings.get(stage);        if (timing == null)            continue;        sb.append("- stage: " + stage + "\n");        for (String r : runs) {            Long Time = timing.get(r);            if (Time == null) {                continue;            }            sb.append("\trun " + r + "\t" + Time + "\n");        }    }    return sb.toString();}
0
public List<String> getStages()
{    return stages;}
0
public List<String> getRuns()
{    return runs;}
0
public int run(String[] args) throws Exception
{    String plugins = "protocol-http|parse-tika|scoring-opic|urlfilter-regex|urlnormalizer-pass";    int seeds = 1;    int depth = 10;    int threads = 10;    boolean delete = true;    long topN = Long.MAX_VALUE;    if (args.length == 0) {        System.err.println("Usage: Benchmark [-seeds NN] [-depth NN] [-threads NN] [-keep] [-maxPerHost NN] [-plugins <regex>]");        System.err.println("\t-seeds NN\tcreate NN unique hosts in a seed list (default: 1)");        System.err.println("\t-depth NN\tperform NN crawl cycles (default: 10)");        System.err.println("\t-threads NN\tuse NN threads per Fetcher task (default: 10)");        System.err.println("\t-keep\tkeep segment data (default: delete after updatedb)");        System.err.println("\t-plugins <regex>\toverride 'plugin.includes'.");        System.err.println("\tNOTE: if not specified, this is reset to: " + plugins);        System.err.println("\tNOTE: if 'default' is specified then a value set in nutch-default/nutch-site is used.");        System.err.println("\t-maxPerHost NN\tmax. # of URLs per host in a fetchlist");        return -1;    }    int maxPerHost = Integer.MAX_VALUE;    for (int i = 0; i < args.length; i++) {        if (args[i].equals("-seeds")) {            seeds = Integer.parseInt(args[++i]);        } else if (args[i].equals("-threads")) {            threads = Integer.parseInt(args[++i]);        } else if (args[i].equals("-depth")) {            depth = Integer.parseInt(args[++i]);        } else if (args[i].equals("-keep")) {            delete = false;        } else if (args[i].equals("-plugins")) {            plugins = args[++i];        } else if (args[i].equalsIgnoreCase("-maxPerHost")) {            maxPerHost = Integer.parseInt(args[++i]);        } else {                        return -1;        }    }    BenchmarkResults res = benchmark(seeds, depth, threads, maxPerHost, topN, delete, plugins);    System.out.println(res);    return 0;}
1
public BenchmarkResults benchmark(int seeds, int depth, int threads, int maxPerHost, long topN, boolean delete, String plugins) throws Exception
{    Configuration conf = getConf();    conf.set("http.proxy.host", "localhost");    conf.setInt("http.proxy.port", 8181);    conf.set("http.agent.name", "test");    conf.set("http.robots.agents", "test,*");    if (!plugins.equals("default")) {        conf.set("plugin.includes", plugins);    }    conf.setInt(Generator.GENERATOR_MAX_COUNT, maxPerHost);    conf.set(Generator.GENERATOR_COUNT_MODE, Generator.GENERATOR_COUNT_VALUE_HOST);    @SuppressWarnings("unused")    Job job = NutchJob.getInstance(getConf());    FileSystem fs = FileSystem.get(conf);    Path dir = new Path(getConf().get("hadoop.tmp.dir"), "bench-" + System.currentTimeMillis());    fs.mkdirs(dir);    Path rootUrlDir = new Path(dir, "seed");    fs.mkdirs(rootUrlDir);    createSeeds(fs, rootUrlDir, seeds);    if (LOG.isInfoEnabled()) {                                    }    BenchmarkResults res = new BenchmarkResults();    res.delete = delete;    res.depth = depth;    res.plugins = plugins;    res.seeds = seeds;    res.threads = threads;    res.topN = topN;    Path crawlDb = new Path(dir + "/crawldb");    Path linkDb = new Path(dir + "/linkdb");    Path segments = new Path(dir + "/segments");    res.elapsed = System.currentTimeMillis();    Injector injector = new Injector(getConf());    Generator generator = new Generator(getConf());    Fetcher fetcher = new Fetcher(getConf());    ParseSegment parseSegment = new ParseSegment(getConf());    CrawlDb crawlDbTool = new CrawlDb(getConf());    LinkDb linkDbTool = new LinkDb(getConf());        long start = System.currentTimeMillis();    injector.inject(crawlDb, rootUrlDir);    long delta = System.currentTimeMillis() - start;    res.addTiming("inject", "0", delta);    int i;    for (i = 0; i < depth; i++) {                start = System.currentTimeMillis();        Path[] segs = generator.generate(crawlDb, segments, -1, topN, System.currentTimeMillis());        delta = System.currentTimeMillis() - start;        res.addTiming("generate", i + "", delta);        if (segs == null) {                        break;        }        start = System.currentTimeMillis();                fetcher.fetch(segs[0], threads);        delta = System.currentTimeMillis() - start;        res.addTiming("fetch", i + "", delta);        if (!Fetcher.isParsing(conf)) {            start = System.currentTimeMillis();                        parseSegment.parse(segs[0]);            delta = System.currentTimeMillis() - start;            res.addTiming("parse", i + "", delta);        }        start = System.currentTimeMillis();                crawlDbTool.update(crawlDb, segs, true, true);        delta = System.currentTimeMillis() - start;        res.addTiming("update", i + "", delta);        start = System.currentTimeMillis();                linkDbTool.invert(linkDb, segs, true, true, false);        delta = System.currentTimeMillis() - start;        res.addTiming("invert", i + "", delta);                if (delete) {            for (Path p : segs) {                fs.delete(p, true);            }        }    }    if (i == 0) {            }    if (LOG.isInfoEnabled()) {            }    res.elapsed = System.currentTimeMillis() - res.elapsed;    @SuppressWarnings("resource")    CrawlDbReader dbreader = new CrawlDbReader();    dbreader.processStatJob(crawlDb.toString(), conf, false);    return res;}
1
private void init(InputStream stream)
{    if (stream == null) {        return;    }    Properties properties = new Properties();    try {        properties.load(stream);    } catch (IOException e) {        } finally {        try {            stream.close();        } catch (IOException e) {                }    }    setKeyPrefix(properties.getProperty("keyPrefix", ""));    setSimpleDateFormat(Boolean.parseBoolean(properties.getProperty("simpleDateFormat", "False")));    setJsonArray(Boolean.parseBoolean(properties.getProperty("jsonArray", "False")));    setReverseKey(Boolean.parseBoolean(properties.getProperty("reverseKey", "False")));}
0
public void setKeyPrefix(String keyPrefix)
{    this.keyPrefix = keyPrefix;}
0
public void setSimpleDateFormat(boolean simpleDateFormat)
{    this.simpleDateFormat = simpleDateFormat;}
0
public void setJsonArray(boolean jsonArray)
{    this.jsonArray = jsonArray;}
0
public void setReverseKey(boolean reverseKey)
{    this.reverseKey = reverseKey;}
0
public void setReverseKeyValue(String reverseKeyValue)
{    this.reverseKeyValue = reverseKeyValue;}
0
public String getKeyPrefix()
{    return this.keyPrefix;}
0
public boolean getSimpleDateFormat()
{    return this.simpleDateFormat;}
0
public boolean getJsonArray()
{    return this.jsonArray;}
0
public boolean getReverseKey()
{    return this.reverseKey;}
0
public String getReverseKeyValue()
{    return this.reverseKeyValue;}
0
public boolean isCompressed()
{    return compressed;}
0
public void setCompressed(boolean compressed)
{    this.compressed = compressed;}
0
public long getWarcSize()
{    return warcSize;}
0
public void setWarcSize(long warcSize)
{    this.warcSize = warcSize;}
0
public String getOutputDir()
{    return outputDir;}
0
public void setOutputDir(String outputDir)
{    this.outputDir = outputDir;}
0
public static void main(String[] args) throws Exception
{    Configuration conf = NutchConfiguration.create();    int res = ToolRunner.run(conf, new CommonCrawlDataDumper(), args);    System.exit(res);}
0
public void dump(File outputDir, File segmentRootDir, File linkdb, boolean gzip, String[] mimeTypes, boolean epochFilename, String extension, boolean warc) throws Exception
{    if (gzip) {            }        Map<String, Integer> typeCounts = new HashMap<>();        Map<String, Integer> filteredCounts = new HashMap<>();    Configuration nutchConfig = NutchConfiguration.create();    Path segmentRootPath = new Path(segmentRootDir.toString());    FileSystem fs = segmentRootPath.getFileSystem(nutchConfig);        List<Path> parts = new ArrayList<>();    RemoteIterator<LocatedFileStatus> files = fs.listFiles(segmentRootPath, true);    String partPattern = ".*" + File.separator + Content.DIR_NAME + File.separator + "part-[0-9]{5}" + File.separator + "data";    while (files.hasNext()) {        LocatedFileStatus next = files.next();        if (next.isFile()) {            Path path = next.getPath();            if (path.toString().matches(partPattern)) {                parts.add(path);            }        }    }    LinkDbReader linkDbReader = null;    if (linkdb != null) {        linkDbReader = new LinkDbReader(nutchConfig, new Path(linkdb.toString()));    }    if (parts == null || parts.size() == 0) {                System.exit(1);    }        if (gzip && !warc) {        fileList = new ArrayList<>();        constructNewStream(outputDir);    }    for (Path segmentPart : parts) {                try {            SequenceFile.Reader reader = new SequenceFile.Reader(nutchConfig, SequenceFile.Reader.file(segmentPart));            Writable key = (Writable) reader.getKeyClass().getConstructor().newInstance();            Content content = null;            while (reader.next(key)) {                content = new Content();                reader.getCurrentValue(content);                Metadata metadata = content.getMetadata();                String url = key.toString();                String baseName = FilenameUtils.getBaseName(url);                String extensionName = FilenameUtils.getExtension(url);                if (!extension.isEmpty()) {                    extensionName = extension;                } else if ((extensionName == null) || extensionName.isEmpty()) {                    extensionName = "html";                }                String outputFullPath = null;                String outputRelativePath = null;                String filename = null;                String timestamp = null;                String reverseKey = null;                if (epochFilename || config.getReverseKey()) {                    try {                        long epoch = new SimpleDateFormat("EEE, d MMM yyyy HH:mm:ss z").parse(getDate(metadata.get("Date"))).getTime();                        timestamp = String.valueOf(epoch);                    } catch (ParseException pe) {                                            }                    reverseKey = reverseUrl(url);                    config.setReverseKeyValue(reverseKey.replace("/", "_") + "_" + DigestUtils.sha1Hex(url) + "_" + timestamp);                }                if (!warc) {                    if (epochFilename) {                        outputFullPath = DumpFileUtil.createFileNameFromUrl(outputDir.getAbsolutePath(), reverseKey, url, timestamp, extensionName, !gzip);                        outputRelativePath = outputFullPath.substring(0, outputFullPath.lastIndexOf(File.separator) - 1);                        filename = content.getMetadata().get(Metadata.DATE) + "." + extensionName;                    } else {                        String md5Ofurl = DumpFileUtil.getUrlMD5(url);                        String fullDir = DumpFileUtil.createTwoLevelsDirectory(outputDir.getAbsolutePath(), md5Ofurl, !gzip);                        filename = DumpFileUtil.createFileName(md5Ofurl, baseName, extensionName);                        outputFullPath = String.format("%s/%s", fullDir, filename);                        String[] fullPathLevels = fullDir.split(Pattern.quote(File.separator));                        String firstLevelDirName = fullPathLevels[fullPathLevels.length - 2];                        String secondLevelDirName = fullPathLevels[fullPathLevels.length - 1];                        outputRelativePath = firstLevelDirName + secondLevelDirName;                    }                }                                Boolean filter = (mimeTypes == null);                String jsonData = "";                try {                    String mimeType = new Tika().detect(content.getContent());                                                            Set<String> inUrls = null;                    if (linkDbReader != null) {                        Inlinks inlinks = linkDbReader.getInlinks((Text) key);                        if (inlinks != null) {                            Iterator<Inlink> iterator = inlinks.iterator();                            inUrls = new LinkedHashSet<>();                            while (inUrls.size() <= MAX_INLINKS && iterator.hasNext()) {                                inUrls.add(iterator.next().getFromUrl());                            }                        }                    }                                        try (CommonCrawlFormat format = CommonCrawlFormatFactory.getCommonCrawlFormat(warc ? "WARC" : "JACKSON", nutchConfig, config)) {                        if (inUrls != null) {                            format.setInLinks(new ArrayList<>(inUrls));                        }                        jsonData = format.getJsonData(url, content, metadata);                    }                    collectStats(typeCounts, mimeType);                                        if ((mimeType != null) && (mimeTypes != null) && Arrays.asList(mimeTypes).contains(mimeType)) {                        collectStats(filteredCounts, mimeType);                        filter = true;                    }                } catch (IOException ioe) {                                        return;                }                if (!warc) {                    if (filter) {                        byte[] byteData = serializeCBORData(jsonData);                        if (!gzip) {                            File outputFile = new File(outputFullPath);                            if (outputFile.exists()) {                                                            } else {                                                                IOUtils.copy(new ByteArrayInputStream(byteData), new FileOutputStream(outputFile));                            }                        } else {                            if (fileList.contains(outputFullPath)) {                                                            } else {                                fileList.add(outputFullPath);                                                                                                TarArchiveEntry tarEntry = new TarArchiveEntry(outputRelativePath + File.separator + filename);                                tarEntry.setSize(byteData.length);                                tarOutput.putArchiveEntry(tarEntry);                                tarOutput.write(byteData);                                tarOutput.closeArchiveEntry();                            }                        }                    }                }            }            reader.close();        } catch (Exception e) {                    } finally {            fs.close();        }    }    if (gzip && !warc) {        closeStream();    }    if (!typeCounts.isEmpty()) {            }}
1
private void closeStream()
{    try {        tarOutput.finish();        tarOutput.close();        gzipOutput.close();        bufOutput.close();        fileOutput.close();    } catch (IOException ioe) {            }}
1
private void constructNewStream(File outputDir) throws IOException
{    String archiveName = new SimpleDateFormat("yyyyMMddhhmm'.tar.gz'").format(new Date());        fileOutput = new FileOutputStream(new File(outputDir + File.separator + archiveName));    bufOutput = new BufferedOutputStream(fileOutput);    gzipOutput = new GzipCompressorOutputStream(bufOutput);    tarOutput = new TarArchiveOutputStream(gzipOutput);    tarOutput.setLongFileMode(TarArchiveOutputStream.LONGFILE_GNU);}
1
private void writeMagicHeader(CBORGenerator generator) throws IOException
{                byte[] header = new byte[3];    header[0] = (byte) 0xd9;    header[1] = (byte) 0xd9;    header[2] = (byte) 0xf7;    generator.writeBytes(header, 0, header.length);}
0
private byte[] serializeCBORData(String jsonData)
{    CBORFactory factory = new CBORFactory();    CBORGenerator generator = null;    ByteArrayOutputStream stream = null;    try {        stream = new ByteArrayOutputStream();        generator = factory.createGenerator(stream);                writeMagicHeader(generator);        generator.writeString(jsonData);        generator.flush();        stream.flush();        return stream.toByteArray();    } catch (Exception e) {            } finally {        try {            generator.close();            stream.close();        } catch (IOException e) {                }    }    return null;}
1
private void collectStats(Map<String, Integer> typeCounts, String mimeType)
{    typeCounts.put(mimeType, typeCounts.containsKey(mimeType) ? typeCounts.get(mimeType) + 1 : 1);}
0
private String getDate(String timestamp)
{    if (timestamp == null || timestamp.isEmpty()) {        DateFormat dateFormat = new SimpleDateFormat("EEE, d MMM yyyy HH:mm:ss z");        timestamp = dateFormat.format(new Date());    }    return timestamp;}
0
public static String reverseUrl(String urlString)
{    URL url;    String reverseKey = null;    try {        url = new URL(urlString);        String[] hostPart = url.getHost().replace('.', '/').split("/");        StringBuilder sb = new StringBuilder();        sb.append(hostPart[hostPart.length - 1]);        for (int i = hostPart.length - 2; i >= 0; i--) {            sb.append("/" + hostPart[i]);        }        reverseKey = sb.toString();    } catch (MalformedURLException e) {            }    return reverseKey;}
1
public int run(String[] args) throws Exception
{    Option helpOpt = new Option("h", "help", false, "show this help message.");        @SuppressWarnings("static-access")    Option outputOpt = OptionBuilder.withArgName("outputDir").hasArg().withDescription("output directory (which will be created) to host the CBOR data.").create("outputDir");        Option warcOpt = new Option("warc", "export to a WARC file");    @SuppressWarnings("static-access")    Option segOpt = OptionBuilder.withArgName("segment").hasArgs().withDescription("the segment or directory containing segments to use").create("segment");        @SuppressWarnings("static-access")    Option mimeOpt = OptionBuilder.isRequired(false).withArgName("mimetype").hasArgs().withDescription("an optional list of mimetypes to dump, excluding all others. Defaults to all.").create("mimetype");    @SuppressWarnings("static-access")    Option gzipOpt = OptionBuilder.withArgName("gzip").hasArg(false).withDescription("an optional flag indicating whether to additionally gzip the data.").create("gzip");    @SuppressWarnings("static-access")    Option keyPrefixOpt = OptionBuilder.withArgName("keyPrefix").hasArg(true).withDescription("an optional prefix for key in the output format.").create("keyPrefix");    @SuppressWarnings("static-access")    Option simpleDateFormatOpt = OptionBuilder.withArgName("SimpleDateFormat").hasArg(false).withDescription("an optional format for timestamp in GMT epoch milliseconds.").create("SimpleDateFormat");    @SuppressWarnings("static-access")    Option epochFilenameOpt = OptionBuilder.withArgName("epochFilename").hasArg(false).withDescription("an optional format for output filename.").create("epochFilename");    @SuppressWarnings("static-access")    Option jsonArrayOpt = OptionBuilder.withArgName("jsonArray").hasArg(false).withDescription("an optional format for JSON output.").create("jsonArray");    @SuppressWarnings("static-access")    Option reverseKeyOpt = OptionBuilder.withArgName("reverseKey").hasArg(false).withDescription("an optional format for key value in JSON output.").create("reverseKey");    @SuppressWarnings("static-access")    Option extensionOpt = OptionBuilder.withArgName("extension").hasArg(true).withDescription("an optional file extension for output documents.").create("extension");    @SuppressWarnings("static-access")    Option sizeOpt = OptionBuilder.withArgName("warcSize").hasArg(true).withType(Number.class).withDescription("an optional file size in bytes for the WARC file(s)").create("warcSize");    @SuppressWarnings("static-access")    Option linkDbOpt = OptionBuilder.withArgName("linkdb").hasArg(true).withDescription("an optional linkdb parameter to include inlinks in dump files").isRequired(false).create("linkdb");        Options options = new Options();    options.addOption(helpOpt);    options.addOption(outputOpt);    options.addOption(segOpt);        options.addOption(warcOpt);    options.addOption(mimeOpt);    options.addOption(gzipOpt);        options.addOption(keyPrefixOpt);        options.addOption(simpleDateFormatOpt);    options.addOption(epochFilenameOpt);    options.addOption(jsonArrayOpt);    options.addOption(reverseKeyOpt);    options.addOption(extensionOpt);    options.addOption(sizeOpt);    options.addOption(linkDbOpt);    CommandLineParser parser = new GnuParser();    try {        CommandLine line = parser.parse(options, args);        if (line.hasOption("help") || !line.hasOption("outputDir") || (!line.hasOption("segment"))) {            HelpFormatter formatter = new HelpFormatter();            formatter.printHelp(CommonCrawlDataDumper.class.getName(), options, true);            return 0;        }        File outputDir = new File(line.getOptionValue("outputDir"));        File segmentRootDir = new File(line.getOptionValue("segment"));        String[] mimeTypes = line.getOptionValues("mimetype");        boolean gzip = line.hasOption("gzip");        boolean epochFilename = line.hasOption("epochFilename");        String keyPrefix = line.getOptionValue("keyPrefix", "");        boolean simpleDateFormat = line.hasOption("SimpleDateFormat");        boolean jsonArray = line.hasOption("jsonArray");        boolean reverseKey = line.hasOption("reverseKey");        String extension = line.getOptionValue("extension", "");        boolean warc = line.hasOption("warc");        long warcSize = 0;        if (line.getParsedOptionValue("warcSize") != null) {            warcSize = (Long) line.getParsedOptionValue("warcSize");        }        String linkdbPath = line.getOptionValue("linkdb");        File linkdb = linkdbPath == null ? null : new File(linkdbPath);        CommonCrawlConfig config = new CommonCrawlConfig();        config.setKeyPrefix(keyPrefix);        config.setSimpleDateFormat(simpleDateFormat);        config.setJsonArray(jsonArray);        config.setReverseKey(reverseKey);        config.setCompressed(gzip);        config.setWarcSize(warcSize);        config.setOutputDir(line.getOptionValue("outputDir"));        if (!outputDir.exists()) {                        if (!outputDir.mkdirs())                throw new Exception("Unable to create: [" + outputDir.getAbsolutePath() + "]");        }        CommonCrawlDataDumper dumper = new CommonCrawlDataDumper(config);        dumper.dump(outputDir, segmentRootDir, linkdb, gzip, mimeTypes, epochFilename, extension, warc);    } catch (Exception e) {                e.printStackTrace();        return -1;    }    return 0;}
1
public Map<String, Object> run(Map<String, Object> args, String crawlId) throws Exception
{    String keyPrefix = args.containsKey("keyPrefix") ? (String) args.get("keyPrefix") : "";    File outputDir = new File((String) args.get("outputDir"));    File segmentRootDir = new File((String) args.get(Nutch.ARG_SEGMENTDIR));    ArrayList<String> mimeTypesList = args.containsKey("mimetypes") ? (ArrayList<String>) args.get("mimetypes") : null;    String[] mimeTypes = null;    if (mimeTypesList != null) {        mimeTypes = new String[mimeTypesList.size()];        int i = 0;        for (String m : mimeTypesList) mimeTypes[i++] = m;    }    boolean gzip = args.containsKey("gzip") ? (boolean) args.get("gzip") : false;    boolean epochFilename = args.containsKey("epochFilename") ? (boolean) args.get("epochFilename") : false;    boolean simpleDateFormat = args.containsKey("simpleDateFormat") ? (boolean) args.get("simpleDateFormat") : false;    boolean jsonArray = args.containsKey("jsonArray") ? (boolean) args.get("jsonArray") : false;    boolean reverseKey = args.containsKey("reverseKey") ? (boolean) args.get("reverseKey") : false;    String extension = args.containsKey("extension") ? (String) args.get("extension") : "";    boolean warc = args.containsKey("warc") ? (boolean) args.get("warc") : false;    long warcSize = args.containsKey("warcSize") ? (Long) args.get("warcSize") : 0;    CommonCrawlConfig config = new CommonCrawlConfig();    config.setKeyPrefix(keyPrefix);    config.setSimpleDateFormat(simpleDateFormat);    config.setJsonArray(jsonArray);    config.setReverseKey(reverseKey);    config.setCompressed(gzip);    config.setWarcSize(warcSize);    config.setOutputDir((String) args.get("outputDir"));    if (!outputDir.exists()) {        if (!outputDir.mkdirs())            throw new Exception("Unable to create: [" + outputDir.getAbsolutePath() + "]");    }    CommonCrawlDataDumper dumper = new CommonCrawlDataDumper(config);    dumper.dump(outputDir, segmentRootDir, null, gzip, mimeTypes, epochFilename, extension, warc);    return null;}
0
public static CommonCrawlFormat getCommonCrawlFormat(String formatType, String url, Content content, Metadata metadata, Configuration nutchConf, CommonCrawlConfig config) throws IOException
{    if (formatType == null) {        return null;    }    if (formatType.equalsIgnoreCase("jackson")) {        return new CommonCrawlFormatJackson(url, content, metadata, nutchConf, config);    } else if (formatType.equalsIgnoreCase("jettinson")) {        return new CommonCrawlFormatJettinson(url, content, metadata, nutchConf, config);    } else if (formatType.equalsIgnoreCase("simple")) {        return new CommonCrawlFormatSimple(url, content, metadata, nutchConf, config);    }    return null;}
0
public static CommonCrawlFormat getCommonCrawlFormat(String formatType, Configuration nutchConf, CommonCrawlConfig config) throws IOException
{    if (formatType.equalsIgnoreCase("WARC")) {        return new CommonCrawlFormatWARC(nutchConf, config);    }    if (formatType.equalsIgnoreCase("JACKSON")) {        return new CommonCrawlFormatJackson(nutchConf, config);    }    return null;}
0
protected void writeKeyValue(String key, String value) throws IOException
{    generator.writeFieldName(key);    generator.writeString(value);}
0
protected void writeKeyNull(String key) throws IOException
{    generator.writeFieldName(key);    generator.writeNull();}
0
protected void startArray(String key, boolean nested, boolean newline) throws IOException
{    if (key != null) {        generator.writeFieldName(key);    }    generator.writeStartArray();}
0
protected void closeArray(String key, boolean nested, boolean newline) throws IOException
{    generator.writeEndArray();}
0
protected void writeArrayValue(String value) throws IOException
{    generator.writeString(value);}
0
protected void startObject(String key) throws IOException
{    if (key != null) {        generator.writeFieldName(key);    }    generator.writeStartObject();}
0
protected void closeObject(String key) throws IOException
{    generator.writeEndObject();}
0
protected String generateJson() throws IOException
{    this.generator.flush();    return this.out.toString();}
0
protected void writeKeyValue(String key, String value) throws IOException
{    try {        stackObjects.getFirst().put(key, value);    } catch (JSONException jsone) {        throw new IOException(jsone.getMessage());    }}
0
protected void writeKeyNull(String key) throws IOException
{    try {        stackObjects.getFirst().put(key, JSONObject.NULL);    } catch (JSONException jsone) {        throw new IOException(jsone.getMessage());    }}
0
protected void startArray(String key, boolean nested, boolean newline) throws IOException
{    JSONArray array = new JSONArray();    stackArrays.push(array);}
0
protected void closeArray(String key, boolean nested, boolean newline) throws IOException
{    try {        if (stackArrays.size() > 1) {            JSONArray array = stackArrays.pop();            if (nested) {                stackArrays.getFirst().put(array);            } else {                stackObjects.getFirst().put(key, array);            }        }    } catch (JSONException jsone) {        throw new IOException(jsone.getMessage());    }}
0
protected void writeArrayValue(String value) throws IOException
{    if (stackArrays.size() > 1) {        stackArrays.getFirst().put(value);    }}
0
protected void startObject(String key) throws IOException
{    JSONObject object = new JSONObject();    stackObjects.push(object);}
0
protected void closeObject(String key) throws IOException
{    try {        if (stackObjects.size() > 1) {            JSONObject object = stackObjects.pop();            stackObjects.getFirst().put(key, object);        }    } catch (JSONException jsone) {        throw new IOException(jsone.getMessage());    }}
0
protected String generateJson() throws IOException
{    try {        return stackObjects.getFirst().toString(2);    } catch (JSONException jsone) {        throw new IOException(jsone.getMessage());    }}
0
protected void writeKeyValue(String key, String value) throws IOException
{    sb.append(printTabs() + "\"" + key + "\": " + quote(value) + ",\n");}
0
protected void writeKeyNull(String key) throws IOException
{    sb.append(printTabs() + "\"" + key + "\": null,\n");}
0
protected void startArray(String key, boolean nested, boolean newline) throws IOException
{    String name = (key != null) ? "\"" + key + "\": " : "";    String nl = (newline) ? "\n" : "";    sb.append(printTabs() + name + "[" + nl);    if (newline) {        this.tabCount++;    }}
0
protected void closeArray(String key, boolean nested, boolean newline) throws IOException
{    if (sb.charAt(sb.length() - 1) == ',') {                sb.deleteCharAt(sb.length() - 1);    } else if (sb.charAt(sb.length() - 2) == ',') {                sb.deleteCharAt(sb.length() - 2);    }    String nl = (newline) ? printTabs() : "";    if (newline) {        this.tabCount++;    }    sb.append(nl + "],\n");}
0
protected void writeArrayValue(String value)
{    sb.append("\"" + value + "\",");}
0
protected void startObject(String key) throws IOException
{    String name = "";    if (key != null) {        name = "\"" + key + "\": ";    }    sb.append(printTabs() + name + "{\n");    this.tabCount++;}
0
protected void closeObject(String key) throws IOException
{    if (sb.charAt(sb.length() - 2) == ',') {                sb.deleteCharAt(sb.length() - 2);    }    this.tabCount--;    sb.append(printTabs() + "},\n");}
0
protected String generateJson() throws IOException
{        sb.deleteCharAt(sb.length() - 1);        sb.deleteCharAt(sb.length() - 1);    return sb.toString();}
0
private String printTabs()
{    StringBuilder sb = new StringBuilder();    for (int i = 0; i < this.tabCount; i++) {        sb.append("\t");    }    return sb.toString();}
0
private static String quote(String string) throws IOException
{    StringBuilder sb = new StringBuilder();    if (string == null || string.length() == 0) {        sb.append("\"\"");        return sb.toString();    }    char b;    char c = 0;    String hhhh;    int i;    int len = string.length();    sb.append('"');    for (i = 0; i < len; i += 1) {        b = c;        c = string.charAt(i);        switch(c) {            case '\\':            case '"':                sb.append('\\');                sb.append(c);                break;            case '/':                if (b == '<') {                    sb.append('\\');                }                sb.append(c);                break;            case '\b':                sb.append("\\b");                break;            case '\t':                sb.append("\\t");                break;            case '\n':                sb.append("\\n");                break;            case '\f':                sb.append("\\f");                break;            case '\r':                sb.append("\\r");                break;            default:                if (c < ' ' || (c >= '\u0080' && c < '\u00a0') || (c >= '\u2000' && c < '\u2100')) {                    sb.append("\\u");                    hhhh = Integer.toHexString(c);                    sb.append("0000", 0, 4 - hhhh.length());                    sb.append(hhhh);                } else {                    sb.append(c);                }        }    }    sb.append('"');    return sb.toString();}
0
public String getJsonData(String url, Content content, Metadata metadata, ParseData parseData) throws IOException
{    this.url = url;    this.content = content;    this.metadata = metadata;    this.parseData = parseData;    return this.getJsonData();}
0
public String getJsonData() throws IOException
{    long position = writer.getPosition();    try {                        writer.checkSize();        if (writer.getPosition() != position) {                        position = writer.getPosition();        }                URI id = writeResponse();        if (StringUtils.isNotBlank(metadata.get("_request_"))) {                        writeRequest(id);        }    } catch (IOException e) {                throw e;    } catch (ParseException e) {                            }    return null;}
1
protected URI writeResponse() throws IOException, ParseException
{    WARCRecordInfo record = new WARCRecordInfo();    record.setType(WARCConstants.WARCRecordType.response);    record.setUrl(getUrl());    record.setCreate14DigitDate(DateUtils.getLog14Date(Long.parseLong(metadata.get("nutch.fetch.time"))));    record.setMimetype(WARCConstants.HTTP_RESPONSE_MIMETYPE);    record.setRecordId(GENERATOR.getRecordID());    String IP = getResponseAddress();    if (StringUtils.isNotBlank(IP))        record.addExtraHeader(WARCConstants.HEADER_KEY_IP, IP);    if (ParseSegment.isTruncated(content))        record.addExtraHeader(WARCConstants.HEADER_KEY_TRUNCATED, "unspecified");    ByteArrayOutputStream output = new ByteArrayOutputStream();    String httpHeaders = metadata.get("_response.headers_");    httpHeaders = WARCUtils.fixHttpHeaders(httpHeaders, content.getContent().length);    if (StringUtils.isNotBlank(httpHeaders)) {        output.write(httpHeaders.getBytes());    } else {                        record.setType(WARCConstants.WARCRecordType.resource);        record.setMimetype(content.getContentType());    }    output.write(getResponseContent().getBytes());    record.setContentLength(output.size());    record.setContentStream(new ByteArrayInputStream(output.toByteArray()));    if (output.size() > 0) {                        writer.writeRecord(record);    }    return record.getRecordId();}
0
protected URI writeRequest(URI id) throws IOException, ParseException
{    WARCRecordInfo record = new WARCRecordInfo();    record.setType(WARCConstants.WARCRecordType.request);    record.setUrl(getUrl());    record.setCreate14DigitDate(DateUtils.getLog14Date(Long.parseLong(metadata.get("nutch.fetch.time"))));    record.setMimetype(WARCConstants.HTTP_REQUEST_MIMETYPE);    record.setRecordId(GENERATOR.getRecordID());    if (id != null) {        ANVLRecord headers = new ANVLRecord();        headers.addLabelValue(WARCConstants.HEADER_KEY_CONCURRENT_TO, '<' + id.toString() + '>');        record.setExtraHeaders(headers);    }    ByteArrayOutputStream output = new ByteArrayOutputStream();    output.write(metadata.get("_request_").getBytes());    record.setContentLength(output.size());    record.setContentStream(new ByteArrayInputStream(output.toByteArray()));    writer.writeRecord(record);    return record.getRecordId();}
0
protected String generateJson() throws IOException
{    return null;}
0
protected void writeKeyValue(String key, String value) throws IOException
{    throw new NotImplementedException();}
0
protected void writeKeyNull(String key) throws IOException
{    throw new NotImplementedException();}
0
protected void startArray(String key, boolean nested, boolean newline) throws IOException
{    throw new NotImplementedException();}
0
protected void closeArray(String key, boolean nested, boolean newline) throws IOException
{    throw new NotImplementedException();}
0
protected void writeArrayValue(String value) throws IOException
{    throw new NotImplementedException();}
0
protected void startObject(String key) throws IOException
{    throw new NotImplementedException();}
0
protected void closeObject(String key) throws IOException
{    throw new NotImplementedException();}
0
public void close()
{    if (writer != null)        try {            writer.close();        } catch (IOException e) {            throw new RuntimeException(e);        }}
0
public int read() throws IOException
{    int c = in.read();    int value = c;    if (    c != -1 && !(XMLChar.isValid(c)))        value = 'X';    else if (lastBad && c == '<') {                in.mark(1);        if (in.read() != '/')            value = 'X';        in.reset();    }    lastBad = (c == 65533);    return value;}
0
public int read(char[] cbuf, int off, int len) throws IOException
{    int n = in.read(cbuf, off, len);    if (n != -1) {        for (int i = 0; i < n; i++) {            char c = cbuf[off + i];            char value = c;            if (            !(XMLChar.isValid(c)))                value = 'X';            else if (lastBad && c == '<') {                                if (i != n - 1 && cbuf[off + i + 1] != '/')                    value = 'X';            }            lastBad = (c == 65533);            cbuf[off + i] = value;        }    }    return n;}
0
public void startElement(String namespaceURI, String localName, String qName, Attributes atts) throws SAXException
{    if ("Topic".equals(qName)) {        curSection = atts.getValue("r:id");    } else if ("ExternalPage".equals(qName)) {                if ((!includeAdult) && curSection.startsWith("Top/Adult")) {            return;        }        if (topicPattern != null && !topicPattern.matcher(curSection).matches()) {            return;        }                        String url = atts.getValue("about");        int hashValue = MD5Hash.digest(url).hashCode();        hashValue = Math.abs(hashValue ^ hashSkew);        if ((hashValue % subsetDenom) != 0) {            return;        }                curURL = url;    } else if (curURL != null && "d:Title".equals(qName)) {        titlePending = true;    } else if (curURL != null && "d:Description".equals(qName)) {        descPending = true;    }}
0
public void characters(char[] ch, int start, int length)
{    if (titlePending) {        title.append(ch, start, length);    } else if (descPending) {        desc.append(ch, start, length);    }}
0
public void endElement(String namespaceURI, String localName, String qName) throws SAXException
{    if (curURL != null) {        if ("ExternalPage".equals(qName)) {                                                            System.out.println(curURL);            pages++;                        if (title.length() > 0) {                title.delete(0, title.length());            }            if (desc.length() > 0) {                desc.delete(0, desc.length());            }                        curURL = null;        } else if ("d:Title".equals(qName)) {            titlePending = false;        } else if ("d:Description".equals(qName)) {            descPending = false;        }    }}
0
public void startDocument()
{    }
1
public void endDocument()
{    }
1
public void setDocumentLocator(Locator locator)
{    location = locator;}
0
public void error(SAXParseException spe)
{    if (LOG.isErrorEnabled()) {            }}
1
public void warning(SAXParseException spe)
{    if (LOG.isWarnEnabled()) {            }}
1
public void parseDmozFile(File dmozFile, int subsetDenom, boolean includeAdult, int skew, Pattern topicPattern) throws IOException, SAXException, ParserConfigurationException
{    SAXParserFactory parserFactory = SAXParserFactory.newInstance();    SAXParser parser = parserFactory.newSAXParser();    XMLReader reader = parser.getXMLReader();        RDFProcessor rp = new RDFProcessor(reader, subsetDenom, includeAdult, skew, topicPattern);    reader.setContentHandler(rp);    reader.setErrorHandler(rp);            try (XMLCharFilter in = new XMLCharFilter(new BufferedReader(new InputStreamReader(new BufferedInputStream(new FileInputStream(dmozFile)), "UTF-8")))) {        InputSource is = new InputSource(in);        reader.parse(is);    } catch (Exception e) {        if (LOG.isErrorEnabled()) {                    }        System.exit(0);    }}
1
private static void addTopicsFromFile(String topicFile, Vector<String> topics) throws IOException
{    try (BufferedReader in = new BufferedReader(new InputStreamReader(new FileInputStream(topicFile), "UTF-8"))) {        String line = null;        while ((line = in.readLine()) != null) {            topics.addElement(line);        }    } catch (Exception e) {        if (LOG.isErrorEnabled()) {                    }        System.exit(0);    }}
1
public static void main(String[] argv) throws Exception
{    if (argv.length < 1) {        System.err.println("Usage: DmozParser <dmoz_file> [-subset <subsetDenominator>] [-includeAdultMaterial] [-skew skew] [-topicFile <topic list file>] [-topic <topic> [-topic <topic> [...]]]");        return;    }                    int subsetDenom = 1;    int skew = 0;    String dmozFile = argv[0];    boolean includeAdult = false;    Pattern topicPattern = null;    Vector<String> topics = new Vector<>();    Configuration conf = NutchConfiguration.create();    try (FileSystem fs = FileSystem.get(conf)) {        for (int i = 1; i < argv.length; i++) {            if ("-includeAdultMaterial".equals(argv[i])) {                includeAdult = true;            } else if ("-subset".equals(argv[i])) {                subsetDenom = Integer.parseInt(argv[i + 1]);                i++;            } else if ("-topic".equals(argv[i])) {                topics.addElement(argv[i + 1]);                i++;            } else if ("-topicFile".equals(argv[i])) {                addTopicsFromFile(argv[i + 1], topics);                i++;            } else if ("-skew".equals(argv[i])) {                skew = Integer.parseInt(argv[i + 1]);                i++;            }        }        DmozParser parser = new DmozParser();        if (!topics.isEmpty()) {            String regExp = "^(";            int j = 0;            for (; j < topics.size() - 1; ++j) {                regExp = regExp.concat(topics.get(j));                regExp = regExp.concat("|");            }            regExp = regExp.concat(topics.get(j));            regExp = regExp.concat(").*");                        topicPattern = Pattern.compile(regExp);        }        parser.parseDmozFile(new File(dmozFile), subsetDenom, includeAdult, skew, topicPattern);    }}
1
public static void main(String[] args) throws Exception
{        Option helpOpt = new Option("h", "help", false, "show this help message");        @SuppressWarnings("static-access")    Option outputOpt = OptionBuilder.withArgName("outputDir").hasArg().withDescription("output directory (which will be created) to host the raw data").create("outputDir");    @SuppressWarnings("static-access")    Option segOpt = OptionBuilder.withArgName("segment").hasArgs().withDescription("the segment(s) to use").create("segment");    @SuppressWarnings("static-access")    Option mimeOpt = OptionBuilder.withArgName("mimetype").hasArgs().withDescription("an optional list of mimetypes to dump, excluding all others. Defaults to all.").create("mimetype");    @SuppressWarnings("static-access")    Option mimeStat = OptionBuilder.withArgName("mimeStats").withDescription("only display mimetype stats for the segment(s) instead of dumping file.").create("mimeStats");    @SuppressWarnings("static-access")    Option dirStructureOpt = OptionBuilder.withArgName("flatdir").withDescription("optionally specify that the output directory should only contain files.").create("flatdir");    @SuppressWarnings("static-access")    Option reverseURLOutput = OptionBuilder.withArgName("reverseUrlDirs").withDescription("optionally specify to use reverse URL folders for output structure.").create("reverseUrlDirs");        Options options = new Options();    options.addOption(helpOpt);    options.addOption(outputOpt);    options.addOption(segOpt);    options.addOption(mimeOpt);    options.addOption(mimeStat);    options.addOption(dirStructureOpt);    options.addOption(reverseURLOutput);    CommandLineParser parser = new GnuParser();    try {        CommandLine line = parser.parse(options, args);        if (line.hasOption("help") || !line.hasOption("outputDir") || (!line.hasOption("segment"))) {            HelpFormatter formatter = new HelpFormatter();            formatter.printHelp("FileDumper", options, true);            return;        }        File outputDir = new File(line.getOptionValue("outputDir"));        File segmentRootDir = new File(line.getOptionValue("segment"));        String[] mimeTypes = line.getOptionValues("mimetype");        boolean flatDir = line.hasOption("flatdir");        boolean shouldDisplayStats = false;        if (line.hasOption("mimeStats"))            shouldDisplayStats = true;        boolean reverseURLDump = false;        if (line.hasOption("reverseUrlDirs"))            reverseURLDump = true;        if (!outputDir.exists()) {                        if (!shouldDisplayStats) {                if (!outputDir.mkdirs())                    throw new Exception("Unable to create: [" + outputDir.getAbsolutePath() + "]");            }        }        FileDumper dumper = new FileDumper();        dumper.dump(outputDir, segmentRootDir, mimeTypes, flatDir, shouldDisplayStats, reverseURLDump);    } catch (Exception e) {                e.printStackTrace();        return;    }}
1
private void collectStats(Map<String, Integer> typeCounts, String mimeType)
{    typeCounts.put(mimeType, typeCounts.containsKey(mimeType) ? typeCounts.get(mimeType) + 1 : 1);}
0
public void setup(Mapper<WritableComparable<?>, Text, Text, Generator.SelectorEntry>.Context context)
{    Configuration conf = context.getConfiguration();    defaultInterval = conf.getInt("db.fetch.interval.default", 0);    scfilters = new ScoringFilters(conf);    if (conf.getBoolean(FILTER_KEY, false)) {        filters = new URLFilters(conf);    }    if (conf.getBoolean(NORMALIZE_KEY, false)) {        normalizers = new URLNormalizers(conf, URLNormalizers.SCOPE_INJECT);    }}
0
public void map(WritableComparable<?> key, Text value, Context context) throws IOException, InterruptedException
{        String urlString = value.toString();    try {        if (normalizers != null) {            urlString = normalizers.normalize(urlString, URLNormalizers.SCOPE_INJECT);        }        if (urlString != null && filters != null) {            urlString = filters.filter(urlString);        }        if (urlString != null) {            url.set(urlString);            scfilters.injectedScore(url, datum);        }    } catch (Exception e) {                return;    }    if (urlString == null) {        if (LOG.isDebugEnabled()) {                    }        return;    }    entry.datum = datum;    entry.url = url;        entry.datum.setFetchInterval(defaultInterval);    context.write(url, entry);}
1
public void reduce(Text key, Iterable<Generator.SelectorEntry> values, Context context) throws IOException, InterruptedException
{            HashMap<Text, CrawlDatum> unique = new HashMap<>();    for (Generator.SelectorEntry entry : values) {        unique.put(entry.url, entry.datum);    }        for (Entry<Text, CrawlDatum> e : unique.entrySet()) {        context.write(e.getKey(), e.getValue());    }}
0
public int run(String[] args) throws Exception
{    if (args.length < 2) {        System.err.println("Usage: FreeGenerator <inputDir> <segmentsDir> [-filter] [-normalize]");        System.err.println("\tinputDir\tinput directory containing one or more input files.");        System.err.println("\t\tEach text file contains a list of URLs, one URL per line");        System.err.println("\tsegmentsDir\toutput directory, where new segment will be created");        System.err.println("\t-filter\trun current URLFilters on input URLs");        System.err.println("\t-normalize\trun current URLNormalizers on input URLs");        return -1;    }    boolean filter = false;    boolean normalize = false;    if (args.length > 2) {        for (int i = 2; i < args.length; i++) {            if (args[i].equals("-filter")) {                filter = true;            } else if (args[i].equals("-normalize")) {                normalize = true;            } else {                                return -1;            }        }    }    SimpleDateFormat sdf = new SimpleDateFormat("yyyy-MM-dd HH:mm:ss");    long start = System.currentTimeMillis();        Job job = NutchJob.getInstance(getConf());    Configuration conf = job.getConfiguration();    conf.setBoolean(FILTER_KEY, filter);    conf.setBoolean(NORMALIZE_KEY, normalize);    FileInputFormat.addInputPath(job, new Path(args[0]));    job.setInputFormatClass(TextInputFormat.class);    job.setJarByClass(FG.class);    job.setMapperClass(FG.FGMapper.class);    job.setMapOutputKeyClass(Text.class);    job.setMapOutputValueClass(Generator.SelectorEntry.class);    job.setPartitionerClass(URLPartitioner.class);    job.setReducerClass(FG.FGReducer.class);    String segName = Generator.generateSegmentName();    job.setNumReduceTasks(Integer.parseInt(conf.get("mapreduce.job.maps")));    job.setOutputFormatClass(SequenceFileOutputFormat.class);    job.setOutputKeyClass(Text.class);    job.setOutputValueClass(CrawlDatum.class);    job.setSortComparatorClass(Generator.HashComparator.class);    FileOutputFormat.setOutputPath(job, new Path(args[1], new Path(segName, CrawlDatum.GENERATE_DIR_NAME)));    try {        boolean success = job.waitForCompletion(true);        if (!success) {            String message = "FreeGenerator job did not succeed, job status:" + job.getStatus().getState() + ", reason: " + job.getStatus().getFailureInfo();                        throw new RuntimeException(message);        }    } catch (IOException | InterruptedException | ClassNotFoundException e) {                return -1;    }    long end = System.currentTimeMillis();        return 0;}
1
public static void main(String[] args) throws Exception
{    int res = ToolRunner.run(NutchConfiguration.create(), new FreeGenerator(), args);    System.exit(res);}
0
public void run()
{    numTotal.incrementAndGet();    String host = URLUtil.getHost(url);    long start = System.currentTimeMillis();    try {                        InetAddress.getByName(host);                numResolved.incrementAndGet();    } catch (Exception uhe) {                numErrored.incrementAndGet();    }    long end = System.currentTimeMillis();    long total = (end - start);    totalTime.addAndGet(total);    }
1
public void resolveUrls()
{    try {                pool = Executors.newFixedThreadPool(numThreads);                BufferedReader buffRead = new BufferedReader(new FileReader(new File(urlsFile)));        String urlStr = null;        while ((urlStr = buffRead.readLine()) != null) {                                    pool.execute(new ResolverThread(urlStr));        }                        buffRead.close();        pool.awaitTermination(60, TimeUnit.SECONDS);    } catch (Exception e) {                pool.shutdownNow();            }        pool.shutdown();    }
1
public static void main(String[] args)
{    Options options = new Options();    OptionBuilder.withArgName("help");    OptionBuilder.withDescription("show this help message");    Option helpOpts = OptionBuilder.create("help");    options.addOption(helpOpts);    OptionBuilder.withArgName("urls");    OptionBuilder.hasArg();    OptionBuilder.withDescription("the urls file to check");    Option urlOpts = OptionBuilder.create("urls");    options.addOption(urlOpts);    OptionBuilder.withArgName("numThreads");    OptionBuilder.hasArgs();    OptionBuilder.withDescription("the number of threads to use");    Option numThreadOpts = OptionBuilder.create("numThreads");    options.addOption(numThreadOpts);    CommandLineParser parser = new GnuParser();    try {                CommandLine line = parser.parse(options, args);        if (line.hasOption("help") || !line.hasOption("urls")) {            HelpFormatter formatter = new HelpFormatter();            formatter.printHelp("ResolveUrls", options);            return;        }                String urls = line.getOptionValue("urls");        int numThreads = 100;        String numThreadsStr = line.getOptionValue("numThreads");        if (numThreadsStr != null) {            numThreads = Integer.parseInt(numThreadsStr);        }        ResolveUrls resolve = new ResolveUrls(urls, numThreads);        resolve.resolveUrls();    } catch (Exception e) {            }}
1
public void close() throws IOException
{}
0
public void setup(Mapper<Text, Writable, Text, NutchWritable>.Context context)
{}
0
public void map(Text key, Writable value, Context context) throws IOException, InterruptedException
{    context.write(key, new NutchWritable(value));}
0
public void setup(Reducer<Text, NutchWritable, NullWritable, WARCWritable>.Context context)
{}
0
public void reduce(Text key, Iterable<NutchWritable> values, Context context) throws IOException, InterruptedException
{    Content content = null;    CrawlDatum cd = null;    SimpleDateFormat warcdf = new SimpleDateFormat("yyyy-MM-dd'T'HH:mm:ss'Z'", Locale.ENGLISH);        for (NutchWritable val : values) {                final Writable value = val.get();        if (value instanceof Content) {            content = (Content) value;            continue;        }        if (value instanceof CrawlDatum) {            cd = (CrawlDatum) value;            continue;        }    }        if (content == null) {                context.getCounter("WARCExporter", "missing content").increment(1);        return;    }    if (cd == null) {                context.getCounter("WARCExporter", "missing metadata").increment(1);        return;    }        String headersVerbatim = content.getMetadata().get("_response.headers_");    headersVerbatim = WARCUtils.fixHttpHeaders(headersVerbatim, content.getContent().length);    byte[] httpheaders = new byte[0];    if (StringUtils.isNotBlank(headersVerbatim)) {                if (!headersVerbatim.endsWith(CRLF + CRLF)) {            headersVerbatim += CRLF + CRLF;        }        httpheaders = headersVerbatim.getBytes();    }    StringBuilder buffer = new StringBuilder();    buffer.append(WARCRecord.WARC_VERSION);    buffer.append(CRLF);    buffer.append("WARC-Record-ID").append(": ").append("<urn:uuid:").append(UUID.randomUUID().toString()).append(">").append(CRLF);    int contentLength = 0;    if (content != null) {        contentLength = content.getContent().length;    }        contentLength += httpheaders.length;    buffer.append("Content-Length").append(": ").append(Integer.toString(contentLength)).append(CRLF);    Date fetchedDate = new Date(cd.getFetchTime());    buffer.append("WARC-Date").append(": ").append(warcdf.format(fetchedDate)).append(CRLF);            String WARCTypeValue = "resource";    if (StringUtils.isNotBlank(headersVerbatim)) {        WARCTypeValue = "response";    }    buffer.append("WARC-Type").append(": ").append(WARCTypeValue).append(CRLF);        String IP = content.getMetadata().get("_ip_");    if (StringUtils.isNotBlank(IP)) {        buffer.append("WARC-IP-Address").append(": ").append("IP").append(CRLF);    }        String status = CrawlDatum.getStatusName(cd.getStatus());    if (status.equalsIgnoreCase("STATUS_FETCH_SUCCESS") && ParseSegment.isTruncated(content)) {        buffer.append("WARC-Truncated").append(": ").append("unspecified").append(CRLF);    }        try {        String normalised = key.toString().replaceAll(" ", "%20");        URI uri = URI.create(normalised);        buffer.append("WARC-Target-URI").append(": ").append(uri.toASCIIString()).append(CRLF);    } catch (Exception e) {                context.getCounter("WARCExporter", "invalid URI").increment(1);        return;    }        if (WARCTypeValue.equals("response")) {        buffer.append("Content-Type: application/http; msgtype=response").append(CRLF);    }        ByteArrayOutputStream bos = new ByteArrayOutputStream();        bos.write(buffer.toString().getBytes("UTF-8"));    bos.write(CRLF_BYTES);        bos.write(httpheaders);        if (content.getContent() != null) {        bos.write(content.getContent());    }    bos.write(CRLF_BYTES);    bos.write(CRLF_BYTES);    try {        DataInput in = new DataInputStream(new ByteArrayInputStream(bos.toByteArray()));        WARCRecord record = new WARCRecord(in);        context.write(NullWritable.get(), new WARCWritable(record));        context.getCounter("WARCExporter", "records generated").increment(1);    } catch (IOException | IllegalStateException exception) {                context.getCounter("WARCExporter", "exception").increment(1);    }}
1
public int generateWARC(String output, List<Path> segments) throws IOException
{    SimpleDateFormat sdf = new SimpleDateFormat("yyyy-MM-dd HH:mm:ss");    long start = System.currentTimeMillis();        final Job job = NutchJob.getInstance(getConf());    job.setJobName("warc-exporter " + output);    for (final Path segment : segments) {                FileInputFormat.addInputPath(job, new Path(segment, Content.DIR_NAME));        FileInputFormat.addInputPath(job, new Path(segment, CrawlDatum.FETCH_DIR_NAME));    }    job.setInputFormatClass(SequenceFileInputFormat.class);    job.setJarByClass(WARCMapReduce.class);    job.setMapperClass(WARCMapReduce.WARCMapper.class);    job.setReducerClass(WARCMapReduce.WARCReducer.class);    job.setMapOutputKeyClass(Text.class);    job.setMapOutputValueClass(NutchWritable.class);    FileOutputFormat.setOutputPath(job, new Path(output));        job.setOutputFormatClass(WARCOutputFormat.class);    job.setOutputKeyClass(NullWritable.class);    job.setOutputValueClass(WARCWritable.class);    try {        boolean success = job.waitForCompletion(true);        if (!success) {            String message = "WARCExporter job did not succeed, job status:" + job.getStatus().getState() + ", reason: " + job.getStatus().getFailureInfo();                        throw new RuntimeException(message);        }                long end = System.currentTimeMillis();            } catch (IOException | InterruptedException | ClassNotFoundException e) {                return -1;    }    return 0;}
1
public int run(String[] args) throws Exception
{    if (args.length < 2) {        System.err.println("Usage: WARCExporter <output> (<segment> ... | -dir <segments>)");        return -1;    }    final List<Path> segments = new ArrayList<>();    for (int i = 1; i < args.length; i++) {        if (args[i].equals("-dir")) {            Path dir = new Path(args[++i]);            FileSystem fs = dir.getFileSystem(getConf());            FileStatus[] fstats = fs.listStatus(dir, HadoopFSUtil.getPassDirectoriesFilter(fs));            Path[] files = HadoopFSUtil.getPaths(fstats);            for (Path p : files) {                segments.add(p);            }        } else {            segments.add(new Path(args[i]));        }    }    return generateWARC(args[0], segments);}
0
public static void main(String[] args) throws Exception
{    final int res = ToolRunner.run(NutchConfiguration.create(), new WARCExporter(), args);    System.exit(res);}
0
public static final ANVLRecord getWARCInfoContent(Configuration conf)
{    ANVLRecord record = new ANVLRecord();        record.addLabelValue(FORMAT, "WARC File Format 1.0");    record.addLabelValue(CONFORMS_TO, "http://bibnum.bnf.fr/WARC/WARC_ISO_28500_version1_latestdraft.pdf");    record.addLabelValue(SOFTWARE, conf.get("http.agent.name", ""));    record.addLabelValue(HTTP_HEADER_USER_AGENT, getAgentString(conf.get("http.agent.name", ""), conf.get("http.agent.version", ""), conf.get("http.agent.description", ""), conf.get("http.agent.url", ""), conf.get("http.agent.email", "")));    record.addLabelValue(HTTP_HEADER_FROM, conf.get("http.agent.email", ""));    try {        record.addLabelValue(HOSTNAME, getHostname(conf));        record.addLabelValue(IP, getIPAddress(conf));    } catch (UnknownHostException ignored) {        }        record.addLabelValue(ROBOTS, "classic");    record.addLabelValue(OPERATOR, conf.get("http.agent.email", ""));    return record;}
0
public static final String getHostname(Configuration conf) throws UnknownHostException
{    return StringUtil.isEmpty(conf.get("http.agent.host", "")) ? InetAddress.getLocalHost().getHostName() : conf.get("http.agent.host");}
0
public static final String getIPAddress(Configuration conf) throws UnknownHostException
{    return InetAddress.getLocalHost().getHostAddress();}
0
public static final byte[] toByteArray(HttpHeaders headers) throws IOException
{    ByteArrayOutputStream out = new ByteArrayOutputStream();    headers.write(out);    return out.toByteArray();}
0
public static final String getAgentString(String name, String version, String description, String URL, String email)
{    StringBuffer buf = new StringBuffer();    buf.append(name);    if (version != null) {        buf.append("/").append(version);    }    if (((description != null) && (description.length() != 0)) || ((email != null) && (email.length() != 0)) || ((URL != null) && (URL.length() != 0))) {        buf.append(" (");        if ((description != null) && (description.length() != 0)) {            buf.append(description);            if ((URL != null) || (email != null))                buf.append("; ");        }        if ((URL != null) && (URL.length() != 0)) {            buf.append(URL);            if (email != null)                buf.append("; ");        }        if ((email != null) && (email.length() != 0))            buf.append(email);        buf.append(")");    }    return buf.toString();}
0
public static final WARCRecordInfo docToMetadata(NutchDocument doc) throws UnsupportedEncodingException
{    WARCRecordInfo record = new WARCRecordInfo();    record.setType(WARCConstants.WARCRecordType.metadata);    record.setUrl((String) doc.getFieldValue("id"));    record.setCreate14DigitDate(DateUtils.get14DigitDate((Date) doc.getFieldValue("tstamp")));    record.setMimetype("application/warc-fields");    record.setRecordId(generator.getRecordID());        ANVLRecord metadata = new ANVLRecord();    for (String field : doc.getFieldNames()) {        List<Object> values = doc.getField(field).getValues();        for (Object value : values) {            if (value instanceof Date) {                metadata.addLabelValue(field, DateUtils.get14DigitDate());            } else {                metadata.addLabelValue(field, (String) value);            }        }    }    record.setContentLength(metadata.getLength());    record.setContentStream(new ByteArrayInputStream(metadata.getUTF8Bytes()));    return record;}
0
public static final String fixHttpHeaders(String headers, int contentLength)
{    if (headers == null) {        return null;    }    int start = 0, lineEnd = 0, last = 0, trailingCrLf = 0;    StringBuilder replace = new StringBuilder();    while (start < headers.length()) {        lineEnd = headers.indexOf(CRLF, start);        trailingCrLf = 1;        if (lineEnd == -1) {            lineEnd = headers.length();            trailingCrLf = 0;        }        int colonPos = -1;        for (int i = start; i < lineEnd; i++) {            if (headers.charAt(i) == ':') {                colonPos = i;                break;            }        }        if (colonPos == -1) {            boolean valid = true;            if (start == 0) {                                    } else if ((lineEnd + 4) == headers.length() && headers.endsWith(CRLF + CRLF)) {                                trailingCrLf = 2;            } else {                valid = false;            }            if (!valid) {                if (last < start) {                    replace.append(headers.substring(last, start));                }                last = lineEnd + 2 * trailingCrLf;            }            start = lineEnd + 2 * trailingCrLf;            /*         * skip over invalid header line, no further check for problematic         * headers required         */            continue;        }        String name = headers.substring(start, colonPos);        if (PROBLEMATIC_HEADERS.matcher(name).matches()) {            boolean needsFix = true;            if (name.equalsIgnoreCase("content-length")) {                String value = headers.substring(colonPos + 1, lineEnd).trim();                try {                    int l = Integer.parseInt(value);                    if (l == contentLength) {                        needsFix = false;                    }                } catch (NumberFormatException e) {                                }            }            if (needsFix) {                if (last < start) {                    replace.append(headers.substring(last, start));                }                last = lineEnd + 2 * trailingCrLf;                replace.append(X_HIDE_HEADER).append(headers.substring(start, lineEnd + 2 * trailingCrLf));                if (trailingCrLf == 0) {                    replace.append(CRLF);                    trailingCrLf = 1;                }                if (name.equalsIgnoreCase("content-length")) {                                        replace.append("Content-Length").append(COLONSP).append(contentLength).append(CRLF);                }            }        }        start = lineEnd + 2 * trailingCrLf;    }    if (last > 0 || trailingCrLf != 2) {        if (last < headers.length()) {                        replace.append(headers.substring(last));        }        while (trailingCrLf < 2) {            replace.append(CRLF);            trailingCrLf++;        }        return replace.toString();    }    return headers;}
0
protected int parseArgs(String[] args, int i)
{    if (args[i].equals("-listen")) {        tcpPort = Integer.parseInt(args[++i]);        return 2;    } else if (args[i].equals("-keepClientCnxOpen")) {        keepClientCnxOpen = true;        return 1;    } else if (args[i].equals("-stdin")) {        stdin = true;        return 1;    }    return 0;}
0
protected int run() throws Exception
{        if (tcpPort != -1) {        processTCP(tcpPort);        return 0;    } else if (stdin) {        return processStdin();    }        return -1;}
0
protected int processSingle(String input) throws Exception
{    StringBuilder output = new StringBuilder();    int ret = process(input, output);    System.out.println(output);    return ret;}
0
protected int processStdin() throws Exception
{    BufferedReader in = new BufferedReader(new InputStreamReader(System.in));    String line;    while ((line = in.readLine()) != null) {        StringBuilder output = new StringBuilder();        @SuppressWarnings("unused")        int ret = process(line, output);        System.out.println(output);    }    return 0;}
0
protected void processTCP(int tcpPort) throws Exception
{    ServerSocket server = null;    try {        server = new ServerSocket();        server.bind(new InetSocketAddress(tcpPort));            } catch (Exception e) {                System.exit(-1);    }    while (true) {        Worker worker;        try {            worker = new Worker(server.accept());            Thread thread = new Thread(worker);            thread.start();        } catch (Exception e) {                        System.exit(-1);        }    }}
1
public void run()
{        BufferedReader in = null;    OutputStream out = null;    try {        in = new BufferedReader(new InputStreamReader(client.getInputStream()));        out = client.getOutputStream();    } catch (IOException e) {                return;    }        if (keepClientCnxOpen) {        try {                        while (readWrite(in, out)) {            }        } catch (Exception e) {                    }    } else {        try {            readWrite(in, out);        } catch (Exception e) {                    }    }    try {                client.close();    } catch (Exception e) {            }}
1
protected boolean readWrite(BufferedReader in, OutputStream out) throws Exception
{    String line = in.readLine();    if (line == null) {                return false;    }    if (line.trim().length() > 1) {                StringBuilder output = new StringBuilder();        process(line, output);        output.append("\n");        out.write(output.toString().getBytes(StandardCharsets.UTF_8));    }    return true;}
0
protected ProtocolOutput getProtocolOutput(String url, CrawlDatum datum) throws Exception
{    ProtocolFactory factory = new ProtocolFactory(getConf());    Protocol protocol = factory.getProtocol(url);    Text turl = new Text(url);    return protocol.getProtocolOutput(turl, datum);}
0
public int getExitValue()
{    return _xit;}
0
public void setCommand(String s)
{    _command = s;}
0
public String getCommand()
{    return _command;}
0
public void setInputStream(InputStream is)
{    _stdin = is;}
0
public void setStdOutputStream(OutputStream os)
{    _stdout = os;}
0
public void setStdErrorStream(OutputStream os)
{    _stderr = os;}
0
public void evaluate() throws IOException
{    this.exec();}
0
public int exec() throws IOException
{    Process proc = Runtime.getRuntime().exec(_command);    _barrier = new CyclicBarrier(3 + ((_stdin != null) ? 1 : 0));    PullerThread so = new PullerThread("STDOUT", proc.getInputStream(), _stdout);    so.setDaemon(true);    so.start();    PullerThread se = new PullerThread("STDERR", proc.getErrorStream(), _stderr);    se.setDaemon(true);    se.start();    PusherThread si = null;    if (_stdin != null) {        si = new PusherThread("STDIN", _stdin, proc.getOutputStream());        si.setDaemon(true);        si.start();    }    boolean _timedout = false;    long end = System.currentTimeMillis() + _timeout * 1000;    try {        if (_timeout == 0) {            _barrier.await();        } else {            _barrier.await(_timeout, TimeUnit.SECONDS);        }    } catch (TimeoutException ex) {        _timedout = true;    } catch (BrokenBarrierException bbe) {    /* IGNORE */    } catch (InterruptedException e) {    /* IGNORE */    }        if (si != null) {        si.interrupt();    }    so.interrupt();    se.interrupt();    _xit = -1;    if (!_timedout) {        if (_waitForExit) {            do {                try {                    Thread.sleep(1000);                    _xit = proc.exitValue();                } catch (InterruptedException ie) {                    if (Thread.interrupted()) {                                                break;                    } else {                        continue;                    }                } catch (IllegalThreadStateException iltse) {                    continue;                }                break;            } while (!(_timedout = (System.currentTimeMillis() > end)));        } else {            try {                _xit = proc.exitValue();            } catch (IllegalThreadStateException iltse) {                _timedout = true;            }        }    }    if (_waitForExit) {        proc.destroy();    }    return _xit;}
0
public Throwable getThrownError()
{    return _thrownError;}
0
public void run()
{    try {        byte[] buf = new byte[BUF];        int read = 0;        while (!isInterrupted() && (read = _is.read(buf)) != -1) {            if (read == 0)                continue;            _os.write(buf, 0, read);            _os.flush();        }    } catch (InterruptedIOException iioe) {        } catch (Throwable t) {        _thrownError = t;    } finally {        try {            if (_closeInput) {                _is.close();            } else {                _os.close();            }        } catch (IOException ioe) {        /* IGNORE */        }    }    try {        _barrier.await();    } catch (InterruptedException ie) {    /* IGNORE */    } catch (BrokenBarrierException bbe) {    /* IGNORE */    }}
0
public int getTimeout()
{    return _timeout;}
0
public void setTimeout(int timeout)
{    _timeout = timeout;}
0
public boolean getWaitForExit()
{    return _waitForExit;}
0
public void setWaitForExit(boolean waitForExit)
{    _waitForExit = waitForExit;}
0
public static void main(String[] args) throws Exception
{    String commandPath = null;    String filePath = null;    int timeout = 10;    String usage = "Usage: CommandRunner [-timeout timeoutSecs] commandPath filePath";    if (args.length < 2) {        System.err.println(usage);        System.exit(-1);    }    for (int i = 0; i < args.length; i++) {        if (args[i].equals("-timeout")) {            timeout = Integer.parseInt(args[++i]);        } else if (i != args.length - 2) {            System.err.println(usage);            System.exit(-1);        } else {            commandPath = args[i];            filePath = args[++i];        }    }    CommandRunner cr = new CommandRunner();    cr.setCommand(commandPath);    cr.setInputStream(new java.io.FileInputStream(filePath));    cr.setStdErrorStream(System.err);    cr.setStdOutputStream(System.out);    cr.setTimeout(timeout);    cr.evaluate();    System.err.println("output value: " + cr.getExitValue());}
0
public int run(String[] args) throws Exception
{    Option helpOpt = new Option("h", "help", false, "Show this message");    @SuppressWarnings("static-access")    Option inDirs = OptionBuilder.withArgName("inputDirs").isRequired().withDescription("Comma separated list of crawl directories (e.g., \"./crawl1,./crawl2\")").hasArgs().create("inputDirs");    @SuppressWarnings("static-access")    Option outDir = OptionBuilder.withArgName("outputDir").isRequired().withDescription("Output directory where results should be dumped").hasArgs().create("outputDir");    @SuppressWarnings("static-access")    Option modeOpt = OptionBuilder.withArgName("mode").isRequired().withDescription("Set statistics gathering mode (by 'host' or by 'domain')").hasArgs().create("mode");    @SuppressWarnings("static-access")    Option numReducers = OptionBuilder.withArgName("numReducers").withDescription("Optional number of reduce jobs to use. Defaults to 1").hasArgs().create("numReducers");    Options options = new Options();    options.addOption(helpOpt);    options.addOption(inDirs);    options.addOption(outDir);    options.addOption(modeOpt);    options.addOption(numReducers);    CommandLineParser parser = new GnuParser();    CommandLine cli;    try {        cli = parser.parse(options, args);    } catch (MissingOptionException e) {        HelpFormatter formatter = new HelpFormatter();        formatter.printHelp("CrawlCompletionStats", options, true);        return 1;    }    if (cli.hasOption("help")) {        HelpFormatter formatter = new HelpFormatter();        formatter.printHelp("CrawlCompletionStats", options, true);        return 1;    }    String inputDir = cli.getOptionValue("inputDirs");    String outputDir = cli.getOptionValue("outputDir");    int numOfReducers = 1;    if (cli.hasOption("numReducers")) {        numOfReducers = Integer.parseInt(args[3]);    }    SimpleDateFormat sdf = new SimpleDateFormat("yyyy-MM-dd HH:mm:ss");    long start = System.currentTimeMillis();        int mode = 0;    String jobName = "CrawlCompletionStats";    if (cli.getOptionValue("mode").equals("host")) {        jobName = "Host CrawlCompletionStats";        mode = MODE_HOST;    } else if (cli.getOptionValue("mode").equals("domain")) {        jobName = "Domain CrawlCompletionStats";        mode = MODE_DOMAIN;    }    Configuration conf = getConf();    conf.setInt("domain.statistics.mode", mode);    conf.setBoolean("mapreduce.fileoutputcommitter.marksuccessfuljobs", false);    Job job = Job.getInstance(conf, jobName);    job.setJarByClass(CrawlCompletionStats.class);    String[] inputDirsSpecs = inputDir.split(",");    for (int i = 0; i < inputDirsSpecs.length; i++) {        File completeInputPath = new File(new File(inputDirsSpecs[i]), "crawldb/current");        FileInputFormat.addInputPath(job, new Path(completeInputPath.toString()));    }    job.setInputFormatClass(SequenceFileInputFormat.class);    FileOutputFormat.setOutputPath(job, new Path(outputDir));    job.setOutputFormatClass(TextOutputFormat.class);    job.setMapOutputKeyClass(Text.class);    job.setMapOutputValueClass(LongWritable.class);    job.setOutputKeyClass(Text.class);    job.setOutputValueClass(LongWritable.class);    job.setMapperClass(CrawlCompletionStatsMapper.class);    job.setReducerClass(CrawlCompletionStatsReducer.class);    job.setCombinerClass(CrawlCompletionStatsCombiner.class);    job.setNumReduceTasks(numOfReducers);    try {        boolean success = job.waitForCompletion(true);        if (!success) {            String message = jobName + " job did not succeed, job status: " + job.getStatus().getState() + ", reason: " + job.getStatus().getFailureInfo();                                    throw new RuntimeException(message);        }    } catch (IOException | InterruptedException | ClassNotFoundException e) {                throw e;    }    long end = System.currentTimeMillis();        return 0;}
1
public void setup(Context context)
{    mode = context.getConfiguration().getInt("domain.statistics.mode", MODE_DOMAIN);}
0
public void map(Text urlText, CrawlDatum datum, Context context) throws IOException, InterruptedException
{    URL url = new URL(urlText.toString());    String out = "";    switch(mode) {        case MODE_HOST:            out = url.getHost();            break;        case MODE_DOMAIN:            out = URLUtil.getDomainName(url);            break;    }    if (datum.getStatus() == CrawlDatum.STATUS_DB_FETCHED || datum.getStatus() == CrawlDatum.STATUS_DB_NOTMODIFIED) {        context.write(new Text(out + " FETCHED"), new LongWritable(1));    } else {        context.write(new Text(out + " UNFETCHED"), new LongWritable(1));    }}
0
public void reduce(Text key, Iterable<LongWritable> values, Context context) throws IOException, InterruptedException
{    long total = 0;    for (LongWritable val : values) {        total += val.get();    }    context.write(new LongWritable(total), key);}
0
public void reduce(Text key, Iterable<LongWritable> values, Context context) throws IOException, InterruptedException
{    long total = 0;    for (LongWritable val : values) {        total += val.get();    }    context.write(key, new LongWritable(total));}
0
public static void main(String[] args) throws Exception
{    ToolRunner.run(NutchConfiguration.create(), new CrawlCompletionStats(), args);}
0
public static final byte[] inflateBestEffort(byte[] in)
{    return inflateBestEffort(in, Integer.MAX_VALUE);}
0
public static final byte[] inflateBestEffort(byte[] in, int sizeLimit)
{        ByteArrayOutputStream outStream = new ByteArrayOutputStream(EXPECTED_COMPRESSION_RATIO * in.length);        Inflater inflater = new Inflater(true);    InflaterInputStream inStream = new InflaterInputStream(new ByteArrayInputStream(in), inflater);    byte[] buf = new byte[BUF_SIZE];    int written = 0;    while (true) {        try {            int size = inStream.read(buf);            if (size <= 0)                break;            if ((written + size) > sizeLimit) {                outStream.write(buf, 0, sizeLimit - written);                break;            }            outStream.write(buf, 0, size);            written += size;        } catch (Exception e) {                        break;        }    }    try {        outStream.close();    } catch (IOException e) {    }    return outStream.toByteArray();}
1
public static final byte[] inflate(byte[] in) throws IOException
{        ByteArrayOutputStream outStream = new ByteArrayOutputStream(EXPECTED_COMPRESSION_RATIO * in.length);    InflaterInputStream inStream = new InflaterInputStream(new ByteArrayInputStream(in));    byte[] buf = new byte[BUF_SIZE];    while (true) {        int size = inStream.read(buf);        if (size <= 0)            break;        outStream.write(buf, 0, size);    }    outStream.close();    return outStream.toByteArray();}
0
public static final byte[] deflate(byte[] in)
{        ByteArrayOutputStream byteOut = new ByteArrayOutputStream(in.length / EXPECTED_COMPRESSION_RATIO);    DeflaterOutputStream outStream = new DeflaterOutputStream(byteOut);    try {        outStream.write(in);    } catch (Exception e) {            }    try {        outStream.close();    } catch (IOException e) {            }    return byteOut.toByteArray();}
1
public int run(String[] args) throws Exception
{    if (args.length < 3) {        System.err.println("Usage: DomainStatistics inputDirs outDir mode [numOfReducer]");        System.err.println("\tinputDirs\tComma separated list of crawldb input directories");        System.err.println("\t\t\tE.g.: crawl/crawldb/");        System.err.println("\toutDir\t\tOutput directory where results should be dumped");        System.err.println("\tmode\t\tSet statistics gathering mode");        System.err.println("\t\t\t\thost\tGather statistics by host");        System.err.println("\t\t\t\tdomain\tGather statistics by domain");        System.err.println("\t\t\t\tsuffix\tGather statistics by suffix");        System.err.println("\t\t\t\ttld\tGather statistics by top level directory");        System.err.println("\t[numOfReducers]\tOptional number of reduce jobs to use. Defaults to 1.");        return 1;    }    String inputDir = args[0];    String outputDir = args[1];    int numOfReducers = 1;    if (args.length > 3) {        numOfReducers = Integer.parseInt(args[3]);    }    SimpleDateFormat sdf = new SimpleDateFormat("yyyy-MM-dd HH:mm:ss");    long start = System.currentTimeMillis();        int mode = 0;    String jobName = "DomainStatistics";    if (args[2].equals("host")) {        jobName = "Host statistics";        mode = MODE_HOST;    } else if (args[2].equals("domain")) {        jobName = "Domain statistics";        mode = MODE_DOMAIN;    } else if (args[2].equals("suffix")) {        jobName = "Suffix statistics";        mode = MODE_SUFFIX;    } else if (args[2].equals("tld")) {        jobName = "TLD statistics";        mode = MODE_TLD;    }    Configuration conf = getConf();    conf.setInt("domain.statistics.mode", mode);    conf.setBoolean("mapreduce.fileoutputcommitter.marksuccessfuljobs", false);    Job job = Job.getInstance(conf, jobName);    job.setJarByClass(DomainStatistics.class);    String[] inputDirsSpecs = inputDir.split(",");    for (int i = 0; i < inputDirsSpecs.length; i++) {        File completeInputPath = new File(new File(inputDirsSpecs[i]), "current");        FileInputFormat.addInputPath(job, new Path(completeInputPath.toString()));    }    job.setInputFormatClass(SequenceFileInputFormat.class);    FileOutputFormat.setOutputPath(job, new Path(outputDir));    job.setOutputFormatClass(TextOutputFormat.class);    job.setMapOutputKeyClass(Text.class);    job.setMapOutputValueClass(LongWritable.class);    job.setOutputKeyClass(Text.class);    job.setOutputValueClass(LongWritable.class);    job.setMapperClass(DomainStatisticsMapper.class);    job.setReducerClass(DomainStatisticsReducer.class);    job.setCombinerClass(DomainStatisticsCombiner.class);    job.setNumReduceTasks(numOfReducers);    try {        boolean success = job.waitForCompletion(true);        if (!success) {            String message = "Injector job did not succeed, job status: " + job.getStatus().getState() + ", reason: " + job.getStatus().getFailureInfo();                                    throw new RuntimeException(message);        }    } catch (IOException | InterruptedException | ClassNotFoundException e) {                throw e;    }    long end = System.currentTimeMillis();        return 0;}
1
public void setup(Context context)
{    mode = context.getConfiguration().getInt("domain.statistics.mode", MODE_DOMAIN);}
0
public void map(Text urlText, CrawlDatum datum, Context context) throws IOException, InterruptedException
{    if (datum.getStatus() == CrawlDatum.STATUS_DB_FETCHED || datum.getStatus() == CrawlDatum.STATUS_DB_NOTMODIFIED) {        try {            URL url = new URL(urlText.toString());            String out = null;            switch(mode) {                case MODE_HOST:                    out = url.getHost();                    break;                case MODE_DOMAIN:                    out = URLUtil.getDomainName(url);                    break;                case MODE_SUFFIX:                    out = URLUtil.getDomainSuffix(url).getDomain();                    break;                case MODE_TLD:                    out = URLUtil.getTopLevelDomainName(url);                    break;            }            if (out.trim().equals("")) {                                context.getCounter(MyCounter.EMPTY_RESULT).increment(1);            }            context.write(new Text(out), new LongWritable(1));        } catch (Exception ex) {        }        context.getCounter(MyCounter.FETCHED).increment(1);        context.write(FETCHED_TEXT, new LongWritable(1));    } else {        context.getCounter(MyCounter.NOT_FETCHED).increment(1);        context.write(NOT_FETCHED_TEXT, new LongWritable(1));    }}
1
public void reduce(Text key, Iterable<LongWritable> values, Context context) throws IOException, InterruptedException
{    long total = 0;    for (LongWritable val : values) {        total += val.get();    }    context.write(new LongWritable(total), key);}
0
public void reduce(Text key, Iterable<LongWritable> values, Context context) throws IOException, InterruptedException
{    long total = 0;    for (LongWritable val : values) {        total += val.get();    }    context.write(key, new LongWritable(total));}
0
public static void main(String[] args) throws Exception
{    ToolRunner.run(NutchConfiguration.create(), new DomainStatistics(), args);}
0
public String getDomain()
{    return domain;}
0
public Status getStatus()
{    return status;}
0
public float getBoost()
{    return boost;}
0
public String toString()
{    return domain;}
0
public static DomainSuffixes getInstance()
{    if (instance == null) {        instance = new DomainSuffixes();    }    return instance;}
0
 void addDomainSuffix(DomainSuffix tld)
{    domains.put(tld.getDomain(), tld);}
0
public boolean isDomainSuffix(String extension)
{    return domains.containsKey(extension);}
0
public DomainSuffix get(String extension)
{    return domains.get(extension);}
0
 void read(DomainSuffixes tldEntries, InputStream input) throws IOException
{    try {        DocumentBuilderFactory factory = DocumentBuilderFactory.newInstance();        factory.setIgnoringComments(true);        DocumentBuilder builder = factory.newDocumentBuilder();        Document document = builder.parse(new InputSource(input));        Element root = document.getDocumentElement();        if (root != null && root.getTagName().equals("domains")) {            Element tlds = (Element) root.getElementsByTagName("tlds").item(0);            Element suffixes = (Element) root.getElementsByTagName("suffixes").item(0);                        readITLDs(tldEntries, (Element) tlds.getElementsByTagName("itlds").item(0));            readGTLDs(tldEntries, (Element) tlds.getElementsByTagName("gtlds").item(0));            readCCTLDs(tldEntries, (Element) tlds.getElementsByTagName("cctlds").item(0));            readSuffixes(tldEntries, suffixes);        } else {            throw new IOException("xml file is not valid");        }    } catch (ParserConfigurationException ex) {                throw new IOException(ex.getMessage());    } catch (SAXException ex) {                throw new IOException(ex.getMessage());    }}
1
 void readITLDs(DomainSuffixes tldEntries, Element el)
{    NodeList children = el.getElementsByTagName("tld");    for (int i = 0; i < children.getLength(); i++) {        tldEntries.addDomainSuffix(readGTLD((Element) children.item(i), Type.INFRASTRUCTURE));    }}
0
 void readGTLDs(DomainSuffixes tldEntries, Element el)
{    NodeList children = el.getElementsByTagName("tld");    for (int i = 0; i < children.getLength(); i++) {        tldEntries.addDomainSuffix(readGTLD((Element) children.item(i), Type.GENERIC));    }}
0
 void readCCTLDs(DomainSuffixes tldEntries, Element el) throws IOException
{    NodeList children = el.getElementsByTagName("tld");    for (int i = 0; i < children.getLength(); i++) {        tldEntries.addDomainSuffix(readCCTLD((Element) children.item(i)));    }}
0
 TopLevelDomain readGTLD(Element el, Type type)
{    String domain = el.getAttribute("domain");    Status status = readStatus(el);    float boost = readBoost(el);    return new TopLevelDomain(domain, type, status, boost);}
0
 TopLevelDomain readCCTLD(Element el) throws IOException
{    String domain = el.getAttribute("domain");    Status status = readStatus(el);    float boost = readBoost(el);    String countryName = readCountryName(el);    return new TopLevelDomain(domain, status, boost, countryName);}
0
 Status readStatus(Element el)
{    NodeList list = el.getElementsByTagName("status");    if (list == null || list.getLength() == 0)        return DomainSuffix.DEFAULT_STATUS;    return Status.valueOf(list.item(0).getFirstChild().getNodeValue());}
0
 float readBoost(Element el)
{    NodeList list = el.getElementsByTagName("boost");    if (list == null || list.getLength() == 0)        return DomainSuffix.DEFAULT_BOOST;    return Float.parseFloat(list.item(0).getFirstChild().getNodeValue());}
0
 String readCountryName(Element el) throws IOException
{    NodeList list = el.getElementsByTagName("country");    if (list == null || list.getLength() == 0)        throw new IOException("Country name should be given");    return list.item(0).getNodeValue();}
0
 void readSuffixes(DomainSuffixes tldEntries, Element el)
{    NodeList children = el.getElementsByTagName("suffix");    for (int i = 0; i < children.getLength(); i++) {        tldEntries.addDomainSuffix(readSuffix((Element) children.item(i)));    }}
0
 DomainSuffix readSuffix(Element el)
{    String domain = el.getAttribute("domain");    Status status = readStatus(el);    float boost = readBoost(el);    return new DomainSuffix(domain, status, boost);}
0
public Type getType()
{    return type;}
0
public String getCountryName()
{    return countryName;}
0
public static Element getDom(InputStream is)
{    Element element = null;    DOMParser parser = new DOMParser();    InputSource input;    try {        input = new InputSource(is);        input.setEncoding("UTF-8");        parser.parse(input);        int i = 0;        while (!(parser.getDocument().getChildNodes().item(i) instanceof Element)) {            i++;        }        element = (Element) parser.getDocument().getChildNodes().item(i);    } catch (FileNotFoundException e) {            } catch (SAXException e) {            } catch (IOException e) {            }    return element;}
1
public static void saveDom(OutputStream os, Element e)
{    DOMSource source = new DOMSource(e);    TransformerFactory transFactory = TransformerFactory.newInstance();    Transformer transformer;    try {        transformer = transFactory.newTransformer();        transformer.setOutputProperty("indent", "yes");        StreamResult result = new StreamResult(os);        transformer.transform(source, result);        os.flush();    } catch (UnsupportedEncodingException e1) {            } catch (IOException e1) {            } catch (TransformerConfigurationException e2) {            } catch (TransformerException ex) {            }}
1
public static void saveDom(OutputStream os, DocumentFragment doc)
{    NodeList docChildren = doc.getChildNodes();    for (int i = 0; i < docChildren.getLength(); i++) {        saveDom(os, (Element) docChildren.item(i));    }}
0
public static String getUrlMD5(String url)
{    byte[] digest = MD5Hash.digest(url).getDigest();    StringBuffer sb = new StringBuffer();    for (byte b : digest) {        sb.append(String.format("%02x", b & 0xff));    }    return sb.toString();}
0
public static String createTwoLevelsDirectory(String basePath, String md5, boolean makeDir)
{    String firstLevelDirName = new StringBuilder().append(md5.charAt(0)).append(md5.charAt(8)).toString();    String secondLevelDirName = new StringBuilder().append(md5.charAt(16)).append(md5.charAt(24)).toString();    String fullDirPath = String.format(DIR_PATTERN, basePath, firstLevelDirName, secondLevelDirName);    if (makeDir) {        try {            FileUtils.forceMkdir(new File(fullDirPath));        } catch (IOException e) {                        fullDirPath = null;        }    }    return fullDirPath;}
1
public static String createTwoLevelsDirectory(String basePath, String md5)
{    return createTwoLevelsDirectory(basePath, md5, true);}
0
public static String createFileName(String md5, String fileBaseName, String fileExtension)
{    if (fileBaseName.length() > MAX_LENGTH_OF_FILENAME) {                fileBaseName = StringUtils.substring(fileBaseName, 0, MAX_LENGTH_OF_FILENAME);    }    if (fileExtension.length() > MAX_LENGTH_OF_EXTENSION) {                fileExtension = StringUtils.substring(fileExtension, 0, MAX_LENGTH_OF_EXTENSION);    }        fileBaseName = fileBaseName.replaceAll("\\?", "");    fileExtension = fileExtension.replaceAll("\\?", "");    return String.format(FILENAME_PATTERN, md5, fileBaseName, fileExtension);}
1
public static String createFileNameFromUrl(String basePath, String reverseKey, String urlString, String epochScrapeTime, String fileExtension, boolean makeDir)
{    String fullDirPath = basePath + File.separator + reverseKey + File.separator + DigestUtils.sha1Hex(urlString);    if (makeDir) {        try {            FileUtils.forceMkdir(new File(fullDirPath));        } catch (IOException e) {                        fullDirPath = null;        }    }    if (fileExtension.length() > MAX_LENGTH_OF_EXTENSION) {                fileExtension = StringUtils.substring(fileExtension, 0, MAX_LENGTH_OF_EXTENSION);    }    String outputFullPath = fullDirPath + File.separator + epochScrapeTime + "." + fileExtension;    return outputFullPath;}
1
public static String displayFileTypes(Map<String, Integer> typeCounts, Map<String, Integer> filteredCounts)
{    StringBuilder builder = new StringBuilder();        builder.append("\nTOTAL Stats:\n");    builder.append("[\n");    int mimetypeCount = 0;    for (String mimeType : typeCounts.keySet()) {        builder.append("    {\"mimeType\":\"");        builder.append(mimeType);        builder.append("\",\"count\":\"");        builder.append(typeCounts.get(mimeType));        builder.append("\"}\n");        mimetypeCount += typeCounts.get(mimeType);    }    builder.append("]\n");    builder.append("Total count: " + mimetypeCount + "\n");        mimetypeCount = 0;    if (!filteredCounts.isEmpty()) {        builder.append("\nFILTERED Stats:\n");        builder.append("[\n");        for (String mimeType : filteredCounts.keySet()) {            builder.append("    {\"mimeType\":\"");            builder.append(mimeType);            builder.append("\",\"count\":\"");            builder.append(filteredCounts.get(mimeType));            builder.append("\"}\n");            mimetypeCount += filteredCounts.get(mimeType);        }        builder.append("]\n");        builder.append("Total filtered count: " + mimetypeCount + "\n");    }    return builder.toString();}
0
public String getSource()
{    return source;}
0
public String getValue()
{    return value;}
0
public String toString()
{    return value + " (" + source + ((confidence >= 0) ? ", " + confidence + "% confidence" : "") + ")";}
0
public boolean isEmpty()
{    return (value == null || "".equals(value));}
0
public boolean meetsThreshold()
{    return (confidence < 0 || (minConfidence >= 0 && confidence >= minConfidence));}
0
public void autoDetectClues(Content content, boolean filter)
{    byte[] data = content.getContent();    if (minConfidence >= 0 && DETECTABLES.contains(content.getContentType()) && data.length > MIN_LENGTH) {        CharsetMatch[] matches = null;                try {            detector.enableInputFilter(filter);            detector.setText(data);            matches = detector.detectAll();        } catch (Exception e) {                    }        if (matches != null) {            for (CharsetMatch match : matches) {                addClue(match.getName(), "detect", match.getConfidence());            }        }    }        addClue(parseCharacterEncoding(content.getMetadata().get(Response.CONTENT_TYPE)), "header");}
1
public void addClue(String value, String source, int confidence)
{    if (value == null || "".equals(value)) {        return;    }    value = resolveEncodingAlias(value);    if (value != null) {        clues.add(new EncodingClue(value, source, confidence));    }}
0
public void addClue(String value, String source)
{    addClue(value, source, NO_THRESHOLD);}
0
public String guessEncoding(Content content, String defaultValue)
{    /*     * This algorithm could be replaced by something more sophisticated; ideally     * we would gather a bunch of data on where various clues (autodetect, HTTP     * headers, HTML meta tags, etc.) disagree, tag each with the correct     * answer, and use machine learning/some statistical method to generate a     * better heuristic.     */    String base = content.getBaseUrl();    if (LOG.isTraceEnabled()) {        findDisagreements(base, clues);    }    /*     * Go down the list of encoding "clues". Use a clue if: 1. Has a confidence     * value which meets our confidence threshold, OR 2. Doesn't meet the     * threshold, but is the best try, since nothing else is available.     */    EncodingClue defaultClue = new EncodingClue(defaultValue, "default");    EncodingClue bestClue = defaultClue;    for (EncodingClue clue : clues) {        if (LOG.isTraceEnabled()) {            LOG.trace(base + ": charset " + clue);        }        String charset = clue.value;        if (minConfidence >= 0 && clue.confidence >= minConfidence) {            if (LOG.isTraceEnabled()) {                LOG.trace(base + ": Choosing encoding: " + charset + " with confidence " + clue.confidence);            }            return resolveEncodingAlias(charset).toLowerCase();        } else if (clue.confidence == NO_THRESHOLD && bestClue == defaultClue) {            bestClue = clue;        }    }    if (LOG.isTraceEnabled()) {        LOG.trace(base + ": Choosing encoding: " + bestClue);    }    return bestClue.value.toLowerCase();}
0
public void clearClues()
{    clues.clear();}
0
private void findDisagreements(String url, List<EncodingClue> newClues)
{    HashSet<String> valsSeen = new HashSet<>();    HashSet<String> sourcesSeen = new HashSet<>();    boolean disagreement = false;    for (int i = 0; i < newClues.size(); i++) {        EncodingClue clue = newClues.get(i);        if (!clue.isEmpty() && !sourcesSeen.contains(clue.source)) {            if (valsSeen.size() > 0 && !valsSeen.contains(clue.value) && clue.meetsThreshold()) {                disagreement = true;            }            if (clue.meetsThreshold()) {                valsSeen.add(clue.value);            }            sourcesSeen.add(clue.source);        }    }    if (disagreement) {                StringBuffer sb = new StringBuffer();        sb.append("Disagreement: " + url + "; ");        for (int i = 0; i < newClues.size(); i++) {            if (i > 0) {                sb.append(", ");            }            sb.append(newClues.get(i));        }        LOG.trace(sb.toString());    }}
0
public static String resolveEncodingAlias(String encoding)
{    try {        if (encoding == null || !Charset.isSupported(encoding))            return null;        String canonicalName = new String(Charset.forName(encoding).name());        return ALIASES.containsKey(canonicalName) ? ALIASES.get(canonicalName) : canonicalName;    } catch (Exception e) {                return null;    }}
1
public static String parseCharacterEncoding(String contentType)
{    if (contentType == null)        return (null);    int start = contentType.indexOf("charset=");    if (start < 0)        return (null);    String encoding = contentType.substring(start + 8);    int end = encoding.indexOf(';');    if (end >= 0)        encoding = encoding.substring(0, end);    encoding = encoding.trim();    if ((encoding.length() > 2) && (encoding.startsWith("\"")) && (encoding.endsWith("\"")))        encoding = encoding.substring(1, encoding.length() - 1);    return (encoding.trim());}
0
public static void main(String[] args) throws IOException
{    if (args.length != 1) {        System.err.println("Usage: EncodingDetector <file>");        System.exit(1);    }    Configuration conf = NutchConfiguration.create();    EncodingDetector detector = new EncodingDetector(NutchConfiguration.create());        @SuppressWarnings("resource")    BufferedInputStream istr = new BufferedInputStream(new FileInputStream(args[0]));    ByteArrayOutputStream ostr = new ByteArrayOutputStream();    byte[] bytes = new byte[1000];    boolean more = true;    while (more) {        int len = istr.read(bytes);        if (len < bytes.length) {            more = false;            if (len > 0) {                ostr.write(bytes, 0, len);            }        } else {            ostr.write(bytes);        }    }    byte[] data = ostr.toByteArray();        Content content = new Content("", "", data, "text/html", new Metadata(), conf);    detector.autoDetectClues(content, true);    String encoding = detector.guessEncoding(content, conf.get("parser.character.encoding.default"));    System.out.println("Guessed encoding: " + encoding);}
0
public static void replace(FileSystem fs, Path current, Path replacement, boolean removeOld) throws IOException
{        Path old = new Path(current + ".old");    if (fs.exists(current)) {        fs.rename(current, old);    }        fs.rename(replacement, current);    if (fs.exists(old) && removeOld) {        fs.delete(old, true);    }}
0
public static void closeReaders(SequenceFile.Reader[] readers) throws IOException
{        if (readers != null) {        for (int i = 0; i < readers.length; i++) {            SequenceFile.Reader reader = readers[i];            if (reader != null) {                reader.close();            }        }    }}
0
public static void closeReaders(MapFile.Reader[] readers) throws IOException
{        if (readers != null) {        for (int i = 0; i < readers.length; i++) {            MapFile.Reader reader = readers[i];            if (reader != null) {                reader.close();            }        }    }}
0
public Configuration getConf()
{    return conf;}
0
public void setConf(Configuration conf)
{    this.conf = conf;}
0
public void readFields(DataInput in) throws IOException
{    byte type = in.readByte();    Class<?> clazz = getTypes()[type];    try {        set((Writable) clazz.getConstructor().newInstance());    } catch (Exception e) {        e.printStackTrace();        throw new IOException("Cannot initialize the class: " + clazz);    }    Writable w = get();    if (w instanceof Configurable)        ((Configurable) w).setConf(conf);    w.readFields(in);}
0
public static final byte[] unzipBestEffort(byte[] in)
{    return unzipBestEffort(in, Integer.MAX_VALUE);}
0
public static final byte[] unzipBestEffort(byte[] in, int sizeLimit)
{    try {                ByteArrayOutputStream outStream = new ByteArrayOutputStream(EXPECTED_COMPRESSION_RATIO * in.length);        GZIPInputStream inStream = new GZIPInputStream(new ByteArrayInputStream(in));        byte[] buf = new byte[BUF_SIZE];        int written = 0;        while (true) {            try {                int size = inStream.read(buf);                if (size <= 0)                    break;                if ((written + size) > sizeLimit) {                    outStream.write(buf, 0, sizeLimit - written);                    break;                }                outStream.write(buf, 0, size);                written += size;            } catch (Exception e) {                break;            }        }        try {            outStream.close();        } catch (IOException e) {        }        return outStream.toByteArray();    } catch (IOException e) {        return null;    }}
0
public static final byte[] unzip(byte[] in) throws IOException
{        ByteArrayOutputStream outStream = new ByteArrayOutputStream(EXPECTED_COMPRESSION_RATIO * in.length);    GZIPInputStream inStream = new GZIPInputStream(new ByteArrayInputStream(in));    byte[] buf = new byte[BUF_SIZE];    while (true) {        int size = inStream.read(buf);        if (size <= 0)            break;        outStream.write(buf, 0, size);    }    outStream.close();    return outStream.toByteArray();}
0
public static final byte[] zip(byte[] in)
{    try {                ByteArrayOutputStream byteOut = new ByteArrayOutputStream(in.length / EXPECTED_COMPRESSION_RATIO);        GZIPOutputStream outStream = new GZIPOutputStream(byteOut);        try {            outStream.write(in);        } catch (Exception e) {                    }        try {            outStream.close();        } catch (IOException e) {                    }        return byteOut.toByteArray();    } catch (IOException e) {                return null;    }}
1
public static PathFilter getPassAllFilter()
{    return arg0 -> true;}
0
public static PathFilter getPassDirectoriesFilter(final FileSystem fs)
{    return path -> {        try {            return fs.getFileStatus(path).isDirectory();        } catch (IOException ioe) {            return false;        }    };}
0
public static Path[] getPaths(FileStatus[] stats)
{    if (stats == null) {        return null;    }    if (stats.length == 0) {        return new Path[0];    }    Path[] res = new Path[stats.length];    for (int i = 0; i < stats.length; i++) {        res[i] = stats[i].getPath();    }    return res;}
0
public static Expression parseExpression(String expr)
{    if (expr == null)        return null;    try {                        Matcher matcher = DATE_PATTERN.matcher(expr);        if (matcher.find()) {            String date = matcher.group();                        Date parsedDate = DateUtils.parseDateStrictly(date, new String[] { "yyyy-MM-dd'T'HH:mm:ss'Z'" });            long time = parsedDate.getTime();                        expr = expr.replace(date, Long.toString(time));        }        JexlEngine jexl = new JexlEngine();        jexl.setSilent(true);        jexl.setStrict(true);        return jexl.createExpression(expr);    } catch (Exception e) {            }    return null;}
1
public static void createLockFile(FileSystem fs, Path lockFile, boolean accept) throws IOException
{    if (fs.exists(lockFile)) {        if (!accept)            throw new IOException("lock file " + lockFile + " already exists.");        if (fs.getFileStatus(lockFile).isDirectory())            throw new IOException("lock file " + lockFile + " already exists and is a directory.");        } else {                fs.mkdirs(lockFile.getParent());        fs.createNewFile(lockFile);    }}
0
public static void createLockFile(Configuration conf, Path lockFile, boolean accept) throws IOException
{    FileSystem fs = lockFile.getFileSystem(conf);    createLockFile(fs, lockFile, accept);}
0
public static boolean removeLockFile(FileSystem fs, Path lockFile) throws IOException
{    if (!fs.exists(lockFile))        return false;    if (fs.getFileStatus(lockFile).isDirectory())        throw new IOException("lock file " + lockFile + " exists but is a directory!");    return fs.delete(lockFile, false);}
0
public static boolean removeLockFile(Configuration conf, Path lockFile) throws IOException
{    FileSystem fs = lockFile.getFileSystem(conf);    return removeLockFile(fs, lockFile);}
0
public static String cleanMimeType(String origType)
{    if (origType == null)        return null;        String[] tokenizedMimeType = origType.split(SEPARATOR);    if (tokenizedMimeType.length > 1) {                return tokenizedMimeType[0];    } else {                return origType;    }}
0
public String autoResolveContentType(String typeName, String url, byte[] data)
{    String retType = null;    MimeType type = null;    String cleanedMimeType = null;    cleanedMimeType = MimeUtil.cleanMimeType(typeName);        if (cleanedMimeType != null) {        try {            type = mimeTypes.forName(cleanedMimeType);            cleanedMimeType = type.getName();        } catch (MimeTypeException mte) {                        cleanedMimeType = null;        }    }        if (type == null || type.getName().equals(MimeTypes.OCTET_STREAM)) {                try {            retType = tika.detect(url) != null ? tika.detect(url) : null;        } catch (Exception e) {            String message = "Problem loading default Tika configuration";                        throw new RuntimeException(e);        }    } else {        retType = type.getName();    }        if (this.mimeMagic) {        String magicType = null;                Metadata tikaMeta = new Metadata();        tikaMeta.add(Metadata.RESOURCE_NAME_KEY, url);        tikaMeta.add(Metadata.CONTENT_TYPE, (cleanedMimeType != null ? cleanedMimeType : typeName));        try {            try (InputStream stream = TikaInputStream.get(data)) {                magicType = mimeTypes.detect(stream, tikaMeta).toString();            }        } catch (IOException ignore) {        }        if (magicType != null && !magicType.equals(MimeTypes.OCTET_STREAM) && retType != null && !retType.equals(magicType)) {                                    retType = magicType;        }                if (retType == null) {            try {                retType = MimeTypes.OCTET_STREAM;            } catch (Exception ignore) {            }        }    }    return retType;}
1
public String getMimeType(String url)
{    return tika.detect(url);}
0
public String forName(String name)
{    try {        return this.mimeTypes.forName(name).toString();    } catch (MimeTypeException e) {                return null;    }}
1
public String getMimeType(File f)
{    try {        return tika.detect(f);    } catch (Exception e) {                return null;    }}
1
public Node nextNode()
{        if (!hasNext()) {        return null;    }            currentNode = nodes.pop();    currentChildren = currentNode.getChildNodes();    int childLen = (currentChildren != null) ? currentChildren.getLength() : 0;        for (int i = childLen - 1; i >= 0; i--) {        nodes.add(currentChildren.item(i));    }    return currentNode;}
0
public void skipChildren()
{    int childLen = (currentChildren != null) ? currentChildren.getLength() : 0;    for (int i = 0; i < childLen; i++) {        Node child = nodes.peek();        if (child.equals(currentChildren.item(i))) {            nodes.pop();        }    }}
0
public Node getCurrentNode()
{    return currentNode;}
0
public boolean hasNext()
{    return (nodes.size() > 0);}
0
private static void setUUID(Configuration conf)
{    UUID uuid = UUID.randomUUID();    conf.set(UUID_KEY, uuid.toString());}
0
public static String getUUID(Configuration conf)
{    return conf.get(UUID_KEY);}
0
public static Configuration create()
{    Configuration conf = new Configuration();    setUUID(conf);    addNutchResources(conf);    return conf;}
0
public static Configuration create(boolean addNutchResources, Properties nutchProperties)
{    Configuration conf = new Configuration();    setUUID(conf);    if (addNutchResources) {        addNutchResources(conf);    }    for (Entry<Object, Object> e : nutchProperties.entrySet()) {        conf.set(e.getKey().toString(), e.getValue().toString());    }    return conf;}
0
private static Configuration addNutchResources(Configuration conf)
{    conf.addResource("nutch-default.xml");    conf.addResource("nutch-site.xml");    return conf;}
0
public static Job getInstance(Configuration conf) throws IOException
{    return Job.getInstance(conf);}
0
public static void cleanupAfterFailure(Path tempDir, Path lock, FileSystem fs) throws IOException
{    try {        if (fs.exists(tempDir)) {            fs.delete(tempDir, true);        }        LockUtil.removeLockFile(fs, lock);    } catch (IOException e) {                throw e;    }}
1
public float getProgress()
{    float res = 0;    if (currentJob != null) {        try {            res = (currentJob.mapProgress() + currentJob.reduceProgress()) / 2.0f;        } catch (IOException e) {            e.printStackTrace();            res = 0;        } catch (IllegalStateException ile) {            ile.printStackTrace();            res = 0;        }    }        if (numJobs > 1) {        res = (currentJobNum + res) / (float) numJobs;    }    status.put(Nutch.STAT_PROGRESS, res);    return res;}
0
public Map<String, Object> getStatus()
{    return status;}
0
public boolean stopJob() throws Exception
{    return killJob();}
0
public boolean killJob() throws Exception
{    if (currentJob != null && !currentJob.isComplete()) {        try {            currentJob.killJob();            return true;        } catch (Exception e) {            e.printStackTrace();            return false;        }    }    return false;}
0
public static synchronized ObjectCache get(Configuration conf)
{    ObjectCache objectCache = CACHE.get(conf);    if (objectCache == null) {                objectCache = new ObjectCache();        CACHE.put(conf, objectCache);    }    return objectCache;}
1
public synchronized Object getObject(String key)
{    return objectMap.get(key);}
0
public boolean hasObject(String key)
{    return objectMap.containsKey(key);}
0
public synchronized void setObject(String key, Object value)
{    objectMap.put(key, value);}
0
public boolean matches(String input)
{    TrieNode node = root;    for (int i = 0; i < input.length(); i++) {        node = node.getChild(input.charAt(i));        if (node == null)            return false;        if (node.isTerminal())            return true;    }    return false;}
0
public String shortestMatch(String input)
{    TrieNode node = root;    for (int i = 0; i < input.length(); i++) {        node = node.getChild(input.charAt(i));        if (node == null)            return null;        if (node.isTerminal())            return input.substring(0, i + 1);    }    return null;}
0
public String longestMatch(String input)
{    TrieNode node = root;    String result = null;    for (int i = 0; i < input.length(); i++) {        node = node.getChild(input.charAt(i));        if (node == null)            break;        if (node.isTerminal())            result = input.substring(0, i + 1);    }    return result;}
0
public static final void main(String[] argv)
{    String[] prefixes = new String[] { "abcd", "abc", "aac", "baz", "foo", "foobar" };    PrefixStringMatcher matcher = new PrefixStringMatcher(prefixes);    String[] tests = { "a", "ab", "abc", "abcdefg", "apple", "aa", "aac", "aaccca", "abaz", "baz", "bazooka", "fo", "foobar", "kite" };    for (int i = 0; i < tests.length; i++) {        System.out.println("testing: " + tests[i]);        System.out.println("   matches: " + matcher.matches(tests[i]));        System.out.println("  shortest: " + matcher.shortestMatch(tests[i]));        System.out.println("   longest: " + matcher.longestMatch(tests[i]));    }    int iterations = 1000;    System.out.println("Testing thread-safety (NUTCH-2585) with " + iterations + " iterations:");    List<String> testsList = Arrays.asList(tests);    for (int i = 0; i < iterations; i++) {        matcher = new PrefixStringMatcher(prefixes);        Collections.shuffle(testsList);        try {            long count = testsList.parallelStream().filter(matcher::matches).count();            System.out.print(String.format("Cycle %4d : %d matches\r", i, count));        } catch (Exception e) {                        System.out.println("");            throw e;        }    }    System.out.println("");}
0
public int run(String[] args) throws Exception
{    if (args.length < 2) {        System.err.println("Usage: ProtocolStatistics inputDirs outDir [numOfReducer]");        System.err.println("\tinputDirs\tComma separated list of crawldb input directories");        System.err.println("\t\t\tE.g.: crawl/crawldb/");        System.err.println("\toutDir\t\tOutput directory where results should be dumped");        System.err.println("\t[numOfReducers]\tOptional number of reduce jobs to use. Defaults to 1.");        return 1;    }    String inputDir = args[0];    String outputDir = args[1];    int numOfReducers = 1;    if (args.length > 3) {        numOfReducers = Integer.parseInt(args[3]);    }    SimpleDateFormat sdf = new SimpleDateFormat("yyyy-MM-dd HH:mm:ss");    long start = System.currentTimeMillis();        String jobName = "ProtocolStatistics";    Configuration conf = getConf();    conf.setBoolean("mapreduce.fileoutputcommitter.marksuccessfuljobs", false);    Job job = Job.getInstance(conf, jobName);    job.setJarByClass(ProtocolStatusStatistics.class);    String[] inputDirsSpecs = inputDir.split(",");    for (int i = 0; i < inputDirsSpecs.length; i++) {        File completeInputPath = new File(new File(inputDirsSpecs[i]), "current");        FileInputFormat.addInputPath(job, new Path(completeInputPath.toString()));    }    job.setInputFormatClass(SequenceFileInputFormat.class);    FileOutputFormat.setOutputPath(job, new Path(outputDir));    job.setOutputFormatClass(TextOutputFormat.class);    job.setMapOutputKeyClass(Text.class);    job.setMapOutputValueClass(LongWritable.class);    job.setOutputKeyClass(Text.class);    job.setOutputValueClass(LongWritable.class);    job.setMapperClass(ProtocolStatusStatisticsMapper.class);    job.setReducerClass(ProtocolStatusStatisticsReducer.class);    job.setCombinerClass(ProtocolStatusStatisticsCombiner.class);    job.setNumReduceTasks(numOfReducers);    try {        boolean success = job.waitForCompletion(true);        if (!success) {            String message = jobName + " job did not succeed, job status: " + job.getStatus().getState() + ", reason: " + job.getStatus().getFailureInfo();                                    throw new RuntimeException(message);        }    } catch (IOException | InterruptedException | ClassNotFoundException e) {                throw e;    }    long end = System.currentTimeMillis();        return 0;}
1
public void map(Text urlText, CrawlDatum datum, Context context) throws IOException, InterruptedException
{    if (datum.getMetaData().containsKey(Nutch.PROTOCOL_STATUS_CODE_KEY)) {        context.write((Text) datum.getMetaData().get(Nutch.PROTOCOL_STATUS_CODE_KEY), new LongWritable(1));    } else {        context.write(UNFETCHED_TEXT, new LongWritable(1));    }}
0
public void reduce(Text key, Iterable<LongWritable> values, Context context) throws IOException, InterruptedException
{    long total = 0;    for (LongWritable val : values) {        total += val.get();    }    context.write(new LongWritable(total), key);}
0
public void reduce(Text key, Iterable<LongWritable> values, Context context) throws IOException, InterruptedException
{    long total = 0;    for (LongWritable val : values) {        total += val.get();    }    context.write(key, new LongWritable(total));}
0
public static void main(String[] args) throws Exception
{    ToolRunner.run(NutchConfiguration.create(), new ProtocolStatusStatistics(), args);}
0
public static SequenceFile.Reader[] getReaders(Path dir, Configuration conf) throws IOException
{    FileSystem fs = dir.getFileSystem(conf);    Path[] names = FileUtil.stat2Paths(fs.listStatus(dir));    Arrays.sort(names);    SequenceFile.Reader[] parts = new SequenceFile.Reader[names.length];    for (int i = 0; i < names.length; i++) {        parts[i] = new SequenceFile.Reader(conf, SequenceFile.Reader.file(names[i]));    }    return parts;}
0
public void setup(Context context)
{    Configuration conf = context.getConfiguration();    this.protocolFactory = new ProtocolFactory(conf);    this.filter = conf.getBoolean(SITEMAP_URL_FILTERING, true);    this.normalize = conf.getBoolean(SITEMAP_URL_NORMALIZING, true);    this.strict = conf.getBoolean(SITEMAP_STRICT_PARSING, true);    this.tryDefaultSitemapXml = conf.getBoolean(SITEMAP_ALWAYS_TRY_SITEMAPXML_ON_ROOT, true);    this.maxRedir = conf.getInt(SITEMAP_REDIR_MAX, 3);    this.parser = new SiteMapParser(strict);    if (filter) {        filters = new URLFilters(conf);    }    if (normalize) {        normalizers = new URLNormalizers(conf, URLNormalizers.SCOPE_DEFAULT);    }}
0
public void map(Text key, Writable value, Context context) throws IOException, InterruptedException
{    String url;    try {        if (value instanceof CrawlDatum) {                        context.write(key, (CrawlDatum) value);        } else if (value instanceof HostDatum) {            generateSitemapsFromHostname(key.toString(), context);        } else if (value instanceof Text) {                        url = key.toString();            if (url.startsWith("http://") || url.startsWith("https://") || url.startsWith("ftp://") || url.startsWith("file:/")) {                                if ((url = filterNormalize(url)) == null) {                    context.getCounter("Sitemap", "filtered_records").increment(1);                    return;                }                context.getCounter("Sitemap", "sitemap_seeds").increment(1);                generateSitemapUrlDatum(protocolFactory.getProtocol(url), url, context);            } else {                                generateSitemapsFromHostname(key.toString(), context);            }        }    } catch (Exception e) {            }}
1
private String filterNormalize(String url)
{    try {        if (normalizers != null)            url = normalizers.normalize(url, URLNormalizers.SCOPE_DEFAULT);        if (filters != null)            url = filters.filter(url);    } catch (Exception e) {        return null;    }    return url;}
0
private void generateSitemapsFromHostname(String host, Context context)
{    try {                                String url;        if ((url = filterNormalize("http://" + host + "/")) == null && (url = filterNormalize("https://" + host + "/")) == null && (url = filterNormalize("ftp://" + host + "/")) == null && (url = filterNormalize("file:/" + host + "/")) == null) {            context.getCounter("Sitemap", "filtered_records").increment(1);            return;        }                BaseRobotRules rules = protocolFactory.getProtocol(url).getRobotRules(new Text(url), datum, null);        List<String> sitemaps = rules.getSitemaps();        if (tryDefaultSitemapXml && sitemaps.size() == 0) {            sitemaps.add(url + "sitemap.xml");        }        for (String sitemap : sitemaps) {            context.getCounter("Sitemap", "sitemaps_from_hostname").increment(1);            sitemap = filterNormalize(sitemap);            if (sitemap == null) {                context.getCounter("Sitemap", "filtered_sitemaps_from_hostname").increment(1);            } else {                generateSitemapUrlDatum(protocolFactory.getProtocol(sitemap), sitemap, context);            }        }    } catch (Exception e) {            }}
1
private void generateSitemapUrlDatum(Protocol protocol, String url, Context context) throws Exception
{    ProtocolOutput output = protocol.getProtocolOutput(new Text(url), datum);    ProtocolStatus status = output.getStatus();    Content content = output.getContent();        int maxRedir = this.maxRedir;    while (!output.getStatus().isSuccess() && output.getStatus().isRedirect() && maxRedir > 0) {        String[] stuff = output.getStatus().getArgs();        url = filterNormalize(stuff[0]);                if (url == null) {            break;        }        output = protocol.getProtocolOutput(new Text(url), datum);        status = output.getStatus();        content = output.getContent();        maxRedir--;    }    if (status.getCode() != ProtocolStatus.SUCCESS) {                        context.getCounter("Sitemap", "failed_fetches").increment(1);                return;    }    AbstractSiteMap asm = parser.parseSiteMap(content.getContentType(), content.getContent(), new URL(url));    if (asm instanceof SiteMap) {                SiteMap sm = (SiteMap) asm;        Collection<SiteMapURL> sitemapUrls = sm.getSiteMapUrls();        for (SiteMapURL sitemapUrl : sitemapUrls) {                        if (!strict || sitemapUrl.isValid()) {                String key = filterNormalize(sitemapUrl.getUrl().toString());                if (key != null) {                    CrawlDatum sitemapUrlDatum = new CrawlDatum();                    sitemapUrlDatum.setStatus(CrawlDatum.STATUS_INJECTED);                    sitemapUrlDatum.setScore((float) sitemapUrl.getPriority());                    if (sitemapUrl.getChangeFrequency() != null) {                        int fetchInterval = -1;                        switch(sitemapUrl.getChangeFrequency()) {                            case ALWAYS:                                fetchInterval = 1;                                break;                                                        case HOURLY:                                fetchInterval = 3600;                                break;                                                        case DAILY:                                fetchInterval = 86400;                                break;                                                        case WEEKLY:                                fetchInterval = 604800;                                break;                                                        case MONTHLY:                                fetchInterval = 2592000;                                break;                                                        case YEARLY:                                fetchInterval = 31536000;                                break;                                                        case NEVER:                                fetchInterval = Integer.MAX_VALUE;                                break;                        }                        sitemapUrlDatum.setFetchInterval(fetchInterval);                    }                    if (sitemapUrl.getLastModified() != null) {                        sitemapUrlDatum.setModifiedTime(sitemapUrl.getLastModified().getTime());                    }                    context.write(new Text(key), sitemapUrlDatum);                }            }        }    } else if (asm instanceof SiteMapIndex) {        SiteMapIndex index = (SiteMapIndex) asm;        Collection<AbstractSiteMap> sitemapUrls = index.getSitemaps();        if (sitemapUrls.isEmpty()) {            return;        }                for (AbstractSiteMap sitemap : sitemapUrls) {            String sitemapUrl = filterNormalize(sitemap.getUrl().toString());            if (sitemapUrl != null) {                generateSitemapUrlDatum(protocol, sitemapUrl, context);            }        }    }}
1
public void setup(Context context)
{    Configuration conf = context.getConfiguration();    this.overwriteExisting = conf.getBoolean(SITEMAP_OVERWRITE_EXISTING, false);}
0
public void reduce(Text key, Iterable<CrawlDatum> values, Context context) throws IOException, InterruptedException
{    sitemapDatum = null;    originalDatum = null;    for (CrawlDatum curr : values) {        if (curr.getStatus() == CrawlDatum.STATUS_INJECTED) {            sitemapDatum = new CrawlDatum();            sitemapDatum.set(curr);        } else {            originalDatum = new CrawlDatum();            originalDatum.set(curr);        }    }    if (originalDatum != null) {                if (sitemapDatum != null && overwriteExisting) {            originalDatum.setScore(sitemapDatum.getScore());            originalDatum.setFetchInterval(sitemapDatum.getFetchInterval());            originalDatum.setModifiedTime(sitemapDatum.getModifiedTime());        }        context.getCounter("Sitemap", "existing_sitemap_entries").increment(1);        context.write(key, originalDatum);    } else if (sitemapDatum != null) {                context.getCounter("Sitemap", "new_sitemap_entries").increment(1);        sitemapDatum.setStatus(CrawlDatum.STATUS_DB_UNFETCHED);        context.write(key, sitemapDatum);    }}
0
public void sitemap(Path crawldb, Path hostdb, Path sitemapUrlDir, boolean strict, boolean filter, boolean normalize, int threads) throws Exception
{    long start = System.currentTimeMillis();    if (LOG.isInfoEnabled()) {            }    FileSystem fs = crawldb.getFileSystem(getConf());    Path old = new Path(crawldb, "old");    Path current = new Path(crawldb, "current");    Path tempCrawlDb = new Path(crawldb, "crawldb-" + Integer.toString(new Random().nextInt(Integer.MAX_VALUE)));        Path lock = new Path(crawldb, LOCK_NAME);    if (!fs.exists(current))        fs.mkdirs(current);    LockUtil.createLockFile(fs, lock, false);    Configuration conf = getConf();    conf.setBoolean(SITEMAP_STRICT_PARSING, strict);    conf.setBoolean(SITEMAP_URL_FILTERING, filter);    conf.setBoolean(SITEMAP_URL_NORMALIZING, normalize);    conf.setBoolean("mapreduce.fileoutputcommitter.marksuccessfuljobs", false);    Job job = Job.getInstance(conf, "SitemapProcessor_" + crawldb.toString());    job.setJarByClass(SitemapProcessor.class);        MultipleInputs.addInputPath(job, current, SequenceFileInputFormat.class);    if (sitemapUrlDir != null)        MultipleInputs.addInputPath(job, sitemapUrlDir, KeyValueTextInputFormat.class);    if (hostdb != null)        MultipleInputs.addInputPath(job, new Path(hostdb, CURRENT_NAME), SequenceFileInputFormat.class);    FileOutputFormat.setOutputPath(job, tempCrawlDb);    job.setOutputFormatClass(MapFileOutputFormat.class);    job.setOutputKeyClass(Text.class);    job.setOutputValueClass(CrawlDatum.class);    job.setMapperClass(MultithreadedMapper.class);    MultithreadedMapper.setMapperClass(job, SitemapMapper.class);    MultithreadedMapper.setNumberOfThreads(job, threads);    job.setReducerClass(SitemapReducer.class);    try {        boolean success = job.waitForCompletion(true);        if (!success) {            String message = "SitemapProcessor_" + crawldb.toString() + " job did not succeed, job status: " + job.getStatus().getState() + ", reason: " + job.getStatus().getFailureInfo();                        NutchJob.cleanupAfterFailure(tempCrawlDb, lock, fs);                        throw new RuntimeException(message);        }        boolean preserveBackup = conf.getBoolean("db.preserve.backup", true);        if (!preserveBackup && fs.exists(old))            fs.delete(old, true);        else            FSUtils.replace(fs, old, current, true);        FSUtils.replace(fs, current, tempCrawlDb, true);        LockUtil.removeLockFile(fs, lock);        if (LOG.isInfoEnabled()) {            long filteredRecords = job.getCounters().findCounter("Sitemap", "filtered_records").getValue();            long fromHostname = job.getCounters().findCounter("Sitemap", "sitemaps_from_hostname").getValue();            long fromSeeds = job.getCounters().findCounter("Sitemap", "sitemap_seeds").getValue();            long failedFetches = job.getCounters().findCounter("Sitemap", "failed_fetches").getValue();            long newSitemapEntries = job.getCounters().findCounter("Sitemap", "new_sitemap_entries").getValue();                                                                        long end = System.currentTimeMillis();                    }    } catch (IOException | InterruptedException | ClassNotFoundException e) {                NutchJob.cleanupAfterFailure(tempCrawlDb, lock, fs);        throw e;    }}
1
public static void main(String[] args) throws Exception
{    int res = ToolRunner.run(NutchConfiguration.create(), new SitemapProcessor(), args);    System.exit(res);}
0
public static void usage()
{    System.err.println("Usage:\n SitemapProcessor <crawldb> [-hostdb <hostdb>] [-sitemapUrls <url_dir>] " + "[-threads <threads>] [-force] [-noStrict] [-noFilter] [-noNormalize]\n");    System.err.println("\t<crawldb>\t\tpath to crawldb where the sitemap urls would be injected");    System.err.println("\t-hostdb <hostdb>\tpath of a hostdb. Sitemap(s) from these hosts would be downloaded");    System.err.println("\t-sitemapUrls <url_dir>\tpath to directory with sitemap urls or hostnames");    System.err.println("\t-threads <threads>\tNumber of threads created per mapper to fetch sitemap urls (default: 8)");    System.err.println("\t-force\t\t\tforce update even if CrawlDb appears to be locked (CAUTION advised)");    System.err.println("\t-noStrict\t\tBy default Sitemap parser rejects invalid urls. '-noStrict' disables that.");    System.err.println("\t-noFilter\t\tturn off URLFilters on urls (optional)");    System.err.println("\t-noNormalize\t\tturn off URLNormalizer on urls (optional)");}
0
public int run(String[] args) throws Exception
{    if (args.length < 3) {        usage();        return -1;    }    Path crawlDb = new Path(args[0]);    Path hostDb = null;    Path urlDir = null;    boolean strict = true;    boolean filter = true;    boolean normalize = true;    int threads = 8;    for (int i = 1; i < args.length; i++) {        if (args[i].equals("-hostdb")) {            hostDb = new Path(args[++i]);                    } else if (args[i].equals("-sitemapUrls")) {            urlDir = new Path(args[++i]);                    } else if (args[i].equals("-threads")) {            threads = Integer.valueOf(args[++i]);                    } else if (args[i].equals("-noStrict")) {                        strict = false;        } else if (args[i].equals("-noFilter")) {                        filter = false;        } else if (args[i].equals("-noNormalize")) {                        normalize = false;        } else {                        usage();            return -1;        }    }    try {        sitemap(crawlDb, hostDb, urlDir, strict, filter, normalize, threads);        return 0;    } catch (Exception e) {                return -1;    }}
1
public static String rightPad(String s, int length)
{    StringBuffer sb = new StringBuffer(s);    for (int i = length - s.length(); i > 0; i--) sb.append(" ");    return sb.toString();}
0
public static String leftPad(String s, int length)
{    StringBuffer sb = new StringBuffer();    for (int i = length - s.length(); i > 0; i--) sb.append(" ");    sb.append(s);    return sb.toString();}
0
public static String toHexString(byte[] buf)
{    return toHexString(buf, null, Integer.MAX_VALUE);}
0
public static String toHexString(byte[] buf, String sep, int lineLen)
{    if (buf == null)        return null;    if (lineLen <= 0)        lineLen = Integer.MAX_VALUE;    StringBuffer res = new StringBuffer(buf.length * 2);    for (int i = 0; i < buf.length; i++) {        int b = buf[i];        res.append(HEX_DIGITS[(b >> 4) & 0xf]);        res.append(HEX_DIGITS[b & 0xf]);        if (i > 0 && (i % lineLen) == 0)            res.append('\n');        else if (sep != null && i < lineLen - 1)            res.append(sep);    }    return res.toString();}
0
public static byte[] fromHexString(String text)
{    text = text.trim();    if (text.length() % 2 != 0)        text = "0" + text;    int resLen = text.length() / 2;    int loNibble, hiNibble;    byte[] res = new byte[resLen];    for (int i = 0; i < resLen; i++) {        int j = i << 1;        hiNibble = charToNibble(text.charAt(j));        loNibble = charToNibble(text.charAt(j + 1));        if (loNibble == -1 || hiNibble == -1)            return null;        res[i] = (byte) (hiNibble << 4 | loNibble);    }    return res;}
0
private static final int charToNibble(char c)
{    if (c >= '0' && c <= '9') {        return c - '0';    } else if (c >= 'a' && c <= 'f') {        return 0xa + (c - 'a');    } else if (c >= 'A' && c <= 'F') {        return 0xA + (c - 'A');    } else {        return -1;    }}
0
public static boolean isEmpty(String str)
{    return (str == null) || (str.equals(""));}
0
public static String cleanField(String value)
{    return value.replaceAll("�", "");}
0
public static void main(String[] args)
{    if (args.length != 1)        System.out.println("Usage: StringUtil <encoding name>");    else        System.out.println(args[0] + " is resolved to " + EncodingDetector.resolveEncodingAlias(args[0]));}
0
public boolean matches(String input)
{    TrieNode node = root;    for (int i = input.length() - 1; i >= 0; i--) {        node = node.getChild(input.charAt(i));        if (node == null)            return false;        if (node.isTerminal())            return true;    }    return false;}
0
public String shortestMatch(String input)
{    TrieNode node = root;    for (int i = input.length() - 1; i >= 0; i--) {        node = node.getChild(input.charAt(i));        if (node == null)            return null;        if (node.isTerminal())            return input.substring(i);    }    return null;}
0
public String longestMatch(String input)
{    TrieNode node = root;    String result = null;    for (int i = input.length() - 1; i >= 0; i--) {        node = node.getChild(input.charAt(i));        if (node == null)            break;        if (node.isTerminal())            result = input.substring(i);    }    return result;}
0
public static final void main(String[] argv)
{    SuffixStringMatcher matcher = new SuffixStringMatcher(new String[] { "a", "abcd", "bcd", "bcdefg", "defg", "aac", "baz", "foo", "foobar" });    String[] tests = { "a", "ac", "abcd", "abcdefg", "apple", "aa", "aac", "aaccca", "abaz", "baz", "bazooka", "fo", "foobar", "kite" };    for (int i = 0; i < tests.length; i++) {        System.out.println("testing: " + tests[i]);        System.out.println("   matches: " + matcher.matches(tests[i]));        System.out.println("  shortest: " + matcher.shortestMatch(tests[i]));        System.out.println("   longest: " + matcher.longestMatch(tests[i]));    }}
0
public static String reverseUrl(String urlString) throws MalformedURLException
{    return reverseUrl(new URL(urlString));}
0
public static String reverseUrl(URL url)
{    String host = url.getHost();    String file = url.getFile();    String protocol = url.getProtocol();    int port = url.getPort();    StringBuilder buf = new StringBuilder();    /* reverse host */    reverseAppendSplits(host, buf);    /* add protocol */    buf.append(':');    buf.append(protocol);    /* add port if necessary */    if (port != -1) {        buf.append(':');        buf.append(port);    }    /* add path */    if (file.length() > 0 && '/' != file.charAt(0)) {        buf.append('/');    }    buf.append(file);    return buf.toString();}
0
public static String unreverseUrl(String reversedUrl)
{    StringBuilder buf = new StringBuilder(reversedUrl.length() + 2);    int pathBegin = reversedUrl.indexOf('/');    if (pathBegin == -1)        pathBegin = reversedUrl.length();    String sub = reversedUrl.substring(0, pathBegin);        String[] splits = StringUtils.splitPreserveAllTokens(sub, ':');                    buf.append(splits[1]);    buf.append("://");        reverseAppendSplits(splits[0], buf);        if (splits.length == 3) {                buf.append(':');        buf.append(splits[2]);    }    buf.append(reversedUrl.substring(pathBegin));    return buf.toString();}
0
public static String getReversedHost(String reversedUrl)
{    return reversedUrl.substring(0, reversedUrl.indexOf(':'));}
0
private static void reverseAppendSplits(String string, StringBuilder buf)
{    String[] splits = StringUtils.split(string, '.');    if (splits.length > 0) {        for (int i = splits.length - 1; i > 0; i--) {            buf.append(splits[i]);            buf.append('.');        }        buf.append(splits[0]);    } else {        buf.append(string);    }}
0
public static String reverseHost(String hostName)
{    StringBuilder buf = new StringBuilder();    reverseAppendSplits(hostName, buf);    return buf.toString();}
0
public static String unreverseHost(String reversedHostName)
{        return reverseHost(reversedHostName);}
0
public static String toString(CharSequence utf8)
{    return (utf8 == null ? null : StringUtil.cleanField(utf8.toString()));}
0
public static String logDateMillis(long millis)
{    return logDateFormat.format(LocalDateTime.ofInstant(Instant.ofEpochMilli(millis), ZoneId.systemDefault()));}
0
public static String elapsedTime(long start, long end)
{    if (start > end) {        return null;    }    return secondsToHMS((end - start) / 1000);}
0
public static String secondsToHMS(long seconds)
{    long hours = TimeUnit.SECONDS.toHours(seconds);    long minutes = TimeUnit.SECONDS.toMinutes(seconds) % TimeUnit.HOURS.toMinutes(1);    seconds = TimeUnit.SECONDS.toSeconds(seconds) % TimeUnit.MINUTES.toSeconds(1);    return String.format("%02d:%02d:%02d", hours, minutes, seconds);}
0
public static String secondsToDaysHMS(long seconds)
{    long days = TimeUnit.SECONDS.toDays(seconds);    if (days == 0)        return secondsToHMS(seconds);    String hhmmss = secondsToHMS(seconds % TimeUnit.DAYS.toSeconds(1));    return String.format("%d days, %s", days, hhmmss);}
0
 boolean isTerminal()
{    return terminal;}
0
 TrieNode getChildAddIfNotPresent(char nextChar, boolean isTerminal)
{    if (childrenList == null) {        childrenList = new LinkedList<>();        childrenList.addAll(Arrays.asList(children));        children = null;    }    if (childrenList.size() == 0) {        TrieNode newNode = new TrieNode(nextChar, isTerminal);        childrenList.add(newNode);        return newNode;    }    ListIterator<TrieNode> iter = childrenList.listIterator();    TrieNode node = iter.next();    while ((node.nodeChar < nextChar) && iter.hasNext()) node = iter.next();    if (node.nodeChar == nextChar) {        node.terminal = node.terminal | isTerminal;        return node;    }    if (node.nodeChar > nextChar)        iter.previous();    TrieNode newNode = new TrieNode(nextChar, isTerminal);    iter.add(newNode);    return newNode;}
0
 TrieNode getChild(char nextChar)
{    if (children == null) {        compile();    }    int min = 0;    int max = children.length - 1;    int mid = 0;    while (min < max) {        mid = (min + max) / 2;        if (children[mid].nodeChar == nextChar)            return children[mid];        if (children[mid].nodeChar < nextChar)            min = mid + 1;        else                        max = mid - 1;    }    if (min == max)        if (children[min].nodeChar == nextChar)            return children[min];    return null;}
0
public int compareTo(TrieNode other)
{    if (this.nodeChar < other.nodeChar)        return -1;    if (this.nodeChar == other.nodeChar)        return 0;        return 1;}
0
 synchronized void compile()
{    if (childrenList != null) {        children = childrenList.toArray(new TrieNode[childrenList.size()]);        childrenList = null;        Arrays.sort(children);    }}
0
protected final TrieNode matchChar(TrieNode node, String s, int idx)
{    return node.getChild(s.charAt(idx));}
0
protected final void addPatternForward(String s)
{    TrieNode node = root;    int stop = s.length() - 1;    int i;    if (s.length() > 0) {        for (i = 0; i < stop; i++) node = node.getChildAddIfNotPresent(s.charAt(i), false);        node = node.getChildAddIfNotPresent(s.charAt(i), true);    }}
0
protected final void addPatternBackward(String s)
{    TrieNode node = root;    if (s.length() > 0) {        for (int i = s.length() - 1; i > 0; i--) node = node.getChildAddIfNotPresent(s.charAt(i), false);        node = node.getChildAddIfNotPresent(s.charAt(0), true);    }}
0
public static URL resolveURL(URL base, String target) throws MalformedURLException
{    target = target.trim();        if (target.startsWith("?")) {        return fixPureQueryTargets(base, target);    }    return new URL(base, target);}
0
 static URL fixPureQueryTargets(URL base, String target) throws MalformedURLException
{    if (!target.startsWith("?"))        return new URL(base, target);    String basePath = base.getPath();    String baseRightMost = "";    int baseRightMostIdx = basePath.lastIndexOf("/");    if (baseRightMostIdx != -1) {        baseRightMost = basePath.substring(baseRightMostIdx + 1);    }    if (target.startsWith("?"))        target = baseRightMost + target;    return new URL(base, target);}
0
public static String getDomainName(URL url)
{    DomainSuffixes tlds = DomainSuffixes.getInstance();    String host = url.getHost();        if (host.endsWith("."))        host = host.substring(0, host.length() - 1);    if (IP_PATTERN.matcher(host).matches())        return host;    int index = 0;    String candidate = host;    for (; index >= 0; ) {        index = candidate.indexOf('.');        String subCandidate = candidate.substring(index + 1);        if (tlds.isDomainSuffix(subCandidate)) {            return candidate;        }        candidate = subCandidate;    }    return candidate;}
0
public static String getDomainName(String url) throws MalformedURLException
{    return getDomainName(new URL(url));}
0
public static String getTopLevelDomainName(URL url) throws MalformedURLException
{    String suffix = getDomainSuffix(url).toString();    int idx = suffix.lastIndexOf(".");    if (idx != -1) {        return suffix.substring(idx + 1);    } else {        return suffix;    }}
0
public static String getTopLevelDomainName(String url) throws MalformedURLException
{    return getTopLevelDomainName(new URL(url));}
0
public static boolean isSameDomainName(URL url1, URL url2)
{    return getDomainName(url1).equalsIgnoreCase(getDomainName(url2));}
0
public static boolean isSameDomainName(String url1, String url2) throws MalformedURLException
{    return isSameDomainName(new URL(url1), new URL(url2));}
0
public static DomainSuffix getDomainSuffix(URL url)
{    DomainSuffixes tlds = DomainSuffixes.getInstance();    String host = url.getHost();    if (IP_PATTERN.matcher(host).matches())        return null;    int index = 0;    String candidate = host;    for (; index >= 0; ) {        index = candidate.indexOf('.');        String subCandidate = candidate.substring(index + 1);        DomainSuffix d = tlds.get(subCandidate);        if (d != null) {            return d;        }        candidate = subCandidate;    }    return null;}
0
public static DomainSuffix getDomainSuffix(String url) throws MalformedURLException
{    return getDomainSuffix(new URL(url));}
0
public static String[] getHostSegments(URL url)
{    String host = url.getHost();        if (IP_PATTERN.matcher(host).matches())        return new String[] { host };    return host.split("\\.");}
0
public static String[] getHostSegments(String url) throws MalformedURLException
{    return getHostSegments(new URL(url));}
0
public static String chooseRepr(String src, String dst, boolean temp)
{        URL srcUrl;    URL dstUrl;    try {        srcUrl = new URL(src);        dstUrl = new URL(dst);    } catch (MalformedURLException e) {        return dst;    }        String srcDomain = URLUtil.getDomainName(srcUrl);    String dstDomain = URLUtil.getDomainName(dstUrl);    String srcHost = srcUrl.getHost();    String dstHost = dstUrl.getHost();    String srcFile = srcUrl.getFile();    String dstFile = dstUrl.getFile();        boolean srcRoot = (srcFile.equals("/") || srcFile.length() == 0);    boolean destRoot = (dstFile.equals("/") || dstFile.length() == 0);        if (!srcDomain.equals(dstDomain)) {        return dst;    }        if (!temp) {                if (srcRoot) {            return src;        } else {            return dst;        }    } else {                if (srcRoot && !destRoot) {            return src;        } else if (!srcRoot && destRoot) {                        return dst;        } else if (!srcRoot && !destRoot && (srcHost.equals(dstHost))) {                        int numSrcPaths = srcFile.split("/").length;            int numDstPaths = dstFile.split("/").length;            if (numSrcPaths != numDstPaths) {                return (numDstPaths < numSrcPaths ? dst : src);            } else {                int srcPathLength = srcFile.length();                int dstPathLength = dstFile.length();                return (dstPathLength < srcPathLength ? dst : src);            }        } else {                        int numSrcSubs = srcHost.split("\\.").length;            int numDstSubs = dstHost.split("\\.").length;            return (numDstSubs < numSrcSubs ? dst : src);        }    }}
0
public static String getHost(String url)
{    try {        return new URL(url).getHost().toLowerCase();    } catch (MalformedURLException e) {        return null;    }}
0
public static String getPage(String url)
{    try {                url = url.toLowerCase();        String queryStr = new URL(url).getQuery();        return (queryStr != null) ? url.replace("?" + queryStr, "") : url;    } catch (MalformedURLException e) {        return null;    }}
0
public static String getProtocol(String url)
{    try {        return getProtocol(new URL(url));    } catch (Exception e) {        return null;    }}
0
public static String getProtocol(URL url)
{    return url.getProtocol();}
0
public static String toASCII(String url)
{    try {        URL u = new URL(url);        String host = u.getHost();        if (host == null || host.isEmpty()) {                        return url;        }        URI p = new URI(u.getProtocol(), u.getUserInfo(), IDN.toASCII(host), u.getPort(), u.getPath(), u.getQuery(), u.getRef());        return p.toString();    } catch (Exception e) {        return null;    }}
0
public static String toUNICODE(String url)
{    try {        URL u = new URL(url);        String host = u.getHost();        if (host == null || host.isEmpty()) {                        return url;        }        StringBuilder sb = new StringBuilder();        sb.append(u.getProtocol());        sb.append("://");        if (u.getUserInfo() != null) {            sb.append(u.getUserInfo());            sb.append('@');        }        sb.append(IDN.toUnicode(host));        if (u.getPort() != -1) {            sb.append(':');            sb.append(u.getPort());        }                sb.append(u.getFile());        if (u.getRef() != null) {            sb.append('#');            sb.append(u.getRef());        }        return sb.toString();    } catch (Exception e) {        return null;    }}
0
public static void main(String[] args)
{    if (args.length != 1) {        System.err.println("Usage : URLUtil <url>");        return;    }    String url = args[0];    try {        System.out.println(URLUtil.getDomainName(new URL(url)));    } catch (MalformedURLException ex) {        ex.printStackTrace();    }}
0
public synchronized void executeCrawlCycle()
{    listener.crawlingStarted(crawl);    for (RemoteCommand command : remoteCommands) {        JobInfo jobInfo = executor.executeRemoteJob(command);        command.setJobInfo(jobInfo);                if (jobInfo.getState() == State.FAILED) {            listener.onCrawlError(crawl, jobInfo.getMsg());            return;        }        executedCommands.add(command);        listener.commandExecuted(crawl, command, calculateProgress());    }    listener.crawlingFinished(crawl);}
1
private int calculateProgress()
{    if (CollectionUtils.isEmpty(remoteCommands)) {        return 0;    }    return (int) ((float) executedCommands.size() / (float) remoteCommands.size() * 100);}
0
public void createClient()
{    ClientConfig clientConfig = new DefaultClientConfig();    clientConfig.getFeatures().put(JSONConfiguration.FEATURE_POJO_MAPPING, true);    this.client = Client.create(clientConfig);    this.nutchResource = client.resource(instance.getUrl());}
0
public NutchStatus getNutchStatus()
{    return nutchResource.path("/admin").type(APPLICATION_JSON).get(NutchStatus.class);}
0
public ConnectionStatus getConnectionStatus()
{    getNutchStatus();    return ConnectionStatus.CONNECTED;}
0
public String executeJob(JobConfig jobConfig)
{    JobInfo jobInfo = nutchResource.path("/job/create").type(APPLICATION_JSON).post(JobInfo.class, jobConfig);    return jobInfo.getId();}
0
public JobInfo getJobInfo(String jobId)
{    return nutchResource.path("/job/" + jobId).type(APPLICATION_JSON).get(JobInfo.class);}
0
public NutchInstance getNutchInstance()
{    return instance;}
0
public Map<String, String> getNutchConfig(String config)
{    return nutchResource.path("/config/" + config).type(APPLICATION_JSON).get(Map.class);}
0
public String createSeed(SeedList seedList)
{    return nutchResource.path("/seed/create").type(APPLICATION_JSON).post(String.class, seedList);}
0
public JobConfig getJobConfig()
{    return jobConfig;}
0
public void setJobConfig(JobConfig jobConfig)
{    this.jobConfig = jobConfig;}
0
public JobInfo getJobInfo()
{    return jobInfo;}
0
public void setJobInfo(JobInfo jobInfo)
{    this.jobInfo = jobInfo;}
0
public Duration getTimeout()
{    return timeout;}
0
public void setTimeout(Duration timeout)
{    this.timeout = timeout;}
0
public String toString()
{    String statusInfo = StringUtils.EMPTY;    if (jobInfo != null) {        statusInfo = MessageFormat.format("{0}", jobInfo.getState());    }    return MessageFormat.format("{0} status: {1}", jobConfig.getType(), statusInfo);}
0
public static RemoteCommandBuilder instance(JobType jobType)
{    return new RemoteCommandBuilder().withJobType(jobType);}
0
public RemoteCommandBuilder withJobType(JobType jobType)
{    jobConfig.setType(jobType);    return this;}
0
public RemoteCommandBuilder withConfigId(String configId)
{    jobConfig.setConfId(configId);    return this;}
0
public RemoteCommandBuilder withCrawlId(String crawlId)
{    jobConfig.setCrawlId(crawlId);    return this;}
0
public RemoteCommandBuilder withArgument(String key, String value)
{    jobConfig.setArgument(key, value);    return this;}
0
public RemoteCommandBuilder withTimeout(Duration timeout)
{    this.timeout = timeout;    return this;}
0
public RemoteCommand build()
{    RemoteCommand remoteCommand = new RemoteCommand(jobConfig);    remoteCommand.setTimeout(timeout);    return remoteCommand;}
0
public JobInfo executeRemoteJob(RemoteCommand command)
{    try {        String jobId = client.executeJob(command.getJobConfig());        Future<JobInfo> chekerFuture = executor.submit(new JobStateChecker(jobId));        return chekerFuture.get(getTimeout(command), TimeUnit.MILLISECONDS);    } catch (Exception e) {                JobInfo jobInfo = new JobInfo();        jobInfo.setState(State.FAILED);        jobInfo.setMsg(ExceptionUtils.getStackTrace(e));        return jobInfo;    }}
1
private long getTimeout(RemoteCommand command)
{    if (command.getTimeout() == null) {        return DEFAULT_TIMEOUT_SEC * DateTimeConstants.MILLIS_PER_SECOND;    }    return command.getTimeout().getMillis();}
0
public void setRequestDelay(Duration requestDelay)
{    this.requestDelay = requestDelay;}
0
public JobInfo call() throws Exception
{    while (!Thread.interrupted()) {        JobInfo jobInfo = client.getJobInfo(jobId);        checkState(jobInfo != null, "Cannot get job info!");        State state = jobInfo.getState();        checkState(state != null, "Unknown job state!");        if (state == State.RUNNING || state == State.ANY || state == State.IDLE) {            Thread.sleep(requestDelay.getMillis());            continue;        }        return jobInfo;    }    return null;}
0
public List<RemoteCommand> createCommands(Crawl crawl)
{    this.crawl = crawl;    this.remoteCommands = Lists.newArrayList();    remoteCommands.add(inject());    for (int i = 0; i < crawl.getNumberOfRounds(); i++) {        remoteCommands.addAll(createBatchCommands());    }    return remoteCommands;}
0
private List<RemoteCommand> createBatchCommands()
{    this.batchId = UUID.randomUUID().toString();    List<RemoteCommand> batchCommands = Lists.newArrayList();    batchCommands.add(createGenerateCommand());    batchCommands.add(createFetchCommand());    batchCommands.add(createParseCommand());    batchCommands.add(createUpdateDbCommand());    batchCommands.add(createIndexCommand());    return batchCommands;}
0
private RemoteCommand inject()
{    RemoteCommandBuilder builder = RemoteCommandBuilder.instance(JobType.INJECT).withCrawlId(crawl.getCrawlId()).withArgument("url_dir", crawl.getSeedDirectory());    return builder.build();}
0
private RemoteCommand createGenerateCommand()
{    return createBuilder(JobType.GENERATE).build();}
0
private RemoteCommand createFetchCommand()
{    return createBuilder(JobType.FETCH).withTimeout(Duration.standardSeconds(50)).build();}
0
private RemoteCommand createParseCommand()
{    return createBuilder(JobType.PARSE).build();}
0
private RemoteCommand createIndexCommand()
{    return createBuilder(JobType.INDEX).build();}
0
private RemoteCommand createUpdateDbCommand()
{    return createBuilder(JobType.UPDATEDB).build();}
0
private RemoteCommandBuilder createBuilder(JobType jobType)
{    return RemoteCommandBuilder.instance(jobType).withCrawlId(crawl.getCrawlId()).withArgument("batch", batchId);}
0
public Integer getNumberOfRounds()
{    return numberOfRounds;}
0
public void setNumberOfRounds(Integer numberOfRounds)
{    this.numberOfRounds = numberOfRounds;}
0
public String getCrawlId()
{    return crawlId;}
0
public void setCrawlId(String crawlId)
{    this.crawlId = crawlId;}
0
public CrawlStatus getStatus()
{    return status;}
0
public void setStatus(CrawlStatus status)
{    this.status = status;}
0
public String getCrawlName()
{    return crawlName;}
0
public void setCrawlName(String crawlName)
{    this.crawlName = crawlName;}
0
public SeedList getSeedList()
{    return seedList;}
0
public void setSeedList(SeedList seedList)
{    this.seedList = seedList;}
0
public Long getId()
{    return id;}
0
public void setId(Long id)
{    this.id = id;}
0
public String getSeedDirectory()
{    return seedDirectory;}
0
public void setSeedDirectory(String seedDirectory)
{    this.seedDirectory = seedDirectory;}
0
public int getProgress()
{    return progress;}
0
public void setProgress(int progress)
{    this.progress = progress;}
0
public void setArgument(String key, String value)
{    args.put(key, value);}
0
public String getCrawlId()
{    return crawlId;}
0
public void setCrawlId(String crawlId)
{    this.crawlId = crawlId;}
0
public JobType getType()
{    return type;}
0
public void setType(JobType type)
{    this.type = type;}
0
public String getConfId()
{    return confId;}
0
public void setConfId(String confId)
{    this.confId = confId;}
0
public Map<String, Object> getArgs()
{    return Collections.unmodifiableMap(args);}
0
public void setArgs(Map<String, Object> args)
{    this.args = args;}
0
public String getJobClassName()
{    return jobClassName;}
0
public void setJobClassName(String jobClass)
{    this.jobClassName = jobClass;}
0
public String getMsg()
{    return msg;}
0
public void setMsg(String msg)
{    this.msg = msg;}
0
public State getState()
{    return state;}
0
public void setState(State state)
{    this.state = state;}
0
public Map<String, Object> getResult()
{    return result;}
0
public void setResult(Map<String, Object> result)
{    this.result = result;}
0
public Map<String, Object> getArgs()
{    return args;}
0
public void setArgs(Map<String, Object> args)
{    this.args = args;}
0
public String getConfId()
{    return confId;}
0
public void setConfId(String confId)
{    this.confId = confId;}
0
public String getId()
{    return id;}
0
public void setId(String id)
{    this.id = id;}
0
public String getCrawlId()
{    return crawlId;}
0
public void setCrawlId(String crawlId)
{    this.crawlId = crawlId;}
0
public String getType()
{    return type;}
0
public void setType(String type)
{    this.type = type;}
0
public Date getStartDate()
{    return startDate;}
0
public void setStartDate(Date startDate)
{    this.startDate = startDate;}
0
public Set<String> getConfiguration()
{    return configuration;}
0
public void setConfiguration(Set<String> configuration)
{    this.configuration = configuration;}
0
public Collection<JobInfo> getJobs()
{    return jobs;}
0
public void setJobs(Collection<JobInfo> jobs)
{    this.jobs = jobs;}
0
public Collection<JobInfo> getRunningJobs()
{    return runningJobs;}
0
public void setRunningJobs(Collection<JobInfo> runningJobs)
{    this.runningJobs = runningJobs;}
0
public NutchClient getClient(NutchInstance instance)
{    try {        return cache.get(instance);    } catch (ExecutionException e) {        throw new IllegalStateException(e);    }}
0
public NutchClient load(NutchInstance key) throws Exception
{    return new NutchClientImpl(key);}
0
public Dao<T, ID> createDao(Class<T> clazz)
{    try {        Dao<T, ID> dao = DaoFactory.createDao(connectionSource, clazz);        register(dao);        return dao;    } catch (SQLException e) {        throw new RuntimeException(e);    }}
0
private void register(Dao<T, ID> dao)
{    synchronized (registredDaos) {        registredDaos.add(dao);    }}
0
public List<Dao<?, ?>> getCreatedDaos()
{    synchronized (registredDaos) {        return Collections.unmodifiableList(registredDaos);    }}
0
private void initialize()
{    if (configuredDaos == null) {        throw new IllegalStateException("configuredDaos was not set in " + getClass().getSimpleName());    }    for (Dao<?, ?> dao : configuredDaos) {        createTableForDao(dao);    }}
0
private void createTableForDao(Dao<?, ?> dao)
{    DatabaseTableConfig<?> tableConfig = getTableConfig(dao);    createTableIfNotExists(tableConfig);}
0
private DatabaseTableConfig<?> getTableConfig(Dao<?, ?> dao)
{    Class<?> clazz = dao.getDataClass();    DatabaseTableConfig<?> tableConfig = null;    if (dao instanceof BaseDaoImpl) {        tableConfig = ((BaseDaoImpl<?, ?>) dao).getTableConfig();    }    if (tableConfig == null) {        return getConfigFromClass(clazz);    }    return tableConfig;}
0
private DatabaseTableConfig<?> getConfigFromClass(Class<?> clazz)
{    try {        return DatabaseTableConfig.fromClass(connectionSource, clazz);    } catch (SQLException e) {        throw new RuntimeException(e);    }}
0
private void createTableIfNotExists(DatabaseTableConfig<?> tableConfig)
{    try {        TableUtils.createTableIfNotExists(connectionSource, tableConfig);    } catch (SQLException e) {        throw new RuntimeException(e);    }}
0
public List<NutchInstance> getInstances()
{    return instances;}
0
public void setInstances(List<NutchInstance> instances)
{    this.instances = instances;}
0
public Executor getAsyncExecutor()
{        ThreadPoolTaskExecutor executor = new ThreadPoolTaskExecutor();    executor.setCorePoolSize(7);    executor.setMaxPoolSize(42);    executor.setQueueCapacity(11);    executor.setThreadNamePrefix("SpringExecutor-");    executor.initialize();    return executor;}
0
public JdbcConnectionSource getConnectionSource() throws SQLException
{    JdbcConnectionSource source = new JdbcConnectionSource("jdbc:h2:~/.nutch/config", new H2DatabaseType());    source.initialize();    return source;}
0
public CustomDaoFactory getDaoFactory() throws SQLException
{    return new CustomDaoFactory(getConnectionSource());}
0
public Dao<NutchInstance, Long> createNutchDao() throws SQLException
{    return getDaoFactory().createDao(NutchInstance.class);}
0
public Dao<SeedList, Long> createSeedListDao() throws SQLException
{    return getDaoFactory().createDao(SeedList.class);}
0
public Dao<SeedUrl, Long> createSeedUrlDao() throws SQLException
{    return getDaoFactory().createDao(SeedUrl.class);}
0
public Dao<Crawl, Long> createCrawlDao() throws SQLException
{    return getDaoFactory().createDao(Crawl.class);}
0
public CustomTableCreator createTableCreator() throws SQLException
{    return new CustomTableCreator(getConnectionSource(), getDaoFactory().getCreatedDaos());}
0
public void setName(String name)
{    this.name = name;}
0
public String getName()
{    return this.name;}
0
public String getValue()
{    return value;}
0
public void setValue(String value)
{    this.value = value;}
0
public String getName()
{    return name;}
0
public void setName(String name)
{    this.name = name;}
0
public String getHost()
{    return host;}
0
public void setUsername(String username)
{    this.username = username;}
0
public String getUsername()
{    return username;}
0
public void setHost(String host)
{    this.host = host;}
0
public Integer getPort()
{    return port;}
0
public void setPort(Integer port)
{    this.port = port;}
0
public ConnectionStatus getConnectionStatus()
{    return connectionStatus;}
0
public void setConnectionStatus(ConnectionStatus connectionStatus)
{    this.connectionStatus = connectionStatus;}
0
public URI getUrl()
{    try {        return new URI("http", null, host, port, null, null, null);    } catch (URISyntaxException e) {        throw new IllegalStateException("Cannot parse url parameters", e);    }}
0
public String getPassword()
{    return password;}
0
public void setPassword(String password)
{    this.password = password;}
0
public Long getId()
{    return id;}
0
public void setId(Long id)
{    this.id = id;}
0
public Long getId()
{    return id;}
0
public void setId(Long id)
{    this.id = id;}
0
public int getSeedUrlsCount()
{    if (CollectionUtils.isEmpty(seedUrls)) {        return 0;    }    return seedUrls.size();}
0
public Collection<SeedUrl> getSeedUrls()
{    return seedUrls;}
0
public void setSeedUrls(Collection<SeedUrl> seedUrls)
{    this.seedUrls = seedUrls;}
0
public String getName()
{    return name;}
0
public void setName(String name)
{    this.name = name;}
0
public int hashCode()
{    final int prime = 31;    int result = 1;    result = prime * result + ((id == null) ? 0 : id.hashCode());    return result;}
0
public boolean equals(Object obj)
{    if (this == obj)        return true;    if (obj == null)        return false;    if (getClass() != obj.getClass())        return false;    SeedList other = (SeedList) obj;    if (id == null) {        if (other.id != null)            return false;    } else if (!id.equals(other.id))        return false;    return true;}
0
public Long getId()
{    return id;}
0
public void setId(Long id)
{    this.id = id;}
0
public String getUrl()
{    return url;}
0
public void setUrl(String url)
{    this.url = url;}
0
public SeedList getSeedList()
{    return seedList;}
0
public void setSeedList(SeedList seedList)
{    this.seedList = seedList;}
0
public int hashCode()
{    final int prime = 31;    int result = 1;    result = prime * result + ((id == null) ? 0 : id.hashCode());    return result;}
0
public boolean equals(Object obj)
{    if (this == obj)        return true;    if (obj == null)        return false;    if (getClass() != obj.getClass())        return false;    SeedUrl other = (SeedUrl) obj;    if (id == null) {        if (other.id != null)            return false;    } else if (!id.equals(other.id))        return false;    return true;}
0
public Class<? extends WebPage> getHomePage()
{    return DashboardPage.class;}
0
public void init()
{    super.init();    BootstrapSettings settings = new BootstrapSettings();    Bootstrap.install(this, settings);    configureTheme(settings);    getComponentInstantiationListeners().add(new SpringComponentInjector(this, context));}
0
private void configureTheme(BootstrapSettings settings)
{    Theme theme = new Theme(THEME_NAME, BootstrapCssReference.instance(), FontAwesomeCssReference.instance(), NutchUiCssReference.instance());    settings.setThemeProvider(new SingleThemeProvider(theme));}
0
public void setApplicationContext(ApplicationContext applicationContext) throws BeansException
{    this.context = applicationContext;}
0
public static void main(String[] args) throws Exception
{    CommandLineParser parser = new GnuParser();    Options options = createWebAppOptions();    CommandLine commandLine = null;    HelpFormatter formatter = new HelpFormatter();    try {        commandLine = parser.parse(options, args);    } catch (Exception e) {        formatter.printHelp("NutchUiServer", options, true);        StringUtils.stringifyException(e);    }    if (commandLine.hasOption("help")) {        formatter.printHelp("NutchUiServer", options, true);        return;    }    if (commandLine.hasOption(CMD_PORT)) {        port = Integer.parseInt(commandLine.getOptionValue(CMD_PORT));    }    startServer();}
0
private static void startServer() throws Exception, InterruptedException
{    Server server = new Server(port);    Context context = new Context(server, "/", Context.SESSIONS);    context.addServlet(DefaultServlet.class, "/*");    context.addEventListener(new ContextLoaderListener(getContext()));    context.addEventListener(new RequestContextListener());    WicketFilter filter = new WicketFilter();    filter.setFilterPath("/");    FilterHolder holder = new FilterHolder(filter);    holder.setInitParameter("applicationFactoryClassName", APP_FACTORY_NAME);    context.addFilter(holder, "/*", Handler.DEFAULT);    server.setHandler(context);    server.start();    server.join();}
0
private static WebApplicationContext getContext()
{    AnnotationConfigWebApplicationContext context = new AnnotationConfigWebApplicationContext();    context.setConfigLocation(CONFIG_LOCATION);    return context;}
0
private static Options createWebAppOptions()
{    Options options = new Options();    Option helpOpt = new Option("h", "help", false, "show this help message");    OptionBuilder.withDescription("Port to run the WebApplication on.");    OptionBuilder.hasOptionalArg();    OptionBuilder.withArgName("port number");    options.addOption(OptionBuilder.create(CMD_PORT));    options.addOption(helpOpt);    return options;}
0
protected Component addUserMenu()
{    DropDownButton userMenu = new NavbarDropDownButton(Model.of("Username")) {        /**         */        private static final long serialVersionUID = 1L;        @Override        protected List<AbstractLink> newSubMenuButtons(final String buttonMarkupId) {            List<AbstractLink> subMenu = Lists.newArrayList();            subMenu.add(new MenuBookmarkablePageLink<Void>(UserSettingsPage.class, new ResourceModel("navbar.userMenu.settings")).setIconType(FontAwesomeIconType.gear));            subMenu.add(new MenuDivider());            subMenu.add(new MenuBookmarkablePageLink<Void>(LogOutPage.class, new ResourceModel("navbar.userMenu.logout")).setIconType(FontAwesomeIconType.power_off));            return subMenu;        }    }.setIconType(FontAwesomeIconType.user);    return userMenu;}
0
protected List<AbstractLink> newSubMenuButtons(final String buttonMarkupId)
{    List<AbstractLink> subMenu = Lists.newArrayList();    subMenu.add(new MenuBookmarkablePageLink<Void>(UserSettingsPage.class, new ResourceModel("navbar.userMenu.settings")).setIconType(FontAwesomeIconType.gear));    subMenu.add(new MenuDivider());    subMenu.add(new MenuBookmarkablePageLink<Void>(LogOutPage.class, new ResourceModel("navbar.userMenu.logout")).setIconType(FontAwesomeIconType.power_off));    return subMenu;}
0
protected Component addInstancesMenuMenu()
{    IModel<String> instanceName = PropertyModel.of(currentInstance, "name");    DropDownButton instancesMenu = new NavbarDropDownButton(instanceName) {        /**         */        private static final long serialVersionUID = 1L;        @Override        protected List<AbstractLink> newSubMenuButtons(String buttonMarkupId) {            List<NutchInstance> instances = instanceService.getInstances();            List<AbstractLink> subMenu = Lists.newArrayList();            for (NutchInstance instance : instances) {                subMenu.add(new Link<NutchInstance>(buttonMarkupId, Model.of(instance)) {                    /**                     */                    private static final long serialVersionUID = 1L;                    @Override                    public void onClick() {                        currentInstance.setObject(getModelObject());                        setResponsePage(DashboardPage.class);                    }                }.setBody(Model.of(instance.getName())));            }            return subMenu;        }    }.setIconType(FontAwesomeIconType.gears);    return instancesMenu;}
0
protected List<AbstractLink> newSubMenuButtons(String buttonMarkupId)
{    List<NutchInstance> instances = instanceService.getInstances();    List<AbstractLink> subMenu = Lists.newArrayList();    for (NutchInstance instance : instances) {        subMenu.add(new Link<NutchInstance>(buttonMarkupId, Model.of(instance)) {            /**             */            private static final long serialVersionUID = 1L;            @Override            public void onClick() {                currentInstance.setObject(getModelObject());                setResponsePage(DashboardPage.class);            }        }.setBody(Model.of(instance.getName())));    }    return subMenu;}
0
public void onClick()
{    currentInstance.setObject(getModelObject());    setResponsePage(DashboardPage.class);}
0
private void addMenuItem(Class<P> page, String label, IconType icon)
{    Component button = new NavbarButton<Void>(page, Model.of(getString(label))).setIconType(icon);    navbar.addComponents(NavbarComponents.transform(LEFT, button));}
0
protected NutchInstance getCurrentInstance()
{    return currentInstance.getObject();}
0
public void setObject(NutchInstance instance)
{    super.setObject(instance);    getSession().setAttribute("instanceId", instance.getId());}
0
protected NutchInstance load()
{    Long instanceId = (Long) getSession().getAttribute("instanceId");    if (instanceId == null) {        return getFirstInstance();    }    return instanceService.getInstance(instanceId);}
0
private NutchInstance getFirstInstance()
{    return Iterables.getFirst(instanceService.getInstances(), null);}
0
public static NutchUiCssReference instance()
{    return INSTANCE;}
0
protected void onInitialize()
{    super.onInitialize();    setOutputMarkupId(true);    add(new LabelBehavior(new EnumCssModel(getModel())));}
0
public LabelType getObject()
{    LabelType labelType = labelTypeMap.get(model.getObject());    if (labelType == null) {        return LabelType.Default;    }    return labelType;}
0
public static ColorEnumLabelBuilder<E> getBuilder(String id)
{    return new ColorEnumLabelBuilder<>(id);}
0
public ColorEnumLabelBuilder<E> withModel(IModel<E> model)
{    this.model = model;    return this;}
0
public ColorEnumLabelBuilder<E> withEnumColor(E e, LabelType type)
{    labelTypeMap.put(e, type);    return this;}
0
public ColorEnumLabel<E> build()
{    return new ColorEnumLabel<>(id, model, labelTypeMap);}
0
protected IModel<T> model(T object)
{    return new CompoundPropertyModel<>(object);}
0
protected void onSubmit(AjaxRequestTarget target, Form<?> ajaxForm)
{    crawlService.saveCrawl(form.getModelObject());    target.add(this.getPage());}
0
protected void onError(AjaxRequestTarget target, Form<?> form)
{    target.add(notificationPanel);}
0
public void setModel(IModel<Crawl> model)
{    form.setModel(model);}
0
private List<Integer> getNumbersOfRounds()
{    List<Integer> numbers = Lists.newArrayList();    for (int i = 1; i <= MAX_ROUNDS; i++) {        numbers.add(i);    }    return numbers;}
0
protected Iterator<IModel<Crawl>> getItemModels()
{    return new CpmIteratorAdapter<>(crawlService.getCrawls());}
0
protected void populateItem(Item<Crawl> item)
{    populateCrawlRow(item);}
0
public void onClick(AjaxRequestTarget target)
{    editCrawl(target, new CompoundPropertyModel<>(createNewCrawl()));}
0
private void populateCrawlRow(Item<Crawl> item)
{    item.add(new AjaxLink<Crawl>("edit", item.getModel()) {        @Override        public void onClick(AjaxRequestTarget target) {            editCrawl(target, getModel());        }    }.add(new Label("crawlName")));    item.add(new Label("seedList.name"));    item.add(new Label("progress"));    item.add(createStatusLabel());    item.add(new Link<Crawl>("start", item.getModel()) {        @Override        public void onClick() {            crawlService.startCrawl(getModelObject().getId(), getCurrentInstance());        }    });    item.add(new Link<Crawl>("delete", item.getModel()) {        @Override        public void onClick() {            crawlService.deleteCrawl(getModelObject().getId());        }    });}
0
public void onClick(AjaxRequestTarget target)
{    editCrawl(target, getModel());}
0
public void onClick()
{    crawlService.startCrawl(getModelObject().getId(), getCurrentInstance());}
0
public void onClick()
{    crawlService.deleteCrawl(getModelObject().getId());}
0
private void editCrawl(AjaxRequestTarget target, IModel<Crawl> model)
{    crawlPanel.setModel(model);    target.add(crawlPanel);    crawlPanel.appendShowDialogJavaScript(target);}
0
private Crawl createNewCrawl()
{    return new Crawl();}
0
private EnumLabel<CrawlStatus> createStatusLabel()
{    return new ColorEnumLabelBuilder<CrawlStatus>("status").withEnumColor(NEW, Default).withEnumColor(ERROR, Danger).withEnumColor(FINISHED, Success).withEnumColor(CRAWLING, Info).build();}
0
protected Integer load()
{    NutchInstance currentInstance = getCurrentInstance();    Long id = currentInstance.getId();    NutchStatus nutchStatus = nutchService.getNutchStatus(id);    return nutchStatus.getRunningJobs().size();}
0
protected void onSubmit(AjaxRequestTarget target, Form<?> ajaxForm)
{    instanceService.saveInstance(form.getModelObject());    target.add(this.getPage());}
0
protected void onError(AjaxRequestTarget target, Form<?> form)
{    target.add(notificationPanel);}
0
public void setModel(IModel<NutchInstance> model)
{    form.setModel(model);}
0
private RefreshingView<NutchInstance> refreshingView()
{    RefreshingView<NutchInstance> instances = new RefreshingView<NutchInstance>("instances") {        @Override        protected Iterator<IModel<NutchInstance>> getItemModels() {            return new CpmIteratorAdapter<>(instanceService.getInstances());        }        @Override        protected void populateItem(Item<NutchInstance> item) {            populateInstanceRow(item);        }    };    return instances;}
0
protected Iterator<IModel<NutchInstance>> getItemModels()
{    return new CpmIteratorAdapter<>(instanceService.getInstances());}
0
protected void populateItem(Item<NutchInstance> item)
{    populateInstanceRow(item);}
0
private AjaxLink<NutchInstance> addInstanceButton()
{    return new AjaxLink<NutchInstance>("addInstance") {        @Override        public void onClick(AjaxRequestTarget target) {            instancePanel.setModel(new CompoundPropertyModel<>(new NutchInstance()));            target.add(instancePanel);            instancePanel.appendShowDialogJavaScript(target);        }    };}
0
public void onClick(AjaxRequestTarget target)
{    instancePanel.setModel(new CompoundPropertyModel<>(new NutchInstance()));    target.add(instancePanel);    instancePanel.appendShowDialogJavaScript(target);}
0
private void populateInstanceRow(final Item<NutchInstance> item)
{    item.add(new AjaxLink<NutchInstance>("editInstance") {        @Override        public void onClick(AjaxRequestTarget target) {            instancePanel.setModel(item.getModel());            target.add(instancePanel);            instancePanel.appendShowDialogJavaScript(target);        }    }.add(new Label("name")));    item.add(new Label("host"));    item.add(new Label("username"));    item.add(createStatusLabel());    item.add(new AjaxLink<NutchInstance>("instanceDelete", item.getModel()) {        @Override        public void onClick(AjaxRequestTarget target) {            instanceService.removeInstance(getModelObject().getId());            target.add(instancesTable);        }    });}
0
public void onClick(AjaxRequestTarget target)
{    instancePanel.setModel(item.getModel());    target.add(instancePanel);    instancePanel.appendShowDialogJavaScript(target);}
0
public void onClick(AjaxRequestTarget target)
{    instanceService.removeInstance(getModelObject().getId());    target.add(instancesTable);}
0
private ColorEnumLabel<ConnectionStatus> createStatusLabel()
{    return new ColorEnumLabelBuilder<ConnectionStatus>("connectionStatus").withEnumColor(CONNECTED, Success).withEnumColor(CONNECTING, Info).withEnumColor(DISCONNECTED, Danger).build();}
0
protected Iterator<IModel<SeedList>> getItemModels()
{    return new CpmIteratorAdapter<>(seedListService.findAll());}
0
protected void populateItem(final Item<SeedList> item)
{    PageParameters params = new PageParameters();    params.add("id", item.getModelObject().getId());    Link<Void> edit = new BookmarkablePageLink<>("edit", SeedPage.class, params);    edit.add(new Label("name"));    item.add(edit);    item.add(new Label("seedUrlsCount"));    item.add(new Link<SeedList>("delete", item.getModel()) {        @Override        public void onClick() {            seedListService.delete(item.getModelObject().getId());        }    });}
0
public void onClick()
{    seedListService.delete(item.getModelObject().getId());}
0
protected SeedList load()
{    Long seedListId = parameters.get("id").toLongObject();    return seedListService.getSeedList(seedListId);}
0
public void initPage(IModel<SeedList> model)
{    setModel(new CompoundPropertyModel<>(model));    addBaseForm();    addSeedUrlsList();    addUrlForm();}
0
private void addBaseForm()
{    Form<SeedList> form = new Form<SeedList>("seedList", getModel()) {        @Override        protected void onSubmit() {            seedListService.save(getModelObject());            setResponsePage(SeedListsPage.class);        }    };    form.add(new TextField<String>("name"));    add(form);}
0
protected void onSubmit()
{    seedListService.save(getModelObject());    setResponsePage(SeedListsPage.class);}
0
private void addSeedUrlsList()
{    seedUrlsTable = new WebMarkupContainer("seedUrlsTable");    seedUrlsTable.setOutputMarkupId(true);    RefreshingView<SeedUrl> seedUrls = new RefreshingView<SeedUrl>("seedUrls") {        @Override        protected Iterator<IModel<SeedUrl>> getItemModels() {            return new CpmIteratorAdapter<>(getModelObject().getSeedUrls());        }        @Override        protected void populateItem(Item<SeedUrl> item) {            item.add(new Label("url"));            item.add(new AjaxLink<SeedUrl>("delete", item.getModel()) {                @Override                public void onClick(AjaxRequestTarget target) {                    deleteSeedUrl(getModelObject());                    target.add(seedUrlsTable);                }            });        }    };    seedUrlsTable.add(seedUrls);    add(seedUrlsTable);}
0
protected Iterator<IModel<SeedUrl>> getItemModels()
{    return new CpmIteratorAdapter<>(getModelObject().getSeedUrls());}
0
protected void populateItem(Item<SeedUrl> item)
{    item.add(new Label("url"));    item.add(new AjaxLink<SeedUrl>("delete", item.getModel()) {        @Override        public void onClick(AjaxRequestTarget target) {            deleteSeedUrl(getModelObject());            target.add(seedUrlsTable);        }    });}
0
public void onClick(AjaxRequestTarget target)
{    deleteSeedUrl(getModelObject());    target.add(seedUrlsTable);}
0
private void addUrlForm()
{    urlForm = new Form<>("urlForm", CompoundPropertyModel.of(Model.of(new SeedUrl())));    urlForm.setOutputMarkupId(true);    urlForm.add(new TextField<String>("url"));    urlForm.add(new AjaxSubmitLink("addUrl", urlForm) {        @Override        protected void onSubmit(AjaxRequestTarget target, Form<?> form) {            addSeedUrl();            urlForm.setModelObject(new SeedUrl());            target.add(urlForm);            target.add(seedUrlsTable);        }    });    add(urlForm);}
0
protected void onSubmit(AjaxRequestTarget target, Form<?> form)
{    addSeedUrl();    urlForm.setModelObject(new SeedUrl());    target.add(urlForm);    target.add(seedUrlsTable);}
0
private void addSeedUrl()
{    SeedUrl url = urlForm.getModelObject();    SeedList seedList = getModelObject();    url.setSeedList(seedList);    seedList.getSeedUrls().add(url);}
0
private void deleteSeedUrl(SeedUrl url)
{    SeedList seedList = getModelObject();    seedList.getSeedUrls().remove(url);}
0
protected Iterator<IModel<NutchConfig>> getItemModels()
{    return new CpmIteratorAdapter<>(convertNutchConfig(nutchService.getNutchConfig(getCurrentInstance().getId())));}
0
protected void populateItem(Item<NutchConfig> item)
{    item.add(new Label("name"));    item.add(new TextField<String>("value"));}
0
private List<NutchConfig> convertNutchConfig(Map<String, String> map)
{    List<NutchConfig> listNutchConfigs = new LinkedList<>();    for (String key : map.keySet()) {        NutchConfig conf = new NutchConfig();        conf.setName(key);        conf.setValue(map.get(key));        listNutchConfigs.add(conf);    }    return listNutchConfigs;}
0
public void startCrawl(Long crawlId, NutchInstance instance)
{    Crawl crawl = null;    try {        crawl = crawlDao.queryForId(crawlId);        if (crawl.getCrawlId() == null) {            crawl.setCrawlId("crawl-" + crawlId.toString());        }        NutchClient client = nutchClientFactory.getClient(instance);        String seedDirectory = client.createSeed(crawl.getSeedList());        crawl.setSeedDirectory(seedDirectory);        List<RemoteCommand> commands = commandFactory.createCommands(crawl);        RemoteCommandExecutor executor = new RemoteCommandExecutor(client);        CrawlingCycle cycle = new CrawlingCycle(this, executor, crawl, commands);        cycle.executeCrawlCycle();    } catch (Exception e) {        crawl.setStatus(CrawlStatus.ERROR);        saveCrawl(crawl);            }}
1
public List<Crawl> getCrawls()
{    try {        return crawlDao.queryForAll();    } catch (SQLException e) {        throw new RuntimeException(e);    }}
0
public void saveCrawl(Crawl crawl)
{    try {        crawlDao.createOrUpdate(crawl);    } catch (SQLException e) {        throw new RuntimeException(e);    }}
0
public void deleteCrawl(Long crawlId)
{    try {        crawlDao.deleteById(crawlId);    } catch (SQLException e) {        throw new RuntimeException(e);    }}
0
public void crawlingStarted(Crawl crawl)
{    crawl.setStatus(CrawlStatus.CRAWLING);    crawl.setProgress(0);    saveCrawl(crawl);}
0
public void onCrawlError(Crawl crawl, String msg)
{    crawl.setStatus(CrawlStatus.ERROR);    saveCrawl(crawl);}
0
public void commandExecuted(Crawl crawl, RemoteCommand command, int progress)
{    crawl.setProgress(progress);    saveCrawl(crawl);}
0
public void crawlingFinished(Crawl crawl)
{    crawl.setStatus(CrawlStatus.FINISHED);    saveCrawl(crawl);}
0
public List<NutchInstance> getInstances()
{    try {        return instancesDao.queryForAll();    } catch (SQLException e) {        throw new RuntimeException(e);    }}
0
public NutchInstance getInstance(Long id)
{    try {        return instancesDao.queryForId(id);    } catch (SQLException e) {        throw new RuntimeException(e);    }}
0
public void saveInstance(NutchInstance instance)
{    try {        instancesDao.createOrUpdate(instance);    } catch (SQLException e) {        throw new RuntimeException(e);    }}
0
public void removeInstance(Long id)
{    try {        instancesDao.deleteById(id);    } catch (SQLException e) {        throw new RuntimeException(e);    }}
0
public ConnectionStatus getConnectionStatus(Long instanceId)
{    NutchInstance instance = instanceService.getInstance(instanceId);    try {        NutchStatus nutchStatus = nutchClientFactory.getClient(instance).getNutchStatus();        if (nutchStatus.getStartDate() != null) {            return ConnectionStatus.CONNECTED;        }    } catch (Exception e) {        if (e.getCause() instanceof ConnectException) {            return ConnectionStatus.DISCONNECTED;        }            }    return null;}
1
public Map<String, String> getNutchConfig(Long instanceId)
{    NutchInstance instance = instanceService.getInstance(instanceId);    try {        return nutchClientFactory.getClient(instance).getNutchConfig("default");    } catch (ClientHandlerException exception) {        return Collections.emptyMap();    }}
0
public NutchStatus getNutchStatus(Long instanceId)
{    NutchInstance instance = instanceService.getInstance(instanceId);    return nutchClientFactory.getClient(instance).getNutchStatus();}
0
public void save(SeedList seedList)
{    try {        seedListDao.createOrUpdate(seedList);    } catch (SQLException e) {        throw new RuntimeException(e);    }}
0
public void delete(Long seedListId)
{    try {        seedListDao.deleteById(seedListId);    } catch (SQLException e) {        throw new RuntimeException(e);    }}
0
public List<SeedList> findAll()
{    try {        return seedListDao.queryForAll();    } catch (SQLException e) {        throw new RuntimeException(e);    }}
0
public SeedList getSeedList(Long seedListId)
{    try {        return seedListDao.queryForId(seedListId);    } catch (SQLException e) {        throw new RuntimeException(e);    }}
0
public Configuration getConf()
{    return this.conf;}
0
public void setConf(Configuration conf)
{    this.conf = conf;}
0
public NutchDocument filter(NutchDocument doc, Parse parse, Text url, CrawlDatum datum, Inlinks inlinks) throws IndexingException
{    String[] metadata = parse.getData().getParseMeta().getValues(Any23ParseFilter.ANY23_TRIPLES);    if (metadata != null) {        for (String triple : metadata) {            Pattern pattern = Pattern.compile("^([^ ]+) ([^ ]+) (.+) \\.");            Matcher matcher = pattern.matcher(triple);            if (matcher.find()) {                Map<String, String> map = new HashMap<>();                map.put("node", matcher.group(1));                map.put("key", matcher.group(2));                map.put("short_key", keyToShortKey(matcher.group(2)));                map.put("value", matcher.group(3));                doc.add("structured_data", map);            } else {                            }        }    }    return doc;}
1
private String keyToShortKey(String key)
{    if (key.startsWith("<") && key.endsWith(">")) {        key = key.substring(1, key.length() - 1);    }    String[] keyParts = key.split("/");    String[] keySubParts = keyParts[keyParts.length - 1].split("#");    return keySubParts[keySubParts.length - 1];}
0
private Set<String> getTriples()
{    return triples;}
0
private void parse(String url, String htmlContent, String contentType, String... extractorNames) throws URISyntaxException, IOException, TripleHandlerException
{    Any23 any23 = new Any23(extractorNames);    any23.setMIMETypeDetector(null);    ByteArrayOutputStream baos = new ByteArrayOutputStream();    try {        TripleHandler tHandler = new NTriplesWriter(baos);        BenchmarkTripleHandler bHandler = new BenchmarkTripleHandler(tHandler);        try {            any23.extract(htmlContent, url, contentType, "UTF-8", bHandler);        } catch (IOException e) {                    } catch (ExtractionException e) {                    } finally {            tHandler.close();            bHandler.close();        }                String n3 = baos.toString("UTF-8");        String[] triplesStrings = n3.split("\n");        Collections.addAll(triples, triplesStrings);    } catch (IOException e) {            }}
1
public Configuration getConf()
{    return this.conf;}
0
public void setConf(Configuration conf)
{    this.conf = conf;}
0
public ParseResult filter(Content content, ParseResult parseResult, HTMLMetaTags metaTags, DocumentFragment doc)
{    String[] extractorNames = conf.getStrings(ANY_23_EXTRACTORS_CONF, "html-head-meta");    String[] supportedContentTypes = conf.getStrings(ANY_23_CONTENT_TYPES_CONF, "text/html", "application/xhtml+xml");    String contentType = content.getContentType();    if (supportedContentTypes != null && !Arrays.asList(supportedContentTypes).contains(contentType)) {                return parseResult;    }    Any23Parser parser;    try {        String htmlContent = new String(content.getContent(), Charset.forName("UTF-8"));        parser = new Any23Parser(content.getUrl(), htmlContent, contentType, extractorNames);    } catch (TripleHandlerException e) {        throw new RuntimeException("Error running Any23 parser: " + e.getMessage());    }    Set<String> triples = parser.getTriples();    Parse parse = parseResult.get(content.getUrl());    Metadata metadata = parse.getData().getParseMeta();    for (String triple : triples) {        metadata.add(ANY23_TRIPLES, triple);    }    return parseResult;}
1
public void testAny23TriplesFields() throws Exception
{    Configuration conf = NutchConfiguration.create();    Any23IndexingFilter filter = new Any23IndexingFilter();    filter.setConf(conf);    Assert.assertNotNull(filter);    NutchDocument doc = new NutchDocument();    ParseData parseData = new ParseData(ParseStatus.STATUS_SUCCESS, "The Foo Page", new Outlink[] {}, new Metadata());    ParseImpl parse = new ParseImpl("test page", parseData);    String[] triples = new String[] { "<http://dbpedia.org/resource/Z\u00FCrich> <http://www.w3.org/2002/07/owl#sameAs> <http://rdf.freebase.com/ns/m.08966> .", "<http://dbpedia.org/resource/Z\u00FCrich> <http://dbpedia.org/property/yearHumidity> \"77\" .", "<http://dbpedia.org/resource/Z\u00FCrich> <http://www.w3.org/2000/01/rdf-schema#label> \"Zurique\"@pt ." };    for (String triple : triples) {        parse.getData().getParseMeta().add(Any23ParseFilter.ANY23_TRIPLES, triple);    }    try {        doc = filter.filter(doc, parse, new Text("http://nutch.apache.org/"), new CrawlDatum(), new Inlinks());    } catch (Exception e) {        e.printStackTrace();        Assert.fail(e.getMessage());    }    List<Object> docTriples = doc.getField(Any23IndexingFilter.STRUCTURED_DATA).getValues();    Assert.assertEquals(docTriples.size(), triples.length);    Object triple = docTriples.get(0);    Assert.assertTrue(triple instanceof Map<?, ?>);    @SuppressWarnings("unchecked")    Map<String, String> structuredData = (Map<String, String>) triple;    Assert.assertEquals(structuredData.get("node"), "<http://dbpedia.org/resource/Z\u00FCrich>");    Assert.assertEquals(structuredData.get("key"), "<http://www.w3.org/2002/07/owl#sameAs>");    Assert.assertEquals(structuredData.get("short_key"), "sameAs");    Assert.assertEquals(structuredData.get("value"), "<http://rdf.freebase.com/ns/m.08966>");    triple = docTriples.get(1);    Assert.assertTrue(triple instanceof Map<?, ?>);    structuredData = (Map<String, String>) triple;    Assert.assertEquals(structuredData.get("node"), "<http://dbpedia.org/resource/Z\u00FCrich>");    Assert.assertEquals(structuredData.get("key"), "<http://dbpedia.org/property/yearHumidity>");    Assert.assertEquals(structuredData.get("short_key"), "yearHumidity");    Assert.assertEquals(structuredData.get("value"), "\"77\"");}
0
public void setUp()
{    this.conf = NutchConfiguration.create();    conf.set("file.content.limit", "-1");    conf.set("parser.timeout", "-1");    conf.set(Any23ParseFilter.ANY_23_EXTRACTORS_CONF, "html-embedded-jsonld,html-head-icbm,html-head-links," + "html-head-meta,html-head-title,html-mf-adr,html-mf-geo,html-mf-hcalendar,html-mf-hcard," + "html-mf-hlisting,html-mf-hrecipe,html-mf-hresume,html-mf-hreview,html-mf-hreview-aggregate," + "html-mf-license,html-mf-species,html-mf-xfn,html-microdata,html-rdfa11,html-xpath");    conf.set(Any23ParseFilter.ANY_23_CONTENT_TYPES_CONF, "text/html");}
0
public void testExtractTriplesFromHTML() throws IOException, ParserNotFound, ParseException
{    String[] triplesArray = getTriples(file1);    Assert.assertEquals("We expect 117 tab-separated triples extracted by the filter", EXPECTED_TRIPLES_1, triplesArray.length);}
0
public void extractMicroDataFromHTML() throws ParserNotFound, IOException, ParseException
{    String[] triplesArray = getTriples(file2);    Assert.assertEquals("We expect 40 tab-separated triples extracted by the filter", EXPECTED_TRIPLES_2, triplesArray.length);}
0
public void ignoreUnsupported() throws ParserNotFound, IOException, ParseException
{    String[] triplesArray = getTriples(file1, "application/pdf");    Assert.assertEquals("We expect no triples extracted by the filter since content-type should be ignored", 0, triplesArray.length);}
0
public String[] extract(String urlString, File file, String contentType)
{    try {        System.out.println(urlString);        Protocol protocol = new ProtocolFactory(conf).getProtocol(urlString);        Content content = protocol.getProtocolOutput(new Text(urlString), new CrawlDatum()).getContent();        content.setContentType(contentType);        Parse parse = new ParseUtil(conf).parse(content).get(content.getUrl());        return parse.getData().getParseMeta().getValues(Any23ParseFilter.ANY23_TRIPLES);    } catch (Exception e) {        e.printStackTrace();        Assert.fail(e.toString());    }    return null;}
0
private String[] getTriples(String fileName)
{    return getTriples(fileName, "text/html");}
0
private String[] getTriples(String fileName, String contentType)
{    String urlString = "file:" + sampleDir + fileSeparator + fileName;    File file = new File(sampleDir + fileSeparator + fileName);    return extract(urlString, file, contentType);}
0
public NutchDocument filter(NutchDocument doc, Parse parse, Text url, CrawlDatum datum, Inlinks inlinks) throws IndexingException
{    Metadata metadata = parse.getData().getParseMeta();        String licenseUrl = metadata.get(CreativeCommons.LICENSE_URL);    if (licenseUrl != null) {        if (LOG.isInfoEnabled()) {                    }                addFeature(doc, "license=" + licenseUrl);                addUrlFeatures(doc, licenseUrl);    }        String licenseLocation = metadata.get(CreativeCommons.LICENSE_LOCATION);    if (licenseLocation != null) {        addFeature(doc, "meta=" + licenseLocation);    }        String workType = metadata.get(CreativeCommons.WORK_TYPE);    if (workType != null) {        addFeature(doc, workType);    }    return doc;}
1
public void addUrlFeatures(NutchDocument doc, String urlString)
{    try {        URL url = new URL(urlString);                StringTokenizer names = new StringTokenizer(url.getPath(), "/-");        if (names.hasMoreTokens())                        names.nextToken();                while (names.hasMoreTokens()) {            String feature = names.nextToken();            addFeature(doc, feature);        }    } catch (MalformedURLException e) {        if (LOG.isWarnEnabled()) {                    }    }}
1
private void addFeature(NutchDocument doc, String feature)
{    doc.add(FIELD, feature);}
0
public void setConf(Configuration conf)
{    this.conf = conf;}
0
public Configuration getConf()
{    return this.conf;}
0
public static void walk(Node doc, URL base, Metadata metadata, Configuration conf) throws ParseException
{        Walker walker = new Walker(base);    walker.walk(doc);        String licenseUrl = null;    String licenseLocation = null;    if (walker.rdfLicense != null) {                licenseLocation = "rdf";        licenseUrl = walker.rdfLicense;    } else if (walker.relLicense != null) {                licenseLocation = "rel";        licenseUrl = walker.relLicense.toString();    } else if (walker.anchorLicense != null) {                licenseLocation = "a";        licenseUrl = walker.anchorLicense.toString();    } else if (conf.getBoolean("creativecommons.exclude.unlicensed", false)) {        throw new ParseException("No CC license.  Excluding.");    }        if (licenseUrl != null) {        if (LOG.isInfoEnabled()) {                    }        metadata.add(CreativeCommons.LICENSE_URL, licenseUrl);        metadata.add(CreativeCommons.LICENSE_LOCATION, licenseLocation);    }    if (walker.workType != null) {        if (LOG.isInfoEnabled()) {                    }        metadata.add(CreativeCommons.WORK_TYPE, walker.workType);    }}
1
private void walk(Node node)
{        if (node instanceof Element) {        findLicenseUrl((Element) node);    }        if (node instanceof Comment) {        findRdf(((Comment) node).getData());    }        NodeList children = node.getChildNodes();    for (int i = 0; children != null && i < children.getLength(); i++) {        walk(children.item(i));    }}
0
private void findLicenseUrl(Element element)
{        if (!"a".equalsIgnoreCase(element.getTagName()))        return;        String href = element.getAttribute("href");    if (href == null)        return;    try {                URL url = new URL(base, href);                if ("http".equalsIgnoreCase(url.getProtocol()) && "creativecommons.org".equalsIgnoreCase(url.getHost()) && url.getPath() != null && url.getPath().startsWith("/licenses/") && url.getPath().length() > "/licenses/".length()) {                        String rel = element.getAttribute("rel");            if (rel != null && "license".equals(rel) && this.relLicense == null) {                                this.relLicense = url;            } else if (this.anchorLicense == null) {                                this.anchorLicense = url;            }        }    } catch (MalformedURLException e) {        }}
0
private void findRdf(String comment)
{        int rdfPosition = comment.indexOf("RDF");    if (rdfPosition < 0)                return;    int nsPosition = comment.indexOf(CC_NS);    if (nsPosition < 0)                return;        Document doc;    try {        DocumentBuilder parser = FACTORY.newDocumentBuilder();        doc = parser.parse(new InputSource(new StringReader(comment)));    } catch (Exception e) {        if (LOG.isWarnEnabled()) {                    }        return;    }        NodeList roots = doc.getElementsByTagNameNS(RDF_NS, "RDF");    if (roots.getLength() != 1) {        if (LOG.isWarnEnabled()) {                    }        return;    }    Element rdf = (Element) roots.item(0);        NodeList licenses = rdf.getElementsByTagNameNS(CC_NS, "License");    for (int i = 0; i < licenses.getLength(); i++) {        Element l = (Element) licenses.item(i);                this.rdfLicense = l.getAttributeNodeNS(RDF_NS, "about").getValue();                NodeList predicates = l.getChildNodes();        for (int j = 0; j < predicates.getLength(); j++) {            Node predicateNode = predicates.item(j);            if (!(predicateNode instanceof Element))                continue;            Element predicateElement = (Element) predicateNode;                        if (!CC_NS.equals(predicateElement.getNamespaceURI())) {                continue;            }        }    }        NodeList works = rdf.getElementsByTagNameNS(CC_NS, "Work");    for (int i = 0; i < works.getLength(); i++) {                NodeList types = rdf.getElementsByTagNameNS(DC_NS, "type");        for (int j = 0; j < types.getLength(); j++) {            Element type = (Element) types.item(j);            String workUri = type.getAttributeNodeNS(RDF_NS, "resource").getValue();            this.workType = WORK_TYPE_NAMES.get(workUri);        }    }}
1
public ParseResult filter(Content content, ParseResult parseResult, HTMLMetaTags metaTags, DocumentFragment doc)
{        Parse parse = parseResult.get(content.getUrl());        URL base;    try {        base = new URL(content.getBaseUrl());    } catch (MalformedURLException e) {        Parse emptyParse = new ParseStatus(e).getEmptyParse(getConf());        parseResult.put(content.getUrl(), new ParseText(emptyParse.getText()), emptyParse.getData());        return parseResult;    }    try {                Walker.walk(doc, base, parse.getData().getParseMeta(), getConf());    } catch (ParseException e) {        Parse emptyParse = new ParseStatus(e).getEmptyParse(getConf());        parseResult.put(content.getUrl(), new ParseText(emptyParse.getText()), emptyParse.getData());        return parseResult;    }    return parseResult;}
0
public void setConf(Configuration conf)
{    this.conf = conf;}
0
public Configuration getConf()
{    return this.conf;}
0
public void testPages() throws Exception
{    pageTest(new File(testDir, "anchor.html"), "http://foo.com/", "http://creativecommons.org/licenses/by-nc-sa/1.0", "a", null);            pageTest(new File(testDir, "rel.html"), "http://foo.com/", "http://creativecommons.org/licenses/by-nc/2.0", "rel", null);            pageTest(new File(testDir, "rdf.html"), "http://foo.com/", "http://creativecommons.org/licenses/by-nc/1.0", "rdf", "text");}
0
public void pageTest(File file, String url, String license, String location, String type) throws Exception
{    String contentType = "text/html";    InputStream in = new FileInputStream(file);    ByteArrayOutputStream out = new ByteArrayOutputStream((int) file.length());    byte[] buffer = new byte[1024];    int i;    while ((i = in.read(buffer)) != -1) {        out.write(buffer, 0, i);    }    in.close();    byte[] bytes = out.toByteArray();    Configuration conf = NutchConfiguration.create();    Content content = new Content(url, url, bytes, contentType, new Metadata(), conf);    Parse parse = new ParseUtil(conf).parse(content).get(content.getUrl());    Metadata metadata = parse.getData().getParseMeta();    Assert.assertEquals(license, metadata.get("License-Url"));    Assert.assertEquals(location, metadata.get("License-Location"));    Assert.assertEquals(type, metadata.get("Work-Type"));}
0
public void open(Map<String, String> parameters)
{    expression = JexlUtil.parseExpression(parameters.get(EXPRESSION_KEY));}
0
public boolean match(NutchDocument doc)
{        JexlContext jexlContext = new MapContext();    jexlContext.set("doc", doc);    try {        if (Boolean.TRUE.equals(expression.evaluate(jexlContext))) {            return true;        }    } catch (Exception ignored) {    }    return false;}
0
public void setConf(Configuration configuration)
{    this.conf = configuration;}
0
public Configuration getConf()
{    return conf;}
0
public NutchDocument filter(NutchDocument doc, Parse parse, Text url, CrawlDatum datum, Inlinks inlinks) throws IndexingException
{    ParseData parseData = parse.getData();    Metadata parseMeta = parseData.getParseMeta();    String[] authors = parseMeta.getValues(Feed.FEED_AUTHOR);    String[] tags = parseMeta.getValues(Feed.FEED_TAGS);    String published = parseMeta.get(Feed.FEED_PUBLISHED);    String updated = parseMeta.get(Feed.FEED_UPDATED);    String feed = parseMeta.get(Feed.FEED);    if (authors != null) {        for (String author : authors) {            doc.add(Feed.FEED_AUTHOR, author);        }    }    if (tags != null) {        for (String tag : tags) {            doc.add(Feed.FEED_TAGS, tag);        }    }    if (feed != null)        doc.add(Feed.FEED, feed);    if (published != null) {        Date date = new Date(Long.parseLong(published));        doc.add(PUBLISHED_DATE, date);    }    if (updated != null) {        Date date = new Date(Long.parseLong(updated));        doc.add(UPDATED_DATE, date);    }    return doc;}
0
public Configuration getConf()
{    return conf;}
0
public void setConf(Configuration conf)
{    this.conf = conf;}
0
public ParseResult getParse(Content content)
{    SyndFeed feed = null;    ParseResult parseResult = new ParseResult(content.getUrl());    EncodingDetector detector = new EncodingDetector(conf);    detector.autoDetectClues(content, true);    String encoding = detector.guessEncoding(content, defaultEncoding);    try {        InputSource input = new InputSource(new ByteArrayInputStream(content.getContent()));        input.setEncoding(encoding);        SyndFeedInput feedInput = new SyndFeedInput();        feed = feedInput.build(input);    } catch (Exception e) {                        return new ParseStatus(e).getEmptyParseResult(content.getUrl(), getConf());    }    String feedLink = feed.getLink();    try {        feedLink = normalizers.normalize(feedLink, URLNormalizers.SCOPE_OUTLINK);        if (feedLink != null)            feedLink = filters.filter(feedLink);    } catch (Exception e) {        feedLink = null;    }    List<?> entries = feed.getEntries();    for (Object entry : entries) {        addToMap(parseResult, feed, feedLink, (SyndEntry) entry, content);    }    String feedDesc = stripTags(feed.getDescriptionEx());    String feedTitle = stripTags(feed.getTitleEx());    parseResult.put(content.getUrl(), new ParseText(feedDesc), new ParseData(new ParseStatus(ParseStatus.SUCCESS), feedTitle, new Outlink[0], content.getMetadata()));    return parseResult;}
1
public void setConf(Configuration conf)
{    this.conf = conf;    this.parserFactory = new ParserFactory(conf);    this.normalizers = new URLNormalizers(conf, URLNormalizers.SCOPE_OUTLINK);    this.filters = new URLFilters(conf);    this.defaultEncoding = conf.get("parser.character.encoding.default", "windows-1252");}
0
public Configuration getConf()
{    return this.conf;}
0
public static void main(String[] args) throws Exception
{    if (args.length != 1) {        System.err.println("Usage: FeedParser <feed>");        System.exit(1);    }    String name = args[0];    String url = "file:" + name;    Configuration conf = NutchConfiguration.create();    FeedParser parser = new FeedParser();    parser.setConf(conf);    File file = new File(name);    byte[] bytes = new byte[(int) file.length()];    DataInputStream in = new DataInputStream(new FileInputStream(file));    in.readFully(bytes);    in.close();    ParseResult parseResult = parser.getParse(new Content(url, url, bytes, "application/rss+xml", new Metadata(), conf));    for (Entry<Text, Parse> entry : parseResult) {        System.out.println("key: " + entry.getKey());        Parse parse = entry.getValue();        System.out.println("data: " + parse.getData());        System.out.println("text: " + parse.getText() + "\n");    }}
0
private void addToMap(ParseResult parseResult, SyndFeed feed, String feedLink, SyndEntry entry, Content content)
{    String link = entry.getLink(), text = null, title = null;    Metadata parseMeta = new Metadata(), contentMeta = content.getMetadata();    Parse parse = null;    SyndContent description = entry.getDescription();    try {        link = normalizers.normalize(link, URLNormalizers.SCOPE_OUTLINK);        if (link != null)            link = filters.filter(link);    } catch (Exception e) {        e.printStackTrace();        return;    }    if (link == null)        return;    title = stripTags(entry.getTitleEx());    if (feedLink != null)        parseMeta.set("feed", feedLink);    addFields(parseMeta, contentMeta, feed, entry);                String contentType = contentMeta.get(Response.CONTENT_TYPE);    if (description != null)        text = description.getValue();    if (text == null) {        List<?> contents = entry.getContents();        StringBuilder buf = new StringBuilder();        for (Object syndContent : contents) {            buf.append(((SyndContent) syndContent).getValue());        }        text = buf.toString();    }    try {        Parser parser = parserFactory.getParsers(contentType, link)[0];        parse = parser.getParse(new Content(link, link, text.getBytes(), contentType, contentMeta, conf)).get(link);    } catch (ParserNotFound e) {    /* ignore */    }    if (parse != null) {        ParseData data = parse.getData();        data.getContentMeta().remove(Response.CONTENT_TYPE);        mergeMetadata(data.getParseMeta(), parseMeta);        parseResult.put(link, new ParseText(parse.getText()), new ParseData(ParseStatus.STATUS_SUCCESS, title, data.getOutlinks(), data.getContentMeta(), data.getParseMeta()));    } else {        contentMeta.remove(Response.CONTENT_TYPE);        parseResult.put(link, new ParseText(text), new ParseData(ParseStatus.STATUS_FAILURE, title, new Outlink[0], contentMeta, parseMeta));    }}
0
private static String stripTags(SyndContent c)
{    if (c == null)        return "";    String value = c.getValue();    String[] parts = value.split("<[^>]*>");    StringBuffer buf = new StringBuffer();    for (String part : parts) buf.append(part);    return buf.toString().trim();}
0
private void addFields(Metadata parseMeta, Metadata contentMeta, SyndFeed feed, SyndEntry entry)
{    List<?> authors = entry.getAuthors(), categories = entry.getCategories();    Date published = entry.getPublishedDate(), updated = entry.getUpdatedDate();    String contentType = null;    if (authors != null) {        for (Object o : authors) {            SyndPerson author = (SyndPerson) o;            String authorName = author.getName();            if (checkString(authorName)) {                parseMeta.add(Feed.FEED_AUTHOR, authorName);            }        }    } else {                        String authorName = entry.getAuthor();        if (checkString(authorName)) {            parseMeta.set(Feed.FEED_AUTHOR, authorName);        }    }    for (Object i : categories) {        parseMeta.add(Feed.FEED_TAGS, ((SyndCategory) i).getName());    }    if (published != null) {        parseMeta.set(Feed.FEED_PUBLISHED, Long.toString(published.getTime()));    }    if (updated != null) {        parseMeta.set(Feed.FEED_UPDATED, Long.toString(updated.getTime()));    }    SyndContent description = entry.getDescription();    if (description != null) {        contentType = description.getType();    } else {                List<?> contents = entry.getContents();        if (contents.size() > 0) {            contentType = ((SyndContent) contents.get(0)).getType();        }    }    if (checkString(contentType)) {                if (contentType.equals("html"))            contentType = "text/html";        else if (contentType.equals("xhtml"))            contentType = "text/xhtml";        contentMeta.set(Response.CONTENT_TYPE, contentType + "; " + CHARSET_UTF8);    } else {        contentMeta.set(Response.CONTENT_TYPE, TEXT_PLAIN_CONTENT_TYPE);    }}
0
private void mergeMetadata(Metadata first, Metadata second)
{    for (String name : second.names()) {        String[] values = second.getValues(name);        for (String value : values) {            first.add(name, value);        }    }}
0
private boolean checkString(String s)
{    return s != null && !s.equals("");}
0
public void testParseFetchChannel() throws ProtocolNotFound, ParseException
{    String urlString;    Protocol protocol;    Content content;    ParseResult parseResult;    Configuration conf = NutchConfiguration.create();    for (int i = 0; i < sampleFiles.length; i++) {        urlString = "file:" + sampleDir + fileSeparator + sampleFiles[i];        urlString = urlString.replace('\\', '/');        protocol = new ProtocolFactory(conf).getProtocol(urlString);        content = protocol.getProtocolOutput(new Text(urlString), new CrawlDatum()).getContent();        parseResult = new ParseUtil(conf).parseByExtensionId("feed", content);        Assert.assertEquals(3, parseResult.size());        boolean hasLink1 = false, hasLink2 = false, hasLink3 = false;        for (Iterator<Map.Entry<Text, Parse>> j = parseResult.iterator(); j.hasNext(); ) {            Map.Entry<Text, Parse> entry = j.next();            if (entry.getKey().toString().equals("http://www-scf.usc.edu/~mattmann/")) {                hasLink1 = true;            } else if (entry.getKey().toString().equals("http://www.nutch.org/")) {                hasLink2 = true;            } else if (entry.getKey().toString().equals(urlString)) {                hasLink3 = true;            }            Assert.assertNotNull(entry.getValue());            Assert.assertNotNull(entry.getValue().getData());        }        if (!hasLink1 || !hasLink2 || !hasLink3) {            Assert.fail("Outlinks read from sample rss file are not correct!");        }    }}
0
public ParseResult filter(Content content, ParseResult parseResult, HTMLMetaTags metaTags, DocumentFragment doc)
{    Parse parse = parseResult.get(content.getUrl());    for (int i = 0; headings != null && i < headings.length; i++) {        List<String> discoveredHeadings = getElement(doc, headings[i]);        if (discoveredHeadings.size() > 0) {            for (String heading : discoveredHeadings) {                if (heading != null) {                    heading = heading.trim();                    if (heading.length() > 0) {                        parse.getData().getParseMeta().add(headings[i], heading);                    }                }            }        }    }    return parseResult;}
0
public void setConf(Configuration conf)
{    this.conf = conf;    headings = conf.getStrings("headings");    multiValued = conf.getBoolean("headings.multivalued", false);}
0
public Configuration getConf()
{    return this.conf;}
0
protected List<String> getElement(DocumentFragment doc, String element)
{    List<String> headings = new ArrayList<>();    NodeWalker walker = new NodeWalker(doc);    while (walker.hasNext()) {        Node currentNode = walker.nextNode();        if (currentNode.getNodeType() == Node.ELEMENT_NODE) {            if (element.equalsIgnoreCase(currentNode.getNodeName())) {                headings.add(getNodeValue(currentNode));                                if (!multiValued) {                    break;                }            }        }    }    return headings;}
0
protected static String getNodeValue(Node node)
{    StringBuilder buffer = new StringBuilder();    NodeWalker walker = new NodeWalker(node);    while (walker.hasNext()) {        final Node n = walker.nextNode();        if (n.getNodeType() == Node.TEXT_NODE) {            buffer.append(n.getNodeValue());        }    }        Matcher matcher = whitespacePattern.matcher(buffer.toString().trim());    return matcher.replaceAll(" ").trim();}
0
public void testExtractHeadingFromNestedNodes() throws IOException, SAXException
{    conf.setStrings("headings", "h1", "h2");    HtmlParseFilter filter = new HeadingsParseFilter();    filter.setConf(conf);    Content content = new Content("http://www.foo.com/", "http://www.foo.com/", "".getBytes("UTF8"), "text/html; charset=UTF-8", new Metadata(), conf);    ParseImpl parse = new ParseImpl("foo bar", new ParseData());    ParseResult parseResult = ParseResult.createParseResult("http://www.foo.com/", parse);    HTMLMetaTags metaTags = new HTMLMetaTags();    DOMFragmentParser parser = new DOMFragmentParser();    DocumentFragment node = new HTMLDocumentImpl().createDocumentFragment();    parser.parse(new InputSource(new ByteArrayInputStream(("<html><head><title>test header with span element</title></head><body><h1>header with <span>span element</span></h1></body></html>").getBytes())), node);    parseResult = filter.filter(content, parseResult, metaTags, node);    Assert.assertEquals("The h1 tag must include the content of the inner span node", "header with span element", parseResult.get(content.getUrl()).getData().getParseMeta().get("h1"));}
0
public void setConf(Configuration conf)
{    this.conf = conf;    deduplicate = conf.getBoolean("anchorIndexingFilter.deduplicate", false);    }
1
public Configuration getConf()
{    return this.conf;}
0
public NutchDocument filter(NutchDocument doc, Parse parse, Text url, CrawlDatum datum, Inlinks inlinks) throws IndexingException
{    String[] anchors = (inlinks != null ? inlinks.getAnchors() : new String[0]);    HashSet<String> set = null;    for (int i = 0; i < anchors.length; i++) {        if (deduplicate) {            if (set == null)                set = new HashSet<String>();            String lcAnchor = anchors[i].toLowerCase();                        if (!set.contains(lcAnchor)) {                doc.add("anchor", anchors[i]);                                set.add(lcAnchor);            }        } else {            doc.add("anchor", anchors[i]);        }    }    return doc;}
0
public void testDeduplicateAnchor() throws Exception
{    Configuration conf = NutchConfiguration.create();    conf.setBoolean("anchorIndexingFilter.deduplicate", true);    AnchorIndexingFilter filter = new AnchorIndexingFilter();    filter.setConf(conf);    Assert.assertNotNull(filter);    NutchDocument doc = new NutchDocument();    ParseImpl parse = new ParseImpl("foo bar", new ParseData());    Inlinks inlinks = new Inlinks();    inlinks.add(new Inlink("http://test1.com/", "text1"));    inlinks.add(new Inlink("http://test2.com/", "text2"));    inlinks.add(new Inlink("http://test3.com/", "text2"));    try {        filter.filter(doc, parse, new Text("http://nutch.apache.org/index.html"), new CrawlDatum(), inlinks);    } catch (Exception e) {        e.printStackTrace();        Assert.fail(e.getMessage());    }    Assert.assertNotNull(doc);    Assert.assertTrue("test if there is an anchor at all", doc.getFieldNames().contains("anchor"));    Assert.assertEquals("test dedup, we expect 2", 2, doc.getField("anchor").getValues().size());}
0
public NutchDocument filter(NutchDocument doc, Parse parse, Text url, CrawlDatum datum, Inlinks inlinks) throws IndexingException
{    Text reprUrl = (Text) datum.getMetaData().get(Nutch.WRITABLE_REPR_URL_KEY);    String reprUrlString = reprUrl != null ? reprUrl.toString() : null;    String urlString = url.toString();    String host = null;    try {        URL u;        if (reprUrlString != null) {            u = new URL(reprUrlString);        } else {            u = new URL(urlString);        }        if (addDomain) {            doc.add("domain", URLUtil.getDomainName(u));        }        host = u.getHost();    } catch (MalformedURLException e) {        throw new IndexingException(e);    }    if (host != null) {        doc.add("host", host);    }    doc.add("url", reprUrlString == null ? urlString : reprUrlString);        String content = parse.getText();    if (MAX_CONTENT_LENGTH > -1 && content.length() > MAX_CONTENT_LENGTH) {        content = content.substring(0, MAX_CONTENT_LENGTH);    }    doc.add("content", StringUtil.cleanField(content));        String title = parse.getData().getTitle();    if (MAX_TITLE_LENGTH > -1 && title.length() > MAX_TITLE_LENGTH) {                                        title = title.substring(0, MAX_TITLE_LENGTH);    }    if (title.length() > 0) {                doc.add("title", StringUtil.cleanField(title));    }        String caching = parse.getData().getMeta(Nutch.CACHING_FORBIDDEN_KEY);    if (caching != null && !caching.equals(Nutch.CACHING_FORBIDDEN_NONE)) {        doc.add("cache", caching);    }        doc.add("tstamp", new Date(datum.getFetchTime()));    return doc;}
0
public void setConf(Configuration conf)
{    this.conf = conf;    this.MAX_TITLE_LENGTH = conf.getInt("indexer.max.title.length", 100);    this.addDomain = conf.getBoolean("indexer.add.domain", false);    this.MAX_CONTENT_LENGTH = conf.getInt("indexer.max.content.length", -1);}
0
public Configuration getConf()
{    return this.conf;}
0
public void testBasicIndexingFilter() throws Exception
{    Configuration conf = NutchConfiguration.create();    conf.setInt("indexer.max.title.length", 10);    conf.setBoolean("indexer.add.domain", true);    conf.setInt("indexer.max.content.length", 20);    BasicIndexingFilter filter = new BasicIndexingFilter();    filter.setConf(conf);    Assert.assertNotNull(filter);    NutchDocument doc = new NutchDocument();    String title = "The Foo Page";    Outlink[] outlinks = new Outlink[] { new Outlink("http://foo.com/", "Foo") };    Metadata metaData = new Metadata();    metaData.add("Language", "en/us");    ParseData parseData = new ParseData(ParseStatus.STATUS_SUCCESS, title, outlinks, metaData);    ParseImpl parse = new ParseImpl("this is a sample foo bar page. hope you enjoy it.", parseData);    CrawlDatum crawlDatum = new CrawlDatum();    crawlDatum.setFetchTime(100L);    Inlinks inlinks = new Inlinks();    try {        filter.filter(doc, parse, new Text("http://nutch.apache.org/index.html"), crawlDatum, inlinks);    } catch (Exception e) {        e.printStackTrace();        Assert.fail(e.getMessage());    }    Assert.assertNotNull(doc);    Assert.assertEquals("test title, expect \"The Foo Pa\"", "The Foo Pa", doc.getField("title").getValues().get(0));    Assert.assertEquals("test domain, expect \"apache.org\"", "apache.org", doc.getField("domain").getValues().get(0));    Assert.assertEquals("test host, expect \"nutch.apache.org\"", "nutch.apache.org", doc.getField("host").getValues().get(0));    Assert.assertEquals("test url, expect \"http://nutch.apache.org/index.html\"", "http://nutch.apache.org/index.html", doc.getField("url").getValues().get(0));    Assert.assertEquals("test content", "this is a sample foo", doc.getField("content").getValues().get(0));    Assert.assertEquals("test fetch time", new Date(100L), (Date) doc.getField("tstamp").getValues().get(0));}
0
public static void addIfNotNull(NutchDocument doc, String name, String value)
{    if (value != null) {        doc.add(name, value);    }}
0
public static void addIfNotNull(NutchDocument doc, String name, Integer value)
{    if (value != null) {        doc.add(name, value);    }}
0
public static NutchDocument createDocFromInsightsService(String serverIp, NutchDocument doc, WebServiceClient client) throws UnknownHostException, IOException, GeoIp2Exception
{    addIfNotNull(doc, "ip", serverIp);    InsightsResponse response = client.insights(InetAddress.getByName(serverIp));        City city = response.getCity();        addIfNotNull(doc, "cityName", city.getName());        addIfNotNull(doc, "cityConfidence", city.getConfidence());    addIfNotNull(doc, "cityGeoNameId", city.getGeoNameId());    Continent continent = response.getContinent();    addIfNotNull(doc, "continentCode", continent.getCode());    addIfNotNull(doc, "continentGeoNameId", continent.getGeoNameId());    addIfNotNull(doc, "continentName", continent.getName());    Country country = response.getCountry();        addIfNotNull(doc, "countryIsoCode", country.getIsoCode());        addIfNotNull(doc, "countryName", country.getName());        addIfNotNull(doc, "countryConfidence", country.getConfidence());    addIfNotNull(doc, "countryGeoName", country.getGeoNameId());    Location location = response.getLocation();        addIfNotNull(doc, "latLon", location.getLatitude() + "," + location.getLongitude());            addIfNotNull(doc, "accRadius", location.getAccuracyRadius());        addIfNotNull(doc, "timeZone", location.getTimeZone());    addIfNotNull(doc, "metroCode", location.getMetroCode());    Postal postal = response.getPostal();        addIfNotNull(doc, "postalCode", postal.getCode());        addIfNotNull(doc, "postalConfidence", postal.getConfidence());    RepresentedCountry rCountry = response.getRepresentedCountry();    addIfNotNull(doc, "countryType", rCountry.getType());    Subdivision subdivision = response.getMostSpecificSubdivision();        addIfNotNull(doc, "subDivName", subdivision.getName());        addIfNotNull(doc, "subDivIdoCode", subdivision.getIsoCode());        addIfNotNull(doc, "subDivConfidence", subdivision.getConfidence());    addIfNotNull(doc, "subDivGeoNameId", subdivision.getGeoNameId());    Traits traits = response.getTraits();    addIfNotNull(doc, "autonSystemNum", traits.getAutonomousSystemNumber());    addIfNotNull(doc, "autonSystemOrg", traits.getAutonomousSystemOrganization());    addIfNotNull(doc, "domain", traits.getDomain());    addIfNotNull(doc, "isp", traits.getIsp());    addIfNotNull(doc, "org", traits.getOrganization());    addIfNotNull(doc, "userType", traits.getUserType());            addIfNotNull(doc, "isAnonProxy", String.valueOf(traits.isAnonymousProxy()));    return doc;}
0
public static NutchDocument createDocFromCityService(String serverIp, NutchDocument doc, WebServiceClient client) throws UnknownHostException, IOException, GeoIp2Exception
{    CityResponse response = client.city(InetAddress.getByName(serverIp));    return doc;}
0
public static NutchDocument createDocFromCountryService(String serverIp, NutchDocument doc, WebServiceClient client) throws UnknownHostException, IOException, GeoIp2Exception
{    CountryResponse response = client.country(InetAddress.getByName(serverIp));    return doc;}
0
public static NutchDocument createDocFromIspDb(String serverIp, NutchDocument doc, DatabaseReader reader) throws UnknownHostException, IOException, GeoIp2Exception
{    IspResponse response = reader.isp(InetAddress.getByName(serverIp));    addIfNotNull(doc, "ip", serverIp);    addIfNotNull(doc, "autonSystemNum", response.getAutonomousSystemNumber());    addIfNotNull(doc, "autonSystemOrg", response.getAutonomousSystemOrganization());    addIfNotNull(doc, "isp", response.getIsp());    addIfNotNull(doc, "org", response.getOrganization());    return doc;}
0
public static NutchDocument createDocFromDomainDb(String serverIp, NutchDocument doc, DatabaseReader reader) throws UnknownHostException, IOException, GeoIp2Exception
{    DomainResponse response = reader.domain(InetAddress.getByName(serverIp));    addIfNotNull(doc, "ip", serverIp);    addIfNotNull(doc, "domain", response.getDomain());    return doc;}
0
public static NutchDocument createDocFromConnectionDb(String serverIp, NutchDocument doc, DatabaseReader reader) throws UnknownHostException, IOException, GeoIp2Exception
{    ConnectionTypeResponse response = reader.connectionType(InetAddress.getByName(serverIp));    addIfNotNull(doc, "ip", serverIp);    addIfNotNull(doc, "connType", response.getConnectionType().toString());    return doc;}
0
public static NutchDocument createDocFromCityDb(String serverIp, NutchDocument doc, DatabaseReader reader) throws UnknownHostException, IOException, GeoIp2Exception
{    addIfNotNull(doc, "ip", serverIp);    CityResponse response = reader.city(InetAddress.getByName(serverIp));    City city = response.getCity();        addIfNotNull(doc, "cityName", city.getName());        addIfNotNull(doc, "cityConfidence", city.getConfidence());    addIfNotNull(doc, "cityGeoNameId", city.getGeoNameId());    Continent continent = response.getContinent();    addIfNotNull(doc, "continentCode", continent.getCode());    addIfNotNull(doc, "continentGeoNameId", continent.getGeoNameId());    addIfNotNull(doc, "continentName", continent.getName());    Country country = response.getCountry();        addIfNotNull(doc, "countryIsoCode", country.getIsoCode());        addIfNotNull(doc, "countryName", country.getName());        addIfNotNull(doc, "countryConfidence", country.getConfidence());    addIfNotNull(doc, "countryGeoName", country.getGeoNameId());    Location location = response.getLocation();        addIfNotNull(doc, "latLon", location.getLatitude() + "," + location.getLongitude());            addIfNotNull(doc, "accRadius", location.getAccuracyRadius());        addIfNotNull(doc, "timeZone", location.getTimeZone());    addIfNotNull(doc, "metroCode", location.getMetroCode());    Postal postal = response.getPostal();        addIfNotNull(doc, "postalCode", postal.getCode());        addIfNotNull(doc, "postalConfidence", postal.getConfidence());    RepresentedCountry rCountry = response.getRepresentedCountry();    addIfNotNull(doc, "countryType", rCountry.getType());    Subdivision subdivision = response.getMostSpecificSubdivision();        addIfNotNull(doc, "subDivName", subdivision.getName());        addIfNotNull(doc, "subDivIdoCode", subdivision.getIsoCode());        addIfNotNull(doc, "subDivConfidence", subdivision.getConfidence());    addIfNotNull(doc, "subDivGeoNameId", subdivision.getGeoNameId());    return doc;}
0
public Configuration getConf()
{    return this.conf;}
0
public void setConf(Configuration conf)
{    this.conf = conf;    usage = conf.get("index.geoip.usage", "insightsService");        if (usage.equalsIgnoreCase("insightsService")) {        client = new WebServiceClient.Builder(conf.getInt("index.geoip.userid", 12345), conf.get("index.geoip.licensekey")).build();    } else {        String db = null;        if (usage.equalsIgnoreCase("cityDatabase")) {            db = "GeoIP2-City.mmdb";        } else if (usage.equalsIgnoreCase("connectionTypeDatabase")) {            db = "GeoIP2-Connection-Type.mmdb";        } else if (usage.equalsIgnoreCase("domainDatabase")) {            db = "GeoIP2-Domain.mmdb";        } else if (usage.equalsIgnoreCase("ispDatabase")) {            db = "GeoIP2-ISP.mmdb";        }        URL dbFileUrl = conf.getResource(db);        if (dbFileUrl == null) {                    } else {            try {                buildDb(new File(dbFileUrl.getFile()));            } catch (Exception e) {                            }        }    }    if (!conf.getBoolean("store.ip.address", false)) {            }}
1
private void buildDb(File geoDb)
{    try {        reader = new DatabaseReader.Builder(geoDb).build();    } catch (IOException e) {            }}
1
public NutchDocument filter(NutchDocument doc, Parse parse, Text url, CrawlDatum datum, Inlinks inlinks) throws IndexingException
{    return addServerGeo(doc, parse.getData(), url.toString());}
0
private NutchDocument addServerGeo(NutchDocument doc, ParseData data, String url)
{    String serverIp = data.getContentMeta().get("_ip_");    if (serverIp != null && reader != null) {        try {            if (usage.equalsIgnoreCase("cityDatabase")) {                doc = GeoIPDocumentCreator.createDocFromCityDb(serverIp, doc, reader);            } else if (usage.equalsIgnoreCase("connectionTypeDatabase")) {                doc = GeoIPDocumentCreator.createDocFromConnectionDb(serverIp, doc, reader);            } else if (usage.equalsIgnoreCase("domainDatabase")) {                doc = GeoIPDocumentCreator.createDocFromDomainDb(serverIp, doc, reader);            } else if (usage.equalsIgnoreCase("ispDatabase")) {                doc = GeoIPDocumentCreator.createDocFromIspDb(serverIp, doc, reader);            } else if (usage.equalsIgnoreCase("insightsService")) {                doc = GeoIPDocumentCreator.createDocFromInsightsService(serverIp, doc, client);            }        } catch (Exception e) {                    }    }    return doc;}
1
public NutchDocument filter(NutchDocument doc, Parse parse, Text url, CrawlDatum datum, Inlinks inlinks) throws IndexingException
{        JexlContext jcontext = new MapContext();    jcontext.set("status", CrawlDatum.getStatusName(datum.getStatus()));    jcontext.set("fetchTime", (long) (datum.getFetchTime()));    jcontext.set("modifiedTime", (long) (datum.getModifiedTime()));    jcontext.set("retries", datum.getRetriesSinceFetch());    jcontext.set("interval", Integer.valueOf(datum.getFetchInterval()));    jcontext.set("score", datum.getScore());    jcontext.set("signature", StringUtil.toHexString(datum.getSignature()));    jcontext.set("url", url.toString());    jcontext.set("text", parse.getText());    jcontext.set("title", parse.getData().getTitle());    JexlContext httpStatusContext = new MapContext();    httpStatusContext.set("majorCode", parse.getData().getStatus().getMajorCode());    httpStatusContext.set("minorCode", parse.getData().getStatus().getMinorCode());    httpStatusContext.set("message", parse.getData().getStatus().getMessage());    jcontext.set("httpStatus", httpStatusContext);    jcontext.set("documentMeta", metadataToContext(doc.getDocumentMeta()));    jcontext.set("contentMeta", metadataToContext(parse.getData().getContentMeta()));    jcontext.set("parseMeta", metadataToContext(parse.getData().getParseMeta()));    JexlContext context = new MapContext();    for (Entry<String, NutchField> entry : doc) {        List<Object> values = entry.getValue().getValues();        context.set(entry.getKey(), values.size() > 1 ? values : values.get(0));    }    jcontext.set("doc", context);    try {        if (Boolean.TRUE.equals(expr.evaluate(jcontext))) {            return doc;        }    } catch (Exception e) {            }    return null;}
1
public void setConf(Configuration conf)
{    this.conf = conf;    String strExpr = conf.get("index.jexl.filter");    if (strExpr == null) {                throw new RuntimeException("The property index.jexl.filter must have a value when index-jexl-filter is used. You can use 'true' or 'false' to index all/none");    }    expr = JexlUtil.parseExpression(strExpr);    if (expr == null) {                throw new RuntimeException("Failed parsing JEXL from index.jexl.filter");    }}
1
public Configuration getConf()
{    return this.conf;}
0
private JexlContext metadataToContext(Metadata metadata)
{    JexlContext context = new MapContext();    for (String name : metadata.names()) {        String[] values = metadata.getValues(name);        context.set(name, values.length > 1 ? values : values[0]);    }    return context;}
0
public void testAllowMatchingDocument() throws Exception
{    Configuration conf = NutchConfiguration.create();    conf.set("index.jexl.filter", "doc.lang=='en'");    JexlIndexingFilter filter = new JexlIndexingFilter();    filter.setConf(conf);    Assert.assertNotNull(filter);    NutchDocument doc = new NutchDocument();    String title = "The Foo Page";    Outlink[] outlinks = new Outlink[] { new Outlink("http://foo.com/", "Foo") };    Metadata metaData = new Metadata();    metaData.add("Language", "en/us");    ParseData parseData = new ParseData(ParseStatus.STATUS_SUCCESS, title, outlinks, metaData);    ParseImpl parse = new ParseImpl("this is a sample foo bar page. hope you enjoy it.", parseData);    CrawlDatum crawlDatum = new CrawlDatum();    crawlDatum.setFetchTime(100L);    Inlinks inlinks = new Inlinks();    doc.add("lang", "en");    NutchDocument result = filter.filter(doc, parse, new Text("http://nutch.apache.org/index.html"), crawlDatum, inlinks);    Assert.assertNotNull(result);    Assert.assertEquals(doc, result);}
0
public void testBlockNotMatchingDocuments() throws Exception
{    Configuration conf = NutchConfiguration.create();    conf.set("index.jexl.filter", "doc.lang=='en'");    JexlIndexingFilter filter = new JexlIndexingFilter();    filter.setConf(conf);    Assert.assertNotNull(filter);    NutchDocument doc = new NutchDocument();    String title = "The Foo Page";    Outlink[] outlinks = new Outlink[] { new Outlink("http://foo.com/", "Foo") };    Metadata metaData = new Metadata();    metaData.add("Language", "en/us");    ParseData parseData = new ParseData(ParseStatus.STATUS_SUCCESS, title, outlinks, metaData);    ParseImpl parse = new ParseImpl("this is a sample foo bar page. hope you enjoy it.", parseData);    CrawlDatum crawlDatum = new CrawlDatum();    crawlDatum.setFetchTime(100L);    Inlinks inlinks = new Inlinks();    doc.add("lang", "ru");    NutchDocument result = filter.filter(doc, parse, new Text("http://nutch.apache.org/index.html"), crawlDatum, inlinks);    Assert.assertNull(result);}
0
public void testMissingConfiguration() throws Exception
{    Configuration conf = NutchConfiguration.create();    JexlIndexingFilter filter = new JexlIndexingFilter();    thrown.expect(RuntimeException.class);    filter.setConf(conf);}
0
public void testInvalidExpression() throws Exception
{    Configuration conf = NutchConfiguration.create();    conf.set("index.jexl.filter", "doc.lang=<>:='en'");    JexlIndexingFilter filter = new JexlIndexingFilter();    thrown.expect(RuntimeException.class);    filter.setConf(conf);}
0
public NutchDocument filter(NutchDocument doc, Parse parse, Text url, CrawlDatum datum, Inlinks inlinks) throws IndexingException
{        Outlink[] outlinks = parse.getData().getOutlinks();    if (outlinks != null) {        Set<String> hosts = new HashSet<String>();        for (Outlink outlink : outlinks) {            try {                String linkUrl = outlink.getToUrl();                String outHost = new URL(linkUrl).getHost().toLowerCase();                if (indexHost) {                    linkUrl = outHost;                    if (hosts.contains(linkUrl))                        continue;                    hosts.add(linkUrl);                }                addFilteredLink("outlinks", url.toString(), linkUrl, outHost, filterOutlinks, doc);            } catch (MalformedURLException e) {                            }        }    }        if (null != inlinks) {        Iterator<Inlink> iterator = inlinks.iterator();        Set<String> inlinkHosts = new HashSet<String>();        while (iterator.hasNext()) {            try {                Inlink link = iterator.next();                String linkUrl = link.getFromUrl();                String inHost = new URL(linkUrl).getHost().toLowerCase();                if (indexHost) {                    linkUrl = inHost;                    if (inlinkHosts.contains(linkUrl))                        continue;                    inlinkHosts.add(linkUrl);                }                addFilteredLink("inlinks", url.toString(), linkUrl, inHost, filterInlinks, doc);            } catch (MalformedURLException e) {                            }        }    }    return doc;}
1
private void addFilteredLink(String fieldName, String url, String linkUrl, String urlHost, boolean filter, NutchDocument doc) throws MalformedURLException
{    if (filter) {        String host = new URL(url.toString()).getHost().toLowerCase();        if (!host.equalsIgnoreCase(urlHost)) {            doc.add(fieldName, linkUrl);        }    } else {        doc.add(fieldName, linkUrl);    }}
0
public void setConf(Configuration conf)
{    this.conf = conf;    filterOutlinks = conf.getBoolean(LINKS_OUTLINKS_HOST, false);    filterInlinks = conf.getBoolean(LINKS_INLINKS_HOST, false);    indexHost = conf.getBoolean(LINKS_ONLY_HOSTS, false);}
0
public Configuration getConf()
{    return this.conf;}
0
public void setUp() throws Exception
{    metadata.add(Response.CONTENT_TYPE, "text/html");}
0
private Outlink[] generateOutlinks() throws Exception
{    return generateOutlinks(false);}
0
private Outlink[] generateOutlinks(boolean parts) throws Exception
{    Outlink[] outlinks = new Outlink[2];    outlinks[0] = new Outlink("http://www.test.com", "test");    outlinks[1] = new Outlink("http://www.example.com", "example");    if (parts) {        outlinks[0] = new Outlink(outlinks[0].getToUrl() + "/index.php?param=1", "test");        outlinks[1] = new Outlink(outlinks[1].getToUrl() + "/index.php?param=2", "test");    }    return outlinks;}
0
public void testFilterOutlinks() throws Exception
{    conf.set(LinksIndexingFilter.LINKS_OUTLINKS_HOST, "true");    filter.setConf(conf);    Outlink[] outlinks = generateOutlinks();    NutchDocument doc = filter.filter(new NutchDocument(), new ParseImpl("text", new ParseData(new ParseStatus(), "title", outlinks, metadata)), new Text("http://www.example.com/"), new CrawlDatum(), new Inlinks());    Assert.assertEquals(1, doc.getField("outlinks").getValues().size());    Assert.assertEquals("Filter outlinks, allow only those from a different host", outlinks[0].getToUrl(), doc.getFieldValue("outlinks"));}
0
public void testFilterInlinks() throws Exception
{    conf.set(LinksIndexingFilter.LINKS_INLINKS_HOST, "true");    filter.setConf(conf);    Inlinks inlinks = new Inlinks();    inlinks.add(new Inlink("http://www.test.com", "test"));    inlinks.add(new Inlink("http://www.example.com", "example"));    NutchDocument doc = filter.filter(new NutchDocument(), new ParseImpl("text", new ParseData(new ParseStatus(), "title", new Outlink[0], metadata)), new Text("http://www.example.com/"), new CrawlDatum(), inlinks);    Assert.assertEquals(1, doc.getField("inlinks").getValues().size());    Assert.assertEquals("Filter inlinks, allow only those from a different host", "http://www.test.com", doc.getFieldValue("inlinks"));}
0
public void testNoFilterOutlinks() throws Exception
{    filter.setConf(conf);    Outlink[] outlinks = generateOutlinks();    NutchDocument doc = filter.filter(new NutchDocument(), new ParseImpl("text", new ParseData(new ParseStatus(), "title", outlinks, metadata)), new Text("http://www.example.com/"), new CrawlDatum(), new Inlinks());    Assert.assertEquals("All outlinks must be indexed even those from the same host", outlinks.length, doc.getField("outlinks").getValues().size());}
0
public void testNoFilterInlinks() throws Exception
{    conf.set(LinksIndexingFilter.LINKS_INLINKS_HOST, "false");    filter.setConf(conf);    Inlinks inlinks = new Inlinks();    inlinks.add(new Inlink("http://www.test.com", "test"));    inlinks.add(new Inlink("http://www.example.com", "example"));    NutchDocument doc = filter.filter(new NutchDocument(), new ParseImpl("text", new ParseData(new ParseStatus(), "title", new Outlink[0], metadata)), new Text("http://www.example.com/"), new CrawlDatum(), inlinks);    Assert.assertEquals("All inlinks must be indexed even those from the same host", inlinks.size(), doc.getField("inlinks").getValues().size());}
0
public void testIndexOnlyHostPart() throws Exception
{    conf.set(LinksIndexingFilter.LINKS_INLINKS_HOST, "true");    conf.set(LinksIndexingFilter.LINKS_OUTLINKS_HOST, "true");    conf.set(LinksIndexingFilter.LINKS_ONLY_HOSTS, "true");    filter.setConf(conf);    Outlink[] outlinks = generateOutlinks(true);    Inlinks inlinks = new Inlinks();    inlinks.add(new Inlink("http://www.test.com/one-awesome-page", "test"));    inlinks.add(new Inlink("http://www.test.com/other-awesome-page", "test"));    inlinks.add(new Inlink("http://www.example.com/my-first-awesome-example", "example"));    NutchDocument doc = filter.filter(new NutchDocument(), new ParseImpl("text", new ParseData(new ParseStatus(), "title", outlinks, metadata)), new Text("http://www.example.com/"), new CrawlDatum(), inlinks);    NutchField docOutlinks = doc.getField("outlinks");    Assert.assertEquals("Only the host portion of the outlink URL must be indexed", new URL("http://www.test.com").getHost(), docOutlinks.getValues().get(0));    Assert.assertEquals("The inlinks coming from the same host must count only once", 1, doc.getField("inlinks").getValues().size());    Assert.assertEquals("Only the host portion of the inlinks URL must be indexed", new URL("http://www.test.com").getHost(), doc.getFieldValue("inlinks"));}
0
public void testIndexHostsOnlyAndFilterOutlinks() throws Exception
{    conf = NutchConfiguration.create();    conf.set(LinksIndexingFilter.LINKS_ONLY_HOSTS, "true");    conf.set(LinksIndexingFilter.LINKS_OUTLINKS_HOST, "true");    Outlink[] outlinks = generateOutlinks(true);    filter.setConf(conf);    NutchDocument doc = filter.filter(new NutchDocument(), new ParseImpl("text", new ParseData(new ParseStatus(), "title", outlinks, metadata)), new Text("http://www.example.com/"), new CrawlDatum(), new Inlinks());    Assert.assertEquals(1, doc.getField("outlinks").getValues().size());    Assert.assertEquals("Index only the host portion of the outlinks after filtering", new URL("http://www.test.com").getHost(), doc.getFieldValue("outlinks"));}
0
public void testIndexHostsOnlyAndFilterInlinks() throws Exception
{    conf = NutchConfiguration.create();    conf.set(LinksIndexingFilter.LINKS_ONLY_HOSTS, "true");    conf.set(LinksIndexingFilter.LINKS_INLINKS_HOST, "true");    filter.setConf(conf);    Inlinks inlinks = new Inlinks();    inlinks.add(new Inlink("http://www.test.com", "test"));    inlinks.add(new Inlink("http://www.example.com", "example"));    NutchDocument doc = filter.filter(new NutchDocument(), new ParseImpl("text", new ParseData(new ParseStatus(), "title", new Outlink[0], metadata)), new Text("http://www.example.com/"), new CrawlDatum(), inlinks);    Assert.assertEquals(1, doc.getField("inlinks").getValues().size());    Assert.assertEquals("Index only the host portion of the inlinks after filtering", new URL("http://www.test.com").getHost(), doc.getFieldValue("inlinks"));}
0
public NutchDocument filter(NutchDocument doc, Parse parse, Text url, CrawlDatum datum, Inlinks inlinks) throws IndexingException
{        if (doc == null)        return doc;        if (dbFieldnames != null) {        for (String metatag : dbFieldnames) {            Writable metadata = datum.getMetaData().get(new Text(metatag));            if (metadata != null)                add(doc, metatag, metadata.toString());        }    }        if (parseFieldnames != null) {        for (String metatag : parseFieldnames.keySet()) {            for (String value : parse.getData().getParseMeta().getValues(metatag)) {                if (value != null)                    add(doc, parseFieldnames.get(metatag), value);            }        }    }        if (contentFieldnames != null) {        for (String metatag : contentFieldnames) {            for (String value : parse.getData().getContentMeta().getValues(metatag)) {                if (value != null)                    add(doc, metatag, value);            }        }    }    return doc;}
0
protected void add(NutchDocument doc, String key, String value)
{    if (separator == null || value.indexOf(separator) == -1 || !mvFields.contains(key)) {        value = value.trim();        if (!value.isEmpty()) {            doc.add(key, value);        }    } else {        String[] parts = value.split(separator);        for (String part : parts) {            part = part.trim();            if (!part.isEmpty()) {                doc.add(key, part);            }        }    }}
0
public void setConf(Configuration conf)
{    this.conf = conf;    dbFieldnames = conf.getStrings(db_CONF_PROPERTY);    parseFieldnames = new HashMap<String, String>();    for (String metatag : conf.getStrings(parse_CONF_PROPERTY)) {        parseFieldnames.put(metatag.toLowerCase(Locale.ROOT), metatag);    }    contentFieldnames = conf.getStrings(content_CONF_PROPERTY);    separator = conf.get(separator_CONF_PROPERTY, null);    mvFields = new HashSet(Arrays.asList(conf.getStrings(mvfields_CONF_PROPERTY, new String[0])));}
0
public Configuration getConf()
{    return this.conf;}
0
public NutchDocument filter(NutchDocument doc, Parse parse, Text url, CrawlDatum datum, Inlinks inlinks) throws IndexingException
{    String url_s = url.toString();    addTime(doc, parse.getData(), url_s, datum);    addLength(doc, parse.getData(), url_s);    addType(doc, parse.getData(), url_s, datum);    resetTitle(doc, parse.getData(), url_s);    return doc;}
0
private NutchDocument addTime(NutchDocument doc, ParseData data, String url, CrawlDatum datum)
{    long time = -1;    String lastModified = data.getMeta(Metadata.LAST_MODIFIED);    if (lastModified != null) {                        time = getTime(lastModified, url);                if (time > -1) {            doc.add("lastModified", new Date(time));        }    }    if (time == -1) {                        time = datum.getModifiedTime();        if (time <= 0) {                                    time = datum.getFetchTime();                }    }        doc.add("date", new Date(time));    return doc;}
0
private long getTime(String date, String url)
{    long time = -1;    try {        time = HttpDateFormat.toLong(date);    } catch (ParseException e) {                try {            Date parsedDate = DateUtils.parseDate(date, new String[] { "EEE MMM dd HH:mm:ss yyyy", "EEE MMM dd HH:mm:ss yyyy zzz", "EEE MMM dd HH:mm:ss zzz yyyy", "EEE, MMM dd HH:mm:ss yyyy zzz", "EEE, dd MMM yyyy HH:mm:ss zzz", "EEE,dd MMM yyyy HH:mm:ss zzz", "EEE, dd MMM yyyy HH:mm:sszzz", "EEE, dd MMM yyyy HH:mm:ss", "EEE, dd-MMM-yy HH:mm:ss zzz", "yyyy/MM/dd HH:mm:ss.SSS zzz", "yyyy/MM/dd HH:mm:ss.SSS", "yyyy/MM/dd HH:mm:ss zzz", "yyyy/MM/dd", "yyyy.MM.dd HH:mm:ss", "yyyy-MM-dd HH:mm", "MMM dd yyyy HH:mm:ss. zzz", "MMM dd yyyy HH:mm:ss zzz", "dd.MM.yyyy HH:mm:ss zzz", "dd MM yyyy HH:mm:ss zzz", "dd.MM.yyyy; HH:mm:ss", "dd.MM.yyyy HH:mm:ss", "dd.MM.yyyy zzz", "yyyy-MM-dd'T'HH:mm:ssXXX" });            time = parsedDate.getTime();                                } catch (Exception e2) {            if (LOG.isWarnEnabled()) {                            }        }    }    return time;}
1
private NutchDocument addLength(NutchDocument doc, ParseData data, String url)
{    String contentLength = data.getMeta(Response.CONTENT_LENGTH);    if (contentLength != null) {                String trimmed = contentLength.toString().trim();        if (!trimmed.isEmpty())            doc.add("contentLength", trimmed);    }    return doc;}
0
private NutchDocument addType(NutchDocument doc, ParseData data, String url, CrawlDatum datum)
{    String mimeType = null;    String contentType = null;    Writable tcontentType = datum.getMetaData().get(new Text(Response.CONTENT_TYPE));    if (tcontentType != null) {        contentType = tcontentType.toString();    } else        contentType = data.getMeta(Response.CONTENT_TYPE);    if (contentType == null) {                                                                                                mimeType = tika.detect(url);    } else {        mimeType = MIME.forName(MimeUtil.cleanMimeType(contentType));    }        if (mimeType == null) {        return doc;    }        if (mapMimes) {                if (mimeMap.containsKey(mimeType)) {            if (mapFieldName != null) {                doc.add(mapFieldName, mimeMap.get(mimeType));            } else {                mimeType = mimeMap.get(mimeType);            }        }    }    contentType = mimeType;    doc.add("type", contentType);        if (conf.getBoolean("moreIndexingFilter.indexMimeTypeParts", true)) {        String[] parts = getParts(contentType);        for (String part : parts) {            doc.add("type", part);        }    }    return doc;}
0
 static String[] getParts(String mimeType)
{    return mimeType.split("/");}
0
private NutchDocument resetTitle(NutchDocument doc, ParseData data, String url)
{    String contentDisposition = data.getMeta(Metadata.CONTENT_DISPOSITION);    if (contentDisposition == null || doc.getFieldValue("title") != null)        return doc;    for (int i = 0; i < patterns.length; i++) {        Matcher matcher = patterns[i].matcher(contentDisposition);        if (matcher.find()) {            doc.add("title", matcher.group(1));            break;        }    }    return doc;}
0
public void setConf(Configuration conf)
{    this.conf = conf;    MIME = new MimeUtil(conf);    if (conf.getBoolean("moreIndexingFilter.mapMimeTypes", false)) {        mapMimes = true;        mapFieldName = conf.get("moreIndexingFilter.mapMimeTypes.field");                try {            readConfiguration();        } catch (Exception e) {                    }    }}
1
public Configuration getConf()
{    return this.conf;}
0
private void readConfiguration() throws IOException
{        BufferedReader reader = new BufferedReader(conf.getConfResourceAsReader("contenttype-mapping.txt"));    String line;    String[] parts;    boolean formatWarningShown = false;    mimeMap = new HashMap<String, String>();    while ((line = reader.readLine()) != null) {        if (StringUtils.isNotBlank(line) && !line.startsWith("#")) {            line = line.trim();            parts = line.split("\t");                        if (parts.length > 1) {                for (int i = 1; i < parts.length; i++) {                    mimeMap.put(parts[i].trim(), parts[0].trim());                }            } else {                                if (!formatWarningShown) {                                        formatWarningShown = true;                }            }        }    }}
1
public void testContentType() throws IndexingException
{    Configuration conf = NutchConfiguration.create();    assertContentType(conf, "text/html", "text/html");    assertContentType(conf, "text/html; charset=UTF-8", "text/html");}
0
public void testGetParts()
{    String[] parts = MoreIndexingFilter.getParts("text/html");    assertParts(parts, 2, "text", "html");}
0
public void testNoParts()
{    Configuration conf = NutchConfiguration.create();    conf.setBoolean("moreIndexingFilter.indexMimeTypeParts", false);    MoreIndexingFilter filter = new MoreIndexingFilter();    filter.setConf(conf);    Assert.assertNotNull(filter);    NutchDocument doc = new NutchDocument();    ParseImpl parse = new ParseImpl("foo bar", new ParseData());    try {        filter.filter(doc, parse, new Text("http://nutch.apache.org/index.html"), new CrawlDatum(), new Inlinks());    } catch (Exception e) {        e.printStackTrace();        Assert.fail(e.getMessage());    }    Assert.assertNotNull(doc);    Assert.assertTrue(doc.getFieldNames().contains("type"));    Assert.assertEquals(1, doc.getField("type").getValues().size());    Assert.assertEquals("text/html", doc.getFieldValue("type"));}
0
public void testContentDispositionTitle() throws IndexingException
{    Configuration conf = NutchConfiguration.create();    Metadata metadata = new Metadata();    metadata.add(Response.CONTENT_DISPOSITION, "filename=filename.ext");    MoreIndexingFilter filter = new MoreIndexingFilter();    filter.setConf(conf);    Text url = new Text("http://www.example.com/");    ParseImpl parseImpl = new ParseImpl("text", new ParseData(new ParseStatus(), "title", new Outlink[0], metadata));    NutchDocument doc = new NutchDocument();    doc = filter.filter(doc, parseImpl, url, new CrawlDatum(), new Inlinks());    Assert.assertEquals("content-disposition not detected", "filename.ext", doc.getFieldValue("title"));    /* NUTCH-1140: do not add second title to avoid a multi-valued title field */    doc = new NutchDocument();    doc.add("title", "title");    doc = filter.filter(doc, parseImpl, url, new CrawlDatum(), new Inlinks());    Assert.assertEquals("do not add second title by content-disposition", "title", doc.getFieldValue("title"));}
0
private void assertParts(String[] parts, int count, String... expected)
{    Assert.assertEquals(count, parts.length);    for (int i = 0; i < expected.length; i++) {        Assert.assertEquals(expected[i], parts[i]);    }}
0
private void assertContentType(Configuration conf, String source, String expected) throws IndexingException
{    Metadata metadata = new Metadata();    metadata.add(Response.CONTENT_TYPE, source);    MoreIndexingFilter filter = new MoreIndexingFilter();    filter.setConf(conf);    NutchDocument doc = filter.filter(new NutchDocument(), new ParseImpl("text", new ParseData(new ParseStatus(), "title", new Outlink[0], metadata)), new Text("http://www.example.com/"), new CrawlDatum(), new Inlinks());    Assert.assertEquals("mime type not detected", expected, doc.getFieldValue("type"));}
0
public void testDates() throws IndexingException
{    Configuration conf = NutchConfiguration.create();    Metadata metadata = new Metadata();    MoreIndexingFilter filter = new MoreIndexingFilter();    filter.setConf(conf);    Text url = new Text("http://www.example.com/");    ParseImpl parseImpl = new ParseImpl("text", new ParseData(new ParseStatus(), "title", new Outlink[0], metadata));    CrawlDatum fetchDatum = new CrawlDatum();    NutchDocument doc = new NutchDocument();        long dateEpocheSeconds = 1537898340;    fetchDatum.setModifiedTime(dateEpocheSeconds * 1000);        fetchDatum.setFetchTime((dateEpocheSeconds + 30 * 24 * 60 * 60) * 1000);                doc = filter.filter(doc, parseImpl, url, fetchDatum, new Inlinks());    Assert.assertEquals("last fetch date not extracted", new Date(dateEpocheSeconds * 1000), doc.getFieldValue("date"));        Date lastModifiedDate = new Date((dateEpocheSeconds - 7 * 24 * 60 * 60) * 1000);    String lastModifiedDateStr = DateTimeFormatter.ISO_INSTANT.format(lastModifiedDate.toInstant());    parseImpl.getData().getParseMeta().set(Metadata.LAST_MODIFIED, lastModifiedDateStr);    doc = filter.filter(doc, parseImpl, url, fetchDatum, new Inlinks());    Assert.assertEquals("last-modified date not extracted", lastModifiedDate, doc.getFieldValue("lastModified"));}
0
public String getFieldName()
{    return this.fieldName;}
0
public String getToFieldName()
{    return this.toFieldName;}
0
public Pattern getPattern()
{    return this.pattern;}
0
public String getReplacement()
{    return this.replacement;}
0
public boolean isValid()
{    return this.isValid;}
0
public String replace(String value)
{    if (this.isValid) {        return this.pattern.matcher(value).replaceAll(replacement);    } else {        return value;    }}
0
public String checkAndReplace(String fieldName, String value)
{    if (this.fieldName.equals(fieldName)) {        if (value != null && value.length() > 0) {            if (this.isValid) {                Matcher m = this.pattern.matcher(value);                if (m.find()) {                    return m.replaceAll(this.replacement);                }            }        }    }    return null;}
0
public void setConf(Configuration conf)
{    this.conf = conf;    FIELDREPLACERS_BY_HOST.clear();    FIELDREPLACERS_BY_URL.clear();    String value = conf.get("index.replace.regexp", null);    if (value != null) {                this.parseConf(value);    }}
1
public Configuration getConf()
{    return this.conf;}
0
private void parseConf(String propertyValue)
{    if (propertyValue == null || propertyValue.trim().length() == 0) {        return;    }        Pattern hostPattern = Pattern.compile(".*");    Pattern urlPattern = null;        Matcher lineMatcher = LINE_SPLIT.matcher(propertyValue);    while (lineMatcher.find()) {        String line = lineMatcher.group();        if (line != null && line.length() > 0) {                        Matcher nameValueMatcher = NAME_VALUE_SPLIT.matcher(line.trim());            if (nameValueMatcher.find()) {                String fieldName = nameValueMatcher.group(1).trim();                String value = nameValueMatcher.group(2);                if (fieldName != null && value != null) {                                        if (HOSTMATCH.equals(fieldName)) {                        urlPattern = null;                        try {                            hostPattern = Pattern.compile(value);                        } catch (PatternSyntaxException pse) {                                                                                    hostPattern = Pattern.compile("willnotmatchanyhost");                        }                    } else if (URLMATCH.equals(fieldName)) {                        try {                            urlPattern = Pattern.compile(value);                        } catch (PatternSyntaxException pse) {                                                                                    urlPattern = Pattern.compile("willnotmatchanyurl");                        }                    } else if (value.length() > 3) {                        String toFieldName = fieldName;                                                if (fieldName.indexOf(':') > 0) {                            toFieldName = fieldName.substring(fieldName.indexOf(':') + 1);                            fieldName = fieldName.substring(0, fieldName.indexOf(':'));                        }                        String sep = value.substring(0, 1);                                                value = value.substring(1);                        if (!value.contains(sep)) {                                                        continue;                        }                        String pattern = value.substring(0, value.indexOf(sep));                        value = value.substring(pattern.length() + 1);                        String replacement = value;                        if (value.contains(sep)) {                            replacement = value.substring(0, value.indexOf(sep));                        }                        int flags = 0;                        if (value.length() > replacement.length() + 1) {                            value = value.substring(replacement.length() + 1).trim();                            try {                                flags = Integer.parseInt(value);                            } catch (NumberFormatException e) {                                                                continue;                            }                        }                        Integer iFlags = (flags > 0) ? Integer.valueOf(flags) : null;                                                FieldReplacer fr = new FieldReplacer(fieldName, toFieldName, pattern, replacement, iFlags);                                                if (urlPattern != null) {                            List<FieldReplacer> lfp = FIELDREPLACERS_BY_URL.get(urlPattern);                            if (lfp == null) {                                lfp = new ArrayList<FieldReplacer>();                            }                            lfp.add(fr);                            FIELDREPLACERS_BY_URL.put(urlPattern, lfp);                        } else {                            List<FieldReplacer> lfp = FIELDREPLACERS_BY_HOST.get(hostPattern);                            if (lfp == null) {                                lfp = new ArrayList<FieldReplacer>();                            }                            lfp.add(fr);                            FIELDREPLACERS_BY_HOST.put(hostPattern, lfp);                        }                    }                }            }        }    }}
1
public NutchDocument filter(NutchDocument doc, Parse parse, Text url, CrawlDatum datum, Inlinks inlinks) throws IndexingException
{    if (doc != null) {        if (FIELDREPLACERS_BY_HOST.size() > 0) {            this.doReplace(doc, "host", FIELDREPLACERS_BY_HOST);        }        if (FIELDREPLACERS_BY_URL.size() > 0) {            this.doReplace(doc, "url", FIELDREPLACERS_BY_URL);        }    }    return doc;}
0
private void doReplace(NutchDocument doc, String keyName, Map<Pattern, List<FieldReplacer>> replaceMap)
{    if (doc == null || replaceMap.size() == 0) {        return;    }    Collection<String> docFieldNames = doc.getFieldNames();    NutchField keyField = doc.getField(keyName);    if (keyField == null) {                return;    }    List<Object> keyFieldValues = keyField.getValues();    if (keyFieldValues.size() == 0) {                return;    }        for (Object oKeyFieldValue : keyFieldValues) {        if (oKeyFieldValue != null && oKeyFieldValue instanceof java.lang.String) {            String keyFieldValue = (String) oKeyFieldValue;                        for (Map.Entry<Pattern, List<FieldReplacer>> entries : replaceMap.entrySet()) {                                if (entries.getKey().matcher(keyFieldValue).find()) {                                        for (FieldReplacer fp : entries.getValue()) {                        String fieldName = fp.getFieldName();                                                if (docFieldNames.contains(fieldName)) {                            NutchField docField = doc.getField(fieldName);                            List<Object> fieldValues = docField.getValues();                            ArrayList<String> newFieldValues = new ArrayList<String>();                                                        for (Object oFieldValue : fieldValues) {                                if (oFieldValue != null && oFieldValue instanceof java.lang.String) {                                    String fieldValue = (String) oFieldValue;                                    String newValue = fp.replace(fieldValue);                                    newFieldValues.add(newValue);                                }                            }                                                        String targetFieldName = fp.getToFieldName();                            doc.removeField(targetFieldName);                            for (String newFieldValue : newFieldValues) {                                doc.add(targetFieldName, newFieldValue);                            }                        }                    }                }            }        }    }}
0
public NutchDocument parseAndFilterFile(String fileName, Configuration conf)
{    NutchDocument doc = new NutchDocument();    BasicIndexingFilter basicIndexer = new BasicIndexingFilter();    basicIndexer.setConf(conf);    Assert.assertNotNull(basicIndexer);    MetadataIndexer metaIndexer = new MetadataIndexer();    metaIndexer.setConf(conf);    Assert.assertNotNull(basicIndexer);    ReplaceIndexer replaceIndexer = new ReplaceIndexer();    replaceIndexer.setConf(conf);    Assert.assertNotNull(replaceIndexer);    try {        String urlString = "file:" + sampleDir + fileSeparator + fileName;        Text text = new Text(urlString);        CrawlDatum crawlDatum = new CrawlDatum();        Protocol protocol = new ProtocolFactory(conf).getProtocol(urlString);        Content content = protocol.getProtocolOutput(text, crawlDatum).getContent();        Parse parse = new ParseUtil(conf).parse(content).get(content.getUrl());        crawlDatum.setFetchTime(100L);        Inlinks inlinks = new Inlinks();        doc = basicIndexer.filter(doc, parse, text, crawlDatum, inlinks);        doc = metaIndexer.filter(doc, parse, text, crawlDatum, inlinks);        doc = replaceIndexer.filter(doc, parse, text, crawlDatum, inlinks);    } catch (Exception e) {        e.printStackTrace();        Assert.fail(e.toString());    }    return doc;}
0
public void testPropertyParse()
{    Configuration conf = NutchConfiguration.create();    String indexReplaceProperty = "  metatag.description=/this(.*)plugin/this awesome plugin/2\n" + "  metatag.keywords=/\\,/\\!/\n" + "  hostmatch=.*.com\n" + "  metatag.keywords=/\\,/\\?/\n" + "  metatag.author:dc_author=/\\s+/ David /\n" + "  urlmatch=.*.html\n" + "  metatag.keywords=/\\,/\\./\n" + "  metatag.author=/\\s+/ D. /\n";    conf.set(INDEX_REPLACE_PROPERTY, indexReplaceProperty);    ReplaceIndexer rp = new ReplaceIndexer();    try {        rp.setConf(conf);    } catch (RuntimeException ohno) {        Assert.fail("Unable to parse a valid index.replace.regexp property! " + ohno.getMessage());    }    Configuration parsedConf = rp.getConf();        Assert.assertEquals(indexReplaceProperty, parsedConf.get(INDEX_REPLACE_PROPERTY));}
0
public void testGlobalReplacement()
{    String expectedDescription = "With this awesome plugin, I control the description! Bwuhuhuhaha!";    String expectedKeywords = "Breathtaking! Riveting! Two Thumbs Up!";    String expectedAuthor = "Peter D. Ciuffetti";    String indexReplaceProperty = "  metatag.description=/this(.*)plugin/this awesome plugin/\n" + "  metatag.keywords=/\\,/\\!/\n" + "  metatag.author=/\\s+/ D. /\n";    Configuration conf = NutchConfiguration.create();    conf.set("plugin.includes", "protocol-file|urlfilter-regex|parse-(html|metatags)|index-(basic|anchor|metadata|static|replace)|urlnormalizer-(pass|regex|basic)");    conf.set(INDEX_REPLACE_PROPERTY, indexReplaceProperty);    conf.set("metatags.names", "author,description,keywords");    conf.set("index.parse.md", "metatag.author,metatag.description,metatag.keywords");        conf.set("http.timeout", "99999999999");        NutchDocument doc = parseAndFilterFile(sampleFile, conf);    Assert.assertEquals(expectedDescription, doc.getFieldValue("metatag.description"));    Assert.assertEquals(expectedKeywords, doc.getFieldValue("metatag.keywords"));    Assert.assertEquals(expectedAuthor, doc.getFieldValue("metatag.author"));}
0
public void testInvalidPatterns()
{    String expectedDescription = "With this plugin, I control the description! Bwuhuhuhaha!";    String expectedKeywords = "Breathtaking, Riveting, Two Thumbs Up!";    String expectedAuthor = "Peter Ciuffetti";        String indexReplaceProperty = "  metatag.description=/this\\s+**plugin/this awesome plugin/\n" + "  metatag.keywords=/\\,/\\!/what\n" + " metatag.author=#notcomplete";    Configuration conf = NutchConfiguration.create();    conf.set("plugin.includes", "protocol-file|urlfilter-regex|parse-(html|metatags)|index-(basic|anchor|metadata|static|replace)|urlnormalizer-(pass|regex|basic)");    conf.set(INDEX_REPLACE_PROPERTY, indexReplaceProperty);    conf.set("metatags.names", "author,description,keywords");    conf.set("index.parse.md", "metatag.author,metatag.description,metatag.keywords");        conf.set("http.timeout", "99999999999");        NutchDocument doc = parseAndFilterFile(sampleFile, conf);        Assert.assertEquals(expectedDescription, doc.getFieldValue("metatag.description"));    Assert.assertEquals(expectedKeywords, doc.getFieldValue("metatag.keywords"));    Assert.assertEquals(expectedAuthor, doc.getFieldValue("metatag.author"));}
0
public void testUrlMatchesPattern()
{    String expectedDescription = "With this awesome plugin, I control the description! Bwuhuhuhaha!";    String expectedKeywords = "Breathtaking! Riveting! Two Thumbs Up!";    String expectedAuthor = "Peter D. Ciuffetti";    String indexReplaceProperty = " urlmatch=.*.html\n" + "  metatag.description=/this(.*)plugin/this awesome plugin/\n" + "  metatag.keywords=/\\,/\\!/\n" + "  metatag.author=/\\s+/ D. /\n";    Configuration conf = NutchConfiguration.create();    conf.set("plugin.includes", "protocol-file|urlfilter-regex|parse-(html|metatags)|index-(basic|anchor|metadata|static|replace)|urlnormalizer-(pass|regex|basic)");    conf.set(INDEX_REPLACE_PROPERTY, indexReplaceProperty);    conf.set("metatags.names", "author,description,keywords");    conf.set("index.parse.md", "metatag.author,metatag.description,metatag.keywords");        conf.set("http.timeout", "99999999999");        NutchDocument doc = parseAndFilterFile(sampleFile, conf);        Assert.assertEquals(expectedDescription, doc.getFieldValue("metatag.description"));    Assert.assertEquals(expectedKeywords, doc.getFieldValue("metatag.keywords"));    Assert.assertEquals(expectedAuthor, doc.getFieldValue("metatag.author"));}
0
public void testUrlNotMatchesPattern()
{    String expectedDescription = "With this plugin, I control the description! Bwuhuhuhaha!";    String expectedKeywords = "Breathtaking, Riveting, Two Thumbs Up!";    String expectedAuthor = "Peter Ciuffetti";    String indexReplaceProperty = " urlmatch=.*.xml\n" + "  metatag.description=/this(.*)plugin/this awesome plugin/\n" + "  metatag.keywords=/\\,/\\!/\n" + "  metatag.author=/\\s+/ D. /\n";    Configuration conf = NutchConfiguration.create();    conf.set("plugin.includes", "protocol-file|urlfilter-regex|parse-(html|metatags)|index-(basic|anchor|metadata|static|replace)|urlnormalizer-(pass|regex|basic)");    conf.set(INDEX_REPLACE_PROPERTY, indexReplaceProperty);    conf.set("metatags.names", "author,description,keywords");    conf.set("index.parse.md", "metatag.author,metatag.description,metatag.keywords");        conf.set("http.timeout", "99999999999");        NutchDocument doc = parseAndFilterFile(sampleFile, conf);        Assert.assertEquals(expectedDescription, doc.getFieldValue("metatag.description"));    Assert.assertEquals(expectedKeywords, doc.getFieldValue("metatag.keywords"));    Assert.assertEquals(expectedAuthor, doc.getFieldValue("metatag.author"));}
0
public void testGlobalAndUrlMatchesPattern()
{    String expectedDescription = "With this awesome plugin, I control the description! Bwuhuhuhaha!";    String expectedKeywords = "Breathtaking! Riveting! Two Thumbs Up!";    String expectedAuthor = "Peter D. Ciuffetti";    String indexReplaceProperty = "  metatag.description=/this(.*)plugin/this$1awesome$1plugin/\n" + "  urlmatch=.*.html\n" + "  metatag.keywords=/\\,/\\!/\n" + "  metatag.author=/\\s+/ D. /\n";    Configuration conf = NutchConfiguration.create();    conf.set("plugin.includes", "protocol-file|urlfilter-regex|parse-(html|metatags)|index-(basic|anchor|metadata|static|replace)|urlnormalizer-(pass|regex|basic)");    conf.set(INDEX_REPLACE_PROPERTY, indexReplaceProperty);    conf.set("metatags.names", "author,description,keywords");    conf.set("index.parse.md", "metatag.author,metatag.description,metatag.keywords");        conf.set("http.timeout", "99999999999");        NutchDocument doc = parseAndFilterFile(sampleFile, conf);        Assert.assertEquals(expectedDescription, doc.getFieldValue("metatag.description"));    Assert.assertEquals(expectedKeywords, doc.getFieldValue("metatag.keywords"));    Assert.assertEquals(expectedAuthor, doc.getFieldValue("metatag.author"));}
0
public void testGlobalAndUrlNotMatchesPattern()
{    String expectedDescription = "With this awesome plugin, I control the description! Bwuhuhuhaha!";    String expectedKeywords = "Breathtaking, Riveting, Two Thumbs Up!";    String expectedAuthor = "Peter Ciuffetti";    String indexReplaceProperty = "  metatag.description=/this(.*)plugin/this$1awesome$1plugin/\n" + "  urlmatch=.*.xml\n" + "  metatag.keywords=/\\,/\\!/\n" + "  metatag.author=/\\s+/ D. /\n";    Configuration conf = NutchConfiguration.create();    conf.set("plugin.includes", "protocol-file|urlfilter-regex|parse-(html|metatags)|index-(basic|anchor|metadata|static|replace)|urlnormalizer-(pass|regex|basic)");    conf.set(INDEX_REPLACE_PROPERTY, indexReplaceProperty);    conf.set("metatags.names", "author,description,keywords");    conf.set("index.parse.md", "metatag.author,metatag.description,metatag.keywords");        conf.set("http.timeout", "99999999999");        NutchDocument doc = parseAndFilterFile(sampleFile, conf);        Assert.assertEquals(expectedDescription, doc.getFieldValue("metatag.description"));    Assert.assertEquals(expectedKeywords, doc.getFieldValue("metatag.keywords"));    Assert.assertEquals(expectedAuthor, doc.getFieldValue("metatag.author"));}
0
public void testReplacementsRunInSpecifedOrder()
{    String expectedDescription = "With this awesome plugin, I control the description! Bwuhuhuhaha!";    String indexReplaceProperty = "  metatag.description=/this plugin/this amazing plugin/\n" + "  metatag.description=/this amazing plugin/this valuable plugin/\n" + "  metatag.description=/this valuable plugin/this cool plugin/\n" + "  metatag.description=/this cool plugin/this wicked plugin/\n" + "  metatag.description=/this wicked plugin/this awesome plugin/\n";    Configuration conf = NutchConfiguration.create();    conf.set("plugin.includes", "protocol-file|urlfilter-regex|parse-(html|metatags)|index-(basic|anchor|metadata|static|replace)|urlnormalizer-(pass|regex|basic)");    conf.set(INDEX_REPLACE_PROPERTY, indexReplaceProperty);    conf.set("metatags.names", "author,description,keywords");    conf.set("index.parse.md", "metatag.author,metatag.description,metatag.keywords");        conf.set("http.timeout", "99999999999");        NutchDocument doc = parseAndFilterFile(sampleFile, conf);        Assert.assertEquals(expectedDescription, doc.getFieldValue("metatag.description"));}
0
public void testReplacementsWithFlags()
{    String expectedDescription = "With this awesome plugin, I control the description! Bwuhuhuhaha!";    String indexReplaceProperty = "  metatag.description=/THIS PLUGIN/this awesome plugin/2";    Configuration conf = NutchConfiguration.create();    conf.set("plugin.includes", "protocol-file|urlfilter-regex|parse-(html|metatags)|index-(basic|anchor|metadata|static|replace)|urlnormalizer-(pass|regex|basic)");    conf.set(INDEX_REPLACE_PROPERTY, indexReplaceProperty);    conf.set("metatags.names", "author,description,keywords");    conf.set("index.parse.md", "metatag.author,metatag.description,metatag.keywords");        conf.set("http.timeout", "99999999999");        NutchDocument doc = parseAndFilterFile(sampleFile, conf);            Assert.assertEquals(expectedDescription, doc.getFieldValue("metatag.description"));}
0
public void testReplacementsDifferentTarget()
{    String expectedDescription = "With this plugin, I control the description! Bwuhuhuhaha!";    String expectedTargetDescription = "With this awesome plugin, I control the description! Bwuhuhuhaha!";    String indexReplaceProperty = "  metatag.description:new=/this plugin/this awesome plugin/";    Configuration conf = NutchConfiguration.create();    conf.set("plugin.includes", "protocol-file|urlfilter-regex|parse-(html|metatags)|index-(basic|anchor|metadata|static|replace)|urlnormalizer-(pass|regex|basic)");    conf.set(INDEX_REPLACE_PROPERTY, indexReplaceProperty);    conf.set("metatags.names", "author,description,keywords");    conf.set("index.parse.md", "metatag.author,metatag.description,metatag.keywords");        conf.set("http.timeout", "99999999999");        NutchDocument doc = parseAndFilterFile(sampleFile, conf);        Assert.assertEquals(expectedDescription, doc.getFieldValue("metatag.description"));        Assert.assertEquals(expectedTargetDescription, doc.getFieldValue("new"));}
0
public NutchDocument filter(NutchDocument doc, Parse parse, Text url, CrawlDatum datum, Inlinks inlinks) throws IndexingException
{    if (this.addStaticFields == true) {        for (Entry<String, String[]> entry : this.fields.entrySet()) {            for (String val : entry.getValue()) {                doc.add(entry.getKey(), val);            }        }    }    return doc;}
0
private HashMap<String, String[]> parseFields(String fieldsString)
{    HashMap<String, String[]> fields = new HashMap<String, String[]>();    /*     * The format is very easy, it's a comma-separated list of fields in the     * form <name>:<value>     */    for (String field : fieldsString.split(this.fieldSep)) {        String[] entry = field.split(this.kevSep);        if (entry.length == 2)            fields.put(entry[0].trim(), entry[1].trim().split(this.valueSep));    }    return fields;}
0
public void setConf(Configuration conf)
{    this.conf = conf;        this.fieldSep = this.regexEscape(conf.get("index.static.fieldsep", ","));    this.kevSep = this.regexEscape(conf.get("index.static.keysep", ":"));    this.valueSep = this.regexEscape(conf.get("index.static.valuesep", " "));    String fieldsString = conf.get("index.static", null);    if (fieldsString != null) {        this.addStaticFields = true;        this.fields = parseFields(fieldsString);    }}
0
public Configuration getConf()
{    return this.conf;}
0
protected String regexEscape(String in)
{    String result = in;    if (in != null) {        StringBuffer sb = new StringBuffer();        for (int i = 0; i < in.length(); i++) {            CharSequence c = in.subSequence(i, i + 1);            if ("<([{\\^-=$!|]})?*+.>".contains(c)) {                sb.append('\\');            }            sb.append(c);        }        result = sb.toString();    }    return result;}
0
public void setUp() throws Exception
{    conf = NutchConfiguration.create();    parse = new ParseImpl();    url = new Text("http://nutch.apache.org/index.html");    crawlDatum = new CrawlDatum();    inlinks = new Inlinks();    filter = new StaticFieldIndexer();}
0
public void testEmptyIndexStatic() throws Exception
{    Assert.assertNotNull(filter);    filter.setConf(conf);    NutchDocument doc = new NutchDocument();    try {        filter.filter(doc, parse, url, crawlDatum, inlinks);    } catch (Exception e) {        e.printStackTrace();        Assert.fail(e.getMessage());    }    Assert.assertNotNull(doc);    Assert.assertTrue("tests if no field is set for empty index.static", doc.getFieldNames().isEmpty());}
0
public void testNormalScenario() throws Exception
{    conf.set("index.static", "field1:val1, field2    :      val2 val3     , field3, field4 :val4 , ");    Assert.assertNotNull(filter);    filter.setConf(conf);    NutchDocument doc = new NutchDocument();    try {        filter.filter(doc, parse, url, crawlDatum, inlinks);    } catch (Exception e) {        e.printStackTrace();        Assert.fail(e.getMessage());    }    Assert.assertNotNull(doc);    Assert.assertFalse("test if doc is not empty", doc.getFieldNames().isEmpty());    Assert.assertEquals("test if doc has 3 fields", 3, doc.getFieldNames().size());    Assert.assertTrue("test if doc has field1", doc.getField("field1").getValues().contains("val1"));    Assert.assertTrue("test if doc has field2", doc.getField("field2").getValues().contains("val2"));    Assert.assertTrue("test if doc has field4", doc.getField("field4").getValues().contains("val4"));}
0
public void testCustomDelimiters() throws Exception
{    conf.set("index.static.fieldsep", ">");    conf.set("index.static.keysep", "=");    conf.set("index.static.valuesep", "|");    conf.set("index.static", "field1=val1>field2    =      val2|val3     >field3>field4 =val4 > ");    Assert.assertNotNull(filter);    filter.setConf(conf);    NutchDocument doc = new NutchDocument();    try {        filter.filter(doc, parse, url, crawlDatum, inlinks);    } catch (Exception e) {        e.printStackTrace();        Assert.fail(e.getMessage());    }    Assert.assertNotNull(doc);    Assert.assertFalse("test if doc is not empty", doc.getFieldNames().isEmpty());    Assert.assertEquals("test if doc has 3 fields", 3, doc.getFieldNames().size());    Assert.assertTrue("test if doc has field1", doc.getField("field1").getValues().contains("val1"));    Assert.assertTrue("test if doc has field2", doc.getField("field2").getValues().contains("val2"));    Assert.assertTrue("test if doc has field4", doc.getField("field4").getValues().contains("val4"));}
0
public void testCustomMulticharacterDelimiters() throws Exception
{    conf.set("index.static.fieldsep", "\n\n");    conf.set("index.static.keysep", "\t\t");    conf.set("index.static.valuesep", "***");    conf.set("index.static", "field1\t\tval1\n\n" + "field2\t\tval2***val3\n\n" + "field3\n\n" + "field4\t\tval4\n\n\n\n");    Assert.assertNotNull(filter);    filter.setConf(conf);    NutchDocument doc = new NutchDocument();    try {        filter.filter(doc, parse, url, crawlDatum, inlinks);    } catch (Exception e) {        e.printStackTrace();        Assert.fail(e.getMessage());    }    Assert.assertNotNull(doc);    Assert.assertFalse("test if doc is not empty", doc.getFieldNames().isEmpty());    Assert.assertEquals("test if doc has 3 fields", 3, doc.getFieldNames().size());    Assert.assertTrue("test if doc has field1", doc.getField("field1").getValues().contains("val1"));    Assert.assertTrue("test if doc has field2", doc.getField("field2").getValues().contains("val2"));    Assert.assertTrue("test if doc has field4", doc.getField("field4").getValues().contains("val4"));}
0
public void open(Configuration conf, String name) throws IOException
{}
0
public void open(IndexWriterParams parameters) throws IOException
{        endpoint = parameters.get(CloudSearchConstants.ENDPOINT);    dumpBatchFilesToTemp = parameters.getBoolean(CloudSearchConstants.BATCH_DUMP, false);    this.regionName = parameters.get(CloudSearchConstants.REGION);    if (StringUtils.isBlank(endpoint) && !dumpBatchFilesToTemp) {        String message = "Missing CloudSearch endpoint. Should set it set via -D " + CloudSearchConstants.ENDPOINT + " or in nutch-site.xml";        message += "\n" + describe();                throw new RuntimeException(message);    }    maxDocsInBatch = parameters.getInt(CloudSearchConstants.MAX_DOCS_BATCH, -1);    buffer = new StringBuffer(MAX_SIZE_BATCH_BYTES).append('[');    if (dumpBatchFilesToTemp) {                return;    }    if (StringUtils.isBlank(endpoint)) {        throw new RuntimeException("endpoint not set for CloudSearch");    }    AmazonCloudSearchClient cl = new AmazonCloudSearchClient();    if (StringUtils.isNotBlank(regionName)) {        cl.setRegion(RegionUtils.getRegion(regionName));    }    String domainName = null;        DescribeDomainsResult domains = cl.describeDomains(new DescribeDomainsRequest());    Iterator<DomainStatus> dsiter = domains.getDomainStatusList().iterator();    while (dsiter.hasNext()) {        DomainStatus ds = dsiter.next();        if (ds.getDocService().getEndpoint().equals(endpoint)) {            domainName = ds.getDomainName();            break;        }    }        if (StringUtils.isBlank(domainName)) {        throw new RuntimeException("No domain name found for CloudSearch endpoint");    }    DescribeIndexFieldsResult indexDescription = cl.describeIndexFields(new DescribeIndexFieldsRequest().withDomainName(domainName));    for (IndexFieldStatus ifs : indexDescription.getIndexFields()) {        String indexname = ifs.getOptions().getIndexFieldName();        String indextype = ifs.getOptions().getIndexFieldType();                csfields.put(indexname, indextype);    }    client = new AmazonCloudSearchDomainClient();    client.setEndpoint(endpoint);}
1
public void delete(String url) throws IOException
{    try {        JSONObject doc_builder = new JSONObject();        doc_builder.put("type", "delete");                String ID = CloudSearchUtils.getID(url);        doc_builder.put("id", ID);                addToBatch(doc_builder.toString(2), url);    } catch (JSONException e) {            }}
1
public void update(NutchDocument doc) throws IOException
{    write(doc);}
0
public void write(NutchDocument doc) throws IOException
{    try {        JSONObject doc_builder = new JSONObject();        doc_builder.put("type", "add");        String url = doc.getField("url").toString();                String ID = CloudSearchUtils.getID(url);        doc_builder.put("id", ID);        JSONObject fields = new JSONObject();        for (final Entry<String, NutchField> e : doc) {            String fieldname = cleanFieldName(e.getKey());            String type = csfields.get(fieldname);                        if (!dumpBatchFilesToTemp && type == null) {                                continue;            }            List<Object> values = e.getValue().getValues();                        for (Object value : values) {                                if (value instanceof Date) {                    Date d = (Date) value;                    value = DATE_FORMAT.format(d);                } else                 if (value instanceof String) {                    value = CloudSearchUtils.stripNonCharCodepoints((String) value);                }                fields.accumulate(fieldname, value);            }        }        doc_builder.put("fields", fields);        addToBatch(doc_builder.toString(2), url);    } catch (JSONException e) {            }}
1
private void addToBatch(String currentDoc, String url) throws IOException
{    int currentDocLength = currentDoc.getBytes(StandardCharsets.UTF_8).length;        if (currentDocLength > MAX_SIZE_DOC_BYTES) {                return;    }    int currentBufferLength = buffer.toString().getBytes(StandardCharsets.UTF_8).length;            if (currentDocLength + 2 + currentBufferLength < MAX_SIZE_BATCH_BYTES) {        if (numDocsInBatch != 0)            buffer.append(',');        buffer.append(currentDoc);        numDocsInBatch++;    } else     {        commit();        buffer.append(currentDoc);        numDocsInBatch++;    }        if (maxDocsInBatch > 0 && numDocsInBatch == maxDocsInBatch) {        commit();    }}
1
public void commit() throws IOException
{        if (numDocsInBatch == 0) {        return;    }        buffer.append(']');        byte[] bb = buffer.toString().getBytes(StandardCharsets.UTF_8);    if (dumpBatchFilesToTemp) {        try {            File temp = File.createTempFile("CloudSearch_", ".json");            FileUtils.writeByteArrayToFile(temp, bb);                    } catch (IOException e1) {                    } finally {                        buffer = new StringBuffer(MAX_SIZE_BATCH_BYTES).append('[');            numDocsInBatch = 0;        }        return;    }        try (InputStream inputStream = new ByteArrayInputStream(bb)) {        UploadDocumentsRequest batch = new UploadDocumentsRequest();        batch.setContentLength((long) bb.length);        batch.setContentType(ContentType.Applicationjson);        batch.setDocuments(inputStream);        @SuppressWarnings("unused")        UploadDocumentsResult result = client.uploadDocuments(batch);    } catch (Exception e) {                    } finally {                buffer = new StringBuffer(MAX_SIZE_BATCH_BYTES).append('[');        numDocsInBatch = 0;    }}
1
public void close() throws IOException
{        commit();        if (client != null) {        client.shutdown();    }}
0
public Configuration getConf()
{    return this.conf;}
0
public void setConf(Configuration conf)
{    this.conf = conf;}
0
public Map<String, Entry<String, Object>> describe()
{    Map<String, Map.Entry<String, Object>> properties = new LinkedHashMap<>();    properties.put(CloudSearchConstants.ENDPOINT, new AbstractMap.SimpleEntry<>("Endpoint where service requests should be submitted.", this.endpoint));    properties.put(CloudSearchConstants.REGION, new AbstractMap.SimpleEntry<>("Region name.", this.regionName));    properties.put(CloudSearchConstants.BATCH_DUMP, new AbstractMap.SimpleEntry<>("true to send documents to a local file.", this.dumpBatchFilesToTemp));    properties.put(CloudSearchConstants.MAX_DOCS_BATCH, new AbstractMap.SimpleEntry<>("Maximum number of documents to send as a batch to CloudSearch.", this.maxDocsInBatch));    return properties;}
0
 String cleanFieldName(String name)
{    String lowercase = name.toLowerCase();    return lowercase.replaceAll("[^a-z_0-9]", "_");}
0
public static String getID(String url)
{                                byte[] dig = digester.digest(url.getBytes(StandardCharsets.UTF_8));    String ID = Hex.encodeHexString(dig);        if (ID.length() > 128) {        throw new RuntimeException("ID larger than max 128 chars");    }    return ID;}
0
public static String stripNonCharCodepoints(String input)
{    StringBuilder retval = new StringBuilder();    char ch;    for (int i = 0; i < input.length(); i++) {        ch = input.charAt(i);                if ((ch == 0x9 || ch == 0xa || ch == 0xd) || (ch >= 0x20 && ch <= 0xFFFD)) {            retval.append(ch);        }    }    return retval.toString();}
0
protected void set(String str)
{    if (str != null) {        sepStr = str;        if (str.length() == 0) {                        chars = new char[0];        } else {            chars = str.toCharArray();        }    }        bytes = sepStr.getBytes(encoding);}
0
public String toString()
{    StringBuilder sb = new StringBuilder();    for (char c : chars) {        if (c == '\n') {            sb.append("\\n");        } else if (c == '\r') {            sb.append("\\r");        } else if (c == '\t') {            sb.append("\\t");        } else if (c >= 0x7f || c <= 0x20) {            sb.append(String.format("\\u%04x", (int) c));        } else {            sb.append(c);        }    }    return sb.toString();}
0
protected void setFromConf(IndexWriterParams parameters, String property)
{    setFromConf(parameters, property, false);}
0
protected void setFromConf(IndexWriterParams parameters, String property, boolean isChar)
{    String str = parameters.get(property);    if (isChar && str != null && !str.isEmpty()) {                str = str.substring(0, 1);    }    set(str);    }
1
protected int find(String value, int start)
{    if (chars.length == 0)        return -1;    if (chars.length == 1)        return value.indexOf(chars[0], start);    int index;    for (char c : chars) {        if ((index = value.indexOf(c, start)) >= 0) {            return index;        }    }    return -1;}
0
public void open(Configuration conf, String name) throws IOException
{}
0
public void open(IndexWriterParams parameters) throws IOException
{    outputPath = parameters.get(CSVConstants.CSV_OUTPATH, outputPath);    String charset = parameters.get(CSVConstants.CSV_CHARSET);    if (charset != null) {        encoding = Charset.forName(charset);    }    fieldSeparator.setFromConf(parameters, CSVConstants.CSV_FIELD_SEPARATOR);    quoteCharacter.setFromConf(parameters, CSVConstants.CSV_QUOTECHARACTER, true);    escapeCharacter.setFromConf(parameters, CSVConstants.CSV_ESCAPECHARACTER, true);    valueSeparator.setFromConf(parameters, CSVConstants.CSV_VALUESEPARATOR);    withHeader = parameters.getBoolean(CSVConstants.CSV_WITHHEADER, true);    maxFieldLength = parameters.getInt(CSVConstants.CSV_MAXFIELDLENGTH, maxFieldLength);        maxFieldValues = parameters.getInt(CSVConstants.CSV_MAXFIELDVALUES, maxFieldValues);        fields = parameters.getStrings(CSVConstants.CSV_FIELDS, "id", "title", "content");        for (String f : fields) {            }    fs = FileSystem.get(config);        Path outputDir = new Path(outputPath);    fs = outputDir.getFileSystem(config);    csvLocalOutFile = new Path(outputDir, "nutch.csv");    if (!fs.exists(outputDir)) {        fs.mkdirs(outputDir);    }    if (fs.exists(csvLocalOutFile)) {                        fs.delete(csvLocalOutFile, true);    }    csvout = fs.create(csvLocalOutFile);    if (withHeader) {        for (int i = 0; i < fields.length; i++) {            if (i > 0)                csvout.write(fieldSeparator.bytes);            csvout.write(fields[i].getBytes(encoding));        }    }    csvout.write(recordSeparator.bytes);}
1
public void write(NutchDocument doc) throws IOException
{    for (int i = 0; i < fields.length; i++) {        if (i > 0) {            csvout.write(fieldSeparator.bytes);        }        NutchField field = doc.getField(fields[i]);        if (field != null) {            List<Object> values = field.getValues();            int nValues = values.size();            if (nValues > maxFieldValues) {                nValues = maxFieldValues;            }            if (nValues > 1) {                                csvout.write(quoteCharacter.bytes);            }            ListIterator<Object> it = values.listIterator();            int j = 0;            while (it.hasNext() && j <= nValues) {                Object objval = it.next();                String value;                if (objval == null) {                    continue;                } else if (objval instanceof Date) {                                        value = objval.toString();                } else {                    value = (String) objval;                }                if (nValues > 1) {                                        writeEscaped(value);                    if (it.hasNext()) {                        csvout.write(valueSeparator.bytes);                    }                } else {                    writeQuoted(value);                }            }            if (nValues > 1) {                                csvout.write(quoteCharacter.bytes);            }        }    }    csvout.write(recordSeparator.bytes);}
0
public void delete(String key)
{}
0
public void update(NutchDocument doc) throws IOException
{    write(doc);}
0
public void close() throws IOException
{    csvout.close();    }
1
public void commit()
{}
0
public Configuration getConf()
{    return config;}
0
public Map<String, Map.Entry<String, Object>> describe()
{    Map<String, Map.Entry<String, Object>> properties = new LinkedHashMap<>();    properties.put(CSVConstants.CSV_FIELDS, new AbstractMap.SimpleEntry<>("Ordered list of fields (columns) in the CSV file", this.fields == null ? "" : String.join(",", this.fields)));    properties.put(CSVConstants.CSV_FIELD_SEPARATOR, new AbstractMap.SimpleEntry<>("Separator between fields (columns), default: , (U+002C, comma)", this.fieldSeparator));    properties.put(CSVConstants.CSV_QUOTECHARACTER, new AbstractMap.SimpleEntry<>("Quote character used to quote fields containing separators or quotes, default: \" (U+0022, quotation mark)", this.quoteCharacter));    properties.put(CSVConstants.CSV_ESCAPECHARACTER, new AbstractMap.SimpleEntry<>("Escape character used to escape a quote character, default: \" (U+0022, quotation mark)", this.escapeCharacter));    properties.put(CSVConstants.CSV_VALUESEPARATOR, new AbstractMap.SimpleEntry<>("Separator between multiple values of one field, default: | (U+007C)", this.valueSeparator));    properties.put(CSVConstants.CSV_MAXFIELDVALUES, new AbstractMap.SimpleEntry<>("Max. number of values of one field, useful for, e.g., the anchor texts field, default: 12", this.maxFieldValues));    properties.put(CSVConstants.CSV_MAXFIELDLENGTH, new AbstractMap.SimpleEntry<>("Max. length of a single field value in characters, default: 4096", this.maxFieldLength));    properties.put(CSVConstants.CSV_CHARSET, new AbstractMap.SimpleEntry<>("Encoding of CSV file, default: UTF-8", this.encoding));    properties.put(CSVConstants.CSV_WITHHEADER, new AbstractMap.SimpleEntry<>("Write CSV column headers, default: true", this.withHeader));    properties.put(CSVConstants.CSV_OUTPATH, new AbstractMap.SimpleEntry<>("Output path / directory, default: csvindexwriter. ", this.outputPath));    return properties;}
0
public void setConf(Configuration conf)
{    config = conf;}
0
private void writeQuoted(String value) throws IOException
{    int nextQuoteChar;    if (quoteCharacter.chars.length > 0 && (((nextQuoteChar = quoteCharacter.find(value, 0)) >= 0) || (fieldSeparator.find(value, 0) >= 0) || (recordSeparator.find(value, 0) >= 0))) {                csvout.write(quoteCharacter.bytes);        writeEscaped(value, nextQuoteChar);        csvout.write(quoteCharacter.bytes);    } else {        if (value.length() > maxFieldLength) {            csvout.write(value.substring(0, maxFieldLength).getBytes(encoding));        } else {            csvout.write(value.getBytes(encoding));        }    }}
0
private void writeEscaped(String value, int nextQuoteChar) throws IOException
{    int start = 0;    int max = value.length();    if (max > maxFieldLength) {        max = maxFieldLength;    }    while (nextQuoteChar > 0 && nextQuoteChar < max) {        csvout.write(value.substring(start, nextQuoteChar).getBytes(encoding));        csvout.write(escapeCharacter.bytes);        csvout.write(quoteCharacter.bytes);        start = nextQuoteChar + 1;        nextQuoteChar = quoteCharacter.find(value, start);        if (nextQuoteChar > max)            break;    }    csvout.write(value.substring(start, max).getBytes(encoding));}
0
private void writeEscaped(String value) throws IOException
{    int nextQuoteChar = quoteCharacter.find(value, 0);    writeEscaped(value, nextQuoteChar);}
0
public static void main(String[] args) throws Exception
{    final int res = ToolRunner.run(NutchConfiguration.create(), new IndexingJob(), args);    System.exit(res);}
0
public void open(IndexWriterParams parameters) throws IOException
{    super.open(parameters);    byteBuffer = new ByteArrayOutputStream();    fsStats = new FileSystem.Statistics("testCSVIndexWriter");    csvout = new FSDataOutputStream(byteBuffer, fsStats);}
0
public void close() throws IOException
{}
0
public String getData()
{    try {        return byteBuffer.toString(encoding.name());    } catch (UnsupportedEncodingException e) {        return "";    }}
0
private String getCSV(final String[] configParams, NutchDocument[] docs) throws IOException
{    Configuration conf = NutchConfiguration.create();    IndexWriterParams params = new IndexWriterParams(new HashMap<>());    for (int i = 0; i < configParams.length; i += 2) {        params.put(configParams[i], configParams[i + 1]);    }    CSVByteArrayIndexWriter out = new CSVByteArrayIndexWriter();    out.setConf(conf);    out.open(params);    for (NutchDocument doc : docs) {        out.write(doc);    }    out.close();    String csv = out.getData();        return csv;}
1
private String getCSV(final String[] configParams, final String[] fieldContent) throws IOException
{    NutchDocument[] docs = new NutchDocument[1];    docs[0] = new NutchDocument();    for (int i = 0; i < fieldContent.length; i += 2) {        docs[0].add(fieldContent[i], fieldContent[i + 1]);    }    return getCSV(configParams, docs);}
0
public void testCSVdefault() throws IOException
{    String[] fields = { "id", "http://nutch.apache.org/", "title", "Welcome to Apache Nutch", "content", "Apache Nutch is an open source web-search software project. ..." };    String csv = getCSV(new String[0], fields);    for (int i = 0; i < fields.length; i += 2) {        assertTrue("Testing field " + i + " (" + fields[i] + ")", csv.contains(fields[i + 1]));    }}
0
public void testCSVquoteFieldSeparators() throws IOException
{    String[] params = { CSVConstants.CSV_FIELDS, "test,test2" };    String[] fields = { "test", "a,b", "test2", "c,d" };    String csv = getCSV(params, fields);    assertEquals("If field contains a fields separator, it must be quoted", "\"a,b\",\"c,d\"", csv.trim());}
0
public void testCSVquoteRecordSeparators() throws IOException
{    String[] params = { CSVConstants.CSV_FIELDS, "test" };    String[] fields = { "test", "a\nb" };    String csv = getCSV(params, fields);    assertEquals("If field contains a fields separator, it must be quoted", "\"a\nb\"", csv.trim());}
0
public void testCSVescapeQuotes() throws IOException
{    String[] params = { CSVConstants.CSV_FIELDS, "test" };    String[] fields = { "test", "a,b:\"quote\",c" };    String csv = getCSV(params, fields);    assertEquals("Quotes inside a quoted field must be escaped", "\"a,b:\"\"quote\"\",c\"", csv.trim());}
0
public void testCSVclipMaxLength() throws IOException
{    String[] params = { CSVConstants.CSV_FIELDS, "test", CSVConstants.CSV_MAXFIELDLENGTH, "8" };    String[] fields = { "test", "0123456789" };    String csv = getCSV(params, fields);    assertEquals("Field clipped to max. length = 8", "01234567", csv.trim());}
0
public void testCSVclipMaxLengthQuote() throws IOException
{    String[] params = { CSVConstants.CSV_FIELDS, "test", CSVConstants.CSV_MAXFIELDLENGTH, "7" };    String[] fields = { "test", "1,\"2\",3,\"4\"" };    String csv = getCSV(params, fields);    assertEquals("Field clipped to max. length = 7", "\"1,\"\"2\"\",3\"", csv.trim());}
0
public void testCSVmultiValueFields() throws IOException
{    String[] params = { CSVConstants.CSV_FIELDS, "test", CSVConstants.CSV_VALUESEPARATOR, "|", CSVConstants.CSV_QUOTECHARACTER, "" };    String[] fields = { "test", "abc", "test", "def" };    String csv = getCSV(params, fields);    assertEquals("Values of multi-value fields are concatenated by |", "abc|def", csv.trim());}
0
public void testCSVEncoding() throws IOException
{    String[] charsets = { "iso-8859-1",     "\u00e4\u00f6\u00fc\u00df\u00e9\u00f4\u00ee",     "iso-8859-2",     "\u0161\u010d\u0159\u016f",     "iso-8859-5",     "\u0430\u0441\u0434\u0444" };    for (int i = 0; i < charsets.length; i += 2) {        String charset = charsets[i];        String test = charsets[i + 1];        String[] params = { CSVConstants.CSV_FIELDS, "test", CSVConstants.CSV_CHARSET, charset };        String[] fields = { "test", test };        String csv = getCSV(params, fields);        assertEquals("wrong charset conversion", test, csv.trim());    }}
0
public void testCSVEncodingSeparator() throws IOException
{    String[] params = { CSVConstants.CSV_FIELDS, "test", CSVConstants.CSV_CHARSET, "iso-8859-1",     CSVConstants.CSV_VALUESEPARATOR,     "\u00a6", CSVConstants.CSV_QUOTECHARACTER, "" };    String[] fields = { "test", "abc", "test", "def" };    String csv = getCSV(params, fields);    assertEquals("Values of multi-value fields are concatenated by ¦", "abc\u00a6def", csv.trim());}
0
public void testCSVtabSeparated() throws IOException
{    String[] params = { CSVConstants.CSV_FIELDS, "1,2,3", CSVConstants.CSV_FIELD_SEPARATOR, "\t", CSVConstants.CSV_QUOTECHARACTER, "" };    NutchDocument[] docs = new NutchDocument[2];    docs[0] = new NutchDocument();    docs[0].add("1", "a");    docs[0].add("1", "b");    docs[0].add("2", "a\"2\"b");    docs[0].add("3", "c,d");    docs[1] = new NutchDocument();    docs[1].add("1", "A");    docs[1].add("2", "B");    docs[1].add("3", "C");    String csv = getCSV(params, docs);    String[] records = csv.trim().split("\\r\\n");    assertEquals("tab-separated output", "a|b\ta\"2\"b\tc,d", records[0]);    assertEquals("tab-separated output", "A\tB\tC", records[1]);}
0
public void testCSVdateField() throws IOException
{    TimeZone.setDefault(TimeZone.getTimeZone("UTC"));    String[] params = { CSVConstants.CSV_FIELDS, "date" };    NutchDocument[] docs = new NutchDocument[1];    docs[0] = new NutchDocument();        docs[0].add("date", new Date(0));    String csv = getCSV(params, docs);    assertTrue("date conversion", csv.contains("1970"));}
0
public void open(Configuration conf, String name) throws IOException
{}
0
public void open(IndexWriterParams parameters) throws IOException
{    delete = parameters.getBoolean(DummyConstants.DELETE, false);    path = parameters.get(DummyConstants.PATH, "/");    if (path == null) {        String message = "Missing path.";        message += "\n" + describe();                throw new RuntimeException(message);    }    if (writer != null) {                return;    }    try {                writer = new BufferedWriter(new FileWriter(path));    } catch (IOException ex) {            }}
1
public void delete(String key) throws IOException
{    if (delete) {        writer.write("delete\t" + key + "\n");    }}
0
public void update(NutchDocument doc) throws IOException
{    writer.write("update\t" + doc.getFieldValue("id") + "\n");}
0
public void write(NutchDocument doc) throws IOException
{    writer.write("add\t" + doc.getFieldValue("id") + "\n");}
0
public void close() throws IOException
{        writer.flush();    writer.close();}
1
public void commit() throws IOException
{    writer.write("commit\n");}
0
public Configuration getConf()
{    return config;}
0
public void setConf(Configuration conf)
{    config = conf;}
0
public Map<String, Map.Entry<String, Object>> describe()
{    Map<String, Map.Entry<String, Object>> properties = new LinkedHashMap<>();    properties.put(DummyConstants.DELETE, new AbstractMap.SimpleEntry<>("If delete operations should be written to the file.", this.delete));    properties.put(DummyConstants.PATH, new AbstractMap.SimpleEntry<>("Path where the file will be created.", this.path));    return properties;}
0
public void open(Configuration conf, String name) throws IOException
{}
0
public void open(IndexWriterParams parameters) throws IOException
{    cluster = parameters.get(ElasticConstants.CLUSTER);    String hosts = parameters.get(ElasticConstants.HOSTS);    if (StringUtils.isBlank(cluster) && StringUtils.isBlank(hosts)) {        String message = "Missing elastic.cluster and elastic.host. At least one of them should be set in index-writers.xml ";        message += "\n" + describe();                throw new RuntimeException(message);    }    bulkCloseTimeout = parameters.getLong(ElasticConstants.BULK_CLOSE_TIMEOUT, DEFAULT_BULK_CLOSE_TIMEOUT);    defaultIndex = parameters.get(ElasticConstants.INDEX, DEFAULT_INDEX);    maxBulkDocs = parameters.getInt(ElasticConstants.MAX_BULK_DOCS, DEFAULT_MAX_BULK_DOCS);    maxBulkLength = parameters.getInt(ElasticConstants.MAX_BULK_LENGTH, DEFAULT_MAX_BULK_LENGTH);    expBackoffMillis = parameters.getInt(ElasticConstants.EXPONENTIAL_BACKOFF_MILLIS, DEFAULT_EXP_BACKOFF_MILLIS);    expBackoffRetries = parameters.getInt(ElasticConstants.EXPONENTIAL_BACKOFF_RETRIES, DEFAULT_EXP_BACKOFF_RETRIES);    client = makeClient(parameters);        bulkProcessor = BulkProcessor.builder(client, bulkProcessorListener()).setBulkActions(maxBulkDocs).setBulkSize(new ByteSizeValue(maxBulkLength, ByteSizeUnit.BYTES)).setConcurrentRequests(1).setBackoffPolicy(BackoffPolicy.exponentialBackoff(TimeValue.timeValueMillis(expBackoffMillis), expBackoffRetries)).build();}
1
protected Client makeClient(IndexWriterParams parameters) throws IOException
{    hosts = parameters.getStrings(ElasticConstants.HOSTS);    port = parameters.getInt(ElasticConstants.PORT, DEFAULT_PORT);    Settings.Builder settingsBuilder = Settings.builder();    String options = parameters.get(ElasticConstants.OPTIONS);    if (options != null) {        String[] lines = options.trim().split(",");        for (String line : lines) {            if (StringUtils.isNotBlank(line)) {                String[] parts = line.trim().split("=");                if (parts.length == 2) {                    settingsBuilder.put(parts[0].trim(), parts[1].trim());                }            }        }    }        if (StringUtils.isNotBlank(cluster)) {        settingsBuilder.put("cluster.name", cluster);    }    Settings settings = settingsBuilder.build();    Client client = null;        if (hosts != null && port > 1) {        @SuppressWarnings("resource")        TransportClient transportClient = new PreBuiltTransportClient(settings);        for (String host : hosts) transportClient.addTransportAddress(new InetSocketTransportAddress(InetAddress.getByName(host), port));        client = transportClient;    } else if (cluster != null) {        node = new Node(settings);        client = node.client();    }    return client;}
0
protected BulkProcessor.Listener bulkProcessorListener()
{    return new BulkProcessor.Listener() {        @Override        public void beforeBulk(long executionId, BulkRequest request) {        }        @Override        public void afterBulk(long executionId, BulkRequest request, Throwable failure) {            throw new RuntimeException(failure);        }        @Override        public void afterBulk(long executionId, BulkRequest request, BulkResponse response) {            if (response.hasFailures()) {                            }        }    };}
1
public void beforeBulk(long executionId, BulkRequest request)
{}
0
public void afterBulk(long executionId, BulkRequest request, Throwable failure)
{    throw new RuntimeException(failure);}
0
public void afterBulk(long executionId, BulkRequest request, BulkResponse response)
{    if (response.hasFailures()) {            }}
1
public void write(NutchDocument doc) throws IOException
{    String id = (String) doc.getFieldValue("id");    String type = doc.getDocumentMeta().get("type");    if (type == null)        type = "doc";        Map<String, Object> source = new HashMap<String, Object>();    for (final Map.Entry<String, NutchField> e : doc) {        final List<Object> values = e.getValue().getValues();        if (values.size() > 1) {            source.put(e.getKey(), values);        } else {            source.put(e.getKey(), values.get(0));        }    }    IndexRequest request = new IndexRequest(defaultIndex, type, id).source(source);    bulkProcessor.add(request);}
0
public void delete(String key) throws IOException
{    DeleteRequest request = new DeleteRequest(defaultIndex, "doc", key);    bulkProcessor.add(request);}
0
public void update(NutchDocument doc) throws IOException
{    write(doc);}
0
public void commit() throws IOException
{    bulkProcessor.flush();}
0
public void close() throws IOException
{        try {        bulkProcessor.awaitClose(bulkCloseTimeout, TimeUnit.SECONDS);    } catch (InterruptedException e) {            }    client.close();    if (node != null) {        node.close();    }}
1
public Map<String, Map.Entry<String, Object>> describe()
{    Map<String, Map.Entry<String, Object>> properties = new LinkedHashMap<>();    properties.put(ElasticConstants.CLUSTER, new AbstractMap.SimpleEntry<>("The cluster name to discover. Either host and port must be defined or cluster.", this.cluster));    properties.put(ElasticConstants.HOSTS, new AbstractMap.SimpleEntry<>("Ordered list of fields (columns) in the CSV fileComma-separated list of " + "hostnames to send documents to using TransportClient. " + "Either host and port must be defined or cluster.", this.hosts == null ? "" : String.join(",", hosts)));    properties.put(ElasticConstants.PORT, new AbstractMap.SimpleEntry<>("The port to connect to using TransportClient.", this.port));    properties.put(ElasticConstants.INDEX, new AbstractMap.SimpleEntry<>("Default index to send documents to.", this.defaultIndex));    properties.put(ElasticConstants.MAX_BULK_DOCS, new AbstractMap.SimpleEntry<>("Maximum size of the bulk in number of documents.", this.maxBulkDocs));    properties.put(ElasticConstants.MAX_BULK_LENGTH, new AbstractMap.SimpleEntry<>("Maximum size of the bulk in bytes.", this.maxBulkLength));    properties.put(ElasticConstants.EXPONENTIAL_BACKOFF_MILLIS, new AbstractMap.SimpleEntry<>("Initial delay for the BulkProcessor exponential backoff policy.", this.expBackoffMillis));    properties.put(ElasticConstants.EXPONENTIAL_BACKOFF_RETRIES, new AbstractMap.SimpleEntry<>("Number of times the BulkProcessor exponential backoff policy should retry bulk operations.", this.expBackoffRetries));    properties.put(ElasticConstants.BULK_CLOSE_TIMEOUT, new AbstractMap.SimpleEntry<>("Number of seconds allowed for the BulkProcessor to complete its last operation.", this.bulkCloseTimeout));    return properties;}
0
public void setConf(Configuration conf)
{    config = conf;}
0
public Configuration getConf()
{    return config;}
0
public void setup()
{    conf = NutchConfiguration.create();    conf.addResource("nutch-site-test.xml");    bulkRequestSuccessful = false;    clusterSaturated = false;    curNumFailures = 0;    maxNumFailures = 0;    Settings settings = Settings.builder().build();    ThreadPool threadPool = new ThreadPool(settings);        client = new AbstractClient(settings, threadPool) {        @Override        public void close() {        }        @Override        protected <Request extends ActionRequest, Response extends ActionResponse, RequestBuilder extends ActionRequestBuilder<Request, Response, RequestBuilder>> void doExecute(Action<Request, Response, RequestBuilder> action, Request request, ActionListener<Response> listener) {            BulkResponse response = null;            if (clusterSaturated) {                                curNumFailures++;                if (curNumFailures >= maxNumFailures) {                                        clusterSaturated = false;                }                                BulkItemResponse failed = new BulkItemResponse(0, OpType.INDEX, new BulkItemResponse.Failure("nutch", "index", "failure0", new EsRejectedExecutionException("saturated")));                response = new BulkResponse(new BulkItemResponse[] { failed }, 0);            } else {                                BulkItemResponse success = new BulkItemResponse(0, OpType.INDEX, new IndexResponse(new ShardId("nutch", UUID.randomUUID().toString(), 0), "index", "index0", 0, true));                response = new BulkResponse(new BulkItemResponse[] { success }, 0);            }            listener.onResponse((Response) response);        }    };        testIndexWriter = new ElasticIndexWriter() {        @Override        protected Client makeClient(IndexWriterParams parameters) {            return client;        }        @Override        protected BulkProcessor.Listener bulkProcessorListener() {            return new BulkProcessor.Listener() {                @Override                public void afterBulk(long executionId, BulkRequest request, BulkResponse response) {                    if (!response.hasFailures()) {                        bulkRequestSuccessful = true;                    }                }                @Override                public void afterBulk(long executionId, BulkRequest request, Throwable failure) {                }                @Override                public void beforeBulk(long executionId, BulkRequest request) {                }            };        }    };}
0
public void close()
{}
0
protected void doExecute(Action<Request, Response, RequestBuilder> action, Request request, ActionListener<Response> listener)
{    BulkResponse response = null;    if (clusterSaturated) {                curNumFailures++;        if (curNumFailures >= maxNumFailures) {                        clusterSaturated = false;        }                BulkItemResponse failed = new BulkItemResponse(0, OpType.INDEX, new BulkItemResponse.Failure("nutch", "index", "failure0", new EsRejectedExecutionException("saturated")));        response = new BulkResponse(new BulkItemResponse[] { failed }, 0);    } else {                BulkItemResponse success = new BulkItemResponse(0, OpType.INDEX, new IndexResponse(new ShardId("nutch", UUID.randomUUID().toString(), 0), "index", "index0", 0, true));        response = new BulkResponse(new BulkItemResponse[] { success }, 0);    }    listener.onResponse((Response) response);}
0
protected Client makeClient(IndexWriterParams parameters)
{    return client;}
0
protected BulkProcessor.Listener bulkProcessorListener()
{    return new BulkProcessor.Listener() {        @Override        public void afterBulk(long executionId, BulkRequest request, BulkResponse response) {            if (!response.hasFailures()) {                bulkRequestSuccessful = true;            }        }        @Override        public void afterBulk(long executionId, BulkRequest request, Throwable failure) {        }        @Override        public void beforeBulk(long executionId, BulkRequest request) {        }    };}
0
public void afterBulk(long executionId, BulkRequest request, BulkResponse response)
{    if (!response.hasFailures()) {        bulkRequestSuccessful = true;    }}
0
public void afterBulk(long executionId, BulkRequest request, Throwable failure)
{}
0
public void beforeBulk(long executionId, BulkRequest request)
{}
0
public void testBulkMaxDocs() throws IOException
{    int numDocs = 10;    conf.setInt(ElasticConstants.MAX_BULK_DOCS, numDocs);    @SuppressWarnings("unused")    Job job = Job.getInstance(conf);    Map<String, String> parameters = new HashMap<>();    parameters.put(ElasticConstants.CLUSTER, "nutch");    parameters.put(ElasticConstants.MAX_BULK_DOCS, String.valueOf(numDocs));    testIndexWriter.setConf(conf);    testIndexWriter.open(new IndexWriterParams(parameters));    NutchDocument doc = new NutchDocument();    doc.add("id", "http://www.example.com");    Assert.assertFalse(bulkRequestSuccessful);    for (int i = 0; i < numDocs; i++) {        testIndexWriter.write(doc);    }    testIndexWriter.close();    Assert.assertTrue(bulkRequestSuccessful);}
0
public void testBulkMaxLength() throws IOException
{    String key = "id";    String value = "http://www.example.com";    int defaultMaxBulkLength = conf.getInt(ElasticConstants.MAX_BULK_LENGTH, 2500500);        int testMaxBulkLength = defaultMaxBulkLength / 10;                int numDocs = testMaxBulkLength / (key.length() + value.length());    conf.setInt(ElasticConstants.MAX_BULK_LENGTH, testMaxBulkLength);    @SuppressWarnings("unused")    Job job = Job.getInstance(conf);    Map<String, String> parameters = new HashMap<>();    parameters.put(ElasticConstants.CLUSTER, "nutch");    parameters.put(ElasticConstants.MAX_BULK_LENGTH, String.valueOf(testMaxBulkLength));    testIndexWriter.setConf(conf);    testIndexWriter.open(new IndexWriterParams(parameters));    NutchDocument doc = new NutchDocument();    doc.add(key, value);    Assert.assertFalse(bulkRequestSuccessful);    for (int i = 0; i < numDocs; i++) {        testIndexWriter.write(doc);    }    testIndexWriter.close();    Assert.assertTrue(bulkRequestSuccessful);}
0
public void testBackoffPolicy() throws IOException
{        maxNumFailures = 5;    conf.setInt(ElasticConstants.EXPONENTIAL_BACKOFF_RETRIES, maxNumFailures);    int numDocs = 10;    conf.setInt(ElasticConstants.MAX_BULK_DOCS, numDocs);    @SuppressWarnings("unused")    Job job = Job.getInstance(conf);    Map<String, String> parameters = new HashMap<>();    parameters.put(ElasticConstants.CLUSTER, "nutch");    parameters.put(ElasticConstants.EXPONENTIAL_BACKOFF_RETRIES, String.valueOf(maxNumFailures));    parameters.put(ElasticConstants.MAX_BULK_DOCS, String.valueOf(numDocs));    testIndexWriter.setConf(conf);    testIndexWriter.open(new IndexWriterParams(parameters));    NutchDocument doc = new NutchDocument();    doc.add("id", "http://www.example.com");        clusterSaturated = true;    Assert.assertFalse(bulkRequestSuccessful);        for (int i = 0; i < numDocs; i++) {        testIndexWriter.write(doc);    }    testIndexWriter.close();        Assert.assertTrue(bulkRequestSuccessful);}
0
public void open(Configuration conf, String name) throws IOException
{}
0
public void open(IndexWriterParams parameters) throws IOException
{    host = parameters.get(ElasticRestConstants.HOST);    if (StringUtils.isBlank(host)) {        String message = "Missing host. It should be set in index-writers.xml";        message += "\n" + describe();                throw new RuntimeException(message);    }    port = parameters.getInt(ElasticRestConstants.PORT, 9200);    user = parameters.get(ElasticRestConstants.USER);    password = parameters.get(ElasticRestConstants.PASSWORD);    https = parameters.getBoolean(ElasticRestConstants.HTTPS, false);    trustAllHostnames = parameters.getBoolean(ElasticRestConstants.HOSTNAME_TRUST, false);    languages = parameters.getStrings(ElasticRestConstants.LANGUAGES);    separator = parameters.get(ElasticRestConstants.SEPARATOR, DEFAULT_SEPARATOR);    sink = parameters.get(ElasticRestConstants.SINK, DEFAULT_SINK);        SSLContext sslContext = null;    try {        sslContext = new SSLContextBuilder().loadTrustMaterial(new TrustStrategy() {            public boolean isTrusted(X509Certificate[] arg0, String arg1) throws CertificateException {                return true;            }        }).build();    } catch (NoSuchAlgorithmException | KeyManagementException | KeyStoreException e) {                throw new SecurityException();    }        HostnameVerifier hostnameVerifier = null;    if (trustAllHostnames) {        hostnameVerifier = NoopHostnameVerifier.INSTANCE;    } else {        hostnameVerifier = new DefaultHostnameVerifier();    }    SSLConnectionSocketFactory sslSocketFactory = new SSLConnectionSocketFactory(sslContext);    SchemeIOSessionStrategy httpsIOSessionStrategy = new SSLIOSessionStrategy(sslContext, hostnameVerifier);    JestClientFactory jestClientFactory = new JestClientFactory();    URL urlOfElasticsearchNode = new URL(https ? "https" : "http", host, port, "");    if (host != null && port > 1) {        HttpClientConfig.Builder builder = new HttpClientConfig.Builder(urlOfElasticsearchNode.toString()).multiThreaded(true).connTimeout(300000).readTimeout(300000);        if (https) {            if (user != null && password != null) {                builder.defaultCredentials(user, password);            }            builder.defaultSchemeForDiscoveredNodes("https").sslSocketFactory(            sslSocketFactory).httpsIOSessionStrategy(            httpsIOSessionStrategy);        }        jestClientFactory.setHttpClientConfig(builder.build());    } else {        throw new IllegalStateException("No host or port specified. Please set the host and port in nutch-site.xml");    }    client = jestClientFactory.getObject();    defaultIndex = parameters.get(ElasticRestConstants.INDEX, "nutch");    defaultType = parameters.get(ElasticRestConstants.TYPE, "doc");    maxBulkDocs = parameters.getInt(ElasticRestConstants.MAX_BULK_DOCS, DEFAULT_MAX_BULK_DOCS);    maxBulkLength = parameters.getInt(ElasticRestConstants.MAX_BULK_LENGTH, DEFAULT_MAX_BULK_LENGTH);    bulkBuilder = new Bulk.Builder().defaultIndex(defaultIndex).defaultType(defaultType);}
1
public boolean isTrusted(X509Certificate[] arg0, String arg1) throws CertificateException
{    return true;}
0
private static Object normalizeValue(Object value)
{    if (value == null) {        return null;    }    if (value instanceof Map || value instanceof Date) {        return value;    }    return value.toString();}
0
public void write(NutchDocument doc) throws IOException
{    String id = (String) doc.getFieldValue("id");    String type = doc.getDocumentMeta().get("type");    if (type == null) {        type = defaultType;    }    Map<String, Object> source = new HashMap<String, Object>();        for (String fieldName : doc.getFieldNames()) {        Set<Object> allFieldValues = new LinkedHashSet<>(doc.getField(fieldName).getValues());        if (allFieldValues.size() > 1) {            Object[] normalizedFieldValues = allFieldValues.stream().map(ElasticRestIndexWriter::normalizeValue).toArray();                        for (Object value : normalizedFieldValues) {                bulkLength += value.toString().length();            }            source.put(fieldName, normalizedFieldValues);        } else if (allFieldValues.size() == 1) {            Object normalizedFieldValue = normalizeValue(allFieldValues.iterator().next());            source.put(fieldName, normalizedFieldValue);            bulkLength += normalizedFieldValue.toString().length();        }    }    String index;    if (languages != null && languages.length > 0) {        String language = (String) doc.getFieldValue("lang");        boolean exists = false;        for (String lang : languages) {            if (lang.equals(language)) {                exists = true;                break;            }        }        if (exists) {            index = getLanguageIndexName(language);        } else {            index = getSinkIndexName();        }    } else {        index = defaultIndex;    }    Index indexRequest = new Index.Builder(source).index(index).type(type).id(id).build();        bulkBuilder.addAction(indexRequest);    indexedDocs++;    bulkDocs++;    if (bulkDocs >= maxBulkDocs || bulkLength >= maxBulkLength) {                        createNewBulk = true;        commit();    }}
1
public void delete(String key) throws IOException
{    try {        if (languages != null && languages.length > 0) {            Bulk.Builder bulkBuilder = new Bulk.Builder().defaultType(defaultType);            for (String lang : languages) {                bulkBuilder.addAction(new Delete.Builder(key).index(getLanguageIndexName(lang)).type(defaultType).build());            }            bulkBuilder.addAction(new Delete.Builder(key).index(getSinkIndexName()).type(defaultType).build());            client.execute(bulkBuilder.build());        } else {            client.execute(new Delete.Builder(key).index(defaultIndex).type(defaultType).build());        }    } catch (IOException e) {                throw e;    }}
1
public void update(NutchDocument doc) throws IOException
{    try {        write(doc);    } catch (IOException e) {                throw e;    }}
1
public void commit() throws IOException
{    if (basicFuture != null) {                long beforeWait = System.currentTimeMillis();        try {            JestResult result = basicFuture.get();            if (result == null) {                throw new RuntimeException();            }            long msWaited = System.currentTimeMillis() - beforeWait;                    } catch (InterruptedException | ExecutionException e) {                    }        basicFuture = null;    }    if (bulkBuilder != null) {        if (bulkDocs > 0) {                        basicFuture = new BasicFuture<>(null);            millis = System.currentTimeMillis();            client.executeAsync(bulkBuilder.build(), new JestResultHandler<BulkResult>() {                @Override                public void completed(BulkResult bulkResult) {                    basicFuture.completed(bulkResult);                    millis = System.currentTimeMillis() - millis;                }                @Override                public void failed(Exception e) {                    basicFuture.completed(null);                                    }            });        }        bulkBuilder = null;    }    if (createNewBulk) {                bulkBuilder = new Bulk.Builder().defaultIndex(defaultIndex).defaultType(defaultType);        bulkDocs = 0;        bulkLength = 0;    }}
1
public void completed(BulkResult bulkResult)
{    basicFuture.completed(bulkResult);    millis = System.currentTimeMillis() - millis;}
0
public void failed(Exception e)
{    basicFuture.completed(null);    }
1
public void close() throws IOException
{            createNewBulk = false;    commit();            createNewBulk = false;    commit();        client.shutdownClient();}
1
public Map<String, Map.Entry<String, Object>> describe()
{    Map<String, Map.Entry<String, Object>> properties = new LinkedHashMap<>();    properties.put(ElasticRestConstants.HOST, new AbstractMap.SimpleEntry<>("The hostname or a list of comma separated hostnames to send documents " + "to using Elasticsearch Jest. Both host and port must be defined.", this.host));    properties.put(ElasticRestConstants.PORT, new AbstractMap.SimpleEntry<>("The port to connect to using Elasticsearch Jest.", this.port));    properties.put(ElasticRestConstants.INDEX, new AbstractMap.SimpleEntry<>("Default index to send documents to.", this.defaultIndex));    properties.put(ElasticRestConstants.MAX_BULK_DOCS, new AbstractMap.SimpleEntry<>("Maximum size of the bulk in number of documents.", this.maxBulkDocs));    properties.put(ElasticRestConstants.MAX_BULK_LENGTH, new AbstractMap.SimpleEntry<>("Maximum size of the bulk in bytes.", this.maxBulkLength));    properties.put(ElasticRestConstants.USER, new AbstractMap.SimpleEntry<>("Username for auth credentials (only used when https is enabled)", this.user));    properties.put(ElasticRestConstants.PASSWORD, new AbstractMap.SimpleEntry<>("Password for auth credentials (only used when https is enabled)", this.password));    properties.put(ElasticRestConstants.TYPE, new AbstractMap.SimpleEntry<>("Default type to send documents to.", this.defaultType));    properties.put(ElasticRestConstants.HTTPS, new AbstractMap.SimpleEntry<>("true to enable https, false to disable https. If you've disabled http " + "access (by forcing https), be sure to set this to true, otherwise " + "you might get \"connection reset by peer\".", this.https));    properties.put(ElasticRestConstants.HOSTNAME_TRUST, new AbstractMap.SimpleEntry<>("true to trust elasticsearch server's certificate even if its listed " + "domain name does not match the domain they are hosted or false " + "to check if the elasticsearch server's certificate's listed " + "domain is the same domain that it is hosted on, and if " + "it doesn't, then fail to index (only used when https is enabled)", this.trustAllHostnames));    properties.put(ElasticRestConstants.LANGUAGES, new AbstractMap.SimpleEntry<>("A list of strings denoting the supported languages (e.g. en, de, fr, it). " + "If this value is empty all documents will be sent to index property. " + "If not empty the Rest client will distribute documents in different " + "indices based on their languages property. Indices are named with the " + "following schema: index separator language (e.g. nutch_de). " + "Entries with an unsupported languages value will be added to " + "index index separator sink (e.g. nutch_others).", this.languages == null ? "" : String.join(",", languages)));    properties.put(ElasticRestConstants.SEPARATOR, new AbstractMap.SimpleEntry<>("Is used only if languages property is defined to build the index name " + "(i.e. index separator lang).", this.separator));    properties.put(ElasticRestConstants.SINK, new AbstractMap.SimpleEntry<>("Is used only if languages property is defined to build the index name " + "where to store documents with unsupported languages " + "(i.e. index separator sink).", this.sink));    return properties;}
0
public void setConf(Configuration conf)
{    config = conf;}
0
public Configuration getConf()
{    return config;}
0
private String getLanguageIndexName(String lang)
{    return getComposedIndexName(defaultIndex, lang);}
0
private String getSinkIndexName()
{    return getComposedIndexName(defaultIndex, sink);}
0
private String getComposedIndexName(String prefix, String postfix)
{    return prefix + separator + postfix;}
0
public void open(Configuration job, String name) throws IOException
{}
0
public void open(IndexWriterParams params) throws IOException
{    host = params.get(KafkaConstants.HOST);    port = params.getInt(KafkaConstants.PORT, 9092);    keySerializer = params.get(KafkaConstants.KEY_SERIALIZER, "org.apache.kafka.common.serialization.ByteArraySerializer");    valueSerializer = params.get(KafkaConstants.VALUE_SERIALIZER, "org.apache.kafka.connect.json.JsonSerializer");    topic = params.get(KafkaConstants.TOPIC);    maxDocCount = params.getInt(KafkaConstants.MAX_DOC_COUNT, 100);    inputDocs = new ArrayList<ProducerRecord<String, JsonNode>>(maxDocCount);    if (StringUtils.isBlank(host)) {        String message = "Missing host. It should be set in index-writers.xml";        message += "\n" + describe();                throw new RuntimeException(message);    }    Properties configProperties = new Properties();    configProperties.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, host + ":" + port);    configProperties.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, keySerializer);    configProperties.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, valueSerializer);    Thread.currentThread().setContextClassLoader(this.getClass().getClassLoader());    producer = new KafkaProducer<String, JsonNode>(configProperties);}
1
public void write(NutchDocument doc) throws IOException
{    Map<String, Object> source = new HashMap<String, Object>();        for (String fieldName : doc.getFieldNames()) {        Set<String> allFieldValues = new HashSet<String>();        for (Object value : doc.getField(fieldName).getValues()) {            allFieldValues.add(value.toString());        }        String[] fieldValues = allFieldValues.toArray(new String[allFieldValues.size()]);        source.put(fieldName, fieldValues);    }    try {        jsonString = new ObjectMapper().writeValueAsString(source);        json = new ObjectMapper().readTree(jsonString);        data = new ProducerRecord<String, JsonNode>(topic, json);        inputDocs.add(data);        if (inputDocs.size() == maxDocCount) {            commit();        }    } catch (NullPointerException e) {            }}
1
public void delete(String key) throws IOException
{}
0
public void update(NutchDocument doc) throws IOException
{    try {        write(doc);    } catch (IOException e) {                throw e;    }}
1
public void commit() throws IOException
{    try {        for (ProducerRecord<String, JsonNode> datum : inputDocs) {            producer.send(datum);        }        inputDocs.clear();    } catch (NullPointerException e) {            }}
1
public void close() throws IOException
{    commit();    producer.close();}
0
public Map<String, Map.Entry<String, Object>> describe()
{    Map<String, Map.Entry<String, Object>> properties = new LinkedHashMap<>();    properties.put(KafkaConstants.HOST, new AbstractMap.SimpleEntry<>("Location of the host Kafka cluster to connect to using producerConfig", this.host));    properties.put(KafkaConstants.PORT, new AbstractMap.SimpleEntry<>("The port to connect to using the producerConfig", this.port));    properties.put(KafkaConstants.TOPIC, new AbstractMap.SimpleEntry<>("Default index to attach to documents", this.topic));    properties.put(KafkaConstants.KEY_SERIALIZER, new AbstractMap.SimpleEntry<>("instruct how to turn the key object the user provides with their ProducerRecord into bytes", this.keySerializer));    properties.put(KafkaConstants.VALUE_SERIALIZER, new AbstractMap.SimpleEntry<>("instruct how to turn the value object the user provides with their ProducerRecord into bytes", this.valueSerializer));    properties.put(KafkaConstants.MAX_DOC_COUNT, new AbstractMap.SimpleEntry<>("Maximum number of documents before a commit is forced", this.maxDocCount));    return properties;}
0
public void setConf(Configuration conf)
{    config = conf;}
0
public Configuration getConf()
{    return config;}
0
 List<RabbitDocumentField> getFields()
{    return fields;}
0
 void setDocumentBoost(float documentBoost)
{    this.documentBoost = documentBoost;}
0
 void addField(RabbitDocumentField field)
{    fields.add(field);}
0
 byte[] getBytes()
{    Gson gson = new Gson();    return gson.toJson(this).getBytes();}
0
public String getKey()
{    return key;}
0
public List<Object> getValues()
{    return values;}
0
public Configuration getConf()
{    return config;}
0
public void setConf(Configuration conf)
{    config = conf;}
0
public void open(Configuration conf, String name) throws IOException
{}
0
public void open(IndexWriterParams parameters) throws IOException
{    exchange = parameters.get(RabbitMQConstants.EXCHANGE_NAME);    routingKey = parameters.get(RabbitMQConstants.ROUTING_KEY);    commitSize = parameters.getInt(RabbitMQConstants.COMMIT_SIZE, 250);    commitMode = parameters.get(RabbitMQConstants.COMMIT_MODE, "multiple");    headersStatic = parameters.get(RabbitMQConstants.HEADERS_STATIC, "");    headersDynamic = Arrays.asList(parameters.getStrings(RabbitMQConstants.HEADERS_DYNAMIC, ""));    uri = parameters.get(RabbitMQConstants.SERVER_URI);    client = new RabbitMQClient(uri);    client.openChannel();    binding = parameters.getBoolean(RabbitMQConstants.BINDING, false);    if (binding) {        queueName = parameters.get(RabbitMQConstants.QUEUE_NAME);        queueOptions = parameters.get(RabbitMQConstants.QUEUE_OPTIONS);        exchangeOptions = parameters.get(RabbitMQConstants.EXCHANGE_OPTIONS);        bindingArguments = parameters.get(RabbitMQConstants.BINDING_ARGUMENTS, "");        client.bind(exchange, exchangeOptions, queueName, queueOptions, routingKey, bindingArguments);    }}
0
public void write(NutchDocument doc) throws IOException
{    RabbitDocument rabbitDocument = new RabbitDocument();    for (final Map.Entry<String, NutchField> e : doc) {        RabbitDocument.RabbitDocumentField field = new RabbitDocument.RabbitDocumentField(e.getKey(), e.getValue().getWeight(), e.getValue().getValues());        rabbitDocument.addField(field);    }    rabbitDocument.setDocumentBoost(doc.getWeight());    rabbitMessage.addDocToWrite(rabbitDocument);    if (rabbitMessage.size() >= commitSize) {        commit();    }}
0
public void delete(String url) throws IOException
{    rabbitMessage.addDocToDelete(url);    if (rabbitMessage.size() >= commitSize) {        commit();    }}
0
public void update(NutchDocument doc) throws IOException
{    RabbitDocument rabbitDocument = new RabbitDocument();    for (final Map.Entry<String, NutchField> e : doc) {        RabbitDocument.RabbitDocumentField field = new RabbitDocument.RabbitDocumentField(e.getKey(), e.getValue().getWeight(), e.getValue().getValues());        rabbitDocument.addField(field);    }    rabbitDocument.setDocumentBoost(doc.getWeight());    rabbitMessage.addDocToUpdate(rabbitDocument);    if (rabbitMessage.size() >= commitSize) {        commit();    }}
0
public void commit() throws IOException
{    if (!rabbitMessage.isEmpty()) {        if ("single".equals(commitMode)) {                        for (String s : rabbitMessage.getDocsToDelete()) {                RabbitMQMessage message = new RabbitMQMessage();                message.setBody(s.getBytes());                message.setHeaders(headersStatic);                message.addHeader("action", "delete");                client.publish(exchange, routingKey, message);            }                        for (RabbitDocument rabbitDocument : rabbitMessage.getDocsToUpdate()) {                RabbitMQMessage message = new RabbitMQMessage();                message.setBody(rabbitDocument.getBytes());                addHeaders(message, rabbitDocument);                message.addHeader("action", "update");                client.publish(exchange, routingKey, message);            }                        for (RabbitDocument rabbitDocument : rabbitMessage.getDocsToWrite()) {                RabbitMQMessage message = new RabbitMQMessage();                message.setBody(rabbitDocument.getBytes());                addHeaders(message, rabbitDocument);                message.addHeader("action", "write");                client.publish(exchange, routingKey, message);            }        } else {            RabbitMQMessage message = new RabbitMQMessage();            message.setBody(rabbitMessage.getBytes());            message.setHeaders(headersStatic);            client.publish(exchange, routingKey, message);        }    }    rabbitMessage.clear();}
0
public void close() throws IOException
{        commit();    client.close();}
0
public Map<String, Map.Entry<String, Object>> describe()
{    Map<String, Map.Entry<String, Object>> properties = new LinkedHashMap<>();    properties.put(RabbitMQConstants.SERVER_URI, new AbstractMap.SimpleEntry<>("URI with connection parameters in the form amqp://<username>:<password>@<hostname>:<port>/<virtualHost>", this.uri));    properties.put(RabbitMQConstants.BINDING, new AbstractMap.SimpleEntry<>("Whether the relationship between an exchange and a queue is created automatically. " + "NOTE: Binding between exchanges is not supported.", this.binding));    properties.put(RabbitMQConstants.BINDING_ARGUMENTS, new AbstractMap.SimpleEntry<>("Arguments used in binding. It must have the form key1=value1,key2=value2. " + "This value is only used when the exchange's type is headers and " + "the value of binding property is true. In other cases is ignored.", this.bindingArguments));    properties.put(RabbitMQConstants.EXCHANGE_NAME, new AbstractMap.SimpleEntry<>("Name for the exchange where the messages will be sent.", this.exchange));    properties.put(RabbitMQConstants.EXCHANGE_OPTIONS, new AbstractMap.SimpleEntry<>("Options used when the exchange is created. Only used when the value of binding property is true. " + "It must have the form type=<type>,durable=<durable>", this.exchangeOptions));    properties.put(RabbitMQConstants.QUEUE_NAME, new AbstractMap.SimpleEntry<>("Name of the queue used to create the binding. Only used when the value " + "of binding property is true.", this.queueName));    properties.put(RabbitMQConstants.QUEUE_OPTIONS, new AbstractMap.SimpleEntry<>("Options used when the queue is created. Only used when the value of " + "binding property is true. It must have the form " + "durable=<durable>,exclusive=<exclusive>,auto-delete=<auto-delete>,arguments=<arguments>", this.queueOptions));    properties.put(RabbitMQConstants.ROUTING_KEY, new AbstractMap.SimpleEntry<>("The routing key used to route messages in the exchange. " + "It only makes sense when the exchange type is topic or direct.", this.routingKey));    properties.put(RabbitMQConstants.COMMIT_MODE, new AbstractMap.SimpleEntry<>("single if a message contains only one document. " + "In this case, a header with the action (write, update or delete) will be added. " + "multiple if a message contains all documents.", this.commitMode));    properties.put(RabbitMQConstants.COMMIT_SIZE, new AbstractMap.SimpleEntry<>("Amount of documents to send into each message if the value of commit.mode " + "property is multiple. In single mode this value represents " + "the amount of messages to be sent.", this.commitSize));    properties.put(RabbitMQConstants.HEADERS_STATIC, new AbstractMap.SimpleEntry<>("Headers to add to each message. It must have the form key1=value1,key2=value2.", this.headersStatic));    properties.put(RabbitMQConstants.HEADERS_DYNAMIC, new AbstractMap.SimpleEntry<>("Document's fields to add as headers to each message. " + "It must have the form field1,field2. " + "Only used when the value of commit.mode property is single", this.headersDynamic));    return properties;}
0
private void addHeaders(final RabbitMQMessage message, RabbitDocument document)
{    message.setHeaders(headersStatic);    for (RabbitDocument.RabbitDocumentField rabbitDocumentField : document.getFields()) {        if (headersDynamic.contains(rabbitDocumentField.getKey())) {            message.addHeader(rabbitDocumentField.getKey(), rabbitDocumentField.getValues().get(0));        }    }}
0
 boolean addDocToWrite(RabbitDocument doc)
{    return docsToWrite.add(doc);}
0
 boolean addDocToUpdate(RabbitDocument doc)
{    return docsToUpdate.add(doc);}
0
 boolean addDocToDelete(String url)
{    return docsToDelete.add(url);}
0
 byte[] getBytes()
{    Gson gson = new Gson();    return gson.toJson(this).getBytes();}
0
 boolean isEmpty()
{    return docsToWrite.isEmpty() && docsToUpdate.isEmpty() && docsToDelete.isEmpty();}
0
public List<RabbitDocument> getDocsToWrite()
{    return docsToWrite;}
0
public List<RabbitDocument> getDocsToUpdate()
{    return docsToUpdate;}
0
public List<String> getDocsToDelete()
{    return docsToDelete;}
0
public int size()
{    return docsToWrite.size() + docsToUpdate.size() + docsToDelete.size();}
0
public void clear()
{    docsToWrite.clear();    docsToUpdate.clear();    docsToDelete.clear();}
0
public void open(Configuration conf, String name)
{}
0
public void open(IndexWriterParams parameters)
{    this.type = parameters.get(SolrConstants.SERVER_TYPE, "http");    this.urls = parameters.getStrings(SolrConstants.SERVER_URLS);    this.collection = parameters.get(SolrConstants.COLLECTION);    if (urls == null) {        String message = "Missing SOLR URL.\n" + describe();                throw new RuntimeException(message);    }    this.auth = parameters.getBoolean(SolrConstants.USE_AUTH, false);    this.username = parameters.get(SolrConstants.USERNAME);    this.password = parameters.get(SolrConstants.PASSWORD);    this.solrClients = new ArrayList<>();    switch(type) {        case "http":            for (String url : urls) {                solrClients.add(SolrUtils.getHttpSolrClient(url));            }            break;        case "cloud":            CloudSolrClient sc = this.auth ? SolrUtils.getCloudSolrClient(Arrays.asList(urls), this.username, this.password) : SolrUtils.getCloudSolrClient(Arrays.asList(urls));            sc.setDefaultCollection(this.collection);            solrClients.add(sc);            break;        case "concurrent":                        throw new UnsupportedOperationException("The type \"concurrent\" is not yet supported.");        case "lb":                        throw new UnsupportedOperationException("The type \"lb\" is not yet supported.");        default:            throw new IllegalArgumentException("The type \"" + type + "\" is not supported.");    }    init(parameters);}
1
private void init(IndexWriterParams properties)
{    batchSize = properties.getInt(SolrConstants.COMMIT_SIZE, 1000);    delete = config.getBoolean(IndexerMapReduce.INDEXER_DELETE, false);    weightField = properties.get(SolrConstants.WEIGHT_FIELD, "");        params = new ModifiableSolrParams();    String paramString = config.get(IndexerMapReduce.INDEXER_PARAMS);    if (paramString != null) {        String[] values = paramString.split("&");        for (String v : values) {            String[] kv = v.split("=");            if (kv.length < 2) {                continue;            }            params.add(kv[0], kv[1]);        }    }}
0
public void delete(String key) throws IOException
{        key = key.replaceAll("!", "\\!");    if (delete) {        deleteIds.add(key);        totalDeletes++;    }    if (deleteIds.size() >= batchSize) {        push();    }}
0
public void update(NutchDocument doc) throws IOException
{    write(doc);}
0
public void write(NutchDocument doc) throws IOException
{    final SolrInputDocument inputDoc = new SolrInputDocument();    for (final Entry<String, NutchField> e : doc) {        for (final Object val : e.getValue().getValues()) {                        Object val2 = val;            if (val instanceof Date) {                val2 = DateTimeFormatter.ISO_INSTANT.format(((Date) val).toInstant());            }            if (e.getKey().equals("content") || e.getKey().equals("title")) {                val2 = SolrUtils.stripNonCharCodepoints((String) val);            }            inputDoc.addField(e.getKey(), val2);        }    }    if (!weightField.isEmpty()) {        inputDoc.addField(weightField, doc.getWeight());    }    inputDocs.add(inputDoc);    totalAdds++;    if (inputDocs.size() + numDeletes >= batchSize) {        push();    }}
0
public void close() throws IOException
{    commit();    for (SolrClient solrClient : solrClients) {        solrClient.close();    }}
0
public void commit() throws IOException
{    push();    try {        for (SolrClient solrClient : solrClients) {            if (this.auth) {                UpdateRequest req = new UpdateRequest();                req.setAction(UpdateRequest.ACTION.COMMIT, true, true);                req.setBasicAuthCredentials(this.username, this.password);                solrClient.request(req);            } else {                solrClient.commit();            }        }    } catch (final SolrServerException e) {            }}
1
private void push() throws IOException
{    if (inputDocs.size() > 0) {        try {                                    numDeletes = 0;            UpdateRequest req = new UpdateRequest();            req.add(inputDocs);            req.setAction(UpdateRequest.ACTION.OPTIMIZE, false, false);            req.setParams(params);            if (this.auth) {                req.setBasicAuthCredentials(this.username, this.password);            }            for (SolrClient solrClient : solrClients) {                solrClient.request(req);            }        } catch (final SolrServerException e) {            throw makeIOException(e);        }        inputDocs.clear();    }    if (deleteIds.size() > 0) {        try {                        UpdateRequest req = new UpdateRequest();            req.deleteById(deleteIds);            req.setAction(UpdateRequest.ACTION.OPTIMIZE, false, false);            req.setParams(params);            if (this.auth) {                req.setBasicAuthCredentials(this.username, this.password);            }            for (SolrClient solrClient : solrClients) {                solrClient.request(req);            }        } catch (final SolrServerException e) {                        throw makeIOException(e);        }        deleteIds.clear();    }}
1
private static IOException makeIOException(SolrServerException e)
{    return new IOException(e);}
0
public Configuration getConf()
{    return config;}
0
public void setConf(Configuration conf)
{    config = conf;}
0
public Map<String, Entry<String, Object>> describe()
{    Map<String, Entry<String, Object>> properties = new LinkedHashMap<>();    properties.put(SolrConstants.SERVER_TYPE, new AbstractMap.SimpleEntry<>("Specifies the SolrClient implementation to use. This is a string value of one of the following \"cloud\" or \"http\"." + " The values represent CloudSolrServer or HttpSolrServer respectively.", this.type));    properties.put(SolrConstants.SERVER_URLS, new AbstractMap.SimpleEntry<>("Defines the fully qualified URL of Solr into which data should be indexed. Multiple URL can be provided using comma as a delimiter." + " When the value of type property is cloud, the URL should not include any collections or cores; just the root Solr path.", this.urls == null ? "" : String.join(",", urls)));    properties.put(SolrConstants.COLLECTION, new AbstractMap.SimpleEntry<>("The collection used in requests. Only used when the value of type property is cloud.", this.collection));    properties.put(SolrConstants.COMMIT_SIZE, new AbstractMap.SimpleEntry<>("Defines the number of documents to send to Solr in a single update batch. " + "Decrease when handling very large documents to prevent Nutch from running out of memory.\n" + "Note: It does not explicitly trigger a server side commit.", this.batchSize));    properties.put(SolrConstants.WEIGHT_FIELD, new AbstractMap.SimpleEntry<>("Field's name where the weight of the documents will be written. If it is empty no field will be used.", this.weightField));    properties.put(SolrConstants.USE_AUTH, new AbstractMap.SimpleEntry<>("Whether to enable HTTP basic authentication for communicating with Solr. Use the username and password properties to configure your credentials.", this.auth));    properties.put(SolrConstants.USERNAME, new AbstractMap.SimpleEntry<>("The username of Solr server.", this.username));    properties.put(SolrConstants.PASSWORD, new AbstractMap.SimpleEntry<>("The password of Solr server.", this.password));    return properties;}
0
 static CloudSolrClient getCloudSolrClient(List<String> urls)
{    CloudSolrClient sc = new CloudSolrClient.Builder(urls).withParallelUpdates(true).build();    sc.connect();    return sc;}
0
 static CloudSolrClient getCloudSolrClient(List<String> urls, String username, String password)
{        CredentialsProvider provider = new BasicCredentialsProvider();    UsernamePasswordCredentials credentials = new UsernamePasswordCredentials(username, password);    provider.setCredentials(AuthScope.ANY, credentials);    HttpClient client = HttpClientBuilder.create().setDefaultCredentialsProvider(provider).build();        CloudSolrClient sc = new CloudSolrClient.Builder(urls).withParallelUpdates(true).withHttpClient(client).build();    sc.connect();    return sc;}
0
 static SolrClient getHttpSolrClient(String url)
{    return new HttpSolrClient.Builder(url).build();}
0
 static String stripNonCharCodepoints(String input)
{    StringBuilder retval = new StringBuilder();    char ch;    for (int i = 0; i < input.length(); i++) {        ch = input.charAt(i);                if (        ch % 0x10000 != 0xffff &&         ch % 0x10000 != 0xfffe &&         (ch <= 0xfdd0 || ch >= 0xfdef) && (ch > 0x1F || ch == 0x9 || ch == 0xa || ch == 0xd)) {            retval.append(ch);        }    }    return retval.toString();}
0
public ParseResult filter(Content content, ParseResult parseResult, HTMLMetaTags metaTags, DocumentFragment doc)
{    String lang = null;    Parse parse = parseResult.get(content.getUrl());    if (detect >= 0 && identify < 0) {        lang = detectLanguage(parse, doc);    } else if (detect < 0 && identify >= 0) {        lang = identifyLanguage(parse);    } else if (detect < identify) {        lang = detectLanguage(parse, doc);        if (lang == null) {            lang = identifyLanguage(parse);        }    } else if (identify < detect) {        lang = identifyLanguage(parse);        if (lang == null) {            lang = detectLanguage(parse, doc);        }    } else {                return parseResult;    }    if (lang != null) {        parse.getData().getParseMeta().set(Metadata.LANGUAGE, lang);        return parseResult;    }    return parseResult;}
1
private String detectLanguage(Parse page, DocumentFragment doc)
{    String lang = getLanguageFromMetadata(page.getData().getParseMeta());    if (lang == null) {        LanguageParser parser = new LanguageParser(doc);        lang = parser.getLanguage();    }    if (lang != null) {        return lang;    }    lang = page.getData().getContentMeta().get(Response.CONTENT_LANGUAGE);    return lang;}
0
private String identifyLanguage(Parse parse)
{    StringBuilder text = new StringBuilder();    if (parse == null)        return null;    String title = parse.getData().getTitle();    if (title != null) {        text.append(title.toString());    }    String content = parse.getText();    if (content != null) {        text.append(" ").append(content.toString());    }        String titleandcontent = text.toString();    if (this.contentMaxlength != -1 && titleandcontent.length() > this.contentMaxlength)        titleandcontent = titleandcontent.substring(0, contentMaxlength);    LanguageIdentifier identifier = new LanguageIdentifier(titleandcontent);    if (onlyCertain) {        if (identifier.isReasonablyCertain())            return identifier.getLanguage();        else            return null;    }    return identifier.getLanguage();}
0
private static String getLanguageFromMetadata(Metadata meta)
{    if (meta == null)        return null;        String lang = meta.get("dc.language");    if (lang != null)        return lang;        lang = meta.get("content-language");    if (lang != null)        return lang;        return meta.get("lang");}
0
 String getLanguage()
{    return language;}
0
 void parse(Node node)
{    NodeWalker walker = new NodeWalker(node);    while (walker.hasNext()) {        Node currentNode = walker.nextNode();        String nodeName = currentNode.getNodeName();        short nodeType = currentNode.getNodeType();        if (nodeType == Node.ELEMENT_NODE) {                        if (htmlAttribute == null) {                htmlAttribute = parseLanguage(((Element) currentNode).getAttribute("lang"));            }                        if ("meta".equalsIgnoreCase(nodeName)) {                NamedNodeMap attrs = currentNode.getAttributes();                                if (dublinCore == null) {                    for (int i = 0; i < attrs.getLength(); i++) {                        Node attrnode = attrs.item(i);                        if ("name".equalsIgnoreCase(attrnode.getNodeName())) {                            if ("dc.language".equalsIgnoreCase(attrnode.getNodeValue())) {                                Node valueattr = attrs.getNamedItem("content");                                if (valueattr != null) {                                    dublinCore = parseLanguage(valueattr.getNodeValue());                                }                            }                        }                    }                }                                if (httpEquiv == null) {                    for (int i = 0; i < attrs.getLength(); i++) {                        Node attrnode = attrs.item(i);                        if ("http-equiv".equalsIgnoreCase(attrnode.getNodeName())) {                            if ("content-language".equals(attrnode.getNodeValue().toLowerCase())) {                                Node valueattr = attrs.getNamedItem("content");                                if (valueattr != null) {                                    httpEquiv = parseLanguage(valueattr.getNodeValue());                                }                            }                        }                    }                }            }        }        if ((dublinCore != null) && (htmlAttribute != null) && (httpEquiv != null)) {            return;        }    }}
0
 static final String parseLanguage(String lang)
{    if (lang == null) {        return null;    }    String code = null;    String language = null;        String[] langs = lang.split(",| |;|\\.|\\(|\\)|=", -1);    int i = 0;    while ((language == null) && (i < langs.length)) {                code = langs[i].split("-")[0];        code = code.split("_")[0];                language = (String) LANGUAGES_MAP.get(code.toLowerCase());        i++;    }    return language;}
0
public void setConf(Configuration conf)
{    this.conf = conf;    contentMaxlength = conf.getInt("lang.analyze.max.length", -1);    onlyCertain = conf.getBoolean("lang.identification.only.certain", false);    String[] policy = conf.getStrings("lang.extraction.policy");    for (int i = 0; i < policy.length; i++) {        if (policy[i].equals("detect")) {            detect = i;        } else if (policy[i].equals("identify")) {            identify = i;        }    }}
0
public Configuration getConf()
{    return this.conf;}
0
public NutchDocument filter(NutchDocument doc, Parse parse, Text url, CrawlDatum datum, Inlinks inlinks) throws IndexingException
{        String lang = parse.getData().getParseMeta().get(Metadata.LANGUAGE);        if (lang == null) {        lang = parse.getData().getContentMeta().get(Response.CONTENT_LANGUAGE);    }    if (lang == null || lang.length() == 0) {        lang = "unknown";    }    if (!indexLangs.isEmpty() && !indexLangs.contains(lang)) {        return null;    }    doc.add("lang", lang);    return doc;}
0
public void setConf(Configuration conf)
{    this.conf = conf;    indexLangs = new HashSet<>(conf.getStringCollection("lang.index.languages"));}
0
public Configuration getConf()
{    return this.conf;}
0
public void testMetaHTMLParsing()
{    try {        ParseUtil parser = new ParseUtil(NutchConfiguration.create());        /* loop through the test documents and validate result */        for (int t = 0; t < docs.length; t++) {            Content content = getContent(docs[t]);            Parse parse = parser.parse(content).get(content.getUrl());            Assert.assertEquals(metalanguages[t], (String) parse.getData().getParseMeta().get(Metadata.LANGUAGE));        }    } catch (Exception e) {        e.printStackTrace(System.out);        Assert.fail(e.toString());    }}
0
public void testParseLanguage()
{    String[][] tests = { { "(SCHEME=ISO.639-1) sv", "sv" }, { "(SCHEME=RFC1766) sv-FI", "sv" }, { "(SCHEME=Z39.53) SWE", "sv" }, { "EN_US, SV, EN, EN_UK", "en" }, { "English Swedish", "en" }, { "English, swedish", "en" }, { "English,Swedish", "en" }, { "Other (Svenska)", "sv" }, { "SE", "se" }, { "SV", "sv" }, { "SV charset=iso-8859-1", "sv" }, { "SV-FI", "sv" }, { "SV; charset=iso-8859-1", "sv" }, { "SVE", "sv" }, { "SW", "sw" }, { "SWE", "sv" }, { "SWEDISH", "sv" }, { "Sv", "sv" }, { "Sve", "sv" }, { "Svenska", "sv" }, { "Swedish", "sv" }, { "Swedish, svenska", "sv" }, { "en, sv", "en" }, { "sv", "sv" }, { "sv, be, dk, de, fr, no, pt, ch, fi, en", "sv" }, { "sv,en", "sv" }, { "sv-FI", "sv" }, { "sv-SE", "sv" }, { "sv-en", "sv" }, { "sv-fi", "sv" }, { "sv-se", "sv" }, { "sv; Content-Language: sv", "sv" }, { "sv_SE", "sv" }, { "sve", "sv" }, { "svenska, swedish, engelska, english", "sv" }, { "sw", "sw" }, { "swe", "sv" }, { "swe.SPR.", "sv" }, { "sweden", "sv" }, { "swedish", "sv" }, { "swedish,", "sv" }, { "text/html; charset=sv-SE", "sv" }, { "text/html; sv", "sv" }, { "torp, stuga, uthyres, bed & breakfast", null } };    for (int i = 0; i < 44; i++) {        Assert.assertEquals(tests[i][1], HTMLLanguageParser.LanguageParser.parseLanguage(tests[i][0]));    }}
0
private Content getContent(String text)
{    Metadata meta = new Metadata();    meta.add("Content-Type", "text/html");    return new Content(URL, BASE, text.getBytes(), "text/html", meta, NutchConfiguration.create());}
0
public void testLanguageIndentifier()
{    try {        long total = 0;        LanguageIdentifier identifier;        BufferedReader in = new BufferedReader(new InputStreamReader(this.getClass().getResourceAsStream("test-referencial.txt")));        String line = null;        while ((line = in.readLine()) != null) {            String[] tokens = line.split(";");            if (!tokens[0].equals("")) {                StringBuilder content = new StringBuilder();                                BufferedReader testFile = new BufferedReader(new InputStreamReader(this.getClass().getResourceAsStream(tokens[0]), "UTF-8"));                String testLine = null, lang = null;                while ((testLine = testFile.readLine()) != null) {                    content.append(testLine + "\n");                    testLine = testLine.trim();                    if (testLine.length() > 256) {                        identifier = new LanguageIdentifier(testLine);                        lang = identifier.getLanguage();                        Assert.assertEquals(tokens[1], lang);                    }                }                testFile.close();                                long start = System.currentTimeMillis();                System.out.println(content.toString());                identifier = new LanguageIdentifier(content.toString());                lang = identifier.getLanguage();                System.out.println(lang);                total += System.currentTimeMillis() - start;                Assert.assertEquals(tokens[1], lang);            }        }        in.close();        System.out.println("Total Time=" + total);    } catch (Exception e) {        e.printStackTrace();        Assert.fail(e.toString());    }}
0
protected WebClient modifyWebClient(WebClient client)
{    client.getOptions().setJavaScriptEnabled(enableJavascript);    client.getOptions().setCssEnabled(enableCss);    client.getOptions().setRedirectEnabled(enableRedirect);    if (enableJavascript)        client.setJavaScriptTimeout(javascriptTimeout);    client.getOptions().setThrowExceptionOnScriptError(false);    if (enableRedirect)        client.addWebWindowListener(new HtmlUnitWebWindowListener(maxRedirects));    return client;}
0
public static WebDriver getDriverForPage(String url, Configuration conf)
{    long pageLoadTimout = conf.getLong("page.load.delay", 3);    enableJavascript = conf.getBoolean("htmlunit.enable.javascript", true);    enableCss = conf.getBoolean("htmlunit.enable.css", false);    javascriptTimeout = conf.getLong("htmlunit.javascript.timeout", 3500);    int redirects = Integer.parseInt(conf.get("http.redirect.max", "0"));    enableRedirect = redirects <= 0 ? false : true;    maxRedirects = redirects;    WebDriver driver = null;    try {        driver = new HtmlUnitWebDriver();        driver.manage().timeouts().pageLoadTimeout(pageLoadTimout, TimeUnit.SECONDS);        driver.get(url);    } catch (Exception e) {        if (e instanceof TimeoutException) {                        return driver;        }        cleanUpDriver(driver);        throw new RuntimeException(e);    }    return driver;}
1
public static String getHTMLContent(WebDriver driver, Configuration conf)
{    try {        if (conf.getBoolean("take.screenshot", false))            takeScreenshot(driver, conf);        String innerHtml = "";        if (enableJavascript) {            WebElement body = driver.findElement(By.tagName("body"));            innerHtml = (String) ((JavascriptExecutor) driver).executeScript("return arguments[0].innerHTML;", body);        } else            innerHtml = driver.getPageSource().replaceAll("&amp;", "&");        return innerHtml;    } catch (Exception e) {        TemporaryFilesystem.getDefaultTmpFS().deleteTemporaryFiles();        cleanUpDriver(driver);        throw new RuntimeException(e);    }}
0
public static void cleanUpDriver(WebDriver driver)
{    if (driver != null) {        try {            driver.close();            driver.quit();            TemporaryFilesystem.getDefaultTmpFS().deleteTemporaryFiles();        } catch (Exception e) {            throw new RuntimeException(e);        }    }}
0
public static String getHtmlPage(String url, Configuration conf)
{    WebDriver driver = getDriverForPage(url, conf);    try {        if (conf.getBoolean("take.screenshot", false))            takeScreenshot(driver, conf);        String innerHtml = "";        if (enableJavascript) {            WebElement body = driver.findElement(By.tagName("body"));            innerHtml = (String) ((JavascriptExecutor) driver).executeScript("return arguments[0].innerHTML;", body);        } else            innerHtml = driver.getPageSource().replaceAll("&amp;", "&");        return innerHtml;    } catch (Exception e) {        TemporaryFilesystem.getDefaultTmpFS().deleteTemporaryFiles();        throw new RuntimeException(e);    } finally {        cleanUpDriver(driver);    }}
0
private static void takeScreenshot(WebDriver driver, Configuration conf)
{    try {        String url = driver.getCurrentUrl();        File srcFile = ((TakesScreenshot) driver).getScreenshotAs(OutputType.FILE);                FileSystem fs = FileSystem.get(conf);        if (conf.get("screenshot.location") != null) {            Path screenshotPath = new Path(conf.get("screenshot.location") + "/" + srcFile.getName());            OutputStream os = null;            if (!fs.exists(screenshotPath)) {                                os = fs.create(screenshotPath);            }            InputStream is = new BufferedInputStream(new FileInputStream(srcFile));            IOUtils.copyBytes(is, os, conf);                    } else {                    }    } catch (Exception e) {        cleanUpDriver(driver);        throw new RuntimeException(e);    }}
1
public void webWindowOpened(WebWindowEvent event)
{}
0
public void webWindowContentChanged(WebWindowEvent event)
{    redirectCount++;    if (redirectCount > maxRedirects)        throw new RuntimeException("Redirect Count: " + redirectCount + " exceeded the Maximum Redirects allowed: " + maxRedirects);}
0
public void webWindowClosed(WebWindowEvent event)
{}
0
public void setConf(Configuration conf)
{    this.conf = conf;    this.proxyHost = conf.get("http.proxy.host");    this.proxyPort = conf.getInt("http.proxy.port", 8080);    this.proxyType = Proxy.Type.valueOf(conf.get("http.proxy.type", "HTTP"));    this.proxyException = arrayToMap(conf.getStrings("http.proxy.exception.list"));    this.useProxy = (proxyHost != null && proxyHost.length() > 0);    this.timeout = conf.getInt("http.timeout", 10000);    this.maxContent = conf.getInt("http.content.limit", 1024 * 1024);    this.maxDuration = conf.getInt("http.time.limit", -1);    this.partialAsTruncated = conf.getBoolean("http.partial.truncated", false);    this.userAgent = getAgentString(conf.get("http.agent.name"), conf.get("http.agent.version"), conf.get("http.agent.description"), conf.get("http.agent.url"), conf.get("http.agent.email"));    this.acceptLanguage = conf.get("http.accept.language", acceptLanguage).trim();    this.acceptCharset = conf.get("http.accept.charset", acceptCharset).trim();    this.accept = conf.get("http.accept", accept).trim();    this.mimeTypes = new MimeUtil(conf);        this.useHttp11 = conf.getBoolean("http.useHttp11", true);    this.useHttp2 = conf.getBoolean("http.useHttp2", false);    this.tlsCheckCertificate = conf.getBoolean("http.tls.certificates.check", false);    this.responseTime = conf.getBoolean("http.store.responsetime", true);    this.storeIPAddress = conf.getBoolean("store.ip.address", false);    this.storeHttpRequest = conf.getBoolean("store.http.request", false);    this.storeHttpHeaders = conf.getBoolean("store.http.headers", false);    this.enableIfModifiedsinceHeader = conf.getBoolean("http.enable.if.modified.since.header", true);    this.enableCookieHeader = conf.getBoolean("http.enable.cookie.header", true);    this.robots.setConf(conf);    this.logUtil.setConf(conf);        if (conf.getBoolean("http.agent.rotate", false)) {        String agentsFile = conf.get("http.agent.rotate.file", "agents.txt");        BufferedReader br = null;        try {            Reader reader = conf.getConfResourceAsReader(agentsFile);            br = new BufferedReader(reader);            userAgentNames = new ArrayList<String>();            String word = "";            while ((word = br.readLine()) != null) {                if (!word.trim().isEmpty())                    userAgentNames.add(word.trim());            }            if (userAgentNames.size() == 0) {                                userAgentNames = null;            }        } catch (Exception e) {                        userAgentNames = null;        } finally {            if (br != null) {                try {                    br.close();                } catch (IOException e) {                                }            }        }        if (userAgentNames == null) {                    }    }        if (enableCookieHeader) {        String cookieFile = conf.get("http.agent.host.cookie.file", "cookies.txt");        BufferedReader br = null;        try {            Reader reader = conf.getConfResourceAsReader(cookieFile);            br = new BufferedReader(reader);            hostCookies = new HashMap<String, String>();            String word = "";            while ((word = br.readLine()) != null) {                if (!word.trim().isEmpty()) {                    if (word.indexOf("#") == -1) {                                                String[] parts = word.split("\t");                        if (parts.length == 2) {                            hostCookies.put(parts[0], parts[1]);                        } else {                                                    }                    }                }            }        } catch (Exception e) {                        hostCookies = null;        } finally {            if (br != null) {                try {                    br.close();                } catch (IOException e) {                                }            }        }    }    String[] protocols = conf.getStrings("http.tls.supported.protocols", "TLSv1.2", "TLSv1.1", "TLSv1", "SSLv3");    String[] ciphers = conf.getStrings("http.tls.supported.cipher.suites", "TLS_ECDHE_ECDSA_WITH_AES_256_CBC_SHA384", "TLS_ECDHE_RSA_WITH_AES_256_CBC_SHA384", "TLS_RSA_WITH_AES_256_CBC_SHA256", "TLS_ECDH_ECDSA_WITH_AES_256_CBC_SHA384", "TLS_ECDH_RSA_WITH_AES_256_CBC_SHA384", "TLS_DHE_RSA_WITH_AES_256_CBC_SHA256", "TLS_DHE_DSS_WITH_AES_256_CBC_SHA256", "TLS_ECDHE_ECDSA_WITH_AES_256_CBC_SHA", "TLS_ECDHE_RSA_WITH_AES_256_CBC_SHA", "TLS_RSA_WITH_AES_256_CBC_SHA", "TLS_ECDH_ECDSA_WITH_AES_256_CBC_SHA", "TLS_ECDH_RSA_WITH_AES_256_CBC_SHA", "TLS_DHE_RSA_WITH_AES_256_CBC_SHA", "TLS_DHE_DSS_WITH_AES_256_CBC_SHA", "TLS_ECDHE_ECDSA_WITH_AES_128_CBC_SHA256", "TLS_ECDHE_RSA_WITH_AES_128_CBC_SHA256", "TLS_RSA_WITH_AES_128_CBC_SHA256", "TLS_ECDH_ECDSA_WITH_AES_128_CBC_SHA256", "TLS_ECDH_RSA_WITH_AES_128_CBC_SHA256", "TLS_DHE_RSA_WITH_AES_128_CBC_SHA256", "TLS_DHE_DSS_WITH_AES_128_CBC_SHA256", "TLS_ECDHE_ECDSA_WITH_AES_128_CBC_SHA", "TLS_ECDHE_RSA_WITH_AES_128_CBC_SHA", "TLS_RSA_WITH_AES_128_CBC_SHA", "TLS_ECDH_ECDSA_WITH_AES_128_CBC_SHA", "TLS_ECDH_RSA_WITH_AES_128_CBC_SHA", "TLS_DHE_RSA_WITH_AES_128_CBC_SHA", "TLS_DHE_DSS_WITH_AES_128_CBC_SHA", "TLS_ECDHE_ECDSA_WITH_RC4_128_SHA", "TLS_ECDHE_RSA_WITH_RC4_128_SHA", "SSL_RSA_WITH_RC4_128_SHA", "TLS_ECDH_ECDSA_WITH_RC4_128_SHA", "TLS_ECDH_RSA_WITH_RC4_128_SHA", "TLS_ECDHE_ECDSA_WITH_3DES_EDE_CBC_SHA", "TLS_ECDHE_RSA_WITH_3DES_EDE_CBC_SHA", "SSL_RSA_WITH_3DES_EDE_CBC_SHA", "TLS_ECDH_ECDSA_WITH_3DES_EDE_CBC_SHA", "TLS_ECDH_RSA_WITH_3DES_EDE_CBC_SHA", "SSL_DHE_RSA_WITH_3DES_EDE_CBC_SHA", "SSL_DHE_DSS_WITH_3DES_EDE_CBC_SHA", "SSL_RSA_WITH_RC4_128_MD5", "TLS_EMPTY_RENEGOTIATION_INFO_SCSV", "TLS_RSA_WITH_NULL_SHA256", "TLS_ECDHE_ECDSA_WITH_NULL_SHA", "TLS_ECDHE_RSA_WITH_NULL_SHA", "SSL_RSA_WITH_NULL_SHA", "TLS_ECDH_ECDSA_WITH_NULL_SHA", "TLS_ECDH_RSA_WITH_NULL_SHA", "SSL_RSA_WITH_NULL_MD5", "SSL_RSA_WITH_DES_CBC_SHA", "SSL_DHE_RSA_WITH_DES_CBC_SHA", "SSL_DHE_DSS_WITH_DES_CBC_SHA", "TLS_KRB5_WITH_RC4_128_SHA", "TLS_KRB5_WITH_RC4_128_MD5", "TLS_KRB5_WITH_3DES_EDE_CBC_SHA", "TLS_KRB5_WITH_3DES_EDE_CBC_MD5", "TLS_KRB5_WITH_DES_CBC_SHA", "TLS_KRB5_WITH_DES_CBC_MD5");    tlsPreferredProtocols = new HashSet<String>(Arrays.asList(protocols));    tlsPreferredCipherSuites = new HashSet<String>(Arrays.asList(ciphers));    logConf();}
1
public Configuration getConf()
{    return this.conf;}
0
public ProtocolOutput getProtocolOutput(Text url, CrawlDatum datum)
{    String urlString = url.toString();    try {        URL u = new URL(urlString);        long startTime = System.currentTimeMillis();                Response response = getResponse(u, datum, false);        if (this.responseTime) {            int elapsedTime = (int) (System.currentTimeMillis() - startTime);            datum.getMetaData().put(RESPONSE_TIME, new IntWritable(elapsedTime));        }        int code = response.getCode();        datum.getMetaData().put(Nutch.PROTOCOL_STATUS_CODE_KEY, new Text(Integer.toString(code)));        byte[] content = response.getContent();        Content c = new Content(u.toString(), u.toString(), (content == null ? EMPTY_CONTENT : content), response.getHeader("Content-Type"), response.getHeaders(), mimeTypes);        if (code == 200) {                        return new ProtocolOutput(c);        } else if (code >= 300 && code < 400) {                        String location = response.getHeader("Location");                        if (location == null)                location = response.getHeader("location");            if (location == null)                location = "";            u = new URL(u, location);            int protocolStatusCode;            switch(code) {                case                 300:                    protocolStatusCode = ProtocolStatus.MOVED;                    break;                                case 301:                case                 305:                    protocolStatusCode = ProtocolStatus.MOVED;                    break;                                case 302:                                case 303:                case                 307:                    protocolStatusCode = ProtocolStatus.TEMP_MOVED;                    break;                case                 304:                    protocolStatusCode = ProtocolStatus.NOTMODIFIED;                    break;                default:                    protocolStatusCode = ProtocolStatus.MOVED;            }                        return new ProtocolOutput(c, new ProtocolStatus(protocolStatusCode, u));        } else if (code == 400) {                        if (logger.isTraceEnabled()) {                logger.trace("400 Bad request: " + u);            }            return new ProtocolOutput(c, new ProtocolStatus(ProtocolStatus.GONE, u));        } else if (code == 401) {                        if (logger.isTraceEnabled()) {                logger.trace("401 Authentication Required");            }            return new ProtocolOutput(c, new ProtocolStatus(ProtocolStatus.ACCESS_DENIED, "Authentication required: " + urlString));        } else if (code == 404) {            return new ProtocolOutput(c, new ProtocolStatus(ProtocolStatus.NOTFOUND, u));        } else if (code == 410) {                        return new ProtocolOutput(c, new ProtocolStatus(ProtocolStatus.GONE, "Http: " + code + " url=" + u));        } else {            return new ProtocolOutput(c, new ProtocolStatus(ProtocolStatus.EXCEPTION, "Http code=" + code + ", url=" + u));        }    } catch (Throwable e) {        if (logger.isDebugEnabled() || !logUtil.logShort(e)) {                    } else {                    }        return new ProtocolOutput(null, new ProtocolStatus(e));    }}
1
public String getProxyHost()
{    return proxyHost;}
0
public int getProxyPort()
{    return proxyPort;}
0
public boolean useProxy(URL url)
{    return useProxy(url.getHost());}
0
public boolean useProxy(URI uri)
{    return useProxy(uri.getHost());}
0
public boolean useProxy(String host)
{    if (useProxy && proxyException.containsKey(host)) {        return false;    }    return useProxy;}
0
public int getTimeout()
{    return timeout;}
0
public boolean isIfModifiedSinceEnabled()
{    return enableIfModifiedsinceHeader;}
0
public boolean isCookieEnabled()
{    return enableCookieHeader;}
0
public boolean isStoreIPAddress()
{    return storeIPAddress;}
0
public boolean isStoreHttpRequest()
{    return storeHttpRequest;}
0
public boolean isStoreHttpHeaders()
{    return storeHttpHeaders;}
0
public int getMaxContent()
{    return maxContent;}
0
public int getMaxDuration()
{    return maxDuration;}
0
public boolean isStorePartialAsTruncated()
{    return partialAsTruncated;}
0
public String getUserAgent()
{    if (userAgentNames != null) {        return userAgentNames.get(ThreadLocalRandom.current().nextInt(userAgentNames.size()));    }    return userAgent;}
0
public String getCookie(URL url)
{    if (hostCookies != null) {        return hostCookies.get(url.getHost());    }    return null;}
0
public String getAcceptLanguage()
{    return acceptLanguage;}
0
public String getAcceptCharset()
{    return acceptCharset;}
0
public String getAccept()
{    return accept;}
0
public boolean getUseHttp11()
{    return useHttp11;}
0
public boolean isTlsCheckCertificates()
{    return tlsCheckCertificate;}
0
public Set<String> getTlsPreferredCipherSuites()
{    return tlsPreferredCipherSuites;}
0
public Set<String> getTlsPreferredProtocols()
{    return tlsPreferredProtocols;}
0
private static String getAgentString(String agentName, String agentVersion, String agentDesc, String agentURL, String agentEmail)
{    if ((agentName == null) || (agentName.trim().length() == 0)) {                if (LOG.isErrorEnabled()) {                    }    }    StringBuffer buf = new StringBuffer();    buf.append(agentName);    if (agentVersion != null && !agentVersion.trim().isEmpty()) {        buf.append("/");        buf.append(agentVersion);    }    if (((agentDesc != null) && (agentDesc.length() != 0)) || ((agentEmail != null) && (agentEmail.length() != 0)) || ((agentURL != null) && (agentURL.length() != 0))) {        buf.append(" (");        if ((agentDesc != null) && (agentDesc.length() != 0)) {            buf.append(agentDesc);            if ((agentURL != null) || (agentEmail != null))                buf.append("; ");        }        if ((agentURL != null) && (agentURL.length() != 0)) {            buf.append(agentURL);            if (agentEmail != null)                buf.append("; ");        }        if ((agentEmail != null) && (agentEmail.length() != 0))            buf.append(agentEmail);        buf.append(")");    }    return buf.toString();}
1
protected void logConf()
{    if (logger.isInfoEnabled()) {                                                                            }}
1
public byte[] processGzipEncoded(byte[] compressed, URL url) throws IOException
{    if (LOG.isTraceEnabled()) {        LOG.trace("uncompressing....");    }        if (compressed.length == 0)        return compressed;    byte[] content;    if (getMaxContent() >= 0) {        content = GZIPUtils.unzipBestEffort(compressed, getMaxContent());    } else {        content = GZIPUtils.unzipBestEffort(compressed);    }    if (content == null)        throw new IOException("unzipBestEffort returned null");    if (LOG.isTraceEnabled()) {        LOG.trace("fetched " + compressed.length + " bytes of compressed content (expanded to " + content.length + " bytes) from " + url);    }    return content;}
0
public byte[] processDeflateEncoded(byte[] compressed, URL url) throws IOException
{        if (compressed.length == 0)        return compressed;    if (LOG.isTraceEnabled()) {        LOG.trace("inflating....");    }    byte[] content;    if (getMaxContent() >= 0) {        content = DeflateUtils.inflateBestEffort(compressed, getMaxContent());    } else {        content = DeflateUtils.inflateBestEffort(compressed);    }    if (content == null)        throw new IOException("inflateBestEffort returned null");    if (LOG.isTraceEnabled()) {        LOG.trace("fetched " + compressed.length + " bytes of compressed content (expanded to " + content.length + " bytes) from " + url);    }    return content;}
0
protected static void main(HttpBase http, String[] args) throws Exception
{    String url = null;    String usage = "Usage: Http [-verbose] [-timeout N] url";    if (args.length == 0) {        System.err.println(usage);        System.exit(-1);    }    for (int i = 0; i < args.length; i++) {                if (args[i].equals("-timeout")) {                        http.timeout = Integer.parseInt(args[++i]) * 1000;        } else if (args[i].equals("-verbose")) {                } else if (i != args.length - 1) {            System.err.println(usage);            System.exit(-1);        } else                        url = args[i];    }    ProtocolOutput out = http.getProtocolOutput(new Text(url), new CrawlDatum());    Content content = out.getContent();    System.out.println("Status: " + out.getStatus());    if (content != null) {        System.out.println("Content Type: " + content.getContentType());        System.out.println("Content Length: " + content.getMetadata().get(Response.CONTENT_LENGTH));        System.out.println("Content:");        String text = new String(content.getContent());        System.out.println(text);    }}
0
public BaseRobotRules getRobotRules(Text url, CrawlDatum datum, List<Content> robotsTxtContent)
{    return robots.getRobotRulesSet(this, url, robotsTxtContent);}
0
private HashMap<String, String> arrayToMap(String[] input)
{    if (input == null || input.length == 0) {        return new HashMap<String, String>();    }    HashMap<String, String> hm = new HashMap<>();    for (int i = 0; i < input.length; i++) {        if (!"".equals(input[i].trim())) {            hm.put(input[i], input[i]);        }    }    return hm;}
0
public void setConf(Configuration conf)
{    super.setConf(conf);    allowForbidden = conf.getBoolean("http.robots.403.allow", true);}
0
protected static String getCacheKey(URL url)
{        String protocol = url.getProtocol().toLowerCase();            String host = url.getHost().toLowerCase();    int port = url.getPort();    if (port == -1) {        port = url.getDefaultPort();    }    /*     * Robot rules apply only to host, protocol, and port where robots.txt is     * hosted (cf. NUTCH-1752). Consequently     */    String cacheKey = protocol + ":" + host + ":" + port;    return cacheKey;}
0
public BaseRobotRules getRobotRulesSet(Protocol http, URL url, List<Content> robotsTxtContent)
{    if (LOG.isTraceEnabled() && isWhiteListed(url)) {        LOG.trace("Ignoring robots.txt (host is whitelisted) for URL: {}", url);    }    String cacheKey = getCacheKey(url);    BaseRobotRules robotRules = CACHE.get(cacheKey);    if (robotRules != null) {                return robotRules;    } else if (LOG.isTraceEnabled()) {        LOG.trace("cache miss " + url);    }    boolean cacheRule = true;    URL redir = null;    if (isWhiteListed(url)) {                        robotRules = EMPTY_RULES;                    } else {        try {            URL robotsUrl = new URL(url, "/robots.txt");            Response response = ((HttpBase) http).getResponse(robotsUrl, new CrawlDatum(), true);            if (robotsTxtContent != null) {                addRobotsContent(robotsTxtContent, robotsUrl, response);            }                        if (response.getCode() == 301 || response.getCode() == 302) {                String redirection = response.getHeader("Location");                if (redirection == null) {                                        redirection = response.getHeader("location");                }                if (redirection != null) {                    if (!redirection.startsWith("http")) {                                                redir = new URL(url, redirection);                    } else {                        redir = new URL(redirection);                    }                    response = ((HttpBase) http).getResponse(redir, new CrawlDatum(), true);                    if (robotsTxtContent != null) {                        addRobotsContent(robotsTxtContent, redir, response);                    }                }            }            if (            response.getCode() == 200)                robotRules = parseRules(url.toString(), response.getContent(), response.getHeader("Content-Type"), agentNames);            else if ((response.getCode() == 403) && (!allowForbidden))                                robotRules = FORBID_ALL_RULES;            else if (response.getCode() >= 500) {                                cacheRule = false;                robotRules = EMPTY_RULES;            } else                                robotRules = EMPTY_RULES;        } catch (Throwable t) {            if (LOG.isInfoEnabled()) {                            }                        cacheRule = false;            robotRules = EMPTY_RULES;        }    }    if (cacheRule) {                CACHE.put(cacheKey, robotRules);        if (redir != null && !redir.getHost().equalsIgnoreCase(url.getHost()) && "/robots.txt".equals(redir.getFile())) {                                    CACHE.put(getCacheKey(redir), robotRules);        }    }    return robotRules;}
1
protected void addRobotsContent(List<Content> robotsTxtContent, URL robotsUrl, Response robotsResponse)
{    byte[] robotsBytes = robotsResponse.getContent();    if (robotsBytes == null)        robotsBytes = new byte[0];    Content content = new Content(robotsUrl.toString(), robotsUrl.toString(), robotsBytes, robotsResponse.getHeader("Content-Type"), robotsResponse.getHeaders(), getConf());    robotsTxtContent.add(content);}
0
public void testRobotsAgent()
{    rules = parser.parseRules("testRobotsAgent", ROBOTS_STRING.getBytes(), CONTENT_TYPE, SINGLE_AGENT);    for (int counter = 0; counter < TEST_PATHS.length; counter++) {        Assert.assertTrue("testing on agent (" + SINGLE_AGENT + "), and " + "path " + TEST_PATHS[counter] + " got " + rules.isAllowed(TEST_PATHS[counter]), rules.isAllowed(TEST_PATHS[counter]) == RESULTS[counter]);    }    rules = parser.parseRules("testRobotsAgent", ROBOTS_STRING.getBytes(), CONTENT_TYPE, MULTIPLE_AGENTS);    for (int counter = 0; counter < TEST_PATHS.length; counter++) {        Assert.assertTrue("testing on agents (" + MULTIPLE_AGENTS + "), and " + "path " + TEST_PATHS[counter] + " got " + rules.isAllowed(TEST_PATHS[counter]), rules.isAllowed(TEST_PATHS[counter]) == RESULTS[counter]);    }}
0
public void testCrawlDelay()
{            rules = parser.parseRules("testCrawlDelay", ROBOTS_STRING.getBytes(), CONTENT_TYPE, SINGLE_AGENT);    Assert.assertTrue("testing crawl delay for agent " + SINGLE_AGENT + " : ", (rules.getCrawlDelay() == 10000));        rules = parser.parseRules("testCrawlDelay", ROBOTS_STRING.getBytes(), CONTENT_TYPE, UNKNOWN_AGENT);    Assert.assertTrue("testing crawl delay for agent " + UNKNOWN_AGENT + " : ", (rules.getCrawlDelay() == Long.MIN_VALUE));}
0
public void openChannel() throws IOException
{    channel = connection.createChannel();}
0
public void bind(String exchangeName, String exchangeOptions, String queueName, String queueOptions, String bindingKey, String bindingArguments) throws IOException
{    String exchangeType = exchangeDeclare(exchangeName, exchangeOptions);    queueDeclare(queueName, queueOptions);    switch(exchangeType) {        case "fanout":            channel.queueBind(queueName, exchangeName, "");            break;        case "direct":            channel.queueBind(queueName, exchangeName, getValue(bindingKey, DEFAULT_ROUTING_KEY));            break;        case "headers":            channel.queueBind(queueName, exchangeName, "", RabbitMQOptionParser.parseOptionAndConvertValue(bindingArguments));            break;        case "topic":            channel.queueBind(queueName, exchangeName, getValue(bindingKey, DEFAULT_ROUTING_KEY));            break;        default:            break;    }}
0
public void publish(String exchangeName, String routingKey, RabbitMQMessage message) throws IOException
{    channel.basicPublish(getValue(exchangeName, DEFAULT_EXCHANGE_NAME), getValue(routingKey, DEFAULT_ROUTING_KEY), new AMQP.BasicProperties.Builder().contentType(message.getContentType()).headers(message.getHeaders()).build(), message.getBody());}
0
public void close() throws IOException
{    try {        channel.close();        connection.close();    } catch (TimeoutException e) {        throw makeIOException(e);    }}
0
private String exchangeDeclare(String name, String options) throws IOException
{    Map<String, String> values = RabbitMQOptionParser.parseOption(options);    String type = values.getOrDefault("type", DEFAULT_EXCHANGE_TYPE);    channel.exchangeDeclare(getValue(name, DEFAULT_EXCHANGE_NAME), type, Boolean.parseBoolean(values.getOrDefault("durable", DEFAULT_EXCHANGE_DURABLE)));    return type;}
0
private void queueDeclare(String name, String options) throws IOException
{    Map<String, String> values = RabbitMQOptionParser.parseOption(options);    channel.queueDeclare(getValue(name, DEFAULT_QUEUE_NAME), Boolean.parseBoolean(values.getOrDefault("durable", DEFAULT_QUEUE_DURABLE)), Boolean.parseBoolean(values.getOrDefault("exclusive", DEFAULT_QUEUE_EXCLUSIVE)), Boolean.parseBoolean(values.getOrDefault("auto-delete", DEFAULT_QUEUE_AUTO_DELETE)), RabbitMQOptionParser.parseSubOption(values.getOrDefault("arguments", DEFAULT_QUEUE_ARGUMENTS)));}
0
private static String getValue(String value, String defaultValue)
{    if (value == null || value.trim().isEmpty()) {        return defaultValue;    }    return value;}
0
private static Integer getValue(Integer value, Integer defaultValue)
{    if (value == null) {        return defaultValue;    }    return value;}
0
private static IOException makeIOException(Exception e)
{    return new IOException(e);}
0
public Map<String, Object> getHeaders()
{    return headers;}
0
public void setHeaders(final Map<String, Object> headers)
{    this.headers = headers;}
0
public void setHeaders(final String headers)
{    this.headers = RabbitMQOptionParser.parseOptionAndConvertValue(headers);}
0
public void addHeader(final String key, final Object value)
{    this.headers.put(key, value);}
0
public byte[] getBody()
{    return body;}
0
public void setBody(final byte[] body)
{    this.body = body;}
0
public String getContentType()
{    return contentType;}
0
public void setContentType(final String contentType)
{    this.contentType = contentType;}
0
 static Map<String, String> parseOption(final String option)
{    Map<String, String> values = new HashMap<>();    if (option.isEmpty()) {        return values;    }    String[] split = option.split(",");    for (String s : split) {        String[] ss = s.split("=");        values.put(ss[0], ss[1]);    }    return values;}
0
 static Map<String, Object> parseOptionAndConvertValue(final String option)
{    Map<String, Object> values = new HashMap<>();    if (option.isEmpty()) {        return values;    }    String[] split = option.split(",");    for (String s : split) {        String[] ss = s.split("=");        values.put(ss[0], convert(ss[1]));    }    return values;}
0
 static Map<String, Object> parseSubOption(final String subOption)
{    Map<String, Object> values = new HashMap<>();    if (subOption.isEmpty()) {        return values;    }    String[] split = subOption.replaceAll("\\{|}", "").split(";");    for (String s : split) {        String[] ss = s.split(":");        values.put(ss[0], convert(ss[1]));    }    return values;}
0
private static Object convert(String s)
{    try {        return Integer.parseInt(s);    } catch (Exception ex) {        }    try {        return Float.parseFloat(s);    } catch (Exception ex) {        }    if (s.equalsIgnoreCase("true") || s.equalsIgnoreCase("false")) {        return Boolean.parseBoolean(s);    }    return s;}
0
protected boolean accept()
{    return sign;}
0
protected String hostOrDomain()
{    return hostOrDomain;}
0
protected String regex()
{    return regex;}
0
public String filter(String url)
{    String host = null;    String domain = null;    if (hasHostDomainRules) {        host = URLUtil.getHost(url);        try {            domain = URLUtil.getDomainName(url);        } catch (MalformedURLException e) {                }            }    for (RegexRule rule : rules) {                if (rule.hostOrDomain() != null && !rule.hostOrDomain().equals(host) && !rule.hostOrDomain().equals(domain)) {                        continue;        }                if (rule.match(url)) {            return rule.accept() ? url : null;        }    }    ;    return null;}
1
public void setConf(Configuration conf)
{    this.conf = conf;    Reader reader = null;    try {        reader = getRulesReader(conf);    } catch (Exception e) {        if (LOG.isErrorEnabled()) {                    }        throw new RuntimeException(e.getMessage(), e);    }    try {        rules = readRules(reader);    } catch (IOException e) {        if (LOG.isErrorEnabled()) {                    }        throw new RuntimeException(e.getMessage(), e);    }}
1
public Configuration getConf()
{    return this.conf;}
0
private List<RegexRule> readRules(Reader reader) throws IOException, IllegalArgumentException
{    BufferedReader in = new BufferedReader(reader);    List<RegexRule> rules = new ArrayList<RegexRule>();    String line;    String hostOrDomain = null;    while ((line = in.readLine()) != null) {        if (line.length() == 0) {            continue;        }        char first = line.charAt(0);        boolean sign = false;        switch(first) {            case '+':                sign = true;                break;            case '-':                sign = false;                break;            case ' ':            case '\n':            case             '#':                continue;            case '>':                hostOrDomain = line.substring(1).trim();                hasHostDomainRules = true;                continue;            case '<':                hostOrDomain = null;                continue;            default:                throw new IOException("Invalid first character: " + line);        }        String regex = line.substring(1);        if (LOG.isTraceEnabled()) {            LOG.trace("Adding rule [" + regex + "] for " + hostOrDomain);        }        RegexRule rule = createRule(sign, regex, hostOrDomain);        rules.add(rule);    }    return rules;}
0
public static void main(RegexURLFilterBase filter, String[] args) throws IOException, IllegalArgumentException
{    BufferedReader in = new BufferedReader(new InputStreamReader(System.in));    String line;    while ((line = in.readLine()) != null) {        String out = filter.filter(line);        if (out != null) {            System.out.print("+");            System.out.println(out);        } else {            System.out.print("-");            System.out.println(line);        }    }}
0
protected void bench(int loops, String file)
{    try {        bench(loops, new FileReader(SAMPLES + SEPARATOR + file + ".rules"), new FileReader(SAMPLES + SEPARATOR + file + ".urls"));    } catch (Exception e) {        Assert.fail(e.toString());    }}
0
protected void bench(int loops, Reader rules, Reader urls)
{    long start = System.currentTimeMillis();    try {        URLFilter filter = getURLFilter(rules);        FilteredURL[] expected = readURLFile(urls);        for (int i = 0; i < loops; i++) {            test(filter, expected);        }    } catch (Exception e) {        Assert.fail(e.toString());    }    }
1
protected void bench(int loops, String rulesFile, String urlsFile)
{    try {        bench(loops, new FileReader(SAMPLES + SEPARATOR + rulesFile), new FileReader(SAMPLES + SEPARATOR + urlsFile));    } catch (Exception e) {        Assert.fail(e.toString());    }}
0
protected void test(String rulesFile, String urlsFile)
{    try {        test(new FileReader(SAMPLES + SEPARATOR + rulesFile), new FileReader(SAMPLES + SEPARATOR + urlsFile));    } catch (Exception e) {        Assert.fail(e.toString());    }}
0
protected void test(String file)
{    try {        test(new FileReader(SAMPLES + SEPARATOR + file + ".rules"), new FileReader(SAMPLES + SEPARATOR + file + ".urls"));    } catch (Exception e) {        Assert.fail(e.toString());    }}
0
protected void test(Reader rules, Reader urls)
{    try {        test(getURLFilter(rules), readURLFile(urls));    } catch (Exception e) {        Assert.fail(e.toString());    }}
0
protected void test(URLFilter filter, FilteredURL[] expected)
{    for (int i = 0; i < expected.length; i++) {        String result = filter.filter(expected[i].url);        if (result != null) {            Assert.assertTrue(expected[i].url, expected[i].sign);        } else {            Assert.assertFalse(expected[i].url, expected[i].sign);        }    }}
0
private static FilteredURL[] readURLFile(Reader reader) throws IOException
{    BufferedReader in = new BufferedReader(reader);    List<FilteredURL> list = new ArrayList<FilteredURL>();    String line;    while ((line = in.readLine()) != null) {        if (line.length() != 0) {            list.add(new FilteredURL(line));        }    }    return (FilteredURL[]) list.toArray(new FilteredURL[list.size()]);}
0
public static WebDriver getDriverForPage(String url, Configuration conf)
{    WebDriver driver = null;    long pageLoadWait = conf.getLong("page.load.delay", 3);    try {        String driverType = conf.get("selenium.driver", "firefox");        boolean enableHeadlessMode = conf.getBoolean("selenium.enable.headless", false);        switch(driverType) {            case "firefox":                String geckoDriverPath = conf.get("selenium.grid.binary", "/root/geckodriver");                driver = createFirefoxWebDriver(geckoDriverPath, enableHeadlessMode);                break;            case "chrome":                String chromeDriverPath = conf.get("selenium.grid.binary", "/root/chromedriver");                driver = createChromeWebDriver(chromeDriverPath, enableHeadlessMode);                break;                        case "remote":                String seleniumHubHost = conf.get("selenium.hub.host", "localhost");                int seleniumHubPort = Integer.parseInt(conf.get("selenium.hub.port", "4444"));                String seleniumHubPath = conf.get("selenium.hub.path", "/wd/hub");                String seleniumHubProtocol = conf.get("selenium.hub.protocol", "http");                URL seleniumHubUrl = new URL(seleniumHubProtocol, seleniumHubHost, seleniumHubPort, seleniumHubPath);                String seleniumGridDriver = conf.get("selenium.grid.driver", "firefox");                switch(seleniumGridDriver) {                    case "firefox":                        driver = createFirefoxRemoteWebDriver(seleniumHubUrl, enableHeadlessMode);                        break;                    case "chrome":                        driver = createChromeRemoteWebDriver(seleniumHubUrl, enableHeadlessMode);                        break;                    case "random":                        driver = createRandomRemoteWebDriver(seleniumHubUrl, enableHeadlessMode);                        break;                    default:                                                driver = createDefaultRemoteWebDriver(seleniumHubUrl, enableHeadlessMode);                        break;                }                break;            default:                                FirefoxOptions options = new FirefoxOptions();                driver = new FirefoxDriver(options);                break;        }                driver.manage().timeouts().pageLoadTimeout(pageLoadWait, TimeUnit.SECONDS);        driver.get(url);    } catch (Exception e) {        if (e instanceof TimeoutException) {                        return driver;        } else {                    }        cleanUpDriver(driver);        throw new RuntimeException(e);    }    return driver;}
1
public static WebDriver createFirefoxWebDriver(String firefoxDriverPath, boolean enableHeadlessMode)
{    System.setProperty("webdriver.gecko.driver", firefoxDriverPath);    FirefoxOptions firefoxOptions = new FirefoxOptions();    if (enableHeadlessMode) {        firefoxOptions.addArguments("--headless");    }    WebDriver driver = new FirefoxDriver(firefoxOptions);    return driver;}
0
public static WebDriver createChromeWebDriver(String chromeDriverPath, boolean enableHeadlessMode)
{        System.setProperty("webdriver.chrome.driver", chromeDriverPath);    ChromeOptions chromeOptions = new ChromeOptions();    chromeOptions.addArguments("--no-sandbox");    chromeOptions.addArguments("--disable-extensions");        if (enableHeadlessMode) {        chromeOptions.addArguments("--headless");    }    WebDriver driver = new ChromeDriver(chromeOptions);    return driver;}
0
public static WebDriver createOperaWebDriver(String operaDriverPath, boolean enableHeadlessMode)
{        System.setProperty("webdriver.opera.driver", operaDriverPath);    OperaOptions operaOptions = new OperaOptions();        operaOptions.addArguments("--no-sandbox");    operaOptions.addArguments("--disable-extensions");        if (enableHeadlessMode) {        operaOptions.addArguments("--headless");    }    WebDriver driver = new OperaDriver(operaOptions);    return driver;}
0
public static RemoteWebDriver createFirefoxRemoteWebDriver(URL seleniumHubUrl, boolean enableHeadlessMode)
{    FirefoxOptions firefoxOptions = new FirefoxOptions();    if (enableHeadlessMode) {        firefoxOptions.setHeadless(true);    }    RemoteWebDriver driver = new RemoteWebDriver(seleniumHubUrl, firefoxOptions);    return driver;}
0
public static RemoteWebDriver createChromeRemoteWebDriver(URL seleniumHubUrl, boolean enableHeadlessMode)
{    ChromeOptions chromeOptions = new ChromeOptions();    if (enableHeadlessMode) {        chromeOptions.setHeadless(true);    }    RemoteWebDriver driver = new RemoteWebDriver(seleniumHubUrl, chromeOptions);    return driver;}
0
public static RemoteWebDriver createRandomRemoteWebDriver(URL seleniumHubUrl, boolean enableHeadlessMode)
{                Random r = new Random();    int min = 0;                                int max = 1;    int num = r.nextInt((max - min) + 1) + min;    if (num == 0) {        return createFirefoxRemoteWebDriver(seleniumHubUrl, enableHeadlessMode);    }    return createChromeRemoteWebDriver(seleniumHubUrl, enableHeadlessMode);}
0
public static RemoteWebDriver createDefaultRemoteWebDriver(URL seleniumHubUrl, boolean enableHeadlessMode)
{    return createFirefoxRemoteWebDriver(seleniumHubUrl, enableHeadlessMode);}
0
public static void cleanUpDriver(WebDriver driver)
{    if (driver != null) {        try {                        driver.quit();            TemporaryFilesystem.getDefaultTmpFS().deleteTemporaryFiles();        } catch (Exception e) {                            }    }}
1
public static String getHtmlPage(String url, Configuration conf)
{    WebDriver driver = getDriverForPage(url, conf);    try {        if (conf.getBoolean("take.screenshot", false)) {            takeScreenshot(driver, conf);        }        String innerHtml = driver.findElement(By.tagName("body")).getAttribute("innerHTML");        return innerHtml;            } catch (Exception e) {        TemporaryFilesystem.getDefaultTmpFS().deleteTemporaryFiles();                        throw new RuntimeException(e);    } finally {        cleanUpDriver(driver);    }}
1
public static String getHtmlPage(String url)
{    return getHtmlPage(url, null);}
0
private static void takeScreenshot(WebDriver driver, Configuration conf)
{    try {        String url = driver.getCurrentUrl();        File srcFile = ((TakesScreenshot) driver).getScreenshotAs(OutputType.FILE);                FileSystem fs = FileSystem.get(conf);        if (conf.get("screenshot.location") != null) {            Path screenshotPath = new Path(conf.get("screenshot.location") + "/" + srcFile.getName());            OutputStream os = null;            if (!fs.exists(screenshotPath)) {                                os = fs.create(screenshotPath);            }            InputStream is = new BufferedInputStream(new FileInputStream(srcFile));            IOUtils.copyBytes(is, os, conf);                    } else {                    }    } catch (Exception e) {                cleanUpDriver(driver);        throw new RuntimeException(e);    }}
1
public NutchDocument filter(NutchDocument doc, Parse parse, Text url, CrawlDatum datum, Inlinks inlinks) throws IndexingException
{        String[] tags = parse.getData().getParseMeta().getValues(RelTagParser.REL_TAG);    if (tags != null) {        for (int i = 0; i < tags.length; i++) {            doc.add("tag", tags[i]);        }    }    return doc;}
0
public void setConf(Configuration conf)
{    this.conf = conf;}
0
public Configuration getConf()
{    return this.conf;}
0
public ParseResult filter(Content content, ParseResult parseResult, HTMLMetaTags metaTags, DocumentFragment doc)
{        Parse parse = parseResult.get(content.getUrl());        Parser parser = new Parser(doc);    Set<?> tags = parser.getRelTags();    Iterator<?> iter = tags.iterator();    Metadata metadata = parse.getData().getParseMeta();    while (iter.hasNext()) metadata.add(REL_TAG, (String) iter.next());    return parseResult;}
0
 Set<String> getRelTags()
{    return tags;}
0
 void parse(Node node)
{    if (node.getNodeType() == Node.ELEMENT_NODE) {                if ("a".equalsIgnoreCase(node.getNodeName())) {            NamedNodeMap attrs = node.getAttributes();            Node hrefNode = attrs.getNamedItem("href");                        if (hrefNode != null) {                Node relNode = attrs.getNamedItem("rel");                                if (relNode != null) {                                        if ("tag".equalsIgnoreCase(relNode.getNodeValue())) {                        String tag = parseTag(hrefNode.getNodeValue());                        if (!StringUtil.isEmpty(tag)) {                            if (!tags.contains(tag)) {                                tags.add(tag);                                                            }                        }                    }                }            }        }    }        NodeList children = node.getChildNodes();    for (int i = 0; children != null && i < children.getLength(); i++) parse(children.item(i));}
1
private static final String parseTag(String url)
{    String tag = null;    try {        URL u = new URL(url);        String path = u.getPath();        tag = URLDecoder.decode(path.substring(path.lastIndexOf('/') + 1), "UTF-8");    } catch (Exception e) {                tag = null;    }    return tag;}
0
public void setConf(Configuration conf)
{    this.conf = conf;}
0
public Configuration getConf()
{    return this.conf;}
0
public NutchDocument filter(NutchDocument doc, Parse parse, Text url, CrawlDatum datum, Inlinks inlinks) throws IndexingException
{    String mimeType;    String contentType;    Writable tcontentType = datum.getMetaData().get(new Text(Response.CONTENT_TYPE));    if (tcontentType != null) {        contentType = tcontentType.toString();    } else {        contentType = parse.getData().getMeta(Response.CONTENT_TYPE);    }    if (contentType == null) {        mimeType = tika.detect(url.toString());    } else {        mimeType = MIME.forName(MimeUtil.cleanMimeType(contentType));    }    contentType = mimeType;    if (LOG.isInfoEnabled()) {            }    if (trie != null) {        if (trie.shortestMatch(contentType) == null) {                        if (acceptMode) {                return doc;            }            return null;        } else {                        if (acceptMode) {                return null;            }        }    }    return doc;}
1
public void setConf(Configuration conf)
{    this.conf = conf;    MIME = new MimeUtil(conf);        String file = conf.get(MIMEFILTER_REGEX_FILE, "");    if (file != null) {        if (file.isEmpty()) {                    } else {            Reader reader = conf.getConfResourceAsReader(file);            try {                readConfiguration(reader);            } catch (IOException e) {                if (LOG.isErrorEnabled()) {                                    }                throw new RuntimeException(e.getMessage(), e);            }        }    }}
1
private void readConfiguration(Reader reader) throws IOException
{    BufferedReader in = new BufferedReader(reader);    String line;    List<String> rules = new ArrayList<String>();    while (null != (line = in.readLine())) {        if (line.length() == 0) {            continue;        }        char first = line.charAt(0);        switch(first) {            case ' ':            case '\n':            case             '#':                break;            case '+':                acceptMode = true;                break;            case '-':                acceptMode = false;                break;            default:                rules.add(line);                break;        }    }    trie = new PrefixStringMatcher(rules);}
0
public Configuration getConf()
{    return this.conf;}
0
public static void main(String[] args) throws IOException, IndexingException
{    Option helpOpt = new Option("h", "help", false, "show this help message");    @SuppressWarnings("static-access")    Option rulesOpt = OptionBuilder.withArgName("file").hasArg().withDescription("Rules file to be used in the tests relative to the conf directory").isRequired().create("rules");    Options options = new Options();    options.addOption(helpOpt).addOption(rulesOpt);    CommandLineParser parser = new GnuParser();    HelpFormatter formatter = new HelpFormatter();    String rulesFile;    try {        CommandLine line = parser.parse(options, args);        if (line.hasOption("help") || !line.hasOption("rules")) {            formatter.printHelp("org.apache.nutch.indexer.filter.MimeTypeIndexingFilter", options, true);            return;        }        rulesFile = line.getOptionValue("rules");    } catch (UnrecognizedOptionException e) {        formatter.printHelp("org.apache.nutch.indexer.filter.MimeTypeIndexingFilter", options, true);        return;    } catch (Exception e) {                e.printStackTrace();        return;    }    MimeTypeIndexingFilter filter = new MimeTypeIndexingFilter();    Configuration conf = NutchConfiguration.create();    conf.set(MimeTypeIndexingFilter.MIMEFILTER_REGEX_FILE, rulesFile);    filter.setConf(conf);    BufferedReader in = new BufferedReader(new InputStreamReader(System.in));    String line;    while ((line = in.readLine()) != null && !line.isEmpty()) {        Metadata metadata = new Metadata();        metadata.set(Response.CONTENT_TYPE, line);        ParseImpl parse = new ParseImpl("text", new ParseData(new ParseStatus(), "title", new Outlink[0], metadata));        NutchDocument doc = filter.filter(new NutchDocument(), parse, new Text("http://www.example.com/"), new CrawlDatum(), new Inlinks());        if (doc != null) {            System.out.print("+ ");            System.out.println(line);        } else {            System.out.print("- ");            System.out.println(line);        }    }}
1
public void setUp() throws Exception
{    for (int i = 0; i < MIME_TYPES.length; i++) {        Metadata metadata = new Metadata();        metadata.add(Response.CONTENT_TYPE, MIME_TYPES[i]);        ParseImpl parse = new ParseImpl("text", new ParseData(new ParseStatus(), "title", new Outlink[0], metadata));        parses[i] = parse;    }}
0
public void testMissingConfigFile() throws Exception
{    String file = conf.get(MimeTypeIndexingFilter.MIMEFILTER_REGEX_FILE, "");    Assert.assertEquals(String.format("Property %s must not be present in the the configuration file", MimeTypeIndexingFilter.MIMEFILTER_REGEX_FILE), "", file);    filter.setConf(conf);        for (int i = 0; i < parses.length; i++) {        NutchDocument doc = filter.filter(new NutchDocument(), parses[i], new Text("http://www.example.com/"), new CrawlDatum(), new Inlinks());        Assert.assertNotNull("All documents must be allowed by default", doc);    }}
0
public void testAllowOnlyImages() throws Exception
{    conf.set(MimeTypeIndexingFilter.MIMEFILTER_REGEX_FILE, "allow-images.txt");    filter.setConf(conf);    for (int i = 0; i < parses.length; i++) {        NutchDocument doc = filter.filter(new NutchDocument(), parses[i], new Text("http://www.example.com/"), new CrawlDatum(), new Inlinks());        if (MIME_TYPES[i].contains("image")) {            Assert.assertNotNull("Allow only images", doc);        } else {            Assert.assertNull("Block everything else", doc);        }    }}
0
public void testBlockHTML() throws Exception
{    conf.set(MimeTypeIndexingFilter.MIMEFILTER_REGEX_FILE, "block-html.txt");    filter.setConf(conf);    for (int i = 0; i < parses.length; i++) {        NutchDocument doc = filter.filter(new NutchDocument(), parses[i], new Text("http://www.example.com/"), new CrawlDatum(), new Inlinks());        if (MIME_TYPES[i].contains("html")) {            Assert.assertNull("Block only HTML documents", doc);        } else {            Assert.assertNotNull("Allow everything else", doc);        }    }}
0
public ParseResult getParse(Content content)
{    String contentType = content.getContentType();    String[] params = (String[]) TYPE_PARAMS_MAP.get(contentType);    if (params == null)        return new ParseStatus(ParseStatus.FAILED, "No external command defined for contentType: " + contentType).getEmptyParseResult(content.getUrl(), getConf());    String command = params[0];    int timeout = Integer.parseInt(params[1]);    String encoding = params[2];    if (LOG.isTraceEnabled()) {        LOG.trace("Use " + command + " with timeout=" + timeout + "secs");    }    String text = null;    String title = null;    try {        byte[] raw = content.getContent();        String contentLength = content.getMetadata().get(Response.CONTENT_LENGTH);        if (contentLength != null && raw.length != Integer.parseInt(contentLength)) {            return new ParseStatus(ParseStatus.FAILED, ParseStatus.FAILED_TRUNCATED, "Content truncated at " + raw.length + " bytes. Parser can't handle incomplete " + contentType + " file.").getEmptyParseResult(content.getUrl(), getConf());        }        ByteArrayOutputStream os = new ByteArrayOutputStream(BUFFER_SIZE);        ByteArrayOutputStream es = new ByteArrayOutputStream(BUFFER_SIZE / 4);        CommandRunner cr = new CommandRunner();        cr.setCommand(command + " " + contentType);        cr.setInputStream(new ByteArrayInputStream(raw));        cr.setStdOutputStream(os);        cr.setStdErrorStream(es);        cr.setTimeout(timeout);        cr.evaluate();        if (cr.getExitValue() != 0)            return new ParseStatus(ParseStatus.FAILED, "External command " + command + " failed with error: " + es.toString()).getEmptyParseResult(content.getUrl(), getConf());        text = os.toString(encoding);    } catch (Exception e) {                return new ParseStatus(e).getEmptyParseResult(content.getUrl(), getConf());    }    if (text == null)        text = "";    if (title == null)        title = "";        Outlink[] outlinks = OutlinkExtractor.getOutlinks(text, getConf());    ParseData parseData = new ParseData(ParseStatus.STATUS_SUCCESS, title, outlinks, content.getMetadata());    return ParseResult.createParseResult(content.getUrl(), new ParseImpl(text, parseData));}
0
public void setConf(Configuration conf)
{    this.conf = conf;    Extension[] extensions = PluginRepository.get(conf).getExtensionPoint("org.apache.nutch.parse.Parser").getExtensions();    String contentType, command, timeoutString, encoding;    for (int i = 0; i < extensions.length; i++) {        Extension extension = extensions[i];                if (!extension.getDescriptor().getPluginId().equals("parse-ext"))            continue;        contentType = extension.getAttribute("contentType");        if (contentType == null || contentType.equals(""))            continue;        command = extension.getAttribute("command");        if (command == null || command.equals(""))            continue;                encoding = extension.getAttribute("encoding");        if (encoding == null)            encoding = Charset.defaultCharset().name();        timeoutString = extension.getAttribute("timeout");        if (timeoutString == null || timeoutString.equals(""))            timeoutString = "" + TIMEOUT_DEFAULT;        TYPE_PARAMS_MAP.put(contentType, new String[] { command, timeoutString, encoding });    }}
0
public Configuration getConf()
{    return this.conf;}
0
protected void setUp() throws ProtocolException, IOException
{            String path = System.getProperty("test.data");    if (path != null) {        File tempDir = new File(path);        if (!tempDir.exists())            tempDir.mkdir();        tempFile = File.createTempFile("nutch.test.plugin.ExtParser.", ".txt", tempDir);    } else {                tempFile = File.createTempFile("nutch.test.plugin.ExtParser.", ".txt");    }    urlString = tempFile.toURI().toURL().toString();    FileOutputStream fos = new FileOutputStream(tempFile);    fos.write(expectedText.getBytes());    fos.close();        Protocol protocol = new ProtocolFactory(NutchConfiguration.create()).getProtocol(urlString);    content = protocol.getProtocolOutput(new Text(urlString), new CrawlDatum()).getContent();    protocol = null;}
0
protected void tearDown()
{        content = null;}
0
public void testIt() throws ParseException
{    String contentType;        if (!System.getProperty("os.name").equalsIgnoreCase("linux")) {        System.err.println("Current OS is " + System.getProperty("os.name") + ".");        System.err.println("No test is run on OS other than linux.");        return;    }    Configuration conf = NutchConfiguration.create();        for (int i = 0; i < 10; i++) {                contentType = "application/vnd.nutch.example.cat";        content.setContentType(contentType);        parse = new ParseUtil(conf).parseByExtensionId("parse-ext", content).get(content.getUrl());        Assert.assertEquals(expectedText, parse.getText());                contentType = "application/vnd.nutch.example.md5sum";        content.setContentType(contentType);        parse = new ParseUtil(conf).parseByExtensionId("parse-ext", content).get(content.getUrl());        Assert.assertTrue(parse.getText().startsWith(expectedMD5sum));    }}
0
public Node getRootNode()
{    return (null != m_docFrag) ? (Node) m_docFrag : (Node) m_doc;}
0
public Node getCurrentNode()
{    return m_currentNode;}
0
public java.io.Writer getWriter()
{    return null;}
0
protected void append(Node newNode) throws org.xml.sax.SAXException
{    Node currentNode = m_currentNode;    if (null != currentNode) {        currentNode.appendChild(newNode);        } else if (null != m_docFrag) {        m_docFrag.appendChild(newNode);    } else {        boolean ok = true;        short type = newNode.getNodeType();        if (type == Node.TEXT_NODE) {            String data = newNode.getNodeValue();            if ((null != data) && (data.trim().length() > 0)) {                throw new org.xml.sax.SAXException("Warning: can't output text before document element!  Ignoring...");            }            ok = false;        } else if (type == Node.ELEMENT_NODE) {            if (m_doc.getDocumentElement() != null) {                throw new org.xml.sax.SAXException("Can't have more than one root on a DOM!");            }        }        if (ok)            m_doc.appendChild(newNode);    }}
0
public void setDocumentLocator(Locator locator)
{}
0
public void startDocument() throws org.xml.sax.SAXException
{}
0
public void endDocument() throws org.xml.sax.SAXException
{}
0
public void startElement(String ns, String localName, String name, Attributes atts) throws org.xml.sax.SAXException
{    Element elem;        if ((null == ns) || (ns.length() == 0))        elem = m_doc.createElementNS(null, name);    else        elem = m_doc.createElementNS(ns, name);    append(elem);    try {        int nAtts = atts.getLength();        if (0 != nAtts) {            for (int i = 0; i < nAtts; i++) {                                if (atts.getType(i).equalsIgnoreCase("ID"))                    setIDAttribute(atts.getValue(i), elem);                String attrNS = atts.getURI(i);                if ("".equals(attrNS))                                        attrNS = null;                                                                String attrQName = atts.getQName(i);                                if (attrQName.startsWith("xmlns:"))                    attrNS = "http://www.w3.org/2000/xmlns/";                                elem.setAttributeNS(attrNS, attrQName, atts.getValue(i));            }        }                m_elemStack.push(elem);        m_currentNode = elem;        } catch (java.lang.Exception de) {                throw new org.xml.sax.SAXException(de);    }}
0
public void endElement(String ns, String localName, String name) throws org.xml.sax.SAXException
{    m_elemStack.pop();    m_currentNode = m_elemStack.isEmpty() ? null : (Node) m_elemStack.peek();}
0
public void setIDAttribute(String id, Element elem)
{}
0
public void characters(char[] ch, int start, int length) throws org.xml.sax.SAXException
{    if (isOutsideDocElem() && XMLCharacterRecognizer.isWhiteSpace(ch, start, length))                return;    if (m_inCData) {        cdata(ch, start, length);        return;    }    String s = new String(ch, start, length);    Node childNode;    childNode = m_currentNode != null ? m_currentNode.getLastChild() : null;    if (childNode != null && childNode.getNodeType() == Node.TEXT_NODE) {        ((Text) childNode).appendData(s);    } else {        Text text = m_doc.createTextNode(s);        append(text);    }}
0
public void charactersRaw(char[] ch, int start, int length) throws org.xml.sax.SAXException
{    if (isOutsideDocElem() && XMLCharacterRecognizer.isWhiteSpace(ch, start, length))                return;    String s = new String(ch, start, length);    append(m_doc.createProcessingInstruction("xslt-next-is-raw", "formatter-to-dom"));    append(m_doc.createTextNode(s));}
0
public void startEntity(String name) throws org.xml.sax.SAXException
{}
0
public void endEntity(String name) throws org.xml.sax.SAXException
{}
0
public void entityReference(String name) throws org.xml.sax.SAXException
{    append(m_doc.createEntityReference(name));}
0
public void ignorableWhitespace(char[] ch, int start, int length) throws org.xml.sax.SAXException
{    if (isOutsideDocElem())                return;    String s = new String(ch, start, length);    append(m_doc.createTextNode(s));}
0
private boolean isOutsideDocElem()
{    return (null == m_docFrag) && m_elemStack.size() == 0 && (null == m_currentNode || m_currentNode.getNodeType() == Node.DOCUMENT_NODE);}
0
public void processingInstruction(String target, String data) throws org.xml.sax.SAXException
{    append(m_doc.createProcessingInstruction(target, data));}
0
public void comment(char[] ch, int start, int length) throws org.xml.sax.SAXException
{        if (ch == null || start < 0 || length >= (ch.length - start) || length < 0)        return;    append(m_doc.createComment(new String(ch, start, length)));}
0
public void startCDATA() throws org.xml.sax.SAXException
{    m_inCData = true;    append(m_doc.createCDATASection(""));}
0
public void endCDATA() throws org.xml.sax.SAXException
{    m_inCData = false;}
0
public void cdata(char[] ch, int start, int length) throws org.xml.sax.SAXException
{    if (isOutsideDocElem() && XMLCharacterRecognizer.isWhiteSpace(ch, start, length))                return;    String s = new String(ch, start, length);        Node n = m_currentNode.getLastChild();    if (n instanceof CDATASection)        ((CDATASection) n).appendData(s);    else if (n instanceof Comment)        ((Comment) n).appendData(s);}
0
public void startDTD(String name, String publicId, String systemId) throws org.xml.sax.SAXException
{}
0
public void endDTD() throws org.xml.sax.SAXException
{}
0
public void startPrefixMapping(String prefix, String uri) throws org.xml.sax.SAXException
{/*     *      * if((null != m_currentNode) && (m_currentNode.getNodeType() ==     * Node.ELEMENT_NODE)) { String qname; if(((null != prefix) &&     * (prefix.length() == 0)) || (null == prefix)) qname = "xmlns"; else qname     * = "xmlns:"+prefix;     *      * Element elem = (Element)m_currentNode; String val =     * elem.getAttribute(qname);      * { elem.setAttributeNS("http://www.w3.org/XML/1998/namespace", qname,     * uri); } }     */}
0
public void endPrefixMapping(String prefix) throws org.xml.sax.SAXException
{}
0
public void skippedEntity(String name) throws org.xml.sax.SAXException
{}
0
public String toString()
{    return "LP[el=" + elName + ",attr=" + attrName + ",len=" + childLen + "]";}
0
public void setConf(Configuration conf)
{        Collection<String> forceTags = new ArrayList<String>(1);    this.conf = conf;    linkParams.clear();    linkParams.put("a", new LinkParams("a", "href", 1));    linkParams.put("area", new LinkParams("area", "href", 0));    if (conf.getBoolean("parser.html.form.use_action", true)) {        linkParams.put("form", new LinkParams("form", "action", 1));        if (conf.get("parser.html.form.use_action") != null)            forceTags.add("form");    }    linkParams.put("frame", new LinkParams("frame", "src", 0));    linkParams.put("iframe", new LinkParams("iframe", "src", 0));    linkParams.put("script", new LinkParams("script", "src", 0));    linkParams.put("link", new LinkParams("link", "href", 0));    linkParams.put("img", new LinkParams("img", "src", 0));    linkParams.put("source", new LinkParams("source", "src", 0));        String[] ignoreTags = conf.getStrings("parser.html.outlinks.ignore_tags");    for (int i = 0; ignoreTags != null && i < ignoreTags.length; i++) {        if (!forceTags.contains(ignoreTags[i]))            linkParams.remove(ignoreTags[i]);    }        srcTagMetaName = this.conf.get("parser.html.outlinks.htmlnode_metadata_name");    keepNodenames = (srcTagMetaName != null && srcTagMetaName.length() > 0);    blockNodes = new HashSet<>(conf.getTrimmedStringCollection("parser.html.line.separators"));}
0
public boolean getText(StringBuffer sb, Node node, boolean abortOnNestedAnchors)
{    if (getTextHelper(sb, node, abortOnNestedAnchors, 0)) {        return true;    }    return false;}
0
public void getText(StringBuffer sb, Node node)
{    getText(sb, node, false);}
0
private boolean getTextHelper(StringBuffer sb, Node node, boolean abortOnNestedAnchors, int anchorDepth)
{    boolean abort = false;    NodeWalker walker = new NodeWalker(node);    while (walker.hasNext()) {        Node currentNode = walker.nextNode();        String nodeName = currentNode.getNodeName();        short nodeType = currentNode.getNodeType();        Node previousSibling = currentNode.getPreviousSibling();        if (previousSibling != null && blockNodes.contains(previousSibling.getNodeName().toLowerCase())) {            appendParagraphSeparator(sb);        } else if (blockNodes.contains(nodeName.toLowerCase())) {            appendParagraphSeparator(sb);        }        if ("script".equalsIgnoreCase(nodeName)) {            walker.skipChildren();        }        if ("style".equalsIgnoreCase(nodeName)) {            walker.skipChildren();        }        if (abortOnNestedAnchors && "a".equalsIgnoreCase(nodeName)) {            anchorDepth++;            if (anchorDepth > 1) {                abort = true;                break;            }        }        if (nodeType == Node.COMMENT_NODE) {            walker.skipChildren();        }        if (nodeType == Node.TEXT_NODE) {                        String text = currentNode.getNodeValue();            text = text.replaceAll("\\s+", " ");            text = text.trim();            if (text.length() > 0) {                appendSpace(sb);                sb.append(text);            } else {                appendParagraphSeparator(sb);            }        }    }    return abort;}
0
private void appendParagraphSeparator(StringBuffer buffer)
{    if (buffer.length() == 0) {        return;    }    char lastChar = buffer.charAt(buffer.length() - 1);    if ('\n' != lastChar) {                while (lastChar == ' ') {            buffer.deleteCharAt(buffer.length() - 1);            lastChar = buffer.charAt(buffer.length() - 1);        }        if ('\n' != lastChar) {            buffer.append('\n');        }    }}
0
private void appendSpace(StringBuffer buffer)
{    if (buffer.length() == 0) {        return;    }    char lastChar = buffer.charAt(buffer.length() - 1);    if (' ' != lastChar && '\n' != lastChar) {        buffer.append(' ');    }}
0
public boolean getTitle(StringBuffer sb, Node node)
{    NodeWalker walker = new NodeWalker(node);    while (walker.hasNext()) {        Node currentNode = walker.nextNode();        String nodeName = currentNode.getNodeName();        short nodeType = currentNode.getNodeType();        if ("body".equalsIgnoreCase(nodeName)) {                        return false;        }        if (nodeType == Node.ELEMENT_NODE) {            if ("title".equalsIgnoreCase(nodeName)) {                getText(sb, currentNode);                return true;            }        }    }    return false;}
0
public String getBase(Node node)
{    NodeWalker walker = new NodeWalker(node);    while (walker.hasNext()) {        Node currentNode = walker.nextNode();        String nodeName = currentNode.getNodeName();        short nodeType = currentNode.getNodeType();                if (nodeType == Node.ELEMENT_NODE) {            if ("body".equalsIgnoreCase(nodeName)) {                                return null;            }            if ("base".equalsIgnoreCase(nodeName)) {                NamedNodeMap attrs = currentNode.getAttributes();                for (int i = 0; i < attrs.getLength(); i++) {                    Node attr = attrs.item(i);                    if ("href".equalsIgnoreCase(attr.getNodeName())) {                        return attr.getNodeValue();                    }                }            }        }    }        return null;}
0
private boolean hasOnlyWhiteSpace(Node node)
{    String val = node.getNodeValue();    for (int i = 0; i < val.length(); i++) {        if (!Character.isWhitespace(val.charAt(i)))            return false;    }    return true;}
0
private boolean shouldThrowAwayLink(Node node, NodeList children, int childLen, LinkParams params)
{    if (childLen == 0) {                if (params.childLen == 0)            return false;        else            return true;    } else if ((childLen == 1) && (children.item(0).getNodeType() == Node.ELEMENT_NODE) && (params.elName.equalsIgnoreCase(children.item(0).getNodeName()))) {                return true;    } else if (childLen == 2) {        Node c0 = children.item(0);        Node c1 = children.item(1);        if ((c0.getNodeType() == Node.ELEMENT_NODE) && (params.elName.equalsIgnoreCase(c0.getNodeName())) && (c1.getNodeType() == Node.TEXT_NODE) && hasOnlyWhiteSpace(c1)) {                        return true;        }        if ((c1.getNodeType() == Node.ELEMENT_NODE) && (params.elName.equalsIgnoreCase(c1.getNodeName())) && (c0.getNodeType() == Node.TEXT_NODE) && hasOnlyWhiteSpace(c0)) {                        return true;        }    } else if (childLen == 3) {        Node c0 = children.item(0);        Node c1 = children.item(1);        Node c2 = children.item(2);        if ((c1.getNodeType() == Node.ELEMENT_NODE) && (params.elName.equalsIgnoreCase(c1.getNodeName())) && (c0.getNodeType() == Node.TEXT_NODE) && (c2.getNodeType() == Node.TEXT_NODE) && hasOnlyWhiteSpace(c0) && hasOnlyWhiteSpace(c2)) {                        return true;        }    }    return false;}
0
public void getOutlinks(URL base, ArrayList<Outlink> outlinks, Node node)
{    NodeWalker walker = new NodeWalker(node);    while (walker.hasNext()) {        Node currentNode = walker.nextNode();        String nodeName = currentNode.getNodeName();        short nodeType = currentNode.getNodeType();        NodeList children = currentNode.getChildNodes();        int childLen = (children != null) ? children.getLength() : 0;        if (nodeType == Node.ELEMENT_NODE) {            nodeName = nodeName.toLowerCase();            LinkParams params = (LinkParams) linkParams.get(nodeName);            if (params != null) {                if (!shouldThrowAwayLink(currentNode, children, childLen, params)) {                    StringBuffer linkText = new StringBuffer();                    getText(linkText, currentNode, true);                    if (linkText.toString().trim().length() == 0) {                                                NodeWalker subWalker = new NodeWalker(currentNode);                        while (subWalker.hasNext()) {                            Node subNode = subWalker.nextNode();                            if (subNode.getNodeType() == Node.ELEMENT_NODE) {                                if (subNode.getNodeName().toLowerCase().equals("img")) {                                    NamedNodeMap subAttrs = subNode.getAttributes();                                    Node alt = subAttrs.getNamedItem("alt");                                    if (alt != null) {                                        String altTxt = alt.getTextContent();                                        if (altTxt != null && altTxt.trim().length() > 0) {                                            if (linkText.length() > 0)                                                linkText.append(' ');                                            linkText.append(altTxt);                                        }                                    }                                } else {                                                                }                            } else if (subNode.getNodeType() == Node.TEXT_NODE) {                                String txt = subNode.getTextContent();                                if (txt != null && txt.length() > 0) {                                    if (linkText.length() > 0)                                        linkText.append(' ');                                    linkText.append(txt);                                }                            }                        }                    }                    NamedNodeMap attrs = currentNode.getAttributes();                    String target = null;                    boolean noFollow = false;                    boolean post = false;                    for (int i = 0; i < attrs.getLength(); i++) {                        Node attr = attrs.item(i);                        String attrName = attr.getNodeName();                        if (params.attrName.equalsIgnoreCase(attrName)) {                            target = attr.getNodeValue();                        } else if ("rel".equalsIgnoreCase(attrName) && "nofollow".equalsIgnoreCase(attr.getNodeValue())) {                            noFollow = true;                        } else if ("method".equalsIgnoreCase(attrName) && "post".equalsIgnoreCase(attr.getNodeValue())) {                            post = true;                        }                    }                    if (target != null && !noFollow && !post)                        try {                            URL url = URLUtil.resolveURL(base, target);                            Outlink outlink = new Outlink(url.toString(), linkText.toString().trim());                            outlinks.add(outlink);                                                        if (keepNodenames) {                                MapWritable metadata = new MapWritable();                                metadata.put(new Text(srcTagMetaName), new Text(nodeName));                                outlink.setMetadata(metadata);                            }                        } catch (MalformedURLException e) {                                                }                }                                if (params.childLen == 0)                    continue;            }        }    }}
0
public static final void getMetaTags(HTMLMetaTags metaTags, Node node, URL currURL)
{    metaTags.reset();    getMetaTagsHelper(metaTags, node, currURL);}
0
private static final void getMetaTagsHelper(HTMLMetaTags metaTags, Node node, URL currURL)
{    if (node.getNodeType() == Node.ELEMENT_NODE) {        if ("body".equalsIgnoreCase(node.getNodeName())) {                        return;        }        if ("meta".equalsIgnoreCase(node.getNodeName())) {            NamedNodeMap attrs = node.getAttributes();            Node nameNode = null;            Node equivNode = null;            Node contentNode = null;                        for (int i = 0; i < attrs.getLength(); i++) {                Node attr = attrs.item(i);                String attrName = attr.getNodeName().toLowerCase();                if (attrName.equals("name")) {                    nameNode = attr;                } else if (attrName.equals("http-equiv")) {                    equivNode = attr;                } else if (attrName.equals("content")) {                    contentNode = attr;                }            }            if (nameNode != null) {                if (contentNode != null) {                    String name = nameNode.getNodeValue().toLowerCase();                    metaTags.getGeneralTags().add(name, contentNode.getNodeValue());                    if ("robots".equals(name)) {                        String directives = contentNode.getNodeValue().toLowerCase();                        int index = directives.indexOf("none");                        if (index >= 0) {                            metaTags.setNoIndex();                            metaTags.setNoFollow();                        }                        index = directives.indexOf("all");                        if (index >= 0) {                                                }                        index = directives.indexOf("noindex");                        if (index >= 0) {                            metaTags.setNoIndex();                        }                        index = directives.indexOf("nofollow");                        if (index >= 0) {                            metaTags.setNoFollow();                        }                        index = directives.indexOf("noarchive");                        if (index >= 0) {                            metaTags.setNoCache();                        }                    }                                }            }            if (equivNode != null) {                if (contentNode != null) {                    String name = equivNode.getNodeValue().toLowerCase();                    String content = contentNode.getNodeValue();                    metaTags.getHttpEquivTags().setProperty(name, content);                    if ("pragma".equals(name)) {                        content = content.toLowerCase();                        int index = content.indexOf("no-cache");                        if (index >= 0)                            metaTags.setNoCache();                    } else if ("refresh".equals(name)) {                        int idx = content.indexOf(';');                        String time = null;                        if (idx == -1) {                                                        time = content;                        } else                            time = content.substring(0, idx);                        try {                            metaTags.setRefreshTime(Integer.parseInt(time));                                                        metaTags.setRefresh(true);                        } catch (Exception e) {                            ;                        }                        URL refreshUrl = null;                        if (metaTags.getRefresh() && idx != -1) {                                                        idx = content.toLowerCase().indexOf("url=");                            if (idx == -1) {                                                                                                idx = content.indexOf(';') + 1;                            } else                                idx += 4;                            if (idx != -1) {                                String url = content.substring(idx);                                try {                                    refreshUrl = new URL(url);                                } catch (Exception e) {                                                                        try {                                        refreshUrl = new URL(currURL, url);                                    } catch (Exception e1) {                                        refreshUrl = null;                                    }                                }                            }                        }                        if (metaTags.getRefresh()) {                            if (refreshUrl == null) {                                                                                                refreshUrl = currURL;                            }                            metaTags.setRefreshHref(refreshUrl);                        }                    }                }            }        } else if ("base".equalsIgnoreCase(node.getNodeName())) {            NamedNodeMap attrs = node.getAttributes();            Node hrefNode = attrs.getNamedItem("href");            if (hrefNode != null) {                String urlString = hrefNode.getNodeValue();                URL url = null;                try {                    if (currURL == null)                        url = new URL(urlString);                    else                        url = new URL(currURL, urlString);                } catch (Exception e) {                    ;                }                if (url != null)                    metaTags.setBaseHref(url);            }        }    }    NodeList children = node.getChildNodes();    if (children != null) {        int len = children.getLength();        for (int i = 0; i < len; i++) {            getMetaTagsHelper(metaTags, children.item(i), currURL);        }    }}
0
private static String sniffCharacterEncoding(byte[] content)
{    int length = content.length < CHUNK_SIZE ? content.length : CHUNK_SIZE;                    String str = new String(content, 0, length, StandardCharsets.US_ASCII);    Matcher metaMatcher = metaPattern.matcher(str);    String encoding = null;    if (metaMatcher.find()) {        Matcher charsetMatcher = charsetPattern.matcher(metaMatcher.group(1));        if (charsetMatcher.find())            encoding = charsetMatcher.group(1);    }    if (encoding == null) {                metaMatcher = charsetPatternHTML5.matcher(str);        if (metaMatcher.find()) {            encoding = metaMatcher.group(1);        }    }    if (encoding == null) {                if (content.length >= 3 && content[0] == (byte) 0xEF && content[1] == (byte) 0xBB && content[2] == (byte) 0xBF) {            encoding = "UTF-8";        } else if (content.length >= 2) {            if (content[0] == (byte) 0xFF && content[1] == (byte) 0xFE) {                encoding = "UTF-16LE";            } else if (content[0] == (byte) 0xFE && content[1] == (byte) 0xFF) {                encoding = "UTF-16BE";            }        }    }    return encoding;}
0
public ParseResult getParse(Content content)
{    HTMLMetaTags metaTags = new HTMLMetaTags();    URL base;    try {        base = new URL(content.getBaseUrl());    } catch (MalformedURLException e) {        return new ParseStatus(e).getEmptyParseResult(content.getUrl(), getConf());    }    String text = "";    String title = "";    Outlink[] outlinks = new Outlink[0];    Metadata metadata = new Metadata();        DocumentFragment root;    try {        byte[] contentInOctets = content.getContent();        InputSource input = new InputSource(new ByteArrayInputStream(contentInOctets));        EncodingDetector detector = new EncodingDetector(conf);        detector.autoDetectClues(content, true);        detector.addClue(sniffCharacterEncoding(contentInOctets), "sniffed");        String encoding = detector.guessEncoding(content, defaultCharEncoding);        metadata.set(Metadata.ORIGINAL_CHAR_ENCODING, encoding);        metadata.set(Metadata.CHAR_ENCODING_FOR_CONVERSION, encoding);        input.setEncoding(encoding);        if (LOG.isTraceEnabled()) {            LOG.trace("Parsing...");        }        root = parse(input);    } catch (IOException e) {        return new ParseStatus(e).getEmptyParseResult(content.getUrl(), getConf());    } catch (DOMException e) {        return new ParseStatus(e).getEmptyParseResult(content.getUrl(), getConf());    } catch (SAXException e) {        return new ParseStatus(e).getEmptyParseResult(content.getUrl(), getConf());    } catch (Exception e) {                return new ParseStatus(e).getEmptyParseResult(content.getUrl(), getConf());    }        HTMLMetaProcessor.getMetaTags(metaTags, root, base);        metadata.addAll(metaTags.getGeneralTags());    if (LOG.isTraceEnabled()) {        LOG.trace("Meta tags for " + base + ": " + metaTags.toString());    }        if (!metaTags.getNoIndex()) {                StringBuffer sb = new StringBuffer();        if (LOG.isTraceEnabled()) {            LOG.trace("Getting text...");        }                utils.getText(sb, root);        text = sb.toString();        sb.setLength(0);        if (LOG.isTraceEnabled()) {            LOG.trace("Getting title...");        }                utils.getTitle(sb, root);        title = sb.toString().trim();    }    if (!metaTags.getNoFollow()) {                        ArrayList<Outlink> l = new ArrayList<Outlink>();        URL baseTag = base;        String baseTagHref = utils.getBase(root);        if (baseTagHref != null) {            try {                baseTag = new URL(base, baseTagHref);            } catch (MalformedURLException e) {                baseTag = base;            }        }        if (LOG.isTraceEnabled()) {            LOG.trace("Getting links...");        }        utils.getOutlinks(baseTag, l, root);        outlinks = l.toArray(new Outlink[l.size()]);        if (LOG.isTraceEnabled()) {            LOG.trace("found " + outlinks.length + " outlinks in " + content.getUrl());        }    }    ParseStatus status = new ParseStatus(ParseStatus.SUCCESS);    if (metaTags.getRefresh()) {        status.setMinorCode(ParseStatus.SUCCESS_REDIRECT);        status.setArgs(new String[] { metaTags.getRefreshHref().toString(), Integer.toString(metaTags.getRefreshTime()) });    }    ParseData parseData = new ParseData(status, title, outlinks, content.getMetadata(), metadata);    ParseResult parseResult = ParseResult.createParseResult(content.getUrl(), new ParseImpl(text, parseData));        ParseResult filteredParse = this.htmlParseFilters.filter(content, parseResult, metaTags, root);    if (metaTags.getNoCache()) {                for (Map.Entry<org.apache.hadoop.io.Text, Parse> entry : filteredParse) entry.getValue().getData().getParseMeta().set(Nutch.CACHING_FORBIDDEN_KEY, cachingPolicy);    }    return filteredParse;}
1
private DocumentFragment parse(InputSource input) throws Exception
{    if ("tagsoup".equalsIgnoreCase(parserImpl))        return parseTagSoup(input);    else        return parseNeko(input);}
0
private DocumentFragment parseTagSoup(InputSource input) throws Exception
{    HTMLDocumentImpl doc = new HTMLDocumentImpl();    DocumentFragment frag = doc.createDocumentFragment();    DOMBuilder builder = new DOMBuilder(doc, frag);    org.ccil.cowan.tagsoup.Parser reader = new org.ccil.cowan.tagsoup.Parser();    reader.setContentHandler(builder);    reader.setFeature(org.ccil.cowan.tagsoup.Parser.ignoreBogonsFeature, true);    reader.setFeature(org.ccil.cowan.tagsoup.Parser.bogonsEmptyFeature, false);    reader.setProperty("http://xml.org/sax/properties/lexical-handler", builder);    reader.parse(input);    return frag;}
0
private DocumentFragment parseNeko(InputSource input) throws Exception
{    DOMFragmentParser parser = new DOMFragmentParser();    try {        parser.setFeature("http://cyberneko.org/html/features/scanner/allow-selfclosing-iframe", true);        parser.setFeature("http://cyberneko.org/html/features/augmentations", true);        parser.setProperty("http://cyberneko.org/html/properties/default-encoding", defaultCharEncoding);        parser.setFeature("http://cyberneko.org/html/features/scanner/ignore-specified-charset", true);        parser.setFeature("http://cyberneko.org/html/features/balance-tags/ignore-outside-content", false);        parser.setFeature("http://cyberneko.org/html/features/balance-tags/document-fragment", true);        parser.setFeature("http://cyberneko.org/html/features/report-errors", LOG.isTraceEnabled());    } catch (SAXException e) {    }        HTMLDocumentImpl doc = new HTMLDocumentImpl();    doc.setErrorChecking(false);    DocumentFragment res = doc.createDocumentFragment();    DocumentFragment frag = doc.createDocumentFragment();    parser.parse(input, frag);    res.appendChild(frag);    try {        while (true) {            frag = doc.createDocumentFragment();            parser.parse(input, frag);            if (!frag.hasChildNodes())                break;            if (LOG.isInfoEnabled()) {                            }            res.appendChild(frag);        }    } catch (Exception e) {            }    ;    return res;}
1
public static void main(String[] args) throws Exception
{    String name = args[0];    String url = "file:" + name;    File file = new File(name);    byte[] bytes = new byte[(int) file.length()];    @SuppressWarnings("resource")    DataInputStream in = new DataInputStream(new FileInputStream(file));    in.readFully(bytes);    Configuration conf = NutchConfiguration.create();    HtmlParser parser = new HtmlParser();    parser.setConf(conf);    Parse parse = parser.getParse(new Content(url, url, bytes, "text/html", new Metadata(), conf)).get(url);    System.out.println("data: " + parse.getData());    System.out.println("text: " + parse.getText());}
0
public void setConf(Configuration conf)
{    this.conf = conf;    this.htmlParseFilters = new HtmlParseFilters(getConf());    this.parserImpl = getConf().get("parser.html.impl", "neko");    this.defaultCharEncoding = getConf().get("parser.character.encoding.default", "windows-1252");    this.utils = new DOMContentUtils(conf);    this.cachingPolicy = getConf().get("parser.caching.forbidden.policy", Nutch.CACHING_FORBIDDEN_CONTENT);}
0
public Configuration getConf()
{    return this.conf;}
0
public static boolean isWhiteSpace(char ch)
{    return (ch == 0x20) || (ch == 0x09) || (ch == 0xD) || (ch == 0xA);}
0
public static boolean isWhiteSpace(char[] ch, int start, int length)
{    int end = start + length;    for (int s = start; s < end; s++) {        if (!isWhiteSpace(ch[s]))            return false;    }    return true;}
0
public static boolean isWhiteSpace(StringBuffer buf)
{    int n = buf.length();    for (int i = 0; i < n; i++) {        if (!isWhiteSpace(buf.charAt(i)))            return false;    }    return true;}
0
public static boolean isWhiteSpace(String s)
{    if (null != s) {        int n = s.length();        for (int i = 0; i < n; i++) {            if (!isWhiteSpace(s.charAt(i)))                return false;        }    }    return true;}
0
public void setup()
{    conf = NutchConfiguration.create();    conf.setBoolean("parser.html.form.use_action", true);    utils = new DOMContentUtils(conf);    DOMFragmentParser parser = new DOMFragmentParser();    try {        parser.setFeature("http://cyberneko.org/html/features/scanner/allow-selfclosing-iframe", true);    } catch (SAXException e) {    }    for (int i = 0; i < testPages.length; i++) {        DocumentFragment node = new HTMLDocumentImpl().createDocumentFragment();        try {            parser.parse(new InputSource(new ByteArrayInputStream(testPages[i].getBytes())), node);            testBaseHrefURLs[i] = new URL(testBaseHrefs[i]);        } catch (Exception e) {            Assert.assertTrue("caught exception: " + e, false);        }        testDOMs[i] = node;    }    try {        answerOutlinks = new Outlink[][] { { new Outlink("http://www.nutch.org", "anchor") }, { new Outlink("http://www.nutch.org/", "home"), new Outlink("http://www.nutch.org/docs/bot.html", "bots") }, { new Outlink("http://www.nutch.org/", "separate this"), new Outlink("http://www.nutch.org/docs/ok", "from this") }, { new Outlink("http://www.nutch.org/", "home"), new Outlink("http://www.nutch.org/docs/1", "1"), new Outlink("http://www.nutch.org/docs/2", "2") }, { new Outlink("http://www.nutch.org/frames/top.html", ""), new Outlink("http://www.nutch.org/frames/left.html", ""), new Outlink("http://www.nutch.org/frames/invalid.html", ""), new Outlink("http://www.nutch.org/frames/right.html", "") }, { new Outlink("http://www.nutch.org/maps/logo.gif", ""), new Outlink("http://www.nutch.org/index.html", ""), new Outlink("http://www.nutch.org/maps/#bottom", ""), new Outlink("http://www.nutch.org/bot.html", ""), new Outlink("http://www.nutch.org/docs/index.html", "") }, { new Outlink("http://www.nutch.org/index.html", "whitespace test") }, {}, { new Outlink("http://www.nutch.org/dummy.jsp", "test2") }, {}, { new Outlink("http://www.nutch.org/;x", "anchor1"), new Outlink("http://www.nutch.org/g;x", "anchor2"), new Outlink("http://www.nutch.org/g;x?y#s", "anchor3") }, {         new Outlink("http://www.nutch.org/g", "anchor1"), new Outlink("http://www.nutch.org/g?y#s", "anchor2"), new Outlink("http://www.nutch.org/;something?y=1", "anchor3"), new Outlink("http://www.nutch.org/;something?y=1#s", "anchor4"), new Outlink("http://www.nutch.org/;something?y=1;somethingelse", "anchor5") }, { new Outlink("http://www.nutch.org/g", ""), new Outlink("http://www.nutch.org/g1", ""), new Outlink("http://www.nutch.org/g2", "bla bla"), new Outlink("http://www.nutch.org/test.gif", "bla bla") }, { new Outlink("http://www.nutch.org/movie.mp4", "") } };    } catch (MalformedURLException e) {    }}
0
private static boolean equalsIgnoreWhitespace(String s1, String s2)
{    StringTokenizer st1 = new StringTokenizer(s1);    StringTokenizer st2 = new StringTokenizer(s2);    while (st1.hasMoreTokens()) {        if (!st2.hasMoreTokens())            return false;        if (!st1.nextToken().equals(st2.nextToken()))            return false;    }    if (st2.hasMoreTokens())        return false;    return true;}
0
public void testGetText()
{    if (testDOMs[0] == null)        setup();    for (int i = 0; i < testPages.length; i++) {        StringBuffer sb = new StringBuffer();        utils.getText(sb, testDOMs[i]);        String text = sb.toString();        Assert.assertTrue("expecting text: " + answerText[i] + System.getProperty("line.separator") + System.getProperty("line.separator") + "got text: " + text, equalsIgnoreWhitespace(answerText[i], text));    }}
0
public void testGetTitle()
{    if (testDOMs[0] == null)        setup();    for (int i = 0; i < testPages.length; i++) {        StringBuffer sb = new StringBuffer();        utils.getTitle(sb, testDOMs[i]);        String text = sb.toString();        Assert.assertTrue("expecting text: " + answerText[i] + System.getProperty("line.separator") + System.getProperty("line.separator") + "got text: " + text, equalsIgnoreWhitespace(answerTitle[i], text));    }}
0
public void testGetOutlinks()
{    if (testDOMs[0] == null)        setup();    for (int i = 0; i < testPages.length; i++) {        ArrayList<Outlink> outlinks = new ArrayList<Outlink>();        if (i == SKIP) {            conf.setBoolean("parser.html.form.use_action", false);            utils.setConf(conf);        } else {            conf.setBoolean("parser.html.form.use_action", true);            utils.setConf(conf);        }        utils.getOutlinks(testBaseHrefURLs[i], outlinks, testDOMs[i]);        Outlink[] outlinkArr = new Outlink[outlinks.size()];        outlinkArr = (Outlink[]) outlinks.toArray(outlinkArr);        compareOutlinks(answerOutlinks[i], outlinkArr);    }}
0
private static final void appendOutlinks(StringBuffer sb, Outlink[] o)
{    for (int i = 0; i < o.length; i++) {        sb.append(o[i].toString());        sb.append(System.getProperty("line.separator"));    }}
0
private static final String outlinksString(Outlink[] o)
{    StringBuffer sb = new StringBuffer();    appendOutlinks(sb, o);    return sb.toString();}
0
private static final void compareOutlinks(Outlink[] o1, Outlink[] o2)
{    if (o1.length != o2.length) {        Assert.assertTrue("got wrong number of outlinks (expecting " + o1.length + ", got " + o2.length + ")" + System.getProperty("line.separator") + "answer: " + System.getProperty("line.separator") + outlinksString(o1) + System.getProperty("line.separator") + "got: " + System.getProperty("line.separator") + outlinksString(o2) + System.getProperty("line.separator"), false);    }    for (int i = 0; i < o1.length; i++) {        if (!o1[i].equals(o2[i])) {            Assert.assertTrue("got wrong outlinks at position " + i + System.getProperty("line.separator") + "answer: " + System.getProperty("line.separator") + "'" + o1[i].getToUrl() + "', anchor: '" + o1[i].getAnchor() + "'" + System.getProperty("line.separator") + "got: " + System.getProperty("line.separator") + "'" + o2[i].getToUrl() + "', anchor: '" + o2[i].getAnchor() + "'", false);        }    }}
0
protected Parse parse(byte[] contentBytes)
{    String dummyUrl = "http://example.com/";    return parser.getParse(new Content(dummyUrl, dummyUrl, contentBytes, "text/html", new Metadata(), conf)).get(dummyUrl);}
0
public void testEncodingDetection()
{    for (String[] testPage : encodingTestPages) {        String name = testPage[0];        Charset charset = Charset.forName(testPage[1]);        byte[] contentBytes = testPage[2].getBytes(charset);        Parse parse = parse(contentBytes);        String text = parse.getText();        String title = parse.getData().getTitle();        String keywords = parse.getData().getMeta("keywords");                                        Assert.assertEquals("Title not extracted properly (" + name + ")", encodingTestKeywords, title);        for (String keyword : encodingTestKeywords.split(",\\s*")) {            Assert.assertTrue(keyword + " not found in text (" + name + ")", text.contains(keyword));        }        Assert.assertNotNull("No keywords extracted", keywords);        Assert.assertEquals("Keywords not extracted properly (" + name + ")", encodingTestKeywords, keywords);    }}
1
public void testResolveBaseUrl()
{    byte[] contentBytes = resolveBaseUrlTestContent.getBytes(StandardCharsets.UTF_8);        Parse parse = parse(contentBytes);        Outlink[] outlinks = parse.getData().getOutlinks();    Assert.assertEquals(1, outlinks.length);    Assert.assertEquals("http://www.example.com/index.html", outlinks[0].getToUrl());}
1
public void testRobotsMetaProcessor()
{    DOMFragmentParser parser = new DOMFragmentParser();    ;    try {        currURLsAndAnswers = new URL[][] { { new URL("http://www.nutch.org"), null }, { new URL("http://www.nutch.org"), null }, { new URL("http://www.nutch.org"), null }, { new URL("http://www.nutch.org"), null }, { new URL("http://www.nutch.org"), null }, { new URL("http://www.nutch.org"), null }, { new URL("http://www.nutch.org"), null }, { new URL("http://www.nutch.org/foo/"), new URL("http://www.nutch.org/") }, { new URL("http://www.nutch.org"), new URL("http://www.nutch.org/base/") } };    } catch (Exception e) {        Assert.assertTrue("couldn't make test URLs!", false);    }    for (int i = 0; i < tests.length; i++) {        byte[] bytes = tests[i].getBytes();        DocumentFragment node = new HTMLDocumentImpl().createDocumentFragment();        try {            parser.parse(new InputSource(new ByteArrayInputStream(bytes)), node);        } catch (Exception e) {            e.printStackTrace();        }        HTMLMetaTags robotsMeta = new HTMLMetaTags();        HTMLMetaProcessor.getMetaTags(robotsMeta, node, currURLsAndAnswers[i][0]);        Assert.assertTrue("got index wrong on test " + i, robotsMeta.getNoIndex() == answers[i][0]);        Assert.assertTrue("got follow wrong on test " + i, robotsMeta.getNoFollow() == answers[i][1]);        Assert.assertTrue("got cache wrong on test " + i, robotsMeta.getNoCache() == answers[i][2]);        Assert.assertTrue("got base href wrong on test " + i + " (got " + robotsMeta.getBaseHref() + ")", ((robotsMeta.getBaseHref() == null) && (currURLsAndAnswers[i][1] == null)) || ((robotsMeta.getBaseHref() != null) && robotsMeta.getBaseHref().equals(currURLsAndAnswers[i][1])));    }}
0
public ParseResult filter(Content content, ParseResult parseResult, HTMLMetaTags metaTags, DocumentFragment doc)
{    Parse parse = parseResult.get(content.getUrl());    String url = content.getBaseUrl();    ArrayList<Outlink> outlinks = new ArrayList<Outlink>();    walk(doc, parse, metaTags, url, outlinks);    if (outlinks.size() > 0) {        Outlink[] old = parse.getData().getOutlinks();        String title = parse.getData().getTitle();        List<Outlink> list = Arrays.asList(old);        outlinks.addAll(list);        ParseStatus status = parse.getData().getStatus();        String text = parse.getText();        Outlink[] newlinks = (Outlink[]) outlinks.toArray(new Outlink[outlinks.size()]);        ParseData parseData = new ParseData(status, title, newlinks, parse.getData().getContentMeta(), parse.getData().getParseMeta());                parseResult.put(content.getUrl(), new ParseText(text), parseData);    }    return parseResult;}
0
private void walk(Node n, Parse parse, HTMLMetaTags metaTags, String base, List<Outlink> outlinks)
{    if (n instanceof Element) {        String name = n.getNodeName();        if (name.equalsIgnoreCase("script")) {            /*         * String lang = null; Node lNode =         * n.getAttributes().getNamedItem("language"); if (lNode == null) lang =         * "javascript"; else lang = lNode.getNodeValue();         */            StringBuffer script = new StringBuffer();            NodeList nn = n.getChildNodes();            if (nn.getLength() > 0) {                for (int i = 0; i < nn.getLength(); i++) {                    if (i > 0)                        script.append('\n');                    script.append(nn.item(i).getNodeValue());                }                                                                                Outlink[] links = getJSLinks(script.toString(), "", base);                if (links != null && links.length > 0)                    outlinks.addAll(Arrays.asList(links));                                return;            }        } else {                        NamedNodeMap attrs = n.getAttributes();            int len = attrs.getLength();            for (int i = 0; i < len; i++) {                                                                                                Node anode = attrs.item(i);                Outlink[] links = null;                if (anode.getNodeName().startsWith("on")) {                    links = getJSLinks(anode.getNodeValue(), "", base);                } else if (anode.getNodeName().equalsIgnoreCase("href")) {                    String val = anode.getNodeValue();                    if (val != null && val.toLowerCase().indexOf("javascript:") != -1) {                        links = getJSLinks(val, "", base);                    }                }                if (links != null && links.length > 0)                    outlinks.addAll(Arrays.asList(links));            }        }    }    NodeList nl = n.getChildNodes();    for (int i = 0; i < nl.getLength(); i++) {        walk(nl.item(i), parse, metaTags, base, outlinks);    }}
0
public ParseResult getParse(Content c)
{    String script = new String(c.getContent());    Outlink[] outlinks = getJSLinks(script, "", c.getUrl());    if (outlinks == null)        outlinks = new Outlink[0];        String title;    int idx = script.indexOf('\n');    if (idx != -1) {        if (idx > MAX_TITLE_LEN)            idx = MAX_TITLE_LEN;        title = script.substring(0, idx);    } else {        idx = Math.min(MAX_TITLE_LEN, script.length());        title = script.substring(0, idx);    }    ParseData pd = new ParseData(ParseStatus.STATUS_SUCCESS, title, outlinks, c.getMetadata());    return ParseResult.createParseResult(c.getUrl(), new ParseImpl(script, pd));}
0
private Outlink[] getJSLinks(String plainText, String anchor, String base)
{    final List<Outlink> outlinks = new ArrayList<Outlink>();    URL baseURL = null;    try {        baseURL = new URL(base);    } catch (Exception e) {        if (LOG.isErrorEnabled()) {                    }    }    try {        Matcher matcher = STRING_PATTERN.matcher(plainText);        String url;        while (matcher.find()) {            url = matcher.group(2);            Matcher matcherUri = URI_PATTERN.matcher(url);            if (!matcherUri.matches()) {                continue;            }            if (url.startsWith("www.")) {                url = "http://" + url;            } else {                                try {                    url = new URL(baseURL, url).toString();                } catch (MalformedURLException ex) {                    if (LOG.isTraceEnabled()) {                        LOG.trace(" - failed URL parse '" + url + "' and baseURL '" + baseURL + "'", ex);                    }                    continue;                }            }            url = url.replaceAll("&amp;", "&");            if (LOG.isTraceEnabled()) {                LOG.trace(" - outlink from JS: '" + url + "'");            }            outlinks.add(new Outlink(url, anchor));        }    } catch (Exception ex) {                if (LOG.isErrorEnabled()) {                    }    }    final Outlink[] retval;        if (outlinks != null && outlinks.size() > 0) {        retval = outlinks.toArray(new Outlink[0]);    } else {        retval = new Outlink[0];    }    return retval;}
1
public static void main(String[] args) throws Exception
{    if (args.length < 2) {        System.err.println(JSParseFilter.class.getName() + " file.js baseURL");        return;    }    InputStream in = new FileInputStream(args[0]);    BufferedReader br = new BufferedReader(new InputStreamReader(in, "UTF-8"));    StringBuffer sb = new StringBuffer();    String line = null;    while ((line = br.readLine()) != null) sb.append(line + "\n");    br.close();    JSParseFilter parseFilter = new JSParseFilter();    parseFilter.setConf(NutchConfiguration.create());    Outlink[] links = parseFilter.getJSLinks(sb.toString(), "", args[1]);    System.out.println("Outlinks extracted: " + links.length);    for (int i = 0; i < links.length; i++) System.out.println(" - " + links[i]);}
0
public void setConf(Configuration conf)
{    this.conf = conf;}
0
public Configuration getConf()
{    return this.conf;}
0
public void setUp()
{    conf = NutchConfiguration.create();    conf.set("file.content.limit", "-1");    conf.set("plugin.includes", "protocol-file|parse-(html|js)");}
0
public Outlink[] getOutlinks(String sampleFile) throws ProtocolException, ParseException, IOException
{    String urlString;    Parse parse;    urlString = "file:" + sampleDir + fileSeparator + sampleFile;        Protocol protocol = new ProtocolFactory(conf).getProtocol(urlString);    Content content = protocol.getProtocolOutput(new Text(urlString), new CrawlDatum()).getContent();    parse = new ParseUtil(conf).parse(content).get(content.getUrl());        return parse.getData().getOutlinks();}
1
public void testJavaScriptOutlinkExtraction() throws ProtocolException, ParseException, IOException
{    String[] filenames = new File(sampleDir).list();    for (int i = 0; i < filenames.length; i++) {        Outlink[] outlinks = getOutlinks(filenames[i]);        if (filenames[i].endsWith("parse_pure_js_test.js")) {            assertEquals("number of outlinks in .js test file should be X", 2, outlinks.length);            assertEquals("http://search.lucidimagination.com/p:nutch", outlinks[0].getToUrl());            assertEquals("http://search-lucene.com/nutch", outlinks[1].getToUrl());        } else {            assertTrue("number of outlinks in .html file should be at least 2", outlinks.length >= 2);            Set<String> outlinkSet = new TreeSet<>();            for (Outlink o : outlinks) {                outlinkSet.add(o.getToUrl());            }            assertTrue("http://search.lucidimagination.com/p:nutch not in outlinks", outlinkSet.contains("http://search.lucidimagination.com/p:nutch"));            assertTrue("http://search-lucene.com/nutch not in outlinks", outlinkSet.contains("http://search-lucene.com/nutch"));        }    }}
0
public void setConf(Configuration conf)
{    this.conf = conf;            String[] values = conf.getStrings("metatags.names", "*");    for (String val : values) {        metatagset.add(val.toLowerCase(Locale.ROOT));    }}
0
public Configuration getConf()
{    return this.conf;}
0
private void addIndexedMetatags(Metadata metadata, String metatag, String value)
{    String lcMetatag = metatag.toLowerCase(Locale.ROOT);    if (metatagset.contains("*") || metatagset.contains(lcMetatag)) {        if (LOG.isDebugEnabled()) {                    }        metadata.add("metatag." + lcMetatag, value);    }}
1
private void addIndexedMetatags(Metadata metadata, String metatag, String[] values)
{    String lcMetatag = metatag.toLowerCase(Locale.ROOT);    if (metatagset.contains("*") || metatagset.contains(lcMetatag)) {        for (String value : values) {            if (LOG.isDebugEnabled()) {                            }            metadata.add("metatag." + lcMetatag, value);        }    }}
1
public ParseResult filter(Content content, ParseResult parseResult, HTMLMetaTags metaTags, DocumentFragment doc)
{    Parse parse = parseResult.get(content.getUrl());    Metadata metadata = parse.getData().getParseMeta();        for (String mdName : metadata.names()) {        addIndexedMetatags(metadata, mdName, metadata.getValues(mdName));    }    Metadata generalMetaTags = metaTags.getGeneralTags();    for (String tagName : generalMetaTags.names()) {        addIndexedMetatags(metadata, tagName, generalMetaTags.getValues(tagName));    }    Properties httpequiv = metaTags.getHttpEquivTags();    for (Enumeration<?> tagNames = httpequiv.propertyNames(); tagNames.hasMoreElements(); ) {        String name = (String) tagNames.nextElement();        String value = httpequiv.getProperty(name);        addIndexedMetatags(metadata, name, value);    }    return parseResult;}
0
public Metadata parseMeta(String fileName, Configuration conf)
{    Metadata metadata = null;    try {        String urlString = "file:" + sampleDir + fileSeparator + fileName;        Protocol protocol = new ProtocolFactory(conf).getProtocol(urlString);        Content content = protocol.getProtocolOutput(new Text(urlString), new CrawlDatum()).getContent();        Parse parse = new ParseUtil(conf).parse(content).get(content.getUrl());        metadata = parse.getData().getParseMeta();    } catch (Exception e) {        e.printStackTrace();        Assert.fail(e.toString());    }    return metadata;}
0
public void testIt()
{    Configuration conf = NutchConfiguration.create();        Metadata parseMeta = parseMeta(sampleFile, conf);    Assert.assertEquals(description, parseMeta.get("metatag.description"));    Assert.assertEquals(keywords, parseMeta.get("metatag.keywords"));}
0
public void testMultiValueMetatags()
{    Configuration conf = NutchConfiguration.create();    conf.set("metatags.names", "keywords,DC.creator");    conf.set("index.parse.md", "metatag.keywords,metatag.dc.creator");    Metadata parseMeta = parseMeta(sampleFileMultival, conf);    String failMessage = "One value of metatag with multiple values is missing: ";    Set<String> valueSet = new TreeSet<String>();    for (String val : parseMeta.getValues("metatag.dc.creator")) {        valueSet.add(val);    }    String[] expectedValues1 = { "Doug Cutting", "Michael Cafarella" };    for (String val : expectedValues1) {        Assert.assertTrue(failMessage + val, valueSet.contains(val));    }    valueSet.clear();    for (String val : parseMeta.getValues("metatag.keywords")) {        valueSet.add(val);    }    String[] expectedValues2 = { "robot d'indexation", "web crawler", "Webcrawler" };    for (String val : expectedValues2) {        Assert.assertTrue(failMessage + val, valueSet.contains(val));    }}
0
public void setConf(Configuration conf)
{    this.conf = conf;}
0
public Configuration getConf()
{    return conf;}
0
public ParseResult getParse(Content content)
{    String text = null;    Vector<Outlink> outlinks = new Vector<>();    try {        byte[] raw = content.getContent();        String contentLength = content.getMetadata().get(Response.CONTENT_LENGTH);        if (contentLength != null && raw.length != Integer.parseInt(contentLength)) {            return new ParseStatus(ParseStatus.FAILED, ParseStatus.FAILED_TRUNCATED, "Content truncated at " + raw.length + " bytes. Parser can't handle incomplete files.").getEmptyParseResult(content.getUrl(), getConf());        }        ExtractText extractor = new ExtractText();                TagParser parser = new TagParser(extractor);                                SWFReader reader = new SWFReader(parser, new InStream(raw));                reader.readFile();        text = extractor.getText();        String atext = extractor.getActionText();        if (atext != null && atext.length() > 0)            text += "\n--------\n" + atext;                String[] links = extractor.getUrls();        for (int i = 0; i < links.length; i++) {            Outlink out = new Outlink(links[i], "");            outlinks.add(out);        }        Outlink[] olinks = OutlinkExtractor.getOutlinks(text, conf);        if (olinks != null)            for (int i = 0; i < olinks.length; i++) {                outlinks.add(olinks[i]);            }    } catch (Exception e) {                        return new ParseStatus(ParseStatus.FAILED, "Can't be handled as SWF document. " + e).getEmptyParseResult(content.getUrl(), getConf());    }    if (text == null)        text = "";    Outlink[] links = (Outlink[]) outlinks.toArray(new Outlink[outlinks.size()]);    ParseData parseData = new ParseData(ParseStatus.STATUS_SUCCESS, "", links, content.getMetadata());    return ParseResult.createParseResult(content.getUrl(), new ParseImpl(text, parseData));}
1
public static void main(String[] args) throws IOException
{    FileInputStream in = new FileInputStream(args[0]);    byte[] buf = new byte[in.available()];    in.read(buf);    in.close();    SWFParser parser = new SWFParser();    ParseResult parseResult = parser.getParse(new Content("file:" + args[0], "file:" + args[0], buf, "application/x-shockwave-flash", new Metadata(), NutchConfiguration.create()));    Parse p = parseResult.get("file:" + args[0]);    System.out.println("Parse Text:");    System.out.println(p.getText());    System.out.println("Parse Data:");    System.out.println(p.getData());}
0
public String getText()
{    StringBuffer res = new StringBuffer();    Iterator<String> it = strings.iterator();    while (it.hasNext()) {        if (res.length() > 0)            res.append(' ');        res.append(it.next());    }    return res.toString();}
0
public String getActionText()
{    StringBuffer res = new StringBuffer();    String[] strings = (String[]) actionStrings.toArray(new String[actionStrings.size()]);    Arrays.sort(strings);    for (int i = 0; i < strings.length; i++) {        if (i > 0)            res.append('\n');        res.append(strings[i]);    }    return res.toString();}
0
public String[] getUrls()
{    String[] res = new String[urls.size()];    int i = 0;    Iterator<String> it = urls.iterator();    while (it.hasNext()) {        res[i] = it.next();        i++;    }    return res;}
0
public void tagDefineFontInfo2(int arg0, String arg1, int arg2, int[] arg3, int arg4) throws IOException
{    tagDefineFontInfo(arg0, arg1, arg2, arg3);}
0
public void tagDefineFontInfo(int fontId, String fontName, int flags, int[] codes) throws IOException
{            fontCodes.put(Integer.valueOf(fontId), codes);}
0
public SWFVectors tagDefineFont2(int id, int flags, String name, int numGlyphs, int ascent, int descent, int leading, int[] codes, int[] advances, Rect[] bounds, int[] kernCodes1, int[] kernCodes2, int[] kernAdjustments) throws IOException
{    fontCodes.put(Integer.valueOf(id), (codes != null) ? codes : new int[0]);    return null;}
0
public void tagDefineTextField(int fieldId, String fieldName, String initialText, Rect boundary, int flags, AlphaColor textColor, int alignment, int fontId, int fontSize, int charLimit, int leftMargin, int rightMargin, int indentation, int lineSpacing) throws IOException
{    if (initialText != null) {        strings.add(initialText);    }}
0
public SWFText tagDefineText(int id, Rect bounds, Matrix matrix) throws IOException
{    lastBounds = curBounds;    curBounds = bounds;    return new TextDumper();}
0
public SWFText tagDefineText2(int id, Rect bounds, Matrix matrix) throws IOException
{    lastBounds = curBounds;    curBounds = bounds;    return new TextDumper();}
0
public void font(int fontId, int textHeight)
{    this.fontId = fontId;}
0
public void setY(int y)
{    if (firstY)        firstY = false;    else                strings.add("\n");}
0
public void text(int[] glyphIndices, int[] glyphAdvances)
{    int[] codes = (int[]) fontCodes.get(fontId);    if (codes == null) {                strings.add("\n**** ?????????????? ****\n");        return;    }        char[] chars = new char[glyphIndices.length];    for (int i = 0; i < chars.length; i++) {        int index = glyphIndices[i];        if (        index >= codes.length) {            chars[i] = (char) index;        } else {            chars[i] = (char) (codes[index]);        }    }    strings.add(new String(chars));}
0
public void color(Color color)
{}
0
public void setX(int x)
{}
0
public void done()
{    strings.add("\n");}
0
public SWFActions tagDoAction() throws IOException
{    return new NutchSWFActions(actionStrings, urls);}
0
public SWFActions tagDoInitAction(int arg0) throws IOException
{    return new NutchSWFActions(actionStrings, urls);}
0
public void lookupTable(String[] values) throws IOException
{    for (int i = 0; i < values.length; i++) {        if (!strings.contains(values[i]))            strings.add(values[i]);    }    super.lookupTable(values);    dict = values;}
0
public void defineLocal() throws IOException
{    stack.pop();    super.defineLocal();}
0
public void getURL(int vars, int mode)
{}
0
public void getURL(String url, String target) throws IOException
{    stack.push(url);    stack.push(target);    strings.remove(url);    strings.remove(target);    urls.add(url);    super.getURL(url, target);}
0
public SWFActionBlock.TryCatchFinally _try(String var) throws IOException
{    strings.remove(var);    return super._try(var);}
0
public void comment(String var) throws IOException
{    strings.remove(var);    super.comment(var);}
0
public void goToFrame(String var) throws IOException
{    stack.push(var);    strings.remove(var);    super.gotoFrame(var);}
0
public void ifJump(String var) throws IOException
{    strings.remove(var);    super.ifJump(var);}
0
public void jump(String var) throws IOException
{    strings.remove(var);    super.jump(var);}
0
public void jumpLabel(String var) throws IOException
{    strings.remove(var);    super.jumpLabel(var);}
0
public void lookup(int var) throws IOException
{    if (dict != null && var >= 0 && var < dict.length) {        stack.push(dict[var]);    }    super.lookup(var);}
0
public void push(String var) throws IOException
{    stack.push(var);    strings.remove(var);    super.push(var);}
0
public void setTarget(String var) throws IOException
{    stack.push(var);    strings.remove(var);    super.setTarget(var);}
0
public SWFActionBlock startFunction(String var, String[] params) throws IOException
{    stack.push(var);    strings.remove(var);    if (params != null) {        for (int i = 0; i < params.length; i++) {            strings.remove(params[i]);        }    }    return this;}
0
public SWFActionBlock startFunction2(String var, int arg1, int arg2, String[] params, int[] arg3) throws IOException
{    stack.push(var);    strings.remove(var);    if (params != null) {        for (int i = 0; i < params.length; i++) {            strings.remove(params[i]);        }    }    return this;}
0
public void waitForFrame(int num, String var) throws IOException
{    stack.push(var);    strings.remove(var);    super.waitForFrame(num, var);}
0
public void waitForFrame(String var) throws IOException
{    stack.push(var);    strings.remove(var);    super.waitForFrame(var);}
0
public void done() throws IOException
{    while (stack.size() > 0) {        strings.remove(stack.pop());    }}
0
public SWFActionBlock start(int arg0, int arg1) throws IOException
{    return this;}
0
public SWFActionBlock start(int arg0) throws IOException
{    return this;}
0
public void add() throws IOException
{    super.add();}
0
public void asciiToChar() throws IOException
{    super.asciiToChar();}
0
public void asciiToCharMB() throws IOException
{    super.asciiToCharMB();}
0
public void push(int var) throws IOException
{    if (dict != null && var >= 0 && var < dict.length) {        stack.push(dict[var]);    }    super.push(var);}
0
public void callFunction() throws IOException
{    strings.remove(stack.pop());    super.callFunction();}
0
public void callMethod() throws IOException
{    strings.remove(stack.pop());    super.callMethod();}
0
public void getMember() throws IOException
{        String val = (String) stack.pop();    strings.remove(val);    super.getMember();}
0
public void setMember() throws IOException
{            stack.pop();    String name = (String) stack.pop();    strings.remove(name);    super.setMember();}
0
public void setProperty() throws IOException
{    super.setProperty();}
0
public void setVariable() throws IOException
{    super.setVariable();}
0
public void call() throws IOException
{    strings.remove(stack.pop());    super.call();}
0
public void setTarget() throws IOException
{    strings.remove(stack.pop());    super.setTarget();}
0
public void pop() throws IOException
{    strings.remove(stack.pop());    super.pop();}
0
public void push(boolean arg0) throws IOException
{    stack.push("" + arg0);    super.push(arg0);}
0
public void push(double arg0) throws IOException
{    stack.push("" + arg0);    super.push(arg0);}
0
public void push(float arg0) throws IOException
{    stack.push("" + arg0);    super.push(arg0);}
0
public void pushNull() throws IOException
{    stack.push("");    super.pushNull();}
0
public void pushRegister(int arg0) throws IOException
{    stack.push("" + arg0);    super.pushRegister(arg0);}
0
public void pushUndefined() throws IOException
{    stack.push("???");    super.pushUndefined();}
0
public void getProperty() throws IOException
{    stack.pop();    super.getProperty();}
0
public void getVariable() throws IOException
{    strings.remove(stack.pop());    super.getVariable();}
0
public void gotoFrame(boolean arg0) throws IOException
{    stack.push("" + arg0);    super.gotoFrame(arg0);}
0
public void gotoFrame(int arg0) throws IOException
{    stack.push("" + arg0);    super.gotoFrame(arg0);}
0
public void gotoFrame(String arg0) throws IOException
{    stack.push("" + arg0);    strings.remove(arg0);    super.gotoFrame(arg0);}
0
public void newObject() throws IOException
{    stack.pop();    super.newObject();}
0
public SWFActionBlock startWith() throws IOException
{    return this;}
0
public Object push(Object o)
{        if (this.size() > maxSize) {        String val = (String) remove(0);        strings.remove(val);    }    return super.push(o);}
0
public Object pop()
{        if (this.size() == 0)        return null;    else        return super.pop();}
0
public void testIt() throws ProtocolException, ParseException
{    String urlString;    Protocol protocol;    Content content;    Parse parse;    Configuration conf = NutchConfiguration.create();    for (int i = 0; i < sampleFiles.length; i++) {        urlString = "file:" + sampleDir + fileSeparator + sampleFiles[i];        protocol = new ProtocolFactory(conf).getProtocol(urlString);        content = protocol.getProtocolOutput(new Text(urlString), new CrawlDatum()).getContent();        parse = new ParseUtil(conf).parse(content).get(content.getUrl());        String text = parse.getText().replaceAll("[ \t\r\n]+", " ").trim();        Assert.assertTrue(sampleTexts[i].equals(text));    }}
0
public static synchronized BoilerpipeExtractor getExtractor(String boilerpipeExtractorName)
{        if (!extractorRepository.containsKey(boilerpipeExtractorName)) {                boilerpipeExtractorName = "de.l3s.boilerpipe.extractors." + boilerpipeExtractorName;                try {            ClassLoader loader = BoilerpipeExtractor.class.getClassLoader();            Class extractorClass = loader.loadClass(boilerpipeExtractorName);                        extractorRepository.put(boilerpipeExtractorName, (BoilerpipeExtractor) extractorClass.getConstructor().newInstance());        } catch (ClassNotFoundException e) {                    } catch (InstantiationException e) {                    } catch (Exception e) {                    }    }    return extractorRepository.get(boilerpipeExtractorName);}
1
 Node getRootNode()
{    return (null != m_docFrag) ? (Node) m_docFrag : (Node) m_doc;}
0
 Node getCurrentNode()
{    return m_currentNode;}
0
 java.io.Writer getWriter()
{    return null;}
0
protected void append(Node newNode) throws org.xml.sax.SAXException
{    Node currentNode = m_currentNode;    if (null != currentNode) {        currentNode.appendChild(newNode);        } else if (null != m_docFrag) {        m_docFrag.appendChild(newNode);    } else {        boolean ok = true;        short type = newNode.getNodeType();        if (type == Node.TEXT_NODE) {            String data = newNode.getNodeValue();            if ((null != data) && (data.trim().length() > 0)) {                throw new org.xml.sax.SAXException("Warning: can't output text before document element!  Ignoring...");            }            ok = false;        } else if (type == Node.ELEMENT_NODE) {            if (m_doc.getDocumentElement() != null) {                throw new org.xml.sax.SAXException("Can't have more than one root on a DOM!");            }        }        if (ok)            m_doc.appendChild(newNode);    }}
0
public void setDocumentLocator(Locator locator)
{}
0
public void startDocument() throws org.xml.sax.SAXException
{}
0
public void endDocument() throws org.xml.sax.SAXException
{}
0
public void startElement(String ns, String localName, String name, Attributes atts) throws org.xml.sax.SAXException
{    Element elem;    if (upperCaseElementNames)        name = name.toUpperCase();        if ((null == ns) || (ns.length() == 0) || ns.equals(defaultNamespaceURI))        elem = m_doc.createElementNS(null, name);    else        elem = m_doc.createElementNS(ns, name);    append(elem);    try {        int nAtts = atts.getLength();        if (0 != nAtts) {            for (int i = 0; i < nAtts; i++) {                                if (atts.getType(i).equalsIgnoreCase("ID"))                    setIDAttribute(atts.getValue(i), elem);                String attrNS = atts.getURI(i);                if ("".equals(attrNS))                                        attrNS = null;                                                                String attrQName = atts.getQName(i);                                if (attrQName.startsWith("xmlns:"))                    attrNS = "http://www.w3.org/2000/xmlns/";                                elem.setAttributeNS(attrNS, attrQName, atts.getValue(i));            }        }                m_elemStack.push(elem);        m_currentNode = elem;        } catch (java.lang.Exception de) {                throw new org.xml.sax.SAXException(de);    }}
0
public void endElement(String ns, String localName, String name) throws org.xml.sax.SAXException
{    if (!m_elemStack.isEmpty()) {        m_elemStack.pop();    }    m_currentNode = m_elemStack.isEmpty() ? null : (Node) m_elemStack.peek();}
0
public void setIDAttribute(String id, Element elem)
{}
0
public void characters(char[] ch, int start, int length) throws org.xml.sax.SAXException
{    if (isOutsideDocElem() && XMLCharacterRecognizer.isWhiteSpace(ch, start, length))                return;    if (m_inCData) {        cdata(ch, start, length);        return;    }    String s = new String(ch, start, length);    Node childNode;    childNode = m_currentNode != null ? m_currentNode.getLastChild() : null;    if (childNode != null && childNode.getNodeType() == Node.TEXT_NODE) {        ((Text) childNode).appendData(s);    } else {        Text text = m_doc.createTextNode(s);        append(text);    }}
0
public void charactersRaw(char[] ch, int start, int length) throws org.xml.sax.SAXException
{    if (isOutsideDocElem() && XMLCharacterRecognizer.isWhiteSpace(ch, start, length))                return;    String s = new String(ch, start, length);    append(m_doc.createProcessingInstruction("xslt-next-is-raw", "formatter-to-dom"));    append(m_doc.createTextNode(s));}
0
public void startEntity(String name) throws org.xml.sax.SAXException
{}
0
public void endEntity(String name) throws org.xml.sax.SAXException
{}
0
public void entityReference(String name) throws org.xml.sax.SAXException
{    append(m_doc.createEntityReference(name));}
0
public void ignorableWhitespace(char[] ch, int start, int length) throws org.xml.sax.SAXException
{    if (isOutsideDocElem())                return;    String s = new String(ch, start, length);    append(m_doc.createTextNode(s));}
0
private boolean isOutsideDocElem()
{    return (null == m_docFrag) && m_elemStack.size() == 0 && (null == m_currentNode || m_currentNode.getNodeType() == Node.DOCUMENT_NODE);}
0
public void processingInstruction(String target, String data) throws org.xml.sax.SAXException
{    append(m_doc.createProcessingInstruction(target, data));}
0
public void comment(char[] ch, int start, int length) throws org.xml.sax.SAXException
{        if (ch == null || start < 0 || length >= (ch.length - start) || length < 0)        return;    append(m_doc.createComment(new String(ch, start, length)));}
0
public void startCDATA() throws org.xml.sax.SAXException
{    m_inCData = true;    append(m_doc.createCDATASection(""));}
0
public void endCDATA() throws org.xml.sax.SAXException
{    m_inCData = false;}
0
public void cdata(char[] ch, int start, int length) throws org.xml.sax.SAXException
{    if (isOutsideDocElem() && XMLCharacterRecognizer.isWhiteSpace(ch, start, length))                return;    String s = new String(ch, start, length);        Node n = m_currentNode.getLastChild();    if (n instanceof CDATASection)        ((CDATASection) n).appendData(s);    else if (n instanceof Comment)        ((Comment) n).appendData(s);}
0
public void startDTD(String name, String publicId, String systemId) throws org.xml.sax.SAXException
{}
0
public void endDTD() throws org.xml.sax.SAXException
{}
0
public void startPrefixMapping(String prefix, String uri) throws org.xml.sax.SAXException
{/*     *      * if((null != m_currentNode) && (m_currentNode.getNodeType() ==     * Node.ELEMENT_NODE)) { String qname; if(((null != prefix) &&     * (prefix.length() == 0)) || (null == prefix)) qname = "xmlns"; else qname     * = "xmlns:"+prefix;     *      * Element elem = (Element)m_currentNode; String val =     * elem.getAttribute(qname);      * { elem.setAttributeNS("http://www.w3.org/XML/1998/namespace", qname,     * uri); } }     */}
0
public void endPrefixMapping(String prefix) throws org.xml.sax.SAXException
{}
0
public void skippedEntity(String name) throws org.xml.sax.SAXException
{}
0
public boolean isUpperCaseElementNames()
{    return upperCaseElementNames;}
0
public void setUpperCaseElementNames(boolean upperCaseElementNames)
{    this.upperCaseElementNames = upperCaseElementNames;}
0
public String getDefaultNamespaceURI()
{    return defaultNamespaceURI;}
0
public void setDefaultNamespaceURI(String defaultNamespaceURI)
{    this.defaultNamespaceURI = defaultNamespaceURI;}
0
public String toString()
{    return "LP[el=" + elName + ",attr=" + attrName + ",len=" + childLen + "]";}
0
public void setConf(Configuration conf)
{        Collection<String> forceTags = new ArrayList<String>(1);    this.conf = conf;    linkParams.clear();    linkParams.put("a", new LinkParams("a", "href", 1));    linkParams.put("area", new LinkParams("area", "href", 0));    if (conf.getBoolean("parser.html.form.use_action", true)) {        linkParams.put("form", new LinkParams("form", "action", 1));        if (conf.get("parser.html.form.use_action") != null)            forceTags.add("form");    }    linkParams.put("frame", new LinkParams("frame", "src", 0));    linkParams.put("iframe", new LinkParams("iframe", "src", 0));    linkParams.put("script", new LinkParams("script", "src", 0));    linkParams.put("link", new LinkParams("link", "href", 0));    linkParams.put("img", new LinkParams("img", "src", 0));    linkParams.put("source", new LinkParams("source", "src", 0));        String[] ignoreTags = conf.getStrings("parser.html.outlinks.ignore_tags");    for (int i = 0; ignoreTags != null && i < ignoreTags.length; i++) {        ignoredTags.add(ignoreTags[i].toLowerCase());        if (!forceTags.contains(ignoreTags[i]))            linkParams.remove(ignoreTags[i]);    }        srcTagMetaName = this.conf.get("parser.html.outlinks.htmlnode_metadata_name");    keepNodenames = (srcTagMetaName != null && srcTagMetaName.length() > 0);    blockNodes = new HashSet<>(conf.getTrimmedStringCollection("parser.html.line.separators"));}
0
private boolean getText(StringBuffer sb, Node node, boolean abortOnNestedAnchors)
{    if (getTextHelper(sb, node, abortOnNestedAnchors, 0)) {        return true;    }    return false;}
0
public void getText(StringBuffer sb, Node node)
{    getText(sb, node, false);}
0
private boolean getTextHelper(StringBuffer sb, Node node, boolean abortOnNestedAnchors, int anchorDepth)
{    boolean abort = false;    NodeWalker walker = new NodeWalker(node);    while (walker.hasNext()) {        Node currentNode = walker.nextNode();        String nodeName = currentNode.getNodeName();        short nodeType = currentNode.getNodeType();        Node previousSibling = currentNode.getPreviousSibling();        if (previousSibling != null && blockNodes.contains(previousSibling.getNodeName().toLowerCase())) {            appendParagraphSeparator(sb);        } else if (blockNodes.contains(nodeName.toLowerCase())) {            appendParagraphSeparator(sb);        }        if ("script".equalsIgnoreCase(nodeName)) {            walker.skipChildren();        }        if ("style".equalsIgnoreCase(nodeName)) {            walker.skipChildren();        }        if (abortOnNestedAnchors && "a".equalsIgnoreCase(nodeName)) {            anchorDepth++;            if (anchorDepth > 1) {                abort = true;                break;            }        }        if (nodeType == Node.COMMENT_NODE) {            walker.skipChildren();        }        if (nodeType == Node.TEXT_NODE) {                        String text = currentNode.getNodeValue();            text = text.replaceAll("\\s+", " ");            text = text.trim();            if (text.length() > 0) {                appendSpace(sb);                sb.append(text);            } else {                appendParagraphSeparator(sb);            }        }    }    return abort;}
0
private void appendParagraphSeparator(StringBuffer buffer)
{    if (buffer.length() == 0) {        return;    }    char lastChar = buffer.charAt(buffer.length() - 1);    if ('\n' != lastChar) {                while (lastChar == ' ') {            buffer.deleteCharAt(buffer.length() - 1);            lastChar = buffer.charAt(buffer.length() - 1);        }        if ('\n' != lastChar) {            buffer.append('\n');        }    }}
0
private void appendSpace(StringBuffer buffer)
{    if (buffer.length() == 0) {        return;    }    char lastChar = buffer.charAt(buffer.length() - 1);    if (' ' != lastChar && '\n' != lastChar) {        buffer.append(' ');    }}
0
public boolean getTitle(StringBuffer sb, Node node)
{    NodeWalker walker = new NodeWalker(node);    while (walker.hasNext()) {        Node currentNode = walker.nextNode();        String nodeName = currentNode.getNodeName();        short nodeType = currentNode.getNodeType();        if ("body".equalsIgnoreCase(nodeName)) {                        return false;        }        if (nodeType == Node.ELEMENT_NODE) {            if ("title".equalsIgnoreCase(nodeName)) {                getText(sb, currentNode);                return true;            }        }    }    return false;}
0
public String getBase(Node node)
{    NodeWalker walker = new NodeWalker(node);    while (walker.hasNext()) {        Node currentNode = walker.nextNode();        String nodeName = currentNode.getNodeName();        short nodeType = currentNode.getNodeType();                if (nodeType == Node.ELEMENT_NODE) {            if ("body".equalsIgnoreCase(nodeName)) {                                return null;            }            if ("base".equalsIgnoreCase(nodeName)) {                NamedNodeMap attrs = currentNode.getAttributes();                for (int i = 0; i < attrs.getLength(); i++) {                    Node attr = attrs.item(i);                    if ("href".equalsIgnoreCase(attr.getNodeName())) {                        return attr.getNodeValue();                    }                }            }        }    }        return null;}
0
private boolean hasOnlyWhiteSpace(Node node)
{    String val = node.getNodeValue();    for (int i = 0; i < val.length(); i++) {        if (!Character.isWhitespace(val.charAt(i)))            return false;    }    return true;}
0
private boolean shouldThrowAwayLink(Node node, NodeList children, int childLen, LinkParams params)
{    if (childLen == 0) {                if (params.childLen == 0)            return false;        else            return true;    } else if ((childLen == 1) && (children.item(0).getNodeType() == Node.ELEMENT_NODE) && (params.elName.equalsIgnoreCase(children.item(0).getNodeName()))) {                return true;    } else if (childLen == 2) {        Node c0 = children.item(0);        Node c1 = children.item(1);        if ((c0.getNodeType() == Node.ELEMENT_NODE) && (params.elName.equalsIgnoreCase(c0.getNodeName())) && (c1.getNodeType() == Node.TEXT_NODE) && hasOnlyWhiteSpace(c1)) {                        return true;        }        if ((c1.getNodeType() == Node.ELEMENT_NODE) && (params.elName.equalsIgnoreCase(c1.getNodeName())) && (c0.getNodeType() == Node.TEXT_NODE) && hasOnlyWhiteSpace(c0)) {                        return true;        }    } else if (childLen == 3) {        Node c0 = children.item(0);        Node c1 = children.item(1);        Node c2 = children.item(2);        if ((c1.getNodeType() == Node.ELEMENT_NODE) && (params.elName.equalsIgnoreCase(c1.getNodeName())) && (c0.getNodeType() == Node.TEXT_NODE) && (c2.getNodeType() == Node.TEXT_NODE) && hasOnlyWhiteSpace(c0) && hasOnlyWhiteSpace(c2)) {                        return true;        }    }    return false;}
0
public void getOutlinks(URL base, ArrayList<Outlink> outlinks, Node node)
{    NodeWalker walker = new NodeWalker(node);    while (walker.hasNext()) {        Node currentNode = walker.nextNode();        String nodeName = currentNode.getNodeName();        short nodeType = currentNode.getNodeType();        NodeList children = currentNode.getChildNodes();        int childLen = (children != null) ? children.getLength() : 0;        if (nodeType == Node.ELEMENT_NODE) {            nodeName = nodeName.toLowerCase();            LinkParams params = (LinkParams) linkParams.get(nodeName);            if (params != null) {                if (!shouldThrowAwayLink(currentNode, children, childLen, params)) {                    StringBuffer linkText = new StringBuffer();                    getText(linkText, currentNode, true);                    NamedNodeMap attrs = currentNode.getAttributes();                    String target = null;                    boolean noFollow = false;                    boolean post = false;                    for (int i = 0; i < attrs.getLength(); i++) {                        Node attr = attrs.item(i);                        String attrName = attr.getNodeName();                        if (params.attrName.equalsIgnoreCase(attrName)) {                            target = attr.getNodeValue();                        } else if ("rel".equalsIgnoreCase(attrName) && "nofollow".equalsIgnoreCase(attr.getNodeValue())) {                            noFollow = true;                        } else if ("method".equalsIgnoreCase(attrName) && "post".equalsIgnoreCase(attr.getNodeValue())) {                            post = true;                        }                    }                    if (target != null && !noFollow && !post)                        try {                            URL url = URLUtil.resolveURL(base, target);                            Outlink outlink = new Outlink(url.toString(), linkText.toString().trim());                            outlinks.add(outlink);                                                        if (keepNodenames) {                                MapWritable metadata = new MapWritable();                                metadata.put(new Text(srcTagMetaName), new Text(nodeName));                                outlink.setMetadata(metadata);                            }                        } catch (MalformedURLException e) {                                                }                }                                if (params.childLen == 0)                    continue;            }        }    }}
0
public void getOutlinks(URL base, ArrayList<Outlink> outlinks, List<Link> tikaExtractedOutlinks)
{    String target = null;    String anchor = null;    boolean noFollow = false;    for (Link link : tikaExtractedOutlinks) {        target = link.getUri();        noFollow = (link.getRel().toLowerCase().equals("nofollow")) ? true : false;        anchor = link.getText();        if (!ignoredTags.contains(link.getType())) {            if (target != null && !noFollow) {                try {                    URL url = URLUtil.resolveURL(base, target);                                        anchor = anchor.replaceAll("\\s+", " ");                    anchor = anchor.trim();                    outlinks.add(new Outlink(url.toString(), anchor));                } catch (MalformedURLException e) {                                }            }        }    }}
0
public static final void getMetaTags(HTMLMetaTags metaTags, Node node, URL currURL)
{    metaTags.reset();    getMetaTagsHelper(metaTags, node, currURL);}
0
private static final void getMetaTagsHelper(HTMLMetaTags metaTags, Node node, URL currURL)
{    if (node.getNodeType() == Node.ELEMENT_NODE) {        if ("body".equalsIgnoreCase(node.getNodeName())) {                        return;        }        if ("meta".equalsIgnoreCase(node.getNodeName())) {            NamedNodeMap attrs = node.getAttributes();            Node nameNode = null;            Node equivNode = null;            Node contentNode = null;                        for (int i = 0; i < attrs.getLength(); i++) {                Node attr = attrs.item(i);                String attrName = attr.getNodeName().toLowerCase();                if (attrName.equals("name")) {                    nameNode = attr;                } else if (attrName.equals("http-equiv")) {                    equivNode = attr;                } else if (attrName.equals("content")) {                    contentNode = attr;                }            }            if (nameNode != null) {                if (contentNode != null) {                    String name = nameNode.getNodeValue().toLowerCase();                    metaTags.getGeneralTags().add(name, contentNode.getNodeValue());                    if ("robots".equals(name)) {                        String directives = contentNode.getNodeValue().toLowerCase();                        int index = directives.indexOf("none");                        if (index >= 0) {                            metaTags.setNoIndex();                            metaTags.setNoFollow();                        }                        index = directives.indexOf("all");                        if (index >= 0) {                                                }                        index = directives.indexOf("noindex");                        if (index >= 0) {                            metaTags.setNoIndex();                        }                        index = directives.indexOf("nofollow");                        if (index >= 0) {                            metaTags.setNoFollow();                        }                        index = directives.indexOf("noarchive");                        if (index >= 0) {                            metaTags.setNoCache();                        }                    } else                     if (name.equals("pragma")) {                        String content = contentNode.getNodeValue().toLowerCase();                        if (content.contains("no-cache")) {                            metaTags.setNoCache();                        }                    } else if (name.equals("refresh")) {                        String content = contentNode.getNodeValue().toLowerCase();                        setRefresh(metaTags, content, currURL);                    } else if (name.equals("content-location")) {                        String urlString = contentNode.getNodeValue();                        URL url = null;                        try {                            if (currURL == null) {                                url = new URL(urlString);                            } else {                                url = new URL(currURL, urlString);                            }                            metaTags.setBaseHref(url);                        } catch (MalformedURLException e) {                                                }                    }                }            }            if (equivNode != null) {                if (contentNode != null) {                    String name = equivNode.getNodeValue().toLowerCase();                    String content = contentNode.getNodeValue();                    metaTags.getHttpEquivTags().setProperty(name, content);                    if ("pragma".equals(name)) {                        content = content.toLowerCase();                        int index = content.indexOf("no-cache");                        if (index >= 0)                            metaTags.setNoCache();                    } else if ("refresh".equals(name)) {                        setRefresh(metaTags, content, currURL);                    }                }            }        } else if ("base".equalsIgnoreCase(node.getNodeName())) {            NamedNodeMap attrs = node.getAttributes();            Node hrefNode = attrs.getNamedItem("href");            if (hrefNode != null) {                String urlString = hrefNode.getNodeValue();                URL url = null;                try {                    if (currURL == null)                        url = new URL(urlString);                    else                        url = new URL(currURL, urlString);                } catch (Exception e) {                    ;                }                if (url != null)                    metaTags.setBaseHref(url);            }        }    }    NodeList children = node.getChildNodes();    if (children != null) {        int len = children.getLength();        for (int i = 0; i < len; i++) {            getMetaTagsHelper(metaTags, children.item(i), currURL);        }    }}
0
private static void setRefresh(HTMLMetaTags metaTags, String content, URL currURL)
{    int idx = content.indexOf(';');    String time = null;    if (idx == -1) {                time = content;    } else        time = content.substring(0, idx);    try {        metaTags.setRefreshTime(Integer.parseInt(time));                metaTags.setRefresh(true);    } catch (Exception e) {        ;    }    URL refreshUrl = null;    if (metaTags.getRefresh() && idx != -1) {                idx = content.toLowerCase().indexOf("url=");        if (idx == -1) {                                    idx = content.indexOf(';') + 1;        } else            idx += 4;        if (idx != -1) {            String url = content.substring(idx);            try {                refreshUrl = new URL(url);            } catch (Exception e) {                                try {                    refreshUrl = new URL(currURL, url);                } catch (Exception e1) {                    refreshUrl = null;                }            }        }    }    if (metaTags.getRefresh()) {        if (refreshUrl == null) {                                    refreshUrl = currURL;        }        metaTags.setRefreshHref(refreshUrl);    }}
0
public ParseResult getParse(Content content)
{    HTMLDocumentImpl doc = new HTMLDocumentImpl();    doc.setErrorChecking(false);    DocumentFragment root = doc.createDocumentFragment();    return getParse(content, doc, root);}
0
 ParseResult getParse(Content content, HTMLDocumentImpl doc, DocumentFragment root)
{    String mimeType = content.getContentType();    URL base;    try {        base = new URL(content.getBaseUrl());    } catch (MalformedURLException e) {        return new ParseStatus(e).getEmptyParseResult(content.getUrl(), getConf());    }        CompositeParser compositeParser = (CompositeParser) tikaConfig.getParser();    Parser parser = compositeParser.getParsers().get(MediaType.parse(mimeType));    if (parser == null) {        String message = "Can't retrieve Tika parser for mime-type " + mimeType;                return new ParseStatus(ParseStatus.FAILED, message).getEmptyParseResult(content.getUrl(), getConf());    }        byte[] raw = content.getContent();    Metadata tikamd = new Metadata();    ContentHandler domHandler;        if (useBoilerpipe && boilerpipeMimeTypes.contains(mimeType)) {        BoilerpipeContentHandler bpHandler = new BoilerpipeContentHandler((ContentHandler) new DOMBuilder(doc, root), BoilerpipeExtractorRepository.getExtractor(boilerpipeExtractorName));        bpHandler.setIncludeMarkup(true);        domHandler = (ContentHandler) bpHandler;    } else {        DOMBuilder domBuilder = new DOMBuilder(doc, root);        domBuilder.setUpperCaseElementNames(upperCaseElementNames);        domBuilder.setDefaultNamespaceURI(XHTMLContentHandler.XHTML);        domHandler = (ContentHandler) domBuilder;    }    LinkContentHandler linkContentHandler = new LinkContentHandler();    ParseContext context = new ParseContext();    if (parseEmbedded) {        context.set(Parser.class, new AutoDetectParser(tikaConfig));    }    TeeContentHandler teeContentHandler = new TeeContentHandler(domHandler, linkContentHandler);    if (HTMLMapper != null)        context.set(HtmlMapper.class, HTMLMapper);    tikamd.set(Metadata.CONTENT_TYPE, mimeType);    try {        parser.parse(new ByteArrayInputStream(raw), (ContentHandler) teeContentHandler, tikamd, context);    } catch (Exception e) {                return new ParseStatus(ParseStatus.FAILED, e.getMessage()).getEmptyParseResult(content.getUrl(), getConf());    }    HTMLMetaTags metaTags = new HTMLMetaTags();    String text = "";    String title = "";    Outlink[] outlinks = new Outlink[0];    org.apache.nutch.metadata.Metadata nutchMetadata = new org.apache.nutch.metadata.Metadata();                HTMLMetaProcessor.getMetaTags(metaTags, root, base);    if (LOG.isTraceEnabled()) {        LOG.trace("Meta tags for " + base + ": " + metaTags.toString());    }        if (!metaTags.getNoIndex()) {                StringBuffer sb = new StringBuffer();        if (LOG.isTraceEnabled()) {            LOG.trace("Getting text...");        }                utils.getText(sb, root);        text = sb.toString();        sb.setLength(0);        if (LOG.isTraceEnabled()) {            LOG.trace("Getting title...");        }                utils.getTitle(sb, root);        title = sb.toString().trim();    }    if (!metaTags.getNoFollow()) {                        ArrayList<Outlink> l = new ArrayList<Outlink>();        URL baseTag = base;        String baseTagHref = tikamd.get("Content-Location");        if (baseTagHref != null) {            try {                baseTag = new URL(base, baseTagHref);            } catch (MalformedURLException e) {                LOG.trace("Invalid <base href=\"{}\">", baseTagHref);            }        }        if (LOG.isTraceEnabled()) {            LOG.trace("Getting links (base URL = {}) ...", baseTag);        }                                List<Link> tikaExtractedOutlinks = linkContentHandler.getLinks();        utils.getOutlinks(baseTag, l, tikaExtractedOutlinks);        outlinks = l.toArray(new Outlink[l.size()]);        if (LOG.isTraceEnabled()) {            LOG.trace("found " + outlinks.length + " outlinks in " + content.getUrl());        }    }        String[] TikaMDNames = tikamd.names();    for (String tikaMDName : TikaMDNames) {        if (tikaMDName.equalsIgnoreCase(Metadata.TITLE))            continue;        String[] values = tikamd.getValues(tikaMDName);        for (String v : values) nutchMetadata.add(tikaMDName, v);    }    if (outlinks.length == 0) {        outlinks = OutlinkExtractor.getOutlinks(text, getConf());    }    ParseStatus status = new ParseStatus(ParseStatus.SUCCESS);    if (metaTags.getRefresh()) {        status.setMinorCode(ParseStatus.SUCCESS_REDIRECT);        status.setArgs(new String[] { metaTags.getRefreshHref().toString(), Integer.toString(metaTags.getRefreshTime()) });    }    ParseData parseData = new ParseData(status, title, outlinks, content.getMetadata(), nutchMetadata);    ParseResult parseResult = ParseResult.createParseResult(content.getUrl(), new ParseImpl(text, parseData));        ParseResult filteredParse = this.htmlParseFilters.filter(content, parseResult, metaTags, root);    if (metaTags.getNoCache()) {                for (Map.Entry<org.apache.hadoop.io.Text, Parse> entry : filteredParse) entry.getValue().getData().getParseMeta().set(Nutch.CACHING_FORBIDDEN_KEY, cachingPolicy);    }    return filteredParse;}
1
public void setConf(Configuration conf)
{    this.conf = conf;    this.tikaConfig = null;                String customConfFile = conf.get("tika.config.file");    if (customConfFile != null) {        try {                        URL customTikaConfig = conf.getResource(customConfFile);            if (customTikaConfig != null) {                tikaConfig = new TikaConfig(customTikaConfig, this.getClass().getClassLoader());            }        } catch (Exception e1) {            String message = "Problem loading custom Tika configuration from " + customConfFile;                    }    }    if (tikaConfig == null) {        try {            tikaConfig = new TikaConfig(this.getClass().getClassLoader());        } catch (Exception e2) {            String message = "Problem loading default Tika configuration";                    }    }        String htmlmapperClassName = conf.get("tika.htmlmapper.classname");    if (StringUtils.isNotBlank(htmlmapperClassName)) {        try {            Class<?> HTMLMapperClass = Class.forName(htmlmapperClassName);            boolean interfaceOK = HtmlMapper.class.isAssignableFrom(HTMLMapperClass);            if (!interfaceOK) {                throw new RuntimeException("Class " + htmlmapperClassName + " does not implement HtmlMapper");            }            HTMLMapper = (HtmlMapper) HTMLMapperClass.getConstructor().newInstance();        } catch (Exception e) {            String message = "Can't generate instance for class " + htmlmapperClassName;                        throw new RuntimeException(message);        }    }    htmlParseFilters = new HtmlParseFilters(conf);    utils = new DOMContentUtils(conf);    cachingPolicy = conf.get("parser.caching.forbidden.policy", Nutch.CACHING_FORBIDDEN_CONTENT);    upperCaseElementNames = conf.getBoolean("tika.uppercase.element.names", true);    useBoilerpipe = conf.get("tika.extractor", "none").equals("boilerpipe");    boilerpipeExtractorName = conf.get("tika.extractor.boilerpipe.algorithm", "ArticleExtractor");    boilerpipeMimeTypes = new HashSet<>(Arrays.asList(conf.getTrimmedStrings("tika.extractor.boilerpipe.mime.types", "text/html", "application/xhtml+xml")));    parseEmbedded = conf.getBoolean("tika.parse.embedded", true);}
1
public Configuration getConf()
{    return this.conf;}
0
 static boolean isWhiteSpace(char ch)
{    return (ch == 0x20) || (ch == 0x09) || (ch == 0xD) || (ch == 0xA);}
0
 static boolean isWhiteSpace(char[] ch, int start, int length)
{    int end = start + length;    for (int s = start; s < end; s++) {        if (!isWhiteSpace(ch[s]))            return false;    }    return true;}
0
 static boolean isWhiteSpace(StringBuffer buf)
{    int n = buf.length();    for (int i = 0; i < n; i++) {        if (!isWhiteSpace(buf.charAt(i)))            return false;    }    return true;}
0
 static boolean isWhiteSpace(String s)
{    if (null != s) {        int n = s.length();        for (int i = 0; i < n; i++) {            if (!isWhiteSpace(s.charAt(i)))                return false;        }    }    return true;}
0
public void setup() throws Exception
{    conf = NutchConfiguration.create();    utils = new DOMContentUtils(conf);    conf.set("plugin.includes", "parse-tika");    TikaParser parser = new TikaParser();    parser.setConf(conf);    for (int i = 0; i < testPages.length; i++) {        try {            String url = testBaseHrefs[i];            testBaseHrefURLs[i] = new URL(url);            Content content = new Content(url, url, testPages[i].getBytes(StandardCharsets.UTF_8), "text/html", new Metadata(), conf);            HTMLDocumentImpl doc = new HTMLDocumentImpl();            doc.setErrorChecking(false);            DocumentFragment root = doc.createDocumentFragment();            parser.getParse(content, doc, root);            testDOMs[i] = root;        } catch (Exception e) {            Assert.assertTrue("caught exception: " + e, false);        }    }    answerOutlinks = new Outlink[][] { { new Outlink("http://www.nutch.org", "anchor") }, { new Outlink("http://www.nutch.org/", "home"), new Outlink("http://www.nutch.org/docs/bot.html", "bots") }, { new Outlink("http://www.nutch.org/", "separate this"), new Outlink("http://www.nutch.org/docs/ok", "from this") }, { new Outlink("http://www.nutch.org/", "home"), new Outlink("http://www.nutch.org/docs/1", "1"), new Outlink("http://www.nutch.org/docs/2", "2") }, { new Outlink("http://www.nutch.org/frames/top.html", ""), new Outlink("http://www.nutch.org/frames/left.html", ""), new Outlink("http://www.nutch.org/frames/invalid.html", ""), new Outlink("http://www.nutch.org/frames/right.html", "") }, { new Outlink("http://www.nutch.org/maps/logo.gif", ""), new Outlink("http://www.nutch.org/index.html", ""), new Outlink("http://www.nutch.org/maps/#bottom", ""), new Outlink("http://www.nutch.org/bot.html", ""), new Outlink("http://www.nutch.org/docs/index.html", "") }, { new Outlink("http://www.nutch.org/index.html", "whitespace test") }, {}, {}, {}, { new Outlink("http://www.nutch.org/;x", "anchor1"), new Outlink("http://www.nutch.org/g;x", "anchor2"), new Outlink("http://www.nutch.org/g;x?y#s", "anchor3") }, {     new Outlink("http://www.nutch.org/g", "anchor1"), new Outlink("http://www.nutch.org/g?y#s", "anchor2"), new Outlink("http://www.nutch.org/;something?y=1", "anchor3"), new Outlink("http://www.nutch.org/;something?y=1#s", "anchor4"), new Outlink("http://www.nutch.org/;something?y=1;somethingelse", "anchor5") }, {} };}
0
private static boolean equalsIgnoreWhitespace(String s1, String s2)
{    StringTokenizer st1 = new StringTokenizer(s1);    StringTokenizer st2 = new StringTokenizer(s2);    while (st1.hasMoreTokens()) {        if (!st2.hasMoreTokens())            return false;        if (!st1.nextToken().equals(st2.nextToken()))            return false;    }    if (st2.hasMoreTokens())        return false;    return true;}
0
public void testGetText() throws Exception
{    if (testDOMs[0] == null)        setup();    for (int i = 0; i < testPages.length; i++) {        StringBuffer sb = new StringBuffer();        utils.getText(sb, testDOMs[i]);        String text = sb.toString();        Assert.assertTrue("expecting text: " + answerText[i] + System.getProperty("line.separator") + System.getProperty("line.separator") + "got text: " + text, equalsIgnoreWhitespace(answerText[i], text));    }}
0
public void testGetTitle() throws Exception
{    if (testDOMs[0] == null)        setup();    for (int i = 0; i < testPages.length; i++) {        StringBuffer sb = new StringBuffer();        utils.getTitle(sb, testDOMs[i]);        String text = sb.toString();        Assert.assertTrue("expecting text: " + answerText[i] + System.getProperty("line.separator") + System.getProperty("line.separator") + "got text: " + text, equalsIgnoreWhitespace(answerTitle[i], text));    }}
0
public void testGetOutlinks() throws Exception
{    if (testDOMs[0] == null)        setup();    for (int i = 0; i < testPages.length; i++) {        ArrayList<Outlink> outlinks = new ArrayList<Outlink>();        if (i == SKIP) {            conf.setBoolean("parser.html.form.use_action", false);            utils.setConf(conf);        } else {            conf.setBoolean("parser.html.form.use_action", true);            utils.setConf(conf);        }        utils.getOutlinks(testBaseHrefURLs[i], outlinks, testDOMs[i]);        Outlink[] outlinkArr = new Outlink[outlinks.size()];        outlinkArr = outlinks.toArray(outlinkArr);        compareOutlinks(answerOutlinks[i], outlinkArr);    }}
0
private static final void appendOutlinks(StringBuffer sb, Outlink[] o)
{    for (int i = 0; i < o.length; i++) {        sb.append(o[i].toString());        sb.append(System.getProperty("line.separator"));    }}
0
private static final String outlinksString(Outlink[] o)
{    StringBuffer sb = new StringBuffer();    appendOutlinks(sb, o);    return sb.toString();}
0
private static final void compareOutlinks(Outlink[] o1, Outlink[] o2)
{    if (o1.length != o2.length) {        Assert.assertTrue("got wrong number of outlinks (expecting " + o1.length + ", got " + o2.length + ")" + System.getProperty("line.separator") + "answer: " + System.getProperty("line.separator") + outlinksString(o1) + System.getProperty("line.separator") + "got: " + System.getProperty("line.separator") + outlinksString(o2) + System.getProperty("line.separator"), false);    }    for (int i = 0; i < o1.length; i++) {        if (!o1[i].equals(o2[i])) {            Assert.assertTrue("got wrong outlinks at position " + i + System.getProperty("line.separator") + "answer: " + System.getProperty("line.separator") + "'" + o1[i].getToUrl() + "', anchor: '" + o1[i].getAnchor() + "'" + System.getProperty("line.separator") + "got: " + System.getProperty("line.separator") + "'" + o2[i].getToUrl() + "', anchor: '" + o2[i].getAnchor() + "'", false);        }    }}
0
public void setUp()
{    conf = NutchConfiguration.create();    conf.set("file.content.limit", "-1");    conf.setBoolean("tika.parse.embedded", true);}
0
public String getTextContent(String fileName) throws ProtocolException, ParseException
{    String urlString = "file:" + sampleDir + fileSeparator + fileName;    Protocol protocol = new ProtocolFactory(conf).getProtocol(urlString);    Content content = protocol.getProtocolOutput(new Text(urlString), new CrawlDatum()).getContent();    Parse parse = new ParseUtil(conf).parseByExtensionId("parse-tika", content).get(content.getUrl());    return parse.getText();}
0
public void testIt() throws ProtocolException, ParseException
{    for (int i = 0; i < sampleFiles.length; i++) {        String found = getTextContent(sampleFiles[i]);        Assert.assertTrue("text found : '" + found + "'", found.contains(expectedText));    }}
0
public void testIt() throws ProtocolException, ParseException
{    String urlString;    Protocol protocol;    Content content;    Parse parse;    Configuration conf = NutchConfiguration.create();    for (int i = 0; i < sampleFiles.length; i++) {        urlString = "file:" + sampleDir + fileSeparator + sampleFiles[i];        protocol = new ProtocolFactory(conf).getProtocol(urlString);        content = protocol.getProtocolOutput(new Text(urlString), new CrawlDatum()).getContent();        parse = new ParseUtil(conf).parseByExtensionId("parse-tika", content).get(content.getUrl());                                                        ParseData theParseData = parse.getData();        Outlink[] theOutlinks = theParseData.getOutlinks();        Assert.assertTrue("There aren't 2 outlinks read!", theOutlinks.length == 2);                boolean hasLink1 = false, hasLink2 = false;        for (int j = 0; j < theOutlinks.length; j++) {            if (theOutlinks[j].getToUrl().equals("http://www-scf.usc.edu/~mattmann/")) {                hasLink1 = true;            }            if (theOutlinks[j].getToUrl().equals("http://www.nutch.org/")) {                hasLink2 = true;            }        }        if (!hasLink1 || !hasLink2) {            Assert.fail("Outlinks read from sample rss file are not correct!");        }    }}
0
protected Parse parse(byte[] contentBytes)
{    String dummyUrl = "http://example.com/";    return parser.getParse(new Content(dummyUrl, dummyUrl, contentBytes, "text/html", new Metadata(), conf)).get(dummyUrl);}
0
public void testEncodingDetection()
{    for (String[] testPage : encodingTestPages) {        String name = testPage[0];        Charset charset = Charset.forName(testPage[1]);        byte[] contentBytes = testPage[2].getBytes(charset);        Parse parse = parse(contentBytes);        String text = parse.getText();        String title = parse.getData().getTitle();        String keywords = parse.getData().getMeta("keywords");                                        Assert.assertEquals("Title not extracted properly (" + name + ")", encodingTestKeywords, title);        for (String keyword : encodingTestKeywords.split(",\\s*")) {            Assert.assertTrue(keyword + " not found in text (" + name + ")", text.contains(keyword));        }        Assert.assertNotNull("No keywords extracted", keywords);        Assert.assertEquals("Keywords not extracted properly (" + name + ")", encodingTestKeywords, keywords);    }}
1
public void testResolveBaseUrl()
{    byte[] contentBytes = resolveBaseUrlTestContent.getBytes(StandardCharsets.UTF_8);        Parse parse = parse(contentBytes);        Outlink[] outlinks = parse.getData().getOutlinks();    Assert.assertEquals(1, outlinks.length);    Assert.assertEquals("http://www.example.com/index.html", outlinks[0].getToUrl());}
1
public void testIt() throws ProtocolException, ParseException
{    String urlString;    Protocol protocol;    Content content;    Parse parse;    for (int i = 0; i < sampleFiles.length; i++) {        urlString = "file:" + sampleDir + fileSeparator + sampleFiles[i];        Configuration conf = NutchConfiguration.create();        protocol = new ProtocolFactory(conf).getProtocol(urlString);        content = protocol.getProtocolOutput(new Text(urlString), new CrawlDatum()).getContent();        parse = new ParseUtil(conf).parseByExtensionId("parse-tika", content).get(content.getUrl());        Assert.assertEquals("121", parse.getData().getMeta("width"));        Assert.assertEquals("48", parse.getData().getMeta("height"));    }}
0
public void setUp()
{    conf = NutchConfiguration.create();    conf.set("file.content.limit", "-1");}
0
public String getTextContent(String fileName) throws ProtocolException, ParseException
{    String urlString = "file:" + sampleDir + fileSeparator + fileName;    Protocol protocol = new ProtocolFactory(conf).getProtocol(urlString);    Content content = protocol.getProtocolOutput(new Text(urlString), new CrawlDatum()).getContent();    Parse parse = new ParseUtil(conf).parseByExtensionId("parse-tika", content).get(content.getUrl());    return parse.getText();}
0
public void testIt() throws ProtocolException, ParseException
{    for (int i = 0; i < sampleFiles.length; i++) {        String found = getTextContent(sampleFiles[i]);        Assert.assertTrue("text found : '" + found + "'", found.startsWith(expectedText));    }}
0
public void testOpeningDocs() throws ProtocolException, ParseException
{    String[] filenames = new File(sampleDir).list();    for (int i = 0; i < filenames.length; i++) {        if (filenames[i].endsWith(".doc") == false)            continue;        Assert.assertTrue("can't read content of " + filenames[i], getTextContent(filenames[i]).length() > 0);    }}
0
public void testIt() throws ProtocolException, ParseException
{    String urlString;    Content content;    Parse parse;    Configuration conf = NutchConfiguration.create();    Protocol protocol;    ProtocolFactory factory = new ProtocolFactory(conf);    System.out.println("Expected : " + expectedText);    for (int i = 0; i < sampleFiles.length; i++) {        urlString = "file:" + sampleDir + fileSeparator + sampleFiles[i];        if (sampleFiles[i].startsWith("ootest") == false)            continue;        protocol = factory.getProtocol(urlString);        content = protocol.getProtocolOutput(new Text(urlString), new CrawlDatum()).getContent();        parse = new ParseUtil(conf).parseByExtensionId("parse-tika", content).get(content.getUrl());        String text = parse.getText().replaceAll("[ \t\r\n]+", " ").trim();                                Assert.assertTrue(text != null && text.length() > 0);        System.out.println("Found " + sampleFiles[i] + ": " + text);    }}
0
public void testIt() throws ProtocolException, ParseException
{    String urlString;    Protocol protocol;    Content content;    Parse parse;    for (int i = 0; i < sampleFiles.length; i++) {        urlString = "file:" + sampleDir + fileSeparator + sampleFiles[i];        Configuration conf = NutchConfiguration.create();        protocol = new ProtocolFactory(conf).getProtocol(urlString);        content = protocol.getProtocolOutput(new Text(urlString), new CrawlDatum()).getContent();        parse = new ParseUtil(conf).parseByExtensionId("parse-tika", content).get(content.getUrl());        int index = parse.getText().indexOf(expectedText);        Assert.assertTrue(index > 0);    }}
0
public void testRobotsMetaProcessor()
{    Configuration conf = NutchConfiguration.create();    TikaParser parser = new TikaParser();    parser.setConf(conf);    try {        currURLsAndAnswers = new URL[][] { { new URL("http://www.nutch.org"), null }, { new URL("http://www.nutch.org"), null }, { new URL("http://www.nutch.org"), null }, { new URL("http://www.nutch.org"), null }, { new URL("http://www.nutch.org"), null }, { new URL("http://www.nutch.org"), null }, { new URL("http://www.nutch.org"), null }, { new URL("http://www.nutch.org/foo/"), new URL("http://www.nutch.org/") }, { new URL("http://www.nutch.org"), new URL("http://www.nutch.org/base/") }, { new URL("http://www.nutch.org"), null } };    } catch (Exception e) {        Assert.assertTrue("couldn't make test URLs!", false);    }    for (int i = 0; i < tests.length; i++) {        byte[] bytes = tests[i].getBytes();        HTMLDocumentImpl doc = new HTMLDocumentImpl();        doc.setErrorChecking(false);        DocumentFragment root = doc.createDocumentFragment();        String url = "http://www.nutch.org";        Content content = new Content(url, url, bytes, "text/html", new Metadata(), conf);        Parse parse = null;        try {            parse = parser.getParse(content, doc, root).get(url);        } catch (Exception e) {            e.printStackTrace();        }        HTMLMetaTags robotsMeta = new HTMLMetaTags();        HTMLMetaProcessor.getMetaTags(robotsMeta, root, currURLsAndAnswers[i][0]);        Assert.assertEquals("got noindex wrong on test " + i, answers[i][0], robotsMeta.getNoIndex());        Assert.assertEquals("got nofollow wrong on test " + i, answers[i][1], robotsMeta.getNoFollow());        Assert.assertEquals("got nocache wrong on test " + i, answers[i][2], robotsMeta.getNoCache());        Assert.assertTrue("got base href wrong on test " + i + " (got " + robotsMeta.getBaseHref() + ")", ((robotsMeta.getBaseHref() == null) && (currURLsAndAnswers[i][1] == null)) || ((robotsMeta.getBaseHref() != null) && robotsMeta.getBaseHref().equals(currURLsAndAnswers[i][1])));        if (tests[i].contains("meta-refresh redirect")) {                        URL metaRefreshUrl = robotsMeta.getRefreshHref();            Assert.assertNotNull("failed to get meta-refresh redirect", metaRefreshUrl);            Assert.assertEquals("failed to get meta-refresh redirect", "http://example.com/", metaRefreshUrl.toString());            Assert.assertEquals("failed to add meta-refresh redirect to parse status", "http://example.com/", parse.getData().getStatus().getArgs()[0]);        }    }}
0
public void testIt() throws ProtocolException, ParseException
{    String urlString;    Protocol protocol;    Content content;    Parse parse;    Configuration conf = NutchConfiguration.create();    urlString = "file:" + sampleDir + fileSeparator + rtfFile;    protocol = new ProtocolFactory(conf).getProtocol(urlString);    content = protocol.getProtocolOutput(new Text(urlString), new CrawlDatum()).getContent();    parse = new ParseUtil(conf).parseByExtensionId("parse-tika", content).get(content.getUrl());    String text = parse.getText();    Assert.assertEquals("The quick brown fox jumps over the lazy dog", text.trim());    String title = parse.getData().getTitle();    Metadata meta = parse.getData().getParseMeta();    Assert.assertEquals("test rft document", title);    Assert.assertEquals("tests", meta.get(DublinCore.SUBJECT));}
0
public ParseResult getParse(final Content content)
{    String resultText = null;    String resultTitle = null;    Outlink[] outlinks = null;    List<Outlink> outLinksList = new ArrayList<Outlink>();    try {        final String contentLen = content.getMetadata().get(Response.CONTENT_LENGTH);        final int len = Integer.parseInt(contentLen);        if (LOG.isDebugEnabled()) {                    }        final byte[] contentInBytes = content.getContent();        if (contentLen != null && contentInBytes.length != len) {            return new ParseStatus(ParseStatus.FAILED, ParseStatus.FAILED_TRUNCATED, "Content truncated at " + contentInBytes.length + " bytes. Parser can't handle incomplete zip file.").getEmptyParseResult(content.getUrl(), getConf());        }        ZipTextExtractor extractor = new ZipTextExtractor(getConf());                resultText = extractor.extractText(new ByteArrayInputStream(contentInBytes), content.getUrl(), outLinksList);    } catch (Exception e) {        return new ParseStatus(ParseStatus.FAILED, "Can't be handled as Zip document. " + e).getEmptyParseResult(content.getUrl(), getConf());    }    if (resultText == null) {        resultText = "";    }    if (resultTitle == null) {        resultTitle = "";    }    outlinks = (Outlink[]) outLinksList.toArray(new Outlink[0]);    final ParseData parseData = new ParseData(ParseStatus.STATUS_SUCCESS, resultTitle, outlinks, content.getMetadata());    if (LOG.isTraceEnabled()) {        LOG.trace("Zip file parsed sucessfully !!");    }    return ParseResult.createParseResult(content.getUrl(), new ParseImpl(resultText, parseData));}
1
public void setConf(Configuration conf)
{    this.conf = conf;}
0
public Configuration getConf()
{    return this.conf;}
0
public static void main(String[] args) throws IOException
{    if (args.length < 1) {        System.out.println("ZipParser <zip_file>");        System.exit(1);    }    File file = new File(args[0]);    String url = "file:" + file.getCanonicalPath();    FileInputStream in = new FileInputStream(file);    byte[] bytes = new byte[in.available()];    in.read(bytes);    in.close();    Configuration conf = NutchConfiguration.create();    ZipParser parser = new ZipParser();    parser.setConf(conf);    Metadata meta = new Metadata();    meta.add(Response.CONTENT_LENGTH, "" + file.length());    ParseResult parseResult = parser.getParse(new Content(url, url, bytes, "application/zip", meta, conf));    Parse p = parseResult.get(url);    System.out.println(parseResult.size());    System.out.println("Parse Text:");    System.out.println(p.getText());    System.out.println("Parse Data:");    System.out.println(p.getData());}
0
public String extractText(InputStream input, String url, List<Outlink> outLinksList) throws IOException
{    String resultText = "";    ZipInputStream zin = new ZipInputStream(input);    ZipEntry entry;    while ((entry = zin.getNextEntry()) != null) {        if (!entry.isDirectory()) {            int size = (int) entry.getSize();            byte[] b = new byte[size];            for (int x = 0; x < size; x++) {                int err = zin.read();                if (err != -1) {                    b[x] = (byte) err;                }            }            String newurl = url + "/";            String fname = entry.getName();            newurl += fname;            URL aURL = new URL(newurl);            String base = aURL.toString();            int i = fname.lastIndexOf('.');            if (i != -1) {                                Tika tika = new Tika();                String contentType = tika.detect(fname);                try {                    Metadata metadata = new Metadata();                    metadata.set(Response.CONTENT_LENGTH, Long.toString(entry.getSize()));                    metadata.set(Response.CONTENT_TYPE, contentType);                    Content content = new Content(newurl, base, b, contentType, metadata, this.conf);                    Parse parse = new ParseUtil(this.conf).parse(content).get(content.getUrl());                    ParseData theParseData = parse.getData();                    Outlink[] theOutlinks = theParseData.getOutlinks();                    for (int count = 0; count < theOutlinks.length; count++) {                        outLinksList.add(new Outlink(theOutlinks[count].getToUrl(), theOutlinks[count].getAnchor()));                    }                    resultText += entry.getName() + " " + parse.getText() + " ";                } catch (ParseException e) {                    if (LOG.isInfoEnabled()) {                                            }                }            }        }    }    return resultText;}
1
public void testIt() throws ProtocolException, ParseException
{    String urlString;    Protocol protocol;    Content content;    Parse parse;    Configuration conf = NutchConfiguration.create();    for (int i = 0; i < sampleFiles.length; i++) {        urlString = "file:" + sampleDir + fileSeparator + sampleFiles[i];        protocol = new ProtocolFactory(conf).getProtocol(urlString);        content = protocol.getProtocolOutput(new Text(urlString), new CrawlDatum()).getContent();        parse = new ParseUtil(conf).parseByExtensionId("parse-zip", content).get(content.getUrl());        Assert.assertTrue("Extracted text does not start with <" + expectedText + ">: <" + parse.getText() + ">", parse.getText().startsWith(expectedText));    }}
0
public static HashMap<String, Integer> unflattenToHashmap(String line)
{    HashMap<String, Integer> dict = new HashMap<String, Integer>();    String[] dictarray = line.split(",");    for (String field : dictarray) {        dict.put(field.split(":")[0], Integer.valueOf(field.split(":")[1]));    }    return dict;}
0
public static String classify(String line) throws IOException
{    double prob_ir = 0;    double prob_r = 0;    String result = "1";    String[] linearray = line.replaceAll("[^a-zA-Z ]", "").toLowerCase().split(" ");        if (!ismodel) {        Configuration configuration = new Configuration();        FileSystem fs = FileSystem.get(configuration);        BufferedReader bufferedReader = new BufferedReader(new InputStreamReader(fs.open(new Path("naivebayes-model"))));        uniquewords_size = Integer.valueOf(bufferedReader.readLine());        bufferedReader.readLine();        numof_ir = Integer.valueOf(bufferedReader.readLine());        numwords_ir = Integer.valueOf(bufferedReader.readLine());        wordfreq_ir = unflattenToHashmap(bufferedReader.readLine());        bufferedReader.readLine();        numof_r = Integer.valueOf(bufferedReader.readLine());        numwords_r = Integer.valueOf(bufferedReader.readLine());        wordfreq_r = unflattenToHashmap(bufferedReader.readLine());        ismodel = true;        bufferedReader.close();    }    for (String word : linearray) {        if (wordfreq_ir.containsKey(word))            prob_ir += Math.log(wordfreq_ir.get(word)) + 1 - Math.log(numwords_ir + uniquewords_size);        else            prob_ir += 1 - Math.log(numwords_ir + uniquewords_size);        if (wordfreq_r.containsKey(word))            prob_r += Math.log(wordfreq_r.get(word)) + 1 - Math.log(numwords_r + uniquewords_size);        else            prob_r += 1 - Math.log(numwords_r + uniquewords_size);    }    prob_ir += Math.log(numof_ir) - Math.log(numof_ir + numof_r);    prob_r += Math.log(numof_r) - Math.log(numof_ir + numof_r);    if (prob_ir > prob_r)        result = "0";    else        result = "1";    return result;}
0
public boolean filterParse(String text)
{    try {        return classify(text);    } catch (IOException e) {            }    return false;}
1
public boolean filterUrl(String url)
{    return containsWord(url, wordlist);}
0
public boolean classify(String text) throws IOException
{        if (Classify.classify(text).equals("1"))        return true;    return false;}
0
public void train() throws Exception
{        if (!FileSystem.get(conf).exists(new Path("naivebayes-model"))) {                Train.start(inputFilePath);    } else {            }}
1
public boolean containsWord(String url, ArrayList<String> wordlist)
{    for (String word : wordlist) {        if (url.contains(word)) {            return true;        }    }    return false;}
0
public void setConf(Configuration conf)
{    this.conf = conf;    inputFilePath = conf.get(TRAINFILE_MODELFILTER);    dictionaryFile = conf.get(DICTFILE_MODELFILTER);    if (inputFilePath == null || inputFilePath.trim().length() == 0 || dictionaryFile == null || dictionaryFile.trim().length() == 0) {        String message = "ParseFilter: NaiveBayes: trainfile or wordlist not set in the parsefilte.naivebayes.trainfile or parsefilte.naivebayes.wordlist";        if (LOG.isErrorEnabled()) {                    }        throw new IllegalArgumentException(message);    }    try {        if ((FileSystem.get(conf).exists(new Path(inputFilePath))) || (FileSystem.get(conf).exists(new Path(dictionaryFile)))) {            String message = "ParseFilter: NaiveBayes: " + inputFilePath + " or " + dictionaryFile + " not found!";            if (LOG.isErrorEnabled()) {                            }            throw new IllegalArgumentException(message);        }        BufferedReader br = null;        String CurrentLine;        Reader reader = conf.getConfResourceAsReader(dictionaryFile);        br = new BufferedReader(reader);        while ((CurrentLine = br.readLine()) != null) {            wordlist.add(CurrentLine);        }    } catch (IOException e) {            }    try {        train();    } catch (Exception e) {            }}
1
public Configuration getConf()
{    return this.conf;}
0
public ParseResult filter(Content content, ParseResult parseResult, HTMLMetaTags metaTags, DocumentFragment doc)
{    Parse parse = parseResult.get(content.getUrl());    String url = content.getBaseUrl();    ArrayList<Outlink> tempOutlinks = new ArrayList<Outlink>();    String text = parse.getText();    if (!filterParse(text)) {                                                Outlink[] out = null;        for (int i = 0; i < parse.getData().getOutlinks().length; i++) {                        if (filterUrl(parse.getData().getOutlinks()[i].getToUrl())) {                tempOutlinks.add(parse.getData().getOutlinks()[i]);                            } else {                            }        }        out = new Outlink[tempOutlinks.size()];        for (int i = 0; i < tempOutlinks.size(); i++) {            out[i] = tempOutlinks.get(i);        }        parse.getData().setOutlinks(out);    } else {            }    return parseResult;}
1
public static String replacefirstoccuranceof(String tomatch, String line)
{    int index = line.indexOf(tomatch);    if (index == -1) {        return line;    } else {        return line.substring(0, index) + line.substring(index + tomatch.length());    }}
0
public static void updateHashMap(HashMap<String, Integer> dict, String key)
{    if (!key.equals("")) {        if (dict.containsKey(key))            dict.put(key, dict.get(key) + 1);        else            dict.put(key, 1);    }}
0
public static String flattenHashMap(HashMap<String, Integer> dict)
{    String result = "";    for (String key : dict.keySet()) {        result += key + ":" + dict.get(key) + ",";    }        result = result.substring(0, result.length() - 1);    return result;}
0
public static void start(String filepath) throws IOException
{                int numof_ir = 0;    int numof_r = 0;    int numwords_ir = 0;    int numwords_r = 0;    HashSet<String> uniquewords = new HashSet<String>();    HashMap<String, Integer> wordfreq_ir = new HashMap<String, Integer>();    HashMap<String, Integer> wordfreq_r = new HashMap<String, Integer>();    String line = "";    String target = "";    String[] linearray = null;        Configuration configuration = new Configuration();    FileSystem fs = FileSystem.get(configuration);    BufferedReader bufferedReader = new BufferedReader(configuration.getConfResourceAsReader(filepath));    while ((line = bufferedReader.readLine()) != null) {        target = line.split("\t")[0];        line = replacefirstoccuranceof(target + "\t", line);        linearray = line.replaceAll("[^a-zA-Z ]", "").toLowerCase().split(" ");                if (target.equals("0")) {            numof_ir += 1;            numwords_ir += linearray.length;            for (int i = 0; i < linearray.length; i++) {                uniquewords.add(linearray[i]);                updateHashMap(wordfreq_ir, linearray[i]);            }        } else {            numof_r += 1;            numwords_r += linearray.length;            for (int i = 0; i < linearray.length; i++) {                uniquewords.add(linearray[i]);                updateHashMap(wordfreq_r, linearray[i]);            }        }    }        Path path = new Path("naivebayes-model");    Writer writer = new BufferedWriter(new OutputStreamWriter(fs.create(path, true)));    writer.write(String.valueOf(uniquewords.size()) + "\n");    writer.write("0\n");    writer.write(String.valueOf(numof_ir) + "\n");    writer.write(String.valueOf(numwords_ir) + "\n");    writer.write(flattenHashMap(wordfreq_ir) + "\n");    writer.write("1\n");    writer.write(String.valueOf(numof_r) + "\n");    writer.write(String.valueOf(numwords_r) + "\n");    writer.write(flattenHashMap(wordfreq_r) + "\n");    writer.close();    bufferedReader.close();}
0
public ParseResult filter(Content content, ParseResult parseResult, HTMLMetaTags metaTags, DocumentFragment doc)
{    Parse parse = parseResult.get(content.getUrl());    String html = new String(content.getContent());    String text = parse.getText();    for (Map.Entry<String, RegexRule> entry : rules.entrySet()) {        String field = entry.getKey();        RegexRule regexRule = entry.getValue();        String source = null;        if (regexRule.source.equalsIgnoreCase("html")) {            source = html;        }        if (regexRule.source.equalsIgnoreCase("text")) {            source = text;        }        if (source == null) {                    }        if (matches(source, regexRule.regex)) {            parse.getData().getParseMeta().set(field, "true");        } else {            parse.getData().getParseMeta().set(field, "false");        }    }    return parseResult;}
1
public void setConf(Configuration conf)
{    this.conf = conf;        String pluginName = "parsefilter-regex";    Extension[] extensions = PluginRepository.get(conf).getExtensionPoint(HtmlParseFilter.class.getName()).getExtensions();    for (int i = 0; i < extensions.length; i++) {        Extension extension = extensions[i];        if (extension.getDescriptor().getPluginId().equals(pluginName)) {            attributeFile = extension.getAttribute("file");            break;        }    }        if (attributeFile != null && attributeFile.trim().equals("")) {        attributeFile = null;    }    if (attributeFile != null) {        if (LOG.isInfoEnabled()) {                    }    } else {        if (LOG.isWarnEnabled()) {                    }    }        String file = conf.get("parsefilter.regex.file");    String stringRules = conf.get("parsefilter.regex.rules");    if (regexFile != null) {        file = regexFile;    } else if (attributeFile != null) {        file = attributeFile;    }    Reader reader = null;    if (stringRules != null) {                reader = new StringReader(stringRules);    } else {        reader = conf.getConfResourceAsReader(file);    }    try {        if (reader == null) {            reader = new FileReader(file);        }        readConfiguration(reader);    } catch (IOException e) {            }}
1
public Configuration getConf()
{    return this.conf;}
0
private boolean matches(String value, Pattern pattern)
{    if (value != null) {        Matcher matcher = pattern.matcher(value);        return matcher.find();    }    return false;}
0
private synchronized void readConfiguration(Reader configReader) throws IOException
{    if (rules.size() > 0) {        return;    }    String line;    BufferedReader reader = new BufferedReader(configReader);    while ((line = reader.readLine()) != null) {        if (StringUtils.isNotBlank(line) && !line.startsWith("#")) {            line = line.trim();            String[] parts = line.split("\\s");            if (parts.length == 3) {                String field = parts[0].trim();                String source = parts[1].trim();                String regex = parts[2].trim();                rules.put(field, new RegexRule(source, regex));            } else {                            }        }    }}
1
public void testPositiveFilter() throws Exception
{    Configuration conf = NutchConfiguration.create();    String file = SAMPLES + SEPARATOR + "regex-parsefilter.txt";    RegexParseFilter filter = new RegexParseFilter(file);    filter.setConf(conf);    String url = "http://nutch.apache.org/";    String html = "<body><html><h1>nutch</h1><p>this is the extracted text blablabla</p></body></html>";    Content content = new Content(url, url, html.getBytes("UTF-8"), "text/html", new Metadata(), conf);    Parse parse = new ParseImpl("nutch this is the extracted text blablabla", new ParseData());    ParseResult result = ParseResult.createParseResult(url, parse);    result = filter.filter(content, result, null, null);    Metadata meta = parse.getData().getParseMeta();    assertEquals("true", meta.get("first"));    assertEquals("true", meta.get("second"));}
0
public void testNegativeFilter() throws Exception
{    Configuration conf = NutchConfiguration.create();    String file = SAMPLES + SEPARATOR + "regex-parsefilter.txt";    RegexParseFilter filter = new RegexParseFilter(file);    filter.setConf(conf);    String url = "http://nutch.apache.org/";    String html = "<body><html><h2>nutch</h2><p>this is the extracted text no bla</p></body></html>";    Content content = new Content(url, url, html.getBytes("UTF-8"), "text/html", new Metadata(), conf);    Parse parse = new ParseImpl("nutch this is the extracted text bla", new ParseData());    ParseResult result = ParseResult.createParseResult(url, parse);    result = filter.filter(content, result, null, null);    Metadata meta = parse.getData().getParseMeta();    assertEquals("false", meta.get("first"));    assertEquals("false", meta.get("second"));}
0
public void setConf(Configuration conf)
{    this.conf = conf;    this.maxContentLength = conf.getInt("file.content.limit", 1024 * 1024);    this.crawlParents = conf.getBoolean("file.crawl.parent", true);    this.symlinksAsRedirects = conf.getBoolean("file.crawl.redirect_noncanonical", true);}
0
public Configuration getConf()
{    return this.conf;}
0
public void setMaxContentLength(int maxContentLength)
{    this.maxContentLength = maxContentLength;}
0
public ProtocolOutput getProtocolOutput(Text url, CrawlDatum datum)
{    String urlString = url.toString();    try {        URL u = new URL(urlString);        int redirects = 0;        while (true) {            FileResponse response;                        response = new FileResponse(u, datum, this, getConf());                        int code = response.getCode();            if (code == 200) {                                return new ProtocolOutput(response.toContent());            } else if (code == 304) {                                return new ProtocolOutput(response.toContent(), ProtocolStatus.STATUS_NOTMODIFIED);            } else if (code == 401) {                                return new ProtocolOutput(response.toContent(), new ProtocolStatus(ProtocolStatus.ACCESS_DENIED));            } else if (code == 404) {                                return new ProtocolOutput(response.toContent(), ProtocolStatus.STATUS_NOTFOUND);            } else if (code >= 300 && code < 400) {                                u = new URL(response.getHeader("Location"));                if (LOG.isTraceEnabled()) {                    LOG.trace("redirect to " + u);                }                if (symlinksAsRedirects) {                    return new ProtocolOutput(response.toContent(), new ProtocolStatus(ProtocolStatus.MOVED, u));                } else if (redirects == MAX_REDIRECTS) {                    LOG.trace("Too many redirects: {}", url);                    return new ProtocolOutput(response.toContent(), new ProtocolStatus(ProtocolStatus.REDIR_EXCEEDED, u));                }                redirects++;            } else {                                throw new FileError(code);            }        }    } catch (Exception e) {        e.printStackTrace();        return new ProtocolOutput(null, new ProtocolStatus(e));    }}
0
public static void main(String[] args) throws Exception
{    int maxContentLength = Integer.MIN_VALUE;    boolean dumpContent = false;    String urlString = null;    String usage = "Usage: File [-maxContentLength L] [-dumpContent] url";    if (args.length == 0) {        System.err.println(usage);        System.exit(-1);    }    for (int i = 0; i < args.length; i++) {        if (args[i].equals("-maxContentLength")) {            maxContentLength = Integer.parseInt(args[++i]);        } else if (args[i].equals("-dumpContent")) {            dumpContent = true;        } else if (i != args.length - 1) {            System.err.println(usage);            System.exit(-1);        } else            urlString = args[i];    }    File file = new File();    file.setConf(NutchConfiguration.create());    if (    maxContentLength != Integer.MIN_VALUE)        file.setMaxContentLength(maxContentLength);            ProtocolOutput output = file.getProtocolOutput(new Text(urlString), new CrawlDatum());    Content content = output.getContent();    System.err.println("URL: " + content.getUrl());    System.err.println("Status: " + output.getStatus());    System.err.println("Content-Type: " + content.getContentType());    System.err.println("Content-Length: " + content.getMetadata().get(Response.CONTENT_LENGTH));    System.err.println("Last-Modified: " + content.getMetadata().get(Response.LAST_MODIFIED));    String redirectLocation = content.getMetadata().get("Location");    if (redirectLocation != null) {        System.err.println("Location: " + redirectLocation);    }    if (dumpContent) {        System.out.print(new String(content.getContent()));    }    file = null;}
0
public BaseRobotRules getRobotRules(Text url, CrawlDatum datum, List<Content> robotsTxtContent)
{    return RobotRulesParser.EMPTY_RULES;}
0
public int getCode(int code)
{    return code;}
0
public int getCode()
{    return code;}
0
public String getHeader(String name)
{    return headers.get(name);}
0
public byte[] getContent()
{    return content;}
0
public Content toContent()
{    return new Content(orig, base, (content != null ? content : EMPTY_CONTENT), getHeader(Response.CONTENT_TYPE), headers, this.conf);}
0
private void getDirAsHttpResponse(java.io.File f) throws IOException
{    String path = f.toString();    if (this.file.crawlParents)        this.content = list2html(f.listFiles(), path, "/".equals(path) ? false : true);    else        this.content = list2html(f.listFiles(), path, false);        headers.set(Response.CONTENT_LENGTH, Integer.valueOf(this.content.length).toString());    headers.set(Response.CONTENT_TYPE, "text/html");    headers.set(Response.LAST_MODIFIED, HttpDateFormat.toString(f.lastModified()));            this.code = 200;}
0
private byte[] list2html(java.io.File[] list, String path, boolean includeDotDot)
{    StringBuffer x = new StringBuffer("<html><head>");    x.append("<title>Index of " + path + "</title></head>\n");    x.append("<body><h1>Index of " + path + "</h1><pre>\n");    if (includeDotDot) {        x.append("<a href='../'>../</a>\t-\t-\t-\n");    }        java.io.File f;    for (int i = 0; i < list.length; i++) {        f = list[i];        String name = f.getName();        String time = HttpDateFormat.toString(f.lastModified());        if (f.isDirectory()) {                                                            x.append("<a href='" + name + "/" + "'>" + name + "/</a>\t");            x.append(time + "\t-\n");        } else if (f.isFile()) {            x.append("<a href='" + name + "'>" + name + "</a>\t");            x.append(time + "\t" + f.length() + "\n");        } else {                }    }    x.append("</pre></body></html>\n");    return new String(x).getBytes();}
0
public void setUp()
{    conf = NutchConfiguration.create();}
0
public void testSetContentType() throws ProtocolException
{    for (String testTextFile : testTextFiles) {        setContentType(testTextFile);    }}
0
public void setContentType(String testTextFile) throws ProtocolException
{    String urlString = "file:" + sampleDir + fileSeparator + testTextFile;    Assert.assertNotNull(urlString);    Protocol protocol = new ProtocolFactory(conf).getProtocol(urlString);    ProtocolOutput output = protocol.getProtocolOutput(new Text(urlString), datum);    Assert.assertNotNull(output);    Assert.assertEquals("Status code: [" + output.getStatus().getCode() + "], not equal to: [" + ProtocolStatus.SUCCESS + "]: args: [" + output.getStatus().getArgs() + "]", ProtocolStatus.SUCCESS, output.getStatus().getCode());    Assert.assertNotNull(output.getContent());    Assert.assertNotNull(output.getContent().getContentType());    Assert.assertEquals(expectedMimeType, output.getContent().getContentType());    Assert.assertNotNull(output.getContent().getMetadata());    Assert.assertEquals(expectedMimeType, output.getContent().getMetadata().get(Response.CONTENT_TYPE));}
0
private void __initDefaults()
{    __passiveHost = null;    __passivePort = -1;    __systemName = null;}
0
private void __parsePassiveModeReply(String reply) throws MalformedServerReplyException
{    int i, index, lastIndex;    String octet1, octet2;    StringBuffer host;    reply = reply.substring(reply.indexOf('(') + 1, reply.indexOf(')')).trim();    host = new StringBuffer(24);    lastIndex = 0;    index = reply.indexOf(',');    host.append(reply.substring(lastIndex, index));    for (i = 0; i < 3; i++) {        host.append('.');        lastIndex = index + 1;        index = reply.indexOf(',', lastIndex);        host.append(reply.substring(lastIndex, index));    }    lastIndex = index + 1;    index = reply.indexOf(',', lastIndex);    octet1 = reply.substring(lastIndex, index);    octet2 = reply.substring(index + 1);        try {        index = Integer.parseInt(octet1);        lastIndex = Integer.parseInt(octet2);    } catch (NumberFormatException e) {        throw new MalformedServerReplyException("Could not parse passive host information.\nServer Reply: " + reply);    }    index <<= 8;    index |= lastIndex;    __passiveHost = host.toString();    __passivePort = index;}
0
protected Socket __openPassiveDataConnection(int command, String arg) throws IOException, FtpExceptionCanNotHaveDataConnection
{    Socket socket;    if (pasv() != FTPReply.ENTERING_PASSIVE_MODE)        throw new FtpExceptionCanNotHaveDataConnection("pasv() failed. " + getReplyString());    try {        __parsePassiveModeReply(getReplyStrings()[0]);    } catch (MalformedServerReplyException e) {        throw new FtpExceptionCanNotHaveDataConnection(e.getMessage());    }                                                                                    socket = _socketFactory_.createSocket(__passiveHost, __passivePort);    if (!FTPReply.isPositivePreliminary(sendCommand(command, arg))) {        socket.close();        return null;    }    if (__remoteVerificationEnabled && !verifyRemote(socket)) {        InetAddress host1, host2;        host1 = socket.getInetAddress();        host2 = getRemoteAddress();        socket.close();                throw new FtpExceptionCanNotHaveDataConnection("Host attempting data connection " + host1.getHostAddress() + " is not same as server " + host2.getHostAddress() + " So we intentionally close it for security precaution.");    }    if (__dataTimeout >= 0)        socket.setSoTimeout(__dataTimeout);    return socket;}
0
public void setDataTimeout(int timeout)
{    __dataTimeout = timeout;}
0
public void disconnect() throws IOException
{    __initDefaults();    super.disconnect();}
0
public void setRemoteVerificationEnabled(boolean enable)
{    __remoteVerificationEnabled = enable;}
0
public boolean isRemoteVerificationEnabled()
{    return __remoteVerificationEnabled;}
0
public boolean login(String username, String password) throws IOException
{    user(username);    if (FTPReply.isPositiveCompletion(getReplyCode()))        return true;        if (!FTPReply.isPositiveIntermediate(getReplyCode()))        return false;    return FTPReply.isPositiveCompletion(pass(password));}
0
public boolean logout() throws IOException
{    return FTPReply.isPositiveCompletion(quit());}
0
public void retrieveList(String path, List<FTPFile> entries, int limit, FTPFileEntryParser parser) throws IOException, FtpExceptionCanNotHaveDataConnection, FtpExceptionUnknownForcedDataClose, FtpExceptionControlClosedByForcedDataClose
{    Socket socket = __openPassiveDataConnection(FTPCommand.LIST, path);    if (socket == null)        throw new FtpExceptionCanNotHaveDataConnection("LIST " + ((path == null) ? "" : path));    BufferedReader reader = new BufferedReader(new InputStreamReader(socket.getInputStream()));                int count = 0;    String line = parser.readNextEntry(reader);    while (line != null) {        FTPFile ftpFile = parser.parseFTPEntry(line);                if (ftpFile == null) {            line = parser.readNextEntry(reader);            continue;        }        entries.add(ftpFile);        count += line.length();                if (limit >= 0 && count > limit) {                        break;        }        line = parser.readNextEntry(reader);    }                socket.close();    try {        int reply = getReply();        if (!_notBadReply(reply))            throw new FtpExceptionUnknownForcedDataClose(getReplyString());    } catch (FTPConnectionClosedException e) {                throw new FtpExceptionControlClosedByForcedDataClose(e.getMessage());    }}
0
public void retrieveFile(String path, OutputStream os, int limit) throws IOException, FtpExceptionCanNotHaveDataConnection, FtpExceptionUnknownForcedDataClose, FtpExceptionControlClosedByForcedDataClose
{    Socket socket = __openPassiveDataConnection(FTPCommand.RETR, path);    if (socket == null)        throw new FtpExceptionCanNotHaveDataConnection("RETR " + ((path == null) ? "" : path));    InputStream input = socket.getInputStream();                                int len;    int count = 0;    byte[] buf = new byte[org.apache.commons.net.io.Util.DEFAULT_COPY_BUFFER_SIZE];    while ((len = input.read(buf, 0, buf.length)) != -1) {        count += len;                if (limit >= 0 && count > limit) {            os.write(buf, 0, len - (count - limit));                        break;        }        os.write(buf, 0, len);        os.flush();    }                socket.close();    try {        int reply = getReply();        if (!_notBadReply(reply))            throw new FtpExceptionUnknownForcedDataClose(getReplyString());    } catch (FTPConnectionClosedException e) {                throw new FtpExceptionControlClosedByForcedDataClose(e.getMessage());    }}
0
private boolean _notBadReply(int reply)
{    if (FTPReply.isPositiveCompletion(reply)) {        } else if (reply == 426) {                        } else if (reply == 450) {                        } else if (reply == 451) {                        } else {                return false;    }    return true;}
0
public boolean setFileType(int fileType) throws IOException
{    if (FTPReply.isPositiveCompletion(type(fileType))) {        /*       * __fileType = fileType; __fileFormat = FTP.NON_PRINT_TEXT_FORMAT;       */        return true;    }    return false;}
0
public String getSystemName() throws IOException, FtpExceptionBadSystResponse
{        if (__systemName == null && FTPReply.isPositiveCompletion(syst())) {        __systemName = (getReplyStrings()[0]).substring(4);    } else {        throw new FtpExceptionBadSystResponse("Bad response of SYST: " + getReplyString());    }    return __systemName;}
0
public boolean sendNoOp() throws IOException
{    return FTPReply.isPositiveCompletion(noop());}
0
public void setTimeout(int to)
{    timeout = to;}
0
public void setMaxContentLength(int length)
{    maxContentLength = length;}
0
public void setFollowTalk(boolean followTalk)
{    this.followTalk = followTalk;}
0
public void setKeepConnection(boolean keepConnection)
{    this.keepConnection = keepConnection;}
0
public ProtocolOutput getProtocolOutput(Text url, CrawlDatum datum)
{    String urlString = url.toString();    try {        URL u = new URL(urlString);        int redirects = 0;        while (true) {            FtpResponse response;                        response = new FtpResponse(u, datum, this, getConf());            int code = response.getCode();            datum.getMetaData().put(Nutch.PROTOCOL_STATUS_CODE_KEY, new Text(Integer.toString(code)));            if (code == 200) {                                return new ProtocolOutput(response.toContent());            } else if (code >= 300 && code < 400) {                                if (redirects == MAX_REDIRECTS)                    throw new FtpException("Too many redirects: " + url);                String loc = response.getHeader("Location");                try {                    u = new URL(u, loc);                } catch (MalformedURLException mue) {                                        return new ProtocolOutput(null, new ProtocolStatus(mue));                }                redirects++;                if (LOG.isTraceEnabled()) {                    LOG.trace("redirect to " + u);                }            } else {                                throw new FtpError(code);            }        }    } catch (Exception e) {                return new ProtocolOutput(null, new ProtocolStatus(e));    }}
1
protected void finalize()
{    try {        if (this.client != null && this.client.isConnected()) {            this.client.logout();            this.client.disconnect();        }    } catch (IOException e) {        }}
0
public static void main(String[] args) throws Exception
{    int timeout = Integer.MIN_VALUE;    int maxContentLength = Integer.MIN_VALUE;    @SuppressWarnings("unused")    String logLevel = "info";    boolean followTalk = false;    boolean keepConnection = false;    boolean dumpContent = false;    String urlString = null;    String usage = "Usage: Ftp [-logLevel level] [-followTalk] [-keepConnection] [-timeout N] [-maxContentLength L] [-dumpContent] url";    if (args.length == 0) {        System.err.println(usage);        System.exit(-1);    }    for (int i = 0; i < args.length; i++) {        if (args[i].equals("-logLevel")) {            logLevel = args[++i];        } else if (args[i].equals("-followTalk")) {            followTalk = true;        } else if (args[i].equals("-keepConnection")) {            keepConnection = true;        } else if (args[i].equals("-timeout")) {            timeout = Integer.parseInt(args[++i]) * 1000;        } else if (args[i].equals("-maxContentLength")) {            maxContentLength = Integer.parseInt(args[++i]);        } else if (args[i].equals("-dumpContent")) {            dumpContent = true;        } else if (i != args.length - 1) {            System.err.println(usage);            System.exit(-1);        } else {            urlString = args[i];        }    }    Ftp ftp = new Ftp();    ftp.setFollowTalk(followTalk);    ftp.setKeepConnection(keepConnection);    if (    timeout != Integer.MIN_VALUE)        ftp.setTimeout(timeout);    if (    maxContentLength != Integer.MIN_VALUE)        ftp.setMaxContentLength(maxContentLength);            Content content = ftp.getProtocolOutput(new Text(urlString), new CrawlDatum()).getContent();    System.err.println("Content-Type: " + content.getContentType());    System.err.println("Content-Length: " + content.getMetadata().get(Response.CONTENT_LENGTH));    System.err.println("Last-Modified: " + content.getMetadata().get(Response.LAST_MODIFIED));    if (dumpContent) {        System.out.print(new String(content.getContent()));    }    ftp = null;}
0
public void setConf(Configuration conf)
{    this.conf = conf;    this.maxContentLength = conf.getInt("ftp.content.limit", 1024 * 1024);    this.timeout = conf.getInt("ftp.timeout", 10000);    this.userName = conf.get("ftp.username", "anonymous");    this.passWord = conf.get("ftp.password", "anonymous@example.com");    this.serverTimeout = conf.getInt("ftp.server.timeout", 60 * 1000);    this.keepConnection = conf.getBoolean("ftp.keep.connection", false);    this.followTalk = conf.getBoolean("ftp.follow.talk", false);    this.robots.setConf(conf);}
0
public Configuration getConf()
{    return this.conf;}
0
public BaseRobotRules getRobotRules(Text url, CrawlDatum datum, List<Content> robotsTxtContent)
{    return robots.getRobotRulesSet(this, url, robotsTxtContent);}
0
public int getBufferSize()
{    return BUFFER_SIZE;}
0
public int getCode(int code)
{    return code;}
0
public int getCode()
{    return code;}
0
public String getHeader(String name)
{    return headers.get(name);}
0
public byte[] getContent()
{    return content;}
0
public Content toContent()
{    return new Content(orig, base, (content != null ? content : EMPTY_CONTENT), getHeader(Response.CONTENT_TYPE), headers, this.conf);}
0
private byte[] list2html(List<FTPFile> list, String path, boolean includeDotDot)
{            StringBuffer x = new StringBuffer("<html><head>");    x.append("<title>Index of " + path + "</title></head>\n");    x.append("<body><h1>Index of " + path + "</h1><pre>\n");    if (includeDotDot) {        x.append("<a href='../'>../</a>\t-\t-\t-\n");    }    for (int i = 0; i < list.size(); i++) {        FTPFile f = (FTPFile) list.get(i);        String name = f.getName();        String time = HttpDateFormat.toString(f.getTimestamp());        if (f.isDirectory()) {                        if (name.equals(".") || name.equals(".."))                continue;            x.append("<a href='" + name + "/" + "'>" + name + "/</a>\t");            x.append(time + "\t-\n");        } else if (f.isFile()) {            x.append("<a href='" + name + "'>" + name + "</a>\t");            x.append(time + "\t" + f.getSize() + "\n");        } else {                        }    }    x.append("</pre></body></html>\n");    return new String(x).getBytes();}
0
public BaseRobotRules getRobotRulesSet(Protocol ftp, URL url, List<Content> robotsTxtContent)
{        String protocol = url.getProtocol().toLowerCase();            String host = url.getHost().toLowerCase();    if (LOG.isTraceEnabled() && isWhiteListed(url)) {        LOG.trace("Ignoring robots.txt (host is whitelisted) for URL: {}", url);    }    BaseRobotRules robotRules = CACHE.get(protocol + ":" + host);    if (robotRules != null) {                return robotRules;    } else if (LOG.isTraceEnabled()) {        LOG.trace("cache miss " + url);    }    boolean cacheRule = true;    if (isWhiteListed(url)) {                        robotRules = EMPTY_RULES;                    } else {        try {            Text robotsUrl = new Text(new URL(url, "/robots.txt").toString());            ProtocolOutput output = ((Ftp) ftp).getProtocolOutput(robotsUrl, new CrawlDatum());            ProtocolStatus status = output.getStatus();            if (robotsTxtContent != null) {                robotsTxtContent.add(output.getContent());            }            if (status.getCode() == ProtocolStatus.SUCCESS) {                robotRules = parseRules(url.toString(), output.getContent().getContent(), CONTENT_TYPE, agentNames);            } else {                                robotRules = EMPTY_RULES;            }        } catch (Throwable t) {            if (LOG.isInfoEnabled()) {                            }                        cacheRule = false;            robotRules = EMPTY_RULES;        }    }    if (cacheRule)                CACHE.put(protocol + ":" + host, robotRules);    return robotRules;}
1
public void setConf(Configuration conf)
{    super.setConf(conf);}
0
public static void main(String[] args) throws Exception
{    Http http = new Http();    http.setConf(NutchConfiguration.create());    main(http, args);}
0
protected Response getResponse(URL url, CrawlDatum datum, boolean redirect) throws ProtocolException, IOException
{    return new HttpResponse(this, url, datum);}
0
public URL getUrl()
{    return url;}
0
public int getCode()
{    return code;}
0
public String getHeader(String name)
{    return headers.get(name);}
0
public Metadata getHeaders()
{    return headers;}
0
public byte[] getContent()
{    return content;}
0
private void readContentFromHtmlUnit(URL url) throws IOException
{    String page = HtmlUnitWebDriver.getHtmlPage(url.toString(), conf);    content = page.getBytes("UTF-8");}
0
private void readPlainContent(InputStream in) throws HttpException, IOException
{        int contentLength = Integer.MAX_VALUE;    String contentLengthString = headers.get(Response.CONTENT_LENGTH);    if (contentLengthString != null) {        contentLengthString = contentLengthString.trim();        try {            if (!contentLengthString.isEmpty())                contentLength = Integer.parseInt(contentLengthString);        } catch (NumberFormatException e) {            throw new HttpException("bad content length: " + contentLengthString);        }    }    if (http.getMaxContent() >= 0 && contentLength > http.getMaxContent())                        contentLength = http.getMaxContent();    ByteArrayOutputStream out = new ByteArrayOutputStream(Http.BUFFER_SIZE);    byte[] bytes = new byte[Http.BUFFER_SIZE];    int length = 0;        if (contentLength == 0) {        content = new byte[0];        return;    }        int i = in.read(bytes);    while (i != -1) {        out.write(bytes, 0, i);        length += i;        if (length >= contentLength) {            break;        }        if ((length + Http.BUFFER_SIZE) > contentLength) {                                    i = in.read(bytes, 0, (contentLength - length));        } else {            i = in.read(bytes);        }    }    content = out.toByteArray();}
0
private void readChunkedContent(PushbackInputStream in, StringBuffer line) throws HttpException, IOException
{    boolean doneChunks = false;    int contentBytesRead = 0;    byte[] bytes = new byte[Http.BUFFER_SIZE];    ByteArrayOutputStream out = new ByteArrayOutputStream(Http.BUFFER_SIZE);    while (!doneChunks) {        if (Http.LOG.isTraceEnabled()) {            Http.LOG.trace("Http: starting chunk");        }        readLine(in, line, false);        String chunkLenStr;                        int pos = line.indexOf(";");        if (pos < 0) {            chunkLenStr = line.toString();        } else {            chunkLenStr = line.substring(0, pos);                        }        chunkLenStr = chunkLenStr.trim();        int chunkLen;        try {            chunkLen = Integer.parseInt(chunkLenStr, 16);        } catch (NumberFormatException e) {            throw new HttpException("bad chunk length: " + line.toString());        }        if (chunkLen == 0) {            doneChunks = true;            break;        }        if (http.getMaxContent() >= 0 && (contentBytesRead + chunkLen) > http.getMaxContent())            chunkLen = http.getMaxContent() - contentBytesRead;                int chunkBytesRead = 0;        while (chunkBytesRead < chunkLen) {            int toRead = (chunkLen - chunkBytesRead) < Http.BUFFER_SIZE ? (chunkLen - chunkBytesRead) : Http.BUFFER_SIZE;            int len = in.read(bytes, 0, toRead);            if (len == -1)                throw new HttpException("chunk eof after " + contentBytesRead + " bytes in successful chunks" + " and " + chunkBytesRead + " in current chunk");                                                            out.write(bytes, 0, len);            chunkBytesRead += len;        }        readLine(in, line, false);    }    if (!doneChunks) {        if (contentBytesRead != http.getMaxContent())            throw new HttpException("chunk eof: !doneChunk && didn't max out");        return;    }    content = out.toByteArray();    parseHeaders(in, line, null);}
0
private int parseStatusLine(PushbackInputStream in, StringBuffer line) throws IOException, HttpException
{    readLine(in, line, false);    int codeStart = line.indexOf(" ");    int codeEnd = line.indexOf(" ", codeStart + 1);        if (codeEnd == -1)        codeEnd = line.length();    int code;    try {        code = Integer.parseInt(line.substring(codeStart + 1, codeEnd));    } catch (NumberFormatException e) {        throw new HttpException("bad status line '" + line + "': " + e.getMessage(), e);    }    return code;}
0
private void processHeaderLine(StringBuffer line) throws IOException, HttpException
{        int colonIndex = line.indexOf(":");    if (colonIndex == -1) {        int i;        for (i = 0; i < line.length(); i++) if (!Character.isWhitespace(line.charAt(i)))            break;        if (i == line.length())            return;        throw new HttpException("No colon in header:" + line);    }    String key = line.substring(0, colonIndex);        int valueStart = colonIndex + 1;    while (valueStart < line.length()) {        int c = line.charAt(valueStart);        if (c != ' ' && c != '\t')            break;        valueStart++;    }    String value = line.substring(valueStart);    headers.set(key, value);}
0
private static int readLine(PushbackInputStream in, StringBuffer line, boolean allowContinuedLine) throws IOException
{    line.setLength(0);    for (int c = in.read(); c != -1; c = in.read()) {        switch(c) {            case '\r':                if (peek(in) == '\n') {                    in.read();                }            case '\n':                if (line.length() > 0) {                                        if (allowContinuedLine)                        switch(peek(in)) {                            case ' ':                            case                             '\t':                                in.read();                                continue;                        }                }                                return line.length();            default:                line.append((char) c);        }    }    throw new EOFException();}
0
private static int peek(PushbackInputStream in) throws IOException
{    int value = in.read();    in.unread(value);    return value;}
0
public boolean isClientTrusted(X509Certificate[] certificates)
{    return true;}
0
public boolean isServerTrusted(X509Certificate[] certificates)
{    return true;}
0
public X509Certificate[] getAcceptedIssuers()
{    return this.standardTrustManager.getAcceptedIssuers();}
0
public void checkClientTrusted(X509Certificate[] arg0, String arg1) throws CertificateException
{}
0
public void checkServerTrusted(X509Certificate[] arg0, String arg1) throws CertificateException
{}
0
public void setConf(Configuration conf)
{    super.setConf(conf);}
0
public static void main(String[] args) throws Exception
{    Http http = new Http();    http.setConf(NutchConfiguration.create());    main(http, args);}
0
protected Response getResponse(URL url, CrawlDatum datum, boolean redirect) throws ProtocolException, IOException
{    return new HttpResponse(this, url, datum);}
0
public URL getUrl()
{    return url;}
0
public int getCode()
{    return code;}
0
public String getHeader(String name)
{    return headers.get(name);}
0
public Metadata getHeaders()
{    return headers;}
0
public byte[] getContent()
{    return content;}
0
private SSLSocket getSSLSocket(Socket socket, String sockHost, int sockPort) throws Exception
{    SSLSocketFactory factory;    if (http.isTlsCheckCertificates()) {        factory = (SSLSocketFactory) SSLSocketFactory.getDefault();    } else {        SSLContext sslContext = SSLContext.getInstance("TLS");        sslContext.init(null, new TrustManager[] { new DummyX509TrustManager(null) }, null);        factory = sslContext.getSocketFactory();    }    SSLSocket sslsocket = (SSLSocket) factory.createSocket(socket, sockHost, sockPort, true);    sslsocket.setUseClientMode(true);        Set<String> protocols = new HashSet<String>(Arrays.asList(sslsocket.getSupportedProtocols()));    Set<String> ciphers = new HashSet<String>(Arrays.asList(sslsocket.getSupportedCipherSuites()));        protocols.retainAll(http.getTlsPreferredProtocols());    ciphers.retainAll(http.getTlsPreferredCipherSuites());    sslsocket.setEnabledProtocols(protocols.toArray(new String[protocols.size()]));    sslsocket.setEnabledCipherSuites(ciphers.toArray(new String[ciphers.size()]));    return sslsocket;}
0
private void readChunkedContent(PushbackInputStream in, StringBuffer line) throws HttpException, IOException
{    boolean doneChunks = false;    int contentBytesRead = 0;    byte[] bytes = new byte[Http.BUFFER_SIZE];    ByteArrayOutputStream out = new ByteArrayOutputStream(Http.BUFFER_SIZE);    while (true) {        if (Http.LOG.isTraceEnabled()) {            Http.LOG.trace("Http: starting chunk");        }        readLine(in, line, false);        String chunkLenStr;                        int pos = line.indexOf(";");        if (pos < 0) {            chunkLenStr = line.toString();        } else {            chunkLenStr = line.substring(0, pos);                        }        chunkLenStr = chunkLenStr.trim();        int chunkLen;        try {            chunkLen = Integer.parseInt(chunkLenStr, 16);        } catch (NumberFormatException e) {            throw new HttpException("bad chunk length: " + line.toString());        }        if (chunkLen == 0) {            doneChunks = true;            break;        }        if (http.getMaxContent() >= 0 && (contentBytesRead + chunkLen) > http.getMaxContent()) {                        chunkLen = http.getMaxContent() - contentBytesRead;        }                int chunkBytesRead = 0;        while (chunkBytesRead < chunkLen) {            int toRead = (chunkLen - chunkBytesRead) < Http.BUFFER_SIZE ? (chunkLen - chunkBytesRead) : Http.BUFFER_SIZE;            int len = in.read(bytes, 0, toRead);            if (len == -1)                throw new HttpException("chunk eof after " + contentBytesRead + " bytes in successful chunks" + " and " + chunkBytesRead + " in current chunk");                                                            out.write(bytes, 0, len);            chunkBytesRead += len;        }        contentBytesRead += chunkBytesRead;        if (http.getMaxContent() >= 0 && contentBytesRead >= http.getMaxContent()) {            Http.LOG.trace("Http: content limit reached");            break;        }        readLine(in, line, false);    }    content = out.toByteArray();    if (!doneChunks) {                if (contentBytesRead != http.getMaxContent())            throw new HttpException("chunk eof: !doneChunk && didn't max out");        return;    }        parseHeaders(in, line, null);}
0
private int parseStatusLine(PushbackInputStream in, StringBuffer line, StringBuffer lineSeparator) throws IOException, HttpException
{    readLine(in, line, false, 2048, lineSeparator);    int codeStart = line.indexOf(" ");    int codeEnd;    int lineLength = line.length();        for (codeEnd = codeStart + 1; codeEnd < lineLength; codeEnd++) {        if (!Character.isDigit(line.charAt(codeEnd)))            break;            }    try {        return Integer.parseInt(line.substring(codeStart + 1, codeEnd));    } catch (NumberFormatException e) {        throw new HttpException("Bad status line, no HTTP response code: " + line, e);    }}
0
private static int readLine(PushbackInputStream in, StringBuffer line, boolean allowContinuedLine) throws IOException
{    return readLine(in, line, allowContinuedLine, Http.BUFFER_SIZE, null);}
0
private static int readLine(PushbackInputStream in, StringBuffer line, boolean allowContinuedLine, int maxBytes, StringBuffer lineSeparator) throws IOException
{    line.setLength(0);    int bytesRead = 0;    for (int c = in.read(); c != -1 && bytesRead < maxBytes; c = in.read(), bytesRead++) {        switch(c) {            case '\r':                if (lineSeparator != null) {                    lineSeparator.append((char) c);                }                if (peek(in) == '\n') {                    in.read();                    if (lineSeparator != null) {                        lineSeparator.append((char) c);                    }                }                        case '\n':                if (lineSeparator != null) {                    lineSeparator.append((char) c);                }                if (line.length() > 0) {                                        if (allowContinuedLine)                        switch(peek(in)) {                            case ' ':                            case                             '\t':                                in.read();                                if (lineSeparator != null) {                                    lineSeparator.replace(0, lineSeparator.length(), "");                                }                                continue;                        }                }                                return line.length();            default:                line.append((char) c);        }    }    if (bytesRead >= maxBytes) {        throw new IOException("Line exceeds max. buffer size: " + line.substring(0, Math.min(32, line.length())));    }    return line.length();}
0
private static int peek(PushbackInputStream in) throws IOException
{    int value = in.read();    in.unread(value);    return value;}
0
public void setUp() throws Exception
{    conf = new Configuration();    conf.addResource("nutch-default.xml");    conf.addResource("nutch-site-test.xml");    conf.setBoolean("store.http.headers", true);    http = new Http();    http.setConf(conf);}
0
public void tearDown() throws Exception
{    server.close();}
0
private void runServer(int port, String response) throws Exception
{    server = new ServerSocket();    server.bind(new InetSocketAddress("127.0.0.1", port));    Pattern requestPattern = Pattern.compile("(?i)^GET\\s+(\\S+)");    while (true) {                Socket socket = server.accept();                try (BufferedReader in = new BufferedReader(new InputStreamReader(socket.getInputStream(), StandardCharsets.UTF_8));            PrintWriter out = new PrintWriter(new OutputStreamWriter(socket.getOutputStream(), StandardCharsets.UTF_8), true)) {            String line;            while ((line = in.readLine()) != null) {                                if (line.trim().isEmpty()) {                    break;                }                Matcher m = requestPattern.matcher(line);                if (m.find()) {                                        if (!m.group(1).startsWith("/")) {                        response = "HTTP/1.1 400 Bad request\r\n\r\n";                    }                }            }                        out.print(response);        } catch (Exception e) {                    }    }}
1
private void launchServer(String response) throws InterruptedException
{    Thread serverThread = new Thread(() -> {        try {            runServer(port, response);        } catch (Exception e) {                    }    });    serverThread.start();    Thread.sleep(50);}
1
private Response fetchPage(String page, int expectedCode) throws Exception
{    URL url = new URL("http", "127.0.0.1", port, page);        CrawlDatum crawlDatum = new CrawlDatum();    Response response = http.getResponse(url, crawlDatum, true);    assertEquals("HTTP Status Code for " + url, expectedCode, response.getCode());    return response;}
1
public void testBadHttpServer() throws Exception
{    setUp();            launchServer(responseHeader + simpleContent);    fetchPage("/", 200);}
0
public void testRequestNotStartingWithSlash() throws Exception
{    setUp();    launchServer(responseHeader + simpleContent);    fetchPage("?171", 200);}
0
public void testContentLengthNotANumber() throws Exception
{    setUp();    launchServer(responseHeader + "Content-Length: thousand\r\n" + simpleContent);    fetchPage("/", 200);}
0
public void testHeaderWithColon() throws Exception
{    setUp();    launchServer("HTTP/1.1 200: OK\r\n" + simpleContent);    fetchPage("/", 200);}
0
public void testHeaderSpellChecking() throws Exception
{    setUp();    launchServer(responseHeader + "Client-Transfer-Encoding: chunked\r\n" + simpleContent);    fetchPage("/", 200);}
0
public void testIgnoreErrorInRedirectPayload() throws Exception
{    setUp();    launchServer("HTTP/1.1 302 Found\r\nLocation: http://example.com/\r\n" + "Transfer-Encoding: chunked\r\n\r\nNot a valid chunk.");    Response fetched = fetchPage("/", 302);    assertNotNull("No redirect Location.", fetched.getHeader("Location"));    assertEquals("Wrong redirect Location.", "http://example.com/", fetched.getHeader("Location"));}
0
public void testNoStatusLine() throws Exception
{    setUp();    String text = "This is a text containing non-ASCII characters: \u00e4\u00f6\u00fc\u00df";    launchServer(text);    Response fetched = fetchPage("/", 200);    assertEquals("Wrong text returned for response with no status line.", text, new String(fetched.getContent(), StandardCharsets.UTF_8));    server.close();    text = "<!DOCTYPE html>\n<html>\n<head>\n" + "<title>Testing no HTTP header èéâ</title>\n" + "<meta charset=\"utf-8\">\n" + "</head>\n<body>This is a text containing non-ASCII characters:" + "\u00e4\u00f6\u00fc\u00df</body>\n</html";    launchServer(text);    fetched = fetchPage("/", 200);    assertEquals("Wrong text returned for response with no status line.", text, new String(fetched.getContent(), StandardCharsets.UTF_8));}
0
public void testMultiLineHeader() throws Exception
{    setUp();    launchServer(responseHeader + "Set-Cookie: UserID=JohnDoe;\r\n  Max-Age=3600;\r\n  Version=1\r\n" + simpleContent);    Response fetched = fetchPage("/", 200);        assertNotNull("Failed to set multi-line \"Set-Cookie\" header.", fetched.getHeader("Set-Cookie"));    assertTrue("Failed to set multi-line \"Set-Cookie\" header.", fetched.getHeader("Set-Cookie").contains("Version=1"));}
1
public void testOverlongHeader() throws Exception
{    setUp();    StringBuilder response = new StringBuilder();    response.append(responseHeader);    for (int i = 0; i < 80; i++) {        response.append("X-Custom-Header-");        for (int j = 0; j < 10000; j++) {            response.append('x');        }        response.append(": hello\r\n");    }    response.append("\r\n" + simpleContent);    launchServer(response.toString());        fetchPage("/", 200);}
0
public void testChunkedContent() throws Exception
{    setUp();    StringBuilder response = new StringBuilder();    response.append(responseHeader);    response.append("Content-Type: text/html\r\n");    response.append("Transfer-Encoding: chunked\r\n");        for (int i = 0; i < 80; i++) {        response.append(String.format("\r\n400\r\n%02x\r\n", i));        for (int j = 0; j < 1012; j++) {            response.append('x');        }        response.append(String.format("\r\n%02x\r\n", i));        response.append("\r\n");    }    response.append("\r\n0\r\n\r\n");    launchServer(response.toString());    Response fetched = fetchPage("/", 200);    assertEquals("Chunked content not truncated according to http.content.limit", 65536, fetched.getContent().length);}
0
public void setUp(boolean redirection) throws Exception
{    conf = new Configuration();    conf.addResource("nutch-default.xml");    conf.addResource("nutch-site-test.xml");    http = new Http();    http.setConf(conf);    server = new Server();    if (redirection) {        root = new Context(server, "/redirection", Context.SESSIONS);        root.setAttribute("newContextURL", "/redirect");    } else {        root = new Context(server, "/", Context.SESSIONS);    }    ServletHolder sh = new ServletHolder(org.apache.jasper.servlet.JspServlet.class);    root.addServlet(sh, "*.jsp");    root.setResourceBase(RES_DIR);}
0
public void tearDown() throws Exception
{    server.stop();}
0
public void testStatusCode() throws Exception
{    startServer(47504, false);    fetchPage("/basic-http.jsp", 200);    fetchPage("/redirect301.jsp", 301);    fetchPage("/redirect302.jsp", 302);    fetchPage("/nonexists.html", 404);    fetchPage("/brokenpage.jsp", 500);}
0
public void testRedirectionJetty() throws Exception
{        startServer(47503, true);    fetchPage("/redirection", 302);}
0
private void startServer(int portno, boolean redirection) throws Exception
{    port = portno;    setUp(redirection);    SelectChannelConnector connector = new SelectChannelConnector();    connector.setHost("127.0.0.1");    connector.setPort(port);    server.addConnector(connector);    server.start();}
0
private void fetchPage(String page, int expectedCode) throws Exception
{    URL url = new URL("http", "127.0.0.1", port, page);    CrawlDatum crawlDatum = new CrawlDatum();    Response response = http.getResponse(url, crawlDatum, true);    ProtocolOutput out = http.getProtocolOutput(new Text(url.toString()), crawlDatum);    Content content = out.getContent();    assertEquals("HTTP Status Code for " + url, expectedCode, response.getCode());    if (page.compareTo("/nonexists.html") != 0 && page.compareTo("/brokenpage.jsp") != 0 && page.compareTo("/redirection") != 0) {        assertEquals("ContentType " + url, "text/html", content.getContentType());    }}
0
private static SSLContext createEasySSLContext()
{    try {        SSLContext context = SSLContext.getInstance("SSL");        context.init(null, new TrustManager[] { new DummyX509TrustManager(null) }, null);        return context;    } catch (Exception e) {        if (LOG.isErrorEnabled()) {                    }        throw new HttpClientError(e.toString());    }}
1
private SSLContext getSSLContext()
{    if (this.sslcontext == null) {        this.sslcontext = createEasySSLContext();    }    return this.sslcontext;}
0
public Socket createSocket(String host, int port, InetAddress clientHost, int clientPort) throws IOException, UnknownHostException
{    return getSSLContext().getSocketFactory().createSocket(host, port, clientHost, clientPort);}
0
public Socket createSocket(final String host, final int port, final InetAddress localAddress, final int localPort, final HttpConnectionParams params) throws IOException, UnknownHostException, ConnectTimeoutException
{    if (params == null) {        throw new IllegalArgumentException("Parameters may not be null");    }    int timeout = params.getConnectionTimeout();    if (timeout == 0) {        return createSocket(host, port, localAddress, localPort);    } else {                return ControllerThreadSocketFactory.createSocket(this, host, port, localAddress, localPort, timeout);    }}
0
public Socket createSocket(String host, int port) throws IOException, UnknownHostException
{    return getSSLContext().getSocketFactory().createSocket(host, port);}
0
public Socket createSocket(Socket socket, String host, int port, boolean autoClose) throws IOException, UnknownHostException
{    return getSSLContext().getSocketFactory().createSocket(socket, host, port, autoClose);}
0
public boolean equals(Object obj)
{    return ((obj != null) && obj.getClass().equals(DummySSLProtocolSocketFactory.class));}
0
public int hashCode()
{    return DummySSLProtocolSocketFactory.class.hashCode();}
0
public boolean isClientTrusted(X509Certificate[] certificates)
{    return true;}
0
public boolean isServerTrusted(X509Certificate[] certificates)
{    return true;}
0
public X509Certificate[] getAcceptedIssuers()
{    return this.standardTrustManager.getAcceptedIssuers();}
0
public void checkClientTrusted(X509Certificate[] arg0, String arg1) throws CertificateException
{}
0
public void checkServerTrusted(X509Certificate[] arg0, String arg1) throws CertificateException
{}
0
 static synchronized HttpClient getClient()
{    return client;}
0
public void setConf(Configuration conf)
{    super.setConf(conf);    Http.conf = conf;    this.maxThreadsTotal = conf.getInt("fetcher.threads.fetch", 10);    this.proxyUsername = conf.get("http.proxy.username", "");    this.proxyPassword = conf.get("http.proxy.password", "");    this.proxyRealm = conf.get("http.proxy.realm", "");    agentHost = conf.get("http.agent.host", "");    authFile = conf.get("http.auth.file", "");    configureClient();    try {        setCredentials();    } catch (Exception ex) {        if (LOG.isErrorEnabled()) {                                }    }}
1
public static void main(String[] args) throws Exception
{    Http http = new Http();    http.setConf(NutchConfiguration.create());    main(http, args);}
0
protected Response getResponse(URL url, CrawlDatum datum, boolean redirect) throws ProtocolException, IOException
{    resolveCredentials(url);    return new HttpResponse(this, url, datum, redirect);}
0
private void configureClient()
{        ProtocolSocketFactory factory;    if (tlsCheckCertificate) {        factory = new SSLProtocolSocketFactory();    } else {        factory = new DummySSLProtocolSocketFactory();    }    Protocol https = new Protocol("https", factory, 443);    Protocol.registerProtocol("https", https);    HttpConnectionManagerParams params = connectionManager.getParams();    params.setConnectionTimeout(timeout);    params.setSoTimeout(timeout);    params.setSendBufferSize(BUFFER_SIZE);    params.setReceiveBufferSize(BUFFER_SIZE);                    params.setMaxTotalConnections(conf.getInt("mapreduce.tasktracker.map.tasks.maximum", 5) * conf.getInt("fetcher.threads.fetch", maxThreadsTotal));                params.setDefaultMaxConnectionsPerHost(conf.getInt("fetcher.threads.fetch", maxThreadsTotal));                client.getParams().setConnectionManagerTimeout(timeout);    HostConfiguration hostConf = client.getHostConfiguration();    ArrayList<Header> headers = new ArrayList<Header>();        if (!acceptLanguage.isEmpty()) {        headers.add(new Header("Accept-Language", acceptLanguage));    }    if (!acceptCharset.isEmpty()) {        headers.add(new Header("Accept-Charset", acceptCharset));    }    if (!accept.isEmpty()) {        headers.add(new Header("Accept", accept));    }        headers.add(new Header("Accept-Encoding", "x-gzip, gzip, deflate"));    hostConf.getParams().setParameter("http.default-headers", headers);        if (useProxy) {        hostConf.setProxy(proxyHost, proxyPort);        if (proxyUsername.length() > 0) {            AuthScope proxyAuthScope = getAuthScope(this.proxyHost, this.proxyPort, this.proxyRealm);            NTCredentials proxyCredentials = new NTCredentials(this.proxyUsername, this.proxyPassword, Http.agentHost, this.proxyRealm);            client.getState().setProxyCredentials(proxyAuthScope, proxyCredentials);        }    }}
0
private static HttpFormAuthConfigurer readFormAuthConfigurer(Element credElement, String authMethod)
{    if ("formAuth".equals(authMethod)) {        HttpFormAuthConfigurer formConfigurer = new HttpFormAuthConfigurer();        String str = credElement.getAttribute("loginUrl");        if (StringUtils.isNotBlank(str)) {            formConfigurer.setLoginUrl(str.trim());        } else {            throw new IllegalArgumentException("Must set loginUrl.");        }        str = credElement.getAttribute("loginFormId");        if (StringUtils.isNotBlank(str)) {            formConfigurer.setLoginFormId(str.trim());        } else {            throw new IllegalArgumentException("Must set loginFormId.");        }        str = credElement.getAttribute("loginRedirect");        if (StringUtils.isNotBlank(str)) {            formConfigurer.setLoginRedirect(Boolean.parseBoolean(str));        }        NodeList nodeList = credElement.getChildNodes();        for (int j = 0; j < nodeList.getLength(); j++) {            Node node = nodeList.item(j);            if (!(node instanceof Element))                continue;            Element element = (Element) node;            if ("loginPostData".equals(element.getTagName())) {                Map<String, String> loginPostData = new HashMap<String, String>();                NodeList childNodes = element.getChildNodes();                for (int k = 0; k < childNodes.getLength(); k++) {                    Node fieldNode = childNodes.item(k);                    if (!(fieldNode instanceof Element))                        continue;                    Element fieldElement = (Element) fieldNode;                    String name = fieldElement.getAttribute("name");                    String value = fieldElement.getAttribute("value");                    loginPostData.put(name, value);                }                formConfigurer.setLoginPostData(loginPostData);            } else if ("additionalPostHeaders".equals(element.getTagName())) {                Map<String, String> additionalPostHeaders = new HashMap<String, String>();                NodeList childNodes = element.getChildNodes();                for (int k = 0; k < childNodes.getLength(); k++) {                    Node fieldNode = childNodes.item(k);                    if (!(fieldNode instanceof Element))                        continue;                    Element fieldElement = (Element) fieldNode;                    String name = fieldElement.getAttribute("name");                    String value = fieldElement.getAttribute("value");                    additionalPostHeaders.put(name, value);                }                formConfigurer.setAdditionalPostHeaders(additionalPostHeaders);            } else if ("removedFormFields".equals(element.getTagName())) {                Set<String> removedFormFields = new HashSet<String>();                NodeList childNodes = element.getChildNodes();                for (int k = 0; k < childNodes.getLength(); k++) {                    Node fieldNode = childNodes.item(k);                    if (!(fieldNode instanceof Element))                        continue;                    Element fieldElement = (Element) fieldNode;                    String name = fieldElement.getAttribute("name");                    removedFormFields.add(name);                }                formConfigurer.setRemovedFormFields(removedFormFields);            } else if ("loginCookie".equals(element.getTagName())) {                                                NodeList childNodes = element.getChildNodes();                for (int k = 0; k < childNodes.getLength(); k++) {                    Node fieldNode = childNodes.item(k);                    if (!(fieldNode instanceof Element))                        continue;                    Element fieldElement = (Element) fieldNode;                    if ("policy".equals(fieldElement.getTagName())) {                        String policy = fieldElement.getTextContent();                        formConfigurer.setCookiePolicy(policy);                                            }                }            }        }        return formConfigurer;    } else {        throw new IllegalArgumentException("Unsupported authMethod: " + authMethod);    }}
1
private void resolveCredentials(URL url)
{    if (formConfigurer != null) {        HttpFormAuthentication formAuther = new HttpFormAuthentication(formConfigurer, client, this);        try {            formAuther.login();        } catch (Exception e) {            throw new RuntimeException(e);        }        return;    }    if (defaultUsername != null && defaultUsername.length() > 0) {        int port = url.getPort();        if (port == -1) {            if ("https".equals(url.getProtocol()))                port = 443;            else                port = 80;        }        AuthScope scope = new AuthScope(url.getHost(), port);        if (client.getState().getCredentials(scope) != null) {            if (LOG.isTraceEnabled())                LOG.trace("Pre-configured credentials with scope - host: " + url.getHost() + "; port: " + port + "; found for url: " + url);                        return;        }        if (LOG.isTraceEnabled())            LOG.trace("Pre-configured credentials with scope -  host: " + url.getHost() + "; port: " + port + "; not found for url: " + url);        AuthScope serverAuthScope = getAuthScope(url.getHost(), port, defaultRealm, defaultScheme);        NTCredentials serverCredentials = new NTCredentials(defaultUsername, defaultPassword, agentHost, defaultRealm);        client.getState().setCredentials(serverAuthScope, serverCredentials);    }}
0
private static AuthScope getAuthScope(String host, int port, String realm, String scheme)
{    if (host.length() == 0)        host = null;    if (port < 0)        port = -1;    if (realm.length() == 0)        realm = null;    if (scheme.length() == 0)        scheme = null;    return new AuthScope(host, port, realm, scheme);}
0
private static AuthScope getAuthScope(String host, int port, String realm)
{    return getAuthScope(host, port, realm, "");}
0
public void setConf(Configuration conf)
{    this.conf = conf;}
0
public Configuration getConf()
{    return conf;}
0
public HttpAuthentication findAuthentication(Metadata header)
{    if (header == null)        return null;    try {        Collection<String> challenge = new ArrayList<String>();        challenge.add(header.get(WWW_AUTHENTICATE));        for (String challengeString : challenge) {            if (challengeString.equals("NTLM"))                challengeString = "Basic realm=techweb";            if (LOG.isTraceEnabled())                LOG.trace("Checking challengeString=" + challengeString);            HttpAuthentication auth = HttpBasicAuthentication.getAuthentication(challengeString, conf);            if (auth != null)                return auth;                }    } catch (Exception e) {            }    return null;}
1
public void setConf(Configuration conf)
{    this.conf = conf;}
0
public Configuration getConf()
{    return this.conf;}
0
public List<String> getCredentials()
{    return credentials;}
0
public String getRealm()
{    return realm;}
0
public static HttpBasicAuthentication getAuthentication(String challenge, Configuration conf)
{    if (challenge == null)        return null;    Matcher basicMatcher = basic.matcher(challenge);    if (basicMatcher.matches()) {        String realm = basicMatcher.group(1);        Object auth = authMap.get(realm);        if (auth == null) {            HttpBasicAuthentication newAuth = null;            try {                newAuth = new HttpBasicAuthentication(realm, conf);            } catch (HttpAuthenticationException hae) {                if (LOG.isTraceEnabled()) {                    LOG.trace("HttpBasicAuthentication failed for " + challenge);                }            }            authMap.put(realm, newAuth);            return newAuth;        } else {            return (HttpBasicAuthentication) auth;        }    }    return null;}
0
public static final Pattern getBasicPattern()
{    return basic;}
0
public String getLoginUrl()
{    return loginUrl;}
0
public HttpFormAuthConfigurer setLoginUrl(String loginUrl)
{    this.loginUrl = loginUrl;    return this;}
0
public String getLoginFormId()
{    return loginFormId;}
0
public HttpFormAuthConfigurer setLoginFormId(String loginForm)
{    this.loginFormId = loginForm;    return this;}
0
public Map<String, String> getLoginPostData()
{    return loginPostData == null ? new HashMap<String, String>() : loginPostData;}
0
public HttpFormAuthConfigurer setLoginPostData(Map<String, String> loginPostData)
{    this.loginPostData = loginPostData;    return this;}
0
public Map<String, String> getAdditionalPostHeaders()
{    return additionalPostHeaders == null ? new HashMap<String, String>() : additionalPostHeaders;}
0
public HttpFormAuthConfigurer setAdditionalPostHeaders(Map<String, String> additionalPostHeaders)
{    this.additionalPostHeaders = additionalPostHeaders;    return this;}
0
public boolean isLoginRedirect()
{    return loginRedirect;}
0
public HttpFormAuthConfigurer setLoginRedirect(boolean redirect)
{    this.loginRedirect = redirect;    return this;}
0
public Set<String> getRemovedFormFields()
{    return removedFormFields == null ? new HashSet<String>() : removedFormFields;}
0
public HttpFormAuthConfigurer setRemovedFormFields(Set<String> removedFormFields)
{    this.removedFormFields = removedFormFields;    return this;}
0
public void setCookiePolicy(String policy)
{    this.cookiePolicy = policy;}
0
public String getCookiePolicy()
{    return this.cookiePolicy;}
0
public void login() throws Exception
{        CookieHandler.setDefault(new CookieManager());    String pageContent = httpGetPageContent(authConfigurer.getLoginUrl());    List<NameValuePair> params = getLoginFormParams(pageContent);    sendPost(authConfigurer.getLoginUrl(), params);}
0
private void sendPost(String url, List<NameValuePair> params) throws Exception
{    PostMethod post = null;    try {        if (authConfigurer.isLoginRedirect()) {            post = new PostMethod(url) {                @Override                public boolean getFollowRedirects() {                    return true;                }            };        } else {            post = new PostMethod(url);        }                                        setLoginHeader(post);                        this.setCookieParams(authConfigurer, post.getParams());        post.addParameters(params.toArray(new NameValuePair[0]));        int rspCode = client.executeMethod(post);        if (LOG.isDebugEnabled()) {                                                            for (Header header : post.getRequestHeaders()) {                            }        }        String rst = IOUtils.toString(post.getResponseBodyAsStream());            } finally {        if (post != null) {            post.releaseConnection();        }    }}
1
public boolean getFollowRedirects()
{    return true;}
0
private void setCookieParams(HttpFormAuthConfigurer formConfigurer, HttpMethodParams params) throws NoSuchFieldException, SecurityException, IllegalArgumentException, IllegalAccessException
{        if (formConfigurer.getCookiePolicy() != null) {        String policy = formConfigurer.getCookiePolicy();        Object p = FieldUtils.readDeclaredStaticField(CookiePolicy.class, policy);        if (null != p) {                        params.setParameter(HttpMethodParams.COOKIE_POLICY, p);        }    }}
1
private void setLoginHeader(PostMethod post)
{    Map<String, String> headers = new HashMap<String, String>();    headers.putAll(defaultLoginHeaders);        headers.putAll(authConfigurer.getAdditionalPostHeaders());    for (Entry<String, String> entry : headers.entrySet()) {        post.addRequestHeader(entry.getKey(), entry.getValue());    }    post.addRequestHeader("Cookie", getCookies());}
0
private String httpGetPageContent(String url) throws IOException
{    GetMethod get = new GetMethod(url);    try {        for (Entry<String, String> entry : authConfigurer.getAdditionalPostHeaders().entrySet()) {            get.addRequestHeader(entry.getKey(), entry.getValue());        }        client.executeMethod(get);        Header cookieHeader = get.getResponseHeader("Set-Cookie");        if (cookieHeader != null) {            setCookies(cookieHeader.getValue());        }        String rst = IOUtils.toString(get.getResponseBodyAsStream());        return rst;    } finally {        get.releaseConnection();    }}
0
private List<NameValuePair> getLoginFormParams(String pageContent) throws UnsupportedEncodingException
{    List<NameValuePair> params = new ArrayList<NameValuePair>();    Document doc = Jsoup.parse(pageContent);    Element loginform = doc.getElementById(authConfigurer.getLoginFormId());    if (loginform == null) {                loginform = doc.select("form[name=" + authConfigurer.getLoginFormId() + "]").first();        if (loginform == null) {                        throw new IllegalArgumentException("No form exists: " + authConfigurer.getLoginFormId());        }    }    Elements inputElements = loginform.getElementsByTag("input");        for (Element inputElement : inputElements) {        String key = inputElement.attr("name");        String value = inputElement.attr("value");        if (authConfigurer.getLoginPostData().containsKey(key) || authConfigurer.getRemovedFormFields().contains(key)) {                        continue;        }        params.add(new NameValuePair(key, value));    }        for (Entry<String, String> entry : authConfigurer.getLoginPostData().entrySet()) {        params.add(new NameValuePair(entry.getKey(), entry.getValue()));    }    return params;}
1
public String getCookies()
{    return cookies;}
0
public void setCookies(String cookies)
{    this.cookies = cookies;}
0
public boolean isRedirect()
{    return authConfigurer.isLoginRedirect();}
0
public void setRedirect(boolean redirect)
{    this.authConfigurer.setLoginRedirect(redirect);}
0
public URL getUrl()
{    return url;}
0
public int getCode()
{    return code;}
0
public String getHeader(String name)
{    return headers.get(name);}
0
public Metadata getHeaders()
{    return headers;}
0
public byte[] getContent()
{    return content;}
0
public void setUp() throws Exception
{    ContextHandler context = new ContextHandler();    context.setContextPath("/");    context.setResourceBase(RES_DIR);    ServletHandler sh = new ServletHandler();    sh.addServletWithMapping("org.apache.jasper.servlet.JspServlet", "*.jsp");    context.addHandler(sh);    context.addHandler(new SessionHandler());    server = new Server();    server.addHandler(context);    conf = new Configuration();    conf.addResource("nutch-default.xml");    conf.addResource("nutch-site-test.xml");    http = new Http();    http.setConf(conf);}
0
public void tearDown() throws Exception
{    server.stop();    for (int i = 0; i < 5; i++) {        if (!server.isStopped()) {            Thread.sleep(1000);        }    }}
0
public void testCookies() throws Exception
{    startServer(47500);    fetchPage("/cookies.jsp", 200);    fetchPage("/cookies.jsp?cookie=yes", 200);}
0
public void testNoPreemptiveAuth() throws Exception
{    startServer(47500);    fetchPage("/noauth.jsp", 200);}
0
public void testDefaultCredentials() throws Exception
{    startServer(47502);    fetchPage("/basic.jsp", 200);}
0
public void testBasicAuth() throws Exception
{    startServer(47500);    fetchPage("/basic.jsp", 200);    fetchPage("/basic.jsp?case=1", 200);    fetchPage("/basic.jsp?case=2", 200);    server.start();}
0
public void testOtherRealmsNoAuth() throws Exception
{    startServer(47501);    fetchPage("/basic.jsp", 200);    fetchPage("/basic.jsp?case=1", 401);    fetchPage("/basic.jsp?case=2", 401);}
0
public void testDigestAuth() throws Exception
{    startServer(47500);    fetchPage("/digest.jsp", 200);}
0
public void testNtlmAuth() throws Exception
{    startServer(47501);    fetchPage("/ntlm.jsp", 200);}
0
private void startServer(int portno) throws Exception
{    SocketConnector listener = new SocketConnector();    listener.setHost("127.0.0.1");    server.addConnector(listener);    for (int p = portno; p < portno + 10; p++) {        port = portno;        listener.setPort(port);        try {            server.start();            break;        } catch (Exception e) {            if (p == portno + 9) {                throw e;            }        }    }}
0
private void fetchPage(String page, int expectedCode) throws Exception
{    URL url = new URL("http", "127.0.0.1", port, page);    Response response = null;    response = http.getResponse(url, new CrawlDatum(), true);    int code = response.getCode();    Assert.assertEquals("HTTP Status Code for " + url, expectedCode, code);}
0
public String processDriver(WebDriver driver)
{        String accumulatedData = "";    try {                JavascriptExecutor jsx = (JavascriptExecutor) driver;        jsx.executeScript("document.body.innerHTML=document.body.innerHTML " + accumulatedData + ";");    } catch (Exception e) {            }    return accumulatedData;}
1
public boolean shouldProcessURL(String URL)
{    return true;}
0
public String processDriver(WebDriver driver)
{    String accumulatedData = "";    try {        driver.findElement(By.tagName("body")).getAttribute("innerHTML");        Configuration conf = NutchConfiguration.create();        new WebDriverWait(driver, conf.getLong("libselenium.page.load.delay", 3));        List<WebElement> atags = driver.findElements(By.tagName("a"));        int numberofajaxlinks = atags.size();        for (int i = 0; i < numberofajaxlinks; i++) {            if (atags.get(i).getAttribute("href") != null && atags.get(i).getAttribute("href").equals("javascript:void(null);")) {                atags.get(i).click();                if (i == numberofajaxlinks - 1) {                                        JavascriptExecutor jsx = (JavascriptExecutor) driver;                    jsx.executeScript("document.body.innerHTML=document.body.innerHTML " + accumulatedData + ";");                    continue;                }                accumulatedData += driver.findElement(By.tagName("body")).getAttribute("innerHTML");                                driver.navigate().refresh();                new WebDriverWait(driver, conf.getLong("libselenium.page.load.delay", 3));                atags = driver.findElements(By.tagName("a"));            }        }    } catch (Exception e) {            }    return accumulatedData;}
1
public boolean shouldProcessURL(String URL)
{    return true;}
0
public String processDriver(WebDriver driver)
{    return null;}
0
public boolean shouldProcessURL(String url)
{    return true;}
0
public void setConf(Configuration conf)
{    super.setConf(conf);}
0
public static void main(String[] args) throws Exception
{    Http http = new Http();    http.setConf(NutchConfiguration.create());    main(http, args);}
0
protected Response getResponse(URL url, CrawlDatum datum, boolean redirect) throws ProtocolException, IOException
{    return new HttpResponse(this, url, datum);}
0
public URL getUrl()
{    return url;}
0
public int getCode()
{    return code;}
0
public String getHeader(String name)
{    return headers.get(name);}
0
public Metadata getHeaders()
{    return headers;}
0
public byte[] getContent()
{    return content;}
0
private void readPlainContent(URL url) throws IOException
{    if (handlers == null)        loadSeleniumHandlers();    String processedPage = "";    for (InteractiveSeleniumHandler handler : this.handlers) {        if (!handler.shouldProcessURL(url.toString())) {            continue;        }        WebDriver driver = HttpWebClient.getDriverForPage(url.toString(), conf);        processedPage += handler.processDriver(driver);        HttpWebClient.cleanUpDriver(driver);    }    content = processedPage.getBytes("UTF-8");}
0
private int parseStatusLine(PushbackInputStream in, StringBuffer line) throws IOException, HttpException
{    readLine(in, line, false);    int codeStart = line.indexOf(" ");    int codeEnd = line.indexOf(" ", codeStart + 1);        if (codeEnd == -1)        codeEnd = line.length();    int code;    try {        code = Integer.parseInt(line.substring(codeStart + 1, codeEnd));    } catch (NumberFormatException e) {        throw new HttpException("bad status line '" + line + "': " + e.getMessage(), e);    }    return code;}
0
private void processHeaderLine(StringBuffer line) throws IOException, HttpException
{        int colonIndex = line.indexOf(":");    if (colonIndex == -1) {        int i;        for (i = 0; i < line.length(); i++) if (!Character.isWhitespace(line.charAt(i)))            break;        if (i == line.length())            return;        throw new HttpException("No colon in header:" + line);    }    String key = line.substring(0, colonIndex);        int valueStart = colonIndex + 1;    while (valueStart < line.length()) {        int c = line.charAt(valueStart);        if (c != ' ' && c != '\t')            break;        valueStart++;    }    String value = line.substring(valueStart);    headers.set(key, value);}
0
private static int readLine(PushbackInputStream in, StringBuffer line, boolean allowContinuedLine) throws IOException
{    line.setLength(0);    for (int c = in.read(); c != -1; c = in.read()) {        switch(c) {            case '\r':                if (peek(in) == '\n') {                    in.read();                }            case '\n':                if (line.length() > 0) {                                        if (allowContinuedLine)                        switch(peek(in)) {                            case ' ':                            case                             '\t':                                in.read();                                continue;                        }                }                                return line.length();            default:                line.append((char) c);        }    }    throw new EOFException();}
0
private static int peek(PushbackInputStream in) throws IOException
{    int value = in.read();    in.unread(value);    return value;}
0
public void checkClientTrusted(java.security.cert.X509Certificate[] chain, String authType) throws CertificateException
{}
0
public void checkServerTrusted(java.security.cert.X509Certificate[] chain, String authType) throws CertificateException
{}
0
public java.security.cert.X509Certificate[] getAcceptedIssuers()
{    return new java.security.cert.X509Certificate[] {};}
0
public void setConf(Configuration conf)
{    super.setConf(conf);        List<okhttp3.Protocol> protocols = new ArrayList<>();    if (useHttp2) {        protocols.add(okhttp3.Protocol.HTTP_2);    }    protocols.add(okhttp3.Protocol.HTTP_1_1);    okhttp3.OkHttpClient.Builder builder = new OkHttpClient.Builder().protocols(    protocols).retryOnConnectionFailure(    true).followRedirects(    false).connectTimeout(timeout, TimeUnit.MILLISECONDS).writeTimeout(timeout, TimeUnit.MILLISECONDS).readTimeout(timeout, TimeUnit.MILLISECONDS);    if (!tlsCheckCertificate) {        builder.sslSocketFactory(trustAllSslSocketFactory, (X509TrustManager) trustAllCerts[0]);        builder.hostnameVerifier(new HostnameVerifier() {            @Override            public boolean verify(String hostname, SSLSession session) {                return true;            }        });    }    if (!accept.isEmpty()) {        getCustomRequestHeaders().add(new String[] { "Accept", accept });    }    if (!acceptLanguage.isEmpty()) {        getCustomRequestHeaders().add(new String[] { "Accept-Language", acceptLanguage });    }    if (!acceptCharset.isEmpty()) {        getCustomRequestHeaders().add(new String[] { "Accept-Charset", acceptCharset });    }    if (useProxy) {        Proxy proxy = new Proxy(proxyType, new InetSocketAddress(proxyHost, proxyPort));        String proxyUsername = conf.get("http.proxy.username");        if (proxyUsername == null) {            ProxySelector selector = new ProxySelector() {                @SuppressWarnings("serial")                private final List<Proxy> noProxyList = new ArrayList<Proxy>() {                    {                        add(Proxy.NO_PROXY);                    }                };                @SuppressWarnings("serial")                private final List<Proxy> proxyList = new ArrayList<Proxy>() {                    {                        add(proxy);                    }                };                @Override                public List<Proxy> select(URI uri) {                    if (useProxy(uri)) {                        return proxyList;                    }                    return noProxyList;                }                @Override                public void connectFailed(URI uri, SocketAddress sa, IOException ioe) {                                    }            };            builder.proxySelector(selector);        } else {            /*         * NOTE: the proxy exceptions list does NOT work with proxy         * username/password because an okhttp3 bug         * (https://github.com/square/okhttp/issues/3995) when using the         * ProxySelector class with proxy auth. If a proxy username is present,         * the configured proxy will be used for ALL requests.         */            if (proxyException.size() > 0) {                            }            builder.proxy(proxy);            String proxyPassword = conf.get("http.proxy.password");            Authenticator proxyAuthenticator = new Authenticator() {                @Override                public Request authenticate(okhttp3.Route route, okhttp3.Response response) throws IOException {                    String credential = okhttp3.Credentials.basic(proxyUsername, proxyPassword);                    return response.request().newBuilder().header("Proxy-Authorization", credential).build();                }            };            builder.proxyAuthenticator(proxyAuthenticator);        }    }    if (storeIPAddress || storeHttpHeaders || storeHttpRequest) {        builder.addNetworkInterceptor(new HTTPHeadersInterceptor());    }    client = builder.build();}
1
public boolean verify(String hostname, SSLSession session)
{    return true;}
0
public List<Proxy> select(URI uri)
{    if (useProxy(uri)) {        return proxyList;    }    return noProxyList;}
0
public void connectFailed(URI uri, SocketAddress sa, IOException ioe)
{    }
1
public Request authenticate(okhttp3.Route route, okhttp3.Response response) throws IOException
{    String credential = okhttp3.Credentials.basic(proxyUsername, proxyPassword);    return response.request().newBuilder().header("Proxy-Authorization", credential).build();}
0
public okhttp3.Response intercept(Interceptor.Chain chain) throws IOException
{    Connection connection = chain.connection();    String ipAddress = null;    if (storeIPAddress) {        InetAddress address = connection.socket().getInetAddress();        ipAddress = address.getHostAddress();    }    Request request = chain.request();    okhttp3.Response response = chain.proceed(request);    String httpProtocol = response.protocol().toString().toUpperCase(Locale.ROOT);    if (useHttp2 && "H2".equals(httpProtocol)) {                httpProtocol = "HTTP/2";    }    StringBuilder requestverbatim = null;    StringBuilder responseverbatim = null;    if (storeHttpRequest) {        requestverbatim = new StringBuilder();        requestverbatim.append(request.method()).append(' ');        requestverbatim.append(request.url().encodedPath());        String query = request.url().encodedQuery();        if (query != null) {            requestverbatim.append('?').append(query);        }        requestverbatim.append(' ').append(httpProtocol).append("\r\n");        Headers headers = request.headers();        for (int i = 0, size = headers.size(); i < size; i++) {            String key = headers.name(i);            String value = headers.value(i);            requestverbatim.append(key).append(": ").append(value).append("\r\n");        }        requestverbatim.append("\r\n");    }    if (storeHttpHeaders) {        responseverbatim = new StringBuilder();        responseverbatim.append(httpProtocol).append(' ').append(response.code());        if (!response.message().isEmpty()) {            responseverbatim.append(' ').append(response.message());        }        responseverbatim.append("\r\n");        Headers headers = response.headers();        for (int i = 0, size = headers.size(); i < size; i++) {            String key = headers.name(i);            String value = headers.value(i);            responseverbatim.append(key).append(": ").append(value).append("\r\n");        }        responseverbatim.append("\r\n");    }    okhttp3.Response.Builder builder = response.newBuilder();    if (ipAddress != null) {        builder = builder.header(Response.IP_ADDRESS, ipAddress);    }    if (requestverbatim != null) {        byte[] encodedBytesRequest = Base64.getEncoder().encode(requestverbatim.toString().getBytes());        builder = builder.header(Response.REQUEST, new String(encodedBytesRequest));    }    if (responseverbatim != null) {        byte[] encodedBytesResponse = Base64.getEncoder().encode(responseverbatim.toString().getBytes());        builder = builder.header(Response.RESPONSE_HEADERS, new String(encodedBytesResponse));    }        return builder.build();}
0
protected List<String[]> getCustomRequestHeaders()
{    return customRequestHeaders;}
0
protected OkHttpClient getClient()
{    return client;}
0
protected Response getResponse(URL url, CrawlDatum datum, boolean redirect) throws ProtocolException, IOException
{    return new OkHttpResponse(this, url, datum);}
0
public static void main(String[] args) throws Exception
{    OkHttp okhttp = new OkHttp();    okhttp.setConf(NutchConfiguration.create());    main(okhttp, args);}
0
public void setReason(TruncatedContentReason val)
{    value = val;}
0
public TruncatedContentReason getReason()
{    return value;}
0
public boolean booleanValue()
{    return value != TruncatedContentReason.NOT_TRUNCATED;}
0
private final byte[] toByteArray(final ResponseBody responseBody, TruncatedContent truncated, int maxContent, int maxDuration, boolean partialAsTruncated) throws IOException
{    if (responseBody == null) {        return new byte[] {};    }    long endDueFor = -1;    if (maxDuration != -1) {        endDueFor = System.currentTimeMillis() + (maxDuration * 1000);    }    int maxContentBytes = Integer.MAX_VALUE;    if (maxContent >= 0) {        maxContentBytes = Math.min(maxContentBytes, maxContent);    }    BufferedSource source = responseBody.source();    int bytesRequested = 0;    int bufferGrowStepBytes = 8192;    while (source.buffer().size() <= maxContentBytes) {        bytesRequested += Math.min(bufferGrowStepBytes, /*           * request one byte more than required to reliably detect truncated           * content, but beware of integer overflows           */        (maxContentBytes == Integer.MAX_VALUE ? maxContentBytes : (1 + maxContentBytes)) - bytesRequested);        boolean success = false;        try {            success = source.request(bytesRequested);        } catch (IOException e) {            if (partialAsTruncated && source.buffer().size() > 0) {                                truncated.setReason(TruncatedContentReason.DISCONNECT);                            } else {                throw e;            }        }        if (LOG.isDebugEnabled()) {                    }        if (!success) {                        break;        }        if (endDueFor != -1 && endDueFor <= System.currentTimeMillis()) {                        truncated.setReason(TruncatedContentReason.TIME);            break;        }        if (source.buffer().size() >= maxContentBytes) {                    }                bytesRequested = (int) source.buffer().size();    }    int bytesBuffered = (int) source.buffer().size();    int bytesToCopy = bytesBuffered;    if (maxContent >= 0 && bytesToCopy > maxContent) {                truncated.setReason(TruncatedContentReason.LENGTH);        bytesToCopy = maxContentBytes;    }    byte[] arr = new byte[bytesToCopy];    source.buffer().readFully(arr);    if (LOG.isDebugEnabled()) {            }    return arr;}
1
public URL getUrl()
{    return url;}
0
public int getCode()
{    return code;}
0
public String getHeader(String name)
{    return headers.get(name);}
0
public Metadata getHeaders()
{    return headers;}
0
public byte[] getContent()
{    return content;}
0
public void setUp() throws Exception
{    conf = NutchConfiguration.create();    conf.addResource("nutch-default.xml");            conf.addResource("nutch-site-test.xml");    conf.setBoolean("store.http.headers", true);    http = new ProtocolFactory(conf).getProtocolById("org.apache.nutch.protocol.okhttp.OkHttp");}
0
public void tearDown() throws Exception
{    server.close();}
0
public static String getHeaders(ProtocolOutput response)
{    return response.getContent().getMetadata().get(Response.RESPONSE_HEADERS);}
0
public static String getHeader(ProtocolOutput response, String header)
{    for (String line : getHeaders(response).split("\r\n")) {        String[] parts = line.split(": ", 1);        if (parts[0].equals(header)) {            return parts[1];        }    }    return null;}
0
private void runServer(int port, byte[] response) throws Exception
{    server = new ServerSocket();    server.bind(new InetSocketAddress("127.0.0.1", port));    Pattern requestPattern = Pattern.compile("(?i)^GET\\s+(\\S+)");    while (true) {                Socket socket = server.accept();                try (BufferedReader in = new BufferedReader(new InputStreamReader(socket.getInputStream(), StandardCharsets.UTF_8))) {            String line;            while ((line = in.readLine()) != null) {                                if (line.trim().isEmpty()) {                    break;                }                Matcher m = requestPattern.matcher(line);                if (m.find()) {                                        if (!m.group(1).startsWith("/")) {                        response = "HTTP/1.1 400 Bad request\r\n\r\n".getBytes(StandardCharsets.UTF_8);                    }                }            }            socket.getOutputStream().write(response);        } catch (Exception e) {                    }    }}
1
private void launchServer(String response) throws InterruptedException
{    launchServer(response.getBytes(StandardCharsets.UTF_8));}
0
private void launchServer(byte[] response) throws InterruptedException
{    Thread serverThread = new Thread(() -> {        try {            runServer(port, response);        } catch (Exception e) {                    }    });    serverThread.start();    Thread.sleep(50);}
1
private ProtocolOutput fetchPage(String page, int expectedCode) throws MalformedURLException
{    URL url = new URL("http", "127.0.0.1", port, page);        CrawlDatum crawlDatum = new CrawlDatum();    ProtocolOutput out = http.getProtocolOutput(new Text(url.toString()), crawlDatum);    int httpStatusCode = -1;    if (crawlDatum.getMetaData().containsKey(Nutch.PROTOCOL_STATUS_CODE_KEY)) {        httpStatusCode = Integer.parseInt(crawlDatum.getMetaData().get(Nutch.PROTOCOL_STATUS_CODE_KEY).toString());    }    assertEquals("HTTP Status Code for " + url, expectedCode, httpStatusCode);    return out;}
1
public void testBadHttpServer() throws Exception
{    setUp();            launchServer(responseHeader + simpleContent);    fetchPage("/", 200);}
0
public void testRequestNotStartingWithSlash() throws Exception
{    setUp();    launchServer(responseHeader + simpleContent);    fetchPage("?171", 200);}
0
public void testContentLengthNotANumber() throws Exception
{    setUp();    launchServer(responseHeader + "Content-Length: thousand\r\n" + simpleContent);    fetchPage("/", 200);}
0
public void testHeaderWithColon() throws Exception
{    setUp();    launchServer("HTTP/1.1 200: OK\r\n" + simpleContent);    fetchPage("/", 200);}
0
public void testHeaderSpellChecking() throws Exception
{    setUp();    launchServer(responseHeader + "Client-Transfer-Encoding: chunked\r\n" + simpleContent);    fetchPage("/", 200);}
0
public void testIgnoreErrorInRedirectPayload() throws Exception
{    setUp();    launchServer("HTTP/1.1 302 Found\r\nLocation: http://example.com/\r\n" + "Transfer-Encoding: chunked\r\n\r\nNot a valid chunk.");    ProtocolOutput fetched = fetchPage("/", 302);    assertNotNull("No redirect Location.", getHeader(fetched, "Location"));    assertEquals("Wrong redirect Location.", "http://example.com/", getHeader(fetched, "Location"));}
0
public void testNoStatusLine() throws Exception
{    setUp();    String text = "This is a text containing non-ASCII characters: \u00e4\u00f6\u00fc\u00df";    launchServer(text);    ProtocolOutput fetched = fetchPage("/", 200);    assertEquals("Wrong text returned for response with no status line.", text, new String(fetched.getContent().getContent(), StandardCharsets.UTF_8));    server.close();    text = "<!DOCTYPE html>\n<html>\n<head>\n" + "<title>Testing no HTTP header èéâ</title>\n" + "<meta charset=\"utf-8\">\n" + "</head>\n<body>This is a text containing non-ASCII characters:" + "\u00e4\u00f6\u00fc\u00df</body>\n</html";    launchServer(text);    fetched = fetchPage("/", 200);    assertEquals("Wrong text returned for response with no status line.", text, new String(fetched.getContent().getContent(), StandardCharsets.UTF_8));}
0
public void testMultiLineHeader() throws Exception
{    setUp();    launchServer(responseHeader + "Set-Cookie: UserID=JohnDoe;\r\n  Max-Age=3600;\r\n  Version=1\r\n" + simpleContent);    ProtocolOutput fetched = fetchPage("/", 200);        assertNotNull("Failed to set multi-line \"Set-Cookie\" header.", getHeader(fetched, "Set-Cookie"));    assertTrue("Failed to set multi-line \"Set-Cookie\" header.", getHeader(fetched, "Set-Cookie").contains("Version=1"));}
1
public void testOverlongHeader() throws Exception
{    setUp();    StringBuilder response = new StringBuilder();    response.append(responseHeader);    for (int i = 0; i < 80; i++) {        response.append("X-Custom-Header-");        for (int j = 0; j < 10000; j++) {            response.append('x');        }        response.append(": hello\r\n");    }    response.append("\r\n" + simpleContent);    launchServer(response.toString());        fetchPage("/", -1);}
0
public void testChunkedContent() throws Exception
{    setUp();    StringBuilder response = new StringBuilder();    response.append(responseHeader);    response.append("Content-Type: text/html\r\n");    response.append("Transfer-Encoding: chunked\r\n");        for (int i = 0; i < 80; i++) {        response.append(String.format("\r\n400\r\n%02x\r\n", i));        for (int j = 0; j < 1012; j++) {            response.append('x');        }        response.append(String.format("\r\n%02x\r\n", i));        response.append("\r\n");    }    response.append("\r\n0\r\n\r\n");    launchServer(response.toString());    ProtocolOutput fetched = fetchPage("/", 200);    assertEquals("Chunked content not truncated according to http.content.limit", 65536, fetched.getContent().getContent().length);    assertNotNull("Content truncation not marked", fetched.getContent().getMetadata().get(Response.TRUNCATED_CONTENT));    assertEquals("Content truncation not marked", Response.TruncatedContentReason.LENGTH.toString().toLowerCase(), fetched.getContent().getMetadata().get(Response.TRUNCATED_CONTENT_REASON));}
0
public void testTruncationMarking() throws Exception
{    setUp();    int[] kBs = { 63, 64, 65 };    for (int kB : kBs) {        StringBuilder response = new StringBuilder();        response.append(responseHeader);        response.append("Content-Type: text/plain\r\nContent-Length: " + (kB * 1024) + "\r\n\r\n");        for (int i = 0; i < kB; i++) {            for (int j = 0; j < 16; j++) {                                response.append("abcdefghijklmnopqurstuvxyz0123456789-ABCDEFGHIJKLMNOPQURSTUVXYZ\n");            }        }        launchServer(response.toString());        ProtocolOutput fetched = fetchPage("/", 200);        assertEquals("Content not truncated according to http.content.limit", Math.min(kB * 1024, 65536), fetched.getContent().getContent().length);        if (kB * 1024 > 65536) {            assertNotNull("Content truncation not marked", fetched.getContent().getMetadata().get(Response.TRUNCATED_CONTENT));            assertEquals("Content truncation not marked", Response.TruncatedContentReason.LENGTH.toString().toLowerCase(), fetched.getContent().getMetadata().get(Response.TRUNCATED_CONTENT_REASON));        }                server.close();    }}
0
public void testTruncationMarkingGzip() throws Exception
{    setUp();    int[] kBs = { 63, 64, 65 };    for (int kB : kBs) {        StringBuilder payload = new StringBuilder();        for (int i = 0; i < kB; i++) {            for (int j = 0; j < 16; j++) {                                payload.append("abcdefghijklmnopqurstuvxyz0123456789-ABCDEFGHIJKLMNOPQURSTUVXYZ\n");            }        }        ByteArrayOutputStream bytes = new ByteArrayOutputStream();        GZIPOutputStream gzip = new GZIPOutputStream(bytes);        gzip.write(payload.toString().getBytes(StandardCharsets.UTF_8));        gzip.close();        StringBuilder responseHead = new StringBuilder();        responseHead.append(responseHeader);        responseHead.append("Content-Type: text/plain\r\nContent-Length: " + bytes.size() + "\r\nContent-Encoding: gzip\r\n\r\n");        ByteArrayOutputStream response = new ByteArrayOutputStream();        response.write(responseHead.toString().getBytes(StandardCharsets.UTF_8));        response.write(bytes.toByteArray());        launchServer(response.toByteArray());        ProtocolOutput fetched = fetchPage("/", 200);        assertEquals("Content not truncated according to http.content.limit", Math.min(kB * 1024, 65536), fetched.getContent().getContent().length);        if (kB * 1024 > 65536) {            assertNotNull("Content truncation not marked", fetched.getContent().getMetadata().get(Response.TRUNCATED_CONTENT));            assertEquals("Content truncation not marked", Response.TruncatedContentReason.LENGTH.toString().toLowerCase(), fetched.getContent().getMetadata().get(Response.TRUNCATED_CONTENT_REASON));        }                server.close();    }}
0
public void testPartialContentTruncated() throws Exception
{    setUp();    conf.setBoolean("http.partial.truncated", true);    http.setConf(conf);    String testContent = "This is a text.";    launchServer(responseHeader + "Content-Length: 50000\r\n\r\n" + testContent);    ProtocolOutput fetched = fetchPage("/", 200);    assertEquals("Content not saved as truncated", testContent, new String(fetched.getContent().getContent(), StandardCharsets.UTF_8));    assertNotNull("Content truncation not marked", fetched.getContent().getMetadata().get(Response.TRUNCATED_CONTENT));}
0
public void testNoContentLimit() throws Exception
{    setUp();    conf.setInt("http.content.limit", -1);    http.setConf(conf);    StringBuilder response = new StringBuilder();    response.append(responseHeader);            int kB = 128;    response.append("Content-Type: text/plain\r\nContent-Length: " + (kB * 1024) + "\r\n\r\n");    for (int i = 0; i < kB; i++) {        for (int j = 0; j < 16; j++) {                        response.append("abcdefghijklmnopqurstuvxyz0123456789-ABCDEFGHIJKLMNOPQURSTUVXYZ\n");        }    }    launchServer(response.toString());    ProtocolOutput fetched = fetchPage("/", 200);    assertEquals("Content truncated although http.content.limit == -1", (kB * 1024), fetched.getContent().getContent().length);}
0
public void setUp(boolean redirection) throws Exception
{    conf = NutchConfiguration.create();    conf.addResource("nutch-default.xml");            conf.addResource("nutch-site-test.xml");    http = new ProtocolFactory(conf).getProtocolById("org.apache.nutch.protocol.okhttp.OkHttp");    server = new Server();    if (redirection) {        root = new Context(server, "/redirection", Context.SESSIONS);        root.setAttribute("newContextURL", "/redirect");    } else {        root = new Context(server, "/", Context.SESSIONS);    }    ServletHolder sh = new ServletHolder(org.apache.jasper.servlet.JspServlet.class);    root.addServlet(sh, "*.jsp");    root.setResourceBase(RES_DIR);}
0
public void tearDown() throws Exception
{    server.stop();}
0
public void testStatusCode() throws Exception
{    startServer(47504, false);    fetchPage("/basic-http.jsp", 200);    fetchPage("/redirect301.jsp", 301);    fetchPage("/redirect302.jsp", 302);    fetchPage("/nonexists.html", 404);    fetchPage("/brokenpage.jsp", 500);}
0
public void testRedirectionJetty() throws Exception
{        startServer(47503, true);    fetchPage("/redirection", 302);}
0
private void startServer(int portno, boolean redirection) throws Exception
{    port = portno;    setUp(redirection);    SelectChannelConnector connector = new SelectChannelConnector();    connector.setHost("127.0.0.1");    connector.setPort(port);    server.addConnector(connector);    server.start();}
0
private void fetchPage(String page, int expectedCode) throws Exception
{    URL url = new URL("http", "127.0.0.1", port, page);    CrawlDatum crawlDatum = new CrawlDatum();    ProtocolOutput out = http.getProtocolOutput(new Text(url.toString()), crawlDatum);    int httpStatusCode = -1;    if (crawlDatum.getMetaData().containsKey(Nutch.PROTOCOL_STATUS_CODE_KEY)) {        httpStatusCode = Integer.parseInt(crawlDatum.getMetaData().get(Nutch.PROTOCOL_STATUS_CODE_KEY).toString());    }    Content content = out.getContent();    assertEquals("HTTP Status Code for " + url, expectedCode, httpStatusCode);    if (page.compareTo("/nonexists.html") != 0 && page.compareTo("/brokenpage.jsp") != 0 && page.compareTo("/redirection") != 0) {        assertEquals("ContentType " + url, "text/html", content.getContentType());    }}
0
public void setConf(Configuration conf)
{    super.setConf(conf);}
0
public static void main(String[] args) throws Exception
{    Http http = new Http();    http.setConf(NutchConfiguration.create());    main(http, args);}
0
protected Response getResponse(URL url, CrawlDatum datum, boolean redirect) throws ProtocolException, IOException
{    return new HttpResponse(this, url, datum);}
0
public URL getUrl()
{    return url;}
0
public int getCode()
{    return code;}
0
public String getHeader(String name)
{    return headers.get(name);}
0
public Metadata getHeaders()
{    return headers;}
0
public byte[] getContent()
{    return content;}
0
private void readPlainContent(URL url) throws IOException
{    String page = HttpWebClient.getHtmlPage(url.toString(), conf);    content = page.getBytes("UTF-8");}
0
private int parseStatusLine(PushbackInputStream in, StringBuffer line) throws IOException, HttpException
{    readLine(in, line, false);    int codeStart = line.indexOf(" ");    int codeEnd = line.indexOf(" ", codeStart + 1);        if (codeEnd == -1)        codeEnd = line.length();    int code;    try {        code = Integer.parseInt(line.substring(codeStart + 1, codeEnd));    } catch (NumberFormatException e) {        throw new HttpException("bad status line '" + line + "': " + e.getMessage(), e);    }    return code;}
0
private void processHeaderLine(StringBuffer line) throws IOException, HttpException
{        int colonIndex = line.indexOf(":");    if (colonIndex == -1) {        int i;        for (i = 0; i < line.length(); i++) if (!Character.isWhitespace(line.charAt(i)))            break;        if (i == line.length())            return;        throw new HttpException("No colon in header:" + line);    }    String key = line.substring(0, colonIndex);        int valueStart = colonIndex + 1;    while (valueStart < line.length()) {        int c = line.charAt(valueStart);        if (c != ' ' && c != '\t')            break;        valueStart++;    }    String value = line.substring(valueStart);    headers.set(key, value);}
0
private static int readLine(PushbackInputStream in, StringBuffer line, boolean allowContinuedLine) throws IOException
{    line.setLength(0);    for (int c = in.read(); c != -1; c = in.read()) {        switch(c) {            case '\r':                if (peek(in) == '\n') {                    in.read();                }            case '\n':                if (line.length() > 0) {                                        if (allowContinuedLine)                        switch(peek(in)) {                            case ' ':                            case                             '\t':                                in.read();                                continue;                        }                }                                return line.length();            default:                line.append((char) c);        }    }    throw new EOFException();}
0
private static int peek(PushbackInputStream in) throws IOException
{    int value = in.read();    in.unread(value);    return value;}
0
public boolean setConfig(Configuration conf)
{    try {        exchange = conf.get(RabbitMQConstants.EXCHANGE_NAME);        routingKey = conf.get(RabbitMQConstants.ROUTING_KEY);        headersStatic = conf.get(RabbitMQConstants.HEADERS_STATIC, "");        String uri = conf.get(RabbitMQConstants.SERVER_URI);        client = new RabbitMQClient(uri);        client.openChannel();        boolean binding = conf.getBoolean(RabbitMQConstants.BINDING, false);        if (binding) {            String queueName = conf.get(RabbitMQConstants.QUEUE_NAME);            String queueOptions = conf.get(RabbitMQConstants.QUEUE_OPTIONS);            String exchangeOptions = conf.get(RabbitMQConstants.EXCHANGE_OPTIONS);            String bindingArguments = conf.get(RabbitMQConstants.BINDING_ARGUMENTS, "");            client.bind(exchange, exchangeOptions, queueName, queueOptions, routingKey, bindingArguments);        }                return true;    } catch (Exception e) {                return false;    }}
1
public void publish(Object event, Configuration conf)
{    try {        RabbitMQMessage message = new RabbitMQMessage();        message.setBody(getJSONString(event).getBytes());        message.setHeaders(headersStatic);        client.publish(exchange, routingKey, message);    } catch (Exception e) {            }}
1
private String getJSONString(Object obj)
{    ObjectMapper mapper = new ObjectMapper();    try {        return mapper.writeValueAsString(obj);    } catch (JsonProcessingException e) {            }    return null;}
1
public void setConf(Configuration arg0)
{}
0
public Configuration getConf()
{    return null;}
0
public void setConf(Configuration conf)
{    super.setConf(conf);    if (conf == null)        return;    defaultMaxDepth = conf.getInt("scoring.depth.max", DEFAULT_MAX_DEPTH);    if (defaultMaxDepth <= 0) {        defaultMaxDepth = DEFAULT_MAX_DEPTH;    }}
0
public CrawlDatum distributeScoreToOutlinks(Text fromUrl, ParseData parseData, Collection<Entry<Text, CrawlDatum>> targets, CrawlDatum adjust, int allCount) throws ScoringFilterException
{    if (targets.isEmpty()) {        return adjust;    }    String depthString = parseData.getMeta(DEPTH_KEY);    if (depthString == null) {                targets.clear();        return adjust;    }    int curDepth = Integer.parseInt(depthString);    int curMaxDepth = defaultMaxDepth;    IntWritable customMaxDepth = null;        String maxDepthString = parseData.getMeta(MAX_DEPTH_KEY);    if (maxDepthString != null) {        curMaxDepth = Integer.parseInt(maxDepthString);        customMaxDepth = new IntWritable(curMaxDepth);    }    if (curDepth >= curMaxDepth) {                        targets.clear();        return adjust;    }    Iterator<Entry<Text, CrawlDatum>> it = targets.iterator();    while (it.hasNext()) {        Entry<Text, CrawlDatum> e = it.next();                e.getValue().getMetaData().put(DEPTH_KEY_W, new IntWritable(curDepth + 1));                if (customMaxDepth != null) {            e.getValue().getMetaData().put(MAX_DEPTH_KEY_W, customMaxDepth);        }    }    return adjust;}
1
public float generatorSortValue(Text url, CrawlDatum datum, float initSort) throws ScoringFilterException
{        int curDepth, curMaxDepth;    IntWritable maxDepth = (IntWritable) datum.getMetaData().get(MAX_DEPTH_KEY_W);    if (maxDepth != null) {        curMaxDepth = maxDepth.get();    } else {        curMaxDepth = defaultMaxDepth;    }    IntWritable depth = (IntWritable) datum.getMetaData().get(DEPTH_KEY_W);    if (depth == null) {                curDepth = curMaxDepth;    } else {        curDepth = depth.get();    }    int mul = curMaxDepth - curDepth;    return initSort * (1 + mul);}
0
public float indexerScore(Text url, NutchDocument doc, CrawlDatum dbDatum, CrawlDatum fetchDatum, Parse parse, Inlinks inlinks, float initScore) throws ScoringFilterException
{    return initScore;}
0
public void initialScore(Text url, CrawlDatum datum) throws ScoringFilterException
{        if (datum.getMetaData().get(MAX_DEPTH_KEY_W) == null)        datum.getMetaData().put(MAX_DEPTH_KEY_W, new IntWritable(defaultMaxDepth));        if (datum.getMetaData().get(DEPTH_KEY_W) == null)        datum.getMetaData().put(DEPTH_KEY_W, new IntWritable(1));}
0
public void injectedScore(Text url, CrawlDatum datum) throws ScoringFilterException
{        if (datum.getMetaData().get(MAX_DEPTH_KEY_W) != null) {                String depthString = datum.getMetaData().get(MAX_DEPTH_KEY_W).toString();        datum.getMetaData().remove(MAX_DEPTH_KEY_W);        int depth = Integer.parseInt(depthString);        datum.getMetaData().put(MAX_DEPTH_KEY_W, new IntWritable(depth));    } else {                datum.getMetaData().put(MAX_DEPTH_KEY_W, new IntWritable(defaultMaxDepth));    }        datum.getMetaData().put(DEPTH_KEY_W, new IntWritable(1));}
0
public void passScoreAfterParsing(Text url, Content content, Parse parse) throws ScoringFilterException
{    String depth = content.getMetadata().get(DEPTH_KEY);    if (depth != null) {        parse.getData().getParseMeta().set(DEPTH_KEY, depth);    }    String maxdepth = content.getMetadata().get(MAX_DEPTH_KEY);    if (maxdepth != null) {        parse.getData().getParseMeta().set(MAX_DEPTH_KEY, maxdepth);    }}
0
public void passScoreBeforeParsing(Text url, CrawlDatum datum, Content content) throws ScoringFilterException
{    IntWritable depth = (IntWritable) datum.getMetaData().get(DEPTH_KEY_W);    if (depth != null) {        content.getMetadata().set(DEPTH_KEY, depth.toString());    }    IntWritable maxdepth = (IntWritable) datum.getMetaData().get(MAX_DEPTH_KEY_W);    if (maxdepth != null) {        content.getMetadata().set(MAX_DEPTH_KEY, maxdepth.toString());    }}
0
public void updateDbScore(Text url, CrawlDatum old, CrawlDatum datum, List<CrawlDatum> inlinked) throws ScoringFilterException
{        int newDepth = DEFAULT_MAX_DEPTH;    if (old != null) {        IntWritable oldDepth = (IntWritable) old.getMetaData().get(DEPTH_KEY_W);        if (oldDepth != null) {            newDepth = oldDepth.get();        } else {                        initialScore(url, old);        }    }    for (CrawlDatum lnk : inlinked) {        IntWritable depth = (IntWritable) lnk.getMetaData().get(DEPTH_KEY_W);        if (depth != null && depth.get() < newDepth) {            newDepth = depth.get();        }    }    datum.getMetaData().put(DEPTH_KEY_W, new IntWritable(newDepth));}
0
public Configuration getConf()
{    return conf;}
0
public void setConf(Configuration conf)
{    this.conf = conf;    normalizedScore = conf.getFloat("link.analyze.normalize.score", 1.00f);}
0
public CrawlDatum distributeScoreToOutlinks(Text fromUrl, ParseData parseData, Collection<Entry<Text, CrawlDatum>> targets, CrawlDatum adjust, int allCount) throws ScoringFilterException
{    return adjust;}
0
public float generatorSortValue(Text url, CrawlDatum datum, float initSort) throws ScoringFilterException
{    return datum.getScore() * initSort;}
0
public float indexerScore(Text url, NutchDocument doc, CrawlDatum dbDatum, CrawlDatum fetchDatum, Parse parse, Inlinks inlinks, float initScore) throws ScoringFilterException
{    if (dbDatum == null) {        return initScore;    }    return (normalizedScore * dbDatum.getScore());}
0
public void initialScore(Text url, CrawlDatum datum) throws ScoringFilterException
{    datum.setScore(initialScore);}
0
public void injectedScore(Text url, CrawlDatum datum) throws ScoringFilterException
{}
0
public void passScoreAfterParsing(Text url, Content content, Parse parse) throws ScoringFilterException
{    parse.getData().getContentMeta().set(Nutch.SCORE_KEY, content.getMetadata().get(Nutch.SCORE_KEY));}
0
public void passScoreBeforeParsing(Text url, CrawlDatum datum, Content content) throws ScoringFilterException
{    content.getMetadata().set(Nutch.SCORE_KEY, "" + datum.getScore());}
0
public void updateDbScore(Text url, CrawlDatum old, CrawlDatum datum, List<CrawlDatum> inlinked) throws ScoringFilterException
{}
0
public Configuration getConf()
{    return conf;}
0
public void setConf(Configuration conf)
{    this.conf = conf;    scorePower = conf.getFloat("indexer.score.power", 0.5f);    internalScoreFactor = conf.getFloat("db.score.link.internal", 1.0f);    externalScoreFactor = conf.getFloat("db.score.link.external", 1.0f);    countFiltered = conf.getBoolean("db.score.count.filtered", false);}
0
public void injectedScore(Text url, CrawlDatum datum) throws ScoringFilterException
{}
0
public void initialScore(Text url, CrawlDatum datum) throws ScoringFilterException
{    datum.setScore(0.0f);}
0
public float generatorSortValue(Text url, CrawlDatum datum, float initSort) throws ScoringFilterException
{    return datum.getScore() * initSort;}
0
public void updateDbScore(Text url, CrawlDatum old, CrawlDatum datum, List<CrawlDatum> inlinked) throws ScoringFilterException
{    float adjust = 0.0f;    for (int i = 0; i < inlinked.size(); i++) {        CrawlDatum linked = inlinked.get(i);        adjust += linked.getScore();    }    if (old == null)        old = datum;    datum.setScore(old.getScore() + adjust);}
0
public void passScoreBeforeParsing(Text url, CrawlDatum datum, Content content)
{    content.getMetadata().set(Nutch.SCORE_KEY, "" + datum.getScore());}
0
public void passScoreAfterParsing(Text url, Content content, Parse parse)
{    parse.getData().getContentMeta().set(Nutch.SCORE_KEY, content.getMetadata().get(Nutch.SCORE_KEY));}
0
public CrawlDatum distributeScoreToOutlinks(Text fromUrl, ParseData parseData, Collection<Entry<Text, CrawlDatum>> targets, CrawlDatum adjust, int allCount) throws ScoringFilterException
{    float score = scoreInjected;    String scoreString = parseData.getContentMeta().get(Nutch.SCORE_KEY);    if (scoreString != null) {        try {            score = Float.parseFloat(scoreString);        } catch (Exception e) {                    }    }    int validCount = targets.size();    if (countFiltered) {        score /= allCount;    } else {        if (validCount == 0) {                        return adjust;        }        score /= validCount;    }        float internalScore = score * internalScoreFactor;    float externalScore = score * externalScoreFactor;    for (Entry<Text, CrawlDatum> target : targets) {        try {            String toHost = new URL(target.getKey().toString()).getHost();            String fromHost = new URL(fromUrl.toString()).getHost();            if (toHost.equalsIgnoreCase(fromHost)) {                target.getValue().setScore(internalScore);            } else {                target.getValue().setScore(externalScore);            }        } catch (MalformedURLException e) {                        target.getValue().setScore(externalScore);        }    }        return adjust;}
1
public float indexerScore(Text url, NutchDocument doc, CrawlDatum dbDatum, CrawlDatum fetchDatum, Parse parse, Inlinks inlinks, float initScore) throws ScoringFilterException
{    if (dbDatum == null) {        return initScore;    }    return (float) Math.pow(dbDatum.getScore(), scorePower) * initScore;}
0
public void setConf(Configuration conf)
{    markGoneAfter = conf.getInt("scoring.orphan.mark.gone.after", DEFAULT_GONE_TIME);    markOrphanAfter = conf.getInt("scoring.orphan.mark.orphan.after", DEFAULT_ORPHAN_TIME);    if (markGoneAfter > markOrphanAfter) {            }}
1
public void updateDbScore(Text url, CrawlDatum old, CrawlDatum datum, List<CrawlDatum> inlinks) throws ScoringFilterException
{    int now = (int) (System.currentTimeMillis() / 1000);        if (inlinks.size() > 0) {                datum.getMetaData().put(ORPHAN_KEY_WRITABLE, new IntWritable(now));    } else {        orphanedScore(url, datum);    }}
0
public void orphanedScore(Text url, CrawlDatum datum)
{        if (datum.getMetaData().containsKey(ORPHAN_KEY_WRITABLE)) {                IntWritable writable = (IntWritable) datum.getMetaData().get(ORPHAN_KEY_WRITABLE);        int lastInlinkTime = writable.get();        int now = (int) (System.currentTimeMillis() / 1000);        int elapsedSinceLastInLinkTime = now - lastInlinkTime;        if (elapsedSinceLastInLinkTime > markOrphanAfter) {                        datum.setStatus(CrawlDatum.STATUS_DB_ORPHAN);        } else if (elapsedSinceLastInLinkTime > markGoneAfter) {                        datum.setStatus(CrawlDatum.STATUS_DB_GONE);        }    }}
0
public void testOrphanScoringFilter() throws Exception
{    Configuration conf = NutchConfiguration.create();    conf.setInt("scoring.orphan.mark.gone.after", 5);    conf.setInt("scoring.orphan.mark.orphan.after", 10);    ScoringFilter filter = new OrphanScoringFilter();    filter.setConf(conf);    Text url = new Text("http://nutch.apache.org/");    CrawlDatum datum = new CrawlDatum();    datum.setStatus(CrawlDatum.STATUS_DB_NOTMODIFIED);    List<CrawlDatum> emptyListOfInlinks = new ArrayList<CrawlDatum>();    List<CrawlDatum> populatedListOfInlinks = new ArrayList<CrawlDatum>();    populatedListOfInlinks.add(datum);        filter.updateDbScore(url, null, datum, populatedListOfInlinks);    int firstOrphanTime = getTime(datum);    assertTrue(datum.getMetaData().containsKey(OrphanScoringFilter.ORPHAN_KEY_WRITABLE));        try {        Thread.sleep(1000);    } catch (Exception e) {    }        filter.updateDbScore(url, null, datum, populatedListOfInlinks);    int secondOrphanTime = getTime(datum);    assertTrue(secondOrphanTime > firstOrphanTime);            filter.updateDbScore(url, null, datum, emptyListOfInlinks);    int thirdOrphanTime = getTime(datum);    assertEquals(thirdOrphanTime, secondOrphanTime);    assertEquals("Expected status db_notmodified but got " + CrawlDatum.getStatusName(datum.getStatus()), CrawlDatum.STATUS_DB_NOTMODIFIED, datum.getStatus());        try {        Thread.sleep(1000);    } catch (Exception e) {    }            filter.updateDbScore(url, null, datum, emptyListOfInlinks);    assertEquals("Expected status db_notmodified but got " + CrawlDatum.getStatusName(datum.getStatus()), CrawlDatum.STATUS_DB_NOTMODIFIED, datum.getStatus());        try {        Thread.sleep(5000);    } catch (Exception e) {    }        filter.updateDbScore(url, null, datum, emptyListOfInlinks);    int fourthOrphanTime = getTime(datum);    assertEquals(fourthOrphanTime, thirdOrphanTime);    assertEquals("Expected status db_gone but got " + CrawlDatum.getStatusName(datum.getStatus()), CrawlDatum.STATUS_DB_GONE, datum.getStatus());        try {        Thread.sleep(5000);    } catch (Exception e) {    }        filter.updateDbScore(url, null, datum, emptyListOfInlinks);    assertEquals("Expected status db_orphan but got " + CrawlDatum.getStatusName(datum.getStatus()), CrawlDatum.STATUS_DB_ORPHAN, datum.getStatus());}
0
protected int getTime(CrawlDatum datum)
{    IntWritable writable = (IntWritable) datum.getMetaData().get(OrphanScoringFilter.ORPHAN_KEY_WRITABLE);    return writable.get();}
0
public void setConf(Configuration conf)
{    this.conf = conf;}
0
public float setURLScoreAfterParsing(Text url, Content content, Parse parse)
{    float score = 1;    try {        if (!Model.isModelCreated) {            Model.createModel(conf);        }        String metatags = parse.getData().getParseMeta().get("metatag.keyword");        String metaDescription = parse.getData().getParseMeta().get("metatag.description");        int[] ngramArr = Model.retrieveNgrams(conf);        int mingram = ngramArr[0];        int maxgram = ngramArr[1];        DocVector docVector = Model.createDocVector(parse.getText() + metaDescription + metatags, mingram, maxgram);        if (docVector != null) {            score = Model.computeCosineSimilarity(docVector);                    } else {            throw new Exception("Could not create DocVector from parsed text");        }    } catch (Exception e) {            }    return score;}
1
public CrawlDatum distributeScoreToOutlinks(Text fromUrl, ParseData parseData, Collection<Entry<Text, CrawlDatum>> targets, CrawlDatum adjust, int allCount)
{    float score = Float.parseFloat(parseData.getContentMeta().get(Nutch.SCORE_KEY));    for (Entry<Text, CrawlDatum> target : targets) {        target.getValue().setScore(score);    }    return adjust;}
0
public void setTermFreqVector(HashMap<String, Integer> termFreqVector)
{    this.termFreqVector = termFreqVector;}
0
public void setVectorEntry(int pos, long freq)
{    termVector.put(pos, freq);}
0
public float dotProduct(DocVector docVector)
{    float product = 0;    for (Map.Entry<String, Integer> entry : termFreqVector.entrySet()) {        if (docVector.termFreqVector.containsKey(entry.getKey())) {            product += docVector.termFreqVector.get(entry.getKey()) * entry.getValue();        }    }    return product;}
0
public float getL2Norm()
{    float sum = 0;    for (Map.Entry<String, Integer> entry : termFreqVector.entrySet()) {        sum += entry.getValue() * entry.getValue();    }    return (float) Math.sqrt(sum);}
0
public static DocVector createDocVector(String content, int mingram, int maxgram)
{    LuceneTokenizer tokenizer;    if (mingram > 1 && maxgram > 1) {                tokenizer = new LuceneTokenizer(content, TokenizerType.STANDARD, StemFilterType.PORTERSTEM_FILTER, mingram, maxgram);    } else if (mingram > 1) {        maxgram = mingram;                tokenizer = new LuceneTokenizer(content, TokenizerType.STANDARD, StemFilterType.PORTERSTEM_FILTER, mingram, maxgram);    } else if (stopWords != null) {        tokenizer = new LuceneTokenizer(content, TokenizerType.STANDARD, stopWords, true, StemFilterType.PORTERSTEM_FILTER);    } else {        tokenizer = new LuceneTokenizer(content, TokenizerType.STANDARD, true, StemFilterType.PORTERSTEM_FILTER);    }    TokenStream tStream = tokenizer.getTokenStream();    HashMap<String, Integer> termVector = new HashMap<>();    try {        CharTermAttribute charTermAttribute = tStream.addAttribute(CharTermAttribute.class);        tStream.reset();        while (tStream.incrementToken()) {            String term = charTermAttribute.toString();                        if (termVector.containsKey(term)) {                int count = termVector.get(term);                count++;                termVector.put(term, count);            } else {                termVector.put(term, 1);            }        }        DocVector docVector = new DocVector();        docVector.setTermFreqVector(termVector);        return docVector;    } catch (IOException e) {            }    return null;}
1
public static float computeCosineSimilarity(DocVector docVector)
{    float[] scores = new float[docVectors.size()];    int i = 0;    float maxScore = 0;    for (DocVector corpusDoc : docVectors) {        float numerator = docVector.dotProduct(corpusDoc);        float denominator = docVector.getL2Norm() * corpusDoc.getL2Norm();        float currentScore = numerator / denominator;        scores[i++] = currentScore;        maxScore = (currentScore > maxScore) ? currentScore : maxScore;    }        return maxScore;}
0
public static int[] retrieveNgrams(Configuration conf)
{    int[] ngramArr = new int[2];        String[] ngramStr = conf.getStrings("scoring.similarity.ngrams", "1,1");        ngramArr[0] = Integer.parseInt(ngramStr[0]);    if (ngramStr.length > 1) {                ngramArr[1] = Integer.parseInt(ngramStr[1]);    } else {                ngramArr[1] = ngramArr[0];    }    return ngramArr;}
0
public Configuration getConf()
{    return conf;}
0
public void setConf(Configuration conf)
{    this.conf = conf;    switch(conf.get("scoring.similarity.model", "cosine")) {        case "cosine":            similarityModel = (SimilarityModel) new CosineSimilarity();            break;    }    similarityModel.setConf(conf);}
0
public void passScoreAfterParsing(Text url, Content content, Parse parse) throws ScoringFilterException
{    float score = similarityModel.setURLScoreAfterParsing(url, content, parse);    parse.getData().getContentMeta().set(Nutch.SCORE_KEY, score + "");}
0
public CrawlDatum distributeScoreToOutlinks(Text fromUrl, ParseData parseData, Collection<Entry<Text, CrawlDatum>> targets, CrawlDatum adjust, int allCount) throws ScoringFilterException
{    similarityModel.distributeScoreToOutlinks(fromUrl, parseData, targets, adjust, allCount);    return adjust;}
0
protected TokenStreamComponents createComponents(String fieldName)
{    Tokenizer source = new ClassicTokenizer();    TokenStream filter = new LowerCaseFilter(source);    if (stopSet != null) {        filter = new StopFilter(filter, stopSet);    }    switch(stemFilterType) {        case PORTERSTEM_FILTER:            filter = new PorterStemFilter(filter);            break;        case ENGLISHMINIMALSTEM_FILTER:            filter = new EnglishMinimalStemFilter(filter);            break;        default:            break;    }    return new TokenStreamComponents(source, filter);}
0
public TokenStream getTokenStream()
{    return tokenStream;}
0
private TokenStream createTokenStream(String content)
{    tokenStream = generateTokenStreamFromText(content, tokenizer);    tokenStream = new LowerCaseFilter(tokenStream);    if (stopSet != null) {        tokenStream = applyStopFilter(stopSet);    }    tokenStream = applyStemmer(stemFilterType);    return tokenStream;}
0
private TokenStream generateTokenStreamFromText(String content, TokenizerType tokenizerType)
{    Tokenizer tokenizer = null;    switch(tokenizerType) {        case CLASSIC:            tokenizer = new ClassicTokenizer();            break;        case STANDARD:        default:            tokenizer = new StandardTokenizer();    }    tokenizer.setReader(new StringReader(content));    tokenStream = tokenizer;    return tokenStream;}
0
private TokenStream createNGramTokenStream(String content, int mingram, int maxgram)
{    Tokenizer tokenizer = new StandardTokenizer();    tokenizer.setReader(new StringReader(content));    tokenStream = new LowerCaseFilter(tokenizer);    tokenStream = applyStemmer(stemFilterType);    ShingleFilter shingleFilter = new ShingleFilter(tokenStream, mingram, maxgram);    shingleFilter.setOutputUnigrams(false);    tokenStream = (TokenStream) shingleFilter;    return tokenStream;}
0
private TokenStream applyStopFilter(CharArraySet stopWords)
{    tokenStream = new StopFilter(tokenStream, stopWords);    return tokenStream;}
0
private TokenStream applyStemmer(StemFilterType stemFilterType)
{    switch(stemFilterType) {        case ENGLISHMINIMALSTEM_FILTER:            tokenStream = new EnglishMinimalStemFilter(tokenStream);            break;        case PORTERSTEM_FILTER:            tokenStream = new PorterStemFilter(tokenStream);            break;        default:            break;    }    return tokenStream;}
0
protected void init()
{    try {        if (LOG.isInfoEnabled()) {                    }                configfile = getConf().getResource(getConf().get("subcollections.config", DEFAULT_FILE_NAME));        InputStream input = getConf().getConfResourceAsInputStream(getConf().get("subcollections.config", DEFAULT_FILE_NAME));        parse(input);    } catch (Exception e) {        if (LOG.isWarnEnabled()) {                    }    }}
1
protected void parse(InputStream input)
{    Element collections = DomUtil.getDom(input);    if (collections != null) {        NodeList nodeList = collections.getElementsByTagName(Subcollection.TAG_COLLECTION);        if (LOG.isInfoEnabled()) {                    }        for (int i = 0; i < nodeList.getLength(); i++) {            Element scElem = (Element) nodeList.item(i);            Subcollection subCol = new Subcollection(getConf());            subCol.initialize(scElem);            collectionMap.put(subCol.name, subCol);        }    } else if (LOG.isInfoEnabled()) {            }}
1
public static CollectionManager getCollectionManager(Configuration conf)
{    String key = "collectionmanager";    ObjectCache objectCache = ObjectCache.get(conf);    CollectionManager impl = (CollectionManager) objectCache.getObject(key);    if (impl == null) {        try {            if (LOG.isInfoEnabled()) {                            }            impl = new CollectionManager(conf);            objectCache.setObject(key, impl);        } catch (Exception e) {            throw new RuntimeException("Couldn't create CollectionManager", e);        }    }    return impl;}
1
public Subcollection getSubColection(final String id)
{    return (Subcollection) collectionMap.get(id);}
0
public void deleteSubCollection(final String id) throws IOException
{    final Subcollection subCol = getSubColection(id);    if (subCol != null) {        collectionMap.remove(id);    }}
0
public Subcollection createSubCollection(final String id, final String name)
{    Subcollection subCol = null;    if (!collectionMap.containsKey(id)) {        subCol = new Subcollection(id, name, getConf());        collectionMap.put(id, subCol);    }    return subCol;}
0
public List<Subcollection> getSubCollections(final String url)
{    List<Subcollection> collections = new ArrayList<Subcollection>();    final Iterator iterator = collectionMap.values().iterator();    while (iterator.hasNext()) {        final Subcollection subCol = (Subcollection) iterator.next();        if (subCol.filter(url) != null) {            collections.add(subCol);        }    }    if (LOG.isTraceEnabled()) {        LOG.trace("subcollections:" + Arrays.toString(collections.toArray()));    }    return collections;}
0
public Collection getAll()
{    return collectionMap.values();}
0
public void save() throws IOException
{    try {        final FileOutputStream fos = new FileOutputStream(new File(configfile.getFile()));        final Document doc = new DocumentImpl();        final Element collections = doc.createElement(Subcollection.TAG_COLLECTIONS);        final Iterator iterator = collectionMap.values().iterator();        while (iterator.hasNext()) {            final Subcollection subCol = (Subcollection) iterator.next();            final Element collection = doc.createElement(Subcollection.TAG_COLLECTION);            collections.appendChild(collection);            final Element name = doc.createElement(Subcollection.TAG_NAME);            name.setNodeValue(subCol.getName());            collection.appendChild(name);            final Element whiteList = doc.createElement(Subcollection.TAG_WHITELIST);            whiteList.setNodeValue(subCol.getWhiteListString());            collection.appendChild(whiteList);            final Element blackList = doc.createElement(Subcollection.TAG_BLACKLIST);            blackList.setNodeValue(subCol.getBlackListString());            collection.appendChild(blackList);        }        DomUtil.saveDom(fos, collections);        fos.flush();        fos.close();    } catch (FileNotFoundException e) {        throw new IOException(e.toString());    }}
0
public String getName()
{    return name;}
0
public String getKey()
{    return key;}
0
public String getId()
{    return id;}
0
public List<String> getWhiteList()
{    return whiteList;}
0
public String getWhiteListString()
{    return wlString;}
0
public String getBlackListString()
{    return blString;}
0
public void setWhiteList(ArrayList<String> whiteList)
{    this.whiteList = whiteList;}
0
public String filter(String urlString)
{        Iterator<String> i = blackList.iterator();    while (i.hasNext()) {        String row = (String) i.next();        if (urlString.contains(row))            return null;    }        i = whiteList.iterator();    while (i.hasNext()) {        String row = (String) i.next();        if (urlString.contains(row))            return urlString;    }    return null;}
0
public void initialize(Element collection)
{    this.id = DOMUtil.getChildText(collection.getElementsByTagName(TAG_ID).item(0)).trim();    this.name = DOMUtil.getChildText(collection.getElementsByTagName(TAG_NAME).item(0)).trim();    this.wlString = DOMUtil.getChildText(collection.getElementsByTagName(TAG_WHITELIST).item(0)).trim();    parseList(this.whiteList, wlString);        NodeList nodeList = collection.getElementsByTagName(TAG_BLACKLIST);    if (nodeList.getLength() > 0) {        this.blString = DOMUtil.getChildText(nodeList.item(0)).trim();        parseList(this.blackList, blString);    }        nodeList = collection.getElementsByTagName(TAG_KEY);    if (nodeList.getLength() == 1) {        this.key = DOMUtil.getChildText(nodeList.item(0)).trim();    }}
0
protected void parseList(List<String> list, String text)
{    list.clear();    StringTokenizer st = new StringTokenizer(text, "\n\r");    while (st.hasMoreElements()) {        String line = (String) st.nextElement();        line = line.trim();        if (caseInsensitive) {            line = line.toLowerCase();        }        list.add(line);    }}
0
public void setBlackList(String list)
{    this.blString = list;    parseList(blackList, list);}
0
public void setWhiteList(String list)
{    this.wlString = list;    parseList(whiteList, list);}
0
public void setConf(Configuration conf)
{    this.conf = conf;    fieldName = conf.get("subcollection.default.fieldname", "subcollection");    metadataSource = conf.get("subcollection.metadata.source", "subcollection");    caseInsensitive = conf.getBoolean("subcollection.case.insensitive", false);}
0
public Configuration getConf()
{    return this.conf;}
0
private void addSubCollectionField(NutchDocument doc, String url)
{    for (Subcollection coll : CollectionManager.getCollectionManager(getConf()).getSubCollections(url)) {        if (coll.getKey() == null) {            doc.add(fieldName, coll.getName());        } else {            doc.add(coll.getKey(), coll.getName());        }    }}
0
public NutchDocument filter(NutchDocument doc, Parse parse, Text url, CrawlDatum datum, Inlinks inlinks) throws IndexingException
{        String subcollection = parse.getData().getMeta(metadataSource);    if (subcollection != null) {        subcollection = subcollection.trim();        if (subcollection.length() > 0) {            doc.add(fieldName, subcollection);            return doc;        }    }    String sUrl = url.toString();    if (caseInsensitive) {        sUrl = sUrl.toLowerCase();    }    addSubCollectionField(doc, sUrl);    return doc;}
0
public void testFilter() throws Exception
{    Subcollection sc = new Subcollection(NutchConfiguration.create());    sc.setWhiteList("www.nutch.org\nwww.apache.org");    sc.setBlackList("jpg\nwww.apache.org/zecret/");        Assert.assertEquals("http://www.apache.org/index.html", sc.filter("http://www.apache.org/index.html"));        Assert.assertEquals(null, sc.filter("http://www.apache.org/zecret/index.html"));    Assert.assertEquals(null, sc.filter("http://www.apache.org/img/image.jpg"));        Assert.assertEquals(null, sc.filter("http://www.google.com/"));}
0
public void testInput()
{    StringBuffer xml = new StringBuffer();    xml.append("<?xml version=\"1.0\" encoding=\"UTF-8\"?>");    xml.append("<!-- just a comment -->");    xml.append("<subcollections>");    xml.append("<subcollection>");    xml.append("<name>nutch collection</name>");    xml.append("<id>nutch</id>");    xml.append("<whitelist>");    xml.append("http://lucene.apache.org/nutch/\n");    xml.append("http://wiki.apache.org/nutch/\n");    xml.append("</whitelist>");    xml.append("<blacklist>");    xml.append("http://www.xxx.yyy\n");    xml.append("</blacklist>");    xml.append("</subcollection>");    xml.append("</subcollections>");    InputStream is = new ByteArrayInputStream(xml.toString().getBytes());    CollectionManager cm = new CollectionManager();    cm.parse(is);    Collection<?> c = cm.getAll();        Assert.assertEquals(1, c.size());    Subcollection collection = (Subcollection) c.toArray()[0];        Assert.assertEquals("nutch", collection.getId());        Assert.assertEquals("nutch collection", collection.getName());        Assert.assertEquals(2, collection.whiteList.size());    String wlUrl = (String) collection.whiteList.get(0);    Assert.assertEquals("http://lucene.apache.org/nutch/", wlUrl);    wlUrl = (String) collection.whiteList.get(1);    Assert.assertEquals("http://wiki.apache.org/nutch/", wlUrl);        Assert.assertEquals("http://lucene.apache.org/nutch/", collection.filter("http://lucene.apache.org/nutch/"));        Assert.assertEquals(1, collection.blackList.size());    String blUrl = (String) collection.blackList.get(0);    Assert.assertEquals("http://www.xxx.yyy", blUrl);        Assert.assertEquals(null, collection.filter("http://www.google.com/"));}
0
public NutchDocument filter(NutchDocument doc, Parse parse, Text urlText, CrawlDatum datum, Inlinks inlinks) throws IndexingException
{    try {        URL url = new URL(urlText.toString());        DomainSuffix d = URLUtil.getDomainSuffix(url);        doc.add("tld", d.getDomain());    } catch (Exception ex) {            }    return doc;}
1
public void setConf(Configuration conf)
{    this.conf = conf;}
0
public Configuration getConf()
{    return this.conf;}
0
public float indexerScore(Text url, NutchDocument doc, CrawlDatum dbDatum, CrawlDatum fetchDatum, Parse parse, Inlinks inlinks, float initScore) throws ScoringFilterException
{    NutchField tlds = doc.getField("tld");    float boost = 1.0f;    if (tlds != null) {        for (Object tld : tlds.getValues()) {            DomainSuffix entry = tldEntries.get(tld.toString());            if (entry != null)                boost *= entry.getBoost();        }    }    return initScore * boost;}
0
public CrawlDatum distributeScoreToOutlink(Text fromUrl, Text toUrl, ParseData parseData, CrawlDatum target, CrawlDatum adjust, int allCount, int validCount) throws ScoringFilterException
{    return adjust;}
0
public float generatorSortValue(Text url, CrawlDatum datum, float initSort) throws ScoringFilterException
{    return initSort;}
0
public void initialScore(Text url, CrawlDatum datum) throws ScoringFilterException
{}
0
public void injectedScore(Text url, CrawlDatum datum) throws ScoringFilterException
{}
0
public void passScoreAfterParsing(Text url, Content content, Parse parse) throws ScoringFilterException
{}
0
public void passScoreBeforeParsing(Text url, CrawlDatum datum, Content content) throws ScoringFilterException
{}
0
public void updateDbScore(Text url, CrawlDatum old, CrawlDatum datum, List<CrawlDatum> inlinked) throws ScoringFilterException
{}
0
public Configuration getConf()
{    return conf;}
0
public void setConf(Configuration conf)
{    this.conf = conf;}
0
public CrawlDatum distributeScoreToOutlinks(Text fromUrl, ParseData parseData, Collection<Entry<Text, CrawlDatum>> targets, CrawlDatum adjust, int allCount) throws ScoringFilterException
{    return adjust;}
0
protected Reader getRulesReader(Configuration conf) throws IOException
{    String stringRules = conf.get(URLFILTER_AUTOMATON_RULES);    if (stringRules != null) {        return new StringReader(stringRules);    }    String fileRules = conf.get(URLFILTER_AUTOMATON_FILE);    return conf.getConfResourceAsReader(fileRules);}
0
protected RegexRule createRule(boolean sign, String regex)
{    return new Rule(sign, regex);}
0
protected RegexRule createRule(boolean sign, String regex, String hostOrDomain)
{    return new Rule(sign, regex, hostOrDomain);}
0
public static void main(String[] args) throws IOException
{    main(new AutomatonURLFilter(), args);}
0
protected boolean match(String url)
{    return automaton.run(url);}
0
protected URLFilter getURLFilter(Reader rules)
{    try {        return new AutomatonURLFilter(rules);    } catch (IOException e) {        Assert.fail(e.toString());        return null;    }}
0
public void test()
{    test("WholeWebCrawling");    test("IntranetCrawling");    bench(50, "Benchmarks");    bench(100, "Benchmarks");    bench(200, "Benchmarks");    bench(400, "Benchmarks");    bench(800, "Benchmarks");}
0
private void readConfiguration(Reader configReader) throws IOException
{        BufferedReader reader = new BufferedReader(configReader);    String line = null;    while ((line = reader.readLine()) != null) {        if (StringUtils.isNotBlank(line) && !line.startsWith("#")) {                        domainSet.add(StringUtils.lowerCase(line.trim()));        }    }}
0
public void setConf(Configuration conf)
{    this.conf = conf;        String pluginName = "urlfilter-domain";    Extension[] extensions = PluginRepository.get(conf).getExtensionPoint(URLFilter.class.getName()).getExtensions();    for (int i = 0; i < extensions.length; i++) {        Extension extension = extensions[i];        if (extension.getDescriptor().getPluginId().equals(pluginName)) {            attributeFile = extension.getAttribute("file");            break;        }    }        if (attributeFile != null && attributeFile.trim().equals("")) {        attributeFile = null;    }    if (attributeFile != null) {        if (LOG.isInfoEnabled()) {                    }    } else {        if (LOG.isWarnEnabled()) {                    }    }        String file = conf.get("urlfilter.domain.file");    String stringRules = conf.get("urlfilter.domain.rules");    if (domainFile != null) {        file = domainFile;    } else if (attributeFile != null) {        file = attributeFile;    }    Reader reader = null;    if (stringRules != null) {                reader = new StringReader(stringRules);    } else {        reader = conf.getConfResourceAsReader(file);    }    try {        if (reader == null) {            reader = new FileReader(file);        }        readConfiguration(reader);    } catch (IOException e) {            }}
1
public Configuration getConf()
{    return this.conf;}
0
public String filter(String url)
{        if (domainSet.size() == 0)        return url;    try {                        String domain = URLUtil.getDomainName(url).toLowerCase().trim();        String host = URLUtil.getHost(url);        String suffix = null;        DomainSuffix domainSuffix = URLUtil.getDomainSuffix(url);        if (domainSuffix != null) {            suffix = domainSuffix.getDomain();        }        if (domainSet.contains(suffix) || domainSet.contains(domain) || domainSet.contains(host)) {            return url;        }                return null;    } catch (Exception e) {                        return null;    }}
1
public void testFilter() throws Exception
{    String domainFile = SAMPLES + SEPARATOR + "hosts.txt";    Configuration conf = NutchConfiguration.create();    DomainURLFilter domainFilter = new DomainURLFilter(domainFile);    domainFilter.setConf(conf);    Assert.assertNotNull(domainFilter.filter("http://lucene.apache.org"));    Assert.assertNotNull(domainFilter.filter("http://hadoop.apache.org"));    Assert.assertNotNull(domainFilter.filter("http://www.apache.org"));    Assert.assertNull(domainFilter.filter("http://www.google.com"));    Assert.assertNull(domainFilter.filter("http://mail.yahoo.com"));    Assert.assertNotNull(domainFilter.filter("http://www.foobar.net"));    Assert.assertNotNull(domainFilter.filter("http://www.foobas.net"));    Assert.assertNotNull(domainFilter.filter("http://www.yahoo.com"));    Assert.assertNotNull(domainFilter.filter("http://www.foobar.be"));    Assert.assertNull(domainFilter.filter("http://www.adobe.com"));}
0
public void testNoFilter() throws Exception
{        String domainFile = SAMPLES + SEPARATOR + "this-file-does-not-exist.txt";    Configuration conf = NutchConfiguration.create();    DomainURLFilter domainFilter = new DomainURLFilter(domainFile);    domainFilter.setConf(conf);    Assert.assertNotNull(domainFilter.filter("http://lucene.apache.org"));    Assert.assertNotNull(domainFilter.filter("http://hadoop.apache.org"));    Assert.assertNotNull(domainFilter.filter("http://www.apache.org"));    Assert.assertNotNull(domainFilter.filter("http://www.google.com"));    Assert.assertNotNull(domainFilter.filter("http://mail.yahoo.com"));    Assert.assertNotNull(domainFilter.filter("http://www.foobar.net"));    Assert.assertNotNull(domainFilter.filter("http://www.foobas.net"));    Assert.assertNotNull(domainFilter.filter("http://www.yahoo.com"));    Assert.assertNotNull(domainFilter.filter("http://www.foobar.be"));    Assert.assertNotNull(domainFilter.filter("http://www.adobe.com"));}
0
private void readConfiguration(Reader configReader) throws IOException
{        BufferedReader reader = new BufferedReader(configReader);    String line = null;    while ((line = reader.readLine()) != null) {        if (StringUtils.isNotBlank(line) && !line.startsWith("#")) {                        domainSet.add(StringUtils.lowerCase(line.trim()));        }    }}
0
public void setConf(Configuration conf)
{    this.conf = conf;        String pluginName = "urlfilter-domainblacklist";    Extension[] extensions = PluginRepository.get(conf).getExtensionPoint(URLFilter.class.getName()).getExtensions();    for (int i = 0; i < extensions.length; i++) {        Extension extension = extensions[i];        if (extension.getDescriptor().getPluginId().equals(pluginName)) {            attributeFile = extension.getAttribute("file");            break;        }    }        if (attributeFile != null && attributeFile.trim().equals("")) {        attributeFile = null;    }    if (attributeFile != null) {        if (LOG.isInfoEnabled()) {                    }    } else {        if (LOG.isWarnEnabled()) {                    }    }        String file = conf.get("urlfilter.domainblacklist.file");    String stringRules = conf.get("urlfilter.domainblacklist.rules");    if (domainFile != null) {        file = domainFile;    } else if (attributeFile != null) {        file = attributeFile;    }    Reader reader = null;    if (stringRules != null) {                reader = new StringReader(stringRules);    } else {        reader = conf.getConfResourceAsReader(file);    }    try {        if (reader == null) {            reader = new FileReader(file);        }        readConfiguration(reader);    } catch (IOException e) {            }}
1
public Configuration getConf()
{    return this.conf;}
0
public String filter(String url)
{    try {                        String domain = URLUtil.getDomainName(url).toLowerCase().trim();        String host = URLUtil.getHost(url);        String suffix = null;        DomainSuffix domainSuffix = URLUtil.getDomainSuffix(url);        if (domainSuffix != null) {            suffix = domainSuffix.getDomain();        }        if (domainSet.contains(suffix) || domainSet.contains(domain) || domainSet.contains(host)) {                        return null;        }                return url;    } catch (Exception e) {                        return null;    }}
1
public void testFilter() throws Exception
{    String domainBlacklistFile = SAMPLES + SEPARATOR + "hosts.txt";    Configuration conf = NutchConfiguration.create();    DomainBlacklistURLFilter domainBlacklistFilter = new DomainBlacklistURLFilter(domainBlacklistFile);    domainBlacklistFilter.setConf(conf);    Assert.assertNull(domainBlacklistFilter.filter("http://lucene.apache.org"));    Assert.assertNull(domainBlacklistFilter.filter("http://hadoop.apache.org"));    Assert.assertNull(domainBlacklistFilter.filter("http://www.apache.org"));    Assert.assertNotNull(domainBlacklistFilter.filter("http://www.google.com"));    Assert.assertNotNull(domainBlacklistFilter.filter("http://mail.yahoo.com"));    Assert.assertNull(domainBlacklistFilter.filter("http://www.foobar.net"));    Assert.assertNull(domainBlacklistFilter.filter("http://www.foobas.net"));    Assert.assertNull(domainBlacklistFilter.filter("http://www.yahoo.com"));    Assert.assertNull(domainBlacklistFilter.filter("http://www.foobar.be"));    Assert.assertNotNull(domainBlacklistFilter.filter("http://www.adobe.com"));}
0
public void setConf(Configuration conf)
{    this.conf = conf;    try {        reloadRules();    } catch (Exception e) {                throw new RuntimeException(e.getMessage(), e);    }}
1
public Configuration getConf()
{    return this.conf;}
0
public String filter(String url)
{    URL u;    try {        u = new URL(url);    } catch (Exception e) {                return null;    }    String hostname = u.getHost();        for (Rule rule : hostRules.get(hostname)) {        if (rule.match(u)) {            return null;        }    }        for (Rule rule : domainRules.get(hostname)) {        if (rule.match(u)) {            return null;        }    }            int start = 0;    int pos;    while ((pos = hostname.indexOf('.', start)) != -1) {        start = pos + 1;        String domain = hostname.substring(start);        for (Rule rule : domainRules.get(domain)) {            if (rule.match(u)) {                return null;            }        }    }        for (Rule rule : domainRules.get(".")) {        if (rule.match(u)) {            return null;        }    }        return url;}
1
public void reloadRules() throws IOException
{    String fileRules = conf.get(URLFILTER_FAST_FILE);    try (Reader reader = conf.getConfResourceAsReader(fileRules)) {        reloadRules(reader);    }}
0
private void reloadRules(Reader rules) throws IOException
{    domainRules.clear();    hostRules.clear();    BufferedReader reader = new BufferedReader(rules);    String current = null;    boolean host = false;    int lineno = 0;    String line;    try {        while ((line = reader.readLine()) != null) {            lineno++;            line = line.trim();            if (line.indexOf("#") != -1) {                                line = line.substring(0, line.indexOf("#")).trim();            }            if (StringUtils.isBlank(line)) {                continue;            }            if (line.startsWith("Host")) {                host = true;                current = line.split("\\s+")[1];            } else if (line.startsWith("Domain")) {                host = false;                current = line.split("\\s+")[1];            } else {                if (current == null) {                    continue;                }                Rule rule = null;                try {                    if (CATCH_ALL_RULE.matcher(line).matches()) {                        rule = DenyAllRule.getInstance();                    } else if (line.startsWith("DenyPathQuery")) {                        rule = new DenyPathQueryRule(line.split("\\s+")[1]);                    } else if (line.startsWith("DenyPath")) {                        rule = new DenyPathRule(line.split("\\s+")[1]);                    } else {                                                continue;                    }                } catch (Exception e) {                                        continue;                }                if (host) {                    LOG.trace("Adding host rule [{}] [{}]", current, rule);                    hostRules.put(current, rule);                } else {                    LOG.trace("Adding domain rule [{}] [{}]", current, rule);                    domainRules.put(current, rule);                }            }        }    } catch (IOException e) {                throw e;    }}
1
public boolean match(URL url)
{    return pattern.matcher(url.toString()).find();}
0
public String toString()
{    return pattern.toString();}
0
public boolean match(URL url)
{    String haystack = url.getPath();    return pattern.matcher(haystack).find();}
0
public static Rule getInstance()
{    return instance;}
0
public boolean match(URL url)
{    return true;}
0
public boolean match(URL url)
{    String haystack = url.getFile();    return pattern.matcher(haystack).find();}
0
protected URLFilter getURLFilter(Reader rules)
{    try {        return new FastURLFilter(rules);    } catch (IOException e) {        Assert.fail(e.toString());        return null;    }}
0
public void test()
{    test("fast-urlfilter-test.txt", "test.urls");    test("fast-urlfilter-benchmark.txt", "Benchmarks.urls");}
0
public void benchmark()
{    bench(50, "fast-urlfilter-benchmark.txt", "Benchmarks.urls");    bench(100, "fast-urlfilter-benchmark.txt", "Benchmarks.urls");    bench(200, "fast-urlfilter-benchmark.txt", "Benchmarks.urls");    bench(400, "fast-urlfilter-benchmark.txt", "Benchmarks.urls");    bench(800, "fast-urlfilter-benchmark.txt", "Benchmarks.urls");}
0
public List<Pattern> getExemptions()
{    return exemptions;}
0
public boolean filter(String fromUrl, String toUrl)
{        return this.filter(toUrl) != null;}
0
protected Reader getRulesReader(Configuration conf) throws IOException
{    String fileRules = conf.get(DB_IGNORE_EXTERNAL_EXEMPTIONS_FILE);    return conf.getConfResourceAsReader(fileRules);}
0
public static void main(String[] args)
{    if (args.length != 1) {        System.out.println("Error: Invalid Args");        System.out.println("Usage: " + ExemptionUrlFilter.class.getName() + " <url>");        return;    }    String url = args[0];    ExemptionUrlFilter instance = new ExemptionUrlFilter();    instance.setConf(NutchConfiguration.create());    System.out.println(instance.filter(null, url));}
0
public String filter(String url)
{    if (trie.shortestMatch(url) == null)        return null;    else        return url;}
0
private TrieStringMatcher readConfiguration(Reader reader) throws IOException
{    BufferedReader in = new BufferedReader(reader);    List<String> urlprefixes = new ArrayList<>();    String line;    while ((line = in.readLine()) != null) {        if (line.length() == 0)            continue;        char first = line.charAt(0);        switch(first) {            case ' ':            case '\n':            case             '#':                continue;            default:                urlprefixes.add(line);        }    }    return new PrefixStringMatcher(urlprefixes);}
0
public static void main(String[] args) throws IOException
{    PrefixURLFilter filter;    if (args.length >= 1)        filter = new PrefixURLFilter(args[0]);    else        filter = new PrefixURLFilter();    BufferedReader in = new BufferedReader(new InputStreamReader(System.in));    String line;    while ((line = in.readLine()) != null) {        String out = filter.filter(line);        if (out != null) {            System.out.println(out);        }    }}
0
public void setConf(Configuration conf)
{    this.conf = conf;    String pluginName = "urlfilter-prefix";    Extension[] extensions = PluginRepository.get(conf).getExtensionPoint(URLFilter.class.getName()).getExtensions();    for (int i = 0; i < extensions.length; i++) {        Extension extension = extensions[i];        if (extension.getDescriptor().getPluginId().equals(pluginName)) {            attributeFile = extension.getAttribute("file");            break;        }    }    if (attributeFile != null && attributeFile.trim().equals(""))        attributeFile = null;    if (attributeFile != null) {        if (LOG.isInfoEnabled()) {                    }    } else {                    }    String file = conf.get("urlfilter.prefix.file");    String stringRules = conf.get("urlfilter.prefix.rules");        if (attributeFile != null)        file = attributeFile;    Reader reader = null;    if (stringRules != null) {                reader = new StringReader(stringRules);    } else {        reader = conf.getConfResourceAsReader(file);    }    if (reader == null) {        trie = new PrefixStringMatcher(new String[0]);    } else {        try {            trie = readConfiguration(reader);        } catch (IOException e) {            if (LOG.isErrorEnabled()) {                            }                        throw new RuntimeException(e.getMessage(), e);        }    }}
1
public Configuration getConf()
{    return this.conf;}
0
public static Test suite()
{    return new TestSuite(TestPrefixURLFilter.class);}
0
public static void main(String[] args)
{    TestRunner.run(suite());}
0
public void setUp() throws IOException
{    filter = new PrefixURLFilter(prefixes);}
0
public void testModeAccept()
{    for (int i = 0; i < urls.length; i++) {        assertTrue(urlsModeAccept[i] == filter.filter(urls[i]));    }}
0
protected Reader getRulesReader(Configuration conf) throws IOException
{    String stringRules = conf.get(URLFILTER_REGEX_RULES);    if (stringRules != null) {        return new StringReader(stringRules);    }    String fileRules = conf.get(URLFILTER_REGEX_FILE);    return conf.getConfResourceAsReader(fileRules);}
0
protected RegexRule createRule(boolean sign, String regex)
{    return new Rule(sign, regex);}
0
protected RegexRule createRule(boolean sign, String regex, String hostOrDomain)
{    return new Rule(sign, regex, hostOrDomain);}
0
public static void main(String[] args) throws IOException
{    RegexURLFilter filter = new RegexURLFilter();    filter.setConf(NutchConfiguration.create());    main(filter, args);}
0
protected boolean match(String url)
{    return pattern.matcher(url).find();}
0
protected URLFilter getURLFilter(Reader rules)
{    try {        return new RegexURLFilter(rules);    } catch (IOException e) {        Assert.fail(e.toString());        return null;    }}
0
public void test()
{    test("WholeWebCrawling");    test("IntranetCrawling");    bench(50, "Benchmarks");    bench(100, "Benchmarks");    bench(200, "Benchmarks");    bench(400, "Benchmarks");    bench(800, "Benchmarks");}
0
public void test1838()
{    test("nutch1838");}
0
public String filter(String url)
{    if (url == null)        return null;    String _url;    if (ignoreCase)        _url = url.toLowerCase();    else        _url = url;    if (filterFromPath) {        try {            URL pUrl = new URL(_url);            _url = pUrl.getPath();        } catch (MalformedURLException e) {                }    }    String a = suffixes.shortestMatch(_url);    if (a == null) {        if (modeAccept)            return url;        else            return null;    } else {        if (modeAccept)            return null;        else            return url;    }}
0
public void readConfiguration(Reader reader) throws IOException
{        if (reader == null) {        if (LOG.isWarnEnabled()) {                    }        suffixes = new SuffixStringMatcher(new String[0]);        modeAccept = false;        ignoreCase = false;        return;    }    BufferedReader in = new BufferedReader(reader);    List<String> aSuffixes = new ArrayList<String>();    boolean allow = false;    boolean ignore = false;    String line;    while ((line = in.readLine()) != null) {        line = line.trim();        if (line.length() == 0)            continue;        char first = line.charAt(0);        switch(first) {            case ' ':            case '\n':            case             '#':                break;            case '-':                allow = false;                if (line.contains("P"))                    filterFromPath = true;                if (line.contains("I"))                    ignore = true;                break;            case '+':                allow = true;                if (line.contains("P"))                    filterFromPath = true;                if (line.contains("I"))                    ignore = true;                break;            default:                aSuffixes.add(line);        }    }    if (ignore) {        for (int i = 0; i < aSuffixes.size(); i++) {            aSuffixes.set(i, ((String) aSuffixes.get(i)).toLowerCase());        }    }    suffixes = new SuffixStringMatcher(aSuffixes);    modeAccept = allow;    ignoreCase = ignore;}
1
public static void main(String[] args) throws IOException
{    SuffixURLFilter filter;    if (args.length >= 1)        filter = new SuffixURLFilter(new FileReader(args[0]));    else {        filter = new SuffixURLFilter();        filter.setConf(NutchConfiguration.create());    }    BufferedReader in = new BufferedReader(new InputStreamReader(System.in));    String line;    while ((line = in.readLine()) != null) {        String out = filter.filter(line);        if (out != null) {            System.out.println("ACCEPTED " + out);        } else {            System.out.println("REJECTED " + out);        }    }}
0
public void setConf(Configuration conf)
{    this.conf = conf;    String pluginName = "urlfilter-suffix";    Extension[] extensions = PluginRepository.get(conf).getExtensionPoint(URLFilter.class.getName()).getExtensions();    for (int i = 0; i < extensions.length; i++) {        Extension extension = extensions[i];        if (extension.getDescriptor().getPluginId().equals(pluginName)) {            attributeFile = extension.getAttribute("file");            break;        }    }    if (attributeFile != null && attributeFile.trim().equals(""))        attributeFile = null;    if (attributeFile != null) {        if (LOG.isInfoEnabled()) {                    }    } else {                    }    String file = conf.get("urlfilter.suffix.file");    String stringRules = conf.get("urlfilter.suffix.rules");        if (attributeFile != null)        file = attributeFile;    Reader reader = null;    if (stringRules != null) {                reader = new StringReader(stringRules);    } else {        reader = conf.getConfResourceAsReader(file);    }    try {        readConfiguration(reader);    } catch (IOException e) {        if (LOG.isErrorEnabled()) {                    }        throw new RuntimeException(e.getMessage(), e);    }}
1
public Configuration getConf()
{    return this.conf;}
0
public boolean isModeAccept()
{    return modeAccept;}
0
public void setModeAccept(boolean modeAccept)
{    this.modeAccept = modeAccept;}
0
public boolean isIgnoreCase()
{    return ignoreCase;}
0
public void setIgnoreCase(boolean ignoreCase)
{    this.ignoreCase = ignoreCase;}
0
public void setFilterFromPath(boolean filterFromPath)
{    this.filterFromPath = filterFromPath;}
0
public void setUp() throws IOException
{    filter = new SuffixURLFilter(new StringReader(suffixes));}
0
public void testModeAccept()
{    filter.setIgnoreCase(false);    filter.setModeAccept(true);    for (int i = 0; i < urls.length; i++) {        Assert.assertTrue(urlsModeAccept[i] == filter.filter(urls[i]));    }}
0
public void testModeReject()
{    filter.setIgnoreCase(false);    filter.setModeAccept(false);    for (int i = 0; i < urls.length; i++) {        Assert.assertTrue(urlsModeReject[i] == filter.filter(urls[i]));    }}
0
public void testModeAcceptIgnoreCase()
{    filter.setIgnoreCase(true);    filter.setModeAccept(true);    for (int i = 0; i < urls.length; i++) {        Assert.assertTrue(urlsModeAcceptIgnoreCase[i] == filter.filter(urls[i]));    }}
0
public void testModeRejectIgnoreCase()
{    filter.setIgnoreCase(true);    filter.setModeAccept(false);    for (int i = 0; i < urls.length; i++) {        Assert.assertTrue(urlsModeRejectIgnoreCase[i] == filter.filter(urls[i]));    }}
0
public void testModeAcceptAndNonPathFilter()
{    filter.setModeAccept(true);    filter.setFilterFromPath(false);    for (int i = 0; i < urls.length; i++) {        Assert.assertTrue(urlsModeAcceptAndNonPathFilter[i] == filter.filter(urls[i]));    }}
0
public void testModeAcceptAndPathFilter()
{    filter.setModeAccept(true);    filter.setFilterFromPath(true);    for (int i = 0; i < urls.length; i++) {        Assert.assertTrue(urlsModeAcceptAndPathFilter[i] == filter.filter(urls[i]));    }}
0
public String filter(String urlString)
{    return isValid(urlString) ? urlString : null;}
0
public Configuration getConf()
{    return conf;}
0
public void setConf(Configuration conf)
{    this.conf = conf;}
0
private boolean isValid(String value)
{    if (value == null) {        return false;    }    Matcher matchUrlPat = URL_PATTERN.matcher(value);    if (!LEGAL_ASCII_PATTERN.matcher(value).matches()) {        return false;    }        if (!matchUrlPat.matches()) {        return false;    }    if (!isValidScheme(matchUrlPat.group(PARSE_URL_SCHEME))) {        return false;    }    if (!isValidAuthority(matchUrlPat.group(PARSE_URL_AUTHORITY))) {        return false;    }    if (!isValidPath(matchUrlPat.group(PARSE_URL_PATH))) {        return false;    }    if (!isValidQuery(matchUrlPat.group(PARSE_URL_QUERY))) {        return false;    }    return true;}
0
private boolean isValidScheme(String scheme)
{    if (scheme == null) {        return false;    }    return SCHEME_PATTERN.matcher(scheme).matches();}
0
private boolean isValidAuthority(String authority)
{    if (authority == null) {        return false;    }    Matcher authorityMatcher = AUTHORITY_PATTERN.matcher(authority);    if (!authorityMatcher.matches()) {        return false;    }    boolean ipV4Address = false;    boolean hostname = false;        String hostIP = authorityMatcher.group(PARSE_AUTHORITY_HOST_IP);    Matcher matchIPV4Pat = IP_V4_DOMAIN_PATTERN.matcher(hostIP);    ipV4Address = matchIPV4Pat.matches();    if (ipV4Address) {                for (int i = 1; i <= 4; i++) {            String ipSegment = matchIPV4Pat.group(i);            if (ipSegment == null || ipSegment.length() <= 0) {                return false;            }            try {                if (Integer.parseInt(ipSegment) > 255) {                    return false;                }            } catch (NumberFormatException e) {                return false;            }        }    } else {                hostname = DOMAIN_PATTERN.matcher(hostIP).matches();    }        if (hostname) {                        char[] chars = hostIP.toCharArray();        int size = 1;        for (int i = 0; i < chars.length; i++) {            if (chars[i] == '.') {                size++;            }        }        String[] domainSegment = new String[size];        int segCount = 0;        int segLen = 0;        Matcher atomMatcher = ATOM_PATTERN.matcher(hostIP);        while (atomMatcher.find()) {            domainSegment[segCount] = atomMatcher.group();            segLen = domainSegment[segCount].length() + 1;            hostIP = (segLen >= hostIP.length()) ? "" : hostIP.substring(segLen);            segCount++;        }        String topLevel = domainSegment[segCount - 1];        if (topLevel.length() < 2) {            return false;        }                if (!ALPHA_PATTERN.matcher(topLevel.substring(0, 1)).matches()) {            return false;        }                if (segCount < 2) {            return false;        }    }    if (!hostname && !ipV4Address) {        return false;    }    String port = authorityMatcher.group(PARSE_AUTHORITY_PORT);    if (port != null) {        if (!PORT_PATTERN.matcher(port).matches()) {            return false;        }    }    String extra = authorityMatcher.group(PARSE_AUTHORITY_EXTRA);    return isBlankOrNull(extra);}
0
private boolean isBlankOrNull(String value)
{    return ((value == null) || (value.trim().length() == 0));}
0
private boolean isValidPath(String path)
{    if (path == null) {        return false;    }    if (!PATH_PATTERN.matcher(path).matches()) {        return false;    }    int slash2Count = countToken("//", path);    int slashCount = countToken("/", path);    int dot2Count = countToken("..", path);    return (dot2Count <= 0) || ((slashCount - slash2Count - 1) > dot2Count);}
0
private boolean isValidQuery(String query)
{    if (query == null) {        return true;    }    return QUERY_PATTERN.matcher(query).matches();}
0
private int countToken(String token, String target)
{    int tokenIndex = 0;    int count = 0;    while (tokenIndex != -1) {        tokenIndex = target.indexOf(token, tokenIndex);        if (tokenIndex > -1) {            tokenIndex++;            count++;        }    }    return count;}
0
public void testFilter()
{    UrlValidator url_validator = new UrlValidator();    Assert.assertNotNull(url_validator);    Assert.assertNull("Filtering on a null object should return null", url_validator.filter(null));    Assert.assertNull("Invalid url: example.com/file[/].html", url_validator.filter("example.com/file[/].html"));    Assert.assertNull("Invalid url: http://www.example.com/space here.html", url_validator.filter("http://www.example.com/space here.html"));    Assert.assertNull("Invalid url: /main.html", url_validator.filter("/main.html"));    Assert.assertNull("Invalid url: www.example.com/main.html", url_validator.filter("www.example.com/main.html"));    Assert.assertNull("Invalid url: ftp:www.example.com/main.html", url_validator.filter("ftp:www.example.com/main.html"));    Assert.assertNull("Inalid url: http://999.000.456.32/nutch/trunk/README.txt", url_validator.filter("http://999.000.456.32/nutch/trunk/README.txt"));    Assert.assertNull("Invalid url: http://www.example.com/ma|in\\toc.html", url_validator.filter(" http://www.example.com/ma|in\\toc.html"));    Assert.assertNotNull("Valid url: https://issues.apache.org/jira/NUTCH-1127", url_validator.filter("https://issues.apache.org/jira/NUTCH-1127"));    Assert.assertNotNull("Valid url: http://domain.tld/function.cgi?url=http://fonzi.com/&amp;name=Fonzi&amp;mood=happy&amp;coat=leather", url_validator.filter("http://domain.tld/function.cgi?url=http://fonzi.com/&amp;name=Fonzi&amp;mood=happy&amp;coat=leather"));    Assert.assertNotNull("Valid url: http://validator.w3.org/feed/check.cgi?url=http%3A%2F%2Ffeeds.feedburner.com%2Fperishablepress", url_validator.filter("http://validator.w3.org/feed/check.cgi?url=http%3A%2F%2Ffeeds.feedburner.com%2Fperishablepress"));    Assert.assertNotNull("Valid url: ftp://alfa.bravo.pi/foo/bar/plan.pdf", url_validator.filter("ftp://alfa.bravo.pi/mike/check/plan.pdf"));}
0
public NutchDocument filter(NutchDocument doc, Parse parse, Text url, CrawlDatum datum, Inlinks inlinks) throws IndexingException
{    if (conf != null)        this.setConf(conf);    if (urlMetaTags == null || doc == null)        return doc;    for (String metatag : urlMetaTags) {        Text metadata = (Text) datum.getMetaData().get(new Text(metatag));        if (metadata != null)            doc.add(metatag, metadata.toString());    }    return doc;}
0
public Configuration getConf()
{    return conf;}
0
public void setConf(Configuration conf)
{    this.conf = conf;    if (conf == null)        return;    urlMetaTags = conf.getStrings(CONF_PROPERTY);}
0
public CrawlDatum distributeScoreToOutlinks(Text fromUrl, ParseData parseData, Collection<Entry<Text, CrawlDatum>> targets, CrawlDatum adjust, int allCount) throws ScoringFilterException
{    if (urlMetaTags == null || targets == null || parseData == null)        return adjust;    Iterator<Entry<Text, CrawlDatum>> targetIterator = targets.iterator();    while (targetIterator.hasNext()) {        Entry<Text, CrawlDatum> nextTarget = targetIterator.next();        for (String metatag : urlMetaTags) {            String metaFromParse = parseData.getMeta(metatag);            if (metaFromParse == null)                continue;            nextTarget.getValue().getMetaData().put(new Text(metatag), new Text(metaFromParse));        }    }    return adjust;}
0
public void passScoreBeforeParsing(Text url, CrawlDatum datum, Content content)
{    if (urlMetaTags == null || content == null || datum == null)        return;    for (String metatag : urlMetaTags) {        Text metaFromDatum = (Text) datum.getMetaData().get(new Text(metatag));        if (metaFromDatum == null)            continue;        content.getMetadata().set(metatag, metaFromDatum.toString());    }}
0
public void passScoreAfterParsing(Text url, Content content, Parse parse)
{    if (urlMetaTags == null || content == null || parse == null)        return;    for (String metatag : urlMetaTags) {        String metaFromContent = content.getMetadata().get(metatag);        if (metaFromContent == null)            continue;        parse.getData().getParseMeta().set(metatag, metaFromContent);    }}
0
public float generatorSortValue(Text url, CrawlDatum datum, float initSort) throws ScoringFilterException
{    return initSort;}
0
public float indexerScore(Text url, NutchDocument doc, CrawlDatum dbDatum, CrawlDatum fetchDatum, Parse parse, Inlinks inlinks, float initScore) throws ScoringFilterException
{    return initScore;}
0
public void initialScore(Text url, CrawlDatum datum) throws ScoringFilterException
{    return;}
0
public void injectedScore(Text url, CrawlDatum datum) throws ScoringFilterException
{    return;}
0
public void updateDbScore(Text url, CrawlDatum old, CrawlDatum datum, List<CrawlDatum> inlinked) throws ScoringFilterException
{    return;}
0
public void setConf(Configuration conf)
{    super.setConf(conf);    if (conf == null)        return;    urlMetaTags = conf.getStrings(CONF_PROPERTY);}
0
public Configuration getConf()
{    return conf;}
0
protected String normalizeHashedFragment(String urlString) throws MalformedURLException
{    URL u = new URL(urlString);    int pos = urlString.indexOf(AJAX_URL_PART);    StringBuilder sb = new StringBuilder(urlString.substring(0, pos));        String escapedFragment = escape(urlString.substring(pos + AJAX_URL_PART.length()));        if (u.getQuery() == null) {        sb.append("?");    } else {        sb.append("&");    }        sb.append(ESCAPED_URL_PART);    sb.append(escapedFragment);    return sb.toString();}
0
protected String normalizeEscapedFragment(String urlString) throws MalformedURLException
{    URL u = new URL(urlString);    StringBuilder sb = new StringBuilder();        sb.append(u.getProtocol());    sb.append("://");    sb.append(u.getHost());    if (u.getPort() != -1) {        sb.append(":");        sb.append(u.getPort());    }    sb.append(u.getPath());        String queryString = u.getQuery();        int ampPos = queryString.indexOf("&");    String keyValuePair = null;        if (ampPos == -1) {        keyValuePair = queryString;        queryString = "";    } else {                keyValuePair = queryString.substring(ampPos + 1);                queryString = queryString.replaceFirst("&" + keyValuePair, "");    }        keyValuePair = keyValuePair.replaceFirst(ESCAPED_URL_PART, "");        String unescapedFragment = unescape(keyValuePair);        if (queryString.length() > 0) {        sb.append("?");        sb.append(queryString);    }        sb.append("#!");    sb.append(unescapedFragment);    return sb.toString();}
0
protected String unescape(String fragmentPart)
{    try {        fragmentPart = URLDecoder.decode(fragmentPart, "UTF-8");    } catch (Exception e) {        }    return fragmentPart;}
0
protected String escape(String fragmentPart)
{    String hex = null;    StringBuilder sb = new StringBuilder(fragmentPart.length());    for (byte b : fragmentPart.getBytes(utf8)) {        if (b < 33) {            sb.append('%');            hex = Integer.toHexString(b & 0xFF).toUpperCase();                        if (hex.length() % 2 != 0) {                sb.append('0');            }            sb.append(hex);        } else if (b == 35) {            sb.append("%23");        } else if (b == 37) {            sb.append("%25");        } else if (b == 38) {            sb.append("%26");        } else if (b == 43) {            sb.append("%2B");        } else {            sb.append((char) b);        }    }    return sb.toString();}
0
public void setConf(Configuration conf)
{    this.conf = conf;}
0
public Configuration getConf()
{    return this.conf;}
0
public void testNormalizer() throws Exception
{        normalizeTest("http://example.org/#!k=v", "http://example.org/?_escaped_fragment_=k=v");        normalizeTest("http://example.org/#!k=v&something=is wrong", "http://example.org/?_escaped_fragment_=k=v%26something=is%20wrong");        normalizeTest("http://example.org/path.html?queryparam=queryvalue#!key1=value1&key2=value2", "http://example.org/path.html?queryparam=queryvalue&_escaped_fragment_=key1=value1%26key2=value2");}
0
public void testNormalizerWhenIndexing() throws Exception
{        normalizeTest("http://example.org/?_escaped_fragment_=key=value", "http://example.org/#!key=value", URLNormalizers.SCOPE_INDEXER);    normalizeTest("http://example.org/?key=value&_escaped_fragment_=key=value", "http://example.org/?key=value#!key=value", URLNormalizers.SCOPE_INDEXER);    normalizeTest("http://example.org/page.html?key=value&_escaped_fragment_=key=value%26something=is%20wrong", "http://example.org/page.html?key=value#!key=value&something=is wrong", URLNormalizers.SCOPE_INDEXER);}
0
private void normalizeTest(String weird, String normal) throws Exception
{    assertEquals(normal, normalizer.normalize(weird, URLNormalizers.SCOPE_DEFAULT));}
0
private void normalizeTest(String weird, String normal, String scope) throws Exception
{    assertEquals(normal, normalizer.normalize(weird, scope));}
0
public static void main(String[] args) throws Exception
{    new TestAjaxURLNormalizer("test").testNormalizer();}
0
private static boolean isAlphaNumeric(int c)
{    return (0x41 <= c && c <= 0x5A) || (0x61 <= c && c <= 0x7A) || (0x30 <= c && c <= 0x39);}
0
private static boolean isHexCharacter(int c)
{    return (0x41 <= c && c <= 0x46) || (0x61 <= c && c <= 0x66) || (0x30 <= c && c <= 0x39);}
0
public String normalize(String urlString, String scope) throws MalformedURLException
{    if (    "".equals(urlString))        return urlString;        urlString = urlString.trim();    URL url = new URL(urlString);    String protocol = url.getProtocol();    String host = url.getHost();    int port = url.getPort();    String file = url.getFile();    boolean changed = false;    boolean normalizePath = false;    if (    !urlString.startsWith(protocol))        changed = true;    if ("http".equals(protocol) || "https".equals(protocol) || "ftp".equals(protocol)) {        if (host != null && url.getAuthority() != null) {                        String newHost = host.toLowerCase(Locale.ROOT);            if (!host.equals(newHost)) {                host = newHost;                changed = true;            } else if (!url.getAuthority().equals(newHost)) {                                                changed = true;            }        } else {                        changed = true;        }        if (port == url.getDefaultPort()) {                                    port = -1;            changed = true;        }        normalizePath = true;        if (file == null || "".equals(file)) {            file = "/";            changed = true;                        normalizePath = false;        } else if (!file.startsWith("/")) {            file = "/" + file;            changed = true;                        normalizePath = false;        }        if (url.getRef() != null) {                        changed = true;        }    } else if (protocol.equals("file")) {        normalizePath = true;    }        String file2 = unescapePath(file);    file2 = escapePath(file2);    if (!file.equals(file2)) {        changed = true;        file = file2;    }    if (normalizePath) {                if (changed) {            url = new URL(protocol, host, port, file);        }        file2 = getFileWithNormalizedPath(url);        if (!file.equals(file2)) {            changed = true;            file = file2;        }    }    if (changed) {        url = new URL(protocol, host, port, file);        urlString = url.toString();    }    return urlString;}
0
private String getFileWithNormalizedPath(URL url) throws MalformedURLException
{    String file;    if (hasNormalizablePathPattern.matcher(url.getPath()).find()) {                try {            file = url.toURI().normalize().toURL().getFile();                                    int start = 0;            while (file.startsWith("/..", start) && ((start + 3) == file.length() || file.charAt(3) == '/')) {                start += 3;            }            if (start > 0) {                file = file.substring(start);            }        } catch (URISyntaxException e) {            file = url.getFile();        }    } else {        file = url.getFile();    }        if (file.isEmpty()) {        file = "/";    } else if (!file.startsWith("/")) {        file = "/" + file;    }    return file;}
0
private String unescapePath(String path)
{    StringBuilder sb = new StringBuilder();    Matcher matcher = unescapeRulePattern.matcher(path);    int end = -1;    int letter;        while (matcher.find()) {                sb.append(path.substring(end + 1, matcher.start()));                letter = Integer.valueOf(matcher.group().substring(1), 16);        if (letter < 128 && unescapedCharacters[letter]) {                        sb.append(Character.valueOf((char) letter));        } else {                        sb.append(matcher.group().toUpperCase(Locale.ROOT));        }        end = matcher.start() + 2;    }    letter = path.length();        if (end <= letter - 1) {        sb.append(path.substring(end + 1, letter));    }        return sb.toString();}
0
private String escapePath(String path)
{    StringBuilder sb = new StringBuilder(path.length());        byte[] bytes = path.getBytes(utf8);    for (int i = 0; i < bytes.length; i++) {        byte b = bytes[i];                if (b < 0 || escapedCharacters[b]) {                        sb.append('%');                        String hex = Integer.toHexString(b & 0xFF).toUpperCase(Locale.ROOT);                        if (hex.length() % 2 != 0) {                sb.append('0');                sb.append(hex);            } else {                                sb.append(hex);            }        } else if (b == 0x25) {                        if ((i + 2) >= bytes.length) {                                sb.append("%25");            } else {                byte e1 = bytes[i + 1];                byte e2 = bytes[i + 2];                if (isHexCharacter(e1) && isHexCharacter(e2)) {                                        i += 2;                    sb.append((char) b);                    sb.append((char) e1);                    sb.append((char) e2);                } else {                    sb.append("%25");                }            }        } else {                        sb.append((char) b);        }    }    return sb.toString();}
0
public static void main(String[] args) throws IOException
{    BasicURLNormalizer normalizer = new BasicURLNormalizer();    normalizer.setConf(NutchConfiguration.create());    String scope = URLNormalizers.SCOPE_DEFAULT;    if (args.length >= 1) {        scope = args[0];        System.out.println("Scope: " + scope);    }    String line, normUrl;    BufferedReader in = new BufferedReader(new InputStreamReader(System.in, utf8));    while ((line = in.readLine()) != null) {        try {            normUrl = normalizer.normalize(line, scope);            System.out.println(normUrl);        } catch (MalformedURLException e) {            System.out.println("failed: " + line);        }    }    System.exit(0);}
0
public void testNUTCH1098() throws Exception
{        normalizeTest("http://foo.com/%66oo.html", "http://foo.com/foo.html");        normalizeTest("http://foo.com/%66oo.htm%6c", "http://foo.com/foo.html");    normalizeTest("http://foo.com/%66oo.ht%6dl", "http://foo.com/foo.html");        normalizeTest("http://foo.com/%66oo.ht%6d%6c", "http://foo.com/foo.html");        normalizeTest("http://foo.com/%66oo.htm%C0", "http://foo.com/foo.htm%C0");        normalizeTest("http://foo.com/%66oo.htm%1A", "http://foo.com/foo.htm%1A");        normalizeTest("http://foo.com/%66oo.htm%c0", "http://foo.com/foo.htm%C0");        normalizeTest("http://foo.com/you%20too.html", "http://foo.com/you%20too.html");        normalizeTest("http://foo.com/you too.html", "http://foo.com/you%20too.html");        normalizeTest("http://foo.com/file.html%23cz", "http://foo.com/file.html%23cz");        normalizeTest("http://foo.com/fast/dir%2fcz", "http://foo.com/fast/dir%2Fcz");        normalizeTest("http://foo.com/\u001a!", "http://foo.com/%1A!");        normalizeTest("http://foo.com/\u0001!", "http://foo.com/%01!");        normalizeTest("http://mydomain.com/en Espa\u00F1ol.aspx", "http://mydomain.com/en%20Espa%C3%B1ol.aspx");}
0
public void testNUTCH2064() throws Exception
{        normalizeTest("http://x.com/s?q=a%26b&m=10", "http://x.com/s?q=a%26b&m=10");    normalizeTest("http://x.com/show?http%3A%2F%2Fx.com%2Fb", "http://x.com/show?http%3A%2F%2Fx.com%2Fb");    normalizeTest("http://google.com/search?q=c%2B%2B", "http://google.com/search?q=c%2B%2B");        normalizeTest("http://x.com/s?q=a+b", "http://x.com/s?q=a+b");                normalizeTest("http://b\u00fccher.de/", "http://b\u00fccher.de/");        normalizeTest("http://x.com/./a/../%66.html", "http://x.com/f.html");        normalizeTest("http://x.com/?x[y]=1", "http://x.com/?x%5By%5D=1");        normalizeTest("http://x.com/foo\u0080", "http://x.com/foo%C2%80");    normalizeTest("http://x.com/foo%c2%80", "http://x.com/foo%C2%80");}
0
public void testNormalizer() throws Exception
{        normalizeTest(" http://foo.com/ ", "http://foo.com/");        normalizeTest("HTTP://foo.com/", "http://foo.com/");        normalizeTest("http://Foo.Com/index.html", "http://foo.com/index.html");    normalizeTest("http://Foo.Com/index.html", "http://foo.com/index.html");        normalizeTest("http://foo.com:80/index.html", "http://foo.com/index.html");    normalizeTest("http://foo.com:81/", "http://foo.com:81/");        normalizeTest("http://example.com:/", "http://example.com/");    normalizeTest("https://example.com:/foobar.html", "https://example.com/foobar.html");        normalizeTest("http://foo.com", "http://foo.com/");        normalizeTest("http://foo.com/foo.html#ref", "http://foo.com/foo.html");                normalizeTest("http://foo.com/..", "http://foo.com/");    normalizeTest("http://foo.com/aa/./foo.html", "http://foo.com/aa/foo.html");    normalizeTest("http://foo.com/aa/../", "http://foo.com/");    normalizeTest("http://foo.com/aa/bb/../", "http://foo.com/aa/");    normalizeTest("http://foo.com/aa/..", "http://foo.com/");    normalizeTest("http://foo.com/aa/bb/cc/../../foo.html", "http://foo.com/aa/foo.html");    normalizeTest("http://foo.com/aa/bb/../cc/dd/../ee/foo.html", "http://foo.com/aa/cc/ee/foo.html");    normalizeTest("http://foo.com/../foo.html", "http://foo.com/foo.html");    normalizeTest("http://foo.com/../../foo.html", "http://foo.com/foo.html");    normalizeTest("http://foo.com/../aa/../foo.html", "http://foo.com/foo.html");    normalizeTest("http://foo.com/aa/../../foo.html", "http://foo.com/foo.html");    normalizeTest("http://foo.com/aa/../bb/../foo.html/../../", "http://foo.com/");    normalizeTest("http://foo.com/../aa/foo.html", "http://foo.com/aa/foo.html");    normalizeTest("http://foo.com/../aa/../foo.html", "http://foo.com/foo.html");    normalizeTest("http://foo.com/a..a/foo.html", "http://foo.com/a..a/foo.html");    normalizeTest("http://foo.com/a..a/../foo.html", "http://foo.com/foo.html");    normalizeTest("http://foo.com/foo.foo/../foo.html", "http://foo.com/foo.html");    normalizeTest("http://foo.com//aa/bb/foo.html", "http://foo.com/aa/bb/foo.html");    normalizeTest("http://foo.com/aa//bb/foo.html", "http://foo.com/aa/bb/foo.html");    normalizeTest("http://foo.com/aa/bb//foo.html", "http://foo.com/aa/bb/foo.html");    normalizeTest("http://foo.com//aa//bb//foo.html", "http://foo.com/aa/bb/foo.html");    normalizeTest("http://foo.com////aa////bb////foo.html", "http://foo.com/aa/bb/foo.html");    normalizeTest("http://foo.com/aa?referer=http://bar.com", "http://foo.com/aa?referer=http://bar.com");        normalizeTest("file:///foo/bar.txt", "file:///foo/bar.txt");    normalizeTest("ftp:/", "ftp:/");    normalizeTest("http:", "http:/");    normalizeTest("http:////", "http:/");    normalizeTest("http:///////", "http:/");        normalizeTest("http://example.com?a=1", "http://example.com/?a=1");        normalizeTest("http://www.example.com/a/c/../b/search?q=foobar|", "http://www.example.com/a/b/search?q=foobar%7C");    normalizeTest("http://www.example.com/a/c/../b/search?q=foobar%", "http://www.example.com/a/b/search?q=foobar%25");    normalizeTest("http://www.example.com/a/c/../b/search?q=foobar\"", "http://www.example.com/a/b/search?q=foobar%22");    normalizeTest("http://www.example.com/a/c/../b/search?q=foobar^", "http://www.example.com/a/b/search?q=foobar%5E");    normalizeTest("http://www.example.com/a/c/../b/search?q=foobar<", "http://www.example.com/a/b/search?q=foobar%3C");    normalizeTest("http://www.example.com/a/c/../b/search?q=foobar>", "http://www.example.com/a/b/search?q=foobar%3E");    normalizeTest("http://www.example.com/a/c/../b/search?q=foobar`", "http://www.example.com/a/b/search?q=foobar%60");        normalizeTest("http://www.example.com/p%zz%77%v", "http://www.example.com/p%25zzw%25v");        normalizeTest("http://www.example.com/search?q=foobar%", "http://www.example.com/search?q=foobar%25");    normalizeTest("http://www.example.com/search?q=foobar%2", "http://www.example.com/search?q=foobar%252");    normalizeTest("http://www.example.com/search?q=foobar%25", "http://www.example.com/search?q=foobar%25");    normalizeTest("http://www.example.com/search?q=foobar%252", "http://www.example.com/search?q=foobar%252");        normalizeTest("file:/var/www/html/foo/../bar/index.html", "file:/var/www/html/bar/index.html");    normalizeTest("file:/var/www/html/////./bar/index.html", "file:/var/www/html/bar/index.html");}
0
public void testCurlyBraces() throws Exception
{        normalizeTest("http://foo.com/{{stuff}} ", "http://foo.com/%7B%7Bstuff%7D%7D");}
0
private void normalizeTest(String weird, String normal) throws Exception
{    Assert.assertEquals("normalizing: " + weird, normal, normalizer.normalize(weird, URLNormalizers.SCOPE_DEFAULT));    try {        (new URL(normal)).toURI();    } catch (MalformedURLException | URISyntaxException e) {        Assert.fail("Output of normalization fails to validate as URL or URI: " + e.getMessage());    }}
0
public static void main(String[] args) throws Exception
{    new TestBasicURLNormalizer().testNormalizer();}
0
private synchronized void readConfiguration(Reader configReader) throws IOException
{    if (hostsMap.size() > 0) {        return;    }    BufferedReader reader = new BufferedReader(configReader);    String line, host, target;    int delimiterIndex;    while ((line = reader.readLine()) != null) {        if (StringUtils.isNotBlank(line) && !line.startsWith("#")) {            line = line.trim();            delimiterIndex = line.indexOf(" ");            host = line.substring(0, delimiterIndex);            target = line.substring(delimiterIndex + 1);            hostsMap.put(host, target);        }    }}
0
public Configuration getConf()
{    return conf;}
0
public void setConf(Configuration conf)
{    this.conf = conf;        String pluginName = "urlnormalizer-host";    Extension[] extensions = PluginRepository.get(conf).getExtensionPoint(URLNormalizer.class.getName()).getExtensions();    for (int i = 0; i < extensions.length; i++) {        Extension extension = extensions[i];        if (extension.getDescriptor().getPluginId().equals(pluginName)) {            attributeFile = extension.getAttribute("file");            break;        }    }        if (attributeFile != null && attributeFile.trim().equals("")) {        attributeFile = null;    }    if (attributeFile != null) {        if (LOG.isInfoEnabled()) {                    }    } else {        if (LOG.isWarnEnabled()) {                    }    }        String file = conf.get("urlnormalizer.hosts.file");    String stringRules = conf.get("urlnormalizer.hosts.rules");    if (hostsFile != null) {        file = hostsFile;    } else if (attributeFile != null) {        file = attributeFile;    }    Reader reader = null;    if (stringRules != null) {                reader = new StringReader(stringRules);    } else {        reader = conf.getConfResourceAsReader(file);    }    try {        if (reader == null) {            reader = new FileReader(file);        }        readConfiguration(reader);    } catch (IOException e) {            }}
1
public String normalize(String urlString, String scope) throws MalformedURLException
{    String host = new URL(urlString).getHost();        if (hostsMap.containsKey(host)) {        return replaceHost(urlString, host, hostsMap.get(host));    }        String[] hostParts = host.split("\\.");        StringBuilder hostBuffer = new StringBuilder();        String wildCardHost = new String();        hostBuffer.append(hostParts[hostParts.length - 1]);    for (int i = hostParts.length - 2; i > 0; i--) {                hostBuffer.insert(0, hostParts[i] + ".");                wildCardHost = "*." + hostBuffer.toString();                if (hostsMap.containsKey(wildCardHost)) {                        return replaceHost(urlString, host, hostsMap.get(wildCardHost));        }    }    return urlString;}
0
protected String replaceHost(String urlString, String host, String target)
{    int hostIndex = urlString.indexOf(host);    StringBuilder buffer = new StringBuilder();    buffer.append(urlString.substring(0, hostIndex));    buffer.append(target);    buffer.append(urlString.substring(hostIndex + host.length()));    return buffer.toString();}
0
public void testHostURLNormalizer() throws Exception
{    Configuration conf = NutchConfiguration.create();    String hostsFile = SAMPLES + SEPARATOR + "hosts.txt";    HostURLNormalizer normalizer = new HostURLNormalizer(hostsFile);    normalizer.setConf(conf);        Assert.assertEquals("http://www.example.org/page.html", normalizer.normalize("http://example.org/page.html", URLNormalizers.SCOPE_DEFAULT));        Assert.assertEquals("http://example.net/path/to/something.html", normalizer.normalize("http://www.example.net/path/to/something.html", URLNormalizers.SCOPE_DEFAULT));        Assert.assertEquals("http://example.com/?does=it&still=work", normalizer.normalize("http://example.com/?does=it&still=work", URLNormalizers.SCOPE_DEFAULT));    Assert.assertEquals("http://example.com/buh", normalizer.normalize("http://http.www.example.com/buh", URLNormalizers.SCOPE_DEFAULT));    Assert.assertEquals("http://example.com/blaat", normalizer.normalize("http://whatever.example.com/blaat", URLNormalizers.SCOPE_DEFAULT));}
0
public String normalize(String urlString, String scope) throws MalformedURLException
{    return urlString;}
0
public Configuration getConf()
{    return conf;}
0
public void setConf(Configuration conf)
{    this.conf = conf;}
0
public void testPassURLNormalizer()
{    Configuration conf = NutchConfiguration.create();    PassURLNormalizer normalizer = new PassURLNormalizer();    normalizer.setConf(conf);    String url = "http://www.example.com/test/..//";    String result = null;    try {        result = normalizer.normalize(url, URLNormalizers.SCOPE_DEFAULT);    } catch (MalformedURLException mue) {        Assert.fail(mue.toString());    }    Assert.assertEquals(url, result);}
0
private synchronized void readConfiguration(Reader configReader) throws IOException
{    if (protocolsMap.size() > 0) {        return;    }    BufferedReader reader = new BufferedReader(configReader);    String line, host;    String protocol;    int delimiterIndex;    while ((line = reader.readLine()) != null) {        if (StringUtils.isNotBlank(line) && !line.startsWith("#")) {            line = line.trim();            delimiterIndex = line.indexOf(" ");                        if (delimiterIndex == -1) {                delimiterIndex = line.indexOf("\t");            }            host = line.substring(0, delimiterIndex);            protocol = line.substring(delimiterIndex + 1).trim();            protocolsMap.put(host, protocol);        }    }}
0
public Configuration getConf()
{    return conf;}
0
public void setConf(Configuration conf)
{    this.conf = conf;        String pluginName = "urlnormalizer-protocol";    Extension[] extensions = PluginRepository.get(conf).getExtensionPoint(URLNormalizer.class.getName()).getExtensions();    for (int i = 0; i < extensions.length; i++) {        Extension extension = extensions[i];        if (extension.getDescriptor().getPluginId().equals(pluginName)) {            attributeFile = extension.getAttribute("file");            break;        }    }        if (attributeFile != null && attributeFile.trim().equals("")) {        attributeFile = null;    }    if (attributeFile != null) {        if (LOG.isInfoEnabled()) {                    }    } else {        if (LOG.isWarnEnabled()) {                    }    }        String file = conf.get("urlnormalizer.protocols.file");    String stringRules = conf.get("urlnormalizer.protocols.rules");    if (protocolsFile != null) {        file = protocolsFile;    } else if (attributeFile != null) {        file = attributeFile;    }    Reader reader = null;    if (stringRules != null) {                reader = new StringReader(stringRules);    } else {        reader = conf.getConfResourceAsReader(file);    }    try {        if (reader == null) {            reader = new FileReader(file);        }        readConfiguration(reader);    } catch (IOException e) {            }}
1
public String normalize(String url, String scope) throws MalformedURLException
{    return normalize(url, null, scope);}
0
public String normalize(String url, CrawlDatum crawlDatum, String scope) throws MalformedURLException
{        URL u = new URL(url);        String host = u.getHost();        if (protocolsMap.containsKey(host)) {        String protocol = u.getProtocol();        String requiredProtocol = protocolsMap.get(host);                if (!protocol.equals(requiredProtocol)) {                        StringBuilder buffer = new StringBuilder(requiredProtocol);            buffer.append(PROTOCOL_DELIMITER);            buffer.append(host);            buffer.append(u.getPath());            String queryString = u.getQuery();            if (queryString != null) {                buffer.append(QUESTION_MARK);                buffer.append(queryString);            }            url = buffer.toString();        }    }    return url;}
0
public void testProtocolURLNormalizer() throws Exception
{    Configuration conf = NutchConfiguration.create();    String protocolsFile = SAMPLES + SEPARATOR + "protocols.txt";    ProtocolURLNormalizer normalizer = new ProtocolURLNormalizer(protocolsFile);    normalizer.setConf(conf);        assertEquals("http://example.org/", normalizer.normalize("https://example.org/", URLNormalizers.SCOPE_DEFAULT));    assertEquals("http://example.net/", normalizer.normalize("https://example.net/", URLNormalizers.SCOPE_DEFAULT));        assertEquals("http://example.org/", normalizer.normalize("https://example.org/", URLNormalizers.SCOPE_DEFAULT));    assertEquals("http://example.net/", normalizer.normalize("https://example.net/", URLNormalizers.SCOPE_DEFAULT));        assertEquals("https://example.io/", normalizer.normalize("https://example.io/", URLNormalizers.SCOPE_DEFAULT));    assertEquals("https://example.nl/", normalizer.normalize("https://example.nl/", URLNormalizers.SCOPE_DEFAULT));        assertEquals("https://example.io/", normalizer.normalize("http://example.io/", URLNormalizers.SCOPE_DEFAULT));    assertEquals("https://example.nl/", normalizer.normalize("http://example.nl/", URLNormalizers.SCOPE_DEFAULT));}
0
public Configuration getConf()
{    return conf;}
0
public void setConf(Configuration conf)
{    this.conf = conf;}
0
public String normalize(String urlString, String scope) throws MalformedURLException
{    URL url = new URL(urlString);    String queryString = url.getQuery();    if (queryString == null) {        return urlString;    }    List<String> queryStringParts = Arrays.asList(queryString.split("&"));    Collections.sort(queryStringParts);    StringBuilder sb = new StringBuilder();    sb.append(url.getProtocol());    sb.append("://");    sb.append(url.getHost());    if (url.getPort() > -1) {        sb.append(":");        sb.append(url.getPort());    }    sb.append(url.getPath());    sb.append("?");    sb.append(StringUtils.join(queryStringParts, "&"));    if (url.getRef() != null) {        sb.append("#");        sb.append(url.getRef());    }    return sb.toString();}
0
public void testQuerystringURLNormalizer() throws Exception
{    Configuration conf = NutchConfiguration.create();    QuerystringURLNormalizer normalizer = new QuerystringURLNormalizer();    normalizer.setConf(conf);    assertEquals("http://example.com/?a=b&c=d", normalizer.normalize("http://example.com/?c=d&a=b", URLNormalizers.SCOPE_DEFAULT));    assertEquals("http://example.com/a/b/c", normalizer.normalize("http://example.com/a/b/c", URLNormalizers.SCOPE_DEFAULT));    assertEquals("http://example.com:1234/a/b/c", normalizer.normalize("http://example.com:1234/a/b/c", URLNormalizers.SCOPE_DEFAULT));    assertEquals("http://example.com:1234/a/b/c#ref", normalizer.normalize("http://example.com:1234/a/b/c#ref", URLNormalizers.SCOPE_DEFAULT));    assertEquals("http://example.com:1234/a/b/c?a=b&c=d#ref", normalizer.normalize("http://example.com:1234/a/b/c?c=d&a=b#ref", URLNormalizers.SCOPE_DEFAULT));    assertEquals("http://example.com/?a=b&a=c&c=d", normalizer.normalize("http://example.com/?c=d&a=b&a=c", URLNormalizers.SCOPE_DEFAULT));}
0
protected java.util.HashMap<String, java.util.List<Rule>> initialValue()
{    return new HashMap<String, List<Rule>>();}
0
public HashMap<String, List<Rule>> getScopedRules()
{    return scopedRulesThreadLocal.get();}
0
public void setConf(Configuration conf)
{    super.setConf(conf);    if (conf == null)        return;        String filename = getConf().get("urlnormalizer.regex.file");    String stringRules = getConf().get("urlnormalizer.regex.rules");    Reader reader = null;    if (stringRules != null) {        reader = new StringReader(stringRules);    } else {        reader = getConf().getConfResourceAsReader(filename);    }    List<Rule> rules = null;    if (reader == null) {                rules = EMPTY_RULES;    } else {        try {            rules = readConfiguration(reader);        } catch (Exception e) {                        rules = EMPTY_RULES;        }    }    defaultRules = rules;}
1
 void setConfiguration(Reader reader, String scope)
{    List<Rule> rules = readConfiguration(reader);    getScopedRules().put(scope, rules);    }
1
public String regexNormalize(String urlString, String scope)
{    HashMap<String, List<Rule>> scopedRules = getScopedRules();    List<Rule> curRules = scopedRules.get(scope);    if (curRules == null) {                String configFile = getConf().get("urlnormalizer.regex.file." + scope);        if (configFile != null) {                        try {                Reader reader = getConf().getConfResourceAsReader(configFile);                curRules = readConfiguration(reader);                scopedRules.put(scope, curRules);            } catch (Exception e) {                            }        }        if (curRules == EMPTY_RULES || curRules == null) {                        scopedRules.put(scope, EMPTY_RULES);        }    }    if (curRules == EMPTY_RULES || curRules == null) {        curRules = defaultRules;    }    Iterator<Rule> i = curRules.iterator();    while (i.hasNext()) {        Rule r = (Rule) i.next();        Matcher matcher = r.pattern.matcher(urlString);        urlString = matcher.replaceAll(r.substitution);    }    return urlString;}
1
public String normalize(String urlString, String scope) throws MalformedURLException
{    return regexNormalize(urlString, scope);}
0
private List<Rule> readConfigurationFile(String filename)
{    if (LOG.isInfoEnabled()) {            }    try {        FileReader reader = new FileReader(filename);        return readConfiguration(reader);    } catch (Exception e) {                return EMPTY_RULES;    }}
1
private List<Rule> readConfiguration(Reader reader)
{    List<Rule> rules = new ArrayList<Rule>();    try {                Document doc = DocumentBuilderFactory.newInstance().newDocumentBuilder().parse(new InputSource(reader));        Element root = doc.getDocumentElement();        if ((!"regex-normalize".equals(root.getTagName())) && (LOG.isErrorEnabled())) {                    }        NodeList regexes = root.getChildNodes();        for (int i = 0; i < regexes.getLength(); i++) {            Node regexNode = regexes.item(i);            if (!(regexNode instanceof Element))                continue;            Element regex = (Element) regexNode;            if ((!"regex".equals(regex.getTagName())) && (LOG.isWarnEnabled())) {                            }            NodeList fields = regex.getChildNodes();            String patternValue = null;            String subValue = null;            for (int j = 0; j < fields.getLength(); j++) {                Node fieldNode = fields.item(j);                if (!(fieldNode instanceof Element))                    continue;                Element field = (Element) fieldNode;                if ("pattern".equals(field.getTagName()) && field.hasChildNodes())                    patternValue = ((Text) field.getFirstChild()).getData();                if ("substitution".equals(field.getTagName()) && field.hasChildNodes())                    subValue = ((Text) field.getFirstChild()).getData();                if (!field.hasChildNodes())                    subValue = "";            }            if (patternValue != null && subValue != null) {                Rule rule = new Rule();                try {                    rule.pattern = Pattern.compile(patternValue);                } catch (PatternSyntaxException e) {                    if (LOG.isErrorEnabled()) {                                            }                    continue;                }                rule.substitution = subValue;                rules.add(rule);            }        }    } catch (Exception e) {        if (LOG.isErrorEnabled()) {                    }        return EMPTY_RULES;    }    if (rules.size() == 0)        return EMPTY_RULES;    return rules;}
1
public static void main(String[] args) throws PatternSyntaxException, IOException
{    RegexURLNormalizer normalizer = new RegexURLNormalizer();    normalizer.setConf(NutchConfiguration.create());    HashMap<String, List<Rule>> scopedRules = normalizer.getScopedRules();    Iterator<Rule> i = normalizer.defaultRules.iterator();    System.out.println("* Rules for 'DEFAULT' scope:");    while (i.hasNext()) {        Rule r = i.next();        System.out.print("  " + r.pattern.pattern() + " -> ");        System.out.println(r.substitution);    }        if (args.length > 1) {        normalizer.normalize("http://test.com", args[1]);    }    if (scopedRules.size() > 1) {        Iterator<String> it = scopedRules.keySet().iterator();        while (it.hasNext()) {            String scope = it.next();            if (URLNormalizers.SCOPE_DEFAULT.equals(scope))                continue;            System.out.println("* Rules for '" + scope + "' scope:");            i = ((List<Rule>) scopedRules.get(scope)).iterator();            while (i.hasNext()) {                Rule r = (Rule) i.next();                System.out.print("  " + r.pattern.pattern() + " -> ");                System.out.println(r.substitution);            }        }    }    if (args.length > 0) {        System.out.println("\n---------- Normalizer test -----------");        String scope = URLNormalizers.SCOPE_DEFAULT;        if (args.length > 1)            scope = args[1];        System.out.println("Scope: " + scope);        System.out.println("Input url:  '" + args[0] + "'");        System.out.println("Output url: '" + normalizer.normalize(args[0], scope) + "'");    }    System.exit(0);}
0
public boolean accept(File f)
{    if (f.getName().endsWith(".xml") && f.getName().startsWith("regex-normalize-"))        return true;    return false;}
0
public void testNormalizerDefault() throws Exception
{    normalizeTest((NormalizedURL[]) testData.get(URLNormalizers.SCOPE_DEFAULT), URLNormalizers.SCOPE_DEFAULT);}
0
public void testNormalizerScope() throws Exception
{    Iterator<String> it = testData.keySet().iterator();    while (it.hasNext()) {        String scope = it.next();        normalizeTest((NormalizedURL[]) testData.get(scope), scope);    }}
0
private void normalizeTest(NormalizedURL[] urls, String scope) throws Exception
{    for (int i = 0; i < urls.length; i++) {        String url = urls[i].url;        String normalized = normalizer.normalize(urls[i].url, scope);        String expected = urls[i].expectedURL;                Assert.assertEquals(urls[i].expectedURL, normalized);    }}
1
private void bench(int loops, String scope)
{    long start = System.currentTimeMillis();    try {        NormalizedURL[] expected = (NormalizedURL[]) testData.get(scope);        if (expected == null)            return;        for (int i = 0; i < loops; i++) {            normalizeTest(expected, scope);        }    } catch (Exception e) {        Assert.fail(e.toString());    }    }
1
private NormalizedURL[] readTestFile(String scope) throws IOException
{    File f = new File(sampleDir, "regex-normalize-" + scope + ".test");    @SuppressWarnings("resource")    BufferedReader in = new BufferedReader(new InputStreamReader(new FileInputStream(f), "UTF-8"));    List<NormalizedURL> list = new ArrayList<NormalizedURL>();    String line;    while ((line = in.readLine()) != null) {        if (line.trim().length() == 0 || line.startsWith("#") || line.startsWith(" "))            continue;        list.add(new NormalizedURL(line));    }    return (NormalizedURL[]) list.toArray(new NormalizedURL[list.size()]);}
0
public static void main(String[] args) throws Exception
{    if (args.length == 0) {        System.err.println("TestRegexURLNormalizer [-bench <iter>] <scope>");        System.exit(-1);    }    boolean bench = false;    int iter = -1;    String scope = null;    for (int i = 0; i < args.length; i++) {        if (args[i].equals("-bench")) {            bench = true;            iter = Integer.parseInt(args[++i]);        } else            scope = args[i];    }    if (scope == null) {        System.err.println("Missing required scope name.");        System.exit(-1);    }    if (bench && iter < 0) {        System.err.println("Invalid number of iterations: " + iter);        System.exit(-1);    }    TestRegexURLNormalizer test = new TestRegexURLNormalizer();    NormalizedURL[] urls = (NormalizedURL[]) test.testData.get(scope);    if (urls == null) {                scope = URLNormalizers.SCOPE_DEFAULT;        urls = (NormalizedURL[]) test.testData.get(scope);    }    if (bench) {        test.bench(iter, scope);    } else {        test.normalizeTest(urls, scope);    }}
1
private synchronized void readConfiguration(Reader configReader) throws IOException
{    if (slashesMap.size() > 0) {        return;    }    BufferedReader reader = new BufferedReader(configReader);    String line, host;    String rule;    int delimiterIndex;    while ((line = reader.readLine()) != null) {        if (StringUtils.isNotBlank(line) && !line.startsWith("#")) {            line = line.trim();            delimiterIndex = line.indexOf(" ");                        if (delimiterIndex == -1) {                delimiterIndex = line.indexOf("\t");            }            host = line.substring(0, delimiterIndex);            rule = line.substring(delimiterIndex + 1).trim();            if (rule.equals("+")) {                slashesMap.put(host, true);            } else {                slashesMap.put(host, false);            }        }    }}
0
public Configuration getConf()
{    return conf;}
0
public void setConf(Configuration conf)
{    this.conf = conf;        String pluginName = "urlnormalizer-slash";    Extension[] extensions = PluginRepository.get(conf).getExtensionPoint(URLNormalizer.class.getName()).getExtensions();    for (int i = 0; i < extensions.length; i++) {        Extension extension = extensions[i];        if (extension.getDescriptor().getPluginId().equals(pluginName)) {            attributeFile = extension.getAttribute("file");            break;        }    }        if (attributeFile != null && attributeFile.trim().equals("")) {        attributeFile = null;    }    if (attributeFile != null) {        if (LOG.isInfoEnabled()) {                    }    } else {        if (LOG.isWarnEnabled()) {                    }    }        String file = conf.get("urlnormalizer.slashes.file");    String stringRules = conf.get("urlnormalizer.slashes.rules");    if (slashesFile != null) {        file = slashesFile;    } else if (attributeFile != null) {        file = attributeFile;    }    Reader reader = null;    if (stringRules != null) {                reader = new StringReader(stringRules);    } else {        reader = conf.getConfResourceAsReader(file);    }    try {        if (reader == null) {            reader = new FileReader(file);        }        readConfiguration(reader);    } catch (IOException e) {            }}
1
public String normalize(String url, String scope) throws MalformedURLException
{    return normalize(url, null, scope);}
0
public String normalize(String url, CrawlDatum crawlDatum, String scope) throws MalformedURLException
{        URL u = new URL(url);        String host = u.getHost();        if (slashesMap.containsKey(host)) {                String protocol = u.getProtocol();        String path = u.getPath();                if (path.length() > 1) {            String queryString = u.getQuery();                        boolean rule = slashesMap.get(host);                        int lastIndexOfSlash = path.lastIndexOf(SLASH);            boolean trailingSlash = (lastIndexOfSlash == path.length() - 1);                        if (!trailingSlash && rule) {                                int lastIndexOfDot = path.lastIndexOf(DOT);                if (path.length() < 6 || lastIndexOfDot == -1 || lastIndexOfDot < path.length() - 6) {                    StringBuilder buffer = new StringBuilder(protocol);                    buffer.append(PROTOCOL_DELIMITER);                    buffer.append(host);                    buffer.append(path);                    buffer.append(SLASH);                    if (queryString != null) {                        buffer.append(QUESTION_MARK);                        buffer.append(queryString);                    }                    url = buffer.toString();                }            } else             if (trailingSlash && !rule) {                StringBuilder buffer = new StringBuilder(protocol);                buffer.append(PROTOCOL_DELIMITER);                buffer.append(host);                buffer.append(path.substring(0, lastIndexOfSlash));                if (queryString != null) {                    buffer.append(QUESTION_MARK);                    buffer.append(queryString);                }                url = buffer.toString();            }        }    }    return url;}
0
public void testSlashURLNormalizer() throws Exception
{    Configuration conf = NutchConfiguration.create();    String slashesFile = SAMPLES + SEPARATOR + "slashes.txt";    SlashURLNormalizer normalizer = new SlashURLNormalizer(slashesFile);    normalizer.setConf(conf);        assertEquals("http://example.org/", normalizer.normalize("http://example.org/", URLNormalizers.SCOPE_DEFAULT));    assertEquals("http://example.net/", normalizer.normalize("http://example.net/", URLNormalizers.SCOPE_DEFAULT));        assertEquals("http://example.org", normalizer.normalize("http://example.org", URLNormalizers.SCOPE_DEFAULT));    assertEquals("http://example.net", normalizer.normalize("http://example.net", URLNormalizers.SCOPE_DEFAULT));    assertEquals("http://example.org/", normalizer.normalize("http://example.org/", URLNormalizers.SCOPE_DEFAULT));    assertEquals("http://example.net/", normalizer.normalize("http://example.net/", URLNormalizers.SCOPE_DEFAULT));        assertEquals("http://www.example.org/page/", normalizer.normalize("http://www.example.org/page", URLNormalizers.SCOPE_DEFAULT));    assertEquals("http://www.example.net/path/to/something", normalizer.normalize("http://www.example.net/path/to/something/", URLNormalizers.SCOPE_DEFAULT));        assertEquals("http://example.org/buh/", normalizer.normalize("http://example.org/buh/", URLNormalizers.SCOPE_DEFAULT));    assertEquals("http://example.net/blaat", normalizer.normalize("http://example.net/blaat", URLNormalizers.SCOPE_DEFAULT));        assertEquals("http://example.nl/buh/", normalizer.normalize("http://example.nl/buh/", URLNormalizers.SCOPE_DEFAULT));    assertEquals("http://example.de/blaat", normalizer.normalize("http://example.de/blaat", URLNormalizers.SCOPE_DEFAULT));        assertEquals("http://www.example.org/page/?a=b&c=d", normalizer.normalize("http://www.example.org/page?a=b&c=d", URLNormalizers.SCOPE_DEFAULT));    assertEquals("http://www.example.net/path/to/something?a=b&c=d", normalizer.normalize("http://www.example.net/path/to/something/?a=b&c=d", URLNormalizers.SCOPE_DEFAULT));        assertEquals("http://www.example.org/noise.mp3", normalizer.normalize("http://www.example.org/noise.mp3", URLNormalizers.SCOPE_DEFAULT));    assertEquals("http://www.example.org/page.html", normalizer.normalize("http://www.example.org/page.html", URLNormalizers.SCOPE_DEFAULT));    assertEquals("http://www.example.org/page.shtml", normalizer.normalize("http://www.example.org/page.shtml", URLNormalizers.SCOPE_DEFAULT));        assertEquals("http://www.example.org/this.is.not.an_extension/", normalizer.normalize("http://www.example.org/this.is.not.an_extension", URLNormalizers.SCOPE_DEFAULT));}
0
protected void setInterval(int seconds)
{    interval = seconds * 1000L;}
0
protected void setDuraction(int seconds)
{    duration = seconds * 1000L;}
0
protected CrawlDatum fetch(CrawlDatum datum, long currentTime)
{    datum.setStatus(fetchStatus);    datum.setFetchTime(currentTime);    return datum;}
0
protected byte[] getSignature()
{    return signatureImpl.calculate(content, null);}
0
protected void changeContent()
{    byte[] data = Arrays.copyOf(content.getContent(), content.getContent().length + 1);        data[content.getContent().length] = '2';    content.setContent(data);    }
1
protected List<CrawlDatum> parse(CrawlDatum fetchDatum)
{    List<CrawlDatum> parseDatums = new ArrayList<CrawlDatum>(0);    if (fetchDatum.getStatus() == CrawlDatum.STATUS_FETCH_SUCCESS) {        CrawlDatum signatureDatum = new CrawlDatum(CrawlDatum.STATUS_SIGNATURE, 0);        signatureDatum.setSignature(getSignature());        parseDatums.add(signatureDatum);    }    return parseDatums;}
0
protected boolean check(CrawlDatum datum)
{    if (datum.getStatus() != expectedDbStatus)        return false;    return true;}
0
protected boolean run(int maxErrors) throws IOException
{    long now = System.currentTimeMillis();    CrawlDbUpdateUtil<CrawlDbReducer> updateDb = new CrawlDbUpdateUtil<CrawlDbReducer>(new CrawlDbReducer(), context);    /* start with a db_unfetched */    CrawlDatum dbDatum = new CrawlDatum();    dbDatum.setStatus(CrawlDatum.STATUS_DB_UNFETCHED);        schedule.initializeSchedule(dummyURL, dbDatum);    dbDatum.setFetchTime(now);        long maxTime = (now + duration);    long nextTime = now;    long lastFetchTime = -1;        boolean ok = true;    CrawlDatum fetchDatum = new CrawlDatum();    /*     * Keep copies because CrawlDbReducer.reduce() and     * FetchSchedule.shouldFetch() may alter the references. Copies are used for     * verbose logging in case of an error.     */    CrawlDatum copyDbDatum = new CrawlDatum();    CrawlDatum copyFetchDatum = new CrawlDatum();    CrawlDatum afterShouldFetch = new CrawlDatum();    int errorCount = 0;    while (nextTime < maxTime) {                fetchDatum.set(dbDatum);        copyDbDatum.set(dbDatum);        if (schedule.shouldFetch(dummyURL, fetchDatum, nextTime)) {                        if (lastFetchTime > -1) {                            }            lastFetchTime = nextTime;            afterShouldFetch.set(fetchDatum);            fetchDatum = fetch(fetchDatum, nextTime);            copyFetchDatum.set(fetchDatum);            List<CrawlDatum> values = new ArrayList<CrawlDatum>();            values.add(dbDatum);            values.add(fetchDatum);            values.addAll(parse(fetchDatum));            List<CrawlDatum> res = updateDb.update(values);            assertNotNull("null returned", res);            assertFalse("no CrawlDatum", 0 == res.size());            assertEquals("more than one CrawlDatum", 1, res.size());            if (!check(res.get(0))) {                                                                                if (++errorCount >= maxErrors) {                    if (maxErrors > 0) {                                            }                    return false;                } else {                                        ok = false;                }            }            /* use the returned CrawlDatum for the next fetch */            dbDatum = res.get(0);        }        nextTime += interval;    }    return ok;}
1
public static void createCrawlDb(Configuration conf, FileSystem fs, Path crawldb, List<URLCrawlDatum> init) throws Exception
{    LOG.trace("* creating crawldb: " + crawldb);    Path dir = new Path(crawldb, CrawlDb.CURRENT_NAME);    Option wKeyOpt = MapFile.Writer.keyClass(Text.class);    org.apache.hadoop.io.SequenceFile.Writer.Option wValueOpt = SequenceFile.Writer.valueClass(CrawlDatum.class);    MapFile.Writer writer = new MapFile.Writer(conf, new Path(dir, "part-r-00000"), wKeyOpt, wValueOpt);    Iterator<URLCrawlDatum> it = init.iterator();    while (it.hasNext()) {        URLCrawlDatum row = it.next();                writer.append(new Text(row.url), row.datum);    }    writer.close();}
1
public void write(Text key, CrawlDatum value) throws IOException, InterruptedException
{    values.add(value);}
0
public List<CrawlDatum> getValues()
{    return values;}
0
public CrawlDatum getCurrentValue() throws UnsupportedOperationException
{    throw new UnsupportedOperationException("Dummy context");}
0
public Text getCurrentKey() throws UnsupportedOperationException
{    throw new UnsupportedOperationException("Dummy context with no keys");}
0
public void progress()
{}
0
public Counter getCounter(Enum<?> arg0)
{    return dummyCounters.getGroup("dummy").getCounterForName("dummy");}
0
public Counter getCounter(String arg0, String arg1)
{    return dummyCounters.getGroup("dummy").getCounterForName("dummy");}
0
public void setStatus(String arg0) throws UnsupportedOperationException
{    throw new UnsupportedOperationException("Dummy context with no status");}
0
public String getStatus() throws UnsupportedOperationException
{    throw new UnsupportedOperationException("Dummy context with no status");}
0
public float getProgress()
{    return 1f;}
0
public OutputCommitter getOutputCommitter()
{    throw new UnsupportedOperationException("Dummy context without committer");}
0
public boolean nextKey()
{    return false;}
0
public boolean nextKeyValue()
{    return false;}
0
public TaskAttemptID getTaskAttemptID() throws UnsupportedOperationException
{    throw new UnsupportedOperationException("Dummy context without TaskAttemptID");}
0
public Path[] getArchiveClassPaths()
{    return null;}
0
public String[] getArchiveTimestamps()
{    return null;}
0
public URI[] getCacheArchives() throws IOException
{    return null;}
0
public URI[] getCacheFiles() throws IOException
{    return null;}
0
public Class<? extends Reducer<?, ?, ?, ?>> getCombinerClass() throws ClassNotFoundException
{    return null;}
0
public RawComparator<?> getCombinerKeyGroupingComparator()
{    return null;}
0
public Configuration getConfiguration()
{    return conf;}
0
public Credentials getCredentials()
{    return null;}
0
public Path[] getFileClassPaths()
{    return null;}
0
public String[] getFileTimestamps()
{    return null;}
0
public RawComparator<?> getGroupingComparator()
{    return null;}
0
public Class<? extends InputFormat<?, ?>> getInputFormatClass() throws ClassNotFoundException
{    return null;}
0
public String getJar()
{    return null;}
0
public JobID getJobID()
{    return null;}
0
public String getJobName()
{    return null;}
0
public boolean getJobSetupCleanupNeeded()
{    return false;}
0
public Path[] getLocalCacheArchives() throws IOException
{    return null;}
0
public Path[] getLocalCacheFiles() throws IOException
{    return null;}
0
public Class<?> getMapOutputKeyClass()
{    return null;}
0
public Class<?> getMapOutputValueClass()
{    return null;}
0
public Class<? extends Mapper<?, ?, ?, ?>> getMapperClass() throws ClassNotFoundException
{    return null;}
0
public int getMaxMapAttempts()
{    return 0;}
0
public int getMaxReduceAttempts()
{    return 0;}
0
public int getNumReduceTasks()
{    return 0;}
0
public Class<? extends OutputFormat<?, ?>> getOutputFormatClass() throws ClassNotFoundException
{    return null;}
0
public Class<?> getOutputKeyClass()
{    return null;}
0
public Class<?> getOutputValueClass()
{    return null;}
0
public Class<? extends Partitioner<?, ?>> getPartitionerClass() throws ClassNotFoundException
{    return null;}
0
public boolean getProfileEnabled()
{    return false;}
0
public String getProfileParams()
{    return null;}
0
public IntegerRanges getProfileTaskRange(boolean arg0)
{    return null;}
0
public Class<? extends Reducer<?, ?, ?, ?>> getReducerClass() throws ClassNotFoundException
{    return null;}
0
public RawComparator<?> getSortComparator()
{    return null;}
0
public boolean getSymlink()
{    return false;}
0
public boolean getTaskCleanupNeeded()
{    return false;}
0
public String getUser()
{    return null;}
0
public Path getWorkingDirectory() throws IOException
{    return null;}
0
public static Reducer<Text, CrawlDatum, Text, CrawlDatum>.Context createContext()
{    DummyContext context = new DummyContext();    Configuration conf = context.getConfiguration();    conf.addResource("nutch-default.xml");    conf.addResource("crawl-tests.xml");    return (Reducer<Text, CrawlDatum, Text, CrawlDatum>.Context) context;}
0
public static void generateSeedList(FileSystem fs, Path urlPath, List<String> urls) throws IOException
{    generateSeedList(fs, urlPath, urls, new ArrayList<String>());}
0
public static void generateSeedList(FileSystem fs, Path urlPath, List<String> urls, List<String> metadata) throws IOException
{    FSDataOutputStream out;    Path file = new Path(urlPath, "urls.txt");    fs.mkdirs(urlPath);    out = fs.create(file);    Iterator<String> urls_i = urls.iterator();    Iterator<String> metadata_i = metadata.iterator();    String url;    String md;    while (urls_i.hasNext()) {        url = urls_i.next();        out.writeBytes(url);        if (metadata_i.hasNext()) {            md = metadata_i.next();            out.writeBytes(md);        }        out.writeBytes("\n");    }    out.flush();    out.close();}
0
public static Server getServer(int port, String staticContent) throws UnknownHostException
{    Server webServer = new org.mortbay.jetty.Server();    SocketConnector listener = new SocketConnector();    listener.setPort(port);    listener.setHost("127.0.0.1");    webServer.addConnector(listener);    ContextHandler staticContext = new ContextHandler();    staticContext.setContextPath("/");    staticContext.setResourceBase(staticContent);    staticContext.addHandler(new ResourceHandler());    webServer.addHandler(staticContext);    return webServer;}
0
public List<CrawlDatum> update(List<CrawlDatum> values)
{    List<CrawlDatum> result = new ArrayList<CrawlDatum>(0);    if (values == null || values.size() == 0) {        return result;    }        Collections.shuffle(values);    reduceDriver = ReduceDriver.newReduceDriver(reducer);    reduceDriver.getConfiguration().addResource(configuration);    reduceDriver.withInput(dummyURL, values);    List<Pair<Text, CrawlDatum>> reduceResult;    try {        reduceResult = reduceDriver.run();        for (Pair<Text, CrawlDatum> p : reduceResult) {            if (p.getFirst().equals(dummyURL)) {                result.add(p.getSecond());            }        }    } catch (IOException e) {                return result;    }    return result;}
1
public List<CrawlDatum> update(CrawlDatum dbDatum, CrawlDatum fetchDatum)
{    List<CrawlDatum> values = new ArrayList<CrawlDatum>();    if (dbDatum != null)        values.add(dbDatum);    if (fetchDatum != null)        values.add(fetchDatum);    return update(values);}
0
public List<CrawlDatum> update(CrawlDatum... values)
{    return update(Arrays.asList(values));}
0
public void write(Text key, CrawlDatum value) throws IOException, InterruptedException
{    values.add(value);}
0
public List<CrawlDatum> getValues()
{    return values;}
0
public CrawlDatum getCurrentValue() throws UnsupportedOperationException
{    throw new UnsupportedOperationException("Dummy context");}
0
public Text getCurrentKey() throws UnsupportedOperationException
{    throw new UnsupportedOperationException("Dummy context with no keys");}
0
public void progress()
{}
0
public Counter getCounter(Enum<?> arg0)
{    return dummyCounters.getGroup("dummy").getCounterForName("dummy");}
0
public Counter getCounter(String arg0, String arg1)
{    return dummyCounters.getGroup("dummy").getCounterForName("dummy");}
0
public void setStatus(String arg0) throws UnsupportedOperationException
{    throw new UnsupportedOperationException("Dummy context with no status");}
0
public String getStatus() throws UnsupportedOperationException
{    throw new UnsupportedOperationException("Dummy context with no status");}
0
public float getProgress()
{    return 1f;}
0
public OutputCommitter getOutputCommitter()
{    throw new UnsupportedOperationException("Dummy context without committer");}
0
public boolean nextKey()
{    return false;}
0
public boolean nextKeyValue()
{    return false;}
0
public TaskAttemptID getTaskAttemptID() throws UnsupportedOperationException
{    throw new UnsupportedOperationException("Dummy context without TaskAttemptID");}
0
public Path[] getArchiveClassPaths()
{    return null;}
0
public String[] getArchiveTimestamps()
{    return null;}
0
public URI[] getCacheArchives() throws IOException
{    return null;}
0
public URI[] getCacheFiles() throws IOException
{    return null;}
0
public Class<? extends Reducer<?, ?, ?, ?>> getCombinerClass() throws ClassNotFoundException
{    return null;}
0
public RawComparator<?> getCombinerKeyGroupingComparator()
{    return null;}
0
public Configuration getConfiguration()
{    return null;}
0
public Credentials getCredentials()
{    return null;}
0
public Path[] getFileClassPaths()
{    return null;}
0
public String[] getFileTimestamps()
{    return null;}
0
public RawComparator<?> getGroupingComparator()
{    return null;}
0
public Class<? extends InputFormat<?, ?>> getInputFormatClass() throws ClassNotFoundException
{    return null;}
0
public String getJar()
{    return null;}
0
public JobID getJobID()
{    return null;}
0
public String getJobName()
{    return null;}
0
public boolean getJobSetupCleanupNeeded()
{    return false;}
0
public Path[] getLocalCacheArchives() throws IOException
{    return null;}
0
public Path[] getLocalCacheFiles() throws IOException
{    return null;}
0
public Class<?> getMapOutputKeyClass()
{    return null;}
0
public Class<?> getMapOutputValueClass()
{    return null;}
0
public Class<? extends Mapper<?, ?, ?, ?>> getMapperClass() throws ClassNotFoundException
{    return null;}
0
public int getMaxMapAttempts()
{    return 0;}
0
public int getMaxReduceAttempts()
{    return 0;}
0
public int getNumReduceTasks()
{    return 0;}
0
public Class<? extends OutputFormat<?, ?>> getOutputFormatClass() throws ClassNotFoundException
{    return null;}
0
public Class<?> getOutputKeyClass()
{    return null;}
0
public Class<?> getOutputValueClass()
{    return null;}
0
public Class<? extends Partitioner<?, ?>> getPartitionerClass() throws ClassNotFoundException
{    return null;}
0
public boolean getProfileEnabled()
{    return false;}
0
public String getProfileParams()
{    return null;}
0
public IntegerRanges getProfileTaskRange(boolean arg0)
{    return null;}
0
public Class<? extends Reducer<?, ?, ?, ?>> getReducerClass() throws ClassNotFoundException
{    return null;}
0
public RawComparator<?> getSortComparator()
{    return null;}
0
public boolean getSymlink()
{    return false;}
0
public boolean getTaskCleanupNeeded()
{    return false;}
0
public String getUser()
{    return null;}
0
public Path getWorkingDirectory() throws IOException
{    return null;}
0
public List<CrawlDatum> update(List<CrawlDatum> values)
{    if (values == null || values.size() == 0) {        return new ArrayList<CrawlDatum>(0);    }        Collections.shuffle(values);    DummyContext context = new DummyContext();    try {        Iterable<CrawlDatum> iterable_values = (Iterable) values;        reducer.reduce(dummyURL, iterable_values, (Reducer<Text, CrawlDatum, Text, CrawlDatum>.Context) context);    } catch (IOException e) {            } catch (InterruptedException e) {            }    return context.getValues();}
1
public List<CrawlDatum> update(CrawlDatum dbDatum, CrawlDatum fetchDatum)
{    List<CrawlDatum> values = new ArrayList<CrawlDatum>();    if (dbDatum != null)        values.add(dbDatum);    if (fetchDatum != null)        values.add(fetchDatum);    return update(values);}
0
public List<CrawlDatum> update(CrawlDatum... values)
{    return update(Arrays.asList(values));}
0
public void setUp() throws Exception
{    super.setUp();    conf = NutchConfiguration.create();    inc_rate = conf.getFloat("db.fetch.schedule.adaptive.inc_rate", 0.2f);    dec_rate = conf.getFloat("db.fetch.schedule.adaptive.dec_rate", 0.2f);    interval = 100;    lastModified = 0;}
0
public void testAdaptiveFetchSchedule()
{    FetchSchedule fs = new AdaptiveFetchSchedule();    fs.setConf(conf);    CrawlDatum p = prepareCrawlDatum();    Text url = new Text("http://www.example.com");    changed = FetchSchedule.STATUS_UNKNOWN;    fs.setFetchSchedule(url, p, p.getFetchTime(), p.getModifiedTime(), curTime, lastModified, changed);    validateFetchInterval(changed, p.getFetchInterval());    changed = FetchSchedule.STATUS_MODIFIED;    fs.setFetchSchedule(url, p, p.getFetchTime(), p.getModifiedTime(), curTime, lastModified, changed);    validateFetchInterval(changed, p.getFetchInterval());    p.setFetchInterval(interval);    changed = FetchSchedule.STATUS_NOTMODIFIED;    fs.setFetchSchedule(url, p, p.getFetchTime(), p.getModifiedTime(), curTime, lastModified, changed);    validateFetchInterval(changed, p.getFetchInterval());}
0
public CrawlDatum prepareCrawlDatum()
{    CrawlDatum p = new CrawlDatum();    p.setStatus(CrawlDatum.STATUS_DB_UNFETCHED);    p.setFetchInterval(interval);    p.setScore(1.0f);    p.setFetchTime(0);    return p;}
0
private void validateFetchInterval(int changed, int getInterval)
{    if (changed == FetchSchedule.STATUS_UNKNOWN) {        assertEquals(getInterval, interval);    } else if (changed == FetchSchedule.STATUS_MODIFIED) {        calculateInterval = (int) (interval - (interval * dec_rate));        assertEquals(getInterval, calculateInterval);    } else if (changed == FetchSchedule.STATUS_NOTMODIFIED) {        calculateInterval = (int) (interval + (interval * inc_rate));        assertEquals(getInterval, calculateInterval);    }}
0
public void setUp() throws Exception
{    conf = CrawlDBTestUtil.createContext().getConfiguration();    fs = FileSystem.get(conf);    fs.delete(testdir, true);}
0
public void tearDown()
{    delete(testdir);}
0
private void delete(Path p)
{    try {        fs.delete(p, true);    } catch (IOException e) {    }}
0
public void testUrl404Purging() throws Exception
{        ArrayList<URLCrawlDatum> list = new ArrayList<URLCrawlDatum>();    list.add(new URLCrawlDatum(new Text("http://www.example.com"), new CrawlDatum(CrawlDatum.STATUS_DB_GONE, 0, 0.0f)));    list.add(new URLCrawlDatum(new Text("http://www.example1.com"), new CrawlDatum(CrawlDatum.STATUS_DB_FETCHED, 0, 0.0f)));    list.add(new URLCrawlDatum(new Text("http://www.example2.com"), new CrawlDatum(CrawlDatum.STATUS_DB_UNFETCHED, 0, 0.0f)));    dbDir = new Path(testdir, "crawldb");    newCrawlDb = new Path(testdir, "newcrawldb");        CrawlDBTestUtil.createCrawlDb(conf, fs, dbDir, list);        conf.setBoolean(CrawlDb.CRAWLDB_PURGE_404, true);    conf.setBoolean(CrawlDbFilter.URL_NORMALIZING, true);    conf.setBoolean(CrawlDbFilter.URL_FILTERING, false);    conf.setInt("urlnormalizer.loop.count", 2);    Job job = NutchJob.getInstance(conf);    job.setJobName("Test CrawlDbFilter");    Path current = new Path(dbDir, "current");    if (FileSystem.get(conf).exists(current)) {        FileInputFormat.addInputPath(job, current);    }    job.setInputFormatClass(SequenceFileInputFormat.class);    job.setMapperClass(CrawlDbFilter.class);    job.setReducerClass(CrawlDbReducer.class);    FileOutputFormat.setOutputPath(job, newCrawlDb);    job.setOutputFormatClass(MapFileOutputFormat.class);    job.setOutputKeyClass(Text.class);    job.setOutputValueClass(CrawlDatum.class);    job.setJarByClass(CrawlDbFilter.class);    job.waitForCompletion(true);    Path fetchlist = new Path(new Path(newCrawlDb, "part-r-00000"), "data");    ArrayList<URLCrawlDatum> l = readContents(fetchlist);        Assert.assertEquals(2, l.size());}
0
private ArrayList<URLCrawlDatum> readContents(Path fetchlist) throws IOException
{        Option fFile = SequenceFile.Reader.file(fetchlist);    SequenceFile.Reader reader = new SequenceFile.Reader(conf, fFile);    ArrayList<URLCrawlDatum> l = new ArrayList<URLCrawlDatum>();    READ: do {        Text key = new Text();        CrawlDatum value = new CrawlDatum();        if (!reader.next(key, value)) {            break READ;        }        l.add(new URLCrawlDatum(key, value));    } while (true);    reader.close();    return l;}
0
public void setUp() throws Exception
{    init1.add(url10);    init1.add(url11);    init2.add(url20);    init2.add(url21);    long time = System.currentTimeMillis();    cd1 = new CrawlDatum();    cd1.setFetchInterval(1.0f);    cd1.setFetchTime(time);    cd1.getMetaData().put(new Text("name"), new Text("cd1"));    cd1.getMetaData().put(new Text("cd1"), new Text("cd1"));    cd2 = new CrawlDatum();    cd2.setFetchInterval(1.0f);    cd2.setFetchTime(time + 10000);    cd2.getMetaData().put(new Text("name"), new Text("cd2"));    cd3 = new CrawlDatum();    cd3.setFetchInterval(1.0f);    cd3.setFetchTime(time + 10000);    cd3.getMetaData().putAll(cd1.getMetaData());    cd3.getMetaData().putAll(cd2.getMetaData());    expected.put(url10, cd3);    expected.put(url11, cd1);    expected.put(url21, cd2);    conf = NutchConfiguration.create();    fs = FileSystem.get(conf);    testDir = new Path("test-crawldb-" + new java.util.Random().nextInt());    fs.mkdirs(testDir);}
0
public void tearDown()
{    try {        if (fs.exists(testDir))            fs.delete(testDir, true);    } catch (Exception e) {    }    try {        reader.close();    } catch (Exception e) {    }}
0
public void testMerge() throws Exception
{    Path crawldb1 = new Path(testDir, "crawldb1");    Path crawldb2 = new Path(testDir, "crawldb2");    Path output = new Path(testDir, "output");    createCrawlDb(conf, fs, crawldb1, init1, cd1);    createCrawlDb(conf, fs, crawldb2, init2, cd2);    CrawlDbMerger merger = new CrawlDbMerger(conf);        merger.merge(output, new Path[] { crawldb1, crawldb2 }, false, false);        reader = new CrawlDbReader();    String crawlDb = output.toString();    Iterator<String> it = expected.keySet().iterator();    while (it.hasNext()) {        String url = it.next();                CrawlDatum cd = expected.get(url);        CrawlDatum res = reader.get(crawlDb, url, conf);                System.out.println("url=" + url);        System.out.println(" cd " + cd);        System.out.println(" res " + res);                Assert.assertNotNull(res);        Assert.assertTrue(cd.equals(res));    }    reader.close();    fs.delete(testDir, true);}
1
private void createCrawlDb(Configuration config, FileSystem fs, Path crawldb, TreeSet<String> init, CrawlDatum cd) throws Exception
{        Path dir = new Path(crawldb, CrawlDb.CURRENT_NAME);    Option wKeyOpt = MapFile.Writer.keyClass(Text.class);    org.apache.hadoop.io.SequenceFile.Writer.Option wValueOpt = SequenceFile.Writer.valueClass(CrawlDatum.class);    MapFile.Writer writer = new MapFile.Writer(config, new Path(dir, "part-r-00000"), wKeyOpt, wValueOpt);    Iterator<String> it = init.iterator();    while (it.hasNext()) {        String key = it.next();        writer.append(new Text(key), cd);    }    writer.close();}
1
public void testCrawlDbStateTransitionMatrix()
{        Reducer<Text, CrawlDatum, Text, CrawlDatum>.Context context = CrawlDBTestUtil.createContext();    Configuration conf = context.getConfiguration();    CrawlDbUpdateUtil updateDb = null;    try {        updateDb = new CrawlDbUpdateUtil(new CrawlDbReducer(), context);    } catch (IOException e) {        e.printStackTrace();    }    int retryMax = conf.getInt("db.fetch.retry.max", 3);    for (String sched : schedules) {                conf.set("db.fetch.schedule.class", "org.apache.nutch.crawl." + sched);        FetchSchedule schedule = FetchScheduleFactory.getFetchSchedule(conf);        for (int i = 0; i < fetchDbStatusPairs.length; i++) {            byte fromDbStatus = fetchDbStatusPairs[i][1];            for (int j = 0; j < fetchDbStatusPairs.length; j++) {                byte fetchStatus = fetchDbStatusPairs[j][0];                CrawlDatum fromDb = null;                if (fromDbStatus == -1) {                                                } else {                    fromDb = new CrawlDatum();                    fromDb.setStatus(fromDbStatus);                                        schedule.initializeSchedule(CrawlDbUpdateUtil.dummyURL, fromDb);                }                                byte toDbStatus = fetchDbStatusPairs[j][1];                if (fetchStatus == -1) {                    if (fromDbStatus == -1) {                                                toDbStatus = STATUS_DB_UNFETCHED;                    } else {                                                toDbStatus = fromDbStatus;                    }                } else if (fetchStatus == STATUS_FETCH_RETRY) {                                        if (fromDb == null || fromDb.getRetriesSinceFetch() < retryMax) {                        toDbStatus = STATUS_DB_UNFETCHED;                    } else {                        toDbStatus = STATUS_DB_GONE;                    }                }                String fromDbStatusName = (fromDbStatus == -1 ? "<not in CrawlDb>" : getStatusName(fromDbStatus));                String fetchStatusName = (fetchStatus == -1 ? "<only inlinks>" : CrawlDatum.getStatusName(fetchStatus));                                List<CrawlDatum> values = new ArrayList<CrawlDatum>();                for (int l = 0; l <= 2; l++) {                                        CrawlDatum fetch = null;                    if (fetchStatus == -1) {                                                if (l == 0)                            continue;                    } else {                        fetch = new CrawlDatum();                        if (fromDb != null) {                            fetch.set(fromDb);                        } else {                                                        schedule.initializeSchedule(CrawlDbUpdateUtil.dummyURL, fetch);                        }                        fetch.setStatus(fetchStatus);                        fetch.setFetchTime(System.currentTimeMillis());                    }                    if (fromDb != null)                        values.add(fromDb);                    if (fetch != null)                        values.add(fetch);                    for (int n = 0; n < l; n++) {                        values.add(linked);                    }                    List<CrawlDatum> res = updateDb.update(values);                    if (res.size() != 1) {                        fail("CrawlDb update didn't result in one single CrawlDatum per URL");                        continue;                    }                    byte status = res.get(0).getStatus();                    if (status != toDbStatus) {                        fail("CrawlDb update for " + fromDbStatusName + " and " + fetchStatusName + " and " + l + " inlinks results in " + getStatusName(status) + " (expected: " + getStatusName(toDbStatus) + ")");                    }                    values.clear();                }            }        }    }}
1
public void testCrawlDbStatTransitionInject()
{        Configuration conf = CrawlDBTestUtil.createContext().getConfiguration();    Injector.InjectReducer injector = new Injector.InjectReducer();    CrawlDbUpdateTestDriver<Injector.InjectReducer> injectDriver = new CrawlDbUpdateTestDriver<Injector.InjectReducer>(injector, conf);    ScoringFilters scfilters = new ScoringFilters(conf);    for (String sched : schedules) {                conf.set("db.fetch.schedule.class", "org.apache.nutch.crawl." + sched);        FetchSchedule schedule = FetchScheduleFactory.getFetchSchedule(conf);        List<CrawlDatum> values = new ArrayList<CrawlDatum>();        for (int i = 0; i < fetchDbStatusPairs.length; i++) {            byte fromDbStatus = fetchDbStatusPairs[i][1];            byte toDbStatus = fromDbStatus;            if (fromDbStatus == -1) {                toDbStatus = STATUS_DB_UNFETCHED;            } else {                CrawlDatum fromDb = new CrawlDatum();                fromDb.setStatus(fromDbStatus);                schedule.initializeSchedule(CrawlDbUpdateUtil.dummyURL, fromDb);                values.add(fromDb);            }                        CrawlDatum injected = new CrawlDatum(STATUS_INJECTED, conf.getInt("db.fetch.interval.default", 2592000), 0.1f);            schedule.initializeSchedule(CrawlDbUpdateUtil.dummyURL, injected);            try {                scfilters.injectedScore(CrawlDbUpdateUtil.dummyURL, injected);            } catch (ScoringFilterException e) {                            }            values.add(injected);            List<CrawlDatum> res = injectDriver.update(values);            if (res.size() != 1) {                fail("Inject didn't result in one single CrawlDatum per URL");                continue;            }            byte status = res.get(0).getStatus();            if (status != toDbStatus) {                fail("Inject for " + (fromDbStatus == -1 ? "" : getStatusName(fromDbStatus) + " and ") + getStatusName(STATUS_INJECTED) + " results in " + getStatusName(status) + " (expected: " + getStatusName(toDbStatus) + ")");            }            values.clear();        }    }}
1
public void testCrawlDbReducerNotModified()
{        Context context = CrawlDBTestUtil.createContext();    Configuration conf = context.getConfiguration();    ;        for (String sched : schedules) {        String desc = "test notmodified by signature comparison + " + sched;                conf.set("db.fetch.schedule.class", "org.apache.nutch.crawl." + sched);        ContinuousCrawlTestUtil crawlUtil = new CrawlTestFetchNotModified(context);        try {            if (!crawlUtil.run(20)) {                fail("failed: " + desc);            }        } catch (IOException e) {            e.printStackTrace();        }    }        for (String sched : schedules) {        String desc = "test notmodified by HTTP 304 + " + sched;                conf.set("db.fetch.schedule.class", "org.apache.nutch.crawl." + sched);        ContinuousCrawlTestUtil crawlUtil = new CrawlTestFetchNotModifiedHttp304(context);        try {            if (!crawlUtil.run(20)) {                fail("failed: " + desc);            }        } catch (IOException e) {            e.printStackTrace();        }    }}
1
protected boolean check(CrawlDatum result)
{    if (lastFetchTime > 0 && (currFetchTime - lastFetchTime) > maxFetchInterval) {                return false;    }    switch(result.getStatus()) {        case STATUS_DB_NOTMODIFIED:                        if ((previousDbState == STATUS_DB_FETCHED || previousDbState == STATUS_DB_NOTMODIFIED)) {                if (lastSignature != null && result.getSignature() != null && SignatureComparator._compare(lastSignature, result.getSignature()) != 0) {                                        return false;                }                                return checkModifiedTime(result, firstFetchTime);            }                        break;        case STATUS_DB_FETCHED:            if (previousDbState == STATUS_DB_UNFETCHED) {                                return checkModifiedTime(result, firstFetchTime);            } else if (lastSignature != null && result.getSignature() != null && SignatureComparator._compare(lastSignature, result.getSignature()) != 0) {                                                return checkModifiedTime(result, currFetchTime);            } else {                            }            break;        case STATUS_DB_UNFETCHED:            /**             * Status db_unfetched is possible with {@link AdaptiveFetchSchedule}             * because {@link CrawlDbReducer#reduce} calls             * {@link FetchSchedule#forceRefetch} to force a re-fetch if fetch             * interval grows too large.             */            if (schedule.getClass() == AdaptiveFetchSchedule.class) {                                if (result.getSignature() != null) {                                        return false;                }                                firstFetchTime = 0;                return true;            }    }        return false;}
1
private boolean checkModifiedTime(CrawlDatum result, long modifiedTime)
{    if (modifiedTime == result.getModifiedTime()) {        return true;    }        return false;}
1
protected CrawlDatum fetch(CrawlDatum datum, long currentTime)
{    lastFetchTime = currFetchTime;    currFetchTime = currentTime;    if (lastFetchTime > 0)        elapsedDuration += (currFetchTime - lastFetchTime);    previousDbState = datum.getStatus();    lastSignature = datum.getSignature();    datum = super.fetch(datum, currentTime);    if (firstFetchTime == 0) {        firstFetchTime = currFetchTime;    } else if (elapsedDuration < (duration / 2)) {                changeContent();        firstFetchTime = currFetchTime;    }    return datum;}
0
protected CrawlDatum fetch(CrawlDatum datum, long currentTime)
{    lastFetchTime = currFetchTime;    currFetchTime = currentTime;    previousDbState = datum.getStatus();    lastSignature = datum.getSignature();    int httpCode;    /*       * document is "really" fetched (no HTTP 304) - if last-modified time or       * signature are unset (page has not been fetched before or fetch is       * forced) - for test purposes, we simulate a modified after "one year"       */    if (datum.getModifiedTime() == 0 && datum.getSignature() == null || (currFetchTime - firstFetchTime) > (duration / 2)) {        firstFetchTime = currFetchTime;        httpCode = 200;        datum.setStatus(STATUS_FETCH_SUCCESS);                changeContent();    } else {        httpCode = 304;        datum.setStatus(STATUS_FETCH_NOTMODIFIED);    }        datum.setFetchTime(currentTime);    return datum;}
1
public void testCrawlDbReducerPageGoneSchedule1()
{        ContinuousCrawlTestUtil crawlUtil = new ContinuousCrawlTestUtil(STATUS_FETCH_GONE, STATUS_DB_GONE);    try {        if (!crawlUtil.run(20)) {            fail("fetch_gone did not result in a db_gone (NUTCH-1245)");        }    } catch (IOException e) {        e.printStackTrace();    }}
1
public void testCrawlDbReducerPageGoneSchedule2()
{        Context context = CrawlDBTestUtil.createContext();    Configuration conf = context.getConfiguration();    int fetchIntervalMax = conf.getInt("db.fetch.interval.max", 0);    conf.setInt("db.fetch.interval.default", 3 + (int) (fetchIntervalMax * 1.5));    ContinuousCrawlTestUtil crawlUtil = new ContinuousCrawlTestUtil(context, STATUS_FETCH_GONE, STATUS_DB_GONE);    try {        if (!crawlUtil.run(0)) {            fail("fetch_gone did not result in a db_gone (NUTCH-1245)");        }    } catch (IOException e) {        e.printStackTrace();    }}
1
public void testSignatureReset()
{        Context context = CrawlDBTestUtil.createContext();    Configuration conf = context.getConfiguration();    for (String sched : schedules) {                conf.set("db.fetch.schedule.class", "org.apache.nutch.crawl." + sched);        ContinuousCrawlTestUtil crawlUtil = new CrawlTestSignatureReset(context);        try {            if (!crawlUtil.run(20)) {                fail("failed: signature not reset");            }        } catch (IOException e) {            e.printStackTrace();        }    }}
1
protected CrawlDatum fetch(CrawlDatum datum, long currentTime)
{    datum = super.fetch(datum, currentTime);    counter++;        if (counter % 2 == 1) {        fetchState = STATUS_FETCH_SUCCESS;    } else {        fetchState = noContentStates[(counter % 6) / 2][0];    }        datum.setStatus(fetchState);    return datum;}
1
protected boolean check(CrawlDatum result)
{    if (result.getStatus() == STATUS_DB_NOTMODIFIED && !(fetchState == STATUS_FETCH_SUCCESS || fetchState == STATUS_FETCH_NOTMODIFIED)) {                return false;    }    if (result.getSignature() != null && !(result.getStatus() == STATUS_DB_FETCHED || result.getStatus() == STATUS_DB_NOTMODIFIED)) {                    }    return true;}
1
public void setUp() throws Exception
{    conf = CrawlDBTestUtil.createContext().getConfiguration();    fs = FileSystem.get(conf);    fs.delete(testdir, true);}
0
public void tearDown()
{    delete(testdir);}
0
private void delete(Path p)
{    try {        fs.delete(p, true);    } catch (IOException e) {    }}
0
public void testGenerateHighest() throws Exception
{    final int NUM_RESULTS = 2;    ArrayList<URLCrawlDatum> list = new ArrayList<URLCrawlDatum>();    for (int i = 0; i <= 100; i++) {        list.add(createURLCrawlDatum("http://aaa/" + pad(i), 1, i));    }    createCrawlDB(list);    Path generatedSegment = generateFetchlist(NUM_RESULTS, conf, false);    Path fetchlist = new Path(new Path(generatedSegment, CrawlDatum.GENERATE_DIR_NAME), "part-r-00000");    ArrayList<URLCrawlDatum> l = readContents(fetchlist);        Collections.sort(l, new ScoreComparator());        Assert.assertEquals(NUM_RESULTS, l.size());        Assert.assertEquals("http://aaa/100", (l.get(0).url.toString()));    Assert.assertEquals("http://aaa/099", (l.get(1).url.toString()));}
0
private String pad(int i)
{    String s = Integer.toString(i);    while (s.length() < 3) {        s = "0" + s;    }    return s;}
0
public int compare(URLCrawlDatum tuple1, URLCrawlDatum tuple2)
{    if (tuple2.datum.getScore() - tuple1.datum.getScore() < 0) {        return -1;    }    if (tuple2.datum.getScore() - tuple1.datum.getScore() > 0) {        return 1;    }    return 0;}
0
public void testGenerateHostLimit() throws Exception
{    ArrayList<URLCrawlDatum> list = new ArrayList<URLCrawlDatum>();    list.add(createURLCrawlDatum("http://www.example.com/index1.html", 1, 1));    list.add(createURLCrawlDatum("http://www.example.com/index2.html", 1, 1));    list.add(createURLCrawlDatum("http://www.example.com/index3.html", 1, 1));    createCrawlDB(list);    int maxPerHost = 1;    Configuration myConfiguration = new Configuration(conf);    myConfiguration.setInt(Generator.GENERATOR_MAX_COUNT, maxPerHost);    Path generatedSegment = generateFetchlist(Integer.MAX_VALUE, myConfiguration, false);    Path fetchlistPath = new Path(new Path(generatedSegment, CrawlDatum.GENERATE_DIR_NAME), "part-r-00000");    ArrayList<URLCrawlDatum> fetchList = readContents(fetchlistPath);        int expectedFetchListSize = Math.min(maxPerHost, list.size());    Assert.assertEquals("Failed to apply generate.max.count by host", expectedFetchListSize, fetchList.size());    maxPerHost = 2;    myConfiguration = new Configuration(conf);    myConfiguration.setInt(Generator.GENERATOR_MAX_COUNT, maxPerHost);    generatedSegment = generateFetchlist(Integer.MAX_VALUE, myConfiguration, false);    fetchlistPath = new Path(new Path(generatedSegment, CrawlDatum.GENERATE_DIR_NAME), "part-r-00000");    fetchList = readContents(fetchlistPath);        expectedFetchListSize = Math.min(maxPerHost, list.size());    Assert.assertEquals("Failed to apply generate.max.count by host", expectedFetchListSize, fetchList.size());    maxPerHost = 3;    myConfiguration = new Configuration(conf);    myConfiguration.setInt(Generator.GENERATOR_MAX_COUNT, maxPerHost);    generatedSegment = generateFetchlist(Integer.MAX_VALUE, myConfiguration, false);    fetchlistPath = new Path(new Path(generatedSegment, CrawlDatum.GENERATE_DIR_NAME), "part-r-00000");    fetchList = readContents(fetchlistPath);        expectedFetchListSize = Math.min(maxPerHost, list.size());    Assert.assertEquals("Failed to apply generate.max.count by host", expectedFetchListSize, fetchList.size());}
0
public void testGenerateDomainLimit() throws Exception
{    ArrayList<URLCrawlDatum> list = new ArrayList<URLCrawlDatum>();    list.add(createURLCrawlDatum("http://a.example.com/index.html", 1, 1));    list.add(createURLCrawlDatum("http://b.example.com/index.html", 1, 1));    list.add(createURLCrawlDatum("http://c.example.com/index.html", 1, 1));    createCrawlDB(list);    int maxPerDomain = 1;    Configuration myConfiguration = new Configuration(conf);    myConfiguration.setInt(Generator.GENERATOR_MAX_COUNT, maxPerDomain);    myConfiguration.set(Generator.GENERATOR_COUNT_MODE, Generator.GENERATOR_COUNT_VALUE_DOMAIN);    Path generatedSegment = generateFetchlist(Integer.MAX_VALUE, myConfiguration, false);    Path fetchlistPath = new Path(new Path(generatedSegment, CrawlDatum.GENERATE_DIR_NAME), "part-r-00000");    ArrayList<URLCrawlDatum> fetchList = readContents(fetchlistPath);        int expectedFetchListSize = Math.min(maxPerDomain, list.size());    Assert.assertEquals("Failed to apply generate.max.count by domain", expectedFetchListSize, fetchList.size());    maxPerDomain = 2;    myConfiguration = new Configuration(myConfiguration);    myConfiguration.setInt(Generator.GENERATOR_MAX_COUNT, maxPerDomain);    generatedSegment = generateFetchlist(Integer.MAX_VALUE, myConfiguration, false);    fetchlistPath = new Path(new Path(generatedSegment, CrawlDatum.GENERATE_DIR_NAME), "part-r-00000");    fetchList = readContents(fetchlistPath);        expectedFetchListSize = Math.min(maxPerDomain, list.size());    Assert.assertEquals("Failed to apply generate.max.count by domain", expectedFetchListSize, fetchList.size());    maxPerDomain = 3;    myConfiguration = new Configuration(myConfiguration);    myConfiguration.setInt(Generator.GENERATOR_MAX_COUNT, maxPerDomain);    generatedSegment = generateFetchlist(Integer.MAX_VALUE, myConfiguration, false);    fetchlistPath = new Path(new Path(generatedSegment, CrawlDatum.GENERATE_DIR_NAME), "part-r-00000");    fetchList = readContents(fetchlistPath);        expectedFetchListSize = Math.min(maxPerDomain, list.size());    Assert.assertEquals("Failed to apply generate.max.count by domain", expectedFetchListSize, fetchList.size());}
0
public void testFilter() throws IOException, Exception
{    ArrayList<URLCrawlDatum> list = new ArrayList<URLCrawlDatum>();    list.add(createURLCrawlDatum("http://www.example.com/index.html", 1, 1));    list.add(createURLCrawlDatum("http://www.example.net/index.html", 1, 1));    list.add(createURLCrawlDatum("http://www.example.org/index.html", 1, 1));    createCrawlDB(list);    Configuration myConfiguration = new Configuration(conf);    myConfiguration.set("urlfilter.suffix.file", "filter-all.txt");    Path generatedSegment = generateFetchlist(Integer.MAX_VALUE, myConfiguration, true);    Assert.assertNull("should be null (0 entries)", generatedSegment);    generatedSegment = generateFetchlist(Integer.MAX_VALUE, myConfiguration, false);    Path fetchlistPath = new Path(new Path(generatedSegment, CrawlDatum.GENERATE_DIR_NAME), "part-r-00000");    ArrayList<URLCrawlDatum> fetchList = readContents(fetchlistPath);        Assert.assertEquals(list.size(), fetchList.size());}
0
private ArrayList<URLCrawlDatum> readContents(Path fetchlist) throws IOException
{        Option rFile = SequenceFile.Reader.file(fetchlist);    SequenceFile.Reader reader = new SequenceFile.Reader(conf, rFile);    ArrayList<URLCrawlDatum> l = new ArrayList<URLCrawlDatum>();    READ: do {        Text key = new Text();        CrawlDatum value = new CrawlDatum();        if (!reader.next(key, value)) {            break READ;        }        l.add(new URLCrawlDatum(key, value));    } while (true);    reader.close();    return l;}
0
private Path generateFetchlist(int numResults, Configuration config, boolean filter) throws IOException, ClassNotFoundException, InterruptedException
{        Generator g = new Generator(config);    Path[] generatedSegment = g.generate(dbDir, segmentsDir, -1, numResults, Long.MAX_VALUE, filter, false);    if (generatedSegment == null)        return null;    return generatedSegment[0];}
0
private void createCrawlDB(ArrayList<URLCrawlDatum> list) throws IOException, Exception
{    dbDir = new Path(testdir, "crawldb");    segmentsDir = new Path(testdir, "segments");    fs.mkdirs(dbDir);    fs.mkdirs(segmentsDir);        CrawlDBTestUtil.createCrawlDb(conf, fs, dbDir, list);}
0
private URLCrawlDatum createURLCrawlDatum(final String url, final int fetchInterval, final float score)
{    return new CrawlDBTestUtil.URLCrawlDatum(new Text(url), new CrawlDatum(CrawlDatum.STATUS_DB_UNFETCHED, fetchInterval, score));}
0
public void setUp() throws Exception
{    conf = CrawlDBTestUtil.createContext().getConfiguration();    urlPath = new Path(testdir, "urls");    crawldbPath = new Path(testdir, "crawldb");    fs = FileSystem.get(conf);    if (fs.exists(urlPath))        fs.delete(urlPath, false);    if (fs.exists(crawldbPath))        fs.delete(crawldbPath, true);}
0
public void tearDown() throws IOException
{    fs.delete(testdir, true);}
0
public void testInject() throws IOException, ClassNotFoundException, InterruptedException
{    ArrayList<String> urls = new ArrayList<String>();            ArrayList<String> metadata = new ArrayList<String>();    for (int i = 0; i < 100; i++) {        urls.add("http://zzz.com/" + i + ".html");        metadata.add("\tnutch.score=2." + i + "\tnutch.fetchInterval=171717\tkey=value");    }    CrawlDBTestUtil.generateSeedList(fs, urlPath, urls, metadata);    Injector injector = new Injector(conf);    injector.inject(crawldbPath, urlPath);        List<String> read = readCrawldb();    Collections.sort(read);    Collections.sort(urls);    Assert.assertEquals(urls.size(), read.size());    Assert.assertTrue(read.containsAll(urls));    Assert.assertTrue(urls.containsAll(read));        ArrayList<String> urls2 = new ArrayList<String>();    for (int i = 0; i < 100; i++) {        urls2.add("http://xxx.com/" + i + ".html");                        urls2.add("http://zzz.com/" + i + ".html");    }    CrawlDBTestUtil.generateSeedList(fs, urlPath, urls2);    injector = new Injector(conf);    conf.setBoolean("db.injector.update", true);    injector.inject(crawldbPath, urlPath);    urls.addAll(urls2);        read = readCrawldb();    Collections.sort(read);    Collections.sort(urls);        Assert.assertEquals(urls.size() - 100, read.size());    Assert.assertTrue(read.containsAll(urls));    Assert.assertTrue(urls.containsAll(read));        Map<String, CrawlDatum> records = readCrawldbRecords();            Text writableKey = new Text("key");    Text writableValue = new Text("value");    for (String url : urls) {        if (url.indexOf("http://zzz") == 0) {                        Assert.assertTrue(records.get(url).getFetchInterval() == 171717);                        Assert.assertTrue(records.get(url).getScore() != 1.0);                        Assert.assertEquals(writableValue, records.get(url).getMetaData().get(writableKey));        }    }}
0
private List<String> readCrawldb() throws IOException
{    Path dbfile = new Path(crawldbPath, CrawlDb.CURRENT_NAME + "/part-r-00000/data");    System.out.println("reading:" + dbfile);    Option rFile = SequenceFile.Reader.file(dbfile);    @SuppressWarnings("resource")    SequenceFile.Reader reader = new SequenceFile.Reader(conf, rFile);    ArrayList<String> read = new ArrayList<String>();    READ: do {        Text key = new Text();        CrawlDatum value = new CrawlDatum();        if (!reader.next(key, value))            break READ;        read.add(key.toString());    } while (true);    return read;}
0
private HashMap<String, CrawlDatum> readCrawldbRecords() throws IOException
{    Path dbfile = new Path(crawldbPath, CrawlDb.CURRENT_NAME + "/part-r-00000/data");    System.out.println("reading:" + dbfile);    Option rFile = SequenceFile.Reader.file(dbfile);    @SuppressWarnings("resource")    SequenceFile.Reader reader = new SequenceFile.Reader(conf, rFile);    HashMap<String, CrawlDatum> read = new HashMap<String, CrawlDatum>();    READ: do {        Text key = new Text();        CrawlDatum value = new CrawlDatum();        if (!reader.next(key, value))            break READ;        read.put(key.toString(), value);    } while (true);    return read;}
0
public void setUp() throws Exception
{    init1.put(url10, urls10);    init1.put(url11, urls11);    init2.put(url20, urls20);    init2.put(url21, urls21);    expected.put(url10, urls10_expected);    expected.put(url11, urls11_expected);    expected.put(url20, urls20_expected);    expected.put(url21, urls21_expected);    conf = NutchConfiguration.create();    fs = FileSystem.get(conf);    testDir = new Path("build/test/test-linkdb-" + new java.util.Random().nextInt());    fs.mkdirs(testDir);}
0
public void tearDown()
{    try {        if (fs.exists(testDir))            fs.delete(testDir, true);    } catch (Exception e) {    }    try {        reader.close();    } catch (Exception e) {    }}
0
public void testMerge() throws Exception
{    Configuration conf = NutchConfiguration.create();    FileSystem fs = FileSystem.get(conf);    fs.mkdirs(testDir);    Path linkdb1 = new Path(testDir, "linkdb1");    Path linkdb2 = new Path(testDir, "linkdb2");    Path output = new Path(testDir, "output");    createLinkDb(conf, fs, linkdb1, init1);    createLinkDb(conf, fs, linkdb2, init2);    LinkDbMerger merger = new LinkDbMerger(conf);        merger.merge(output, new Path[] { linkdb1, linkdb2 }, false, false);        reader = new LinkDbReader(conf, output);    Iterator<String> it = expected.keySet().iterator();    while (it.hasNext()) {        String url = it.next();                String[] vals = expected.get(url);        Inlinks inlinks = reader.getInlinks(new Text(url));                Assert.assertNotNull(inlinks);        ArrayList<String> links = new ArrayList<String>();        Iterator<?> it2 = inlinks.iterator();        while (it2.hasNext()) {            Inlink in = (Inlink) it2.next();            links.add(in.getFromUrl());        }        for (int i = 0; i < vals.length; i++) {                        Assert.assertTrue(links.contains(vals[i]));        }    }    reader.close();    fs.delete(testDir, true);}
1
private void createLinkDb(Configuration config, FileSystem fs, Path linkdb, TreeMap<String, String[]> init) throws Exception
{        Path dir = new Path(linkdb, LinkDb.CURRENT_NAME);    Option wKeyOpt = MapFile.Writer.keyClass(Text.class);    org.apache.hadoop.io.SequenceFile.Writer.Option wValueOpt = SequenceFile.Writer.valueClass(Inlinks.class);    MapFile.Writer writer = new MapFile.Writer(config, new Path(dir, "part-00000"), wKeyOpt, wValueOpt);    Iterator<String> it = init.keySet().iterator();    while (it.hasNext()) {        String key = it.next();        Inlinks inlinks = new Inlinks();        String[] vals = init.get(key);        for (int i = 0; i < vals.length; i++) {            Inlink in = new Inlink(vals[i], vals[i]);            inlinks.add(in);        }        writer.append(new Text(key), inlinks);    }    writer.close();}
1
public void testGetSignature()
{    Configuration conf = NutchConfiguration.create();    Signature signature1 = SignatureFactory.getSignature(conf);    Signature signature2 = SignatureFactory.getSignature(conf);    Assert.assertNotNull(signature1);    Assert.assertNotNull(signature2);    Assert.assertEquals(signature1, signature2);}
0
public void testGetSignature()
{    Configuration conf = NutchConfiguration.create();    Signature textProf = new TextProfileSignature();    textProf.setConf(conf);    String text = "Hello World The Quick Brown Fox Jumped Over the Lazy Fox";    ParseData pd = new ParseData(ParseStatus.STATUS_SUCCESS, "Hello World", new Outlink[0], new Metadata());    byte[] signature1 = textProf.calculate(new Content(), new ParseImpl(text, pd));    Assert.assertNotNull(signature1);    List<String> words = Arrays.asList(text.split("\\s"));    Collections.shuffle(words);    String text2 = String.join(" ", words);    byte[] signature2 = textProf.calculate(new Content(), new ParseImpl(text2, pd));    Assert.assertNotNull(signature2);    Assert.assertEquals(StringUtil.toHexString(signature1), StringUtil.toHexString(signature2));}
0
public void testCrawlDbReducerPageRetrySchedule()
{        ContinuousCrawlTestUtil crawlUtil = new ContinuousCrawlTestFetchRetry();        try {        if (!crawlUtil.run(150)) {            fail("fetch_retry did not result in a db_gone if retry counter > maxRetries (NUTCH-578)");        }    } catch (IOException e) {        e.printStackTrace();    }}
1
protected CrawlDatum fetch(CrawlDatum datum, long currentTime)
{    datum.setStatus(fetchStatus);    datum.setFetchTime(currentTime);    totalRetries++;    return datum;}
0
protected boolean check(CrawlDatum result)
{    if (result.getRetriesSinceFetch() > retryMax) {            } else if (result.getRetriesSinceFetch() == Byte.MAX_VALUE) {            } else if (result.getRetriesSinceFetch() < 0) {                return false;    }        if (totalRetries < retryMax) {        if (result.getStatus() == STATUS_DB_UNFETCHED) {                        result.getRetriesSinceFetch();            return true;        }    } else {        if (result.getStatus() == STATUS_DB_GONE) {                        return true;        }    }        return false;}
1
public void testAdaptiveFetchScheduleSyncDelta()
{        Context context = CrawlDBTestUtil.createContext();    Configuration conf = context.getConfiguration();        conf.setLong("db.fetch.interval.default", 172800);        conf.setLong("db.fetch.schedule.adaptive.min_interval", 86400);        conf.setLong("db.fetch.schedule.adaptive.max_interval", 604800);        conf.setLong("db.fetch.interval.max", 604800);    conf.set("db.fetch.schedule.class", "org.apache.nutch.crawl.AdaptiveFetchSchedule");    ContinuousCrawlTestUtil crawlUtil = new CrawlTestFetchScheduleNotModifiedFetchTime(context);    crawlUtil.setInterval(FetchSchedule.SECONDS_PER_DAY / 3);    try {        if (!crawlUtil.run(100)) {            fail("failed: sync_delta calculation with AdaptiveFetchSchedule");        }    } catch (IOException e) {        e.printStackTrace();    }}
1
protected CrawlDatum fetch(CrawlDatum datum, long currentTime)
{        fetchTime = currentTime;    return super.fetch(datum, currentTime);}
0
protected boolean check(CrawlDatum result)
{    if (result.getStatus() == STATUS_DB_NOTMODIFIED) {                long secondsUntilNextFetch = (result.getFetchTime() - fetchTime) / 1000L;        if (secondsUntilNextFetch < -1) {                                    return false;        }        if (secondsUntilNextFetch < 60) {                                            }                if (secondsUntilNextFetch + 60 < minInterval || secondsUntilNextFetch - 60 > maxInterval) {                            }    }    return true;}
1
public void setUp() throws Exception
{    conf = CrawlDBTestUtil.createContext().getConfiguration();    fs = FileSystem.get(conf);    fs.delete(testdir, true);    urlPath = new Path(testdir, "urls");    crawldbPath = new Path(testdir, "crawldb");    segmentsPath = new Path(testdir, "segments");    server = CrawlDBTestUtil.getServer(conf.getInt("content.server.port", 50000), "build/test/data/fetch-test-site");    server.start();}
0
public void tearDown() throws Exception
{    server.stop();    for (int i = 0; i < 5; i++) {        if (!server.isStopped()) {            Thread.sleep(1000);        }    }    fs.delete(testdir, true);}
0
public void testFetch() throws IOException, ClassNotFoundException, InterruptedException
{        ArrayList<String> urls = new ArrayList<String>();    addUrl(urls, "index.html");    addUrl(urls, "pagea.html");    addUrl(urls, "pageb.html");    addUrl(urls, "dup_of_pagea.html");    addUrl(urls, "nested_spider_trap.html");    addUrl(urls, "exception.html");    CrawlDBTestUtil.generateSeedList(fs, urlPath, urls);        Injector injector = new Injector(conf);    injector.inject(crawldbPath, urlPath);        Generator g = new Generator(conf);    Path[] generatedSegment = g.generate(crawldbPath, segmentsPath, 1, Long.MAX_VALUE, Long.MAX_VALUE, false, false);    long time = System.currentTimeMillis();        Fetcher fetcher = new Fetcher(conf);        conf.setBoolean("fetcher.parse", true);    fetcher.fetch(generatedSegment[0], 1);    time = System.currentTimeMillis() - time;        int minimumTime = (int) ((urls.size() + 1) * 1000 * conf.getFloat("fetcher.server.delay", 5));    Assert.assertTrue(time > minimumTime);        Path content = new Path(new Path(generatedSegment[0], Content.DIR_NAME), "part-r-00000/data");    @SuppressWarnings("resource")    SequenceFile.Reader reader = new SequenceFile.Reader(conf, SequenceFile.Reader.file(content));    ArrayList<String> handledurls = new ArrayList<String>();    READ_CONTENT: do {        Text key = new Text();        Content value = new Content();        if (!reader.next(key, value))            break READ_CONTENT;        String contentString = new String(value.getContent());        if (contentString.indexOf("Nutch fetcher test page") != -1) {            handledurls.add(key.toString());        }    } while (true);    reader.close();    Collections.sort(urls);    Collections.sort(handledurls);        Assert.assertEquals(urls.size(), handledurls.size());        Assert.assertTrue(handledurls.containsAll(urls));    Assert.assertTrue(urls.containsAll(handledurls));    handledurls.clear();        Path parseData = new Path(new Path(generatedSegment[0], ParseData.DIR_NAME), "part-r-00000/data");    reader = new SequenceFile.Reader(conf, SequenceFile.Reader.file(parseData));    READ_PARSE_DATA: do {        Text key = new Text();        ParseData value = new ParseData();        if (!reader.next(key, value))            break READ_PARSE_DATA;                                Metadata contentMeta = value.getContentMeta();        if (contentMeta.get(Nutch.SEGMENT_NAME_KEY) != null && contentMeta.get(Nutch.SIGNATURE_KEY) != null) {            handledurls.add(key.toString());        }    } while (true);    Collections.sort(handledurls);    Assert.assertEquals(urls.size(), handledurls.size());    Assert.assertTrue(handledurls.containsAll(urls));    Assert.assertTrue(urls.containsAll(handledurls));}
0
private void addUrl(ArrayList<String> urls, String page)
{    urls.add("http://127.0.0.1:" + server.getConnectors()[0].getPort() + "/" + page);}
0
public void testAgentNameCheck()
{    boolean failedNoAgentName = false;    conf.set("http.agent.name", "");    try {        conf.setBoolean("fetcher.parse", false);        Fetcher fetcher = new Fetcher(conf);        fetcher.fetch(null, 1);    } catch (IllegalArgumentException iae) {        String message = iae.getMessage();        failedNoAgentName = message.equals("Fetcher: No agents listed in " + "'http.agent.name' property.");    } catch (Exception e) {    }    Assert.assertTrue(failedNoAgentName);}
0
public void testBinaryContentBase64()
{    configuration = NutchConfiguration.create();    configuration.setBoolean(IndexerMapReduce.INDEXER_BINARY_AS_BASE64, true);    Charset[] testCharsets = { StandardCharsets.UTF_8, Charset.forName("iso-8859-1"), Charset.forName("iso-8859-2") };    for (Charset charset : testCharsets) {                String htmlDoc = testHtmlDoc;        if (charset != StandardCharsets.UTF_8) {            htmlDoc = htmlDoc.replaceAll("utf-8", charset.name());            if (charset.name().equalsIgnoreCase("iso-8859-1")) {                                htmlDoc = htmlDoc.replaceAll("\\s*<[^>]+\\slang=\"cs\".+?\\n", "");            } else if (charset.name().equalsIgnoreCase("iso-8859-2")) {                                htmlDoc = htmlDoc.replaceAll("\\s*<[^>]+\\slang=\"fr\".+?\\n", "");            }        }        Content content = new Content(testUrl, testUrl, htmlDoc.getBytes(charset), htmlContentType, htmlMeta, configuration);        NutchDocument doc = runIndexer(crawlDatumDbFetched, crawlDatumFetchSuccess, parseText, parseData, content);        assertNotNull("No NutchDocument indexed", doc);        String binaryContentBase64 = (String) doc.getField("binaryContent").getValues().get(0);                String binaryContent = new String(Base64.decodeBase64(binaryContentBase64), charset);                assertEquals("Binary content (" + charset + ") not correctly saved as base64", htmlDoc, binaryContent);    }}
1
public NutchDocument runIndexer(CrawlDatum dbDatum, CrawlDatum fetchDatum, ParseText parseText, ParseData parseData, Content content)
{    List<NutchWritable> values = new ArrayList<NutchWritable>();    values.add(new NutchWritable(dbDatum));    values.add(new NutchWritable(fetchDatum));    values.add(new NutchWritable(parseText));    values.add(new NutchWritable(parseData));    values.add(new NutchWritable(content));    reduceDriver = ReduceDriver.newReduceDriver(reducer);    reduceDriver.getConfiguration().addResource(configuration);    reduceDriver.withInput(testUrlText, values);    List<Pair<Text, NutchIndexAction>> reduceResult;    NutchDocument doc = null;    try {        reduceResult = reduceDriver.run();        for (Pair<Text, NutchIndexAction> p : reduceResult) {            if (p.getSecond().action != NutchIndexAction.DELETE) {                doc = p.getSecond().doc;            }        }    } catch (IOException e) {            }    return doc;}
1
public void testNonExistingIndexingFilter() throws IndexingException
{    Configuration conf = NutchConfiguration.create();    conf.addResource("nutch-default.xml");    conf.addResource("crawl-tests.xml");    String class1 = "NonExistingFilter";    String class2 = "org.apache.nutch.indexer.basic.BasicIndexingFilter";    conf.set(IndexingFilters.INDEXINGFILTER_ORDER, class1 + " " + class2);    IndexingFilters filters = new IndexingFilters(conf);    filters.filter(new NutchDocument(), new ParseImpl("text", new ParseData(new ParseStatus(), "title", new Outlink[0], new Metadata())), new Text("http://www.example.com/"), new CrawlDatum(), new Inlinks());}
0
public void testNutchDocumentNullIndexingFilter() throws IndexingException
{    Configuration conf = NutchConfiguration.create();    conf.addResource("nutch-default.xml");    conf.addResource("crawl-tests.xml");    IndexingFilters filters = new IndexingFilters(conf);    NutchDocument doc = filters.filter(null, new ParseImpl("text", new ParseData(new ParseStatus(), "title", new Outlink[0], new Metadata())), new Text("http://www.example.com/"), new CrawlDatum(), new Inlinks());    Assert.assertNull(doc);}
0
public void testFilterCacheIndexingFilter() throws IndexingException
{    Configuration conf = NutchConfiguration.create();    conf.addResource("nutch-default.xml");    conf.addResource("crawl-tests.xml");    String class1 = "org.apache.nutch.indexer.basic.BasicIndexingFilter";    conf.set(IndexingFilters.INDEXINGFILTER_ORDER, class1);    IndexingFilters filters1 = new IndexingFilters(conf);    NutchDocument fdoc1 = filters1.filter(new NutchDocument(), new ParseImpl("text", new ParseData(new ParseStatus(), "title", new Outlink[0], new Metadata())), new Text("http://www.example.com/"), new CrawlDatum(), new Inlinks());        String class2 = "org.apache.nutch.indexer.metadata.MetadataIndexer";        Metadata md = new Metadata();    md.add("example", "data");        conf.set("index.content.md", "example");        conf.set(IndexingFilters.INDEXINGFILTER_ORDER, class1 + " " + class2);    IndexingFilters filters2 = new IndexingFilters(conf);    NutchDocument fdoc2 = filters2.filter(new NutchDocument(), new ParseImpl("text", new ParseData(new ParseStatus(), "title", new Outlink[0], md)), new Text("http://www.example.com/"), new CrawlDatum(), new Inlinks());    Assert.assertEquals(fdoc1.getFieldNames().size(), fdoc2.getFieldNames().size());}
0
public void testWriteNonNull()
{    Metadata met = new Metadata();    met.add(CONTENTTYPE, null);    met.add(CONTENTTYPE, "text/bogus");    met.add(CONTENTTYPE, "text/bogus2");    met = writeRead(met);    Assert.assertNotNull(met);    Assert.assertEquals(met.size(), 1);    boolean hasBogus = false, hasBogus2 = false;    String[] values = met.getValues(CONTENTTYPE);    Assert.assertNotNull(values);    Assert.assertEquals(values.length, 2);    for (int i = 0; i < values.length; i++) {        if (values[i].equals("text/bogus")) {            hasBogus = true;        }        if (values[i].equals("text/bogus2")) {            hasBogus2 = true;        }    }    Assert.assertTrue(hasBogus && hasBogus2);}
0
public void testAdd()
{    String[] values = null;    Metadata meta = new Metadata();    values = meta.getValues(CONTENTTYPE);    Assert.assertEquals(0, values.length);    meta.add(CONTENTTYPE, "value1");    values = meta.getValues(CONTENTTYPE);    Assert.assertEquals(1, values.length);    Assert.assertEquals("value1", values[0]);    meta.add(CONTENTTYPE, "value2");    values = meta.getValues(CONTENTTYPE);    Assert.assertEquals(2, values.length);    Assert.assertEquals("value1", values[0]);    Assert.assertEquals("value2", values[1]);            meta.add(CONTENTTYPE, "value1");    values = meta.getValues(CONTENTTYPE);    Assert.assertEquals(3, values.length);    Assert.assertEquals("value1", values[0]);    Assert.assertEquals("value2", values[1]);    Assert.assertEquals("value1", values[2]);}
0
public void testSet()
{    String[] values = null;    Metadata meta = new Metadata();    values = meta.getValues(CONTENTTYPE);    Assert.assertEquals(0, values.length);    meta.set(CONTENTTYPE, "value1");    values = meta.getValues(CONTENTTYPE);    Assert.assertEquals(1, values.length);    Assert.assertEquals("value1", values[0]);    meta.set(CONTENTTYPE, "value2");    values = meta.getValues(CONTENTTYPE);    Assert.assertEquals(1, values.length);    Assert.assertEquals("value2", values[0]);    meta.set(CONTENTTYPE, "new value 1");    meta.add("contenttype", "new value 2");    values = meta.getValues(CONTENTTYPE);    Assert.assertEquals(2, values.length);    Assert.assertEquals("new value 1", values[0]);    Assert.assertEquals("new value 2", values[1]);}
0
public void testSetProperties()
{    String[] values = null;    Metadata meta = new Metadata();    Properties props = new Properties();    meta.setAll(props);    Assert.assertEquals(0, meta.size());    props.setProperty("name-one", "value1.1");    meta.setAll(props);    Assert.assertEquals(1, meta.size());    values = meta.getValues("name-one");    Assert.assertEquals(1, values.length);    Assert.assertEquals("value1.1", values[0]);    props.setProperty("name-two", "value2.1");    meta.setAll(props);    Assert.assertEquals(2, meta.size());    values = meta.getValues("name-one");    Assert.assertEquals(1, values.length);    Assert.assertEquals("value1.1", values[0]);    values = meta.getValues("name-two");    Assert.assertEquals(1, values.length);    Assert.assertEquals("value2.1", values[0]);}
0
public void testGet()
{    Metadata meta = new Metadata();    Assert.assertNull(meta.get("a-name"));    meta.add("a-name", "value-1");    Assert.assertEquals("value-1", meta.get("a-name"));    meta.add("a-name", "value-2");    Assert.assertEquals("value-1", meta.get("a-name"));}
0
public void testIsMultiValued()
{    Metadata meta = new Metadata();    Assert.assertFalse(meta.isMultiValued("key"));    meta.add("key", "value1");    Assert.assertFalse(meta.isMultiValued("key"));    meta.add("key", "value2");    Assert.assertTrue(meta.isMultiValued("key"));}
0
public void testNames()
{    String[] names = null;    Metadata meta = new Metadata();    names = meta.names();    Assert.assertEquals(0, names.length);    meta.add("name-one", "value");    names = meta.names();    Assert.assertEquals(1, names.length);    Assert.assertEquals("name-one", names[0]);    meta.add("name-two", "value");    names = meta.names();    Assert.assertEquals(2, names.length);}
0
public void testRemove()
{    Metadata meta = new Metadata();    meta.remove("name-one");    Assert.assertEquals(0, meta.size());    meta.add("name-one", "value-1.1");    meta.add("name-one", "value-1.2");    meta.add("name-two", "value-2.2");    Assert.assertEquals(2, meta.size());    Assert.assertNotNull(meta.get("name-one"));    Assert.assertNotNull(meta.get("name-two"));    meta.remove("name-one");    Assert.assertEquals(1, meta.size());    Assert.assertNull(meta.get("name-one"));    Assert.assertNotNull(meta.get("name-two"));    meta.remove("name-two");    Assert.assertEquals(0, meta.size());    Assert.assertNull(meta.get("name-one"));    Assert.assertNull(meta.get("name-two"));}
0
public void testObject()
{    Metadata meta1 = new Metadata();    Metadata meta2 = new Metadata();    Assert.assertFalse(meta1.equals(null));    Assert.assertFalse(meta1.equals("String"));    Assert.assertTrue(meta1.equals(meta2));    meta1.add("name-one", "value-1.1");    Assert.assertFalse(meta1.equals(meta2));    meta2.add("name-one", "value-1.1");    Assert.assertTrue(meta1.equals(meta2));    meta1.add("name-one", "value-1.2");    Assert.assertFalse(meta1.equals(meta2));    meta2.add("name-one", "value-1.2");    Assert.assertTrue(meta1.equals(meta2));    meta1.add("name-two", "value-2.1");    Assert.assertFalse(meta1.equals(meta2));    meta2.add("name-two", "value-2.1");    Assert.assertTrue(meta1.equals(meta2));    meta1.add("name-two", "value-2.2");    Assert.assertFalse(meta1.equals(meta2));    meta2.add("name-two", "value-2.x");    Assert.assertFalse(meta1.equals(meta2));}
0
public void testWritable()
{    Metadata result = null;    Metadata meta = new Metadata();    result = writeRead(meta);    Assert.assertEquals(0, result.size());    meta.add("name-one", "value-1.1");    result = writeRead(meta);    Assert.assertEquals(1, result.size());    Assert.assertEquals(1, result.getValues("name-one").length);    Assert.assertEquals("value-1.1", result.get("name-one"));    meta.add("name-two", "value-2.1");    meta.add("name-two", "value-2.2");    result = writeRead(meta);    Assert.assertEquals(2, result.size());    Assert.assertEquals(1, result.getValues("name-one").length);    Assert.assertEquals("value-1.1", result.getValues("name-one")[0]);    Assert.assertEquals(2, result.getValues("name-two").length);    Assert.assertEquals("value-2.1", result.getValues("name-two")[0]);    Assert.assertEquals("value-2.2", result.getValues("name-two")[1]);}
0
private Metadata writeRead(Metadata meta)
{    Metadata readed = new Metadata();    try {        ByteArrayOutputStream out = new ByteArrayOutputStream();        meta.write(new DataOutputStream(out));        readed.readFields(new DataInputStream(new ByteArrayInputStream(out.toByteArray())));    } catch (IOException ioe) {        Assert.fail(ioe.toString());    }    return readed;}
0
public void testGetNormalizedName()
{    Assert.assertEquals("Content-Type", SpellCheckedMetadata.getNormalizedName("Content-Type"));    Assert.assertEquals("Content-Type", SpellCheckedMetadata.getNormalizedName("ContentType"));    Assert.assertEquals("Content-Type", SpellCheckedMetadata.getNormalizedName("Content-type"));    Assert.assertEquals("Content-Type", SpellCheckedMetadata.getNormalizedName("contenttype"));    Assert.assertEquals("Content-Type", SpellCheckedMetadata.getNormalizedName("contentype"));    Assert.assertEquals("Content-Type", SpellCheckedMetadata.getNormalizedName("contntype"));}
0
public void testAdd()
{    String[] values = null;    SpellCheckedMetadata meta = new SpellCheckedMetadata();    values = meta.getValues("contentype");    Assert.assertEquals(0, values.length);    meta.add("contentype", "value1");    values = meta.getValues("contentype");    Assert.assertEquals(1, values.length);    Assert.assertEquals("value1", values[0]);    meta.add("Content-Type", "value2");    values = meta.getValues("contentype");    Assert.assertEquals(2, values.length);    Assert.assertEquals("value1", values[0]);    Assert.assertEquals("value2", values[1]);            meta.add("ContentType", "value1");    values = meta.getValues("Content-Type");    Assert.assertEquals(3, values.length);    Assert.assertEquals("value1", values[0]);    Assert.assertEquals("value2", values[1]);    Assert.assertEquals("value1", values[2]);}
0
public void testSet()
{    String[] values = null;    SpellCheckedMetadata meta = new SpellCheckedMetadata();    values = meta.getValues("contentype");    Assert.assertEquals(0, values.length);    meta.set("contentype", "value1");    values = meta.getValues("contentype");    Assert.assertEquals(1, values.length);    Assert.assertEquals("value1", values[0]);    meta.set("Content-Type", "value2");    values = meta.getValues("contentype");    Assert.assertEquals(1, values.length);    Assert.assertEquals("value2", values[0]);    meta.set("contenttype", "new value 1");    meta.add("contenttype", "new value 2");    values = meta.getValues("contentype");    Assert.assertEquals(2, values.length);    Assert.assertEquals("new value 1", values[0]);    Assert.assertEquals("new value 2", values[1]);}
0
public void testSetProperties()
{    String[] values = null;    SpellCheckedMetadata meta = new SpellCheckedMetadata();    Properties props = new Properties();    meta.setAll(props);    Assert.assertEquals(0, meta.size());    props.setProperty("name-one", "value1.1");    meta.setAll(props);    Assert.assertEquals(1, meta.size());    values = meta.getValues("name-one");    Assert.assertEquals(1, values.length);    Assert.assertEquals("value1.1", values[0]);    props.setProperty("name-two", "value2.1");    meta.setAll(props);    Assert.assertEquals(2, meta.size());    values = meta.getValues("name-one");    Assert.assertEquals(1, values.length);    Assert.assertEquals("value1.1", values[0]);    values = meta.getValues("name-two");    Assert.assertEquals(1, values.length);    Assert.assertEquals("value2.1", values[0]);}
0
public void testGet()
{    SpellCheckedMetadata meta = new SpellCheckedMetadata();    Assert.assertNull(meta.get("a-name"));    meta.add("a-name", "value-1");    Assert.assertEquals("value-1", meta.get("a-name"));    meta.add("a-name", "value-2");    Assert.assertEquals("value-1", meta.get("a-name"));}
0
public void testIsMultiValued()
{    SpellCheckedMetadata meta = new SpellCheckedMetadata();    Assert.assertFalse(meta.isMultiValued("key"));    meta.add("key", "value1");    Assert.assertFalse(meta.isMultiValued("key"));    meta.add("key", "value2");    Assert.assertTrue(meta.isMultiValued("key"));}
0
public void testNames()
{    String[] names = null;    SpellCheckedMetadata meta = new SpellCheckedMetadata();    names = meta.names();    Assert.assertEquals(0, names.length);    meta.add("name-one", "value");    names = meta.names();    Assert.assertEquals(1, names.length);    Assert.assertEquals("name-one", names[0]);    meta.add("name-two", "value");    names = meta.names();    Assert.assertEquals(2, names.length);}
0
public void testRemove()
{    SpellCheckedMetadata meta = new SpellCheckedMetadata();    meta.remove("name-one");    Assert.assertEquals(0, meta.size());    meta.add("name-one", "value-1.1");    meta.add("name-one", "value-1.2");    meta.add("name-two", "value-2.2");    Assert.assertEquals(2, meta.size());    Assert.assertNotNull(meta.get("name-one"));    Assert.assertNotNull(meta.get("name-two"));    meta.remove("name-one");    Assert.assertEquals(1, meta.size());    Assert.assertNull(meta.get("name-one"));    Assert.assertNotNull(meta.get("name-two"));    meta.remove("name-two");    Assert.assertEquals(0, meta.size());    Assert.assertNull(meta.get("name-one"));    Assert.assertNull(meta.get("name-two"));}
0
public void testObject()
{    SpellCheckedMetadata meta1 = new SpellCheckedMetadata();    SpellCheckedMetadata meta2 = new SpellCheckedMetadata();    Assert.assertFalse(meta1.equals(null));    Assert.assertFalse(meta1.equals("String"));    Assert.assertTrue(meta1.equals(meta2));    meta1.add("name-one", "value-1.1");    Assert.assertFalse(meta1.equals(meta2));    meta2.add("name-one", "value-1.1");    Assert.assertTrue(meta1.equals(meta2));    meta1.add("name-one", "value-1.2");    Assert.assertFalse(meta1.equals(meta2));    meta2.add("name-one", "value-1.2");    Assert.assertTrue(meta1.equals(meta2));    meta1.add("name-two", "value-2.1");    Assert.assertFalse(meta1.equals(meta2));    meta2.add("name-two", "value-2.1");    Assert.assertTrue(meta1.equals(meta2));    meta1.add("name-two", "value-2.2");    Assert.assertFalse(meta1.equals(meta2));    meta2.add("name-two", "value-2.x");    Assert.assertFalse(meta1.equals(meta2));}
0
public void testWritable()
{    SpellCheckedMetadata result = null;    SpellCheckedMetadata meta = new SpellCheckedMetadata();    result = writeRead(meta);    Assert.assertEquals(0, result.size());    meta.add("name-one", "value-1.1");    result = writeRead(meta);    meta.add("Contenttype", "text/html");    Assert.assertEquals(1, result.size());    Assert.assertEquals(1, result.getValues("name-one").length);    Assert.assertEquals("value-1.1", result.get("name-one"));    meta.add("name-two", "value-2.1");    meta.add("name-two", "value-2.2");    result = writeRead(meta);    Assert.assertEquals(3, result.size());    Assert.assertEquals(1, result.getValues("name-one").length);    Assert.assertEquals("value-1.1", result.getValues("name-one")[0]);    Assert.assertEquals(2, result.getValues("name-two").length);    Assert.assertEquals("value-2.1", result.getValues("name-two")[0]);    Assert.assertEquals("value-2.2", result.getValues("name-two")[1]);    Assert.assertEquals("text/html", result.get(Metadata.CONTENT_TYPE));}
0
public final void testHandlingSpeed()
{    @SuppressWarnings("unused")    SpellCheckedMetadata result;    long start = System.currentTimeMillis();    for (int i = 0; i < NUM_ITERATIONS; i++) {        SpellCheckedMetadata scmd = constructSpellCheckedMetadata();        result = writeRead(scmd);    }    System.out.println(NUM_ITERATIONS + " spellchecked metadata I/O time:" + (System.currentTimeMillis() - start) + "ms.");}
0
private SpellCheckedMetadata writeRead(SpellCheckedMetadata meta)
{    SpellCheckedMetadata readed = new SpellCheckedMetadata();    try {        ByteArrayOutputStream out = new ByteArrayOutputStream();        meta.write(new DataOutputStream(out));        readed.readFields(new DataInputStream(new ByteArrayInputStream(out.toByteArray())));    } catch (IOException ioe) {        Assert.fail(ioe.toString());    }    return readed;}
0
public static final SpellCheckedMetadata constructSpellCheckedMetadata()
{    SpellCheckedMetadata scmd = new SpellCheckedMetadata();    scmd.add("Content-type", "foo/bar");    scmd.add("Connection", "close");    scmd.add("Last-Modified", "Sat, 09 Dec 2006 15:09:57 GMT");    scmd.add("Server", "Foobar");    scmd.add("Date", "Sat, 09 Dec 2006 18:07:20 GMT");    scmd.add("Accept-Ranges", "bytes");    scmd.add("ETag", "\"1234567-89-01234567\"");    scmd.add("Content-Length", "123");    scmd.add(Nutch.SEGMENT_NAME_KEY, "segmentzzz");    scmd.add(Nutch.SIGNATURE_KEY, "123");    return scmd;}
0
public void testNonExistingUrlFilter() throws URLFilterException
{    Configuration conf = NutchConfiguration.create();    String class1 = "NonExistingFilter";    String class2 = "org.apache.nutch.urlfilter.prefix.PrefixURLFilter";    conf.set(URLFilters.URLFILTER_ORDER, class1 + " " + class2);    URLFilters normalizers = new URLFilters(conf);    normalizers.filter("http://someurl/");}
0
public void testURLNormalizers()
{    Configuration conf = NutchConfiguration.create();    String clazz1 = "org.apache.nutch.net.urlnormalizer.regex.RegexURLNormalizer";    String clazz2 = "org.apache.nutch.net.urlnormalizer.basic.BasicURLNormalizer";    conf.set("urlnormalizer.order", clazz1 + " " + clazz2);    URLNormalizers normalizers = new URLNormalizers(conf, URLNormalizers.SCOPE_DEFAULT);    Assert.assertNotNull(normalizers);    try {        normalizers.normalize("http://www.example.com/", URLNormalizers.SCOPE_DEFAULT);    } catch (MalformedURLException mue) {        Assert.fail(mue.toString());    }        try {        String normalizedSlashes = normalizers.normalize("http://www.example.com//path/to//somewhere.html", URLNormalizers.SCOPE_DEFAULT);        Assert.assertEquals(normalizedSlashes, "http://www.example.com/path/to/somewhere.html");    } catch (MalformedURLException mue) {        Assert.fail(mue.toString());    }        try {        String normalizedHost = normalizers.normalize("http://www.example.org//path/to//somewhere.html", URLNormalizers.SCOPE_DEFAULT);        Assert.assertEquals(normalizedHost, "http://www.example.org/path/to/somewhere.html");    } catch (MalformedURLException mue) {        Assert.fail(mue.toString());    }        int pos1 = -1, pos2 = -1;    URLNormalizer[] impls = normalizers.getURLNormalizers(URLNormalizers.SCOPE_DEFAULT);    for (int i = 0; i < impls.length; i++) {        if (impls[i].getClass().getName().equals(clazz1))            pos1 = i;        if (impls[i].getClass().getName().equals(clazz2))            pos2 = i;    }    if (pos1 != -1 && pos2 != -1) {        Assert.assertTrue("RegexURLNormalizer before BasicURLNormalizer", pos1 < pos2);    }}
0
public void testGetNoOutlinks()
{    Outlink[] outlinks = null;    outlinks = OutlinkExtractor.getOutlinks(null, conf);    Assert.assertNotNull(outlinks);    Assert.assertEquals(0, outlinks.length);    outlinks = OutlinkExtractor.getOutlinks("", conf);    Assert.assertNotNull(outlinks);    Assert.assertEquals(0, outlinks.length);}
0
public void testGetOutlinksHttp()
{    Outlink[] outlinks = OutlinkExtractor.getOutlinks("Test with http://www.nutch.org/index.html is it found? " + "What about www.google.com at http://www.google.de " + "A longer URL could be http://www.sybit.com/solutions/portals.html", conf);    Assert.assertTrue("Url not found!", outlinks.length == 3);    Assert.assertEquals("Wrong URL", "http://www.nutch.org/index.html", outlinks[0].getToUrl());    Assert.assertEquals("Wrong URL", "http://www.google.de", outlinks[1].getToUrl());    Assert.assertEquals("Wrong URL", "http://www.sybit.com/solutions/portals.html", outlinks[2].getToUrl());}
0
public void testGetOutlinksHttp2()
{    Outlink[] outlinks = OutlinkExtractor.getOutlinks("Test with http://www.nutch.org/index.html is it found? " + "What about www.google.com at http://www.google.de " + "A longer URL could be http://www.sybit.com/solutions/portals.html", "http://www.sybit.de", conf);    Assert.assertTrue("Url not found!", outlinks.length == 3);    Assert.assertEquals("Wrong URL", "http://www.nutch.org/index.html", outlinks[0].getToUrl());    Assert.assertEquals("Wrong URL", "http://www.google.de", outlinks[1].getToUrl());    Assert.assertEquals("Wrong URL", "http://www.sybit.com/solutions/portals.html", outlinks[2].getToUrl());}
0
public void testGetOutlinksFtp()
{    Outlink[] outlinks = OutlinkExtractor.getOutlinks("Test with ftp://www.nutch.org is it found? " + "What about www.google.com at ftp://www.google.de", conf);    Assert.assertTrue("Url not found!", outlinks.length > 1);    Assert.assertEquals("Wrong URL", "ftp://www.nutch.org", outlinks[0].getToUrl());    Assert.assertEquals("Wrong URL", "ftp://www.google.de", outlinks[1].getToUrl());}
0
public void testAddSameObject() throws Exception
{    Set<Outlink> set = new HashSet<>();    Outlink o = new Outlink("http://www.example.com", "Example");    set.add(o);    set.add(o);    assertEquals("Adding the same Outlink twice", 1, set.size());}
0
public void testAddOtherObjectWithSameData() throws Exception
{    Set<Outlink> set = new HashSet<>();    Outlink o = new Outlink("http://www.example.com", "Example");    Outlink o1 = new Outlink("http://www.example.com", "Example");    assertTrue("The two Outlink objects are the same", o.equals(o1));    set.add(o);    set.add(o1);    assertEquals("The set should contain only 1 Outlink", 1, set.size());}
0
public void testParseData() throws Exception
{    String title = "The Foo Page";    Outlink[] outlinks = new Outlink[] { new Outlink("http://foo.com/", "Foo"), new Outlink("http://bar.com/", "Bar") };    Metadata metaData = new Metadata();    metaData.add("Language", "en/us");    metaData.add("Charset", "UTF-8");    ParseData r = new ParseData(ParseStatus.STATUS_SUCCESS, title, outlinks, metaData);    WritableTestUtils.testWritable(r, null);}
0
public void testMaxOutlinks() throws Exception
{    Outlink[] outlinks = new Outlink[128];    for (int i = 0; i < outlinks.length; i++) {        outlinks[i] = new Outlink("http://outlink.com/" + i, "Outlink" + i);    }    ParseData original = new ParseData(ParseStatus.STATUS_SUCCESS, "Max Outlinks Title", outlinks, new Metadata());    ParseData data = (ParseData) WritableTestUtils.writeRead(original, null);    Assert.assertEquals(outlinks.length, data.getOutlinks().length);}
0
public void setUp() throws Exception
{    conf = NutchConfiguration.create();    conf.set("plugin.includes", ".*");    conf.set("parse.plugin.file", "org/apache/nutch/parse/parse-plugin-test.xml");    parserFactory = new ParserFactory(conf);}
0
public void testGetExtensions() throws Exception
{    Extension ext = parserFactory.getExtensions("text/html").get(0);    Assert.assertEquals("parse-tika", ext.getDescriptor().getPluginId());    ext = parserFactory.getExtensions("text/html; charset=ISO-8859-1").get(0);    Assert.assertEquals("parse-tika", ext.getDescriptor().getPluginId());    ext = parserFactory.getExtensions("foo/bar").get(0);    Assert.assertEquals("parse-tika", ext.getDescriptor().getPluginId());}
0
public void testGetParsers() throws Exception
{    Parser[] parsers = parserFactory.getParsers("text/html", "http://foo.com");    Assert.assertNotNull(parsers);    Assert.assertEquals(1, parsers.length);    Assert.assertEquals("org.apache.nutch.parse.tika.TikaParser", parsers[0].getClass().getName());    parsers = parserFactory.getParsers("text/html; charset=ISO-8859-1", "http://foo.com");    Assert.assertNotNull(parsers);    Assert.assertEquals(1, parsers.length);    Assert.assertEquals("org.apache.nutch.parse.tika.TikaParser", parsers[0].getClass().getName());    parsers = parserFactory.getParsers("application/x-javascript", "http://foo.com");    Assert.assertNotNull(parsers);    Assert.assertEquals(1, parsers.length);    Assert.assertEquals("org.apache.nutch.parse.js.JSParseFilter", parsers[0].getClass().getName());    parsers = parserFactory.getParsers("text/plain", "http://foo.com");    Assert.assertNotNull(parsers);    Assert.assertEquals(1, parsers.length);    Assert.assertEquals("org.apache.nutch.parse.tika.TikaParser", parsers[0].getClass().getName());    Parser parser1 = parserFactory.getParsers("text/plain", "http://foo.com")[0];    Parser parser2 = parserFactory.getParsers("*", "http://foo.com")[0];    Assert.assertEquals("Different instances!", parser1.hashCode(), parser2.hashCode());                parsers = parserFactory.getParsers("text/rss", "http://foo.com");    Assert.assertNotNull(parsers);    Assert.assertEquals(1, parsers.length);    Assert.assertEquals("org.apache.nutch.parse.tika.TikaParser", parsers[0].getClass().getName());}
0
public void testParseText() throws Exception
{    String page = "Hello World The Quick Brown Fox Jumped Over the Lazy Fox";    ParseText s = new ParseText(page);    WritableTestUtils.testWritable(s);}
0
public String testGetExtension(String hello)
{    return hello + " World";}
0
public void startUp() throws PluginRuntimeException
{    System.err.println("start up Plugin: " + getDescriptor().getPluginId());}
0
public void shutDown() throws PluginRuntimeException
{    System.err.println("shutdown Plugin: " + getDescriptor().getPluginId());}
0
public void setUp() throws Exception
{    this.conf = NutchConfiguration.create();    conf.set("plugin.includes", ".*");            fPluginCount = 5;    createDummyPlugins(fPluginCount);    this.repository = PluginRepository.get(conf);}
0
public void tearDown() throws Exception
{    for (int i = 0; i < fFolders.size(); i++) {        File folder = fFolders.get(i);        delete(folder);        folder.delete();    }}
0
public void testPluginConfiguration()
{    String string = getPluginFolder();    File file = new File(string);    if (!file.exists()) {        file.mkdir();    }    Assert.assertTrue(file.exists());}
0
public void testLoadPlugins()
{    PluginDescriptor[] descriptors = repository.getPluginDescriptors();    int k = descriptors.length;    Assert.assertTrue(fPluginCount <= k);    for (int i = 0; i < descriptors.length; i++) {        PluginDescriptor descriptor = descriptors[i];        if (!descriptor.getPluginId().startsWith("getPluginFolder()")) {            continue;        }        Assert.assertEquals(1, descriptor.getExportedLibUrls().length);        Assert.assertEquals(1, descriptor.getNotExportedLibUrls().length);    }}
0
public void testRepositoryCache() throws IOException
{    Configuration config = NutchConfiguration.create();    PluginRepository repo = PluginRepository.get(config);    Job job = NutchJob.getInstance(config);    config = job.getConfiguration();    PluginRepository repo1 = PluginRepository.get(config);    Assert.assertTrue(repo == repo1);        config = new Configuration();    config.addResource("nutch-default.xml");    config.addResource("nutch-site.xml");    repo = PluginRepository.get(config);    job = NutchJob.getInstance(config);    config = job.getConfiguration();    repo1 = PluginRepository.get(config);    Assert.assertTrue(repo1 != repo);}
0
public void testGetExtensionAndAttributes()
{    String xpId = " sdsdsd";    ExtensionPoint extensionPoint = repository.getExtensionPoint(xpId);    Assert.assertEquals(extensionPoint, null);    Extension[] extension1 = repository.getExtensionPoint(getGetExtensionId()).getExtensions();    Assert.assertEquals(extension1.length, fPluginCount);    for (int i = 0; i < extension1.length; i++) {        Extension extension2 = extension1[i];        String string = extension2.getAttribute(getGetConfigElementName());        Assert.assertEquals(string, getParameterValue());    }}
0
public void testGetExtensionInstances() throws PluginRuntimeException
{    Extension[] extensions = repository.getExtensionPoint(getGetExtensionId()).getExtensions();    Assert.assertEquals(extensions.length, fPluginCount);    for (int i = 0; i < extensions.length; i++) {        Extension extension = extensions[i];        Object object = extension.getExtensionInstance();        if (!(object instanceof HelloWorldExtension))            Assert.fail(" object is not a instance of HelloWorldExtension");        ((ITestExtension) object).testGetExtension("Bla ");        String string = ((ITestExtension) object).testGetExtension("Hello");        Assert.assertEquals("Hello World", string);    }}
0
public void testGetClassLoader()
{    PluginDescriptor[] descriptors = repository.getPluginDescriptors();    for (int i = 0; i < descriptors.length; i++) {        PluginDescriptor descriptor = descriptors[i];        Assert.assertNotNull(descriptor.getClassLoader());    }}
0
public void testGetResources() throws IOException
{    PluginDescriptor[] descriptors = repository.getPluginDescriptors();    for (int i = 0; i < descriptors.length; i++) {        PluginDescriptor descriptor = descriptors[i];        if (!descriptor.getPluginId().startsWith("getPluginFolder()")) {            continue;        }        String value = descriptor.getResourceString("key", Locale.UK);        Assert.assertEquals("value", value);        value = descriptor.getResourceString("key", Locale.TRADITIONAL_CHINESE);        Assert.assertEquals("value", value);    }}
0
private String getPluginFolder()
{    String[] strings = conf.getStrings("plugin.folders");    if (strings == null || strings.length == 0)        Assert.fail("no plugin directory setuped..");    String name = strings[0];    return new PluginManifestParser(conf, this.repository).getPluginFolder(name).toString();}
0
private void createDummyPlugins(int pCount)
{    String string = getPluginFolder();    try {        File folder = new File(string);        folder.mkdir();        for (int i = 0; i < pCount; i++) {            String pluginFolder = string + File.separator + "DummyPlugin" + i;            File file = new File(pluginFolder);            file.mkdir();            fFolders.add(file);            createPluginManifest(i, file.getAbsolutePath());            createResourceFile(file.getAbsolutePath());        }    } catch (IOException e) {        e.printStackTrace();    }}
0
private void createResourceFile(String pFolderPath) throws FileNotFoundException, IOException
{    Properties properties = new Properties();    properties.setProperty("key", "value");    properties.store(new FileOutputStream(pFolderPath + File.separator + "messages" + ".properties"), "");}
0
private void delete(File path) throws IOException
{    File[] files = path.listFiles();    for (int i = 0; i < files.length; ++i) {        if (files[i].isDirectory())            delete(files[i]);        files[i].delete();    }}
0
private void createPluginManifest(int i, String pFolderPath) throws IOException
{    FileWriter out = new FileWriter(pFolderPath + File.separator + "plugin.xml");    String xml = "<?xml version=\"1.0\" encoding=\"UTF-8\"?>" + "<!--this is just a simple plugin for testing issues.-->" + "<plugin id=\"org.apache.nutch.plugin." + i + "\" name=\"" + i + "\" version=\"1.0\" provider-name=\"joa23\" " + "class=\"org.apache.nutch.plugin.SimpleTestPlugin\">" + "<extension-point id=\"aExtensioID\" " + "name=\"simple Parser Extension\" " + "schema=\"schema/testExtensionPoint.exsd\"/>" + "<runtime><library name=\"libs/exported.jar\"><extport/></library>" + "<library name=\"libs/not_exported.jar\"/></runtime>" + "<extension point=\"aExtensioID\">" + "<implementation name=\"simple Parser Extension\" " + "id=\"aExtensionId.\" class=\"org.apache.nutch.plugin.HelloWorldExtension\">" + "<parameter name=\"dummy-name\" value=\"a simple param value\"/>" + "</implementation></extension></plugin>";    out.write(xml);    out.flush();    out.close();}
0
private String getParameterValue()
{    return "a simple param value";}
0
private static String getGetExtensionId()
{    return "aExtensioID";}
0
private static String getGetConfigElementName()
{    return "dummy-name";}
0
public static void main(String[] args) throws IOException
{    new TestPluginSystem().createPluginManifest(1, "/");}
0
public void testContent() throws Exception
{    String page = "<HTML><BODY><H1>Hello World</H1><P>The Quick Brown Fox Jumped Over the Lazy Fox.</BODY></HTML>";    String url = "http://www.foo.com/";    SpellCheckedMetadata metaData = new SpellCheckedMetadata();    metaData.add("Host", "www.foo.com");    metaData.add("Content-Type", "text/html");    Content r = new Content(url, url, page.getBytes("UTF8"), "text/html", metaData, conf);    WritableTestUtils.testWritable(r);    Assert.assertEquals("text/html", r.getMetadata().get("Content-Type"));    Assert.assertEquals("text/html", r.getMetadata().get("content-type"));    Assert.assertEquals("text/html", r.getMetadata().get("CONTENTYPE"));}
0
public void testGetContentType() throws Exception
{    Content c = null;    Metadata p = new Metadata();    c = new Content("http://www.foo.com/", "http://www.foo.com/", "".getBytes("UTF8"), "text/html; charset=UTF-8", p, conf);    Assert.assertEquals("text/html", c.getContentType());    c = new Content("http://www.foo.com/foo.html", "http://www.foo.com/", "".getBytes("UTF8"), "", p, conf);    Assert.assertEquals("text/html", c.getContentType());    c = new Content("http://www.foo.com/foo.html", "http://www.foo.com/", "".getBytes("UTF8"), null, p, conf);    Assert.assertEquals("text/html", c.getContentType());    c = new Content("http://www.foo.com/", "http://www.foo.com/", "<html></html>".getBytes("UTF8"), "", p, conf);    Assert.assertEquals("text/html", c.getContentType());    c = new Content("http://www.foo.com/foo.html", "http://www.foo.com/", "<html></html>".getBytes("UTF8"), "text/plain", p, conf);    Assert.assertEquals("text/html", c.getContentType());    c = new Content("http://www.foo.com/foo.png", "http://www.foo.com/", "<html></html>".getBytes("UTF8"), "text/plain", p, conf);    Assert.assertEquals("text/html", c.getContentType());    c = new Content("http://www.foo.com/", "http://www.foo.com/", "".getBytes("UTF8"), "", p, conf);    Assert.assertEquals(MimeTypes.OCTET_STREAM, c.getContentType());    c = new Content("http://www.foo.com/", "http://www.foo.com/", "".getBytes("UTF8"), null, p, conf);    Assert.assertNotNull(c.getContentType());}
0
public void setUp() throws Exception
{    conf = NutchConfiguration.create();    conf.set("plugin.includes", ".*");    conf.set("http.agent.name", "test-bot");    factory = new ProtocolFactory(conf);}
0
public void testGetProtocol()
{        try {        factory.getProtocol("xyzxyz://somehost");        Assert.fail("Must throw ProtocolNotFound");    } catch (ProtocolNotFound e) {        } catch (Exception ex) {        Assert.fail("Must not throw any other exception");    }    Protocol httpProtocol = null;        try {        httpProtocol = factory.getProtocol("http://somehost");        Assert.assertNotNull(httpProtocol);    } catch (Exception ex) {        Assert.fail("Must not throw any other exception");    }        try {        Assert.assertTrue(httpProtocol == factory.getProtocol("http://somehost"));    } catch (ProtocolNotFound e) {        Assert.fail("Must not throw any exception");    }}
0
public void testContains()
{    Assert.assertTrue(factory.contains("http", "http"));    Assert.assertTrue(factory.contains("http", "http,ftp"));    Assert.assertTrue(factory.contains("http", "   http ,   ftp"));    Assert.assertTrue(factory.contains("smb", "ftp,smb,http"));    Assert.assertFalse(factory.contains("smb", "smbb"));}
0
public void setUp() throws Exception
{    conf = NutchConfiguration.create();    fs = FileSystem.get(conf);    testDir = new Path(conf.get("hadoop.tmp.dir"), "merge-" + System.currentTimeMillis());    seg1 = new Path(testDir, "seg1");    seg2 = new Path(testDir, "seg2");    out = new Path(testDir, "out");        System.err.println("Creating large segment 1...");    DecimalFormat df = new DecimalFormat("0000000");    Text k = new Text();    Path ptPath = new Path(new Path(seg1, ParseText.DIR_NAME), "part-00000");    Option kOpt = MapFile.Writer.keyClass(Text.class);    org.apache.hadoop.io.SequenceFile.Writer.Option vOpt = SequenceFile.Writer.valueClass(ParseText.class);    MapFile.Writer w = new MapFile.Writer(conf, ptPath, kOpt, vOpt);    long curSize = 0;    countSeg1 = 0;    FileStatus fileStatus = fs.getFileStatus(ptPath);    long blkSize = fileStatus.getBlockSize();    while (curSize < blkSize * 2) {        k.set("seg1-" + df.format(countSeg1));        w.append(k, new ParseText("seg1 text " + countSeg1));        countSeg1++;                curSize += 40;    }    w.close();    System.err.println(" - done: " + countSeg1 + " records.");    System.err.println("Creating large segment 2...");    ptPath = new Path(new Path(seg2, ParseText.DIR_NAME), "part-00000");    Option wKeyOpt = MapFile.Writer.keyClass(Text.class);    org.apache.hadoop.io.SequenceFile.Writer.Option wValueOpt = SequenceFile.Writer.valueClass(ParseText.class);    w = new MapFile.Writer(conf, ptPath, wKeyOpt, wValueOpt);    curSize = 0;    countSeg2 = 0;    while (curSize < blkSize * 2) {        k.set("seg2-" + df.format(countSeg2));        w.append(k, new ParseText("seg2 text " + countSeg2));        countSeg2++;                curSize += 40;    }    w.close();    System.err.println(" - done: " + countSeg2 + " records.");}
0
public void tearDown() throws Exception
{    fs.delete(testDir, true);}
0
public void testLargeMerge() throws Exception
{    SegmentMerger merger = new SegmentMerger(conf);    merger.merge(out, new Path[] { seg1, seg2 }, false, false, -1);        FileStatus[] stats = fs.listStatus(out);        Assert.assertEquals(1, stats.length);    Path outSeg = stats[0].getPath();    Text k = new Text();    ParseText v = new ParseText();    MapFile.Reader[] readers = MapFileOutputFormat.getReaders(new Path(outSeg, ParseText.DIR_NAME), conf);    int cnt1 = 0, cnt2 = 0;    for (MapFile.Reader r : readers) {        while (r.next(k, v)) {            String ks = k.toString();            String vs = v.getText();            if (ks.startsWith("seg1-")) {                cnt1++;                Assert.assertTrue(vs.startsWith("seg1 "));            } else if (ks.startsWith("seg2-")) {                cnt2++;                Assert.assertTrue(vs.startsWith("seg2 "));            }        }        r.close();    }    Assert.assertEquals(countSeg1, cnt1);    Assert.assertEquals(countSeg2, cnt2);}
0
public void setUp() throws Exception
{    conf = NutchConfiguration.create();    fs = FileSystem.get(conf);    rnd = new Random();}
0
public void testSingleRandomSequence() throws Exception
{    Assert.assertEquals(Byte.valueOf(CrawlDatum.STATUS_FETCH_SUCCESS), Byte.valueOf(executeSequence(CrawlDatum.STATUS_FETCH_GONE, CrawlDatum.STATUS_FETCH_SUCCESS, 256, false)));}
0
public void testMostlyRedirects() throws Exception
{        Path testDir = new Path(conf.get("hadoop.tmp.dir"), "merge-" + System.currentTimeMillis());    Path segment1 = new Path(testDir, "20140110114943");    Path segment2 = new Path(testDir, "20140110114832");    Path segment3 = new Path(testDir, "20140110114558");    Path segment4 = new Path(testDir, "20140110114930");    Path segment5 = new Path(testDir, "20140110114545");    Path segment6 = new Path(testDir, "20140110114507");    Path segment7 = new Path(testDir, "20140110114903");    Path segment8 = new Path(testDir, "20140110114724");    createSegment(segment1, CrawlDatum.STATUS_FETCH_SUCCESS, true);    createSegment(segment2, CrawlDatum.STATUS_FETCH_SUCCESS, true);    createSegment(segment3, CrawlDatum.STATUS_FETCH_SUCCESS, true);    createSegment(segment4, CrawlDatum.STATUS_FETCH_SUCCESS, true);    createSegment(segment5, CrawlDatum.STATUS_FETCH_SUCCESS, true);    createSegment(segment6, CrawlDatum.STATUS_FETCH_SUCCESS, false);    createSegment(segment7, CrawlDatum.STATUS_FETCH_SUCCESS, true);    createSegment(segment8, CrawlDatum.STATUS_FETCH_SUCCESS, true);        Path mergedSegment = merge(testDir, new Path[] { segment1, segment2, segment3, segment4, segment5, segment6, segment7, segment8 });    Byte status = Byte.valueOf(status = checkMergedSegment(testDir, mergedSegment));    Assert.assertEquals(Byte.valueOf(CrawlDatum.STATUS_FETCH_SUCCESS), status);}
0
public void testRandomizedSequences() throws Exception
{    for (int i = 0; i < rnd.nextInt(16) + 16; i++) {        byte expectedStatus = (byte) (rnd.nextInt(6) + 0x21);        while (expectedStatus == CrawlDatum.STATUS_FETCH_RETRY || expectedStatus == CrawlDatum.STATUS_FETCH_NOTMODIFIED) {                        expectedStatus = (byte) (rnd.nextInt(6) + 0x21);        }        byte randomStatus = (byte) (rnd.nextInt(6) + 0x21);        int rounds = rnd.nextInt(16) + 32;        boolean withRedirects = rnd.nextBoolean();        byte resultStatus = executeSequence(randomStatus, expectedStatus, rounds, withRedirects);        Assert.assertEquals("Expected status = " + CrawlDatum.getStatusName(expectedStatus) + ", but got " + CrawlDatum.getStatusName(resultStatus) + " when merging " + rounds + " segments" + (withRedirects ? " with redirects" : ""), expectedStatus, resultStatus);    }}
0
public void testRandomTestSequenceWithRedirects() throws Exception
{    Assert.assertEquals(Byte.valueOf(CrawlDatum.STATUS_FETCH_SUCCESS), Byte.valueOf(executeSequence(CrawlDatum.STATUS_FETCH_GONE, CrawlDatum.STATUS_FETCH_SUCCESS, 128, true)));}
0
public void testFixedSequence() throws Exception
{        Path testDir = new Path(conf.get("hadoop.tmp.dir"), "merge-" + System.currentTimeMillis());    Path segment1 = new Path(testDir, "00001");    Path segment2 = new Path(testDir, "00002");    Path segment3 = new Path(testDir, "00003");    createSegment(segment1, CrawlDatum.STATUS_FETCH_GONE, false);    createSegment(segment2, CrawlDatum.STATUS_FETCH_GONE, true);    createSegment(segment3, CrawlDatum.STATUS_FETCH_SUCCESS, false);        Path mergedSegment = merge(testDir, new Path[] { segment1, segment2, segment3 });    Byte status = Byte.valueOf(status = checkMergedSegment(testDir, mergedSegment));    Assert.assertEquals(Byte.valueOf(CrawlDatum.STATUS_FETCH_SUCCESS), status);}
0
public void testRedirFetchInOneSegment() throws Exception
{        Path testDir = new Path(conf.get("hadoop.tmp.dir"), "merge-" + System.currentTimeMillis());    Path segment = new Path(testDir, "00001");    createSegment(segment, CrawlDatum.STATUS_FETCH_SUCCESS, true, true);        Path mergedSegment = merge(testDir, new Path[] { segment });    Byte status = Byte.valueOf(status = checkMergedSegment(testDir, mergedSegment));    Assert.assertEquals(Byte.valueOf(CrawlDatum.STATUS_FETCH_SUCCESS), status);}
0
public void testEndsWithRedirect() throws Exception
{        Path testDir = new Path(conf.get("hadoop.tmp.dir"), "merge-" + System.currentTimeMillis());    Path segment1 = new Path(testDir, "00001");    Path segment2 = new Path(testDir, "00002");    createSegment(segment1, CrawlDatum.STATUS_FETCH_SUCCESS, false);    createSegment(segment2, CrawlDatum.STATUS_FETCH_SUCCESS, true);        Path mergedSegment = merge(testDir, new Path[] { segment1, segment2 });    Byte status = Byte.valueOf(status = checkMergedSegment(testDir, mergedSegment));    Assert.assertEquals(Byte.valueOf(CrawlDatum.STATUS_FETCH_SUCCESS), status);}
0
protected byte executeSequence(byte firstStatus, byte lastStatus, int rounds, boolean redirect) throws Exception
{        Path testDir = new Path(conf.get("hadoop.tmp.dir"), "merge-" + System.currentTimeMillis());        DecimalFormat df = new DecimalFormat("0000000");        Path[] segmentPaths = new Path[rounds];    for (int i = 0; i < rounds; i++) {        String segmentName = df.format(i);        segmentPaths[i] = new Path(testDir, segmentName);    }        createSegment(segmentPaths[0], firstStatus, false);        for (int i = 1; i < rounds - 1; i++) {                byte status = (byte) (rnd.nextInt(6) + 0x21);                boolean addRedirect = redirect ? rnd.nextBoolean() : false;                        boolean addFetch = addRedirect ? rnd.nextBoolean() : true;        createSegment(segmentPaths[i], status, addFetch, addRedirect);    }            createSegment(segmentPaths[rounds - 1], lastStatus, true, redirect ? rnd.nextBoolean() : false);        Path mergedSegment = merge(testDir, segmentPaths);        return checkMergedSegment(testDir, mergedSegment);}
0
protected byte checkMergedSegment(Path testDir, Path mergedSegment) throws Exception
{        MapFile.Reader[] readers = MapFileOutputFormat.getReaders(new Path(mergedSegment, CrawlDatum.FETCH_DIR_NAME), conf);    Text key = new Text();    CrawlDatum value = new CrawlDatum();    byte finalStatus = 0x0;    for (MapFile.Reader reader : readers) {        while (reader.next(key, value)) {                                    if (CrawlDatum.hasFetchStatus(value) && key.toString().equals("http://nutch.apache.org/")) {                finalStatus = value.getStatus();            }        }                reader.close();    }        fs.delete(testDir, true);            return finalStatus;}
1
protected Path merge(Path testDir, Path[] segments) throws Exception
{        Path out = new Path(testDir, "out");        SegmentMerger merger = new SegmentMerger(conf);    merger.merge(out, segments, false, false, -1);    FileStatus[] stats = fs.listStatus(out);    Assert.assertEquals(1, stats.length);    return stats[0].getPath();}
0
protected void createSegment(Path segment, byte status, boolean redirect) throws Exception
{    if (redirect) {        createSegment(segment, status, false, true);    } else {        createSegment(segment, status, true, false);    }}
0
protected void createSegment(Path segment, byte status, boolean fetch, boolean redirect) throws Exception
{            String url = "http://nutch.apache.org/";        String redirectUrl = "http://nutch.apache.org/i_redirect_to_the_root/";        CrawlDatum value = new CrawlDatum();        Path crawlFetchPath = new Path(new Path(segment, CrawlDatum.FETCH_DIR_NAME), "part-00000");        Option wKeyOpt = MapFile.Writer.keyClass(Text.class);    org.apache.hadoop.io.SequenceFile.Writer.Option wValueOpt = SequenceFile.Writer.valueClass(CrawlDatum.class);    MapFile.Writer writer = new MapFile.Writer(conf, crawlFetchPath, wKeyOpt, wValueOpt);        if (redirect) {                        value = new CrawlDatum();        value.setStatus(CrawlDatum.STATUS_LINKED);        writer.append(new Text(url), value);    }        if (fetch) {                        value.setStatus(status);                writer.append(new Text(url), value);    }        if (redirect) {                        value.setStatus(CrawlDatum.STATUS_FETCH_REDIR_TEMP);        writer.append(new Text(redirectUrl), value);    }        writer.close();}
1
public void testNutchServerStartup()
{    boolean isRunning = false;    for (int i = 0; i < port.length; i++) {        try {            startServer(port[i]);            isRunning = true;            break;        } catch (Exception e) {                    }    }    if (!isRunning) {            } else {                WebClient client = WebClient.create(ENDPOINT_ADDRESS + server.getPort());        @SuppressWarnings("unused")        Response response = client.path("admin").get();                response = client.path("stop").get();        }}
1
private void startServer(int port) throws Exception
{    NutchServer.setPort(port);    NutchServer.startServer();}
0
public void handle(String target, HttpServletRequest req, HttpServletResponse res, int dispatch) throws IOException, ServletException
{    Request base_request = (req instanceof Request) ? (Request) req : HttpConnection.getCurrentConnection().getRequest();    res.addHeader("X-TestbedHandlers", this.getClass().getSimpleName());    handle(base_request, res, target, dispatch);}
0
public void addMyHeader(HttpServletResponse res, String name, String value)
{    name = "X-" + this.getClass().getSimpleName() + "-" + name;    res.addHeader(name, value);}
0
public void handle(Request req, HttpServletResponse res, String target, int dispatch) throws IOException, ServletException
{    try {        int del = random ? r.nextInt(delay) : delay;        Thread.sleep(del);        addMyHeader(res, "Delay", String.valueOf(del));    } catch (Exception e) {    }}
0
public void handle(Request req, HttpServletResponse res, String target, int dispatch) throws IOException, ServletException
{    HttpURI u = req.getUri();    String uri = u.toString();        addMyHeader(res, "URI", uri);        req.setHandled(true);    res.addHeader("X-Handled-By", getClass().getSimpleName());    if (uri.endsWith("/robots.txt")) {        return;    }    res.setContentType("text/html");    try {        OutputStream os = res.getOutputStream();        byte[] bytes = testA.getBytes("UTF-8");        os.write(bytes);                String p = "<p>URI: " + uri + "</p>\r\n";        os.write(p.getBytes());                String base;        if (u.getPath().length() > 5) {            base = u.getPath().substring(0, u.getPath().length() - 5);        } else {            base = u.getPath();        }        String prefix = u.getScheme() + "://" + u.getHost();        if (u.getPort() != 80 && u.getPort() != -1)            base += ":" + u.getPort();        if (!base.startsWith("/"))            prefix += "/";        prefix = prefix + base;        for (int i = 0; i < 10; i++) {            String link = "<p><a href='" + prefix;            if (!prefix.endsWith("/")) {                link += "/";            }            link += i + ".html'>outlink " + i + "</a></p>\r\n";            os.write(link.getBytes());        }                for (int i = 0; i < 5; i++) {                        int h = r.nextInt(1000000);            String link = "<p><a href='http://www.fake-" + h + ".com/'>fake host " + h + "</a></p>\r\n";            os.write(link.getBytes());        }                String link = "<p><a href='" + u.getScheme() + "://" + u.getHost();        if (u.getPort() != 80 && u.getPort() != -1)            link += ":" + u.getPort();        link += "/'>site " + u.getHost() + "</a></p>\r\n";        os.write(link.getBytes());        os.write(testB.getBytes());        res.flushBuffer();    } catch (IOException ioe) {    }}
0
public void handle(Request req, HttpServletResponse res, String target, int dispatch) throws IOException, ServletException
{    }
1
public void doFilter(ServletRequest req, ServletResponse res, FilterChain chain) throws IOException, ServletException
{    ((HttpServletResponse) res).addHeader("X-Handled-By", "AsyncProxyHandler");    ((HttpServletResponse) res).addHeader("X-TestbedHandlers", "AsyncProxyHandler");    try {        chain.doFilter(req, res);    } catch (Throwable e) {        ((HttpServletResponse) res).sendError(HttpServletResponse.SC_BAD_REQUEST, e.toString());    }}
0
public void init(FilterConfig arg0) throws ServletException
{}
0
public void handle(Request req, HttpServletResponse res, String target, int dispatch) throws IOException, ServletException
{        req.setHandled(true);    res.addHeader("X-Handled-By", getClass().getSimpleName());    addMyHeader(res, "URI", req.getUri().toString());    res.sendError(HttpServletResponse.SC_NOT_FOUND, "Not found: " + req.getUri().toString());}
0
public static void main(String[] args) throws Exception
{    if (args.length == 0) {        System.err.println("TestbedProxy [-seg <segment_name> | -segdir <segments>] [-port <nnn>] [-forward] [-fake] [-delay nnn] [-debug]");        System.err.println("-seg <segment_name>\tpath to a single segment (can be specified multiple times)");        System.err.println("-segdir <segments>\tpath to a parent directory of multiple segments (as above)");        System.err.println("-port <nnn>\trun the proxy on port <nnn> (special permissions may be needed for ports < 1024)");        System.err.println("-forward\tif specified, requests to all unknown urls will be passed to");        System.err.println("\t\toriginal servers. If false (default) unknown urls generate 404 Not Found.");        System.err.println("-delay\tdelay every response by nnn seconds. If delay is negative use a random value up to nnn");        System.err.println("-fake\tif specified, requests to all unknown urls will succeed with fake content");        System.exit(-1);    }    Configuration conf = NutchConfiguration.create();    int port = conf.getInt("segment.proxy.port", 8181);    boolean forward = false;    boolean fake = false;    boolean delay = false;    boolean debug = false;    int delayVal = 0;    HashSet<Path> segs = new HashSet<Path>();    for (int i = 0; i < args.length; i++) {        if (args[i].equals("-segdir")) {            FileSystem fs = FileSystem.get(conf);            FileStatus[] fstats = fs.listStatus(new Path(args[++i]));            Path[] paths = HadoopFSUtil.getPaths(fstats);            segs.addAll(Arrays.asList(paths));        } else if (args[i].equals("-port")) {            port = Integer.parseInt(args[++i]);        } else if (args[i].equals("-forward")) {            forward = true;        } else if (args[i].equals("-delay")) {            delay = true;            delayVal = Integer.parseInt(args[++i]);        } else if (args[i].equals("-fake")) {            fake = true;        } else if (args[i].equals("-debug")) {            debug = true;        } else if (args[i].equals("-seg")) {            segs.add(new Path(args[++i]));        } else {                        System.exit(-1);        }    }        Server server = new Server();    SocketConnector connector = new SocketConnector();    connector.setPort(port);    connector.setResolveNames(false);    server.addConnector(connector);        HandlerList list = new HandlerList();    server.addHandler(list);    if (debug) {                list.addHandler(new LogDebugHandler());    }    if (delay) {                list.addHandler(new DelayHandler(delayVal));    }                Iterator<Path> it = segs.iterator();    while (it.hasNext()) {        Path p = it.next();        try {            SegmentHandler segment = new SegmentHandler(conf, p);            list.addHandler(segment);                    } catch (Exception e) {                    }    }    if (forward) {                ServletHandler servlets = new ServletHandler();        servlets.addServletWithMapping(AsyncProxyServlet.class, "/*");        servlets.addFilterWithMapping(LogDebugHandler.class, "/*", Handler.ALL);        list.addHandler(servlets);    }    if (fake) {                list.addHandler(new FakeHandler());    }    list.addHandler(new NotFoundHandler());        server.start();    server.join();}
1
public boolean accept(Path p)
{    return p.getName().startsWith("part-");}
0
public CrawlDatum getCrawlDatum(Text url) throws IOException
{    synchronized (crawlLock) {        if (crawl == null)            crawl = getReaders(CrawlDatum.FETCH_DIR_NAME);    }    return (CrawlDatum) getEntry(crawl, url, new CrawlDatum());}
0
public Content getContent(Text url) throws IOException
{    synchronized (cLock) {        if (content == null)            content = getReaders(Content.DIR_NAME);    }    return (Content) getEntry(content, url, new Content());}
0
private MapFile.Reader[] getReaders(String subDir) throws IOException
{    Path dir = new Path(segmentDir, subDir);    FileSystem fs = dir.getFileSystem(conf);    Path[] names = FileUtil.stat2Paths(fs.listStatus(dir, SegmentPathFilter.INSTANCE));        Arrays.sort(names);    MapFile.Reader[] parts = new MapFile.Reader[names.length];    for (int i = 0; i < names.length; i++) {        parts[i] = new MapFile.Reader(names[i], conf);    }    return parts;}
0
private Writable getEntry(MapFile.Reader[] readers, Text url, Writable entry) throws IOException
{    return MapFileOutputFormat.getEntry(readers, PARTITIONER, url, entry);}
0
public void close() throws IOException
{    if (content != null) {        closeReaders(content);    }    if (parseText != null) {        closeReaders(parseText);    }    if (parseData != null) {        closeReaders(parseData);    }    if (crawl != null) {        closeReaders(crawl);    }}
0
private void closeReaders(MapFile.Reader[] readers) throws IOException
{    for (int i = 0; i < readers.length; i++) {        readers[i].close();    }}
0
public void handle(Request req, HttpServletResponse res, String target, int dispatch) throws IOException, ServletException
{    try {        String uri = req.getUri().toString();                addMyHeader(res, "URI", uri);        Text url = new Text(uri.toString());        CrawlDatum cd = seg.getCrawlDatum(url);        if (cd != null) {            addMyHeader(res, "Res", "found");                        ProtocolStatus ps = (ProtocolStatus) cd.getMetaData().get(Nutch.WRITABLE_PROTO_STATUS_KEY);            if (ps != null) {                Integer TrCode = protoCodes.get(ps.getCode());                if (TrCode != null) {                    res.setStatus(TrCode.intValue());                } else {                    res.setStatus(HttpServletResponse.SC_OK);                }                addMyHeader(res, "ProtocolStatus", ps.toString());            } else {                res.setStatus(HttpServletResponse.SC_OK);            }            Content c = seg.getContent(url);            if (c == null) {                                req.setHandled(true);                res.addHeader("X-Handled-By", getClass().getSimpleName());                return;            }            byte[] data = c.getContent();                        Metadata meta = c.getMetadata();            String[] names = meta.names();                        for (int i = 0; i < names.length; i++) {                boolean my = true;                char ch = names[i].charAt(0);                if (Character.isLetter(ch) && Character.isUpperCase(ch)) {                                        my = false;                }                String[] values = meta.getValues(names[i]);                for (int k = 0; k < values.length; k++) {                    if (my) {                        addMyHeader(res, names[i], values[k]);                    } else {                        res.addHeader(names[i], values[k]);                    }                }            }            req.setHandled(true);            res.addHeader("X-Handled-By", getClass().getSimpleName());            res.setContentType(meta.get(Metadata.CONTENT_TYPE));            res.setContentLength(data.length);            OutputStream os = res.getOutputStream();            os.write(data, 0, data.length);            res.flushBuffer();        } else {            addMyHeader(res, "Res", "not found");                    }    } catch (Exception e) {        e.printStackTrace();                addMyHeader(res, "Res", "Exception: " + StringUtils.stringifyException(e));    }}
1
public void testDump() throws Exception
{    File sampleSegmentDir = new File(System.getProperty("test.build.data", "."), "test-segments");    File tempDir = Files.createTempDirectory("temp").toFile();    String[] crawledFiles = { "c463a4381eb837f9f5d45978cfbde79e_.html", "a974b8d74f7779ab6c6f90b9b279467e_.html", "6bc6497314656a3129732efd708e9f96_.html", "6e88c40abe26cad0a726102997aed048_.html", "5cafdd88f4e9cf3f0cd4c298c6873358_apachecon-europe.html", "932dc10a76e894a2baa8ea4086ad72a8_apachecon-north-america.html", "8540187d75b9cd405b8fa97d665f9f90_.html", "e501bc976c8693b4d28a55b79c390a32_.html", "6add662f9f5758b7d75eec5cfa1f340b_.html", "d4f20df3c37033dc516067ee1f424e4e_.html", "d7b8fa9a02cdc95546030d04be4a98f3_solr.html", "3cbe876e3a8e7a397811de3bb6a945cd_.html", "5b987dde0da79d7f2e3f22b46437f514_bot.html", "3d742820d9a701a1f02e10d5bf5ae633_credits.html", "693673f3c73d04a26276effdea69b7ee_downloads.html", "4f7e3469dafabb4c3b87b00531f81aa4_index.html", "15c5330675be8a69995aab18ff9859e0_javadoc.html", "bc624e1b49e29870ef095819bb0e977a_mailing_lists.html", "a7d66b68754c3665c66e62225255e3fd_version_control.html", "32fb7fe362e1a0d8a1b15addf2a00bdc_1.9-rel", "54ab3db10fe7b26415a04e21045125a8_1zE.html", "1012a41c08092c40340598bd8ee0bfa6_PGa.html", "c830cfc5c28bed10e69d5b83e9c1bcdc_nutch_2.3", "687d915dc264a77f35c61ba841936730_oHY.html", "2bf1afb650010128b4cf4afe677db3c5_1pav9xl.html", "550cab79e14110bbee61c36c61c830b0_1pbE15n.html", "664ff07b46520cc1414494ae49da91f6_.html", "04223714e648a6a43d7c8af8b095f733_.html", "3c8ccb865cd72cca06635d74c7f2f3c4_.html", "90fe47b28716a2230c5122c83f0b8562_Becoming_A_Nutch_Developer.html", "ac0fefe70007d40644e2b8bd5da3c305_FAQ.html", "bc9bc7f11c1262e8924032ab1c7ce112_NutchPropertiesCompleteList.html", "78d04611985e7375b441e478fa36f610_.html", "64adaebadd44e487a8b58894e979dc70_CHANGES.txt", "a48e9c2659b703fdea3ad332877708d8_.html", "159d66d679dd4442d2d8ffe6a83b2912_sponsorship.html", "66f1ce6872c9195c665fc8bdde95f6dc_thanks.html", "ef7ee7e929a048c4a119af78492095b3_.html", "e4251896a982c2b2b68678b5c9c57f4d_.html", "5384764a16fab767ebcbc17d87758a24_.html", "a6ba75a218ef2a09d189cb7dffcecc0f_.html", "f2fa63bd7a3aca63841eed4cd10fb519_SolrCloud.html", "f8de0fbda874e1a140f1b07dcebab374_NUTCH-1047.html", "9c120e94f52d690e9cfd044c34134649_NUTCH-1591.html", "7dd70378379aa452279ce9200d0a5fed_NUTCH-841.html", "ddf78b1fe5c268d59fd62bc745815b92_.html", "401c9f04887dbbf8d29ad52841b8bdb3_ApacheNutch.html", "8f984e2d3c2ba68d1695288f1738deaf_Nutch.html", "c2ef09a95a956207cea073a515172be2_FrontPage.html", "90d9b76e8eabdab1cbcc29bea437c7ae_NutchRESTAPI.html" };    CommonCrawlDataDumper dumper = new CommonCrawlDataDumper(new CommonCrawlConfig());    dumper.dump(tempDir, sampleSegmentDir, null, false, null, false, "", false);    Collection<File> tempFiles = FileUtils.listFiles(tempDir, FileFilterUtils.fileFileFilter(), FileFilterUtils.directoryFileFilter());    for (String expectedFileName : crawledFiles) {        assertTrue("Missed file " + expectedFileName + " in dump", hasFile(expectedFileName, tempFiles));    }}
0
private boolean hasFile(String fileName, Collection<File> files)
{    for (File f : files) {        if (f.getName().equals(fileName)) {            return true;        }    }    return false;}
0
public void testGetUrlMD5() throws Exception
{    String testUrl = "http://apache.org";    String result = DumpFileUtil.getUrlMD5(testUrl);    assertEquals("991e599262e04ea2ec76b6c5aed499a7", result);}
0
public void testCreateTwoLevelsDirectory() throws Exception
{    String testUrl = "http://apache.org";    String basePath = "/tmp";    String fullDir = DumpFileUtil.createTwoLevelsDirectory(basePath, DumpFileUtil.getUrlMD5(testUrl));    assertEquals("/tmp/96/ea", fullDir);    String basePath2 = "/this/path/is/not/existed/just/for/testing";    String fullDir2 = DumpFileUtil.createTwoLevelsDirectory(basePath2, DumpFileUtil.getUrlMD5(testUrl));    assertNull(fullDir2);}
0
public void testCreateFileName() throws Exception
{    String testUrl = "http://apache.org";    String baseName = "test";    String extension = "html";    String fullDir = DumpFileUtil.createFileName(DumpFileUtil.getUrlMD5(testUrl), baseName, extension);    assertEquals("991e599262e04ea2ec76b6c5aed499a7_test.html", fullDir);    String tooLongBaseName = "testtesttesttesttesttesttesttesttesttesttesttesttesttesttesttesttesttest";    String fullDir2 = DumpFileUtil.createFileName(DumpFileUtil.getUrlMD5(testUrl), tooLongBaseName, extension);    assertEquals("991e599262e04ea2ec76b6c5aed499a7_testtesttesttesttesttesttesttest.html", fullDir2);    String tooLongExtension = "testtesttesttesttesttesttesttesttesttesttesttesttesttesttesttesttesttest";    String fullDir3 = DumpFileUtil.createFileName(DumpFileUtil.getUrlMD5(testUrl), baseName, tooLongExtension);    assertEquals("991e599262e04ea2ec76b6c5aed499a7_test.testt", fullDir3);}
0
public void testGuessing()
{        conf.setInt(EncodingDetector.MIN_CONFIDENCE_KEY, -1);    Metadata metadata = new Metadata();    EncodingDetector detector;    Content content;    String encoding;    content = new Content("http://www.example.com", "http://www.example.com/", contentInOctets, "text/plain", metadata, conf);    detector = new EncodingDetector(conf);    detector.autoDetectClues(content, true);    encoding = detector.guessEncoding(content, "windows-1252");        Assert.assertEquals("windows-1252", encoding.toLowerCase());    metadata.clear();    metadata.set(Response.CONTENT_TYPE, "text/plain; charset=UTF-16");    content = new Content("http://www.example.com", "http://www.example.com/", contentInOctets, "text/plain", metadata, conf);    detector = new EncodingDetector(conf);    detector.autoDetectClues(content, true);    encoding = detector.guessEncoding(content, "windows-1252");    Assert.assertEquals("utf-16", encoding.toLowerCase());    metadata.clear();    content = new Content("http://www.example.com", "http://www.example.com/", contentInOctets, "text/plain", metadata, conf);    detector = new EncodingDetector(conf);    detector.autoDetectClues(content, true);    detector.addClue("windows-1254", "sniffed");    encoding = detector.guessEncoding(content, "windows-1252");    Assert.assertEquals("windows-1254", encoding.toLowerCase());        conf.setInt(EncodingDetector.MIN_CONFIDENCE_KEY, 50);    metadata.clear();    metadata.set(Response.CONTENT_TYPE, "text/plain; charset=UTF-16");    content = new Content("http://www.example.com", "http://www.example.com/", contentInOctets, "text/plain", metadata, conf);    detector = new EncodingDetector(conf);    detector.autoDetectClues(content, true);    detector.addClue("utf-32", "sniffed");    encoding = detector.guessEncoding(content, "windows-1252");    Assert.assertEquals("utf-8", encoding.toLowerCase());}
0
public void testZipUnzip()
{    byte[] testBytes = SHORT_TEST_STRING.getBytes();    testZipUnzip(testBytes);    testBytes = LONGER_TEST_STRING.getBytes();    testZipUnzip(testBytes);    testBytes = WEBPAGE.getBytes();    testZipUnzip(testBytes);}
0
public void testZipUnzipBestEffort()
{    byte[] testBytes = SHORT_TEST_STRING.getBytes();    testZipUnzipBestEffort(testBytes);    testBytes = LONGER_TEST_STRING.getBytes();    testZipUnzipBestEffort(testBytes);    testBytes = WEBPAGE.getBytes();    testZipUnzipBestEffort(testBytes);}
0
public void testTruncation()
{    byte[] testBytes = SHORT_TEST_STRING.getBytes();    testTruncation(testBytes);    testBytes = LONGER_TEST_STRING.getBytes();    testTruncation(testBytes);    testBytes = WEBPAGE.getBytes();    testTruncation(testBytes);}
0
public void testLimit()
{    byte[] testBytes = SHORT_TEST_STRING.getBytes();    testLimit(testBytes);    testBytes = LONGER_TEST_STRING.getBytes();    testLimit(testBytes);    testBytes = WEBPAGE.getBytes();    testLimit(testBytes);}
0
public void testZipUnzip(byte[] origBytes)
{    byte[] compressedBytes = GZIPUtils.zip(origBytes);    Assert.assertTrue("compressed array is not smaller!", compressedBytes.length < origBytes.length);    byte[] uncompressedBytes = null;    try {        uncompressedBytes = GZIPUtils.unzip(compressedBytes);    } catch (IOException e) {        e.printStackTrace();        Assert.assertTrue("caught exception '" + e + "' during unzip()", false);    }    Assert.assertTrue("uncompressedBytes is wrong size", uncompressedBytes.length == origBytes.length);    for (int i = 0; i < origBytes.length; i++) if (origBytes[i] != uncompressedBytes[i])        Assert.assertTrue("uncompressedBytes does not match origBytes", false);}
0
public void testZipUnzipBestEffort(byte[] origBytes)
{    byte[] compressedBytes = GZIPUtils.zip(origBytes);    Assert.assertTrue("compressed array is not smaller!", compressedBytes.length < origBytes.length);    byte[] uncompressedBytes = GZIPUtils.unzipBestEffort(compressedBytes);    Assert.assertTrue("uncompressedBytes is wrong size", uncompressedBytes.length == origBytes.length);    for (int i = 0; i < origBytes.length; i++) if (origBytes[i] != uncompressedBytes[i])        Assert.assertTrue("uncompressedBytes does not match origBytes", false);}
0
public void testTruncation(byte[] origBytes)
{    byte[] compressedBytes = GZIPUtils.zip(origBytes);    System.out.println("original data has len " + origBytes.length);    System.out.println("compressed data has len " + compressedBytes.length);    for (int i = compressedBytes.length; i >= 0; i--) {        byte[] truncCompressed = new byte[i];        for (int j = 0; j < i; j++) truncCompressed[j] = compressedBytes[j];        byte[] trunc = GZIPUtils.unzipBestEffort(truncCompressed);        if (trunc == null) {            System.out.println("truncated to len " + i + ", trunc is null");        } else {            System.out.println("truncated to len " + i + ", trunc.length=  " + trunc.length);            for (int j = 0; j < trunc.length; j++) if (trunc[j] != origBytes[j])                Assert.assertTrue("truncated/uncompressed array differs at pos " + j + " (compressed data had been truncated to len " + i + ")", false);        }    }}
0
public void testLimit(byte[] origBytes)
{    byte[] compressedBytes = GZIPUtils.zip(origBytes);    Assert.assertTrue("compressed array is not smaller!", compressedBytes.length < origBytes.length);    for (int i = 0; i < origBytes.length; i++) {        byte[] uncompressedBytes = GZIPUtils.unzipBestEffort(compressedBytes, i);        Assert.assertTrue("uncompressedBytes is wrong size", uncompressedBytes.length == i);        for (int j = 0; j < i; j++) if (origBytes[j] != uncompressedBytes[j])            Assert.assertTrue("uncompressedBytes does not match origBytes", false);    }}
0
private String getMimeType(String url, File file, String contentType, boolean useMagic) throws IOException
{    return getMimeType(url, Files.toByteArray(file), contentType, useMagic);}
0
private String getMimeType(String url, byte[] bytes, String contentType, boolean useMagic)
{    Configuration conf = NutchConfiguration.create();    conf.setBoolean("mime.type.magic", useMagic);    MimeUtil mimeUtil = new MimeUtil(conf);    return mimeUtil.autoResolveContentType(contentType, url, bytes);}
0
public void testWithMimeMagic()
{    for (String[] testPage : textBasedFormats) {        String mimeType = getMimeType(urlPrefix, testPage[3].getBytes(defaultCharset), testPage[2], true);        assertEquals("", testPage[0], mimeType);    }}
0
public void testWithoutMimeMagic()
{    for (String[] testPage : textBasedFormats) {        if (testPage.length > 4 && "requires-mime-magic".equals(testPage[4])) {            continue;        }        String mimeType = getMimeType(urlPrefix + testPage[1], testPage[3].getBytes(defaultCharset), testPage[2], false);        assertEquals("", testPage[0], mimeType);    }}
0
public void testOnlyMimeMagic()
{    for (String[] testPage : textBasedFormats) {        String mimeType = getMimeType(urlPrefix, testPage[3].getBytes(defaultCharset), "", true);        assertEquals("", testPage[0], mimeType);    }}
0
public void testBinaryFiles() throws IOException
{    for (String[] testPage : binaryFiles) {        File dataFile = new File(sampleDir, testPage[1]);        String mimeType = getMimeType(urlPrefix + testPage[1], dataFile, testPage[2], false);        assertEquals("", testPage[0], mimeType);    }}
0
public void setUp() throws Exception
{    ULCONTENT[0] = "crawl several billion pages per month";    ULCONTENT[1] = "maintain an index of these pages";    ULCONTENT[2] = "search that index up to 1000 times per second";    ULCONTENT[3] = "operate at minimal cost";}
0
public void testSkipChildren()
{    DOMParser parser = new DOMParser();    try {        parser.setFeature("http://xml.org/sax/features/validation", false);        parser.setFeature("http://apache.org/xml/features/nonvalidating/load-external-dtd", false);        parser.parse(new InputSource(new ByteArrayInputStream(WEBPAGE.getBytes())));    } catch (Exception e) {        e.printStackTrace();    }    StringBuffer sb = new StringBuffer();    NodeWalker walker = new NodeWalker(parser.getDocument());    while (walker.hasNext()) {        Node currentNode = walker.nextNode();        short nodeType = currentNode.getNodeType();        if (nodeType == Node.TEXT_NODE) {            String text = currentNode.getNodeValue();            text = text.replaceAll("\\s+", " ");            sb.append(text);        }    }    Assert.assertTrue("UL Content can NOT be found in the node", findSomeUlContent(sb.toString()));    StringBuffer sbSkip = new StringBuffer();    NodeWalker walkerSkip = new NodeWalker(parser.getDocument());    while (walkerSkip.hasNext()) {        Node currentNode = walkerSkip.nextNode();        String nodeName = currentNode.getNodeName();        short nodeType = currentNode.getNodeType();        if ("ul".equalsIgnoreCase(nodeName)) {            walkerSkip.skipChildren();        }        if (nodeType == Node.TEXT_NODE) {            String text = currentNode.getNodeValue();            text = text.replaceAll("\\s+", " ");            sbSkip.append(text);        }    }    Assert.assertFalse("UL Content can be found in the node", findSomeUlContent(sbSkip.toString()));}
0
public boolean findSomeUlContent(String str)
{    for (int i = 0; i < ULCONTENT.length; i++) {        if (str.contains(ULCONTENT[i]))            return true;    }    return false;}
0
private String makeRandString(int minLen, int maxLen)
{    int len = minLen + (int) (Math.random() * (maxLen - minLen));    char[] chars = new char[len];    for (int pos = 0; pos < len; pos++) {        chars[pos] = alphabet[(int) (Math.random() * alphabet.length)];    }    return new String(chars);}
0
public void testPrefixMatcher()
{    int numMatches = 0;    int numInputsTested = 0;    for (int round = 0; round < NUM_TEST_ROUNDS; round++) {                int numPrefixes = (int) (Math.random() * MAX_TEST_PREFIXES);        String[] prefixes = new String[numPrefixes];        for (int i = 0; i < numPrefixes; i++) {            prefixes[i] = makeRandString(0, MAX_PREFIX_LEN);        }        PrefixStringMatcher prematcher = new PrefixStringMatcher(prefixes);                for (int i = 0; i < NUM_TEST_INPUTS_PER_ROUND; i++) {            String input = makeRandString(0, MAX_INPUT_LEN);            boolean matches = false;            int longestMatch = -1;            int shortestMatch = -1;            for (int j = 0; j < prefixes.length; j++) {                if ((prefixes[j].length() > 0) && input.startsWith(prefixes[j])) {                    matches = true;                    int matchSize = prefixes[j].length();                    if (matchSize > longestMatch)                        longestMatch = matchSize;                    if ((matchSize < shortestMatch) || (shortestMatch == -1))                        shortestMatch = matchSize;                }            }            if (matches)                numMatches++;            numInputsTested++;            Assert.assertTrue("'" + input + "' should " + (matches ? "" : "not ") + "match!", matches == prematcher.matches(input));            if (matches) {                Assert.assertTrue(shortestMatch == prematcher.shortestMatch(input).length());                Assert.assertTrue(input.substring(0, shortestMatch).equals(prematcher.shortestMatch(input)));                Assert.assertTrue(longestMatch == prematcher.longestMatch(input).length());                Assert.assertTrue(input.substring(0, longestMatch).equals(prematcher.longestMatch(input)));            }        }    }    System.out.println("got " + numMatches + " matches out of " + numInputsTested + " tests");}
0
public void testRightPad()
{    String s = "my string";    String ps = StringUtil.rightPad(s, 0);    Assert.assertTrue(s.equals(ps));    ps = StringUtil.rightPad(s, 9);    Assert.assertTrue(s.equals(ps));    ps = StringUtil.rightPad(s, 10);    Assert.assertTrue((s + " ").equals(ps));    ps = StringUtil.rightPad(s, 15);    Assert.assertTrue((s + "      ").equals(ps));}
0
public void testLeftPad()
{    String s = "my string";    String ps = StringUtil.leftPad(s, 0);    Assert.assertTrue(s.equals(ps));    ps = StringUtil.leftPad(s, 9);    Assert.assertTrue(s.equals(ps));    ps = StringUtil.leftPad(s, 10);    Assert.assertTrue((" " + s).equals(ps));    ps = StringUtil.leftPad(s, 15);    Assert.assertTrue(("      " + s).equals(ps));}
0
private String makeRandString(int minLen, int maxLen)
{    int len = minLen + (int) (Math.random() * (maxLen - minLen));    char[] chars = new char[len];    for (int pos = 0; pos < len; pos++) {        chars[pos] = alphabet[(int) (Math.random() * alphabet.length)];    }    return new String(chars);}
0
public void testSuffixMatcher()
{    int numMatches = 0;    int numInputsTested = 0;    for (int round = 0; round < NUM_TEST_ROUNDS; round++) {                int numSuffixes = (int) (Math.random() * MAX_TEST_SUFFIXES);        String[] suffixes = new String[numSuffixes];        for (int i = 0; i < numSuffixes; i++) {            suffixes[i] = makeRandString(0, MAX_SUFFIX_LEN);        }        SuffixStringMatcher sufmatcher = new SuffixStringMatcher(suffixes);                for (int i = 0; i < NUM_TEST_INPUTS_PER_ROUND; i++) {            String input = makeRandString(0, MAX_INPUT_LEN);            boolean matches = false;            int longestMatch = -1;            int shortestMatch = -1;            for (int j = 0; j < suffixes.length; j++) {                if ((suffixes[j].length() > 0) && input.endsWith(suffixes[j])) {                    matches = true;                    int matchSize = suffixes[j].length();                    if (matchSize > longestMatch)                        longestMatch = matchSize;                    if ((matchSize < shortestMatch) || (shortestMatch == -1))                        shortestMatch = matchSize;                }            }            if (matches)                numMatches++;            numInputsTested++;            Assert.assertTrue("'" + input + "' should " + (matches ? "" : "not ") + "match!", matches == sufmatcher.matches(input));            if (matches) {                Assert.assertTrue(shortestMatch == sufmatcher.shortestMatch(input).length());                Assert.assertTrue(input.substring(input.length() - shortestMatch).equals(sufmatcher.shortestMatch(input)));                Assert.assertTrue(longestMatch == sufmatcher.longestMatch(input).length());                Assert.assertTrue(input.substring(input.length() - longestMatch).equals(sufmatcher.longestMatch(input)));            }        }    }    System.out.println("got " + numMatches + " matches out of " + numInputsTested + " tests");}
0
public void testReverseUrl() throws Exception
{    assertReverse(urlString1, reversedUrlString1);    assertReverse(urlString2, reversedUrlString2);    assertReverse(urlString3, reversedUrlString3);    assertReverse(urlString4, reversedUrlString4);    assertReverse(urlString5, reversedUrlString5);    assertReverse(urlString5, reversedUrlString5);    assertReverse(urlString6, reversedUrlString6);    assertReverse(urlString7, reversedUrlString7);}
0
public void testUnreverseUrl() throws Exception
{    assertUnreverse(reversedUrlString1, urlString1);    assertUnreverse(reversedUrlString2, urlString2);    assertUnreverse(reversedUrlString3, urlString3);    assertUnreverse(reversedUrlString4, urlString4);    assertUnreverse(reversedUrlString5, urlString5rev);    assertUnreverse(reversedUrlString6, urlString6);    assertUnreverse(reversedUrlString7, urlString7);}
0
private static void assertReverse(String url, String expectedReversedUrl) throws Exception
{    String reversed = TableUtil.reverseUrl(url);    assertEquals(expectedReversedUrl, reversed);}
0
private static void assertUnreverse(String reversedUrl, String expectedUrl)
{    String unreversed = TableUtil.unreverseUrl(reversedUrl);    assertEquals(expectedUrl, unreversed);}
0
public void testGetDomainName() throws Exception
{    URL url = null;    url = new URL("http://lucene.apache.org/nutch");    Assert.assertEquals("apache.org", URLUtil.getDomainName(url));    url = new URL("http://en.wikipedia.org/wiki/Java_coffee");    Assert.assertEquals("wikipedia.org", URLUtil.getDomainName(url));    url = new URL("http://140.211.11.130/foundation/contributing.html");    Assert.assertEquals("140.211.11.130", URLUtil.getDomainName(url));    url = new URL("http://www.example.co.uk:8080/index.html");    Assert.assertEquals("example.co.uk", URLUtil.getDomainName(url));    url = new URL("http://com");    Assert.assertEquals("com", URLUtil.getDomainName(url));    url = new URL("http://www.example.co.uk.com");    Assert.assertEquals("uk.com", URLUtil.getDomainName(url));        url = new URL("http://example.com.nn");    Assert.assertEquals("nn", URLUtil.getDomainName(url));    url = new URL("http://");    Assert.assertEquals("", URLUtil.getDomainName(url));    url = new URL("http://www.edu.tr.xyz");    Assert.assertEquals("xyz", URLUtil.getDomainName(url));    url = new URL("http://www.example.c.se");    Assert.assertEquals("example.c.se", URLUtil.getDomainName(url));        url = new URL("http://www.example.plc.co.im");    Assert.assertEquals("example.plc.co.im", URLUtil.getDomainName(url));        url = new URL("http://www.example.2000.hu");    Assert.assertEquals("example.2000.hu", URLUtil.getDomainName(url));        url = new URL("http://www.example.商業.tw");    Assert.assertEquals("example.商業.tw", URLUtil.getDomainName(url));}
0
public void testGetDomainSuffix() throws Exception
{    URL url = null;    url = new URL("http://lucene.apache.org/nutch");    Assert.assertEquals("org", URLUtil.getDomainSuffix(url).getDomain());    url = new URL("http://140.211.11.130/foundation/contributing.html");    Assert.assertNull(URLUtil.getDomainSuffix(url));    url = new URL("http://www.example.co.uk:8080/index.html");    Assert.assertEquals("co.uk", URLUtil.getDomainSuffix(url).getDomain());    url = new URL("http://com");    Assert.assertEquals("com", URLUtil.getDomainSuffix(url).getDomain());    url = new URL("http://www.example.co.uk.com");    Assert.assertEquals("com", URLUtil.getDomainSuffix(url).getDomain());        url = new URL("http://example.com.nn");    Assert.assertNull(URLUtil.getDomainSuffix(url));    url = new URL("http://");    Assert.assertNull(URLUtil.getDomainSuffix(url));    url = new URL("http://www.edu.tr.xyz");    Assert.assertNull(URLUtil.getDomainSuffix(url));    url = new URL("http://subdomain.example.edu.tr");    Assert.assertEquals("edu.tr", URLUtil.getDomainSuffix(url).getDomain());    url = new URL("http://subdomain.example.presse.fr");    Assert.assertEquals("presse.fr", URLUtil.getDomainSuffix(url).getDomain());    url = new URL("http://subdomain.example.presse.tr");    Assert.assertEquals("tr", URLUtil.getDomainSuffix(url).getDomain());        url = new URL("http://www.example.plc.co.im");    Assert.assertEquals("plc.co.im", URLUtil.getDomainSuffix(url).getDomain());        url = new URL("http://www.example.2000.hu");    Assert.assertEquals("2000.hu", URLUtil.getDomainSuffix(url).getDomain());        url = new URL("http://www.example.商業.tw");    Assert.assertEquals("商業.tw", URLUtil.getDomainSuffix(url).getDomain());}
0
public void testGetHostSegments() throws Exception
{    URL url;    String[] segments;    url = new URL("http://subdomain.example.edu.tr");    segments = URLUtil.getHostSegments(url);    Assert.assertEquals("subdomain", segments[0]);    Assert.assertEquals("example", segments[1]);    Assert.assertEquals("edu", segments[2]);    Assert.assertEquals("tr", segments[3]);    url = new URL("http://");    segments = URLUtil.getHostSegments(url);    Assert.assertEquals(1, segments.length);    Assert.assertEquals("", segments[0]);    url = new URL("http://140.211.11.130/foundation/contributing.html");    segments = URLUtil.getHostSegments(url);    Assert.assertEquals(1, segments.length);    Assert.assertEquals("140.211.11.130", segments[0]);        url = new URL("http://www.example.商業.tw");    segments = URLUtil.getHostSegments(url);    Assert.assertEquals("www", segments[0]);    Assert.assertEquals("example", segments[1]);    Assert.assertEquals("商業", segments[2]);    Assert.assertEquals("tw", segments[3]);}
0
public void testChooseRepr() throws Exception
{    String aDotCom = "http://www.a.com";    String bDotCom = "http://www.b.com";    String aSubDotCom = "http://www.news.a.com";    String aQStr = "http://www.a.com?y=1";    String aPath = "http://www.a.com/xyz/index.html";    String aPath2 = "http://www.a.com/abc/page.html";    String aPath3 = "http://www.news.a.com/abc/page.html";            Assert.assertEquals(bDotCom, URLUtil.chooseRepr(aDotCom, bDotCom, true));    Assert.assertEquals(bDotCom, URLUtil.chooseRepr(aDotCom, bDotCom, false));            Assert.assertEquals(aDotCom, URLUtil.chooseRepr(aDotCom, aQStr, false));    Assert.assertEquals(aDotCom, URLUtil.chooseRepr(aDotCom, aPath, false));            Assert.assertEquals(aDotCom, URLUtil.chooseRepr(aPath, aDotCom, false));            Assert.assertEquals(aPath2, URLUtil.chooseRepr(aPath, aPath2, false));            Assert.assertEquals(aDotCom, URLUtil.chooseRepr(aDotCom, aPath, true));            Assert.assertEquals(aDotCom, URLUtil.chooseRepr(aPath, aDotCom, true));                    Assert.assertEquals(aPath2, URLUtil.chooseRepr(aPath, aPath2, true));    Assert.assertEquals(aPath, URLUtil.chooseRepr(aPath, aPath3, true));            Assert.assertEquals(aDotCom, URLUtil.chooseRepr(aDotCom, aSubDotCom, true));}
0
public void testResolveURL() throws Exception
{        URL u436 = new URL("http://a/b/c/d;p?q#f");    Assert.assertEquals("http://a/b/c/d;p?q#f", u436.toString());    URL abs = URLUtil.resolveURL(u436, "?y");    Assert.assertEquals("http://a/b/c/d;p?y", abs.toString());        URL u566 = new URL("http://www.fleurie.org/entreprise.asp");    abs = URLUtil.resolveURL(u566, "?id_entrep=111");    Assert.assertEquals("http://www.fleurie.org/entreprise.asp?id_entrep=111", abs.toString());    URL base = new URL(baseString);    Assert.assertEquals("base url parsing", baseString, base.toString());    for (int i = 0; i < targets.length; i++) {        URL u = URLUtil.resolveURL(base, targets[i][0]);        Assert.assertEquals(targets[i][1], targets[i][1], u.toString());    }}
0
public void testToUNICODE() throws Exception
{    Assert.assertEquals("http://www.çevir.com", URLUtil.toUNICODE("http://www.xn--evir-zoa.com"));    Assert.assertEquals("http://uni-tübingen.de/", URLUtil.toUNICODE("http://xn--uni-tbingen-xhb.de/"));    Assert.assertEquals("http://www.medizin.uni-tübingen.de:8080/search.php?q=abc#p1", URLUtil.toUNICODE("http://www.medizin.xn--uni-tbingen-xhb.de:8080/search.php?q=abc#p1"));}
0
public void testToASCII() throws Exception
{    Assert.assertEquals("http://www.xn--evir-zoa.com", URLUtil.toASCII("http://www.çevir.com"));    Assert.assertEquals("http://xn--uni-tbingen-xhb.de/", URLUtil.toASCII("http://uni-tübingen.de/"));    Assert.assertEquals("http://www.medizin.xn--uni-tbingen-xhb.de:8080/search.php?q=abc#p1", URLUtil.toASCII("http://www.medizin.uni-tübingen.de:8080/search.php?q=abc#p1"));}
0
public void testFileProtocol() throws Exception
{        Assert.assertEquals("file:/path/file.html", URLUtil.toASCII("file:/path/file.html"));    Assert.assertEquals("file:/path/file.html", URLUtil.toUNICODE("file:/path/file.html"));}
0
public static void testWritable(Writable before) throws Exception
{    testWritable(before, null);}
0
public static void testWritable(Writable before, Configuration conf) throws Exception
{    Assert.assertEquals(before, writeRead(before, conf));}
0
public static Writable writeRead(Writable before, Configuration conf) throws Exception
{    DataOutputBuffer dob = new DataOutputBuffer();    before.write(dob);    DataInputBuffer dib = new DataInputBuffer();    dib.reset(dob.getData(), dob.getLength());    Writable after = (Writable) before.getClass().getConstructor().newInstance();    if (conf != null) {        ((Configurable) after).setConf(conf);    }    after.readFields(dib);    return after;}
0
