public void nutch_f0_1(Configuration conf)
{    super.setConf(conf);    if (conf == null)        return;    defaultInterval = conf.getInt("db.fetch.interval.default", 0);    maxInterval = conf.getInt("db.fetch.interval.max", 0);        }
public CrawlDatum nutch_f1_0(Text url, CrawlDatum datum)
{    datum.setFetchTime(System.currentTimeMillis());    datum.setFetchInterval(defaultInterval);    datum.setRetriesSinceFetch(0);    return datum;}
public CrawlDatum nutch_f2_0(Text url, CrawlDatum datum, long prevFetchTime, long prevModifiedTime, long fetchTime, long modifiedTime, int state)
{    datum.setRetriesSinceFetch(0);    return datum;}
public CrawlDatum nutch_f3_0(Text url, CrawlDatum datum, long prevFetchTime, long prevModifiedTime, long fetchTime)
{        if ((datum.getFetchInterval() * 1.5f) < maxInterval)        datum.setFetchInterval(datum.getFetchInterval() * 1.5f);    else        datum.setFetchInterval(maxInterval * 0.9f);    datum.setFetchTime(fetchTime + (long) datum.getFetchInterval() * 1000);    return datum;}
public CrawlDatum nutch_f4_0(Text url, CrawlDatum datum, long prevFetchTime, long prevModifiedTime, long fetchTime)
{    datum.setFetchTime(fetchTime + (long) SECONDS_PER_DAY * 1000);    datum.setRetriesSinceFetch(datum.getRetriesSinceFetch() + 1);    return datum;}
public long nutch_f5_0(CrawlDatum datum)
{    if (datum.getStatus() == CrawlDatum.STATUS_DB_UNFETCHED) {        return 0L;    } else {        return datum.getFetchTime() - (long) datum.getFetchInterval() * 1000;    }}
public boolean nutch_f6_0(Text url, CrawlDatum datum, long curTime)
{        if (datum.getFetchTime() - curTime > (long) maxInterval * 1000) {        if (datum.getFetchInterval() > maxInterval) {            datum.setFetchInterval(maxInterval * 0.9f);        }        datum.setFetchTime(curTime);    }    if (datum.getFetchTime() > curTime) {                return false;    }    return true;}
public CrawlDatum nutch_f7_0(Text url, CrawlDatum datum, boolean asap)
{        if (datum.getFetchInterval() > maxInterval)        datum.setFetchInterval(maxInterval * 0.9f);    datum.setStatus(CrawlDatum.STATUS_DB_UNFETCHED);    datum.setRetriesSinceFetch(0);    datum.setSignature(null);    datum.setModifiedTime(0L);    if (asap)        datum.setFetchTime(System.currentTimeMillis());    return datum;}
public void nutch_f8_0(Configuration conf)
{    super.setConf(conf);    if (conf == null)        return;    INC_RATE = conf.getFloat("db.fetch.schedule.adaptive.inc_rate", 0.2f);    DEC_RATE = conf.getFloat("db.fetch.schedule.adaptive.dec_rate", 0.2f);    MIN_INTERVAL = conf.getFloat("db.fetch.schedule.adaptive.min_interval", (float) 60.0);    MAX_INTERVAL = conf.getFloat("db.fetch.schedule.adaptive.max_interval",     (float) SECONDS_PER_DAY * 365);    SYNC_DELTA = conf.getBoolean("db.fetch.schedule.adaptive.sync_delta", true);    SYNC_DELTA_RATE = conf.getFloat("db.fetch.schedule.adaptive.sync_delta_rate", 0.2f);}
public CrawlDatum nutch_f9_0(Text url, CrawlDatum datum, long prevFetchTime, long prevModifiedTime, long fetchTime, long modifiedTime, int state)
{    super.setFetchSchedule(url, datum, prevFetchTime, prevModifiedTime, fetchTime, modifiedTime, state);    float interval = datum.getFetchInterval();    long refTime = fetchTime;        interval = (interval == 0) ? defaultInterval : interval;    if (datum.getMetaData().containsKey(Nutch.WRITABLE_FIXED_INTERVAL_KEY)) {                FloatWritable customIntervalWritable = (FloatWritable) (datum.getMetaData().get(Nutch.WRITABLE_FIXED_INTERVAL_KEY));        interval = customIntervalWritable.get();    } else {        if (modifiedTime <= 0)            modifiedTime = fetchTime;        switch(state) {            case FetchSchedule.STATUS_MODIFIED:                interval *= (1.0f - DEC_RATE);                modifiedTime = fetchTime;                break;            case FetchSchedule.STATUS_NOTMODIFIED:                interval *= (1.0f + INC_RATE);                break;            case FetchSchedule.STATUS_UNKNOWN:                break;        }        if (SYNC_DELTA) {                        long delta = (fetchTime - modifiedTime) / 1000L;            if (delta > interval)                interval = delta;            refTime = fetchTime - Math.round(delta * SYNC_DELTA_RATE * 1000);        }        if (interval < MIN_INTERVAL) {            interval = MIN_INTERVAL;        } else if (interval > MAX_INTERVAL) {            interval = MAX_INTERVAL;        }    }    datum.setFetchInterval(interval);    datum.setFetchTime(refTime + Math.round(interval * 1000.0));    datum.setModifiedTime(modifiedTime);    return datum;}
public static void nutch_f10_1(String[] args) throws Exception
{    FetchSchedule fs = new AdaptiveFetchSchedule();    fs.setConf(NutchConfiguration.create());        long curTime = 0;        long delta = 1000L * 3600L * 24L;            long update = 1000L * 3600L * 24L * 30L;    boolean changed = true;    long lastModified = 0;    int miss = 0;    int totalMiss = 0;    int maxMiss = 0;    int fetchCnt = 0;    int changeCnt = 0;        CrawlDatum p = new CrawlDatum(1, 3600 * 24 * 30, 1.0f);    p.setFetchTime(0);            for (int i = 0; i < 10000; i++) {        if (lastModified + update < curTime) {                                    changed = true;            changeCnt++;            lastModified = curTime;        }                if (p.getFetchTime() <= curTime) {            fetchCnt++;            fs.setFetchSchedule(new Text("http://www.example.com"), p, p.getFetchTime(), p.getModifiedTime(), curTime, lastModified, changed ? FetchSchedule.STATUS_MODIFIED : FetchSchedule.STATUS_NOTMODIFIED);                        if (!changed)                miss++;            if (miss > maxMiss)                maxMiss = miss;            changed = false;            totalMiss += miss;            miss = 0;        }        if (changed)            miss++;        curTime += delta;    }        }
public static boolean nutch_f11_0(CrawlDatum datum)
{    if (datum.status <= STATUS_DB_MAX)        return true;    return false;}
public static boolean nutch_f12_0(CrawlDatum datum)
{    if (datum.status > STATUS_DB_MAX && datum.status <= STATUS_FETCH_MAX)        return true;    return false;}
public byte nutch_f13_0()
{    return status;}
public static String nutch_f14_0(byte value)
{    String res = statNames.get(value);    if (res == null)        res = "unknown";    return res;}
public static byte nutch_f15_0(String name)
{    for (Entry<Byte, String> status : statNames.entrySet()) {        if (name.equalsIgnoreCase(status.getValue())) {            return status.getKey();        }    }    return -1;}
public void nutch_f16_0(int status)
{    this.status = (byte) status;}
public long nutch_f17_0()
{    return fetchTime;}
public void nutch_f18_0(long fetchTime)
{    this.fetchTime = fetchTime;}
public long nutch_f19_0()
{    return modifiedTime;}
public void nutch_f20_0(long modifiedTime)
{    this.modifiedTime = modifiedTime;}
public byte nutch_f21_0()
{    return retries;}
public void nutch_f22_0(int retries)
{    this.retries = (byte) retries;}
public int nutch_f23_0()
{    return fetchInterval;}
public void nutch_f24_0(int fetchInterval)
{    this.fetchInterval = fetchInterval;}
public void nutch_f25_0(float fetchInterval)
{    this.fetchInterval = Math.round(fetchInterval);}
public float nutch_f26_0()
{    return score;}
public void nutch_f27_0(float score)
{    this.score = score;}
public byte[] nutch_f28_0()
{    return signature;}
public void nutch_f29_0(byte[] signature)
{    if (signature != null && signature.length > 256)        throw new RuntimeException("Max signature length (256) exceeded: " + signature.length);    this.signature = signature;}
public void nutch_f30_0(org.apache.hadoop.io.MapWritable mapWritable)
{    this.metaData = new org.apache.hadoop.io.MapWritable(mapWritable);}
public void nutch_f31_0(CrawlDatum other)
{    for (Entry<Writable, Writable> e : other.getMetaData().entrySet()) {        getMetaData().put(e.getKey(), e.getValue());    }}
public org.apache.hadoop.io.MapWritable nutch_f32_0()
{    if (this.metaData == null)        this.metaData = new org.apache.hadoop.io.MapWritable();    return this.metaData;}
public static CrawlDatum nutch_f33_0(DataInput in) throws IOException
{    CrawlDatum result = new CrawlDatum();    result.readFields(in);    return result;}
public void nutch_f34_0(DataInput in) throws IOException
{        byte version = in.readByte();    if (    version > CUR_VERSION)        throw new VersionMismatchException(CUR_VERSION, version);    status = in.readByte();    fetchTime = in.readLong();    retries = in.readByte();    if (version > 5) {        fetchInterval = in.readInt();    } else        fetchInterval = Math.round(in.readFloat());    score = in.readFloat();    if (version > 2) {        modifiedTime = in.readLong();        int cnt = in.readByte();        if (cnt > 0) {            signature = new byte[cnt];            in.readFully(signature);        } else            signature = null;    }    if (version > 3) {        boolean hasMetadata = false;        if (version < 7) {            org.apache.hadoop.io.MapWritable oldMetaData = new org.apache.hadoop.io.MapWritable();            if (in.readBoolean()) {                hasMetadata = true;                metaData = new org.apache.hadoop.io.MapWritable();                oldMetaData.readFields(in);            }            for (Writable key : oldMetaData.keySet()) {                metaData.put(key, oldMetaData.get(key));            }        } else {            if (in.readBoolean()) {                hasMetadata = true;                metaData = new org.apache.hadoop.io.MapWritable();                metaData.readFields(in);            }        }        if (hasMetadata == false)            metaData = null;    }        if (version < 5) {        if (oldToNew.containsKey(status))            status = oldToNew.get(status);        else            status = STATUS_DB_UNFETCHED;    }}
public void nutch_f35_0(DataOutput out) throws IOException
{        out.writeByte(CUR_VERSION);    out.writeByte(status);    out.writeLong(fetchTime);    out.writeByte(retries);    out.writeInt(fetchInterval);    out.writeFloat(score);    out.writeLong(modifiedTime);    if (signature == null) {        out.writeByte(0);    } else {        out.writeByte(signature.length);        out.write(signature);    }    if (metaData != null && metaData.size() > 0) {        out.writeBoolean(true);        metaData.write(out);    } else {        out.writeBoolean(false);    }}
public void nutch_f36_0(CrawlDatum that)
{    this.status = that.status;    this.fetchTime = that.fetchTime;    this.retries = that.retries;    this.fetchInterval = that.fetchInterval;    this.score = that.score;    this.modifiedTime = that.modifiedTime;    this.signature = that.signature;    if (that.metaData != null) {                this.metaData = new org.apache.hadoop.io.MapWritable(that.metaData);    } else {        this.metaData = null;    }}
public int nutch_f37_0(CrawlDatum that)
{    if (that.score != this.score)        return (that.score - this.score) > 0 ? 1 : -1;    if (that.status != this.status)        return this.status - that.status;    if (that.fetchTime != this.fetchTime)        return (that.fetchTime - this.fetchTime) > 0 ? 1 : -1;    if (that.retries != this.retries)        return that.retries - this.retries;    if (that.fetchInterval != this.fetchInterval)        return (that.fetchInterval - this.fetchInterval) > 0 ? 1 : -1;    if (that.modifiedTime != this.modifiedTime)        return (that.modifiedTime - this.modifiedTime) > 0 ? 1 : -1;    return SignatureComparator._compare(this, that);}
public int nutch_f38_0(byte[] b1, int s1, int l1, byte[] b2, int s2, int l2)
{    float score1 = readFloat(b1, s1 + SCORE_OFFSET);    float score2 = readFloat(b2, s2 + SCORE_OFFSET);    if (score2 != score1) {        return (score2 - score1) > 0 ? 1 : -1;    }    int status1 = b1[s1 + 1];    int status2 = b2[s2 + 1];    if (status2 != status1)        return status1 - status2;    long fetchTime1 = readLong(b1, s1 + 2);    long fetchTime2 = readLong(b2, s2 + 2);    if (fetchTime2 != fetchTime1)        return (fetchTime2 - fetchTime1) > 0 ? 1 : -1;    int retries1 = b1[s1 + 10];    int retries2 = b2[s2 + 10];    if (retries2 != retries1)        return retries2 - retries1;    int fetchInterval1 = readInt(b1, s1 + 11);    int fetchInterval2 = readInt(b2, s2 + 11);    if (fetchInterval2 != fetchInterval1)        return (fetchInterval2 - fetchInterval1) > 0 ? 1 : -1;    long modifiedTime1 = readLong(b1, s1 + SCORE_OFFSET + 4);    long modifiedTime2 = readLong(b2, s2 + SCORE_OFFSET + 4);    if (modifiedTime2 != modifiedTime1)        return (modifiedTime2 - modifiedTime1) > 0 ? 1 : -1;    int sigl1 = b1[s1 + SIG_OFFSET];    int sigl2 = b2[s2 + SIG_OFFSET];    return SignatureComparator._compare(b1, SIG_OFFSET, sigl1, b2, SIG_OFFSET, sigl2);}
public String nutch_f39_0()
{    StringBuilder buf = new StringBuilder();    buf.append("Version: " + CUR_VERSION + "\n");    buf.append("Status: " + getStatus() + " (" + getStatusName(getStatus()) + ")\n");    buf.append("Fetch time: " + new Date(getFetchTime()) + "\n");    buf.append("Modified time: " + new Date(getModifiedTime()) + "\n");    buf.append("Retries since fetch: " + getRetriesSinceFetch() + "\n");    buf.append("Retry interval: " + getFetchInterval() + " seconds (" + (getFetchInterval() / FetchSchedule.SECONDS_PER_DAY) + " days)\n");    buf.append("Score: " + getScore() + "\n");    buf.append("Signature: " + StringUtil.toHexString(getSignature()) + "\n");    buf.append("Metadata: \n ");    if (metaData != null) {        for (Entry<Writable, Writable> e : metaData.entrySet()) {            buf.append("\t");            buf.append(e.getKey());            buf.append("=");            buf.append(e.getValue());            buf.append("\n");        }    }    return buf.toString();}
private boolean nutch_f40_0(org.apache.hadoop.io.MapWritable otherMetaData)
{    if (metaData == null || metaData.size() == 0) {        return otherMetaData == null || otherMetaData.size() == 0;    }    if (otherMetaData == null) {                return false;    }    HashSet<Entry<Writable, Writable>> set1 = new HashSet<>(metaData.entrySet());    HashSet<Entry<Writable, Writable>> set2 = new HashSet<>(otherMetaData.entrySet());    return set1.equals(set2);}
public boolean nutch_f41_0(Object o)
{    if (!(o instanceof CrawlDatum))        return false;    CrawlDatum other = (CrawlDatum) o;    boolean res = (this.status == other.status) && (this.fetchTime == other.fetchTime) && (this.modifiedTime == other.modifiedTime) && (this.retries == other.retries) && (this.fetchInterval == other.fetchInterval) && (SignatureComparator._compare(this.signature, other.signature) == 0) && (this.score == other.score);    if (!res)        return res;    return metadataEquals(other.metaData);}
public int nutch_f42_0()
{    int res = 0;    if (signature != null) {        for (int i = 0; i < signature.length / 4; i += 4) {            res ^= (signature[i] << 24 + signature[i + 1] << 16 + signature[i + 2] << 8 + signature[i + 3]);        }    }    if (metaData != null) {        res ^= metaData.entrySet().hashCode();    }    return res ^ status ^ ((int) fetchTime) ^ ((int) modifiedTime) ^ retries ^ fetchInterval ^ Float.floatToIntBits(score);}
public Object nutch_f43_0()
{    try {        return super.clone();    } catch (CloneNotSupportedException e) {        throw new RuntimeException(e);    }}
public boolean nutch_f44_0(Expression expr, String url)
{    if (expr != null && url != null) {                JexlContext jcontext = new MapContext();                jcontext.set("url", url);        jcontext.set("status", getStatusName(getStatus()));        jcontext.set("fetchTime", (long) (getFetchTime()));        jcontext.set("modifiedTime", (long) (getModifiedTime()));        jcontext.set("retries", getRetriesSinceFetch());        jcontext.set("interval", Integer.valueOf(getFetchInterval()));        jcontext.set("score", getScore());        jcontext.set("signature", StringUtil.toHexString(getSignature()));                for (Map.Entry<Writable, Writable> entry : getMetaData().entrySet()) {            Object value = entry.getValue();            Text tkey = (Text) entry.getKey();            if (value instanceof FloatWritable) {                FloatWritable fvalue = (FloatWritable) value;                jcontext.set(tkey.toString(), fvalue.get());            }            if (value instanceof IntWritable) {                IntWritable ivalue = (IntWritable) value;                jcontext.set(tkey.toString(), ivalue.get());            }            if (value instanceof Text) {                Text tvalue = (Text) value;                jcontext.set(tkey.toString().replace("-", "_"), tvalue.toString());            }            if (value instanceof ProtocolStatus) {                ProtocolStatus pvalue = (ProtocolStatus) value;                jcontext.set(tkey.toString().replace("-", "_"), pvalue.toString());            }        }        try {            if (Boolean.TRUE.equals(expr.evaluate(jcontext))) {                return true;            }        } catch (Exception e) {                }    }    return false;}
public void nutch_f45_0(Path crawlDb, Path[] segments, boolean normalize, boolean filter) throws IOException, InterruptedException, ClassNotFoundException
{    boolean additionsAllowed = getConf().getBoolean(CRAWLDB_ADDITIONS_ALLOWED, true);    update(crawlDb, segments, normalize, filter, additionsAllowed, false);}
public void nutch_f46_1(Path crawlDb, Path[] segments, boolean normalize, boolean filter, boolean additionsAllowed, boolean force) throws IOException, InterruptedException, ClassNotFoundException
{    Path lock = lock(getConf(), crawlDb, force);    SimpleDateFormat sdf = new SimpleDateFormat("yyyy-MM-dd HH:mm:ss");    long start = System.currentTimeMillis();    Job job = CrawlDb.createJob(getConf(), crawlDb);    Configuration conf = job.getConfiguration();    conf.setBoolean(CRAWLDB_ADDITIONS_ALLOWED, additionsAllowed);    conf.setBoolean(CrawlDbFilter.URL_FILTERING, filter);    conf.setBoolean(CrawlDbFilter.URL_NORMALIZING, normalize);    boolean url404Purging = conf.getBoolean(CRAWLDB_PURGE_404, false);    if (LOG.isInfoEnabled()) {                                                            }    for (int i = 0; i < segments.length; i++) {        FileSystem sfs = segments[i].getFileSystem(getConf());        Path fetch = new Path(segments[i], CrawlDatum.FETCH_DIR_NAME);        Path parse = new Path(segments[i], CrawlDatum.PARSE_DIR_NAME);        if (sfs.exists(fetch)) {            FileInputFormat.addInputPath(job, fetch);            if (sfs.exists(parse)) {                FileInputFormat.addInputPath(job, parse);            } else {                            }        } else {                    }    }    if (LOG.isInfoEnabled()) {            }    FileSystem fs = crawlDb.getFileSystem(getConf());    Path outPath = FileOutputFormat.getOutputPath(job);    try {        boolean success = job.waitForCompletion(true);        if (!success) {            String message = "CrawlDb update job did not succeed, job status:" + job.getStatus().getState() + ", reason: " + job.getStatus().getFailureInfo();                        NutchJob.cleanupAfterFailure(outPath, lock, fs);            throw new RuntimeException(message);        }    } catch (IOException | InterruptedException | ClassNotFoundException e) {                NutchJob.cleanupAfterFailure(outPath, lock, fs);        throw e;    }    CrawlDb.install(job, crawlDb);    if (filter) {        long urlsFiltered = job.getCounters().findCounter("CrawlDB filter", "URLs filtered").getValue();            }    long end = System.currentTimeMillis();    }
public static Job nutch_f47_0(Configuration config, Path crawlDb) throws IOException
{    Path newCrawlDb = new Path(crawlDb, Integer.toString(new Random().nextInt(Integer.MAX_VALUE)));    Job job = NutchJob.getInstance(config);    job.setJobName("crawldb " + crawlDb);    Path current = new Path(crawlDb, CURRENT_NAME);    if (current.getFileSystem(job.getConfiguration()).exists(current)) {        FileInputFormat.addInputPath(job, current);    }    job.setInputFormatClass(SequenceFileInputFormat.class);    job.setMapperClass(CrawlDbFilter.class);    job.setReducerClass(CrawlDbReducer.class);    job.setJarByClass(CrawlDb.class);    FileOutputFormat.setOutputPath(job, newCrawlDb);    job.setOutputFormatClass(MapFileOutputFormat.class);    job.setOutputKeyClass(Text.class);    job.setOutputValueClass(CrawlDatum.class);        job.getConfiguration().setBoolean("mapreduce.fileoutputcommitter.marksuccessfuljobs", false);    return job;}
public static Path nutch_f48_0(Configuration job, Path crawlDb, boolean force) throws IOException
{    Path lock = new Path(crawlDb, LOCK_NAME);    LockUtil.createLockFile(job, lock, force);    return lock;}
private static void nutch_f49_0(Configuration conf, Path crawlDb, Path tempCrawlDb) throws IOException
{    boolean preserveBackup = conf.getBoolean("db.preserve.backup", true);    FileSystem fs = crawlDb.getFileSystem(conf);    Path old = new Path(crawlDb, "old");    Path current = new Path(crawlDb, CURRENT_NAME);    if (fs.exists(current)) {        FSUtils.replace(fs, old, current, true);    }    FSUtils.replace(fs, current, tempCrawlDb, true);    Path lock = new Path(crawlDb, LOCK_NAME);    LockUtil.removeLockFile(fs, lock);    if (!preserveBackup && fs.exists(old)) {        fs.delete(old, true);    }}
public static void nutch_f50_0(Job job, Path crawlDb) throws IOException
{    Configuration conf = job.getConfiguration();    Path tempCrawlDb = org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.getOutputPath(job);    install(conf, crawlDb, tempCrawlDb);}
public static void nutch_f51_0(String[] args) throws Exception
{    int res = ToolRunner.run(NutchConfiguration.create(), new CrawlDb(), args);    System.exit(res);}
public int nutch_f52_1(String[] args) throws Exception
{    if (args.length < 1) {        System.err.println("Usage: CrawlDb <crawldb> (-dir <segments> | <seg1> <seg2> ...) [-force] [-normalize] [-filter] [-noAdditions]");        System.err.println("\tcrawldb\tCrawlDb to update");        System.err.println("\t-dir segments\tparent directory containing all segments to update from");        System.err.println("\tseg1 seg2 ...\tlist of segment names to update from");        System.err.println("\t-force\tforce update even if CrawlDb appears to be locked (CAUTION advised)");        System.err.println("\t-normalize\tuse URLNormalizer on urls in CrawlDb and segment (usually not needed)");        System.err.println("\t-filter\tuse URLFilters on urls in CrawlDb and segment");        System.err.println("\t-noAdditions\tonly update already existing URLs, don't add any newly discovered URLs");        return -1;    }    boolean normalize = getConf().getBoolean(CrawlDbFilter.URL_NORMALIZING, false);    boolean filter = getConf().getBoolean(CrawlDbFilter.URL_FILTERING, false);    boolean additionsAllowed = getConf().getBoolean(CRAWLDB_ADDITIONS_ALLOWED, true);    boolean force = false;    HashSet<Path> dirs = new HashSet<>();    for (int i = 1; i < args.length; i++) {        if (args[i].equals("-normalize")) {            normalize = true;        } else if (args[i].equals("-filter")) {            filter = true;        } else if (args[i].equals("-force")) {            force = true;        } else if (args[i].equals("-noAdditions")) {            additionsAllowed = false;        } else if (args[i].equals("-dir")) {            Path dirPath = new Path(args[++i]);            FileSystem fs = dirPath.getFileSystem(getConf());            FileStatus[] paths = fs.listStatus(dirPath, HadoopFSUtil.getPassDirectoriesFilter(fs));            dirs.addAll(Arrays.asList(HadoopFSUtil.getPaths(paths)));        } else {            dirs.add(new Path(args[i]));        }    }    try {        update(new Path(args[0]), dirs.toArray(new Path[dirs.size()]), normalize, filter, additionsAllowed, force);        return 0;    } catch (Exception e) {                return -1;    }}
public Map<String, Object> nutch_f53_1(Map<String, Object> args, String crawlId) throws Exception
{    Map<String, Object> results = new HashMap<>();    boolean normalize = getConf().getBoolean(CrawlDbFilter.URL_NORMALIZING, false);    boolean filter = getConf().getBoolean(CrawlDbFilter.URL_FILTERING, false);    boolean additionsAllowed = getConf().getBoolean(CRAWLDB_ADDITIONS_ALLOWED, true);    boolean force = false;    HashSet<Path> dirs = new HashSet<>();    if (args.containsKey("normalize")) {        normalize = true;    }    if (args.containsKey("filter")) {        filter = true;    }    if (args.containsKey("force")) {        force = true;    }    if (args.containsKey("noAdditions")) {        additionsAllowed = false;    }    Path crawlDb;    if (args.containsKey(Nutch.ARG_CRAWLDB)) {        Object crawldbPath = args.get(Nutch.ARG_CRAWLDB);        if (crawldbPath instanceof Path) {            crawlDb = (Path) crawldbPath;        } else {            crawlDb = new Path(crawldbPath.toString());        }    } else {        crawlDb = new Path(crawlId + "/crawldb");    }    Path segmentsDir;    if (args.containsKey(Nutch.ARG_SEGMENTDIR)) {        Object segDir = args.get(Nutch.ARG_SEGMENTDIR);        if (segDir instanceof Path) {            segmentsDir = (Path) segDir;        } else {            segmentsDir = new Path(segDir.toString());        }        FileSystem fs = segmentsDir.getFileSystem(getConf());        FileStatus[] paths = fs.listStatus(segmentsDir, HadoopFSUtil.getPassDirectoriesFilter(fs));        dirs.addAll(Arrays.asList(HadoopFSUtil.getPaths(paths)));    } else if (args.containsKey(Nutch.ARG_SEGMENTS)) {        Object segments = args.get(Nutch.ARG_SEGMENTS);        ArrayList<String> segmentList = new ArrayList<>();        if (segments instanceof ArrayList) {            segmentList = (ArrayList<String>) segments;        } else if (segments instanceof Path) {            segmentList.add(segments.toString());        }        for (String segment : segmentList) {            dirs.add(new Path(segment));        }    } else {        String segmentDir = crawlId + "/segments";        File dir = new File(segmentDir);        File[] segmentsList = dir.listFiles();        Arrays.sort(segmentsList, (f1, f2) -> {            if (f1.lastModified() > f2.lastModified())                return -1;            else                return 0;        });        dirs.add(new Path(segmentsList[0].getPath()));    }    try {        update(crawlDb, dirs.toArray(new Path[dirs.size()]), normalize, filter, additionsAllowed, force);        results.put(Nutch.VAL_RESULT, Integer.toString(0));        return results;    } catch (Exception e) {                results.put(Nutch.VAL_RESULT, Integer.toString(-1));        return results;    }}
public void nutch_f54_0(Mapper<Text, CrawlDatum, Text, CrawlDatum>.Context context)
{    Configuration conf = context.getConfiguration();    urlFiltering = conf.getBoolean(URL_FILTERING, false);    urlNormalizers = conf.getBoolean(URL_NORMALIZING, false);    url404Purging = conf.getBoolean(CrawlDb.CRAWLDB_PURGE_404, false);    purgeOrphans = conf.getBoolean(CrawlDb.CRAWLDB_PURGE_ORPHANS, false);    if (urlFiltering) {        filters = new URLFilters(conf);    }    if (urlNormalizers) {        scope = conf.get(URL_NORMALIZING_SCOPE, URLNormalizers.SCOPE_CRAWLDB);        normalizers = new URLNormalizers(conf, scope);    }}
public void nutch_f55_0()
{}
public void nutch_f56_1(Text key, CrawlDatum value, Context context) throws IOException, InterruptedException
{    String url = key.toString();        if (url404Purging && CrawlDatum.STATUS_DB_GONE == value.getStatus()) {        context.getCounter("CrawlDB filter", "Gone records removed").increment(1);        return;    }        if (purgeOrphans && CrawlDatum.STATUS_DB_ORPHAN == value.getStatus()) {        context.getCounter("CrawlDB filter", "Orphan records removed").increment(1);        return;    }    if (url != null && urlNormalizers) {        try {                        url = normalizers.normalize(url, scope);        } catch (Exception e) {                        url = null;        }    }    if (url != null && urlFiltering) {        try {                        url = filters.filter(url);        } catch (Exception e) {                        url = null;        }    }    if (url == null) {        context.getCounter("CrawlDB filter", "URLs filtered").increment(1);    } else {                        newKey.set(url);        context.write(newKey, value);    }}
public void nutch_f57_0() throws IOException
{}
public void nutch_f58_0(Reducer<Text, CrawlDatum, Text, CrawlDatum>.Context context)
{    Configuration conf = context.getConfiguration();    schedule = FetchScheduleFactory.getFetchSchedule(conf);}
public void nutch_f59_0(Text key, Iterable<CrawlDatum> values, Context context) throws IOException, InterruptedException
{    CrawlDatum res = new CrawlDatum();        res.setFetchTime(-1);    MapWritable meta = new MapWritable();    for (CrawlDatum val : values) {        if (isNewer(res, val)) {                        meta = mergeMeta(val.getMetaData(), meta);            res.set(val);        } else {                        meta = mergeMeta(meta, val.getMetaData());        }    }    res.setMetaData(meta);    context.write(key, res);}
private boolean nutch_f60_0(CrawlDatum cd1, CrawlDatum cd2)
{    return schedule.calculateLastFetchTime(cd2) > schedule.calculateLastFetchTime(cd1) || schedule.calculateLastFetchTime(cd2) == schedule.calculateLastFetchTime(cd1) && cd2.getFetchTime() > cd1.getFetchTime();}
private MapWritable nutch_f61_0(MapWritable from, MapWritable to)
{    for (Entry<Writable, Writable> e : from.entrySet()) {        to.put(e.getKey(), e.getValue());    }    return to;}
public void nutch_f62_1(Path output, Path[] dbs, boolean normalize, boolean filter) throws Exception
{    Path lock = CrawlDb.lock(getConf(), output, false);    SimpleDateFormat sdf = new SimpleDateFormat("yyyy-MM-dd HH:mm:ss");    long start = System.currentTimeMillis();        Job job = createMergeJob(getConf(), output, normalize, filter);    for (int i = 0; i < dbs.length; i++) {        if (LOG.isInfoEnabled()) {                    }        FileInputFormat.addInputPath(job, new Path(dbs[i], CrawlDb.CURRENT_NAME));    }    Path outPath = FileOutputFormat.getOutputPath(job);    FileSystem fs = outPath.getFileSystem(getConf());    try {        boolean success = job.waitForCompletion(true);        if (!success) {            String message = "CrawlDbMerger job did not succeed, job status:" + job.getStatus().getState() + ", reason: " + job.getStatus().getFailureInfo();                        NutchJob.cleanupAfterFailure(outPath, lock, fs);            throw new RuntimeException(message);        }        CrawlDb.install(job, output);    } catch (IOException | InterruptedException | ClassNotFoundException e) {                NutchJob.cleanupAfterFailure(outPath, lock, fs);        throw e;    }    long end = System.currentTimeMillis();    }
public static Job nutch_f63_0(Configuration conf, Path output, boolean normalize, boolean filter) throws IOException
{    Path newCrawlDb = new Path(output, "merge-" + Integer.toString(new Random().nextInt(Integer.MAX_VALUE)));    Job job = NutchJob.getInstance(conf);    conf = job.getConfiguration();    job.setJobName("crawldb merge " + output);    job.setInputFormatClass(SequenceFileInputFormat.class);    job.setJarByClass(CrawlDbMerger.class);    job.setMapperClass(CrawlDbFilter.class);    conf.setBoolean(CrawlDbFilter.URL_FILTERING, filter);    conf.setBoolean(CrawlDbFilter.URL_NORMALIZING, normalize);    job.setReducerClass(Merger.class);    FileOutputFormat.setOutputPath(job, newCrawlDb);    job.setOutputFormatClass(MapFileOutputFormat.class);    job.setOutputKeyClass(Text.class);    job.setOutputValueClass(CrawlDatum.class);    return job;}
public static void nutch_f64_0(String[] args) throws Exception
{    int res = ToolRunner.run(NutchConfiguration.create(), new CrawlDbMerger(), args);    System.exit(res);}
public int nutch_f65_1(String[] args) throws Exception
{    if (args.length < 2) {        System.err.println("Usage: CrawlDbMerger <output_crawldb> <crawldb1> [<crawldb2> <crawldb3> ...] [-normalize] [-filter]");        System.err.println("\toutput_crawldb\toutput CrawlDb");        System.err.println("\tcrawldb1 ...\tinput CrawlDb-s (single input CrawlDb is ok)");        System.err.println("\t-normalize\tuse URLNormalizer on urls in the crawldb(s) (usually not needed)");        System.err.println("\t-filter\tuse URLFilters on urls in the crawldb(s)");        return -1;    }    Path output = new Path(args[0]);    ArrayList<Path> dbs = new ArrayList<>();    boolean filter = false;    boolean normalize = false;    for (int i = 1; i < args.length; i++) {        if ("-filter".equals(args[i])) {            filter = true;            continue;        } else if ("-normalize".equals(args[i])) {            normalize = true;            continue;        }        final Path dbPath = new Path(args[i]);        FileSystem fs = dbPath.getFileSystem(getConf());        if (fs.exists(dbPath))            dbs.add(dbPath);    }    try {        merge(output, dbs.toArray(new Path[dbs.size()]), normalize, filter);        return 0;    } catch (Exception e) {                return -1;    }}
private void nutch_f66_0(String crawlDb, Configuration config) throws IOException
{    if (readers != null)        return;    Path crawlDbPath = new Path(crawlDb, CrawlDb.CURRENT_NAME);    readers = MapFileOutputFormat.getReaders(crawlDbPath, config);}
private void nutch_f67_0()
{    if (readers == null)        return;    for (int i = 0; i < readers.length; i++) {        try {            readers[i].close();        } catch (Exception e) {        }    }    readers = null;}
public synchronized void nutch_f68_0(Text key, CrawlDatum value) throws IOException
{    out.writeByte('"');    out.writeBytes(key.toString());    out.writeByte('"');    out.writeByte(',');    out.writeBytes(Integer.toString(value.getStatus()));    out.writeByte(',');    out.writeByte('"');    out.writeBytes(CrawlDatum.getStatusName(value.getStatus()));    out.writeByte('"');    out.writeByte(',');    out.writeBytes(new Date(value.getFetchTime()).toString());    out.writeByte(',');    out.writeBytes(new Date(value.getModifiedTime()).toString());    out.writeByte(',');    out.writeBytes(Integer.toString(value.getRetriesSinceFetch()));    out.writeByte(',');    out.writeBytes(Float.toString(value.getFetchInterval()));    out.writeByte(',');    out.writeBytes(Float.toString((value.getFetchInterval() / FetchSchedule.SECONDS_PER_DAY)));    out.writeByte(',');    out.writeBytes(Float.toString(value.getScore()));    out.writeByte(',');    out.writeByte('"');    out.writeBytes(value.getSignature() != null ? StringUtil.toHexString(value.getSignature()) : "null");    out.writeByte('"');    out.writeByte(',');    out.writeByte('"');    if (value.getMetaData() != null) {        for (Entry<Writable, Writable> e : value.getMetaData().entrySet()) {            out.writeBytes(e.getKey().toString());            out.writeByte(':');            out.writeBytes(e.getValue().toString());            out.writeBytes("|||");        }    }    out.writeByte('"');    out.writeByte('\n');}
public synchronized void nutch_f69_0(TaskAttemptContext context) throws IOException
{    out.close();}
public RecordWriter<Text, CrawlDatum> nutch_f70_0(TaskAttemptContext context) throws IOException
{    String name = getUniqueFile(context, "part", "");    Path dir = FileOutputFormat.getOutputPath(context);    FileSystem fs = dir.getFileSystem(context.getConfiguration());    DataOutputStream fileOut = fs.create(new Path(dir, name), context);    return new LineRecordWriter(fileOut);}
public void nutch_f71_0(Mapper<Text, CrawlDatum, Text, NutchWritable>.Context context)
{    Configuration conf = context.getConfiguration();    sort = conf.getBoolean("db.reader.stats.sort", false);}
public void nutch_f72_0(Text key, CrawlDatum value, Context context) throws IOException, InterruptedException
{    context.write(new Text("T"), COUNT_1);    context.write(new Text("status " + value.getStatus()), COUNT_1);    context.write(new Text("retry " + value.getRetriesSinceFetch()), COUNT_1);    if (Float.isNaN(value.getScore())) {        context.write(new Text("scNaN"), COUNT_1);    } else {        NutchWritable score = new NutchWritable(new FloatWritable(value.getScore()));        context.write(new Text("sc"), score);        context.write(new Text("sct"), score);        context.write(new Text("scd"), score);    }        NutchWritable fetchTime = new NutchWritable(new LongWritable(value.getFetchTime() / (1000 * 60)));    context.write(new Text("ft"), fetchTime);    context.write(new Text("ftt"), fetchTime);        NutchWritable fetchInterval = new NutchWritable(new LongWritable(value.getFetchInterval()));    context.write(new Text("fi"), fetchInterval);    context.write(new Text("fit"), fetchInterval);    if (sort) {        URL u = new URL(key.toString());        String host = u.getHost();        context.write(new Text("status " + value.getStatus() + " " + host), COUNT_1);    }}
public void nutch_f73_0(Reducer<Text, NutchWritable, Text, NutchWritable>.Context context)
{}
public void nutch_f74_0(Text key, Iterable<NutchWritable> values, Context context) throws IOException, InterruptedException
{    String k = key.toString();    if (k.equals("T") || k.startsWith("status") || k.startsWith("retry") || k.equals("ftt") || k.equals("fit")) {                long sum = 0;        for (NutchWritable value : values) {            sum += ((LongWritable) value.get()).get();        }                context.write(key, new NutchWritable(new LongWritable(sum)));    } else if (k.equals("sc")) {        float min = Float.MAX_VALUE;        float max = Float.MIN_VALUE;        for (NutchWritable nvalue : values) {            float value = ((FloatWritable) nvalue.get()).get();            if (max < value) {                max = value;            }            if (min > value) {                min = value;            }        }        context.write(key, new NutchWritable(new FloatWritable(min)));        context.write(key, new NutchWritable(new FloatWritable(max)));    } else if (k.equals("ft") || k.equals("fi")) {        long min = Long.MAX_VALUE;        long max = Long.MIN_VALUE;        for (NutchWritable nvalue : values) {            long value = ((LongWritable) nvalue.get()).get();            if (max < value) {                max = value;            }            if (min > value) {                min = value;            }        }        context.write(key, new NutchWritable(new LongWritable(min)));        context.write(key, new NutchWritable(new LongWritable(max)));    } else if (k.equals("sct")) {        float cnt = 0.0f;        for (NutchWritable nvalue : values) {            float value = ((FloatWritable) nvalue.get()).get();            cnt += value;        }        context.write(key, new NutchWritable(new FloatWritable(cnt)));    } else if (k.equals("scd")) {        MergingDigest tdigest = null;        for (NutchWritable nvalue : values) {            Writable value = nvalue.get();            if (value instanceof BytesWritable) {                byte[] bytes = ((BytesWritable) value).getBytes();                MergingDigest tdig = MergingDigest.fromBytes(ByteBuffer.wrap(bytes));                if (tdigest == null) {                    tdigest = tdig;                } else {                    tdigest.add(tdig);                }            } else if (value instanceof FloatWritable) {                float val = ((FloatWritable) value).get();                if (!Float.isNaN(val)) {                    if (tdigest == null) {                        tdigest = (MergingDigest) TDigest.createMergingDigest(100.0);                    }                    tdigest.add(val);                }            }        }        ByteBuffer tdigestBytes = ByteBuffer.allocate(tdigest.smallByteSize());        tdigest.asSmallBytes(tdigestBytes);        context.write(key, new NutchWritable(new BytesWritable(tdigestBytes.array())));    }}
public void nutch_f75_0(Mapper<Text, CrawlDatum, FloatWritable, Text>.Context context)
{    Configuration conf = context.getConfiguration();    min = conf.getFloat("db.reader.topn.min", 0.0f);}
public void nutch_f76_0(Text key, CrawlDatum value, Context context) throws IOException, InterruptedException
{    if (value.getScore() < min)                return;        fw.set(-value.getScore());        context.write(fw, key);}
public void nutch_f77_0(FloatWritable key, Iterable<Text> values, Context context) throws IOException, InterruptedException
{    for (Text value : values) {        if (count < topN) {            key.set(-key.get());            context.write(key, value);            count++;        }    }}
public void nutch_f78_0(Reducer<FloatWritable, Text, FloatWritable, Text>.Context context)
{    Configuration conf = context.getConfiguration();    topN = conf.getLong("db.reader.topn", 100) / Integer.parseInt(conf.get("mapreduce.job.reduces"));}
public void nutch_f79_0()
{    closeReaders();}
private TreeMap<String, Writable> nutch_f80_1(String crawlDb, Configuration config, boolean sort) throws IOException, InterruptedException, ClassNotFoundException
{    Path tmpFolder = new Path(crawlDb, "stat_tmp" + System.currentTimeMillis());    Job job = NutchJob.getInstance(config);    config = job.getConfiguration();    job.setJobName("stats " + crawlDb);    config.setBoolean("db.reader.stats.sort", sort);    FileInputFormat.addInputPath(job, new Path(crawlDb, CrawlDb.CURRENT_NAME));    job.setInputFormatClass(SequenceFileInputFormat.class);    job.setJarByClass(CrawlDbReader.class);    job.setMapperClass(CrawlDbStatMapper.class);    job.setCombinerClass(CrawlDbStatReducer.class);    job.setReducerClass(CrawlDbStatReducer.class);    FileOutputFormat.setOutputPath(job, tmpFolder);    job.setOutputFormatClass(SequenceFileOutputFormat.class);    job.setOutputKeyClass(Text.class);    job.setOutputValueClass(NutchWritable.class);        config.setBoolean("mapreduce.fileoutputcommitter.marksuccessfuljobs", false);    FileSystem fileSystem = tmpFolder.getFileSystem(config);    try {        boolean success = job.waitForCompletion(true);        if (!success) {            String message = "CrawlDbReader job did not succeed, job status:" + job.getStatus().getState() + ", reason: " + job.getStatus().getFailureInfo();                        fileSystem.delete(tmpFolder, true);            throw new RuntimeException(message);        }    } catch (IOException | InterruptedException | ClassNotFoundException e) {                fileSystem.delete(tmpFolder, true);        throw e;    }        SequenceFile.Reader[] readers = SegmentReaderUtil.getReaders(tmpFolder, config);    Text key = new Text();    NutchWritable value = new NutchWritable();    TreeMap<String, Writable> stats = new TreeMap<>();    for (int i = 0; i < readers.length; i++) {        SequenceFile.Reader reader = readers[i];        while (reader.next(key, value)) {            String k = key.toString();            Writable val = stats.get(k);            if (val == null) {                stats.put(k, value.get());                continue;            }            if (k.equals("sc")) {                float min = Float.MAX_VALUE;                float max = Float.MIN_VALUE;                if (stats.containsKey("scn")) {                    min = ((FloatWritable) stats.get("scn")).get();                } else {                    min = ((FloatWritable) stats.get("sc")).get();                }                if (stats.containsKey("scx")) {                    max = ((FloatWritable) stats.get("scx")).get();                } else {                    max = ((FloatWritable) stats.get("sc")).get();                }                float fvalue = ((FloatWritable) value.get()).get();                if (min > fvalue) {                    min = fvalue;                }                if (max < fvalue) {                    max = fvalue;                }                stats.put("scn", new FloatWritable(min));                stats.put("scx", new FloatWritable(max));            } else if (k.equals("ft") || k.equals("fi")) {                long min = Long.MAX_VALUE;                long max = Long.MIN_VALUE;                String minKey = k + "n";                String maxKey = k + "x";                if (stats.containsKey(minKey)) {                    min = ((LongWritable) stats.get(minKey)).get();                } else if (stats.containsKey(k)) {                    min = ((LongWritable) stats.get(k)).get();                }                if (stats.containsKey(maxKey)) {                    max = ((LongWritable) stats.get(maxKey)).get();                } else if (stats.containsKey(k)) {                    max = ((LongWritable) stats.get(k)).get();                }                long lvalue = ((LongWritable) value.get()).get();                if (min > lvalue) {                    min = lvalue;                }                if (max < lvalue) {                    max = lvalue;                }                stats.put(k + "n", new LongWritable(min));                stats.put(k + "x", new LongWritable(max));            } else if (k.equals("sct")) {                FloatWritable fvalue = (FloatWritable) value.get();                ((FloatWritable) val).set(((FloatWritable) val).get() + fvalue.get());            } else if (k.equals("scd")) {                MergingDigest tdigest = null;                MergingDigest tdig = MergingDigest.fromBytes(ByteBuffer.wrap(((BytesWritable) value.get()).getBytes()));                if (val instanceof BytesWritable) {                    tdigest = MergingDigest.fromBytes(ByteBuffer.wrap(((BytesWritable) val).getBytes()));                    tdigest.add(tdig);                } else {                    tdigest = tdig;                }                ByteBuffer tdigestBytes = ByteBuffer.allocate(tdigest.smallByteSize());                tdigest.asSmallBytes(tdigestBytes);                stats.put(k, new BytesWritable(tdigestBytes.array()));            } else {                LongWritable lvalue = (LongWritable) value.get();                ((LongWritable) val).set(((LongWritable) val).get() + lvalue.get());            }        }        reader.close();    }            stats.remove("sc");    stats.remove("fi");    stats.remove("ft");        fileSystem.delete(tmpFolder, true);    return stats;}
public CrawlDatum nutch_f82_0(String crawlDb, String url, Configuration config) throws IOException
{    Text key = new Text(url);    CrawlDatum val = new CrawlDatum();    openReaders(crawlDb, config);    CrawlDatum res = (CrawlDatum) MapFileOutputFormat.getEntry(readers, new HashPartitioner<>(), key, val);    return res;}
protected int nutch_f83_0(String line, StringBuilder output) throws Exception
{    Job job = NutchJob.getInstance(getConf());    Configuration config = job.getConfiguration();        closeReaders();    readUrl(this.crawlDb, line, config, output);    return 0;}
public void nutch_f84_0(String crawlDb, String url, Configuration config, StringBuilder output) throws IOException
{    CrawlDatum res = get(crawlDb, url, config);    output.append("URL: " + url + "\n");    if (res != null) {        output.append(res);    } else {        output.append("not found");    }    output.append("\n");}
public void nutch_f85_1(String crawlDb, String output, Configuration config, String format, String regex, String status, Integer retry, String expr, Float sample) throws IOException, ClassNotFoundException, InterruptedException
{    if (LOG.isInfoEnabled()) {                    }    Path outFolder = new Path(output);    Job job = NutchJob.getInstance(config);    job.setJobName("dump " + crawlDb);    Configuration jobConf = job.getConfiguration();    FileInputFormat.addInputPath(job, new Path(crawlDb, CrawlDb.CURRENT_NAME));    job.setInputFormatClass(SequenceFileInputFormat.class);    FileOutputFormat.setOutputPath(job, outFolder);    if (format.equals("csv")) {        job.setOutputFormatClass(CrawlDatumCsvOutputFormat.class);    } else if (format.equals("crawldb")) {        job.setOutputFormatClass(MapFileOutputFormat.class);    } else {        job.setOutputFormatClass(TextOutputFormat.class);    }    if (status != null)        jobConf.set("status", status);    if (regex != null)        jobConf.set("regex", regex);    if (retry != null)        jobConf.setInt("retry", retry);    if (expr != null) {        jobConf.set("expr", expr);            }    if (sample != null) {        jobConf.setFloat("sample", sample);    }    job.setMapperClass(CrawlDbDumpMapper.class);    job.setOutputKeyClass(Text.class);    job.setOutputValueClass(CrawlDatum.class);    job.setJarByClass(CrawlDbReader.class);    try {        boolean success = job.waitForCompletion(true);        if (!success) {            String message = "CrawlDbReader job did not succeed, job status:" + job.getStatus().getState() + ", reason: " + job.getStatus().getFailureInfo();                        throw new RuntimeException(message);        }    } catch (IOException | InterruptedException | ClassNotFoundException e) {                throw e;    }    if (LOG.isInfoEnabled()) {            }}
public void nutch_f86_0(Mapper<Text, CrawlDatum, Text, CrawlDatum>.Context context)
{    Configuration config = context.getConfiguration();    if (config.get("regex", null) != null) {        pattern = Pattern.compile(config.get("regex"));    }    status = config.get("status", null);    retry = config.getInt("retry", -1);    if (config.get("expr", null) != null) {        expr = JexlUtil.parseExpression(config.get("expr", null));    }    sample = config.getFloat("sample", 1);}
public void nutch_f87_0(Text key, CrawlDatum value, Context context) throws IOException, InterruptedException
{        if (sample < 1 && Math.random() > sample) {        return;    }        if (retry != -1) {        if (value.getRetriesSinceFetch() < retry) {            return;        }    }        if (status != null && !status.equalsIgnoreCase(CrawlDatum.getStatusName(value.getStatus())))        return;        if (pattern != null) {        matcher = pattern.matcher(key.toString());        if (!matcher.matches()) {            return;        }    }        if (expr != null) {        if (!value.evaluate(expr, key.toString())) {            return;        }    }    context.write(key, value);}
public void nutch_f88_1(String crawlDb, long topN, float min, String output, Configuration config) throws IOException, ClassNotFoundException, InterruptedException
{    if (LOG.isInfoEnabled()) {                    }    Path outFolder = new Path(output);    Path tempDir = new Path(config.get("mapreduce.cluster.temp.dir", ".") + "/readdb-topN-temp-" + Integer.toString(new Random().nextInt(Integer.MAX_VALUE)));    Job job = NutchJob.getInstance(config);    job.setJobName("topN prepare " + crawlDb);    FileInputFormat.addInputPath(job, new Path(crawlDb, CrawlDb.CURRENT_NAME));    job.setInputFormatClass(SequenceFileInputFormat.class);    job.setJarByClass(CrawlDbReader.class);    job.setMapperClass(CrawlDbTopNMapper.class);    job.setReducerClass(Reducer.class);    FileOutputFormat.setOutputPath(job, tempDir);    job.setOutputFormatClass(SequenceFileOutputFormat.class);    job.setOutputKeyClass(FloatWritable.class);    job.setOutputValueClass(Text.class);    job.getConfiguration().setFloat("db.reader.topn.min", min);    FileSystem fs = tempDir.getFileSystem(config);    try {        boolean success = job.waitForCompletion(true);        if (!success) {            String message = "CrawlDbReader job did not succeed, job status:" + job.getStatus().getState() + ", reason: " + job.getStatus().getFailureInfo();                        fs.delete(tempDir, true);            throw new RuntimeException(message);        }    } catch (IOException | InterruptedException | ClassNotFoundException e) {                fs.delete(tempDir, true);        throw e;    }    if (LOG.isInfoEnabled()) {            }    job = NutchJob.getInstance(config);    job.setJobName("topN collect " + crawlDb);    job.getConfiguration().setLong("db.reader.topn", topN);    FileInputFormat.addInputPath(job, tempDir);    job.setInputFormatClass(SequenceFileInputFormat.class);    job.setMapperClass(Mapper.class);    job.setReducerClass(CrawlDbTopNReducer.class);    job.setJarByClass(CrawlDbReader.class);    FileOutputFormat.setOutputPath(job, outFolder);    job.setOutputFormatClass(TextOutputFormat.class);    job.setOutputKeyClass(FloatWritable.class);    job.setOutputValueClass(Text.class);        job.setNumReduceTasks(1);    try {        boolean success = job.waitForCompletion(true);        if (!success) {            String message = "CrawlDbReader job did not succeed, job status:" + job.getStatus().getState() + ", reason: " + job.getStatus().getFailureInfo();                        fs.delete(tempDir, true);            throw new RuntimeException(message);        }    } catch (IOException | InterruptedException | ClassNotFoundException e) {                fs.delete(tempDir, true);        throw e;    }    fs.delete(tempDir, true);    if (LOG.isInfoEnabled()) {            }}
public int nutch_f89_0(String[] args) throws IOException, InterruptedException, ClassNotFoundException, Exception
{    @SuppressWarnings("resource")    CrawlDbReader dbr = new CrawlDbReader();    if (args.length < 2) {        System.err.println("Usage: CrawlDbReader <crawldb> (-stats | -dump <out_dir> | -topN <nnnn> <out_dir> [<min>] | -url <url>)");        System.err.println("\t<crawldb>\tdirectory name where crawldb is located");        System.err.println("\t-stats [-sort] \tprint overall statistics to System.out");        System.err.println("\t\t[-sort]\tlist status sorted by host");        System.err.println("\t-dump <out_dir> [-format normal|csv|crawldb]\tdump the whole db to a text file in <out_dir>");        System.err.println("\t\t[-format csv]\tdump in Csv format");        System.err.println("\t\t[-format normal]\tdump in standard format (default option)");        System.err.println("\t\t[-format crawldb]\tdump as CrawlDB");        System.err.println("\t\t[-regex <expr>]\tfilter records with expression");        System.err.println("\t\t[-retry <num>]\tminimum retry count");        System.err.println("\t\t[-status <status>]\tfilter records by CrawlDatum status");        System.err.println("\t\t[-expr <expr>]\tJexl expression to evaluate for this record");        System.err.println("\t\t[-sample <fraction>]\tOnly process a random sample with this ratio");        System.err.println("\t-url <url>\tprint information on <url> to System.out");        System.err.println("\t-topN <nnnn> <out_dir> [<min>]\tdump top <nnnn> urls sorted by score to <out_dir>");        System.err.println("\t\t[<min>]\tskip records with scores below this value.");        System.err.println("\t\t\tThis can significantly improve performance.");        return -1;    }    String param = null;    String crawlDb = args[0];    this.crawlDb = crawlDb;    int numConsumed = 0;    Configuration config = getConf();    for (int i = 1; i < args.length; i++) {        if (args[i].equals("-stats")) {            boolean toSort = false;            if (i < args.length - 1 && "-sort".equals(args[i + 1])) {                toSort = true;                i++;            }            dbr.processStatJob(crawlDb, config, toSort);        } else if (args[i].equals("-dump")) {            param = args[++i];            String format = "normal";            String regex = null;            Integer retry = null;            String status = null;            String expr = null;            Float sample = null;            for (int j = i + 1; j < args.length; j++) {                if (args[j].equals("-format")) {                    format = args[++j];                    i = i + 2;                }                if (args[j].equals("-regex")) {                    regex = args[++j];                    i = i + 2;                }                if (args[j].equals("-retry")) {                    retry = Integer.parseInt(args[++j]);                    i = i + 2;                }                if (args[j].equals("-status")) {                    status = args[++j];                    i = i + 2;                }                if (args[j].equals("-expr")) {                    expr = args[++j];                    i = i + 2;                }                if (args[j].equals("-sample")) {                    sample = Float.parseFloat(args[++j]);                    i = i + 2;                }            }            dbr.processDumpJob(crawlDb, param, config, format, regex, status, retry, expr, sample);        } else if (args[i].equals("-url")) {            param = args[++i];            StringBuilder output = new StringBuilder();            dbr.readUrl(crawlDb, param, config, output);            System.out.print(output);        } else if (args[i].equals("-topN")) {            param = args[++i];            long topN = Long.parseLong(param);            param = args[++i];            float min = 0.0f;            if (i < args.length - 1) {                min = Float.parseFloat(args[++i]);            }            dbr.processTopNJob(crawlDb, topN, min, param, config);        } else if ((numConsumed = super.parseArgs(args, i)) > 0) {            i += numConsumed - 1;        } else {            System.err.println("\nError: wrong argument " + args[i]);            return -1;        }    }    if (numConsumed > 0) {                return super.run();    }    return 0;}
public static void nutch_f90_0(String[] args) throws Exception
{    int result = ToolRunner.run(NutchConfiguration.create(), new CrawlDbReader(), args);    System.exit(result);}
public Object nutch_f91_0(Map<String, String> args, Configuration conf, String type, String crawlId) throws Exception
{    Map<String, Object> results = new HashMap<>();    String crawlDb = crawlId + "/crawldb";    if (type.equalsIgnoreCase("stats")) {        boolean sort = false;        if (args.containsKey("sort")) {            if (args.get("sort").equalsIgnoreCase("true"))                sort = true;        }        TreeMap<String, Writable> stats = processStatJobHelper(crawlDb, NutchConfiguration.create(), sort);        LongWritable totalCnt = (LongWritable) stats.get("T");        stats.remove("T");        results.put("totalUrls", String.valueOf(totalCnt.get()));        Map<String, Object> statusMap = new HashMap<>();        for (Map.Entry<String, Writable> entry : stats.entrySet()) {            String k = entry.getKey();            long val = 0L;            double fval = 0.0;            if (entry.getValue() instanceof LongWritable) {                val = ((LongWritable) entry.getValue()).get();            } else if (entry.getValue() instanceof FloatWritable) {                fval = ((FloatWritable) entry.getValue()).get();            } else if (entry.getValue() instanceof BytesWritable) {                continue;            }            if (k.equals("scn")) {                results.put("minScore", String.valueOf(fval));            } else if (k.equals("scx")) {                results.put("maxScore", String.valueOf(fval));            } else if (k.equals("sct")) {                results.put("avgScore", String.valueOf((fval / totalCnt.get())));            } else if (k.startsWith("status")) {                String[] st = k.split(" ");                int code = Integer.parseInt(st[1]);                if (st.length > 2) {                    @SuppressWarnings("unchecked")                    Map<String, Object> individualStatusInfo = (Map<String, Object>) statusMap.get(String.valueOf(code));                    Map<String, String> hostValues;                    if (individualStatusInfo.containsKey("hostValues")) {                        hostValues = (Map<String, String>) individualStatusInfo.get("hostValues");                    } else {                        hostValues = new HashMap<>();                        individualStatusInfo.put("hostValues", hostValues);                    }                    hostValues.put(st[2], String.valueOf(val));                } else {                    Map<String, Object> individualStatusInfo = new HashMap<>();                    individualStatusInfo.put("statusValue", CrawlDatum.getStatusName((byte) code));                    individualStatusInfo.put("count", String.valueOf(val));                    statusMap.put(String.valueOf(code), individualStatusInfo);                }            } else {                results.put(k, String.valueOf(val));            }        }        results.put("status", statusMap);        return results;    }    if (type.equalsIgnoreCase("dump")) {        String output = args.get("out_dir");        String format = "normal";        String regex = null;        Integer retry = null;        String status = null;        String expr = null;        Float sample = null;        if (args.containsKey("format")) {            format = args.get("format");        }        if (args.containsKey("regex")) {            regex = args.get("regex");        }        if (args.containsKey("retry")) {            retry = Integer.parseInt(args.get("retry"));        }        if (args.containsKey("status")) {            status = args.get("status");        }        if (args.containsKey("expr")) {            expr = args.get("expr");        }        if (args.containsKey("sample")) {            sample = Float.parseFloat(args.get("sample"));        }        processDumpJob(crawlDb, output, conf, format, regex, status, retry, expr, sample);        File dumpFile = new File(output + "/part-00000");        return dumpFile;    }    if (type.equalsIgnoreCase("topN")) {        String output = args.get("out_dir");        long topN = Long.parseLong(args.get("nnn"));        float min = 0.0f;        if (args.containsKey("min")) {            min = Float.parseFloat(args.get("min"));        }        processTopNJob(crawlDb, topN, min, output, conf);        File dumpFile = new File(output + "/part-00000");        return dumpFile;    }    if (type.equalsIgnoreCase("url")) {        String url = args.get("url");        CrawlDatum res = get(crawlDb, url, conf);        results.put("status", res.getStatus());        results.put("fetchTime", new Date(res.getFetchTime()));        results.put("modifiedTime", new Date(res.getModifiedTime()));        results.put("retriesSinceFetch", res.getRetriesSinceFetch());        results.put("retryInterval", res.getFetchInterval());        results.put("score", res.getScore());        results.put("signature", StringUtil.toHexString(res.getSignature()));        Map<String, String> metadata = new HashMap<>();        if (res.getMetaData() != null) {            for (Entry<Writable, Writable> e : res.getMetaData().entrySet()) {                metadata.put(String.valueOf(e.getKey()), String.valueOf(e.getValue()));            }        }        results.put("metadata", metadata);        return results;    }    return results;}
public void nutch_f92_0(Reducer<Text, CrawlDatum, Text, CrawlDatum>.Context context)
{    Configuration conf = context.getConfiguration();    retryMax = conf.getInt("db.fetch.retry.max", 3);    scfilters = new ScoringFilters(conf);    additionsAllowed = conf.getBoolean(CrawlDb.CRAWLDB_ADDITIONS_ALLOWED, true);    maxInterval = conf.getInt("db.fetch.interval.max", 0);    schedule = FetchScheduleFactory.getFetchSchedule(conf);    int maxLinks = conf.getInt("db.update.max.inlinks", 10000);    linked = new InlinkPriorityQueue(maxLinks);}
public void nutch_f93_0()
{}
public void nutch_f94_1(Text key, Iterable<CrawlDatum> values, Context context) throws IOException, InterruptedException
{    CrawlDatum fetch = new CrawlDatum();    CrawlDatum old = new CrawlDatum();    boolean fetchSet = false;    boolean oldSet = false;    byte[] signature = null;        boolean multiple = false;    linked.clear();    org.apache.hadoop.io.MapWritable metaFromParse = null;    for (CrawlDatum datum : values) {        if (!multiple)            multiple = true;        if (CrawlDatum.hasDbStatus(datum)) {            if (!oldSet) {                if (multiple) {                    old.set(datum);                } else {                                        old = datum;                }                oldSet = true;            } else {                                if (old.getFetchTime() < datum.getFetchTime())                    old.set(datum);            }            continue;        }        if (CrawlDatum.hasFetchStatus(datum)) {            if (!fetchSet) {                if (multiple) {                    fetch.set(datum);                } else {                    fetch = datum;                }                fetchSet = true;            } else {                                if (fetch.getFetchTime() < datum.getFetchTime())                    fetch.set(datum);            }            continue;        }        switch(        datum.getStatus()) {            case CrawlDatum.STATUS_LINKED:                CrawlDatum link;                if (multiple) {                    link = new CrawlDatum();                    link.set(datum);                } else {                    link = datum;                }                linked.insert(link);                break;            case CrawlDatum.STATUS_SIGNATURE:                signature = datum.getSignature();                break;            case CrawlDatum.STATUS_PARSE_META:                metaFromParse = datum.getMetaData();                break;            default:                        }    }            int numLinks = linked.size();    List<CrawlDatum> linkList = new ArrayList<>(numLinks);    for (int i = numLinks - 1; i >= 0; i--) {        linkList.add(linked.pop());    }        if (!oldSet && !additionsAllowed)        return;        if (!fetchSet && linkList.size() > 0) {        fetch = linkList.get(0);        fetchSet = true;    }        if (!fetchSet) {        if (oldSet) {                        try {                scfilters.orphanedScore(key, old);            } catch (ScoringFilterException e) {                if (LOG.isWarnEnabled()) {                                    }            }            context.write(key, old);            context.getCounter("CrawlDB status", CrawlDatum.getStatusName(old.getStatus())).increment(1);        } else {                    }        return;    }    if (signature == null)        signature = fetch.getSignature();    long prevModifiedTime = oldSet ? old.getModifiedTime() : 0L;    long prevFetchTime = oldSet ? old.getFetchTime() : 0L;        result.set(fetch);    if (oldSet) {                if (old.getMetaData().size() > 0) {            result.putAllMetaData(old);                        if (fetch.getMetaData().size() > 0)                result.putAllMetaData(fetch);        }                if (old.getModifiedTime() > 0 && fetch.getModifiedTime() == 0) {            result.setModifiedTime(old.getModifiedTime());        }    }    switch(    fetch.getStatus()) {        case         CrawlDatum.STATUS_LINKED:            if (oldSet) {                                                result.set(old);            } else {                result = schedule.initializeSchedule(key, result);                result.setStatus(CrawlDatum.STATUS_DB_UNFETCHED);                try {                    scfilters.initialScore(key, result);                } catch (ScoringFilterException e) {                    if (LOG.isWarnEnabled()) {                                            }                    result.setScore(0.0f);                }            }            break;                case CrawlDatum.STATUS_FETCH_SUCCESS:                case CrawlDatum.STATUS_FETCH_REDIR_TEMP:        case CrawlDatum.STATUS_FETCH_REDIR_PERM:        case         CrawlDatum.STATUS_FETCH_NOTMODIFIED:                        if (metaFromParse != null) {                for (Entry<Writable, Writable> e : metaFromParse.entrySet()) {                    result.getMetaData().put(e.getKey(), e.getValue());                }            }                        int modified = FetchSchedule.STATUS_UNKNOWN;            if (fetch.getStatus() == CrawlDatum.STATUS_FETCH_NOTMODIFIED) {                modified = FetchSchedule.STATUS_NOTMODIFIED;            } else if (fetch.getStatus() == CrawlDatum.STATUS_FETCH_SUCCESS) {                                if (oldSet && old.getSignature() != null && signature != null) {                    if (SignatureComparator._compare(old.getSignature(), signature) != 0) {                        modified = FetchSchedule.STATUS_MODIFIED;                    } else {                        modified = FetchSchedule.STATUS_NOTMODIFIED;                    }                }            }                        result = schedule.setFetchSchedule(key, result, prevFetchTime, prevModifiedTime, fetch.getFetchTime(), fetch.getModifiedTime(), modified);                        if (modified == FetchSchedule.STATUS_NOTMODIFIED) {                result.setStatus(CrawlDatum.STATUS_DB_NOTMODIFIED);                                                result.setModifiedTime(prevModifiedTime);                if (oldSet)                    result.setSignature(old.getSignature());            } else {                switch(fetch.getStatus()) {                    case CrawlDatum.STATUS_FETCH_SUCCESS:                        result.setStatus(CrawlDatum.STATUS_DB_FETCHED);                        break;                    case CrawlDatum.STATUS_FETCH_REDIR_PERM:                        result.setStatus(CrawlDatum.STATUS_DB_REDIR_PERM);                        break;                    case CrawlDatum.STATUS_FETCH_REDIR_TEMP:                        result.setStatus(CrawlDatum.STATUS_DB_REDIR_TEMP);                        break;                    default:                                                if (oldSet)                            result.setStatus(old.getStatus());                        else                            result.setStatus(CrawlDatum.STATUS_DB_UNFETCHED);                }                result.setSignature(signature);            }                        if (maxInterval < result.getFetchInterval())                result = schedule.forceRefetch(key, result, false);            break;        case CrawlDatum.STATUS_SIGNATURE:            if (LOG.isWarnEnabled()) {                            }            return;        case         CrawlDatum.STATUS_FETCH_RETRY:            if (oldSet) {                                result.setSignature(old.getSignature());            }            result = schedule.setPageRetrySchedule(key, result, prevFetchTime, prevModifiedTime, fetch.getFetchTime());            if (result.getRetriesSinceFetch() < retryMax) {                result.setStatus(CrawlDatum.STATUS_DB_UNFETCHED);            } else {                result.setStatus(CrawlDatum.STATUS_DB_GONE);                result = schedule.setPageGoneSchedule(key, result, prevFetchTime, prevModifiedTime, fetch.getFetchTime());            }            break;        case         CrawlDatum.STATUS_FETCH_GONE:            if (oldSet)                                result.setSignature(old.getSignature());            result.setStatus(CrawlDatum.STATUS_DB_GONE);            result = schedule.setPageGoneSchedule(key, result, prevFetchTime, prevModifiedTime, fetch.getFetchTime());            break;        default:            throw new RuntimeException("Unknown status: " + fetch.getStatus() + " " + key);    }    try {        scfilters.updateDbScore(key, oldSet ? old : null, result, linkList);    } catch (Exception e) {        if (LOG.isWarnEnabled()) {                    }    }        result.getMetaData().remove(Nutch.WRITABLE_GENERATE_TIME_KEY);    context.write(key, result);    context.getCounter("CrawlDB status", CrawlDatum.getStatusName(result.getStatus())).increment(1);}
protected boolean nutch_f95_0(Object arg0, Object arg1)
{    CrawlDatum candidate = (CrawlDatum) arg0;    CrawlDatum least = (CrawlDatum) arg1;    return candidate.getScore() > least.getScore();}
public void nutch_f96_0(Mapper<Text, CrawlDatum, BytesWritable, CrawlDatum>.Context context)
{    Configuration arg0 = context.getConfiguration();    groupMode = arg0.get(DEDUPLICATION_GROUP_MODE);}
public void nutch_f97_0(Text key, CrawlDatum value, Context context) throws IOException, InterruptedException
{    if (value.getStatus() == CrawlDatum.STATUS_DB_FETCHED || value.getStatus() == CrawlDatum.STATUS_DB_NOTMODIFIED) {                byte[] signature = value.getSignature();        if (signature == null)            return;        String url = key.toString();        BytesWritable sig = null;        byte[] data;        switch(groupMode) {            case "none":                sig = new BytesWritable(signature);                break;            case "host":                byte[] host = URLUtil.getHost(url).getBytes();                data = new byte[signature.length + host.length];                System.arraycopy(signature, 0, data, 0, signature.length);                System.arraycopy(host, 0, data, signature.length, host.length);                sig = new BytesWritable(data);                break;            case "domain":                byte[] domain = URLUtil.getDomainName(url).getBytes();                data = new byte[signature.length + domain.length];                System.arraycopy(signature, 0, data, 0, signature.length);                System.arraycopy(domain, 0, data, signature.length, domain.length);                sig = new BytesWritable(data);                break;        }                value.getMetaData().put(urlKey, key);                context.write(sig, value);    }}
public void nutch_f98_0(Reducer<BytesWritable, CrawlDatum, Text, CrawlDatum>.Context context)
{    Configuration conf = context.getConfiguration();    compareOrder = conf.get(DEDUPLICATION_COMPARE_ORDER).split(",");}
private void nutch_f99_0(CrawlDatum datum, Context context) throws IOException, InterruptedException
{    datum.setStatus(CrawlDatum.STATUS_DB_DUPLICATE);    Text key = (Text) datum.getMetaData().remove(urlKey);    context.getCounter("DeduplicationJobStatus", "Documents marked as duplicate").increment(1);    context.write(key, datum);}
public void nutch_f100_0(BytesWritable key, Iterable<CrawlDatum> values, Context context) throws IOException, InterruptedException
{    CrawlDatum existingDoc = null;    for (CrawlDatum newDoc : values) {        if (existingDoc == null) {            existingDoc = new CrawlDatum();            existingDoc.set(newDoc);            continue;        }        CrawlDatum duplicate = getDuplicate(existingDoc, newDoc);        if (duplicate != null) {            writeOutAsDuplicate(duplicate, context);            if (duplicate == existingDoc) {                                existingDoc.set(newDoc);            }        }    }}
private CrawlDatum nutch_f101_1(CrawlDatum existingDoc, CrawlDatum newDoc) throws IOException
{    for (int i = 0; i < compareOrder.length; i++) {        switch(compareOrder[i]) {            case "score":                                if (existingDoc.getScore() < newDoc.getScore()) {                    return existingDoc;                } else if (existingDoc.getScore() > newDoc.getScore()) {                                        return newDoc;                }                break;            case "fetchTime":                                if (existingDoc.getFetchTime() > newDoc.getFetchTime()) {                                        return newDoc;                } else if (existingDoc.getFetchTime() < newDoc.getFetchTime()) {                                        return existingDoc;                }                break;            case "httpsOverHttp":                                                String url1 = existingDoc.getMetaData().get(urlKey).toString();                String url2 = newDoc.getMetaData().get(urlKey).toString();                if (url1.startsWith("https://") && url2.startsWith("http://") && url1.substring(8).equals(url2.substring(7))) {                                        return newDoc;                } else if (url2.startsWith("https://") && url1.startsWith("http://") && url2.substring(8).equals(url1.substring(7))) {                                        return existingDoc;                }                break;            case "urlLength":                                String urlExisting;                String urlnewDoc;                try {                    urlExisting = URLDecoder.decode(existingDoc.getMetaData().get(urlKey).toString(), "UTF8");                    urlnewDoc = URLDecoder.decode(newDoc.getMetaData().get(urlKey).toString(), "UTF8");                } catch (UnsupportedEncodingException e) {                                        throw new IOException("UnsupportedEncodingException for " + urlKey);                }                if (urlExisting.length() < urlnewDoc.length()) {                                        return newDoc;                } else if (urlExisting.length() > urlnewDoc.length()) {                                        return existingDoc;                }                break;        }    }        return null;}
public void nutch_f102_0(Reducer<Text, CrawlDatum, Text, CrawlDatum>.Context context)
{}
public void nutch_f103_0(Text key, Iterable<CrawlDatum> values, Context context) throws IOException, InterruptedException
{    boolean duplicateSet = false;    for (CrawlDatum val : values) {        if (val.getStatus() == CrawlDatum.STATUS_DB_DUPLICATE) {            duplicate.set(val);            duplicateSet = true;        } else {            old.set(val);        }    }        if (duplicateSet) {        context.write(key, duplicate);        return;    }        context.write(key, old);}
public int nutch_f104_1(String[] args) throws IOException
{    if (args.length < 1) {        System.err.println("Usage: DeduplicationJob <crawldb> [-group <none|host|domain>] [-compareOrder <score>,<fetchTime>,<httpsOverHttp>,<urlLength>]");        return 1;    }    String group = "none";    Path crawlDb = new Path(args[0]);    String compareOrder = "score,fetchTime,urlLength";    for (int i = 1; i < args.length; i++) {        if (args[i].equals("-group"))            group = args[++i];        if (args[i].equals("-compareOrder")) {            compareOrder = args[++i];            if (compareOrder.indexOf("score") == -1 || compareOrder.indexOf("fetchTime") == -1 || compareOrder.indexOf("urlLength") == -1) {                System.err.println("DeduplicationJob: compareOrder must contain score, fetchTime and urlLength.");                return 1;            }        }    }    SimpleDateFormat sdf = new SimpleDateFormat("yyyy-MM-dd HH:mm:ss");    long start = System.currentTimeMillis();        Path tempDir = new Path(crawlDb, "dedup-temp-" + Integer.toString(new Random().nextInt(Integer.MAX_VALUE)));    Job job = NutchJob.getInstance(getConf());    Configuration conf = job.getConfiguration();    job.setJobName("Deduplication on " + crawlDb);    conf.set(DEDUPLICATION_GROUP_MODE, group);    conf.set(DEDUPLICATION_COMPARE_ORDER, compareOrder);    job.setJarByClass(DeduplicationJob.class);    FileInputFormat.addInputPath(job, new Path(crawlDb, CrawlDb.CURRENT_NAME));    job.setInputFormatClass(SequenceFileInputFormat.class);    FileOutputFormat.setOutputPath(job, tempDir);    job.setOutputFormatClass(SequenceFileOutputFormat.class);    job.setMapOutputKeyClass(BytesWritable.class);    job.setMapOutputValueClass(CrawlDatum.class);    job.setOutputKeyClass(Text.class);    job.setOutputValueClass(CrawlDatum.class);    job.setMapperClass(DBFilter.class);    job.setReducerClass(DedupReducer.class);    FileSystem fs = tempDir.getFileSystem(getConf());    try {        boolean success = job.waitForCompletion(true);        if (!success) {            String message = "Crawl job did not succeed, job status:" + job.getStatus().getState() + ", reason: " + job.getStatus().getFailureInfo();                        fs.delete(tempDir, true);            throw new RuntimeException(message);        }        CounterGroup g = job.getCounters().getGroup("DeduplicationJobStatus");        if (g != null) {            Counter counter = g.findCounter("Documents marked as duplicate");            long dups = counter.getValue();                    }    } catch (IOException | InterruptedException | ClassNotFoundException e) {                fs.delete(tempDir, true);        return -1;    }        if (LOG.isInfoEnabled()) {            }    Job mergeJob = CrawlDb.createJob(getConf(), crawlDb);    FileInputFormat.addInputPath(mergeJob, tempDir);    mergeJob.setReducerClass(StatusUpdateReducer.class);    mergeJob.setJarByClass(DeduplicationJob.class);    fs = crawlDb.getFileSystem(getConf());    Path outPath = FileOutputFormat.getOutputPath(job);    Path lock = CrawlDb.lock(getConf(), crawlDb, false);    try {        boolean success = mergeJob.waitForCompletion(true);        if (!success) {            String message = "Crawl job did not succeed, job status:" + mergeJob.getStatus().getState() + ", reason: " + mergeJob.getStatus().getFailureInfo();                        fs.delete(tempDir, true);            NutchJob.cleanupAfterFailure(outPath, lock, fs);            throw new RuntimeException(message);        }    } catch (IOException | InterruptedException | ClassNotFoundException e) {                fs.delete(tempDir, true);        NutchJob.cleanupAfterFailure(outPath, lock, fs);        return -1;    }    CrawlDb.install(mergeJob, crawlDb);        fs.delete(tempDir, true);    long end = System.currentTimeMillis();        return 0;}
public static void nutch_f105_0(String[] args) throws Exception
{    int result = ToolRunner.run(NutchConfiguration.create(), new DeduplicationJob(), args);    System.exit(result);}
public Map<String, Object> nutch_f106_0(Map<String, Object> args, String crawlId) throws Exception
{    Map<String, Object> results = new HashMap<>();    String[] arg = new String[1];    String crawldb;    if (args.containsKey(Nutch.ARG_CRAWLDB)) {        crawldb = (String) args.get(Nutch.ARG_CRAWLDB);    } else {        crawldb = crawlId + "/crawldb";    }    arg[0] = crawldb;    int res = run(arg);    results.put(Nutch.VAL_RESULT, Integer.toString(res));    return results;}
public CrawlDatum nutch_f107_0(Text url, CrawlDatum datum, long prevFetchTime, long prevModifiedTime, long fetchTime, long modifiedTime, int state)
{    datum = super.setFetchSchedule(url, datum, prevFetchTime, prevModifiedTime, fetchTime, modifiedTime, state);    if (datum.getFetchInterval() == 0) {        datum.setFetchInterval(defaultInterval);    }    datum.setFetchTime(fetchTime + (long) datum.getFetchInterval() * 1000);    if (modifiedTime <= 0 || state == FetchSchedule.STATUS_MODIFIED) {                modifiedTime = fetchTime;    }    datum.setModifiedTime(modifiedTime);    return datum;}
public static synchronized FetchSchedule nutch_f108_1(Configuration conf)
{    String clazz = conf.get("db.fetch.schedule.class", DefaultFetchSchedule.class.getName());    ObjectCache objectCache = ObjectCache.get(conf);    FetchSchedule impl = (FetchSchedule) objectCache.getObject(clazz);    if (impl == null) {        try {                        Class<?> implClass = Class.forName(clazz);            impl = (FetchSchedule) implClass.getConstructor().newInstance();            impl.setConf(conf);            objectCache.setObject(clazz, impl);        } catch (Exception e) {            throw new RuntimeException("Couldn't create " + clazz, e);        }    }    return impl;}
public void nutch_f109_0(DataInput in) throws IOException
{    url.readFields(in);    datum.readFields(in);    segnum.readFields(in);}
public void nutch_f110_0(DataOutput out) throws IOException
{    url.write(out);    datum.write(out);    segnum.write(out);}
public String nutch_f111_0()
{    return "url=" + url.toString() + ", datum=" + datum.toString() + ", segnum=" + segnum.toString();}
public int nutch_f112_0(FloatWritable key, Writable value, int numReduceTasks)
{    return partitioner.getPartition(((SelectorEntry) value).url, key, numReduceTasks);}
public Configuration nutch_f113_0()
{    return partitioner.getConf();}
public void nutch_f114_0(Configuration conf)
{    partitioner.setConf(conf);}
public void nutch_f115_0(Mapper<Text, CrawlDatum, FloatWritable, SelectorEntry>.Context context) throws IOException
{    conf = context.getConfiguration();    curTime = conf.getLong(GENERATOR_CUR_TIME, System.currentTimeMillis());    filters = new URLFilters(conf);    scfilters = new ScoringFilters(conf);    filter = conf.getBoolean(GENERATOR_FILTER, true);    /* CrawlDb items are unblocked after 7 days as default */    genDelay = conf.getLong(GENERATOR_DELAY, 604800000L);    long time = conf.getLong(Nutch.GENERATE_TIME_KEY, 0L);    if (time > 0)        genTime.set(time);    schedule = FetchScheduleFactory.getFetchSchedule(conf);    scoreThreshold = conf.getFloat(GENERATOR_MIN_SCORE, Float.NaN);    intervalThreshold = conf.getInt(GENERATOR_MIN_INTERVAL, -1);    String restrictStatusString = conf.getTrimmed(GENERATOR_RESTRICT_STATUS, "");    if (!restrictStatusString.isEmpty()) {        restrictStatus = CrawlDatum.getStatusByName(restrictStatusString);    }    expr = JexlUtil.parseExpression(conf.get(GENERATOR_EXPR, null));}
public void nutch_f116_1(Text key, CrawlDatum value, Context context) throws IOException, InterruptedException
{    Text url = key;    if (filter) {                try {            if (filters.filter(url.toString()) == null)                return;        } catch (URLFilterException e) {                    }    }    CrawlDatum crawlDatum = value;        if (!schedule.shouldFetch(url, crawlDatum, curTime)) {                context.getCounter("Generator", "SCHEDULE_REJECTED").increment(1);        return;    }    LongWritable oldGenTime = (LongWritable) crawlDatum.getMetaData().get(Nutch.WRITABLE_GENERATE_TIME_KEY);    if (oldGenTime != null) {                if (        oldGenTime.get() + genDelay > curTime)                        context.getCounter("Generator", "WAIT_FOR_UPDATE").increment(1);        return;    }    float sort = 1.0f;    try {        sort = scfilters.generatorSortValue(key, crawlDatum, sort);    } catch (ScoringFilterException sfe) {        if (LOG.isWarnEnabled()) {                    }    }        if (expr != null) {        if (!crawlDatum.evaluate(expr, key.toString())) {            context.getCounter("Generator", "EXPR_REJECTED").increment(1);            return;        }    }    if (restrictStatus != -1 && restrictStatus != crawlDatum.getStatus()) {        context.getCounter("Generator", "STATUS_REJECTED").increment(1);        return;    }        if (!Float.isNaN(scoreThreshold) && sort < scoreThreshold) {        context.getCounter("Generator", "SCORE_TOO_LOW").increment(1);        return;    }        if (intervalThreshold != -1 && crawlDatum.getFetchInterval() > intervalThreshold) {        context.getCounter("Generator", "INTERVAL_REJECTED").increment(1);        return;    }        sortValue.set(sort);        crawlDatum.getMetaData().put(Nutch.WRITABLE_GENERATE_TIME_KEY, genTime);    entry.datum = crawlDatum;    entry.url = key;        context.write(sortValue, entry);}
public void nutch_f117_1()
{    if (conf.get(GENERATOR_HOSTDB) != null) {        try {            Path path = new Path(conf.get(GENERATOR_HOSTDB), "current");            hostdbReaders = SegmentReaderUtil.getReaders(path, conf);        } catch (IOException e) {                    }    }}
public void nutch_f118_1()
{    if (hostdbReaders != null) {        try {            for (int i = 0; i < hostdbReaders.length; i++) {                hostdbReaders[i].close();            }        } catch (IOException e) {                    }    }}
private JexlContext nutch_f119_0(HostDatum datum)
{    JexlContext context = new MapContext();    context.set("dnsFailures", datum.getDnsFailures());    context.set("connectionFailures", datum.getConnectionFailures());    context.set("unfetched", datum.getUnfetched());    context.set("fetched", datum.getFetched());    context.set("notModified", datum.getNotModified());    context.set("redirTemp", datum.getRedirTemp());    context.set("redirPerm", datum.getRedirPerm());    context.set("gone", datum.getGone());    context.set("conf", conf);        for (Map.Entry<Writable, Writable> entry : datum.getMetaData().entrySet()) {        Object value = entry.getValue();        if (value instanceof FloatWritable) {            FloatWritable fvalue = (FloatWritable) value;            Text tkey = (Text) entry.getKey();            context.set(tkey.toString(), fvalue.get());        }        if (value instanceof IntWritable) {            IntWritable ivalue = (IntWritable) value;            Text tkey = (Text) entry.getKey();            context.set(tkey.toString(), ivalue.get());        }        if (value instanceof Text) {            Text tvalue = (Text) value;            Text tkey = (Text) entry.getKey();            context.set(tkey.toString().replace("-", "_"), tvalue.toString());        }    }    return context;}
public void nutch_f120_0(Context context) throws IOException
{    conf = context.getConfiguration();    mos = new MultipleOutputs<FloatWritable, SelectorEntry>(context);    Job job = Job.getInstance(conf);    limit = conf.getLong(GENERATOR_TOP_N, Long.MAX_VALUE) / job.getNumReduceTasks();    maxNumSegments = conf.getInt(GENERATOR_MAX_NUM_SEGMENTS, 1);    segCounts = new int[maxNumSegments];    maxCount = conf.getInt(GENERATOR_MAX_COUNT, -1);    if (maxCount == -1) {        byDomain = false;    }    if (GENERATOR_COUNT_VALUE_DOMAIN.equals(conf.get(GENERATOR_COUNT_MODE)))        byDomain = true;    normalise = conf.getBoolean(GENERATOR_NORMALISE, true);    if (normalise)        normalizers = new URLNormalizers(conf, URLNormalizers.SCOPE_GENERATE_HOST_COUNT);    if (conf.get(GENERATOR_HOSTDB) != null) {        maxCountExpr = JexlUtil.parseExpression(conf.get(GENERATOR_MAX_COUNT_EXPR, null));        fetchDelayExpr = JexlUtil.parseExpression(conf.get(GENERATOR_FETCH_DELAY_EXPR, null));    }}
public void nutch_f121_0(Context context) throws IOException, InterruptedException
{    mos.close();}
public void nutch_f122_1(FloatWritable key, Iterable<SelectorEntry> values, Context context) throws IOException, InterruptedException
{    String hostname = null;    HostDatum host = null;        LongWritable variableFetchDelayWritable = null;    Text variableFetchDelayKey = new Text("_variableFetchDelay_");        int maxCount = this.maxCount;    for (SelectorEntry entry : values) {        Text url = entry.url;        String urlString = url.toString();        URL u = null;                if (host == null) {            try {                hostname = URLUtil.getHost(urlString);                host = getHostDatum(hostname);            } catch (Exception e) {            }                        if (host == null) {                                host = new HostDatum();            } else {                if (maxCountExpr != null) {                    long variableMaxCount = Math.round((double) maxCountExpr.evaluate(createContext(host)));                                        maxCount = (int) variableMaxCount;                }                if (fetchDelayExpr != null) {                    long variableFetchDelay = Math.round((double) fetchDelayExpr.evaluate(createContext(host)));                                        variableFetchDelayWritable = new LongWritable(variableFetchDelay);                }            }        }                if (variableFetchDelayWritable != null) {            entry.datum.getMetaData().put(variableFetchDelayKey, variableFetchDelayWritable);        }        if (count == limit) {                        if (currentsegmentnum < maxNumSegments) {                count = 0;                currentsegmentnum++;            } else                break;        }        String hostordomain = null;        try {            if (normalise && normalizers != null) {                urlString = normalizers.normalize(urlString, URLNormalizers.SCOPE_GENERATE_HOST_COUNT);            }            u = new URL(urlString);            if (byDomain) {                hostordomain = URLUtil.getDomainName(u);            } else {                hostordomain = u.getHost();            }        } catch (MalformedURLException e) {                        context.getCounter("Generator", "MALFORMED_URL").increment(1);            continue;        }        hostordomain = hostordomain.toLowerCase();                if (maxCount > 0) {            int[] hostCount = hostCounts.get(hostordomain);            if (hostCount == null) {                hostCount = new int[] { 1, 0 };                hostCounts.put(hostordomain, hostCount);            }                        hostCount[1]++;                        while (segCounts[hostCount[0] - 1] >= limit && hostCount[0] < maxNumSegments) {                hostCount[0]++;                hostCount[1] = 0;            }                        if (hostCount[1] > maxCount) {                if (hostCount[0] < maxNumSegments) {                    hostCount[0]++;                    hostCount[1] = 1;                } else {                    if (hostCount[1] == (maxCount + 1)) {                        context.getCounter("Generator", "HOSTS_AFFECTED_PER_HOST_OVERFLOW").increment(1);                                            }                                        context.getCounter("Generator", "URLS_SKIPPED_PER_HOST_OVERFLOW").increment(1);                    continue;                }            }            entry.segnum = new IntWritable(hostCount[0]);            segCounts[hostCount[0] - 1]++;        } else {            entry.segnum = new IntWritable(currentsegmentnum);            segCounts[currentsegmentnum - 1]++;        }        outputFile = generateFileName(entry);        mos.write("sequenceFiles", key, entry, outputFile);                        count++;    }}
private String nutch_f123_0(SelectorEntry entry)
{    return "fetchlist-" + entry.segnum.toString() + "/part";}
private HostDatum nutch_f124_0(String host) throws Exception
{    Text key = new Text();    HostDatum value = new HostDatum();    open();    for (int i = 0; i < hostdbReaders.length; i++) {        while (hostdbReaders[i].next(key, value)) {            if (host.equals(key.toString())) {                close();                return value;            }        }    }    close();    return null;}
public int nutch_f125_0(byte[] b1, int s1, int l1, byte[] b2, int s2, int l2)
{    return super.compare(b2, s2, l2, b1, s1, l1);}
public void nutch_f126_0(FloatWritable key, SelectorEntry value, Context context) throws IOException, InterruptedException
{    SelectorEntry entry = value;    context.write(entry.url, entry);}
public void nutch_f127_0(Text key, Iterable<SelectorEntry> values, Context context) throws IOException, InterruptedException
{        for (SelectorEntry entry : values) {        context.write(entry.url, entry.datum);    }}
public int nutch_f128_0(WritableComparable a, WritableComparable b)
{    Text url1 = (Text) a;    Text url2 = (Text) b;    int hash1 = hash(url1.getBytes(), 0, url1.getLength());    int hash2 = hash(url2.getBytes(), 0, url2.getLength());    return (hash1 < hash2 ? -1 : (hash1 == hash2 ? 0 : 1));}
public int nutch_f129_0(byte[] b1, int s1, int l1, byte[] b2, int s2, int l2)
{    int hash1 = hash(b1, s1, l1);    int hash2 = hash(b2, s2, l2);    return (hash1 < hash2 ? -1 : (hash1 == hash2 ? 0 : 1));}
private static int nutch_f130_0(byte[] bytes, int start, int length)
{    int hash = 1;        for (int i = length - 1; i >= 0; i--) hash = (31 * hash) + (int) bytes[start + i];    return hash;}
public void nutch_f131_0(Text key, CrawlDatum value, Context context) throws IOException, InterruptedException
{    context.write(key, value);}
public void nutch_f132_0(Reducer<Text, CrawlDatum, Text, CrawlDatum>.Context context)
{    Configuration conf = context.getConfiguration();    generateTime = conf.getLong(Nutch.GENERATE_TIME_KEY, 0L);}
public void nutch_f133_0(Text key, Iterable<CrawlDatum> values, Context context) throws IOException, InterruptedException
{    genTime.set(0L);    for (CrawlDatum val : values) {        if (val.getMetaData().containsKey(Nutch.WRITABLE_GENERATE_TIME_KEY)) {            LongWritable gt = (LongWritable) val.getMetaData().get(Nutch.WRITABLE_GENERATE_TIME_KEY);            genTime.set(gt.get());            if (genTime.get() != generateTime) {                orig.set(val);                genTime.set(0L);                continue;            }        } else {            orig.set(val);        }    }    if (genTime.get() != 0L) {        orig.getMetaData().put(Nutch.WRITABLE_GENERATE_TIME_KEY, genTime);    }    context.write(key, orig);}
public Path[] nutch_f134_0(Path dbDir, Path segments, int numLists, long topN, long curTime) throws IOException, InterruptedException, ClassNotFoundException
{    Job job = NutchJob.getInstance(getConf());    Configuration conf = job.getConfiguration();    boolean filter = conf.getBoolean(GENERATOR_FILTER, true);    boolean normalise = conf.getBoolean(GENERATOR_NORMALISE, true);    return generate(dbDir, segments, numLists, topN, curTime, filter, normalise, false, 1, null);}
public Path[] nutch_f135_0(Path dbDir, Path segments, int numLists, long topN, long curTime, boolean filter, boolean force) throws IOException, InterruptedException, ClassNotFoundException
{    return generate(dbDir, segments, numLists, topN, curTime, filter, true, force, 1, null);}
public Path[] nutch_f136_0(Path dbDir, Path segments, int numLists, long topN, long curTime, boolean filter, boolean norm, boolean force, int maxNumSegments, String expr) throws IOException, InterruptedException, ClassNotFoundException
{    return generate(dbDir, segments, numLists, topN, curTime, filter, true, force, 1, expr, null);}
public Path[] nutch_f137_1(Path dbDir, Path segments, int numLists, long topN, long curTime, boolean filter, boolean norm, boolean force, int maxNumSegments, String expr, String hostdb) throws IOException, InterruptedException, ClassNotFoundException
{    Path tempDir = new Path(getConf().get("mapreduce.cluster.temp.dir", ".") + "/generate-temp-" + java.util.UUID.randomUUID().toString());    FileSystem fs = tempDir.getFileSystem(getConf());    Path lock = CrawlDb.lock(getConf(), dbDir, force);    SimpleDateFormat sdf = new SimpleDateFormat("yyyy-MM-dd HH:mm:ss");    long start = System.currentTimeMillis();                    if (topN != Long.MAX_VALUE) {            }    if (expr != null) {            }    if (hostdb != null) {            }        Job job = NutchJob.getInstance(getConf());    job.setJobName("generate: select from " + dbDir);    Configuration conf = job.getConfiguration();    if (numLists == -1) {        /* for politeness create exactly one partition per fetch task */        numLists = Integer.parseInt(conf.get("mapreduce.job.maps"));    }    if ("local".equals(conf.get("mapreduce.framework.name")) && numLists != 1) {                        numLists = 1;    }    conf.setLong(GENERATOR_CUR_TIME, curTime);        long generateTime = System.currentTimeMillis();    conf.setLong(Nutch.GENERATE_TIME_KEY, generateTime);    conf.setLong(GENERATOR_TOP_N, topN);    conf.setBoolean(GENERATOR_FILTER, filter);    conf.setBoolean(GENERATOR_NORMALISE, norm);    conf.setInt(GENERATOR_MAX_NUM_SEGMENTS, maxNumSegments);    if (expr != null) {        conf.set(GENERATOR_EXPR, expr);    }    if (hostdb != null) {        conf.set(GENERATOR_HOSTDB, hostdb);    }    FileInputFormat.addInputPath(job, new Path(dbDir, CrawlDb.CURRENT_NAME));    job.setInputFormatClass(SequenceFileInputFormat.class);    job.setJarByClass(Selector.class);    job.setMapperClass(SelectorMapper.class);    job.setPartitionerClass(Selector.class);    job.setReducerClass(SelectorReducer.class);    FileOutputFormat.setOutputPath(job, tempDir);    job.setOutputKeyClass(FloatWritable.class);    job.setSortComparatorClass(DecreasingFloatComparator.class);    job.setOutputValueClass(SelectorEntry.class);    MultipleOutputs.addNamedOutput(job, "sequenceFiles", SequenceFileOutputFormat.class, FloatWritable.class, SelectorEntry.class);    try {        boolean success = job.waitForCompletion(true);        if (!success) {            String message = "Generator job did not succeed, job status:" + job.getStatus().getState() + ", reason: " + job.getStatus().getFailureInfo();                        NutchJob.cleanupAfterFailure(tempDir, lock, fs);            throw new RuntimeException(message);        }    } catch (IOException | InterruptedException | ClassNotFoundException e) {                NutchJob.cleanupAfterFailure(tempDir, lock, fs);        throw e;    }        for (Counter counter : job.getCounters().getGroup("Generator")) {            }            List<Path> generatedSegments = new ArrayList<>();    FileStatus[] status = fs.listStatus(tempDir);    try {        for (FileStatus stat : status) {            Path subfetchlist = stat.getPath();            if (!subfetchlist.getName().startsWith("fetchlist-"))                continue;                        Path newSeg = partitionSegment(segments, subfetchlist, numLists);            generatedSegments.add(newSeg);        }    } catch (Exception e) {                LockUtil.removeLockFile(getConf(), lock);        fs.delete(tempDir, true);        return null;    }    if (generatedSegments.size() == 0) {                LockUtil.removeLockFile(getConf(), lock);        fs.delete(tempDir, true);        return null;    }    if (getConf().getBoolean(GENERATE_UPDATE_CRAWLDB, false)) {                Path tempDir2 = new Path(dbDir, "generate-temp-" + java.util.UUID.randomUUID().toString());        job = NutchJob.getInstance(getConf());        job.setJobName("generate: updatedb " + dbDir);        job.getConfiguration().setLong(Nutch.GENERATE_TIME_KEY, generateTime);        for (Path segmpaths : generatedSegments) {            Path subGenDir = new Path(segmpaths, CrawlDatum.GENERATE_DIR_NAME);            FileInputFormat.addInputPath(job, subGenDir);        }        FileInputFormat.addInputPath(job, new Path(dbDir, CrawlDb.CURRENT_NAME));        job.setInputFormatClass(SequenceFileInputFormat.class);        job.setMapperClass(CrawlDbUpdater.CrawlDbUpdateMapper.class);        job.setReducerClass(CrawlDbUpdater.CrawlDbUpdateReducer.class);        job.setJarByClass(CrawlDbUpdater.class);        job.setOutputFormatClass(MapFileOutputFormat.class);        job.setOutputKeyClass(Text.class);        job.setOutputValueClass(CrawlDatum.class);        FileOutputFormat.setOutputPath(job, tempDir2);        try {            boolean success = job.waitForCompletion(true);            if (!success) {                String message = "Generator job did not succeed, job status:" + job.getStatus().getState() + ", reason: " + job.getStatus().getFailureInfo();                                NutchJob.cleanupAfterFailure(tempDir, lock, fs);                NutchJob.cleanupAfterFailure(tempDir2, lock, fs);                throw new RuntimeException(message);            }            CrawlDb.install(job, dbDir);        } catch (IOException | InterruptedException | ClassNotFoundException e) {                        NutchJob.cleanupAfterFailure(tempDir, lock, fs);            NutchJob.cleanupAfterFailure(tempDir2, lock, fs);            throw e;        }        fs.delete(tempDir2, true);    }    LockUtil.removeLockFile(getConf(), lock);    fs.delete(tempDir, true);    long end = System.currentTimeMillis();        Path[] patharray = new Path[generatedSegments.size()];    return generatedSegments.toArray(patharray);}
private Path nutch_f138_1(Path segmentsDir, Path inputDir, int numLists) throws IOException, ClassNotFoundException, InterruptedException
{        if (LOG.isInfoEnabled()) {            }    Path segment = new Path(segmentsDir, generateSegmentName());    Path output = new Path(segment, CrawlDatum.GENERATE_DIR_NAME);        Job job = NutchJob.getInstance(getConf());    job.setJobName("generate: partition " + segment);    Configuration conf = job.getConfiguration();    conf.setInt("partition.url.seed", new Random().nextInt());    FileInputFormat.addInputPath(job, inputDir);    job.setInputFormatClass(SequenceFileInputFormat.class);    job.setJarByClass(Generator.class);    job.setMapperClass(SelectorInverseMapper.class);    job.setMapOutputKeyClass(Text.class);    job.setMapOutputValueClass(SelectorEntry.class);    job.setPartitionerClass(URLPartitioner.class);    job.setReducerClass(PartitionReducer.class);    job.setNumReduceTasks(numLists);    FileOutputFormat.setOutputPath(job, output);    job.setOutputFormatClass(SequenceFileOutputFormat.class);    job.setOutputKeyClass(Text.class);    job.setOutputValueClass(CrawlDatum.class);    job.setSortComparatorClass(HashComparator.class);    try {        boolean success = job.waitForCompletion(true);        if (!success) {            String message = "Generator job did not succeed, job status:" + job.getStatus().getState() + ", reason: " + job.getStatus().getFailureInfo();                        throw new RuntimeException(message);        }    } catch (IOException | InterruptedException | ClassNotFoundException e) {                throw e;    }    return segment;}
public static synchronized String nutch_f139_0()
{    try {        Thread.sleep(1000);    } catch (Throwable t) {    }    ;    return sdf.format(new Date(System.currentTimeMillis()));}
public static void nutch_f140_0(String[] args) throws Exception
{    int res = ToolRunner.run(NutchConfiguration.create(), new Generator(), args);    System.exit(res);}
public int nutch_f141_1(String[] args) throws Exception
{    if (args.length < 2) {        System.out.println("Usage: Generator <crawldb> <segments_dir> [-force] [-topN N] [-numFetchers numFetchers] [-expr <expr>] [-adddays <numDays>] [-noFilter] [-noNorm] [-maxNumSegments <num>]");        return -1;    }    Path dbDir = new Path(args[0]);    Path segmentsDir = new Path(args[1]);    String hostdb = null;    long curTime = System.currentTimeMillis();    long topN = Long.MAX_VALUE;    int numFetchers = -1;    boolean filter = true;    boolean norm = true;    boolean force = false;    String expr = null;    int maxNumSegments = 1;    for (int i = 2; i < args.length; i++) {        if ("-topN".equals(args[i])) {            topN = Long.parseLong(args[i + 1]);            i++;        } else if ("-numFetchers".equals(args[i])) {            numFetchers = Integer.parseInt(args[i + 1]);            i++;        } else if ("-hostdb".equals(args[i])) {            hostdb = args[i + 1];            i++;        } else if ("-adddays".equals(args[i])) {            long numDays = Integer.parseInt(args[i + 1]);            curTime += numDays * 1000L * 60 * 60 * 24;        } else if ("-noFilter".equals(args[i])) {            filter = false;        } else if ("-noNorm".equals(args[i])) {            norm = false;        } else if ("-force".equals(args[i])) {            force = true;        } else if ("-maxNumSegments".equals(args[i])) {            maxNumSegments = Integer.parseInt(args[i + 1]);        } else if ("-expr".equals(args[i])) {            expr = args[i + 1];        }    }    try {        Path[] segs = generate(dbDir, segmentsDir, numFetchers, topN, curTime, filter, norm, force, maxNumSegments, expr, hostdb);        if (segs == null)            return 1;    } catch (Exception e) {                return -1;    }    return 0;}
public Map<String, Object> nutch_f142_1(Map<String, Object> args, String crawlId) throws Exception
{    Map<String, Object> results = new HashMap<>();    long curTime = System.currentTimeMillis();    long topN = Long.MAX_VALUE;    int numFetchers = -1;    boolean filter = true;    boolean norm = true;    boolean force = false;    int maxNumSegments = 1;    String expr = null;    String hostdb = null;    Path crawlDb;    if (args.containsKey(Nutch.ARG_CRAWLDB)) {        Object crawldbPath = args.get(Nutch.ARG_CRAWLDB);        if (crawldbPath instanceof Path) {            crawlDb = (Path) crawldbPath;        } else {            crawlDb = new Path(crawldbPath.toString());        }    } else {        crawlDb = new Path(crawlId + "/crawldb");    }    Path segmentsDir;    if (args.containsKey(Nutch.ARG_SEGMENTDIR)) {        Object segDir = args.get(Nutch.ARG_SEGMENTDIR);        if (segDir instanceof Path) {            segmentsDir = (Path) segDir;        } else {            segmentsDir = new Path(segDir.toString());        }    } else {        segmentsDir = new Path(crawlId + "/segments");    }    if (args.containsKey(Nutch.ARG_HOSTDB)) {        hostdb = (String) args.get(Nutch.ARG_HOSTDB);    }    if (args.containsKey("expr")) {        expr = (String) args.get("expr");    }    if (args.containsKey("topN")) {        topN = Long.parseLong((String) args.get("topN"));    }    if (args.containsKey("numFetchers")) {        numFetchers = Integer.parseInt((String) args.get("numFetchers"));    }    if (args.containsKey("adddays")) {        long numDays = Integer.parseInt((String) args.get("adddays"));        curTime += numDays * 1000L * 60 * 60 * 24;    }    if (args.containsKey("noFilter")) {        filter = false;    }    if (args.containsKey("noNorm")) {        norm = false;    }    if (args.containsKey("force")) {        force = true;    }    if (args.containsKey("maxNumSegments")) {        maxNumSegments = Integer.parseInt((String) args.get("maxNumSegments"));    }    try {        Path[] segs = generate(crawlDb, segmentsDir, numFetchers, topN, curTime, filter, norm, force, maxNumSegments, expr, hostdb);        if (segs == null) {            results.put(Nutch.VAL_RESULT, Integer.toString(1));            return results;        }    } catch (Exception e) {                results.put(Nutch.VAL_RESULT, Integer.toString(-1));        return results;    }    results.put(Nutch.VAL_RESULT, Integer.toString(0));    return results;}
public void nutch_f143_0(Context context)
{    Configuration conf = context.getConfiguration();    boolean normalize = conf.getBoolean(CrawlDbFilter.URL_NORMALIZING, true);    boolean filter = conf.getBoolean(CrawlDbFilter.URL_FILTERING, true);    filterNormalizeAll = conf.getBoolean(URL_FILTER_NORMALIZE_ALL, false);    if (normalize) {        scope = conf.get(URL_NORMALIZING_SCOPE, URLNormalizers.SCOPE_INJECT);        urlNormalizers = new URLNormalizers(conf, scope);    }    interval = conf.getInt("db.fetch.interval.default", 2592000);    if (filter) {        filters = new URLFilters(conf);    }    scfilters = new ScoringFilters(conf);    scoreInjected = conf.getFloat("db.score.injected", 1.0f);    curTime = conf.getLong("injector.current.time", System.currentTimeMillis());    url404Purging = conf.getBoolean(CrawlDb.CRAWLDB_PURGE_404, false);}
private String nutch_f144_1(String url)
{    if (url != null) {        try {            if (urlNormalizers != null)                                url = urlNormalizers.normalize(url, scope);            if (filters != null)                                url = filters.filter(url);        } catch (Exception e) {                        url = null;        }    }    return url;}
private void nutch_f145_1(String metadata, CrawlDatum datum, String url)
{    String[] splits = metadata.split(TAB_CHARACTER);    for (String split : splits) {                int indexEquals = split.indexOf(EQUAL_CHARACTER);        if (        indexEquals == -1)            continue;        String metaname = split.substring(0, indexEquals);        String metavalue = split.substring(indexEquals + 1);        try {            if (metaname.equals(nutchScoreMDName)) {                datum.setScore(Float.parseFloat(metavalue));            } else if (metaname.equals(nutchFetchIntervalMDName)) {                datum.setFetchInterval(Integer.parseInt(metavalue));            } else if (metaname.equals(nutchFixedFetchIntervalMDName)) {                int fixedInterval = Integer.parseInt(metavalue);                if (fixedInterval > -1) {                                                            datum.getMetaData().put(Nutch.WRITABLE_FIXED_INTERVAL_KEY, new FloatWritable(fixedInterval));                    datum.setFetchInterval(fixedInterval);                }            } else {                datum.getMetaData().put(new Text(metaname), new Text(metavalue));            }        } catch (NumberFormatException nfe) {                    }    }}
public void nutch_f146_1(Text key, Writable value, Context context) throws IOException, InterruptedException
{    if (value instanceof Text) {                String url = key.toString().trim();                if (url.length() == 0 || url.startsWith("#"))            return;        url = filterNormalize(url);        if (url == null) {            context.getCounter("injector", "urls_filtered").increment(1);        } else {            CrawlDatum datum = new CrawlDatum();            datum.setStatus(CrawlDatum.STATUS_INJECTED);            datum.setFetchTime(curTime);            datum.setScore(scoreInjected);            datum.setFetchInterval(interval);            String metadata = value.toString().trim();            if (metadata.length() > 0)                processMetaData(metadata, datum, url);            try {                key.set(url);                scfilters.injectedScore(key, datum);            } catch (ScoringFilterException e) {                if (LOG.isWarnEnabled()) {                                    }            }            context.getCounter("injector", "urls_injected").increment(1);            context.write(key, datum);        }    } else if (value instanceof CrawlDatum) {                        CrawlDatum datum = (CrawlDatum) value;                if (url404Purging && CrawlDatum.STATUS_DB_GONE == datum.getStatus()) {            context.getCounter("injector", "urls_purged_404").increment(1);            return;        }        if (filterNormalizeAll) {            String url = filterNormalize(key.toString());            if (url == null) {                context.getCounter("injector", "urls_purged_filter").increment(1);            } else {                key.set(url);                context.write(key, datum);            }        } else {            context.write(key, datum);        }    }}
public void nutch_f147_1(Context context)
{    Configuration conf = context.getConfiguration();    interval = conf.getInt("db.fetch.interval.default", 2592000);    scoreInjected = conf.getFloat("db.score.injected", 1.0f);    overwrite = conf.getBoolean("db.injector.overwrite", false);    update = conf.getBoolean("db.injector.update", false);        }
public void nutch_f148_0(Text key, Iterable<CrawlDatum> values, Context context) throws IOException, InterruptedException
{    boolean oldSet = false;    boolean injectedSet = false;        for (CrawlDatum val : values) {        if (val.getStatus() == CrawlDatum.STATUS_INJECTED) {            injected.set(val);            injected.setStatus(CrawlDatum.STATUS_DB_UNFETCHED);            injectedSet = true;        } else {            old.set(val);            oldSet = true;        }    }    CrawlDatum result;    if (injectedSet && (!oldSet || overwrite)) {                result = injected;    } else {                result = old;        if (injectedSet && update) {                        old.putAllMetaData(injected);            old.setScore(injected.getScore() != scoreInjected ? injected.getScore() : old.getScore());            old.setFetchInterval(injected.getFetchInterval() != interval ? injected.getFetchInterval() : old.getFetchInterval());        }    }    if (injectedSet && oldSet) {        context.getCounter("injector", "urls_merged").increment(1);    }    context.write(key, result);}
public void nutch_f149_0(Path crawlDb, Path urlDir) throws IOException, ClassNotFoundException, InterruptedException
{    inject(crawlDb, urlDir, false, false);}
public void nutch_f150_0(Path crawlDb, Path urlDir, boolean overwrite, boolean update) throws IOException, ClassNotFoundException, InterruptedException
{    inject(crawlDb, urlDir, overwrite, update, true, true, false);}
public void nutch_f151_1(Path crawlDb, Path urlDir, boolean overwrite, boolean update, boolean normalize, boolean filter, boolean filterNormalizeAll) throws IOException, ClassNotFoundException, InterruptedException
{    SimpleDateFormat sdf = new SimpleDateFormat("yyyy-MM-dd HH:mm:ss");    long start = System.currentTimeMillis();    if (LOG.isInfoEnabled()) {                                    }        Configuration conf = getConf();    conf.setLong("injector.current.time", System.currentTimeMillis());    conf.setBoolean("db.injector.overwrite", overwrite);    conf.setBoolean("db.injector.update", update);    conf.setBoolean(CrawlDbFilter.URL_NORMALIZING, normalize);    conf.setBoolean(CrawlDbFilter.URL_FILTERING, filter);    conf.setBoolean(URL_FILTER_NORMALIZE_ALL, filterNormalizeAll);    conf.setBoolean("mapreduce.fileoutputcommitter.marksuccessfuljobs", false);        FileSystem fs = crawlDb.getFileSystem(conf);    Path current = new Path(crawlDb, CrawlDb.CURRENT_NAME);    if (!fs.exists(current))        fs.mkdirs(current);    Path tempCrawlDb = new Path(crawlDb, "crawldb-" + Integer.toString(new Random().nextInt(Integer.MAX_VALUE)));        Path lock = CrawlDb.lock(conf, crawlDb, false);        Job job = Job.getInstance(conf, "inject " + urlDir);    job.setJarByClass(Injector.class);    job.setMapperClass(InjectMapper.class);    job.setReducerClass(InjectReducer.class);    job.setOutputFormatClass(MapFileOutputFormat.class);    job.setOutputKeyClass(Text.class);    job.setOutputValueClass(CrawlDatum.class);    job.setSpeculativeExecution(false);        MultipleInputs.addInputPath(job, current, SequenceFileInputFormat.class);    FileStatus[] seedFiles = urlDir.getFileSystem(getConf()).listStatus(urlDir);    int numSeedFiles = 0;    for (FileStatus seedFile : seedFiles) {        if (seedFile.isFile()) {            MultipleInputs.addInputPath(job, seedFile.getPath(), KeyValueTextInputFormat.class);            numSeedFiles++;                    } else {                    }    }    if (numSeedFiles == 0) {                LockUtil.removeLockFile(fs, lock);        return;    }    FileOutputFormat.setOutputPath(job, tempCrawlDb);    try {                boolean success = job.waitForCompletion(true);        if (!success) {            String message = "Injector job did not succeed, job status: " + job.getStatus().getState() + ", reason: " + job.getStatus().getFailureInfo();                        NutchJob.cleanupAfterFailure(tempCrawlDb, lock, fs);                        throw new RuntimeException(message);        }                CrawlDb.install(job, crawlDb);        if (LOG.isInfoEnabled()) {            long urlsInjected = job.getCounters().findCounter("injector", "urls_injected").getValue();            long urlsFiltered = job.getCounters().findCounter("injector", "urls_filtered").getValue();            long urlsMerged = job.getCounters().findCounter("injector", "urls_merged").getValue();            long urlsPurged404 = job.getCounters().findCounter("injector", "urls_purged_404").getValue();            long urlsPurgedFilter = job.getCounters().findCounter("injector", "urls_purged_filter").getValue();                                                            if (filterNormalizeAll) {                            }            if (conf.getBoolean(CrawlDb.CRAWLDB_PURGE_404, false)) {                            }            long end = System.currentTimeMillis();                    }    } catch (IOException | InterruptedException | ClassNotFoundException | NullPointerException e) {                NutchJob.cleanupAfterFailure(tempCrawlDb, lock, fs);        throw e;    }}
public void nutch_f152_0()
{    System.err.println("Usage: Injector [-D...] <crawldb> <url_dir> [-overwrite|-update] [-noFilter] [-noNormalize] [-filterNormalizeAll]\n");    System.err.println("  <crawldb>\tPath to a crawldb directory. If not present, a new one would be created.");    System.err.println("  <url_dir>\tPath to URL file or directory with URL file(s) containing URLs to be injected.");    System.err.println("           \tA URL file should have one URL per line, optionally followed by custom metadata.");    System.err.println("           \tBlank lines or lines starting with a '#' would be ignored. Custom metadata must");    System.err.println("           \tbe of form 'key=value' and separated by tabs.");    System.err.println("           \tBelow are reserved metadata keys:\n");    System.err.println("           \t\tnutch.score: A custom score for a url");    System.err.println("           \t\tnutch.fetchInterval: A custom fetch interval for a url");    System.err.println("           \t\tnutch.fetchInterval.fixed: A custom fetch interval for a url that is not " + "changed by AdaptiveFetchSchedule\n");    System.err.println("           \tExample:");    System.err.println("           \t http://www.apache.org/");    System.err.println("           \t http://www.nutch.org/ \\t nutch.score=10 \\t nutch.fetchInterval=2592000 \\t userType=open_source\n");    System.err.println(" -overwrite\tOverwite existing crawldb records by the injected records. Has precedence over 'update'");    System.err.println(" -update   \tUpdate existing crawldb records with the injected records. Old metadata is preserved");    System.err.println();    System.err.println(" -nonormalize\tDo not normalize URLs before injecting");    System.err.println(" -nofilter \tDo not apply URL filters to injected URLs");    System.err.println(" -filterNormalizeAll\n" + "           \tNormalize and filter all URLs including the URLs of existing CrawlDb records");    System.err.println();    System.err.println(" -D...     \tset or overwrite configuration property (property=value)");    System.err.println(" -Ddb.update.purge.404=true\n" + "           \tremove URLs with status gone (404) from CrawlDb");}
public static void nutch_f153_0(String[] args) throws Exception
{    int res = ToolRunner.run(NutchConfiguration.create(), new Injector(), args);    System.exit(res);}
public int nutch_f154_1(String[] args) throws Exception
{    if (args.length < 2) {        usage();        return -1;    }    boolean overwrite = false;    boolean update = false;    boolean normalize = true;    boolean filter = true;    boolean filterNormalizeAll = false;    for (int i = 2; i < args.length; i++) {        if (args[i].equals("-overwrite")) {            overwrite = true;        } else if (args[i].equals("-update")) {            update = true;        } else if (args[i].equals("-noNormalize")) {            normalize = false;        } else if (args[i].equals("-noFilter")) {            filter = false;        } else if (args[i].equals("-filterNormalizeAll")) {            filterNormalizeAll = true;        } else {                        usage();            return -1;        }    }    try {        inject(new Path(args[0]), new Path(args[1]), overwrite, update, normalize, filter, filterNormalizeAll);        return 0;    } catch (Exception e) {                return -1;    }}
public Map<String, Object> nutch_f155_0(Map<String, Object> args, String crawlId) throws Exception
{    if (args.size() < 1) {        throw new IllegalArgumentException("Required arguments <url_dir> or <seedName>");    }    Path input;    Object path = null;    if (args.containsKey(Nutch.ARG_SEEDDIR)) {        path = args.get(Nutch.ARG_SEEDDIR);    } else if (args.containsKey(Nutch.ARG_SEEDNAME)) {        path = NutchServer.getInstance().getSeedManager().getSeedList((String) args.get(Nutch.ARG_SEEDNAME)).getSeedFilePath();    } else {        throw new IllegalArgumentException("Required arguments <url_dir> or <seedName>");    }    if (path instanceof Path) {        input = (Path) path;    } else {        input = new Path(path.toString());    }    Map<String, Object> results = new HashMap<>();    Path crawlDb;    if (args.containsKey(Nutch.ARG_CRAWLDB)) {        Object crawldbPath = args.get(Nutch.ARG_CRAWLDB);        if (crawldbPath instanceof Path) {            crawlDb = (Path) crawldbPath;        } else {            crawlDb = new Path(crawldbPath.toString());        }    } else {        crawlDb = new Path(crawlId + "/crawldb");    }    inject(crawlDb, input);    results.put(Nutch.VAL_RESULT, Integer.toString(0));    return results;}
public void nutch_f156_0(DataInput in) throws IOException
{    fromUrl = Text.readString(in);    anchor = Text.readString(in);}
public static void nutch_f157_0(DataInput in) throws IOException
{        Text.skip(in);        Text.skip(in);}
public void nutch_f158_0(DataOutput out) throws IOException
{    Text.writeString(out, fromUrl);    Text.writeString(out, anchor);}
public static Inlink nutch_f159_0(DataInput in) throws IOException
{    Inlink inlink = new Inlink();    inlink.readFields(in);    return inlink;}
public String nutch_f160_0()
{    return fromUrl;}
public String nutch_f161_0()
{    return anchor;}
public boolean nutch_f162_0(Object o)
{    if (!(o instanceof Inlink))        return false;    Inlink other = (Inlink) o;    return this.fromUrl.equals(other.fromUrl) && this.anchor.equals(other.anchor);}
public int nutch_f163_0()
{    return fromUrl.hashCode() ^ anchor.hashCode();}
public String nutch_f164_0()
{    return "fromUrl: " + fromUrl + " anchor: " + anchor;}
public void nutch_f165_0(Inlink inlink)
{    inlinks.add(inlink);}
public void nutch_f166_0(Inlinks inlinks)
{    this.inlinks.addAll(inlinks.inlinks);}
public Iterator<Inlink> nutch_f167_0()
{    return this.inlinks.iterator();}
public int nutch_f168_0()
{    return inlinks.size();}
public void nutch_f169_0()
{    inlinks.clear();}
public void nutch_f170_0(DataInput in) throws IOException
{    int length = in.readInt();    inlinks.clear();    for (int i = 0; i < length; i++) {        add(Inlink.read(in));    }}
public void nutch_f171_0(DataOutput out) throws IOException
{    out.writeInt(inlinks.size());    Iterator<Inlink> it = inlinks.iterator();    while (it.hasNext()) {        it.next().write(out);    }}
public String nutch_f172_0()
{    StringBuilder buffer = new StringBuilder();    buffer.append("Inlinks:\n");    Iterator<Inlink> it = inlinks.iterator();    while (it.hasNext()) {        buffer.append(" ");        buffer.append(it.next());        buffer.append("\n");    }    return buffer.toString();}
public String[] nutch_f173_0()
{    HashMap<String, Set<String>> domainToAnchors = new HashMap<>();    ArrayList<String> results = new ArrayList<>();    Iterator<Inlink> it = inlinks.iterator();    while (it.hasNext()) {        Inlink inlink = it.next();        String anchor = inlink.getAnchor();        if (        anchor.length() == 0)            continue;                String domain = null;        try {            domain = new URL(inlink.getFromUrl()).getHost();        } catch (MalformedURLException e) {        }        Set<String> domainAnchors = domainToAnchors.get(domain);        if (domainAnchors == null) {            domainAnchors = new HashSet<>();            domainToAnchors.put(domain, domainAnchors);        }        if (domainAnchors.add(anchor)) {                                    results.add(anchor);        }    }    return results.toArray(new String[results.size()]);}
public void nutch_f174_0(Mapper<Text, ParseData, Text, Inlinks>.Context context)
{    Configuration conf = context.getConfiguration();    maxAnchorLength = conf.getInt("linkdb.max.anchor.length", 100);    ignoreInternalLinks = conf.getBoolean(IGNORE_INTERNAL_LINKS, true);    ignoreExternalLinks = conf.getBoolean(IGNORE_EXTERNAL_LINKS, false);    if (conf.getBoolean(LinkDbFilter.URL_FILTERING, false)) {        urlFilters = new URLFilters(conf);    }    if (conf.getBoolean(LinkDbFilter.URL_NORMALIZING, false)) {        urlNormalizers = new URLNormalizers(conf, URLNormalizers.SCOPE_LINKDB);    }}
public void nutch_f175_1(Text key, ParseData parseData, Context context) throws IOException, InterruptedException
{    String fromUrl = key.toString();    String fromHost = getHost(fromUrl);    if (urlNormalizers != null) {        try {            fromUrl = urlNormalizers.normalize(fromUrl,             URLNormalizers.SCOPE_LINKDB);        } catch (Exception e) {                        fromUrl = null;        }    }    if (fromUrl != null && urlFilters != null) {        try {                        fromUrl = urlFilters.filter(fromUrl);        } catch (Exception e) {                        fromUrl = null;        }    }    if (fromUrl == null)                return;    Outlink[] outlinks = parseData.getOutlinks();    Inlinks inlinks = new Inlinks();    for (int i = 0; i < outlinks.length; i++) {        Outlink outlink = outlinks[i];        String toUrl = outlink.getToUrl();        if (ignoreInternalLinks) {            String toHost = getHost(toUrl);            if (toHost == null || toHost.equals(fromHost)) {                                continue;            }        } else if (ignoreExternalLinks) {            String toHost = getHost(toUrl);            if (toHost == null || !toHost.equals(fromHost)) {                                continue;            }        }        if (urlNormalizers != null) {            try {                                toUrl = urlNormalizers.normalize(toUrl, URLNormalizers.SCOPE_LINKDB);            } catch (Exception e) {                                toUrl = null;            }        }        if (toUrl != null && urlFilters != null) {            try {                                toUrl = urlFilters.filter(toUrl);            } catch (Exception e) {                                toUrl = null;            }        }        if (toUrl == null)            continue;        inlinks.clear();                String anchor = outlink.getAnchor();        if (anchor.length() > maxAnchorLength) {            anchor = anchor.substring(0, maxAnchorLength);        }                inlinks.add(new Inlink(fromUrl, anchor));        context.write(new Text(toUrl), inlinks);    }}
private static String nutch_f176_0(String url)
{    try {        return new URL(url).getHost().toLowerCase();    } catch (MalformedURLException e) {        return null;    }}
public void nutch_f177_0(Path linkDb, final Path segmentsDir, boolean normalize, boolean filter, boolean force) throws IOException, InterruptedException, ClassNotFoundException
{    FileSystem fs = segmentsDir.getFileSystem(getConf());    FileStatus[] files = fs.listStatus(segmentsDir, HadoopFSUtil.getPassDirectoriesFilter(fs));    invert(linkDb, HadoopFSUtil.getPaths(files), normalize, filter, force);}
public void nutch_f178_1(Path linkDb, Path[] segments, boolean normalize, boolean filter, boolean force) throws IOException, InterruptedException, ClassNotFoundException
{    Job job = LinkDb.createJob(getConf(), linkDb, normalize, filter);    Path lock = new Path(linkDb, LOCK_NAME);    FileSystem fs = linkDb.getFileSystem(getConf());    LockUtil.createLockFile(fs, lock, force);    Path currentLinkDb = new Path(linkDb, CURRENT_NAME);    Configuration conf = job.getConfiguration();    SimpleDateFormat sdf = new SimpleDateFormat("yyyy-MM-dd HH:mm:ss");    long start = System.currentTimeMillis();    if (LOG.isInfoEnabled()) {                                        if (conf.getBoolean(IGNORE_INTERNAL_LINKS, true)) {                    }        if (conf.getBoolean(IGNORE_EXTERNAL_LINKS, false)) {                    }    }    if (conf.getBoolean(IGNORE_INTERNAL_LINKS, true) && conf.getBoolean(IGNORE_EXTERNAL_LINKS, false)) {                LockUtil.removeLockFile(fs, lock);        return;    }    for (int i = 0; i < segments.length; i++) {        if (LOG.isInfoEnabled()) {                    }        FileInputFormat.addInputPath(job, new Path(segments[i], ParseData.DIR_NAME));    }    try {        boolean success = job.waitForCompletion(true);        if (!success) {            String message = "LinkDb job did not succeed, job status:" + job.getStatus().getState() + ", reason: " + job.getStatus().getFailureInfo();                        LockUtil.removeLockFile(fs, lock);            throw new RuntimeException(message);        }    } catch (IOException | InterruptedException | ClassNotFoundException e) {                LockUtil.removeLockFile(fs, lock);        throw e;    }    if (fs.exists(currentLinkDb)) {        if (LOG.isInfoEnabled()) {                    }                Path newLinkDb = FileOutputFormat.getOutputPath(job);        job = LinkDbMerger.createMergeJob(getConf(), linkDb, normalize, filter);        FileInputFormat.addInputPath(job, currentLinkDb);        FileInputFormat.addInputPath(job, newLinkDb);        try {            boolean success = job.waitForCompletion(true);            if (!success) {                String message = "LinkDb job did not succeed, job status:" + job.getStatus().getState() + ", reason: " + job.getStatus().getFailureInfo();                                NutchJob.cleanupAfterFailure(newLinkDb, lock, fs);                throw new RuntimeException(message);            }        } catch (IOException | InterruptedException | ClassNotFoundException e) {                        NutchJob.cleanupAfterFailure(newLinkDb, lock, fs);            throw e;        }        fs.delete(newLinkDb, true);    }    LinkDb.install(job, linkDb);    long end = System.currentTimeMillis();    }
private static Job nutch_f179_1(Configuration config, Path linkDb, boolean normalize, boolean filter) throws IOException
{    Path newLinkDb = new Path(linkDb, Integer.toString(new Random().nextInt(Integer.MAX_VALUE)));    Job job = NutchJob.getInstance(config);    Configuration conf = job.getConfiguration();    job.setJobName("linkdb " + linkDb);    job.setInputFormatClass(SequenceFileInputFormat.class);    job.setJarByClass(LinkDb.class);    job.setMapperClass(LinkDb.LinkDbMapper.class);    job.setJarByClass(LinkDbMerger.class);    job.setCombinerClass(LinkDbMerger.LinkDbMergeReducer.class);        if (normalize || filter) {        try {            FileSystem fs = linkDb.getFileSystem(config);            if (!fs.exists(linkDb)) {                conf.setBoolean(LinkDbFilter.URL_FILTERING, filter);                conf.setBoolean(LinkDbFilter.URL_NORMALIZING, normalize);            }        } catch (Exception e) {                    }    }    job.setReducerClass(LinkDbMerger.LinkDbMergeReducer.class);    FileOutputFormat.setOutputPath(job, newLinkDb);    job.setOutputFormatClass(MapFileOutputFormat.class);    conf.setBoolean("mapreduce.output.fileoutputformat.compress", true);    job.setOutputKeyClass(Text.class);    job.setOutputValueClass(Inlinks.class);    return job;}
public static void nutch_f180_0(Job job, Path linkDb) throws IOException
{    Configuration conf = job.getConfiguration();    Path newLinkDb = FileOutputFormat.getOutputPath(job);    FileSystem fs = linkDb.getFileSystem(conf);    Path old = new Path(linkDb, "old");    Path current = new Path(linkDb, CURRENT_NAME);    if (fs.exists(current)) {        if (fs.exists(old))            fs.delete(old, true);        fs.rename(current, old);    }    fs.mkdirs(linkDb);    fs.rename(newLinkDb, current);    if (fs.exists(old))        fs.delete(old, true);    LockUtil.removeLockFile(fs, new Path(linkDb, LOCK_NAME));}
public static void nutch_f181_0(String[] args) throws Exception
{    int res = ToolRunner.run(NutchConfiguration.create(), new LinkDb(), args);    System.exit(res);}
public int nutch_f182_1(String[] args) throws Exception
{    if (args.length < 2) {        System.err.println("Usage: LinkDb <linkdb> (-dir <segmentsDir> | <seg1> <seg2> ...) [-force] [-noNormalize] [-noFilter]");        System.err.println("\tlinkdb\toutput LinkDb to create or update");        System.err.println("\t-dir segmentsDir\tparent directory of several segments, OR");        System.err.println("\tseg1 seg2 ...\t list of segment directories");        System.err.println("\t-force\tforce update even if LinkDb appears to be locked (CAUTION advised)");        System.err.println("\t-noNormalize\tdon't normalize link URLs");        System.err.println("\t-noFilter\tdon't apply URLFilters to link URLs");        return -1;    }    Path db = new Path(args[0]);    ArrayList<Path> segs = new ArrayList<>();    boolean filter = true;    boolean normalize = true;    boolean force = false;    for (int i = 1; i < args.length; i++) {        if ("-dir".equals(args[i])) {            Path segDir = new Path(args[++i]);            FileSystem fs = segDir.getFileSystem(getConf());            FileStatus[] paths = fs.listStatus(segDir, HadoopFSUtil.getPassDirectoriesFilter(fs));            segs.addAll(Arrays.asList(HadoopFSUtil.getPaths(paths)));        } else if ("-noNormalize".equalsIgnoreCase(args[i])) {            normalize = false;        } else if ("-noFilter".equalsIgnoreCase(args[i])) {            filter = false;        } else if ("-force".equalsIgnoreCase(args[i])) {            force = true;        } else            segs.add(new Path(args[i]));    }    try {        invert(db, segs.toArray(new Path[segs.size()]), normalize, filter, force);        return 0;    } catch (Exception e) {                return -1;    }}
public Map<String, Object> nutch_f183_1(Map<String, Object> args, String crawlId) throws Exception
{    Map<String, Object> results = new HashMap<>();    Path linkdb;    if (args.containsKey(Nutch.ARG_LINKDB)) {        Object path = args.get(Nutch.ARG_LINKDB);        if (path instanceof Path) {            linkdb = (Path) path;        } else {            linkdb = new Path(path.toString());        }    } else {        linkdb = new Path(crawlId + "/linkdb");    }    ArrayList<Path> segs = new ArrayList<>();    boolean filter = true;    boolean normalize = true;    boolean force = false;    if (args.containsKey("noNormalize")) {        normalize = false;    }    if (args.containsKey("noFilter")) {        filter = false;    }    if (args.containsKey("force")) {        force = true;    }    Path segmentsDir;    if (args.containsKey(Nutch.ARG_SEGMENTDIR)) {        Object segDir = args.get(Nutch.ARG_SEGMENTDIR);        if (segDir instanceof Path) {            segmentsDir = (Path) segDir;        } else {            segmentsDir = new Path(segDir.toString());        }        FileSystem fs = segmentsDir.getFileSystem(getConf());        FileStatus[] paths = fs.listStatus(segmentsDir, HadoopFSUtil.getPassDirectoriesFilter(fs));        segs.addAll(Arrays.asList(HadoopFSUtil.getPaths(paths)));    } else if (args.containsKey(Nutch.ARG_SEGMENTS)) {        Object segments = args.get(Nutch.ARG_SEGMENTS);        ArrayList<String> segmentList = new ArrayList<>();        if (segments instanceof ArrayList) {            segmentList = (ArrayList<String>) segments;        } else if (segments instanceof Path) {            segmentList.add(segments.toString());        }        for (String segment : segmentList) {            segs.add(new Path(segment));        }    } else {        String segmentDir = crawlId + "/segments";        File dir = new File(segmentDir);        File[] segmentsList = dir.listFiles();        Arrays.sort(segmentsList, (f1, f2) -> {            if (f1.lastModified() > f2.lastModified())                return -1;            else                return 0;        });        segs.add(new Path(segmentsList[0].getPath()));    }    try {        invert(linkdb, segs.toArray(new Path[segs.size()]), normalize, filter, force);        results.put(Nutch.VAL_RESULT, Integer.toString(0));        return results;    } catch (Exception e) {                results.put(Nutch.VAL_RESULT, Integer.toString(-1));        return results;    }}
public void nutch_f184_0(Mapper<Text, Inlinks, Text, Inlinks>.Context context)
{    Configuration conf = context.getConfiguration();    filter = conf.getBoolean(URL_FILTERING, false);    normalize = conf.getBoolean(URL_NORMALIZING, false);    if (filter) {        filters = new URLFilters(conf);    }    if (normalize) {        scope = conf.get(URL_NORMALIZING_SCOPE, URLNormalizers.SCOPE_LINKDB);        normalizers = new URLNormalizers(conf, scope);    }}
public void nutch_f185_0()
{}
public void nutch_f186_1(Text key, Inlinks value, Context context) throws IOException, InterruptedException
{    String url = key.toString();    Inlinks result = new Inlinks();    if (normalize) {        try {                        url = normalizers.normalize(url, scope);        } catch (Exception e) {                        url = null;        }    }    if (url != null && filter) {        try {                        url = filters.filter(url);        } catch (Exception e) {                        url = null;        }    }    if (url == null)                return;    Iterator<Inlink> it = value.iterator();    String fromUrl = null;    while (it.hasNext()) {        Inlink inlink = it.next();        fromUrl = inlink.getFromUrl();        if (normalize) {            try {                                fromUrl = normalizers.normalize(fromUrl, scope);            } catch (Exception e) {                                fromUrl = null;            }        }        if (fromUrl != null && filter) {            try {                                fromUrl = filters.filter(fromUrl);            } catch (Exception e) {                                fromUrl = null;            }        }        if (fromUrl != null) {            result.add(new Inlink(fromUrl, inlink.getAnchor()));        }    }    if (result.size() > 0) {                newKey.set(url);        context.write(newKey, result);    }}
public void nutch_f187_0(Reducer<Text, Inlinks, Text, Inlinks>.Context context)
{    Configuration conf = context.getConfiguration();    maxInlinks = conf.getInt("linkdb.max.inlinks", 10000);}
public void nutch_f188_0(Text key, Iterable<Inlinks> values, Context context) throws IOException, InterruptedException
{    Inlinks result = new Inlinks();    for (Inlinks inlinks : values) {        int end = Math.min(maxInlinks - result.size(), inlinks.size());        Iterator<Inlink> it = inlinks.iterator();        int i = 0;        while (it.hasNext() && i++ < end) {            result.add(it.next());        }    }    if (result.size() == 0)        return;    context.write(key, result);}
public void nutch_f189_0() throws IOException
{}
public void nutch_f190_1(Path output, Path[] dbs, boolean normalize, boolean filter) throws Exception
{    SimpleDateFormat sdf = new SimpleDateFormat("yyyy-MM-dd HH:mm:ss");    long start = System.currentTimeMillis();        Job job = createMergeJob(getConf(), output, normalize, filter);    for (int i = 0; i < dbs.length; i++) {        FileInputFormat.addInputPath(job, new Path(dbs[i], LinkDb.CURRENT_NAME));    }    try {        boolean success = job.waitForCompletion(true);        if (!success) {            String message = "LinkDbMerge job did not succeed, job status:" + job.getStatus().getState() + ", reason: " + job.getStatus().getFailureInfo();                        throw new RuntimeException(message);        }    } catch (IOException | InterruptedException | ClassNotFoundException e) {                throw e;    }    FileSystem fs = output.getFileSystem(getConf());    fs.mkdirs(output);    fs.rename(FileOutputFormat.getOutputPath(job), new Path(output, LinkDb.CURRENT_NAME));    long end = System.currentTimeMillis();    }
public static Job nutch_f191_0(Configuration config, Path linkDb, boolean normalize, boolean filter) throws IOException
{    Path newLinkDb = new Path(linkDb, "merge-" + Integer.toString(new Random().nextInt(Integer.MAX_VALUE)));    Job job = NutchJob.getInstance(config);    job.setJobName("linkdb merge " + linkDb);    Configuration conf = job.getConfiguration();    job.setInputFormatClass(SequenceFileInputFormat.class);    job.setMapperClass(LinkDbFilter.class);    conf.setBoolean(LinkDbFilter.URL_NORMALIZING, normalize);    conf.setBoolean(LinkDbFilter.URL_FILTERING, filter);    job.setJarByClass(LinkDbMerger.class);    job.setReducerClass(LinkDbMergeReducer.class);    FileOutputFormat.setOutputPath(job, newLinkDb);    job.setOutputFormatClass(MapFileOutputFormat.class);    conf.setBoolean("mapreduce.output.fileoutputformat.compress", true);    job.setOutputKeyClass(Text.class);    job.setOutputValueClass(Inlinks.class);        conf.setBoolean("mapreduce.fileoutputcommitter.marksuccessfuljobs", false);    return job;}
public static void nutch_f192_0(String[] args) throws Exception
{    int res = ToolRunner.run(NutchConfiguration.create(), new LinkDbMerger(), args);    System.exit(res);}
public int nutch_f193_1(String[] args) throws Exception
{    if (args.length < 2) {        System.err.println("Usage: LinkDbMerger <output_linkdb> <linkdb1> [<linkdb2> <linkdb3> ...] [-normalize] [-filter]");        System.err.println("\toutput_linkdb\toutput LinkDb");        System.err.println("\tlinkdb1 ...\tinput LinkDb-s (single input LinkDb is ok)");        System.err.println("\t-normalize\tuse URLNormalizer on both fromUrls and toUrls in linkdb(s) (usually not needed)");        System.err.println("\t-filter\tuse URLFilters on both fromUrls and toUrls in linkdb(s)");        return -1;    }    Path output = new Path(args[0]);    ArrayList<Path> dbs = new ArrayList<>();    boolean normalize = false;    boolean filter = false;    for (int i = 1; i < args.length; i++) {        if (args[i].equals("-filter")) {            filter = true;        } else if (args[i].equals("-normalize")) {            normalize = true;        } else            dbs.add(new Path(args[i]));    }    try {        merge(output, dbs.toArray(new Path[dbs.size()]), normalize, filter);        return 0;    } catch (Exception e) {                return -1;    }}
public void nutch_f194_0(Path directory) throws Exception
{    this.directory = directory;}
public String[] nutch_f195_0(Text url) throws IOException
{    Inlinks inlinks = getInlinks(url);    if (inlinks == null)        return null;    return inlinks.getAnchors();}
public Inlinks nutch_f196_0(Text url) throws IOException
{    if (readers == null) {        synchronized (this) {            readers = MapFileOutputFormat.getReaders(new Path(directory, LinkDb.CURRENT_NAME), getConf());        }    }    return (Inlinks) MapFileOutputFormat.getEntry(readers, PARTITIONER, url, new Inlinks());}
public void nutch_f197_0() throws IOException
{    if (readers != null) {        for (int i = 0; i < readers.length; i++) {            readers[i].close();        }    }}
public void nutch_f198_0(Mapper<Text, Inlinks, Text, Inlinks>.Context context)
{    Configuration conf = context.getConfiguration();    if (conf.get("linkdb.regex", null) != null) {        pattern = Pattern.compile(conf.get("linkdb.regex"));    }}
public void nutch_f199_0(Text key, Inlinks value, Context context) throws IOException, InterruptedException
{    if (pattern != null) {        matcher = pattern.matcher(key.toString());        if (!matcher.matches()) {            return;        }    }    context.write(key, value);}
public void nutch_f200_1(String linkdb, String output, String regex) throws IOException, InterruptedException, ClassNotFoundException
{    SimpleDateFormat sdf = new SimpleDateFormat("yyyy-MM-dd HH:mm:ss");    long start = System.currentTimeMillis();    if (LOG.isInfoEnabled()) {                    }    Path outFolder = new Path(output);    Job job = NutchJob.getInstance(getConf());    job.setJobName("read " + linkdb);    job.setJarByClass(LinkDbReader.class);    Configuration conf = job.getConfiguration();    if (regex != null) {        conf.set("linkdb.regex", regex);        job.setMapperClass(LinkDBDumpMapper.class);    }    FileInputFormat.addInputPath(job, new Path(linkdb, LinkDb.CURRENT_NAME));    job.setInputFormatClass(SequenceFileInputFormat.class);    FileOutputFormat.setOutputPath(job, outFolder);    job.setOutputFormatClass(TextOutputFormat.class);    job.setOutputKeyClass(Text.class);    job.setOutputValueClass(Inlinks.class);    try {        boolean success = job.waitForCompletion(true);        if (!success) {            String message = "LinkDbRead job did not succeed, job status:" + job.getStatus().getState() + ", reason: " + job.getStatus().getFailureInfo();                        throw new RuntimeException(message);        }    } catch (IOException | InterruptedException | ClassNotFoundException e) {                throw e;    }    long end = System.currentTimeMillis();    }
protected int nutch_f201_0(String line, StringBuilder output) throws Exception
{    Inlinks links = getInlinks(new Text(line));    if (links == null) {        output.append(" - no link information.");    } else {        Iterator<Inlink> it = links.iterator();        while (it.hasNext()) {            output.append(it.next().toString());        }    }    output.append("\n");    return 0;}
public static void nutch_f202_0(String[] args) throws Exception
{    int res = ToolRunner.run(NutchConfiguration.create(), new LinkDbReader(), args);    System.exit(res);}
public int nutch_f203_1(String[] args) throws Exception
{    if (args.length < 2) {        System.err.println("Usage: LinkDbReader <linkdb> (-dump <out_dir> [-regex <regex>]) | -url <url>");        System.err.println("\t-dump <out_dir>\tdump whole link db to a text file in <out_dir>");        System.err.println("\t\t-regex <regex>\trestrict to url's matching expression");        System.err.println("\t-url <url>\tprint information about <url> to System.out");        return -1;    }    int numConsumed = 0;    try {        for (int i = 1; i < args.length; i++) {            if (args[i].equals("-dump")) {                String regex = null;                for (int j = i + 1; j < args.length; j++) {                    if (args[i].equals("-regex")) {                        regex = args[++j];                    }                }                processDumpJob(args[0], args[i + 1], regex);                return 0;            } else if (args[i].equals("-url")) {                init(new Path(args[0]));                return processSingle(args[++i]);            } else if ((numConsumed = super.parseArgs(args, i)) > 0) {                init(new Path(args[0]));                i += numConsumed - 1;            } else {                System.err.println("Error: wrong argument " + args[1]);                return -1;            }        }    } catch (Exception e) {                return -1;    }    if (numConsumed > 0) {                return super.run();    }    return 0;}
public byte[] nutch_f204_0(Content content, Parse parse)
{    byte[] data = content.getContent();    if (data == null || (data.length == 0))        data = content.getUrl().getBytes();    return MD5Hash.digest(data).getDigest();}
public void nutch_f205_1(Configuration conf)
{    super.setConf(conf);    if (conf == null)        return;            defaultIncRate = conf.getFloat(SCHEDULE_INC_RATE, 0.2f);    defaultDecRate = conf.getFloat(SCHEDULE_DEC_RATE, 0.2f);        Reader mimeFile = conf.getConfResourceAsReader(conf.get(SCHEDULE_MIME_FILE, "adaptive-mimetypes.txt"));    try {        readMimeFile(mimeFile);    } catch (IOException e) {            }}
public CrawlDatum nutch_f206_0(Text url, CrawlDatum datum, long prevFetchTime, long prevModifiedTime, long fetchTime, long modifiedTime, int state)
{        INC_RATE = defaultIncRate;    DEC_RATE = defaultDecRate;        if (datum.getMetaData().containsKey(HttpHeaders.WRITABLE_CONTENT_TYPE)) {                String currentMime = MimeUtil.cleanMimeType(datum.getMetaData().get(HttpHeaders.WRITABLE_CONTENT_TYPE).toString());                if (mimeMap.containsKey(currentMime)) {                        INC_RATE = mimeMap.get(currentMime).inc;            DEC_RATE = mimeMap.get(currentMime).dec;        }    }    return super.setFetchSchedule(url, datum, prevFetchTime, prevModifiedTime, fetchTime, modifiedTime, state);}
private void nutch_f207_1(Reader mimeFile) throws IOException
{        mimeMap = new HashMap<>();        BufferedReader reader = new BufferedReader(mimeFile);    String line = null;    String[] splits = null;        while ((line = reader.readLine()) != null) {                if (StringUtils.isNotBlank(line) && !line.startsWith("#")) {                        splits = line.split("\t");                        if (splits.length == 3) {                                mimeMap.put(StringUtils.lowerCase(splits[0]), new AdaptiveRate(Float.valueOf(splits[1]), Float.valueOf(splits[2])));            } else {                            }        }    }}
public static void nutch_f208_1(String[] args) throws Exception
{    FetchSchedule fs = new MimeAdaptiveFetchSchedule();    fs.setConf(NutchConfiguration.create());        long curTime = 0;        long delta = 1000L * 3600L * 24L;            long update = 1000L * 3600L * 24L * 30L;    boolean changed = true;    long lastModified = 0;    int miss = 0;    int totalMiss = 0;    int maxMiss = 0;    int fetchCnt = 0;    int changeCnt = 0;        CrawlDatum p = new CrawlDatum(1, 3600 * 24 * 30, 1.0f);        org.apache.hadoop.io.MapWritable x = new org.apache.hadoop.io.MapWritable();    x.put(HttpHeaders.WRITABLE_CONTENT_TYPE, new Text("text/html; charset=utf-8"));    p.setMetaData(x);    p.setFetchTime(0);            for (int i = 0; i < 10000; i++) {        if (lastModified + update < curTime) {                                    changed = true;            changeCnt++;            lastModified = curTime;        }                if (p.getFetchTime() <= curTime) {            fetchCnt++;            fs.setFetchSchedule(new Text("http://www.example.com"), p, p.getFetchTime(), p.getModifiedTime(), curTime, lastModified, changed ? FetchSchedule.STATUS_MODIFIED : FetchSchedule.STATUS_NOTMODIFIED);                        if (!changed)                miss++;            if (miss > maxMiss)                maxMiss = miss;            changed = false;            totalMiss += miss;            miss = 0;        }        if (changed)            miss++;        curTime += delta;    }        }
protected Class<? extends Writable>[] nutch_f209_0()
{    return CLASSES;}
public Configuration nutch_f210_0()
{    return conf;}
public void nutch_f211_0(Configuration conf)
{    this.conf = conf;}
public int nutch_f212_0(Object o1, Object o2)
{    return _compare(o1, o2);}
public static int nutch_f213_0(Object o1, Object o2)
{    if (o1 == null && o2 == null)        return 0;    if (o1 == null)        return -1;    if (o2 == null)        return 1;    if (!(o1 instanceof byte[]))        return -1;    if (!(o2 instanceof byte[]))        return 1;    byte[] data1 = (byte[]) o1;    byte[] data2 = (byte[]) o2;    return _compare(data1, 0, data1.length, data2, 0, data2.length);}
public static int nutch_f214_0(byte[] data1, int s1, int l1, byte[] data2, int s2, int l2)
{    if (l2 > l1)        return -1;    if (l2 < l1)        return 1;    int res = 0;    for (int i = 0; i < l1; i++) {        res = (data1[s1 + i] - data2[s2 + i]);        if (res != 0)            return res;    }    return 0;}
public static synchronized Signature nutch_f215_1(Configuration conf)
{    String clazz = conf.get("db.signature.class", MD5Signature.class.getName());    ObjectCache objectCache = ObjectCache.get(conf);    Signature impl = (Signature) objectCache.getObject(clazz);    if (impl == null) {        try {            if (LOG.isInfoEnabled()) {                            }            Class<?> implClass = Class.forName(clazz);            impl = (Signature) implClass.getConstructor().newInstance();            impl.setConf(conf);            objectCache.setObject(clazz, impl);        } catch (Exception e) {            throw new RuntimeException("Couldn't create " + clazz, e);        }    }    return impl;}
public byte[] nutch_f216_0(Content content, Parse parse)
{    String text = parse.getText();    if (text == null || text.length() == 0) {        return fallback.calculate(content, parse);    }    return MD5Hash.digest(text).getDigest();}
public void nutch_f217_0(Configuration conf)
{    super.setConf(conf);    MIN_TOKEN_LEN = conf.getInt("db.signature.text_profile.min_token_len", 2);    QUANT_RATE = conf.getFloat("db.signature.text_profile.quant_rate", 0.01f);    secondaryLexicographicSorting = conf.getBoolean("db.signature.text_profile.sec_sort_lex", true);}
public byte[] nutch_f218_0(Content content, Parse parse)
{    HashMap<String, Token> tokens = new HashMap<>();    String text = null;    if (parse != null)        text = parse.getText();    if (text == null || text.length() == 0)        return fallback.calculate(content, parse);    StringBuffer curToken = new StringBuffer();    int maxFreq = 0;    for (int i = 0; i < text.length(); i++) {        char c = text.charAt(i);        if (Character.isLetterOrDigit(c)) {            curToken.append(Character.toLowerCase(c));        } else {            if (curToken.length() > 0) {                if (curToken.length() > MIN_TOKEN_LEN) {                                        String s = curToken.toString();                    Token tok = tokens.get(s);                    if (tok == null) {                        tok = new Token(0, s);                        tokens.put(s, tok);                    }                    tok.cnt++;                    if (tok.cnt > maxFreq)                        maxFreq = tok.cnt;                }                curToken.setLength(0);            }        }    }        if (curToken.length() > MIN_TOKEN_LEN) {                String s = curToken.toString();        Token tok = tokens.get(s);        if (tok == null) {            tok = new Token(0, s);            tokens.put(s, tok);        }        tok.cnt++;        if (tok.cnt > maxFreq)            maxFreq = tok.cnt;    }    Iterator<Token> it = tokens.values().iterator();    ArrayList<Token> profile = new ArrayList<>();        int QUANT = Math.round(maxFreq * QUANT_RATE);    if (QUANT < 2) {        if (maxFreq > 1)            QUANT = 2;        else            QUANT = 1;    }    while (it.hasNext()) {        Token t = it.next();                t.cnt = (t.cnt / QUANT) * QUANT;                if (t.cnt < QUANT) {            continue;        }        profile.add(t);    }    Collections.sort(profile, new TokenComparator());    StringBuffer newText = new StringBuffer();    it = profile.iterator();    while (it.hasNext()) {        Token t = it.next();        if (newText.length() > 0)            newText.append("\n");        newText.append(t.toString());    }    return MD5Hash.digest(newText.toString()).getDigest();}
public String nutch_f219_0()
{    return val + " " + cnt;}
public int nutch_f220_0(Token t1, Token t2)
{    int diffCnt = t2.cnt - t1.cnt;    if (diffCnt == 0 && secondaryLexicographicSorting) {        return t1.val.compareTo(t2.val);    }    return diffCnt;}
public static void nutch_f221_0(String[] args) throws Exception
{    TextProfileSignature sig = new TextProfileSignature();    sig.setConf(NutchConfiguration.create());    HashMap<String, byte[]> res = new HashMap<>();    File[] files = new File(args[0]).listFiles();    for (int i = 0; i < files.length; i++) {        FileInputStream fis = new FileInputStream(files[i]);        BufferedReader br = new BufferedReader(new InputStreamReader(fis, "UTF-8"));        StringBuffer text = new StringBuffer();        String line = null;        while ((line = br.readLine()) != null) {            if (text.length() > 0)                text.append("\n");            text.append(line);        }        br.close();        byte[] signature = sig.calculate(null, new ParseImpl(text.toString(), null));        res.put(files[i].toString(), signature);    }    Iterator<String> it = res.keySet().iterator();    while (it.hasNext()) {        String name = it.next();        byte[] signature = res.get(name);        System.out.println(name + "\t" + StringUtil.toHexString(signature));    }}
public void nutch_f222_1(Configuration conf)
{    this.conf = conf;    seed = conf.getInt("partition.url.seed", 0);    mode = conf.get(PARTITION_MODE_KEY, PARTITION_MODE_HOST);        if (!mode.equals(PARTITION_MODE_IP) && !mode.equals(PARTITION_MODE_DOMAIN) && !mode.equals(PARTITION_MODE_HOST)) {                mode = PARTITION_MODE_HOST;    }    normalizers = new URLNormalizers(conf, URLNormalizers.SCOPE_PARTITION);}
public Configuration nutch_f223_0()
{    return conf;}
public void nutch_f224_0()
{}
public static ExchangeConfig nutch_f226_0(Element element)
{    String id = element.getAttribute("id");    String clazz = element.getAttribute("class");        NodeList writerList = element.getElementsByTagName("writer");    String[] writers = new String[writerList.getLength()];    for (int i = 0; i < writerList.getLength(); i++) {        writers[i] = ((Element) writerList.item(i)).getAttribute("id");    }        NodeList paramList = element.getElementsByTagName("param");    Map<String, String> paramsMap = new HashMap<>();    for (int i = 0; i < paramList.getLength(); i++) {        Element param = (Element) paramList.item(i);        paramsMap.put(param.getAttribute("name"), param.getAttribute("value"));    }    return new ExchangeConfig(id, clazz, writers, paramsMap);}
public String nutch_f227_0()
{    return id;}
public String nutch_f228_0()
{    return clazz;}
 String[] nutch_f229_0()
{    return writersIDs;}
public Map<String, String> nutch_f230_0()
{    return parameters;}
public boolean nutch_f231_0()
{    return availableExchanges;}
private ExchangeConfig[] nutch_f232_1(Configuration conf)
{    String filename = conf.get("exchanges.exchanges.file", "exchanges.xml");    InputSource inputSource = new InputSource(conf.getConfResourceAsInputStream(filename));    final List<ExchangeConfig> configList = new LinkedList<>();    try {        DocumentBuilderFactory factory = DocumentBuilderFactory.newInstance();        DocumentBuilder builder = factory.newDocumentBuilder();        Element rootElement = builder.parse(inputSource).getDocumentElement();        NodeList exchangeList = rootElement.getElementsByTagName("exchange");        for (int i = 0; i < exchangeList.getLength(); i++) {            Element element = (Element) exchangeList.item(i);            ExchangeConfig exchangeConfig = ExchangeConfig.getInstance(element);            if ("default".equals(exchangeConfig.getClazz())) {                this.defaultExchangeConfig = exchangeConfig;                continue;            }            configList.add(exchangeConfig);        }    } catch (SAXException | IOException | ParserConfigurationException e) {            }    return configList.toArray(new ExchangeConfig[0]);}
public void nutch_f233_0()
{    exchanges.forEach((id, value) -> value.exchange.open(value.config.getParameters()));}
public String[] nutch_f234_0(final NutchDocument nutchDocument)
{    final Set<String> writersIDs = new HashSet<>();    exchanges.forEach((id, value) -> {        if (value.exchange.match(nutchDocument)) {            writersIDs.addAll(Arrays.asList(value.config.getWritersIDs()));        }    });        if (defaultExchangeConfig != null && writersIDs.isEmpty()) {        return defaultExchangeConfig.getWritersIDs();    }    return writersIDs.toArray(new String[0]);}
public List<InputSplit> nutch_f235_0(JobContext job) throws IOException
{    List<FileStatus> files = listStatus(job);    List<InputSplit> splits = new ArrayList<>();    for (FileStatus cur : files) {        splits.add(new FileSplit(cur.getPath(), 0, cur.getLen(), (String[]) null));    }    return splits;}
public static boolean nutch_f236_0(Configuration conf)
{    return conf.getBoolean("fetcher.parse", true);}
public static boolean nutch_f237_0(Configuration conf)
{    return conf.getBoolean("fetcher.store.content", true);}
private AtomicInteger nutch_f238_0()
{    return activeThreads;}
private void nutch_f239_0(Context context, FetchItemQueues fetchQueues, int pagesLastSec, int bytesLastSec) throws IOException
{    StringBuilder status = new StringBuilder();    Long elapsed = Long.valueOf((System.currentTimeMillis() - start) / 1000);    float avgPagesSec = (float) pages.get() / elapsed.floatValue();    long avgBytesSec = (bytes.get() / 128l) / elapsed.longValue();    status.append(activeThreads).append(" threads (").append(spinWaiting.get()).append(" waiting), ");    status.append(fetchQueues.getQueueCount()).append(" queues, ");    status.append(fetchQueues.getTotalSize()).append(" URLs queued, ");    status.append(pages).append(" pages, ").append(errors).append(" errors, ");    status.append(String.format("%.2f", avgPagesSec)).append(" pages/s (");    status.append(pagesLastSec).append(" last sec), ");    status.append(avgBytesSec).append(" kbits/s (").append((bytesLastSec / 128)).append(" last sec)");    context.setStatus(status.toString());}
public void nutch_f240_0(Mapper<Text, CrawlDatum, Text, NutchWritable>.Context context)
{    Configuration conf = context.getConfiguration();    segmentName = conf.get(Nutch.SEGMENT_NAME_KEY);    storingContent = isStoringContent(conf);    parsing = isParsing(conf);}
public void nutch_f241_1(Context innerContext) throws IOException
{    setup(innerContext);    Configuration conf = innerContext.getConfiguration();    LinkedList<FetcherThread> fetcherThreads = new LinkedList<>();    FetchItemQueues fetchQueues = new FetchItemQueues(conf);    QueueFeeder feeder;    int threadCount = conf.getInt("fetcher.threads.fetch", 10);    if (LOG.isInfoEnabled()) {            }    int timeoutDivisor = conf.getInt("fetcher.threads.timeout.divisor", 2);    if (LOG.isInfoEnabled()) {            }    int queueDepthMuliplier = conf.getInt("fetcher.queue.depth.multiplier", 50);    feeder = new QueueFeeder(innerContext, fetchQueues, threadCount * queueDepthMuliplier);            long timelimit = conf.getLong("fetcher.timelimit", -1);    if (timelimit != -1)        feeder.setTimeLimit(timelimit);    feeder.start();    for (int i = 0; i < threadCount; i++) {                FetcherThread t = new FetcherThread(conf, getActiveThreads(), fetchQueues, feeder, spinWaiting, lastRequestStart, innerContext, errors, segmentName, parsing, storingContent, pages, bytes);        fetcherThreads.add(t);        t.start();    }        long timeout = conf.getInt("mapreduce.task.timeout", 10 * 60 * 1000) / timeoutDivisor;            int pagesLastSec;    int bytesLastSec;    int throughputThresholdNumRetries = 0;    int throughputThresholdPages = conf.getInt("fetcher.throughput.threshold.pages", -1);    if (LOG.isInfoEnabled()) {            }    int throughputThresholdMaxRetries = conf.getInt("fetcher.throughput.threshold.retries", 5);    if (LOG.isInfoEnabled()) {            }    long throughputThresholdTimeLimit = conf.getLong("fetcher.throughput.threshold.check.after", -1);    int targetBandwidth = conf.getInt("fetcher.bandwidth.target", -1) * 1000;    int maxNumThreads = conf.getInt("fetcher.maxNum.threads", threadCount);    if (maxNumThreads < threadCount) {                maxNumThreads = threadCount;    }    int bandwidthTargetCheckEveryNSecs = conf.getInt("fetcher.bandwidth.target.check.everyNSecs", 30);    if (bandwidthTargetCheckEveryNSecs < 1) {                bandwidthTargetCheckEveryNSecs = 1;    }    int maxThreadsPerQueue = conf.getInt("fetcher.threads.per.queue", 1);    int bandwidthTargetCheckCounter = 0;    long bytesAtLastBWTCheck = 0l;    do {                pagesLastSec = pages.get();        bytesLastSec = (int) bytes.get();        try {            Thread.sleep(1000);        } catch (InterruptedException e) {        }        pagesLastSec = pages.get() - pagesLastSec;        bytesLastSec = (int) bytes.get() - bytesLastSec;        innerContext.getCounter("FetcherStatus", "bytes_downloaded").increment(bytesLastSec);        reportStatus(innerContext, fetchQueues, pagesLastSec, bytesLastSec);                if (!feeder.isAlive() && fetchQueues.getTotalSize() < 5) {            fetchQueues.dump();        }                if (throughputThresholdTimeLimit < System.currentTimeMillis() && throughputThresholdPages != -1) {                        if (pagesLastSec < throughputThresholdPages) {                throughputThresholdNumRetries++;                                                if (throughputThresholdNumRetries == throughputThresholdMaxRetries) {                                                            throughputThresholdPages = -1;                                                            int hitByThrougputThreshold = fetchQueues.emptyQueues();                    if (hitByThrougputThreshold != 0)                        innerContext.getCounter("FetcherStatus", "hitByThrougputThreshold").increment(hitByThrougputThreshold);                }            }        }                if (targetBandwidth > 0) {            if (bandwidthTargetCheckCounter < bandwidthTargetCheckEveryNSecs)                bandwidthTargetCheckCounter++;            else if (bandwidthTargetCheckCounter == bandwidthTargetCheckEveryNSecs) {                long bpsSinceLastCheck = ((bytes.get() - bytesAtLastBWTCheck) * 8) / bandwidthTargetCheckEveryNSecs;                bytesAtLastBWTCheck = bytes.get();                bandwidthTargetCheckCounter = 0;                int averageBdwPerThread = 0;                if (activeThreads.get() > 0)                    averageBdwPerThread = Math.round(bpsSinceLastCheck / activeThreads.get());                                if (bpsSinceLastCheck < targetBandwidth && averageBdwPerThread > 0) {                    if ((fetchQueues.getQueueCount() * maxThreadsPerQueue) > activeThreads.get()) {                        long remainingBdw = targetBandwidth - bpsSinceLastCheck;                        int additionalThreads = Math.round(remainingBdw / averageBdwPerThread);                        int availableThreads = maxNumThreads - activeThreads.get();                                                                        additionalThreads = (availableThreads < additionalThreads ? availableThreads : additionalThreads);                                                                        for (int i = 0; i < additionalThreads; i++) {                            FetcherThread thread = new FetcherThread(conf, getActiveThreads(), fetchQueues, feeder, spinWaiting, lastRequestStart, innerContext, errors, segmentName, parsing, storingContent, pages, bytes);                            fetcherThreads.add(thread);                            thread.start();                        }                    }                } else if (bpsSinceLastCheck > targetBandwidth && averageBdwPerThread > 0) {                                                            long excessBdw = bpsSinceLastCheck - targetBandwidth;                    int excessThreads = Math.round(excessBdw / averageBdwPerThread);                                                            if (excessThreads >= fetcherThreads.size())                        excessThreads = 0;                                        for (int i = 0; i < excessThreads; i++) {                        FetcherThread thread = fetcherThreads.removeLast();                        thread.setHalted(true);                    }                }            }        }                if (!feeder.isAlive()) {            int hitByTimeLimit = fetchQueues.checkTimelimit();            if (hitByTimeLimit != 0)                innerContext.getCounter("FetcherStatus", "hitByTimeLimit").increment(hitByTimeLimit);        }                if ((System.currentTimeMillis() - lastRequestStart.get()) > timeout) {            if (LOG.isWarnEnabled()) {                                for (int i = 0; i < fetcherThreads.size(); i++) {                    FetcherThread thread = fetcherThreads.get(i);                    if (thread.isAlive()) {                                                if (LOG.isDebugEnabled()) {                            StackTraceElement[] stack = thread.getStackTrace();                            StringBuilder sb = new StringBuilder();                            sb.append("Stack of thread #").append(i).append(":\n");                            for (StackTraceElement s : stack) {                                sb.append(s.toString()).append('\n');                            }                                                    }                    }                }            }            return;        }    } while (activeThreads.get() > 0);    }
public void nutch_f242_1(Path segment, int threads) throws IOException, InterruptedException, ClassNotFoundException
{    checkConfiguration();    long start = System.currentTimeMillis();    if (LOG.isInfoEnabled()) {                    }                long timelimit = getConf().getLong("fetcher.timelimit.mins", -1);    if (timelimit != -1) {        timelimit = System.currentTimeMillis() + (timelimit * 60 * 1000);                getConf().setLong("fetcher.timelimit", timelimit);    }            timelimit = getConf().getLong("fetcher.throughput.threshold.check.after", 10);    timelimit = System.currentTimeMillis() + (timelimit * 60 * 1000);    getConf().setLong("fetcher.throughput.threshold.check.after", timelimit);    int maxOutlinkDepth = getConf().getInt("fetcher.follow.outlinks.depth", -1);    if (maxOutlinkDepth > 0) {                int maxOutlinkDepthNumLinks = getConf().getInt("fetcher.follow.outlinks.num.links", 4);        int outlinksDepthDivisor = getConf().getInt("fetcher.follow.outlinks.depth.divisor", 2);        int totalOutlinksToFollow = 0;        for (int i = 0; i < maxOutlinkDepth; i++) {            totalOutlinksToFollow += (int) Math.floor(outlinksDepthDivisor / (i + 1) * maxOutlinkDepthNumLinks);        }            }    Job job = NutchJob.getInstance(getConf());    job.setJobName("FetchData");    Configuration conf = job.getConfiguration();    conf.setInt("fetcher.threads.fetch", threads);    conf.set(Nutch.SEGMENT_NAME_KEY, segment.getName());        conf.set("mapreduce.map.speculative", "false");    FileInputFormat.addInputPath(job, new Path(segment, CrawlDatum.GENERATE_DIR_NAME));    job.setInputFormatClass(InputFormat.class);    job.setJarByClass(Fetcher.class);    job.setMapperClass(Fetcher.FetcherRun.class);    FileOutputFormat.setOutputPath(job, segment);    job.setOutputFormatClass(FetcherOutputFormat.class);    job.setOutputKeyClass(Text.class);    job.setOutputValueClass(NutchWritable.class);    try {        boolean success = job.waitForCompletion(true);        if (!success) {            String message = "Fetcher job did not succeed, job status:" + job.getStatus().getState() + ", reason: " + job.getStatus().getFailureInfo();                        throw new RuntimeException(message);        }    } catch (InterruptedException | ClassNotFoundException e) {                throw e;    }    long end = System.currentTimeMillis();    }
public static void nutch_f243_0(String[] args) throws Exception
{    int res = ToolRunner.run(NutchConfiguration.create(), new Fetcher(), args);    System.exit(res);}
public int nutch_f244_1(String[] args) throws Exception
{    String usage = "Usage: Fetcher <segment> [-threads n]";    if (args.length < 1) {        System.err.println(usage);        return -1;    }    Path segment = new Path(args[0]);    int threads = getConf().getInt("fetcher.threads.fetch", 10);    for (int i = 1; i < args.length; i++) {                if (args[i].equals("-threads")) {                        threads = Integer.parseInt(args[++i]);        }    }    getConf().setInt("fetcher.threads.fetch", threads);    try {        fetch(segment, threads);        return 0;    } catch (Exception e) {                return -1;    }}
private void nutch_f245_1()
{        String agentName = getConf().get("http.agent.name");    if (agentName == null || agentName.trim().length() == 0) {        String message = "Fetcher: No agents listed in 'http.agent.name'" + " property.";        if (LOG.isErrorEnabled()) {                    }        throw new IllegalArgumentException(message);    }}
public Map<String, Object> nutch_f246_1(Map<String, Object> args, String crawlId) throws Exception
{    Map<String, Object> results = new HashMap<>();    Path segment = null;    if (args.containsKey(Nutch.ARG_SEGMENTS)) {        Object seg = args.get(Nutch.ARG_SEGMENTS);        if (seg instanceof Path) {            segment = (Path) seg;        } else if (seg instanceof String) {            segment = new Path(seg.toString());        } else if (seg instanceof ArrayList) {            String[] segmentsArray = (String[]) seg;            segment = new Path(segmentsArray[0].toString());            if (segmentsArray.length > 1) {                            }        }    } else {        String segmentDir = crawlId + "/segments";        File segmentsDir = new File(segmentDir);        File[] segmentsList = segmentsDir.listFiles();        Arrays.sort(segmentsList, (f1, f2) -> {            if (f1.lastModified() > f2.lastModified())                return -1;            else                return 0;        });        segment = new Path(segmentsList[0].getPath());    }    int threads = getConf().getInt("fetcher.threads.fetch", 10);        if (args.containsKey("threads")) {                threads = Integer.parseInt((String) args.get("threads"));    }    getConf().setInt("fetcher.threads.fetch", threads);    try {        fetch(segment, threads);        results.put(Nutch.VAL_RESULT, Integer.toString(0));        return results;    } catch (Exception e) {                results.put(Nutch.VAL_RESULT, Integer.toString(-1));        return results;    }}
public void nutch_f247_0(JobContext job) throws IOException
{    Configuration conf = job.getConfiguration();    Path out = FileOutputFormat.getOutputPath(job);    if ((out == null) && (job.getNumReduceTasks() != 0)) {        throw new InvalidJobConfException("Output directory not set in conf.");    }    FileSystem fs = out.getFileSystem(conf);    if (fs.exists(new Path(out, CrawlDatum.FETCH_DIR_NAME))) {        throw new IOException("Segment already fetched!");    }}
public RecordWriter<Text, NutchWritable> nutch_f248_0(TaskAttemptContext context) throws IOException
{    Configuration conf = context.getConfiguration();    String name = getUniqueFile(context, "part", "");    Path out = FileOutputFormat.getOutputPath(context);    final Path fetch = new Path(new Path(out, CrawlDatum.FETCH_DIR_NAME), name);    final Path content = new Path(new Path(out, Content.DIR_NAME), name);    final CompressionType compType = SequenceFileOutputFormat.getOutputCompressionType(context);    Option fKeyClassOpt = MapFile.Writer.keyClass(Text.class);    org.apache.hadoop.io.SequenceFile.Writer.Option fValClassOpt = SequenceFile.Writer.valueClass(CrawlDatum.class);    org.apache.hadoop.io.SequenceFile.Writer.Option fProgressOpt = SequenceFile.Writer.progressable((Progressable) context);    org.apache.hadoop.io.SequenceFile.Writer.Option fCompOpt = SequenceFile.Writer.compression(compType);    final MapFile.Writer fetchOut = new MapFile.Writer(conf, fetch, fKeyClassOpt, fValClassOpt, fCompOpt, fProgressOpt);    return new RecordWriter<Text, NutchWritable>() {        private MapFile.Writer contentOut;        private RecordWriter<Text, Parse> parseOut;        {            if (Fetcher.isStoringContent(conf)) {                Option cKeyClassOpt = MapFile.Writer.keyClass(Text.class);                org.apache.hadoop.io.SequenceFile.Writer.Option cValClassOpt = SequenceFile.Writer.valueClass(Content.class);                org.apache.hadoop.io.SequenceFile.Writer.Option cProgressOpt = SequenceFile.Writer.progressable((Progressable) context);                org.apache.hadoop.io.SequenceFile.Writer.Option cCompOpt = SequenceFile.Writer.compression(compType);                contentOut = new MapFile.Writer(conf, content, cKeyClassOpt, cValClassOpt, cCompOpt, cProgressOpt);            }            if (Fetcher.isParsing(conf)) {                parseOut = new ParseOutputFormat().getRecordWriter(context);            }        }        public void write(Text key, NutchWritable value) throws IOException, InterruptedException {            Writable w = value.get();            if (w instanceof CrawlDatum)                fetchOut.append(key, w);            else if (w instanceof Content && contentOut != null)                contentOut.append(key, w);            else if (w instanceof Parse && parseOut != null)                parseOut.write(key, (Parse) w);        }        public void close(TaskAttemptContext context) throws IOException, InterruptedException {            fetchOut.close();            if (contentOut != null) {                contentOut.close();            }            if (parseOut != null) {                parseOut.close(context);            }        }    };}
public void nutch_f249_0(Text key, NutchWritable value) throws IOException, InterruptedException
{    Writable w = value.get();    if (w instanceof CrawlDatum)        fetchOut.append(key, w);    else if (w instanceof Content && contentOut != null)        contentOut.append(key, w);    else if (w instanceof Parse && parseOut != null)        parseOut.write(key, (Parse) w);}
public void nutch_f250_0(TaskAttemptContext context) throws IOException, InterruptedException
{    fetchOut.close();    if (contentOut != null) {        contentOut.close();    }    if (parseOut != null) {        parseOut.close(context);    }}
public void nutch_f251_1()
{        activeThreads.incrementAndGet();    FetchItem fit = null;    try {                if (parsing && NutchServer.getInstance().isRunning())            reportToNutchServer = true;        while (true) {                        if (reportToNutchServer)                this.fetchNode = new FetchNode();            else                this.fetchNode = null;                        if (isHalted()) {                                fit = null;                return;            }            fit = ((FetchItemQueues) fetchQueues).getFetchItem();            if (fit == null) {                if (feeder.isAlive() || ((FetchItemQueues) fetchQueues).getTotalSize() > 0) {                                                            ((AtomicInteger) spinWaiting).incrementAndGet();                    try {                        Thread.sleep(500);                    } catch (Exception e) {                    }                    ((AtomicInteger) spinWaiting).decrementAndGet();                    continue;                } else {                                                            return;                }            }            lastRequestStart.set(System.currentTimeMillis());            Text reprUrlWritable = (Text) fit.datum.getMetaData().get(Nutch.WRITABLE_REPR_URL_KEY);            if (reprUrlWritable == null) {                setReprUrl(fit.url.toString());            } else {                setReprUrl(reprUrlWritable.toString());            }            try {                                redirecting = false;                redirectCount = 0;                                if (activatePublisher) {                    FetcherThreadEvent startEvent = new FetcherThreadEvent(PublishEventType.START, fit.getUrl().toString());                    publisher.publish(startEvent, conf);                }                do {                    if (LOG.isInfoEnabled()) {                                            }                    if (LOG.isDebugEnabled()) {                                            }                    redirecting = false;                    Protocol protocol = this.protocolFactory.getProtocol(fit.u);                    BaseRobotRules rules = protocol.getRobotRules(fit.url, fit.datum, robotsTxtContent);                    if (robotsTxtContent != null) {                        outputRobotsTxt(robotsTxtContent);                        robotsTxtContent.clear();                    }                    if (!rules.isAllowed(fit.url.toString())) {                                                ((FetchItemQueues) fetchQueues).finishFetchItem(fit, true);                                                output(fit.url, fit.datum, null, ProtocolStatus.STATUS_ROBOTS_DENIED, CrawlDatum.STATUS_FETCH_GONE);                        context.getCounter("FetcherStatus", "robots_denied").increment(1);                        continue;                    }                    if (rules.getCrawlDelay() > 0) {                        if (rules.getCrawlDelay() > maxCrawlDelay && maxCrawlDelay >= 0) {                                                        ((FetchItemQueues) fetchQueues).finishFetchItem(fit, true);                                                        output(fit.url, fit.datum, null, ProtocolStatus.STATUS_ROBOTS_DENIED, CrawlDatum.STATUS_FETCH_GONE);                            context.getCounter("FetcherStatus", "robots_denied_maxcrawldelay").increment(1);                            continue;                        } else {                            FetchItemQueue fiq = ((FetchItemQueues) fetchQueues).getFetchItemQueue(fit.queueID);                            fiq.crawlDelay = rules.getCrawlDelay();                            if (LOG.isDebugEnabled()) {                                                            }                        }                    }                    ProtocolOutput output = protocol.getProtocolOutput(fit.url, fit.datum);                    ProtocolStatus status = output.getStatus();                    Content content = output.getContent();                    ParseStatus pstatus = null;                                        ((FetchItemQueues) fetchQueues).finishFetchItem(fit);                                        if (fetchNode != null) {                        fetchNode.setStatus(status.getCode());                        fetchNode.setFetchTime(System.currentTimeMillis());                        fetchNode.setUrl(fit.url);                    }                                        if (activatePublisher) {                        FetcherThreadEvent endEvent = new FetcherThreadEvent(PublishEventType.END, fit.getUrl().toString());                        endEvent.addEventData("status", status.getName());                        publisher.publish(endEvent, conf);                    }                    context.getCounter("FetcherStatus", status.getName()).increment(1);                    switch(status.getCode()) {                        case ProtocolStatus.WOULDBLOCK:                                                        ((FetchItemQueues) fetchQueues).addFetchItem(fit);                            break;                        case                         ProtocolStatus.SUCCESS:                            pstatus = output(fit.url, fit.datum, content, status, CrawlDatum.STATUS_FETCH_SUCCESS, fit.outlinkDepth);                            updateStatus(content.getContent().length);                            if (pstatus != null && pstatus.isSuccess() && pstatus.getMinorCode() == ParseStatus.SUCCESS_REDIRECT) {                                String newUrl = pstatus.getMessage();                                int refreshTime = Integer.valueOf(pstatus.getArgs()[1]);                                Text redirUrl = handleRedirect(fit, newUrl, refreshTime < Fetcher.PERM_REFRESH_TIME, Fetcher.CONTENT_REDIR);                                if (redirUrl != null) {                                    fit = queueRedirect(redirUrl, fit);                                }                            }                            break;                                                case ProtocolStatus.MOVED:                        case ProtocolStatus.TEMP_MOVED:                            int code;                            boolean temp;                            if (status.getCode() == ProtocolStatus.MOVED) {                                code = CrawlDatum.STATUS_FETCH_REDIR_PERM;                                temp = false;                            } else {                                code = CrawlDatum.STATUS_FETCH_REDIR_TEMP;                                temp = true;                            }                            output(fit.url, fit.datum, content, status, code);                            String newUrl = status.getMessage();                            Text redirUrl = handleRedirect(fit, newUrl, temp, Fetcher.PROTOCOL_REDIR);                            if (redirUrl != null) {                                fit = queueRedirect(redirUrl, fit);                            } else {                                                                redirecting = false;                            }                            break;                        case ProtocolStatus.EXCEPTION:                            logError(fit.url, status.getMessage());                            int killedURLs = ((FetchItemQueues) fetchQueues).checkExceptionThreshold(fit.getQueueID());                            if (killedURLs != 0)                                context.getCounter("FetcherStatus", "AboveExceptionThresholdInQueue").increment(killedURLs);                                                case ProtocolStatus.RETRY:                        case ProtocolStatus.BLOCKED:                            output(fit.url, fit.datum, null, status, CrawlDatum.STATUS_FETCH_RETRY);                            break;                                                case ProtocolStatus.GONE:                        case ProtocolStatus.NOTFOUND:                        case ProtocolStatus.ACCESS_DENIED:                        case ProtocolStatus.ROBOTS_DENIED:                            output(fit.url, fit.datum, null, status, CrawlDatum.STATUS_FETCH_GONE);                            break;                        case ProtocolStatus.NOTMODIFIED:                            output(fit.url, fit.datum, null, status, CrawlDatum.STATUS_FETCH_NOTMODIFIED);                            break;                        default:                            if (LOG.isWarnEnabled()) {                                                            }                            output(fit.url, fit.datum, null, status, CrawlDatum.STATUS_FETCH_RETRY);                    }                    if (redirecting && redirectCount > maxRedirect) {                        ((FetchItemQueues) fetchQueues).finishFetchItem(fit);                        if (LOG.isInfoEnabled()) {                                                    }                        output(fit.url, fit.datum, null, ProtocolStatus.STATUS_REDIR_EXCEEDED, CrawlDatum.STATUS_FETCH_GONE);                    }                } while (redirecting && (redirectCount <= maxRedirect));            } catch (Throwable t) {                                                ((FetchItemQueues) fetchQueues).finishFetchItem(fit);                String message;                if (LOG.isDebugEnabled()) {                    message = StringUtils.stringifyException(t);                } else if (logUtil.logShort(t)) {                    message = t.getClass().getName();                } else {                    message = StringUtils.stringifyException(t);                }                logError(fit.url, message);                output(fit.url, fit.datum, null, ProtocolStatus.STATUS_FAILED, CrawlDatum.STATUS_FETCH_RETRY);            }        }    } catch (Throwable e) {        if (LOG.isErrorEnabled()) {                    }    } finally {        if (fit != null)            ((FetchItemQueues) fetchQueues).finishFetchItem(fit);                activeThreads.decrementAndGet();            }}
private Text nutch_f252_1(FetchItem fit, String newUrl, boolean temp, String redirType) throws MalformedURLException, URLFilterException, InterruptedException
{    if (newUrl.length() > maxOutlinkLength) {        return null;    }    newUrl = normalizers.normalize(newUrl, URLNormalizers.SCOPE_FETCHER);    newUrl = urlFilters.filter(newUrl);    String urlString = fit.url.toString();    if (newUrl == null || newUrl.equals(urlString)) {                return null;    }    if (ignoreAlsoRedirects && (ignoreExternalLinks || ignoreInternalLinks)) {        try {            URL origUrl = fit.u;            URL redirUrl = new URL(newUrl);            if (ignoreExternalLinks) {                String origHostOrDomain, newHostOrDomain;                if ("bydomain".equalsIgnoreCase(ignoreExternalLinksMode)) {                    origHostOrDomain = URLUtil.getDomainName(origUrl).toLowerCase();                    newHostOrDomain = URLUtil.getDomainName(redirUrl).toLowerCase();                } else {                                        origHostOrDomain = origUrl.getHost().toLowerCase();                    newHostOrDomain = redirUrl.getHost().toLowerCase();                }                if (!origHostOrDomain.equals(newHostOrDomain)) {                                        return null;                }            }            if (ignoreInternalLinks) {                String origHost = origUrl.getHost().toLowerCase();                String newHost = redirUrl.getHost().toLowerCase();                if (origHost.equals(newHost)) {                                        return null;                }            }        } catch (MalformedURLException e) {            return null;        }    }    reprUrl = URLUtil.chooseRepr(reprUrl, newUrl, temp);    Text url = new Text(newUrl);    if (maxRedirect > 0) {        redirecting = true;        redirectCount++;                return url;    } else {        CrawlDatum newDatum = new CrawlDatum(CrawlDatum.STATUS_LINKED, fit.datum.getFetchInterval(), fit.datum.getScore());                newDatum.getMetaData().putAll(fit.datum.getMetaData());        try {            scfilters.initialScore(url, newDatum);        } catch (ScoringFilterException e) {            e.printStackTrace();        }        if (reprUrl != null) {            newDatum.getMetaData().put(Nutch.WRITABLE_REPR_URL_KEY, new Text(reprUrl));        }        output(url, newDatum, null, null, CrawlDatum.STATUS_LINKED);                return null;    }}
private FetchItem nutch_f253_0(Text redirUrl, FetchItem fit) throws ScoringFilterException
{    CrawlDatum newDatum = new CrawlDatum(CrawlDatum.STATUS_DB_UNFETCHED, fit.datum.getFetchInterval(), fit.datum.getScore());        newDatum.getMetaData().putAll(fit.datum.getMetaData());    scfilters.initialScore(redirUrl, newDatum);    if (reprUrl != null) {        newDatum.getMetaData().put(Nutch.WRITABLE_REPR_URL_KEY, new Text(reprUrl));    }    fit = FetchItem.create(redirUrl, newDatum, queueMode);    if (fit != null) {        FetchItemQueue fiq = ((FetchItemQueues) fetchQueues).getFetchItemQueue(fit.queueID);        fiq.addInProgressFetchItem(fit);    } else {                redirecting = false;        context.getCounter("FetcherStatus", "FetchItem.notCreated.redirect").increment(1);    }    return fit;}
private void nutch_f254_1(Text url, String message)
{    if (LOG.isInfoEnabled()) {            }    errors.incrementAndGet();}
private ParseStatus nutch_f255_0(Text key, CrawlDatum datum, Content content, ProtocolStatus pstatus, int status) throws InterruptedException
{    return output(key, datum, content, pstatus, status, 0);}
private ParseStatus nutch_f256_1(Text key, CrawlDatum datum, Content content, ProtocolStatus pstatus, int status, int outlinkDepth) throws InterruptedException
{    datum.setStatus(status);    datum.setFetchTime(System.currentTimeMillis());    if (pstatus != null)        datum.getMetaData().put(Nutch.WRITABLE_PROTO_STATUS_KEY, pstatus);    ParseResult parseResult = null;    if (content != null) {        Metadata metadata = content.getMetadata();                if (content.getContentType() != null)            datum.getMetaData().put(new Text(Metadata.CONTENT_TYPE), new Text(content.getContentType()));                metadata.set(Nutch.SEGMENT_NAME_KEY, segmentName);                try {            scfilters.passScoreBeforeParsing(key, datum, content);        } catch (Exception e) {            if (LOG.isWarnEnabled()) {                            }        }        if (status == CrawlDatum.STATUS_FETCH_SUCCESS) {            if (parsing && !(skipTruncated && ParseSegment.isTruncated(content))) {                try {                    parseResult = this.parseUtil.parse(content);                } catch (Exception e) {                                    }            }            if (parseResult == null && (parsing || signatureWithoutParsing)) {                byte[] signature = SignatureFactory.getSignature(conf).calculate(content, new ParseStatus().getEmptyParse(conf));                datum.setSignature(signature);            }        }        /*       * Store status code in content So we can read this value during parsing       * (as a separate job) and decide to parse or not.       */        content.getMetadata().add(Nutch.FETCH_STATUS_KEY, Integer.toString(status));    }    try {        context.write(key, new NutchWritable(datum));        if (content != null && storingContent)            context.write(key, new NutchWritable(content));        if (parseResult != null) {            for (Entry<Text, Parse> entry : parseResult) {                Text url = entry.getKey();                Parse parse = entry.getValue();                ParseStatus parseStatus = parse.getData().getStatus();                ParseData parseData = parse.getData();                if (!parseStatus.isSuccess()) {                                        parse = parseStatus.getEmptyParse(conf);                }                                                byte[] signature = SignatureFactory.getSignature(conf).calculate(content, parse);                                parseData.getContentMeta().set(Nutch.SEGMENT_NAME_KEY, segmentName);                parseData.getContentMeta().set(Nutch.SIGNATURE_KEY, StringUtil.toHexString(signature));                                parseData.getContentMeta().set(Nutch.FETCH_TIME_KEY, Long.toString(datum.getFetchTime()));                if (url.equals(key))                    datum.setSignature(signature);                try {                    scfilters.passScoreAfterParsing(url, content, parse);                } catch (Exception e) {                    if (LOG.isWarnEnabled()) {                                            }                }                String origin = null;                                Outlink[] links = parseData.getOutlinks();                int outlinksToStore = Math.min(maxOutlinks, links.length);                if (ignoreExternalLinks || ignoreInternalLinks) {                    URL originURL = new URL(url.toString());                                        if ("bydomain".equalsIgnoreCase(ignoreExternalLinksMode)) {                        origin = URLUtil.getDomainName(originURL).toLowerCase();                    } else                     {                        origin = originURL.getHost().toLowerCase();                    }                }                                if (fetchNode != null) {                    fetchNode.setOutlinks(links);                    fetchNode.setTitle(parseData.getTitle());                    FetchNodeDb.getInstance().put(fetchNode.getUrl().toString(), fetchNode);                }                int validCount = 0;                                List<Outlink> outlinkList = new ArrayList<>(outlinksToStore);                HashSet<String> outlinks = new HashSet<>(outlinksToStore);                for (int i = 0; i < links.length && validCount < outlinksToStore; i++) {                    String toUrl = links[i].getToUrl();                    if (toUrl.length() > maxOutlinkLength) {                        continue;                    }                    toUrl = ParseOutputFormat.filterNormalize(url.toString(), toUrl, origin, ignoreInternalLinks, ignoreExternalLinks, ignoreExternalLinksMode, urlFiltersForOutlinks, urlExemptionFilters, normalizersForOutlinks);                    if (toUrl == null) {                        continue;                    }                    validCount++;                    links[i].setUrl(toUrl);                    outlinkList.add(links[i]);                    outlinks.add(toUrl);                }                                if (activatePublisher) {                    FetcherThreadEvent reportEvent = new FetcherThreadEvent(PublishEventType.REPORT, url.toString());                    reportEvent.addOutlinksToEventData(outlinkList);                    reportEvent.addEventData(Nutch.FETCH_EVENT_TITLE, parseData.getTitle());                    reportEvent.addEventData(Nutch.FETCH_EVENT_CONTENTTYPE, parseData.getContentMeta().get("content-type"));                    reportEvent.addEventData(Nutch.FETCH_EVENT_SCORE, datum.getScore());                    reportEvent.addEventData(Nutch.FETCH_EVENT_FETCHTIME, datum.getFetchTime());                    reportEvent.addEventData(Nutch.FETCH_EVENT_CONTENTLANG, parseData.getContentMeta().get("content-language"));                    publisher.publish(reportEvent, conf);                }                                if (maxOutlinkDepth > 0 && outlinkDepth < maxOutlinkDepth) {                    FetchItem ft = FetchItem.create(url, null, queueMode);                    FetchItemQueue queue = ((FetchItemQueues) fetchQueues).getFetchItemQueue(ft.queueID);                    queue.alreadyFetched.add(url.toString().hashCode());                    context.getCounter("FetcherOutlinks", "outlinks_detected").increment(outlinks.size());                                        int outlinkCounter = 0;                    String followUrl;                                        Iterator<String> iter = outlinks.iterator();                    while (iter.hasNext() && outlinkCounter < maxOutlinkDepthNumLinks) {                        followUrl = iter.next();                                                if (outlinksIgnoreExternal) {                            if (!URLUtil.getHost(url.toString()).equals(URLUtil.getHost(followUrl))) {                                continue;                            }                        }                                                int urlHashCode = followUrl.hashCode();                        if (queue.alreadyFetched.contains(urlHashCode)) {                            continue;                        }                        queue.alreadyFetched.add(urlHashCode);                                                FetchItem fit = FetchItem.create(new Text(followUrl), new CrawlDatum(CrawlDatum.STATUS_LINKED, interval), queueMode, outlinkDepth + 1);                        context.getCounter("FetcherOutlinks", "outlinks_following").increment(1);                        ((FetchItemQueues) fetchQueues).addFetchItem(fit);                        outlinkCounter++;                    }                }                                                parseData.setOutlinks(outlinkList.toArray(new Outlink[outlinkList.size()]));                context.write(url, new NutchWritable(new ParseImpl(new ParseText(parse.getText()), parseData, parse.isCanonical())));            }        }    } catch (IOException e) {        if (LOG.isErrorEnabled()) {                    }    }        if (parseResult != null && !parseResult.isEmpty()) {        Parse p = parseResult.get(content.getUrl());        if (p != null) {            context.getCounter("ParserStatus", ParseStatus.majorCodes[p.getData().getStatus().getMajorCode()]).increment(1);            return p.getData().getStatus();        }    }    return null;}
private void nutch_f257_1(List<Content> robotsTxtContent) throws InterruptedException
{    for (Content robotsTxt : robotsTxtContent) {                try {            context.write(new Text(robotsTxt.getUrl()), new NutchWritable(robotsTxt));        } catch (IOException e) {                    }    }}
private void nutch_f258_0(int bytesInPage) throws IOException
{    pages.incrementAndGet();    bytes.addAndGet(bytesInPage);}
public synchronized void nutch_f259_0(boolean halted)
{    this.halted = halted;}
public synchronized boolean nutch_f260_0()
{    return halted;}
public String nutch_f261_0()
{    return reprUrl;}
private void nutch_f262_0(String urlString)
{    this.reprUrl = urlString;}
public PublishEventType nutch_f263_0()
{    return eventType;}
public void nutch_f264_0(PublishEventType eventType)
{    this.eventType = eventType;}
public Map<String, Object> nutch_f265_0()
{    return eventData;}
public void nutch_f266_0(Map<String, Object> eventData)
{    this.eventData = eventData;}
public String nutch_f267_0()
{    return url;}
public void nutch_f268_0(String url)
{    this.url = url;}
public void nutch_f269_0(String key, Object value)
{    if (eventData == null) {        eventData = new HashMap<>();    }    eventData.put(key, value);}
public void nutch_f270_0(Collection<Outlink> links)
{    ArrayList<Map<String, String>> outlinkList = new ArrayList<>();    for (Outlink link : links) {        Map<String, String> outlink = new HashMap<>();        outlink.put("url", link.getToUrl());        outlink.put("anchor", link.getAnchor());        outlinkList.add(outlink);    }    this.addEventData("outlinks", outlinkList);}
public Long nutch_f271_0()
{    return timestamp;}
public void nutch_f272_0(Long timestamp)
{    this.timestamp = timestamp;}
public void nutch_f273_1(FetcherThreadEvent event, Configuration conf)
{    if (publisher != null) {        publisher.publish(event, conf);    } else {            }}
public static FetchItem nutch_f274_0(Text url, CrawlDatum datum, String queueMode)
{    return create(url, datum, queueMode, 0);}
public static FetchItem nutch_f275_1(Text url, CrawlDatum datum, String queueMode, int outlinkDepth)
{    URL u = null;    try {        u = new URL(url.toString());    } catch (Exception e) {                return null;    }    String key;    if (FetchItemQueues.QUEUE_MODE_IP.equalsIgnoreCase(queueMode)) {        try {            final InetAddress addr = InetAddress.getByName(u.getHost());            key = addr.getHostAddress();        } catch (final UnknownHostException e) {                                    return null;        }    } else if (FetchItemQueues.QUEUE_MODE_DOMAIN.equalsIgnoreCase(queueMode)) {        key = URLUtil.getDomainName(u).toLowerCase(Locale.ROOT);        if (key == null) {                        key = u.toExternalForm();        }    } else {        key = u.getHost().toLowerCase(Locale.ROOT);        if (key == null) {                        key = u.toExternalForm();        }    }    return new FetchItem(url, u, datum, key, outlinkDepth);}
public CrawlDatum nutch_f276_0()
{    return datum;}
public String nutch_f277_0()
{    return queueID;}
public Text nutch_f278_0()
{    return url;}
public URL nutch_f279_0()
{    return u;}
public synchronized int nutch_f280_0()
{    int presize = queue.size();    queue.clear();    return presize;}
public int nutch_f281_0()
{    return queue.size();}
public int nutch_f282_0()
{    return inProgress.get();}
public int nutch_f283_0()
{    return exceptionCounter.incrementAndGet();}
public void nutch_f284_0(FetchItem it, boolean asap)
{    if (it != null) {        inProgress.decrementAndGet();        setEndTime(System.currentTimeMillis(), asap);    }}
public void nutch_f285_0(FetchItem it)
{    if (it == null)        return;        if (it.datum.getMetaData().containsKey(variableFetchDelayKey)) {        if (!variableFetchDelaySet) {            variableFetchDelaySet = true;            crawlDelay = ((LongWritable) (it.datum.getMetaData().get(variableFetchDelayKey))).get();            minCrawlDelay = ((LongWritable) (it.datum.getMetaData().get(variableFetchDelayKey))).get();            setEndTime(System.currentTimeMillis() - crawlDelay);        }                it.datum.getMetaData().remove(variableFetchDelayKey);    }    queue.add(it);}
public void nutch_f286_0(FetchItem it)
{    if (it == null)        return;    inProgress.incrementAndGet();}
public FetchItem nutch_f287_1()
{    if (inProgress.get() >= maxThreads)        return null;    long now = System.currentTimeMillis();    if (nextFetchTime.get() > now)        return null;    FetchItem it = null;    if (queue.size() == 0)        return null;    try {        it = queue.remove(0);        inProgress.incrementAndGet();    } catch (Exception e) {            }    return it;}
public void nutch_f288_0(Text cookie)
{    this.cookie = cookie;}
public Text nutch_f289_0()
{    return cookie;}
public synchronized void nutch_f290_1()
{                            for (int i = 0; i < queue.size(); i++) {        FetchItem it = queue.get(i);            }}
private void nutch_f291_0(long endTime)
{    setEndTime(endTime, false);}
private void nutch_f292_0(long endTime, boolean asap)
{    if (!asap)        nextFetchTime.set(endTime + (maxThreads > 1 ? minCrawlDelay : crawlDelay));    else        nextFetchTime.set(endTime);}
protected static String nutch_f293_1(String queueMode)
{        if (!queueMode.equals(QUEUE_MODE_IP) && !queueMode.equals(QUEUE_MODE_DOMAIN) && !queueMode.equals(QUEUE_MODE_HOST)) {                queueMode = QUEUE_MODE_HOST;    }    return queueMode;}
public int nutch_f294_0()
{    return totalSize.get();}
public int nutch_f295_0()
{    return queues.size();}
public void nutch_f296_0(Text url, CrawlDatum datum)
{    FetchItem it = FetchItem.create(url, datum, queueMode);    if (it != null)        addFetchItem(it);}
public synchronized void nutch_f297_0(FetchItem it)
{    FetchItemQueue fiq = getFetchItemQueue(it.queueID);    fiq.addFetchItem(it);    totalSize.incrementAndGet();}
public void nutch_f298_0(FetchItem it)
{    finishFetchItem(it, false);}
public void nutch_f299_1(FetchItem it, boolean asap)
{    FetchItemQueue fiq = queues.get(it.queueID);    if (fiq == null) {                return;    }    fiq.finishFetchItem(it, asap);}
public synchronized FetchItemQueue nutch_f300_0(String id)
{    FetchItemQueue fiq = queues.get(id);    if (fiq == null) {                fiq = new FetchItemQueue(conf, maxThreads, crawlDelay, minCrawlDelay);        queues.put(id, fiq);    }    return fiq;}
public synchronized FetchItem nutch_f301_0()
{    Iterator<Map.Entry<String, FetchItemQueue>> it = queues.entrySet().iterator();    while (it.hasNext()) {        FetchItemQueue fiq = it.next().getValue();                if (fiq.getQueueSize() == 0 && fiq.getInProgressSize() == 0) {            it.remove();            continue;        }        FetchItem fit = fiq.getFetchItem();        if (fit != null) {            totalSize.decrementAndGet();            return fit;        }    }    return null;}
public synchronized int nutch_f302_0()
{    int count = 0;    if (System.currentTimeMillis() >= timelimit && timelimit != -1) {                count = emptyQueues();                if (totalSize.get() != 0 && queues.size() == 0)            totalSize.set(0);    }    return count;}
public synchronized int nutch_f303_1()
{    int count = 0;    for (String id : queues.keySet()) {        FetchItemQueue fiq = queues.get(id);        if (fiq.getQueueSize() == 0)            continue;                int deleted = fiq.emptyQueue();        for (int i = 0; i < deleted; i++) {            totalSize.decrementAndGet();        }        count += deleted;    }    return count;}
public synchronized int nutch_f304_1(String queueid)
{    FetchItemQueue fiq = queues.get(queueid);    if (fiq == null) {        return 0;    }    if (fiq.getQueueSize() == 0) {        return 0;    }    int excCount = fiq.incrementExceptionCounter();    if (maxExceptionsPerQueue != -1 && excCount >= maxExceptionsPerQueue) {                int deleted = fiq.emptyQueue();                for (int i = 0; i < deleted; i++) {            totalSize.decrementAndGet();        }        return deleted;    }    return 0;}
public synchronized void nutch_f305_1()
{    for (String id : queues.keySet()) {        FetchItemQueue fiq = queues.get(id);        if (fiq.getQueueSize() == 0)            continue;                fiq.dump();    }}
public Text nutch_f306_0()
{    return url;}
public void nutch_f307_0(Text url)
{    this.url = url;}
public Outlink[] nutch_f308_0()
{    return outlinks;}
public void nutch_f309_0(Outlink[] links)
{    this.outlinks = links;}
public int nutch_f310_0()
{    return status;}
public void nutch_f311_0(int status)
{    this.status = status;}
public String nutch_f312_0()
{    return title;}
public void nutch_f313_0(String title)
{    this.title = title;}
public long nutch_f314_0()
{    return fetchTime;}
public void nutch_f315_0(long fetchTime)
{    this.fetchTime = fetchTime;}
public static FetchNodeDb nutch_f316_0()
{    if (fetchNodeDbInstance == null) {        fetchNodeDbInstance = new FetchNodeDb();    }    return fetchNodeDbInstance;}
public void nutch_f317_0(String url, FetchNode fetchNode)
{    System.out.println("FetchNodeDb : putting node - " + fetchNode.hashCode());    fetchNodeDbMap.put(index++, fetchNode);}
public Map<Integer, FetchNode> nutch_f318_0()
{    return fetchNodeDbMap;}
public void nutch_f319_0(long tl)
{    timelimit = tl;}
private String nutch_f320_1(String url)
{    if (url != null) {        try {            if (urlNormalizers != null)                                url = urlNormalizers.normalize(url, urlNormalizerScope);            if (urlFilters != null)                url = urlFilters.filter(url);        } catch (MalformedURLException | URLFilterException e) {                        url = null;        }    }    return url;}
public void nutch_f321_1()
{    boolean hasMore = true;    int cnt = 0;    int timelimitcount = 0;    while (hasMore) {        if (System.currentTimeMillis() >= timelimit && timelimit != -1) {                        try {                hasMore = context.nextKeyValue();                timelimitcount++;            } catch (IOException e) {                                return;            } catch (InterruptedException e) {                                return;            }            continue;        }        int feed = size - queues.getTotalSize();        if (feed <= 0) {                        try {                Thread.sleep(1000);            } catch (InterruptedException e) {            }            continue;        }                while (feed > 0 && hasMore) {            try {                hasMore = context.nextKeyValue();                if (hasMore) {                    Text url = context.getCurrentKey();                    if (urlFilters != null || urlNormalizers != null) {                        String u = filterNormalize(url.toString());                        if (u == null) {                                                        context.getCounter("FetcherStatus", "filtered").increment(1);                            continue;                        }                        url = new Text(u);                    } else /*             * Need to copy key and value objects because MapReduce will reuse             * the original objects while the objects are stored in the queue.             */                    {                        url = new Text(url);                    }                    CrawlDatum datum = new CrawlDatum();                    datum.set((CrawlDatum) context.getCurrentValue());                    queues.addFetchItem(url, datum);                    cnt++;                    feed--;                }            } catch (IOException e) {                                return;            } catch (InterruptedException e) {                            }        }    }    }
public void nutch_f322_0()
{    setDnsFailures(0l);    setConnectionFailures(0l);}
public void nutch_f323_0(Long dnsFailures)
{    this.dnsFailures = dnsFailures;}
public void nutch_f324_0(Long connectionFailures)
{    this.connectionFailures = connectionFailures;}
public void nutch_f325_0()
{    this.dnsFailures++;}
public void nutch_f326_0()
{    this.connectionFailures++;}
public Long nutch_f327_0()
{    return getDnsFailures() + getConnectionFailures();}
public Long nutch_f328_0()
{    return dnsFailures;}
public Long nutch_f329_0()
{    return connectionFailures;}
public void nutch_f330_0(float score)
{    this.score = score;}
public void nutch_f331_0()
{    setLastCheck(new Date());}
public void nutch_f332_0(Date date)
{    lastCheck = date;}
public boolean nutch_f333_0()
{    return (lastCheck.getTime() == 0) ? true : false;}
public float nutch_f334_0()
{    return score;}
public Long nutch_f335_0()
{    return unfetched + fetched + gone + redirPerm + redirTemp + notModified;}
public Date nutch_f336_0()
{    return lastCheck;}
public boolean nutch_f337_0()
{    return homepageUrl.length() > 0;}
public String nutch_f338_0()
{    return homepageUrl;}
public void nutch_f339_0(String homepageUrl)
{    this.homepageUrl = homepageUrl;}
public void nutch_f340_0(long val)
{    unfetched = val;}
public long nutch_f341_0()
{    return unfetched;}
public void nutch_f342_0(long val)
{    fetched = val;}
public long nutch_f343_0()
{    return fetched;}
public void nutch_f344_0(long val)
{    notModified = val;}
public long nutch_f345_0()
{    return notModified;}
public void nutch_f346_0(long val)
{    redirTemp = val;}
public long nutch_f347_0()
{    return redirTemp;}
public void nutch_f348_0(long val)
{    redirPerm = val;}
public long nutch_f349_0()
{    return redirPerm;}
public void nutch_f350_0(long val)
{    gone = val;}
public long nutch_f351_0()
{    return gone;}
public void nutch_f352_0()
{    setUnfetched(0);    setFetched(0);    setGone(0);    setRedirTemp(0);    setRedirPerm(0);    setNotModified(0);}
public void nutch_f353_0(org.apache.hadoop.io.MapWritable mapWritable)
{    this.metaData = new org.apache.hadoop.io.MapWritable(mapWritable);}
public void nutch_f354_0(HostDatum other)
{    for (Entry<Writable, Writable> e : other.getMetaData().entrySet()) {        getMetaData().put(e.getKey(), e.getValue());    }}
public org.apache.hadoop.io.MapWritable nutch_f355_0()
{    if (this.metaData == null)        this.metaData = new org.apache.hadoop.io.MapWritable();    return this.metaData;}
public Object nutch_f356_0() throws CloneNotSupportedException
{    HostDatum result = (HostDatum) super.clone();    result.score = score;    result.lastCheck = lastCheck;    result.homepageUrl = homepageUrl;    result.dnsFailures = dnsFailures;    result.connectionFailures = connectionFailures;    result.unfetched = unfetched;    result.fetched = fetched;    result.notModified = notModified;    result.redirTemp = redirTemp;    result.redirPerm = redirPerm;    result.gone = gone;    result.metaData = metaData;    return result;}
public void nutch_f357_0(DataInput in) throws IOException
{    score = in.readFloat();    lastCheck = new Date(in.readLong());    homepageUrl = Text.readString(in);    dnsFailures = in.readLong();    connectionFailures = in.readLong();    unfetched = in.readLong();    fetched = in.readLong();    notModified = in.readLong();    redirTemp = in.readLong();    redirPerm = in.readLong();    gone = in.readLong();    metaData = new org.apache.hadoop.io.MapWritable();    metaData.readFields(in);}
public void nutch_f358_0(DataOutput out) throws IOException
{    out.writeFloat(score);    out.writeLong(lastCheck.getTime());    Text.writeString(out, homepageUrl);    out.writeLong(dnsFailures);    out.writeLong(connectionFailures);    out.writeLong(unfetched);    out.writeLong(fetched);    out.writeLong(notModified);    out.writeLong(redirTemp);    out.writeLong(redirPerm);    out.writeLong(gone);    metaData.write(out);}
public String nutch_f359_0()
{    StringBuilder buf = new StringBuilder();    buf.append(Long.toString(getUnfetched()));    buf.append("\t");    buf.append(Long.toString(getFetched()));    buf.append("\t");    buf.append(Long.toString(getGone()));    buf.append("\t");    buf.append(Long.toString(getRedirTemp()));    buf.append("\t");    buf.append(Long.toString(getRedirPerm()));    buf.append("\t");    buf.append(Long.toString(getNotModified()));    buf.append("\t");    buf.append(Long.toString(numRecords()));    buf.append("\t");    buf.append(Long.toString(getDnsFailures()));    buf.append("\t");    buf.append(Long.toString(getConnectionFailures()));    buf.append("\t");    buf.append(Long.toString(numFailures()));    buf.append("\t");    buf.append(Float.toString(score));    buf.append("\t");    buf.append(new SimpleDateFormat("yyyy-MM-dd HH:mm:ss").format(lastCheck));    buf.append("\t");    buf.append(homepageUrl);    buf.append("\t");    for (Entry<Writable, Writable> e : getMetaData().entrySet()) {        buf.append(e.getKey().toString());        buf.append(':');        buf.append(e.getValue().toString());        buf.append("|||");    }    return buf.toString();}
public void nutch_f360_0(Context context)
{    dumpHomepages = context.getConfiguration().getBoolean(HOSTDB_DUMP_HOMEPAGES, false);    dumpHostnames = context.getConfiguration().getBoolean(HOSTDB_DUMP_HOSTNAMES, false);    fieldHeader = context.getConfiguration().getBoolean(HOSTDB_DUMP_HEADER, true);    String expr = context.getConfiguration().get(HOSTDB_FILTER_EXPRESSION);    if (expr != null) {                JexlEngine jexl = new JexlEngine();                jexl.setSilent(true);        jexl.setStrict(true);                this.expr = jexl.createExpression(expr);    }}
public void nutch_f361_1(Text key, HostDatum datum, Context context) throws IOException, InterruptedException
{    if (fieldHeader && !dumpHomepages && !dumpHostnames) {        context.write(new Text("hostname"), new Text("unfetched\tfetched\tgone\tredirTemp\tredirPerm\tnotModified\tnumRecords\tdnsFail\tcnxFail\tsumFail\tscore\tlastCheck\thomepage\tmetadata"));        fieldHeader = false;    }    if (expr != null) {                JexlContext jcontext = new MapContext();                jcontext.set("unfetched", datum.getUnfetched());        jcontext.set("fetched", datum.getFetched());        jcontext.set("gone", datum.getGone());        jcontext.set("redirTemp", datum.getRedirTemp());        jcontext.set("redirPerm", datum.getRedirPerm());        jcontext.set("redirs", datum.getRedirPerm() + datum.getRedirTemp());        jcontext.set("notModified", datum.getNotModified());        jcontext.set("ok", datum.getFetched() + datum.getNotModified());        jcontext.set("numRecords", datum.numRecords());        jcontext.set("dnsFailures", datum.getDnsFailures());        jcontext.set("connectionFailures", datum.getConnectionFailures());                for (Map.Entry<Writable, Writable> entry : datum.getMetaData().entrySet()) {            Object value = entry.getValue();            if (value instanceof FloatWritable) {                FloatWritable fvalue = (FloatWritable) value;                Text tkey = (Text) entry.getKey();                jcontext.set(tkey.toString(), fvalue.get());            }            if (value instanceof IntWritable) {                IntWritable ivalue = (IntWritable) value;                Text tkey = (Text) entry.getKey();                jcontext.set(tkey.toString(), ivalue.get());            }        }                try {            if (!Boolean.TRUE.equals(expr.evaluate(jcontext))) {                return;            }        } catch (Exception e) {                    }    }    if (dumpHomepages) {        if (datum.hasHomepageUrl()) {            context.write(new Text(datum.getHomepageUrl()), emptyText);        }        return;    }    if (dumpHostnames) {        context.write(key, emptyText);        return;    }        context.write(key, new Text(datum.toString()));}
private void nutch_f362_1(Path hostDb, Path output, boolean dumpHomepages, boolean dumpHostnames, String expr) throws Exception
{    SimpleDateFormat sdf = new SimpleDateFormat("yyyy-MM-dd HH:mm:ss");    long start = System.currentTimeMillis();        Configuration conf = getConf();    conf.setBoolean(HOSTDB_DUMP_HOMEPAGES, dumpHomepages);    conf.setBoolean(HOSTDB_DUMP_HOSTNAMES, dumpHostnames);    if (expr != null) {        conf.set(HOSTDB_FILTER_EXPRESSION, expr);    }    conf.setBoolean("mapreduce.fileoutputcommitter.marksuccessfuljobs", false);    conf.set("mapreduce.output.textoutputformat.separator", "\t");    Job job = Job.getInstance(conf);    job.setJobName("ReadHostDb");    job.setJarByClass(ReadHostDb.class);    FileInputFormat.addInputPath(job, new Path(hostDb, "current"));    FileOutputFormat.setOutputPath(job, output);    job.setMapperClass(ReadHostDbMapper.class);    job.setInputFormatClass(SequenceFileInputFormat.class);    job.setOutputFormatClass(TextOutputFormat.class);    job.setMapOutputKeyClass(Text.class);    job.setMapOutputValueClass(Text.class);    job.setOutputKeyClass(Text.class);    job.setOutputValueClass(Text.class);    job.setNumReduceTasks(0);    try {        boolean success = job.waitForCompletion(true);        if (!success) {            String message = "ReadHostDb job did not succeed, job status: " + job.getStatus().getState() + ", reason: " + job.getStatus().getFailureInfo();                                    throw new RuntimeException(message);        }    } catch (IOException | InterruptedException | ClassNotFoundException e) {                throw e;    }    long end = System.currentTimeMillis();    }
private void nutch_f363_0(Path hostDb, String host) throws Exception
{    Configuration conf = getConf();    SequenceFile.Reader[] readers = SegmentReaderUtil.getReaders(hostDb, conf);    Class<?> keyClass = readers[0].getKeyClass();    Class<?> valueClass = readers[0].getValueClass();    if (!keyClass.getName().equals("org.apache.hadoop.io.Text"))        throw new IOException("Incompatible key (" + keyClass.getName() + ")");    Text key = (Text) keyClass.getConstructor().newInstance();    HostDatum value = (HostDatum) valueClass.getConstructor().newInstance();    for (int i = 0; i < readers.length; i++) {        while (readers[i].next(key, value)) {            if (host.equals(key.toString())) {                System.out.println(value.toString());            }        }        readers[i].close();    }}
public static void nutch_f364_0(String[] args) throws Exception
{    int res = ToolRunner.run(NutchConfiguration.create(), new ReadHostDb(), args);    System.exit(res);}
public int nutch_f365_1(String[] args) throws Exception
{    if (args.length < 2) {        System.err.println("Usage: ReadHostDb <hostdb> [-get <url>] [<output> [-dumpHomepages | -dumpHostnames | -expr <expr.>]]");        return -1;    }    boolean dumpHomepages = false;    boolean dumpHostnames = false;    String expr = null;    String get = null;    for (int i = 0; i < args.length; i++) {        if (args[i].equals("-dumpHomepages")) {                        dumpHomepages = true;        }        if (args[i].equals("-dumpHostnames")) {                        dumpHostnames = true;        }        if (args[i].equals("-get")) {            get = args[i + 1];                        i++;        }        if (args[i].equals("-expr")) {            expr = args[i + 1];                        i++;        }    }    try {        if (get != null) {            getHostDbRecord(new Path(args[0], "current"), get);        } else {            readHostDb(new Path(args[0]), new Path(args[1]), dumpHomepages, dumpHostnames, expr);        }        return 0;    } catch (Exception e) {                return -1;    }}
public void nutch_f366_1()
{        try {                @SuppressWarnings("unused")        InetAddress inetAddr = InetAddress.getByName(host);        if (datum.isEmpty()) {            context.getCounter("UpdateHostDb", "new_known_host").increment(1);            datum.setLastCheck();                    } else if (datum.getDnsFailures() > 0) {            context.getCounter("UpdateHostDb", "rediscovered_host").increment(1);            datum.setLastCheck();            datum.setDnsFailures(0l);                    } else {            context.getCounter("UpdateHostDb", "existing_known_host").increment(1);            datum.setLastCheck();                    }                context.write(hostText, datum);    } catch (UnknownHostException e) {        try {                        if (datum.isEmpty()) {                datum.setLastCheck();                datum.setDnsFailures(1l);                context.write(hostText, datum);                context.getCounter("UpdateHostDb", "new_unknown_host").increment(1);                            } else {                datum.setLastCheck();                datum.incDnsFailures();                                if (purgeFailedHostsThreshold == -1 || purgeFailedHostsThreshold < datum.getDnsFailures()) {                    context.write(hostText, datum);                    context.getCounter("UpdateHostDb", "existing_unknown_host").increment(1);                                    } else {                    context.getCounter("UpdateHostDb", "purged_unknown_host").increment(1);                                    }            }            context.getCounter("UpdateHostDb", Long.toString(datum.numFailures()) + "_times_failed").increment(1);        } catch (Exception ioe) {                    }    } catch (Exception e) {            }    context.getCounter("UpdateHostDb", "checked_hosts").increment(1);}
private void nutch_f367_1(Path hostDb, Path crawlDb, Path topHosts, boolean checkFailed, boolean checkNew, boolean checkKnown, boolean force, boolean filter, boolean normalize) throws Exception
{    SimpleDateFormat sdf = new SimpleDateFormat("yyyy-MM-dd HH:mm:ss");    long start = System.currentTimeMillis();        Job job = NutchJob.getInstance(getConf());    Configuration conf = job.getConfiguration();    boolean preserveBackup = conf.getBoolean("db.preserve.backup", true);    job.setJarByClass(UpdateHostDb.class);    job.setJobName("UpdateHostDb");    FileSystem fs = hostDb.getFileSystem(conf);    Path old = new Path(hostDb, "old");    Path current = new Path(hostDb, "current");    Path tempHostDb = new Path(hostDb, "hostdb-" + Integer.toString(new Random().nextInt(Integer.MAX_VALUE)));        Path lock = new Path(hostDb, LOCK_NAME);    if (!fs.exists(current)) {        fs.mkdirs(current);    }    LockUtil.createLockFile(fs, lock, false);    MultipleInputs.addInputPath(job, current, SequenceFileInputFormat.class);    if (topHosts != null) {        MultipleInputs.addInputPath(job, topHosts, KeyValueTextInputFormat.class);    }    if (crawlDb != null) {                conf.setBoolean("hostdb.reading.crawldb", true);        MultipleInputs.addInputPath(job, new Path(crawlDb, CrawlDb.CURRENT_NAME), SequenceFileInputFormat.class);    }    FileOutputFormat.setOutputPath(job, tempHostDb);    job.setOutputFormatClass(SequenceFileOutputFormat.class);    job.setMapOutputKeyClass(Text.class);    job.setMapOutputValueClass(NutchWritable.class);    job.setOutputKeyClass(Text.class);    job.setOutputValueClass(HostDatum.class);    job.setMapperClass(UpdateHostDbMapper.class);    job.setReducerClass(UpdateHostDbReducer.class);    job.setSpeculativeExecution(false);    conf.setBoolean("mapreduce.fileoutputcommitter.marksuccessfuljobs", false);    conf.setBoolean(HOSTDB_CHECK_FAILED, checkFailed);    conf.setBoolean(HOSTDB_CHECK_NEW, checkNew);    conf.setBoolean(HOSTDB_CHECK_KNOWN, checkKnown);    conf.setBoolean(HOSTDB_FORCE_CHECK, force);    conf.setBoolean(HOSTDB_URL_FILTERING, filter);    conf.setBoolean(HOSTDB_URL_NORMALIZING, normalize);    conf.setClassLoader(Thread.currentThread().getContextClassLoader());    try {        boolean success = job.waitForCompletion(true);        if (!success) {            String message = "UpdateHostDb job did not succeed, job status:" + job.getStatus().getState() + ", reason: " + job.getStatus().getFailureInfo();                        NutchJob.cleanupAfterFailure(tempHostDb, lock, fs);            throw new RuntimeException(message);        }        FSUtils.replace(fs, old, current, true);        FSUtils.replace(fs, current, tempHostDb, true);        if (!preserveBackup && fs.exists(old))            fs.delete(old, true);    } catch (Exception e) {                NutchJob.cleanupAfterFailure(tempHostDb, lock, fs);        throw e;    }    LockUtil.removeLockFile(fs, lock);    long end = System.currentTimeMillis();    }
public static void nutch_f368_0(String[] args) throws Exception
{    int res = ToolRunner.run(NutchConfiguration.create(), new UpdateHostDb(), args);    System.exit(res);}
public int nutch_f369_1(String[] args) throws Exception
{    if (args.length < 2) {        System.err.println("Usage: UpdateHostDb -hostdb <hostdb> " + "[-tophosts <tophosts>] [-crawldb <crawldb>] [-checkAll] [-checkFailed]" + " [-checkNew] [-checkKnown] [-force] [-filter] [-normalize]");        return -1;    }    Path hostDb = null;    Path crawlDb = null;    Path topHosts = null;    boolean checkFailed = false;    boolean checkNew = false;    boolean checkKnown = false;    boolean force = false;    boolean filter = false;    boolean normalize = false;    for (int i = 0; i < args.length; i++) {        if (args[i].equals("-hostdb")) {            hostDb = new Path(args[i + 1]);                        i++;        }        if (args[i].equals("-crawldb")) {            crawlDb = new Path(args[i + 1]);                        i++;        }        if (args[i].equals("-tophosts")) {            topHosts = new Path(args[i + 1]);                        i++;        }        if (args[i].equals("-checkFailed")) {                        checkFailed = true;        }        if (args[i].equals("-checkNew")) {                        checkNew = true;        }        if (args[i].equals("-checkKnown")) {                        checkKnown = true;        }        if (args[i].equals("-checkAll")) {                        checkFailed = true;            checkNew = true;            checkKnown = true;        }        if (args[i].equals("-force")) {                        force = true;        }        if (args[i].equals("-filter")) {                        filter = true;        }        if (args[i].equals("-normalize")) {                        normalize = true;        }    }    if (hostDb == null) {        System.err.println("hostDb is mandatory");        return -1;    }    try {        updateHostDb(hostDb, crawlDb, topHosts, checkFailed, checkNew, checkKnown, force, filter, normalize);        return 0;    } catch (Exception e) {                return -1;    }}
public void nutch_f370_0(Mapper<Text, Writable, Text, NutchWritable>.Context context)
{    Configuration conf = context.getConfiguration();    readingCrawlDb = conf.getBoolean("hostdb.reading.crawldb", false);    filter = conf.getBoolean(UpdateHostDb.HOSTDB_URL_FILTERING, false);    normalize = conf.getBoolean(UpdateHostDb.HOSTDB_URL_NORMALIZING, false);    if (filter)        filters = new URLFilters(conf);    if (normalize)        normalizers = new URLNormalizers(conf, URLNormalizers.SCOPE_DEFAULT);}
protected String nutch_f371_0(String url)
{                url = "http://" + url + "/";    try {        if (normalize)            url = normalizers.normalize(url, URLNormalizers.SCOPE_DEFAULT);        if (filter)            url = filters.filter(url);        if (url == null)            return null;    } catch (Exception e) {        return null;    }        return URLUtil.getHost(url);}
public void nutch_f372_1(Text key, Writable value, Context context) throws IOException, InterruptedException
{        String keyStr = key.toString();        if (key instanceof Text && value instanceof CrawlDatum) {                buffer = filterNormalize(URLUtil.getHost(keyStr));                if (buffer == null) {            context.getCounter("UpdateHostDb", "filtered_records").increment(1);                        return;        }                host.set(buffer);        crawlDatum = (CrawlDatum) value;        hostDatum = new HostDatum();                if (crawlDatum.getStatus() != CrawlDatum.STATUS_DB_UNFETCHED) {                        String protocol = URLUtil.getProtocol(keyStr);                        String homepage = protocol + "://" + buffer + "/";                        if (keyStr.equals(homepage)) {                                if (crawlDatum.getStatus() == CrawlDatum.STATUS_DB_REDIR_PERM || crawlDatum.getStatus() == CrawlDatum.STATUS_DB_REDIR_TEMP) {                                        ProtocolStatus z = (ProtocolStatus) crawlDatum.getMetaData().get(Nutch.WRITABLE_PROTO_STATUS_KEY);                                        args = z.getArgs();                                        reprUrl = args[0];                                        if (reprUrl != null) {                                                context.write(host, new NutchWritable(hostDatum));                        hostDatum.setHomepageUrl(reprUrl);                    } else {                                            }                } else {                    hostDatum.setHomepageUrl(homepage);                    context.write(host, new NutchWritable(hostDatum));                                    }            }        }                context.write(host, new NutchWritable(crawlDatum));    }        if (key instanceof Text && value instanceof HostDatum) {        buffer = filterNormalize(keyStr);                if (buffer == null) {            context.getCounter("UpdateHostDb", "filtered_records").increment(1);                        return;        }                hostDatum = (HostDatum) value;        key.set(buffer);                if (readingCrawlDb) {            hostDatum.resetStatistics();        }        context.write(key, new NutchWritable(hostDatum));    }        if (key instanceof Text && value instanceof Text) {        buffer = filterNormalize(keyStr);                if (buffer == null) {            context.getCounter("UpdateHostDb", "filtered_records").increment(1);                        return;        }        key.set(buffer);        context.write(key, new NutchWritable(new FloatWritable(Float.parseFloat(value.toString()))));    }}
public void nutch_f373_0(Reducer<Text, NutchWritable, Text, HostDatum>.Context context)
{    Configuration conf = context.getConfiguration();    purgeFailedHostsThreshold = conf.getInt(UpdateHostDb.HOSTDB_PURGE_FAILED_HOSTS_THRESHOLD, -1);    numResolverThreads = conf.getInt(UpdateHostDb.HOSTDB_NUM_RESOLVER_THREADS, 10);    recheckInterval = conf.getInt(UpdateHostDb.HOSTDB_RECHECK_INTERVAL, 86400) * 1000;    checkFailed = conf.getBoolean(UpdateHostDb.HOSTDB_CHECK_FAILED, false);    checkNew = conf.getBoolean(UpdateHostDb.HOSTDB_CHECK_NEW, false);    checkKnown = conf.getBoolean(UpdateHostDb.HOSTDB_CHECK_KNOWN, false);    force = conf.getBoolean(UpdateHostDb.HOSTDB_FORCE_CHECK, false);    numericFields = conf.getStrings(UpdateHostDb.HOSTDB_NUMERIC_FIELDS);    stringFields = conf.getStrings(UpdateHostDb.HOSTDB_STRING_FIELDS);    percentiles = conf.getInts(UpdateHostDb.HOSTDB_PERCENTILES);        if (numericFields != null) {        numericFieldWritables = new Text[numericFields.length];        for (int i = 0; i < numericFields.length; i++) {            numericFieldWritables[i] = new Text(numericFields[i]);        }    }    if (stringFields != null) {        stringFieldWritables = new Text[stringFields.length];        for (int i = 0; i < stringFields.length; i++) {            stringFieldWritables[i] = new Text(stringFields[i]);        }    }        executor = new ThreadPoolExecutor(numResolverThreads, numResolverThreads, 5, TimeUnit.SECONDS, queue);        executor.prestartAllCoreThreads();}
public void nutch_f374_1(Text key, Iterable<NutchWritable> values, Context context) throws IOException, InterruptedException
{    Map<String, Map<String, Long>> stringCounts = new HashMap<>();    Map<String, Float> maximums = new HashMap<>();        Map<String, Float> sums = new HashMap<>();        Map<String, Long> counts = new HashMap<>();    Map<String, Float> minimums = new HashMap<>();    Map<String, TDigest> tdigests = new HashMap<String, TDigest>();    HostDatum hostDatum = new HostDatum();    float score = 0;    if (stringFields != null) {        for (int i = 0; i < stringFields.length; i++) {            stringCounts.put(stringFields[i], new HashMap<>());        }    }        for (NutchWritable val : values) {                final Writable value = val.get();                if (value instanceof CrawlDatum) {            CrawlDatum buffer = (CrawlDatum) value;                        switch(buffer.getStatus()) {                case CrawlDatum.STATUS_DB_UNFETCHED:                    hostDatum.setUnfetched(hostDatum.getUnfetched() + 1l);                    break;                case CrawlDatum.STATUS_DB_FETCHED:                    hostDatum.setFetched(hostDatum.getFetched() + 1l);                    break;                case CrawlDatum.STATUS_DB_GONE:                    hostDatum.setGone(hostDatum.getGone() + 1l);                    break;                case CrawlDatum.STATUS_DB_REDIR_TEMP:                    hostDatum.setRedirTemp(hostDatum.getRedirTemp() + 1l);                    break;                case CrawlDatum.STATUS_DB_REDIR_PERM:                    hostDatum.setRedirPerm(hostDatum.getRedirPerm() + 1l);                    break;                case CrawlDatum.STATUS_DB_NOTMODIFIED:                    hostDatum.setNotModified(hostDatum.getNotModified() + 1l);                    break;            }                        if (buffer.getRetriesSinceFetch() != 0) {                hostDatum.incConnectionFailures();            }                        if (buffer.getStatus() == CrawlDatum.STATUS_DB_FETCHED || buffer.getStatus() == CrawlDatum.STATUS_DB_NOTMODIFIED) {                                if (stringFields != null) {                    for (int i = 0; i < stringFields.length; i++) {                                                if (buffer.getMetaData().get(stringFieldWritables[i]) != null) {                                                        String metadataValue = null;                            try {                                metadataValue = buffer.getMetaData().get(stringFieldWritables[i]).toString();                            } catch (Exception e) {                                                            }                                                        if (stringCounts.get(stringFields[i]).containsKey(metadataValue)) {                                                                stringCounts.get(stringFields[i]).put(metadataValue, stringCounts.get(stringFields[i]).get(metadataValue) + 1l);                            } else {                                                                stringCounts.get(stringFields[i]).put(metadataValue, 1l);                            }                        }                    }                }                                if (numericFields != null) {                    for (int i = 0; i < numericFields.length; i++) {                                                if (buffer.getMetaData().get(numericFieldWritables[i]) != null) {                            try {                                                                Float metadataValue = Float.parseFloat(buffer.getMetaData().get(numericFieldWritables[i]).toString());                                                                if (tdigests.containsKey(numericFields[i])) {                                    tdigests.get(numericFields[i]).add(metadataValue);                                } else {                                                                        TDigest tdigest = TDigest.createDigest(100);                                    tdigest.add((double) metadataValue);                                    tdigests.put(numericFields[i], tdigest);                                }                                                                if (minimums.containsKey(numericFields[i])) {                                                                        if (metadataValue < minimums.get(numericFields[i])) {                                        minimums.put(numericFields[i], metadataValue);                                    }                                } else {                                                                        minimums.put(numericFields[i], metadataValue);                                }                                                                if (maximums.containsKey(numericFields[i])) {                                                                        if (metadataValue > maximums.get(numericFields[i])) {                                        maximums.put(numericFields[i], metadataValue);                                    }                                } else {                                                                        maximums.put(numericFields[i], metadataValue);                                }                                                                if (sums.containsKey(numericFields[i])) {                                                                        sums.put(numericFields[i], sums.get(numericFields[i]) + metadataValue);                                    counts.put(numericFields[i], counts.get(numericFields[i]) + 1l);                                } else {                                                                        sums.put(numericFields[i], metadataValue);                                    counts.put(numericFields[i], 1l);                                }                            } catch (Exception e) {                                                            }                        }                    }                }            }        } else         if (value instanceof HostDatum) {            HostDatum buffer = (HostDatum) value;                        if (buffer.hasHomepageUrl()) {                hostDatum.setHomepageUrl(buffer.getHomepageUrl());            }                        if (!buffer.isEmpty()) {                hostDatum.setLastCheck(buffer.getLastCheck());            }                        if (buffer.getDnsFailures() > 0) {                hostDatum.setDnsFailures(buffer.getDnsFailures());            }                        if (buffer.getConnectionFailures() > 0) {                hostDatum.setConnectionFailures(buffer.getConnectionFailures());            }                        if (!buffer.getMetaData().isEmpty()) {                hostDatum.setMetaData(buffer.getMetaData());            }                        if (buffer.getScore() > 0) {                hostDatum.setScore(buffer.getScore());            }        } else         if (value instanceof FloatWritable) {            FloatWritable buffer = (FloatWritable) value;            score = buffer.get();        } else {                    }    }        if (score > 0) {        hostDatum.setScore(score);    }        for (Map.Entry<String, Map<String, Long>> entry : stringCounts.entrySet()) {        for (Map.Entry<String, Long> subEntry : entry.getValue().entrySet()) {            hostDatum.getMetaData().put(new Text(entry.getKey() + "." + subEntry.getKey()), new LongWritable(subEntry.getValue()));        }    }    for (Map.Entry<String, Float> entry : maximums.entrySet()) {        hostDatum.getMetaData().put(new Text("max." + entry.getKey()), new FloatWritable(entry.getValue()));    }    for (Map.Entry<String, Float> entry : sums.entrySet()) {        hostDatum.getMetaData().put(new Text("avg." + entry.getKey()), new FloatWritable(entry.getValue() / counts.get(entry.getKey())));    }    for (Map.Entry<String, TDigest> entry : tdigests.entrySet()) {                for (int i = 0; i < percentiles.length; i++) {            hostDatum.getMetaData().put(new Text("pct" + Long.toString(percentiles[i]) + "." + entry.getKey()), new FloatWritable((float) entry.getValue().quantile(0.5)));        }    }    for (Map.Entry<String, Float> entry : minimums.entrySet()) {        hostDatum.getMetaData().put(new Text("min." + entry.getKey()), new FloatWritable(entry.getValue()));    }    context.getCounter("UpdateHostDb", "total_hosts").increment(1);        if (shouldCheck(hostDatum)) {                resolverThread = new ResolverThread(key.toString(), hostDatum, context, purgeFailedHostsThreshold);                try {            queue.put(resolverThread);        } catch (InterruptedException e) {                    }                return;    } else {        context.getCounter("UpdateHostDb", "skipped_not_eligible").increment(1);            }        context.write(key, hostDatum);}
protected boolean nutch_f375_0(HostDatum datum)
{        if (checkNew && datum.isEmpty()) {        return true;    }        if (checkKnown && !datum.isEmpty() && datum.getDnsFailures() == 0) {        return isEligibleForCheck(datum);    }        if (checkFailed && datum.getDnsFailures() > 0) {        return isEligibleForCheck(datum);    }        return false;}
protected boolean nutch_f376_0(HostDatum datum)
{        if (force || datum.getLastCheck().getTime() + (recheckInterval * datum.getDnsFailures() + 1) > now) {        return true;    }    return false;}
public void nutch_f377_1(Context context)
{            executor.shutdown();    boolean finished = false;        while (!finished) {        try {                        if (!executor.isTerminated()) {                                Thread.sleep(1000);            } else {                                finished = true;            }        } catch (InterruptedException e) {                                }    }}
public Configuration nutch_f378_0()
{    return conf;}
public void nutch_f379_0(Configuration conf)
{    this.conf = conf;}
public void nutch_f380_0(Text key, CrawlDatum value, Context context) throws IOException, InterruptedException
{    if (value.getStatus() == CrawlDatum.STATUS_DB_GONE || value.getStatus() == CrawlDatum.STATUS_DB_DUPLICATE) {        context.write(OUT, key);    }}
public void nutch_f381_0(Reducer<ByteWritable, Text, Text, ByteWritable>.Context context)
{    Configuration conf = context.getConfiguration();    writers = IndexWriters.get(conf);    try {        writers.open(conf, "Deletion");    } catch (IOException e) {        throw new RuntimeException(e);    }    noCommit = conf.getBoolean("noCommit", false);}
public void nutch_f382_1(Context context) throws IOException
{    if (totalDeleted > 0 && !noCommit) {        writers.commit();    }    writers.close();    }
public void nutch_f383_0(ByteWritable key, Iterable<Text> values, Context context) throws IOException
{    for (Text document : values) {        writers.delete(document.toString());        totalDeleted++;        context.getCounter("CleaningJobStatus", "Deleted documents").increment(1);                                        }}
public void nutch_f384_1(String crawldb, boolean noCommit) throws IOException, InterruptedException, ClassNotFoundException
{    SimpleDateFormat sdf = new SimpleDateFormat("yyyy-MM-dd HH:mm:ss");    long start = System.currentTimeMillis();        Job job = NutchJob.getInstance(getConf());    Configuration conf = job.getConfiguration();    FileInputFormat.addInputPath(job, new Path(crawldb, CrawlDb.CURRENT_NAME));    conf.setBoolean("noCommit", noCommit);    job.setInputFormatClass(SequenceFileInputFormat.class);    job.setOutputFormatClass(NullOutputFormat.class);    job.setMapOutputKeyClass(ByteWritable.class);    job.setMapOutputValueClass(Text.class);    job.setMapperClass(DBFilter.class);    job.setReducerClass(DeleterReducer.class);    job.setJarByClass(CleaningJob.class);    job.setJobName("CleaningJob");        conf.setBoolean(IndexerMapReduce.INDEXER_DELETE, true);    try {        boolean success = job.waitForCompletion(true);        if (!success) {            String message = "CleaningJob did not succeed, job status:" + job.getStatus().getState() + ", reason: " + job.getStatus().getFailureInfo();                        throw new RuntimeException(message);        }    } catch (InterruptedException | ClassNotFoundException e) {                throw e;    }    long end = System.currentTimeMillis();    }
public int nutch_f385_1(String[] args) throws IOException
{    if (args.length < 1) {        String usage = "Usage: CleaningJob <crawldb> [-noCommit]";                System.err.println(usage);        return 1;    }    boolean noCommit = false;    if (args.length == 2 && args[1].equals("-noCommit")) {        noCommit = true;    }    try {        delete(args[0], noCommit);    } catch (final Exception e) {                System.err.println("ERROR CleaningJob: " + StringUtils.stringifyException(e));        return -1;    }    return 0;}
public static void nutch_f386_0(String[] args) throws Exception
{    int result = ToolRunner.run(NutchConfiguration.create(), new CleaningJob(), args);    System.exit(result);}
private static String nutch_f387_1(String url, boolean normalize, URLNormalizers urlNormalizers)
{    if (!normalize) {        return url;    }    String normalized = null;    if (urlNormalizers != null) {        try {                        normalized = urlNormalizers.normalize(url, URLNormalizers.SCOPE_INDEXER);            normalized = normalized.trim();        } catch (Exception e) {                        normalized = null;        }    }    return normalized;}
private static String nutch_f388_0(String url, boolean filter, URLFilters urlFilters)
{    if (!filter) {        return url;    }    try {        url = urlFilters.filter(url);    } catch (Exception e) {        url = null;    }    return url;}
public void nutch_f389_0(Mapper<Text, Writable, Text, NutchWritable>.Context context)
{    Configuration conf = context.getConfiguration();    normalize = conf.getBoolean(URL_NORMALIZING, false);    filter = conf.getBoolean(URL_FILTERING, false);    if (normalize) {        urlNormalizers = new URLNormalizers(conf, URLNormalizers.SCOPE_INDEXER);    }    if (filter) {        urlFilters = new URLFilters(conf);    }}
public void nutch_f390_0(Text key, Writable value, Context context) throws IOException, InterruptedException
{    String urlString = filterUrl(normalizeUrl(key.toString(), normalize, urlNormalizers), filter, urlFilters);    if (urlString == null) {        return;    } else {        key.set(urlString);    }    context.write(key, new NutchWritable(value));}
public void nutch_f391_0(Reducer<Text, NutchWritable, Text, NutchIndexAction>.Context context)
{    Configuration conf = context.getConfiguration();    filters = new IndexingFilters(conf);    scfilters = new ScoringFilters(conf);    delete = conf.getBoolean(INDEXER_DELETE, false);    deleteRobotsNoIndex = conf.getBoolean(INDEXER_DELETE_ROBOTS_NOINDEX, false);    deleteSkippedByIndexingFilter = conf.getBoolean(INDEXER_DELETE_SKIPPED, false);    skip = conf.getBoolean(INDEXER_SKIP_NOTMODIFIED, false);    base64 = conf.getBoolean(INDEXER_BINARY_AS_BASE64, false);    normalize = conf.getBoolean(URL_NORMALIZING, false);    filter = conf.getBoolean(URL_FILTERING, false);    if (normalize) {        urlNormalizers = new URLNormalizers(conf, URLNormalizers.SCOPE_INDEXER);    }    if (filter) {        urlFilters = new URLFilters(conf);    }}
public void nutch_f392_1(Text key, Iterable<NutchWritable> values, Context context) throws IOException, InterruptedException
{    Inlinks inlinks = null;    CrawlDatum dbDatum = null;    CrawlDatum fetchDatum = null;    Content content = null;    ParseData parseData = null;    ParseText parseText = null;    for (NutchWritable val : values) {                final Writable value = val.get();        if (value instanceof Inlinks) {            inlinks = (Inlinks) value;        } else if (value instanceof CrawlDatum) {            final CrawlDatum datum = (CrawlDatum) value;            if (CrawlDatum.hasDbStatus(datum)) {                dbDatum = datum;            } else if (CrawlDatum.hasFetchStatus(datum)) {                                if (datum.getStatus() != CrawlDatum.STATUS_FETCH_NOTMODIFIED) {                    fetchDatum = datum;                }            } else if (CrawlDatum.STATUS_LINKED == datum.getStatus() || CrawlDatum.STATUS_SIGNATURE == datum.getStatus() || CrawlDatum.STATUS_PARSE_META == datum.getStatus()) {                continue;            } else {                throw new RuntimeException("Unexpected status: " + datum.getStatus());            }        } else if (value instanceof ParseData) {            parseData = (ParseData) value;                        if (deleteRobotsNoIndex) {                                String robotsMeta = parseData.getMeta("robots");                                if (robotsMeta != null && robotsMeta.toLowerCase().indexOf("noindex") != -1) {                                        context.write(key, DELETE_ACTION);                    context.getCounter("IndexerStatus", "deleted (robots=noindex)").increment(1);                    return;                }            }        } else if (value instanceof ParseText) {            parseText = (ParseText) value;        } else if (value instanceof Content) {            content = (Content) value;        } else if (LOG.isWarnEnabled()) {                    }    }        if (delete && fetchDatum != null) {        if (fetchDatum.getStatus() == CrawlDatum.STATUS_FETCH_GONE || dbDatum != null && dbDatum.getStatus() == CrawlDatum.STATUS_DB_GONE) {            context.getCounter("IndexerStatus", "deleted (gone)").increment(1);            context.write(key, DELETE_ACTION);            return;        }        if (fetchDatum.getStatus() == CrawlDatum.STATUS_FETCH_REDIR_PERM || fetchDatum.getStatus() == CrawlDatum.STATUS_FETCH_REDIR_TEMP || dbDatum != null && dbDatum.getStatus() == CrawlDatum.STATUS_DB_REDIR_PERM || dbDatum != null && dbDatum.getStatus() == CrawlDatum.STATUS_DB_REDIR_TEMP) {            context.getCounter("IndexerStatus", "deleted (redirects)").increment(1);            context.write(key, DELETE_ACTION);            return;        }    }    if (fetchDatum == null || parseText == null || parseData == null) {                return;    }        if (delete && dbDatum != null && dbDatum.getStatus() == CrawlDatum.STATUS_DB_DUPLICATE) {        context.getCounter("IndexerStatus", "deleted (duplicates)").increment(1);        context.write(key, DELETE_ACTION);        return;    }        if (skip && dbDatum != null && dbDatum.getStatus() == CrawlDatum.STATUS_DB_NOTMODIFIED) {        context.getCounter("IndexerStatus", "skipped (not modified)").increment(1);        return;    }    if (!parseData.getStatus().isSuccess() || fetchDatum.getStatus() != CrawlDatum.STATUS_FETCH_SUCCESS) {        return;    }    NutchDocument doc = new NutchDocument();    doc.add("id", key.toString());    final Metadata metadata = parseData.getContentMeta();        doc.add("segment", metadata.get(Nutch.SEGMENT_NAME_KEY));        doc.add("digest", metadata.get(Nutch.SIGNATURE_KEY));    final Parse parse = new ParseImpl(parseText, parseData);    float boost = 1.0f;        try {        boost = scfilters.indexerScore(key, doc, dbDatum, fetchDatum, parse, inlinks, boost);    } catch (final ScoringFilterException e) {        context.getCounter("IndexerStatus", "errors (ScoringFilter)").increment(1);        if (LOG.isWarnEnabled()) {                    }        return;    }        doc.setWeight(boost);        doc.add("boost", Float.toString(boost));    try {        if (dbDatum != null) {                        fetchDatum.setSignature(dbDatum.getSignature());                                    final Text url = (Text) dbDatum.getMetaData().get(Nutch.WRITABLE_REPR_URL_KEY);            if (url != null) {                                                                                String urlString = filterUrl(normalizeUrl(key.toString(), normalize, urlNormalizers), filter, urlFilters);                if (urlString != null) {                    url.set(urlString);                    fetchDatum.getMetaData().put(Nutch.WRITABLE_REPR_URL_KEY, url);                }            }        }                doc = filters.filter(doc, parse, key, fetchDatum, inlinks);    } catch (final IndexingException e) {        if (LOG.isWarnEnabled()) {                    }        context.getCounter("IndexerStatus", "errors (IndexingFilter)").increment(1);        return;    }        if (doc == null) {                if (deleteSkippedByIndexingFilter) {            NutchIndexAction action = new NutchIndexAction(null, NutchIndexAction.DELETE);            context.write(key, action);            context.getCounter("IndexerStatus", "deleted (IndexingFilter)").increment(1);        } else {            context.getCounter("IndexerStatus", "skipped (IndexingFilter)").increment(1);        }        return;    }    if (content != null) {                String binary;        if (base64) {                                                            binary = StringUtils.newStringUtf8(Base64.encodeBase64(content.getContent(), false, false));        } else {            binary = new String(content.getContent());        }        doc.add("binaryContent", binary);    }    context.getCounter("IndexerStatus", "indexed (add/update)").increment(1);    NutchIndexAction action = new NutchIndexAction(doc, NutchIndexAction.ADD);    context.write(key, action);}
public void nutch_f393_0() throws IOException
{}
public RecordWriter<Text, NutchIndexAction> nutch_f395_1(TaskAttemptContext context) throws IOException
{    Configuration conf = context.getConfiguration();    final IndexWriters writers = IndexWriters.get(conf);    String name = getUniqueFile(context, "part", "");    writers.open(conf, name);        return new RecordWriter<Text, NutchIndexAction>() {        public void close(TaskAttemptContext context) throws IOException {                        boolean noCommit = conf.getBoolean(IndexerMapReduce.INDEXER_NO_COMMIT, false);            if (!noCommit) {                writers.commit();            }            writers.close();        }        public void write(Text key, NutchIndexAction indexAction) throws IOException {            if (indexAction.action == NutchIndexAction.ADD) {                writers.write(indexAction.doc);            } else if (indexAction.action == NutchIndexAction.DELETE) {                writers.delete(key.toString());            }        }    };}
public void nutch_f396_0(TaskAttemptContext context) throws IOException
{        boolean noCommit = conf.getBoolean(IndexerMapReduce.INDEXER_NO_COMMIT, false);    if (!noCommit) {        writers.commit();    }    writers.close();}
public void nutch_f397_0(Text key, NutchIndexAction indexAction) throws IOException
{    if (indexAction.action == NutchIndexAction.ADD) {        writers.write(indexAction.doc);    } else if (indexAction.action == NutchIndexAction.DELETE) {        writers.delete(key.toString());    }}
public NutchDocument nutch_f398_0(NutchDocument doc, Parse parse, Text url, CrawlDatum datum, Inlinks inlinks) throws IndexingException
{    for (int i = 0; i < this.indexingFilters.length; i++) {        doc = this.indexingFilters[i].filter(doc, parse, url, datum, inlinks);                if (doc == null)            return null;    }    return doc;}
public int nutch_f399_0(String[] args) throws Exception
{    String url = null;    String usage =     "Usage:\n" +     "  IndexingFiltersChecker [OPTIONS] <url>\n" +     "    Fetch single URL and index it\n" +     "  IndexingFiltersChecker [OPTIONS] -stdin\n" +     "    Read URLs to be indexed from stdin\n" +     "  IndexingFiltersChecker [OPTIONS] -listen <port> [-keepClientCnxOpen]\n" +     "    Listen on <port> for URLs to be indexed\n" +     "Options:\n" +     "  -D<property>=<value>\tset/overwrite Nutch/Hadoop properties\n" +     "                  \t(a generic Hadoop option to be passed\n" + "                  \t before other command-specific options)\n" +     "  -normalize      \tnormalize URLs\n" +     "  -followRedirects\tfollow redirects when fetching URL\n" +     "  -dumpText       \tshow the entire plain-text content,\n" +     "                  \tnot only the first 100 characters\n" +     "  -doIndex        \tpass document to configured index writers\n" +     "                  \tand let them index it\n" + "  -md <key>=<value>\tmetadata added to CrawlDatum before parsing\n";        if (args.length < 1) {        System.err.println(usage);        System.exit(-1);    }        doIndex = getConf().getBoolean("doIndex", false);    int numConsumed;    for (int i = 0; i < args.length; i++) {        if (args[i].equals("-normalize")) {            normalizers = new URLNormalizers(getConf(), URLNormalizers.SCOPE_DEFAULT);        } else if (args[i].equals("-followRedirects")) {            followRedirects = true;        } else if (args[i].equals("-dumpText")) {            dumpText = true;        } else if (args[i].equals("-doIndex")) {            doIndex = true;        } else if (args[i].equals("-md")) {            String k = null, v = null;            String nextOne = args[++i];            int firstEquals = nextOne.indexOf("=");            if (firstEquals != -1) {                k = nextOne.substring(0, firstEquals);                v = nextOne.substring(firstEquals + 1);            } else                k = nextOne;            metadata.put(k, v);        } else if ((numConsumed = super.parseArgs(args, i)) > 0) {            i += numConsumed - 1;        } else if (i != args.length - 1) {            System.err.println("ERR: Not a recognized argument: " + args[i]);            System.err.println(usage);            System.exit(-1);        } else {            url = args[i];        }    }    if (url != null) {        return super.processSingle(url);    } else {                return super.run();    }}
protected int nutch_f400_1(String url, StringBuilder output) throws Exception
{    if (normalizers != null) {        url = normalizers.normalize(url, URLNormalizers.SCOPE_DEFAULT);    }        CrawlDatum datum = new CrawlDatum();    Iterator<String> iter = metadata.keySet().iterator();    while (iter.hasNext()) {        String key = iter.next();        String value = metadata.get(key);        if (value == null)            value = "";        datum.getMetaData().put(new Text(key), new Text(value));    }    int maxRedirects = getConf().getInt("http.redirect.max", 3);    if (followRedirects) {        if (maxRedirects == 0) {                        maxRedirects = 3;        } else {                    }    }    ProtocolOutput protocolOutput = getProtocolOutput(url, datum);    Text turl = new Text(url);        int numRedirects = 0;    while (!protocolOutput.getStatus().isSuccess() && followRedirects && protocolOutput.getStatus().isRedirect() && maxRedirects >= numRedirects) {        String[] stuff = protocolOutput.getStatus().getArgs();        url = stuff[0];                if (normalizers != null) {            url = normalizers.normalize(url, URLNormalizers.SCOPE_DEFAULT);        }        turl.set(url);                protocolOutput = getProtocolOutput(url, datum);        numRedirects++;    }    if (!protocolOutput.getStatus().isSuccess()) {        System.err.println("Fetch failed with protocol status: " + protocolOutput.getStatus());        if (protocolOutput.getStatus().isRedirect()) {            System.err.println("Redirect(s) not handled due to configuration.");            System.err.println("Max Redirects to handle per config: " + maxRedirects);            System.err.println("Number of Redirects handled: " + numRedirects);        }        return -1;    }    Content content = protocolOutput.getContent();    if (content == null) {        output.append("No content for " + url + "\n");        return 0;    }    String contentType = content.getContentType();    if (contentType == null) {                return -1;    }        datum.getMetaData().put(new Text(Metadata.CONTENT_TYPE), new Text(contentType));    if (ParseSegment.isTruncated(content)) {            }    ScoringFilters scfilters = new ScoringFilters(getConf());        try {        scfilters.passScoreBeforeParsing(turl, datum, content);    } catch (Exception e) {            }            ParseResult parseResult = new ParseUtil(getConf()).parse(content);    NutchDocument doc = new NutchDocument();    doc.add("id", url);    Text urlText = new Text(url);    Inlinks inlinks = null;    Parse parse = parseResult.get(urlText);    if (parse == null) {                        for (Map.Entry<Text, Parse> entry : parseResult) {                    }                        return -1;    }    byte[] signature = SignatureFactory.getSignature(getConf()).calculate(content, parse);    parse.getData().getContentMeta().set(Nutch.SIGNATURE_KEY, StringUtil.toHexString(signature));    String digest = parse.getData().getContentMeta().get(Nutch.SIGNATURE_KEY);    doc.add("digest", digest);    datum.setSignature(signature);        try {        scfilters.passScoreAfterParsing(turl, content, parseResult.get(turl));    } catch (Exception e) {            }    IndexingFilters indexers = new IndexingFilters(getConf());    try {        doc = indexers.filter(doc, parse, urlText, datum, inlinks);    } catch (IndexingException e) {        e.printStackTrace();    }    if (doc == null) {        output.append("Document discarded by indexing filter\n");        return 0;    }    for (String fname : doc.getFieldNames()) {        List<Object> values = doc.getField(fname).getValues();        if (values != null) {            for (Object value : values) {                String str = value.toString();                int minText = dumpText ? str.length() : Math.min(100, str.length());                output.append(fname + " :\t" + str.substring(0, minText) + "\n");            }        }    }        output.append("\n");    if (doIndex) {        IndexWriters writers = IndexWriters.get(getConf());        writers.open(getConf(), "IndexingFilterChecker");        writers.write(doc);        writers.close();    }    return 0;}
public static void nutch_f401_0(String[] args) throws Exception
{    final int res = ToolRunner.run(NutchConfiguration.create(), new IndexingFiltersChecker(), args);    System.exit(res);}
public void nutch_f402_0(Path crawlDb, Path linkDb, List<Path> segments, boolean noCommit) throws IOException, InterruptedException, ClassNotFoundException
{    index(crawlDb, linkDb, segments, noCommit, false, null);}
public void nutch_f403_0(Path crawlDb, Path linkDb, List<Path> segments, boolean noCommit, boolean deleteGone) throws IOException, InterruptedException, ClassNotFoundException
{    index(crawlDb, linkDb, segments, noCommit, deleteGone, null);}
public void nutch_f404_0(Path crawlDb, Path linkDb, List<Path> segments, boolean noCommit, boolean deleteGone, String params) throws IOException, InterruptedException, ClassNotFoundException
{    index(crawlDb, linkDb, segments, noCommit, deleteGone, params, false, false);}
public void nutch_f405_0(Path crawlDb, Path linkDb, List<Path> segments, boolean noCommit, boolean deleteGone, String params, boolean filter, boolean normalize) throws IOException, InterruptedException, ClassNotFoundException
{    index(crawlDb, linkDb, segments, noCommit, deleteGone, params, false, false, false);}
public void nutch_f406_0(Path crawlDb, Path linkDb, List<Path> segments, boolean noCommit, boolean deleteGone, String params, boolean filter, boolean normalize, boolean addBinaryContent) throws IOException, InterruptedException, ClassNotFoundException
{    index(crawlDb, linkDb, segments, noCommit, deleteGone, params, false, false, false, false);}
public void nutch_f407_1(Path crawlDb, Path linkDb, List<Path> segments, boolean noCommit, boolean deleteGone, String params, boolean filter, boolean normalize, boolean addBinaryContent, boolean base64) throws IOException, InterruptedException, ClassNotFoundException
{    SimpleDateFormat sdf = new SimpleDateFormat("yyyy-MM-dd HH:mm:ss");    long start = System.currentTimeMillis();        final Job job = NutchJob.getInstance(getConf());    job.setJobName("Indexer");    Configuration conf = job.getConfiguration();                if (addBinaryContent) {        if (base64) {                    } else {                    }    }    IndexerMapReduce.initMRJob(crawlDb, linkDb, segments, job, addBinaryContent);    conf.setBoolean(IndexerMapReduce.INDEXER_DELETE, deleteGone);    conf.setBoolean(IndexerMapReduce.URL_FILTERING, filter);    conf.setBoolean(IndexerMapReduce.URL_NORMALIZING, normalize);    conf.setBoolean(IndexerMapReduce.INDEXER_BINARY_AS_BASE64, base64);    conf.setBoolean(IndexerMapReduce.INDEXER_NO_COMMIT, noCommit);    if (params != null) {        conf.set(IndexerMapReduce.INDEXER_PARAMS, params);    }    job.setReduceSpeculativeExecution(false);    final Path tmp = new Path("tmp_" + System.currentTimeMillis() + "-" + new Random().nextInt());    FileOutputFormat.setOutputPath(job, tmp);    try {        try {            boolean success = job.waitForCompletion(true);            if (!success) {                String message = "Indexing job did not succeed, job status:" + job.getStatus().getState() + ", reason: " + job.getStatus().getFailureInfo();                                throw new RuntimeException(message);            }        } catch (IOException | InterruptedException | ClassNotFoundException e) {                        throw e;        }                for (Counter counter : job.getCounters().getGroup("IndexerStatus")) {                    }        long end = System.currentTimeMillis();            } finally {        tmp.getFileSystem(conf).delete(tmp, true);    }}
public int nutch_f408_1(String[] args) throws Exception
{    if (args.length < 2) {        System.err.println("Usage: Indexer <crawldb> [-linkdb <linkdb>] [-params k1=v1&k2=v2...] (<segment> ... | -dir <segments>) [-noCommit] [-deleteGone] [-filter] [-normalize] [-addBinaryContent] [-base64]");        return -1;    }    final Path crawlDb = new Path(args[0]);    Path linkDb = null;    final List<Path> segments = new ArrayList<>();    String params = null;    boolean noCommit = false;    boolean deleteGone = false;    boolean filter = false;    boolean normalize = false;    boolean addBinaryContent = false;    boolean base64 = false;    for (int i = 1; i < args.length; i++) {        FileSystem fs = null;        Path dir = null;        if (args[i].equals("-linkdb")) {            linkDb = new Path(args[++i]);        } else if (args[i].equals("-dir")) {            dir = new Path(args[++i]);            fs = dir.getFileSystem(getConf());            FileStatus[] fstats = fs.listStatus(dir, HadoopFSUtil.getPassDirectoriesFilter(fs));            Path[] files = HadoopFSUtil.getPaths(fstats);            for (Path p : files) {                if (SegmentChecker.isIndexable(p, fs)) {                    segments.add(p);                }            }        } else if (args[i].equals("-noCommit")) {            noCommit = true;        } else if (args[i].equals("-deleteGone")) {            deleteGone = true;        } else if (args[i].equals("-filter")) {            filter = true;        } else if (args[i].equals("-normalize")) {            normalize = true;        } else if (args[i].equals("-addBinaryContent")) {            addBinaryContent = true;        } else if (args[i].equals("-base64")) {            base64 = true;        } else if (args[i].equals("-params")) {            params = args[++i];        } else {            dir = new Path(args[i]);            fs = dir.getFileSystem(getConf());            if (SegmentChecker.isIndexable(dir, fs)) {                segments.add(dir);            }        }    }    try {        index(crawlDb, linkDb, segments, noCommit, deleteGone, params, filter, normalize, addBinaryContent, base64);        return 0;    } catch (final Exception e) {                return -1;    }}
public static void nutch_f409_0(String[] args) throws Exception
{    final int res = ToolRunner.run(NutchConfiguration.create(), new IndexingJob(), args);    System.exit(res);}
public Map<String, Object> nutch_f410_0(Map<String, Object> args, String crawlId) throws Exception
{    boolean noCommit = false;    boolean deleteGone = false;    boolean filter = false;    boolean normalize = false;    boolean isSegment = false;    String params = null;    Configuration conf = getConf();    Path crawlDb;    if (args.containsKey(Nutch.ARG_CRAWLDB)) {        Object crawldbPath = args.get(Nutch.ARG_CRAWLDB);        if (crawldbPath instanceof Path) {            crawlDb = (Path) crawldbPath;        } else {            crawlDb = new Path(crawldbPath.toString());        }    } else {        crawlDb = new Path(crawlId + "/crawldb");    }    Path linkdb = null;    List<Path> segments = new ArrayList<>();    if (args.containsKey(Nutch.ARG_LINKDB)) {        Object path = args.get(Nutch.ARG_LINKDB);        if (path instanceof Path) {            linkdb = (Path) path;        } else {            linkdb = new Path(path.toString());        }    } else {        linkdb = new Path(crawlId + "/linkdb");    }    if (args.containsKey(Nutch.ARG_SEGMENTDIR)) {        isSegment = true;        Path segmentsDir;        Object segDir = args.get(Nutch.ARG_SEGMENTDIR);        if (segDir instanceof Path) {            segmentsDir = (Path) segDir;        } else {            segmentsDir = new Path(segDir.toString());        }        FileSystem fs = segmentsDir.getFileSystem(getConf());        FileStatus[] fstats = fs.listStatus(segmentsDir, HadoopFSUtil.getPassDirectoriesFilter(fs));        Path[] files = HadoopFSUtil.getPaths(fstats);        for (Path p : files) {            if (SegmentChecker.isIndexable(p, fs)) {                segments.add(p);            }        }    }    if (args.containsKey(Nutch.ARG_SEGMENTS)) {        Object segmentsFromArg = args.get(Nutch.ARG_SEGMENTS);        ArrayList<String> segmentList = new ArrayList<String>();        if (segmentsFromArg instanceof ArrayList) {            segmentList = (ArrayList<String>) segmentsFromArg;        } else if (segmentsFromArg instanceof Path) {            segmentList.add(segmentsFromArg.toString());        }        for (String segment : segmentList) {            segments.add(new Path(segment));        }    }    if (!isSegment) {        String segment_dir = crawlId + "/segments";        File segmentsDir = new File(segment_dir);        File[] segmentsList = segmentsDir.listFiles();        Arrays.sort(segmentsList, (f1, f2) -> {            if (f1.lastModified() > f2.lastModified())                return -1;            else                return 0;        });        Path segment = new Path(segmentsList[0].getPath());        segments.add(segment);    }    if (args.containsKey("noCommit")) {        noCommit = true;    }    if (args.containsKey("deleteGone")) {        deleteGone = true;    }    if (args.containsKey("normalize")) {        normalize = true;    }    if (args.containsKey("filter")) {        filter = true;    }    if (args.containsKey("params")) {        params = (String) args.get("params");    }    setConf(conf);    index(crawlDb, linkdb, segments, noCommit, deleteGone, params, filter, normalize);    Map<String, Object> results = new HashMap<>();    results.put(Nutch.VAL_RESULT, 0);    return results;}
 static IndexWriterConfig nutch_f411_0(Element rootElement)
{    String id = rootElement.getAttribute("id");    String clazz = rootElement.getAttribute("class");    NodeList parametersList = rootElement.getElementsByTagName("param");    Map<String, String> parameters = new HashMap<>();    for (int i = 0; i < parametersList.getLength(); i++) {        Element parameterNode = (Element) parametersList.item(i);        parameters.put(parameterNode.getAttribute("name"), parameterNode.getAttribute("value"));    }    return new IndexWriterConfig(id, clazz, parameters, MappingReader.parseMapping((Element) rootElement.getElementsByTagName("mapping").item(0)));}
 String nutch_f412_0()
{    return id;}
 String nutch_f413_0()
{    return clazz;}
 IndexWriterParams nutch_f414_0()
{    return params;}
 Map<MappingReader.Actions, Map<String, List<String>>> nutch_f415_0()
{    return mapping;}
public String nutch_f416_0()
{    StringBuilder sb = new StringBuilder();    sb.append("ID: ");    sb.append(id);    sb.append("\n");    sb.append("Class: ");    sb.append(clazz);    sb.append("\n");    sb.append("Params {\n");    for (Map.Entry<String, String> entry : params.entrySet()) {        sb.append("\t");        sb.append(entry.getKey());        sb.append(":\t");        sb.append(entry.getValue());        sb.append("\n");    }    sb.append("}\n");    sb.append("Mapping {\n");    for (Map.Entry<MappingReader.Actions, Map<String, List<String>>> entry : mapping.entrySet()) {        sb.append("\t");        sb.append(entry.getKey());        sb.append(" {\n");        for (Map.Entry<String, List<String>> entry1 : entry.getValue().entrySet()) {            sb.append("\t\t");            sb.append(entry1.getKey());            sb.append(":\t");            sb.append(String.join(",", entry1.getValue()));            sb.append("\n");        }        sb.append("\t}\n");    }    sb.append("}\n");    return sb.toString();}
public String nutch_f417_0(String name, String defaultValue)
{    return this.getOrDefault(name, defaultValue);}
public boolean nutch_f418_0(String name, boolean defaultValue)
{    String value;    if ((value = this.get(name)) != null && !"".equals(value)) {        return Boolean.parseBoolean(value);    }    return defaultValue;}
public long nutch_f419_0(String name, long defaultValue)
{    String value;    if ((value = this.get(name)) != null && !"".equals(value)) {        return Long.parseLong(value);    }    return defaultValue;}
public int nutch_f420_0(String name, int defaultValue)
{    String value;    if ((value = this.get(name)) != null && !"".equals(value)) {        return Integer.parseInt(value);    }    return defaultValue;}
public String[] nutch_f421_0(String name)
{    String value = this.get(name);    return StringUtils.getStrings(value);}
public String[] nutch_f422_0(String name, String... defaultValue)
{    String value;    if ((value = this.get(name)) != null && !"".equals(value)) {        return StringUtils.getStrings(value);    }    return defaultValue;}
public static synchronized IndexWriters nutch_f423_0(Configuration conf)
{    String uuid = NutchConfiguration.getUUID(conf);    if (uuid == null) {                uuid = "nonNutchConf@" + conf.hashCode();    }    return CACHE.computeIfAbsent(uuid, k -> new IndexWriters(conf));}
private IndexWriterConfig[] nutch_f424_1(Configuration conf)
{    String filename = conf.get("indexer.indexwriters.file", "index-writers.xml");    InputStream ssInputStream = conf.getConfResourceAsInputStream(filename);    InputSource inputSource = new InputSource(ssInputStream);    try {        DocumentBuilderFactory factory = DocumentBuilderFactory.newInstance();        DocumentBuilder builder = factory.newDocumentBuilder();        Document document = builder.parse(inputSource);        Element rootElement = document.getDocumentElement();        NodeList writerList = rootElement.getElementsByTagName("writer");        IndexWriterConfig[] indexWriterConfigs = new IndexWriterConfig[writerList.getLength()];        for (int i = 0; i < writerList.getLength(); i++) {            indexWriterConfigs[i] = IndexWriterConfig.getInstanceFromElement((Element) writerList.item(i));        }        return indexWriterConfigs;    } catch (SAXException | IOException | ParserConfigurationException e) {                return new IndexWriterConfig[0];    }}
private NutchDocument nutch_f425_1(final NutchDocument document, final Map<MappingReader.Actions, Map<String, List<String>>> mapping)
{    try {        NutchDocument mappedDocument = document.clone();        mapping.get(MappingReader.Actions.COPY).forEach((key, value) -> {                        if (mappedDocument.getField(key) != null) {                for (String field : value) {                                        if (!key.equals(field)) {                        for (Object val : mappedDocument.getField(key).getValues()) {                            mappedDocument.add(field, val);                        }                    }                }            }        });        mapping.get(MappingReader.Actions.RENAME).forEach((key, value) -> {                        if (mappedDocument.getField(key) != null) {                NutchField field = mappedDocument.removeField(key);                mappedDocument.add(value.get(0), field.getValues());                mappedDocument.getField(value.get(0)).setWeight(field.getWeight());            }        });        mapping.get(MappingReader.Actions.REMOVE).forEach((key, value) -> mappedDocument.removeField(key));        return mappedDocument;    } catch (CloneNotSupportedException e) {                return document;    }}
private Collection<String> nutch_f426_0(NutchDocument doc)
{    if (this.exchanges.areAvailableExchanges()) {        return Arrays.asList(this.exchanges.indexWriters(doc));    }    return this.indexWriters.keySet();}
public void nutch_f427_0(Configuration conf, String name) throws IOException
{    for (Map.Entry<String, IndexWriterWrapper> entry : this.indexWriters.entrySet()) {        entry.getValue().getIndexWriter().open(conf, name);        entry.getValue().getIndexWriter().open(entry.getValue().getIndexWriterConfig().getParams());    }}
public void nutch_f428_1(NutchDocument doc) throws IOException
{    for (String indexWriterId : getIndexWriters(doc)) {        if (!this.indexWriters.containsKey(indexWriterId)) {                        continue;        }        NutchDocument mappedDocument = mapDocument(doc, this.indexWriters.get(indexWriterId).getIndexWriterConfig().getMapping());        this.indexWriters.get(indexWriterId).getIndexWriter().write(mappedDocument);    }}
public void nutch_f429_1(NutchDocument doc) throws IOException
{    for (String indexWriterId : getIndexWriters(doc)) {        if (!this.indexWriters.containsKey(indexWriterId)) {                        continue;        }        NutchDocument mappedDocument = mapDocument(doc, this.indexWriters.get(indexWriterId).getIndexWriterConfig().getMapping());        this.indexWriters.get(indexWriterId).getIndexWriter().update(mappedDocument);    }}
public void nutch_f430_0(String key) throws IOException
{    for (IndexWriterWrapper iww : indexWriters.values()) {        iww.getIndexWriter().delete(key);    }}
public void nutch_f431_0() throws IOException
{    for (Map.Entry<String, IndexWriterWrapper> entry : this.indexWriters.entrySet()) {        entry.getValue().getIndexWriter().close();    }}
public void nutch_f432_0() throws IOException
{    for (Map.Entry<String, IndexWriterWrapper> entry : this.indexWriters.entrySet()) {        entry.getValue().getIndexWriter().commit();    }}
public String nutch_f433_0()
{    StringBuilder builder = new StringBuilder();    if (this.indexWriters.size() == 0)        builder.append("No IndexWriters activated - check your configuration\n");    else        builder.append("Active IndexWriters :\n");    for (IndexWriterWrapper indexWriterWrapper : this.indexWriters.values()) {                builder.append(indexWriterWrapper.getIndexWriter().getClass().getSimpleName()).append(":\n");                AsciiTable at = new AsciiTable();        at.getRenderer().setCWC((rows, colNumbers, tableWidth) -> {            int maxLengthFirstColumn = 0;            int maxLengthLastColumn = 0;            for (AT_Row row : rows) {                if (row.getType() == TableRowType.CONTENT) {                                        int lengthFirstColumn = row.getCells().get(0).toString().length();                    if (lengthFirstColumn > maxLengthFirstColumn) {                        maxLengthFirstColumn = lengthFirstColumn;                    }                                        int lengthLastColumn = row.getCells().get(2).toString().length();                    if (lengthLastColumn > maxLengthLastColumn) {                        maxLengthLastColumn = lengthLastColumn;                    }                }            }            return new int[] { maxLengthFirstColumn, tableWidth - maxLengthFirstColumn - maxLengthLastColumn, maxLengthLastColumn };        });                Map<String, Map.Entry<String, Object>> properties = indexWriterWrapper.getIndexWriter().describe();                properties.forEach((key, value) -> {            at.addRule();            at.addRow(key, value.getKey(), value.getValue() != null ? value.getValue() : "");        });                at.addRule();                builder.append(at.render(150)).append("\n\n");    }    return builder.toString();}
 IndexWriterConfig nutch_f434_0()
{    return indexWriterConfig;}
 void nutch_f435_0(IndexWriterConfig indexWriterConfig)
{    this.indexWriterConfig = indexWriterConfig;}
 IndexWriter nutch_f436_0()
{    return indexWriter;}
 void nutch_f437_0(IndexWriter indexWriter)
{    this.indexWriter = indexWriter;}
 static Map<Actions, Map<String, List<String>>> nutch_f438_0(Element mappingElement)
{    Map<Actions, Map<String, List<String>>> parsedMapping = new HashMap<>();        Node node = mappingElement.getElementsByTagName("rename").item(0);    if (node != null) {        NodeList fieldList = ((Element) node).getElementsByTagName("field");        Map<String, List<String>> fieldsMap = new HashMap<>();        for (int j = 0; j < fieldList.getLength(); j++) {            Element field = (Element) fieldList.item(j);            fieldsMap.put(field.getAttribute("source"), Collections.singletonList(field.getAttribute("dest")));        }        parsedMapping.put(Actions.RENAME, fieldsMap);    }        node = mappingElement.getElementsByTagName("copy").item(0);    if (node != null) {        NodeList fieldList = ((Element) node).getElementsByTagName("field");        Map<String, List<String>> fieldsMap = new HashMap<>();        for (int j = 0; j < fieldList.getLength(); j++) {            Element field = (Element) fieldList.item(j);            fieldsMap.put(field.getAttribute("source"), Arrays.asList(field.getAttribute("dest").split(",")));        }        parsedMapping.put(Actions.COPY, fieldsMap);    }        node = mappingElement.getElementsByTagName("remove").item(0);    if (node != null) {        NodeList fieldList = ((Element) node).getElementsByTagName("field");        Map<String, List<String>> fieldsMap = new HashMap<>();        for (int j = 0; j < fieldList.getLength(); j++) {            Element field = (Element) fieldList.item(j);            fieldsMap.put(field.getAttribute("source"), null);        }        parsedMapping.put(Actions.REMOVE, fieldsMap);    }    return parsedMapping;}
public void nutch_f439_0(String name, Object value)
{    NutchField field = fields.get(name);    if (field == null) {        field = new NutchField(value);        fields.put(name, field);    } else {        field.add(value);    }}
public Object nutch_f440_0(String name)
{    NutchField field = fields.get(name);    if (field == null) {        return null;    }    if (field.getValues().size() == 0) {        return null;    }    return field.getValues().get(0);}
public NutchField nutch_f441_0(String name)
{    return fields.get(name);}
public NutchField nutch_f442_0(String name)
{    return fields.remove(name);}
public Collection<String> nutch_f443_0()
{    return fields.keySet();}
public Iterator<Entry<String, NutchField>> nutch_f444_0()
{    return fields.entrySet().iterator();}
public float nutch_f445_0()
{    return weight;}
public void nutch_f446_0(float weight)
{    this.weight = weight;}
public Metadata nutch_f447_0()
{    return documentMeta;}
public void nutch_f448_0(DataInput in) throws IOException
{    fields.clear();    byte version = in.readByte();    if (version != VERSION) {        throw new VersionMismatchException(VERSION, version);    }    int size = WritableUtils.readVInt(in);    for (int i = 0; i < size; i++) {        String name = Text.readString(in);        NutchField field = new NutchField();        field.readFields(in);        fields.put(name, field);    }    weight = in.readFloat();    documentMeta.readFields(in);}
public void nutch_f449_0(DataOutput out) throws IOException
{    out.writeByte(VERSION);    WritableUtils.writeVInt(out, fields.size());    for (Map.Entry<String, NutchField> entry : fields.entrySet()) {        Text.writeString(out, entry.getKey());        NutchField field = entry.getValue();        field.write(out);    }    out.writeFloat(weight);    documentMeta.write(out);}
public String nutch_f450_0()
{    StringBuilder sb = new StringBuilder();    sb.append("doc {\n");    for (Map.Entry<String, NutchField> entry : fields.entrySet()) {        sb.append("\t");        sb.append(entry.getKey());        sb.append(":\t");        sb.append(entry.getValue());        sb.append("\n");    }    sb.append("}\n");    return sb.toString();}
public NutchDocument nutch_f451_0() throws CloneNotSupportedException
{    NutchDocument clonedDocument = (NutchDocument) super.clone();    clonedDocument.fields = new HashMap<>();    for (Entry<String, NutchField> field : this.fields.entrySet()) {        clonedDocument.fields.put(field.getKey(), field.getValue().clone());    }    return clonedDocument;}
public void nutch_f452_0(Object value)
{    values.add(value);}
public float nutch_f453_0()
{    return weight;}
public void nutch_f454_0(float weight)
{    this.weight = weight;}
public List<Object> nutch_f455_0()
{    return values;}
public void nutch_f456_0()
{    weight = 1.0f;    values.clear();}
public NutchField nutch_f457_0() throws CloneNotSupportedException
{    NutchField result = (NutchField) super.clone();    result.weight = weight;    result.values = (ArrayList<Object>) values.clone();    return result;}
public void nutch_f458_0(DataInput in) throws IOException
{    weight = in.readFloat();    int count = in.readInt();    values = new ArrayList<>();    for (int i = 0; i < count; i++) {        String type = Text.readString(in);        if ("java.lang.String".equals(type)) {            values.add(Text.readString(in));        } else if ("java.lang.Boolean".equals(type)) {            values.add(in.readBoolean());        } else if ("java.lang.Integer".equals(type)) {            values.add(in.readInt());        } else if ("java.lang.Float".equals(type)) {            values.add(in.readFloat());        } else if ("java.lang.Long".equals(type)) {            values.add(in.readLong());        } else if ("java.util.Date".equals(type)) {            values.add(new Date(in.readLong()));        }    }}
public void nutch_f459_0(DataOutput out) throws IOException
{    out.writeFloat(weight);    out.writeInt(values.size());    for (Object value : values) {        Text.writeString(out, value.getClass().getName());        if (value instanceof Boolean) {            out.writeBoolean((Boolean) value);        } else if (value instanceof Integer) {            out.writeInt((Integer) value);        } else if (value instanceof Long) {            out.writeLong((Long) value);        } else if (value instanceof Float) {            out.writeFloat((Float) value);        } else if (value instanceof String) {            Text.writeString(out, (String) value);        } else if (value instanceof Date) {            Date date = (Date) value;            out.writeLong(date.getTime());        }    }}
public String nutch_f460_0()
{    return values.toString();}
public void nutch_f461_0(DataInput in) throws IOException
{    action = in.readByte();    doc = new NutchDocument();    doc.readFields(in);}
public void nutch_f462_0(DataOutput out) throws IOException
{    out.write(action);    doc.write(out);}
public boolean nutch_f463_0(final String name)
{    return metadata.get(name) != null && metadata.get(name).length > 1;}
public String[] nutch_f464_0()
{    return metadata.keySet().toArray(new String[metadata.keySet().size()]);}
public String nutch_f465_0(final String name)
{    String[] values = metadata.get(name);    if (values == null) {        return null;    } else {        return values[0];    }}
public String[] nutch_f466_0(final String name)
{    return _getValues(name);}
private String[] nutch_f467_0(final String name)
{    String[] values = metadata.get(name);    if (values == null) {        values = new String[0];    }    return values;}
public void nutch_f468_0(final String name, final String value)
{    String[] values = metadata.get(name);    if (values == null) {        set(name, value);    } else {        String[] newValues = new String[values.length + 1];        System.arraycopy(values, 0, newValues, 0, values.length);        newValues[newValues.length - 1] = value;        metadata.put(name, newValues);    }}
public void nutch_f469_0(Metadata metadata)
{    for (String name : metadata.names()) {        String[] addValues = metadata.getValues(name);        if (addValues == null)            continue;        String[] oldValues = this.metadata.get(name);        if (oldValues == null) {            this.metadata.put(name, addValues);        } else {            String[] newValues = new String[oldValues.length + addValues.length];            System.arraycopy(oldValues, 0, newValues, 0, oldValues.length);            System.arraycopy(addValues, 0, newValues, oldValues.length, addValues.length);            this.metadata.put(name, newValues);        }    }}
public void nutch_f470_0(Properties properties)
{    Enumeration<?> names = properties.propertyNames();    while (names.hasMoreElements()) {        String name = (String) names.nextElement();        metadata.put(name, new String[] { properties.getProperty(name) });    }}
public void nutch_f471_0(String name, String value)
{    metadata.put(name, new String[] { value });}
public void nutch_f472_0(String name)
{    metadata.remove(name);}
public int nutch_f473_0()
{    return metadata.size();}
public void nutch_f474_0()
{    metadata.clear();}
public boolean nutch_f475_0(Object o)
{    if (o == null) {        return false;    }    Metadata other = null;    try {        other = (Metadata) o;    } catch (ClassCastException cce) {        return false;    }    if (other.size() != size()) {        return false;    }    String[] names = names();    for (int i = 0; i < names.length; i++) {        String[] otherValues = other._getValues(names[i]);        String[] thisValues = _getValues(names[i]);        if (otherValues.length != thisValues.length) {            return false;        }        for (int j = 0; j < otherValues.length; j++) {            if (!otherValues[j].equals(thisValues[j])) {                return false;            }        }    }    return true;}
public String nutch_f476_0()
{    StringBuffer buf = new StringBuffer();    String[] names = names();    for (int i = 0; i < names.length; i++) {        String[] values = _getValues(names[i]);        for (int j = 0; j < values.length; j++) {            buf.append(names[i]).append("=").append(values[j]).append(" ");        }    }    return buf.toString();}
public final void nutch_f477_0(DataOutput out) throws IOException
{    out.writeInt(size());    String[] values = null;    String[] names = names();    for (int i = 0; i < names.length; i++) {        Text.writeString(out, names[i]);        values = _getValues(names[i]);        int cnt = 0;        for (int j = 0; j < values.length; j++) {            if (values[j] != null)                cnt++;        }        out.writeInt(cnt);        for (int j = 0; j < values.length; j++) {            if (values[j] != null) {                Text.writeString(out, values[j]);            }        }    }}
public final void nutch_f478_0(DataInput in) throws IOException
{    int keySize = in.readInt();    String key;    for (int i = 0; i < keySize; i++) {        key = Text.readString(in);        int valueSize = in.readInt();        for (int j = 0; j < valueSize; j++) {            add(key, Text.readString(in));        }    }}
public Metadata nutch_f479_0()
{    return metadata;}
public void nutch_f480_0(String name, String value)
{    metadata.add(name, value);}
public void nutch_f481_0(String name, String value)
{    metadata.set(name, value);}
public String nutch_f482_0(String name)
{    return metadata.get(name);}
public String[] nutch_f483_0(String name)
{    return metadata.getValues(name);}
public void nutch_f484_0(DataInput in) throws IOException
{    super.readFields(in);    metadata = new Metadata();    metadata.readFields(in);}
public void nutch_f485_0(DataOutput out) throws IOException
{    super.write(out);    metadata.write(out);}
private static String nutch_f486_0(final String str)
{    char c;    StringBuffer buf = new StringBuffer();    for (int i = 0; i < str.length(); i++) {        c = str.charAt(i);        if (Character.isLetter(c)) {            buf.append(Character.toLowerCase(c));        }    }    return buf.toString();}
public static String nutch_f487_0(final String name)
{    String searched = normalize(name);    String value = NAMES_IDX.get(searched);    if ((value == null) && (normalized != null)) {        int threshold = Math.min(3, searched.length() / TRESHOLD_DIVIDER);        for (int i = 0; i < normalized.length && value == null; i++) {            if (StringUtils.getLevenshteinDistance(searched, normalized[i]) < threshold) {                value = NAMES_IDX.get(normalized[i]);            }        }    }    return (value != null) ? value : name;}
public void nutch_f488_0(final String name)
{    super.remove(getNormalizedName(name));}
public void nutch_f489_0(final String name, final String value)
{    super.add(getNormalizedName(name), value);}
public String[] nutch_f490_0(final String name)
{    return super.getValues(getNormalizedName(name));}
public String nutch_f491_0(final String name)
{    return super.get(getNormalizedName(name));}
public void nutch_f492_0(final String name, final String value)
{    super.set(getNormalizedName(name), value);}
public static String nutch_f493_0(Date date)
{    String string;    synchronized (format) {        string = format.format(date);    }    return string;}
public static String nutch_f494_0(Calendar cal)
{    String string;    synchronized (format) {        string = format.format(cal.getTime());    }    return string;}
public static String nutch_f495_0(long time)
{    String string;    synchronized (format) {        string = format.format(new Date(time));    }    return string;}
public static Date nutch_f496_0(String dateString) throws ParseException
{    Date date;    synchronized (format) {        date = format.parse(dateString);    }    return date;}
public static long nutch_f497_0(String dateString) throws ParseException
{    long time;    synchronized (format) {        time = format.parse(dateString).getTime();    }    return time;}
public static void nutch_f498_0(String[] args) throws Exception
{    Date now = new Date(System.currentTimeMillis());    String string = HttpDateFormat.toString(now);    long time = HttpDateFormat.toLong(string);    System.out.println(string);    System.out.println(HttpDateFormat.toString(time));}
public Configuration nutch_f499_0()
{    return config;}
public void nutch_f500_1(Configuration conf)
{    config = conf;    for (String exceptClassName : conf.getTrimmedStrings(HTTP_LOG_SUPPRESSION, "java.net.UnknownHostException", "java.net.NoRouteToHostException")) {        Class<?> clazz = conf.getClassByNameOrNull(exceptClassName);        if (clazz == null) {                        continue;        }        if (!Throwable.class.isAssignableFrom(clazz)) {                        continue;        }        exceptionsLogShort.add(clazz.asSubclass(Throwable.class));    }}
public boolean nutch_f501_0(Throwable t)
{    if (exceptionsLogShort.contains(t.getClass())) {        return true;    }    return false;}
public boolean nutch_f502_0(String fromUrl, String toUrl)
{    if (filters.length < 1) {                return false;    }        boolean exempted = fromUrl != null && toUrl != null;        for (int i = 0; i < this.filters.length && exempted; i++) {        exempted = this.filters[i].filter(fromUrl, toUrl);    }    return exempted;}
public int nutch_f503_0(String[] args) throws Exception
{    usage = "Usage: URLFilterChecker [-Dproperty=value]... [-filterName filterName] (-stdin | -listen <port> [-keepClientCnxOpen]) \n" + "\n  -filterName\tURL filter plugin name (eg. urlfilter-regex) to check," + "\n             \t(if not given all configured URL filters are applied)" + "\n  -stdin     \ttool reads a list of URLs from stdin, one URL per line" + "\n  -listen <port>\trun tool as Telnet server listening on <port>\n";        if (args.length < 1) {        System.err.println(usage);        System.exit(-1);    }    int numConsumed;    for (int i = 0; i < args.length; i++) {        if (args[i].equals("-filterName")) {            getConf().set("plugin.includes", args[++i]);        } else if ((numConsumed = super.parseArgs(args, i)) > 0) {            i += numConsumed - 1;        } else {            System.err.println("ERROR: Not a recognized argument: " + args[i]);            System.err.println(usage);            System.exit(-1);        }    }        filters = new URLFilters(getConf());    System.out.print("Checking combination of these URLFilters: ");    for (URLFilter filter : filters.getFilters()) {        System.out.print(filter.getClass().getSimpleName() + " ");    }    System.out.println("");        return super.run();}
protected int nutch_f504_0(String line, StringBuilder output) throws Exception
{    String out = filters.filter(line);    if (out != null) {        output.append("+");        output.append(out);    } else {        output.append("-");        output.append(line);    }    return 0;}
public static void nutch_f505_0(String[] args) throws Exception
{    final int res = ToolRunner.run(NutchConfiguration.create(), new URLFilterChecker(), args);    System.exit(res);}
public URLFilter[] nutch_f506_0()
{    return this.filters;}
public String nutch_f507_0(String urlString) throws URLFilterException
{    for (int i = 0; i < this.filters.length; i++) {        if (urlString == null)            return null;        urlString = this.filters[i].filter(urlString);    }    return urlString;}
public int nutch_f508_0(String[] args) throws Exception
{    usage = "Usage: URLNormalizerChecker [-Dproperty=value]... [-normalizer <normalizerName>] [-scope <scope>] (-stdin | -listen <port> [-keepClientCnxOpen])\n" + "\n  -normalizer\tURL normalizer plugin (eg. urlnormalizer-basic) to check," + "\n             \t(if not given all configured URL normalizers are applied)" + "\n  -scope     \tone of: default,partition,generate_host_count,fetcher,crawldb,linkdb,inject,outlink" + "\n  -stdin     \ttool reads a list of URLs from stdin, one URL per line" + "\n  -listen <port>\trun tool as Telnet server listening on <port>" + "\n\nAn empty line is added to the output if a URL fails to normalize (MalformedURLException or null returned).\n";        if (args.length < 1) {        System.err.println(usage);        System.exit(-1);    }    int numConsumed;    for (int i = 0; i < args.length; i++) {        if (args[i].equals("-normalizer")) {            getConf().set("plugin.includes", args[++i]);        } else if (args[i].equals("-scope")) {            scope = args[++i];        } else if ((numConsumed = super.parseArgs(args, i)) > 0) {            i += numConsumed - 1;        } else {            System.err.println("ERROR: Not a recognized argument: " + args[i]);            System.err.println(usage);            System.exit(-1);        }    }        normalizers = new URLNormalizers(getConf(), scope);    System.out.print("Checking combination of these URLNormalizers: ");    for (URLNormalizer normalizer : normalizers.getURLNormalizers(scope)) {        System.out.print(normalizer.getClass().getSimpleName() + " ");    }    System.out.println("");        return super.run();}
protected int nutch_f509_0(String line, StringBuilder output) throws Exception
{    try {        String norm = normalizers.normalize(line, scope);        if (norm == null) {            output.append("");        } else {            output.append(norm);        }    } catch (MalformedURLException e) {        output.append("");    }    return 0;}
public static void nutch_f510_0(String[] args) throws Exception
{    final int res = ToolRunner.run(NutchConfiguration.create(), new URLNormalizerChecker(), args);    System.exit(res);}
 URLNormalizer[] nutch_f511_1(String scope)
{    List<Extension> extensions = getExtensions(scope);    ObjectCache objectCache = ObjectCache.get(conf);    if (extensions == EMPTY_EXTENSION_LIST) {        return EMPTY_NORMALIZERS;    }    List<URLNormalizer> normalizers = new Vector<>(extensions.size());    Iterator<Extension> it = extensions.iterator();    while (it.hasNext()) {        Extension ext = it.next();        URLNormalizer normalizer = null;        try {                        normalizer = (URLNormalizer) objectCache.getObject(ext.getId());            if (normalizer == null) {                                normalizer = (URLNormalizer) ext.getExtensionInstance();                objectCache.setObject(ext.getId(), normalizer);            }            normalizers.add(normalizer);        } catch (PluginRuntimeException e) {            e.printStackTrace();                    }    }    return normalizers.toArray(new URLNormalizer[normalizers.size()]);}
private List<Extension> nutch_f512_0(String scope)
{    ObjectCache objectCache = ObjectCache.get(conf);    List<Extension> extensions = (List<Extension>) objectCache.getObject(URLNormalizer.X_POINT_ID + "_x_" + scope);        if (extensions == EMPTY_EXTENSION_LIST) {        return EMPTY_EXTENSION_LIST;    }    if (extensions == null) {        extensions = findExtensions(scope);        if (extensions != null) {            objectCache.setObject(URLNormalizer.X_POINT_ID + "_x_" + scope, extensions);        } else {                                    objectCache.setObject(URLNormalizer.X_POINT_ID + "_x_" + scope, EMPTY_EXTENSION_LIST);            extensions = EMPTY_EXTENSION_LIST;        }    }    return extensions;}
private List<Extension> nutch_f513_0(String scope)
{    String[] orders = null;    String orderlist = conf.get("urlnormalizer.order." + scope);    if (orderlist == null)        orderlist = conf.get("urlnormalizer.order");    if (orderlist != null && !orderlist.trim().equals("")) {        orders = orderlist.trim().split("\\s+");    }    String scopelist = conf.get("urlnormalizer.scope." + scope);    Set<String> impls = null;    if (scopelist != null && !scopelist.trim().equals("")) {        String[] names = scopelist.split("\\s+");        impls = new HashSet<>(Arrays.asList(names));    }    Extension[] extensions = this.extensionPoint.getExtensions();    HashMap<String, Extension> normalizerExtensions = new HashMap<>();    for (int i = 0; i < extensions.length; i++) {        Extension extension = extensions[i];        if (impls != null && !impls.contains(extension.getClazz()))            continue;        normalizerExtensions.put(extension.getClazz(), extension);    }    List<Extension> res = new ArrayList<>();    if (orders == null) {        res.addAll(normalizerExtensions.values());    } else {                for (int i = 0; i < orders.length; i++) {            Extension e = normalizerExtensions.get(orders[i]);            if (e != null) {                res.add(e);                normalizerExtensions.remove(orders[i]);            }        }                res.addAll(normalizerExtensions.values());    }    return res;}
public String nutch_f514_0(String urlString, String scope) throws MalformedURLException
{        String initialString = urlString;    for (int k = 0; k < loopCount; k++) {        for (int i = 0; i < this.normalizers.length; i++) {            if (urlString == null)                return null;            urlString = this.normalizers[i].normalize(urlString, scope);        }        if (initialString.equals(urlString))            break;        initialString = urlString;    }    return urlString;}
public void nutch_f515_0()
{    noIndex = false;    noFollow = false;    noCache = false;    refresh = false;    refreshTime = 0;    baseHref = null;    refreshHref = null;    generalTags.clear();    httpEquivTags.clear();}
public void nutch_f516_0()
{    noFollow = true;}
public void nutch_f517_0()
{    noIndex = true;}
public void nutch_f518_0()
{    noCache = true;}
public void nutch_f519_0(boolean refresh)
{    this.refresh = refresh;}
public void nutch_f520_0(URL baseHref)
{    this.baseHref = baseHref;}
public void nutch_f521_0(URL refreshHref)
{    this.refreshHref = refreshHref;}
public void nutch_f522_0(int refreshTime)
{    this.refreshTime = refreshTime;}
public boolean nutch_f523_0()
{    return noIndex;}
public boolean nutch_f524_0()
{    return noFollow;}
public boolean nutch_f525_0()
{    return noCache;}
public boolean nutch_f526_0()
{    return refresh;}
public URL nutch_f527_0()
{    return baseHref;}
public URL nutch_f528_0()
{    return refreshHref;}
public int nutch_f529_0()
{    return refreshTime;}
public Metadata nutch_f530_0()
{    return generalTags;}
public Properties nutch_f531_0()
{    return httpEquivTags;}
public String nutch_f532_0()
{    StringBuffer sb = new StringBuffer();    sb.append("base=" + baseHref + ", noCache=" + noCache + ", noFollow=" + noFollow + ", noIndex=" + noIndex + ", refresh=" + refresh + ", refreshHref=" + refreshHref + "\n");    sb.append(" * general tags:\n");    String[] names = generalTags.names();    for (String name : names) {        String key = name;        sb.append("   - " + key + "\t=\t" + generalTags.get(key) + "\n");    }    sb.append(" * http-equiv tags:\n");    Iterator<Object> it = httpEquivTags.keySet().iterator();    it = httpEquivTags.keySet().iterator();    while (it.hasNext()) {        String key = (String) it.next();        sb.append("   - " + key + "\t=\t" + httpEquivTags.get(key) + "\n");    }    return sb.toString();}
public ParseResult nutch_f533_0(Content content, ParseResult parseResult, HTMLMetaTags metaTags, DocumentFragment doc)
{        for (int i = 0; i < this.htmlParseFilters.length; i++) {                parseResult = htmlParseFilters[i].filter(content, parseResult, metaTags, doc);                if (!parseResult.isSuccess()) {                                                parseResult.filter();            return parseResult;        }    }    return parseResult;}
public void nutch_f534_0(DataInput in) throws IOException
{    toUrl = Text.readString(in);    anchor = Text.readString(in);    boolean hasMD = in.readBoolean();    if (hasMD) {        md = new org.apache.hadoop.io.MapWritable();        md.readFields(in);    } else        md = null;}
public static void nutch_f535_0(DataInput in) throws IOException
{        Text.skip(in);        Text.skip(in);    boolean hasMD = in.readBoolean();    if (hasMD) {        MapWritable metadata = new org.apache.hadoop.io.MapWritable();        metadata.readFields(in);        ;    }}
public void nutch_f536_0(DataOutput out) throws IOException
{    Text.writeString(out, toUrl);    Text.writeString(out, anchor);    if (md != null && md.size() > 0) {        out.writeBoolean(true);        md.write(out);    } else {        out.writeBoolean(false);    }}
public static Outlink nutch_f537_0(DataInput in) throws IOException
{    Outlink outlink = new Outlink();    outlink.readFields(in);    return outlink;}
public String nutch_f538_0()
{    return toUrl;}
public void nutch_f539_0(String toUrl)
{    this.toUrl = toUrl;}
public String nutch_f540_0()
{    return anchor;}
public MapWritable nutch_f541_0()
{    return md;}
public void nutch_f542_0(MapWritable md)
{    this.md = md;}
public boolean nutch_f543_0(Object o)
{    if (!(o instanceof Outlink))        return false;    Outlink other = (Outlink) o;    return this.toUrl.equals(other.toUrl) && this.anchor.equals(other.anchor);}
public String nutch_f544_0()
{    StringBuffer repr = new StringBuffer("toUrl: ");    repr.append(toUrl);    repr.append(" anchor: ");    repr.append(anchor);    if (md != null && !md.isEmpty()) {        for (Entry<Writable, Writable> e : md.entrySet()) {            repr.append(" ");            repr.append(e.getKey());            repr.append(": ");            repr.append(e.getValue());        }    }    return repr.toString();}
public int nutch_f545_0()
{    return toUrl.hashCode() ^ anchor.hashCode();}
public static Outlink[] nutch_f546_0(final String plainText, Configuration conf)
{    return OutlinkExtractor.getOutlinks(plainText, "", conf);}
public static Outlink[] nutch_f547_1(final String plainText, String anchor, Configuration conf)
{    if (plainText == null) {        return new Outlink[0];    }    long start = System.currentTimeMillis();    final List<Outlink> outlinks = new ArrayList<>();    try {        Matcher matcher = URL_PATTERN.matcher(plainText);        String url;                while (matcher.find()) {                        if (System.currentTimeMillis() - start >= 60000L) {                if (LOG.isWarnEnabled()) {                                    }                break;            }            url = matcher.group().trim();            try {                outlinks.add(new Outlink(url, anchor));            } catch (MalformedURLException mue) {                            }        }    } catch (Exception ex) {                if (LOG.isErrorEnabled()) {                    }    }    final Outlink[] retval;        if (outlinks.size() > 0) {        retval = outlinks.toArray(new Outlink[0]);    } else {        retval = new Outlink[0];    }    return retval;}
public ParseResult nutch_f548_0() throws Exception
{    return p.getParse(content);}
public ParseStatus nutch_f549_0()
{    return status;}
public String nutch_f550_0()
{    return title;}
public Outlink[] nutch_f551_0()
{    return outlinks;}
public Metadata nutch_f552_0()
{    return contentMeta;}
public Metadata nutch_f553_0()
{    return parseMeta;}
public void nutch_f554_0(Metadata parseMeta)
{    this.parseMeta = parseMeta;}
public void nutch_f555_0(Outlink[] outlinks)
{    this.outlinks = outlinks;}
public String nutch_f556_0(String name)
{    String value = parseMeta.get(name);    if (value == null) {        value = contentMeta.get(name);    }    return value;}
public byte nutch_f557_0()
{    return version;}
public final void nutch_f558_0(DataInput in) throws IOException
{    version = in.readByte();        if (version != VERSION)        throw new VersionMismatchException(VERSION, version);    status = ParseStatus.read(in);        title = Text.readString(in);    int numOutlinks = in.readInt();    outlinks = new Outlink[numOutlinks];    for (int i = 0; i < numOutlinks; i++) {        outlinks[i] = Outlink.read(in);    }    contentMeta.clear();    contentMeta.readFields(in);    parseMeta.clear();    parseMeta.readFields(in);}
public final void nutch_f559_0(DataOutput out) throws IOException
{        out.writeByte(VERSION);        status.write(out);        Text.writeString(out, title);        out.writeInt(outlinks.length);    for (int i = 0; i < outlinks.length; i++) {        outlinks[i].write(out);    }        contentMeta.write(out);    parseMeta.write(out);}
public static ParseData nutch_f560_0(DataInput in) throws IOException
{    ParseData parseText = new ParseData();    parseText.readFields(in);    return parseText;}
public boolean nutch_f561_0(Object o)
{    if (!(o instanceof ParseData))        return false;    ParseData other = (ParseData) o;    return this.status.equals(other.status) && this.title.equals(other.title) && Arrays.equals(this.outlinks, other.outlinks) && this.contentMeta.equals(other.contentMeta) && this.parseMeta.equals(other.parseMeta);}
public String nutch_f562_0()
{    StringBuffer buffer = new StringBuffer();    buffer.append("Version: " + version + "\n");    buffer.append("Status: " + status + "\n");    buffer.append("Title: " + title + "\n");    if (outlinks != null) {        buffer.append("Outlinks: " + outlinks.length + "\n");        for (int i = 0; i < outlinks.length; i++) {            buffer.append("  outlink: " + outlinks[i] + "\n");        }    }    buffer.append("Content Metadata: " + contentMeta + "\n");    buffer.append("Parse Metadata: " + parseMeta + "\n");    return buffer.toString();}
public static void nutch_f563_0(String[] argv) throws Exception
{    String usage = "ParseData (-local | -dfs <namenode:port>) recno segment";    if (argv.length < 3) {        System.out.println("usage:" + usage);        return;    }    Options opts = new Options();    Configuration conf = NutchConfiguration.create();    GenericOptionsParser parser = new GenericOptionsParser(conf, opts, argv);    String[] remainingArgs = parser.getRemainingArgs();    try (FileSystem fs = FileSystem.get(conf)) {        int recno = Integer.parseInt(remainingArgs[0]);        String segment = remainingArgs[1];        Path file = new Path(segment, DIR_NAME);        System.out.println("Reading from file: " + file);        ArrayFile.Reader parses = new ArrayFile.Reader(fs, file.toString(), conf);        ParseData parseDatum = new ParseData();        parses.get(recno, parseDatum);        System.out.println("Retrieved " + recno + " from file " + file);        System.out.println(parseDatum);        parses.close();    }}
public String nutch_f564_0()
{    return text.getText();}
public ParseData nutch_f565_0()
{    return data;}
public boolean nutch_f566_0()
{    return isCanonical;}
public final void nutch_f567_0(DataOutput out) throws IOException
{    out.writeBoolean(isCanonical);    text.write(out);    data.write(out);}
public void nutch_f568_0(DataInput in) throws IOException
{    isCanonical = in.readBoolean();    text = new ParseText();    text.readFields(in);    data = new ParseData();    data.readFields(in);}
public static ParseImpl nutch_f569_0(DataInput in) throws IOException
{    ParseImpl parseImpl = new ParseImpl();    parseImpl.readFields(in);    return parseImpl;}
public Text nutch_f570_0()
{    return key;}
public CrawlDatum nutch_f571_0()
{    return value;}
public CrawlDatum nutch_f572_0(CrawlDatum value)
{    this.value = value;    return this.value;}
public OutputCommitter nutch_f573_0(TaskAttemptContext context) throws IOException
{    Path path = FileOutputFormat.getOutputPath(context);    return new FileOutputCommitter(path, context);}
public void nutch_f574_0(JobContext context) throws IOException
{    Configuration conf = context.getConfiguration();    Path out = FileOutputFormat.getOutputPath(context);    FileSystem fs = out.getFileSystem(context.getConfiguration());    if ((out == null) && (context.getNumReduceTasks() != 0)) {        throw new IOException("Output directory not set in JobContext.");    }    if (fs == null) {        fs = out.getFileSystem(conf);    }    if (fs.exists(new Path(out, CrawlDatum.PARSE_DIR_NAME))) {        throw new IOException("Segment already parsed!");    }}
public String nutch_f575_0(TaskAttemptContext context, String name)
{    TaskID taskId = context.getTaskAttemptID().getTaskID();    int partition = taskId.getId();    StringBuilder result = new StringBuilder();    result.append(name);    result.append('-');    result.append(TaskID.getRepresentingCharacter(taskId.getTaskType()));    result.append('-');    result.append(NUMBER_FORMAT.format(partition));    return result.toString();}
public RecordWriter<Text, Parse> nutch_f576_1(TaskAttemptContext context) throws IOException
{    Configuration conf = context.getConfiguration();    String name = getUniqueFile(context, "part");    Path dir = FileOutputFormat.getOutputPath(context);    FileSystem fs = dir.getFileSystem(context.getConfiguration());    if (conf.getBoolean("parse.filter.urls", true)) {        filters = new URLFilters(conf);        exemptionFilters = new URLExemptionFilters(conf);    }    if (conf.getBoolean("parse.normalize.urls", true)) {        normalizers = new URLNormalizers(conf, URLNormalizers.SCOPE_OUTLINK);    }    this.scfilters = new ScoringFilters(conf);    final int interval = conf.getInt("db.fetch.interval.default", 2592000);    final boolean ignoreInternalLinks = conf.getBoolean("db.ignore.internal.links", false);    final boolean ignoreExternalLinks = conf.getBoolean("db.ignore.external.links", false);    final String ignoreExternalLinksMode = conf.get("db.ignore.external.links.mode", "byHost");            final boolean storeText = conf.getBoolean("parser.store.text", true);    int maxOutlinksPerPage = conf.getInt("db.max.outlinks.per.page", 100);    final int maxOutlinks = (maxOutlinksPerPage < 0) ? Integer.MAX_VALUE : maxOutlinksPerPage;    int maxOutlinkL = conf.getInt("db.max.outlink.length", 4096);    final int maxOutlinkLength = (maxOutlinkL < 0) ? Integer.MAX_VALUE : maxOutlinkL;    final boolean isParsing = conf.getBoolean("fetcher.parse", true);    final CompressionType compType = SequenceFileOutputFormat.getOutputCompressionType(context);    Path out = FileOutputFormat.getOutputPath(context);    Path text = new Path(new Path(out, ParseText.DIR_NAME), name);    Path data = new Path(new Path(out, ParseData.DIR_NAME), name);    Path crawl = new Path(new Path(out, CrawlDatum.PARSE_DIR_NAME), name);    final String[] parseMDtoCrawlDB = conf.get("db.parsemeta.to.crawldb", "").split(" *, *");        final MapFile.Writer textOut;    if (storeText) {        Option tKeyClassOpt = (Option) MapFile.Writer.keyClass(Text.class);        org.apache.hadoop.io.SequenceFile.Writer.Option tValClassOpt = SequenceFile.Writer.valueClass(ParseText.class);        org.apache.hadoop.io.SequenceFile.Writer.Option tProgressOpt = SequenceFile.Writer.progressable((Progressable) context);        org.apache.hadoop.io.SequenceFile.Writer.Option tCompOpt = SequenceFile.Writer.compression(CompressionType.RECORD);        textOut = new MapFile.Writer(conf, text, tKeyClassOpt, tValClassOpt, tCompOpt, tProgressOpt);    } else {        textOut = null;    }        Option dKeyClassOpt = (Option) MapFile.Writer.keyClass(Text.class);    org.apache.hadoop.io.SequenceFile.Writer.Option dValClassOpt = SequenceFile.Writer.valueClass(ParseData.class);    org.apache.hadoop.io.SequenceFile.Writer.Option dProgressOpt = SequenceFile.Writer.progressable((Progressable) context);    org.apache.hadoop.io.SequenceFile.Writer.Option dCompOpt = SequenceFile.Writer.compression(compType);    final MapFile.Writer dataOut = new MapFile.Writer(conf, data, dKeyClassOpt, dValClassOpt, dCompOpt, dProgressOpt);    final SequenceFile.Writer crawlOut = SequenceFile.createWriter(conf, SequenceFile.Writer.file(crawl), SequenceFile.Writer.keyClass(Text.class), SequenceFile.Writer.valueClass(CrawlDatum.class), SequenceFile.Writer.bufferSize(fs.getConf().getInt("io.file.buffer.size", 4096)), SequenceFile.Writer.replication(fs.getDefaultReplication(crawl)), SequenceFile.Writer.blockSize(1073741824), SequenceFile.Writer.compression(compType, new DefaultCodec()), SequenceFile.Writer.progressable((Progressable) context), SequenceFile.Writer.metadata(new Metadata()));    return new RecordWriter<Text, Parse>() {        public void write(Text key, Parse parse) throws IOException {            String fromUrl = key.toString();                        String origin = null;            if (textOut != null) {                textOut.append(key, new ParseText(parse.getText()));            }            ParseData parseData = parse.getData();                        String sig = parseData.getContentMeta().get(Nutch.SIGNATURE_KEY);            if (sig != null) {                byte[] signature = StringUtil.fromHexString(sig);                if (signature != null) {                                        CrawlDatum d = new CrawlDatum(CrawlDatum.STATUS_SIGNATURE, 0);                    d.setSignature(signature);                    crawlOut.append(key, d);                }            }                                    CrawlDatum parseMDCrawlDatum = null;            for (String mdname : parseMDtoCrawlDB) {                String mdvalue = parse.getData().getParseMeta().get(mdname);                if (mdvalue != null) {                    if (parseMDCrawlDatum == null)                        parseMDCrawlDatum = new CrawlDatum(CrawlDatum.STATUS_PARSE_META, 0);                    parseMDCrawlDatum.getMetaData().put(new Text(mdname), new Text(mdvalue));                }            }            if (parseMDCrawlDatum != null)                crawlOut.append(key, parseMDCrawlDatum);                        if (ignoreExternalLinks || ignoreInternalLinks) {                URL originURL = new URL(fromUrl.toString());                                if ("bydomain".equalsIgnoreCase(ignoreExternalLinksMode)) {                    origin = URLUtil.getDomainName(originURL).toLowerCase();                } else                 {                    origin = originURL.getHost().toLowerCase();                }            }            ParseStatus pstatus = parseData.getStatus();            if (pstatus != null && pstatus.isSuccess() && pstatus.getMinorCode() == ParseStatus.SUCCESS_REDIRECT) {                String newUrl = pstatus.getMessage();                int refreshTime = Integer.valueOf(pstatus.getArgs()[1]);                newUrl = filterNormalize(fromUrl, newUrl, origin, ignoreInternalLinks, ignoreExternalLinks, ignoreExternalLinksMode, filters, exemptionFilters, normalizers, URLNormalizers.SCOPE_FETCHER);                if (newUrl != null) {                    String reprUrl = URLUtil.chooseRepr(fromUrl, newUrl, refreshTime < Fetcher.PERM_REFRESH_TIME);                    CrawlDatum newDatum = new CrawlDatum();                    newDatum.setStatus(CrawlDatum.STATUS_LINKED);                    if (reprUrl != null && !reprUrl.equals(newUrl)) {                        newDatum.getMetaData().put(Nutch.WRITABLE_REPR_URL_KEY, new Text(reprUrl));                    }                    crawlOut.append(new Text(newUrl), newDatum);                }            }                        Outlink[] links = parseData.getOutlinks();            int outlinksToStore = Math.min(maxOutlinks, links.length);            int validCount = 0;            CrawlDatum adjust = null;            List<Entry<Text, CrawlDatum>> targets = new ArrayList<>(outlinksToStore);            List<Outlink> outlinkList = new ArrayList<>(outlinksToStore);            for (int i = 0; i < links.length && validCount < outlinksToStore; i++) {                String toUrl = links[i].getToUrl();                                if (!isParsing) {                    if (toUrl.length() > maxOutlinkLength) {                        continue;                    }                    toUrl = ParseOutputFormat.filterNormalize(fromUrl, toUrl, origin, ignoreInternalLinks, ignoreExternalLinks, ignoreExternalLinksMode, filters, exemptionFilters, normalizers);                    if (toUrl == null) {                        continue;                    }                }                CrawlDatum target = new CrawlDatum(CrawlDatum.STATUS_LINKED, interval);                Text targetUrl = new Text(toUrl);                                                                MapWritable outlinkMD = links[i].getMetadata();                if (outlinkMD != null) {                    target.getMetaData().putAll(outlinkMD);                }                try {                    scfilters.initialScore(targetUrl, target);                } catch (ScoringFilterException e) {                                        target.setScore(0.0f);                }                targets.add(new SimpleEntry(targetUrl, target));                                links[i].setUrl(toUrl);                outlinkList.add(links[i]);                validCount++;            }            try {                                adjust = scfilters.distributeScoreToOutlinks(key, parseData, targets, null, links.length);            } catch (ScoringFilterException e) {                            }            for (Entry<Text, CrawlDatum> target : targets) {                crawlOut.append(target.getKey(), target.getValue());            }            if (adjust != null)                crawlOut.append(key, adjust);            Outlink[] filteredLinks = outlinkList.toArray(new Outlink[outlinkList.size()]);            parseData = new ParseData(parseData.getStatus(), parseData.getTitle(), filteredLinks, parseData.getContentMeta(), parseData.getParseMeta());            dataOut.append(key, parseData);            if (!parse.isCanonical()) {                CrawlDatum datum = new CrawlDatum();                datum.setStatus(CrawlDatum.STATUS_FETCH_SUCCESS);                String timeString = parse.getData().getContentMeta().get(Nutch.FETCH_TIME_KEY);                try {                    datum.setFetchTime(Long.parseLong(timeString));                } catch (Exception e) {                                        datum.setFetchTime(System.currentTimeMillis());                }                crawlOut.append(key, datum);            }        }        public void close(TaskAttemptContext context) throws IOException {            if (textOut != null)                textOut.close();            dataOut.close();            crawlOut.close();        }    };}
public void nutch_f577_1(Text key, Parse parse) throws IOException
{    String fromUrl = key.toString();        String origin = null;    if (textOut != null) {        textOut.append(key, new ParseText(parse.getText()));    }    ParseData parseData = parse.getData();        String sig = parseData.getContentMeta().get(Nutch.SIGNATURE_KEY);    if (sig != null) {        byte[] signature = StringUtil.fromHexString(sig);        if (signature != null) {                        CrawlDatum d = new CrawlDatum(CrawlDatum.STATUS_SIGNATURE, 0);            d.setSignature(signature);            crawlOut.append(key, d);        }    }            CrawlDatum parseMDCrawlDatum = null;    for (String mdname : parseMDtoCrawlDB) {        String mdvalue = parse.getData().getParseMeta().get(mdname);        if (mdvalue != null) {            if (parseMDCrawlDatum == null)                parseMDCrawlDatum = new CrawlDatum(CrawlDatum.STATUS_PARSE_META, 0);            parseMDCrawlDatum.getMetaData().put(new Text(mdname), new Text(mdvalue));        }    }    if (parseMDCrawlDatum != null)        crawlOut.append(key, parseMDCrawlDatum);        if (ignoreExternalLinks || ignoreInternalLinks) {        URL originURL = new URL(fromUrl.toString());                if ("bydomain".equalsIgnoreCase(ignoreExternalLinksMode)) {            origin = URLUtil.getDomainName(originURL).toLowerCase();        } else         {            origin = originURL.getHost().toLowerCase();        }    }    ParseStatus pstatus = parseData.getStatus();    if (pstatus != null && pstatus.isSuccess() && pstatus.getMinorCode() == ParseStatus.SUCCESS_REDIRECT) {        String newUrl = pstatus.getMessage();        int refreshTime = Integer.valueOf(pstatus.getArgs()[1]);        newUrl = filterNormalize(fromUrl, newUrl, origin, ignoreInternalLinks, ignoreExternalLinks, ignoreExternalLinksMode, filters, exemptionFilters, normalizers, URLNormalizers.SCOPE_FETCHER);        if (newUrl != null) {            String reprUrl = URLUtil.chooseRepr(fromUrl, newUrl, refreshTime < Fetcher.PERM_REFRESH_TIME);            CrawlDatum newDatum = new CrawlDatum();            newDatum.setStatus(CrawlDatum.STATUS_LINKED);            if (reprUrl != null && !reprUrl.equals(newUrl)) {                newDatum.getMetaData().put(Nutch.WRITABLE_REPR_URL_KEY, new Text(reprUrl));            }            crawlOut.append(new Text(newUrl), newDatum);        }    }        Outlink[] links = parseData.getOutlinks();    int outlinksToStore = Math.min(maxOutlinks, links.length);    int validCount = 0;    CrawlDatum adjust = null;    List<Entry<Text, CrawlDatum>> targets = new ArrayList<>(outlinksToStore);    List<Outlink> outlinkList = new ArrayList<>(outlinksToStore);    for (int i = 0; i < links.length && validCount < outlinksToStore; i++) {        String toUrl = links[i].getToUrl();                if (!isParsing) {            if (toUrl.length() > maxOutlinkLength) {                continue;            }            toUrl = ParseOutputFormat.filterNormalize(fromUrl, toUrl, origin, ignoreInternalLinks, ignoreExternalLinks, ignoreExternalLinksMode, filters, exemptionFilters, normalizers);            if (toUrl == null) {                continue;            }        }        CrawlDatum target = new CrawlDatum(CrawlDatum.STATUS_LINKED, interval);        Text targetUrl = new Text(toUrl);                                MapWritable outlinkMD = links[i].getMetadata();        if (outlinkMD != null) {            target.getMetaData().putAll(outlinkMD);        }        try {            scfilters.initialScore(targetUrl, target);        } catch (ScoringFilterException e) {                        target.setScore(0.0f);        }        targets.add(new SimpleEntry(targetUrl, target));                links[i].setUrl(toUrl);        outlinkList.add(links[i]);        validCount++;    }    try {                adjust = scfilters.distributeScoreToOutlinks(key, parseData, targets, null, links.length);    } catch (ScoringFilterException e) {            }    for (Entry<Text, CrawlDatum> target : targets) {        crawlOut.append(target.getKey(), target.getValue());    }    if (adjust != null)        crawlOut.append(key, adjust);    Outlink[] filteredLinks = outlinkList.toArray(new Outlink[outlinkList.size()]);    parseData = new ParseData(parseData.getStatus(), parseData.getTitle(), filteredLinks, parseData.getContentMeta(), parseData.getParseMeta());    dataOut.append(key, parseData);    if (!parse.isCanonical()) {        CrawlDatum datum = new CrawlDatum();        datum.setStatus(CrawlDatum.STATUS_FETCH_SUCCESS);        String timeString = parse.getData().getContentMeta().get(Nutch.FETCH_TIME_KEY);        try {            datum.setFetchTime(Long.parseLong(timeString));        } catch (Exception e) {                        datum.setFetchTime(System.currentTimeMillis());        }        crawlOut.append(key, datum);    }}
public void nutch_f578_0(TaskAttemptContext context) throws IOException
{    if (textOut != null)        textOut.close();    dataOut.close();    crawlOut.close();}
public static String nutch_f579_0(String fromUrl, String toUrl, String fromHost, boolean ignoreInternalLinks, boolean ignoreExternalLinks, String ignoreExternalLinksMode, URLFilters filters, URLExemptionFilters exemptionFilters, URLNormalizers normalizers)
{    return filterNormalize(fromUrl, toUrl, fromHost, ignoreInternalLinks, ignoreExternalLinks, ignoreExternalLinksMode, filters, exemptionFilters, normalizers, URLNormalizers.SCOPE_OUTLINK);}
public static String nutch_f580_0(String fromUrl, String toUrl, String origin, boolean ignoreInternalLinks, boolean ignoreExternalLinks, String ignoreExternalLinksMode, URLFilters filters, URLExemptionFilters exemptionFilters, URLNormalizers normalizers, String urlNormalizerScope)
{        if (fromUrl.equals(toUrl)) {        return null;    }    if (ignoreExternalLinks || ignoreInternalLinks) {        URL targetURL = null;        try {            targetURL = new URL(toUrl);        } catch (MalformedURLException e1) {                        return null;        }        if (ignoreExternalLinks) {            if ("bydomain".equalsIgnoreCase(ignoreExternalLinksMode)) {                String toDomain = URLUtil.getDomainName(targetURL).toLowerCase();                                if (toDomain == null || !toDomain.equals(origin)) {                                        return null;                }            } else {                String toHost = targetURL.getHost().toLowerCase();                if (!toHost.equals(origin)) {                                        if (                    exemptionFilters == null || !exemptionFilters.isExempted(fromUrl, toUrl)) {                                                return null;                    }                }            }        }        if (ignoreInternalLinks) {            if ("bydomain".equalsIgnoreCase(ignoreExternalLinksMode)) {                String toDomain = URLUtil.getDomainName(targetURL).toLowerCase();                                if (toDomain == null || toDomain.equals(origin)) {                                        return null;                }            } else {                String toHost = targetURL.getHost().toLowerCase();                                if (toHost == null || toHost.equals(origin)) {                                        return null;                }            }        }    }    try {        if (normalizers != null) {                        toUrl = normalizers.normalize(toUrl, urlNormalizerScope);                }        if (filters != null) {                        toUrl = filters.filter(toUrl);        }        if (toUrl == null) {            return null;        }    } catch (Exception e) {        return null;    }    return toUrl;}
 List<String> nutch_f581_0(String mimeType)
{    return fMimeTypeToPluginMap.get(mimeType);}
 void nutch_f582_0(Map<String, String> aliases)
{    this.aliases = aliases;}
 Map<String, String> nutch_f583_0()
{    return aliases;}
 void nutch_f584_0(String mimeType, List<String> l)
{    fMimeTypeToPluginMap.put(mimeType, l);}
 List<String> nutch_f585_0()
{    return Arrays.asList(fMimeTypeToPluginMap.keySet().toArray(new String[] {}));}
public ParsePluginList nutch_f586_1(Configuration conf)
{    ParsePluginList pList = new ParsePluginList();        DocumentBuilderFactory factory = null;    DocumentBuilder parser = null;    Document document = null;    InputSource inputSource = null;    InputStream ppInputStream = null;    if (fParsePluginsFile != null) {        URL parsePluginUrl = null;        try {            parsePluginUrl = new URL(fParsePluginsFile);            ppInputStream = parsePluginUrl.openStream();        } catch (Exception e) {            if (LOG.isWarnEnabled()) {                            }            return pList;        }    } else {        ppInputStream = conf.getConfResourceAsInputStream(conf.get(PP_FILE_PROP));    }    inputSource = new InputSource(ppInputStream);    try {        factory = DocumentBuilderFactory.newInstance();        parser = factory.newDocumentBuilder();        document = parser.parse(inputSource);    } catch (Exception e) {        if (LOG.isWarnEnabled()) {                    }        return null;    }    Element parsePlugins = document.getDocumentElement();        Map<String, String> aliases = getAliases(parsePlugins);        pList.setAliases(aliases);        NodeList mimeTypes = parsePlugins.getElementsByTagName("mimeType");        for (int i = 0; i < mimeTypes.getLength(); i++) {        Element mimeType = (Element) mimeTypes.item(i);        String mimeTypeStr = mimeType.getAttribute("name");                NodeList pluginList = mimeType.getElementsByTagName("plugin");                if (pluginList != null && pluginList.getLength() > 0) {            List<String> plugList = new ArrayList<>(pluginList.getLength());            for (int j = 0; j < pluginList.getLength(); j++) {                Element plugin = (Element) pluginList.item(j);                String pluginId = plugin.getAttribute("id");                String extId = aliases.get(pluginId);                if (extId == null) {                                        extId = pluginId;                }                String orderStr = plugin.getAttribute("order");                int order = -1;                try {                    order = Integer.parseInt(orderStr);                } catch (NumberFormatException ignore) {                }                if (order != -1) {                    plugList.add(order - 1, extId);                } else {                    plugList.add(extId);                }            }                        pList.setPluginList(mimeTypeStr, plugList);        } else if (LOG.isWarnEnabled()) {                    }    }    return pList;}
public static void nutch_f587_0(String[] args) throws Exception
{    String parsePluginFile = null;    String usage = "ParsePluginsReader [--file <parse plugin file location>]";    if ((args.length != 0 && args.length != 2) || (args.length == 2 && !"--file".equals(args[0]))) {        System.err.println(usage);        System.exit(1);    }    for (int i = 0; i < args.length; i++) {        if (args[i].equals("--file")) {            parsePluginFile = args[++i];        }    }    ParsePluginsReader reader = new ParsePluginsReader();    if (parsePluginFile != null) {        reader.setFParsePluginsFile(parsePluginFile);    }    ParsePluginList prefs = reader.parse(NutchConfiguration.create());    for (String mimeType : prefs.getSupportedMimeTypes()) {        System.out.println("MIMETYPE: " + mimeType);        List<String> plugList = prefs.getPluginList(mimeType);        System.out.println("EXTENSION IDs:");        for (String j : plugList) {            System.out.println(j);        }    }}
public String nutch_f588_0()
{    return fParsePluginsFile;}
public void nutch_f589_0(String parsePluginsFile)
{    fParsePluginsFile = parsePluginsFile;}
private Map<String, String> nutch_f590_1(Element parsePluginsRoot)
{    Map<String, String> aliases = new HashMap<>();    NodeList aliasRoot = parsePluginsRoot.getElementsByTagName("aliases");    if (aliasRoot == null || aliasRoot.getLength() == 0) {        if (LOG.isWarnEnabled()) {                    }        return aliases;    }    if (aliasRoot.getLength() > 1) {                if (LOG.isWarnEnabled()) {                    }    }    Element aliasRootElem = (Element) aliasRoot.item(0);    NodeList aliasElements = aliasRootElem.getElementsByTagName("alias");    if (aliasElements != null && aliasElements.getLength() > 0) {        for (int i = 0; i < aliasElements.getLength(); i++) {            Element aliasElem = (Element) aliasElements.item(i);            String parsePluginId = aliasElem.getAttribute("name");            String extensionId = aliasElem.getAttribute("extension-id");            if (LOG.isTraceEnabled()) {                LOG.trace("Found alias: plugin-id: " + parsePluginId + ", extension-id: " + extensionId);            }            if (parsePluginId != null && extensionId != null) {                aliases.put(parsePluginId, extensionId);            }        }    }    return aliases;}
public int nutch_f591_0(String[] args) throws Exception
{    String url = null;    String usage =     "Usage:\n" +     "  ParserChecker [OPTIONS] <url>\n" +     "    Fetch single URL and parse it\n" +     "  ParserChecker [OPTIONS] -stdin\n" +     "    Read URLs to be parsed from stdin\n" +     "  ParserChecker [OPTIONS] -listen <port> [-keepClientCnxOpen]\n" +     "    Listen on <port> for URLs to be parsed\n" +     "Options:\n" +     "  -D<property>=<value>\tset/overwrite Nutch/Hadoop properties\n" +     "                  \t(a generic Hadoop option to be passed\n" + "                  \t before other command-specific options)\n" +     "  -normalize      \tnormalize URLs\n" +     "  -followRedirects\tfollow redirects when fetching URL\n" +     "  -dumpText       \talso show the plain-text extracted by parsers\n" +     "  -forceAs <mimeType>\tforce parsing as <mimeType>\n" + "  -md <key>=<value>\tmetadata added to CrawlDatum before parsing\n";        if (args.length < 1) {        System.err.println(usage);        System.exit(-1);    }    int numConsumed;    for (int i = 0; i < args.length; i++) {        if (args[i].equals("-normalize")) {            normalizers = new URLNormalizers(getConf(), URLNormalizers.SCOPE_DEFAULT);        } else if (args[i].equals("-followRedirects")) {            followRedirects = true;        } else if (args[i].equals("-forceAs")) {            forceAsContentType = args[++i];        } else if (args[i].equals("-dumpText")) {            dumpText = true;        } else if (args[i].equals("-md")) {            String k = null, v = null;            String nextOne = args[++i];            int firstEquals = nextOne.indexOf("=");            if (firstEquals != -1) {                k = nextOne.substring(0, firstEquals);                v = nextOne.substring(firstEquals + 1);            } else                k = nextOne;            metadata.put(k, v);        } else if ((numConsumed = super.parseArgs(args, i)) > 0) {            i += numConsumed - 1;        } else if (i != args.length - 1) {            System.err.println("ERR: Not a recognized argument: " + args[i]);            System.err.println(usage);            System.exit(-1);        } else {            url = args[i];        }    }    scfilters = new ScoringFilters(getConf());    if (url != null) {        return super.processSingle(url);    } else {                return super.run();    }}
protected int nutch_f592_1(String url, StringBuilder output) throws Exception
{    if (normalizers != null) {        url = normalizers.normalize(url, URLNormalizers.SCOPE_DEFAULT);    }        CrawlDatum datum = new CrawlDatum();    Iterator<String> iter = metadata.keySet().iterator();    while (iter.hasNext()) {        String key = iter.next();        String value = metadata.get(key);        if (value == null)            value = "";        datum.getMetaData().put(new Text(key), new Text(value));    }    int maxRedirects = getConf().getInt("http.redirect.max", 3);    if (followRedirects) {        if (maxRedirects == 0) {                        maxRedirects = 3;        } else {                    }    }    ProtocolOutput protocolOutput = getProtocolOutput(url, datum);    Text turl = new Text(url);        int numRedirects = 0;    while (!protocolOutput.getStatus().isSuccess() && followRedirects && protocolOutput.getStatus().isRedirect() && maxRedirects >= numRedirects) {        String[] stuff = protocolOutput.getStatus().getArgs();        url = stuff[0];                if (normalizers != null) {            url = normalizers.normalize(url, URLNormalizers.SCOPE_DEFAULT);        }        turl.set(url);                protocolOutput = getProtocolOutput(url, datum);        numRedirects++;    }    if (!protocolOutput.getStatus().isSuccess()) {        System.err.println("Fetch failed with protocol status: " + protocolOutput.getStatus());        if (protocolOutput.getStatus().isRedirect()) {            System.err.println("Redirect(s) not handled due to configuration.");            System.err.println("Max Redirects to handle per config: " + maxRedirects);            System.err.println("Number of Redirects handled: " + numRedirects);        }        return -1;    }    Content content = protocolOutput.getContent();    if (content == null) {        output.append("No content for " + url + "\n");        return 0;    }    String contentType;    if (forceAsContentType != null) {        content.setContentType(forceAsContentType);        contentType = forceAsContentType;    } else {        contentType = content.getContentType();    }    if (contentType == null) {                return -1;    }        datum.getMetaData().put(new Text(Metadata.CONTENT_TYPE), new Text(contentType));    if (ParseSegment.isTruncated(content)) {            }        try {        scfilters.passScoreBeforeParsing(turl, datum, content);    } catch (Exception e) {        if (LOG.isWarnEnabled()) {                                }    }    ParseResult parseResult = new ParseUtil(getConf()).parse(content);    if (parseResult == null) {                return (-1);    }        byte[] signature = SignatureFactory.getSignature(getConf()).calculate(content, parseResult.get(new Text(url)));    if (LOG.isInfoEnabled()) {                            }    for (Map.Entry<Text, Parse> entry : parseResult) {        turl = entry.getKey();        Parse parse = entry.getValue();                try {            scfilters.passScoreAfterParsing(turl, content, parse);        } catch (Exception e) {            if (LOG.isWarnEnabled()) {                                            }        }        output.append(turl + "\n");        output.append(parse.getData() + "\n");        if (dumpText) {            output.append(parse.getText());        }    }    return 0;}
public static void nutch_f593_0(String[] args) throws Exception
{    int res = ToolRunner.run(NutchConfiguration.create(), new ParserChecker(), args);    System.exit(res);}
public static ParseResult nutch_f594_0(String url, Parse parse)
{    ParseResult parseResult = new ParseResult(url);    parseResult.put(new Text(url), new ParseText(parse.getText()), parse.getData());    return parseResult;}
public boolean nutch_f595_0()
{    return parseMap.isEmpty();}
public int nutch_f596_0()
{    return parseMap.size();}
public Parse nutch_f597_0(String key)
{    return get(new Text(key));}
public Parse nutch_f598_0(Text key)
{    return parseMap.get(key);}
public void nutch_f599_0(Text key, ParseText text, ParseData data)
{    put(key.toString(), text, data);}
public void nutch_f600_0(String key, ParseText text, ParseData data)
{    parseMap.put(new Text(key), new ParseImpl(text, data, key.equals(originalUrl)));}
public Iterator<Entry<Text, Parse>> nutch_f601_0()
{    return parseMap.entrySet().iterator();}
public void nutch_f602_1()
{    for (Iterator<Entry<Text, Parse>> i = iterator(); i.hasNext(); ) {        Entry<Text, Parse> entry = i.next();        if (!entry.getValue().getData().getStatus().isSuccess()) {                        i.remove();        }    }}
public boolean nutch_f603_0()
{    for (Iterator<Entry<Text, Parse>> i = iterator(); i.hasNext(); ) {        Entry<Text, Parse> entry = i.next();        if (!entry.getValue().getData().getStatus().isSuccess()) {            return false;        }    }    return true;}
public boolean nutch_f604_0()
{    for (Iterator<Entry<Text, Parse>> i = iterator(); i.hasNext(); ) {        Entry<Text, Parse> entry = i.next();        if (entry.getValue().getData().getStatus().isSuccess()) {            return true;        }    }    return false;}
public Parser[] nutch_f605_1(String contentType, String url) throws ParserNotFound
{    List<Parser> parsers = null;    List<Extension> parserExts = null;    ObjectCache objectCache = ObjectCache.get(conf);                                parserExts = getExtensions(contentType);    if (parserExts == null) {        throw new ParserNotFound(url, contentType);    }    parsers = new Vector<>(parserExts.size());    for (Iterator<Extension> i = parserExts.iterator(); i.hasNext(); ) {        Extension ext = i.next();        Parser p = null;        try {                        p = (Parser) objectCache.getObject(ext.getId());            if (p == null) {                                p = (Parser) ext.getExtensionInstance();                objectCache.setObject(ext.getId(), p);            }            parsers.add(p);        } catch (PluginRuntimeException e) {            if (LOG.isWarnEnabled()) {                            }        }    }    return parsers.toArray(new Parser[] {});}
public Parser nutch_f606_1(String id) throws ParserNotFound
{    Extension[] extensions = this.extensionPoint.getExtensions();    Extension parserExt = null;    ObjectCache objectCache = ObjectCache.get(conf);    if (id != null) {        parserExt = getExtension(extensions, id);    }    if (parserExt == null) {        parserExt = getExtensionFromAlias(extensions, id);    }    if (parserExt == null) {        throw new ParserNotFound("No Parser Found for id [" + id + "]");    }        if (objectCache.getObject(parserExt.getId()) != null) {        return (Parser) objectCache.getObject(parserExt.getId());        } else {        try {            Parser p = (Parser) parserExt.getExtensionInstance();            objectCache.setObject(parserExt.getId(), p);            return p;        } catch (PluginRuntimeException e) {            if (LOG.isWarnEnabled()) {                            }            throw new ParserNotFound("Cannot init parser for id [" + id + "]");        }    }}
protected List<Extension> nutch_f607_0(String contentType)
{    ObjectCache objectCache = ObjectCache.get(conf);        String type = null;    type = MimeUtil.cleanMimeType(contentType);    List<Extension> extensions = (List<Extension>) objectCache.getObject(type);        if (extensions == EMPTY_EXTENSION_LIST) {        return null;    }    if (extensions == null) {        extensions = findExtensions(type);        if (extensions != null) {            objectCache.setObject(type, extensions);        } else {                                    objectCache.setObject(type, EMPTY_EXTENSION_LIST);        }    }    return extensions;}
private List<Extension> nutch_f608_0(String contentType)
{    Extension[] extensions = this.extensionPoint.getExtensions();        List<String> parsePluginList = this.parsePluginList.getPluginList(contentType);    List<Extension> extensionList = matchExtensions(parsePluginList, extensions, contentType);    if (extensionList != null) {        return extensionList;    }        parsePluginList = this.parsePluginList.getPluginList(DEFAULT_PLUGIN);    return matchExtensions(parsePluginList, extensions, DEFAULT_PLUGIN);}
private List<Extension> nutch_f609_1(List<String> plugins, Extension[] extensions, String contentType)
{    List<Extension> extList = new ArrayList<>();    if (plugins != null) {        for (String parsePluginId : plugins) {            Extension ext = getExtension(extensions, parsePluginId, contentType);            if (ext == null) {                                ext = getExtension(extensions, parsePluginId);                if (LOG.isWarnEnabled()) {                    if (ext != null) {                                                                                                                    } else {                                                                    }                }            }            if (ext != null) {                                extList.add(ext);            }        }    } else {        for (int i = 0; i < extensions.length; i++) {            if ("*".equals(extensions[i].getAttribute("contentType"))) {                extList.add(0, extensions[i]);            } else if (extensions[i].getAttribute("contentType") != null && contentType.matches(escapeContentType(extensions[i].getAttribute("contentType")))) {                extList.add(extensions[i]);            }        }        if (extList.size() > 0) {            if (LOG.isInfoEnabled()) {                StringBuffer extensionsIDs = new StringBuffer("[");                boolean isFirst = true;                for (Extension ext : extList) {                    if (!isFirst)                        extensionsIDs.append(" - ");                    else                        isFirst = false;                    extensionsIDs.append(ext.getId());                }                extensionsIDs.append("]");                            }        } else if (LOG.isDebugEnabled()) {                    }    }    return (extList.size() > 0) ? extList : null;}
private String nutch_f610_0(String contentType)
{        return contentType.replace("+", "\\+").replace(".", "\\.");}
private boolean nutch_f611_0(Extension extension, String id, String type)
{    return ((id.equals(extension.getId())) && (extension.getAttribute("contentType").equals("*") || type.matches(escapeContentType(extension.getAttribute("contentType"))) || type.equals(DEFAULT_PLUGIN)));}
private Extension nutch_f612_0(Extension[] list, String id, String type)
{    for (int i = 0; i < list.length; i++) {        if (match(list[i], id, type)) {            return list[i];        }    }    return null;}
private Extension nutch_f613_0(Extension[] list, String id)
{    for (int i = 0; i < list.length; i++) {        if (id.equals(list[i].getId())) {            return list[i];        }    }    return null;}
private Extension nutch_f614_0(Extension[] list, String id)
{    return getExtension(list, parsePluginList.getAliases().get(id));}
public String nutch_f615_0()
{    return url;}
public String nutch_f616_0()
{    return contentType;}
public void nutch_f617_0(Mapper<WritableComparable<?>, Content, Text, ParseImpl>.Context context)
{    Configuration conf = context.getConfiguration();    scfilters = new ScoringFilters(conf);    skipTruncated = conf.getBoolean(SKIP_TRUNCATED, true);}
public void nutch_f618_0(Context context)
{}
public void nutch_f619_1(WritableComparable<?> key, Content content, Context context) throws IOException, InterruptedException
{        if (key instanceof Text) {        newKey.set(key.toString());        key = newKey;    }    String fetchStatus = content.getMetadata().get(Nutch.FETCH_STATUS_KEY);    if (fetchStatus == null) {                        return;    } else if (Integer.parseInt(fetchStatus) != CrawlDatum.STATUS_FETCH_SUCCESS) {                        return;    }    if (skipTruncated && isTruncated(content)) {        return;    }    long start = System.currentTimeMillis();    ParseResult parseResult = null;    try {        if (parseUtil == null)            parseUtil = new ParseUtil(context.getConfiguration());        parseResult = parseUtil.parse(content);    } catch (Exception e) {                return;    }    for (Entry<Text, Parse> entry : parseResult) {        Text url = entry.getKey();        Parse parse = entry.getValue();        ParseStatus parseStatus = parse.getData().getStatus();        context.getCounter("ParserStatus", ParseStatus.majorCodes[parseStatus.getMajorCode()]).increment(1);        if (!parseStatus.isSuccess()) {                        parse = parseStatus.getEmptyParse(context.getConfiguration());        }                parse.getData().getContentMeta().set(Nutch.SEGMENT_NAME_KEY, context.getConfiguration().get(Nutch.SEGMENT_NAME_KEY));                byte[] signature = SignatureFactory.getSignature(context.getConfiguration()).calculate(content, parse);        parse.getData().getContentMeta().set(Nutch.SIGNATURE_KEY, StringUtil.toHexString(signature));        try {            scfilters.passScoreAfterParsing(url, content, parse);        } catch (ScoringFilterException e) {            if (LOG.isWarnEnabled()) {                            }        }        long end = System.currentTimeMillis();                context.write(url, new ParseImpl(new ParseText(parse.getText()), parse.getData(), parse.isCanonical()));    }}
public static boolean nutch_f620_1(Content content)
{    byte[] contentBytes = content.getContent();    if (contentBytes == null)        return false;    Metadata metadata = content.getMetadata();    if (metadata == null)        return false;    String lengthStr = metadata.get(Response.CONTENT_LENGTH);    if (lengthStr != null)        lengthStr = lengthStr.trim();    if (StringUtil.isEmpty(lengthStr)) {        return false;    }    int inHeaderSize;    String url = content.getUrl();    try {        inHeaderSize = Integer.parseInt(lengthStr);    } catch (NumberFormatException e) {                return false;    }    int actualSize = contentBytes.length;    if (inHeaderSize > actualSize) {                return true;    }    if (LOG.isDebugEnabled()) {            }    return false;}
public void nutch_f621_0(Text key, Iterable<Writable> values, Context context) throws IOException, InterruptedException
{    Iterator<Writable> valuesIter = values.iterator();        context.write(key, valuesIter.next());}
public void nutch_f622_1(Path segment) throws IOException, InterruptedException, ClassNotFoundException
{    if (SegmentChecker.isParsed(segment, segment.getFileSystem(getConf()))) {                return;    }    SimpleDateFormat sdf = new SimpleDateFormat("yyyy-MM-dd HH:mm:ss");    long start = System.currentTimeMillis();    if (LOG.isInfoEnabled()) {                    }    Job job = NutchJob.getInstance(getConf());    job.setJobName("parse " + segment);    Configuration conf = job.getConfiguration();    FileInputFormat.addInputPath(job, new Path(segment, Content.DIR_NAME));    conf.set(Nutch.SEGMENT_NAME_KEY, segment.getName());    job.setInputFormatClass(SequenceFileInputFormat.class);    job.setJarByClass(ParseSegment.class);    job.setMapperClass(ParseSegment.ParseSegmentMapper.class);    job.setReducerClass(ParseSegment.ParseSegmentReducer.class);    FileOutputFormat.setOutputPath(job, segment);    job.setOutputFormatClass(ParseOutputFormat.class);    job.setOutputKeyClass(Text.class);    job.setOutputValueClass(ParseImpl.class);    try {        boolean success = job.waitForCompletion(true);        if (!success) {            String message = "Parse job did not succeed, job status:" + job.getStatus().getState() + ", reason: " + job.getStatus().getFailureInfo();                        throw new RuntimeException(message);        }    } catch (IOException | InterruptedException | ClassNotFoundException e) {                throw e;    }    long end = System.currentTimeMillis();    }
public static void nutch_f623_0(String[] args) throws Exception
{    int res = ToolRunner.run(NutchConfiguration.create(), new ParseSegment(), args);    System.exit(res);}
public int nutch_f624_0(String[] args) throws Exception
{    Path segment;    String usage = "Usage: ParseSegment segment [-noFilter] [-noNormalize]";    if (args.length == 0) {        System.err.println(usage);        System.exit(-1);    }    if (args.length > 1) {        for (int i = 1; i < args.length; i++) {            String param = args[i];            if ("-nofilter".equalsIgnoreCase(param)) {                getConf().setBoolean("parse.filter.urls", false);            } else if ("-nonormalize".equalsIgnoreCase(param)) {                getConf().setBoolean("parse.normalize.urls", false);            }        }    }    segment = new Path(args[0]);    parse(segment);    return 0;}
public Map<String, Object> nutch_f625_1(Map<String, Object> args, String crawlId) throws Exception
{    Map<String, Object> results = new HashMap<>();    Path segment = null;    if (args.containsKey(Nutch.ARG_SEGMENTS)) {        Object seg = args.get(Nutch.ARG_SEGMENTS);        if (seg instanceof Path) {            segment = (Path) seg;        } else if (seg instanceof String) {            segment = new Path(seg.toString());        } else if (seg instanceof ArrayList) {            String[] segmentsArray = (String[]) seg;            segment = new Path(segmentsArray[0].toString());            if (segmentsArray.length > 1) {                            }        }    } else {        String segment_dir = crawlId + "/segments";        File segmentsDir = new File(segment_dir);        File[] segmentsList = segmentsDir.listFiles();        Arrays.sort(segmentsList, (f1, f2) -> {            if (f1.lastModified() > f2.lastModified())                return -1;            else                return 0;        });        segment = new Path(segmentsList[0].getPath());    }    if (args.containsKey("nofilter")) {        getConf().setBoolean("parse.filter.urls", false);    }    if (args.containsKey("nonormalize")) {        getConf().setBoolean("parse.normalize.urls", false);    }    parse(segment);    results.put(Nutch.VAL_RESULT, Integer.toString(0));    return results;}
public byte nutch_f626_0()
{    return VERSION;}
public static ParseStatus nutch_f627_0(DataInput in) throws IOException
{    ParseStatus res = new ParseStatus();    res.readFields(in);    return res;}
public void nutch_f628_0(DataInput in) throws IOException
{    byte version = in.readByte();    switch(version) {        case 1:            majorCode = in.readByte();            minorCode = in.readShort();            args = WritableUtils.readCompressedStringArray(in);            break;        case 2:            majorCode = in.readByte();            minorCode = in.readShort();            args = WritableUtils.readStringArray(in);            break;        default:            throw new VersionMismatchException(VERSION, version);    }}
public void nutch_f629_0(DataOutput out) throws IOException
{    out.writeByte(VERSION);    out.writeByte(majorCode);    out.writeShort(minorCode);    if (args == null) {        out.writeInt(-1);    } else {        WritableUtils.writeStringArray(out, args);    }}
public boolean nutch_f630_0()
{    return majorCode == SUCCESS;}
public String nutch_f631_0()
{    if (args != null && args.length > 0 && args[0] != null)        return args[0];    return null;}
public String[] nutch_f632_0()
{    return args;}
public int nutch_f633_0()
{    return majorCode;}
public int nutch_f634_0()
{    return minorCode;}
public Parse nutch_f635_0(Configuration conf)
{    return new EmptyParseImpl(this, conf);}
public ParseResult nutch_f636_0(String url, Configuration conf)
{    return ParseResult.createParseResult(url, getEmptyParse(conf));}
public String nutch_f637_0()
{    StringBuffer res = new StringBuffer();    String name = null;    if (majorCode >= 0 && majorCode < majorCodes.length)        name = majorCodes[majorCode];    else        name = "UNKNOWN!";    res.append(name + "(" + majorCode + "," + minorCode + ")");    if (args != null) {        if (args.length == 1) {            res.append(": " + String.valueOf(args[0]));        } else {            for (int i = 0; i < args.length; i++) {                if (args[i] != null)                    res.append(", args[" + i + "]=" + String.valueOf(args[i]));            }        }    }    return res.toString();}
public void nutch_f638_0(String[] args)
{    this.args = args;}
public void nutch_f639_0(String msg)
{    if (args == null || args.length == 0) {        args = new String[1];    }    args[0] = msg;}
public void nutch_f640_0(byte majorCode)
{    this.majorCode = majorCode;}
public void nutch_f641_0(short minorCode)
{    this.minorCode = minorCode;}
public boolean nutch_f642_0(Object o)
{    if (o == null)        return false;    if (!(o instanceof ParseStatus))        return false;    boolean res = true;    ParseStatus other = (ParseStatus) o;    res = res && (this.majorCode == other.majorCode) && (this.minorCode == other.minorCode);    if (!res)        return res;    if (this.args == null) {        if (other.args == null)            return true;        else            return false;    } else {        if (other.args == null)            return false;        if (other.args.length != this.args.length)            return false;        for (int i = 0; i < this.args.length; i++) {            if (!this.args[i].equals(other.args[i]))                return false;        }    }    return true;}
public ParseData nutch_f643_0()
{    return data;}
public String nutch_f644_0()
{    return "";}
public boolean nutch_f645_0()
{    return true;}
public void nutch_f646_0(DataInput in) throws IOException
{    byte version = in.readByte();    switch(version) {        case 1:            text = WritableUtils.readCompressedString(in);            break;        case VERSION:            text = Text.readString(in);            break;        default:            throw new VersionMismatchException(VERSION, version);    }}
public final void nutch_f647_0(DataOutput out) throws IOException
{    out.write(VERSION);    Text.writeString(out, text);}
public static final ParseText nutch_f648_0(DataInput in) throws IOException
{    ParseText parseText = new ParseText();    parseText.readFields(in);    return parseText;}
public String nutch_f649_0()
{    return text;}
public boolean nutch_f650_0(Object o)
{    if (!(o instanceof ParseText))        return false;    ParseText other = (ParseText) o;    return this.text.equals(other.text);}
public String nutch_f651_0()
{    return text;}
public static void nutch_f652_0(String[] argv) throws Exception
{    String usage = "ParseText (-local | -dfs <namenode:port>) recno segment";    if (argv.length < 3) {        System.out.println("usage:" + usage);        return;    }    Options opts = new Options();    Configuration conf = NutchConfiguration.create();    GenericOptionsParser parser = new GenericOptionsParser(conf, opts, argv);    String[] remainingArgs = parser.getRemainingArgs();    try (FileSystem fs = FileSystem.get(conf)) {        int recno = Integer.parseInt(remainingArgs[0]);        String segment = remainingArgs[1];        String filename = new Path(segment, ParseText.DIR_NAME).toString();        ParseText parseText = new ParseText();        ArrayFile.Reader parseTexts = new ArrayFile.Reader(fs, filename, conf);        parseTexts.get(recno, parseText);        System.out.println("Retrieved " + recno + " from file " + filename);        System.out.println(parseText);        parseTexts.close();    }}
public ParseResult nutch_f653_1(Content content) throws ParseException
{    Parser[] parsers = null;    try {        parsers = this.parserFactory.getParsers(content.getContentType(), content.getUrl() != null ? content.getUrl() : "");    } catch (ParserNotFound e) {        if (LOG.isWarnEnabled()) {                    }        throw new ParseException(e.getMessage());    }    ParseResult parseResult = null;    for (int i = 0; i < parsers.length; i++) {        if (LOG.isDebugEnabled()) {                    }        if (maxParseTime != -1) {            parseResult = runParser(parsers[i], content);        } else {            try {                parseResult = parsers[i].getParse(content);            } catch (Throwable e) {                            }        }        if (parseResult != null && parseResult.isAnySuccess()) {            return parseResult;        }        }        if (parseResult != null && !parseResult.isEmpty()) {        return parseResult;    }    if (LOG.isWarnEnabled()) {            }    return new ParseStatus(new ParseException("Unable to successfully parse content")).getEmptyParseResult(content.getUrl(), null);}
public ParseResult nutch_f654_1(String extId, Content content) throws ParseException
{    Parser p = null;    try {        p = this.parserFactory.getParserById(extId);    } catch (ParserNotFound e) {        if (LOG.isWarnEnabled()) {                    }        throw new ParseException(e.getMessage());    }    ParseResult parseResult = null;    if (maxParseTime != -1) {        parseResult = runParser(p, content);    } else {        try {            parseResult = p.getParse(content);        } catch (Throwable e) {                    }    }    if (parseResult != null && !parseResult.isEmpty()) {        return parseResult;    } else {        if (LOG.isWarnEnabled()) {                    }        return new ParseStatus(new ParseException("Unable to successfully parse content")).getEmptyParseResult(content.getUrl(), null);    }}
private ParseResult nutch_f655_1(Parser p, Content content)
{    ParseCallable pc = new ParseCallable(p, content);    Future<ParseResult> task = executorService.submit(pc);    ParseResult res = null;    try {        res = task.get(maxParseTime, TimeUnit.SECONDS);    } catch (Exception e) {                task.cancel(true);    } finally {        pc = null;    }    return res;}
private void nutch_f656_0(String point)
{    fTargetPoint = point;}
public String nutch_f657_0(String pKey)
{    return fAttributes.get(pKey);}
public String nutch_f658_0()
{    return fClazz;}
public String nutch_f659_0()
{    return fId;}
public void nutch_f660_0(String pKey, String pValue)
{    fAttributes.put(pKey, pValue);}
public void nutch_f661_0(String extensionClazz)
{    fClazz = extensionClazz;}
public void nutch_f662_0(String extensionID)
{    fId = extensionID;}
public String nutch_f663_0()
{    return fTargetPoint;}
public Object nutch_f664_0() throws PluginRuntimeException
{        synchronized (getId()) {        try {            PluginRepository pluginRepository = PluginRepository.get(conf);            Class<?> extensionClazz = pluginRepository.getCachedClass(fDescriptor, getClazz());                                    pluginRepository.getPluginInstance(getDescriptor());            Object object = null;            try {                object = extensionClazz.getConstructor().newInstance();            } catch (IllegalArgumentException | InvocationTargetException | NoSuchMethodException | SecurityException e) {                e.printStackTrace();            }            if (object != null && object instanceof Configurable) {                ((Configurable) object).setConf(this.conf);            }            return object;        } catch (ClassNotFoundException e) {            throw new PluginRuntimeException(e);        } catch (InstantiationException e) {            throw new PluginRuntimeException(e);        } catch (IllegalAccessException e) {            throw new PluginRuntimeException(e);        }    }}
public PluginDescriptor nutch_f665_0()
{    return fDescriptor;}
public void nutch_f666_0(PluginDescriptor pDescriptor)
{    fDescriptor = pDescriptor;}
public String nutch_f667_0()
{    return getId() + ", " + getClazz() + ", " + getTargetPoint();}
public String nutch_f668_0()
{    return ftId;}
public String nutch_f669_0()
{    return fName;}
public String nutch_f670_0()
{    return fSchema;}
private void nutch_f671_0(String pId)
{    ftId = pId;}
private void nutch_f672_0(String pName)
{    fName = pName;}
private void nutch_f673_0(String pSchema)
{    fSchema = pSchema;}
public void nutch_f674_0(Extension extension)
{    fExtensions.add(extension);}
public Extension[] nutch_f675_0()
{    return fExtensions.toArray(new Extension[fExtensions.size()]);}
public void nutch_f676_0() throws PluginRuntimeException
{}
public void nutch_f677_0() throws PluginRuntimeException
{}
public PluginDescriptor nutch_f678_0()
{    return fDescriptor;}
private void nutch_f679_0(PluginDescriptor descriptor)
{    fDescriptor = descriptor;}
protected void nutch_f680_0() throws Throwable
{    super.finalize();    shutDown();}
protected synchronized Class<?> nutch_f681_0(String name, boolean resolve) throws ClassNotFoundException
{        Class<?> c = findLoadedClass(name);    if (c == null) {        try {                        c = findClass(name);        } catch (ClassNotFoundException | SecurityException e) {            c = loadClassFromParent(name, resolve);        }    }    if (resolve) {        resolveClass(c);    }    return c;}
private Class<?> nutch_f682_0(String name, boolean resolve) throws ClassNotFoundException
{                Class<?> c;    try {        c = super.loadClass(name, resolve);    } catch (ClassNotFoundException e) {        c = loadClassFromSystem(name);    } catch (SecurityException e) {        c = loadClassFromSystem(name);    }    return c;}
private Class<?> nutch_f683_0(String name) throws ClassNotFoundException
{    Class<?> c = null;    if (system != null) {                c = system.loadClass(name);    }    return c;}
public URL nutch_f684_0(String name)
{    URL url = findResource(name);    if (url == null)        url = super.getResource(name);    if (url == null && system != null)        url = system.getResource(name);    return url;}
public Enumeration<URL> nutch_f685_0(String name) throws IOException
{    /**     * Similar to super, but local resources are enumerated before parent     * resources     */    Enumeration<URL> systemUrls = null;    if (system != null) {        systemUrls = system.getResources(name);    }    Enumeration<URL> localUrls = findResources(name);    Enumeration<URL> parentUrls = null;    if (getParent() != null) {        parentUrls = getParent().getResources(name);    }    final List<URL> urls = new ArrayList<URL>();    if (localUrls != null) {        while (localUrls.hasMoreElements()) {            URL local = localUrls.nextElement();            urls.add(local);        }    }    if (systemUrls != null) {        while (systemUrls.hasMoreElements()) {            urls.add(systemUrls.nextElement());        }    }    if (parentUrls != null) {        while (parentUrls.hasMoreElements()) {            urls.add(parentUrls.nextElement());        }    }    return new Enumeration<URL>() {        Iterator<URL> iter = urls.iterator();        public boolean hasMoreElements() {            return iter.hasNext();        }        public URL nextElement() {            return iter.next();        }    };}
public boolean nutch_f686_0()
{    return iter.hasNext();}
public URL nutch_f687_0()
{    return iter.next();}
public InputStream nutch_f688_0(String name)
{    URL url = getResource(name);    try {        return url != null ? url.openStream() : null;    } catch (IOException e) {    }    return null;}
public int nutch_f689_0()
{    final int PRIME = 31;    int result = 1;    result = PRIME * result + ((parent == null) ? 0 : parent.hashCode());    result = PRIME * result + Arrays.hashCode(urls);    return result;}
public boolean nutch_f690_0(Object obj)
{    if (this == obj)        return true;    if (obj == null)        return false;    if (getClass() != obj.getClass())        return false;    final PluginClassLoader other = (PluginClassLoader) obj;    if (parent == null) {        if (other.parent != null)            return false;    } else if (!parent.equals(other.parent))        return false;    if (!Arrays.equals(urls, other.urls))        return false;    return true;}
private void nutch_f691_0(String pPath)
{    fPluginPath = pPath;}
public String nutch_f692_0()
{    return fName;}
private void nutch_f693_0(String providerName)
{    fProviderName = providerName;}
private void nutch_f694_0(String name)
{    fName = name;}
private void nutch_f695_0(String version)
{    fVersion = version;}
public String nutch_f696_0()
{    return fPluginClass;}
public String nutch_f697_0()
{    return fPluginId;}
public Extension[] nutch_f698_0()
{    return fExtensions.toArray(new Extension[fExtensions.size()]);}
public void nutch_f699_0(Extension pExtension)
{    fExtensions.add(pExtension);}
private void nutch_f700_0(String pluginClass)
{    fPluginClass = pluginClass;}
private void nutch_f701_0(String pluginId)
{    fPluginId = pluginId;}
public void nutch_f702_0(ExtensionPoint extensionPoint)
{    fExtensionPoints.add(extensionPoint);}
public ExtensionPoint[] nutch_f703_0()
{    return fExtensionPoints.toArray(new ExtensionPoint[fExtensionPoints.size()]);}
public String[] nutch_f704_0()
{    return fDependencies.toArray(new String[fDependencies.size()]);}
public void nutch_f705_0(String pId)
{    fDependencies.add(pId);}
public void nutch_f706_0(String pLibPath) throws MalformedURLException
{    URI uri = new File(getPluginPath() + File.separator + pLibPath).toURI();    URL url = uri.toURL();    fExportedLibs.add(url);}
public String nutch_f707_0()
{    return fPluginPath;}
public URL[] nutch_f708_0()
{    return fExportedLibs.toArray(new URL[0]);}
public void nutch_f709_0(String pLibPath) throws MalformedURLException
{    URI uri = new File(getPluginPath() + File.separator + pLibPath).toURI();    URL url = uri.toURL();    fNotExportedLibs.add(url);}
public URL[] nutch_f710_0()
{    return fNotExportedLibs.toArray(new URL[fNotExportedLibs.size()]);}
public PluginClassLoader nutch_f711_1()
{    if (fClassLoader != null)        return fClassLoader;    ArrayList<URL> arrayList = new ArrayList<>();    arrayList.addAll(fExportedLibs);    arrayList.addAll(fNotExportedLibs);    arrayList.addAll(getDependencyLibs());    File file = new File(getPluginPath());    try {        for (File file2 : file.listFiles()) {            if (file2.getAbsolutePath().endsWith("properties"))                arrayList.add(file2.getParentFile().toURI().toURL());        }    } catch (MalformedURLException e) {            }    URL[] urls = arrayList.toArray(new URL[arrayList.size()]);    fClassLoader = new PluginClassLoader(urls, PluginDescriptor.class.getClassLoader());    return fClassLoader;}
private ArrayList<URL> nutch_f712_0()
{    ArrayList<URL> list = new ArrayList<>();    collectLibs(list, this);    return list;}
private void nutch_f713_0(ArrayList<URL> pLibs, PluginDescriptor pDescriptor)
{    for (String id : pDescriptor.getDependencies()) {        PluginDescriptor descriptor = PluginRepository.get(fConf).getPluginDescriptor(id);        for (URL url : descriptor.getExportedLibUrls()) {            pLibs.add(url);        }        collectLibs(pLibs, descriptor);    }}
public String nutch_f714_0(String pKey, Locale pLocale) throws IOException
{    if (fMessages.containsKey(pLocale.toString())) {        ResourceBundle bundle = fMessages.get(pLocale.toString());        try {            return bundle.getString(pKey);        } catch (MissingResourceException e) {            return '!' + pKey + '!';        }    }    try {        ResourceBundle res = ResourceBundle.getBundle("messages", pLocale, getClassLoader());        return res.getString(pKey);    } catch (MissingResourceException x) {        return '!' + pKey + '!';    }}
public String nutch_f715_0()
{    return fProviderName;}
public String nutch_f716_0()
{    return fVersion;}
public Map<String, PluginDescriptor> nutch_f717_1(String[] pluginFolders)
{    Map<String, PluginDescriptor> map = new HashMap<>();    if (pluginFolders == null) {        throw new IllegalArgumentException("plugin.folders is not defined");    }    for (String name : pluginFolders) {        File directory = getPluginFolder(name);        if (directory == null) {            continue;        }                for (File oneSubFolder : directory.listFiles()) {            if (oneSubFolder.isDirectory()) {                String manifestPath = oneSubFolder.getAbsolutePath() + File.separator + "plugin.xml";                try {                                        PluginDescriptor p = parseManifestFile(manifestPath);                    map.put(p.getPluginId(), p);                } catch (Exception e) {                                    }            }        }    }    return map;}
public File nutch_f718_1(String name)
{    File directory = new File(name);    if (!directory.isAbsolute()) {        URL url = PluginManifestParser.class.getClassLoader().getResource(name);        if (url == null && directory.exists() && directory.isDirectory() && directory.listFiles().length > 0) {                        return directory;        } else if (url == null) {                        return null;        } else if (!"file".equals(url.getProtocol())) {                        return null;        }        String path = url.getPath();        if (        WINDOWS && path.startsWith("/"))            path = path.substring(1);        try {                        path = URLDecoder.decode(path, "UTF-8");        } catch (UnsupportedEncodingException e) {        }        directory = new File(path);    } else if (!directory.exists()) {                return null;    }    return directory;}
private PluginDescriptor nutch_f719_0(String pManifestPath) throws MalformedURLException, SAXException, IOException, ParserConfigurationException
{    Document document = parseXML(new File(pManifestPath).toURI().toURL());    String pPath = new File(pManifestPath).getParent();    return parsePlugin(document, pPath);}
private Document nutch_f720_0(URL url) throws SAXException, IOException, ParserConfigurationException
{    DocumentBuilderFactory factory = DocumentBuilderFactory.newInstance();    DocumentBuilder builder = factory.newDocumentBuilder();    return builder.parse(url.openStream());}
private PluginDescriptor nutch_f721_1(Document pDocument, String pPath) throws MalformedURLException
{    Element rootElement = pDocument.getDocumentElement();    String id = rootElement.getAttribute(ATTR_ID);    String name = rootElement.getAttribute(ATTR_NAME);    String version = rootElement.getAttribute("version");    String providerName = rootElement.getAttribute("provider-name");    String pluginClazz = null;    if (rootElement.getAttribute(ATTR_CLASS).trim().length() > 0) {        pluginClazz = rootElement.getAttribute(ATTR_CLASS);    }    PluginDescriptor pluginDescriptor = new PluginDescriptor(id, version, name, providerName, pluginClazz, pPath, this.conf);        parseExtension(rootElement, pluginDescriptor);    parseExtensionPoints(rootElement, pluginDescriptor);    parseLibraries(rootElement, pluginDescriptor);    parseRequires(rootElement, pluginDescriptor);    return pluginDescriptor;}
private void nutch_f722_0(Element pRootElement, PluginDescriptor pDescriptor) throws MalformedURLException
{    NodeList nodelist = pRootElement.getElementsByTagName("requires");    if (nodelist.getLength() > 0) {        Element requires = (Element) nodelist.item(0);        NodeList imports = requires.getElementsByTagName("import");        for (int i = 0; i < imports.getLength(); i++) {            Element anImport = (Element) imports.item(i);            String plugin = anImport.getAttribute("plugin");            if (plugin != null) {                pDescriptor.addDependency(plugin);            }        }    }}
private void nutch_f723_0(Element pRootElement, PluginDescriptor pDescriptor) throws MalformedURLException
{    NodeList nodelist = pRootElement.getElementsByTagName("runtime");    if (nodelist.getLength() > 0) {        Element runtime = (Element) nodelist.item(0);        NodeList libraries = runtime.getElementsByTagName("library");        for (int i = 0; i < libraries.getLength(); i++) {            Element library = (Element) libraries.item(i);            String libName = library.getAttribute(ATTR_NAME);            NodeList list = library.getElementsByTagName("export");            Element exportElement = (Element) list.item(0);            if (exportElement != null)                pDescriptor.addExportedLibRelative(libName);            else                pDescriptor.addNotExportedLibRelative(libName);        }    }}
private void nutch_f724_0(Element pRootElement, PluginDescriptor pPluginDescriptor)
{    NodeList list = pRootElement.getElementsByTagName("extension-point");    if (list != null) {        for (int i = 0; i < list.getLength(); i++) {            Element oneExtensionPoint = (Element) list.item(i);            String id = oneExtensionPoint.getAttribute(ATTR_ID);            String name = oneExtensionPoint.getAttribute(ATTR_NAME);            String schema = oneExtensionPoint.getAttribute("schema");            ExtensionPoint extensionPoint = new ExtensionPoint(id, name, schema);            pPluginDescriptor.addExtensionPoint(extensionPoint);        }    }}
private void nutch_f725_1(Element pRootElement, PluginDescriptor pPluginDescriptor)
{    NodeList extensions = pRootElement.getElementsByTagName("extension");    if (extensions != null) {        for (int i = 0; i < extensions.getLength(); i++) {            Element oneExtension = (Element) extensions.item(i);            String pointId = oneExtension.getAttribute("point");            NodeList extensionImplementations = oneExtension.getChildNodes();            if (extensionImplementations != null) {                for (int j = 0; j < extensionImplementations.getLength(); j++) {                    Node node = extensionImplementations.item(j);                    if (!node.getNodeName().equals("implementation")) {                        continue;                    }                    Element oneImplementation = (Element) node;                    String id = oneImplementation.getAttribute(ATTR_ID);                    String extensionClass = oneImplementation.getAttribute(ATTR_CLASS);                                        Extension extension = new Extension(pPluginDescriptor, pointId, id, extensionClass, this.conf, this.pluginRepository);                    NodeList parameters = oneImplementation.getElementsByTagName("parameter");                    if (parameters != null) {                        for (int k = 0; k < parameters.getLength(); k++) {                            Element param = (Element) parameters.item(k);                            extension.addAttribute(param.getAttribute(ATTR_NAME), param.getAttribute("value"));                        }                    }                    pPluginDescriptor.addExtension(extension);                }            }        }    }}
public static synchronized PluginRepository nutch_f726_0(Configuration conf)
{    String uuid = NutchConfiguration.getUUID(conf);    if (uuid == null) {                uuid = "nonNutchConf@" + conf.hashCode();    }    PluginRepository result = CACHE.get(uuid);    if (result == null) {        result = new PluginRepository(conf);        CACHE.put(uuid, result);    }    return result;}
private void nutch_f727_1(List<PluginDescriptor> plugins)
{    if (plugins == null) {        return;    }    for (PluginDescriptor plugin : plugins) {        for (ExtensionPoint point : plugin.getExtenstionPoints()) {            String xpId = point.getId();                        fExtensionPoints.put(xpId, point);        }    }}
private void nutch_f728_0(List<PluginDescriptor> pRegisteredPlugins) throws PluginRuntimeException
{    for (PluginDescriptor descriptor : pRegisteredPlugins) {        for (Extension extension : descriptor.getExtensions()) {            String xpId = extension.getTargetPoint();            ExtensionPoint point = getExtensionPoint(xpId);            if (point == null) {                throw new PluginRuntimeException("Plugin (" + descriptor.getPluginId() + "), " + "extension point: " + xpId + " does not exist.");            }            point.addExtension(extension);        }    }}
private void nutch_f729_0(PluginDescriptor plugin, Map<String, PluginDescriptor> plugins, Map<String, PluginDescriptor> dependencies, Map<String, PluginDescriptor> branch) throws MissingDependencyException, CircularDependencyException
{    if (dependencies == null) {        dependencies = new HashMap<>();    }    if (branch == null) {        branch = new HashMap<>();    }    branch.put(plugin.getPluginId(), plugin);        for (String id : plugin.getDependencies()) {        PluginDescriptor dependency = plugins.get(id);        if (dependency == null) {            throw new MissingDependencyException("Missing dependency " + id + " for plugin " + plugin.getPluginId());        }        if (branch.containsKey(id)) {            throw new CircularDependencyException("Circular dependency detected " + id + " for plugin " + plugin.getPluginId());        }        dependencies.put(id, dependency);        getPluginCheckedDependencies(plugins.get(id), plugins, dependencies, branch);    }    branch.remove(plugin.getPluginId());}
private Map<String, PluginDescriptor> nutch_f730_0(PluginDescriptor plugin, Map<String, PluginDescriptor> plugins) throws MissingDependencyException, CircularDependencyException
{    Map<String, PluginDescriptor> dependencies = new HashMap<>();    Map<String, PluginDescriptor> branch = new HashMap<>();    getPluginCheckedDependencies(plugin, plugins, dependencies, branch);    return dependencies;}
private List<PluginDescriptor> nutch_f731_1(Map<String, PluginDescriptor> filtered, Map<String, PluginDescriptor> all)
{    if (filtered == null) {        return null;    }    Map<String, PluginDescriptor> checked = new HashMap<>();    for (PluginDescriptor plugin : filtered.values()) {        try {            checked.putAll(getPluginCheckedDependencies(plugin, all));            checked.put(plugin.getPluginId(), plugin);        } catch (MissingDependencyException mde) {                                } catch (CircularDependencyException cde) {                                }    }    return new ArrayList<>(checked.values());}
public PluginDescriptor[] nutch_f732_0()
{    return fRegisteredPlugins.toArray(new PluginDescriptor[fRegisteredPlugins.size()]);}
public PluginDescriptor nutch_f733_0(String pPluginId)
{    for (PluginDescriptor descriptor : fRegisteredPlugins) {        if (descriptor.getPluginId().equals(pPluginId))            return descriptor;    }    return null;}
public ExtensionPoint nutch_f734_0(String pXpId)
{    return this.fExtensionPoints.get(pXpId);}
public Plugin nutch_f735_0(PluginDescriptor pDescriptor) throws PluginRuntimeException
{    if (fActivatedPlugins.containsKey(pDescriptor.getPluginId()))        return fActivatedPlugins.get(pDescriptor.getPluginId());    try {                synchronized (pDescriptor) {            Class<?> pluginClass = getCachedClass(pDescriptor, pDescriptor.getPluginClass());            Constructor<?> constructor = pluginClass.getConstructor(new Class<?>[] { PluginDescriptor.class, Configuration.class });            Plugin plugin = (Plugin) constructor.newInstance(new Object[] { pDescriptor, this.conf });            plugin.startUp();            fActivatedPlugins.put(pDescriptor.getPluginId(), plugin);            return plugin;        }    } catch (ClassNotFoundException e) {        throw new PluginRuntimeException(e);    } catch (InstantiationException e) {        throw new PluginRuntimeException(e);    } catch (IllegalAccessException e) {        throw new PluginRuntimeException(e);    } catch (NoSuchMethodException e) {        throw new PluginRuntimeException(e);    } catch (InvocationTargetException e) {        throw new PluginRuntimeException(e);    }}
public void nutch_f736_0() throws Throwable
{    shutDownActivatedPlugins();}
private void nutch_f737_0() throws PluginRuntimeException
{    for (Plugin plugin : fActivatedPlugins.values()) {        plugin.shutDown();    }}
public Class nutch_f738_0(PluginDescriptor pDescriptor, String className) throws ClassNotFoundException
{    Map<PluginClassLoader, Class> descMap = CLASS_CACHE.get(className);    if (descMap == null) {        descMap = new HashMap<>();        CLASS_CACHE.put(className, descMap);    }    PluginClassLoader loader = pDescriptor.getClassLoader();    Class clazz = descMap.get(loader);    if (clazz == null) {        clazz = loader.loadClass(className);        descMap.put(loader, clazz);    }    return clazz;}
private void nutch_f739_1()
{            if ((fRegisteredPlugins == null) || (fRegisteredPlugins.size() == 0)) {            } else {        for (PluginDescriptor plugin : fRegisteredPlugins) {                    }    }        if ((fExtensionPoints == null) || (fExtensionPoints.size() == 0)) {            } else {        for (ExtensionPoint ep : fExtensionPoints.values()) {                    }    }}
private Map<String, PluginDescriptor> nutch_f740_1(Pattern excludes, Pattern includes, Map<String, PluginDescriptor> plugins)
{    Map<String, PluginDescriptor> map = new HashMap<>();    if (plugins == null) {        return map;    }    for (PluginDescriptor plugin : plugins.values()) {        if (plugin == null) {            continue;        }        String id = plugin.getPluginId();        if (id == null) {            continue;        }        if (!includes.matcher(id).matches()) {                        continue;        }        if (excludes.matcher(id).matches()) {                        continue;        }        map.put(plugin.getPluginId(), plugin);    }    return map;}
public synchronized Object[] nutch_f741_1(Class<?> clazz, String xPointId, String orderProperty)
{    Object[] filters;    ObjectCache objectCache = ObjectCache.get(conf);    filters = (Object[]) objectCache.getObject(clazz.getName());    if (filters == null) {        String order = conf.get(orderProperty);        List<String> orderOfFilters = new ArrayList<>();        boolean userDefinedOrder = false;        if (order != null && !order.trim().isEmpty()) {            orderOfFilters = Arrays.asList(order.trim().split("\\s+"));            userDefinedOrder = true;        }        try {            ExtensionPoint point = PluginRepository.get(conf).getExtensionPoint(xPointId);            if (point == null)                throw new RuntimeException(xPointId + " not found.");            Extension[] extensions = point.getExtensions();            HashMap<String, Object> filterMap = new HashMap<>();            for (int i = 0; i < extensions.length; i++) {                Extension extension = extensions[i];                Object filter = extension.getExtensionInstance();                if (!filterMap.containsKey(filter.getClass().getName())) {                    filterMap.put(filter.getClass().getName(), filter);                    if (!userDefinedOrder)                        orderOfFilters.add(filter.getClass().getName());                }            }            List<Object> sorted = new ArrayList<>();            for (String orderedFilter : orderOfFilters) {                Object f = filterMap.get(orderedFilter);                if (f == null) {                                        continue;                }                sorted.add(f);            }            Object[] filter = (Object[]) Array.newInstance(clazz, sorted.size());            for (int i = 0; i < sorted.size(); i++) {                filter[i] = sorted.get(i);                if (LOG.isTraceEnabled()) {                    LOG.trace(clazz.getSimpleName() + " : filters[" + i + "] = " + filter[i].getClass());                }            }            objectCache.setObject(clazz.getName(), filter);        } catch (PluginRuntimeException e) {            throw new RuntimeException(e);        }        filters = (Object[]) objectCache.getObject(clazz.getName());    }    return filters;}
public static void nutch_f742_0(String[] args) throws Exception
{    if (args.length < 2) {        System.err.println("Usage: PluginRepository pluginId className [arg1 arg2 ...]");        return;    }    Configuration conf = NutchConfiguration.create();    PluginRepository repo = new PluginRepository(conf);        PluginDescriptor d = repo.getPluginDescriptor(args[0]);    if (d == null) {        System.err.println("Plugin '" + args[0] + "' not present or inactive.");        return;    }    ClassLoader cl = d.getClassLoader();        Class<?> clazz = null;    try {        clazz = Class.forName(args[1], true, cl);    } catch (Exception e) {        System.err.println("Could not load the class '" + args[1] + ": " + e.getMessage());        return;    }    Method m = null;    try {        m = clazz.getMethod("main", new Class<?>[] { args.getClass() });    } catch (Exception e) {        System.err.println("Could not find the 'main(String[])' method in class " + args[1] + ": " + e.getMessage());        return;    }    String[] subargs = new String[args.length - 2];    System.arraycopy(args, 2, subargs, 0, subargs.length);    m.invoke(null, new Object[] { subargs });}
private final void nutch_f743_0(DataInput in) throws IOException
{    byte oldVersion = in.readByte();    switch(oldVersion) {        case 0:        case 1:                        url = Text.readString(in);                        base = Text.readString(in);                        content = new byte[in.readInt()];            in.readFully(content);                        contentType = Text.readString(in);                        int keySize = in.readInt();            String key;            for (int i = 0; i < keySize; i++) {                key = Text.readString(in);                int valueSize = in.readInt();                for (int j = 0; j < valueSize; j++) {                    metadata.add(key, Text.readString(in));                }            }            break;        case 2:                        url = Text.readString(in);                        base = Text.readString(in);                        content = new byte[in.readInt()];            in.readFully(content);                        contentType = Text.readString(in);                        metadata.readFields(in);            break;        default:            throw new VersionMismatchException((byte) 2, oldVersion);    }}
public final void nutch_f744_0(DataInput in) throws IOException
{    metadata.clear();    int sizeOrVersion = in.readInt();    if (sizeOrVersion < 0) {                version = sizeOrVersion;        switch(version) {            case VERSION:                url = Text.readString(in);                base = Text.readString(in);                content = new byte[in.readInt()];                in.readFully(content);                contentType = Text.readString(in);                metadata.readFields(in);                break;            default:                throw new VersionMismatchException((byte) VERSION, (byte) version);        }    } else {                byte[] compressed = new byte[sizeOrVersion];        in.readFully(compressed, 0, compressed.length);        ByteArrayInputStream deflated = new ByteArrayInputStream(compressed);        DataInput inflater = new DataInputStream(new InflaterInputStream(deflated));        readFieldsCompressed(inflater);    }}
public final void nutch_f745_0(DataOutput out) throws IOException
{    out.writeInt(VERSION);        Text.writeString(out, url);        Text.writeString(out, base);        out.writeInt(content.length);    out.write(content);        Text.writeString(out, contentType);        metadata.write(out);}
public static Content nutch_f746_0(DataInput in) throws IOException
{    Content content = new Content();    content.readFields(in);    return content;}
public String nutch_f747_0()
{    return url;}
public String nutch_f748_0()
{    return base;}
public byte[] nutch_f749_0()
{    return content;}
public void nutch_f750_0(byte[] content)
{    this.content = content;}
public String nutch_f751_0()
{    return contentType;}
public void nutch_f752_0(String contentType)
{    this.contentType = contentType;}
public Metadata nutch_f753_0()
{    return metadata;}
public void nutch_f754_0(Metadata metadata)
{    this.metadata = metadata;}
public boolean nutch_f755_0(Object o)
{    if (!(o instanceof Content)) {        return false;    }    Content that = (Content) o;    return this.url.equals(that.url) && this.base.equals(that.base) && Arrays.equals(this.getContent(), that.getContent()) && this.contentType.equals(that.contentType) && this.metadata.equals(that.metadata);}
public String nutch_f756_0()
{    StringBuffer buffer = new StringBuffer();    buffer.append("Version: " + version + "\n");    buffer.append("url: " + url + "\n");    buffer.append("base: " + base + "\n");    buffer.append("contentType: " + contentType + "\n");    buffer.append("metadata: " + metadata + "\n");    buffer.append("Content:\n");        buffer.append(new String(content));    return buffer.toString();}
public static void nutch_f757_0(String[] argv) throws Exception
{    String usage = "Content (-local | -dfs <namenode:port>) recno segment";    if (argv.length < 3) {        System.out.println("usage:" + usage);        return;    }    Options opts = new Options();    Configuration conf = NutchConfiguration.create();    GenericOptionsParser parser = new GenericOptionsParser(conf, opts, argv);    String[] remainingArgs = parser.getRemainingArgs();    try (FileSystem fs = FileSystem.get(conf)) {        int recno = Integer.parseInt(remainingArgs[0]);        String segment = remainingArgs[1];        Path file = new Path(segment, DIR_NAME);        System.out.println("Reading from file: " + file);        ArrayFile.Reader contents = new ArrayFile.Reader(fs, file.toString(), conf);        Content content = new Content();        contents.get(recno, content);        System.out.println("Retrieved " + recno + " from file " + file);        System.out.println(content);        contents.close();    }}
private String nutch_f758_0(String typeName, String url, byte[] data)
{    return this.mimeTypes.autoResolveContentType(typeName, url, data);}
public Protocol nutch_f759_0(String urlString) throws ProtocolNotFound
{    try {        URL url = new URL(urlString);        return getProtocol(url);    } catch (MalformedURLException e) {        throw new ProtocolNotFound(urlString, e.toString());    }}
public Protocol nutch_f760_0(URL url) throws ProtocolNotFound
{    try {        Protocol protocol = null;                String host = url.getHost();        if (hostProtocolMapping.containsKey(host)) {            Extension extension = getExtensionById(hostProtocolMapping.get(host));            if (extension != null) {                protocol = getProtocolInstanceByExtension(extension);            }        }                if (protocol == null) {                        if (defaultProtocolImplMapping.containsKey(url.getProtocol())) {                Extension extension = getExtensionById(defaultProtocolImplMapping.get(url.getProtocol()));                if (extension != null) {                    protocol = getProtocolInstanceByExtension(extension);                }            }        }                if (protocol == null) {            Extension extension = findExtension(url.getProtocol(), "protocolName");            if (extension != null) {                protocol = getProtocolInstanceByExtension(extension);            }        }                if (protocol != null) {            return protocol;        }                throw new ProtocolNotFound(url.toString());    } catch (PluginRuntimeException e) {        throw new ProtocolNotFound(url.toString(), e.toString());    }}
private Protocol nutch_f761_0(Extension extension) throws PluginRuntimeException
{    Protocol protocol = null;    String cacheId = extension.getId();    ObjectCache objectCache = ObjectCache.get(conf);    synchronized (objectCache) {        if (!objectCache.hasObject(cacheId)) {            protocol = (Protocol) extension.getExtensionInstance();            objectCache.setObject(cacheId, protocol);        }        protocol = (Protocol) objectCache.getObject(cacheId);    }    return protocol;}
private Extension nutch_f762_0(String id)
{    Extension[] extensions = this.extensionPoint.getExtensions();    for (int i = 0; i < extensions.length; i++) {        if (id.equals(extensions[i].getId())) {            return extensions[i];        }    }    return null;}
private Extension nutch_f763_0(String name, String attribute) throws PluginRuntimeException
{    for (int i = 0; i < this.extensionPoint.getExtensions().length; i++) {        Extension extension = this.extensionPoint.getExtensions()[i];        if (contains(name, extension.getAttribute(attribute)))            return extension;    }    return null;}
 boolean nutch_f764_0(String what, String where)
{    if (where != null) {        String[] parts = where.split("[, ]");        for (int i = 0; i < parts.length; i++) {            if (parts[i].equals(what))                return true;        }    }    return false;}
public Protocol nutch_f765_0(String id) throws PluginRuntimeException
{    Extension ext = getExtensionById(id);    if (ext == null) {        throw new PluginRuntimeException("ID " + id + " not found");    }    return getProtocolInstanceByExtension(ext);}
public String nutch_f766_0()
{    return url;}
public Content nutch_f767_0()
{    return content;}
public void nutch_f768_0(Content content)
{    this.content = content;}
public ProtocolStatus nutch_f769_0()
{    return status;}
public void nutch_f770_0(ProtocolStatus status)
{    this.status = status;}
public static ProtocolStatus nutch_f771_0(DataInput in) throws IOException
{    ProtocolStatus res = new ProtocolStatus();    res.readFields(in);    return res;}
public void nutch_f772_0(DataInput in) throws IOException
{    byte version = in.readByte();    switch(version) {        case 1:            code = in.readByte();            lastModified = in.readLong();            args = WritableUtils.readCompressedStringArray(in);            break;        case VERSION:            code = in.readByte();            lastModified = in.readLong();            args = WritableUtils.readStringArray(in);            break;        default:            throw new VersionMismatchException(VERSION, version);    }}
public void nutch_f773_0(DataOutput out) throws IOException
{    out.writeByte(VERSION);    out.writeByte((byte) code);    out.writeLong(lastModified);    if (args == null) {        out.writeInt(-1);    } else {        WritableUtils.writeStringArray(out, args);    }}
public void nutch_f774_0(String[] args)
{    this.args = args;}
public String[] nutch_f775_0()
{    return args;}
public int nutch_f776_0()
{    return code;}
public String nutch_f777_0()
{    return codeToName.get(this.code);}
public void nutch_f778_0(int code)
{    this.code = code;}
public boolean nutch_f779_0()
{    return code == SUCCESS;}
public boolean nutch_f780_0()
{    return code == ACCESS_DENIED || code == EXCEPTION || code == REDIR_EXCEEDED || code == RETRY || code == TEMP_MOVED || code == WOULDBLOCK || code == PROTO_NOT_FOUND;}
public boolean nutch_f781_0()
{    return code == FAILED || code == GONE || code == MOVED || code == NOTFOUND || code == ROBOTS_DENIED;}
public boolean nutch_f782_0()
{    return code == MOVED || code == TEMP_MOVED;}
public String nutch_f783_0()
{    if (args != null && args.length > 0)        return args[0];    return null;}
public void nutch_f784_0(String msg)
{    if (args != null && args.length > 0)        args[0] = msg;    else        args = new String[] { msg };}
public long nutch_f785_0()
{    return lastModified;}
public void nutch_f786_0(long lastModified)
{    this.lastModified = lastModified;}
public boolean nutch_f787_0(Object o)
{    if (o == null)        return false;    if (!(o instanceof ProtocolStatus))        return false;    ProtocolStatus other = (ProtocolStatus) o;    if (this.code != other.code || this.lastModified != other.lastModified)        return false;    if (this.args == null) {        if (other.args == null)            return true;        else            return false;    } else {        if (other.args == null)            return false;        if (other.args.length != this.args.length)            return false;        for (int i = 0; i < this.args.length; i++) {            if (!this.args[i].equals(other.args[i]))                return false;        }    }    return true;}
public String nutch_f788_0()
{    StringBuffer res = new StringBuffer();    res.append(codeToName.get(Integer.valueOf(code)) + "(" + code + "), lastModified=" + lastModified);    if (args != null) {        if (args.length == 1) {            res.append(": " + String.valueOf(args[0]));        } else {            for (int i = 0; i < args.length; i++) {                if (args[i] != null)                    res.append(", args[" + i + "]=" + String.valueOf(args[i]));            }        }    }    return res.toString();}
public void nutch_f789_1(Configuration conf)
{    this.conf = conf;        String agentName = conf.get("http.agent.name");    if (agentName == null || (agentName = agentName.trim()).isEmpty()) {        throw new RuntimeException("Agent name not configured!");    }    agentNames = agentName;            String otherAgents = conf.get("http.robots.agents");    if (otherAgents != null && !otherAgents.trim().isEmpty()) {        StringTokenizer tok = new StringTokenizer(otherAgents, ",");        StringBuilder sb = new StringBuilder(agentNames);        while (tok.hasMoreTokens()) {            String str = tok.nextToken().trim();            if (str.equals("*") || str.equals(agentName)) {                                                } else {                sb.append(",").append(str);            }        }        agentNames = sb.toString();    }    String[] confWhiteList = conf.getStrings("http.robot.rules.whitelist");    if (confWhiteList == null) {            } else {        for (int i = 0; i < confWhiteList.length; i++) {            if (confWhiteList[i].isEmpty()) {                                continue;            }            whiteList.add(confWhiteList[i]);        }        if (whiteList.size() > 0) {            matcher = new SuffixStringMatcher(whiteList);                    }    }}
public Configuration nutch_f790_0()
{    return conf;}
public boolean nutch_f791_0(URL url)
{    boolean match = false;    String urlString = url.getHost();    if (matcher != null) {        match = matcher.matches(urlString);    }    return match;}
public BaseRobotRules nutch_f792_0(String url, byte[] content, String contentType, String robotName)
{    return robotParser.parseContent(url, content, contentType, robotName);}
public BaseRobotRules nutch_f793_0(Protocol protocol, Text url, List<Content> robotsTxtContent)
{    URL u = null;    try {        u = new URL(url.toString());    } catch (Exception e) {        return EMPTY_RULES;    }    return getRobotRulesSet(protocol, u, robotsTxtContent);}
public int nutch_f794_1(String[] args)
{    if (args.length < 2) {        String[] help = { "Usage: RobotRulesParser [ -Dproperty=... ] <robots-file-or-url> <url-file> [<agent-names>]", "", "<robots-file-or-url>\tlocal file or URL parsed as robots.txt file", "\tIf <robots-file-or-url> starts with a protocol specification", "\t(`http', `https', `ftp' or `file'), robots.txt it is fetched", "\tusing the specified protocol. Otherwise, a local file is assumed.", "", "<url-file>\tlocal file with URLs (one per line), for every URL", "\tthe path part (including the query) is checked whether", "\tit is allowed by the robots.txt rules.  Other parts of the URLs", "\t(mainly the host) are ignored.", "", "<agent-names>\tcomma-separated list of agent names", "\tused to select rules from the robots.txt file.", "\tIf no agent name is given the property http.agent.name is used.", "\tIf http.agent.name is empty, robots.txt is checked for rules", "\tassigned to the user agent `*' (meaning any other).", "", "Important properties:", " -D fetcher.store.robotstxt=true", "\toutput content and HTTP meta data of fetched robots.txt (if not a local file)", " -D http.agent.name=...\tsame as argument <agent-names>", " -D http.robots.agents=...\tadditional agent names", " -D http.robot.rules.whitelist=..." };        for (String s : help) {            System.err.println(s);        }        return -1;    }    Protocol protocol = null;    URL robotsTxtUrl = null;    if (args[0].matches("^(?:https?|ftp|file)://?.*")) {        try {            robotsTxtUrl = new URL(args[0]);        } catch (MalformedURLException e) {                    }        ProtocolFactory factory = new ProtocolFactory(conf);        try {            protocol = factory.getProtocol(robotsTxtUrl);        } catch (ProtocolNotFound e) {                        return -1;        }    }    if (robotsTxtUrl == null) {                File robotsFile = new File(args[0]);        if (!robotsFile.exists()) {                        return -1;        } else {            try {                robotsTxtUrl = robotsFile.toURI().toURL();            } catch (MalformedURLException e) {            }        }    }    File urlFile = new File(args[1]);    if (args.length > 2) {                String agents = args[2];        conf.set("http.agent.name", agents);        setConf(conf);    }    List<Content> robotsTxtContent = null;    if (getConf().getBoolean("fetcher.store.robotstxt", false)) {        robotsTxtContent = new LinkedList<>();    }    try {        BaseRobotRules rules = getRobotRulesSet(protocol, robotsTxtUrl, robotsTxtContent);        if (robotsTxtContent != null) {            for (Content robotsTxt : robotsTxtContent) {                                            }        }        System.out.println("Testing robots.txt for agent names: " + agentNames);        LineNumberReader testsIn = new LineNumberReader(new FileReader(urlFile));        String testPath;        testPath = testsIn.readLine();        while (testPath != null) {            testPath = testPath.trim();            try {                                URL url = new URL(testPath);                String status;                if (isWhiteListed(url)) {                    status = "whitelisted";                } else if (rules.isAllowed(testPath)) {                    status = "allowed";                } else {                    status = "not allowed";                }                System.out.println(status + ":\t" + testPath);            } catch (MalformedURLException e) {                            }            testPath = testsIn.readLine();        }        testsIn.close();    } catch (IOException e) {                return -1;    }    return 0;}
public BaseRobotRules nutch_f795_1(Protocol protocol, URL url, List<Content> robotsTxtContent)
{    BaseRobotRules rules;    if (protocol != null) {        rules = protocol.getRobotRules(new Text(url.toString()), null, robotsTxtContent);    } else {        try {            int contentLength = url.openConnection().getContentLength();            byte[] robotsBytes = new byte[contentLength];            InputStream openStream = url.openStream();            openStream.read(robotsBytes);            openStream.close();            rules = robotParser.parseContent(url.toString(), robotsBytes, "text/plain", this.conf.get("http.agent.name"));        } catch (IOException e) {                        rules = EMPTY_RULES;        }    }    return rules;}
public static void nutch_f796_0(String[] args) throws Exception
{    Configuration conf = NutchConfiguration.create();    int res = ToolRunner.run(conf, new TestRobotRulesParser(conf), args);    System.exit(res);}
public void nutch_f798_1(Object event, Configuration conf)
{    for (int i = 0; i < this.publishers.length; i++) {        try {            this.publishers[i].publish(event, conf);        } catch (Exception e) {                    }    }}
public Configuration nutch_f799_0()
{    return conf;}
public void nutch_f800_0(Configuration arg0)
{}
public Configuration nutch_f801_0()
{    return conf;}
public void nutch_f802_0(Configuration conf)
{    this.conf = conf;}
public void nutch_f803_0(Text url, CrawlDatum datum) throws ScoringFilterException
{}
public void nutch_f804_0(Text url, CrawlDatum datum) throws ScoringFilterException
{}
public float nutch_f805_0(Text url, CrawlDatum datum, float initSort) throws ScoringFilterException
{    return initSort;}
public void nutch_f806_0(Text url, CrawlDatum datum, Content content) throws ScoringFilterException
{}
public void nutch_f807_0(Text url, Content content, Parse parse) throws ScoringFilterException
{}
public CrawlDatum nutch_f808_0(Text fromUrl, ParseData parseData, Collection<Entry<Text, CrawlDatum>> targets, CrawlDatum adjust, int allCount) throws ScoringFilterException
{    return adjust;}
public void nutch_f809_0(Text url, CrawlDatum old, CrawlDatum datum, List<CrawlDatum> inlinked) throws ScoringFilterException
{}
public float nutch_f810_0(Text url, NutchDocument doc, CrawlDatum dbDatum, CrawlDatum fetchDatum, Parse parse, Inlinks inlinks, float initScore) throws ScoringFilterException
{    return initScore;}
public void nutch_f811_0(Text url, CrawlDatum datum) throws ScoringFilterException
{}
public float nutch_f812_0(Text url, CrawlDatum datum, float initSort) throws ScoringFilterException
{    for (int i = 0; i < this.filters.length; i++) {        initSort = this.filters[i].generatorSortValue(url, datum, initSort);    }    return initSort;}
public void nutch_f813_0(Text url, CrawlDatum datum) throws ScoringFilterException
{    for (int i = 0; i < this.filters.length; i++) {        this.filters[i].initialScore(url, datum);    }}
public void nutch_f814_0(Text url, CrawlDatum datum) throws ScoringFilterException
{    for (int i = 0; i < this.filters.length; i++) {        this.filters[i].injectedScore(url, datum);    }}
public void nutch_f815_0(Text url, CrawlDatum old, CrawlDatum datum, List<CrawlDatum> inlinked) throws ScoringFilterException
{    for (int i = 0; i < this.filters.length; i++) {        this.filters[i].updateDbScore(url, old, datum, inlinked);    }}
public void nutch_f816_0(Text url, CrawlDatum datum) throws ScoringFilterException
{    for (int i = 0; i < this.filters.length; i++) {        this.filters[i].orphanedScore(url, datum);    }}
public void nutch_f817_0(Text url, CrawlDatum datum, Content content) throws ScoringFilterException
{    for (int i = 0; i < this.filters.length; i++) {        this.filters[i].passScoreBeforeParsing(url, datum, content);    }}
public void nutch_f818_0(Text url, Content content, Parse parse) throws ScoringFilterException
{    for (int i = 0; i < this.filters.length; i++) {        this.filters[i].passScoreAfterParsing(url, content, parse);    }}
public CrawlDatum nutch_f819_0(Text fromUrl, ParseData parseData, Collection<Entry<Text, CrawlDatum>> targets, CrawlDatum adjust, int allCount) throws ScoringFilterException
{    for (int i = 0; i < this.filters.length; i++) {        adjust = this.filters[i].distributeScoreToOutlinks(fromUrl, parseData, targets, adjust, allCount);    }    return adjust;}
public float nutch_f820_0(Text url, NutchDocument doc, CrawlDatum dbDatum, CrawlDatum fetchDatum, Parse parse, Inlinks inlinks, float initScore) throws ScoringFilterException
{    for (int i = 0; i < this.filters.length; i++) {        initScore = this.filters[i].indexerScore(url, doc, dbDatum, fetchDatum, parse, inlinks, initScore);    }    return initScore;}
public String nutch_f821_0()
{    return url;}
public String nutch_f822_0()
{    return anchor;}
public void nutch_f823_0(String anchor)
{    this.anchor = anchor;}
public float nutch_f824_0()
{    return score;}
public void nutch_f825_0(float score)
{    this.score = score;}
public void nutch_f826_0(String url)
{    this.url = url;}
public long nutch_f827_0()
{    return timestamp;}
public void nutch_f828_0(long timestamp)
{    this.timestamp = timestamp;}
public byte nutch_f829_0()
{    return linkType;}
public void nutch_f830_0(byte linkType)
{    this.linkType = linkType;}
public void nutch_f831_0(DataInput in) throws IOException
{    url = Text.readString(in);    anchor = Text.readString(in);    score = in.readFloat();    timestamp = in.readLong();    linkType = in.readByte();}
public void nutch_f832_0(DataOutput out) throws IOException
{    Text.writeString(out, url);    Text.writeString(out, anchor != null ? anchor : "");    out.writeFloat(score);    out.writeLong(timestamp);    out.writeByte(linkType);}
public String nutch_f833_0()
{    String type = (linkType == INLINK ? "inlink" : (linkType == OUTLINK) ? "outlink" : "unknown");    return "url: " + url + ", anchor: " + anchor + ", score: " + score + ", timestamp: " + timestamp + ", link type: " + type;}
public static void nutch_f834_0(String[] args) throws Exception
{    if (args == null || args.length < 2) {        System.out.println("LinkDumper$Reader usage: <webgraphdb> <url>");        return;    }        Configuration conf = NutchConfiguration.create();    Path webGraphDb = new Path(args[0]);    String url = args[1];    MapFile.Reader[] readers = MapFileOutputFormat.getReaders(new Path(webGraphDb, DUMP_DIR), conf);        Text key = new Text(url);    LinkNodes nodes = new LinkNodes();    MapFileOutputFormat.getEntry(readers, new HashPartitioner<>(), key, nodes);        LinkNode[] linkNodesAr = nodes.getLinks();    System.out.println(url + ":");    for (LinkNode node : linkNodesAr) {        System.out.println("  " + node.getUrl() + " - " + node.getNode().toString());    }        FSUtils.closeReaders(readers);}
public String nutch_f835_0()
{    return url;}
public void nutch_f836_0(String url)
{    this.url = url;}
public Node nutch_f837_0()
{    return node;}
public void nutch_f838_0(Node node)
{    this.node = node;}
public void nutch_f839_0(DataInput in) throws IOException
{    url = in.readUTF();    node = new Node();    node.readFields(in);}
public void nutch_f840_0(DataOutput out) throws IOException
{    out.writeUTF(url);    node.write(out);}
public LinkNode[] nutch_f841_0()
{    return links;}
public void nutch_f842_0(LinkNode[] links)
{    this.links = links;}
public void nutch_f843_0(DataInput in) throws IOException
{    int numLinks = in.readInt();    if (numLinks > 0) {        links = new LinkNode[numLinks];        for (int i = 0; i < numLinks; i++) {            LinkNode node = new LinkNode();            node.readFields(in);            links[i] = node;        }    }}
public void nutch_f844_0(DataOutput out) throws IOException
{    if (links != null && links.length > 0) {        int numLinks = links.length;        out.writeInt(numLinks);        for (int i = 0; i < numLinks; i++) {            links[i].write(out);        }    }}
public void nutch_f845_0(Text key, Writable value, Context context) throws IOException, InterruptedException
{    ObjectWritable objWrite = new ObjectWritable();    objWrite.set(value);    context.write(key, objWrite);}
public void nutch_f846_0(Reducer<Text, ObjectWritable, Text, LinkNode>.Context context)
{    conf = context.getConfiguration();}
public void nutch_f847_0(Text key, Iterable<ObjectWritable> values, Context context) throws IOException, InterruptedException
{    String fromUrl = key.toString();    List<LinkDatum> outlinks = new ArrayList<>();    Node node = null;        for (ObjectWritable write : values) {        Object obj = write.get();        if (obj instanceof Node) {            node = (Node) obj;        } else if (obj instanceof LinkDatum) {            outlinks.add(WritableUtils.clone((LinkDatum) obj, conf));        }    }        int numOutlinks = node.getNumOutlinks();    if (numOutlinks > 0) {        for (int i = 0; i < outlinks.size(); i++) {            LinkDatum outlink = outlinks.get(i);            String toUrl = outlink.getUrl();                        context.write(new Text(toUrl), new LinkNode(fromUrl, node));        }    }}
public void nutch_f848_0(Text key, Iterable<LinkNode> values, Context context) throws IOException, InterruptedException
{    List<LinkNode> nodeList = new ArrayList<>();    int numNodes = 0;    for (LinkNode cur : values) {        if (numNodes < maxInlinks) {            nodeList.add(WritableUtils.clone(cur, conf));            numNodes++;        } else {            break;        }    }    LinkNode[] linkNodesAr = nodeList.toArray(new LinkNode[nodeList.size()]);    LinkNodes linkNodes = new LinkNodes(linkNodesAr);    context.write(key, linkNodes);}
public void nutch_f849_1(Path webGraphDb) throws IOException, InterruptedException, ClassNotFoundException
{    SimpleDateFormat sdf = new SimpleDateFormat("yyyy-MM-dd HH:mm:ss");    long start = System.currentTimeMillis();        Configuration conf = getConf();    FileSystem fs = webGraphDb.getFileSystem(conf);    Path linkdump = new Path(webGraphDb, DUMP_DIR);    Path nodeDb = new Path(webGraphDb, WebGraph.NODE_DIR);    Path outlinkDb = new Path(webGraphDb, WebGraph.OUTLINK_DIR);        Path tempInverted = new Path(webGraphDb, "inverted-" + Integer.toString(new Random().nextInt(Integer.MAX_VALUE)));    Job inverter = NutchJob.getInstance(conf);    inverter.setJobName("LinkDumper: inverter");    FileInputFormat.addInputPath(inverter, nodeDb);    FileInputFormat.addInputPath(inverter, outlinkDb);    inverter.setInputFormatClass(SequenceFileInputFormat.class);    inverter.setJarByClass(Inverter.class);    inverter.setMapperClass(Inverter.InvertMapper.class);    inverter.setReducerClass(Inverter.InvertReducer.class);    inverter.setMapOutputKeyClass(Text.class);    inverter.setMapOutputValueClass(ObjectWritable.class);    inverter.setOutputKeyClass(Text.class);    inverter.setOutputValueClass(LinkNode.class);    FileOutputFormat.setOutputPath(inverter, tempInverted);    inverter.setOutputFormatClass(SequenceFileOutputFormat.class);    try {                boolean success = inverter.waitForCompletion(true);        if (!success) {            String message = "LinkDumper inverter job did not succeed, job status:" + inverter.getStatus().getState() + ", reason: " + inverter.getStatus().getFailureInfo();                        throw new RuntimeException(message);        }            } catch (IOException | InterruptedException | ClassNotFoundException e) {                throw e;    }        Job merger = NutchJob.getInstance(conf);    merger.setJobName("LinkDumper: merger");    FileInputFormat.addInputPath(merger, tempInverted);    merger.setJarByClass(Merger.class);    merger.setInputFormatClass(SequenceFileInputFormat.class);    merger.setReducerClass(Merger.class);    merger.setMapOutputKeyClass(Text.class);    merger.setMapOutputValueClass(LinkNode.class);    merger.setOutputKeyClass(Text.class);    merger.setOutputValueClass(LinkNodes.class);    FileOutputFormat.setOutputPath(merger, linkdump);    merger.setOutputFormatClass(MapFileOutputFormat.class);    try {                boolean success = merger.waitForCompletion(true);        if (!success) {            String message = "LinkDumper merger job did not succeed, job status:" + merger.getStatus().getState() + ", reason: " + merger.getStatus().getFailureInfo();                        throw new RuntimeException(message);        }            } catch (IOException e) {                throw e;    }    fs.delete(tempInverted, true);    long end = System.currentTimeMillis();    }
public static void nutch_f850_0(String[] args) throws Exception
{    int res = ToolRunner.run(NutchConfiguration.create(), new LinkDumper(), args);    System.exit(res);}
public int nutch_f851_1(String[] args) throws Exception
{    Options options = new Options();    OptionBuilder.withArgName("help");    OptionBuilder.withDescription("show this help message");    Option helpOpts = OptionBuilder.create("help");    options.addOption(helpOpts);    OptionBuilder.withArgName("webgraphdb");    OptionBuilder.hasArg();    OptionBuilder.withDescription("the web graph database to use");    Option webGraphDbOpts = OptionBuilder.create("webgraphdb");    options.addOption(webGraphDbOpts);    CommandLineParser parser = new GnuParser();    try {        CommandLine line = parser.parse(options, args);        if (line.hasOption("help") || !line.hasOption("webgraphdb")) {            HelpFormatter formatter = new HelpFormatter();            formatter.printHelp("LinkDumper", options);            return -1;        }        String webGraphDb = line.getOptionValue("webgraphdb");        dumpLinks(new Path(webGraphDb));        return 0;    } catch (Exception e) {                return -2;    }}
private int nutch_f852_1(FileSystem fs, Path webGraphDb) throws IOException, ClassNotFoundException, InterruptedException
{        Path numLinksPath = new Path(webGraphDb, NUM_NODES);    Path nodeDb = new Path(webGraphDb, WebGraph.NODE_DIR);    Job counter = NutchJob.getInstance(getConf());    Configuration conf = counter.getConfiguration();    counter.setJobName("LinkRank Counter");    FileInputFormat.addInputPath(counter, nodeDb);    FileOutputFormat.setOutputPath(counter, numLinksPath);    counter.setInputFormatClass(SequenceFileInputFormat.class);    counter.setJarByClass(Counter.class);    counter.setMapperClass(Counter.CountMapper.class);    counter.setCombinerClass(Counter.CountReducer.class);    counter.setReducerClass(Counter.CountReducer.class);    counter.setMapOutputKeyClass(Text.class);    counter.setMapOutputValueClass(LongWritable.class);    counter.setOutputKeyClass(Text.class);    counter.setOutputValueClass(LongWritable.class);    counter.setNumReduceTasks(1);    counter.setOutputFormatClass(TextOutputFormat.class);    conf.setBoolean("mapreduce.fileoutputcommitter.marksuccessfuljobs", false);            try {        boolean success = counter.waitForCompletion(true);        if (!success) {            String message = "Link counter job did not succeed, job status:" + counter.getStatus().getState() + ", reason: " + counter.getStatus().getFailureInfo();                        throw new RuntimeException(message);        }    } catch (IOException | InterruptedException | ClassNotFoundException e) {                throw e;    }                FileStatus[] numLinksFiles = fs.listStatus(numLinksPath);    if (numLinksFiles.length == 0) {        throw new IOException("Failed to read numlinks temp file: " + " no file found in " + numLinksPath);    } else if (numLinksFiles.length > 1) {        throw new IOException("Failed to read numlinks temp file: " + " expected only one file but found " + numLinksFiles.length + " files in folder " + numLinksPath);    }    Path numLinksFile = numLinksFiles[0].getPath();        FSDataInputStream readLinks = fs.open(numLinksFile);    CompressionCodecFactory cf = new CompressionCodecFactory(conf);    CompressionCodec codec = cf.getCodec(numLinksFiles[0].getPath());    InputStream streamLinks;    if (codec == null) {                streamLinks = readLinks;    } else {                readLinks.seek(0);        streamLinks = codec.createInputStream(readLinks);    }    BufferedReader buffer = new BufferedReader(new InputStreamReader(streamLinks));    String numLinksLine = buffer.readLine();    readLinks.close();        if (numLinksLine == null || numLinksLine.length() == 0) {                fs.delete(numLinksPath, true);        throw new IOException("No links to process, is the webgraph empty?");    }            fs.delete(numLinksPath, true);    String numLinks = numLinksLine.split("\\s+")[1];    return Integer.parseInt(numLinks);}
private void nutch_f853_1(Path nodeDb, Path output) throws IOException, InterruptedException, ClassNotFoundException
{        Job initializer = NutchJob.getInstance(getConf());    Configuration conf = initializer.getConfiguration();    initializer.setJobName("LinkAnalysis Initializer");    FileInputFormat.addInputPath(initializer, nodeDb);    FileOutputFormat.setOutputPath(initializer, output);    initializer.setJarByClass(Initializer.class);    initializer.setInputFormatClass(SequenceFileInputFormat.class);    initializer.setMapperClass(Initializer.class);    initializer.setMapOutputKeyClass(Text.class);    initializer.setMapOutputValueClass(Node.class);    initializer.setOutputKeyClass(Text.class);    initializer.setOutputValueClass(Node.class);    initializer.setOutputFormatClass(MapFileOutputFormat.class);    conf.setBoolean("mapreduce.fileoutputcommitter.marksuccessfuljobs", false);            try {        boolean success = initializer.waitForCompletion(true);        if (!success) {            String message = "Initialization job did not succeed, job status:" + initializer.getStatus().getState() + ", reason: " + initializer.getStatus().getFailureInfo();                        throw new RuntimeException(message);        }    } catch (IOException | InterruptedException | ClassNotFoundException e) {                throw e;    }    }
private void nutch_f854_1(Path nodeDb, Path outlinkDb, Path output) throws IOException, InterruptedException, ClassNotFoundException
{        Job inverter = NutchJob.getInstance(getConf());    Configuration conf = inverter.getConfiguration();    inverter.setJobName("LinkAnalysis Inverter");    FileInputFormat.addInputPath(inverter, nodeDb);    FileInputFormat.addInputPath(inverter, outlinkDb);    FileOutputFormat.setOutputPath(inverter, output);    inverter.setInputFormatClass(SequenceFileInputFormat.class);    inverter.setJarByClass(Inverter.class);    inverter.setMapperClass(Inverter.InvertMapper.class);    inverter.setReducerClass(Inverter.InvertReducer.class);    inverter.setMapOutputKeyClass(Text.class);    inverter.setMapOutputValueClass(ObjectWritable.class);    inverter.setOutputKeyClass(Text.class);    inverter.setOutputValueClass(LinkDatum.class);    inverter.setOutputFormatClass(SequenceFileOutputFormat.class);    conf.setBoolean("mapreduce.fileoutputcommitter.marksuccessfuljobs", false);            try {        boolean success = inverter.waitForCompletion(true);        if (!success) {            String message = "Inverter job did not succeed, job status:" + inverter.getStatus().getState() + ", reason: " + inverter.getStatus().getFailureInfo();                        throw new RuntimeException(message);        }    } catch (IOException | InterruptedException | ClassNotFoundException e) {                throw e;    }    }
private void nutch_f855_1(Path nodeDb, Path inverted, Path output, int iteration, int numIterations, float rankOne) throws IOException, InterruptedException, ClassNotFoundException
{    Job analyzer = NutchJob.getInstance(getConf());    Configuration conf = analyzer.getConfiguration();    conf.set("link.analyze.iteration", String.valueOf(iteration + 1));    analyzer.setJobName("LinkAnalysis Analyzer, iteration " + (iteration + 1) + " of " + numIterations);    FileInputFormat.addInputPath(analyzer, nodeDb);    FileInputFormat.addInputPath(analyzer, inverted);    FileOutputFormat.setOutputPath(analyzer, output);    conf.set("link.analyze.rank.one", String.valueOf(rankOne));    analyzer.setMapOutputKeyClass(Text.class);    analyzer.setMapOutputValueClass(ObjectWritable.class);    analyzer.setInputFormatClass(SequenceFileInputFormat.class);    analyzer.setJarByClass(Analyzer.class);    analyzer.setMapperClass(Analyzer.AnalyzerMapper.class);    analyzer.setReducerClass(Analyzer.AnalyzerReducer.class);    analyzer.setOutputKeyClass(Text.class);    analyzer.setOutputValueClass(Node.class);    analyzer.setOutputFormatClass(MapFileOutputFormat.class);    conf.setBoolean("mapreduce.fileoutputcommitter.marksuccessfuljobs", false);        try {        boolean success = analyzer.waitForCompletion(true);        if (!success) {            String message = "Analysis job did not succeed, job status:" + analyzer.getStatus().getState() + ", reason: " + analyzer.getStatus().getFailureInfo();                        throw new RuntimeException(message);        }    } catch (IOException | InterruptedException | ClassNotFoundException e) {                throw e;    }    }
public void nutch_f856_0(Mapper<Text, Node, Text, LongWritable>.Context context)
{}
public void nutch_f857_0(Text key, Node value, Context context) throws IOException, InterruptedException
{    context.write(numNodes, one);}
public void nutch_f858_0(Reducer<Text, LongWritable, Text, LongWritable>.Context context)
{}
public void nutch_f859_0(Text key, Iterable<LongWritable> values, Context context) throws IOException, InterruptedException
{    long total = 0;    for (LongWritable val : values) {        total += val.get();    }    context.write(numNodes, new LongWritable(total));}
public void nutch_f860_0(Mapper<Text, Node, Text, Node>.Context context)
{    conf = context.getConfiguration();    initialScore = conf.getFloat("link.analyze.initial.score", 1.0f);}
public void nutch_f861_0(Text key, Node node, Context context) throws IOException, InterruptedException
{    String url = key.toString();    Node outNode = WritableUtils.clone(node, conf);    outNode.setInlinkScore(initialScore);    context.write(new Text(url), outNode);}
public void nutch_f862_0(Mapper<Text, Writable, Text, ObjectWritable>.Context context)
{}
public void nutch_f863_0(Text key, Writable value, Context context) throws IOException, InterruptedException
{    ObjectWritable objWrite = new ObjectWritable();    objWrite.set(value);    context.write(key, objWrite);}
public void nutch_f864_0(Reducer<Text, ObjectWritable, Text, LinkDatum>.Context context)
{    conf = context.getConfiguration();}
public void nutch_f865_1(Text key, Iterable<ObjectWritable> values, Context context) throws IOException, InterruptedException
{    String fromUrl = key.toString();    List<LinkDatum> outlinks = new ArrayList<>();    Node node = null;        for (ObjectWritable write : values) {        Object obj = write.get();        if (obj instanceof Node) {            node = (Node) obj;        } else if (obj instanceof LinkDatum) {            outlinks.add(WritableUtils.clone((LinkDatum) obj, conf));        }    }            int numOutlinks = node.getNumOutlinks();    float inlinkScore = node.getInlinkScore();    float outlinkScore = node.getOutlinkScore();            if (numOutlinks > 0) {        for (int i = 0; i < outlinks.size(); i++) {            LinkDatum outlink = outlinks.get(i);            String toUrl = outlink.getUrl();            outlink.setUrl(fromUrl);            outlink.setScore(outlinkScore);                        context.write(new Text(toUrl), outlink);                    }    }}
public void nutch_f866_0(Mapper<Text, Writable, Text, ObjectWritable>.Context context)
{    conf = context.getConfiguration();}
public void nutch_f867_0(Text key, Writable value, Context context) throws IOException, InterruptedException
{    ObjectWritable objWrite = new ObjectWritable();    objWrite.set(WritableUtils.clone(value, conf));    context.write(key, objWrite);}
public void nutch_f868_0(Reducer<Text, ObjectWritable, Text, Node>.Context context)
{    conf = context.getConfiguration();    dampingFactor = conf.getFloat("link.analyze.damping.factor", 0.85f);    rankOne = conf.getFloat("link.analyze.rank.one", 0.0f);    itNum = conf.getInt("link.analyze.iteration", 0);    limitPages = conf.getBoolean("link.ignore.limit.page", true);    limitDomains = conf.getBoolean("link.ignore.limit.domain", true);}
public void nutch_f869_1(Text key, Iterable<ObjectWritable> values, Context context) throws IOException, InterruptedException
{    String url = key.toString();    Set<String> domains = new HashSet<>();    Set<String> pages = new HashSet<>();    Node node = null;        int numInlinks = 0;    float totalInlinkScore = rankOne;    for (ObjectWritable next : values) {        Object value = next.get();        if (value instanceof Node) {            node = (Node) value;        } else if (value instanceof LinkDatum) {            LinkDatum linkDatum = (LinkDatum) value;            float scoreFromInlink = linkDatum.getScore();            String inlinkUrl = linkDatum.getUrl();            String inLinkDomain = URLUtil.getDomainName(inlinkUrl);            String inLinkPage = URLUtil.getPage(inlinkUrl);                        if ((limitPages && pages.contains(inLinkPage)) || (limitDomains && domains.contains(inLinkDomain))) {                                continue;            }                        numInlinks++;            totalInlinkScore += scoreFromInlink;            domains.add(inLinkDomain);            pages.add(inLinkPage);                    }    }        float linkRankScore = (1 - dampingFactor) + (dampingFactor * totalInlinkScore);            Node outNode = WritableUtils.clone(node, conf);    outNode.setInlinkScore(linkRankScore);    context.write(key, outNode);}
public void nutch_f870_0()
{}
public void nutch_f871_1(Path webGraphDb) throws IOException, ClassNotFoundException, InterruptedException
{    SimpleDateFormat sdf = new SimpleDateFormat("yyyy-MM-dd HH:mm:ss");    long start = System.currentTimeMillis();                Path linkRank = new Path(webGraphDb, "linkrank");    Configuration conf = getConf();    FileSystem fs = linkRank.getFileSystem(conf);        if (!fs.exists(linkRank)) {        fs.mkdirs(linkRank);    }        Path wgOutlinkDb = new Path(webGraphDb, WebGraph.OUTLINK_DIR);    Path wgNodeDb = new Path(webGraphDb, WebGraph.NODE_DIR);    Path nodeDb = new Path(linkRank, WebGraph.NODE_DIR);            int numLinks = runCounter(fs, webGraphDb);    runInitializer(wgNodeDb, nodeDb);    float rankOneScore = (1f / (float) numLinks);    if (LOG.isInfoEnabled()) {                    }            int numIterations = conf.getInt("link.analyze.num.iterations", 10);    for (int i = 0; i < numIterations; i++) {                        Path tempRank = new Path(linkRank + "-" + Integer.toString(new Random().nextInt(Integer.MAX_VALUE)));        fs.mkdirs(tempRank);        Path tempInverted = new Path(tempRank, "inverted");        Path tempNodeDb = new Path(tempRank, WebGraph.NODE_DIR);                runInverter(nodeDb, wgOutlinkDb, tempInverted);        runAnalysis(nodeDb, tempInverted, tempNodeDb, i, numIterations, rankOneScore);                        FSUtils.replace(fs, linkRank, tempRank, true);            }            FSUtils.replace(fs, wgNodeDb, nodeDb, true);        fs.delete(linkRank, true);    long end = System.currentTimeMillis();    }
public static void nutch_f872_0(String[] args) throws Exception
{    int res = ToolRunner.run(NutchConfiguration.create(), new LinkRank(), args);    System.exit(res);}
public int nutch_f873_1(String[] args) throws Exception
{    Options options = new Options();    OptionBuilder.withArgName("help");    OptionBuilder.withDescription("show this help message");    Option helpOpts = OptionBuilder.create("help");    options.addOption(helpOpts);    OptionBuilder.withArgName("webgraphdb");    OptionBuilder.hasArg();    OptionBuilder.withDescription("the web graph db to use");    Option webgraphOpts = OptionBuilder.create("webgraphdb");    options.addOption(webgraphOpts);    CommandLineParser parser = new GnuParser();    try {        CommandLine line = parser.parse(options, args);        if (line.hasOption("help") || !line.hasOption("webgraphdb")) {            HelpFormatter formatter = new HelpFormatter();            formatter.printHelp("LinkRank", options);            return -1;        }        String webGraphDb = line.getOptionValue("webgraphdb");        analyze(new Path(webGraphDb));        return 0;    } catch (Exception e) {                return -2;    }}
public int nutch_f874_0()
{    return numInlinks;}
public void nutch_f875_0(int numInlinks)
{    this.numInlinks = numInlinks;}
public int nutch_f876_0()
{    return numOutlinks;}
public void nutch_f877_0(int numOutlinks)
{    this.numOutlinks = numOutlinks;}
public float nutch_f878_0()
{    return inlinkScore;}
public void nutch_f879_0(float inlinkScore)
{    this.inlinkScore = inlinkScore;}
public float nutch_f880_0()
{    return (numOutlinks > 0) ? inlinkScore / numOutlinks : inlinkScore;}
public Metadata nutch_f881_0()
{    return metadata;}
public void nutch_f882_0(Metadata metadata)
{    this.metadata = metadata;}
public void nutch_f883_0(DataInput in) throws IOException
{    numInlinks = in.readInt();    numOutlinks = in.readInt();    inlinkScore = in.readFloat();    metadata.clear();    metadata.readFields(in);}
public void nutch_f884_0(DataOutput out) throws IOException
{    out.writeInt(numInlinks);    out.writeInt(numOutlinks);    out.writeFloat(inlinkScore);    metadata.write(out);}
public String nutch_f885_0()
{    return "num inlinks: " + numInlinks + ", num outlinks: " + numOutlinks + ", inlink score: " + inlinkScore + ", outlink score: " + getOutlinkScore() + ", metadata: " + metadata.toString();}
public void nutch_f886_0(Mapper<Text, Node, FloatWritable, Text>.Context context)
{    conf = context.getConfiguration();    inlinks = conf.getBoolean("inlinks", false);    outlinks = conf.getBoolean("outlinks", false);}
public void nutch_f887_0(Text key, Node node, Context context) throws IOException, InterruptedException
{    float number = 0;    if (inlinks) {        number = node.getNumInlinks();    } else if (outlinks) {        number = node.getNumOutlinks();    } else {        number = node.getInlinkScore();    }        context.write(new FloatWritable(-number), key);}
public void nutch_f888_0(Reducer<FloatWritable, Text, Text, FloatWritable>.Context context)
{    conf = context.getConfiguration();    topn = conf.getLong("topn", Long.MAX_VALUE);}
public void nutch_f889_0(FloatWritable key, Iterable<Text> values, Context context) throws IOException, InterruptedException
{            float val = key.get();    FloatWritable number = new FloatWritable(val == 0 ? 0 : -val);    long numCollected = 0;        for (Text value : values) {        if (numCollected < topn) {            Text url = WritableUtils.clone(value, conf);            context.write(url, number);            numCollected++;        }    }}
public void nutch_f890_0(Mapper<Text, Node, Text, FloatWritable>.Context context)
{    conf = context.getConfiguration();    inlinks = conf.getBoolean("inlinks", false);    outlinks = conf.getBoolean("outlinks", false);    host = conf.getBoolean("host", false);}
public void nutch_f891_0(Text key, Node node, Context context) throws IOException, InterruptedException
{    float number = 0;    if (inlinks) {        number = node.getNumInlinks();    } else if (outlinks) {        number = node.getNumOutlinks();    } else {        number = node.getInlinkScore();    }    if (host) {        key.set(URLUtil.getHost(key.toString()));    } else {        key.set(URLUtil.getDomainName(key.toString()));    }    context.write(key, new FloatWritable(number));}
public void nutch_f892_0(Text key, Iterable<FloatWritable> values, Context context) throws IOException, InterruptedException
{    long numCollected = 0;    float sumOrMax = 0;    float val = 0;        for (FloatWritable value : values) {        if (numCollected < topn) {            val = value.get();            if (sum) {                sumOrMax += val;            } else {                if (sumOrMax < val) {                    sumOrMax = val;                }            }            numCollected++;        } else {            break;        }    }    context.write(key, new FloatWritable(sumOrMax));}
public void nutch_f893_0(Reducer<Text, FloatWritable, Text, FloatWritable>.Context context)
{    conf = context.getConfiguration();    topn = conf.getLong("topn", Long.MAX_VALUE);    sum = conf.getBoolean("sum", false);}
public void nutch_f894_1(Path webGraphDb, DumpType type, long topN, Path output, boolean asEff, NameType nameType, AggrType aggrType, boolean asSequenceFile) throws Exception
{    SimpleDateFormat sdf = new SimpleDateFormat("yyyy-MM-dd HH:mm:ss");    long start = System.currentTimeMillis();        Path nodeDb = new Path(webGraphDb, WebGraph.NODE_DIR);    Job dumper = NutchJob.getInstance(getConf());    Configuration conf = dumper.getConfiguration();    dumper.setJobName("NodeDumper: " + webGraphDb);    FileInputFormat.addInputPath(dumper, nodeDb);    dumper.setInputFormatClass(SequenceFileInputFormat.class);    if (nameType == null) {        dumper.setJarByClass(Sorter.class);        dumper.setMapperClass(Sorter.SorterMapper.class);        dumper.setReducerClass(Sorter.SorterReducer.class);        dumper.setMapOutputKeyClass(FloatWritable.class);        dumper.setMapOutputValueClass(Text.class);    } else {        dumper.setJarByClass(Dumper.class);        dumper.setMapperClass(Dumper.DumperMapper.class);        dumper.setReducerClass(Dumper.DumperReducer.class);        dumper.setMapOutputKeyClass(Text.class);        dumper.setMapOutputValueClass(FloatWritable.class);    }    dumper.setOutputKeyClass(Text.class);    dumper.setOutputValueClass(FloatWritable.class);    FileOutputFormat.setOutputPath(dumper, output);    if (asSequenceFile) {        dumper.setOutputFormatClass(SequenceFileOutputFormat.class);    } else {        dumper.setOutputFormatClass(TextOutputFormat.class);    }    dumper.setNumReduceTasks(1);    conf.setBoolean("inlinks", type == DumpType.INLINKS);    conf.setBoolean("outlinks", type == DumpType.OUTLINKS);    conf.setBoolean("scores", type == DumpType.SCORES);    conf.setBoolean("host", nameType == NameType.HOST);    conf.setBoolean("domain", nameType == NameType.DOMAIN);    conf.setBoolean("sum", aggrType == AggrType.SUM);    conf.setBoolean("max", aggrType == AggrType.MAX);    conf.setLong("topn", topN);        if (asEff) {        conf.set("mapreduce.output.textoutputformat.separator", "=");    }    try {                boolean success = dumper.waitForCompletion(true);        if (!success) {            String message = "NodeDumper job did not succeed, job status:" + dumper.getStatus().getState() + ", reason: " + dumper.getStatus().getFailureInfo();                        throw new RuntimeException(message);        }    } catch (IOException e) {                throw e;    }    long end = System.currentTimeMillis();    }
public static void nutch_f895_0(String[] args) throws Exception
{    int res = ToolRunner.run(NutchConfiguration.create(), new NodeDumper(), args);    System.exit(res);}
public int nutch_f896_1(String[] args) throws Exception
{    Options options = new Options();    OptionBuilder.withArgName("help");    OptionBuilder.withDescription("show this help message");    Option helpOpts = OptionBuilder.create("help");    options.addOption(helpOpts);    OptionBuilder.withArgName("webgraphdb");    OptionBuilder.hasArg();    OptionBuilder.withDescription("the web graph database to use");    Option webGraphDbOpts = OptionBuilder.create("webgraphdb");    options.addOption(webGraphDbOpts);    OptionBuilder.withArgName("inlinks");    OptionBuilder.withDescription("show highest inlinks");    Option inlinkOpts = OptionBuilder.create("inlinks");    options.addOption(inlinkOpts);    OptionBuilder.withArgName("outlinks");    OptionBuilder.withDescription("show highest outlinks");    Option outlinkOpts = OptionBuilder.create("outlinks");    options.addOption(outlinkOpts);    OptionBuilder.withArgName("scores");    OptionBuilder.withDescription("show highest scores");    Option scoreOpts = OptionBuilder.create("scores");    options.addOption(scoreOpts);    OptionBuilder.withArgName("topn");    OptionBuilder.hasOptionalArg();    OptionBuilder.withDescription("show topN scores");    Option topNOpts = OptionBuilder.create("topn");    options.addOption(topNOpts);    OptionBuilder.withArgName("output");    OptionBuilder.hasArg();    OptionBuilder.withDescription("the output directory to use");    Option outputOpts = OptionBuilder.create("output");    options.addOption(outputOpts);    OptionBuilder.withArgName("asEff");    OptionBuilder.withDescription("Solr ExternalFileField compatible output format");    Option effOpts = OptionBuilder.create("asEff");    options.addOption(effOpts);    OptionBuilder.hasArgs(2);    OptionBuilder.withDescription("group <host|domain> <sum|max>");    Option groupOpts = OptionBuilder.create("group");    options.addOption(groupOpts);    OptionBuilder.withArgName("asSequenceFile");    OptionBuilder.withDescription("whether to output as a sequencefile");    Option sequenceFileOpts = OptionBuilder.create("asSequenceFile");    options.addOption(sequenceFileOpts);    CommandLineParser parser = new GnuParser();    try {        CommandLine line = parser.parse(options, args);        if (line.hasOption("help") || !line.hasOption("webgraphdb")) {            HelpFormatter formatter = new HelpFormatter();            formatter.printHelp("NodeDumper", options);            return -1;        }        String webGraphDb = line.getOptionValue("webgraphdb");        boolean inlinks = line.hasOption("inlinks");        boolean outlinks = line.hasOption("outlinks");        long topN = (line.hasOption("topn") ? Long.parseLong(line.getOptionValue("topn")) : Long.MAX_VALUE);                String output = line.getOptionValue("output");        DumpType type = (inlinks ? DumpType.INLINKS : outlinks ? DumpType.OUTLINKS : DumpType.SCORES);        NameType nameType = null;        AggrType aggrType = null;        String[] group = line.getOptionValues("group");        if (group != null && group.length == 2) {            nameType = (group[0].equals("host") ? NameType.HOST : group[0].equals("domain") ? NameType.DOMAIN : null);            aggrType = (group[1].equals("sum") ? AggrType.SUM : group[1].equals("sum") ? AggrType.MAX : null);        }                boolean asEff = line.hasOption("asEff");        boolean asSequenceFile = line.hasOption("asSequenceFile");        dumpNodes(new Path(webGraphDb), type, topN, new Path(output), asEff, nameType, aggrType, asSequenceFile);        return 0;    } catch (Exception e) {                return -2;    }}
public void nutch_f897_0(Path webGraphDb, String url) throws IOException
{    nodeReaders = MapFileOutputFormat.getReaders(new Path(webGraphDb, WebGraph.NODE_DIR), getConf());        Text key = new Text(url);    Node node = new Node();    MapFileOutputFormat.getEntry(nodeReaders, new HashPartitioner<>(), key, node);    System.out.println(url + ":");    System.out.println("  inlink score: " + node.getInlinkScore());    System.out.println("  outlink score: " + node.getOutlinkScore());    System.out.println("  num inlinks: " + node.getNumInlinks());    System.out.println("  num outlinks: " + node.getNumOutlinks());    FSUtils.closeReaders(nodeReaders);}
public static void nutch_f898_0(String[] args) throws Exception
{    Options options = new Options();    OptionBuilder.withArgName("help");    OptionBuilder.withDescription("show this help message");    Option helpOpts = OptionBuilder.create("help");    options.addOption(helpOpts);    OptionBuilder.withArgName("webgraphdb");    OptionBuilder.hasArg();    OptionBuilder.withDescription("the webgraphdb to use");    Option webGraphOpts = OptionBuilder.create("webgraphdb");    options.addOption(webGraphOpts);    OptionBuilder.withArgName("url");    OptionBuilder.hasOptionalArg();    OptionBuilder.withDescription("the url to dump");    Option urlOpts = OptionBuilder.create("url");    options.addOption(urlOpts);    CommandLineParser parser = new GnuParser();    try {                CommandLine line = parser.parse(options, args);        if (line.hasOption("help") || !line.hasOption("webgraphdb") || !line.hasOption("url")) {            HelpFormatter formatter = new HelpFormatter();            formatter.printHelp("WebGraphReader", options);            return;        }                String webGraphDb = line.getOptionValue("webgraphdb");        String url = line.getOptionValue("url");        NodeReader reader = new NodeReader(NutchConfiguration.create());        reader.dumpUrl(new Path(webGraphDb), url);        return;    } catch (Exception e) {        e.printStackTrace();        return;    }}
public void nutch_f899_0(Text key, Writable value, Context context) throws IOException, InterruptedException
{    ObjectWritable objWrite = new ObjectWritable();    objWrite.set(value);    context.write(key, objWrite);}
public void nutch_f900_0(Reducer<Text, ObjectWritable, Text, CrawlDatum>.Context context)
{    Configuration conf = context.getConfiguration();    clearScore = conf.getFloat("link.score.updater.clear.score", 0.0f);}
public void nutch_f901_1(Text key, Iterable<ObjectWritable> values, Context context) throws IOException, InterruptedException
{    String url = key.toString();    Node node = null;    CrawlDatum datum = null;        for (ObjectWritable next : values) {        Object value = next.get();        if (value instanceof Node) {            node = (Node) value;        } else if (value instanceof CrawlDatum) {            datum = (CrawlDatum) value;        }    }        if (datum != null) {        if (node != null) {                        float inlinkScore = node.getInlinkScore();            datum.setScore(inlinkScore);                    } else {                        datum.setScore(clearScore);                    }        context.write(key, datum);    } else {            }}
public void nutch_f902_1(Path crawlDb, Path webGraphDb) throws IOException, ClassNotFoundException, InterruptedException
{    SimpleDateFormat sdf = new SimpleDateFormat("yyyy-MM-dd HH:mm:ss");    long start = System.currentTimeMillis();        Configuration conf = getConf();            Path nodeDb = new Path(webGraphDb, WebGraph.NODE_DIR);    Path crawlDbCurrent = new Path(crawlDb, CrawlDb.CURRENT_NAME);    Path newCrawlDb = new Path(crawlDb, Integer.toString(new Random().nextInt(Integer.MAX_VALUE)));        Job updater = NutchJob.getInstance(conf);    updater.setJobName("Update CrawlDb from WebGraph");    FileInputFormat.addInputPath(updater, crawlDbCurrent);    FileInputFormat.addInputPath(updater, nodeDb);    FileOutputFormat.setOutputPath(updater, newCrawlDb);    updater.setInputFormatClass(SequenceFileInputFormat.class);    updater.setJarByClass(ScoreUpdater.class);    updater.setMapperClass(ScoreUpdater.ScoreUpdaterMapper.class);    updater.setReducerClass(ScoreUpdater.ScoreUpdaterReducer.class);    updater.setMapOutputKeyClass(Text.class);    updater.setMapOutputValueClass(ObjectWritable.class);    updater.setOutputKeyClass(Text.class);    updater.setOutputValueClass(CrawlDatum.class);    updater.setOutputFormatClass(MapFileOutputFormat.class);    try {        boolean success = updater.waitForCompletion(true);        if (!success) {            String message = "Update CrawlDb from WebGraph job did not succeed, job status:" + updater.getStatus().getState() + ", reason: " + updater.getStatus().getFailureInfo();                                    FileSystem fs = newCrawlDb.getFileSystem(conf);            if (fs.exists(newCrawlDb)) {                fs.delete(newCrawlDb, true);            }            throw new RuntimeException(message);        }    } catch (IOException | ClassNotFoundException | InterruptedException e) {                        FileSystem fs = newCrawlDb.getFileSystem(conf);        if (fs.exists(newCrawlDb)) {            fs.delete(newCrawlDb, true);        }        throw e;    }            CrawlDb.install(updater, crawlDb);    long end = System.currentTimeMillis();    }
public static void nutch_f903_0(String[] args) throws Exception
{    int res = ToolRunner.run(NutchConfiguration.create(), new ScoreUpdater(), args);    System.exit(res);}
public int nutch_f904_1(String[] args) throws Exception
{    Options options = new Options();    OptionBuilder.withArgName("help");    OptionBuilder.withDescription("show this help message");    Option helpOpts = OptionBuilder.create("help");    options.addOption(helpOpts);    OptionBuilder.withArgName("crawldb");    OptionBuilder.hasArg();    OptionBuilder.withDescription("the crawldb to use");    Option crawlDbOpts = OptionBuilder.create("crawldb");    options.addOption(crawlDbOpts);    OptionBuilder.withArgName("webgraphdb");    OptionBuilder.hasArg();    OptionBuilder.withDescription("the webgraphdb to use");    Option webGraphOpts = OptionBuilder.create("webgraphdb");    options.addOption(webGraphOpts);    CommandLineParser parser = new GnuParser();    try {        CommandLine line = parser.parse(options, args);        if (line.hasOption("help") || !line.hasOption("webgraphdb") || !line.hasOption("crawldb")) {            HelpFormatter formatter = new HelpFormatter();            formatter.printHelp("ScoreUpdater", options);            return -1;        }        String crawlDb = line.getOptionValue("crawldb");        String webGraphDb = line.getOptionValue("webgraphdb");        update(new Path(crawlDb), new Path(webGraphDb));        return 0;    } catch (Exception e) {                return -1;    }}
private static long nutch_f905_0(ParseData data)
{        long fetchTime = System.currentTimeMillis();    String fetchTimeStr = data.getContentMeta().get(Nutch.FETCH_TIME_KEY);    try {                fetchTime = Long.parseLong(fetchTimeStr);    } catch (Exception e) {        fetchTime = System.currentTimeMillis();    }    return fetchTime;}
private String nutch_f906_1(String url)
{    if (!normalize) {        return url;    }    String normalized = null;    if (urlNormalizers != null) {        try {                        normalized = urlNormalizers.normalize(url, URLNormalizers.SCOPE_DEFAULT);            normalized = normalized.trim();        } catch (Exception e) {                        normalized = null;        }    }    return normalized;}
private String nutch_f907_0(String url)
{    if (!filter) {        return url;    }    try {        url = filters.filter(url);    } catch (Exception e) {        url = null;    }    return url;}
public void nutch_f908_0(Mapper<Text, Writable, Text, NutchWritable>.Context context)
{    Configuration config = context.getConfiguration();    conf = config;    normalize = conf.getBoolean(URL_NORMALIZING, false);    filter = conf.getBoolean(URL_FILTERING, false);    if (normalize) {        urlNormalizers = new URLNormalizers(conf, URLNormalizers.SCOPE_DEFAULT);    }    if (filter) {        filters = new URLFilters(conf);    }}
public void nutch_f909_0(Text key, Writable value, Context context) throws IOException, InterruptedException
{        String url = normalizeUrl(key.toString());    if (url == null) {        return;    }        if (filterUrl(url) == null) {        return;    }        key.set(url);    if (value instanceof CrawlDatum) {        CrawlDatum datum = (CrawlDatum) value;        if (datum.getStatus() == CrawlDatum.STATUS_FETCH_REDIR_TEMP || datum.getStatus() == CrawlDatum.STATUS_FETCH_REDIR_PERM || datum.getStatus() == CrawlDatum.STATUS_FETCH_GONE) {                        context.write(key, new NutchWritable(new BooleanWritable(true)));        }    } else if (value instanceof ParseData) {                        ParseData data = (ParseData) value;        long fetchTime = getFetchTime(data);        Outlink[] outlinkAr = data.getOutlinks();        Map<String, String> outlinkMap = new LinkedHashMap<>();                if (outlinkAr != null && outlinkAr.length > 0) {            for (int i = 0; i < outlinkAr.length; i++) {                Outlink outlink = outlinkAr[i];                String toUrl = normalizeUrl(outlink.getToUrl());                if (filterUrl(toUrl) == null) {                    continue;                }                                                                boolean existingUrl = outlinkMap.containsKey(toUrl);                if (toUrl != null && (!existingUrl || (existingUrl && outlinkMap.get(toUrl) == null))) {                    outlinkMap.put(toUrl, outlink.getAnchor());                }            }        }                for (String outlinkUrl : outlinkMap.keySet()) {            String anchor = outlinkMap.get(outlinkUrl);            LinkDatum datum = new LinkDatum(outlinkUrl, anchor, fetchTime);            context.write(key, new NutchWritable(datum));        }    } else if (value instanceof LinkDatum) {        LinkDatum datum = (LinkDatum) value;        String linkDatumUrl = normalizeUrl(datum.getUrl());        if (filterUrl(linkDatumUrl) != null) {            datum.setUrl(linkDatumUrl);                        context.write(key, new NutchWritable(datum));        }    }}
public void nutch_f910_0(Reducer<Text, NutchWritable, Text, LinkDatum>.Context context)
{    Configuration config = context.getConfiguration();    conf = config;    ignoreHost = conf.getBoolean("link.ignore.internal.host", true);    ignoreDomain = conf.getBoolean("link.ignore.internal.domain", true);    limitPages = conf.getBoolean("link.ignore.limit.page", true);    limitDomains = conf.getBoolean("link.ignore.limit.domain", true);}
public void nutch_f911_0(Text key, Iterable<NutchWritable> values, Context context) throws IOException, InterruptedException
{            long mostRecent = 0L;    List<LinkDatum> outlinkList = new ArrayList<>();    for (NutchWritable val : values) {        final Writable value = val.get();        if (value instanceof LinkDatum) {                        LinkDatum next = (LinkDatum) value;            long timestamp = next.getTimestamp();            if (mostRecent == 0L || mostRecent < timestamp) {                mostRecent = timestamp;            }            outlinkList.add(WritableUtils.clone(next, conf));            context.getCounter("WebGraph.outlinks", "added links").increment(1);        } else if (value instanceof BooleanWritable) {            BooleanWritable delete = (BooleanWritable) value;                        if (delete.get() == true) {                                context.getCounter("WebGraph.outlinks", "removed links").increment(1);                return;            }        }    }        String url = key.toString();    String domain = URLUtil.getDomainName(url);    String host = URLUtil.getHost(url);        Set<String> domains = new HashSet<>();    Set<String> pages = new HashSet<>();        for (LinkDatum datum : outlinkList) {                String toUrl = datum.getUrl();        String toDomain = URLUtil.getDomainName(toUrl);        String toHost = URLUtil.getHost(toUrl);        String toPage = URLUtil.getPage(toUrl);        datum.setLinkType(LinkDatum.OUTLINK);                if (datum.getTimestamp() == mostRecent && (!limitPages || (limitPages && !pages.contains(toPage))) && (!limitDomains || (limitDomains && !domains.contains(toDomain))) && (!ignoreHost || (ignoreHost && !toHost.equalsIgnoreCase(host))) && (!ignoreDomain || (ignoreDomain && !toDomain.equalsIgnoreCase(domain)))) {            context.write(key, datum);            pages.add(toPage);            domains.add(toDomain);        }    }}
public void nutch_f912_0()
{}
public void nutch_f913_0(Mapper<Text, LinkDatum, Text, LinkDatum>.Context context)
{    timestamp = System.currentTimeMillis();}
public void nutch_f914_0(Text key, LinkDatum datum, Context context) throws IOException, InterruptedException
{        String fromUrl = key.toString();    String toUrl = datum.getUrl();    String anchor = datum.getAnchor();        LinkDatum inlink = new LinkDatum(fromUrl, anchor, timestamp);    inlink.setLinkType(LinkDatum.INLINK);    context.write(new Text(toUrl), inlink);}
public void nutch_f915_0(Reducer<Text, LinkDatum, Text, Node>.Context context)
{}
public void nutch_f916_0(Text key, Iterable<LinkDatum> values, Context context) throws IOException, InterruptedException
{    Node node = new Node();    int numInlinks = 0;    int numOutlinks = 0;        for (LinkDatum next : values) {        if (next.getLinkType() == LinkDatum.INLINK) {            numInlinks++;        } else if (next.getLinkType() == LinkDatum.OUTLINK) {            numOutlinks++;        }    }        node.setNumInlinks(numInlinks);    node.setNumOutlinks(numOutlinks);    node.setInlinkScore(0.0f);    context.write(key, node);}
public void nutch_f917_1(Path webGraphDb, Path[] segments, boolean normalize, boolean filter) throws IOException, InterruptedException, ClassNotFoundException
{    SimpleDateFormat sdf = new SimpleDateFormat("yyyy-MM-dd HH:mm:ss");    long start = System.currentTimeMillis();    if (LOG.isInfoEnabled()) {                                    }    FileSystem fs = webGraphDb.getFileSystem(getConf());        Path lock = new Path(webGraphDb, LOCK_NAME);    if (!fs.exists(webGraphDb)) {        fs.mkdirs(webGraphDb);    }    LockUtil.createLockFile(fs, lock, false);        Path outlinkDb = new Path(webGraphDb, OUTLINK_DIR);    Path oldOutlinkDb = new Path(webGraphDb, OLD_OUTLINK_DIR);    if (!fs.exists(outlinkDb)) {        fs.mkdirs(outlinkDb);    }    Path tempOutlinkDb = new Path(outlinkDb + "-" + Integer.toString(new Random().nextInt(Integer.MAX_VALUE)));    Job outlinkJob = NutchJob.getInstance(getConf());    Configuration outlinkJobConf = outlinkJob.getConfiguration();    outlinkJob.setJobName("Outlinkdb: " + outlinkDb);    boolean deleteGone = outlinkJobConf.getBoolean("link.delete.gone", false);    boolean preserveBackup = outlinkJobConf.getBoolean("db.preserve.backup", true);    if (deleteGone) {            }        if (segments != null) {        for (int i = 0; i < segments.length; i++) {            FileSystem sfs = segments[i].getFileSystem(outlinkJobConf);            Path parseData = new Path(segments[i], ParseData.DIR_NAME);            if (sfs.exists(parseData)) {                                FileInputFormat.addInputPath(outlinkJob, parseData);            }            if (deleteGone) {                Path crawlFetch = new Path(segments[i], CrawlDatum.FETCH_DIR_NAME);                if (sfs.exists(crawlFetch)) {                                        FileInputFormat.addInputPath(outlinkJob, crawlFetch);                }            }        }    }            FileInputFormat.addInputPath(outlinkJob, outlinkDb);    outlinkJobConf.setBoolean(OutlinkDb.URL_NORMALIZING, normalize);    outlinkJobConf.setBoolean(OutlinkDb.URL_FILTERING, filter);    outlinkJob.setInputFormatClass(SequenceFileInputFormat.class);    outlinkJob.setJarByClass(OutlinkDb.class);    outlinkJob.setMapperClass(OutlinkDb.OutlinkDbMapper.class);    outlinkJob.setReducerClass(OutlinkDb.OutlinkDbReducer.class);    outlinkJob.setMapOutputKeyClass(Text.class);    outlinkJob.setMapOutputValueClass(NutchWritable.class);    outlinkJob.setOutputKeyClass(Text.class);    outlinkJob.setOutputValueClass(LinkDatum.class);    FileOutputFormat.setOutputPath(outlinkJob, tempOutlinkDb);    outlinkJob.setOutputFormatClass(MapFileOutputFormat.class);    outlinkJobConf.setBoolean("mapreduce.fileoutputcommitter.marksuccessfuljobs", false);        try {                boolean success = outlinkJob.waitForCompletion(true);        if (!success) {            String message = "OutlinkDb job did not succeed, job status:" + outlinkJob.getStatus().getState() + ", reason: " + outlinkJob.getStatus().getFailureInfo();                        NutchJob.cleanupAfterFailure(tempOutlinkDb, lock, fs);            throw new RuntimeException(message);        }                FSUtils.replace(fs, oldOutlinkDb, outlinkDb, true);        FSUtils.replace(fs, outlinkDb, tempOutlinkDb, true);        if (!preserveBackup && fs.exists(oldOutlinkDb))            fs.delete(oldOutlinkDb, true);            } catch (IOException | InterruptedException | ClassNotFoundException e) {                        NutchJob.cleanupAfterFailure(tempOutlinkDb, lock, fs);        throw e;    }        Path inlinkDb = new Path(webGraphDb, INLINK_DIR);    Path tempInlinkDb = new Path(inlinkDb + "-" + Integer.toString(new Random().nextInt(Integer.MAX_VALUE)));    Job inlinkJob = NutchJob.getInstance(getConf());    Configuration inlinkJobConf = inlinkJob.getConfiguration();    inlinkJob.setJobName("Inlinkdb " + inlinkDb);        FileInputFormat.addInputPath(inlinkJob, outlinkDb);    inlinkJob.setInputFormatClass(SequenceFileInputFormat.class);    inlinkJob.setJarByClass(InlinkDb.class);    inlinkJob.setMapperClass(InlinkDb.InlinkDbMapper.class);    inlinkJob.setMapOutputKeyClass(Text.class);    inlinkJob.setMapOutputValueClass(LinkDatum.class);    inlinkJob.setOutputKeyClass(Text.class);    inlinkJob.setOutputValueClass(LinkDatum.class);    FileOutputFormat.setOutputPath(inlinkJob, tempInlinkDb);    inlinkJob.setOutputFormatClass(MapFileOutputFormat.class);    inlinkJobConf.setBoolean("mapreduce.fileoutputcommitter.marksuccessfuljobs", false);    try {                        boolean success = inlinkJob.waitForCompletion(true);        if (!success) {            String message = "InlinkDb job did not succeed, job status:" + inlinkJob.getStatus().getState() + ", reason: " + inlinkJob.getStatus().getFailureInfo();                        NutchJob.cleanupAfterFailure(tempInlinkDb, lock, fs);            throw new RuntimeException(message);        }                FSUtils.replace(fs, inlinkDb, tempInlinkDb, true);            } catch (IOException | InterruptedException | ClassNotFoundException e) {                        NutchJob.cleanupAfterFailure(tempInlinkDb, lock, fs);        throw e;    }        Path nodeDb = new Path(webGraphDb, NODE_DIR);    Path tempNodeDb = new Path(nodeDb + "-" + Integer.toString(new Random().nextInt(Integer.MAX_VALUE)));    Job nodeJob = NutchJob.getInstance(getConf());    Configuration nodeJobConf = nodeJob.getConfiguration();    nodeJob.setJobName("NodeDb " + nodeDb);            FileInputFormat.addInputPath(nodeJob, outlinkDb);    FileInputFormat.addInputPath(nodeJob, inlinkDb);    nodeJob.setInputFormatClass(SequenceFileInputFormat.class);    nodeJob.setJarByClass(NodeDb.class);    nodeJob.setReducerClass(NodeDb.NodeDbReducer.class);    nodeJob.setMapOutputKeyClass(Text.class);    nodeJob.setMapOutputValueClass(LinkDatum.class);    nodeJob.setOutputKeyClass(Text.class);    nodeJob.setOutputValueClass(Node.class);    FileOutputFormat.setOutputPath(nodeJob, tempNodeDb);    nodeJob.setOutputFormatClass(MapFileOutputFormat.class);    nodeJobConf.setBoolean("mapreduce.fileoutputcommitter.marksuccessfuljobs", false);    try {                        boolean success = nodeJob.waitForCompletion(true);        if (!success) {            String message = "NodeDb job did not succeed, job status:" + nodeJob.getStatus().getState() + ", reason: " + nodeJob.getStatus().getFailureInfo();                                    NutchJob.cleanupAfterFailure(tempNodeDb, lock, fs);            throw new RuntimeException(message);        }                FSUtils.replace(fs, nodeDb, tempNodeDb, true);            } catch (IOException | InterruptedException | ClassNotFoundException e) {                        NutchJob.cleanupAfterFailure(tempNodeDb, lock, fs);        throw e;    }        LockUtil.removeLockFile(fs, lock);    long end = System.currentTimeMillis();    }
public static void nutch_f918_0(String[] args) throws Exception
{    int res = ToolRunner.run(NutchConfiguration.create(), new WebGraph(), args);    System.exit(res);}
public int nutch_f919_1(String[] args) throws Exception
{        Option helpOpt = new Option("h", "help", false, "show this help message");    Option normOpt = new Option("n", "normalize", false, "whether to use URLNormalizers on the URL's in the segment");    Option filtOpt = new Option("f", "filter", false, "whether to use URLFilters on the URL's in the segment");        @SuppressWarnings("static-access")    Option graphOpt = OptionBuilder.withArgName("webgraphdb").hasArg().withDescription("the web graph database to create (if none exists) or use if one does").create("webgraphdb");    @SuppressWarnings("static-access")    Option segOpt = OptionBuilder.withArgName("segment").hasArgs().withDescription("the segment(s) to use").create("segment");    @SuppressWarnings("static-access")    Option segDirOpt = OptionBuilder.withArgName("segmentDir").hasArgs().withDescription("the segment directory to use").create("segmentDir");        Options options = new Options();    options.addOption(helpOpt);    options.addOption(normOpt);    options.addOption(filtOpt);    options.addOption(graphOpt);    options.addOption(segOpt);    options.addOption(segDirOpt);    CommandLineParser parser = new GnuParser();    try {        CommandLine line = parser.parse(options, args);        if (line.hasOption("help") || !line.hasOption("webgraphdb") || (!line.hasOption("segment") && !line.hasOption("segmentDir"))) {            HelpFormatter formatter = new HelpFormatter();            formatter.printHelp("WebGraph", options, true);            return -1;        }        String webGraphDb = line.getOptionValue("webgraphdb");        Path[] segPaths = null;                if (line.hasOption("segment")) {            String[] segments = line.getOptionValues("segment");            segPaths = new Path[segments.length];            for (int i = 0; i < segments.length; i++) {                segPaths[i] = new Path(segments[i]);            }        }                if (line.hasOption("segmentDir")) {            Path dir = new Path(line.getOptionValue("segmentDir"));            FileSystem fs = dir.getFileSystem(getConf());            FileStatus[] fstats = fs.listStatus(dir, HadoopFSUtil.getPassDirectoriesFilter(fs));            segPaths = HadoopFSUtil.getPaths(fstats);        }        boolean normalize = false;        if (line.hasOption("normalize")) {            normalize = true;        }        boolean filter = false;        if (line.hasOption("filter")) {            filter = true;        }        createWebGraph(new Path(webGraphDb), segPaths, normalize, filter);        return 0;    } catch (Exception e) {                return -2;    }}
public Text nutch_f920_0()
{    return new Text();}
public Text nutch_f921_0()
{    return new Text();}
public boolean nutch_f922_0()
{    return false;}
public void nutch_f923_0(InputSplit split, TaskAttemptContext context)
{}
public synchronized boolean nutch_f924_0(Text key, Text value) throws IOException, InterruptedException
{        Text tKey = key;    if (!sequenceFileRecordReader.nextKeyValue()) {        return false;    }    tKey.set(innerKey.toString());    String contentAsStr = new String(innerValue.getContent());        contentAsStr = contentAsStr.replaceAll("\n", " ");    value.set(contentAsStr);    return true;}
public float nutch_f925_0() throws IOException
{    return sequenceFileRecordReader.getProgress();}
public synchronized void nutch_f926_0() throws IOException
{    sequenceFileRecordReader.close();}
public RecordReader<Text, Text> nutch_f927_0(InputSplit split, Job job, Context context) throws IOException
{    context.setStatus(split.toString());    Configuration conf = job.getConfiguration();    return new ContentAsTextRecordReader(conf, (FileSplit) split);}
public static boolean nutch_f928_1(Path segmentPath, FileSystem fs) throws IOException
{    if (segmentPath == null || fs == null) {                return false;    }    boolean checkResult = true;    checkResult &= checkSegmentDir(segmentPath, fs);    if (checkResult) {        return true;    } else {        return false;    }}
public static boolean nutch_f929_1(Path segmentPath, FileSystem fs) throws IOException
{    if (segmentPath.getName().length() != 14) {                return false;    }    FileStatus[] fstats_segment = fs.listStatus(segmentPath, HadoopFSUtil.getPassDirectoriesFilter(fs));    Path[] segment_files = HadoopFSUtil.getPaths(fstats_segment);    boolean crawlFetchExists = false;    boolean crawlParseExists = false;    boolean parseDataExists = false;    boolean parseTextExists = false;    for (Path path : segment_files) {        String pathName = path.getName();        crawlFetchExists |= pathName.equals(CrawlDatum.FETCH_DIR_NAME);        crawlParseExists |= pathName.equals(CrawlDatum.PARSE_DIR_NAME);        parseDataExists |= pathName.equals(ParseData.DIR_NAME);        parseTextExists |= pathName.equals(ParseText.DIR_NAME);    }    if (parseTextExists && crawlParseExists && crawlFetchExists && parseDataExists) {                        return true;    } else {                StringBuilder missingDir = new StringBuilder("");        if (parseDataExists == false) {            missingDir.append(ParseData.DIR_NAME + ", ");        }        if (parseTextExists == false) {            missingDir.append(ParseText.DIR_NAME + ", ");        }        if (crawlParseExists == false) {            missingDir.append(CrawlDatum.PARSE_DIR_NAME + ", ");        }        if (crawlFetchExists == false) {            missingDir.append(CrawlDatum.FETCH_DIR_NAME + ", ");        }        String missingDirString = missingDir.toString();                return false;    }}
public static boolean nutch_f930_0(Path segment, FileSystem fs) throws IOException
{    if (fs.exists(new Path(segment, CrawlDatum.PARSE_DIR_NAME))) {        return true;    }    return false;}
public boolean nutch_f931_0(Text key, CrawlDatum generateData, CrawlDatum fetchData, CrawlDatum sigData, Content content, ParseData parseData, ParseText parseText, Collection<CrawlDatum> linked)
{    for (SegmentMergeFilter filter : filters) {        if (!filter.filter(key, generateData, fetchData, sigData, content, parseData, parseText, linked)) {            if (LOG.isTraceEnabled())                LOG.trace("Key " + key + " dropped by " + filter.getClass().getName());            return false;        }    }    if (LOG.isTraceEnabled())        LOG.trace("Key " + key + " accepted for merge.");    return true;}
public RecordReader<Text, MetaWrapper> nutch_f932_1(final InputSplit split, TaskAttemptContext context) throws IOException
{    context.setStatus(split.toString());        SegmentPart segmentPart;    final String spString;    final FileSplit fSplit = (FileSplit) split;    try {        segmentPart = SegmentPart.get(fSplit);        spString = segmentPart.toString();    } catch (IOException e) {        throw new RuntimeException("Cannot identify segment:", e);    }    final SequenceFileRecordReader<Text, Writable> splitReader = new SequenceFileRecordReader<>();    return new SequenceFileRecordReader<Text, MetaWrapper>() {        private Text key;        private MetaWrapper wrapper;        private Writable w;        @Override        public void initialize(InputSplit split, TaskAttemptContext context) throws IOException, InterruptedException {            splitReader.initialize(split, context);        }        @Override        public synchronized boolean nextKeyValue() throws IOException, InterruptedException {            try {                boolean res = splitReader.nextKeyValue();                if (res == false) {                    return res;                }                key = splitReader.getCurrentKey();                w = splitReader.getCurrentValue();                wrapper = new MetaWrapper();                wrapper.set(w);                wrapper.setMeta(SEGMENT_PART_KEY, spString);                return res;            } catch (InterruptedException e) {                                throw e;            }        }        @Override        public Text getCurrentKey() {            return key;        }        @Override        public MetaWrapper getCurrentValue() {            return wrapper;        }        @Override        public synchronized void close() throws IOException {            splitReader.close();        }    };}
public void nutch_f933_0(InputSplit split, TaskAttemptContext context) throws IOException, InterruptedException
{    splitReader.initialize(split, context);}
public synchronized boolean nutch_f934_1() throws IOException, InterruptedException
{    try {        boolean res = splitReader.nextKeyValue();        if (res == false) {            return res;        }        key = splitReader.getCurrentKey();        w = splitReader.getCurrentValue();        wrapper = new MetaWrapper();        wrapper.set(w);        wrapper.setMeta(SEGMENT_PART_KEY, spString);        return res;    } catch (InterruptedException e) {                throw e;    }}
public Text nutch_f935_0()
{    return key;}
public MetaWrapper nutch_f936_0()
{    return wrapper;}
public synchronized void nutch_f937_0() throws IOException
{    splitReader.close();}
public RecordWriter<Text, MetaWrapper> nutch_f938_0(TaskAttemptContext context) throws IOException
{    Configuration conf = context.getConfiguration();    String name = getUniqueFile(context, "part", "");    Path dir = FileOutputFormat.getOutputPath(context);    FileSystem fs = dir.getFileSystem(context.getConfiguration());    return new RecordWriter<Text, MetaWrapper>() {        MapFile.Writer cOut = null;        MapFile.Writer fOut = null;        MapFile.Writer pdOut = null;        MapFile.Writer ptOut = null;        SequenceFile.Writer gOut = null;        SequenceFile.Writer pOut = null;        HashMap<String, Closeable> sliceWriters = new HashMap<>();        String segmentName = conf.get("segment.merger.segmentName");        public void write(Text key, MetaWrapper wrapper) throws IOException {                        SegmentPart sp = SegmentPart.parse(wrapper.getMeta(SEGMENT_PART_KEY));            Writable o = wrapper.get();            String slice = wrapper.getMeta(SEGMENT_SLICE_KEY);            if (o instanceof CrawlDatum) {                if (sp.partName.equals(CrawlDatum.GENERATE_DIR_NAME)) {                    gOut = ensureSequenceFile(slice, CrawlDatum.GENERATE_DIR_NAME);                    gOut.append(key, o);                } else if (sp.partName.equals(CrawlDatum.FETCH_DIR_NAME)) {                    fOut = ensureMapFile(slice, CrawlDatum.FETCH_DIR_NAME, CrawlDatum.class);                    fOut.append(key, o);                } else if (sp.partName.equals(CrawlDatum.PARSE_DIR_NAME)) {                    pOut = ensureSequenceFile(slice, CrawlDatum.PARSE_DIR_NAME);                    pOut.append(key, o);                } else {                    throw new IOException("Cannot determine segment part: " + sp.partName);                }            } else if (o instanceof Content) {                cOut = ensureMapFile(slice, Content.DIR_NAME, Content.class);                cOut.append(key, o);            } else if (o instanceof ParseData) {                                if (slice == null) {                    ((ParseData) o).getContentMeta().set(Nutch.SEGMENT_NAME_KEY, segmentName);                } else {                    ((ParseData) o).getContentMeta().set(Nutch.SEGMENT_NAME_KEY, segmentName + "-" + slice);                }                pdOut = ensureMapFile(slice, ParseData.DIR_NAME, ParseData.class);                pdOut.append(key, o);            } else if (o instanceof ParseText) {                ptOut = ensureMapFile(slice, ParseText.DIR_NAME, ParseText.class);                ptOut.append(key, o);            }        }                private SequenceFile.Writer ensureSequenceFile(String slice, String dirName) throws IOException {            if (slice == null)                slice = DEFAULT_SLICE;            SequenceFile.Writer res = (SequenceFile.Writer) sliceWriters.get(slice + dirName);            if (res != null)                return res;            Path wname;            Path out = FileOutputFormat.getOutputPath(context);            if (slice == DEFAULT_SLICE) {                wname = new Path(new Path(new Path(out, segmentName), dirName), name);            } else {                wname = new Path(new Path(new Path(out, segmentName + "-" + slice), dirName), name);            }            res = SequenceFile.createWriter(conf, SequenceFile.Writer.file(wname), SequenceFile.Writer.keyClass(Text.class), SequenceFile.Writer.valueClass(CrawlDatum.class), SequenceFile.Writer.bufferSize(fs.getConf().getInt("io.file.buffer.size", 4096)), SequenceFile.Writer.replication(fs.getDefaultReplication(wname)), SequenceFile.Writer.blockSize(1073741824), SequenceFile.Writer.compression(SequenceFileOutputFormat.getOutputCompressionType(context), new DefaultCodec()), SequenceFile.Writer.progressable((Progressable) context), SequenceFile.Writer.metadata(new Metadata()));            sliceWriters.put(slice + dirName, res);            return res;        }                private MapFile.Writer ensureMapFile(String slice, String dirName, Class<? extends Writable> clazz) throws IOException {            if (slice == null)                slice = DEFAULT_SLICE;            MapFile.Writer res = (MapFile.Writer) sliceWriters.get(slice + dirName);            if (res != null)                return res;            Path wname;            Path out = FileOutputFormat.getOutputPath(context);            if (slice == DEFAULT_SLICE) {                wname = new Path(new Path(new Path(out, segmentName), dirName), name);            } else {                wname = new Path(new Path(new Path(out, segmentName + "-" + slice), dirName), name);            }            CompressionType compType = SequenceFileOutputFormat.getOutputCompressionType(context);            if (clazz.isAssignableFrom(ParseText.class)) {                compType = CompressionType.RECORD;            }            Option rKeyClassOpt = MapFile.Writer.keyClass(Text.class);            org.apache.hadoop.io.SequenceFile.Writer.Option rValClassOpt = SequenceFile.Writer.valueClass(clazz);            org.apache.hadoop.io.SequenceFile.Writer.Option rProgressOpt = SequenceFile.Writer.progressable((Progressable) context);            org.apache.hadoop.io.SequenceFile.Writer.Option rCompOpt = SequenceFile.Writer.compression(compType);            res = new MapFile.Writer(conf, wname, rKeyClassOpt, rValClassOpt, rCompOpt, rProgressOpt);            sliceWriters.put(slice + dirName, res);            return res;        }        @Override        public void close(TaskAttemptContext context) throws IOException {            Iterator<Closeable> it = sliceWriters.values().iterator();            while (it.hasNext()) {                Object o = it.next();                if (o instanceof SequenceFile.Writer) {                    ((SequenceFile.Writer) o).close();                } else {                    ((MapFile.Writer) o).close();                }            }        }    };}
public void nutch_f939_0(Text key, MetaWrapper wrapper) throws IOException
{        SegmentPart sp = SegmentPart.parse(wrapper.getMeta(SEGMENT_PART_KEY));    Writable o = wrapper.get();    String slice = wrapper.getMeta(SEGMENT_SLICE_KEY);    if (o instanceof CrawlDatum) {        if (sp.partName.equals(CrawlDatum.GENERATE_DIR_NAME)) {            gOut = ensureSequenceFile(slice, CrawlDatum.GENERATE_DIR_NAME);            gOut.append(key, o);        } else if (sp.partName.equals(CrawlDatum.FETCH_DIR_NAME)) {            fOut = ensureMapFile(slice, CrawlDatum.FETCH_DIR_NAME, CrawlDatum.class);            fOut.append(key, o);        } else if (sp.partName.equals(CrawlDatum.PARSE_DIR_NAME)) {            pOut = ensureSequenceFile(slice, CrawlDatum.PARSE_DIR_NAME);            pOut.append(key, o);        } else {            throw new IOException("Cannot determine segment part: " + sp.partName);        }    } else if (o instanceof Content) {        cOut = ensureMapFile(slice, Content.DIR_NAME, Content.class);        cOut.append(key, o);    } else if (o instanceof ParseData) {                if (slice == null) {            ((ParseData) o).getContentMeta().set(Nutch.SEGMENT_NAME_KEY, segmentName);        } else {            ((ParseData) o).getContentMeta().set(Nutch.SEGMENT_NAME_KEY, segmentName + "-" + slice);        }        pdOut = ensureMapFile(slice, ParseData.DIR_NAME, ParseData.class);        pdOut.append(key, o);    } else if (o instanceof ParseText) {        ptOut = ensureMapFile(slice, ParseText.DIR_NAME, ParseText.class);        ptOut.append(key, o);    }}
private SequenceFile.Writer nutch_f940_0(String slice, String dirName) throws IOException
{    if (slice == null)        slice = DEFAULT_SLICE;    SequenceFile.Writer res = (SequenceFile.Writer) sliceWriters.get(slice + dirName);    if (res != null)        return res;    Path wname;    Path out = FileOutputFormat.getOutputPath(context);    if (slice == DEFAULT_SLICE) {        wname = new Path(new Path(new Path(out, segmentName), dirName), name);    } else {        wname = new Path(new Path(new Path(out, segmentName + "-" + slice), dirName), name);    }    res = SequenceFile.createWriter(conf, SequenceFile.Writer.file(wname), SequenceFile.Writer.keyClass(Text.class), SequenceFile.Writer.valueClass(CrawlDatum.class), SequenceFile.Writer.bufferSize(fs.getConf().getInt("io.file.buffer.size", 4096)), SequenceFile.Writer.replication(fs.getDefaultReplication(wname)), SequenceFile.Writer.blockSize(1073741824), SequenceFile.Writer.compression(SequenceFileOutputFormat.getOutputCompressionType(context), new DefaultCodec()), SequenceFile.Writer.progressable((Progressable) context), SequenceFile.Writer.metadata(new Metadata()));    sliceWriters.put(slice + dirName, res);    return res;}
private MapFile.Writer nutch_f941_0(String slice, String dirName, Class<? extends Writable> clazz) throws IOException
{    if (slice == null)        slice = DEFAULT_SLICE;    MapFile.Writer res = (MapFile.Writer) sliceWriters.get(slice + dirName);    if (res != null)        return res;    Path wname;    Path out = FileOutputFormat.getOutputPath(context);    if (slice == DEFAULT_SLICE) {        wname = new Path(new Path(new Path(out, segmentName), dirName), name);    } else {        wname = new Path(new Path(new Path(out, segmentName + "-" + slice), dirName), name);    }    CompressionType compType = SequenceFileOutputFormat.getOutputCompressionType(context);    if (clazz.isAssignableFrom(ParseText.class)) {        compType = CompressionType.RECORD;    }    Option rKeyClassOpt = MapFile.Writer.keyClass(Text.class);    org.apache.hadoop.io.SequenceFile.Writer.Option rValClassOpt = SequenceFile.Writer.valueClass(clazz);    org.apache.hadoop.io.SequenceFile.Writer.Option rProgressOpt = SequenceFile.Writer.progressable((Progressable) context);    org.apache.hadoop.io.SequenceFile.Writer.Option rCompOpt = SequenceFile.Writer.compression(compType);    res = new MapFile.Writer(conf, wname, rKeyClassOpt, rValClassOpt, rCompOpt, rProgressOpt);    sliceWriters.put(slice + dirName, res);    return res;}
public void nutch_f942_0(TaskAttemptContext context) throws IOException
{    Iterator<Closeable> it = sliceWriters.values().iterator();    while (it.hasNext()) {        Object o = it.next();        if (o instanceof SequenceFile.Writer) {            ((SequenceFile.Writer) o).close();        } else {            ((MapFile.Writer) o).close();        }    }}
public void nutch_f943_0(Configuration conf)
{    super.setConf(conf);}
public void nutch_f944_0() throws IOException
{}
public void nutch_f945_0(Mapper<Text, MetaWrapper, Text, MetaWrapper>.Context context)
{    Configuration conf = context.getConfiguration();    if (conf.getBoolean("segment.merger.filter", false)) {        filters = new URLFilters(conf);    }    if (conf.getBoolean("segment.merger.normalizer", false))        normalizers = new URLNormalizers(conf, URLNormalizers.SCOPE_DEFAULT);}
public void nutch_f946_1(Text key, MetaWrapper value, Context context) throws IOException, InterruptedException
{    Text newKey = new Text();    String url = key.toString();    if (normalizers != null) {        try {                        url = normalizers.normalize(url, URLNormalizers.SCOPE_DEFAULT);        } catch (Exception e) {                        url = null;        }    }    if (url != null && filters != null) {        try {            url = filters.filter(url);        } catch (Exception e) {                        url = null;        }    }    if (url != null) {        newKey.set(url);        context.write(newKey, value);    }}
public void nutch_f947_1(Reducer<Text, MetaWrapper, Text, MetaWrapper>.Context context)
{    Configuration conf = context.getConfiguration();    if (conf.getBoolean("segment.merger.filter", false)) {        mergeFilters = new SegmentMergeFilters(conf);    }    sliceSize = conf.getLong("segment.merger.slice", -1);    if ((sliceSize > 0) && (LOG.isInfoEnabled())) {            }    if (sliceSize > 0) {        sliceSize = sliceSize / Integer.parseInt(conf.get("mapreduce.job.reduces"));    }}
public void nutch_f948_0(Text key, Iterable<MetaWrapper> values, Context context) throws IOException, InterruptedException
{    CrawlDatum lastG = null;    CrawlDatum lastF = null;    CrawlDatum lastSig = null;    Content lastC = null;    ParseData lastPD = null;    ParseText lastPT = null;    String lastGname = null;    String lastFname = null;    String lastSigname = null;    String lastCname = null;    String lastPDname = null;    String lastPTname = null;    TreeMap<String, ArrayList<CrawlDatum>> linked = new TreeMap<>();    for (MetaWrapper wrapper : values) {        Object o = wrapper.get();        String spString = wrapper.getMeta(SEGMENT_PART_KEY);        if (spString == null) {            throw new IOException("Null segment part, key=" + key);        }        SegmentPart sp = SegmentPart.parse(spString);        if (o instanceof CrawlDatum) {            CrawlDatum val = (CrawlDatum) o;                        if (sp.partName.equals(CrawlDatum.GENERATE_DIR_NAME)) {                if (lastG == null) {                    lastG = val;                    lastGname = sp.segmentName;                } else {                                        if (lastGname.compareTo(sp.segmentName) < 0) {                        lastG = val;                        lastGname = sp.segmentName;                    }                }            } else if (sp.partName.equals(CrawlDatum.FETCH_DIR_NAME)) {                                if (CrawlDatum.hasFetchStatus(val) && val.getStatus() != CrawlDatum.STATUS_FETCH_RETRY && val.getStatus() != CrawlDatum.STATUS_FETCH_NOTMODIFIED) {                    if (lastF == null) {                        lastF = val;                        lastFname = sp.segmentName;                    } else {                        if (lastFname.compareTo(sp.segmentName) < 0) {                            lastF = val;                            lastFname = sp.segmentName;                        }                    }                }            } else if (sp.partName.equals(CrawlDatum.PARSE_DIR_NAME)) {                if (val.getStatus() == CrawlDatum.STATUS_SIGNATURE) {                    if (lastSig == null) {                        lastSig = val;                        lastSigname = sp.segmentName;                    } else {                                                if (lastSigname.compareTo(sp.segmentName) < 0) {                            lastSig = val;                            lastSigname = sp.segmentName;                        }                    }                    continue;                }                                ArrayList<CrawlDatum> segLinked = linked.get(sp.segmentName);                if (segLinked == null) {                    segLinked = new ArrayList<>();                    linked.put(sp.segmentName, segLinked);                }                segLinked.add(val);            } else {                throw new IOException("Cannot determine segment part: " + sp.partName);            }        } else if (o instanceof Content) {            if (lastC == null) {                lastC = (Content) o;                lastCname = sp.segmentName;            } else {                if (lastCname.compareTo(sp.segmentName) < 0) {                    lastC = (Content) o;                    lastCname = sp.segmentName;                }            }        } else if (o instanceof ParseData) {            if (lastPD == null) {                lastPD = (ParseData) o;                lastPDname = sp.segmentName;            } else {                if (lastPDname.compareTo(sp.segmentName) < 0) {                    lastPD = (ParseData) o;                    lastPDname = sp.segmentName;                }            }        } else if (o instanceof ParseText) {            if (lastPT == null) {                lastPT = (ParseText) o;                lastPTname = sp.segmentName;            } else {                if (lastPTname.compareTo(sp.segmentName) < 0) {                    lastPT = (ParseText) o;                    lastPTname = sp.segmentName;                }            }        }    }        if (mergeFilters != null && !mergeFilters.filter(key, lastG, lastF, lastSig, lastC, lastPD, lastPT, linked.isEmpty() ? null : linked.lastEntry().getValue())) {        return;    }    curCount++;    String sliceName;    MetaWrapper wrapper = new MetaWrapper();    if (sliceSize > 0) {        sliceName = String.valueOf(curCount / sliceSize);        wrapper.setMeta(SEGMENT_SLICE_KEY, sliceName);    }    SegmentPart sp = new SegmentPart();        if (lastG != null) {        wrapper.set(lastG);        sp.partName = CrawlDatum.GENERATE_DIR_NAME;        sp.segmentName = lastGname;        wrapper.setMeta(SEGMENT_PART_KEY, sp.toString());        context.write(key, wrapper);    }    if (lastF != null) {        wrapper.set(lastF);        sp.partName = CrawlDatum.FETCH_DIR_NAME;        sp.segmentName = lastFname;        wrapper.setMeta(SEGMENT_PART_KEY, sp.toString());        context.write(key, wrapper);    }    if (lastSig != null) {        wrapper.set(lastSig);        sp.partName = CrawlDatum.PARSE_DIR_NAME;        sp.segmentName = lastSigname;        wrapper.setMeta(SEGMENT_PART_KEY, sp.toString());        context.write(key, wrapper);    }    if (lastC != null) {        wrapper.set(lastC);        sp.partName = Content.DIR_NAME;        sp.segmentName = lastCname;        wrapper.setMeta(SEGMENT_PART_KEY, sp.toString());        context.write(key, wrapper);    }    if (lastPD != null) {        wrapper.set(lastPD);        sp.partName = ParseData.DIR_NAME;        sp.segmentName = lastPDname;        wrapper.setMeta(SEGMENT_PART_KEY, sp.toString());        context.write(key, wrapper);    }    if (lastPT != null) {        wrapper.set(lastPT);        sp.partName = ParseText.DIR_NAME;        sp.segmentName = lastPTname;        wrapper.setMeta(SEGMENT_PART_KEY, sp.toString());        context.write(key, wrapper);    }    if (linked.size() > 0) {        String name = linked.lastKey();        sp.partName = CrawlDatum.PARSE_DIR_NAME;        sp.segmentName = name;        wrapper.setMeta(SEGMENT_PART_KEY, sp.toString());        ArrayList<CrawlDatum> segLinked = linked.get(name);        for (int i = 0; i < segLinked.size(); i++) {            CrawlDatum link = segLinked.get(i);            wrapper.set(link);            context.write(key, wrapper);        }    }}
public void nutch_f949_1(Path out, Path[] segs, boolean filter, boolean normalize, long slice) throws IOException, ClassNotFoundException, InterruptedException
{    String segmentName = Generator.generateSegmentName();    if (LOG.isInfoEnabled()) {            }    Job job = NutchJob.getInstance(getConf());    Configuration conf = job.getConfiguration();    job.setJobName("mergesegs " + out + "/" + segmentName);    conf.setBoolean("segment.merger.filter", filter);    conf.setBoolean("segment.merger.normalizer", normalize);    conf.setLong("segment.merger.slice", slice);    conf.set("segment.merger.segmentName", segmentName);        boolean g = true;    boolean f = true;    boolean p = true;    boolean c = true;    boolean pd = true;    boolean pt = true;        boolean pg = true;    boolean pf = true;    boolean pp = true;    boolean pc = true;    boolean ppd = true;    boolean ppt = true;    for (int i = 0; i < segs.length; i++) {        FileSystem fs = segs[i].getFileSystem(conf);        if (!fs.exists(segs[i])) {            if (LOG.isWarnEnabled()) {                            }            segs[i] = null;            continue;        }        if (LOG.isInfoEnabled()) {                    }        Path cDir = new Path(segs[i], Content.DIR_NAME);        Path gDir = new Path(segs[i], CrawlDatum.GENERATE_DIR_NAME);        Path fDir = new Path(segs[i], CrawlDatum.FETCH_DIR_NAME);        Path pDir = new Path(segs[i], CrawlDatum.PARSE_DIR_NAME);        Path pdDir = new Path(segs[i], ParseData.DIR_NAME);        Path ptDir = new Path(segs[i], ParseText.DIR_NAME);        c = c && fs.exists(cDir);        g = g && fs.exists(gDir);        f = f && fs.exists(fDir);        p = p && fs.exists(pDir);        pd = pd && fs.exists(pdDir);        pt = pt && fs.exists(ptDir);                if (g != pg || f != pf || p != pp || c != pc || pd != ppd || pt != ppt) {                    }        pg = g;        pf = f;        pp = p;        pc = c;        ppd = pd;        ppt = pt;    }    StringBuilder sb = new StringBuilder();    if (c)        sb.append(" " + Content.DIR_NAME);    if (g)        sb.append(" " + CrawlDatum.GENERATE_DIR_NAME);    if (f)        sb.append(" " + CrawlDatum.FETCH_DIR_NAME);    if (p)        sb.append(" " + CrawlDatum.PARSE_DIR_NAME);    if (pd)        sb.append(" " + ParseData.DIR_NAME);    if (pt)        sb.append(" " + ParseText.DIR_NAME);    if (LOG.isInfoEnabled()) {            }    for (int i = 0; i < segs.length; i++) {        if (segs[i] == null)            continue;        if (g) {            Path gDir = new Path(segs[i], CrawlDatum.GENERATE_DIR_NAME);            FileInputFormat.addInputPath(job, gDir);        }        if (c) {            Path cDir = new Path(segs[i], Content.DIR_NAME);            FileInputFormat.addInputPath(job, cDir);        }        if (f) {            Path fDir = new Path(segs[i], CrawlDatum.FETCH_DIR_NAME);            FileInputFormat.addInputPath(job, fDir);        }        if (p) {            Path pDir = new Path(segs[i], CrawlDatum.PARSE_DIR_NAME);            FileInputFormat.addInputPath(job, pDir);        }        if (pd) {            Path pdDir = new Path(segs[i], ParseData.DIR_NAME);            FileInputFormat.addInputPath(job, pdDir);        }        if (pt) {            Path ptDir = new Path(segs[i], ParseText.DIR_NAME);            FileInputFormat.addInputPath(job, ptDir);        }    }    job.setInputFormatClass(ObjectInputFormat.class);    job.setJarByClass(SegmentMerger.class);    job.setMapperClass(SegmentMerger.SegmentMergerMapper.class);    job.setReducerClass(SegmentMerger.SegmentMergerReducer.class);    FileOutputFormat.setOutputPath(job, out);    job.setOutputKeyClass(Text.class);    job.setOutputValueClass(MetaWrapper.class);    job.setOutputFormatClass(SegmentOutputFormat.class);    setConf(conf);    try {        boolean success = job.waitForCompletion(true);        if (!success) {            String message = "SegmentMerger job did not succeed, job status:" + job.getStatus().getState() + ", reason: " + job.getStatus().getFailureInfo();                        throw new RuntimeException(message);        }    } catch (IOException | InterruptedException | ClassNotFoundException e) {                throw e;    }}
public int nutch_f950_0(String[] args) throws Exception
{    if (args.length < 2) {        System.err.println("SegmentMerger output_dir (-dir segments | seg1 seg2 ...) [-filter] [-slice NNNN]");        System.err.println("\toutput_dir\tname of the parent dir for output segment slice(s)");        System.err.println("\t-dir segments\tparent dir containing several segments");        System.err.println("\tseg1 seg2 ...\tlist of segment dirs");        System.err.println("\t-filter\t\tfilter out URL-s prohibited by current URLFilters");        System.err.println("\t-normalize\t\tnormalize URL via current URLNormalizers");        System.err.println("\t-slice NNNN\tcreate many output segments, each containing NNNN URLs");        return -1;    }    Configuration conf = NutchConfiguration.create();    Path out = new Path(args[0]);    ArrayList<Path> segs = new ArrayList<>();    long sliceSize = 0;    boolean filter = false;    boolean normalize = false;    for (int i = 1; i < args.length; i++) {        if ("-dir".equals(args[i])) {            Path dirPath = new Path(args[++i]);            FileSystem fs = dirPath.getFileSystem(conf);            FileStatus[] fstats = fs.listStatus(dirPath, HadoopFSUtil.getPassDirectoriesFilter(fs));            Path[] files = HadoopFSUtil.getPaths(fstats);            for (int j = 0; j < files.length; j++) segs.add(files[j]);        } else if ("-filter".equals(args[i])) {            filter = true;        } else if ("-normalize".equals(args[i])) {            normalize = true;        } else if ("-slice".equals(args[i])) {            sliceSize = Long.parseLong(args[++i]);        } else {            segs.add(new Path(args[i]));        }    }    if (segs.isEmpty()) {        System.err.println("ERROR: No input segments.");        return -1;    }    merge(out, segs.toArray(new Path[segs.size()]), filter, normalize, sliceSize);    return 0;}
public static void nutch_f951_0(String[] args) throws Exception
{    int result = ToolRunner.run(NutchConfiguration.create(), new SegmentMerger(), args);    System.exit(result);}
public String nutch_f952_0()
{    return segmentName + "/" + partName;}
public static SegmentPart nutch_f953_0(FileSplit split) throws IOException
{    return get(split.getPath().toString());}
public static SegmentPart nutch_f954_0(String path) throws IOException
{        String dir = path.replace('\\', '/');    int idx = dir.lastIndexOf("/part-");    if (idx == -1) {        throw new IOException("Cannot determine segment part: " + dir);    }    dir = dir.substring(0, idx);    idx = dir.lastIndexOf('/');    if (idx == -1) {        throw new IOException("Cannot determine segment part: " + dir);    }    String part = dir.substring(idx + 1);        dir = dir.substring(0, idx);    idx = dir.lastIndexOf('/');    if (idx == -1) {        throw new IOException("Cannot determine segment name: " + dir);    }    String segment = dir.substring(idx + 1);    return new SegmentPart(segment, part);}
public static SegmentPart nutch_f955_0(String string) throws IOException
{    int idx = string.indexOf('/');    if (idx == -1) {        throw new IOException("Invalid SegmentPart: '" + string + "'");    }    String segment = string.substring(0, idx);    String part = string.substring(idx + 1);    return new SegmentPart(segment, part);}
public void nutch_f956_0(WritableComparable<?> key, Writable value, Context context) throws IOException, InterruptedException
{        if (key instanceof Text) {        newKey.set(key.toString());        key = newKey;    }    context.write((Text) key, new NutchWritable(value));}
public RecordWriter<WritableComparable<?>, Writable> nutch_f957_0(TaskAttemptContext context) throws IOException, InterruptedException
{    String name = getUniqueFile(context, "part", "");    Path dir = FileOutputFormat.getOutputPath(context);    FileSystem fs = dir.getFileSystem(context.getConfiguration());    final Path segmentDumpFile = new Path(FileOutputFormat.getOutputPath(context), name);        if (fs.exists(segmentDumpFile))        fs.delete(segmentDumpFile, true);    final PrintStream printStream = new PrintStream(fs.create(segmentDumpFile), false, StandardCharsets.UTF_8.name());    return new RecordWriter<WritableComparable<?>, Writable>() {        public synchronized void write(WritableComparable<?> key, Writable value) throws IOException {            printStream.println(value);        }        public synchronized void close(TaskAttemptContext context) throws IOException {            printStream.close();        }    };}
public synchronized void nutch_f958_0(WritableComparable<?> key, Writable value) throws IOException
{    printStream.println(value);}
public synchronized void nutch_f959_0(TaskAttemptContext context) throws IOException
{    printStream.close();}
public void nutch_f960_0(Job job)
{    Configuration conf = job.getConfiguration();    this.co = conf.getBoolean("segment.reader.co", true);    this.fe = conf.getBoolean("segment.reader.fe", true);    this.ge = conf.getBoolean("segment.reader.ge", true);    this.pa = conf.getBoolean("segment.reader.pa", true);    this.pd = conf.getBoolean("segment.reader.pd", true);    this.pt = conf.getBoolean("segment.reader.pt", true);}
public void nutch_f961_0()
{}
public void nutch_f962_1(Text key, Iterable<NutchWritable> values, Context context) throws IOException, InterruptedException
{    StringBuffer dump = new StringBuffer();    dump.append("\nRecno:: ").append(recNo++).append("\n");    dump.append("URL:: " + key.toString() + "\n");    for (NutchWritable val : values) {                Writable value = val.get();        if (value instanceof CrawlDatum) {            dump.append("\nCrawlDatum::\n").append(((CrawlDatum) value).toString());        } else if (value instanceof Content) {            dump.append("\nContent::\n").append(((Content) value).toString());        } else if (value instanceof ParseData) {            dump.append("\nParseData::\n").append(((ParseData) value).toString());        } else if (value instanceof ParseText) {            dump.append("\nParseText::\n").append(((ParseText) value).toString());        } else if (LOG.isWarnEnabled()) {                    }    }    context.write(key, new Text(dump.toString()));}
public void nutch_f963_1(Path segment, Path output) throws IOException, InterruptedException, ClassNotFoundException
{    if (LOG.isInfoEnabled()) {            }    Job job = Job.getInstance();    job.setJobName("read " + segment);    Configuration conf = job.getConfiguration();    if (ge)        FileInputFormat.addInputPath(job, new Path(segment, CrawlDatum.GENERATE_DIR_NAME));    if (fe)        FileInputFormat.addInputPath(job, new Path(segment, CrawlDatum.FETCH_DIR_NAME));    if (pa)        FileInputFormat.addInputPath(job, new Path(segment, CrawlDatum.PARSE_DIR_NAME));    if (co)        FileInputFormat.addInputPath(job, new Path(segment, Content.DIR_NAME));    if (pd)        FileInputFormat.addInputPath(job, new Path(segment, ParseData.DIR_NAME));    if (pt)        FileInputFormat.addInputPath(job, new Path(segment, ParseText.DIR_NAME));    job.setInputFormatClass(SequenceFileInputFormat.class);    job.setMapperClass(InputCompatMapper.class);    job.setReducerClass(InputCompatReducer.class);    job.setJarByClass(SegmentReader.class);    Path tempDir = new Path(conf.get("hadoop.tmp.dir", "/tmp") + "/segread-" + new java.util.Random().nextInt());    FileSystem fs = tempDir.getFileSystem(conf);    fs.delete(tempDir, true);    FileOutputFormat.setOutputPath(job, tempDir);    job.setOutputFormatClass(TextOutputFormat.class);    job.setOutputKeyClass(Text.class);    job.setOutputValueClass(NutchWritable.class);    try {        boolean success = job.waitForCompletion(true);        if (!success) {            String message = "SegmentReader job did not succeed, job status:" + job.getStatus().getState() + ", reason: " + job.getStatus().getFailureInfo();                        throw new RuntimeException(message);        }    } catch (IOException | InterruptedException | ClassNotFoundException e) {                throw e;    }        Path dumpFile = new Path(output, conf.get("segment.dump.dir", "dump"));    FileSystem outFs = dumpFile.getFileSystem(conf);        outFs.delete(dumpFile, true);    FileStatus[] fstats = fs.listStatus(tempDir, HadoopFSUtil.getPassAllFilter());    Path[] files = HadoopFSUtil.getPaths(fstats);    int currentRecordNumber = 0;    if (files.length > 0) {        try (PrintWriter writer = new PrintWriter(new BufferedWriter(new OutputStreamWriter(outFs.create(dumpFile), StandardCharsets.UTF_8)))) {            for (int i = 0; i < files.length; i++) {                Path partFile = files[i];                try {                    currentRecordNumber = append(fs, conf, partFile, writer, currentRecordNumber);                } catch (IOException exception) {                    if (LOG.isWarnEnabled()) {                                                                    }                }            }        }    }    fs.delete(tempDir, true);    if (LOG.isInfoEnabled()) {            }}
private int nutch_f964_0(FileSystem fs, Configuration conf, Path src, PrintWriter writer, int currentRecordNumber) throws IOException
{    try (BufferedReader reader = new BufferedReader(new InputStreamReader(fs.open(src), StandardCharsets.UTF_8))) {        String line = reader.readLine();        while (line != null) {            if (line.startsWith("Recno:: ")) {                line = "Recno:: " + currentRecordNumber++;            }            writer.println(line);            line = reader.readLine();        }        return currentRecordNumber;    }}
public void nutch_f965_1(final Path segment, final Text key, Writer writer, final Map<String, List<Writable>> results) throws Exception
{        ArrayList<Thread> threads = new ArrayList<>();    if (co)        threads.add(new Thread() {            public void run() {                try {                    List<Writable> res = getMapRecords(new Path(segment, Content.DIR_NAME), key);                    results.put("co", res);                } catch (Exception e) {                                    }            }        });    if (fe)        threads.add(new Thread() {            public void run() {                try {                    List<Writable> res = getMapRecords(new Path(segment, CrawlDatum.FETCH_DIR_NAME), key);                    results.put("fe", res);                } catch (Exception e) {                                    }            }        });    if (ge)        threads.add(new Thread() {            public void run() {                try {                    List<Writable> res = getSeqRecords(new Path(segment, CrawlDatum.GENERATE_DIR_NAME), key);                    results.put("ge", res);                } catch (Exception e) {                                    }            }        });    if (pa)        threads.add(new Thread() {            public void run() {                try {                    List<Writable> res = getSeqRecords(new Path(segment, CrawlDatum.PARSE_DIR_NAME), key);                    results.put("pa", res);                } catch (Exception e) {                                    }            }        });    if (pd)        threads.add(new Thread() {            public void run() {                try {                    List<Writable> res = getMapRecords(new Path(segment, ParseData.DIR_NAME), key);                    results.put("pd", res);                } catch (Exception e) {                                    }            }        });    if (pt)        threads.add(new Thread() {            public void run() {                try {                    List<Writable> res = getMapRecords(new Path(segment, ParseText.DIR_NAME), key);                    results.put("pt", res);                } catch (Exception e) {                                    }            }        });    Iterator<Thread> it = threads.iterator();    while (it.hasNext()) it.next().start();    int cnt;    do {        cnt = 0;        try {            Thread.sleep(5000);        } catch (Exception e) {        }        ;        it = threads.iterator();        while (it.hasNext()) {            if (it.next().isAlive())                cnt++;        }        if ((cnt > 0) && (LOG.isDebugEnabled())) {                    }    } while (cnt > 0);    for (int i = 0; i < keys.length; i++) {        List<Writable> res = results.get(keys[i][0]);        if (res != null && res.size() > 0) {            for (int k = 0; k < res.size(); k++) {                writer.write(keys[i][1]);                writer.write(res.get(k) + "\n");            }        }        writer.flush();    }}
public void nutch_f966_1()
{    try {        List<Writable> res = getMapRecords(new Path(segment, Content.DIR_NAME), key);        results.put("co", res);    } catch (Exception e) {            }}
public void nutch_f967_1()
{    try {        List<Writable> res = getMapRecords(new Path(segment, CrawlDatum.FETCH_DIR_NAME), key);        results.put("fe", res);    } catch (Exception e) {            }}
public void nutch_f968_1()
{    try {        List<Writable> res = getSeqRecords(new Path(segment, CrawlDatum.GENERATE_DIR_NAME), key);        results.put("ge", res);    } catch (Exception e) {            }}
public void nutch_f969_1()
{    try {        List<Writable> res = getSeqRecords(new Path(segment, CrawlDatum.PARSE_DIR_NAME), key);        results.put("pa", res);    } catch (Exception e) {            }}
public void nutch_f970_1()
{    try {        List<Writable> res = getMapRecords(new Path(segment, ParseData.DIR_NAME), key);        results.put("pd", res);    } catch (Exception e) {            }}
public void nutch_f971_1()
{    try {        List<Writable> res = getMapRecords(new Path(segment, ParseText.DIR_NAME), key);        results.put("pt", res);    } catch (Exception e) {            }}
private List<Writable> nutch_f972_0(Path dir, Text key) throws Exception
{    MapFile.Reader[] readers = MapFileOutputFormat.getReaders(dir, getConf());    ArrayList<Writable> res = new ArrayList<>();    Class<?> keyClass = readers[0].getKeyClass();    Class<?> valueClass = readers[0].getValueClass();    if (!keyClass.getName().equals("org.apache.hadoop.io.Text"))        throw new IOException("Incompatible key (" + keyClass.getName() + ")");    Writable value = (Writable) valueClass.getConstructor().newInstance();        for (int i = 0; i < readers.length; i++) {        if (readers[i].get(key, value) != null) {            res.add(value);            value = (Writable) valueClass.getConstructor().newInstance();            Text aKey = (Text) keyClass.getConstructor().newInstance();            while (readers[i].next(aKey, value) && aKey.equals(key)) {                res.add(value);                value = (Writable) valueClass.getConstructor().newInstance();            }        }        readers[i].close();    }    return res;}
private List<Writable> nutch_f973_0(Path dir, Text key) throws Exception
{    SequenceFile.Reader[] readers = org.apache.hadoop.mapred.SequenceFileOutputFormat.getReaders(getConf(), dir);    ArrayList<Writable> res = new ArrayList<>();    Class<?> keyClass = readers[0].getKeyClass();    Class<?> valueClass = readers[0].getValueClass();    if (!keyClass.getName().equals("org.apache.hadoop.io.Text"))        throw new IOException("Incompatible key (" + keyClass.getName() + ")");    WritableComparable<?> aKey = (WritableComparable<?>) keyClass.getConstructor().newInstance();    Writable value = (Writable) valueClass.getConstructor().newInstance();    for (int i = 0; i < readers.length; i++) {        while (readers[i].next(aKey, value)) {            if (aKey.equals(key)) {                res.add(value);                value = (Writable) valueClass.getConstructor().newInstance();            }        }        readers[i].close();    }    return res;}
public void nutch_f974_0(List<Path> dirs, Writer writer) throws Exception
{    writer.write("NAME\t\tGENERATED\tFETCHER START\t\tFETCHER END\t\tFETCHED\tPARSED\n");    for (int i = 0; i < dirs.size(); i++) {        Path dir = dirs.get(i);        SegmentReaderStats stats = new SegmentReaderStats();        getStats(dir, stats);        writer.write(dir.getName() + "\t");        if (stats.generated == -1)            writer.write("?");        else            writer.write(stats.generated + "");        writer.write("\t\t");        if (stats.start == -1)            writer.write("?\t");        else            writer.write(sdf.format(new Date(stats.start)));        writer.write("\t");        if (stats.end == -1)            writer.write("?");        else            writer.write(sdf.format(new Date(stats.end)));        writer.write("\t");        if (stats.fetched == -1)            writer.write("?");        else            writer.write(stats.fetched + "");        writer.write("\t");        if (stats.parsed == -1)            writer.write("?");        else            writer.write(stats.parsed + "");        writer.write("\n");        writer.flush();    }}
public void nutch_f975_0(Path segment, final SegmentReaderStats stats) throws Exception
{    long cnt = 0L;    Text key = new Text();    CrawlDatum val = new CrawlDatum();    FileSystem fs = segment.getFileSystem(getConf());    if (ge) {        SequenceFile.Reader[] readers = SegmentReaderUtil.getReaders(new Path(segment, CrawlDatum.GENERATE_DIR_NAME), getConf());        for (int i = 0; i < readers.length; i++) {            while (readers[i].next(key, val)) cnt++;            readers[i].close();        }        stats.generated = cnt;    }    if (fe) {        Path fetchDir = new Path(segment, CrawlDatum.FETCH_DIR_NAME);        if (fs.exists(fetchDir) && fs.getFileStatus(fetchDir).isDirectory()) {            cnt = 0L;            long start = Long.MAX_VALUE;            long end = Long.MIN_VALUE;            CrawlDatum value = new CrawlDatum();            MapFile.Reader[] mreaders = MapFileOutputFormat.getReaders(fetchDir, getConf());            for (int i = 0; i < mreaders.length; i++) {                while (mreaders[i].next(key, value)) {                    cnt++;                    if (value.getFetchTime() < start)                        start = value.getFetchTime();                    if (value.getFetchTime() > end)                        end = value.getFetchTime();                }                mreaders[i].close();            }            stats.start = start;            stats.end = end;            stats.fetched = cnt;        }    }    if (pd) {        Path parseDir = new Path(segment, ParseData.DIR_NAME);        if (fs.exists(parseDir) && fs.getFileStatus(parseDir).isDirectory()) {            cnt = 0L;            long errors = 0L;            ParseData value = new ParseData();            MapFile.Reader[] mreaders = MapFileOutputFormat.getReaders(parseDir, getConf());            for (int i = 0; i < mreaders.length; i++) {                while (mreaders[i].next(key, value)) {                    cnt++;                    if (!value.getStatus().isSuccess())                        errors++;                }                mreaders[i].close();            }            stats.parsed = cnt;            stats.parseErrors = errors;        }    }}
public int nutch_f976_0(String[] args) throws Exception
{    if (args.length < 2) {        usage();        return -1;    }    int mode = -1;    if (args[0].equals("-dump"))        mode = MODE_DUMP;    else if (args[0].equals("-list"))        mode = MODE_LIST;    else if (args[0].equals("-get"))        mode = MODE_GET;    boolean co = true;    boolean fe = true;    boolean ge = true;    boolean pa = true;    boolean pd = true;    boolean pt = true;        for (int i = 1; i < args.length; i++) {        if (args[i].equals("-nocontent")) {            co = false;            args[i] = null;        } else if (args[i].equals("-nofetch")) {            fe = false;            args[i] = null;        } else if (args[i].equals("-nogenerate")) {            ge = false;            args[i] = null;        } else if (args[i].equals("-noparse")) {            pa = false;            args[i] = null;        } else if (args[i].equals("-noparsedata")) {            pd = false;            args[i] = null;        } else if (args[i].equals("-noparsetext")) {            pt = false;            args[i] = null;        }    }    Configuration conf = NutchConfiguration.create();    SegmentReader segmentReader = new SegmentReader(conf, co, fe, ge, pa, pd, pt);        switch(mode) {        case MODE_DUMP:            this.co = co;            this.fe = fe;            this.ge = ge;            this.pa = pa;            this.pd = pd;            this.pt = pt;            String input = args[1];            if (input == null) {                System.err.println("Missing required argument: <segment_dir>");                usage();                return -1;            }            String output = args.length > 2 ? args[2] : null;            if (output == null) {                System.err.println("Missing required argument: <output>");                usage();                return -1;            }            dump(new Path(input), new Path(output));            return 0;        case MODE_LIST:            ArrayList<Path> dirs = new ArrayList<>();            for (int i = 1; i < args.length; i++) {                if (args[i] == null)                    continue;                if (args[i].equals("-dir")) {                    Path dir = new Path(args[++i]);                    FileSystem fs = dir.getFileSystem(conf);                    FileStatus[] fstats = fs.listStatus(dir, HadoopFSUtil.getPassDirectoriesFilter(fs));                    Path[] files = HadoopFSUtil.getPaths(fstats);                    if (files != null && files.length > 0) {                        dirs.addAll(Arrays.asList(files));                    }                } else                    dirs.add(new Path(args[i]));            }            segmentReader.list(dirs, new OutputStreamWriter(System.out, StandardCharsets.UTF_8));            return 0;        case MODE_GET:            input = args[1];            if (input == null) {                System.err.println("Missing required argument: <segment_dir>");                usage();                return -1;            }            String key = args.length > 2 ? args[2] : null;            if (key == null) {                System.err.println("Missing required argument: <keyValue>");                usage();                return -1;            }            segmentReader.get(new Path(input), new Text(key), new OutputStreamWriter(System.out, StandardCharsets.UTF_8), new HashMap<>());            return 0;        default:            System.err.println("Invalid operation: " + args[0]);            usage();            return -1;    }}
private static void nutch_f977_0()
{    System.err.println("Usage: SegmentReader (-dump ... | -list ... | -get ...) [general options]\n");    System.err.println("* General options:");    System.err.println("\t-nocontent\tignore content directory");    System.err.println("\t-nofetch\tignore crawl_fetch directory");    System.err.println("\t-nogenerate\tignore crawl_generate directory");    System.err.println("\t-noparse\tignore crawl_parse directory");    System.err.println("\t-noparsedata\tignore parse_data directory");    System.err.println("\t-noparsetext\tignore parse_text directory");    System.err.println();    System.err.println("* SegmentReader -dump <segment_dir> <output> [general options]");    System.err.println("  Dumps content of a <segment_dir> as a text file to <output>.\n");    System.err.println("\t<segment_dir>\tname of the segment directory.");    System.err.println("\t<output>\tname of the (non-existent) output directory.");    System.err.println();    System.err.println("* SegmentReader -list (<segment_dir1> ... | -dir <segments>) [general options]");    System.err.println("  List a synopsis of segments in specified directories, or all segments in");    System.err.println("  a directory <segments>, and print it on System.out\n");    System.err.println("\t<segment_dir1> ...\tlist of segment directories to process");    System.err.println("\t-dir <segments>\t\tdirectory that contains multiple segments");    System.err.println();    System.err.println("* SegmentReader -get <segment_dir> <keyValue> [general options]");    System.err.println("  Get a specified record from a segment, and print it on System.out.\n");    System.err.println("\t<segment_dir>\tname of the segment directory.");    System.err.println("\t<keyValue>\tvalue of the key (url).");    System.err.println("\t\tNote: put double-quotes around strings with spaces.");}
public static void nutch_f978_0(String[] args) throws Exception
{    int result = ToolRunner.run(NutchConfiguration.create(), new SegmentReader(), args);    System.exit(result);}
public Configuration nutch_f979_0(String confId)
{    if (confId == null) {        return configurations.get(ConfigResource.DEFAULT);    }    return configurations.get(confId);}
public Map<String, String> nutch_f980_0(String confId)
{    Configuration configuration = configurations.get(confId);    if (configuration == null) {        return Collections.emptyMap();    }    Iterator<Entry<String, String>> iterator = configuration.iterator();    Map<String, String> configMap = Maps.newTreeMap();    while (iterator.hasNext()) {        Entry<String, String> entry = iterator.next();        configMap.put(entry.getKey(), entry.getValue());    }    return configMap;}
public void nutch_f981_0(String confId, String propName, String propValue)
{    if (!configurations.containsKey(confId)) {        throw new IllegalArgumentException("Unknown configId '" + confId + "'");    }    Configuration conf = configurations.get(confId);    conf.set(propName, propValue);}
public Set<String> nutch_f982_0()
{    return configurations.keySet();}
public String nutch_f983_0(NutchConfig nutchConfig)
{    if (StringUtils.isBlank(nutchConfig.getConfigId())) {        nutchConfig.setConfigId(String.valueOf(newConfigId.incrementAndGet()));    }    if (!canCreate(nutchConfig)) {        throw new IllegalArgumentException("Config already exists.");    }    createHadoopConfig(nutchConfig);    return nutchConfig.getConfigId();}
public void nutch_f984_0(String confId)
{    configurations.remove(confId);}
private boolean nutch_f985_0(NutchConfig nutchConfig)
{    if (nutchConfig.isForce()) {        return true;    }    if (!configurations.containsKey(nutchConfig.getConfigId())) {        return true;    }    return false;}
private void nutch_f986_0(NutchConfig nutchConfig)
{    Configuration conf = NutchConfiguration.create();    configurations.put(nutchConfig.getConfigId(), conf);    if (MapUtils.isEmpty(nutchConfig.getParams())) {        return;    }    for (Entry<String, String> e : nutchConfig.getParams().entrySet()) {        conf.set(e.getKey(), e.getValue());    }}
public NutchTool nutch_f987_0(JobType type, Configuration conf)
{    if (!typeToClass.containsKey(type)) {        return null;    }    Class<? extends NutchTool> clz = typeToClass.get(type);    return createTool(clz, conf);}
public NutchTool nutch_f988_0(String className, Configuration conf)
{    try {        Class clz = Class.forName(className);        return createTool(clz, conf);    } catch (ClassNotFoundException e) {        throw new IllegalStateException(e);    }}
private NutchTool nutch_f989_0(Class<? extends NutchTool> clz, Configuration conf)
{    return ReflectionUtils.newInstance(clz, conf);}
public JobInfo nutch_f990_0(JobConfig jobConfig)
{    if (jobConfig.getArgs() == null) {        throw new IllegalArgumentException("Arguments cannot be null!");    }    Configuration conf = cloneConfiguration(jobConfig.getConfId());    NutchTool tool = createTool(jobConfig, conf);    JobWorker worker = new JobWorker(jobConfig, conf, tool);    executor.execute(worker);    executor.purge();    return worker.getInfo();}
private Configuration nutch_f991_0(String confId)
{    Configuration conf = configManager.get(confId);    if (conf == null) {        throw new IllegalArgumentException("Unknown confId " + confId);    }    return new Configuration(conf);}
public Collection<JobInfo> nutch_f992_0(String crawlId, State state)
{    if (state == null || state == State.ANY) {        return executor.getAllJobs();    }    if (state == State.RUNNING || state == State.IDLE) {        return executor.getJobRunning();    }    return executor.getJobHistory();}
public JobInfo nutch_f993_0(String crawlId, String jobId)
{    return executor.getInfo(jobId);}
public boolean nutch_f994_0(String crawlId, String id)
{    return executor.findWorker(id).killJob();}
public boolean nutch_f995_0(String crawlId, String id)
{    return executor.findWorker(id).stopJob();}
private NutchTool nutch_f996_0(JobConfig jobConfig, Configuration conf)
{    if (StringUtils.isNotBlank(jobConfig.getJobClassName())) {        return jobFactory.createToolByClassName(jobConfig.getJobClassName(), conf);    }    return jobFactory.createToolByType(jobConfig.getType(), conf);}
private String nutch_f997_0()
{    if (jobConfig.getCrawlId() == null) {        return MessageFormat.format("{0}-{1}-{2}", jobConfig.getConfId(), jobConfig.getType(), String.valueOf(hashCode()));    }    return MessageFormat.format("{0}-{1}-{2}-{3}", jobConfig.getCrawlId(), jobConfig.getConfId(), jobConfig.getType(), String.valueOf(hashCode()));}
public void nutch_f998_1()
{    try {        getInfo().setState(State.RUNNING);        getInfo().setMsg("OK");        getInfo().setResult(tool.run(getInfo().getArgs(), getInfo().getCrawlId()));        getInfo().setState(State.FINISHED);    } catch (Exception e) {                getInfo().setMsg("ERROR: " + e.toString());        getInfo().setState(State.FAILED);    }}
public JobInfo nutch_f999_0()
{    return jobInfo;}
public boolean nutch_f1000_0()
{    getInfo().setState(State.STOPPING);    try {        return tool.stopJob();    } catch (Exception e) {        throw new RuntimeException("Cannot stop job with id " + getInfo().getId(), e);    }}
public boolean nutch_f1001_0()
{    getInfo().setState(State.KILLING);    try {        boolean result = tool.killJob();        getInfo().setState(State.KILLED);        return result;    } catch (Exception e) {        throw new RuntimeException("Cannot kill job with id " + getInfo().getId(), e);    }}
public void nutch_f1002_0(JobInfo jobInfo)
{    this.jobInfo = jobInfo;}
public List nutch_f1003_1(String path) throws FileNotFoundException
{    List<HashMap> rows = new ArrayList<>();    Path file = new Path(path);    SequenceFile.Reader reader;    try {        reader = new SequenceFile.Reader(conf, Reader.file(file));        Writable key = (Writable) ReflectionUtils.newInstance(reader.getKeyClass(), conf);        LinkDatum value = new LinkDatum();        while (reader.next(key, value)) {            try {                HashMap<String, String> t_row = getLinksRow(key, value);                rows.add(t_row);            } catch (Exception e) {            }        }        reader.close();    } catch (FileNotFoundException fne) {        throw new FileNotFoundException();    } catch (IOException e) {        e.printStackTrace();                throw new WebApplicationException();    }    return rows;}
public List nutch_f1004_1(String path, int nrows) throws FileNotFoundException
{    List<HashMap> rows = new ArrayList<>();    Path file = new Path(path);    SequenceFile.Reader reader;    try {        reader = new SequenceFile.Reader(conf, Reader.file(file));        Writable key = (Writable) ReflectionUtils.newInstance(reader.getKeyClass(), conf);        LinkDatum value = new LinkDatum();        int i = 0;        while (reader.next(key, value) && i < nrows) {            HashMap<String, String> t_row = getLinksRow(key, value);            rows.add(t_row);            i++;        }        reader.close();    } catch (FileNotFoundException fne) {        throw new FileNotFoundException();    } catch (IOException e) {        e.printStackTrace();                throw new WebApplicationException();    }    return rows;}
public List nutch_f1005_1(String path, int start, int end) throws FileNotFoundException
{    List<HashMap> rows = new ArrayList<>();    Path file = new Path(path);    SequenceFile.Reader reader;    try {        reader = new SequenceFile.Reader(conf, Reader.file(file));        Writable key = (Writable) ReflectionUtils.newInstance(reader.getKeyClass(), conf);        LinkDatum value = new LinkDatum();        int i = 0;                for (; i < start && reader.next(key, value); i++) {        }        while (reader.next(key, value) && i < end) {            HashMap<String, String> t_row = getLinksRow(key, value);            rows.add(t_row);            i++;        }        reader.close();    } catch (FileNotFoundException fne) {        throw new FileNotFoundException();    } catch (IOException e) {        e.printStackTrace();                throw new WebApplicationException();    }    return rows;}
public int nutch_f1006_1(String path) throws FileNotFoundException
{    Path file = new Path(path);    SequenceFile.Reader reader;    int i = 0;    try {        reader = new SequenceFile.Reader(conf, Reader.file(file));        Writable key = (Writable) ReflectionUtils.newInstance(reader.getKeyClass(), conf);        Writable value = (Writable) ReflectionUtils.newInstance(reader.getValueClass(), conf);        while (reader.next(key, value)) {            i++;        }        reader.close();    } catch (FileNotFoundException fne) {        throw new FileNotFoundException();    } catch (IOException e) {                throw new WebApplicationException();    }    return i;}
private HashMap<String, String> nutch_f1007_0(Writable key, LinkDatum value)
{    HashMap<String, String> tRow = new HashMap<>();    tRow.put("key_url", key.toString());    tRow.put("url", value.getUrl());    tRow.put("anchor", value.getAnchor());    tRow.put("score", String.valueOf(value.getScore()));    tRow.put("timestamp", String.valueOf(value.getTimestamp()));    tRow.put("linktype", String.valueOf(value.getLinkType()));    return tRow;}
public List nutch_f1008_1(String path) throws FileNotFoundException
{    List<HashMap> rows = new ArrayList<>();    Path file = new Path(path);    SequenceFile.Reader reader;    try {        reader = new SequenceFile.Reader(conf, Reader.file(file));        Writable key = (Writable) ReflectionUtils.newInstance(reader.getKeyClass(), conf);        Node value = new Node();        while (reader.next(key, value)) {            try {                HashMap<String, String> t_row = getNodeRow(key, value);                rows.add(t_row);            } catch (Exception e) {            }        }        reader.close();    } catch (FileNotFoundException fne) {        throw new FileNotFoundException();    } catch (IOException e) {        e.printStackTrace();                throw new WebApplicationException();    }    return rows;}
public List nutch_f1009_1(String path, int nrows) throws FileNotFoundException
{    List<HashMap> rows = new ArrayList<>();    Path file = new Path(path);    SequenceFile.Reader reader;    try {        reader = new SequenceFile.Reader(conf, Reader.file(file));        Writable key = (Writable) ReflectionUtils.newInstance(reader.getKeyClass(), conf);        Node value = new Node();        int i = 0;        while (reader.next(key, value) && i < nrows) {            HashMap<String, String> t_row = getNodeRow(key, value);            rows.add(t_row);            i++;        }        reader.close();    } catch (FileNotFoundException fne) {        throw new FileNotFoundException();    } catch (IOException e) {        e.printStackTrace();                throw new WebApplicationException();    }    return rows;}
public List nutch_f1010_1(String path, int start, int end) throws FileNotFoundException
{    List<HashMap> rows = new ArrayList<>();    Path file = new Path(path);    SequenceFile.Reader reader;    try {        reader = new SequenceFile.Reader(conf, Reader.file(file));        Writable key = (Writable) ReflectionUtils.newInstance(reader.getKeyClass(), conf);        Node value = new Node();        int i = 0;                for (; i < start && reader.next(key, value); i++) {        }        while (reader.next(key, value) && i < end) {            HashMap<String, String> t_row = getNodeRow(key, value);            rows.add(t_row);            i++;        }        reader.close();    } catch (FileNotFoundException fne) {        throw new FileNotFoundException();    } catch (IOException e) {        e.printStackTrace();                throw new WebApplicationException();    }    return rows;}
public int nutch_f1011_1(String path) throws FileNotFoundException
{    Path file = new Path(path);    SequenceFile.Reader reader;    int i = 0;    try {        reader = new SequenceFile.Reader(conf, Reader.file(file));        Writable key = (Writable) ReflectionUtils.newInstance(reader.getKeyClass(), conf);        Node value = new Node();        while (reader.next(key, value)) {            i++;        }        reader.close();    } catch (FileNotFoundException fne) {        throw new FileNotFoundException();    } catch (IOException e) {        e.printStackTrace();                throw new WebApplicationException();    }    return i;}
private HashMap<String, String> nutch_f1012_0(Writable key, Node value)
{    HashMap<String, String> tRow = new HashMap<>();    tRow.put("key_url", key.toString());    tRow.put("num_inlinks", String.valueOf(value.getNumInlinks()));    tRow.put("num_outlinks", String.valueOf(value.getNumOutlinks()));    tRow.put("inlink_score", String.valueOf(value.getInlinkScore()));    tRow.put("outlink_score", String.valueOf(value.getOutlinkScore()));    tRow.put("metadata", value.getMetadata().toString());    return tRow;}
protected void nutch_f1013_0(Thread thread, Runnable runnable)
{    super.beforeExecute(thread, runnable);    synchronized (runningWorkers) {        runningWorkers.offer(((JobWorker) runnable));    }}
protected void nutch_f1014_0(Runnable runnable, Throwable throwable)
{    super.afterExecute(runnable, throwable);    synchronized (runningWorkers) {        runningWorkers.remove((JobWorker) runnable);    }    JobWorker worker = ((JobWorker) runnable);    addStatusToHistory(worker);}
private void nutch_f1015_0(JobWorker worker)
{    synchronized (workersHistory) {        if (!workersHistory.offer(worker)) {            workersHistory.poll();            workersHistory.add(worker);        }    }}
public JobWorker nutch_f1016_0(String jobId)
{    synchronized (runningWorkers) {        for (JobWorker worker : runningWorkers) {            if (StringUtils.equals(worker.getInfo().getId(), jobId)) {                return worker;            }        }    }    return null;}
public Collection<JobInfo> nutch_f1017_0()
{    return getJobsInfo(workersHistory);}
public Collection<JobInfo> nutch_f1018_0()
{    return getJobsInfo(runningWorkers);}
public Collection<JobInfo> nutch_f1019_0()
{    return CollectionUtils.union(getJobRunning(), getJobHistory());}
private Collection<JobInfo> nutch_f1020_0(Collection<JobWorker> workers)
{    List<JobInfo> jobsInfo = Lists.newLinkedList();    for (JobWorker worker : workers) {        jobsInfo.add(worker.getInfo());    }    return jobsInfo;}
public JobInfo nutch_f1021_0(String jobId)
{    for (JobInfo jobInfo : getAllJobs()) {        if (StringUtils.equals(jobId, jobInfo.getId())) {            return jobInfo;        }    }    return null;}
public SeedList nutch_f1022_0(String seedName)
{    if (seeds.containsKey(seedName)) {        return seeds.get(seedName);    } else        return null;}
public void nutch_f1023_0(String seedName, SeedList seedList)
{    seeds.put(seedName, seedList);}
public Map<String, SeedList> nutch_f1024_0()
{    return seeds;}
public boolean nutch_f1025_0(String seedName)
{    if (seeds.containsKey(seedName)) {        seeds.remove(seedName);        return true;    } else        return false;}
public List<List<String>> nutch_f1026_1(String path) throws FileNotFoundException
{        List<List<String>> rows = new ArrayList<>();    Path file = new Path(path);    SequenceFile.Reader reader;    try {        reader = new SequenceFile.Reader(conf, Reader.file(file));        Writable key = (Writable) ReflectionUtils.newInstance(reader.getKeyClass(), conf);        Writable value = (Writable) ReflectionUtils.newInstance(reader.getValueClass(), conf);        while (reader.next(key, value)) {            List<String> row = new ArrayList<>();            row.add(key.toString());            row.add(value.toString());            rows.add(row);        }        reader.close();    } catch (FileNotFoundException fne) {        throw new FileNotFoundException();    } catch (IOException e) {                e.printStackTrace();                throw new WebApplicationException();    }    return rows;}
public List<List<String>> nutch_f1027_1(String path, int nrows) throws FileNotFoundException
{        List<List<String>> rows = new ArrayList<>();    Path file = new Path(path);    SequenceFile.Reader reader;    try {        reader = new SequenceFile.Reader(conf, Reader.file(file));        Writable key = (Writable) ReflectionUtils.newInstance(reader.getKeyClass(), conf);        Writable value = (Writable) ReflectionUtils.newInstance(reader.getValueClass(), conf);        int i = 0;        while (reader.next(key, value) && i < nrows) {            List<String> row = new ArrayList<>();            row.add(key.toString());            row.add(value.toString());            rows.add(row);            i++;        }        reader.close();    } catch (FileNotFoundException fne) {        throw new FileNotFoundException();    } catch (IOException e) {                        throw new WebApplicationException();    }    return rows;}
public List<List<String>> nutch_f1028_1(String path, int start, int end) throws FileNotFoundException
{    List<List<String>> rows = new ArrayList<>();    Path file = new Path(path);    SequenceFile.Reader reader;    try {        reader = new SequenceFile.Reader(conf, Reader.file(file));        Writable key = (Writable) ReflectionUtils.newInstance(reader.getKeyClass(), conf);        Writable value = (Writable) ReflectionUtils.newInstance(reader.getValueClass(), conf);        int i = 0;                for (; i < start && reader.next(key, value); i++) {        }        while (reader.next(key, value) && i < end) {            List<String> row = new ArrayList<>();            row.add(key.toString());            row.add(value.toString());            rows.add(row);            i++;        }        reader.close();    } catch (FileNotFoundException fne) {        throw new FileNotFoundException();    } catch (IOException e) {                        throw new WebApplicationException();    }    return rows;}
public int nutch_f1029_1(String path) throws FileNotFoundException
{    Path file = new Path(path);    SequenceFile.Reader reader;    int i = 0;    try {        reader = new SequenceFile.Reader(conf, Reader.file(file));        Writable key = (Writable) ReflectionUtils.newInstance(reader.getKeyClass(), conf);        Writable value = (Writable) ReflectionUtils.newInstance(reader.getValueClass(), conf);        while (reader.next(key, value)) {            i++;        }        reader.close();    } catch (FileNotFoundException fne) {        throw new FileNotFoundException();    } catch (IOException e) {                        throw new WebApplicationException();    }    return i;}
public void nutch_f1030_1()
{    try {        tool.run(serviceConfig.getArgs(), serviceConfig.getCrawlId());    } catch (Exception e) {                    }}
public String nutch_f1031_0()
{    return confId;}
public void nutch_f1032_0(String confId)
{    this.confId = confId;}
public Map<String, String> nutch_f1033_0()
{    return args;}
public void nutch_f1034_0(Map<String, String> args)
{    this.args = args;}
public String nutch_f1035_0()
{    return type;}
public void nutch_f1036_0(String type)
{    this.type = type;}
public String nutch_f1037_0()
{    return crawlId;}
public void nutch_f1038_0(String crawlId)
{    this.crawlId = crawlId;}
public String nutch_f1039_0()
{    return crawlId;}
public void nutch_f1040_0(String crawlId)
{    this.crawlId = crawlId;}
public JobType nutch_f1041_0()
{    return type;}
public void nutch_f1042_0(JobType type)
{    this.type = type;}
public String nutch_f1043_0()
{    return confId;}
public void nutch_f1044_0(String confId)
{    this.confId = confId;}
public Map<String, Object> nutch_f1045_0()
{    return args;}
public void nutch_f1046_0(Map<String, Object> args)
{    this.args = args;}
public String nutch_f1047_0()
{    return jobClassName;}
public void nutch_f1048_0(String jobClass)
{    this.jobClassName = jobClass;}
public Map<String, String> nutch_f1049_0()
{    return params;}
public void nutch_f1050_0(Map<String, String> params)
{    this.params = params;}
public String nutch_f1051_0()
{    return configId;}
public void nutch_f1052_0(String configId)
{    this.configId = configId;}
public boolean nutch_f1053_0()
{    return force;}
public void nutch_f1054_0(boolean force)
{    this.force = force;}
public String nutch_f1055_0()
{    return path;}
public void nutch_f1056_0(String path)
{    this.path = path;}
public Long nutch_f1057_0()
{    return id;}
public void nutch_f1058_0(Long id)
{    this.id = id;}
public Collection<SeedUrl> nutch_f1059_0()
{    return seedUrls;}
public void nutch_f1060_0(Collection<SeedUrl> seedUrls)
{    this.seedUrls = seedUrls;}
public String nutch_f1061_0()
{    return name;}
public void nutch_f1062_0(String name)
{    this.name = name;}
public String nutch_f1063_0()
{    return seedFilePath;}
public void nutch_f1064_0(String seedFilePath)
{    this.seedFilePath = seedFilePath;}
public int nutch_f1065_0()
{    if (CollectionUtils.isEmpty(seedUrls)) {        return 0;    }    return seedUrls.size();}
public int nutch_f1066_0()
{    final int prime = 31;    int result = 1;    result = prime * result + ((id == null) ? 0 : id.hashCode());    return result;}
public boolean nutch_f1067_0(Object obj)
{    if (this == obj)        return true;    if (obj == null)        return false;    if (getClass() != obj.getClass())        return false;    SeedList other = (SeedList) obj;    if (id == null) {        if (other.id != null)            return false;    } else if (!id.equals(other.id))        return false;    return true;}
public Long nutch_f1068_0()
{    return id;}
public void nutch_f1069_0(Long id)
{    this.id = id;}
public String nutch_f1070_0()
{    return url;}
public void nutch_f1071_0(String url)
{    this.url = url;}
public SeedList nutch_f1072_0()
{    return seedList;}
public void nutch_f1073_0(SeedList seedList)
{    this.seedList = seedList;}
public int nutch_f1074_0()
{    final int prime = 31;    int result = 1;    result = prime * result + ((id == null) ? 0 : id.hashCode());    return result;}
public boolean nutch_f1075_0(Object obj)
{    if (this == obj)        return true;    if (obj == null)        return false;    if (getClass() != obj.getClass())        return false;    SeedUrl other = (SeedUrl) obj;    if (id == null) {        if (other.id != null)            return false;    } else if (!id.equals(other.id))        return false;    return true;}
public String nutch_f1076_0()
{    return crawlId;}
public void nutch_f1077_0(String crawlId)
{    this.crawlId = crawlId;}
public String nutch_f1078_0()
{    return confId;}
public void nutch_f1079_0(String confId)
{    this.confId = confId;}
public Map<String, Object> nutch_f1080_0()
{    return args;}
public void nutch_f1081_0(Map<String, Object> args)
{    this.args = args;}
public String nutch_f1082_0()
{    return url;}
public void nutch_f1083_0(String url)
{    this.url = url;}
public int nutch_f1084_0()
{    return status;}
public void nutch_f1085_0(int status)
{    this.status = status;}
public int nutch_f1086_0()
{    return numOfOutlinks;}
public void nutch_f1087_0(int numOfOutlinks)
{    this.numOfOutlinks = numOfOutlinks;}
public void nutch_f1088_0(Outlink[] links)
{    ChildNode childNode;    for (Outlink outlink : links) {        childNode = new ChildNode(outlink.getToUrl(), outlink.getAnchor());        children.add(childNode);    }}
public String nutch_f1089_0()
{    return anchorText;}
public void nutch_f1090_0(String anchorText)
{    this.anchorText = anchorText;}
public String nutch_f1091_0()
{    return childUrl;}
public void nutch_f1092_0(String childUrl)
{    this.childUrl = childUrl;}
public List<ChildNode> nutch_f1093_0()
{    return children;}
public void nutch_f1094_0(List<ChildNode> children)
{    this.children = children;}
public String nutch_f1095_0()
{    return id;}
public void nutch_f1096_0(String id)
{    this.id = id;}
public JobType nutch_f1097_0()
{    return type;}
public void nutch_f1098_0(JobType type)
{    this.type = type;}
public String nutch_f1099_0()
{    return confId;}
public void nutch_f1100_0(String confId)
{    this.confId = confId;}
public Map<String, Object> nutch_f1101_0()
{    return args;}
public void nutch_f1102_0(Map<String, Object> args)
{    this.args = args;}
public Map<String, Object> nutch_f1103_0()
{    return result;}
public void nutch_f1104_0(Map<String, Object> result)
{    this.result = result;}
public State nutch_f1105_0()
{    return state;}
public void nutch_f1106_0(State state)
{    this.state = state;}
public String nutch_f1107_0()
{    return msg;}
public void nutch_f1108_0(String msg)
{    this.msg = msg;}
public String nutch_f1109_0()
{    return crawlId;}
public void nutch_f1110_0(String crawlId)
{    this.crawlId = crawlId;}
public Date nutch_f1111_0()
{    return startDate;}
public void nutch_f1112_0(Date startDate)
{    this.startDate = startDate;}
public Set<String> nutch_f1113_0()
{    return configuration;}
public void nutch_f1114_0(Set<String> configuration)
{    this.configuration = configuration;}
public Collection<JobInfo> nutch_f1115_0()
{    return jobs;}
public void nutch_f1116_0(Collection<JobInfo> jobs)
{    this.jobs = jobs;}
public Collection<JobInfo> nutch_f1117_0()
{    return runningJobs;}
public void nutch_f1118_0(Collection<JobInfo> runningJobs)
{    this.runningJobs = runningJobs;}
public List<String> nutch_f1119_0()
{    return dumpPaths;}
public void nutch_f1120_0(List<String> dumpPaths)
{    this.dumpPaths = dumpPaths;}
public static NutchServer nutch_f1121_0()
{    return server;}
protected static void nutch_f1122_0()
{    server.start();}
private void nutch_f1123_1()
{        try {        String address = "http://" + host + ":" + port;        sf.setAddress(address);        sf.create();    } catch (Exception e) {        throw new IllegalStateException("Server could not be started", e);    }    started = System.currentTimeMillis();    running = true;    }
private List<Class<?>> nutch_f1124_0()
{    List<Class<?>> resources = new ArrayList<>();    resources.add(JobResource.class);    resources.add(ConfigResource.class);    resources.add(DbResource.class);    resources.add(AdminResource.class);    resources.add(SeedResource.class);    resources.add(ReaderResouce.class);    resources.add(ServicesResource.class);    return resources;}
private List<ResourceProvider> nutch_f1125_0()
{    List<ResourceProvider> resourceProviders = new ArrayList<>();    resourceProviders.add(new SingletonResourceProvider(getConfManager()));    return resourceProviders;}
public ConfManager nutch_f1126_0()
{    return configManager;}
public JobManager nutch_f1127_0()
{    return jobManager;}
public SeedManager nutch_f1128_0()
{    return seedManager;}
public FetchNodeDb nutch_f1129_0()
{    return fetchNodeDb;}
public boolean nutch_f1130_0()
{    return running;}
public long nutch_f1131_0()
{    return started;}
public static void nutch_f1132_0(String[] args) throws ParseException
{    CommandLineParser parser = new PosixParser();    Options options = createOptions();    CommandLine commandLine = parser.parse(options, args);    if (commandLine.hasOption(CMD_HELP)) {        HelpFormatter formatter = new HelpFormatter();        formatter.printHelp("NutchServer", options, true);        return;    }    if (commandLine.hasOption(CMD_PORT)) {        port = Integer.parseInt(commandLine.getOptionValue(CMD_PORT));    }    if (commandLine.hasOption(CMD_HOST)) {        host = commandLine.getOptionValue(CMD_HOST);    }    startServer();}
private static Options nutch_f1133_0()
{    Options options = new Options();    OptionBuilder.withDescription("Show this help");    options.addOption(OptionBuilder.create(CMD_HELP));    OptionBuilder.withArgName("port");    OptionBuilder.hasOptionalArg();    OptionBuilder.withDescription("The port to run the Nutch Server. Default port 8081");    options.addOption(OptionBuilder.create(CMD_PORT));    OptionBuilder.withArgName("host");    OptionBuilder.hasOptionalArg();    OptionBuilder.withDescription("The host to bind the Nutch Server to. Default is localhost.");    options.addOption(OptionBuilder.create(CMD_HOST));    return options;}
public boolean nutch_f1134_0(boolean force)
{    if (force)        return true;    Collection<JobInfo> jobs = getJobManager().list(null, State.RUNNING);    return jobs.isEmpty();}
protected static void nutch_f1135_0(int port)
{    NutchServer.port = port;}
public int nutch_f1136_0()
{    return port;}
public void nutch_f1137_0()
{    System.exit(0);}
protected void nutch_f1138_0(String message)
{    throw new WebApplicationException(Response.status(Status.BAD_REQUEST).entity(message).build());}
public NutchServerInfo nutch_f1139_0()
{    NutchServerInfo serverInfo = new NutchServerInfo();    serverInfo.setConfiguration(configManager.list());    serverInfo.setStartDate(new Date(server.getStarted()));    serverInfo.setJobs(jobManager.list(null, State.ANY));    serverInfo.setRunningJobs(jobManager.list(null, State.RUNNING));    return serverInfo;}
public String nutch_f1140_0(@QueryParam("force") boolean force)
{    if (!server.canStop(force)) {        return "Jobs still running -- Cannot stop server now";    }    scheduleServerStop();    return "Stopping in server on port " + server.getPort();}
private void nutch_f1141_1()
{        Thread thread = new Thread() {        public void run() {            try {                Thread.sleep(DELAY_SEC * 1000);            } catch (InterruptedException e) {                Thread.currentThread().interrupt();            }            server.stop();                    }    };    thread.setDaemon(true);    thread.start();    }
public void nutch_f1142_1()
{    try {        Thread.sleep(DELAY_SEC * 1000);    } catch (InterruptedException e) {        Thread.currentThread().interrupt();    }    server.stop();    }
public Set<String> nutch_f1143_0()
{    return configManager.list();}
public Map<String, String> nutch_f1144_0(@PathParam("configId") String configId)
{    return configManager.getAsMap(configId);}
public String nutch_f1145_0(@PathParam("configId") String configId, @PathParam("propertyId") String propertyId)
{    return configManager.getAsMap(configId).get(propertyId);}
public void nutch_f1146_0(@PathParam("configId") String configId)
{    configManager.delete(configId);}
public Response nutch_f1147_0(NutchConfig newConfig)
{    if (newConfig == null) {        return Response.status(400).entity("Nutch configuration cannot be empty!").build();    }    try {        configManager.create(newConfig);    } catch (Exception e) {        return Response.status(400).entity(e.getMessage()).build();    }    return Response.ok(newConfig.getConfigId()).build();}
public Response nutch_f1148_0(@PathParam("configId") String confId, @PathParam("propertyId") String propertyKey, String value)
{    try {        configManager.setProperty(confId, propertyKey, value);    } catch (Exception e) {        return Response.status(400).entity(e.getMessage()).build();    }    return Response.ok().build();}
public Response nutch_f1149_0(DbQuery dbQuery)
{    if (dbQuery == null)        return Response.status(Status.BAD_REQUEST).build();    Configuration conf = configManager.get(dbQuery.getConfId());    if (conf == null) {        conf = configManager.get(ConfigResource.DEFAULT);    }    if (dbQuery.getCrawlId() == null || dbQuery.getType() == null) {        return Response.status(Status.BAD_REQUEST).build();    }    String type = dbQuery.getType();    if (type.equalsIgnoreCase("stats")) {        return crawlDbStats(conf, dbQuery.getArgs(), dbQuery.getCrawlId());    }    if (type.equalsIgnoreCase("dump")) {        return crawlDbDump(conf, dbQuery.getArgs(), dbQuery.getCrawlId());    }    if (type.equalsIgnoreCase("topN")) {        return crawlDbTopN(conf, dbQuery.getArgs(), dbQuery.getCrawlId());    }    if (type.equalsIgnoreCase("url")) {        return crawlDbUrl(conf, dbQuery.getArgs(), dbQuery.getCrawlId());    }    return null;}
public List<FetchNodeDbInfo> nutch_f1150_0(@DefaultValue("0") @QueryParam("to") int to, @DefaultValue("0") @QueryParam("from") int from)
{    List<FetchNodeDbInfo> listOfFetchedNodes = new ArrayList<>();    Map<Integer, FetchNode> fetchNodedbMap = FetchNodeDb.getInstance().getFetchNodeDb();    if (to == 0 || to > fetchNodedbMap.size()) {        to = fetchNodedbMap.size();    }    for (int i = from; i <= to; i++) {        if (!fetchNodedbMap.containsKey(i)) {            continue;        }        FetchNode node = fetchNodedbMap.get(i);        FetchNodeDbInfo fdbInfo = new FetchNodeDbInfo();        fdbInfo.setUrl(node.getUrl().toString());        fdbInfo.setStatus(node.getStatus());        fdbInfo.setNumOfOutlinks(node.getOutlinks().length);        fdbInfo.setChildNodes(node.getOutlinks());        listOfFetchedNodes.add(fdbInfo);    }    return listOfFetchedNodes;}
private Response nutch_f1151_0(Configuration conf, Map<String, String> args, String crawlId)
{    CrawlDbReader dbr = new CrawlDbReader();    try {        return Response.ok(dbr.query(args, conf, "stats", crawlId)).build();    } catch (Exception e) {        e.printStackTrace();        return Response.serverError().entity(e.getMessage()).type(MediaType.TEXT_PLAIN).build();    }}
private Response nutch_f1152_0(Configuration conf, Map<String, String> args, String crawlId)
{    @SuppressWarnings("resource")    CrawlDbReader dbr = new CrawlDbReader();    try {        return Response.ok(dbr.query(args, conf, "dump", crawlId), MediaType.APPLICATION_OCTET_STREAM).build();    } catch (Exception e) {        e.printStackTrace();        return Response.serverError().entity(e.getMessage()).type(MediaType.TEXT_PLAIN).build();    }}
private Response nutch_f1153_0(Configuration conf, Map<String, String> args, String crawlId)
{    @SuppressWarnings("resource")    CrawlDbReader dbr = new CrawlDbReader();    try {        return Response.ok(dbr.query(args, conf, "topN", crawlId), MediaType.APPLICATION_OCTET_STREAM).build();    } catch (Exception e) {        e.printStackTrace();        return Response.serverError().entity(e.getMessage()).type(MediaType.TEXT_PLAIN).build();    }}
private Response nutch_f1154_0(Configuration conf, Map<String, String> args, String crawlId)
{    @SuppressWarnings("resource")    CrawlDbReader dbr = new CrawlDbReader();    try {        return Response.ok(dbr.query(args, conf, "url", crawlId)).build();    } catch (Exception e) {        e.printStackTrace();        return Response.serverError().entity(e.getMessage()).type(MediaType.TEXT_PLAIN).build();    }}
public Collection<JobInfo> nutch_f1155_0(@QueryParam("crawlId") String crawlId)
{    return jobManager.list(crawlId, State.ANY);}
public JobInfo nutch_f1156_0(@PathParam("id") String id, @QueryParam("crawlId") String crawlId)
{    return jobManager.get(crawlId, id);}
public boolean nutch_f1157_0(@PathParam("id") String id, @QueryParam("crawlId") String crawlId)
{    return jobManager.stop(crawlId, id);}
public boolean nutch_f1158_0(@PathParam("id") String id, @QueryParam("crawlId") String crawlId)
{    return jobManager.abort(crawlId, id);}
public JobInfo nutch_f1159_0(JobConfig config)
{    if (config == null) {        throwBadRequestException("Job configuration is required!");    }    return jobManager.create(config);}
public Response nutch_f1160_0(ReaderConfig readerConf, @DefaultValue("-1") @QueryParam("nrows") int nrows, @DefaultValue("-1") @QueryParam("start") int start, @QueryParam("end") int end, @QueryParam("count") boolean count)
{    NutchReader reader = new SequenceReader();    String path = readerConf.getPath();    return performRead(reader, path, nrows, start, end, count);}
public Response nutch_f1161_0()
{    HashMap<String, String> schema = new HashMap<>();    schema.put("key_url", "string");    schema.put("timestamp", "int");    schema.put("score", "float");    schema.put("anchor", "string");    schema.put("linktype", "string");    schema.put("url", "string");    return Response.ok(schema).type(MediaType.APPLICATION_JSON).build();}
public Response nutch_f1162_0(ReaderConfig readerConf, @DefaultValue("-1") @QueryParam("nrows") int nrows, @DefaultValue("-1") @QueryParam("start") int start, @QueryParam("end") int end, @QueryParam("count") boolean count)
{    NutchReader reader = new LinkReader();    String path = readerConf.getPath();    return performRead(reader, path, nrows, start, end, count);}
public Response nutch_f1163_0()
{    HashMap<String, String> schema = new HashMap<>();    schema.put("key_url", "string");    schema.put("num_inlinks", "int");    schema.put("num_outlinks", "int");    schema.put("inlink_score", "float");    schema.put("outlink_score", "float");    schema.put("metadata", "string");    return Response.ok(schema).type(MediaType.APPLICATION_JSON).build();}
public Response nutch_f1164_0(ReaderConfig readerConf, @DefaultValue("-1") @QueryParam("nrows") int nrows, @DefaultValue("-1") @QueryParam("start") int start, @QueryParam("end") int end, @QueryParam("count") boolean count)
{    NutchReader reader = new NodeReader();    String path = readerConf.getPath();    return performRead(reader, path, nrows, start, end, count);}
private Response nutch_f1165_0(NutchReader reader, String path, int nrows, int start, int end, boolean count)
{    Object result;    try {        if (count) {            result = reader.count(path);            return Response.ok(result).type(MediaType.TEXT_PLAIN).build();        } else if (start > -1 && end > 0) {            result = reader.slice(path, start, end);        } else if (nrows > -1) {            result = reader.head(path, nrows);        } else {            result = reader.read(path);        }        return Response.ok(result).type(MediaType.APPLICATION_JSON).build();    } catch (Exception e) {        return Response.status(Status.BAD_REQUEST).entity("File not found").build();    }}
public Response nutch_f1166_0()
{    Map<String, SeedList> seeds = NutchServer.getInstance().getSeedManager().getSeeds();    if (seeds != null) {        return Response.ok(seeds).build();    } else {        return Response.ok().build();    }}
public Response nutch_f1167_1(SeedList seedList)
{    try {        if (seedList == null) {            return Response.status(Status.BAD_REQUEST).entity("Seed list cannot be empty!").build();        }        Collection<SeedUrl> seedUrls = seedList.getSeedUrls();        String seedFilePath = writeToSeedFile(seedUrls);        seedList.setSeedFilePath(seedFilePath);        NutchServer.getInstance().getSeedManager().setSeedList(seedList.getName(), seedList);        return Response.ok().entity(seedFilePath).build();    } catch (Exception e) {            }    return Response.serverError().build();}
private String nutch_f1168_0(Collection<SeedUrl> seedUrls) throws Exception
{    String seedFilePath = "seedFiles/seed-" + System.currentTimeMillis();    org.apache.hadoop.fs.Path seedFolder = new org.apache.hadoop.fs.Path(seedFilePath);    FileSystem fs = FileSystem.get(new Configuration());    if (!fs.exists(seedFolder)) {        if (!fs.mkdirs(seedFolder)) {            throw new Exception("Could not create seed folder at : " + seedFolder);        }    }    String filename = seedFilePath + System.getProperty("file.separator") + "urls";    org.apache.hadoop.fs.Path seedPath = new org.apache.hadoop.fs.Path(filename);    OutputStream os = fs.create(seedPath);    if (CollectionUtils.isNotEmpty(seedUrls)) {        for (SeedUrl seedUrl : seedUrls) {            os.write(seedUrl.getUrl().getBytes());            os.write("\n".getBytes());        }    }    os.close();    return seedPath.getParent().toString();}
public Response nutch_f1169_0(@PathParam("crawlId") String crawlId)
{    File dumpFilePath = new File(crawlId + File.separator + "dump/");    File[] dumpFileList = dumpFilePath.listFiles();    List<String> fileNames = new ArrayList<>();    if (dumpFileList != null) {        for (File f : dumpFileList) {            fileNames.add(f.getPath());        }    }    ServiceInfo info = new ServiceInfo();    info.setDumpPaths(fileNames);    return Response.ok().entity(info).type(MediaType.APPLICATION_JSON).build();}
public Response nutch_f1170_0(ServiceConfig serviceConfig)
{    String crawlId = serviceConfig.getCrawlId();    String outputDir = crawlId + File.separator + "dump" + File.separator + "commoncrawl-" + sdf.format(System.currentTimeMillis());    Map<String, Object> args = serviceConfig.getArgs();    args.put("outputDir", outputDir);    if (!args.containsKey(Nutch.ARG_SEGMENTDIR)) {        args.put("segment", crawlId + File.separator + "segments");    }    serviceConfig.setArgs(args);    ServiceWorker worker = new ServiceWorker(serviceConfig, new CommonCrawlDataDumper());    worker.run();    return Response.ok(outputDir).type(MediaType.TEXT_PLAIN).build();}
public String nutch_f1171_0(String url, Content content, Metadata metadata) throws IOException
{    this.url = url;    this.content = content;    this.metadata = metadata;    return this.getJsonData();}
public String nutch_f1172_0(String url, Content content, Metadata metadata, ParseData parseData) throws IOException
{        throw new NotImplementedException();}
public String nutch_f1173_1() throws IOException
{    try {        startObject(null);                writeKeyValue("url", getUrl());                writeKeyValue("timestamp", getTimestamp());                startObject("request");        writeKeyValue("method", getMethod());        startObject("client");        writeKeyValue("hostname", getRequestHostName());        writeKeyValue("address", getRequestHostAddress());        writeKeyValue("software", getRequestSoftware());        writeKeyValue("robots", getRequestRobots());        startObject("contact");        writeKeyValue("name", getRequestContactName());        writeKeyValue("email", getRequestContactEmail());        closeObject("contact");        closeObject("client");                startHeaders("headers", false, true);        writeKeyValueWrapper("Accept", getRequestAccept());        writeKeyValueWrapper("Accept-Encoding", getRequestAcceptEncoding());        writeKeyValueWrapper("Accept-Language", getRequestAcceptLanguage());        writeKeyValueWrapper("User-Agent", getRequestUserAgent());                closeHeaders("headers", false, true);        writeKeyNull("body");        closeObject("request");                startObject("response");        writeKeyValue("status", getResponseStatus());        startObject("server");        writeKeyValue("hostname", getResponseHostName());        writeKeyValue("address", getResponseAddress());        closeObject("server");                startHeaders("headers", false, true);        writeKeyValueWrapper("Content-Encoding", getResponseContentEncoding());        writeKeyValueWrapper("Content-Type", getResponseContentType());        writeKeyValueWrapper("Date", getResponseDate());        writeKeyValueWrapper("Server", getResponseServer());        for (String name : metadata.names()) {            if (name.equalsIgnoreCase("Content-Encoding") || name.equalsIgnoreCase("Content-Type") || name.equalsIgnoreCase("Date") || name.equalsIgnoreCase("Server")) {                continue;            }            writeKeyValueWrapper(name, metadata.get(name));        }        closeHeaders("headers", false, true);        writeKeyValue("body", getResponseContent());        closeObject("response");                if (!this.keyPrefix.isEmpty()) {            this.keyPrefix += "-";        }        writeKeyValue("key", this.keyPrefix + getKey());                writeKeyValue("imported", getImported());        if (getInLinks() != null) {            startArray("inlinks", false, true);            for (String link : getInLinks()) {                writeArrayValue(link);            }            closeArray("inlinks", false, true);        }        closeObject(null);        return generateJson();    } catch (IOException ioe) {                throw new IOException("Error in generating JSON:" + ioe.getMessage());    }}
protected String nutch_f1174_1()
{    try {        return URIUtil.encodePath(url);    } catch (URIException e) {            }    return url;}
protected String nutch_f1175_1()
{    if (this.simpleDateFormat) {        String timestamp = null;        try {            long epoch = new SimpleDateFormat("EEE, d MMM yyyy HH:mm:ss z").parse(ifNullString(metadata.get(Metadata.LAST_MODIFIED))).getTime();            timestamp = String.valueOf(epoch);        } catch (ParseException pe) {                    }        return timestamp;    } else {        return ifNullString(metadata.get(Metadata.LAST_MODIFIED));    }}
protected String nutch_f1176_0()
{    return new String("GET");}
protected String nutch_f1177_0()
{    String hostName = "";    try {        hostName = InetAddress.getLocalHost().getHostName();    } catch (UnknownHostException uhe) {    }    return hostName;}
protected String nutch_f1178_0()
{    String hostAddress = "";    try {        hostAddress = InetAddress.getLocalHost().getHostAddress();    } catch (UnknownHostException uhe) {    }    return hostAddress;}
protected String nutch_f1179_0()
{    return conf.get("http.agent.version", "");}
protected String nutch_f1180_0()
{    return new String("CLASSIC");}
protected String nutch_f1181_0()
{    return conf.get("http.agent.name", "");}
protected String nutch_f1182_0()
{    return conf.get("http.agent.email", "");}
protected String nutch_f1183_0()
{    return conf.get("http.accept", "");}
protected String nutch_f1184_0()
{        return new String("");}
protected String nutch_f1185_0()
{    return conf.get("http.accept.language", "");}
protected String nutch_f1186_0()
{    return conf.get("http.robots.agents", "");}
protected String nutch_f1187_0()
{    return ifNullString(metadata.get("status"));}
protected String nutch_f1188_0()
{    return URLUtil.getHost(url);}
protected String nutch_f1189_0()
{    return ifNullString(metadata.get("_ip_"));}
protected String nutch_f1190_0()
{    return ifNullString(metadata.get("Content-Encoding"));}
protected String nutch_f1191_0()
{    return ifNullString(metadata.get("Content-Type"));}
public List<String> nutch_f1192_0()
{    return inLinks;}
public void nutch_f1193_0(List<String> inLinks)
{    this.inLinks = inLinks;}
protected String nutch_f1194_1()
{    if (this.simpleDateFormat) {        String timestamp = null;        try {            long epoch = new SimpleDateFormat("EEE, dd MMM yyyy HH:mm:ss z").parse(ifNullString(metadata.get("Date"))).getTime();            timestamp = String.valueOf(epoch);        } catch (ParseException pe) {                    }        return timestamp;    } else {        return ifNullString(metadata.get("Date"));    }}
protected String nutch_f1195_0()
{    return ifNullString(metadata.get("Server"));}
protected String nutch_f1196_0()
{    return new String(content.getContent());}
protected String nutch_f1197_0()
{    if (this.reverseKey) {        return this.reverseKeyValue;    } else {        return url;    }}
protected String nutch_f1198_1()
{    if (this.simpleDateFormat) {        String timestamp = null;        try {            long epoch = new SimpleDateFormat("EEE, d MMM yyyy HH:mm:ss z").parse(ifNullString(metadata.get("Date"))).getTime();            timestamp = String.valueOf(epoch);        } catch (ParseException pe) {                    }        return timestamp;    } else {        return ifNullString(metadata.get("Date"));    }}
private static String nutch_f1199_0(String value)
{    return (value != null) ? value : "";}
private void nutch_f1200_0(String key, boolean nested, boolean newline) throws IOException
{    if (this.jsonArray) {        startArray(key, nested, newline);    } else {        startObject(key);    }}
private void nutch_f1201_0(String key, boolean nested, boolean newline) throws IOException
{    if (this.jsonArray) {        closeArray(key, nested, newline);    } else {        closeObject(key);    }}
private void nutch_f1202_0(String key, String value) throws IOException
{    if (this.jsonArray) {        startArray(null, true, false);        writeArrayValue(key);        writeArrayValue(value);        closeArray(null, true, false);    } else {        writeKeyValue(key, value);    }}
public void nutch_f1203_0()
{}
public RecordReader<Text, BytesWritable> nutch_f1204_0(InputSplit split, TaskAttemptContext context)
{    return new SequenceFileRecordReader<Text, BytesWritable>();}
public RecordReader<Text, BytesWritable> nutch_f1205_0(InputSplit split, Job job, Context context) throws IOException
{    context.setStatus(split.toString());    Configuration conf = job.getConfiguration();    return new ArcRecordReader(conf, (FileSplit) split);}
public static boolean nutch_f1206_0(byte[] input)
{        if (input == null || input.length != MAGIC.length) {        return false;    }        for (int i = 0; i < MAGIC.length; i++) {        if (MAGIC[i] != input[i]) {            return false;        }    }        return true;}
public void nutch_f1207_0() throws IOException
{    this.in.close();}
public Text nutch_f1208_0()
{    return ReflectionUtils.newInstance(Text.class, conf);}
public BytesWritable nutch_f1209_0()
{    return ReflectionUtils.newInstance(BytesWritable.class, conf);}
public long nutch_f1210_0() throws IOException
{    return in.getPos();}
public float nutch_f1211_0() throws IOException
{        if (splitEnd == splitStart) {        return 0.0f;    } else {                return Math.min(1.0f, (getPos() - splitStart) / (float) splitLen);    }}
public BytesWritable nutch_f1212_0()
{    return new BytesWritable();}
public Text nutch_f1213_0()
{    return new Text();}
public boolean nutch_f1214_0()
{    return false;}
public void nutch_f1215_0(InputSplit split, TaskAttemptContext context)
{}
public boolean nutch_f1216_1(Text key, BytesWritable value) throws IOException
{    try {                long startRead = in.getPos();        byte[] magicBuffer = null;                while (true) {                        if (startRead >= splitEnd) {                return false;            }                        boolean foundStart = false;            while (!foundStart) {                                                startRead = in.getPos();                magicBuffer = new byte[1024];                int read = in.read(magicBuffer);                if (read < 0) {                    break;                }                                for (int i = 0; i < read - 1; i++) {                    byte[] testMagic = new byte[2];                    System.arraycopy(magicBuffer, i, testMagic, 0, 2);                    if (isMagic(testMagic)) {                                                startRead += i;                        foundStart = true;                        break;                    }                }            }                        in.seek(startRead);            ByteArrayOutputStream baos = null;            int totalRead = 0;            try {                                byte[] buffer = new byte[4096];                GZIPInputStream zin = new GZIPInputStream(in);                int gzipRead = -1;                baos = new ByteArrayOutputStream();                while ((gzipRead = zin.read(buffer, 0, buffer.length)) != -1) {                    baos.write(buffer, 0, gzipRead);                    totalRead += gzipRead;                }            } catch (Exception e) {                                                                System.out.println("Ignoring position: " + (startRead));                if (startRead + 1 < fileLen) {                    in.seek(startRead + 1);                }                continue;            }                        byte[] content = baos.toByteArray();                        int eol = 0;            for (int i = 0; i < content.length; i++) {                if (i > 0 && content[i] == '\n') {                    eol = i;                    break;                }            }                        String header = new String(content, 0, eol).trim();            byte[] raw = new byte[(content.length - eol) - 1];            System.arraycopy(content, eol + 1, raw, 0, raw.length);                        Text keyText = key;            keyText.set(header);            BytesWritable valueBytes = value;            valueBytes.set(raw, 0, raw.length);                        if (startRead + 1 < fileLen) {                in.seek(startRead + 1);            }                        return true;        }    } catch (Exception e) {            }        return false;}
public static synchronized String nutch_f1217_0()
{    try {        Thread.sleep(1000);    } catch (Throwable t) {    }    return sdf.format(new Date(System.currentTimeMillis()));}
public void nutch_f1218_0()
{}
private static void nutch_f1219_1(Text url, Throwable t)
{    if (LOG.isInfoEnabled()) {            }}
private ParseStatus nutch_f1220_1(Context context, String segmentName, Text key, CrawlDatum datum, Content content, ProtocolStatus pstatus, int status) throws InterruptedException
{        datum.setStatus(status);    datum.setFetchTime(System.currentTimeMillis());    if (pstatus != null)        datum.getMetaData().put(Nutch.WRITABLE_PROTO_STATUS_KEY, pstatus);    ParseResult parseResult = null;    if (content != null) {        Metadata metadata = content.getMetadata();                metadata.set(Nutch.SEGMENT_NAME_KEY, segmentName);                try {            scfilters.passScoreBeforeParsing(key, datum, content);        } catch (Exception e) {            if (LOG.isWarnEnabled()) {                            }        }        try {                        parseResult = parseUtil.parse(content);        } catch (Exception e) {                    }                if (parseResult == null) {            byte[] signature = SignatureFactory.getSignature(conf).calculate(content, new ParseStatus().getEmptyParse(conf));            datum.setSignature(signature);        }        if (parseResult == null) {            byte[] signature = SignatureFactory.getSignature(conf).calculate(content, new ParseStatus().getEmptyParse(conf));            datum.setSignature(signature);        }        try {            context.write(key, new NutchWritable(datum));            context.write(key, new NutchWritable(content));            if (parseResult != null) {                for (Entry<Text, Parse> entry : parseResult) {                    Text url = entry.getKey();                    Parse parse = entry.getValue();                    ParseStatus parseStatus = parse.getData().getStatus();                    if (!parseStatus.isSuccess()) {                                                parse = parseStatus.getEmptyParse(conf);                    }                                        byte[] signature = SignatureFactory.getSignature(conf).calculate(content, parse);                                        parse.getData().getContentMeta().set(Nutch.SEGMENT_NAME_KEY, segmentName);                    parse.getData().getContentMeta().set(Nutch.SIGNATURE_KEY, StringUtil.toHexString(signature));                                        parse.getData().getContentMeta().set(Nutch.FETCH_TIME_KEY, Long.toString(datum.getFetchTime()));                    if (url.equals(key))                        datum.setSignature(signature);                    try {                        scfilters.passScoreAfterParsing(url, content, parse);                    } catch (Exception e) {                        if (LOG.isWarnEnabled()) {                                                    }                    }                    context.write(url, new NutchWritable(new ParseImpl(new ParseText(parse.getText()), parse.getData(), parse.isCanonical())));                }            }        } catch (IOException e) {            if (LOG.isErrorEnabled()) {                            }        }        if (parseResult != null && !parseResult.isEmpty()) {            Parse p = parseResult.get(content.getUrl());            if (p != null) {                return p.getData().getStatus();            }        }    }    return null;}
public void nutch_f1221_0(Mapper<Text, BytesWritable, Text, NutchWritable>.Context context)
{            conf = context.getConfiguration();    urlFilters = new URLFilters(conf);    scfilters = new ScoringFilters(conf);    parseUtil = new ParseUtil(conf);    normalizers = new URLNormalizers(conf, URLNormalizers.SCOPE_FETCHER);    interval = conf.getInt("db.fetch.interval.default", 2592000);}
public void nutch_f1222_1(Text key, BytesWritable bytes, Context context) throws IOException, InterruptedException
{    String[] headers = key.toString().split("\\s+");    String urlStr = headers[0];    String version = headers[2];    String contentType = headers[3];        if (urlStr.startsWith("filedesc://")) {                return;    }            Text url = new Text();    CrawlDatum datum = new CrawlDatum(CrawlDatum.STATUS_DB_FETCHED, interval, 1.0f);    String segmentName = conf.get(Nutch.SEGMENT_NAME_KEY);        try {        urlStr = normalizers.normalize(urlStr, URLNormalizers.SCOPE_FETCHER);                urlStr = urlFilters.filter(urlStr);    } catch (Exception e) {        if (LOG.isWarnEnabled()) {                    }        urlStr = null;    }        if (urlStr != null) {        url.set(urlStr);        try {                                                            ProtocolStatus status = ProtocolStatus.STATUS_SUCCESS;            Content content = new Content(urlStr, urlStr, bytes.getBytes(), contentType, new Metadata(), conf);                        content.getMetadata().set(URL_VERSION, version);            @SuppressWarnings("unused")            ParseStatus pstatus = null;            pstatus = output(context, segmentName, url, datum, content, status, CrawlDatum.STATUS_FETCH_SUCCESS);            context.progress();        } catch (Throwable t) {                        logError(url, t);            output(context, segmentName, url, datum, null, null, CrawlDatum.STATUS_FETCH_RETRY);        }    }}
public void nutch_f1223_1(Path arcFiles, Path segmentsOutDir) throws IOException, InterruptedException, ClassNotFoundException
{    SimpleDateFormat sdf = new SimpleDateFormat("yyyy-MM-dd HH:mm:ss");    long start = System.currentTimeMillis();    if (LOG.isInfoEnabled()) {                    }    Job job = NutchJob.getInstance(getConf());    Configuration conf = job.getConfiguration();    job.setJobName("ArcSegmentCreator " + arcFiles);    String segName = generateSegmentName();    conf.set(Nutch.SEGMENT_NAME_KEY, segName);    FileInputFormat.addInputPath(job, arcFiles);    job.setInputFormatClass(ArcInputFormat.class);    job.setJarByClass(ArcSegmentCreator.class);    job.setMapperClass(ArcSegmentCreator.ArcSegmentCreatorMapper.class);    FileOutputFormat.setOutputPath(job, new Path(segmentsOutDir, segName));    job.setOutputFormatClass(FetcherOutputFormat.class);    job.setOutputKeyClass(Text.class);    job.setOutputValueClass(NutchWritable.class);    try {        boolean success = job.waitForCompletion(true);        if (!success) {            String message = "ArcSegmentCreator job did not succeed, job status:" + job.getStatus().getState() + ", reason: " + job.getStatus().getFailureInfo();                        throw new RuntimeException(message);        }    } catch (IOException | InterruptedException | ClassNotFoundException e) {                throw e;    }    long end = System.currentTimeMillis();    }
public static void nutch_f1224_0(String[] args) throws Exception
{    int res = ToolRunner.run(NutchConfiguration.create(), new ArcSegmentCreator(), args);    System.exit(res);}
public int nutch_f1225_1(String[] args) throws Exception
{    String usage = "Usage: ArcSegmentCreator <arcFiles> <segmentsOutDir>";    if (args.length < 2) {        System.err.println(usage);        return -1;    }        Path arcFiles = new Path(args[0]);    Path segmentsOutDir = new Path(args[1]);    try {                createSegments(arcFiles, segmentsOutDir);        return 0;    } catch (Exception e) {                return -1;    }}
public static void nutch_f1226_0(String[] args) throws Exception
{    Configuration conf = NutchConfiguration.create();    int res = ToolRunner.run(conf, new Benchmark(), args);    System.exit(res);}
private static String nutch_f1227_0()
{    return new SimpleDateFormat("yyyyMMddHHmmss").format(new Date(System.currentTimeMillis()));}
private void nutch_f1228_0(FileSystem fs, Path seedsDir, int count) throws Exception
{    OutputStream os = fs.create(new Path(seedsDir, "seeds"));    for (int i = 0; i < count; i++) {        String url = "http://www.test-" + i + ".com/\r\n";        os.write(url.getBytes());    }    os.flush();    os.close();}
public void nutch_f1229_0(String stage, String run, long timing)
{    if (!runs.contains(run)) {        runs.add(run);    }    if (!stages.contains(stage)) {        stages.add(stage);    }    Map<String, Long> t = timings.get(stage);    if (t == null) {        t = new HashMap<>();        timings.put(stage, t);    }    t.put(run, timing);}
public String nutch_f1230_0()
{    StringBuilder sb = new StringBuilder();    sb.append("* Plugins:\t" + plugins + "\n");    sb.append("* Seeds:\t" + seeds + "\n");    sb.append("* Depth:\t" + depth + "\n");    sb.append("* Threads:\t" + threads + "\n");    sb.append("* TopN:\t" + topN + "\n");    sb.append("* Delete:\t" + delete + "\n");    sb.append("* TOTAL ELAPSED:\t" + elapsed + "\n");    for (String stage : stages) {        Map<String, Long> timing = timings.get(stage);        if (timing == null)            continue;        sb.append("- stage: " + stage + "\n");        for (String r : runs) {            Long Time = timing.get(r);            if (Time == null) {                continue;            }            sb.append("\trun " + r + "\t" + Time + "\n");        }    }    return sb.toString();}
public List<String> nutch_f1231_0()
{    return stages;}
public List<String> nutch_f1232_0()
{    return runs;}
public int nutch_f1233_1(String[] args) throws Exception
{    String plugins = "protocol-http|parse-tika|scoring-opic|urlfilter-regex|urlnormalizer-pass";    int seeds = 1;    int depth = 10;    int threads = 10;    boolean delete = true;    long topN = Long.MAX_VALUE;    if (args.length == 0) {        System.err.println("Usage: Benchmark [-seeds NN] [-depth NN] [-threads NN] [-keep] [-maxPerHost NN] [-plugins <regex>]");        System.err.println("\t-seeds NN\tcreate NN unique hosts in a seed list (default: 1)");        System.err.println("\t-depth NN\tperform NN crawl cycles (default: 10)");        System.err.println("\t-threads NN\tuse NN threads per Fetcher task (default: 10)");        System.err.println("\t-keep\tkeep segment data (default: delete after updatedb)");        System.err.println("\t-plugins <regex>\toverride 'plugin.includes'.");        System.err.println("\tNOTE: if not specified, this is reset to: " + plugins);        System.err.println("\tNOTE: if 'default' is specified then a value set in nutch-default/nutch-site is used.");        System.err.println("\t-maxPerHost NN\tmax. # of URLs per host in a fetchlist");        return -1;    }    int maxPerHost = Integer.MAX_VALUE;    for (int i = 0; i < args.length; i++) {        if (args[i].equals("-seeds")) {            seeds = Integer.parseInt(args[++i]);        } else if (args[i].equals("-threads")) {            threads = Integer.parseInt(args[++i]);        } else if (args[i].equals("-depth")) {            depth = Integer.parseInt(args[++i]);        } else if (args[i].equals("-keep")) {            delete = false;        } else if (args[i].equals("-plugins")) {            plugins = args[++i];        } else if (args[i].equalsIgnoreCase("-maxPerHost")) {            maxPerHost = Integer.parseInt(args[++i]);        } else {                        return -1;        }    }    BenchmarkResults res = benchmark(seeds, depth, threads, maxPerHost, topN, delete, plugins);    System.out.println(res);    return 0;}
public BenchmarkResults nutch_f1234_1(int seeds, int depth, int threads, int maxPerHost, long topN, boolean delete, String plugins) throws Exception
{    Configuration conf = getConf();    conf.set("http.proxy.host", "localhost");    conf.setInt("http.proxy.port", 8181);    conf.set("http.agent.name", "test");    conf.set("http.robots.agents", "test,*");    if (!plugins.equals("default")) {        conf.set("plugin.includes", plugins);    }    conf.setInt(Generator.GENERATOR_MAX_COUNT, maxPerHost);    conf.set(Generator.GENERATOR_COUNT_MODE, Generator.GENERATOR_COUNT_VALUE_HOST);    @SuppressWarnings("unused")    Job job = NutchJob.getInstance(getConf());    FileSystem fs = FileSystem.get(conf);    Path dir = new Path(getConf().get("hadoop.tmp.dir"), "bench-" + System.currentTimeMillis());    fs.mkdirs(dir);    Path rootUrlDir = new Path(dir, "seed");    fs.mkdirs(rootUrlDir);    createSeeds(fs, rootUrlDir, seeds);    if (LOG.isInfoEnabled()) {                                    }    BenchmarkResults res = new BenchmarkResults();    res.delete = delete;    res.depth = depth;    res.plugins = plugins;    res.seeds = seeds;    res.threads = threads;    res.topN = topN;    Path crawlDb = new Path(dir + "/crawldb");    Path linkDb = new Path(dir + "/linkdb");    Path segments = new Path(dir + "/segments");    res.elapsed = System.currentTimeMillis();    Injector injector = new Injector(getConf());    Generator generator = new Generator(getConf());    Fetcher fetcher = new Fetcher(getConf());    ParseSegment parseSegment = new ParseSegment(getConf());    CrawlDb crawlDbTool = new CrawlDb(getConf());    LinkDb linkDbTool = new LinkDb(getConf());        long start = System.currentTimeMillis();    injector.inject(crawlDb, rootUrlDir);    long delta = System.currentTimeMillis() - start;    res.addTiming("inject", "0", delta);    int i;    for (i = 0; i < depth; i++) {                start = System.currentTimeMillis();        Path[] segs = generator.generate(crawlDb, segments, -1, topN, System.currentTimeMillis());        delta = System.currentTimeMillis() - start;        res.addTiming("generate", i + "", delta);        if (segs == null) {                        break;        }        start = System.currentTimeMillis();                fetcher.fetch(segs[0], threads);        delta = System.currentTimeMillis() - start;        res.addTiming("fetch", i + "", delta);        if (!Fetcher.isParsing(conf)) {            start = System.currentTimeMillis();                        parseSegment.parse(segs[0]);            delta = System.currentTimeMillis() - start;            res.addTiming("parse", i + "", delta);        }        start = System.currentTimeMillis();                crawlDbTool.update(crawlDb, segs, true, true);        delta = System.currentTimeMillis() - start;        res.addTiming("update", i + "", delta);        start = System.currentTimeMillis();                linkDbTool.invert(linkDb, segs, true, true, false);        delta = System.currentTimeMillis() - start;        res.addTiming("invert", i + "", delta);                if (delete) {            for (Path p : segs) {                fs.delete(p, true);            }        }    }    if (i == 0) {            }    if (LOG.isInfoEnabled()) {            }    res.elapsed = System.currentTimeMillis() - res.elapsed;    @SuppressWarnings("resource")    CrawlDbReader dbreader = new CrawlDbReader();    dbreader.processStatJob(crawlDb.toString(), conf, false);    return res;}
private void nutch_f1235_0(InputStream stream)
{    if (stream == null) {        return;    }    Properties properties = new Properties();    try {        properties.load(stream);    } catch (IOException e) {        } finally {        try {            stream.close();        } catch (IOException e) {                }    }    setKeyPrefix(properties.getProperty("keyPrefix", ""));    setSimpleDateFormat(Boolean.parseBoolean(properties.getProperty("simpleDateFormat", "False")));    setJsonArray(Boolean.parseBoolean(properties.getProperty("jsonArray", "False")));    setReverseKey(Boolean.parseBoolean(properties.getProperty("reverseKey", "False")));}
public void nutch_f1236_0(String keyPrefix)
{    this.keyPrefix = keyPrefix;}
public void nutch_f1237_0(boolean simpleDateFormat)
{    this.simpleDateFormat = simpleDateFormat;}
public void nutch_f1238_0(boolean jsonArray)
{    this.jsonArray = jsonArray;}
public void nutch_f1239_0(boolean reverseKey)
{    this.reverseKey = reverseKey;}
public void nutch_f1240_0(String reverseKeyValue)
{    this.reverseKeyValue = reverseKeyValue;}
public String nutch_f1241_0()
{    return this.keyPrefix;}
public boolean nutch_f1242_0()
{    return this.simpleDateFormat;}
public boolean nutch_f1243_0()
{    return this.jsonArray;}
public boolean nutch_f1244_0()
{    return this.reverseKey;}
public String nutch_f1245_0()
{    return this.reverseKeyValue;}
public boolean nutch_f1246_0()
{    return compressed;}
public void nutch_f1247_0(boolean compressed)
{    this.compressed = compressed;}
public long nutch_f1248_0()
{    return warcSize;}
public void nutch_f1249_0(long warcSize)
{    this.warcSize = warcSize;}
public String nutch_f1250_0()
{    return outputDir;}
public void nutch_f1251_0(String outputDir)
{    this.outputDir = outputDir;}
public static void nutch_f1252_0(String[] args) throws Exception
{    Configuration conf = NutchConfiguration.create();    int res = ToolRunner.run(conf, new CommonCrawlDataDumper(), args);    System.exit(res);}
public void nutch_f1253_1(File outputDir, File segmentRootDir, File linkdb, boolean gzip, String[] mimeTypes, boolean epochFilename, String extension, boolean warc) throws Exception
{    if (gzip) {            }        Map<String, Integer> typeCounts = new HashMap<>();        Map<String, Integer> filteredCounts = new HashMap<>();    Configuration nutchConfig = NutchConfiguration.create();    Path segmentRootPath = new Path(segmentRootDir.toString());    FileSystem fs = segmentRootPath.getFileSystem(nutchConfig);        List<Path> parts = new ArrayList<>();    RemoteIterator<LocatedFileStatus> files = fs.listFiles(segmentRootPath, true);    String partPattern = ".*" + File.separator + Content.DIR_NAME + File.separator + "part-[0-9]{5}" + File.separator + "data";    while (files.hasNext()) {        LocatedFileStatus next = files.next();        if (next.isFile()) {            Path path = next.getPath();            if (path.toString().matches(partPattern)) {                parts.add(path);            }        }    }    LinkDbReader linkDbReader = null;    if (linkdb != null) {        linkDbReader = new LinkDbReader(nutchConfig, new Path(linkdb.toString()));    }    if (parts == null || parts.size() == 0) {                System.exit(1);    }        if (gzip && !warc) {        fileList = new ArrayList<>();        constructNewStream(outputDir);    }    for (Path segmentPart : parts) {                try {            SequenceFile.Reader reader = new SequenceFile.Reader(nutchConfig, SequenceFile.Reader.file(segmentPart));            Writable key = (Writable) reader.getKeyClass().getConstructor().newInstance();            Content content = null;            while (reader.next(key)) {                content = new Content();                reader.getCurrentValue(content);                Metadata metadata = content.getMetadata();                String url = key.toString();                String baseName = FilenameUtils.getBaseName(url);                String extensionName = FilenameUtils.getExtension(url);                if (!extension.isEmpty()) {                    extensionName = extension;                } else if ((extensionName == null) || extensionName.isEmpty()) {                    extensionName = "html";                }                String outputFullPath = null;                String outputRelativePath = null;                String filename = null;                String timestamp = null;                String reverseKey = null;                if (epochFilename || config.getReverseKey()) {                    try {                        long epoch = new SimpleDateFormat("EEE, d MMM yyyy HH:mm:ss z").parse(getDate(metadata.get("Date"))).getTime();                        timestamp = String.valueOf(epoch);                    } catch (ParseException pe) {                                            }                    reverseKey = reverseUrl(url);                    config.setReverseKeyValue(reverseKey.replace("/", "_") + "_" + DigestUtils.sha1Hex(url) + "_" + timestamp);                }                if (!warc) {                    if (epochFilename) {                        outputFullPath = DumpFileUtil.createFileNameFromUrl(outputDir.getAbsolutePath(), reverseKey, url, timestamp, extensionName, !gzip);                        outputRelativePath = outputFullPath.substring(0, outputFullPath.lastIndexOf(File.separator) - 1);                        filename = content.getMetadata().get(Metadata.DATE) + "." + extensionName;                    } else {                        String md5Ofurl = DumpFileUtil.getUrlMD5(url);                        String fullDir = DumpFileUtil.createTwoLevelsDirectory(outputDir.getAbsolutePath(), md5Ofurl, !gzip);                        filename = DumpFileUtil.createFileName(md5Ofurl, baseName, extensionName);                        outputFullPath = String.format("%s/%s", fullDir, filename);                        String[] fullPathLevels = fullDir.split(Pattern.quote(File.separator));                        String firstLevelDirName = fullPathLevels[fullPathLevels.length - 2];                        String secondLevelDirName = fullPathLevels[fullPathLevels.length - 1];                        outputRelativePath = firstLevelDirName + secondLevelDirName;                    }                }                                Boolean filter = (mimeTypes == null);                String jsonData = "";                try {                    String mimeType = new Tika().detect(content.getContent());                                                            Set<String> inUrls = null;                    if (linkDbReader != null) {                        Inlinks inlinks = linkDbReader.getInlinks((Text) key);                        if (inlinks != null) {                            Iterator<Inlink> iterator = inlinks.iterator();                            inUrls = new LinkedHashSet<>();                            while (inUrls.size() <= MAX_INLINKS && iterator.hasNext()) {                                inUrls.add(iterator.next().getFromUrl());                            }                        }                    }                                        try (CommonCrawlFormat format = CommonCrawlFormatFactory.getCommonCrawlFormat(warc ? "WARC" : "JACKSON", nutchConfig, config)) {                        if (inUrls != null) {                            format.setInLinks(new ArrayList<>(inUrls));                        }                        jsonData = format.getJsonData(url, content, metadata);                    }                    collectStats(typeCounts, mimeType);                                        if ((mimeType != null) && (mimeTypes != null) && Arrays.asList(mimeTypes).contains(mimeType)) {                        collectStats(filteredCounts, mimeType);                        filter = true;                    }                } catch (IOException ioe) {                                        return;                }                if (!warc) {                    if (filter) {                        byte[] byteData = serializeCBORData(jsonData);                        if (!gzip) {                            File outputFile = new File(outputFullPath);                            if (outputFile.exists()) {                                                            } else {                                                                IOUtils.copy(new ByteArrayInputStream(byteData), new FileOutputStream(outputFile));                            }                        } else {                            if (fileList.contains(outputFullPath)) {                                                            } else {                                fileList.add(outputFullPath);                                                                                                TarArchiveEntry tarEntry = new TarArchiveEntry(outputRelativePath + File.separator + filename);                                tarEntry.setSize(byteData.length);                                tarOutput.putArchiveEntry(tarEntry);                                tarOutput.write(byteData);                                tarOutput.closeArchiveEntry();                            }                        }                    }                }            }            reader.close();        } catch (Exception e) {                    } finally {            fs.close();        }    }    if (gzip && !warc) {        closeStream();    }    if (!typeCounts.isEmpty()) {            }}
private void nutch_f1254_1()
{    try {        tarOutput.finish();        tarOutput.close();        gzipOutput.close();        bufOutput.close();        fileOutput.close();    } catch (IOException ioe) {            }}
private void nutch_f1255_1(File outputDir) throws IOException
{    String archiveName = new SimpleDateFormat("yyyyMMddhhmm'.tar.gz'").format(new Date());        fileOutput = new FileOutputStream(new File(outputDir + File.separator + archiveName));    bufOutput = new BufferedOutputStream(fileOutput);    gzipOutput = new GzipCompressorOutputStream(bufOutput);    tarOutput = new TarArchiveOutputStream(gzipOutput);    tarOutput.setLongFileMode(TarArchiveOutputStream.LONGFILE_GNU);}
private void nutch_f1256_0(CBORGenerator generator) throws IOException
{                byte[] header = new byte[3];    header[0] = (byte) 0xd9;    header[1] = (byte) 0xd9;    header[2] = (byte) 0xf7;    generator.writeBytes(header, 0, header.length);}
private byte[] nutch_f1257_1(String jsonData)
{    CBORFactory factory = new CBORFactory();    CBORGenerator generator = null;    ByteArrayOutputStream stream = null;    try {        stream = new ByteArrayOutputStream();        generator = factory.createGenerator(stream);                writeMagicHeader(generator);        generator.writeString(jsonData);        generator.flush();        stream.flush();        return stream.toByteArray();    } catch (Exception e) {            } finally {        try {            generator.close();            stream.close();        } catch (IOException e) {                }    }    return null;}
private void nutch_f1258_0(Map<String, Integer> typeCounts, String mimeType)
{    typeCounts.put(mimeType, typeCounts.containsKey(mimeType) ? typeCounts.get(mimeType) + 1 : 1);}
private String nutch_f1259_0(String timestamp)
{    if (timestamp == null || timestamp.isEmpty()) {        DateFormat dateFormat = new SimpleDateFormat("EEE, d MMM yyyy HH:mm:ss z");        timestamp = dateFormat.format(new Date());    }    return timestamp;}
public static String nutch_f1260_1(String urlString)
{    URL url;    String reverseKey = null;    try {        url = new URL(urlString);        String[] hostPart = url.getHost().replace('.', '/').split("/");        StringBuilder sb = new StringBuilder();        sb.append(hostPart[hostPart.length - 1]);        for (int i = hostPart.length - 2; i >= 0; i--) {            sb.append("/" + hostPart[i]);        }        reverseKey = sb.toString();    } catch (MalformedURLException e) {            }    return reverseKey;}
public int nutch_f1261_1(String[] args) throws Exception
{    Option helpOpt = new Option("h", "help", false, "show this help message.");        @SuppressWarnings("static-access")    Option outputOpt = OptionBuilder.withArgName("outputDir").hasArg().withDescription("output directory (which will be created) to host the CBOR data.").create("outputDir");        Option warcOpt = new Option("warc", "export to a WARC file");    @SuppressWarnings("static-access")    Option segOpt = OptionBuilder.withArgName("segment").hasArgs().withDescription("the segment or directory containing segments to use").create("segment");        @SuppressWarnings("static-access")    Option mimeOpt = OptionBuilder.isRequired(false).withArgName("mimetype").hasArgs().withDescription("an optional list of mimetypes to dump, excluding all others. Defaults to all.").create("mimetype");    @SuppressWarnings("static-access")    Option gzipOpt = OptionBuilder.withArgName("gzip").hasArg(false).withDescription("an optional flag indicating whether to additionally gzip the data.").create("gzip");    @SuppressWarnings("static-access")    Option keyPrefixOpt = OptionBuilder.withArgName("keyPrefix").hasArg(true).withDescription("an optional prefix for key in the output format.").create("keyPrefix");    @SuppressWarnings("static-access")    Option simpleDateFormatOpt = OptionBuilder.withArgName("SimpleDateFormat").hasArg(false).withDescription("an optional format for timestamp in GMT epoch milliseconds.").create("SimpleDateFormat");    @SuppressWarnings("static-access")    Option epochFilenameOpt = OptionBuilder.withArgName("epochFilename").hasArg(false).withDescription("an optional format for output filename.").create("epochFilename");    @SuppressWarnings("static-access")    Option jsonArrayOpt = OptionBuilder.withArgName("jsonArray").hasArg(false).withDescription("an optional format for JSON output.").create("jsonArray");    @SuppressWarnings("static-access")    Option reverseKeyOpt = OptionBuilder.withArgName("reverseKey").hasArg(false).withDescription("an optional format for key value in JSON output.").create("reverseKey");    @SuppressWarnings("static-access")    Option extensionOpt = OptionBuilder.withArgName("extension").hasArg(true).withDescription("an optional file extension for output documents.").create("extension");    @SuppressWarnings("static-access")    Option sizeOpt = OptionBuilder.withArgName("warcSize").hasArg(true).withType(Number.class).withDescription("an optional file size in bytes for the WARC file(s)").create("warcSize");    @SuppressWarnings("static-access")    Option linkDbOpt = OptionBuilder.withArgName("linkdb").hasArg(true).withDescription("an optional linkdb parameter to include inlinks in dump files").isRequired(false).create("linkdb");        Options options = new Options();    options.addOption(helpOpt);    options.addOption(outputOpt);    options.addOption(segOpt);        options.addOption(warcOpt);    options.addOption(mimeOpt);    options.addOption(gzipOpt);        options.addOption(keyPrefixOpt);        options.addOption(simpleDateFormatOpt);    options.addOption(epochFilenameOpt);    options.addOption(jsonArrayOpt);    options.addOption(reverseKeyOpt);    options.addOption(extensionOpt);    options.addOption(sizeOpt);    options.addOption(linkDbOpt);    CommandLineParser parser = new GnuParser();    try {        CommandLine line = parser.parse(options, args);        if (line.hasOption("help") || !line.hasOption("outputDir") || (!line.hasOption("segment"))) {            HelpFormatter formatter = new HelpFormatter();            formatter.printHelp(CommonCrawlDataDumper.class.getName(), options, true);            return 0;        }        File outputDir = new File(line.getOptionValue("outputDir"));        File segmentRootDir = new File(line.getOptionValue("segment"));        String[] mimeTypes = line.getOptionValues("mimetype");        boolean gzip = line.hasOption("gzip");        boolean epochFilename = line.hasOption("epochFilename");        String keyPrefix = line.getOptionValue("keyPrefix", "");        boolean simpleDateFormat = line.hasOption("SimpleDateFormat");        boolean jsonArray = line.hasOption("jsonArray");        boolean reverseKey = line.hasOption("reverseKey");        String extension = line.getOptionValue("extension", "");        boolean warc = line.hasOption("warc");        long warcSize = 0;        if (line.getParsedOptionValue("warcSize") != null) {            warcSize = (Long) line.getParsedOptionValue("warcSize");        }        String linkdbPath = line.getOptionValue("linkdb");        File linkdb = linkdbPath == null ? null : new File(linkdbPath);        CommonCrawlConfig config = new CommonCrawlConfig();        config.setKeyPrefix(keyPrefix);        config.setSimpleDateFormat(simpleDateFormat);        config.setJsonArray(jsonArray);        config.setReverseKey(reverseKey);        config.setCompressed(gzip);        config.setWarcSize(warcSize);        config.setOutputDir(line.getOptionValue("outputDir"));        if (!outputDir.exists()) {                        if (!outputDir.mkdirs())                throw new Exception("Unable to create: [" + outputDir.getAbsolutePath() + "]");        }        CommonCrawlDataDumper dumper = new CommonCrawlDataDumper(config);        dumper.dump(outputDir, segmentRootDir, linkdb, gzip, mimeTypes, epochFilename, extension, warc);    } catch (Exception e) {                e.printStackTrace();        return -1;    }    return 0;}
public Map<String, Object> nutch_f1262_0(Map<String, Object> args, String crawlId) throws Exception
{    String keyPrefix = args.containsKey("keyPrefix") ? (String) args.get("keyPrefix") : "";    File outputDir = new File((String) args.get("outputDir"));    File segmentRootDir = new File((String) args.get(Nutch.ARG_SEGMENTDIR));    ArrayList<String> mimeTypesList = args.containsKey("mimetypes") ? (ArrayList<String>) args.get("mimetypes") : null;    String[] mimeTypes = null;    if (mimeTypesList != null) {        mimeTypes = new String[mimeTypesList.size()];        int i = 0;        for (String m : mimeTypesList) mimeTypes[i++] = m;    }    boolean gzip = args.containsKey("gzip") ? (boolean) args.get("gzip") : false;    boolean epochFilename = args.containsKey("epochFilename") ? (boolean) args.get("epochFilename") : false;    boolean simpleDateFormat = args.containsKey("simpleDateFormat") ? (boolean) args.get("simpleDateFormat") : false;    boolean jsonArray = args.containsKey("jsonArray") ? (boolean) args.get("jsonArray") : false;    boolean reverseKey = args.containsKey("reverseKey") ? (boolean) args.get("reverseKey") : false;    String extension = args.containsKey("extension") ? (String) args.get("extension") : "";    boolean warc = args.containsKey("warc") ? (boolean) args.get("warc") : false;    long warcSize = args.containsKey("warcSize") ? (Long) args.get("warcSize") : 0;    CommonCrawlConfig config = new CommonCrawlConfig();    config.setKeyPrefix(keyPrefix);    config.setSimpleDateFormat(simpleDateFormat);    config.setJsonArray(jsonArray);    config.setReverseKey(reverseKey);    config.setCompressed(gzip);    config.setWarcSize(warcSize);    config.setOutputDir((String) args.get("outputDir"));    if (!outputDir.exists()) {        if (!outputDir.mkdirs())            throw new Exception("Unable to create: [" + outputDir.getAbsolutePath() + "]");    }    CommonCrawlDataDumper dumper = new CommonCrawlDataDumper(config);    dumper.dump(outputDir, segmentRootDir, null, gzip, mimeTypes, epochFilename, extension, warc);    return null;}
public static CommonCrawlFormat nutch_f1263_0(String formatType, String url, Content content, Metadata metadata, Configuration nutchConf, CommonCrawlConfig config) throws IOException
{    if (formatType == null) {        return null;    }    if (formatType.equalsIgnoreCase("jackson")) {        return new CommonCrawlFormatJackson(url, content, metadata, nutchConf, config);    } else if (formatType.equalsIgnoreCase("jettinson")) {        return new CommonCrawlFormatJettinson(url, content, metadata, nutchConf, config);    } else if (formatType.equalsIgnoreCase("simple")) {        return new CommonCrawlFormatSimple(url, content, metadata, nutchConf, config);    }    return null;}
public static CommonCrawlFormat nutch_f1264_0(String formatType, Configuration nutchConf, CommonCrawlConfig config) throws IOException
{    if (formatType.equalsIgnoreCase("WARC")) {        return new CommonCrawlFormatWARC(nutchConf, config);    }    if (formatType.equalsIgnoreCase("JACKSON")) {        return new CommonCrawlFormatJackson(nutchConf, config);    }    return null;}
protected void nutch_f1265_0(String key, String value) throws IOException
{    generator.writeFieldName(key);    generator.writeString(value);}
protected void nutch_f1266_0(String key) throws IOException
{    generator.writeFieldName(key);    generator.writeNull();}
protected void nutch_f1267_0(String key, boolean nested, boolean newline) throws IOException
{    if (key != null) {        generator.writeFieldName(key);    }    generator.writeStartArray();}
protected void nutch_f1268_0(String key, boolean nested, boolean newline) throws IOException
{    generator.writeEndArray();}
protected void nutch_f1269_0(String value) throws IOException
{    generator.writeString(value);}
protected void nutch_f1270_0(String key) throws IOException
{    if (key != null) {        generator.writeFieldName(key);    }    generator.writeStartObject();}
protected void nutch_f1271_0(String key) throws IOException
{    generator.writeEndObject();}
protected String nutch_f1272_0() throws IOException
{    this.generator.flush();    return this.out.toString();}
protected void nutch_f1273_0(String key, String value) throws IOException
{    try {        stackObjects.getFirst().put(key, value);    } catch (JSONException jsone) {        throw new IOException(jsone.getMessage());    }}
protected void nutch_f1274_0(String key) throws IOException
{    try {        stackObjects.getFirst().put(key, JSONObject.NULL);    } catch (JSONException jsone) {        throw new IOException(jsone.getMessage());    }}
protected void nutch_f1275_0(String key, boolean nested, boolean newline) throws IOException
{    JSONArray array = new JSONArray();    stackArrays.push(array);}
protected void nutch_f1276_0(String key, boolean nested, boolean newline) throws IOException
{    try {        if (stackArrays.size() > 1) {            JSONArray array = stackArrays.pop();            if (nested) {                stackArrays.getFirst().put(array);            } else {                stackObjects.getFirst().put(key, array);            }        }    } catch (JSONException jsone) {        throw new IOException(jsone.getMessage());    }}
protected void nutch_f1277_0(String value) throws IOException
{    if (stackArrays.size() > 1) {        stackArrays.getFirst().put(value);    }}
protected void nutch_f1278_0(String key) throws IOException
{    JSONObject object = new JSONObject();    stackObjects.push(object);}
protected void nutch_f1279_0(String key) throws IOException
{    try {        if (stackObjects.size() > 1) {            JSONObject object = stackObjects.pop();            stackObjects.getFirst().put(key, object);        }    } catch (JSONException jsone) {        throw new IOException(jsone.getMessage());    }}
protected String nutch_f1280_0() throws IOException
{    try {        return stackObjects.getFirst().toString(2);    } catch (JSONException jsone) {        throw new IOException(jsone.getMessage());    }}
protected void nutch_f1281_0(String key, String value) throws IOException
{    sb.append(printTabs() + "\"" + key + "\": " + quote(value) + ",\n");}
protected void nutch_f1282_0(String key) throws IOException
{    sb.append(printTabs() + "\"" + key + "\": null,\n");}
protected void nutch_f1283_0(String key, boolean nested, boolean newline) throws IOException
{    String name = (key != null) ? "\"" + key + "\": " : "";    String nl = (newline) ? "\n" : "";    sb.append(printTabs() + name + "[" + nl);    if (newline) {        this.tabCount++;    }}
protected void nutch_f1284_0(String key, boolean nested, boolean newline) throws IOException
{    if (sb.charAt(sb.length() - 1) == ',') {                sb.deleteCharAt(sb.length() - 1);    } else if (sb.charAt(sb.length() - 2) == ',') {                sb.deleteCharAt(sb.length() - 2);    }    String nl = (newline) ? printTabs() : "";    if (newline) {        this.tabCount++;    }    sb.append(nl + "],\n");}
protected void nutch_f1285_0(String value)
{    sb.append("\"" + value + "\",");}
protected void nutch_f1286_0(String key) throws IOException
{    String name = "";    if (key != null) {        name = "\"" + key + "\": ";    }    sb.append(printTabs() + name + "{\n");    this.tabCount++;}
protected void nutch_f1287_0(String key) throws IOException
{    if (sb.charAt(sb.length() - 2) == ',') {                sb.deleteCharAt(sb.length() - 2);    }    this.tabCount--;    sb.append(printTabs() + "},\n");}
protected String nutch_f1288_0() throws IOException
{        sb.deleteCharAt(sb.length() - 1);        sb.deleteCharAt(sb.length() - 1);    return sb.toString();}
private String nutch_f1289_0()
{    StringBuilder sb = new StringBuilder();    for (int i = 0; i < this.tabCount; i++) {        sb.append("\t");    }    return sb.toString();}
private static String nutch_f1290_0(String string) throws IOException
{    StringBuilder sb = new StringBuilder();    if (string == null || string.length() == 0) {        sb.append("\"\"");        return sb.toString();    }    char b;    char c = 0;    String hhhh;    int i;    int len = string.length();    sb.append('"');    for (i = 0; i < len; i += 1) {        b = c;        c = string.charAt(i);        switch(c) {            case '\\':            case '"':                sb.append('\\');                sb.append(c);                break;            case '/':                if (b == '<') {                    sb.append('\\');                }                sb.append(c);                break;            case '\b':                sb.append("\\b");                break;            case '\t':                sb.append("\\t");                break;            case '\n':                sb.append("\\n");                break;            case '\f':                sb.append("\\f");                break;            case '\r':                sb.append("\\r");                break;            default:                if (c < ' ' || (c >= '\u0080' && c < '\u00a0') || (c >= '\u2000' && c < '\u2100')) {                    sb.append("\\u");                    hhhh = Integer.toHexString(c);                    sb.append("0000", 0, 4 - hhhh.length());                    sb.append(hhhh);                } else {                    sb.append(c);                }        }    }    sb.append('"');    return sb.toString();}
public String nutch_f1291_0(String url, Content content, Metadata metadata, ParseData parseData) throws IOException
{    this.url = url;    this.content = content;    this.metadata = metadata;    this.parseData = parseData;    return this.getJsonData();}
public String nutch_f1292_1() throws IOException
{    long position = writer.getPosition();    try {                        writer.checkSize();        if (writer.getPosition() != position) {                        position = writer.getPosition();        }                URI id = writeResponse();        if (StringUtils.isNotBlank(metadata.get("_request_"))) {                        writeRequest(id);        }    } catch (IOException e) {                throw e;    } catch (ParseException e) {                            }    return null;}
protected URI nutch_f1293_0() throws IOException, ParseException
{    WARCRecordInfo record = new WARCRecordInfo();    record.setType(WARCConstants.WARCRecordType.response);    record.setUrl(getUrl());    record.setCreate14DigitDate(DateUtils.getLog14Date(Long.parseLong(metadata.get("nutch.fetch.time"))));    record.setMimetype(WARCConstants.HTTP_RESPONSE_MIMETYPE);    record.setRecordId(GENERATOR.getRecordID());    String IP = getResponseAddress();    if (StringUtils.isNotBlank(IP))        record.addExtraHeader(WARCConstants.HEADER_KEY_IP, IP);    if (ParseSegment.isTruncated(content))        record.addExtraHeader(WARCConstants.HEADER_KEY_TRUNCATED, "unspecified");    ByteArrayOutputStream output = new ByteArrayOutputStream();    String httpHeaders = metadata.get("_response.headers_");    httpHeaders = WARCUtils.fixHttpHeaders(httpHeaders, content.getContent().length);    if (StringUtils.isNotBlank(httpHeaders)) {        output.write(httpHeaders.getBytes());    } else {                        record.setType(WARCConstants.WARCRecordType.resource);        record.setMimetype(content.getContentType());    }    output.write(getResponseContent().getBytes());    record.setContentLength(output.size());    record.setContentStream(new ByteArrayInputStream(output.toByteArray()));    if (output.size() > 0) {                        writer.writeRecord(record);    }    return record.getRecordId();}
protected URI nutch_f1294_0(URI id) throws IOException, ParseException
{    WARCRecordInfo record = new WARCRecordInfo();    record.setType(WARCConstants.WARCRecordType.request);    record.setUrl(getUrl());    record.setCreate14DigitDate(DateUtils.getLog14Date(Long.parseLong(metadata.get("nutch.fetch.time"))));    record.setMimetype(WARCConstants.HTTP_REQUEST_MIMETYPE);    record.setRecordId(GENERATOR.getRecordID());    if (id != null) {        ANVLRecord headers = new ANVLRecord();        headers.addLabelValue(WARCConstants.HEADER_KEY_CONCURRENT_TO, '<' + id.toString() + '>');        record.setExtraHeaders(headers);    }    ByteArrayOutputStream output = new ByteArrayOutputStream();    output.write(metadata.get("_request_").getBytes());    record.setContentLength(output.size());    record.setContentStream(new ByteArrayInputStream(output.toByteArray()));    writer.writeRecord(record);    return record.getRecordId();}
protected String nutch_f1295_0() throws IOException
{    return null;}
protected void nutch_f1296_0(String key, String value) throws IOException
{    throw new NotImplementedException();}
protected void nutch_f1297_0(String key) throws IOException
{    throw new NotImplementedException();}
protected void nutch_f1298_0(String key, boolean nested, boolean newline) throws IOException
{    throw new NotImplementedException();}
protected void nutch_f1299_0(String key, boolean nested, boolean newline) throws IOException
{    throw new NotImplementedException();}
protected void nutch_f1300_0(String value) throws IOException
{    throw new NotImplementedException();}
protected void nutch_f1301_0(String key) throws IOException
{    throw new NotImplementedException();}
protected void nutch_f1302_0(String key) throws IOException
{    throw new NotImplementedException();}
public void nutch_f1303_0()
{    if (writer != null)        try {            writer.close();        } catch (IOException e) {            throw new RuntimeException(e);        }}
public int nutch_f1304_0() throws IOException
{    int c = in.read();    int value = c;    if (    c != -1 && !(XMLChar.isValid(c)))        value = 'X';    else if (lastBad && c == '<') {                in.mark(1);        if (in.read() != '/')            value = 'X';        in.reset();    }    lastBad = (c == 65533);    return value;}
public int nutch_f1305_0(char[] cbuf, int off, int len) throws IOException
{    int n = in.read(cbuf, off, len);    if (n != -1) {        for (int i = 0; i < n; i++) {            char c = cbuf[off + i];            char value = c;            if (            !(XMLChar.isValid(c)))                value = 'X';            else if (lastBad && c == '<') {                                if (i != n - 1 && cbuf[off + i + 1] != '/')                    value = 'X';            }            lastBad = (c == 65533);            cbuf[off + i] = value;        }    }    return n;}
public void nutch_f1306_0(String namespaceURI, String localName, String qName, Attributes atts) throws SAXException
{    if ("Topic".equals(qName)) {        curSection = atts.getValue("r:id");    } else if ("ExternalPage".equals(qName)) {                if ((!includeAdult) && curSection.startsWith("Top/Adult")) {            return;        }        if (topicPattern != null && !topicPattern.matcher(curSection).matches()) {            return;        }                        String url = atts.getValue("about");        int hashValue = MD5Hash.digest(url).hashCode();        hashValue = Math.abs(hashValue ^ hashSkew);        if ((hashValue % subsetDenom) != 0) {            return;        }                curURL = url;    } else if (curURL != null && "d:Title".equals(qName)) {        titlePending = true;    } else if (curURL != null && "d:Description".equals(qName)) {        descPending = true;    }}
public void nutch_f1307_0(char[] ch, int start, int length)
{    if (titlePending) {        title.append(ch, start, length);    } else if (descPending) {        desc.append(ch, start, length);    }}
public void nutch_f1308_0(String namespaceURI, String localName, String qName) throws SAXException
{    if (curURL != null) {        if ("ExternalPage".equals(qName)) {                                                            System.out.println(curURL);            pages++;                        if (title.length() > 0) {                title.delete(0, title.length());            }            if (desc.length() > 0) {                desc.delete(0, desc.length());            }                        curURL = null;        } else if ("d:Title".equals(qName)) {            titlePending = false;        } else if ("d:Description".equals(qName)) {            descPending = false;        }    }}
public void nutch_f1309_1()
{    }
public void nutch_f1310_1()
{    }
public void nutch_f1311_0(Locator locator)
{    location = locator;}
public void nutch_f1312_1(SAXParseException spe)
{    if (LOG.isErrorEnabled()) {            }}
public void nutch_f1313_1(SAXParseException spe)
{    if (LOG.isWarnEnabled()) {            }}
public void nutch_f1314_1(File dmozFile, int subsetDenom, boolean includeAdult, int skew, Pattern topicPattern) throws IOException, SAXException, ParserConfigurationException
{    SAXParserFactory parserFactory = SAXParserFactory.newInstance();    SAXParser parser = parserFactory.newSAXParser();    XMLReader reader = parser.getXMLReader();        RDFProcessor rp = new RDFProcessor(reader, subsetDenom, includeAdult, skew, topicPattern);    reader.setContentHandler(rp);    reader.setErrorHandler(rp);            try (XMLCharFilter in = new XMLCharFilter(new BufferedReader(new InputStreamReader(new BufferedInputStream(new FileInputStream(dmozFile)), "UTF-8")))) {        InputSource is = new InputSource(in);        reader.parse(is);    } catch (Exception e) {        if (LOG.isErrorEnabled()) {                    }        System.exit(0);    }}
private static void nutch_f1315_1(String topicFile, Vector<String> topics) throws IOException
{    try (BufferedReader in = new BufferedReader(new InputStreamReader(new FileInputStream(topicFile), "UTF-8"))) {        String line = null;        while ((line = in.readLine()) != null) {            topics.addElement(line);        }    } catch (Exception e) {        if (LOG.isErrorEnabled()) {                    }        System.exit(0);    }}
public static void nutch_f1316_1(String[] argv) throws Exception
{    if (argv.length < 1) {        System.err.println("Usage: DmozParser <dmoz_file> [-subset <subsetDenominator>] [-includeAdultMaterial] [-skew skew] [-topicFile <topic list file>] [-topic <topic> [-topic <topic> [...]]]");        return;    }                    int subsetDenom = 1;    int skew = 0;    String dmozFile = argv[0];    boolean includeAdult = false;    Pattern topicPattern = null;    Vector<String> topics = new Vector<>();    Configuration conf = NutchConfiguration.create();    try (FileSystem fs = FileSystem.get(conf)) {        for (int i = 1; i < argv.length; i++) {            if ("-includeAdultMaterial".equals(argv[i])) {                includeAdult = true;            } else if ("-subset".equals(argv[i])) {                subsetDenom = Integer.parseInt(argv[i + 1]);                i++;            } else if ("-topic".equals(argv[i])) {                topics.addElement(argv[i + 1]);                i++;            } else if ("-topicFile".equals(argv[i])) {                addTopicsFromFile(argv[i + 1], topics);                i++;            } else if ("-skew".equals(argv[i])) {                skew = Integer.parseInt(argv[i + 1]);                i++;            }        }        DmozParser parser = new DmozParser();        if (!topics.isEmpty()) {            String regExp = "^(";            int j = 0;            for (; j < topics.size() - 1; ++j) {                regExp = regExp.concat(topics.get(j));                regExp = regExp.concat("|");            }            regExp = regExp.concat(topics.get(j));            regExp = regExp.concat(").*");                        topicPattern = Pattern.compile(regExp);        }        parser.parseDmozFile(new File(dmozFile), subsetDenom, includeAdult, skew, topicPattern);    }}
public static void nutch_f1318_1(String[] args) throws Exception
{        Option helpOpt = new Option("h", "help", false, "show this help message");        @SuppressWarnings("static-access")    Option outputOpt = OptionBuilder.withArgName("outputDir").hasArg().withDescription("output directory (which will be created) to host the raw data").create("outputDir");    @SuppressWarnings("static-access")    Option segOpt = OptionBuilder.withArgName("segment").hasArgs().withDescription("the segment(s) to use").create("segment");    @SuppressWarnings("static-access")    Option mimeOpt = OptionBuilder.withArgName("mimetype").hasArgs().withDescription("an optional list of mimetypes to dump, excluding all others. Defaults to all.").create("mimetype");    @SuppressWarnings("static-access")    Option mimeStat = OptionBuilder.withArgName("mimeStats").withDescription("only display mimetype stats for the segment(s) instead of dumping file.").create("mimeStats");    @SuppressWarnings("static-access")    Option dirStructureOpt = OptionBuilder.withArgName("flatdir").withDescription("optionally specify that the output directory should only contain files.").create("flatdir");    @SuppressWarnings("static-access")    Option reverseURLOutput = OptionBuilder.withArgName("reverseUrlDirs").withDescription("optionally specify to use reverse URL folders for output structure.").create("reverseUrlDirs");        Options options = new Options();    options.addOption(helpOpt);    options.addOption(outputOpt);    options.addOption(segOpt);    options.addOption(mimeOpt);    options.addOption(mimeStat);    options.addOption(dirStructureOpt);    options.addOption(reverseURLOutput);    CommandLineParser parser = new GnuParser();    try {        CommandLine line = parser.parse(options, args);        if (line.hasOption("help") || !line.hasOption("outputDir") || (!line.hasOption("segment"))) {            HelpFormatter formatter = new HelpFormatter();            formatter.printHelp("FileDumper", options, true);            return;        }        File outputDir = new File(line.getOptionValue("outputDir"));        File segmentRootDir = new File(line.getOptionValue("segment"));        String[] mimeTypes = line.getOptionValues("mimetype");        boolean flatDir = line.hasOption("flatdir");        boolean shouldDisplayStats = false;        if (line.hasOption("mimeStats"))            shouldDisplayStats = true;        boolean reverseURLDump = false;        if (line.hasOption("reverseUrlDirs"))            reverseURLDump = true;        if (!outputDir.exists()) {                        if (!shouldDisplayStats) {                if (!outputDir.mkdirs())                    throw new Exception("Unable to create: [" + outputDir.getAbsolutePath() + "]");            }        }        FileDumper dumper = new FileDumper();        dumper.dump(outputDir, segmentRootDir, mimeTypes, flatDir, shouldDisplayStats, reverseURLDump);    } catch (Exception e) {                e.printStackTrace();        return;    }}
private void nutch_f1319_0(Map<String, Integer> typeCounts, String mimeType)
{    typeCounts.put(mimeType, typeCounts.containsKey(mimeType) ? typeCounts.get(mimeType) + 1 : 1);}
public void nutch_f1320_0(Mapper<WritableComparable<?>, Text, Text, Generator.SelectorEntry>.Context context)
{    Configuration conf = context.getConfiguration();    defaultInterval = conf.getInt("db.fetch.interval.default", 0);    scfilters = new ScoringFilters(conf);    if (conf.getBoolean(FILTER_KEY, false)) {        filters = new URLFilters(conf);    }    if (conf.getBoolean(NORMALIZE_KEY, false)) {        normalizers = new URLNormalizers(conf, URLNormalizers.SCOPE_INJECT);    }}
public void nutch_f1321_1(WritableComparable<?> key, Text value, Context context) throws IOException, InterruptedException
{        String urlString = value.toString();    try {        if (normalizers != null) {            urlString = normalizers.normalize(urlString, URLNormalizers.SCOPE_INJECT);        }        if (urlString != null && filters != null) {            urlString = filters.filter(urlString);        }        if (urlString != null) {            url.set(urlString);            scfilters.injectedScore(url, datum);        }    } catch (Exception e) {                return;    }    if (urlString == null) {        if (LOG.isDebugEnabled()) {                    }        return;    }    entry.datum = datum;    entry.url = url;        entry.datum.setFetchInterval(defaultInterval);    context.write(url, entry);}
public void nutch_f1322_0(Text key, Iterable<Generator.SelectorEntry> values, Context context) throws IOException, InterruptedException
{            HashMap<Text, CrawlDatum> unique = new HashMap<>();    for (Generator.SelectorEntry entry : values) {        unique.put(entry.url, entry.datum);    }        for (Entry<Text, CrawlDatum> e : unique.entrySet()) {        context.write(e.getKey(), e.getValue());    }}
public int nutch_f1323_1(String[] args) throws Exception
{    if (args.length < 2) {        System.err.println("Usage: FreeGenerator <inputDir> <segmentsDir> [-filter] [-normalize]");        System.err.println("\tinputDir\tinput directory containing one or more input files.");        System.err.println("\t\tEach text file contains a list of URLs, one URL per line");        System.err.println("\tsegmentsDir\toutput directory, where new segment will be created");        System.err.println("\t-filter\trun current URLFilters on input URLs");        System.err.println("\t-normalize\trun current URLNormalizers on input URLs");        return -1;    }    boolean filter = false;    boolean normalize = false;    if (args.length > 2) {        for (int i = 2; i < args.length; i++) {            if (args[i].equals("-filter")) {                filter = true;            } else if (args[i].equals("-normalize")) {                normalize = true;            } else {                                return -1;            }        }    }    SimpleDateFormat sdf = new SimpleDateFormat("yyyy-MM-dd HH:mm:ss");    long start = System.currentTimeMillis();        Job job = NutchJob.getInstance(getConf());    Configuration conf = job.getConfiguration();    conf.setBoolean(FILTER_KEY, filter);    conf.setBoolean(NORMALIZE_KEY, normalize);    FileInputFormat.addInputPath(job, new Path(args[0]));    job.setInputFormatClass(TextInputFormat.class);    job.setJarByClass(FG.class);    job.setMapperClass(FG.FGMapper.class);    job.setMapOutputKeyClass(Text.class);    job.setMapOutputValueClass(Generator.SelectorEntry.class);    job.setPartitionerClass(URLPartitioner.class);    job.setReducerClass(FG.FGReducer.class);    String segName = Generator.generateSegmentName();    job.setNumReduceTasks(Integer.parseInt(conf.get("mapreduce.job.maps")));    job.setOutputFormatClass(SequenceFileOutputFormat.class);    job.setOutputKeyClass(Text.class);    job.setOutputValueClass(CrawlDatum.class);    job.setSortComparatorClass(Generator.HashComparator.class);    FileOutputFormat.setOutputPath(job, new Path(args[1], new Path(segName, CrawlDatum.GENERATE_DIR_NAME)));    try {        boolean success = job.waitForCompletion(true);        if (!success) {            String message = "FreeGenerator job did not succeed, job status:" + job.getStatus().getState() + ", reason: " + job.getStatus().getFailureInfo();                        throw new RuntimeException(message);        }    } catch (IOException | InterruptedException | ClassNotFoundException e) {                return -1;    }    long end = System.currentTimeMillis();        return 0;}
public static void nutch_f1324_0(String[] args) throws Exception
{    int res = ToolRunner.run(NutchConfiguration.create(), new FreeGenerator(), args);    System.exit(res);}
public void nutch_f1325_1()
{    numTotal.incrementAndGet();    String host = URLUtil.getHost(url);    long start = System.currentTimeMillis();    try {                        InetAddress.getByName(host);                numResolved.incrementAndGet();    } catch (Exception uhe) {                numErrored.incrementAndGet();    }    long end = System.currentTimeMillis();    long total = (end - start);    totalTime.addAndGet(total);    }
public void nutch_f1326_1()
{    try {                pool = Executors.newFixedThreadPool(numThreads);                BufferedReader buffRead = new BufferedReader(new FileReader(new File(urlsFile)));        String urlStr = null;        while ((urlStr = buffRead.readLine()) != null) {                                    pool.execute(new ResolverThread(urlStr));        }                        buffRead.close();        pool.awaitTermination(60, TimeUnit.SECONDS);    } catch (Exception e) {                pool.shutdownNow();            }        pool.shutdown();    }
public static void nutch_f1327_1(String[] args)
{    Options options = new Options();    OptionBuilder.withArgName("help");    OptionBuilder.withDescription("show this help message");    Option helpOpts = OptionBuilder.create("help");    options.addOption(helpOpts);    OptionBuilder.withArgName("urls");    OptionBuilder.hasArg();    OptionBuilder.withDescription("the urls file to check");    Option urlOpts = OptionBuilder.create("urls");    options.addOption(urlOpts);    OptionBuilder.withArgName("numThreads");    OptionBuilder.hasArgs();    OptionBuilder.withDescription("the number of threads to use");    Option numThreadOpts = OptionBuilder.create("numThreads");    options.addOption(numThreadOpts);    CommandLineParser parser = new GnuParser();    try {                CommandLine line = parser.parse(options, args);        if (line.hasOption("help") || !line.hasOption("urls")) {            HelpFormatter formatter = new HelpFormatter();            formatter.printHelp("ResolveUrls", options);            return;        }                String urls = line.getOptionValue("urls");        int numThreads = 100;        String numThreadsStr = line.getOptionValue("numThreads");        if (numThreadsStr != null) {            numThreads = Integer.parseInt(numThreadsStr);        }        ResolveUrls resolve = new ResolveUrls(urls, numThreads);        resolve.resolveUrls();    } catch (Exception e) {            }}
public void nutch_f1328_0() throws IOException
{}
public void nutch_f1329_0(Mapper<Text, Writable, Text, NutchWritable>.Context context)
{}
public void nutch_f1330_0(Text key, Writable value, Context context) throws IOException, InterruptedException
{    context.write(key, new NutchWritable(value));}
public void nutch_f1331_0(Reducer<Text, NutchWritable, NullWritable, WARCWritable>.Context context)
{}
public void nutch_f1332_1(Text key, Iterable<NutchWritable> values, Context context) throws IOException, InterruptedException
{    Content content = null;    CrawlDatum cd = null;    SimpleDateFormat warcdf = new SimpleDateFormat("yyyy-MM-dd'T'HH:mm:ss'Z'", Locale.ENGLISH);        for (NutchWritable val : values) {                final Writable value = val.get();        if (value instanceof Content) {            content = (Content) value;            continue;        }        if (value instanceof CrawlDatum) {            cd = (CrawlDatum) value;            continue;        }    }        if (content == null) {                context.getCounter("WARCExporter", "missing content").increment(1);        return;    }    if (cd == null) {                context.getCounter("WARCExporter", "missing metadata").increment(1);        return;    }        String headersVerbatim = content.getMetadata().get("_response.headers_");    headersVerbatim = WARCUtils.fixHttpHeaders(headersVerbatim, content.getContent().length);    byte[] httpheaders = new byte[0];    if (StringUtils.isNotBlank(headersVerbatim)) {                if (!headersVerbatim.endsWith(CRLF + CRLF)) {            headersVerbatim += CRLF + CRLF;        }        httpheaders = headersVerbatim.getBytes();    }    StringBuilder buffer = new StringBuilder();    buffer.append(WARCRecord.WARC_VERSION);    buffer.append(CRLF);    buffer.append("WARC-Record-ID").append(": ").append("<urn:uuid:").append(UUID.randomUUID().toString()).append(">").append(CRLF);    int contentLength = 0;    if (content != null) {        contentLength = content.getContent().length;    }        contentLength += httpheaders.length;    buffer.append("Content-Length").append(": ").append(Integer.toString(contentLength)).append(CRLF);    Date fetchedDate = new Date(cd.getFetchTime());    buffer.append("WARC-Date").append(": ").append(warcdf.format(fetchedDate)).append(CRLF);            String WARCTypeValue = "resource";    if (StringUtils.isNotBlank(headersVerbatim)) {        WARCTypeValue = "response";    }    buffer.append("WARC-Type").append(": ").append(WARCTypeValue).append(CRLF);        String IP = content.getMetadata().get("_ip_");    if (StringUtils.isNotBlank(IP)) {        buffer.append("WARC-IP-Address").append(": ").append("IP").append(CRLF);    }        String status = CrawlDatum.getStatusName(cd.getStatus());    if (status.equalsIgnoreCase("STATUS_FETCH_SUCCESS") && ParseSegment.isTruncated(content)) {        buffer.append("WARC-Truncated").append(": ").append("unspecified").append(CRLF);    }        try {        String normalised = key.toString().replaceAll(" ", "%20");        URI uri = URI.create(normalised);        buffer.append("WARC-Target-URI").append(": ").append(uri.toASCIIString()).append(CRLF);    } catch (Exception e) {                context.getCounter("WARCExporter", "invalid URI").increment(1);        return;    }        if (WARCTypeValue.equals("response")) {        buffer.append("Content-Type: application/http; msgtype=response").append(CRLF);    }        ByteArrayOutputStream bos = new ByteArrayOutputStream();        bos.write(buffer.toString().getBytes("UTF-8"));    bos.write(CRLF_BYTES);        bos.write(httpheaders);        if (content.getContent() != null) {        bos.write(content.getContent());    }    bos.write(CRLF_BYTES);    bos.write(CRLF_BYTES);    try {        DataInput in = new DataInputStream(new ByteArrayInputStream(bos.toByteArray()));        WARCRecord record = new WARCRecord(in);        context.write(NullWritable.get(), new WARCWritable(record));        context.getCounter("WARCExporter", "records generated").increment(1);    } catch (IOException | IllegalStateException exception) {                context.getCounter("WARCExporter", "exception").increment(1);    }}
public int nutch_f1333_1(String output, List<Path> segments) throws IOException
{    SimpleDateFormat sdf = new SimpleDateFormat("yyyy-MM-dd HH:mm:ss");    long start = System.currentTimeMillis();        final Job job = NutchJob.getInstance(getConf());    job.setJobName("warc-exporter " + output);    for (final Path segment : segments) {                FileInputFormat.addInputPath(job, new Path(segment, Content.DIR_NAME));        FileInputFormat.addInputPath(job, new Path(segment, CrawlDatum.FETCH_DIR_NAME));    }    job.setInputFormatClass(SequenceFileInputFormat.class);    job.setJarByClass(WARCMapReduce.class);    job.setMapperClass(WARCMapReduce.WARCMapper.class);    job.setReducerClass(WARCMapReduce.WARCReducer.class);    job.setMapOutputKeyClass(Text.class);    job.setMapOutputValueClass(NutchWritable.class);    FileOutputFormat.setOutputPath(job, new Path(output));        job.setOutputFormatClass(WARCOutputFormat.class);    job.setOutputKeyClass(NullWritable.class);    job.setOutputValueClass(WARCWritable.class);    try {        boolean success = job.waitForCompletion(true);        if (!success) {            String message = "WARCExporter job did not succeed, job status:" + job.getStatus().getState() + ", reason: " + job.getStatus().getFailureInfo();                        throw new RuntimeException(message);        }                long end = System.currentTimeMillis();            } catch (IOException | InterruptedException | ClassNotFoundException e) {                return -1;    }    return 0;}
public int nutch_f1334_0(String[] args) throws Exception
{    if (args.length < 2) {        System.err.println("Usage: WARCExporter <output> (<segment> ... | -dir <segments>)");        return -1;    }    final List<Path> segments = new ArrayList<>();    for (int i = 1; i < args.length; i++) {        if (args[i].equals("-dir")) {            Path dir = new Path(args[++i]);            FileSystem fs = dir.getFileSystem(getConf());            FileStatus[] fstats = fs.listStatus(dir, HadoopFSUtil.getPassDirectoriesFilter(fs));            Path[] files = HadoopFSUtil.getPaths(fstats);            for (Path p : files) {                segments.add(p);            }        } else {            segments.add(new Path(args[i]));        }    }    return generateWARC(args[0], segments);}
public static void nutch_f1335_0(String[] args) throws Exception
{    final int res = ToolRunner.run(NutchConfiguration.create(), new WARCExporter(), args);    System.exit(res);}
public static final ANVLRecord nutch_f1336_0(Configuration conf)
{    ANVLRecord record = new ANVLRecord();        record.addLabelValue(FORMAT, "WARC File Format 1.0");    record.addLabelValue(CONFORMS_TO, "http://bibnum.bnf.fr/WARC/WARC_ISO_28500_version1_latestdraft.pdf");    record.addLabelValue(SOFTWARE, conf.get("http.agent.name", ""));    record.addLabelValue(HTTP_HEADER_USER_AGENT, getAgentString(conf.get("http.agent.name", ""), conf.get("http.agent.version", ""), conf.get("http.agent.description", ""), conf.get("http.agent.url", ""), conf.get("http.agent.email", "")));    record.addLabelValue(HTTP_HEADER_FROM, conf.get("http.agent.email", ""));    try {        record.addLabelValue(HOSTNAME, getHostname(conf));        record.addLabelValue(IP, getIPAddress(conf));    } catch (UnknownHostException ignored) {        }        record.addLabelValue(ROBOTS, "classic");    record.addLabelValue(OPERATOR, conf.get("http.agent.email", ""));    return record;}
public static final String nutch_f1337_0(Configuration conf) throws UnknownHostException
{    return StringUtil.isEmpty(conf.get("http.agent.host", "")) ? InetAddress.getLocalHost().getHostName() : conf.get("http.agent.host");}
public static final String nutch_f1338_0(Configuration conf) throws UnknownHostException
{    return InetAddress.getLocalHost().getHostAddress();}
public static final byte[] nutch_f1339_0(HttpHeaders headers) throws IOException
{    ByteArrayOutputStream out = new ByteArrayOutputStream();    headers.write(out);    return out.toByteArray();}
public static final String nutch_f1340_0(String name, String version, String description, String URL, String email)
{    StringBuffer buf = new StringBuffer();    buf.append(name);    if (version != null) {        buf.append("/").append(version);    }    if (((description != null) && (description.length() != 0)) || ((email != null) && (email.length() != 0)) || ((URL != null) && (URL.length() != 0))) {        buf.append(" (");        if ((description != null) && (description.length() != 0)) {            buf.append(description);            if ((URL != null) || (email != null))                buf.append("; ");        }        if ((URL != null) && (URL.length() != 0)) {            buf.append(URL);            if (email != null)                buf.append("; ");        }        if ((email != null) && (email.length() != 0))            buf.append(email);        buf.append(")");    }    return buf.toString();}
public static final WARCRecordInfo nutch_f1341_0(NutchDocument doc) throws UnsupportedEncodingException
{    WARCRecordInfo record = new WARCRecordInfo();    record.setType(WARCConstants.WARCRecordType.metadata);    record.setUrl((String) doc.getFieldValue("id"));    record.setCreate14DigitDate(DateUtils.get14DigitDate((Date) doc.getFieldValue("tstamp")));    record.setMimetype("application/warc-fields");    record.setRecordId(generator.getRecordID());        ANVLRecord metadata = new ANVLRecord();    for (String field : doc.getFieldNames()) {        List<Object> values = doc.getField(field).getValues();        for (Object value : values) {            if (value instanceof Date) {                metadata.addLabelValue(field, DateUtils.get14DigitDate());            } else {                metadata.addLabelValue(field, (String) value);            }        }    }    record.setContentLength(metadata.getLength());    record.setContentStream(new ByteArrayInputStream(metadata.getUTF8Bytes()));    return record;}
public static final String nutch_f1342_0(String headers, int contentLength)
{    if (headers == null) {        return null;    }    int start = 0, lineEnd = 0, last = 0, trailingCrLf = 0;    StringBuilder replace = new StringBuilder();    while (start < headers.length()) {        lineEnd = headers.indexOf(CRLF, start);        trailingCrLf = 1;        if (lineEnd == -1) {            lineEnd = headers.length();            trailingCrLf = 0;        }        int colonPos = -1;        for (int i = start; i < lineEnd; i++) {            if (headers.charAt(i) == ':') {                colonPos = i;                break;            }        }        if (colonPos == -1) {            boolean valid = true;            if (start == 0) {                                    } else if ((lineEnd + 4) == headers.length() && headers.endsWith(CRLF + CRLF)) {                                trailingCrLf = 2;            } else {                valid = false;            }            if (!valid) {                if (last < start) {                    replace.append(headers.substring(last, start));                }                last = lineEnd + 2 * trailingCrLf;            }            start = lineEnd + 2 * trailingCrLf;            /*         * skip over invalid header line, no further check for problematic         * headers required         */            continue;        }        String name = headers.substring(start, colonPos);        if (PROBLEMATIC_HEADERS.matcher(name).matches()) {            boolean needsFix = true;            if (name.equalsIgnoreCase("content-length")) {                String value = headers.substring(colonPos + 1, lineEnd).trim();                try {                    int l = Integer.parseInt(value);                    if (l == contentLength) {                        needsFix = false;                    }                } catch (NumberFormatException e) {                                }            }            if (needsFix) {                if (last < start) {                    replace.append(headers.substring(last, start));                }                last = lineEnd + 2 * trailingCrLf;                replace.append(X_HIDE_HEADER).append(headers.substring(start, lineEnd + 2 * trailingCrLf));                if (trailingCrLf == 0) {                    replace.append(CRLF);                    trailingCrLf = 1;                }                if (name.equalsIgnoreCase("content-length")) {                                        replace.append("Content-Length").append(COLONSP).append(contentLength).append(CRLF);                }            }        }        start = lineEnd + 2 * trailingCrLf;    }    if (last > 0 || trailingCrLf != 2) {        if (last < headers.length()) {                        replace.append(headers.substring(last));        }        while (trailingCrLf < 2) {            replace.append(CRLF);            trailingCrLf++;        }        return replace.toString();    }    return headers;}
protected int nutch_f1343_0(String[] args, int i)
{    if (args[i].equals("-listen")) {        tcpPort = Integer.parseInt(args[++i]);        return 2;    } else if (args[i].equals("-keepClientCnxOpen")) {        keepClientCnxOpen = true;        return 1;    } else if (args[i].equals("-stdin")) {        stdin = true;        return 1;    }    return 0;}
protected int nutch_f1344_0() throws Exception
{        if (tcpPort != -1) {        processTCP(tcpPort);        return 0;    } else if (stdin) {        return processStdin();    }        return -1;}
protected int nutch_f1345_0(String input) throws Exception
{    StringBuilder output = new StringBuilder();    int ret = process(input, output);    System.out.println(output);    return ret;}
protected int nutch_f1346_0() throws Exception
{    BufferedReader in = new BufferedReader(new InputStreamReader(System.in));    String line;    while ((line = in.readLine()) != null) {        StringBuilder output = new StringBuilder();        @SuppressWarnings("unused")        int ret = process(line, output);        System.out.println(output);    }    return 0;}
protected void nutch_f1347_1(int tcpPort) throws Exception
{    ServerSocket server = null;    try {        server = new ServerSocket();        server.bind(new InetSocketAddress(tcpPort));            } catch (Exception e) {                System.exit(-1);    }    while (true) {        Worker worker;        try {            worker = new Worker(server.accept());            Thread thread = new Thread(worker);            thread.start();        } catch (Exception e) {                        System.exit(-1);        }    }}
public void nutch_f1348_1()
{        BufferedReader in = null;    OutputStream out = null;    try {        in = new BufferedReader(new InputStreamReader(client.getInputStream()));        out = client.getOutputStream();    } catch (IOException e) {                return;    }        if (keepClientCnxOpen) {        try {                        while (readWrite(in, out)) {            }        } catch (Exception e) {                    }    } else {        try {            readWrite(in, out);        } catch (Exception e) {                    }    }    try {                client.close();    } catch (Exception e) {            }}
protected boolean nutch_f1349_0(BufferedReader in, OutputStream out) throws Exception
{    String line = in.readLine();    if (line == null) {                return false;    }    if (line.trim().length() > 1) {                StringBuilder output = new StringBuilder();        process(line, output);        output.append("\n");        out.write(output.toString().getBytes(StandardCharsets.UTF_8));    }    return true;}
protected ProtocolOutput nutch_f1350_0(String url, CrawlDatum datum) throws Exception
{    ProtocolFactory factory = new ProtocolFactory(getConf());    Protocol protocol = factory.getProtocol(url);    Text turl = new Text(url);    return protocol.getProtocolOutput(turl, datum);}
public int nutch_f1351_0()
{    return _xit;}
public void nutch_f1352_0(String s)
{    _command = s;}
public String nutch_f1353_0()
{    return _command;}
public void nutch_f1354_0(InputStream is)
{    _stdin = is;}
public void nutch_f1355_0(OutputStream os)
{    _stdout = os;}
public void nutch_f1356_0(OutputStream os)
{    _stderr = os;}
public void nutch_f1357_0() throws IOException
{    this.exec();}
public int nutch_f1358_0() throws IOException
{    Process proc = Runtime.getRuntime().exec(_command);    _barrier = new CyclicBarrier(3 + ((_stdin != null) ? 1 : 0));    PullerThread so = new PullerThread("STDOUT", proc.getInputStream(), _stdout);    so.setDaemon(true);    so.start();    PullerThread se = new PullerThread("STDERR", proc.getErrorStream(), _stderr);    se.setDaemon(true);    se.start();    PusherThread si = null;    if (_stdin != null) {        si = new PusherThread("STDIN", _stdin, proc.getOutputStream());        si.setDaemon(true);        si.start();    }    boolean _timedout = false;    long end = System.currentTimeMillis() + _timeout * 1000;    try {        if (_timeout == 0) {            _barrier.await();        } else {            _barrier.await(_timeout, TimeUnit.SECONDS);        }    } catch (TimeoutException ex) {        _timedout = true;    } catch (BrokenBarrierException bbe) {    /* IGNORE */    } catch (InterruptedException e) {    /* IGNORE */    }        if (si != null) {        si.interrupt();    }    so.interrupt();    se.interrupt();    _xit = -1;    if (!_timedout) {        if (_waitForExit) {            do {                try {                    Thread.sleep(1000);                    _xit = proc.exitValue();                } catch (InterruptedException ie) {                    if (Thread.interrupted()) {                                                break;                    } else {                        continue;                    }                } catch (IllegalThreadStateException iltse) {                    continue;                }                break;            } while (!(_timedout = (System.currentTimeMillis() > end)));        } else {            try {                _xit = proc.exitValue();            } catch (IllegalThreadStateException iltse) {                _timedout = true;            }        }    }    if (_waitForExit) {        proc.destroy();    }    return _xit;}
public Throwable nutch_f1359_0()
{    return _thrownError;}
public void nutch_f1360_0()
{    try {        byte[] buf = new byte[BUF];        int read = 0;        while (!isInterrupted() && (read = _is.read(buf)) != -1) {            if (read == 0)                continue;            _os.write(buf, 0, read);            _os.flush();        }    } catch (InterruptedIOException iioe) {        } catch (Throwable t) {        _thrownError = t;    } finally {        try {            if (_closeInput) {                _is.close();            } else {                _os.close();            }        } catch (IOException ioe) {        /* IGNORE */        }    }    try {        _barrier.await();    } catch (InterruptedException ie) {    /* IGNORE */    } catch (BrokenBarrierException bbe) {    /* IGNORE */    }}
public int nutch_f1361_0()
{    return _timeout;}
public void nutch_f1362_0(int timeout)
{    _timeout = timeout;}
public boolean nutch_f1363_0()
{    return _waitForExit;}
public void nutch_f1364_0(boolean waitForExit)
{    _waitForExit = waitForExit;}
public static void nutch_f1365_0(String[] args) throws Exception
{    String commandPath = null;    String filePath = null;    int timeout = 10;    String usage = "Usage: CommandRunner [-timeout timeoutSecs] commandPath filePath";    if (args.length < 2) {        System.err.println(usage);        System.exit(-1);    }    for (int i = 0; i < args.length; i++) {        if (args[i].equals("-timeout")) {            timeout = Integer.parseInt(args[++i]);        } else if (i != args.length - 2) {            System.err.println(usage);            System.exit(-1);        } else {            commandPath = args[i];            filePath = args[++i];        }    }    CommandRunner cr = new CommandRunner();    cr.setCommand(commandPath);    cr.setInputStream(new java.io.FileInputStream(filePath));    cr.setStdErrorStream(System.err);    cr.setStdOutputStream(System.out);    cr.setTimeout(timeout);    cr.evaluate();    System.err.println("output value: " + cr.getExitValue());}
public int nutch_f1366_1(String[] args) throws Exception
{    Option helpOpt = new Option("h", "help", false, "Show this message");    @SuppressWarnings("static-access")    Option inDirs = OptionBuilder.withArgName("inputDirs").isRequired().withDescription("Comma separated list of crawl directories (e.g., \"./crawl1,./crawl2\")").hasArgs().create("inputDirs");    @SuppressWarnings("static-access")    Option outDir = OptionBuilder.withArgName("outputDir").isRequired().withDescription("Output directory where results should be dumped").hasArgs().create("outputDir");    @SuppressWarnings("static-access")    Option modeOpt = OptionBuilder.withArgName("mode").isRequired().withDescription("Set statistics gathering mode (by 'host' or by 'domain')").hasArgs().create("mode");    @SuppressWarnings("static-access")    Option numReducers = OptionBuilder.withArgName("numReducers").withDescription("Optional number of reduce jobs to use. Defaults to 1").hasArgs().create("numReducers");    Options options = new Options();    options.addOption(helpOpt);    options.addOption(inDirs);    options.addOption(outDir);    options.addOption(modeOpt);    options.addOption(numReducers);    CommandLineParser parser = new GnuParser();    CommandLine cli;    try {        cli = parser.parse(options, args);    } catch (MissingOptionException e) {        HelpFormatter formatter = new HelpFormatter();        formatter.printHelp("CrawlCompletionStats", options, true);        return 1;    }    if (cli.hasOption("help")) {        HelpFormatter formatter = new HelpFormatter();        formatter.printHelp("CrawlCompletionStats", options, true);        return 1;    }    String inputDir = cli.getOptionValue("inputDirs");    String outputDir = cli.getOptionValue("outputDir");    int numOfReducers = 1;    if (cli.hasOption("numReducers")) {        numOfReducers = Integer.parseInt(args[3]);    }    SimpleDateFormat sdf = new SimpleDateFormat("yyyy-MM-dd HH:mm:ss");    long start = System.currentTimeMillis();        int mode = 0;    String jobName = "CrawlCompletionStats";    if (cli.getOptionValue("mode").equals("host")) {        jobName = "Host CrawlCompletionStats";        mode = MODE_HOST;    } else if (cli.getOptionValue("mode").equals("domain")) {        jobName = "Domain CrawlCompletionStats";        mode = MODE_DOMAIN;    }    Configuration conf = getConf();    conf.setInt("domain.statistics.mode", mode);    conf.setBoolean("mapreduce.fileoutputcommitter.marksuccessfuljobs", false);    Job job = Job.getInstance(conf, jobName);    job.setJarByClass(CrawlCompletionStats.class);    String[] inputDirsSpecs = inputDir.split(",");    for (int i = 0; i < inputDirsSpecs.length; i++) {        File completeInputPath = new File(new File(inputDirsSpecs[i]), "crawldb/current");        FileInputFormat.addInputPath(job, new Path(completeInputPath.toString()));    }    job.setInputFormatClass(SequenceFileInputFormat.class);    FileOutputFormat.setOutputPath(job, new Path(outputDir));    job.setOutputFormatClass(TextOutputFormat.class);    job.setMapOutputKeyClass(Text.class);    job.setMapOutputValueClass(LongWritable.class);    job.setOutputKeyClass(Text.class);    job.setOutputValueClass(LongWritable.class);    job.setMapperClass(CrawlCompletionStatsMapper.class);    job.setReducerClass(CrawlCompletionStatsReducer.class);    job.setCombinerClass(CrawlCompletionStatsCombiner.class);    job.setNumReduceTasks(numOfReducers);    try {        boolean success = job.waitForCompletion(true);        if (!success) {            String message = jobName + " job did not succeed, job status: " + job.getStatus().getState() + ", reason: " + job.getStatus().getFailureInfo();                                    throw new RuntimeException(message);        }    } catch (IOException | InterruptedException | ClassNotFoundException e) {                throw e;    }    long end = System.currentTimeMillis();        return 0;}
public void nutch_f1367_0(Context context)
{    mode = context.getConfiguration().getInt("domain.statistics.mode", MODE_DOMAIN);}
public void nutch_f1368_0(Text urlText, CrawlDatum datum, Context context) throws IOException, InterruptedException
{    URL url = new URL(urlText.toString());    String out = "";    switch(mode) {        case MODE_HOST:            out = url.getHost();            break;        case MODE_DOMAIN:            out = URLUtil.getDomainName(url);            break;    }    if (datum.getStatus() == CrawlDatum.STATUS_DB_FETCHED || datum.getStatus() == CrawlDatum.STATUS_DB_NOTMODIFIED) {        context.write(new Text(out + " FETCHED"), new LongWritable(1));    } else {        context.write(new Text(out + " UNFETCHED"), new LongWritable(1));    }}
public void nutch_f1369_0(Text key, Iterable<LongWritable> values, Context context) throws IOException, InterruptedException
{    long total = 0;    for (LongWritable val : values) {        total += val.get();    }    context.write(new LongWritable(total), key);}
public void nutch_f1370_0(Text key, Iterable<LongWritable> values, Context context) throws IOException, InterruptedException
{    long total = 0;    for (LongWritable val : values) {        total += val.get();    }    context.write(key, new LongWritable(total));}
public static void nutch_f1371_0(String[] args) throws Exception
{    ToolRunner.run(NutchConfiguration.create(), new CrawlCompletionStats(), args);}
public static final byte[] nutch_f1372_0(byte[] in)
{    return inflateBestEffort(in, Integer.MAX_VALUE);}
public static final byte[] nutch_f1373_1(byte[] in, int sizeLimit)
{        ByteArrayOutputStream outStream = new ByteArrayOutputStream(EXPECTED_COMPRESSION_RATIO * in.length);        Inflater inflater = new Inflater(true);    InflaterInputStream inStream = new InflaterInputStream(new ByteArrayInputStream(in), inflater);    byte[] buf = new byte[BUF_SIZE];    int written = 0;    while (true) {        try {            int size = inStream.read(buf);            if (size <= 0)                break;            if ((written + size) > sizeLimit) {                outStream.write(buf, 0, sizeLimit - written);                break;            }            outStream.write(buf, 0, size);            written += size;        } catch (Exception e) {                        break;        }    }    try {        outStream.close();    } catch (IOException e) {    }    return outStream.toByteArray();}
public static final byte[] nutch_f1374_0(byte[] in) throws IOException
{        ByteArrayOutputStream outStream = new ByteArrayOutputStream(EXPECTED_COMPRESSION_RATIO * in.length);    InflaterInputStream inStream = new InflaterInputStream(new ByteArrayInputStream(in));    byte[] buf = new byte[BUF_SIZE];    while (true) {        int size = inStream.read(buf);        if (size <= 0)            break;        outStream.write(buf, 0, size);    }    outStream.close();    return outStream.toByteArray();}
public static final byte[] nutch_f1375_1(byte[] in)
{        ByteArrayOutputStream byteOut = new ByteArrayOutputStream(in.length / EXPECTED_COMPRESSION_RATIO);    DeflaterOutputStream outStream = new DeflaterOutputStream(byteOut);    try {        outStream.write(in);    } catch (Exception e) {            }    try {        outStream.close();    } catch (IOException e) {            }    return byteOut.toByteArray();}
public int nutch_f1376_1(String[] args) throws Exception
{    if (args.length < 3) {        System.err.println("Usage: DomainStatistics inputDirs outDir mode [numOfReducer]");        System.err.println("\tinputDirs\tComma separated list of crawldb input directories");        System.err.println("\t\t\tE.g.: crawl/crawldb/");        System.err.println("\toutDir\t\tOutput directory where results should be dumped");        System.err.println("\tmode\t\tSet statistics gathering mode");        System.err.println("\t\t\t\thost\tGather statistics by host");        System.err.println("\t\t\t\tdomain\tGather statistics by domain");        System.err.println("\t\t\t\tsuffix\tGather statistics by suffix");        System.err.println("\t\t\t\ttld\tGather statistics by top level directory");        System.err.println("\t[numOfReducers]\tOptional number of reduce jobs to use. Defaults to 1.");        return 1;    }    String inputDir = args[0];    String outputDir = args[1];    int numOfReducers = 1;    if (args.length > 3) {        numOfReducers = Integer.parseInt(args[3]);    }    SimpleDateFormat sdf = new SimpleDateFormat("yyyy-MM-dd HH:mm:ss");    long start = System.currentTimeMillis();        int mode = 0;    String jobName = "DomainStatistics";    if (args[2].equals("host")) {        jobName = "Host statistics";        mode = MODE_HOST;    } else if (args[2].equals("domain")) {        jobName = "Domain statistics";        mode = MODE_DOMAIN;    } else if (args[2].equals("suffix")) {        jobName = "Suffix statistics";        mode = MODE_SUFFIX;    } else if (args[2].equals("tld")) {        jobName = "TLD statistics";        mode = MODE_TLD;    }    Configuration conf = getConf();    conf.setInt("domain.statistics.mode", mode);    conf.setBoolean("mapreduce.fileoutputcommitter.marksuccessfuljobs", false);    Job job = Job.getInstance(conf, jobName);    job.setJarByClass(DomainStatistics.class);    String[] inputDirsSpecs = inputDir.split(",");    for (int i = 0; i < inputDirsSpecs.length; i++) {        File completeInputPath = new File(new File(inputDirsSpecs[i]), "current");        FileInputFormat.addInputPath(job, new Path(completeInputPath.toString()));    }    job.setInputFormatClass(SequenceFileInputFormat.class);    FileOutputFormat.setOutputPath(job, new Path(outputDir));    job.setOutputFormatClass(TextOutputFormat.class);    job.setMapOutputKeyClass(Text.class);    job.setMapOutputValueClass(LongWritable.class);    job.setOutputKeyClass(Text.class);    job.setOutputValueClass(LongWritable.class);    job.setMapperClass(DomainStatisticsMapper.class);    job.setReducerClass(DomainStatisticsReducer.class);    job.setCombinerClass(DomainStatisticsCombiner.class);    job.setNumReduceTasks(numOfReducers);    try {        boolean success = job.waitForCompletion(true);        if (!success) {            String message = "Injector job did not succeed, job status: " + job.getStatus().getState() + ", reason: " + job.getStatus().getFailureInfo();                                    throw new RuntimeException(message);        }    } catch (IOException | InterruptedException | ClassNotFoundException e) {                throw e;    }    long end = System.currentTimeMillis();        return 0;}
public void nutch_f1377_0(Context context)
{    mode = context.getConfiguration().getInt("domain.statistics.mode", MODE_DOMAIN);}
public void nutch_f1378_1(Text urlText, CrawlDatum datum, Context context) throws IOException, InterruptedException
{    if (datum.getStatus() == CrawlDatum.STATUS_DB_FETCHED || datum.getStatus() == CrawlDatum.STATUS_DB_NOTMODIFIED) {        try {            URL url = new URL(urlText.toString());            String out = null;            switch(mode) {                case MODE_HOST:                    out = url.getHost();                    break;                case MODE_DOMAIN:                    out = URLUtil.getDomainName(url);                    break;                case MODE_SUFFIX:                    out = URLUtil.getDomainSuffix(url).getDomain();                    break;                case MODE_TLD:                    out = URLUtil.getTopLevelDomainName(url);                    break;            }            if (out.trim().equals("")) {                                context.getCounter(MyCounter.EMPTY_RESULT).increment(1);            }            context.write(new Text(out), new LongWritable(1));        } catch (Exception ex) {        }        context.getCounter(MyCounter.FETCHED).increment(1);        context.write(FETCHED_TEXT, new LongWritable(1));    } else {        context.getCounter(MyCounter.NOT_FETCHED).increment(1);        context.write(NOT_FETCHED_TEXT, new LongWritable(1));    }}
public void nutch_f1379_0(Text key, Iterable<LongWritable> values, Context context) throws IOException, InterruptedException
{    long total = 0;    for (LongWritable val : values) {        total += val.get();    }    context.write(new LongWritable(total), key);}
public void nutch_f1380_0(Text key, Iterable<LongWritable> values, Context context) throws IOException, InterruptedException
{    long total = 0;    for (LongWritable val : values) {        total += val.get();    }    context.write(key, new LongWritable(total));}
public static void nutch_f1381_0(String[] args) throws Exception
{    ToolRunner.run(NutchConfiguration.create(), new DomainStatistics(), args);}
public String nutch_f1382_0()
{    return domain;}
public Status nutch_f1383_0()
{    return status;}
public float nutch_f1384_0()
{    return boost;}
public String nutch_f1385_0()
{    return domain;}
public static DomainSuffixes nutch_f1386_0()
{    if (instance == null) {        instance = new DomainSuffixes();    }    return instance;}
 void nutch_f1387_0(DomainSuffix tld)
{    domains.put(tld.getDomain(), tld);}
public boolean nutch_f1388_0(String extension)
{    return domains.containsKey(extension);}
public DomainSuffix nutch_f1389_0(String extension)
{    return domains.get(extension);}
 void nutch_f1390_1(DomainSuffixes tldEntries, InputStream input) throws IOException
{    try {        DocumentBuilderFactory factory = DocumentBuilderFactory.newInstance();        factory.setIgnoringComments(true);        DocumentBuilder builder = factory.newDocumentBuilder();        Document document = builder.parse(new InputSource(input));        Element root = document.getDocumentElement();        if (root != null && root.getTagName().equals("domains")) {            Element tlds = (Element) root.getElementsByTagName("tlds").item(0);            Element suffixes = (Element) root.getElementsByTagName("suffixes").item(0);                        readITLDs(tldEntries, (Element) tlds.getElementsByTagName("itlds").item(0));            readGTLDs(tldEntries, (Element) tlds.getElementsByTagName("gtlds").item(0));            readCCTLDs(tldEntries, (Element) tlds.getElementsByTagName("cctlds").item(0));            readSuffixes(tldEntries, suffixes);        } else {            throw new IOException("xml file is not valid");        }    } catch (ParserConfigurationException ex) {                throw new IOException(ex.getMessage());    } catch (SAXException ex) {                throw new IOException(ex.getMessage());    }}
 void nutch_f1391_0(DomainSuffixes tldEntries, Element el)
{    NodeList children = el.getElementsByTagName("tld");    for (int i = 0; i < children.getLength(); i++) {        tldEntries.addDomainSuffix(readGTLD((Element) children.item(i), Type.INFRASTRUCTURE));    }}
 void nutch_f1392_0(DomainSuffixes tldEntries, Element el)
{    NodeList children = el.getElementsByTagName("tld");    for (int i = 0; i < children.getLength(); i++) {        tldEntries.addDomainSuffix(readGTLD((Element) children.item(i), Type.GENERIC));    }}
 void nutch_f1393_0(DomainSuffixes tldEntries, Element el) throws IOException
{    NodeList children = el.getElementsByTagName("tld");    for (int i = 0; i < children.getLength(); i++) {        tldEntries.addDomainSuffix(readCCTLD((Element) children.item(i)));    }}
 TopLevelDomain nutch_f1394_0(Element el, Type type)
{    String domain = el.getAttribute("domain");    Status status = readStatus(el);    float boost = readBoost(el);    return new TopLevelDomain(domain, type, status, boost);}
 TopLevelDomain nutch_f1395_0(Element el) throws IOException
{    String domain = el.getAttribute("domain");    Status status = readStatus(el);    float boost = readBoost(el);    String countryName = readCountryName(el);    return new TopLevelDomain(domain, status, boost, countryName);}
 Status nutch_f1396_0(Element el)
{    NodeList list = el.getElementsByTagName("status");    if (list == null || list.getLength() == 0)        return DomainSuffix.DEFAULT_STATUS;    return Status.valueOf(list.item(0).getFirstChild().getNodeValue());}
 float nutch_f1397_0(Element el)
{    NodeList list = el.getElementsByTagName("boost");    if (list == null || list.getLength() == 0)        return DomainSuffix.DEFAULT_BOOST;    return Float.parseFloat(list.item(0).getFirstChild().getNodeValue());}
 String nutch_f1398_0(Element el) throws IOException
{    NodeList list = el.getElementsByTagName("country");    if (list == null || list.getLength() == 0)        throw new IOException("Country name should be given");    return list.item(0).getNodeValue();}
 void nutch_f1399_0(DomainSuffixes tldEntries, Element el)
{    NodeList children = el.getElementsByTagName("suffix");    for (int i = 0; i < children.getLength(); i++) {        tldEntries.addDomainSuffix(readSuffix((Element) children.item(i)));    }}
 DomainSuffix nutch_f1400_0(Element el)
{    String domain = el.getAttribute("domain");    Status status = readStatus(el);    float boost = readBoost(el);    return new DomainSuffix(domain, status, boost);}
public Type nutch_f1401_0()
{    return type;}
public String nutch_f1402_0()
{    return countryName;}
public static Element nutch_f1403_1(InputStream is)
{    Element element = null;    DOMParser parser = new DOMParser();    InputSource input;    try {        input = new InputSource(is);        input.setEncoding("UTF-8");        parser.parse(input);        int i = 0;        while (!(parser.getDocument().getChildNodes().item(i) instanceof Element)) {            i++;        }        element = (Element) parser.getDocument().getChildNodes().item(i);    } catch (FileNotFoundException e) {            } catch (SAXException e) {            } catch (IOException e) {            }    return element;}
public static void nutch_f1404_1(OutputStream os, Element e)
{    DOMSource source = new DOMSource(e);    TransformerFactory transFactory = TransformerFactory.newInstance();    Transformer transformer;    try {        transformer = transFactory.newTransformer();        transformer.setOutputProperty("indent", "yes");        StreamResult result = new StreamResult(os);        transformer.transform(source, result);        os.flush();    } catch (UnsupportedEncodingException e1) {            } catch (IOException e1) {            } catch (TransformerConfigurationException e2) {            } catch (TransformerException ex) {            }}
public static void nutch_f1405_0(OutputStream os, DocumentFragment doc)
{    NodeList docChildren = doc.getChildNodes();    for (int i = 0; i < docChildren.getLength(); i++) {        saveDom(os, (Element) docChildren.item(i));    }}
public static String nutch_f1406_0(String url)
{    byte[] digest = MD5Hash.digest(url).getDigest();    StringBuffer sb = new StringBuffer();    for (byte b : digest) {        sb.append(String.format("%02x", b & 0xff));    }    return sb.toString();}
public static String nutch_f1407_1(String basePath, String md5, boolean makeDir)
{    String firstLevelDirName = new StringBuilder().append(md5.charAt(0)).append(md5.charAt(8)).toString();    String secondLevelDirName = new StringBuilder().append(md5.charAt(16)).append(md5.charAt(24)).toString();    String fullDirPath = String.format(DIR_PATTERN, basePath, firstLevelDirName, secondLevelDirName);    if (makeDir) {        try {            FileUtils.forceMkdir(new File(fullDirPath));        } catch (IOException e) {                        fullDirPath = null;        }    }    return fullDirPath;}
public static String nutch_f1408_0(String basePath, String md5)
{    return createTwoLevelsDirectory(basePath, md5, true);}
public static String nutch_f1409_1(String md5, String fileBaseName, String fileExtension)
{    if (fileBaseName.length() > MAX_LENGTH_OF_FILENAME) {                fileBaseName = StringUtils.substring(fileBaseName, 0, MAX_LENGTH_OF_FILENAME);    }    if (fileExtension.length() > MAX_LENGTH_OF_EXTENSION) {                fileExtension = StringUtils.substring(fileExtension, 0, MAX_LENGTH_OF_EXTENSION);    }        fileBaseName = fileBaseName.replaceAll("\\?", "");    fileExtension = fileExtension.replaceAll("\\?", "");    return String.format(FILENAME_PATTERN, md5, fileBaseName, fileExtension);}
public static String nutch_f1410_1(String basePath, String reverseKey, String urlString, String epochScrapeTime, String fileExtension, boolean makeDir)
{    String fullDirPath = basePath + File.separator + reverseKey + File.separator + DigestUtils.sha1Hex(urlString);    if (makeDir) {        try {            FileUtils.forceMkdir(new File(fullDirPath));        } catch (IOException e) {                        fullDirPath = null;        }    }    if (fileExtension.length() > MAX_LENGTH_OF_EXTENSION) {                fileExtension = StringUtils.substring(fileExtension, 0, MAX_LENGTH_OF_EXTENSION);    }    String outputFullPath = fullDirPath + File.separator + epochScrapeTime + "." + fileExtension;    return outputFullPath;}
public static String nutch_f1411_0(Map<String, Integer> typeCounts, Map<String, Integer> filteredCounts)
{    StringBuilder builder = new StringBuilder();        builder.append("\nTOTAL Stats:\n");    builder.append("[\n");    int mimetypeCount = 0;    for (String mimeType : typeCounts.keySet()) {        builder.append("    {\"mimeType\":\"");        builder.append(mimeType);        builder.append("\",\"count\":\"");        builder.append(typeCounts.get(mimeType));        builder.append("\"}\n");        mimetypeCount += typeCounts.get(mimeType);    }    builder.append("]\n");    builder.append("Total count: " + mimetypeCount + "\n");        mimetypeCount = 0;    if (!filteredCounts.isEmpty()) {        builder.append("\nFILTERED Stats:\n");        builder.append("[\n");        for (String mimeType : filteredCounts.keySet()) {            builder.append("    {\"mimeType\":\"");            builder.append(mimeType);            builder.append("\",\"count\":\"");            builder.append(filteredCounts.get(mimeType));            builder.append("\"}\n");            mimetypeCount += filteredCounts.get(mimeType);        }        builder.append("]\n");        builder.append("Total filtered count: " + mimetypeCount + "\n");    }    return builder.toString();}
public String nutch_f1412_0()
{    return source;}
public String nutch_f1413_0()
{    return value;}
public String nutch_f1414_0()
{    return value + " (" + source + ((confidence >= 0) ? ", " + confidence + "% confidence" : "") + ")";}
public boolean nutch_f1415_0()
{    return (value == null || "".equals(value));}
public boolean nutch_f1416_0()
{    return (confidence < 0 || (minConfidence >= 0 && confidence >= minConfidence));}
public void nutch_f1417_1(Content content, boolean filter)
{    byte[] data = content.getContent();    if (minConfidence >= 0 && DETECTABLES.contains(content.getContentType()) && data.length > MIN_LENGTH) {        CharsetMatch[] matches = null;                try {            detector.enableInputFilter(filter);            detector.setText(data);            matches = detector.detectAll();        } catch (Exception e) {                    }        if (matches != null) {            for (CharsetMatch match : matches) {                addClue(match.getName(), "detect", match.getConfidence());            }        }    }        addClue(parseCharacterEncoding(content.getMetadata().get(Response.CONTENT_TYPE)), "header");}
public void nutch_f1418_0(String value, String source, int confidence)
{    if (value == null || "".equals(value)) {        return;    }    value = resolveEncodingAlias(value);    if (value != null) {        clues.add(new EncodingClue(value, source, confidence));    }}
public void nutch_f1419_0(String value, String source)
{    addClue(value, source, NO_THRESHOLD);}
public String nutch_f1420_0(Content content, String defaultValue)
{    /*     * This algorithm could be replaced by something more sophisticated; ideally     * we would gather a bunch of data on where various clues (autodetect, HTTP     * headers, HTML meta tags, etc.) disagree, tag each with the correct     * answer, and use machine learning/some statistical method to generate a     * better heuristic.     */    String base = content.getBaseUrl();    if (LOG.isTraceEnabled()) {        findDisagreements(base, clues);    }    /*     * Go down the list of encoding "clues". Use a clue if: 1. Has a confidence     * value which meets our confidence threshold, OR 2. Doesn't meet the     * threshold, but is the best try, since nothing else is available.     */    EncodingClue defaultClue = new EncodingClue(defaultValue, "default");    EncodingClue bestClue = defaultClue;    for (EncodingClue clue : clues) {        if (LOG.isTraceEnabled()) {            LOG.trace(base + ": charset " + clue);        }        String charset = clue.value;        if (minConfidence >= 0 && clue.confidence >= minConfidence) {            if (LOG.isTraceEnabled()) {                LOG.trace(base + ": Choosing encoding: " + charset + " with confidence " + clue.confidence);            }            return resolveEncodingAlias(charset).toLowerCase();        } else if (clue.confidence == NO_THRESHOLD && bestClue == defaultClue) {            bestClue = clue;        }    }    if (LOG.isTraceEnabled()) {        LOG.trace(base + ": Choosing encoding: " + bestClue);    }    return bestClue.value.toLowerCase();}
public void nutch_f1421_0()
{    clues.clear();}
private void nutch_f1422_0(String url, List<EncodingClue> newClues)
{    HashSet<String> valsSeen = new HashSet<>();    HashSet<String> sourcesSeen = new HashSet<>();    boolean disagreement = false;    for (int i = 0; i < newClues.size(); i++) {        EncodingClue clue = newClues.get(i);        if (!clue.isEmpty() && !sourcesSeen.contains(clue.source)) {            if (valsSeen.size() > 0 && !valsSeen.contains(clue.value) && clue.meetsThreshold()) {                disagreement = true;            }            if (clue.meetsThreshold()) {                valsSeen.add(clue.value);            }            sourcesSeen.add(clue.source);        }    }    if (disagreement) {                StringBuffer sb = new StringBuffer();        sb.append("Disagreement: " + url + "; ");        for (int i = 0; i < newClues.size(); i++) {            if (i > 0) {                sb.append(", ");            }            sb.append(newClues.get(i));        }        LOG.trace(sb.toString());    }}
public static String nutch_f1423_1(String encoding)
{    try {        if (encoding == null || !Charset.isSupported(encoding))            return null;        String canonicalName = new String(Charset.forName(encoding).name());        return ALIASES.containsKey(canonicalName) ? ALIASES.get(canonicalName) : canonicalName;    } catch (Exception e) {                return null;    }}
public static String nutch_f1424_0(String contentType)
{    if (contentType == null)        return (null);    int start = contentType.indexOf("charset=");    if (start < 0)        return (null);    String encoding = contentType.substring(start + 8);    int end = encoding.indexOf(';');    if (end >= 0)        encoding = encoding.substring(0, end);    encoding = encoding.trim();    if ((encoding.length() > 2) && (encoding.startsWith("\"")) && (encoding.endsWith("\"")))        encoding = encoding.substring(1, encoding.length() - 1);    return (encoding.trim());}
public static void nutch_f1425_0(String[] args) throws IOException
{    if (args.length != 1) {        System.err.println("Usage: EncodingDetector <file>");        System.exit(1);    }    Configuration conf = NutchConfiguration.create();    EncodingDetector detector = new EncodingDetector(NutchConfiguration.create());        @SuppressWarnings("resource")    BufferedInputStream istr = new BufferedInputStream(new FileInputStream(args[0]));    ByteArrayOutputStream ostr = new ByteArrayOutputStream();    byte[] bytes = new byte[1000];    boolean more = true;    while (more) {        int len = istr.read(bytes);        if (len < bytes.length) {            more = false;            if (len > 0) {                ostr.write(bytes, 0, len);            }        } else {            ostr.write(bytes);        }    }    byte[] data = ostr.toByteArray();        Content content = new Content("", "", data, "text/html", new Metadata(), conf);    detector.autoDetectClues(content, true);    String encoding = detector.guessEncoding(content, conf.get("parser.character.encoding.default"));    System.out.println("Guessed encoding: " + encoding);}
public static void nutch_f1426_0(FileSystem fs, Path current, Path replacement, boolean removeOld) throws IOException
{        Path old = new Path(current + ".old");    if (fs.exists(current)) {        fs.rename(current, old);    }        fs.rename(replacement, current);    if (fs.exists(old) && removeOld) {        fs.delete(old, true);    }}
public static void nutch_f1427_0(SequenceFile.Reader[] readers) throws IOException
{        if (readers != null) {        for (int i = 0; i < readers.length; i++) {            SequenceFile.Reader reader = readers[i];            if (reader != null) {                reader.close();            }        }    }}
public static void nutch_f1428_0(MapFile.Reader[] readers) throws IOException
{        if (readers != null) {        for (int i = 0; i < readers.length; i++) {            MapFile.Reader reader = readers[i];            if (reader != null) {                reader.close();            }        }    }}
public Configuration nutch_f1429_0()
{    return conf;}
public void nutch_f1430_0(Configuration conf)
{    this.conf = conf;}
public void nutch_f1431_0(DataInput in) throws IOException
{    byte type = in.readByte();    Class<?> clazz = getTypes()[type];    try {        set((Writable) clazz.getConstructor().newInstance());    } catch (Exception e) {        e.printStackTrace();        throw new IOException("Cannot initialize the class: " + clazz);    }    Writable w = get();    if (w instanceof Configurable)        ((Configurable) w).setConf(conf);    w.readFields(in);}
public static final byte[] nutch_f1432_0(byte[] in)
{    return unzipBestEffort(in, Integer.MAX_VALUE);}
public static final byte[] nutch_f1433_0(byte[] in, int sizeLimit)
{    try {                ByteArrayOutputStream outStream = new ByteArrayOutputStream(EXPECTED_COMPRESSION_RATIO * in.length);        GZIPInputStream inStream = new GZIPInputStream(new ByteArrayInputStream(in));        byte[] buf = new byte[BUF_SIZE];        int written = 0;        while (true) {            try {                int size = inStream.read(buf);                if (size <= 0)                    break;                if ((written + size) > sizeLimit) {                    outStream.write(buf, 0, sizeLimit - written);                    break;                }                outStream.write(buf, 0, size);                written += size;            } catch (Exception e) {                break;            }        }        try {            outStream.close();        } catch (IOException e) {        }        return outStream.toByteArray();    } catch (IOException e) {        return null;    }}
public static final byte[] nutch_f1434_0(byte[] in) throws IOException
{        ByteArrayOutputStream outStream = new ByteArrayOutputStream(EXPECTED_COMPRESSION_RATIO * in.length);    GZIPInputStream inStream = new GZIPInputStream(new ByteArrayInputStream(in));    byte[] buf = new byte[BUF_SIZE];    while (true) {        int size = inStream.read(buf);        if (size <= 0)            break;        outStream.write(buf, 0, size);    }    outStream.close();    return outStream.toByteArray();}
public static final byte[] nutch_f1435_1(byte[] in)
{    try {                ByteArrayOutputStream byteOut = new ByteArrayOutputStream(in.length / EXPECTED_COMPRESSION_RATIO);        GZIPOutputStream outStream = new GZIPOutputStream(byteOut);        try {            outStream.write(in);        } catch (Exception e) {                    }        try {            outStream.close();        } catch (IOException e) {                    }        return byteOut.toByteArray();    } catch (IOException e) {                return null;    }}
public static PathFilter nutch_f1436_0()
{    return arg0 -> true;}
public static PathFilter nutch_f1437_0(final FileSystem fs)
{    return path -> {        try {            return fs.getFileStatus(path).isDirectory();        } catch (IOException ioe) {            return false;        }    };}
public static Path[] nutch_f1438_0(FileStatus[] stats)
{    if (stats == null) {        return null;    }    if (stats.length == 0) {        return new Path[0];    }    Path[] res = new Path[stats.length];    for (int i = 0; i < stats.length; i++) {        res[i] = stats[i].getPath();    }    return res;}
public static Expression nutch_f1439_1(String expr)
{    if (expr == null)        return null;    try {                        Matcher matcher = DATE_PATTERN.matcher(expr);        if (matcher.find()) {            String date = matcher.group();                        Date parsedDate = DateUtils.parseDateStrictly(date, new String[] { "yyyy-MM-dd'T'HH:mm:ss'Z'" });            long time = parsedDate.getTime();                        expr = expr.replace(date, Long.toString(time));        }        JexlEngine jexl = new JexlEngine();        jexl.setSilent(true);        jexl.setStrict(true);        return jexl.createExpression(expr);    } catch (Exception e) {            }    return null;}
public static void nutch_f1440_0(FileSystem fs, Path lockFile, boolean accept) throws IOException
{    if (fs.exists(lockFile)) {        if (!accept)            throw new IOException("lock file " + lockFile + " already exists.");        if (fs.getFileStatus(lockFile).isDirectory())            throw new IOException("lock file " + lockFile + " already exists and is a directory.");        } else {                fs.mkdirs(lockFile.getParent());        fs.createNewFile(lockFile);    }}
public static void nutch_f1441_0(Configuration conf, Path lockFile, boolean accept) throws IOException
{    FileSystem fs = lockFile.getFileSystem(conf);    createLockFile(fs, lockFile, accept);}
public static boolean nutch_f1442_0(FileSystem fs, Path lockFile) throws IOException
{    if (!fs.exists(lockFile))        return false;    if (fs.getFileStatus(lockFile).isDirectory())        throw new IOException("lock file " + lockFile + " exists but is a directory!");    return fs.delete(lockFile, false);}
public static boolean nutch_f1443_0(Configuration conf, Path lockFile) throws IOException
{    FileSystem fs = lockFile.getFileSystem(conf);    return removeLockFile(fs, lockFile);}
public static String nutch_f1444_0(String origType)
{    if (origType == null)        return null;        String[] tokenizedMimeType = origType.split(SEPARATOR);    if (tokenizedMimeType.length > 1) {                return tokenizedMimeType[0];    } else {                return origType;    }}
public String nutch_f1445_1(String typeName, String url, byte[] data)
{    String retType = null;    MimeType type = null;    String cleanedMimeType = null;    cleanedMimeType = MimeUtil.cleanMimeType(typeName);        if (cleanedMimeType != null) {        try {            type = mimeTypes.forName(cleanedMimeType);            cleanedMimeType = type.getName();        } catch (MimeTypeException mte) {                        cleanedMimeType = null;        }    }        if (type == null || type.getName().equals(MimeTypes.OCTET_STREAM)) {                try {            retType = tika.detect(url) != null ? tika.detect(url) : null;        } catch (Exception e) {            String message = "Problem loading default Tika configuration";                        throw new RuntimeException(e);        }    } else {        retType = type.getName();    }        if (this.mimeMagic) {        String magicType = null;                Metadata tikaMeta = new Metadata();        tikaMeta.add(Metadata.RESOURCE_NAME_KEY, url);        tikaMeta.add(Metadata.CONTENT_TYPE, (cleanedMimeType != null ? cleanedMimeType : typeName));        try {            try (InputStream stream = TikaInputStream.get(data)) {                magicType = mimeTypes.detect(stream, tikaMeta).toString();            }        } catch (IOException ignore) {        }        if (magicType != null && !magicType.equals(MimeTypes.OCTET_STREAM) && retType != null && !retType.equals(magicType)) {                                    retType = magicType;        }                if (retType == null) {            try {                retType = MimeTypes.OCTET_STREAM;            } catch (Exception ignore) {            }        }    }    return retType;}
public String nutch_f1446_0(String url)
{    return tika.detect(url);}
public String nutch_f1447_1(String name)
{    try {        return this.mimeTypes.forName(name).toString();    } catch (MimeTypeException e) {                return null;    }}
public String nutch_f1448_1(File f)
{    try {        return tika.detect(f);    } catch (Exception e) {                return null;    }}
public Node nutch_f1449_0()
{        if (!hasNext()) {        return null;    }            currentNode = nodes.pop();    currentChildren = currentNode.getChildNodes();    int childLen = (currentChildren != null) ? currentChildren.getLength() : 0;        for (int i = childLen - 1; i >= 0; i--) {        nodes.add(currentChildren.item(i));    }    return currentNode;}
public void nutch_f1450_0()
{    int childLen = (currentChildren != null) ? currentChildren.getLength() : 0;    for (int i = 0; i < childLen; i++) {        Node child = nodes.peek();        if (child.equals(currentChildren.item(i))) {            nodes.pop();        }    }}
public Node nutch_f1451_0()
{    return currentNode;}
public boolean nutch_f1452_0()
{    return (nodes.size() > 0);}
private static void nutch_f1453_0(Configuration conf)
{    UUID uuid = UUID.randomUUID();    conf.set(UUID_KEY, uuid.toString());}
public static String nutch_f1454_0(Configuration conf)
{    return conf.get(UUID_KEY);}
public static Configuration nutch_f1455_0()
{    Configuration conf = new Configuration();    setUUID(conf);    addNutchResources(conf);    return conf;}
public static Configuration nutch_f1456_0(boolean addNutchResources, Properties nutchProperties)
{    Configuration conf = new Configuration();    setUUID(conf);    if (addNutchResources) {        addNutchResources(conf);    }    for (Entry<Object, Object> e : nutchProperties.entrySet()) {        conf.set(e.getKey().toString(), e.getValue().toString());    }    return conf;}
private static Configuration nutch_f1457_0(Configuration conf)
{    conf.addResource("nutch-default.xml");    conf.addResource("nutch-site.xml");    return conf;}
public static Job nutch_f1458_0(Configuration conf) throws IOException
{    return Job.getInstance(conf);}
public static void nutch_f1459_1(Path tempDir, Path lock, FileSystem fs) throws IOException
{    try {        if (fs.exists(tempDir)) {            fs.delete(tempDir, true);        }        LockUtil.removeLockFile(fs, lock);    } catch (IOException e) {                throw e;    }}
public float nutch_f1460_0()
{    float res = 0;    if (currentJob != null) {        try {            res = (currentJob.mapProgress() + currentJob.reduceProgress()) / 2.0f;        } catch (IOException e) {            e.printStackTrace();            res = 0;        } catch (IllegalStateException ile) {            ile.printStackTrace();            res = 0;        }    }        if (numJobs > 1) {        res = (currentJobNum + res) / (float) numJobs;    }    status.put(Nutch.STAT_PROGRESS, res);    return res;}
public Map<String, Object> nutch_f1461_0()
{    return status;}
public boolean nutch_f1462_0() throws Exception
{    return killJob();}
public boolean nutch_f1463_0() throws Exception
{    if (currentJob != null && !currentJob.isComplete()) {        try {            currentJob.killJob();            return true;        } catch (Exception e) {            e.printStackTrace();            return false;        }    }    return false;}
public static synchronized ObjectCache nutch_f1464_1(Configuration conf)
{    ObjectCache objectCache = CACHE.get(conf);    if (objectCache == null) {                objectCache = new ObjectCache();        CACHE.put(conf, objectCache);    }    return objectCache;}
public synchronized Object nutch_f1465_0(String key)
{    return objectMap.get(key);}
public boolean nutch_f1466_0(String key)
{    return objectMap.containsKey(key);}
public synchronized void nutch_f1467_0(String key, Object value)
{    objectMap.put(key, value);}
public boolean nutch_f1468_0(String input)
{    TrieNode node = root;    for (int i = 0; i < input.length(); i++) {        node = node.getChild(input.charAt(i));        if (node == null)            return false;        if (node.isTerminal())            return true;    }    return false;}
public String nutch_f1469_0(String input)
{    TrieNode node = root;    for (int i = 0; i < input.length(); i++) {        node = node.getChild(input.charAt(i));        if (node == null)            return null;        if (node.isTerminal())            return input.substring(0, i + 1);    }    return null;}
public String nutch_f1470_0(String input)
{    TrieNode node = root;    String result = null;    for (int i = 0; i < input.length(); i++) {        node = node.getChild(input.charAt(i));        if (node == null)            break;        if (node.isTerminal())            result = input.substring(0, i + 1);    }    return result;}
public static final void nutch_f1471_0(String[] argv)
{    String[] prefixes = new String[] { "abcd", "abc", "aac", "baz", "foo", "foobar" };    PrefixStringMatcher matcher = new PrefixStringMatcher(prefixes);    String[] tests = { "a", "ab", "abc", "abcdefg", "apple", "aa", "aac", "aaccca", "abaz", "baz", "bazooka", "fo", "foobar", "kite" };    for (int i = 0; i < tests.length; i++) {        System.out.println("testing: " + tests[i]);        System.out.println("   matches: " + matcher.matches(tests[i]));        System.out.println("  shortest: " + matcher.shortestMatch(tests[i]));        System.out.println("   longest: " + matcher.longestMatch(tests[i]));    }    int iterations = 1000;    System.out.println("Testing thread-safety (NUTCH-2585) with " + iterations + " iterations:");    List<String> testsList = Arrays.asList(tests);    for (int i = 0; i < iterations; i++) {        matcher = new PrefixStringMatcher(prefixes);        Collections.shuffle(testsList);        try {            long count = testsList.parallelStream().filter(matcher::matches).count();            System.out.print(String.format("Cycle %4d : %d matches\r", i, count));        } catch (Exception e) {                        System.out.println("");            throw e;        }    }    System.out.println("");}
public int nutch_f1472_1(String[] args) throws Exception
{    if (args.length < 2) {        System.err.println("Usage: ProtocolStatistics inputDirs outDir [numOfReducer]");        System.err.println("\tinputDirs\tComma separated list of crawldb input directories");        System.err.println("\t\t\tE.g.: crawl/crawldb/");        System.err.println("\toutDir\t\tOutput directory where results should be dumped");        System.err.println("\t[numOfReducers]\tOptional number of reduce jobs to use. Defaults to 1.");        return 1;    }    String inputDir = args[0];    String outputDir = args[1];    int numOfReducers = 1;    if (args.length > 3) {        numOfReducers = Integer.parseInt(args[3]);    }    SimpleDateFormat sdf = new SimpleDateFormat("yyyy-MM-dd HH:mm:ss");    long start = System.currentTimeMillis();        String jobName = "ProtocolStatistics";    Configuration conf = getConf();    conf.setBoolean("mapreduce.fileoutputcommitter.marksuccessfuljobs", false);    Job job = Job.getInstance(conf, jobName);    job.setJarByClass(ProtocolStatusStatistics.class);    String[] inputDirsSpecs = inputDir.split(",");    for (int i = 0; i < inputDirsSpecs.length; i++) {        File completeInputPath = new File(new File(inputDirsSpecs[i]), "current");        FileInputFormat.addInputPath(job, new Path(completeInputPath.toString()));    }    job.setInputFormatClass(SequenceFileInputFormat.class);    FileOutputFormat.setOutputPath(job, new Path(outputDir));    job.setOutputFormatClass(TextOutputFormat.class);    job.setMapOutputKeyClass(Text.class);    job.setMapOutputValueClass(LongWritable.class);    job.setOutputKeyClass(Text.class);    job.setOutputValueClass(LongWritable.class);    job.setMapperClass(ProtocolStatusStatisticsMapper.class);    job.setReducerClass(ProtocolStatusStatisticsReducer.class);    job.setCombinerClass(ProtocolStatusStatisticsCombiner.class);    job.setNumReduceTasks(numOfReducers);    try {        boolean success = job.waitForCompletion(true);        if (!success) {            String message = jobName + " job did not succeed, job status: " + job.getStatus().getState() + ", reason: " + job.getStatus().getFailureInfo();                                    throw new RuntimeException(message);        }    } catch (IOException | InterruptedException | ClassNotFoundException e) {                throw e;    }    long end = System.currentTimeMillis();        return 0;}
public void nutch_f1473_0(Text urlText, CrawlDatum datum, Context context) throws IOException, InterruptedException
{    if (datum.getMetaData().containsKey(Nutch.PROTOCOL_STATUS_CODE_KEY)) {        context.write((Text) datum.getMetaData().get(Nutch.PROTOCOL_STATUS_CODE_KEY), new LongWritable(1));    } else {        context.write(UNFETCHED_TEXT, new LongWritable(1));    }}
public void nutch_f1474_0(Text key, Iterable<LongWritable> values, Context context) throws IOException, InterruptedException
{    long total = 0;    for (LongWritable val : values) {        total += val.get();    }    context.write(new LongWritable(total), key);}
public void nutch_f1475_0(Text key, Iterable<LongWritable> values, Context context) throws IOException, InterruptedException
{    long total = 0;    for (LongWritable val : values) {        total += val.get();    }    context.write(key, new LongWritable(total));}
public static void nutch_f1476_0(String[] args) throws Exception
{    ToolRunner.run(NutchConfiguration.create(), new ProtocolStatusStatistics(), args);}
public static SequenceFile.Reader[] nutch_f1477_0(Path dir, Configuration conf) throws IOException
{    FileSystem fs = dir.getFileSystem(conf);    Path[] names = FileUtil.stat2Paths(fs.listStatus(dir));    Arrays.sort(names);    SequenceFile.Reader[] parts = new SequenceFile.Reader[names.length];    for (int i = 0; i < names.length; i++) {        parts[i] = new SequenceFile.Reader(conf, SequenceFile.Reader.file(names[i]));    }    return parts;}
public void nutch_f1478_0(Context context)
{    Configuration conf = context.getConfiguration();    this.protocolFactory = new ProtocolFactory(conf);    this.filter = conf.getBoolean(SITEMAP_URL_FILTERING, true);    this.normalize = conf.getBoolean(SITEMAP_URL_NORMALIZING, true);    this.strict = conf.getBoolean(SITEMAP_STRICT_PARSING, true);    this.tryDefaultSitemapXml = conf.getBoolean(SITEMAP_ALWAYS_TRY_SITEMAPXML_ON_ROOT, true);    this.maxRedir = conf.getInt(SITEMAP_REDIR_MAX, 3);    this.parser = new SiteMapParser(strict);    if (filter) {        filters = new URLFilters(conf);    }    if (normalize) {        normalizers = new URLNormalizers(conf, URLNormalizers.SCOPE_DEFAULT);    }}
public void nutch_f1479_1(Text key, Writable value, Context context) throws IOException, InterruptedException
{    String url;    try {        if (value instanceof CrawlDatum) {                        context.write(key, (CrawlDatum) value);        } else if (value instanceof HostDatum) {            generateSitemapsFromHostname(key.toString(), context);        } else if (value instanceof Text) {                        url = key.toString();            if (url.startsWith("http://") || url.startsWith("https://") || url.startsWith("ftp://") || url.startsWith("file:/")) {                                if ((url = filterNormalize(url)) == null) {                    context.getCounter("Sitemap", "filtered_records").increment(1);                    return;                }                context.getCounter("Sitemap", "sitemap_seeds").increment(1);                generateSitemapUrlDatum(protocolFactory.getProtocol(url), url, context);            } else {                                generateSitemapsFromHostname(key.toString(), context);            }        }    } catch (Exception e) {            }}
private String nutch_f1480_0(String url)
{    try {        if (normalizers != null)            url = normalizers.normalize(url, URLNormalizers.SCOPE_DEFAULT);        if (filters != null)            url = filters.filter(url);    } catch (Exception e) {        return null;    }    return url;}
private void nutch_f1481_1(String host, Context context)
{    try {                                String url;        if ((url = filterNormalize("http://" + host + "/")) == null && (url = filterNormalize("https://" + host + "/")) == null && (url = filterNormalize("ftp://" + host + "/")) == null && (url = filterNormalize("file:/" + host + "/")) == null) {            context.getCounter("Sitemap", "filtered_records").increment(1);            return;        }                BaseRobotRules rules = protocolFactory.getProtocol(url).getRobotRules(new Text(url), datum, null);        List<String> sitemaps = rules.getSitemaps();        if (tryDefaultSitemapXml && sitemaps.size() == 0) {            sitemaps.add(url + "sitemap.xml");        }        for (String sitemap : sitemaps) {            context.getCounter("Sitemap", "sitemaps_from_hostname").increment(1);            sitemap = filterNormalize(sitemap);            if (sitemap == null) {                context.getCounter("Sitemap", "filtered_sitemaps_from_hostname").increment(1);            } else {                generateSitemapUrlDatum(protocolFactory.getProtocol(sitemap), sitemap, context);            }        }    } catch (Exception e) {            }}
private void nutch_f1482_1(Protocol protocol, String url, Context context) throws Exception
{    ProtocolOutput output = protocol.getProtocolOutput(new Text(url), datum);    ProtocolStatus status = output.getStatus();    Content content = output.getContent();        int maxRedir = this.maxRedir;    while (!output.getStatus().isSuccess() && output.getStatus().isRedirect() && maxRedir > 0) {        String[] stuff = output.getStatus().getArgs();        url = filterNormalize(stuff[0]);                if (url == null) {            break;        }        output = protocol.getProtocolOutput(new Text(url), datum);        status = output.getStatus();        content = output.getContent();        maxRedir--;    }    if (status.getCode() != ProtocolStatus.SUCCESS) {                        context.getCounter("Sitemap", "failed_fetches").increment(1);                return;    }    AbstractSiteMap asm = parser.parseSiteMap(content.getContentType(), content.getContent(), new URL(url));    if (asm instanceof SiteMap) {                SiteMap sm = (SiteMap) asm;        Collection<SiteMapURL> sitemapUrls = sm.getSiteMapUrls();        for (SiteMapURL sitemapUrl : sitemapUrls) {                        if (!strict || sitemapUrl.isValid()) {                String key = filterNormalize(sitemapUrl.getUrl().toString());                if (key != null) {                    CrawlDatum sitemapUrlDatum = new CrawlDatum();                    sitemapUrlDatum.setStatus(CrawlDatum.STATUS_INJECTED);                    sitemapUrlDatum.setScore((float) sitemapUrl.getPriority());                    if (sitemapUrl.getChangeFrequency() != null) {                        int fetchInterval = -1;                        switch(sitemapUrl.getChangeFrequency()) {                            case ALWAYS:                                fetchInterval = 1;                                break;                                                        case HOURLY:                                fetchInterval = 3600;                                break;                                                        case DAILY:                                fetchInterval = 86400;                                break;                                                        case WEEKLY:                                fetchInterval = 604800;                                break;                                                        case MONTHLY:                                fetchInterval = 2592000;                                break;                                                        case YEARLY:                                fetchInterval = 31536000;                                break;                                                        case NEVER:                                fetchInterval = Integer.MAX_VALUE;                                break;                        }                        sitemapUrlDatum.setFetchInterval(fetchInterval);                    }                    if (sitemapUrl.getLastModified() != null) {                        sitemapUrlDatum.setModifiedTime(sitemapUrl.getLastModified().getTime());                    }                    context.write(new Text(key), sitemapUrlDatum);                }            }        }    } else if (asm instanceof SiteMapIndex) {        SiteMapIndex index = (SiteMapIndex) asm;        Collection<AbstractSiteMap> sitemapUrls = index.getSitemaps();        if (sitemapUrls.isEmpty()) {            return;        }                for (AbstractSiteMap sitemap : sitemapUrls) {            String sitemapUrl = filterNormalize(sitemap.getUrl().toString());            if (sitemapUrl != null) {                generateSitemapUrlDatum(protocol, sitemapUrl, context);            }        }    }}
public void nutch_f1483_0(Context context)
{    Configuration conf = context.getConfiguration();    this.overwriteExisting = conf.getBoolean(SITEMAP_OVERWRITE_EXISTING, false);}
public void nutch_f1484_0(Text key, Iterable<CrawlDatum> values, Context context) throws IOException, InterruptedException
{    sitemapDatum = null;    originalDatum = null;    for (CrawlDatum curr : values) {        if (curr.getStatus() == CrawlDatum.STATUS_INJECTED) {            sitemapDatum = new CrawlDatum();            sitemapDatum.set(curr);        } else {            originalDatum = new CrawlDatum();            originalDatum.set(curr);        }    }    if (originalDatum != null) {                if (sitemapDatum != null && overwriteExisting) {            originalDatum.setScore(sitemapDatum.getScore());            originalDatum.setFetchInterval(sitemapDatum.getFetchInterval());            originalDatum.setModifiedTime(sitemapDatum.getModifiedTime());        }        context.getCounter("Sitemap", "existing_sitemap_entries").increment(1);        context.write(key, originalDatum);    } else if (sitemapDatum != null) {                context.getCounter("Sitemap", "new_sitemap_entries").increment(1);        sitemapDatum.setStatus(CrawlDatum.STATUS_DB_UNFETCHED);        context.write(key, sitemapDatum);    }}
public void nutch_f1485_1(Path crawldb, Path hostdb, Path sitemapUrlDir, boolean strict, boolean filter, boolean normalize, int threads) throws Exception
{    long start = System.currentTimeMillis();    if (LOG.isInfoEnabled()) {            }    FileSystem fs = crawldb.getFileSystem(getConf());    Path old = new Path(crawldb, "old");    Path current = new Path(crawldb, "current");    Path tempCrawlDb = new Path(crawldb, "crawldb-" + Integer.toString(new Random().nextInt(Integer.MAX_VALUE)));        Path lock = new Path(crawldb, LOCK_NAME);    if (!fs.exists(current))        fs.mkdirs(current);    LockUtil.createLockFile(fs, lock, false);    Configuration conf = getConf();    conf.setBoolean(SITEMAP_STRICT_PARSING, strict);    conf.setBoolean(SITEMAP_URL_FILTERING, filter);    conf.setBoolean(SITEMAP_URL_NORMALIZING, normalize);    conf.setBoolean("mapreduce.fileoutputcommitter.marksuccessfuljobs", false);    Job job = Job.getInstance(conf, "SitemapProcessor_" + crawldb.toString());    job.setJarByClass(SitemapProcessor.class);        MultipleInputs.addInputPath(job, current, SequenceFileInputFormat.class);    if (sitemapUrlDir != null)        MultipleInputs.addInputPath(job, sitemapUrlDir, KeyValueTextInputFormat.class);    if (hostdb != null)        MultipleInputs.addInputPath(job, new Path(hostdb, CURRENT_NAME), SequenceFileInputFormat.class);    FileOutputFormat.setOutputPath(job, tempCrawlDb);    job.setOutputFormatClass(MapFileOutputFormat.class);    job.setOutputKeyClass(Text.class);    job.setOutputValueClass(CrawlDatum.class);    job.setMapperClass(MultithreadedMapper.class);    MultithreadedMapper.setMapperClass(job, SitemapMapper.class);    MultithreadedMapper.setNumberOfThreads(job, threads);    job.setReducerClass(SitemapReducer.class);    try {        boolean success = job.waitForCompletion(true);        if (!success) {            String message = "SitemapProcessor_" + crawldb.toString() + " job did not succeed, job status: " + job.getStatus().getState() + ", reason: " + job.getStatus().getFailureInfo();                        NutchJob.cleanupAfterFailure(tempCrawlDb, lock, fs);                        throw new RuntimeException(message);        }        boolean preserveBackup = conf.getBoolean("db.preserve.backup", true);        if (!preserveBackup && fs.exists(old))            fs.delete(old, true);        else            FSUtils.replace(fs, old, current, true);        FSUtils.replace(fs, current, tempCrawlDb, true);        LockUtil.removeLockFile(fs, lock);        if (LOG.isInfoEnabled()) {            long filteredRecords = job.getCounters().findCounter("Sitemap", "filtered_records").getValue();            long fromHostname = job.getCounters().findCounter("Sitemap", "sitemaps_from_hostname").getValue();            long fromSeeds = job.getCounters().findCounter("Sitemap", "sitemap_seeds").getValue();            long failedFetches = job.getCounters().findCounter("Sitemap", "failed_fetches").getValue();            long newSitemapEntries = job.getCounters().findCounter("Sitemap", "new_sitemap_entries").getValue();                                                                        long end = System.currentTimeMillis();                    }    } catch (IOException | InterruptedException | ClassNotFoundException e) {                NutchJob.cleanupAfterFailure(tempCrawlDb, lock, fs);        throw e;    }}
public static void nutch_f1486_0(String[] args) throws Exception
{    int res = ToolRunner.run(NutchConfiguration.create(), new SitemapProcessor(), args);    System.exit(res);}
public static void nutch_f1487_0()
{    System.err.println("Usage:\n SitemapProcessor <crawldb> [-hostdb <hostdb>] [-sitemapUrls <url_dir>] " + "[-threads <threads>] [-force] [-noStrict] [-noFilter] [-noNormalize]\n");    System.err.println("\t<crawldb>\t\tpath to crawldb where the sitemap urls would be injected");    System.err.println("\t-hostdb <hostdb>\tpath of a hostdb. Sitemap(s) from these hosts would be downloaded");    System.err.println("\t-sitemapUrls <url_dir>\tpath to directory with sitemap urls or hostnames");    System.err.println("\t-threads <threads>\tNumber of threads created per mapper to fetch sitemap urls (default: 8)");    System.err.println("\t-force\t\t\tforce update even if CrawlDb appears to be locked (CAUTION advised)");    System.err.println("\t-noStrict\t\tBy default Sitemap parser rejects invalid urls. '-noStrict' disables that.");    System.err.println("\t-noFilter\t\tturn off URLFilters on urls (optional)");    System.err.println("\t-noNormalize\t\tturn off URLNormalizer on urls (optional)");}
public int nutch_f1488_1(String[] args) throws Exception
{    if (args.length < 3) {        usage();        return -1;    }    Path crawlDb = new Path(args[0]);    Path hostDb = null;    Path urlDir = null;    boolean strict = true;    boolean filter = true;    boolean normalize = true;    int threads = 8;    for (int i = 1; i < args.length; i++) {        if (args[i].equals("-hostdb")) {            hostDb = new Path(args[++i]);                    } else if (args[i].equals("-sitemapUrls")) {            urlDir = new Path(args[++i]);                    } else if (args[i].equals("-threads")) {            threads = Integer.valueOf(args[++i]);                    } else if (args[i].equals("-noStrict")) {                        strict = false;        } else if (args[i].equals("-noFilter")) {                        filter = false;        } else if (args[i].equals("-noNormalize")) {                        normalize = false;        } else {                        usage();            return -1;        }    }    try {        sitemap(crawlDb, hostDb, urlDir, strict, filter, normalize, threads);        return 0;    } catch (Exception e) {                return -1;    }}
public static String nutch_f1489_0(String s, int length)
{    StringBuffer sb = new StringBuffer(s);    for (int i = length - s.length(); i > 0; i--) sb.append(" ");    return sb.toString();}
public static String nutch_f1490_0(String s, int length)
{    StringBuffer sb = new StringBuffer();    for (int i = length - s.length(); i > 0; i--) sb.append(" ");    sb.append(s);    return sb.toString();}
public static String nutch_f1491_0(byte[] buf)
{    return toHexString(buf, null, Integer.MAX_VALUE);}
public static String nutch_f1492_0(byte[] buf, String sep, int lineLen)
{    if (buf == null)        return null;    if (lineLen <= 0)        lineLen = Integer.MAX_VALUE;    StringBuffer res = new StringBuffer(buf.length * 2);    for (int i = 0; i < buf.length; i++) {        int b = buf[i];        res.append(HEX_DIGITS[(b >> 4) & 0xf]);        res.append(HEX_DIGITS[b & 0xf]);        if (i > 0 && (i % lineLen) == 0)            res.append('\n');        else if (sep != null && i < lineLen - 1)            res.append(sep);    }    return res.toString();}
public static byte[] nutch_f1493_0(String text)
{    text = text.trim();    if (text.length() % 2 != 0)        text = "0" + text;    int resLen = text.length() / 2;    int loNibble, hiNibble;    byte[] res = new byte[resLen];    for (int i = 0; i < resLen; i++) {        int j = i << 1;        hiNibble = charToNibble(text.charAt(j));        loNibble = charToNibble(text.charAt(j + 1));        if (loNibble == -1 || hiNibble == -1)            return null;        res[i] = (byte) (hiNibble << 4 | loNibble);    }    return res;}
private static final int nutch_f1494_0(char c)
{    if (c >= '0' && c <= '9') {        return c - '0';    } else if (c >= 'a' && c <= 'f') {        return 0xa + (c - 'a');    } else if (c >= 'A' && c <= 'F') {        return 0xA + (c - 'A');    } else {        return -1;    }}
public static boolean nutch_f1495_0(String str)
{    return (str == null) || (str.equals(""));}
public static String nutch_f1496_0(String value)
{    return value.replaceAll("", "");}
public static void nutch_f1497_0(String[] args)
{    if (args.length != 1)        System.out.println("Usage: StringUtil <encoding name>");    else        System.out.println(args[0] + " is resolved to " + EncodingDetector.resolveEncodingAlias(args[0]));}
public boolean nutch_f1498_0(String input)
{    TrieNode node = root;    for (int i = input.length() - 1; i >= 0; i--) {        node = node.getChild(input.charAt(i));        if (node == null)            return false;        if (node.isTerminal())            return true;    }    return false;}
public String nutch_f1499_0(String input)
{    TrieNode node = root;    for (int i = input.length() - 1; i >= 0; i--) {        node = node.getChild(input.charAt(i));        if (node == null)            return null;        if (node.isTerminal())            return input.substring(i);    }    return null;}
public String nutch_f1500_0(String input)
{    TrieNode node = root;    String result = null;    for (int i = input.length() - 1; i >= 0; i--) {        node = node.getChild(input.charAt(i));        if (node == null)            break;        if (node.isTerminal())            result = input.substring(i);    }    return result;}
public static final void nutch_f1501_0(String[] argv)
{    SuffixStringMatcher matcher = new SuffixStringMatcher(new String[] { "a", "abcd", "bcd", "bcdefg", "defg", "aac", "baz", "foo", "foobar" });    String[] tests = { "a", "ac", "abcd", "abcdefg", "apple", "aa", "aac", "aaccca", "abaz", "baz", "bazooka", "fo", "foobar", "kite" };    for (int i = 0; i < tests.length; i++) {        System.out.println("testing: " + tests[i]);        System.out.println("   matches: " + matcher.matches(tests[i]));        System.out.println("  shortest: " + matcher.shortestMatch(tests[i]));        System.out.println("   longest: " + matcher.longestMatch(tests[i]));    }}
public static String nutch_f1502_0(String urlString) throws MalformedURLException
{    return reverseUrl(new URL(urlString));}
public static String nutch_f1503_0(URL url)
{    String host = url.getHost();    String file = url.getFile();    String protocol = url.getProtocol();    int port = url.getPort();    StringBuilder buf = new StringBuilder();    /* reverse host */    reverseAppendSplits(host, buf);    /* add protocol */    buf.append(':');    buf.append(protocol);    /* add port if necessary */    if (port != -1) {        buf.append(':');        buf.append(port);    }    /* add path */    if (file.length() > 0 && '/' != file.charAt(0)) {        buf.append('/');    }    buf.append(file);    return buf.toString();}
public static String nutch_f1504_0(String reversedUrl)
{    StringBuilder buf = new StringBuilder(reversedUrl.length() + 2);    int pathBegin = reversedUrl.indexOf('/');    if (pathBegin == -1)        pathBegin = reversedUrl.length();    String sub = reversedUrl.substring(0, pathBegin);        String[] splits = StringUtils.splitPreserveAllTokens(sub, ':');                    buf.append(splits[1]);    buf.append("://");        reverseAppendSplits(splits[0], buf);        if (splits.length == 3) {                buf.append(':');        buf.append(splits[2]);    }    buf.append(reversedUrl.substring(pathBegin));    return buf.toString();}
public static String nutch_f1505_0(String reversedUrl)
{    return reversedUrl.substring(0, reversedUrl.indexOf(':'));}
private static void nutch_f1506_0(String string, StringBuilder buf)
{    String[] splits = StringUtils.split(string, '.');    if (splits.length > 0) {        for (int i = splits.length - 1; i > 0; i--) {            buf.append(splits[i]);            buf.append('.');        }        buf.append(splits[0]);    } else {        buf.append(string);    }}
public static String nutch_f1507_0(String hostName)
{    StringBuilder buf = new StringBuilder();    reverseAppendSplits(hostName, buf);    return buf.toString();}
public static String nutch_f1508_0(String reversedHostName)
{        return reverseHost(reversedHostName);}
public static String nutch_f1509_0(CharSequence utf8)
{    return (utf8 == null ? null : StringUtil.cleanField(utf8.toString()));}
public static String nutch_f1510_0(long millis)
{    return logDateFormat.format(LocalDateTime.ofInstant(Instant.ofEpochMilli(millis), ZoneId.systemDefault()));}
public static String nutch_f1511_0(long start, long end)
{    if (start > end) {        return null;    }    return secondsToHMS((end - start) / 1000);}
public static String nutch_f1512_0(long seconds)
{    long hours = TimeUnit.SECONDS.toHours(seconds);    long minutes = TimeUnit.SECONDS.toMinutes(seconds) % TimeUnit.HOURS.toMinutes(1);    seconds = TimeUnit.SECONDS.toSeconds(seconds) % TimeUnit.MINUTES.toSeconds(1);    return String.format("%02d:%02d:%02d", hours, minutes, seconds);}
public static String nutch_f1513_0(long seconds)
{    long days = TimeUnit.SECONDS.toDays(seconds);    if (days == 0)        return secondsToHMS(seconds);    String hhmmss = secondsToHMS(seconds % TimeUnit.DAYS.toSeconds(1));    return String.format("%d days, %s", days, hhmmss);}
 boolean nutch_f1514_0()
{    return terminal;}
 TrieNode nutch_f1515_0(char nextChar, boolean isTerminal)
{    if (childrenList == null) {        childrenList = new LinkedList<>();        childrenList.addAll(Arrays.asList(children));        children = null;    }    if (childrenList.size() == 0) {        TrieNode newNode = new TrieNode(nextChar, isTerminal);        childrenList.add(newNode);        return newNode;    }    ListIterator<TrieNode> iter = childrenList.listIterator();    TrieNode node = iter.next();    while ((node.nodeChar < nextChar) && iter.hasNext()) node = iter.next();    if (node.nodeChar == nextChar) {        node.terminal = node.terminal | isTerminal;        return node;    }    if (node.nodeChar > nextChar)        iter.previous();    TrieNode newNode = new TrieNode(nextChar, isTerminal);    iter.add(newNode);    return newNode;}
 TrieNode nutch_f1516_0(char nextChar)
{    if (children == null) {        compile();    }    int min = 0;    int max = children.length - 1;    int mid = 0;    while (min < max) {        mid = (min + max) / 2;        if (children[mid].nodeChar == nextChar)            return children[mid];        if (children[mid].nodeChar < nextChar)            min = mid + 1;        else                        max = mid - 1;    }    if (min == max)        if (children[min].nodeChar == nextChar)            return children[min];    return null;}
public int nutch_f1517_0(TrieNode other)
{    if (this.nodeChar < other.nodeChar)        return -1;    if (this.nodeChar == other.nodeChar)        return 0;        return 1;}
 synchronized void nutch_f1518_0()
{    if (childrenList != null) {        children = childrenList.toArray(new TrieNode[childrenList.size()]);        childrenList = null;        Arrays.sort(children);    }}
protected final TrieNode nutch_f1519_0(TrieNode node, String s, int idx)
{    return node.getChild(s.charAt(idx));}
protected final void nutch_f1520_0(String s)
{    TrieNode node = root;    int stop = s.length() - 1;    int i;    if (s.length() > 0) {        for (i = 0; i < stop; i++) node = node.getChildAddIfNotPresent(s.charAt(i), false);        node = node.getChildAddIfNotPresent(s.charAt(i), true);    }}
protected final void nutch_f1521_0(String s)
{    TrieNode node = root;    if (s.length() > 0) {        for (int i = s.length() - 1; i > 0; i--) node = node.getChildAddIfNotPresent(s.charAt(i), false);        node = node.getChildAddIfNotPresent(s.charAt(0), true);    }}
public static URL nutch_f1522_0(URL base, String target) throws MalformedURLException
{    target = target.trim();        if (target.startsWith("?")) {        return fixPureQueryTargets(base, target);    }    return new URL(base, target);}
 static URL nutch_f1523_0(URL base, String target) throws MalformedURLException
{    if (!target.startsWith("?"))        return new URL(base, target);    String basePath = base.getPath();    String baseRightMost = "";    int baseRightMostIdx = basePath.lastIndexOf("/");    if (baseRightMostIdx != -1) {        baseRightMost = basePath.substring(baseRightMostIdx + 1);    }    if (target.startsWith("?"))        target = baseRightMost + target;    return new URL(base, target);}
public static String nutch_f1524_0(URL url)
{    DomainSuffixes tlds = DomainSuffixes.getInstance();    String host = url.getHost();        if (host.endsWith("."))        host = host.substring(0, host.length() - 1);    if (IP_PATTERN.matcher(host).matches())        return host;    int index = 0;    String candidate = host;    for (; index >= 0; ) {        index = candidate.indexOf('.');        String subCandidate = candidate.substring(index + 1);        if (tlds.isDomainSuffix(subCandidate)) {            return candidate;        }        candidate = subCandidate;    }    return candidate;}
public static String nutch_f1525_0(String url) throws MalformedURLException
{    return getDomainName(new URL(url));}
public static String nutch_f1526_0(URL url) throws MalformedURLException
{    String suffix = getDomainSuffix(url).toString();    int idx = suffix.lastIndexOf(".");    if (idx != -1) {        return suffix.substring(idx + 1);    } else {        return suffix;    }}
public static String nutch_f1527_0(String url) throws MalformedURLException
{    return getTopLevelDomainName(new URL(url));}
public static boolean nutch_f1528_0(URL url1, URL url2)
{    return getDomainName(url1).equalsIgnoreCase(getDomainName(url2));}
public static boolean nutch_f1529_0(String url1, String url2) throws MalformedURLException
{    return isSameDomainName(new URL(url1), new URL(url2));}
public static DomainSuffix nutch_f1530_0(URL url)
{    DomainSuffixes tlds = DomainSuffixes.getInstance();    String host = url.getHost();    if (IP_PATTERN.matcher(host).matches())        return null;    int index = 0;    String candidate = host;    for (; index >= 0; ) {        index = candidate.indexOf('.');        String subCandidate = candidate.substring(index + 1);        DomainSuffix d = tlds.get(subCandidate);        if (d != null) {            return d;        }        candidate = subCandidate;    }    return null;}
public static DomainSuffix nutch_f1531_0(String url) throws MalformedURLException
{    return getDomainSuffix(new URL(url));}
public static String[] nutch_f1532_0(URL url)
{    String host = url.getHost();        if (IP_PATTERN.matcher(host).matches())        return new String[] { host };    return host.split("\\.");}
public static String[] nutch_f1533_0(String url) throws MalformedURLException
{    return getHostSegments(new URL(url));}
public static String nutch_f1534_0(String src, String dst, boolean temp)
{        URL srcUrl;    URL dstUrl;    try {        srcUrl = new URL(src);        dstUrl = new URL(dst);    } catch (MalformedURLException e) {        return dst;    }        String srcDomain = URLUtil.getDomainName(srcUrl);    String dstDomain = URLUtil.getDomainName(dstUrl);    String srcHost = srcUrl.getHost();    String dstHost = dstUrl.getHost();    String srcFile = srcUrl.getFile();    String dstFile = dstUrl.getFile();        boolean srcRoot = (srcFile.equals("/") || srcFile.length() == 0);    boolean destRoot = (dstFile.equals("/") || dstFile.length() == 0);        if (!srcDomain.equals(dstDomain)) {        return dst;    }        if (!temp) {                if (srcRoot) {            return src;        } else {            return dst;        }    } else {                if (srcRoot && !destRoot) {            return src;        } else if (!srcRoot && destRoot) {                        return dst;        } else if (!srcRoot && !destRoot && (srcHost.equals(dstHost))) {                        int numSrcPaths = srcFile.split("/").length;            int numDstPaths = dstFile.split("/").length;            if (numSrcPaths != numDstPaths) {                return (numDstPaths < numSrcPaths ? dst : src);            } else {                int srcPathLength = srcFile.length();                int dstPathLength = dstFile.length();                return (dstPathLength < srcPathLength ? dst : src);            }        } else {                        int numSrcSubs = srcHost.split("\\.").length;            int numDstSubs = dstHost.split("\\.").length;            return (numDstSubs < numSrcSubs ? dst : src);        }    }}
public static String nutch_f1535_0(String url)
{    try {        return new URL(url).getHost().toLowerCase();    } catch (MalformedURLException e) {        return null;    }}
public static String nutch_f1536_0(String url)
{    try {                url = url.toLowerCase();        String queryStr = new URL(url).getQuery();        return (queryStr != null) ? url.replace("?" + queryStr, "") : url;    } catch (MalformedURLException e) {        return null;    }}
public static String nutch_f1537_0(String url)
{    try {        return getProtocol(new URL(url));    } catch (Exception e) {        return null;    }}
public static String nutch_f1538_0(URL url)
{    return url.getProtocol();}
public static String nutch_f1539_0(String url)
{    try {        URL u = new URL(url);        String host = u.getHost();        if (host == null || host.isEmpty()) {                        return url;        }        URI p = new URI(u.getProtocol(), u.getUserInfo(), IDN.toASCII(host), u.getPort(), u.getPath(), u.getQuery(), u.getRef());        return p.toString();    } catch (Exception e) {        return null;    }}
public static String nutch_f1540_0(String url)
{    try {        URL u = new URL(url);        String host = u.getHost();        if (host == null || host.isEmpty()) {                        return url;        }        StringBuilder sb = new StringBuilder();        sb.append(u.getProtocol());        sb.append("://");        if (u.getUserInfo() != null) {            sb.append(u.getUserInfo());            sb.append('@');        }        sb.append(IDN.toUnicode(host));        if (u.getPort() != -1) {            sb.append(':');            sb.append(u.getPort());        }                sb.append(u.getFile());        if (u.getRef() != null) {            sb.append('#');            sb.append(u.getRef());        }        return sb.toString();    } catch (Exception e) {        return null;    }}
public static void nutch_f1541_0(String[] args)
{    if (args.length != 1) {        System.err.println("Usage : URLUtil <url>");        return;    }    String url = args[0];    try {        System.out.println(URLUtil.getDomainName(new URL(url)));    } catch (MalformedURLException ex) {        ex.printStackTrace();    }}
public synchronized void nutch_f1542_1()
{    listener.crawlingStarted(crawl);    for (RemoteCommand command : remoteCommands) {        JobInfo jobInfo = executor.executeRemoteJob(command);        command.setJobInfo(jobInfo);                if (jobInfo.getState() == State.FAILED) {            listener.onCrawlError(crawl, jobInfo.getMsg());            return;        }        executedCommands.add(command);        listener.commandExecuted(crawl, command, calculateProgress());    }    listener.crawlingFinished(crawl);}
private int nutch_f1543_0()
{    if (CollectionUtils.isEmpty(remoteCommands)) {        return 0;    }    return (int) ((float) executedCommands.size() / (float) remoteCommands.size() * 100);}
public void nutch_f1544_0()
{    ClientConfig clientConfig = new DefaultClientConfig();    clientConfig.getFeatures().put(JSONConfiguration.FEATURE_POJO_MAPPING, true);    this.client = Client.create(clientConfig);    this.nutchResource = client.resource(instance.getUrl());}
public NutchStatus nutch_f1545_0()
{    return nutchResource.path("/admin").type(APPLICATION_JSON).get(NutchStatus.class);}
public ConnectionStatus nutch_f1546_0()
{    getNutchStatus();    return ConnectionStatus.CONNECTED;}
public String nutch_f1547_0(JobConfig jobConfig)
{    JobInfo jobInfo = nutchResource.path("/job/create").type(APPLICATION_JSON).post(JobInfo.class, jobConfig);    return jobInfo.getId();}
public JobInfo nutch_f1548_0(String jobId)
{    return nutchResource.path("/job/" + jobId).type(APPLICATION_JSON).get(JobInfo.class);}
public NutchInstance nutch_f1549_0()
{    return instance;}
public Map<String, String> nutch_f1550_0(String config)
{    return nutchResource.path("/config/" + config).type(APPLICATION_JSON).get(Map.class);}
public String nutch_f1551_0(SeedList seedList)
{    return nutchResource.path("/seed/create").type(APPLICATION_JSON).post(String.class, seedList);}
public JobConfig nutch_f1552_0()
{    return jobConfig;}
public void nutch_f1553_0(JobConfig jobConfig)
{    this.jobConfig = jobConfig;}
public JobInfo nutch_f1554_0()
{    return jobInfo;}
public void nutch_f1555_0(JobInfo jobInfo)
{    this.jobInfo = jobInfo;}
public Duration nutch_f1556_0()
{    return timeout;}
public void nutch_f1557_0(Duration timeout)
{    this.timeout = timeout;}
public String nutch_f1558_0()
{    String statusInfo = StringUtils.EMPTY;    if (jobInfo != null) {        statusInfo = MessageFormat.format("{0}", jobInfo.getState());    }    return MessageFormat.format("{0} status: {1}", jobConfig.getType(), statusInfo);}
public static RemoteCommandBuilder nutch_f1559_0(JobType jobType)
{    return new RemoteCommandBuilder().withJobType(jobType);}
public RemoteCommandBuilder nutch_f1560_0(JobType jobType)
{    jobConfig.setType(jobType);    return this;}
public RemoteCommandBuilder nutch_f1561_0(String configId)
{    jobConfig.setConfId(configId);    return this;}
public RemoteCommandBuilder nutch_f1562_0(String crawlId)
{    jobConfig.setCrawlId(crawlId);    return this;}
public RemoteCommandBuilder nutch_f1563_0(String key, String value)
{    jobConfig.setArgument(key, value);    return this;}
public RemoteCommandBuilder nutch_f1564_0(Duration timeout)
{    this.timeout = timeout;    return this;}
public RemoteCommand nutch_f1565_0()
{    RemoteCommand remoteCommand = new RemoteCommand(jobConfig);    remoteCommand.setTimeout(timeout);    return remoteCommand;}
public JobInfo nutch_f1566_1(RemoteCommand command)
{    try {        String jobId = client.executeJob(command.getJobConfig());        Future<JobInfo> chekerFuture = executor.submit(new JobStateChecker(jobId));        return chekerFuture.get(getTimeout(command), TimeUnit.MILLISECONDS);    } catch (Exception e) {                JobInfo jobInfo = new JobInfo();        jobInfo.setState(State.FAILED);        jobInfo.setMsg(ExceptionUtils.getStackTrace(e));        return jobInfo;    }}
private long nutch_f1567_0(RemoteCommand command)
{    if (command.getTimeout() == null) {        return DEFAULT_TIMEOUT_SEC * DateTimeConstants.MILLIS_PER_SECOND;    }    return command.getTimeout().getMillis();}
public void nutch_f1568_0(Duration requestDelay)
{    this.requestDelay = requestDelay;}
public JobInfo nutch_f1569_0() throws Exception
{    while (!Thread.interrupted()) {        JobInfo jobInfo = client.getJobInfo(jobId);        checkState(jobInfo != null, "Cannot get job info!");        State state = jobInfo.getState();        checkState(state != null, "Unknown job state!");        if (state == State.RUNNING || state == State.ANY || state == State.IDLE) {            Thread.sleep(requestDelay.getMillis());            continue;        }        return jobInfo;    }    return null;}
public List<RemoteCommand> nutch_f1570_0(Crawl crawl)
{    this.crawl = crawl;    this.remoteCommands = Lists.newArrayList();    remoteCommands.add(inject());    for (int i = 0; i < crawl.getNumberOfRounds(); i++) {        remoteCommands.addAll(createBatchCommands());    }    return remoteCommands;}
private List<RemoteCommand> nutch_f1571_0()
{    this.batchId = UUID.randomUUID().toString();    List<RemoteCommand> batchCommands = Lists.newArrayList();    batchCommands.add(createGenerateCommand());    batchCommands.add(createFetchCommand());    batchCommands.add(createParseCommand());    batchCommands.add(createUpdateDbCommand());    batchCommands.add(createIndexCommand());    return batchCommands;}
private RemoteCommand nutch_f1572_0()
{    RemoteCommandBuilder builder = RemoteCommandBuilder.instance(JobType.INJECT).withCrawlId(crawl.getCrawlId()).withArgument("url_dir", crawl.getSeedDirectory());    return builder.build();}
private RemoteCommand nutch_f1573_0()
{    return createBuilder(JobType.GENERATE).build();}
private RemoteCommand nutch_f1574_0()
{    return createBuilder(JobType.FETCH).withTimeout(Duration.standardSeconds(50)).build();}
private RemoteCommand nutch_f1575_0()
{    return createBuilder(JobType.PARSE).build();}
private RemoteCommand nutch_f1576_0()
{    return createBuilder(JobType.INDEX).build();}
private RemoteCommand nutch_f1577_0()
{    return createBuilder(JobType.UPDATEDB).build();}
private RemoteCommandBuilder nutch_f1578_0(JobType jobType)
{    return RemoteCommandBuilder.instance(jobType).withCrawlId(crawl.getCrawlId()).withArgument("batch", batchId);}
public Integer nutch_f1579_0()
{    return numberOfRounds;}
public void nutch_f1580_0(Integer numberOfRounds)
{    this.numberOfRounds = numberOfRounds;}
public String nutch_f1581_0()
{    return crawlId;}
public void nutch_f1582_0(String crawlId)
{    this.crawlId = crawlId;}
public CrawlStatus nutch_f1583_0()
{    return status;}
public void nutch_f1584_0(CrawlStatus status)
{    this.status = status;}
public String nutch_f1585_0()
{    return crawlName;}
public void nutch_f1586_0(String crawlName)
{    this.crawlName = crawlName;}
public SeedList nutch_f1587_0()
{    return seedList;}
public void nutch_f1588_0(SeedList seedList)
{    this.seedList = seedList;}
public Long nutch_f1589_0()
{    return id;}
public void nutch_f1590_0(Long id)
{    this.id = id;}
public String nutch_f1591_0()
{    return seedDirectory;}
public void nutch_f1592_0(String seedDirectory)
{    this.seedDirectory = seedDirectory;}
public int nutch_f1593_0()
{    return progress;}
public void nutch_f1594_0(int progress)
{    this.progress = progress;}
public void nutch_f1595_0(String key, String value)
{    args.put(key, value);}
public String nutch_f1596_0()
{    return crawlId;}
public void nutch_f1597_0(String crawlId)
{    this.crawlId = crawlId;}
public JobType nutch_f1598_0()
{    return type;}
public void nutch_f1599_0(JobType type)
{    this.type = type;}
public String nutch_f1600_0()
{    return confId;}
public void nutch_f1601_0(String confId)
{    this.confId = confId;}
public Map<String, Object> nutch_f1602_0()
{    return Collections.unmodifiableMap(args);}
public void nutch_f1603_0(Map<String, Object> args)
{    this.args = args;}
public String nutch_f1604_0()
{    return jobClassName;}
public void nutch_f1605_0(String jobClass)
{    this.jobClassName = jobClass;}
public String nutch_f1606_0()
{    return msg;}
public void nutch_f1607_0(String msg)
{    this.msg = msg;}
public State nutch_f1608_0()
{    return state;}
public void nutch_f1609_0(State state)
{    this.state = state;}
public Map<String, Object> nutch_f1610_0()
{    return result;}
public void nutch_f1611_0(Map<String, Object> result)
{    this.result = result;}
public Map<String, Object> nutch_f1612_0()
{    return args;}
public void nutch_f1613_0(Map<String, Object> args)
{    this.args = args;}
public String nutch_f1614_0()
{    return confId;}
public void nutch_f1615_0(String confId)
{    this.confId = confId;}
public String nutch_f1616_0()
{    return id;}
public void nutch_f1617_0(String id)
{    this.id = id;}
public String nutch_f1618_0()
{    return crawlId;}
public void nutch_f1619_0(String crawlId)
{    this.crawlId = crawlId;}
public String nutch_f1620_0()
{    return type;}
public void nutch_f1621_0(String type)
{    this.type = type;}
public Date nutch_f1622_0()
{    return startDate;}
public void nutch_f1623_0(Date startDate)
{    this.startDate = startDate;}
public Set<String> nutch_f1624_0()
{    return configuration;}
public void nutch_f1625_0(Set<String> configuration)
{    this.configuration = configuration;}
public Collection<JobInfo> nutch_f1626_0()
{    return jobs;}
public void nutch_f1627_0(Collection<JobInfo> jobs)
{    this.jobs = jobs;}
public Collection<JobInfo> nutch_f1628_0()
{    return runningJobs;}
public void nutch_f1629_0(Collection<JobInfo> runningJobs)
{    this.runningJobs = runningJobs;}
public NutchClient nutch_f1630_0(NutchInstance instance)
{    try {        return cache.get(instance);    } catch (ExecutionException e) {        throw new IllegalStateException(e);    }}
public NutchClient nutch_f1631_0(NutchInstance key) throws Exception
{    return new NutchClientImpl(key);}
public Dao<T, ID> nutch_f1632_0(Class<T> clazz)
{    try {        Dao<T, ID> dao = DaoFactory.createDao(connectionSource, clazz);        register(dao);        return dao;    } catch (SQLException e) {        throw new RuntimeException(e);    }}
private void nutch_f1633_0(Dao<T, ID> dao)
{    synchronized (registredDaos) {        registredDaos.add(dao);    }}
public List<Dao<?, ?>> nutch_f1634_0()
{    synchronized (registredDaos) {        return Collections.unmodifiableList(registredDaos);    }}
private void nutch_f1635_0()
{    if (configuredDaos == null) {        throw new IllegalStateException("configuredDaos was not set in " + getClass().getSimpleName());    }    for (Dao<?, ?> dao : configuredDaos) {        createTableForDao(dao);    }}
private void nutch_f1636_0(Dao<?, ?> dao)
{    DatabaseTableConfig<?> tableConfig = getTableConfig(dao);    createTableIfNotExists(tableConfig);}
private DatabaseTableConfig<?> nutch_f1637_0(Dao<?, ?> dao)
{    Class<?> clazz = dao.getDataClass();    DatabaseTableConfig<?> tableConfig = null;    if (dao instanceof BaseDaoImpl) {        tableConfig = ((BaseDaoImpl<?, ?>) dao).getTableConfig();    }    if (tableConfig == null) {        return getConfigFromClass(clazz);    }    return tableConfig;}
private DatabaseTableConfig<?> nutch_f1638_0(Class<?> clazz)
{    try {        return DatabaseTableConfig.fromClass(connectionSource, clazz);    } catch (SQLException e) {        throw new RuntimeException(e);    }}
private void nutch_f1639_0(DatabaseTableConfig<?> tableConfig)
{    try {        TableUtils.createTableIfNotExists(connectionSource, tableConfig);    } catch (SQLException e) {        throw new RuntimeException(e);    }}
public List<NutchInstance> nutch_f1640_0()
{    return instances;}
public void nutch_f1641_0(List<NutchInstance> instances)
{    this.instances = instances;}
public Executor nutch_f1642_0()
{        ThreadPoolTaskExecutor executor = new ThreadPoolTaskExecutor();    executor.setCorePoolSize(7);    executor.setMaxPoolSize(42);    executor.setQueueCapacity(11);    executor.setThreadNamePrefix("SpringExecutor-");    executor.initialize();    return executor;}
public JdbcConnectionSource nutch_f1643_0() throws SQLException
{    JdbcConnectionSource source = new JdbcConnectionSource("jdbc:h2:~/.nutch/config", new H2DatabaseType());    source.initialize();    return source;}
public CustomDaoFactory nutch_f1644_0() throws SQLException
{    return new CustomDaoFactory(getConnectionSource());}
public Dao<NutchInstance, Long> nutch_f1645_0() throws SQLException
{    return getDaoFactory().createDao(NutchInstance.class);}
public Dao<SeedList, Long> nutch_f1646_0() throws SQLException
{    return getDaoFactory().createDao(SeedList.class);}
public Dao<SeedUrl, Long> nutch_f1647_0() throws SQLException
{    return getDaoFactory().createDao(SeedUrl.class);}
public Dao<Crawl, Long> nutch_f1648_0() throws SQLException
{    return getDaoFactory().createDao(Crawl.class);}
public CustomTableCreator nutch_f1649_0() throws SQLException
{    return new CustomTableCreator(getConnectionSource(), getDaoFactory().getCreatedDaos());}
public void nutch_f1650_0(String name)
{    this.name = name;}
public String nutch_f1651_0()
{    return this.name;}
public String nutch_f1652_0()
{    return value;}
public void nutch_f1653_0(String value)
{    this.value = value;}
public String nutch_f1654_0()
{    return name;}
public void nutch_f1655_0(String name)
{    this.name = name;}
public String nutch_f1656_0()
{    return host;}
public void nutch_f1657_0(String username)
{    this.username = username;}
public String nutch_f1658_0()
{    return username;}
public void nutch_f1659_0(String host)
{    this.host = host;}
public Integer nutch_f1660_0()
{    return port;}
public void nutch_f1661_0(Integer port)
{    this.port = port;}
public ConnectionStatus nutch_f1662_0()
{    return connectionStatus;}
public void nutch_f1663_0(ConnectionStatus connectionStatus)
{    this.connectionStatus = connectionStatus;}
public URI nutch_f1664_0()
{    try {        return new URI("http", null, host, port, null, null, null);    } catch (URISyntaxException e) {        throw new IllegalStateException("Cannot parse url parameters", e);    }}
public String nutch_f1665_0()
{    return password;}
public void nutch_f1666_0(String password)
{    this.password = password;}
public Long nutch_f1667_0()
{    return id;}
public void nutch_f1668_0(Long id)
{    this.id = id;}
public Long nutch_f1669_0()
{    return id;}
public void nutch_f1670_0(Long id)
{    this.id = id;}
public int nutch_f1671_0()
{    if (CollectionUtils.isEmpty(seedUrls)) {        return 0;    }    return seedUrls.size();}
public Collection<SeedUrl> nutch_f1672_0()
{    return seedUrls;}
public void nutch_f1673_0(Collection<SeedUrl> seedUrls)
{    this.seedUrls = seedUrls;}
public String nutch_f1674_0()
{    return name;}
public void nutch_f1675_0(String name)
{    this.name = name;}
public int nutch_f1676_0()
{    final int prime = 31;    int result = 1;    result = prime * result + ((id == null) ? 0 : id.hashCode());    return result;}
public boolean nutch_f1677_0(Object obj)
{    if (this == obj)        return true;    if (obj == null)        return false;    if (getClass() != obj.getClass())        return false;    SeedList other = (SeedList) obj;    if (id == null) {        if (other.id != null)            return false;    } else if (!id.equals(other.id))        return false;    return true;}
public Long nutch_f1678_0()
{    return id;}
public void nutch_f1679_0(Long id)
{    this.id = id;}
public String nutch_f1680_0()
{    return url;}
public void nutch_f1681_0(String url)
{    this.url = url;}
public SeedList nutch_f1682_0()
{    return seedList;}
public void nutch_f1683_0(SeedList seedList)
{    this.seedList = seedList;}
public int nutch_f1684_0()
{    final int prime = 31;    int result = 1;    result = prime * result + ((id == null) ? 0 : id.hashCode());    return result;}
public boolean nutch_f1685_0(Object obj)
{    if (this == obj)        return true;    if (obj == null)        return false;    if (getClass() != obj.getClass())        return false;    SeedUrl other = (SeedUrl) obj;    if (id == null) {        if (other.id != null)            return false;    } else if (!id.equals(other.id))        return false;    return true;}
public Class<? extends WebPage> nutch_f1686_0()
{    return DashboardPage.class;}
public void nutch_f1687_0()
{    super.init();    BootstrapSettings settings = new BootstrapSettings();    Bootstrap.install(this, settings);    configureTheme(settings);    getComponentInstantiationListeners().add(new SpringComponentInjector(this, context));}
private void nutch_f1688_0(BootstrapSettings settings)
{    Theme theme = new Theme(THEME_NAME, BootstrapCssReference.instance(), FontAwesomeCssReference.instance(), NutchUiCssReference.instance());    settings.setThemeProvider(new SingleThemeProvider(theme));}
public void nutch_f1689_0(ApplicationContext applicationContext) throws BeansException
{    this.context = applicationContext;}
public static void nutch_f1690_0(String[] args) throws Exception
{    CommandLineParser parser = new GnuParser();    Options options = createWebAppOptions();    CommandLine commandLine = null;    HelpFormatter formatter = new HelpFormatter();    try {        commandLine = parser.parse(options, args);    } catch (Exception e) {        formatter.printHelp("NutchUiServer", options, true);        StringUtils.stringifyException(e);    }    if (commandLine.hasOption("help")) {        formatter.printHelp("NutchUiServer", options, true);        return;    }    if (commandLine.hasOption(CMD_PORT)) {        port = Integer.parseInt(commandLine.getOptionValue(CMD_PORT));    }    startServer();}
private static void nutch_f1691_0() throws Exception, InterruptedException
{    Server server = new Server(port);    Context context = new Context(server, "/", Context.SESSIONS);    context.addServlet(DefaultServlet.class, "/*");    context.addEventListener(new ContextLoaderListener(getContext()));    context.addEventListener(new RequestContextListener());    WicketFilter filter = new WicketFilter();    filter.setFilterPath("/");    FilterHolder holder = new FilterHolder(filter);    holder.setInitParameter("applicationFactoryClassName", APP_FACTORY_NAME);    context.addFilter(holder, "/*", Handler.DEFAULT);    server.setHandler(context);    server.start();    server.join();}
private static WebApplicationContext nutch_f1692_0()
{    AnnotationConfigWebApplicationContext context = new AnnotationConfigWebApplicationContext();    context.setConfigLocation(CONFIG_LOCATION);    return context;}
private static Options nutch_f1693_0()
{    Options options = new Options();    Option helpOpt = new Option("h", "help", false, "show this help message");    OptionBuilder.withDescription("Port to run the WebApplication on.");    OptionBuilder.hasOptionalArg();    OptionBuilder.withArgName("port number");    options.addOption(OptionBuilder.create(CMD_PORT));    options.addOption(helpOpt);    return options;}
protected Component nutch_f1694_0()
{    DropDownButton userMenu = new NavbarDropDownButton(Model.of("Username")) {        /**         */        private static final long serialVersionUID = 1L;        @Override        protected List<AbstractLink> newSubMenuButtons(final String buttonMarkupId) {            List<AbstractLink> subMenu = Lists.newArrayList();            subMenu.add(new MenuBookmarkablePageLink<Void>(UserSettingsPage.class, new ResourceModel("navbar.userMenu.settings")).setIconType(FontAwesomeIconType.gear));            subMenu.add(new MenuDivider());            subMenu.add(new MenuBookmarkablePageLink<Void>(LogOutPage.class, new ResourceModel("navbar.userMenu.logout")).setIconType(FontAwesomeIconType.power_off));            return subMenu;        }    }.setIconType(FontAwesomeIconType.user);    return userMenu;}
protected List<AbstractLink> nutch_f1695_0(final String buttonMarkupId)
{    List<AbstractLink> subMenu = Lists.newArrayList();    subMenu.add(new MenuBookmarkablePageLink<Void>(UserSettingsPage.class, new ResourceModel("navbar.userMenu.settings")).setIconType(FontAwesomeIconType.gear));    subMenu.add(new MenuDivider());    subMenu.add(new MenuBookmarkablePageLink<Void>(LogOutPage.class, new ResourceModel("navbar.userMenu.logout")).setIconType(FontAwesomeIconType.power_off));    return subMenu;}
protected Component nutch_f1696_0()
{    IModel<String> instanceName = PropertyModel.of(currentInstance, "name");    DropDownButton instancesMenu = new NavbarDropDownButton(instanceName) {        /**         */        private static final long serialVersionUID = 1L;        @Override        protected List<AbstractLink> newSubMenuButtons(String buttonMarkupId) {            List<NutchInstance> instances = instanceService.getInstances();            List<AbstractLink> subMenu = Lists.newArrayList();            for (NutchInstance instance : instances) {                subMenu.add(new Link<NutchInstance>(buttonMarkupId, Model.of(instance)) {                    /**                     */                    private static final long serialVersionUID = 1L;                    @Override                    public void onClick() {                        currentInstance.setObject(getModelObject());                        setResponsePage(DashboardPage.class);                    }                }.setBody(Model.of(instance.getName())));            }            return subMenu;        }    }.setIconType(FontAwesomeIconType.gears);    return instancesMenu;}
protected List<AbstractLink> nutch_f1697_0(String buttonMarkupId)
{    List<NutchInstance> instances = instanceService.getInstances();    List<AbstractLink> subMenu = Lists.newArrayList();    for (NutchInstance instance : instances) {        subMenu.add(new Link<NutchInstance>(buttonMarkupId, Model.of(instance)) {            /**             */            private static final long serialVersionUID = 1L;            @Override            public void onClick() {                currentInstance.setObject(getModelObject());                setResponsePage(DashboardPage.class);            }        }.setBody(Model.of(instance.getName())));    }    return subMenu;}
public void nutch_f1698_0()
{    currentInstance.setObject(getModelObject());    setResponsePage(DashboardPage.class);}
private void nutch_f1699_0(Class<P> page, String label, IconType icon)
{    Component button = new NavbarButton<Void>(page, Model.of(getString(label))).setIconType(icon);    navbar.addComponents(NavbarComponents.transform(LEFT, button));}
protected NutchInstance nutch_f1700_0()
{    return currentInstance.getObject();}
public void nutch_f1701_0(NutchInstance instance)
{    super.setObject(instance);    getSession().setAttribute("instanceId", instance.getId());}
protected NutchInstance nutch_f1702_0()
{    Long instanceId = (Long) getSession().getAttribute("instanceId");    if (instanceId == null) {        return getFirstInstance();    }    return instanceService.getInstance(instanceId);}
private NutchInstance nutch_f1703_0()
{    return Iterables.getFirst(instanceService.getInstances(), null);}
public static NutchUiCssReference nutch_f1704_0()
{    return INSTANCE;}
protected void nutch_f1705_0()
{    super.onInitialize();    setOutputMarkupId(true);    add(new LabelBehavior(new EnumCssModel(getModel())));}
public LabelType nutch_f1706_0()
{    LabelType labelType = labelTypeMap.get(model.getObject());    if (labelType == null) {        return LabelType.Default;    }    return labelType;}
public static ColorEnumLabelBuilder<E> nutch_f1707_0(String id)
{    return new ColorEnumLabelBuilder<>(id);}
public ColorEnumLabelBuilder<E> nutch_f1708_0(IModel<E> model)
{    this.model = model;    return this;}
public ColorEnumLabelBuilder<E> nutch_f1709_0(E e, LabelType type)
{    labelTypeMap.put(e, type);    return this;}
public ColorEnumLabel<E> nutch_f1710_0()
{    return new ColorEnumLabel<>(id, model, labelTypeMap);}
protected IModel<T> nutch_f1711_0(T object)
{    return new CompoundPropertyModel<>(object);}
protected void nutch_f1712_0(AjaxRequestTarget target, Form<?> ajaxForm)
{    crawlService.saveCrawl(form.getModelObject());    target.add(this.getPage());}
protected void nutch_f1713_0(AjaxRequestTarget target, Form<?> form)
{    target.add(notificationPanel);}
public void nutch_f1714_0(IModel<Crawl> model)
{    form.setModel(model);}
private List<Integer> nutch_f1715_0()
{    List<Integer> numbers = Lists.newArrayList();    for (int i = 1; i <= MAX_ROUNDS; i++) {        numbers.add(i);    }    return numbers;}
protected Iterator<IModel<Crawl>> nutch_f1716_0()
{    return new CpmIteratorAdapter<>(crawlService.getCrawls());}
protected void nutch_f1717_0(Item<Crawl> item)
{    populateCrawlRow(item);}
public void nutch_f1718_0(AjaxRequestTarget target)
{    editCrawl(target, new CompoundPropertyModel<>(createNewCrawl()));}
private void nutch_f1719_0(Item<Crawl> item)
{    item.add(new AjaxLink<Crawl>("edit", item.getModel()) {        @Override        public void onClick(AjaxRequestTarget target) {            editCrawl(target, getModel());        }    }.add(new Label("crawlName")));    item.add(new Label("seedList.name"));    item.add(new Label("progress"));    item.add(createStatusLabel());    item.add(new Link<Crawl>("start", item.getModel()) {        @Override        public void onClick() {            crawlService.startCrawl(getModelObject().getId(), getCurrentInstance());        }    });    item.add(new Link<Crawl>("delete", item.getModel()) {        @Override        public void onClick() {            crawlService.deleteCrawl(getModelObject().getId());        }    });}
public void nutch_f1720_0(AjaxRequestTarget target)
{    editCrawl(target, getModel());}
public void nutch_f1721_0()
{    crawlService.startCrawl(getModelObject().getId(), getCurrentInstance());}
public void nutch_f1722_0()
{    crawlService.deleteCrawl(getModelObject().getId());}
private void nutch_f1723_0(AjaxRequestTarget target, IModel<Crawl> model)
{    crawlPanel.setModel(model);    target.add(crawlPanel);    crawlPanel.appendShowDialogJavaScript(target);}
private Crawl nutch_f1724_0()
{    return new Crawl();}
private EnumLabel<CrawlStatus> nutch_f1725_0()
{    return new ColorEnumLabelBuilder<CrawlStatus>("status").withEnumColor(NEW, Default).withEnumColor(ERROR, Danger).withEnumColor(FINISHED, Success).withEnumColor(CRAWLING, Info).build();}
protected Integer nutch_f1726_0()
{    NutchInstance currentInstance = getCurrentInstance();    Long id = currentInstance.getId();    NutchStatus nutchStatus = nutchService.getNutchStatus(id);    return nutchStatus.getRunningJobs().size();}
protected void nutch_f1727_0(AjaxRequestTarget target, Form<?> ajaxForm)
{    instanceService.saveInstance(form.getModelObject());    target.add(this.getPage());}
protected void nutch_f1728_0(AjaxRequestTarget target, Form<?> form)
{    target.add(notificationPanel);}
public void nutch_f1729_0(IModel<NutchInstance> model)
{    form.setModel(model);}
private RefreshingView<NutchInstance> nutch_f1730_0()
{    RefreshingView<NutchInstance> instances = new RefreshingView<NutchInstance>("instances") {        @Override        protected Iterator<IModel<NutchInstance>> getItemModels() {            return new CpmIteratorAdapter<>(instanceService.getInstances());        }        @Override        protected void populateItem(Item<NutchInstance> item) {            populateInstanceRow(item);        }    };    return instances;}
protected Iterator<IModel<NutchInstance>> nutch_f1731_0()
{    return new CpmIteratorAdapter<>(instanceService.getInstances());}
protected void nutch_f1732_0(Item<NutchInstance> item)
{    populateInstanceRow(item);}
private AjaxLink<NutchInstance> nutch_f1733_0()
{    return new AjaxLink<NutchInstance>("addInstance") {        @Override        public void onClick(AjaxRequestTarget target) {            instancePanel.setModel(new CompoundPropertyModel<>(new NutchInstance()));            target.add(instancePanel);            instancePanel.appendShowDialogJavaScript(target);        }    };}
public void nutch_f1734_0(AjaxRequestTarget target)
{    instancePanel.setModel(new CompoundPropertyModel<>(new NutchInstance()));    target.add(instancePanel);    instancePanel.appendShowDialogJavaScript(target);}
private void nutch_f1735_0(final Item<NutchInstance> item)
{    item.add(new AjaxLink<NutchInstance>("editInstance") {        @Override        public void onClick(AjaxRequestTarget target) {            instancePanel.setModel(item.getModel());            target.add(instancePanel);            instancePanel.appendShowDialogJavaScript(target);        }    }.add(new Label("name")));    item.add(new Label("host"));    item.add(new Label("username"));    item.add(createStatusLabel());    item.add(new AjaxLink<NutchInstance>("instanceDelete", item.getModel()) {        @Override        public void onClick(AjaxRequestTarget target) {            instanceService.removeInstance(getModelObject().getId());            target.add(instancesTable);        }    });}
public void nutch_f1736_0(AjaxRequestTarget target)
{    instancePanel.setModel(item.getModel());    target.add(instancePanel);    instancePanel.appendShowDialogJavaScript(target);}
public void nutch_f1737_0(AjaxRequestTarget target)
{    instanceService.removeInstance(getModelObject().getId());    target.add(instancesTable);}
private ColorEnumLabel<ConnectionStatus> nutch_f1738_0()
{    return new ColorEnumLabelBuilder<ConnectionStatus>("connectionStatus").withEnumColor(CONNECTED, Success).withEnumColor(CONNECTING, Info).withEnumColor(DISCONNECTED, Danger).build();}
protected Iterator<IModel<SeedList>> nutch_f1739_0()
{    return new CpmIteratorAdapter<>(seedListService.findAll());}
protected void nutch_f1740_0(final Item<SeedList> item)
{    PageParameters params = new PageParameters();    params.add("id", item.getModelObject().getId());    Link<Void> edit = new BookmarkablePageLink<>("edit", SeedPage.class, params);    edit.add(new Label("name"));    item.add(edit);    item.add(new Label("seedUrlsCount"));    item.add(new Link<SeedList>("delete", item.getModel()) {        @Override        public void onClick() {            seedListService.delete(item.getModelObject().getId());        }    });}
public void nutch_f1741_0()
{    seedListService.delete(item.getModelObject().getId());}
protected SeedList nutch_f1742_0()
{    Long seedListId = parameters.get("id").toLongObject();    return seedListService.getSeedList(seedListId);}
public void nutch_f1743_0(IModel<SeedList> model)
{    setModel(new CompoundPropertyModel<>(model));    addBaseForm();    addSeedUrlsList();    addUrlForm();}
private void nutch_f1744_0()
{    Form<SeedList> form = new Form<SeedList>("seedList", getModel()) {        @Override        protected void onSubmit() {            seedListService.save(getModelObject());            setResponsePage(SeedListsPage.class);        }    };    form.add(new TextField<String>("name"));    add(form);}
protected void nutch_f1745_0()
{    seedListService.save(getModelObject());    setResponsePage(SeedListsPage.class);}
private void nutch_f1746_0()
{    seedUrlsTable = new WebMarkupContainer("seedUrlsTable");    seedUrlsTable.setOutputMarkupId(true);    RefreshingView<SeedUrl> seedUrls = new RefreshingView<SeedUrl>("seedUrls") {        @Override        protected Iterator<IModel<SeedUrl>> getItemModels() {            return new CpmIteratorAdapter<>(getModelObject().getSeedUrls());        }        @Override        protected void populateItem(Item<SeedUrl> item) {            item.add(new Label("url"));            item.add(new AjaxLink<SeedUrl>("delete", item.getModel()) {                @Override                public void onClick(AjaxRequestTarget target) {                    deleteSeedUrl(getModelObject());                    target.add(seedUrlsTable);                }            });        }    };    seedUrlsTable.add(seedUrls);    add(seedUrlsTable);}
protected Iterator<IModel<SeedUrl>> nutch_f1747_0()
{    return new CpmIteratorAdapter<>(getModelObject().getSeedUrls());}
protected void nutch_f1748_0(Item<SeedUrl> item)
{    item.add(new Label("url"));    item.add(new AjaxLink<SeedUrl>("delete", item.getModel()) {        @Override        public void onClick(AjaxRequestTarget target) {            deleteSeedUrl(getModelObject());            target.add(seedUrlsTable);        }    });}
public void nutch_f1749_0(AjaxRequestTarget target)
{    deleteSeedUrl(getModelObject());    target.add(seedUrlsTable);}
private void nutch_f1750_0()
{    urlForm = new Form<>("urlForm", CompoundPropertyModel.of(Model.of(new SeedUrl())));    urlForm.setOutputMarkupId(true);    urlForm.add(new TextField<String>("url"));    urlForm.add(new AjaxSubmitLink("addUrl", urlForm) {        @Override        protected void onSubmit(AjaxRequestTarget target, Form<?> form) {            addSeedUrl();            urlForm.setModelObject(new SeedUrl());            target.add(urlForm);            target.add(seedUrlsTable);        }    });    add(urlForm);}
protected void nutch_f1751_0(AjaxRequestTarget target, Form<?> form)
{    addSeedUrl();    urlForm.setModelObject(new SeedUrl());    target.add(urlForm);    target.add(seedUrlsTable);}
private void nutch_f1752_0()
{    SeedUrl url = urlForm.getModelObject();    SeedList seedList = getModelObject();    url.setSeedList(seedList);    seedList.getSeedUrls().add(url);}
private void nutch_f1753_0(SeedUrl url)
{    SeedList seedList = getModelObject();    seedList.getSeedUrls().remove(url);}
protected Iterator<IModel<NutchConfig>> nutch_f1754_0()
{    return new CpmIteratorAdapter<>(convertNutchConfig(nutchService.getNutchConfig(getCurrentInstance().getId())));}
protected void nutch_f1755_0(Item<NutchConfig> item)
{    item.add(new Label("name"));    item.add(new TextField<String>("value"));}
private List<NutchConfig> nutch_f1756_0(Map<String, String> map)
{    List<NutchConfig> listNutchConfigs = new LinkedList<>();    for (String key : map.keySet()) {        NutchConfig conf = new NutchConfig();        conf.setName(key);        conf.setValue(map.get(key));        listNutchConfigs.add(conf);    }    return listNutchConfigs;}
public void nutch_f1757_1(Long crawlId, NutchInstance instance)
{    Crawl crawl = null;    try {        crawl = crawlDao.queryForId(crawlId);        if (crawl.getCrawlId() == null) {            crawl.setCrawlId("crawl-" + crawlId.toString());        }        NutchClient client = nutchClientFactory.getClient(instance);        String seedDirectory = client.createSeed(crawl.getSeedList());        crawl.setSeedDirectory(seedDirectory);        List<RemoteCommand> commands = commandFactory.createCommands(crawl);        RemoteCommandExecutor executor = new RemoteCommandExecutor(client);        CrawlingCycle cycle = new CrawlingCycle(this, executor, crawl, commands);        cycle.executeCrawlCycle();    } catch (Exception e) {        crawl.setStatus(CrawlStatus.ERROR);        saveCrawl(crawl);            }}
public List<Crawl> nutch_f1758_0()
{    try {        return crawlDao.queryForAll();    } catch (SQLException e) {        throw new RuntimeException(e);    }}
public void nutch_f1759_0(Crawl crawl)
{    try {        crawlDao.createOrUpdate(crawl);    } catch (SQLException e) {        throw new RuntimeException(e);    }}
public void nutch_f1760_0(Long crawlId)
{    try {        crawlDao.deleteById(crawlId);    } catch (SQLException e) {        throw new RuntimeException(e);    }}
public void nutch_f1761_0(Crawl crawl)
{    crawl.setStatus(CrawlStatus.CRAWLING);    crawl.setProgress(0);    saveCrawl(crawl);}
public void nutch_f1762_0(Crawl crawl, String msg)
{    crawl.setStatus(CrawlStatus.ERROR);    saveCrawl(crawl);}
public void nutch_f1763_0(Crawl crawl, RemoteCommand command, int progress)
{    crawl.setProgress(progress);    saveCrawl(crawl);}
public void nutch_f1764_0(Crawl crawl)
{    crawl.setStatus(CrawlStatus.FINISHED);    saveCrawl(crawl);}
public List<NutchInstance> nutch_f1765_0()
{    try {        return instancesDao.queryForAll();    } catch (SQLException e) {        throw new RuntimeException(e);    }}
public NutchInstance nutch_f1766_0(Long id)
{    try {        return instancesDao.queryForId(id);    } catch (SQLException e) {        throw new RuntimeException(e);    }}
public void nutch_f1767_0(NutchInstance instance)
{    try {        instancesDao.createOrUpdate(instance);    } catch (SQLException e) {        throw new RuntimeException(e);    }}
public void nutch_f1768_0(Long id)
{    try {        instancesDao.deleteById(id);    } catch (SQLException e) {        throw new RuntimeException(e);    }}
public ConnectionStatus nutch_f1769_1(Long instanceId)
{    NutchInstance instance = instanceService.getInstance(instanceId);    try {        NutchStatus nutchStatus = nutchClientFactory.getClient(instance).getNutchStatus();        if (nutchStatus.getStartDate() != null) {            return ConnectionStatus.CONNECTED;        }    } catch (Exception e) {        if (e.getCause() instanceof ConnectException) {            return ConnectionStatus.DISCONNECTED;        }            }    return null;}
public Map<String, String> nutch_f1770_0(Long instanceId)
{    NutchInstance instance = instanceService.getInstance(instanceId);    try {        return nutchClientFactory.getClient(instance).getNutchConfig("default");    } catch (ClientHandlerException exception) {        return Collections.emptyMap();    }}
public NutchStatus nutch_f1771_0(Long instanceId)
{    NutchInstance instance = instanceService.getInstance(instanceId);    return nutchClientFactory.getClient(instance).getNutchStatus();}
public void nutch_f1772_0(SeedList seedList)
{    try {        seedListDao.createOrUpdate(seedList);    } catch (SQLException e) {        throw new RuntimeException(e);    }}
public void nutch_f1773_0(Long seedListId)
{    try {        seedListDao.deleteById(seedListId);    } catch (SQLException e) {        throw new RuntimeException(e);    }}
public List<SeedList> nutch_f1774_0()
{    try {        return seedListDao.queryForAll();    } catch (SQLException e) {        throw new RuntimeException(e);    }}
public SeedList nutch_f1775_0(Long seedListId)
{    try {        return seedListDao.queryForId(seedListId);    } catch (SQLException e) {        throw new RuntimeException(e);    }}
public Configuration nutch_f1776_0()
{    return this.conf;}
public void nutch_f1777_0(Configuration conf)
{    this.conf = conf;}
public NutchDocument nutch_f1778_1(NutchDocument doc, Parse parse, Text url, CrawlDatum datum, Inlinks inlinks) throws IndexingException
{    String[] metadata = parse.getData().getParseMeta().getValues(Any23ParseFilter.ANY23_TRIPLES);    if (metadata != null) {        for (String triple : metadata) {            Pattern pattern = Pattern.compile("^([^ ]+) ([^ ]+) (.+) \\.");            Matcher matcher = pattern.matcher(triple);            if (matcher.find()) {                Map<String, String> map = new HashMap<>();                map.put("node", matcher.group(1));                map.put("key", matcher.group(2));                map.put("short_key", keyToShortKey(matcher.group(2)));                map.put("value", matcher.group(3));                doc.add("structured_data", map);            } else {                            }        }    }    return doc;}
private String nutch_f1779_0(String key)
{    if (key.startsWith("<") && key.endsWith(">")) {        key = key.substring(1, key.length() - 1);    }    String[] keyParts = key.split("/");    String[] keySubParts = keyParts[keyParts.length - 1].split("#");    return keySubParts[keySubParts.length - 1];}
private Set<String> nutch_f1780_0()
{    return triples;}
private void nutch_f1781_1(String url, String htmlContent, String contentType, String... extractorNames) throws URISyntaxException, IOException, TripleHandlerException
{    Any23 any23 = new Any23(extractorNames);    any23.setMIMETypeDetector(null);    ByteArrayOutputStream baos = new ByteArrayOutputStream();    try {        TripleHandler tHandler = new NTriplesWriter(baos);        BenchmarkTripleHandler bHandler = new BenchmarkTripleHandler(tHandler);        try {            any23.extract(htmlContent, url, contentType, "UTF-8", bHandler);        } catch (IOException e) {                    } catch (ExtractionException e) {                    } finally {            tHandler.close();            bHandler.close();        }                String n3 = baos.toString("UTF-8");        String[] triplesStrings = n3.split("\n");        Collections.addAll(triples, triplesStrings);    } catch (IOException e) {            }}
public Configuration nutch_f1782_0()
{    return this.conf;}
public void nutch_f1783_0(Configuration conf)
{    this.conf = conf;}
public ParseResult nutch_f1784_1(Content content, ParseResult parseResult, HTMLMetaTags metaTags, DocumentFragment doc)
{    String[] extractorNames = conf.getStrings(ANY_23_EXTRACTORS_CONF, "html-head-meta");    String[] supportedContentTypes = conf.getStrings(ANY_23_CONTENT_TYPES_CONF, "text/html", "application/xhtml+xml");    String contentType = content.getContentType();    if (supportedContentTypes != null && !Arrays.asList(supportedContentTypes).contains(contentType)) {                return parseResult;    }    Any23Parser parser;    try {        String htmlContent = new String(content.getContent(), Charset.forName("UTF-8"));        parser = new Any23Parser(content.getUrl(), htmlContent, contentType, extractorNames);    } catch (TripleHandlerException e) {        throw new RuntimeException("Error running Any23 parser: " + e.getMessage());    }    Set<String> triples = parser.getTriples();    Parse parse = parseResult.get(content.getUrl());    Metadata metadata = parse.getData().getParseMeta();    for (String triple : triples) {        metadata.add(ANY23_TRIPLES, triple);    }    return parseResult;}
public void nutch_f1785_0() throws Exception
{    Configuration conf = NutchConfiguration.create();    Any23IndexingFilter filter = new Any23IndexingFilter();    filter.setConf(conf);    Assert.assertNotNull(filter);    NutchDocument doc = new NutchDocument();    ParseData parseData = new ParseData(ParseStatus.STATUS_SUCCESS, "The Foo Page", new Outlink[] {}, new Metadata());    ParseImpl parse = new ParseImpl("test page", parseData);    String[] triples = new String[] { "<http://dbpedia.org/resource/Z\u00FCrich> <http://www.w3.org/2002/07/owl#sameAs> <http://rdf.freebase.com/ns/m.08966> .", "<http://dbpedia.org/resource/Z\u00FCrich> <http://dbpedia.org/property/yearHumidity> \"77\" .", "<http://dbpedia.org/resource/Z\u00FCrich> <http://www.w3.org/2000/01/rdf-schema#label> \"Zurique\"@pt ." };    for (String triple : triples) {        parse.getData().getParseMeta().add(Any23ParseFilter.ANY23_TRIPLES, triple);    }    try {        doc = filter.filter(doc, parse, new Text("http://nutch.apache.org/"), new CrawlDatum(), new Inlinks());    } catch (Exception e) {        e.printStackTrace();        Assert.fail(e.getMessage());    }    List<Object> docTriples = doc.getField(Any23IndexingFilter.STRUCTURED_DATA).getValues();    Assert.assertEquals(docTriples.size(), triples.length);    Object triple = docTriples.get(0);    Assert.assertTrue(triple instanceof Map<?, ?>);    @SuppressWarnings("unchecked")    Map<String, String> structuredData = (Map<String, String>) triple;    Assert.assertEquals(structuredData.get("node"), "<http://dbpedia.org/resource/Z\u00FCrich>");    Assert.assertEquals(structuredData.get("key"), "<http://www.w3.org/2002/07/owl#sameAs>");    Assert.assertEquals(structuredData.get("short_key"), "sameAs");    Assert.assertEquals(structuredData.get("value"), "<http://rdf.freebase.com/ns/m.08966>");    triple = docTriples.get(1);    Assert.assertTrue(triple instanceof Map<?, ?>);    structuredData = (Map<String, String>) triple;    Assert.assertEquals(structuredData.get("node"), "<http://dbpedia.org/resource/Z\u00FCrich>");    Assert.assertEquals(structuredData.get("key"), "<http://dbpedia.org/property/yearHumidity>");    Assert.assertEquals(structuredData.get("short_key"), "yearHumidity");    Assert.assertEquals(structuredData.get("value"), "\"77\"");}
public void nutch_f1786_0()
{    this.conf = NutchConfiguration.create();    conf.set("file.content.limit", "-1");    conf.set("parser.timeout", "-1");    conf.set(Any23ParseFilter.ANY_23_EXTRACTORS_CONF, "html-embedded-jsonld,html-head-icbm,html-head-links," + "html-head-meta,html-head-title,html-mf-adr,html-mf-geo,html-mf-hcalendar,html-mf-hcard," + "html-mf-hlisting,html-mf-hrecipe,html-mf-hresume,html-mf-hreview,html-mf-hreview-aggregate," + "html-mf-license,html-mf-species,html-mf-xfn,html-microdata,html-rdfa11,html-xpath");    conf.set(Any23ParseFilter.ANY_23_CONTENT_TYPES_CONF, "text/html");}
public void nutch_f1787_0() throws IOException, ParserNotFound, ParseException
{    String[] triplesArray = getTriples(file1);    Assert.assertEquals("We expect 117 tab-separated triples extracted by the filter", EXPECTED_TRIPLES_1, triplesArray.length);}
public void nutch_f1788_0() throws ParserNotFound, IOException, ParseException
{    String[] triplesArray = getTriples(file2);    Assert.assertEquals("We expect 40 tab-separated triples extracted by the filter", EXPECTED_TRIPLES_2, triplesArray.length);}
public void nutch_f1789_0() throws ParserNotFound, IOException, ParseException
{    String[] triplesArray = getTriples(file1, "application/pdf");    Assert.assertEquals("We expect no triples extracted by the filter since content-type should be ignored", 0, triplesArray.length);}
public String[] nutch_f1790_0(String urlString, File file, String contentType)
{    try {        System.out.println(urlString);        Protocol protocol = new ProtocolFactory(conf).getProtocol(urlString);        Content content = protocol.getProtocolOutput(new Text(urlString), new CrawlDatum()).getContent();        content.setContentType(contentType);        Parse parse = new ParseUtil(conf).parse(content).get(content.getUrl());        return parse.getData().getParseMeta().getValues(Any23ParseFilter.ANY23_TRIPLES);    } catch (Exception e) {        e.printStackTrace();        Assert.fail(e.toString());    }    return null;}
private String[] nutch_f1791_0(String fileName)
{    return getTriples(fileName, "text/html");}
private String[] nutch_f1792_0(String fileName, String contentType)
{    String urlString = "file:" + sampleDir + fileSeparator + fileName;    File file = new File(sampleDir + fileSeparator + fileName);    return extract(urlString, file, contentType);}
public NutchDocument nutch_f1793_1(NutchDocument doc, Parse parse, Text url, CrawlDatum datum, Inlinks inlinks) throws IndexingException
{    Metadata metadata = parse.getData().getParseMeta();        String licenseUrl = metadata.get(CreativeCommons.LICENSE_URL);    if (licenseUrl != null) {        if (LOG.isInfoEnabled()) {                    }                addFeature(doc, "license=" + licenseUrl);                addUrlFeatures(doc, licenseUrl);    }        String licenseLocation = metadata.get(CreativeCommons.LICENSE_LOCATION);    if (licenseLocation != null) {        addFeature(doc, "meta=" + licenseLocation);    }        String workType = metadata.get(CreativeCommons.WORK_TYPE);    if (workType != null) {        addFeature(doc, workType);    }    return doc;}
public void nutch_f1794_1(NutchDocument doc, String urlString)
{    try {        URL url = new URL(urlString);                StringTokenizer names = new StringTokenizer(url.getPath(), "/-");        if (names.hasMoreTokens())                        names.nextToken();                while (names.hasMoreTokens()) {            String feature = names.nextToken();            addFeature(doc, feature);        }    } catch (MalformedURLException e) {        if (LOG.isWarnEnabled()) {                    }    }}
private void nutch_f1795_0(NutchDocument doc, String feature)
{    doc.add(FIELD, feature);}
public void nutch_f1796_0(Configuration conf)
{    this.conf = conf;}
public Configuration nutch_f1797_0()
{    return this.conf;}
public static void nutch_f1798_1(Node doc, URL base, Metadata metadata, Configuration conf) throws ParseException
{        Walker walker = new Walker(base);    walker.walk(doc);        String licenseUrl = null;    String licenseLocation = null;    if (walker.rdfLicense != null) {                licenseLocation = "rdf";        licenseUrl = walker.rdfLicense;    } else if (walker.relLicense != null) {                licenseLocation = "rel";        licenseUrl = walker.relLicense.toString();    } else if (walker.anchorLicense != null) {                licenseLocation = "a";        licenseUrl = walker.anchorLicense.toString();    } else if (conf.getBoolean("creativecommons.exclude.unlicensed", false)) {        throw new ParseException("No CC license.  Excluding.");    }        if (licenseUrl != null) {        if (LOG.isInfoEnabled()) {                    }        metadata.add(CreativeCommons.LICENSE_URL, licenseUrl);        metadata.add(CreativeCommons.LICENSE_LOCATION, licenseLocation);    }    if (walker.workType != null) {        if (LOG.isInfoEnabled()) {                    }        metadata.add(CreativeCommons.WORK_TYPE, walker.workType);    }}
private void nutch_f1799_0(Node node)
{        if (node instanceof Element) {        findLicenseUrl((Element) node);    }        if (node instanceof Comment) {        findRdf(((Comment) node).getData());    }        NodeList children = node.getChildNodes();    for (int i = 0; children != null && i < children.getLength(); i++) {        walk(children.item(i));    }}
private void nutch_f1800_0(Element element)
{        if (!"a".equalsIgnoreCase(element.getTagName()))        return;        String href = element.getAttribute("href");    if (href == null)        return;    try {                URL url = new URL(base, href);                if ("http".equalsIgnoreCase(url.getProtocol()) && "creativecommons.org".equalsIgnoreCase(url.getHost()) && url.getPath() != null && url.getPath().startsWith("/licenses/") && url.getPath().length() > "/licenses/".length()) {                        String rel = element.getAttribute("rel");            if (rel != null && "license".equals(rel) && this.relLicense == null) {                                this.relLicense = url;            } else if (this.anchorLicense == null) {                                this.anchorLicense = url;            }        }    } catch (MalformedURLException e) {        }}
private void nutch_f1801_1(String comment)
{        int rdfPosition = comment.indexOf("RDF");    if (rdfPosition < 0)                return;    int nsPosition = comment.indexOf(CC_NS);    if (nsPosition < 0)                return;        Document doc;    try {        DocumentBuilder parser = FACTORY.newDocumentBuilder();        doc = parser.parse(new InputSource(new StringReader(comment)));    } catch (Exception e) {        if (LOG.isWarnEnabled()) {                    }        return;    }        NodeList roots = doc.getElementsByTagNameNS(RDF_NS, "RDF");    if (roots.getLength() != 1) {        if (LOG.isWarnEnabled()) {                    }        return;    }    Element rdf = (Element) roots.item(0);        NodeList licenses = rdf.getElementsByTagNameNS(CC_NS, "License");    for (int i = 0; i < licenses.getLength(); i++) {        Element l = (Element) licenses.item(i);                this.rdfLicense = l.getAttributeNodeNS(RDF_NS, "about").getValue();                NodeList predicates = l.getChildNodes();        for (int j = 0; j < predicates.getLength(); j++) {            Node predicateNode = predicates.item(j);            if (!(predicateNode instanceof Element))                continue;            Element predicateElement = (Element) predicateNode;                        if (!CC_NS.equals(predicateElement.getNamespaceURI())) {                continue;            }        }    }        NodeList works = rdf.getElementsByTagNameNS(CC_NS, "Work");    for (int i = 0; i < works.getLength(); i++) {                NodeList types = rdf.getElementsByTagNameNS(DC_NS, "type");        for (int j = 0; j < types.getLength(); j++) {            Element type = (Element) types.item(j);            String workUri = type.getAttributeNodeNS(RDF_NS, "resource").getValue();            this.workType = WORK_TYPE_NAMES.get(workUri);        }    }}
public ParseResult nutch_f1802_0(Content content, ParseResult parseResult, HTMLMetaTags metaTags, DocumentFragment doc)
{        Parse parse = parseResult.get(content.getUrl());        URL base;    try {        base = new URL(content.getBaseUrl());    } catch (MalformedURLException e) {        Parse emptyParse = new ParseStatus(e).getEmptyParse(getConf());        parseResult.put(content.getUrl(), new ParseText(emptyParse.getText()), emptyParse.getData());        return parseResult;    }    try {                Walker.walk(doc, base, parse.getData().getParseMeta(), getConf());    } catch (ParseException e) {        Parse emptyParse = new ParseStatus(e).getEmptyParse(getConf());        parseResult.put(content.getUrl(), new ParseText(emptyParse.getText()), emptyParse.getData());        return parseResult;    }    return parseResult;}
public void nutch_f1803_0(Configuration conf)
{    this.conf = conf;}
public Configuration nutch_f1804_0()
{    return this.conf;}
public void nutch_f1805_0() throws Exception
{    pageTest(new File(testDir, "anchor.html"), "http://foo.com/", "http://creativecommons.org/licenses/by-nc-sa/1.0", "a", null);            pageTest(new File(testDir, "rel.html"), "http://foo.com/", "http://creativecommons.org/licenses/by-nc/2.0", "rel", null);            pageTest(new File(testDir, "rdf.html"), "http://foo.com/", "http://creativecommons.org/licenses/by-nc/1.0", "rdf", "text");}
public void nutch_f1806_0(File file, String url, String license, String location, String type) throws Exception
{    String contentType = "text/html";    InputStream in = new FileInputStream(file);    ByteArrayOutputStream out = new ByteArrayOutputStream((int) file.length());    byte[] buffer = new byte[1024];    int i;    while ((i = in.read(buffer)) != -1) {        out.write(buffer, 0, i);    }    in.close();    byte[] bytes = out.toByteArray();    Configuration conf = NutchConfiguration.create();    Content content = new Content(url, url, bytes, contentType, new Metadata(), conf);    Parse parse = new ParseUtil(conf).parse(content).get(content.getUrl());    Metadata metadata = parse.getData().getParseMeta();    Assert.assertEquals(license, metadata.get("License-Url"));    Assert.assertEquals(location, metadata.get("License-Location"));    Assert.assertEquals(type, metadata.get("Work-Type"));}
public void nutch_f1807_0(Map<String, String> parameters)
{    expression = JexlUtil.parseExpression(parameters.get(EXPRESSION_KEY));}
public boolean nutch_f1808_0(NutchDocument doc)
{        JexlContext jexlContext = new MapContext();    jexlContext.set("doc", doc);    try {        if (Boolean.TRUE.equals(expression.evaluate(jexlContext))) {            return true;        }    } catch (Exception ignored) {    }    return false;}
public void nutch_f1809_0(Configuration configuration)
{    this.conf = configuration;}
public Configuration nutch_f1810_0()
{    return conf;}
public NutchDocument nutch_f1811_0(NutchDocument doc, Parse parse, Text url, CrawlDatum datum, Inlinks inlinks) throws IndexingException
{    ParseData parseData = parse.getData();    Metadata parseMeta = parseData.getParseMeta();    String[] authors = parseMeta.getValues(Feed.FEED_AUTHOR);    String[] tags = parseMeta.getValues(Feed.FEED_TAGS);    String published = parseMeta.get(Feed.FEED_PUBLISHED);    String updated = parseMeta.get(Feed.FEED_UPDATED);    String feed = parseMeta.get(Feed.FEED);    if (authors != null) {        for (String author : authors) {            doc.add(Feed.FEED_AUTHOR, author);        }    }    if (tags != null) {        for (String tag : tags) {            doc.add(Feed.FEED_TAGS, tag);        }    }    if (feed != null)        doc.add(Feed.FEED, feed);    if (published != null) {        Date date = new Date(Long.parseLong(published));        doc.add(PUBLISHED_DATE, date);    }    if (updated != null) {        Date date = new Date(Long.parseLong(updated));        doc.add(UPDATED_DATE, date);    }    return doc;}
public Configuration nutch_f1812_0()
{    return conf;}
public void nutch_f1813_0(Configuration conf)
{    this.conf = conf;}
public ParseResult nutch_f1814_1(Content content)
{    SyndFeed feed = null;    ParseResult parseResult = new ParseResult(content.getUrl());    EncodingDetector detector = new EncodingDetector(conf);    detector.autoDetectClues(content, true);    String encoding = detector.guessEncoding(content, defaultEncoding);    try {        InputSource input = new InputSource(new ByteArrayInputStream(content.getContent()));        input.setEncoding(encoding);        SyndFeedInput feedInput = new SyndFeedInput();        feed = feedInput.build(input);    } catch (Exception e) {                        return new ParseStatus(e).getEmptyParseResult(content.getUrl(), getConf());    }    String feedLink = feed.getLink();    try {        feedLink = normalizers.normalize(feedLink, URLNormalizers.SCOPE_OUTLINK);        if (feedLink != null)            feedLink = filters.filter(feedLink);    } catch (Exception e) {        feedLink = null;    }    List<?> entries = feed.getEntries();    for (Object entry : entries) {        addToMap(parseResult, feed, feedLink, (SyndEntry) entry, content);    }    String feedDesc = stripTags(feed.getDescriptionEx());    String feedTitle = stripTags(feed.getTitleEx());    parseResult.put(content.getUrl(), new ParseText(feedDesc), new ParseData(new ParseStatus(ParseStatus.SUCCESS), feedTitle, new Outlink[0], content.getMetadata()));    return parseResult;}
public void nutch_f1815_0(Configuration conf)
{    this.conf = conf;    this.parserFactory = new ParserFactory(conf);    this.normalizers = new URLNormalizers(conf, URLNormalizers.SCOPE_OUTLINK);    this.filters = new URLFilters(conf);    this.defaultEncoding = conf.get("parser.character.encoding.default", "windows-1252");}
public Configuration nutch_f1816_0()
{    return this.conf;}
public static void nutch_f1817_0(String[] args) throws Exception
{    if (args.length != 1) {        System.err.println("Usage: FeedParser <feed>");        System.exit(1);    }    String name = args[0];    String url = "file:" + name;    Configuration conf = NutchConfiguration.create();    FeedParser parser = new FeedParser();    parser.setConf(conf);    File file = new File(name);    byte[] bytes = new byte[(int) file.length()];    DataInputStream in = new DataInputStream(new FileInputStream(file));    in.readFully(bytes);    in.close();    ParseResult parseResult = parser.getParse(new Content(url, url, bytes, "application/rss+xml", new Metadata(), conf));    for (Entry<Text, Parse> entry : parseResult) {        System.out.println("key: " + entry.getKey());        Parse parse = entry.getValue();        System.out.println("data: " + parse.getData());        System.out.println("text: " + parse.getText() + "\n");    }}
private void nutch_f1818_0(ParseResult parseResult, SyndFeed feed, String feedLink, SyndEntry entry, Content content)
{    String link = entry.getLink(), text = null, title = null;    Metadata parseMeta = new Metadata(), contentMeta = content.getMetadata();    Parse parse = null;    SyndContent description = entry.getDescription();    try {        link = normalizers.normalize(link, URLNormalizers.SCOPE_OUTLINK);        if (link != null)            link = filters.filter(link);    } catch (Exception e) {        e.printStackTrace();        return;    }    if (link == null)        return;    title = stripTags(entry.getTitleEx());    if (feedLink != null)        parseMeta.set("feed", feedLink);    addFields(parseMeta, contentMeta, feed, entry);                String contentType = contentMeta.get(Response.CONTENT_TYPE);    if (description != null)        text = description.getValue();    if (text == null) {        List<?> contents = entry.getContents();        StringBuilder buf = new StringBuilder();        for (Object syndContent : contents) {            buf.append(((SyndContent) syndContent).getValue());        }        text = buf.toString();    }    try {        Parser parser = parserFactory.getParsers(contentType, link)[0];        parse = parser.getParse(new Content(link, link, text.getBytes(), contentType, contentMeta, conf)).get(link);    } catch (ParserNotFound e) {    /* ignore */    }    if (parse != null) {        ParseData data = parse.getData();        data.getContentMeta().remove(Response.CONTENT_TYPE);        mergeMetadata(data.getParseMeta(), parseMeta);        parseResult.put(link, new ParseText(parse.getText()), new ParseData(ParseStatus.STATUS_SUCCESS, title, data.getOutlinks(), data.getContentMeta(), data.getParseMeta()));    } else {        contentMeta.remove(Response.CONTENT_TYPE);        parseResult.put(link, new ParseText(text), new ParseData(ParseStatus.STATUS_FAILURE, title, new Outlink[0], contentMeta, parseMeta));    }}
private static String nutch_f1819_0(SyndContent c)
{    if (c == null)        return "";    String value = c.getValue();    String[] parts = value.split("<[^>]*>");    StringBuffer buf = new StringBuffer();    for (String part : parts) buf.append(part);    return buf.toString().trim();}
private void nutch_f1820_0(Metadata parseMeta, Metadata contentMeta, SyndFeed feed, SyndEntry entry)
{    List<?> authors = entry.getAuthors(), categories = entry.getCategories();    Date published = entry.getPublishedDate(), updated = entry.getUpdatedDate();    String contentType = null;    if (authors != null) {        for (Object o : authors) {            SyndPerson author = (SyndPerson) o;            String authorName = author.getName();            if (checkString(authorName)) {                parseMeta.add(Feed.FEED_AUTHOR, authorName);            }        }    } else {                        String authorName = entry.getAuthor();        if (checkString(authorName)) {            parseMeta.set(Feed.FEED_AUTHOR, authorName);        }    }    for (Object i : categories) {        parseMeta.add(Feed.FEED_TAGS, ((SyndCategory) i).getName());    }    if (published != null) {        parseMeta.set(Feed.FEED_PUBLISHED, Long.toString(published.getTime()));    }    if (updated != null) {        parseMeta.set(Feed.FEED_UPDATED, Long.toString(updated.getTime()));    }    SyndContent description = entry.getDescription();    if (description != null) {        contentType = description.getType();    } else {                List<?> contents = entry.getContents();        if (contents.size() > 0) {            contentType = ((SyndContent) contents.get(0)).getType();        }    }    if (checkString(contentType)) {                if (contentType.equals("html"))            contentType = "text/html";        else if (contentType.equals("xhtml"))            contentType = "text/xhtml";        contentMeta.set(Response.CONTENT_TYPE, contentType + "; " + CHARSET_UTF8);    } else {        contentMeta.set(Response.CONTENT_TYPE, TEXT_PLAIN_CONTENT_TYPE);    }}
private void nutch_f1821_0(Metadata first, Metadata second)
{    for (String name : second.names()) {        String[] values = second.getValues(name);        for (String value : values) {            first.add(name, value);        }    }}
private boolean nutch_f1822_0(String s)
{    return s != null && !s.equals("");}
public void nutch_f1823_0() throws ProtocolNotFound, ParseException
{    String urlString;    Protocol protocol;    Content content;    ParseResult parseResult;    Configuration conf = NutchConfiguration.create();    for (int i = 0; i < sampleFiles.length; i++) {        urlString = "file:" + sampleDir + fileSeparator + sampleFiles[i];        urlString = urlString.replace('\\', '/');        protocol = new ProtocolFactory(conf).getProtocol(urlString);        content = protocol.getProtocolOutput(new Text(urlString), new CrawlDatum()).getContent();        parseResult = new ParseUtil(conf).parseByExtensionId("feed", content);        Assert.assertEquals(3, parseResult.size());        boolean hasLink1 = false, hasLink2 = false, hasLink3 = false;        for (Iterator<Map.Entry<Text, Parse>> j = parseResult.iterator(); j.hasNext(); ) {            Map.Entry<Text, Parse> entry = j.next();            if (entry.getKey().toString().equals("http://www-scf.usc.edu/~mattmann/")) {                hasLink1 = true;            } else if (entry.getKey().toString().equals("http://www.nutch.org/")) {                hasLink2 = true;            } else if (entry.getKey().toString().equals(urlString)) {                hasLink3 = true;            }            Assert.assertNotNull(entry.getValue());            Assert.assertNotNull(entry.getValue().getData());        }        if (!hasLink1 || !hasLink2 || !hasLink3) {            Assert.fail("Outlinks read from sample rss file are not correct!");        }    }}
public ParseResult nutch_f1824_0(Content content, ParseResult parseResult, HTMLMetaTags metaTags, DocumentFragment doc)
{    Parse parse = parseResult.get(content.getUrl());    for (int i = 0; headings != null && i < headings.length; i++) {        List<String> discoveredHeadings = getElement(doc, headings[i]);        if (discoveredHeadings.size() > 0) {            for (String heading : discoveredHeadings) {                if (heading != null) {                    heading = heading.trim();                    if (heading.length() > 0) {                        parse.getData().getParseMeta().add(headings[i], heading);                    }                }            }        }    }    return parseResult;}
public void nutch_f1825_0(Configuration conf)
{    this.conf = conf;    headings = conf.getStrings("headings");    multiValued = conf.getBoolean("headings.multivalued", false);}
public Configuration nutch_f1826_0()
{    return this.conf;}
protected List<String> nutch_f1827_0(DocumentFragment doc, String element)
{    List<String> headings = new ArrayList<>();    NodeWalker walker = new NodeWalker(doc);    while (walker.hasNext()) {        Node currentNode = walker.nextNode();        if (currentNode.getNodeType() == Node.ELEMENT_NODE) {            if (element.equalsIgnoreCase(currentNode.getNodeName())) {                headings.add(getNodeValue(currentNode));                                if (!multiValued) {                    break;                }            }        }    }    return headings;}
protected static String nutch_f1828_0(Node node)
{    StringBuilder buffer = new StringBuilder();    NodeWalker walker = new NodeWalker(node);    while (walker.hasNext()) {        final Node n = walker.nextNode();        if (n.getNodeType() == Node.TEXT_NODE) {            buffer.append(n.getNodeValue());        }    }        Matcher matcher = whitespacePattern.matcher(buffer.toString().trim());    return matcher.replaceAll(" ").trim();}
public void nutch_f1829_0() throws IOException, SAXException
{    conf.setStrings("headings", "h1", "h2");    HtmlParseFilter filter = new HeadingsParseFilter();    filter.setConf(conf);    Content content = new Content("http://www.foo.com/", "http://www.foo.com/", "".getBytes("UTF8"), "text/html; charset=UTF-8", new Metadata(), conf);    ParseImpl parse = new ParseImpl("foo bar", new ParseData());    ParseResult parseResult = ParseResult.createParseResult("http://www.foo.com/", parse);    HTMLMetaTags metaTags = new HTMLMetaTags();    DOMFragmentParser parser = new DOMFragmentParser();    DocumentFragment node = new HTMLDocumentImpl().createDocumentFragment();    parser.parse(new InputSource(new ByteArrayInputStream(("<html><head><title>test header with span element</title></head><body><h1>header with <span>span element</span></h1></body></html>").getBytes())), node);    parseResult = filter.filter(content, parseResult, metaTags, node);    Assert.assertEquals("The h1 tag must include the content of the inner span node", "header with span element", parseResult.get(content.getUrl()).getData().getParseMeta().get("h1"));}
public void nutch_f1830_1(Configuration conf)
{    this.conf = conf;    deduplicate = conf.getBoolean("anchorIndexingFilter.deduplicate", false);    }
public Configuration nutch_f1831_0()
{    return this.conf;}
public NutchDocument nutch_f1832_0(NutchDocument doc, Parse parse, Text url, CrawlDatum datum, Inlinks inlinks) throws IndexingException
{    String[] anchors = (inlinks != null ? inlinks.getAnchors() : new String[0]);    HashSet<String> set = null;    for (int i = 0; i < anchors.length; i++) {        if (deduplicate) {            if (set == null)                set = new HashSet<String>();            String lcAnchor = anchors[i].toLowerCase();                        if (!set.contains(lcAnchor)) {                doc.add("anchor", anchors[i]);                                set.add(lcAnchor);            }        } else {            doc.add("anchor", anchors[i]);        }    }    return doc;}
public void nutch_f1833_0() throws Exception
{    Configuration conf = NutchConfiguration.create();    conf.setBoolean("anchorIndexingFilter.deduplicate", true);    AnchorIndexingFilter filter = new AnchorIndexingFilter();    filter.setConf(conf);    Assert.assertNotNull(filter);    NutchDocument doc = new NutchDocument();    ParseImpl parse = new ParseImpl("foo bar", new ParseData());    Inlinks inlinks = new Inlinks();    inlinks.add(new Inlink("http://test1.com/", "text1"));    inlinks.add(new Inlink("http://test2.com/", "text2"));    inlinks.add(new Inlink("http://test3.com/", "text2"));    try {        filter.filter(doc, parse, new Text("http://nutch.apache.org/index.html"), new CrawlDatum(), inlinks);    } catch (Exception e) {        e.printStackTrace();        Assert.fail(e.getMessage());    }    Assert.assertNotNull(doc);    Assert.assertTrue("test if there is an anchor at all", doc.getFieldNames().contains("anchor"));    Assert.assertEquals("test dedup, we expect 2", 2, doc.getField("anchor").getValues().size());}
public NutchDocument nutch_f1834_0(NutchDocument doc, Parse parse, Text url, CrawlDatum datum, Inlinks inlinks) throws IndexingException
{    Text reprUrl = (Text) datum.getMetaData().get(Nutch.WRITABLE_REPR_URL_KEY);    String reprUrlString = reprUrl != null ? reprUrl.toString() : null;    String urlString = url.toString();    String host = null;    try {        URL u;        if (reprUrlString != null) {            u = new URL(reprUrlString);        } else {            u = new URL(urlString);        }        if (addDomain) {            doc.add("domain", URLUtil.getDomainName(u));        }        host = u.getHost();    } catch (MalformedURLException e) {        throw new IndexingException(e);    }    if (host != null) {        doc.add("host", host);    }    doc.add("url", reprUrlString == null ? urlString : reprUrlString);        String content = parse.getText();    if (MAX_CONTENT_LENGTH > -1 && content.length() > MAX_CONTENT_LENGTH) {        content = content.substring(0, MAX_CONTENT_LENGTH);    }    doc.add("content", StringUtil.cleanField(content));        String title = parse.getData().getTitle();    if (MAX_TITLE_LENGTH > -1 && title.length() > MAX_TITLE_LENGTH) {                                        title = title.substring(0, MAX_TITLE_LENGTH);    }    if (title.length() > 0) {                doc.add("title", StringUtil.cleanField(title));    }        String caching = parse.getData().getMeta(Nutch.CACHING_FORBIDDEN_KEY);    if (caching != null && !caching.equals(Nutch.CACHING_FORBIDDEN_NONE)) {        doc.add("cache", caching);    }        doc.add("tstamp", new Date(datum.getFetchTime()));    return doc;}
public void nutch_f1835_0(Configuration conf)
{    this.conf = conf;    this.MAX_TITLE_LENGTH = conf.getInt("indexer.max.title.length", 100);    this.addDomain = conf.getBoolean("indexer.add.domain", false);    this.MAX_CONTENT_LENGTH = conf.getInt("indexer.max.content.length", -1);}
public Configuration nutch_f1836_0()
{    return this.conf;}
public void nutch_f1837_0() throws Exception
{    Configuration conf = NutchConfiguration.create();    conf.setInt("indexer.max.title.length", 10);    conf.setBoolean("indexer.add.domain", true);    conf.setInt("indexer.max.content.length", 20);    BasicIndexingFilter filter = new BasicIndexingFilter();    filter.setConf(conf);    Assert.assertNotNull(filter);    NutchDocument doc = new NutchDocument();    String title = "The Foo Page";    Outlink[] outlinks = new Outlink[] { new Outlink("http://foo.com/", "Foo") };    Metadata metaData = new Metadata();    metaData.add("Language", "en/us");    ParseData parseData = new ParseData(ParseStatus.STATUS_SUCCESS, title, outlinks, metaData);    ParseImpl parse = new ParseImpl("this is a sample foo bar page. hope you enjoy it.", parseData);    CrawlDatum crawlDatum = new CrawlDatum();    crawlDatum.setFetchTime(100L);    Inlinks inlinks = new Inlinks();    try {        filter.filter(doc, parse, new Text("http://nutch.apache.org/index.html"), crawlDatum, inlinks);    } catch (Exception e) {        e.printStackTrace();        Assert.fail(e.getMessage());    }    Assert.assertNotNull(doc);    Assert.assertEquals("test title, expect \"The Foo Pa\"", "The Foo Pa", doc.getField("title").getValues().get(0));    Assert.assertEquals("test domain, expect \"apache.org\"", "apache.org", doc.getField("domain").getValues().get(0));    Assert.assertEquals("test host, expect \"nutch.apache.org\"", "nutch.apache.org", doc.getField("host").getValues().get(0));    Assert.assertEquals("test url, expect \"http://nutch.apache.org/index.html\"", "http://nutch.apache.org/index.html", doc.getField("url").getValues().get(0));    Assert.assertEquals("test content", "this is a sample foo", doc.getField("content").getValues().get(0));    Assert.assertEquals("test fetch time", new Date(100L), (Date) doc.getField("tstamp").getValues().get(0));}
public static void nutch_f1838_0(NutchDocument doc, String name, String value)
{    if (value != null) {        doc.add(name, value);    }}
public static void nutch_f1839_0(NutchDocument doc, String name, Integer value)
{    if (value != null) {        doc.add(name, value);    }}
public static NutchDocument nutch_f1840_0(String serverIp, NutchDocument doc, WebServiceClient client) throws UnknownHostException, IOException, GeoIp2Exception
{    addIfNotNull(doc, "ip", serverIp);    InsightsResponse response = client.insights(InetAddress.getByName(serverIp));        City city = response.getCity();        addIfNotNull(doc, "cityName", city.getName());        addIfNotNull(doc, "cityConfidence", city.getConfidence());    addIfNotNull(doc, "cityGeoNameId", city.getGeoNameId());    Continent continent = response.getContinent();    addIfNotNull(doc, "continentCode", continent.getCode());    addIfNotNull(doc, "continentGeoNameId", continent.getGeoNameId());    addIfNotNull(doc, "continentName", continent.getName());    Country country = response.getCountry();        addIfNotNull(doc, "countryIsoCode", country.getIsoCode());        addIfNotNull(doc, "countryName", country.getName());        addIfNotNull(doc, "countryConfidence", country.getConfidence());    addIfNotNull(doc, "countryGeoName", country.getGeoNameId());    Location location = response.getLocation();        addIfNotNull(doc, "latLon", location.getLatitude() + "," + location.getLongitude());            addIfNotNull(doc, "accRadius", location.getAccuracyRadius());        addIfNotNull(doc, "timeZone", location.getTimeZone());    addIfNotNull(doc, "metroCode", location.getMetroCode());    Postal postal = response.getPostal();        addIfNotNull(doc, "postalCode", postal.getCode());        addIfNotNull(doc, "postalConfidence", postal.getConfidence());    RepresentedCountry rCountry = response.getRepresentedCountry();    addIfNotNull(doc, "countryType", rCountry.getType());    Subdivision subdivision = response.getMostSpecificSubdivision();        addIfNotNull(doc, "subDivName", subdivision.getName());        addIfNotNull(doc, "subDivIdoCode", subdivision.getIsoCode());        addIfNotNull(doc, "subDivConfidence", subdivision.getConfidence());    addIfNotNull(doc, "subDivGeoNameId", subdivision.getGeoNameId());    Traits traits = response.getTraits();    addIfNotNull(doc, "autonSystemNum", traits.getAutonomousSystemNumber());    addIfNotNull(doc, "autonSystemOrg", traits.getAutonomousSystemOrganization());    addIfNotNull(doc, "domain", traits.getDomain());    addIfNotNull(doc, "isp", traits.getIsp());    addIfNotNull(doc, "org", traits.getOrganization());    addIfNotNull(doc, "userType", traits.getUserType());            addIfNotNull(doc, "isAnonProxy", String.valueOf(traits.isAnonymousProxy()));    return doc;}
public static NutchDocument nutch_f1841_0(String serverIp, NutchDocument doc, WebServiceClient client) throws UnknownHostException, IOException, GeoIp2Exception
{    CityResponse response = client.city(InetAddress.getByName(serverIp));    return doc;}
public static NutchDocument nutch_f1842_0(String serverIp, NutchDocument doc, WebServiceClient client) throws UnknownHostException, IOException, GeoIp2Exception
{    CountryResponse response = client.country(InetAddress.getByName(serverIp));    return doc;}
public static NutchDocument nutch_f1843_0(String serverIp, NutchDocument doc, DatabaseReader reader) throws UnknownHostException, IOException, GeoIp2Exception
{    IspResponse response = reader.isp(InetAddress.getByName(serverIp));    addIfNotNull(doc, "ip", serverIp);    addIfNotNull(doc, "autonSystemNum", response.getAutonomousSystemNumber());    addIfNotNull(doc, "autonSystemOrg", response.getAutonomousSystemOrganization());    addIfNotNull(doc, "isp", response.getIsp());    addIfNotNull(doc, "org", response.getOrganization());    return doc;}
public static NutchDocument nutch_f1844_0(String serverIp, NutchDocument doc, DatabaseReader reader) throws UnknownHostException, IOException, GeoIp2Exception
{    DomainResponse response = reader.domain(InetAddress.getByName(serverIp));    addIfNotNull(doc, "ip", serverIp);    addIfNotNull(doc, "domain", response.getDomain());    return doc;}
public static NutchDocument nutch_f1845_0(String serverIp, NutchDocument doc, DatabaseReader reader) throws UnknownHostException, IOException, GeoIp2Exception
{    ConnectionTypeResponse response = reader.connectionType(InetAddress.getByName(serverIp));    addIfNotNull(doc, "ip", serverIp);    addIfNotNull(doc, "connType", response.getConnectionType().toString());    return doc;}
public static NutchDocument nutch_f1846_0(String serverIp, NutchDocument doc, DatabaseReader reader) throws UnknownHostException, IOException, GeoIp2Exception
{    addIfNotNull(doc, "ip", serverIp);    CityResponse response = reader.city(InetAddress.getByName(serverIp));    City city = response.getCity();        addIfNotNull(doc, "cityName", city.getName());        addIfNotNull(doc, "cityConfidence", city.getConfidence());    addIfNotNull(doc, "cityGeoNameId", city.getGeoNameId());    Continent continent = response.getContinent();    addIfNotNull(doc, "continentCode", continent.getCode());    addIfNotNull(doc, "continentGeoNameId", continent.getGeoNameId());    addIfNotNull(doc, "continentName", continent.getName());    Country country = response.getCountry();        addIfNotNull(doc, "countryIsoCode", country.getIsoCode());        addIfNotNull(doc, "countryName", country.getName());        addIfNotNull(doc, "countryConfidence", country.getConfidence());    addIfNotNull(doc, "countryGeoName", country.getGeoNameId());    Location location = response.getLocation();        addIfNotNull(doc, "latLon", location.getLatitude() + "," + location.getLongitude());            addIfNotNull(doc, "accRadius", location.getAccuracyRadius());        addIfNotNull(doc, "timeZone", location.getTimeZone());    addIfNotNull(doc, "metroCode", location.getMetroCode());    Postal postal = response.getPostal();        addIfNotNull(doc, "postalCode", postal.getCode());        addIfNotNull(doc, "postalConfidence", postal.getConfidence());    RepresentedCountry rCountry = response.getRepresentedCountry();    addIfNotNull(doc, "countryType", rCountry.getType());    Subdivision subdivision = response.getMostSpecificSubdivision();        addIfNotNull(doc, "subDivName", subdivision.getName());        addIfNotNull(doc, "subDivIdoCode", subdivision.getIsoCode());        addIfNotNull(doc, "subDivConfidence", subdivision.getConfidence());    addIfNotNull(doc, "subDivGeoNameId", subdivision.getGeoNameId());    return doc;}
public Configuration nutch_f1847_0()
{    return this.conf;}
public void nutch_f1848_1(Configuration conf)
{    this.conf = conf;    usage = conf.get("index.geoip.usage", "insightsService");        if (usage.equalsIgnoreCase("insightsService")) {        client = new WebServiceClient.Builder(conf.getInt("index.geoip.userid", 12345), conf.get("index.geoip.licensekey")).build();    } else {        String db = null;        if (usage.equalsIgnoreCase("cityDatabase")) {            db = "GeoIP2-City.mmdb";        } else if (usage.equalsIgnoreCase("connectionTypeDatabase")) {            db = "GeoIP2-Connection-Type.mmdb";        } else if (usage.equalsIgnoreCase("domainDatabase")) {            db = "GeoIP2-Domain.mmdb";        } else if (usage.equalsIgnoreCase("ispDatabase")) {            db = "GeoIP2-ISP.mmdb";        }        URL dbFileUrl = conf.getResource(db);        if (dbFileUrl == null) {                    } else {            try {                buildDb(new File(dbFileUrl.getFile()));            } catch (Exception e) {                            }        }    }    if (!conf.getBoolean("store.ip.address", false)) {            }}
private void nutch_f1849_1(File geoDb)
{    try {        reader = new DatabaseReader.Builder(geoDb).build();    } catch (IOException e) {            }}
public NutchDocument nutch_f1850_0(NutchDocument doc, Parse parse, Text url, CrawlDatum datum, Inlinks inlinks) throws IndexingException
{    return addServerGeo(doc, parse.getData(), url.toString());}
private NutchDocument nutch_f1851_1(NutchDocument doc, ParseData data, String url)
{    String serverIp = data.getContentMeta().get("_ip_");    if (serverIp != null && reader != null) {        try {            if (usage.equalsIgnoreCase("cityDatabase")) {                doc = GeoIPDocumentCreator.createDocFromCityDb(serverIp, doc, reader);            } else if (usage.equalsIgnoreCase("connectionTypeDatabase")) {                doc = GeoIPDocumentCreator.createDocFromConnectionDb(serverIp, doc, reader);            } else if (usage.equalsIgnoreCase("domainDatabase")) {                doc = GeoIPDocumentCreator.createDocFromDomainDb(serverIp, doc, reader);            } else if (usage.equalsIgnoreCase("ispDatabase")) {                doc = GeoIPDocumentCreator.createDocFromIspDb(serverIp, doc, reader);            } else if (usage.equalsIgnoreCase("insightsService")) {                doc = GeoIPDocumentCreator.createDocFromInsightsService(serverIp, doc, client);            }        } catch (Exception e) {                    }    }    return doc;}
public NutchDocument nutch_f1852_1(NutchDocument doc, Parse parse, Text url, CrawlDatum datum, Inlinks inlinks) throws IndexingException
{        JexlContext jcontext = new MapContext();    jcontext.set("status", CrawlDatum.getStatusName(datum.getStatus()));    jcontext.set("fetchTime", (long) (datum.getFetchTime()));    jcontext.set("modifiedTime", (long) (datum.getModifiedTime()));    jcontext.set("retries", datum.getRetriesSinceFetch());    jcontext.set("interval", Integer.valueOf(datum.getFetchInterval()));    jcontext.set("score", datum.getScore());    jcontext.set("signature", StringUtil.toHexString(datum.getSignature()));    jcontext.set("url", url.toString());    jcontext.set("text", parse.getText());    jcontext.set("title", parse.getData().getTitle());    JexlContext httpStatusContext = new MapContext();    httpStatusContext.set("majorCode", parse.getData().getStatus().getMajorCode());    httpStatusContext.set("minorCode", parse.getData().getStatus().getMinorCode());    httpStatusContext.set("message", parse.getData().getStatus().getMessage());    jcontext.set("httpStatus", httpStatusContext);    jcontext.set("documentMeta", metadataToContext(doc.getDocumentMeta()));    jcontext.set("contentMeta", metadataToContext(parse.getData().getContentMeta()));    jcontext.set("parseMeta", metadataToContext(parse.getData().getParseMeta()));    JexlContext context = new MapContext();    for (Entry<String, NutchField> entry : doc) {        List<Object> values = entry.getValue().getValues();        context.set(entry.getKey(), values.size() > 1 ? values : values.get(0));    }    jcontext.set("doc", context);    try {        if (Boolean.TRUE.equals(expr.evaluate(jcontext))) {            return doc;        }    } catch (Exception e) {            }    return null;}
public void nutch_f1853_1(Configuration conf)
{    this.conf = conf;    String strExpr = conf.get("index.jexl.filter");    if (strExpr == null) {                throw new RuntimeException("The property index.jexl.filter must have a value when index-jexl-filter is used. You can use 'true' or 'false' to index all/none");    }    expr = JexlUtil.parseExpression(strExpr);    if (expr == null) {                throw new RuntimeException("Failed parsing JEXL from index.jexl.filter");    }}
public Configuration nutch_f1854_0()
{    return this.conf;}
private JexlContext nutch_f1855_0(Metadata metadata)
{    JexlContext context = new MapContext();    for (String name : metadata.names()) {        String[] values = metadata.getValues(name);        context.set(name, values.length > 1 ? values : values[0]);    }    return context;}
public void nutch_f1856_0() throws Exception
{    Configuration conf = NutchConfiguration.create();    conf.set("index.jexl.filter", "doc.lang=='en'");    JexlIndexingFilter filter = new JexlIndexingFilter();    filter.setConf(conf);    Assert.assertNotNull(filter);    NutchDocument doc = new NutchDocument();    String title = "The Foo Page";    Outlink[] outlinks = new Outlink[] { new Outlink("http://foo.com/", "Foo") };    Metadata metaData = new Metadata();    metaData.add("Language", "en/us");    ParseData parseData = new ParseData(ParseStatus.STATUS_SUCCESS, title, outlinks, metaData);    ParseImpl parse = new ParseImpl("this is a sample foo bar page. hope you enjoy it.", parseData);    CrawlDatum crawlDatum = new CrawlDatum();    crawlDatum.setFetchTime(100L);    Inlinks inlinks = new Inlinks();    doc.add("lang", "en");    NutchDocument result = filter.filter(doc, parse, new Text("http://nutch.apache.org/index.html"), crawlDatum, inlinks);    Assert.assertNotNull(result);    Assert.assertEquals(doc, result);}
public void nutch_f1857_0() throws Exception
{    Configuration conf = NutchConfiguration.create();    conf.set("index.jexl.filter", "doc.lang=='en'");    JexlIndexingFilter filter = new JexlIndexingFilter();    filter.setConf(conf);    Assert.assertNotNull(filter);    NutchDocument doc = new NutchDocument();    String title = "The Foo Page";    Outlink[] outlinks = new Outlink[] { new Outlink("http://foo.com/", "Foo") };    Metadata metaData = new Metadata();    metaData.add("Language", "en/us");    ParseData parseData = new ParseData(ParseStatus.STATUS_SUCCESS, title, outlinks, metaData);    ParseImpl parse = new ParseImpl("this is a sample foo bar page. hope you enjoy it.", parseData);    CrawlDatum crawlDatum = new CrawlDatum();    crawlDatum.setFetchTime(100L);    Inlinks inlinks = new Inlinks();    doc.add("lang", "ru");    NutchDocument result = filter.filter(doc, parse, new Text("http://nutch.apache.org/index.html"), crawlDatum, inlinks);    Assert.assertNull(result);}
public void nutch_f1858_0() throws Exception
{    Configuration conf = NutchConfiguration.create();    JexlIndexingFilter filter = new JexlIndexingFilter();    thrown.expect(RuntimeException.class);    filter.setConf(conf);}
public void nutch_f1859_0() throws Exception
{    Configuration conf = NutchConfiguration.create();    conf.set("index.jexl.filter", "doc.lang=<>:='en'");    JexlIndexingFilter filter = new JexlIndexingFilter();    thrown.expect(RuntimeException.class);    filter.setConf(conf);}
public NutchDocument nutch_f1860_1(NutchDocument doc, Parse parse, Text url, CrawlDatum datum, Inlinks inlinks) throws IndexingException
{        Outlink[] outlinks = parse.getData().getOutlinks();    if (outlinks != null) {        Set<String> hosts = new HashSet<String>();        for (Outlink outlink : outlinks) {            try {                String linkUrl = outlink.getToUrl();                String outHost = new URL(linkUrl).getHost().toLowerCase();                if (indexHost) {                    linkUrl = outHost;                    if (hosts.contains(linkUrl))                        continue;                    hosts.add(linkUrl);                }                addFilteredLink("outlinks", url.toString(), linkUrl, outHost, filterOutlinks, doc);            } catch (MalformedURLException e) {                            }        }    }        if (null != inlinks) {        Iterator<Inlink> iterator = inlinks.iterator();        Set<String> inlinkHosts = new HashSet<String>();        while (iterator.hasNext()) {            try {                Inlink link = iterator.next();                String linkUrl = link.getFromUrl();                String inHost = new URL(linkUrl).getHost().toLowerCase();                if (indexHost) {                    linkUrl = inHost;                    if (inlinkHosts.contains(linkUrl))                        continue;                    inlinkHosts.add(linkUrl);                }                addFilteredLink("inlinks", url.toString(), linkUrl, inHost, filterInlinks, doc);            } catch (MalformedURLException e) {                            }        }    }    return doc;}
private void nutch_f1861_0(String fieldName, String url, String linkUrl, String urlHost, boolean filter, NutchDocument doc) throws MalformedURLException
{    if (filter) {        String host = new URL(url.toString()).getHost().toLowerCase();        if (!host.equalsIgnoreCase(urlHost)) {            doc.add(fieldName, linkUrl);        }    } else {        doc.add(fieldName, linkUrl);    }}
public void nutch_f1862_0(Configuration conf)
{    this.conf = conf;    filterOutlinks = conf.getBoolean(LINKS_OUTLINKS_HOST, false);    filterInlinks = conf.getBoolean(LINKS_INLINKS_HOST, false);    indexHost = conf.getBoolean(LINKS_ONLY_HOSTS, false);}
public Configuration nutch_f1863_0()
{    return this.conf;}
public void nutch_f1864_0() throws Exception
{    metadata.add(Response.CONTENT_TYPE, "text/html");}
private Outlink[] nutch_f1865_0() throws Exception
{    return generateOutlinks(false);}
private Outlink[] nutch_f1866_0(boolean parts) throws Exception
{    Outlink[] outlinks = new Outlink[2];    outlinks[0] = new Outlink("http://www.test.com", "test");    outlinks[1] = new Outlink("http://www.example.com", "example");    if (parts) {        outlinks[0] = new Outlink(outlinks[0].getToUrl() + "/index.php?param=1", "test");        outlinks[1] = new Outlink(outlinks[1].getToUrl() + "/index.php?param=2", "test");    }    return outlinks;}
public void nutch_f1867_0() throws Exception
{    conf.set(LinksIndexingFilter.LINKS_OUTLINKS_HOST, "true");    filter.setConf(conf);    Outlink[] outlinks = generateOutlinks();    NutchDocument doc = filter.filter(new NutchDocument(), new ParseImpl("text", new ParseData(new ParseStatus(), "title", outlinks, metadata)), new Text("http://www.example.com/"), new CrawlDatum(), new Inlinks());    Assert.assertEquals(1, doc.getField("outlinks").getValues().size());    Assert.assertEquals("Filter outlinks, allow only those from a different host", outlinks[0].getToUrl(), doc.getFieldValue("outlinks"));}
public void nutch_f1868_0() throws Exception
{    conf.set(LinksIndexingFilter.LINKS_INLINKS_HOST, "true");    filter.setConf(conf);    Inlinks inlinks = new Inlinks();    inlinks.add(new Inlink("http://www.test.com", "test"));    inlinks.add(new Inlink("http://www.example.com", "example"));    NutchDocument doc = filter.filter(new NutchDocument(), new ParseImpl("text", new ParseData(new ParseStatus(), "title", new Outlink[0], metadata)), new Text("http://www.example.com/"), new CrawlDatum(), inlinks);    Assert.assertEquals(1, doc.getField("inlinks").getValues().size());    Assert.assertEquals("Filter inlinks, allow only those from a different host", "http://www.test.com", doc.getFieldValue("inlinks"));}
public void nutch_f1869_0() throws Exception
{    filter.setConf(conf);    Outlink[] outlinks = generateOutlinks();    NutchDocument doc = filter.filter(new NutchDocument(), new ParseImpl("text", new ParseData(new ParseStatus(), "title", outlinks, metadata)), new Text("http://www.example.com/"), new CrawlDatum(), new Inlinks());    Assert.assertEquals("All outlinks must be indexed even those from the same host", outlinks.length, doc.getField("outlinks").getValues().size());}
public void nutch_f1870_0() throws Exception
{    conf.set(LinksIndexingFilter.LINKS_INLINKS_HOST, "false");    filter.setConf(conf);    Inlinks inlinks = new Inlinks();    inlinks.add(new Inlink("http://www.test.com", "test"));    inlinks.add(new Inlink("http://www.example.com", "example"));    NutchDocument doc = filter.filter(new NutchDocument(), new ParseImpl("text", new ParseData(new ParseStatus(), "title", new Outlink[0], metadata)), new Text("http://www.example.com/"), new CrawlDatum(), inlinks);    Assert.assertEquals("All inlinks must be indexed even those from the same host", inlinks.size(), doc.getField("inlinks").getValues().size());}
public void nutch_f1871_0() throws Exception
{    conf.set(LinksIndexingFilter.LINKS_INLINKS_HOST, "true");    conf.set(LinksIndexingFilter.LINKS_OUTLINKS_HOST, "true");    conf.set(LinksIndexingFilter.LINKS_ONLY_HOSTS, "true");    filter.setConf(conf);    Outlink[] outlinks = generateOutlinks(true);    Inlinks inlinks = new Inlinks();    inlinks.add(new Inlink("http://www.test.com/one-awesome-page", "test"));    inlinks.add(new Inlink("http://www.test.com/other-awesome-page", "test"));    inlinks.add(new Inlink("http://www.example.com/my-first-awesome-example", "example"));    NutchDocument doc = filter.filter(new NutchDocument(), new ParseImpl("text", new ParseData(new ParseStatus(), "title", outlinks, metadata)), new Text("http://www.example.com/"), new CrawlDatum(), inlinks);    NutchField docOutlinks = doc.getField("outlinks");    Assert.assertEquals("Only the host portion of the outlink URL must be indexed", new URL("http://www.test.com").getHost(), docOutlinks.getValues().get(0));    Assert.assertEquals("The inlinks coming from the same host must count only once", 1, doc.getField("inlinks").getValues().size());    Assert.assertEquals("Only the host portion of the inlinks URL must be indexed", new URL("http://www.test.com").getHost(), doc.getFieldValue("inlinks"));}
public void nutch_f1872_0() throws Exception
{    conf = NutchConfiguration.create();    conf.set(LinksIndexingFilter.LINKS_ONLY_HOSTS, "true");    conf.set(LinksIndexingFilter.LINKS_OUTLINKS_HOST, "true");    Outlink[] outlinks = generateOutlinks(true);    filter.setConf(conf);    NutchDocument doc = filter.filter(new NutchDocument(), new ParseImpl("text", new ParseData(new ParseStatus(), "title", outlinks, metadata)), new Text("http://www.example.com/"), new CrawlDatum(), new Inlinks());    Assert.assertEquals(1, doc.getField("outlinks").getValues().size());    Assert.assertEquals("Index only the host portion of the outlinks after filtering", new URL("http://www.test.com").getHost(), doc.getFieldValue("outlinks"));}
public void nutch_f1873_0() throws Exception
{    conf = NutchConfiguration.create();    conf.set(LinksIndexingFilter.LINKS_ONLY_HOSTS, "true");    conf.set(LinksIndexingFilter.LINKS_INLINKS_HOST, "true");    filter.setConf(conf);    Inlinks inlinks = new Inlinks();    inlinks.add(new Inlink("http://www.test.com", "test"));    inlinks.add(new Inlink("http://www.example.com", "example"));    NutchDocument doc = filter.filter(new NutchDocument(), new ParseImpl("text", new ParseData(new ParseStatus(), "title", new Outlink[0], metadata)), new Text("http://www.example.com/"), new CrawlDatum(), inlinks);    Assert.assertEquals(1, doc.getField("inlinks").getValues().size());    Assert.assertEquals("Index only the host portion of the inlinks after filtering", new URL("http://www.test.com").getHost(), doc.getFieldValue("inlinks"));}
public NutchDocument nutch_f1874_0(NutchDocument doc, Parse parse, Text url, CrawlDatum datum, Inlinks inlinks) throws IndexingException
{        if (doc == null)        return doc;        if (dbFieldnames != null) {        for (String metatag : dbFieldnames) {            Writable metadata = datum.getMetaData().get(new Text(metatag));            if (metadata != null)                add(doc, metatag, metadata.toString());        }    }        if (parseFieldnames != null) {        for (String metatag : parseFieldnames.keySet()) {            for (String value : parse.getData().getParseMeta().getValues(metatag)) {                if (value != null)                    add(doc, parseFieldnames.get(metatag), value);            }        }    }        if (contentFieldnames != null) {        for (String metatag : contentFieldnames) {            for (String value : parse.getData().getContentMeta().getValues(metatag)) {                if (value != null)                    add(doc, metatag, value);            }        }    }    return doc;}
protected void nutch_f1875_0(NutchDocument doc, String key, String value)
{    if (separator == null || value.indexOf(separator) == -1 || !mvFields.contains(key)) {        value = value.trim();        if (!value.isEmpty()) {            doc.add(key, value);        }    } else {        String[] parts = value.split(separator);        for (String part : parts) {            part = part.trim();            if (!part.isEmpty()) {                doc.add(key, part);            }        }    }}
public void nutch_f1876_0(Configuration conf)
{    this.conf = conf;    dbFieldnames = conf.getStrings(db_CONF_PROPERTY);    parseFieldnames = new HashMap<String, String>();    for (String metatag : conf.getStrings(parse_CONF_PROPERTY)) {        parseFieldnames.put(metatag.toLowerCase(Locale.ROOT), metatag);    }    contentFieldnames = conf.getStrings(content_CONF_PROPERTY);    separator = conf.get(separator_CONF_PROPERTY, null);    mvFields = new HashSet(Arrays.asList(conf.getStrings(mvfields_CONF_PROPERTY, new String[0])));}
public Configuration nutch_f1877_0()
{    return this.conf;}
public NutchDocument nutch_f1878_0(NutchDocument doc, Parse parse, Text url, CrawlDatum datum, Inlinks inlinks) throws IndexingException
{    String url_s = url.toString();    addTime(doc, parse.getData(), url_s, datum);    addLength(doc, parse.getData(), url_s);    addType(doc, parse.getData(), url_s, datum);    resetTitle(doc, parse.getData(), url_s);    return doc;}
private NutchDocument nutch_f1879_0(NutchDocument doc, ParseData data, String url, CrawlDatum datum)
{    long time = -1;    String lastModified = data.getMeta(Metadata.LAST_MODIFIED);    if (lastModified != null) {                        time = getTime(lastModified, url);                if (time > -1) {            doc.add("lastModified", new Date(time));        }    }    if (time == -1) {                        time = datum.getModifiedTime();        if (time <= 0) {                                    time = datum.getFetchTime();                }    }        doc.add("date", new Date(time));    return doc;}
private long nutch_f1880_1(String date, String url)
{    long time = -1;    try {        time = HttpDateFormat.toLong(date);    } catch (ParseException e) {                try {            Date parsedDate = DateUtils.parseDate(date, new String[] { "EEE MMM dd HH:mm:ss yyyy", "EEE MMM dd HH:mm:ss yyyy zzz", "EEE MMM dd HH:mm:ss zzz yyyy", "EEE, MMM dd HH:mm:ss yyyy zzz", "EEE, dd MMM yyyy HH:mm:ss zzz", "EEE,dd MMM yyyy HH:mm:ss zzz", "EEE, dd MMM yyyy HH:mm:sszzz", "EEE, dd MMM yyyy HH:mm:ss", "EEE, dd-MMM-yy HH:mm:ss zzz", "yyyy/MM/dd HH:mm:ss.SSS zzz", "yyyy/MM/dd HH:mm:ss.SSS", "yyyy/MM/dd HH:mm:ss zzz", "yyyy/MM/dd", "yyyy.MM.dd HH:mm:ss", "yyyy-MM-dd HH:mm", "MMM dd yyyy HH:mm:ss. zzz", "MMM dd yyyy HH:mm:ss zzz", "dd.MM.yyyy HH:mm:ss zzz", "dd MM yyyy HH:mm:ss zzz", "dd.MM.yyyy; HH:mm:ss", "dd.MM.yyyy HH:mm:ss", "dd.MM.yyyy zzz", "yyyy-MM-dd'T'HH:mm:ssXXX" });            time = parsedDate.getTime();                                } catch (Exception e2) {            if (LOG.isWarnEnabled()) {                            }        }    }    return time;}
private NutchDocument nutch_f1881_0(NutchDocument doc, ParseData data, String url)
{    String contentLength = data.getMeta(Response.CONTENT_LENGTH);    if (contentLength != null) {                String trimmed = contentLength.toString().trim();        if (!trimmed.isEmpty())            doc.add("contentLength", trimmed);    }    return doc;}
private NutchDocument nutch_f1882_0(NutchDocument doc, ParseData data, String url, CrawlDatum datum)
{    String mimeType = null;    String contentType = null;    Writable tcontentType = datum.getMetaData().get(new Text(Response.CONTENT_TYPE));    if (tcontentType != null) {        contentType = tcontentType.toString();    } else        contentType = data.getMeta(Response.CONTENT_TYPE);    if (contentType == null) {                                                                                                mimeType = tika.detect(url);    } else {        mimeType = MIME.forName(MimeUtil.cleanMimeType(contentType));    }        if (mimeType == null) {        return doc;    }        if (mapMimes) {                if (mimeMap.containsKey(mimeType)) {            if (mapFieldName != null) {                doc.add(mapFieldName, mimeMap.get(mimeType));            } else {                mimeType = mimeMap.get(mimeType);            }        }    }    contentType = mimeType;    doc.add("type", contentType);        if (conf.getBoolean("moreIndexingFilter.indexMimeTypeParts", true)) {        String[] parts = getParts(contentType);        for (String part : parts) {            doc.add("type", part);        }    }    return doc;}
 static String[] nutch_f1883_0(String mimeType)
{    return mimeType.split("/");}
private NutchDocument nutch_f1884_0(NutchDocument doc, ParseData data, String url)
{    String contentDisposition = data.getMeta(Metadata.CONTENT_DISPOSITION);    if (contentDisposition == null || doc.getFieldValue("title") != null)        return doc;    for (int i = 0; i < patterns.length; i++) {        Matcher matcher = patterns[i].matcher(contentDisposition);        if (matcher.find()) {            doc.add("title", matcher.group(1));            break;        }    }    return doc;}
public void nutch_f1885_1(Configuration conf)
{    this.conf = conf;    MIME = new MimeUtil(conf);    if (conf.getBoolean("moreIndexingFilter.mapMimeTypes", false)) {        mapMimes = true;        mapFieldName = conf.get("moreIndexingFilter.mapMimeTypes.field");                try {            readConfiguration();        } catch (Exception e) {                    }    }}
public Configuration nutch_f1886_0()
{    return this.conf;}
private void nutch_f1887_1() throws IOException
{        BufferedReader reader = new BufferedReader(conf.getConfResourceAsReader("contenttype-mapping.txt"));    String line;    String[] parts;    boolean formatWarningShown = false;    mimeMap = new HashMap<String, String>();    while ((line = reader.readLine()) != null) {        if (StringUtils.isNotBlank(line) && !line.startsWith("#")) {            line = line.trim();            parts = line.split("\t");                        if (parts.length > 1) {                for (int i = 1; i < parts.length; i++) {                    mimeMap.put(parts[i].trim(), parts[0].trim());                }            } else {                                if (!formatWarningShown) {                                        formatWarningShown = true;                }            }        }    }}
public void nutch_f1888_0() throws IndexingException
{    Configuration conf = NutchConfiguration.create();    assertContentType(conf, "text/html", "text/html");    assertContentType(conf, "text/html; charset=UTF-8", "text/html");}
public void nutch_f1889_0()
{    String[] parts = MoreIndexingFilter.getParts("text/html");    assertParts(parts, 2, "text", "html");}
public void nutch_f1890_0()
{    Configuration conf = NutchConfiguration.create();    conf.setBoolean("moreIndexingFilter.indexMimeTypeParts", false);    MoreIndexingFilter filter = new MoreIndexingFilter();    filter.setConf(conf);    Assert.assertNotNull(filter);    NutchDocument doc = new NutchDocument();    ParseImpl parse = new ParseImpl("foo bar", new ParseData());    try {        filter.filter(doc, parse, new Text("http://nutch.apache.org/index.html"), new CrawlDatum(), new Inlinks());    } catch (Exception e) {        e.printStackTrace();        Assert.fail(e.getMessage());    }    Assert.assertNotNull(doc);    Assert.assertTrue(doc.getFieldNames().contains("type"));    Assert.assertEquals(1, doc.getField("type").getValues().size());    Assert.assertEquals("text/html", doc.getFieldValue("type"));}
public void nutch_f1891_0() throws IndexingException
{    Configuration conf = NutchConfiguration.create();    Metadata metadata = new Metadata();    metadata.add(Response.CONTENT_DISPOSITION, "filename=filename.ext");    MoreIndexingFilter filter = new MoreIndexingFilter();    filter.setConf(conf);    Text url = new Text("http://www.example.com/");    ParseImpl parseImpl = new ParseImpl("text", new ParseData(new ParseStatus(), "title", new Outlink[0], metadata));    NutchDocument doc = new NutchDocument();    doc = filter.filter(doc, parseImpl, url, new CrawlDatum(), new Inlinks());    Assert.assertEquals("content-disposition not detected", "filename.ext", doc.getFieldValue("title"));    /* NUTCH-1140: do not add second title to avoid a multi-valued title field */    doc = new NutchDocument();    doc.add("title", "title");    doc = filter.filter(doc, parseImpl, url, new CrawlDatum(), new Inlinks());    Assert.assertEquals("do not add second title by content-disposition", "title", doc.getFieldValue("title"));}
private void nutch_f1892_0(String[] parts, int count, String... expected)
{    Assert.assertEquals(count, parts.length);    for (int i = 0; i < expected.length; i++) {        Assert.assertEquals(expected[i], parts[i]);    }}
private void nutch_f1893_0(Configuration conf, String source, String expected) throws IndexingException
{    Metadata metadata = new Metadata();    metadata.add(Response.CONTENT_TYPE, source);    MoreIndexingFilter filter = new MoreIndexingFilter();    filter.setConf(conf);    NutchDocument doc = filter.filter(new NutchDocument(), new ParseImpl("text", new ParseData(new ParseStatus(), "title", new Outlink[0], metadata)), new Text("http://www.example.com/"), new CrawlDatum(), new Inlinks());    Assert.assertEquals("mime type not detected", expected, doc.getFieldValue("type"));}
public void nutch_f1894_0() throws IndexingException
{    Configuration conf = NutchConfiguration.create();    Metadata metadata = new Metadata();    MoreIndexingFilter filter = new MoreIndexingFilter();    filter.setConf(conf);    Text url = new Text("http://www.example.com/");    ParseImpl parseImpl = new ParseImpl("text", new ParseData(new ParseStatus(), "title", new Outlink[0], metadata));    CrawlDatum fetchDatum = new CrawlDatum();    NutchDocument doc = new NutchDocument();        long dateEpocheSeconds = 1537898340;    fetchDatum.setModifiedTime(dateEpocheSeconds * 1000);        fetchDatum.setFetchTime((dateEpocheSeconds + 30 * 24 * 60 * 60) * 1000);                doc = filter.filter(doc, parseImpl, url, fetchDatum, new Inlinks());    Assert.assertEquals("last fetch date not extracted", new Date(dateEpocheSeconds * 1000), doc.getFieldValue("date"));        Date lastModifiedDate = new Date((dateEpocheSeconds - 7 * 24 * 60 * 60) * 1000);    String lastModifiedDateStr = DateTimeFormatter.ISO_INSTANT.format(lastModifiedDate.toInstant());    parseImpl.getData().getParseMeta().set(Metadata.LAST_MODIFIED, lastModifiedDateStr);    doc = filter.filter(doc, parseImpl, url, fetchDatum, new Inlinks());    Assert.assertEquals("last-modified date not extracted", lastModifiedDate, doc.getFieldValue("lastModified"));}
public String nutch_f1895_0()
{    return this.fieldName;}
public String nutch_f1896_0()
{    return this.toFieldName;}
public Pattern nutch_f1897_0()
{    return this.pattern;}
public String nutch_f1898_0()
{    return this.replacement;}
public boolean nutch_f1899_0()
{    return this.isValid;}
public String nutch_f1900_0(String value)
{    if (this.isValid) {        return this.pattern.matcher(value).replaceAll(replacement);    } else {        return value;    }}
public String nutch_f1901_0(String fieldName, String value)
{    if (this.fieldName.equals(fieldName)) {        if (value != null && value.length() > 0) {            if (this.isValid) {                Matcher m = this.pattern.matcher(value);                if (m.find()) {                    return m.replaceAll(this.replacement);                }            }        }    }    return null;}
public void nutch_f1902_1(Configuration conf)
{    this.conf = conf;    FIELDREPLACERS_BY_HOST.clear();    FIELDREPLACERS_BY_URL.clear();    String value = conf.get("index.replace.regexp", null);    if (value != null) {                this.parseConf(value);    }}
public Configuration nutch_f1903_0()
{    return this.conf;}
private void nutch_f1904_1(String propertyValue)
{    if (propertyValue == null || propertyValue.trim().length() == 0) {        return;    }        Pattern hostPattern = Pattern.compile(".*");    Pattern urlPattern = null;        Matcher lineMatcher = LINE_SPLIT.matcher(propertyValue);    while (lineMatcher.find()) {        String line = lineMatcher.group();        if (line != null && line.length() > 0) {                        Matcher nameValueMatcher = NAME_VALUE_SPLIT.matcher(line.trim());            if (nameValueMatcher.find()) {                String fieldName = nameValueMatcher.group(1).trim();                String value = nameValueMatcher.group(2);                if (fieldName != null && value != null) {                                        if (HOSTMATCH.equals(fieldName)) {                        urlPattern = null;                        try {                            hostPattern = Pattern.compile(value);                        } catch (PatternSyntaxException pse) {                                                                                    hostPattern = Pattern.compile("willnotmatchanyhost");                        }                    } else if (URLMATCH.equals(fieldName)) {                        try {                            urlPattern = Pattern.compile(value);                        } catch (PatternSyntaxException pse) {                                                                                    urlPattern = Pattern.compile("willnotmatchanyurl");                        }                    } else if (value.length() > 3) {                        String toFieldName = fieldName;                                                if (fieldName.indexOf(':') > 0) {                            toFieldName = fieldName.substring(fieldName.indexOf(':') + 1);                            fieldName = fieldName.substring(0, fieldName.indexOf(':'));                        }                        String sep = value.substring(0, 1);                                                value = value.substring(1);                        if (!value.contains(sep)) {                                                        continue;                        }                        String pattern = value.substring(0, value.indexOf(sep));                        value = value.substring(pattern.length() + 1);                        String replacement = value;                        if (value.contains(sep)) {                            replacement = value.substring(0, value.indexOf(sep));                        }                        int flags = 0;                        if (value.length() > replacement.length() + 1) {                            value = value.substring(replacement.length() + 1).trim();                            try {                                flags = Integer.parseInt(value);                            } catch (NumberFormatException e) {                                                                continue;                            }                        }                        Integer iFlags = (flags > 0) ? Integer.valueOf(flags) : null;                                                FieldReplacer fr = new FieldReplacer(fieldName, toFieldName, pattern, replacement, iFlags);                                                if (urlPattern != null) {                            List<FieldReplacer> lfp = FIELDREPLACERS_BY_URL.get(urlPattern);                            if (lfp == null) {                                lfp = new ArrayList<FieldReplacer>();                            }                            lfp.add(fr);                            FIELDREPLACERS_BY_URL.put(urlPattern, lfp);                        } else {                            List<FieldReplacer> lfp = FIELDREPLACERS_BY_HOST.get(hostPattern);                            if (lfp == null) {                                lfp = new ArrayList<FieldReplacer>();                            }                            lfp.add(fr);                            FIELDREPLACERS_BY_HOST.put(hostPattern, lfp);                        }                    }                }            }        }    }}
public NutchDocument nutch_f1905_0(NutchDocument doc, Parse parse, Text url, CrawlDatum datum, Inlinks inlinks) throws IndexingException
{    if (doc != null) {        if (FIELDREPLACERS_BY_HOST.size() > 0) {            this.doReplace(doc, "host", FIELDREPLACERS_BY_HOST);        }        if (FIELDREPLACERS_BY_URL.size() > 0) {            this.doReplace(doc, "url", FIELDREPLACERS_BY_URL);        }    }    return doc;}
private void nutch_f1906_0(NutchDocument doc, String keyName, Map<Pattern, List<FieldReplacer>> replaceMap)
{    if (doc == null || replaceMap.size() == 0) {        return;    }    Collection<String> docFieldNames = doc.getFieldNames();    NutchField keyField = doc.getField(keyName);    if (keyField == null) {                return;    }    List<Object> keyFieldValues = keyField.getValues();    if (keyFieldValues.size() == 0) {                return;    }        for (Object oKeyFieldValue : keyFieldValues) {        if (oKeyFieldValue != null && oKeyFieldValue instanceof java.lang.String) {            String keyFieldValue = (String) oKeyFieldValue;                        for (Map.Entry<Pattern, List<FieldReplacer>> entries : replaceMap.entrySet()) {                                if (entries.getKey().matcher(keyFieldValue).find()) {                                        for (FieldReplacer fp : entries.getValue()) {                        String fieldName = fp.getFieldName();                                                if (docFieldNames.contains(fieldName)) {                            NutchField docField = doc.getField(fieldName);                            List<Object> fieldValues = docField.getValues();                            ArrayList<String> newFieldValues = new ArrayList<String>();                                                        for (Object oFieldValue : fieldValues) {                                if (oFieldValue != null && oFieldValue instanceof java.lang.String) {                                    String fieldValue = (String) oFieldValue;                                    String newValue = fp.replace(fieldValue);                                    newFieldValues.add(newValue);                                }                            }                                                        String targetFieldName = fp.getToFieldName();                            doc.removeField(targetFieldName);                            for (String newFieldValue : newFieldValues) {                                doc.add(targetFieldName, newFieldValue);                            }                        }                    }                }            }        }    }}
public NutchDocument nutch_f1907_0(String fileName, Configuration conf)
{    NutchDocument doc = new NutchDocument();    BasicIndexingFilter basicIndexer = new BasicIndexingFilter();    basicIndexer.setConf(conf);    Assert.assertNotNull(basicIndexer);    MetadataIndexer metaIndexer = new MetadataIndexer();    metaIndexer.setConf(conf);    Assert.assertNotNull(basicIndexer);    ReplaceIndexer replaceIndexer = new ReplaceIndexer();    replaceIndexer.setConf(conf);    Assert.assertNotNull(replaceIndexer);    try {        String urlString = "file:" + sampleDir + fileSeparator + fileName;        Text text = new Text(urlString);        CrawlDatum crawlDatum = new CrawlDatum();        Protocol protocol = new ProtocolFactory(conf).getProtocol(urlString);        Content content = protocol.getProtocolOutput(text, crawlDatum).getContent();        Parse parse = new ParseUtil(conf).parse(content).get(content.getUrl());        crawlDatum.setFetchTime(100L);        Inlinks inlinks = new Inlinks();        doc = basicIndexer.filter(doc, parse, text, crawlDatum, inlinks);        doc = metaIndexer.filter(doc, parse, text, crawlDatum, inlinks);        doc = replaceIndexer.filter(doc, parse, text, crawlDatum, inlinks);    } catch (Exception e) {        e.printStackTrace();        Assert.fail(e.toString());    }    return doc;}
public void nutch_f1908_0()
{    Configuration conf = NutchConfiguration.create();    String indexReplaceProperty = "  metatag.description=/this(.*)plugin/this awesome plugin/2\n" + "  metatag.keywords=/\\,/\\!/\n" + "  hostmatch=.*.com\n" + "  metatag.keywords=/\\,/\\?/\n" + "  metatag.author:dc_author=/\\s+/ David /\n" + "  urlmatch=.*.html\n" + "  metatag.keywords=/\\,/\\./\n" + "  metatag.author=/\\s+/ D. /\n";    conf.set(INDEX_REPLACE_PROPERTY, indexReplaceProperty);    ReplaceIndexer rp = new ReplaceIndexer();    try {        rp.setConf(conf);    } catch (RuntimeException ohno) {        Assert.fail("Unable to parse a valid index.replace.regexp property! " + ohno.getMessage());    }    Configuration parsedConf = rp.getConf();        Assert.assertEquals(indexReplaceProperty, parsedConf.get(INDEX_REPLACE_PROPERTY));}
public void nutch_f1909_0()
{    String expectedDescription = "With this awesome plugin, I control the description! Bwuhuhuhaha!";    String expectedKeywords = "Breathtaking! Riveting! Two Thumbs Up!";    String expectedAuthor = "Peter D. Ciuffetti";    String indexReplaceProperty = "  metatag.description=/this(.*)plugin/this awesome plugin/\n" + "  metatag.keywords=/\\,/\\!/\n" + "  metatag.author=/\\s+/ D. /\n";    Configuration conf = NutchConfiguration.create();    conf.set("plugin.includes", "protocol-file|urlfilter-regex|parse-(html|metatags)|index-(basic|anchor|metadata|static|replace)|urlnormalizer-(pass|regex|basic)");    conf.set(INDEX_REPLACE_PROPERTY, indexReplaceProperty);    conf.set("metatags.names", "author,description,keywords");    conf.set("index.parse.md", "metatag.author,metatag.description,metatag.keywords");        conf.set("http.timeout", "99999999999");        NutchDocument doc = parseAndFilterFile(sampleFile, conf);    Assert.assertEquals(expectedDescription, doc.getFieldValue("metatag.description"));    Assert.assertEquals(expectedKeywords, doc.getFieldValue("metatag.keywords"));    Assert.assertEquals(expectedAuthor, doc.getFieldValue("metatag.author"));}
public void nutch_f1910_0()
{    String expectedDescription = "With this plugin, I control the description! Bwuhuhuhaha!";    String expectedKeywords = "Breathtaking, Riveting, Two Thumbs Up!";    String expectedAuthor = "Peter Ciuffetti";        String indexReplaceProperty = "  metatag.description=/this\\s+**plugin/this awesome plugin/\n" + "  metatag.keywords=/\\,/\\!/what\n" + " metatag.author=#notcomplete";    Configuration conf = NutchConfiguration.create();    conf.set("plugin.includes", "protocol-file|urlfilter-regex|parse-(html|metatags)|index-(basic|anchor|metadata|static|replace)|urlnormalizer-(pass|regex|basic)");    conf.set(INDEX_REPLACE_PROPERTY, indexReplaceProperty);    conf.set("metatags.names", "author,description,keywords");    conf.set("index.parse.md", "metatag.author,metatag.description,metatag.keywords");        conf.set("http.timeout", "99999999999");        NutchDocument doc = parseAndFilterFile(sampleFile, conf);        Assert.assertEquals(expectedDescription, doc.getFieldValue("metatag.description"));    Assert.assertEquals(expectedKeywords, doc.getFieldValue("metatag.keywords"));    Assert.assertEquals(expectedAuthor, doc.getFieldValue("metatag.author"));}
public void nutch_f1911_0()
{    String expectedDescription = "With this awesome plugin, I control the description! Bwuhuhuhaha!";    String expectedKeywords = "Breathtaking! Riveting! Two Thumbs Up!";    String expectedAuthor = "Peter D. Ciuffetti";    String indexReplaceProperty = " urlmatch=.*.html\n" + "  metatag.description=/this(.*)plugin/this awesome plugin/\n" + "  metatag.keywords=/\\,/\\!/\n" + "  metatag.author=/\\s+/ D. /\n";    Configuration conf = NutchConfiguration.create();    conf.set("plugin.includes", "protocol-file|urlfilter-regex|parse-(html|metatags)|index-(basic|anchor|metadata|static|replace)|urlnormalizer-(pass|regex|basic)");    conf.set(INDEX_REPLACE_PROPERTY, indexReplaceProperty);    conf.set("metatags.names", "author,description,keywords");    conf.set("index.parse.md", "metatag.author,metatag.description,metatag.keywords");        conf.set("http.timeout", "99999999999");        NutchDocument doc = parseAndFilterFile(sampleFile, conf);        Assert.assertEquals(expectedDescription, doc.getFieldValue("metatag.description"));    Assert.assertEquals(expectedKeywords, doc.getFieldValue("metatag.keywords"));    Assert.assertEquals(expectedAuthor, doc.getFieldValue("metatag.author"));}
public void nutch_f1912_0()
{    String expectedDescription = "With this plugin, I control the description! Bwuhuhuhaha!";    String expectedKeywords = "Breathtaking, Riveting, Two Thumbs Up!";    String expectedAuthor = "Peter Ciuffetti";    String indexReplaceProperty = " urlmatch=.*.xml\n" + "  metatag.description=/this(.*)plugin/this awesome plugin/\n" + "  metatag.keywords=/\\,/\\!/\n" + "  metatag.author=/\\s+/ D. /\n";    Configuration conf = NutchConfiguration.create();    conf.set("plugin.includes", "protocol-file|urlfilter-regex|parse-(html|metatags)|index-(basic|anchor|metadata|static|replace)|urlnormalizer-(pass|regex|basic)");    conf.set(INDEX_REPLACE_PROPERTY, indexReplaceProperty);    conf.set("metatags.names", "author,description,keywords");    conf.set("index.parse.md", "metatag.author,metatag.description,metatag.keywords");        conf.set("http.timeout", "99999999999");        NutchDocument doc = parseAndFilterFile(sampleFile, conf);        Assert.assertEquals(expectedDescription, doc.getFieldValue("metatag.description"));    Assert.assertEquals(expectedKeywords, doc.getFieldValue("metatag.keywords"));    Assert.assertEquals(expectedAuthor, doc.getFieldValue("metatag.author"));}
public void nutch_f1913_0()
{    String expectedDescription = "With this awesome plugin, I control the description! Bwuhuhuhaha!";    String expectedKeywords = "Breathtaking! Riveting! Two Thumbs Up!";    String expectedAuthor = "Peter D. Ciuffetti";    String indexReplaceProperty = "  metatag.description=/this(.*)plugin/this$1awesome$1plugin/\n" + "  urlmatch=.*.html\n" + "  metatag.keywords=/\\,/\\!/\n" + "  metatag.author=/\\s+/ D. /\n";    Configuration conf = NutchConfiguration.create();    conf.set("plugin.includes", "protocol-file|urlfilter-regex|parse-(html|metatags)|index-(basic|anchor|metadata|static|replace)|urlnormalizer-(pass|regex|basic)");    conf.set(INDEX_REPLACE_PROPERTY, indexReplaceProperty);    conf.set("metatags.names", "author,description,keywords");    conf.set("index.parse.md", "metatag.author,metatag.description,metatag.keywords");        conf.set("http.timeout", "99999999999");        NutchDocument doc = parseAndFilterFile(sampleFile, conf);        Assert.assertEquals(expectedDescription, doc.getFieldValue("metatag.description"));    Assert.assertEquals(expectedKeywords, doc.getFieldValue("metatag.keywords"));    Assert.assertEquals(expectedAuthor, doc.getFieldValue("metatag.author"));}
public void nutch_f1914_0()
{    String expectedDescription = "With this awesome plugin, I control the description! Bwuhuhuhaha!";    String expectedKeywords = "Breathtaking, Riveting, Two Thumbs Up!";    String expectedAuthor = "Peter Ciuffetti";    String indexReplaceProperty = "  metatag.description=/this(.*)plugin/this$1awesome$1plugin/\n" + "  urlmatch=.*.xml\n" + "  metatag.keywords=/\\,/\\!/\n" + "  metatag.author=/\\s+/ D. /\n";    Configuration conf = NutchConfiguration.create();    conf.set("plugin.includes", "protocol-file|urlfilter-regex|parse-(html|metatags)|index-(basic|anchor|metadata|static|replace)|urlnormalizer-(pass|regex|basic)");    conf.set(INDEX_REPLACE_PROPERTY, indexReplaceProperty);    conf.set("metatags.names", "author,description,keywords");    conf.set("index.parse.md", "metatag.author,metatag.description,metatag.keywords");        conf.set("http.timeout", "99999999999");        NutchDocument doc = parseAndFilterFile(sampleFile, conf);        Assert.assertEquals(expectedDescription, doc.getFieldValue("metatag.description"));    Assert.assertEquals(expectedKeywords, doc.getFieldValue("metatag.keywords"));    Assert.assertEquals(expectedAuthor, doc.getFieldValue("metatag.author"));}
public void nutch_f1915_0()
{    String expectedDescription = "With this awesome plugin, I control the description! Bwuhuhuhaha!";    String indexReplaceProperty = "  metatag.description=/this plugin/this amazing plugin/\n" + "  metatag.description=/this amazing plugin/this valuable plugin/\n" + "  metatag.description=/this valuable plugin/this cool plugin/\n" + "  metatag.description=/this cool plugin/this wicked plugin/\n" + "  metatag.description=/this wicked plugin/this awesome plugin/\n";    Configuration conf = NutchConfiguration.create();    conf.set("plugin.includes", "protocol-file|urlfilter-regex|parse-(html|metatags)|index-(basic|anchor|metadata|static|replace)|urlnormalizer-(pass|regex|basic)");    conf.set(INDEX_REPLACE_PROPERTY, indexReplaceProperty);    conf.set("metatags.names", "author,description,keywords");    conf.set("index.parse.md", "metatag.author,metatag.description,metatag.keywords");        conf.set("http.timeout", "99999999999");        NutchDocument doc = parseAndFilterFile(sampleFile, conf);        Assert.assertEquals(expectedDescription, doc.getFieldValue("metatag.description"));}
public void nutch_f1916_0()
{    String expectedDescription = "With this awesome plugin, I control the description! Bwuhuhuhaha!";    String indexReplaceProperty = "  metatag.description=/THIS PLUGIN/this awesome plugin/2";    Configuration conf = NutchConfiguration.create();    conf.set("plugin.includes", "protocol-file|urlfilter-regex|parse-(html|metatags)|index-(basic|anchor|metadata|static|replace)|urlnormalizer-(pass|regex|basic)");    conf.set(INDEX_REPLACE_PROPERTY, indexReplaceProperty);    conf.set("metatags.names", "author,description,keywords");    conf.set("index.parse.md", "metatag.author,metatag.description,metatag.keywords");        conf.set("http.timeout", "99999999999");        NutchDocument doc = parseAndFilterFile(sampleFile, conf);            Assert.assertEquals(expectedDescription, doc.getFieldValue("metatag.description"));}
public void nutch_f1917_0()
{    String expectedDescription = "With this plugin, I control the description! Bwuhuhuhaha!";    String expectedTargetDescription = "With this awesome plugin, I control the description! Bwuhuhuhaha!";    String indexReplaceProperty = "  metatag.description:new=/this plugin/this awesome plugin/";    Configuration conf = NutchConfiguration.create();    conf.set("plugin.includes", "protocol-file|urlfilter-regex|parse-(html|metatags)|index-(basic|anchor|metadata|static|replace)|urlnormalizer-(pass|regex|basic)");    conf.set(INDEX_REPLACE_PROPERTY, indexReplaceProperty);    conf.set("metatags.names", "author,description,keywords");    conf.set("index.parse.md", "metatag.author,metatag.description,metatag.keywords");        conf.set("http.timeout", "99999999999");        NutchDocument doc = parseAndFilterFile(sampleFile, conf);        Assert.assertEquals(expectedDescription, doc.getFieldValue("metatag.description"));        Assert.assertEquals(expectedTargetDescription, doc.getFieldValue("new"));}
public NutchDocument nutch_f1918_0(NutchDocument doc, Parse parse, Text url, CrawlDatum datum, Inlinks inlinks) throws IndexingException
{    if (this.addStaticFields == true) {        for (Entry<String, String[]> entry : this.fields.entrySet()) {            for (String val : entry.getValue()) {                doc.add(entry.getKey(), val);            }        }    }    return doc;}
private HashMap<String, String[]> nutch_f1919_0(String fieldsString)
{    HashMap<String, String[]> fields = new HashMap<String, String[]>();    /*     * The format is very easy, it's a comma-separated list of fields in the     * form <name>:<value>     */    for (String field : fieldsString.split(this.fieldSep)) {        String[] entry = field.split(this.kevSep);        if (entry.length == 2)            fields.put(entry[0].trim(), entry[1].trim().split(this.valueSep));    }    return fields;}
public void nutch_f1920_0(Configuration conf)
{    this.conf = conf;        this.fieldSep = this.regexEscape(conf.get("index.static.fieldsep", ","));    this.kevSep = this.regexEscape(conf.get("index.static.keysep", ":"));    this.valueSep = this.regexEscape(conf.get("index.static.valuesep", " "));    String fieldsString = conf.get("index.static", null);    if (fieldsString != null) {        this.addStaticFields = true;        this.fields = parseFields(fieldsString);    }}
public Configuration nutch_f1921_0()
{    return this.conf;}
protected String nutch_f1922_0(String in)
{    String result = in;    if (in != null) {        StringBuffer sb = new StringBuffer();        for (int i = 0; i < in.length(); i++) {            CharSequence c = in.subSequence(i, i + 1);            if ("<([{\\^-=$!|]})?*+.>".contains(c)) {                sb.append('\\');            }            sb.append(c);        }        result = sb.toString();    }    return result;}
public void nutch_f1923_0() throws Exception
{    conf = NutchConfiguration.create();    parse = new ParseImpl();    url = new Text("http://nutch.apache.org/index.html");    crawlDatum = new CrawlDatum();    inlinks = new Inlinks();    filter = new StaticFieldIndexer();}
public void nutch_f1924_0() throws Exception
{    Assert.assertNotNull(filter);    filter.setConf(conf);    NutchDocument doc = new NutchDocument();    try {        filter.filter(doc, parse, url, crawlDatum, inlinks);    } catch (Exception e) {        e.printStackTrace();        Assert.fail(e.getMessage());    }    Assert.assertNotNull(doc);    Assert.assertTrue("tests if no field is set for empty index.static", doc.getFieldNames().isEmpty());}
public void nutch_f1925_0() throws Exception
{    conf.set("index.static", "field1:val1, field2    :      val2 val3     , field3, field4 :val4 , ");    Assert.assertNotNull(filter);    filter.setConf(conf);    NutchDocument doc = new NutchDocument();    try {        filter.filter(doc, parse, url, crawlDatum, inlinks);    } catch (Exception e) {        e.printStackTrace();        Assert.fail(e.getMessage());    }    Assert.assertNotNull(doc);    Assert.assertFalse("test if doc is not empty", doc.getFieldNames().isEmpty());    Assert.assertEquals("test if doc has 3 fields", 3, doc.getFieldNames().size());    Assert.assertTrue("test if doc has field1", doc.getField("field1").getValues().contains("val1"));    Assert.assertTrue("test if doc has field2", doc.getField("field2").getValues().contains("val2"));    Assert.assertTrue("test if doc has field4", doc.getField("field4").getValues().contains("val4"));}
public void nutch_f1926_0() throws Exception
{    conf.set("index.static.fieldsep", ">");    conf.set("index.static.keysep", "=");    conf.set("index.static.valuesep", "|");    conf.set("index.static", "field1=val1>field2    =      val2|val3     >field3>field4 =val4 > ");    Assert.assertNotNull(filter);    filter.setConf(conf);    NutchDocument doc = new NutchDocument();    try {        filter.filter(doc, parse, url, crawlDatum, inlinks);    } catch (Exception e) {        e.printStackTrace();        Assert.fail(e.getMessage());    }    Assert.assertNotNull(doc);    Assert.assertFalse("test if doc is not empty", doc.getFieldNames().isEmpty());    Assert.assertEquals("test if doc has 3 fields", 3, doc.getFieldNames().size());    Assert.assertTrue("test if doc has field1", doc.getField("field1").getValues().contains("val1"));    Assert.assertTrue("test if doc has field2", doc.getField("field2").getValues().contains("val2"));    Assert.assertTrue("test if doc has field4", doc.getField("field4").getValues().contains("val4"));}
public void nutch_f1927_0() throws Exception
{    conf.set("index.static.fieldsep", "\n\n");    conf.set("index.static.keysep", "\t\t");    conf.set("index.static.valuesep", "***");    conf.set("index.static", "field1\t\tval1\n\n" + "field2\t\tval2***val3\n\n" + "field3\n\n" + "field4\t\tval4\n\n\n\n");    Assert.assertNotNull(filter);    filter.setConf(conf);    NutchDocument doc = new NutchDocument();    try {        filter.filter(doc, parse, url, crawlDatum, inlinks);    } catch (Exception e) {        e.printStackTrace();        Assert.fail(e.getMessage());    }    Assert.assertNotNull(doc);    Assert.assertFalse("test if doc is not empty", doc.getFieldNames().isEmpty());    Assert.assertEquals("test if doc has 3 fields", 3, doc.getFieldNames().size());    Assert.assertTrue("test if doc has field1", doc.getField("field1").getValues().contains("val1"));    Assert.assertTrue("test if doc has field2", doc.getField("field2").getValues().contains("val2"));    Assert.assertTrue("test if doc has field4", doc.getField("field4").getValues().contains("val4"));}
public void nutch_f1928_0(Configuration conf, String name) throws IOException
{}
public void nutch_f1929_1(IndexWriterParams parameters) throws IOException
{        endpoint = parameters.get(CloudSearchConstants.ENDPOINT);    dumpBatchFilesToTemp = parameters.getBoolean(CloudSearchConstants.BATCH_DUMP, false);    this.regionName = parameters.get(CloudSearchConstants.REGION);    if (StringUtils.isBlank(endpoint) && !dumpBatchFilesToTemp) {        String message = "Missing CloudSearch endpoint. Should set it set via -D " + CloudSearchConstants.ENDPOINT + " or in nutch-site.xml";        message += "\n" + describe();                throw new RuntimeException(message);    }    maxDocsInBatch = parameters.getInt(CloudSearchConstants.MAX_DOCS_BATCH, -1);    buffer = new StringBuffer(MAX_SIZE_BATCH_BYTES).append('[');    if (dumpBatchFilesToTemp) {                return;    }    if (StringUtils.isBlank(endpoint)) {        throw new RuntimeException("endpoint not set for CloudSearch");    }    AmazonCloudSearchClient cl = new AmazonCloudSearchClient();    if (StringUtils.isNotBlank(regionName)) {        cl.setRegion(RegionUtils.getRegion(regionName));    }    String domainName = null;        DescribeDomainsResult domains = cl.describeDomains(new DescribeDomainsRequest());    Iterator<DomainStatus> dsiter = domains.getDomainStatusList().iterator();    while (dsiter.hasNext()) {        DomainStatus ds = dsiter.next();        if (ds.getDocService().getEndpoint().equals(endpoint)) {            domainName = ds.getDomainName();            break;        }    }        if (StringUtils.isBlank(domainName)) {        throw new RuntimeException("No domain name found for CloudSearch endpoint");    }    DescribeIndexFieldsResult indexDescription = cl.describeIndexFields(new DescribeIndexFieldsRequest().withDomainName(domainName));    for (IndexFieldStatus ifs : indexDescription.getIndexFields()) {        String indexname = ifs.getOptions().getIndexFieldName();        String indextype = ifs.getOptions().getIndexFieldType();                csfields.put(indexname, indextype);    }    client = new AmazonCloudSearchDomainClient();    client.setEndpoint(endpoint);}
public void nutch_f1930_1(String url) throws IOException
{    try {        JSONObject doc_builder = new JSONObject();        doc_builder.put("type", "delete");                String ID = CloudSearchUtils.getID(url);        doc_builder.put("id", ID);                addToBatch(doc_builder.toString(2), url);    } catch (JSONException e) {            }}
public void nutch_f1931_0(NutchDocument doc) throws IOException
{    write(doc);}
public void nutch_f1932_1(NutchDocument doc) throws IOException
{    try {        JSONObject doc_builder = new JSONObject();        doc_builder.put("type", "add");        String url = doc.getField("url").toString();                String ID = CloudSearchUtils.getID(url);        doc_builder.put("id", ID);        JSONObject fields = new JSONObject();        for (final Entry<String, NutchField> e : doc) {            String fieldname = cleanFieldName(e.getKey());            String type = csfields.get(fieldname);                        if (!dumpBatchFilesToTemp && type == null) {                                continue;            }            List<Object> values = e.getValue().getValues();                        for (Object value : values) {                                if (value instanceof Date) {                    Date d = (Date) value;                    value = DATE_FORMAT.format(d);                } else                 if (value instanceof String) {                    value = CloudSearchUtils.stripNonCharCodepoints((String) value);                }                fields.accumulate(fieldname, value);            }        }        doc_builder.put("fields", fields);        addToBatch(doc_builder.toString(2), url);    } catch (JSONException e) {            }}
private void nutch_f1933_1(String currentDoc, String url) throws IOException
{    int currentDocLength = currentDoc.getBytes(StandardCharsets.UTF_8).length;        if (currentDocLength > MAX_SIZE_DOC_BYTES) {                return;    }    int currentBufferLength = buffer.toString().getBytes(StandardCharsets.UTF_8).length;            if (currentDocLength + 2 + currentBufferLength < MAX_SIZE_BATCH_BYTES) {        if (numDocsInBatch != 0)            buffer.append(',');        buffer.append(currentDoc);        numDocsInBatch++;    } else     {        commit();        buffer.append(currentDoc);        numDocsInBatch++;    }        if (maxDocsInBatch > 0 && numDocsInBatch == maxDocsInBatch) {        commit();    }}
public void nutch_f1934_1() throws IOException
{        if (numDocsInBatch == 0) {        return;    }        buffer.append(']');        byte[] bb = buffer.toString().getBytes(StandardCharsets.UTF_8);    if (dumpBatchFilesToTemp) {        try {            File temp = File.createTempFile("CloudSearch_", ".json");            FileUtils.writeByteArrayToFile(temp, bb);                    } catch (IOException e1) {                    } finally {                        buffer = new StringBuffer(MAX_SIZE_BATCH_BYTES).append('[');            numDocsInBatch = 0;        }        return;    }        try (InputStream inputStream = new ByteArrayInputStream(bb)) {        UploadDocumentsRequest batch = new UploadDocumentsRequest();        batch.setContentLength((long) bb.length);        batch.setContentType(ContentType.Applicationjson);        batch.setDocuments(inputStream);        @SuppressWarnings("unused")        UploadDocumentsResult result = client.uploadDocuments(batch);    } catch (Exception e) {                    } finally {                buffer = new StringBuffer(MAX_SIZE_BATCH_BYTES).append('[');        numDocsInBatch = 0;    }}
public void nutch_f1935_0() throws IOException
{        commit();        if (client != null) {        client.shutdown();    }}
public Configuration nutch_f1936_0()
{    return this.conf;}
public void nutch_f1937_0(Configuration conf)
{    this.conf = conf;}
public Map<String, Entry<String, Object>> nutch_f1938_0()
{    Map<String, Map.Entry<String, Object>> properties = new LinkedHashMap<>();    properties.put(CloudSearchConstants.ENDPOINT, new AbstractMap.SimpleEntry<>("Endpoint where service requests should be submitted.", this.endpoint));    properties.put(CloudSearchConstants.REGION, new AbstractMap.SimpleEntry<>("Region name.", this.regionName));    properties.put(CloudSearchConstants.BATCH_DUMP, new AbstractMap.SimpleEntry<>("true to send documents to a local file.", this.dumpBatchFilesToTemp));    properties.put(CloudSearchConstants.MAX_DOCS_BATCH, new AbstractMap.SimpleEntry<>("Maximum number of documents to send as a batch to CloudSearch.", this.maxDocsInBatch));    return properties;}
 String nutch_f1939_0(String name)
{    String lowercase = name.toLowerCase();    return lowercase.replaceAll("[^a-z_0-9]", "_");}
public static String nutch_f1940_0(String url)
{                                byte[] dig = digester.digest(url.getBytes(StandardCharsets.UTF_8));    String ID = Hex.encodeHexString(dig);        if (ID.length() > 128) {        throw new RuntimeException("ID larger than max 128 chars");    }    return ID;}
public static String nutch_f1941_0(String input)
{    StringBuilder retval = new StringBuilder();    char ch;    for (int i = 0; i < input.length(); i++) {        ch = input.charAt(i);                if ((ch == 0x9 || ch == 0xa || ch == 0xd) || (ch >= 0x20 && ch <= 0xFFFD)) {            retval.append(ch);        }    }    return retval.toString();}
protected void nutch_f1942_0(String str)
{    if (str != null) {        sepStr = str;        if (str.length() == 0) {                        chars = new char[0];        } else {            chars = str.toCharArray();        }    }        bytes = sepStr.getBytes(encoding);}
public String nutch_f1943_0()
{    StringBuilder sb = new StringBuilder();    for (char c : chars) {        if (c == '\n') {            sb.append("\\n");        } else if (c == '\r') {            sb.append("\\r");        } else if (c == '\t') {            sb.append("\\t");        } else if (c >= 0x7f || c <= 0x20) {            sb.append(String.format("\\u%04x", (int) c));        } else {            sb.append(c);        }    }    return sb.toString();}
protected void nutch_f1944_0(IndexWriterParams parameters, String property)
{    setFromConf(parameters, property, false);}
protected void nutch_f1945_1(IndexWriterParams parameters, String property, boolean isChar)
{    String str = parameters.get(property);    if (isChar && str != null && !str.isEmpty()) {                str = str.substring(0, 1);    }    set(str);    }
protected int nutch_f1946_0(String value, int start)
{    if (chars.length == 0)        return -1;    if (chars.length == 1)        return value.indexOf(chars[0], start);    int index;    for (char c : chars) {        if ((index = value.indexOf(c, start)) >= 0) {            return index;        }    }    return -1;}
public void nutch_f1947_0(Configuration conf, String name) throws IOException
{}
public void nutch_f1948_1(IndexWriterParams parameters) throws IOException
{    outputPath = parameters.get(CSVConstants.CSV_OUTPATH, outputPath);    String charset = parameters.get(CSVConstants.CSV_CHARSET);    if (charset != null) {        encoding = Charset.forName(charset);    }    fieldSeparator.setFromConf(parameters, CSVConstants.CSV_FIELD_SEPARATOR);    quoteCharacter.setFromConf(parameters, CSVConstants.CSV_QUOTECHARACTER, true);    escapeCharacter.setFromConf(parameters, CSVConstants.CSV_ESCAPECHARACTER, true);    valueSeparator.setFromConf(parameters, CSVConstants.CSV_VALUESEPARATOR);    withHeader = parameters.getBoolean(CSVConstants.CSV_WITHHEADER, true);    maxFieldLength = parameters.getInt(CSVConstants.CSV_MAXFIELDLENGTH, maxFieldLength);        maxFieldValues = parameters.getInt(CSVConstants.CSV_MAXFIELDVALUES, maxFieldValues);        fields = parameters.getStrings(CSVConstants.CSV_FIELDS, "id", "title", "content");        for (String f : fields) {            }    fs = FileSystem.get(config);        Path outputDir = new Path(outputPath);    fs = outputDir.getFileSystem(config);    csvLocalOutFile = new Path(outputDir, "nutch.csv");    if (!fs.exists(outputDir)) {        fs.mkdirs(outputDir);    }    if (fs.exists(csvLocalOutFile)) {                        fs.delete(csvLocalOutFile, true);    }    csvout = fs.create(csvLocalOutFile);    if (withHeader) {        for (int i = 0; i < fields.length; i++) {            if (i > 0)                csvout.write(fieldSeparator.bytes);            csvout.write(fields[i].getBytes(encoding));        }    }    csvout.write(recordSeparator.bytes);}
public void nutch_f1949_0(NutchDocument doc) throws IOException
{    for (int i = 0; i < fields.length; i++) {        if (i > 0) {            csvout.write(fieldSeparator.bytes);        }        NutchField field = doc.getField(fields[i]);        if (field != null) {            List<Object> values = field.getValues();            int nValues = values.size();            if (nValues > maxFieldValues) {                nValues = maxFieldValues;            }            if (nValues > 1) {                                csvout.write(quoteCharacter.bytes);            }            ListIterator<Object> it = values.listIterator();            int j = 0;            while (it.hasNext() && j <= nValues) {                Object objval = it.next();                String value;                if (objval == null) {                    continue;                } else if (objval instanceof Date) {                                        value = objval.toString();                } else {                    value = (String) objval;                }                if (nValues > 1) {                                        writeEscaped(value);                    if (it.hasNext()) {                        csvout.write(valueSeparator.bytes);                    }                } else {                    writeQuoted(value);                }            }            if (nValues > 1) {                                csvout.write(quoteCharacter.bytes);            }        }    }    csvout.write(recordSeparator.bytes);}
public void nutch_f1950_0(String key)
{}
public void nutch_f1951_0(NutchDocument doc) throws IOException
{    write(doc);}
public void nutch_f1952_1() throws IOException
{    csvout.close();    }
public void nutch_f1953_0()
{}
public Configuration nutch_f1954_0()
{    return config;}
public Map<String, Map.Entry<String, Object>> nutch_f1955_0()
{    Map<String, Map.Entry<String, Object>> properties = new LinkedHashMap<>();    properties.put(CSVConstants.CSV_FIELDS, new AbstractMap.SimpleEntry<>("Ordered list of fields (columns) in the CSV file", this.fields == null ? "" : String.join(",", this.fields)));    properties.put(CSVConstants.CSV_FIELD_SEPARATOR, new AbstractMap.SimpleEntry<>("Separator between fields (columns), default: , (U+002C, comma)", this.fieldSeparator));    properties.put(CSVConstants.CSV_QUOTECHARACTER, new AbstractMap.SimpleEntry<>("Quote character used to quote fields containing separators or quotes, default: \" (U+0022, quotation mark)", this.quoteCharacter));    properties.put(CSVConstants.CSV_ESCAPECHARACTER, new AbstractMap.SimpleEntry<>("Escape character used to escape a quote character, default: \" (U+0022, quotation mark)", this.escapeCharacter));    properties.put(CSVConstants.CSV_VALUESEPARATOR, new AbstractMap.SimpleEntry<>("Separator between multiple values of one field, default: | (U+007C)", this.valueSeparator));    properties.put(CSVConstants.CSV_MAXFIELDVALUES, new AbstractMap.SimpleEntry<>("Max. number of values of one field, useful for, e.g., the anchor texts field, default: 12", this.maxFieldValues));    properties.put(CSVConstants.CSV_MAXFIELDLENGTH, new AbstractMap.SimpleEntry<>("Max. length of a single field value in characters, default: 4096", this.maxFieldLength));    properties.put(CSVConstants.CSV_CHARSET, new AbstractMap.SimpleEntry<>("Encoding of CSV file, default: UTF-8", this.encoding));    properties.put(CSVConstants.CSV_WITHHEADER, new AbstractMap.SimpleEntry<>("Write CSV column headers, default: true", this.withHeader));    properties.put(CSVConstants.CSV_OUTPATH, new AbstractMap.SimpleEntry<>("Output path / directory, default: csvindexwriter. ", this.outputPath));    return properties;}
public void nutch_f1956_0(Configuration conf)
{    config = conf;}
private void nutch_f1957_0(String value) throws IOException
{    int nextQuoteChar;    if (quoteCharacter.chars.length > 0 && (((nextQuoteChar = quoteCharacter.find(value, 0)) >= 0) || (fieldSeparator.find(value, 0) >= 0) || (recordSeparator.find(value, 0) >= 0))) {                csvout.write(quoteCharacter.bytes);        writeEscaped(value, nextQuoteChar);        csvout.write(quoteCharacter.bytes);    } else {        if (value.length() > maxFieldLength) {            csvout.write(value.substring(0, maxFieldLength).getBytes(encoding));        } else {            csvout.write(value.getBytes(encoding));        }    }}
private void nutch_f1958_0(String value, int nextQuoteChar) throws IOException
{    int start = 0;    int max = value.length();    if (max > maxFieldLength) {        max = maxFieldLength;    }    while (nextQuoteChar > 0 && nextQuoteChar < max) {        csvout.write(value.substring(start, nextQuoteChar).getBytes(encoding));        csvout.write(escapeCharacter.bytes);        csvout.write(quoteCharacter.bytes);        start = nextQuoteChar + 1;        nextQuoteChar = quoteCharacter.find(value, start);        if (nextQuoteChar > max)            break;    }    csvout.write(value.substring(start, max).getBytes(encoding));}
private void nutch_f1959_0(String value) throws IOException
{    int nextQuoteChar = quoteCharacter.find(value, 0);    writeEscaped(value, nextQuoteChar);}
public static void nutch_f1960_0(String[] args) throws Exception
{    final int res = ToolRunner.run(NutchConfiguration.create(), new IndexingJob(), args);    System.exit(res);}
public void nutch_f1961_0(IndexWriterParams parameters) throws IOException
{    super.open(parameters);    byteBuffer = new ByteArrayOutputStream();    fsStats = new FileSystem.Statistics("testCSVIndexWriter");    csvout = new FSDataOutputStream(byteBuffer, fsStats);}
public void nutch_f1962_0() throws IOException
{}
public String nutch_f1963_0()
{    try {        return byteBuffer.toString(encoding.name());    } catch (UnsupportedEncodingException e) {        return "";    }}
private String nutch_f1964_1(final String[] configParams, NutchDocument[] docs) throws IOException
{    Configuration conf = NutchConfiguration.create();    IndexWriterParams params = new IndexWriterParams(new HashMap<>());    for (int i = 0; i < configParams.length; i += 2) {        params.put(configParams[i], configParams[i + 1]);    }    CSVByteArrayIndexWriter out = new CSVByteArrayIndexWriter();    out.setConf(conf);    out.open(params);    for (NutchDocument doc : docs) {        out.write(doc);    }    out.close();    String csv = out.getData();        return csv;}
private String nutch_f1965_0(final String[] configParams, final String[] fieldContent) throws IOException
{    NutchDocument[] docs = new NutchDocument[1];    docs[0] = new NutchDocument();    for (int i = 0; i < fieldContent.length; i += 2) {        docs[0].add(fieldContent[i], fieldContent[i + 1]);    }    return getCSV(configParams, docs);}
public void nutch_f1966_0() throws IOException
{    String[] fields = { "id", "http://nutch.apache.org/", "title", "Welcome to Apache Nutch", "content", "Apache Nutch is an open source web-search software project. ..." };    String csv = getCSV(new String[0], fields);    for (int i = 0; i < fields.length; i += 2) {        assertTrue("Testing field " + i + " (" + fields[i] + ")", csv.contains(fields[i + 1]));    }}
public void nutch_f1967_0() throws IOException
{    String[] params = { CSVConstants.CSV_FIELDS, "test,test2" };    String[] fields = { "test", "a,b", "test2", "c,d" };    String csv = getCSV(params, fields);    assertEquals("If field contains a fields separator, it must be quoted", "\"a,b\",\"c,d\"", csv.trim());}
public void nutch_f1968_0() throws IOException
{    String[] params = { CSVConstants.CSV_FIELDS, "test" };    String[] fields = { "test", "a\nb" };    String csv = getCSV(params, fields);    assertEquals("If field contains a fields separator, it must be quoted", "\"a\nb\"", csv.trim());}
public void nutch_f1969_0() throws IOException
{    String[] params = { CSVConstants.CSV_FIELDS, "test" };    String[] fields = { "test", "a,b:\"quote\",c" };    String csv = getCSV(params, fields);    assertEquals("Quotes inside a quoted field must be escaped", "\"a,b:\"\"quote\"\",c\"", csv.trim());}
public void nutch_f1970_0() throws IOException
{    String[] params = { CSVConstants.CSV_FIELDS, "test", CSVConstants.CSV_MAXFIELDLENGTH, "8" };    String[] fields = { "test", "0123456789" };    String csv = getCSV(params, fields);    assertEquals("Field clipped to max. length = 8", "01234567", csv.trim());}
public void nutch_f1971_0() throws IOException
{    String[] params = { CSVConstants.CSV_FIELDS, "test", CSVConstants.CSV_MAXFIELDLENGTH, "7" };    String[] fields = { "test", "1,\"2\",3,\"4\"" };    String csv = getCSV(params, fields);    assertEquals("Field clipped to max. length = 7", "\"1,\"\"2\"\",3\"", csv.trim());}
public void nutch_f1972_0() throws IOException
{    String[] params = { CSVConstants.CSV_FIELDS, "test", CSVConstants.CSV_VALUESEPARATOR, "|", CSVConstants.CSV_QUOTECHARACTER, "" };    String[] fields = { "test", "abc", "test", "def" };    String csv = getCSV(params, fields);    assertEquals("Values of multi-value fields are concatenated by |", "abc|def", csv.trim());}
public void nutch_f1973_0() throws IOException
{    String[] charsets = { "iso-8859-1",     "\u00e4\u00f6\u00fc\u00df\u00e9\u00f4\u00ee",     "iso-8859-2",     "\u0161\u010d\u0159\u016f",     "iso-8859-5",     "\u0430\u0441\u0434\u0444" };    for (int i = 0; i < charsets.length; i += 2) {        String charset = charsets[i];        String test = charsets[i + 1];        String[] params = { CSVConstants.CSV_FIELDS, "test", CSVConstants.CSV_CHARSET, charset };        String[] fields = { "test", test };        String csv = getCSV(params, fields);        assertEquals("wrong charset conversion", test, csv.trim());    }}
public void nutch_f1974_0() throws IOException
{    String[] params = { CSVConstants.CSV_FIELDS, "test", CSVConstants.CSV_CHARSET, "iso-8859-1",     CSVConstants.CSV_VALUESEPARATOR,     "\u00a6", CSVConstants.CSV_QUOTECHARACTER, "" };    String[] fields = { "test", "abc", "test", "def" };    String csv = getCSV(params, fields);    assertEquals("Values of multi-value fields are concatenated by ", "abc\u00a6def", csv.trim());}
public void nutch_f1975_0() throws IOException
{    String[] params = { CSVConstants.CSV_FIELDS, "1,2,3", CSVConstants.CSV_FIELD_SEPARATOR, "\t", CSVConstants.CSV_QUOTECHARACTER, "" };    NutchDocument[] docs = new NutchDocument[2];    docs[0] = new NutchDocument();    docs[0].add("1", "a");    docs[0].add("1", "b");    docs[0].add("2", "a\"2\"b");    docs[0].add("3", "c,d");    docs[1] = new NutchDocument();    docs[1].add("1", "A");    docs[1].add("2", "B");    docs[1].add("3", "C");    String csv = getCSV(params, docs);    String[] records = csv.trim().split("\\r\\n");    assertEquals("tab-separated output", "a|b\ta\"2\"b\tc,d", records[0]);    assertEquals("tab-separated output", "A\tB\tC", records[1]);}
public void nutch_f1976_0() throws IOException
{    TimeZone.setDefault(TimeZone.getTimeZone("UTC"));    String[] params = { CSVConstants.CSV_FIELDS, "date" };    NutchDocument[] docs = new NutchDocument[1];    docs[0] = new NutchDocument();        docs[0].add("date", new Date(0));    String csv = getCSV(params, docs);    assertTrue("date conversion", csv.contains("1970"));}
public void nutch_f1977_0(Configuration conf, String name) throws IOException
{}
public void nutch_f1978_1(IndexWriterParams parameters) throws IOException
{    delete = parameters.getBoolean(DummyConstants.DELETE, false);    path = parameters.get(DummyConstants.PATH, "/");    if (path == null) {        String message = "Missing path.";        message += "\n" + describe();                throw new RuntimeException(message);    }    if (writer != null) {                return;    }    try {                writer = new BufferedWriter(new FileWriter(path));    } catch (IOException ex) {            }}
public void nutch_f1979_0(String key) throws IOException
{    if (delete) {        writer.write("delete\t" + key + "\n");    }}
public void nutch_f1980_0(NutchDocument doc) throws IOException
{    writer.write("update\t" + doc.getFieldValue("id") + "\n");}
public void nutch_f1981_0(NutchDocument doc) throws IOException
{    writer.write("add\t" + doc.getFieldValue("id") + "\n");}
public void nutch_f1982_1() throws IOException
{        writer.flush();    writer.close();}
public void nutch_f1983_0() throws IOException
{    writer.write("commit\n");}
public Configuration nutch_f1984_0()
{    return config;}
public void nutch_f1985_0(Configuration conf)
{    config = conf;}
public Map<String, Map.Entry<String, Object>> nutch_f1986_0()
{    Map<String, Map.Entry<String, Object>> properties = new LinkedHashMap<>();    properties.put(DummyConstants.DELETE, new AbstractMap.SimpleEntry<>("If delete operations should be written to the file.", this.delete));    properties.put(DummyConstants.PATH, new AbstractMap.SimpleEntry<>("Path where the file will be created.", this.path));    return properties;}
public void nutch_f1987_0(Configuration conf, String name) throws IOException
{}
public void nutch_f1988_1(IndexWriterParams parameters) throws IOException
{    cluster = parameters.get(ElasticConstants.CLUSTER);    String hosts = parameters.get(ElasticConstants.HOSTS);    if (StringUtils.isBlank(cluster) && StringUtils.isBlank(hosts)) {        String message = "Missing elastic.cluster and elastic.host. At least one of them should be set in index-writers.xml ";        message += "\n" + describe();                throw new RuntimeException(message);    }    bulkCloseTimeout = parameters.getLong(ElasticConstants.BULK_CLOSE_TIMEOUT, DEFAULT_BULK_CLOSE_TIMEOUT);    defaultIndex = parameters.get(ElasticConstants.INDEX, DEFAULT_INDEX);    maxBulkDocs = parameters.getInt(ElasticConstants.MAX_BULK_DOCS, DEFAULT_MAX_BULK_DOCS);    maxBulkLength = parameters.getInt(ElasticConstants.MAX_BULK_LENGTH, DEFAULT_MAX_BULK_LENGTH);    expBackoffMillis = parameters.getInt(ElasticConstants.EXPONENTIAL_BACKOFF_MILLIS, DEFAULT_EXP_BACKOFF_MILLIS);    expBackoffRetries = parameters.getInt(ElasticConstants.EXPONENTIAL_BACKOFF_RETRIES, DEFAULT_EXP_BACKOFF_RETRIES);    client = makeClient(parameters);        bulkProcessor = BulkProcessor.builder(client, bulkProcessorListener()).setBulkActions(maxBulkDocs).setBulkSize(new ByteSizeValue(maxBulkLength, ByteSizeUnit.BYTES)).setConcurrentRequests(1).setBackoffPolicy(BackoffPolicy.exponentialBackoff(TimeValue.timeValueMillis(expBackoffMillis), expBackoffRetries)).build();}
protected Client nutch_f1989_0(IndexWriterParams parameters) throws IOException
{    hosts = parameters.getStrings(ElasticConstants.HOSTS);    port = parameters.getInt(ElasticConstants.PORT, DEFAULT_PORT);    Settings.Builder settingsBuilder = Settings.builder();    String options = parameters.get(ElasticConstants.OPTIONS);    if (options != null) {        String[] lines = options.trim().split(",");        for (String line : lines) {            if (StringUtils.isNotBlank(line)) {                String[] parts = line.trim().split("=");                if (parts.length == 2) {                    settingsBuilder.put(parts[0].trim(), parts[1].trim());                }            }        }    }        if (StringUtils.isNotBlank(cluster)) {        settingsBuilder.put("cluster.name", cluster);    }    Settings settings = settingsBuilder.build();    Client client = null;        if (hosts != null && port > 1) {        @SuppressWarnings("resource")        TransportClient transportClient = new PreBuiltTransportClient(settings);        for (String host : hosts) transportClient.addTransportAddress(new InetSocketTransportAddress(InetAddress.getByName(host), port));        client = transportClient;    } else if (cluster != null) {        node = new Node(settings);        client = node.client();    }    return client;}
protected BulkProcessor.Listener nutch_f1990_1()
{    return new BulkProcessor.Listener() {        @Override        public void beforeBulk(long executionId, BulkRequest request) {        }        @Override        public void afterBulk(long executionId, BulkRequest request, Throwable failure) {            throw new RuntimeException(failure);        }        @Override        public void afterBulk(long executionId, BulkRequest request, BulkResponse response) {            if (response.hasFailures()) {                            }        }    };}
public void nutch_f1991_0(long executionId, BulkRequest request)
{}
public void nutch_f1992_0(long executionId, BulkRequest request, Throwable failure)
{    throw new RuntimeException(failure);}
public void nutch_f1993_1(long executionId, BulkRequest request, BulkResponse response)
{    if (response.hasFailures()) {            }}
public void nutch_f1994_0(NutchDocument doc) throws IOException
{    String id = (String) doc.getFieldValue("id");    String type = doc.getDocumentMeta().get("type");    if (type == null)        type = "doc";        Map<String, Object> source = new HashMap<String, Object>();    for (final Map.Entry<String, NutchField> e : doc) {        final List<Object> values = e.getValue().getValues();        if (values.size() > 1) {            source.put(e.getKey(), values);        } else {            source.put(e.getKey(), values.get(0));        }    }    IndexRequest request = new IndexRequest(defaultIndex, type, id).source(source);    bulkProcessor.add(request);}
public void nutch_f1995_0(String key) throws IOException
{    DeleteRequest request = new DeleteRequest(defaultIndex, "doc", key);    bulkProcessor.add(request);}
public void nutch_f1996_0(NutchDocument doc) throws IOException
{    write(doc);}
public void nutch_f1997_0() throws IOException
{    bulkProcessor.flush();}
public void nutch_f1998_1() throws IOException
{        try {        bulkProcessor.awaitClose(bulkCloseTimeout, TimeUnit.SECONDS);    } catch (InterruptedException e) {            }    client.close();    if (node != null) {        node.close();    }}
public Map<String, Map.Entry<String, Object>> nutch_f1999_0()
{    Map<String, Map.Entry<String, Object>> properties = new LinkedHashMap<>();    properties.put(ElasticConstants.CLUSTER, new AbstractMap.SimpleEntry<>("The cluster name to discover. Either host and port must be defined or cluster.", this.cluster));    properties.put(ElasticConstants.HOSTS, new AbstractMap.SimpleEntry<>("Ordered list of fields (columns) in the CSV fileComma-separated list of " + "hostnames to send documents to using TransportClient. " + "Either host and port must be defined or cluster.", this.hosts == null ? "" : String.join(",", hosts)));    properties.put(ElasticConstants.PORT, new AbstractMap.SimpleEntry<>("The port to connect to using TransportClient.", this.port));    properties.put(ElasticConstants.INDEX, new AbstractMap.SimpleEntry<>("Default index to send documents to.", this.defaultIndex));    properties.put(ElasticConstants.MAX_BULK_DOCS, new AbstractMap.SimpleEntry<>("Maximum size of the bulk in number of documents.", this.maxBulkDocs));    properties.put(ElasticConstants.MAX_BULK_LENGTH, new AbstractMap.SimpleEntry<>("Maximum size of the bulk in bytes.", this.maxBulkLength));    properties.put(ElasticConstants.EXPONENTIAL_BACKOFF_MILLIS, new AbstractMap.SimpleEntry<>("Initial delay for the BulkProcessor exponential backoff policy.", this.expBackoffMillis));    properties.put(ElasticConstants.EXPONENTIAL_BACKOFF_RETRIES, new AbstractMap.SimpleEntry<>("Number of times the BulkProcessor exponential backoff policy should retry bulk operations.", this.expBackoffRetries));    properties.put(ElasticConstants.BULK_CLOSE_TIMEOUT, new AbstractMap.SimpleEntry<>("Number of seconds allowed for the BulkProcessor to complete its last operation.", this.bulkCloseTimeout));    return properties;}
public void nutch_f2000_0(Configuration conf)
{    config = conf;}
public Configuration nutch_f2001_0()
{    return config;}
public void nutch_f2002_0()
{    conf = NutchConfiguration.create();    conf.addResource("nutch-site-test.xml");    bulkRequestSuccessful = false;    clusterSaturated = false;    curNumFailures = 0;    maxNumFailures = 0;    Settings settings = Settings.builder().build();    ThreadPool threadPool = new ThreadPool(settings);        client = new AbstractClient(settings, threadPool) {        @Override        public void close() {        }        @Override        protected <Request extends ActionRequest, Response extends ActionResponse, RequestBuilder extends ActionRequestBuilder<Request, Response, RequestBuilder>> void doExecute(Action<Request, Response, RequestBuilder> action, Request request, ActionListener<Response> listener) {            BulkResponse response = null;            if (clusterSaturated) {                                curNumFailures++;                if (curNumFailures >= maxNumFailures) {                                        clusterSaturated = false;                }                                BulkItemResponse failed = new BulkItemResponse(0, OpType.INDEX, new BulkItemResponse.Failure("nutch", "index", "failure0", new EsRejectedExecutionException("saturated")));                response = new BulkResponse(new BulkItemResponse[] { failed }, 0);            } else {                                BulkItemResponse success = new BulkItemResponse(0, OpType.INDEX, new IndexResponse(new ShardId("nutch", UUID.randomUUID().toString(), 0), "index", "index0", 0, true));                response = new BulkResponse(new BulkItemResponse[] { success }, 0);            }            listener.onResponse((Response) response);        }    };        testIndexWriter = new ElasticIndexWriter() {        @Override        protected Client makeClient(IndexWriterParams parameters) {            return client;        }        @Override        protected BulkProcessor.Listener bulkProcessorListener() {            return new BulkProcessor.Listener() {                @Override                public void afterBulk(long executionId, BulkRequest request, BulkResponse response) {                    if (!response.hasFailures()) {                        bulkRequestSuccessful = true;                    }                }                @Override                public void afterBulk(long executionId, BulkRequest request, Throwable failure) {                }                @Override                public void beforeBulk(long executionId, BulkRequest request) {                }            };        }    };}
public void nutch_f2003_0()
{}
protected void nutch_f2004_0(Action<Request, Response, RequestBuilder> action, Request request, ActionListener<Response> listener)
{    BulkResponse response = null;    if (clusterSaturated) {                curNumFailures++;        if (curNumFailures >= maxNumFailures) {                        clusterSaturated = false;        }                BulkItemResponse failed = new BulkItemResponse(0, OpType.INDEX, new BulkItemResponse.Failure("nutch", "index", "failure0", new EsRejectedExecutionException("saturated")));        response = new BulkResponse(new BulkItemResponse[] { failed }, 0);    } else {                BulkItemResponse success = new BulkItemResponse(0, OpType.INDEX, new IndexResponse(new ShardId("nutch", UUID.randomUUID().toString(), 0), "index", "index0", 0, true));        response = new BulkResponse(new BulkItemResponse[] { success }, 0);    }    listener.onResponse((Response) response);}
protected Client nutch_f2005_0(IndexWriterParams parameters)
{    return client;}
protected BulkProcessor.Listener nutch_f2006_0()
{    return new BulkProcessor.Listener() {        @Override        public void afterBulk(long executionId, BulkRequest request, BulkResponse response) {            if (!response.hasFailures()) {                bulkRequestSuccessful = true;            }        }        @Override        public void afterBulk(long executionId, BulkRequest request, Throwable failure) {        }        @Override        public void beforeBulk(long executionId, BulkRequest request) {        }    };}
public void nutch_f2007_0(long executionId, BulkRequest request, BulkResponse response)
{    if (!response.hasFailures()) {        bulkRequestSuccessful = true;    }}
public void nutch_f2008_0(long executionId, BulkRequest request, Throwable failure)
{}
public void nutch_f2009_0(long executionId, BulkRequest request)
{}
public void nutch_f2010_0() throws IOException
{    int numDocs = 10;    conf.setInt(ElasticConstants.MAX_BULK_DOCS, numDocs);    @SuppressWarnings("unused")    Job job = Job.getInstance(conf);    Map<String, String> parameters = new HashMap<>();    parameters.put(ElasticConstants.CLUSTER, "nutch");    parameters.put(ElasticConstants.MAX_BULK_DOCS, String.valueOf(numDocs));    testIndexWriter.setConf(conf);    testIndexWriter.open(new IndexWriterParams(parameters));    NutchDocument doc = new NutchDocument();    doc.add("id", "http://www.example.com");    Assert.assertFalse(bulkRequestSuccessful);    for (int i = 0; i < numDocs; i++) {        testIndexWriter.write(doc);    }    testIndexWriter.close();    Assert.assertTrue(bulkRequestSuccessful);}
public void nutch_f2011_0() throws IOException
{    String key = "id";    String value = "http://www.example.com";    int defaultMaxBulkLength = conf.getInt(ElasticConstants.MAX_BULK_LENGTH, 2500500);        int testMaxBulkLength = defaultMaxBulkLength / 10;                int numDocs = testMaxBulkLength / (key.length() + value.length());    conf.setInt(ElasticConstants.MAX_BULK_LENGTH, testMaxBulkLength);    @SuppressWarnings("unused")    Job job = Job.getInstance(conf);    Map<String, String> parameters = new HashMap<>();    parameters.put(ElasticConstants.CLUSTER, "nutch");    parameters.put(ElasticConstants.MAX_BULK_LENGTH, String.valueOf(testMaxBulkLength));    testIndexWriter.setConf(conf);    testIndexWriter.open(new IndexWriterParams(parameters));    NutchDocument doc = new NutchDocument();    doc.add(key, value);    Assert.assertFalse(bulkRequestSuccessful);    for (int i = 0; i < numDocs; i++) {        testIndexWriter.write(doc);    }    testIndexWriter.close();    Assert.assertTrue(bulkRequestSuccessful);}
public void nutch_f2012_0() throws IOException
{        maxNumFailures = 5;    conf.setInt(ElasticConstants.EXPONENTIAL_BACKOFF_RETRIES, maxNumFailures);    int numDocs = 10;    conf.setInt(ElasticConstants.MAX_BULK_DOCS, numDocs);    @SuppressWarnings("unused")    Job job = Job.getInstance(conf);    Map<String, String> parameters = new HashMap<>();    parameters.put(ElasticConstants.CLUSTER, "nutch");    parameters.put(ElasticConstants.EXPONENTIAL_BACKOFF_RETRIES, String.valueOf(maxNumFailures));    parameters.put(ElasticConstants.MAX_BULK_DOCS, String.valueOf(numDocs));    testIndexWriter.setConf(conf);    testIndexWriter.open(new IndexWriterParams(parameters));    NutchDocument doc = new NutchDocument();    doc.add("id", "http://www.example.com");        clusterSaturated = true;    Assert.assertFalse(bulkRequestSuccessful);        for (int i = 0; i < numDocs; i++) {        testIndexWriter.write(doc);    }    testIndexWriter.close();        Assert.assertTrue(bulkRequestSuccessful);}
public void nutch_f2013_0(Configuration conf, String name) throws IOException
{}
public void nutch_f2014_1(IndexWriterParams parameters) throws IOException
{    host = parameters.get(ElasticRestConstants.HOST);    if (StringUtils.isBlank(host)) {        String message = "Missing host. It should be set in index-writers.xml";        message += "\n" + describe();                throw new RuntimeException(message);    }    port = parameters.getInt(ElasticRestConstants.PORT, 9200);    user = parameters.get(ElasticRestConstants.USER);    password = parameters.get(ElasticRestConstants.PASSWORD);    https = parameters.getBoolean(ElasticRestConstants.HTTPS, false);    trustAllHostnames = parameters.getBoolean(ElasticRestConstants.HOSTNAME_TRUST, false);    languages = parameters.getStrings(ElasticRestConstants.LANGUAGES);    separator = parameters.get(ElasticRestConstants.SEPARATOR, DEFAULT_SEPARATOR);    sink = parameters.get(ElasticRestConstants.SINK, DEFAULT_SINK);        SSLContext sslContext = null;    try {        sslContext = new SSLContextBuilder().loadTrustMaterial(new TrustStrategy() {            public boolean isTrusted(X509Certificate[] arg0, String arg1) throws CertificateException {                return true;            }        }).build();    } catch (NoSuchAlgorithmException | KeyManagementException | KeyStoreException e) {                throw new SecurityException();    }        HostnameVerifier hostnameVerifier = null;    if (trustAllHostnames) {        hostnameVerifier = NoopHostnameVerifier.INSTANCE;    } else {        hostnameVerifier = new DefaultHostnameVerifier();    }    SSLConnectionSocketFactory sslSocketFactory = new SSLConnectionSocketFactory(sslContext);    SchemeIOSessionStrategy httpsIOSessionStrategy = new SSLIOSessionStrategy(sslContext, hostnameVerifier);    JestClientFactory jestClientFactory = new JestClientFactory();    URL urlOfElasticsearchNode = new URL(https ? "https" : "http", host, port, "");    if (host != null && port > 1) {        HttpClientConfig.Builder builder = new HttpClientConfig.Builder(urlOfElasticsearchNode.toString()).multiThreaded(true).connTimeout(300000).readTimeout(300000);        if (https) {            if (user != null && password != null) {                builder.defaultCredentials(user, password);            }            builder.defaultSchemeForDiscoveredNodes("https").sslSocketFactory(            sslSocketFactory).httpsIOSessionStrategy(            httpsIOSessionStrategy);        }        jestClientFactory.setHttpClientConfig(builder.build());    } else {        throw new IllegalStateException("No host or port specified. Please set the host and port in nutch-site.xml");    }    client = jestClientFactory.getObject();    defaultIndex = parameters.get(ElasticRestConstants.INDEX, "nutch");    defaultType = parameters.get(ElasticRestConstants.TYPE, "doc");    maxBulkDocs = parameters.getInt(ElasticRestConstants.MAX_BULK_DOCS, DEFAULT_MAX_BULK_DOCS);    maxBulkLength = parameters.getInt(ElasticRestConstants.MAX_BULK_LENGTH, DEFAULT_MAX_BULK_LENGTH);    bulkBuilder = new Bulk.Builder().defaultIndex(defaultIndex).defaultType(defaultType);}
public boolean nutch_f2015_0(X509Certificate[] arg0, String arg1) throws CertificateException
{    return true;}
private static Object nutch_f2016_0(Object value)
{    if (value == null) {        return null;    }    if (value instanceof Map || value instanceof Date) {        return value;    }    return value.toString();}
public void nutch_f2017_1(NutchDocument doc) throws IOException
{    String id = (String) doc.getFieldValue("id");    String type = doc.getDocumentMeta().get("type");    if (type == null) {        type = defaultType;    }    Map<String, Object> source = new HashMap<String, Object>();        for (String fieldName : doc.getFieldNames()) {        Set<Object> allFieldValues = new LinkedHashSet<>(doc.getField(fieldName).getValues());        if (allFieldValues.size() > 1) {            Object[] normalizedFieldValues = allFieldValues.stream().map(ElasticRestIndexWriter::normalizeValue).toArray();                        for (Object value : normalizedFieldValues) {                bulkLength += value.toString().length();            }            source.put(fieldName, normalizedFieldValues);        } else if (allFieldValues.size() == 1) {            Object normalizedFieldValue = normalizeValue(allFieldValues.iterator().next());            source.put(fieldName, normalizedFieldValue);            bulkLength += normalizedFieldValue.toString().length();        }    }    String index;    if (languages != null && languages.length > 0) {        String language = (String) doc.getFieldValue("lang");        boolean exists = false;        for (String lang : languages) {            if (lang.equals(language)) {                exists = true;                break;            }        }        if (exists) {            index = getLanguageIndexName(language);        } else {            index = getSinkIndexName();        }    } else {        index = defaultIndex;    }    Index indexRequest = new Index.Builder(source).index(index).type(type).id(id).build();        bulkBuilder.addAction(indexRequest);    indexedDocs++;    bulkDocs++;    if (bulkDocs >= maxBulkDocs || bulkLength >= maxBulkLength) {                        createNewBulk = true;        commit();    }}
public void nutch_f2018_1(String key) throws IOException
{    try {        if (languages != null && languages.length > 0) {            Bulk.Builder bulkBuilder = new Bulk.Builder().defaultType(defaultType);            for (String lang : languages) {                bulkBuilder.addAction(new Delete.Builder(key).index(getLanguageIndexName(lang)).type(defaultType).build());            }            bulkBuilder.addAction(new Delete.Builder(key).index(getSinkIndexName()).type(defaultType).build());            client.execute(bulkBuilder.build());        } else {            client.execute(new Delete.Builder(key).index(defaultIndex).type(defaultType).build());        }    } catch (IOException e) {                throw e;    }}
public void nutch_f2019_1(NutchDocument doc) throws IOException
{    try {        write(doc);    } catch (IOException e) {                throw e;    }}
public void nutch_f2020_1() throws IOException
{    if (basicFuture != null) {                long beforeWait = System.currentTimeMillis();        try {            JestResult result = basicFuture.get();            if (result == null) {                throw new RuntimeException();            }            long msWaited = System.currentTimeMillis() - beforeWait;                    } catch (InterruptedException | ExecutionException e) {                    }        basicFuture = null;    }    if (bulkBuilder != null) {        if (bulkDocs > 0) {                        basicFuture = new BasicFuture<>(null);            millis = System.currentTimeMillis();            client.executeAsync(bulkBuilder.build(), new JestResultHandler<BulkResult>() {                @Override                public void completed(BulkResult bulkResult) {                    basicFuture.completed(bulkResult);                    millis = System.currentTimeMillis() - millis;                }                @Override                public void failed(Exception e) {                    basicFuture.completed(null);                                    }            });        }        bulkBuilder = null;    }    if (createNewBulk) {                bulkBuilder = new Bulk.Builder().defaultIndex(defaultIndex).defaultType(defaultType);        bulkDocs = 0;        bulkLength = 0;    }}
public void nutch_f2021_0(BulkResult bulkResult)
{    basicFuture.completed(bulkResult);    millis = System.currentTimeMillis() - millis;}
public void nutch_f2022_1(Exception e)
{    basicFuture.completed(null);    }
public void nutch_f2023_1() throws IOException
{            createNewBulk = false;    commit();            createNewBulk = false;    commit();        client.shutdownClient();}
public Map<String, Map.Entry<String, Object>> nutch_f2024_0()
{    Map<String, Map.Entry<String, Object>> properties = new LinkedHashMap<>();    properties.put(ElasticRestConstants.HOST, new AbstractMap.SimpleEntry<>("The hostname or a list of comma separated hostnames to send documents " + "to using Elasticsearch Jest. Both host and port must be defined.", this.host));    properties.put(ElasticRestConstants.PORT, new AbstractMap.SimpleEntry<>("The port to connect to using Elasticsearch Jest.", this.port));    properties.put(ElasticRestConstants.INDEX, new AbstractMap.SimpleEntry<>("Default index to send documents to.", this.defaultIndex));    properties.put(ElasticRestConstants.MAX_BULK_DOCS, new AbstractMap.SimpleEntry<>("Maximum size of the bulk in number of documents.", this.maxBulkDocs));    properties.put(ElasticRestConstants.MAX_BULK_LENGTH, new AbstractMap.SimpleEntry<>("Maximum size of the bulk in bytes.", this.maxBulkLength));    properties.put(ElasticRestConstants.USER, new AbstractMap.SimpleEntry<>("Username for auth credentials (only used when https is enabled)", this.user));    properties.put(ElasticRestConstants.PASSWORD, new AbstractMap.SimpleEntry<>("Password for auth credentials (only used when https is enabled)", this.password));    properties.put(ElasticRestConstants.TYPE, new AbstractMap.SimpleEntry<>("Default type to send documents to.", this.defaultType));    properties.put(ElasticRestConstants.HTTPS, new AbstractMap.SimpleEntry<>("true to enable https, false to disable https. If you've disabled http " + "access (by forcing https), be sure to set this to true, otherwise " + "you might get \"connection reset by peer\".", this.https));    properties.put(ElasticRestConstants.HOSTNAME_TRUST, new AbstractMap.SimpleEntry<>("true to trust elasticsearch server's certificate even if its listed " + "domain name does not match the domain they are hosted or false " + "to check if the elasticsearch server's certificate's listed " + "domain is the same domain that it is hosted on, and if " + "it doesn't, then fail to index (only used when https is enabled)", this.trustAllHostnames));    properties.put(ElasticRestConstants.LANGUAGES, new AbstractMap.SimpleEntry<>("A list of strings denoting the supported languages (e.g. en, de, fr, it). " + "If this value is empty all documents will be sent to index property. " + "If not empty the Rest client will distribute documents in different " + "indices based on their languages property. Indices are named with the " + "following schema: index separator language (e.g. nutch_de). " + "Entries with an unsupported languages value will be added to " + "index index separator sink (e.g. nutch_others).", this.languages == null ? "" : String.join(",", languages)));    properties.put(ElasticRestConstants.SEPARATOR, new AbstractMap.SimpleEntry<>("Is used only if languages property is defined to build the index name " + "(i.e. index separator lang).", this.separator));    properties.put(ElasticRestConstants.SINK, new AbstractMap.SimpleEntry<>("Is used only if languages property is defined to build the index name " + "where to store documents with unsupported languages " + "(i.e. index separator sink).", this.sink));    return properties;}
public void nutch_f2025_0(Configuration conf)
{    config = conf;}
public Configuration nutch_f2026_0()
{    return config;}
private String nutch_f2027_0(String lang)
{    return getComposedIndexName(defaultIndex, lang);}
private String nutch_f2028_0()
{    return getComposedIndexName(defaultIndex, sink);}
private String nutch_f2029_0(String prefix, String postfix)
{    return prefix + separator + postfix;}
public void nutch_f2030_0(Configuration job, String name) throws IOException
{}
public void nutch_f2031_1(IndexWriterParams params) throws IOException
{    host = params.get(KafkaConstants.HOST);    port = params.getInt(KafkaConstants.PORT, 9092);    keySerializer = params.get(KafkaConstants.KEY_SERIALIZER, "org.apache.kafka.common.serialization.ByteArraySerializer");    valueSerializer = params.get(KafkaConstants.VALUE_SERIALIZER, "org.apache.kafka.connect.json.JsonSerializer");    topic = params.get(KafkaConstants.TOPIC);    maxDocCount = params.getInt(KafkaConstants.MAX_DOC_COUNT, 100);    inputDocs = new ArrayList<ProducerRecord<String, JsonNode>>(maxDocCount);    if (StringUtils.isBlank(host)) {        String message = "Missing host. It should be set in index-writers.xml";        message += "\n" + describe();                throw new RuntimeException(message);    }    Properties configProperties = new Properties();    configProperties.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, host + ":" + port);    configProperties.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, keySerializer);    configProperties.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, valueSerializer);    Thread.currentThread().setContextClassLoader(this.getClass().getClassLoader());    producer = new KafkaProducer<String, JsonNode>(configProperties);}
public void nutch_f2032_1(NutchDocument doc) throws IOException
{    Map<String, Object> source = new HashMap<String, Object>();        for (String fieldName : doc.getFieldNames()) {        Set<String> allFieldValues = new HashSet<String>();        for (Object value : doc.getField(fieldName).getValues()) {            allFieldValues.add(value.toString());        }        String[] fieldValues = allFieldValues.toArray(new String[allFieldValues.size()]);        source.put(fieldName, fieldValues);    }    try {        jsonString = new ObjectMapper().writeValueAsString(source);        json = new ObjectMapper().readTree(jsonString);        data = new ProducerRecord<String, JsonNode>(topic, json);        inputDocs.add(data);        if (inputDocs.size() == maxDocCount) {            commit();        }    } catch (NullPointerException e) {            }}
public void nutch_f2033_0(String key) throws IOException
{}
public void nutch_f2034_1(NutchDocument doc) throws IOException
{    try {        write(doc);    } catch (IOException e) {                throw e;    }}
public void nutch_f2035_1() throws IOException
{    try {        for (ProducerRecord<String, JsonNode> datum : inputDocs) {            producer.send(datum);        }        inputDocs.clear();    } catch (NullPointerException e) {            }}
public void nutch_f2036_0() throws IOException
{    commit();    producer.close();}
public Map<String, Map.Entry<String, Object>> nutch_f2037_0()
{    Map<String, Map.Entry<String, Object>> properties = new LinkedHashMap<>();    properties.put(KafkaConstants.HOST, new AbstractMap.SimpleEntry<>("Location of the host Kafka cluster to connect to using producerConfig", this.host));    properties.put(KafkaConstants.PORT, new AbstractMap.SimpleEntry<>("The port to connect to using the producerConfig", this.port));    properties.put(KafkaConstants.TOPIC, new AbstractMap.SimpleEntry<>("Default index to attach to documents", this.topic));    properties.put(KafkaConstants.KEY_SERIALIZER, new AbstractMap.SimpleEntry<>("instruct how to turn the key object the user provides with their ProducerRecord into bytes", this.keySerializer));    properties.put(KafkaConstants.VALUE_SERIALIZER, new AbstractMap.SimpleEntry<>("instruct how to turn the value object the user provides with their ProducerRecord into bytes", this.valueSerializer));    properties.put(KafkaConstants.MAX_DOC_COUNT, new AbstractMap.SimpleEntry<>("Maximum number of documents before a commit is forced", this.maxDocCount));    return properties;}
public void nutch_f2038_0(Configuration conf)
{    config = conf;}
public Configuration nutch_f2039_0()
{    return config;}
 List<RabbitDocumentField> nutch_f2040_0()
{    return fields;}
 void nutch_f2041_0(float documentBoost)
{    this.documentBoost = documentBoost;}
 void nutch_f2042_0(RabbitDocumentField field)
{    fields.add(field);}
 byte[] nutch_f2043_0()
{    Gson gson = new Gson();    return gson.toJson(this).getBytes();}
public String nutch_f2044_0()
{    return key;}
public List<Object> nutch_f2045_0()
{    return values;}
public Configuration nutch_f2046_0()
{    return config;}
public void nutch_f2047_0(Configuration conf)
{    config = conf;}
public void nutch_f2048_0(Configuration conf, String name) throws IOException
{}
public void nutch_f2049_0(IndexWriterParams parameters) throws IOException
{    exchange = parameters.get(RabbitMQConstants.EXCHANGE_NAME);    routingKey = parameters.get(RabbitMQConstants.ROUTING_KEY);    commitSize = parameters.getInt(RabbitMQConstants.COMMIT_SIZE, 250);    commitMode = parameters.get(RabbitMQConstants.COMMIT_MODE, "multiple");    headersStatic = parameters.get(RabbitMQConstants.HEADERS_STATIC, "");    headersDynamic = Arrays.asList(parameters.getStrings(RabbitMQConstants.HEADERS_DYNAMIC, ""));    uri = parameters.get(RabbitMQConstants.SERVER_URI);    client = new RabbitMQClient(uri);    client.openChannel();    binding = parameters.getBoolean(RabbitMQConstants.BINDING, false);    if (binding) {        queueName = parameters.get(RabbitMQConstants.QUEUE_NAME);        queueOptions = parameters.get(RabbitMQConstants.QUEUE_OPTIONS);        exchangeOptions = parameters.get(RabbitMQConstants.EXCHANGE_OPTIONS);        bindingArguments = parameters.get(RabbitMQConstants.BINDING_ARGUMENTS, "");        client.bind(exchange, exchangeOptions, queueName, queueOptions, routingKey, bindingArguments);    }}
public void nutch_f2050_0(NutchDocument doc) throws IOException
{    RabbitDocument rabbitDocument = new RabbitDocument();    for (final Map.Entry<String, NutchField> e : doc) {        RabbitDocument.RabbitDocumentField field = new RabbitDocument.RabbitDocumentField(e.getKey(), e.getValue().getWeight(), e.getValue().getValues());        rabbitDocument.addField(field);    }    rabbitDocument.setDocumentBoost(doc.getWeight());    rabbitMessage.addDocToWrite(rabbitDocument);    if (rabbitMessage.size() >= commitSize) {        commit();    }}
public void nutch_f2051_0(String url) throws IOException
{    rabbitMessage.addDocToDelete(url);    if (rabbitMessage.size() >= commitSize) {        commit();    }}
public void nutch_f2052_0(NutchDocument doc) throws IOException
{    RabbitDocument rabbitDocument = new RabbitDocument();    for (final Map.Entry<String, NutchField> e : doc) {        RabbitDocument.RabbitDocumentField field = new RabbitDocument.RabbitDocumentField(e.getKey(), e.getValue().getWeight(), e.getValue().getValues());        rabbitDocument.addField(field);    }    rabbitDocument.setDocumentBoost(doc.getWeight());    rabbitMessage.addDocToUpdate(rabbitDocument);    if (rabbitMessage.size() >= commitSize) {        commit();    }}
public void nutch_f2053_0() throws IOException
{    if (!rabbitMessage.isEmpty()) {        if ("single".equals(commitMode)) {                        for (String s : rabbitMessage.getDocsToDelete()) {                RabbitMQMessage message = new RabbitMQMessage();                message.setBody(s.getBytes());                message.setHeaders(headersStatic);                message.addHeader("action", "delete");                client.publish(exchange, routingKey, message);            }                        for (RabbitDocument rabbitDocument : rabbitMessage.getDocsToUpdate()) {                RabbitMQMessage message = new RabbitMQMessage();                message.setBody(rabbitDocument.getBytes());                addHeaders(message, rabbitDocument);                message.addHeader("action", "update");                client.publish(exchange, routingKey, message);            }                        for (RabbitDocument rabbitDocument : rabbitMessage.getDocsToWrite()) {                RabbitMQMessage message = new RabbitMQMessage();                message.setBody(rabbitDocument.getBytes());                addHeaders(message, rabbitDocument);                message.addHeader("action", "write");                client.publish(exchange, routingKey, message);            }        } else {            RabbitMQMessage message = new RabbitMQMessage();            message.setBody(rabbitMessage.getBytes());            message.setHeaders(headersStatic);            client.publish(exchange, routingKey, message);        }    }    rabbitMessage.clear();}
public void nutch_f2054_0() throws IOException
{        commit();    client.close();}
public Map<String, Map.Entry<String, Object>> nutch_f2055_0()
{    Map<String, Map.Entry<String, Object>> properties = new LinkedHashMap<>();    properties.put(RabbitMQConstants.SERVER_URI, new AbstractMap.SimpleEntry<>("URI with connection parameters in the form amqp://<username>:<password>@<hostname>:<port>/<virtualHost>", this.uri));    properties.put(RabbitMQConstants.BINDING, new AbstractMap.SimpleEntry<>("Whether the relationship between an exchange and a queue is created automatically. " + "NOTE: Binding between exchanges is not supported.", this.binding));    properties.put(RabbitMQConstants.BINDING_ARGUMENTS, new AbstractMap.SimpleEntry<>("Arguments used in binding. It must have the form key1=value1,key2=value2. " + "This value is only used when the exchange's type is headers and " + "the value of binding property is true. In other cases is ignored.", this.bindingArguments));    properties.put(RabbitMQConstants.EXCHANGE_NAME, new AbstractMap.SimpleEntry<>("Name for the exchange where the messages will be sent.", this.exchange));    properties.put(RabbitMQConstants.EXCHANGE_OPTIONS, new AbstractMap.SimpleEntry<>("Options used when the exchange is created. Only used when the value of binding property is true. " + "It must have the form type=<type>,durable=<durable>", this.exchangeOptions));    properties.put(RabbitMQConstants.QUEUE_NAME, new AbstractMap.SimpleEntry<>("Name of the queue used to create the binding. Only used when the value " + "of binding property is true.", this.queueName));    properties.put(RabbitMQConstants.QUEUE_OPTIONS, new AbstractMap.SimpleEntry<>("Options used when the queue is created. Only used when the value of " + "binding property is true. It must have the form " + "durable=<durable>,exclusive=<exclusive>,auto-delete=<auto-delete>,arguments=<arguments>", this.queueOptions));    properties.put(RabbitMQConstants.ROUTING_KEY, new AbstractMap.SimpleEntry<>("The routing key used to route messages in the exchange. " + "It only makes sense when the exchange type is topic or direct.", this.routingKey));    properties.put(RabbitMQConstants.COMMIT_MODE, new AbstractMap.SimpleEntry<>("single if a message contains only one document. " + "In this case, a header with the action (write, update or delete) will be added. " + "multiple if a message contains all documents.", this.commitMode));    properties.put(RabbitMQConstants.COMMIT_SIZE, new AbstractMap.SimpleEntry<>("Amount of documents to send into each message if the value of commit.mode " + "property is multiple. In single mode this value represents " + "the amount of messages to be sent.", this.commitSize));    properties.put(RabbitMQConstants.HEADERS_STATIC, new AbstractMap.SimpleEntry<>("Headers to add to each message. It must have the form key1=value1,key2=value2.", this.headersStatic));    properties.put(RabbitMQConstants.HEADERS_DYNAMIC, new AbstractMap.SimpleEntry<>("Document's fields to add as headers to each message. " + "It must have the form field1,field2. " + "Only used when the value of commit.mode property is single", this.headersDynamic));    return properties;}
private void nutch_f2056_0(final RabbitMQMessage message, RabbitDocument document)
{    message.setHeaders(headersStatic);    for (RabbitDocument.RabbitDocumentField rabbitDocumentField : document.getFields()) {        if (headersDynamic.contains(rabbitDocumentField.getKey())) {            message.addHeader(rabbitDocumentField.getKey(), rabbitDocumentField.getValues().get(0));        }    }}
 boolean nutch_f2057_0(RabbitDocument doc)
{    return docsToWrite.add(doc);}
 boolean nutch_f2058_0(RabbitDocument doc)
{    return docsToUpdate.add(doc);}
 boolean nutch_f2059_0(String url)
{    return docsToDelete.add(url);}
 byte[] nutch_f2060_0()
{    Gson gson = new Gson();    return gson.toJson(this).getBytes();}
 boolean nutch_f2061_0()
{    return docsToWrite.isEmpty() && docsToUpdate.isEmpty() && docsToDelete.isEmpty();}
public List<RabbitDocument> nutch_f2062_0()
{    return docsToWrite;}
public List<RabbitDocument> nutch_f2063_0()
{    return docsToUpdate;}
public List<String> nutch_f2064_0()
{    return docsToDelete;}
public int nutch_f2065_0()
{    return docsToWrite.size() + docsToUpdate.size() + docsToDelete.size();}
public void nutch_f2066_0()
{    docsToWrite.clear();    docsToUpdate.clear();    docsToDelete.clear();}
public void nutch_f2067_0(Configuration conf, String name)
{}
public void nutch_f2068_1(IndexWriterParams parameters)
{    this.type = parameters.get(SolrConstants.SERVER_TYPE, "http");    this.urls = parameters.getStrings(SolrConstants.SERVER_URLS);    this.collection = parameters.get(SolrConstants.COLLECTION);    if (urls == null) {        String message = "Missing SOLR URL.\n" + describe();                throw new RuntimeException(message);    }    this.auth = parameters.getBoolean(SolrConstants.USE_AUTH, false);    this.username = parameters.get(SolrConstants.USERNAME);    this.password = parameters.get(SolrConstants.PASSWORD);    this.solrClients = new ArrayList<>();    switch(type) {        case "http":            for (String url : urls) {                solrClients.add(SolrUtils.getHttpSolrClient(url));            }            break;        case "cloud":            CloudSolrClient sc = this.auth ? SolrUtils.getCloudSolrClient(Arrays.asList(urls), this.username, this.password) : SolrUtils.getCloudSolrClient(Arrays.asList(urls));            sc.setDefaultCollection(this.collection);            solrClients.add(sc);            break;        case "concurrent":                        throw new UnsupportedOperationException("The type \"concurrent\" is not yet supported.");        case "lb":                        throw new UnsupportedOperationException("The type \"lb\" is not yet supported.");        default:            throw new IllegalArgumentException("The type \"" + type + "\" is not supported.");    }    init(parameters);}
private void nutch_f2069_0(IndexWriterParams properties)
{    batchSize = properties.getInt(SolrConstants.COMMIT_SIZE, 1000);    delete = config.getBoolean(IndexerMapReduce.INDEXER_DELETE, false);    weightField = properties.get(SolrConstants.WEIGHT_FIELD, "");        params = new ModifiableSolrParams();    String paramString = config.get(IndexerMapReduce.INDEXER_PARAMS);    if (paramString != null) {        String[] values = paramString.split("&");        for (String v : values) {            String[] kv = v.split("=");            if (kv.length < 2) {                continue;            }            params.add(kv[0], kv[1]);        }    }}
public void nutch_f2070_0(String key) throws IOException
{        key = key.replaceAll("!", "\\!");    if (delete) {        deleteIds.add(key);        totalDeletes++;    }    if (deleteIds.size() >= batchSize) {        push();    }}
public void nutch_f2071_0(NutchDocument doc) throws IOException
{    write(doc);}
public void nutch_f2072_0(NutchDocument doc) throws IOException
{    final SolrInputDocument inputDoc = new SolrInputDocument();    for (final Entry<String, NutchField> e : doc) {        for (final Object val : e.getValue().getValues()) {                        Object val2 = val;            if (val instanceof Date) {                val2 = DateTimeFormatter.ISO_INSTANT.format(((Date) val).toInstant());            }            if (e.getKey().equals("content") || e.getKey().equals("title")) {                val2 = SolrUtils.stripNonCharCodepoints((String) val);            }            inputDoc.addField(e.getKey(), val2);        }    }    if (!weightField.isEmpty()) {        inputDoc.addField(weightField, doc.getWeight());    }    inputDocs.add(inputDoc);    totalAdds++;    if (inputDocs.size() + numDeletes >= batchSize) {        push();    }}
public void nutch_f2073_0() throws IOException
{    commit();    for (SolrClient solrClient : solrClients) {        solrClient.close();    }}
public void nutch_f2074_1() throws IOException
{    push();    try {        for (SolrClient solrClient : solrClients) {            if (this.auth) {                UpdateRequest req = new UpdateRequest();                req.setAction(UpdateRequest.ACTION.COMMIT, true, true);                req.setBasicAuthCredentials(this.username, this.password);                solrClient.request(req);            } else {                solrClient.commit();            }        }    } catch (final SolrServerException e) {            }}
private void nutch_f2075_1() throws IOException
{    if (inputDocs.size() > 0) {        try {                                    numDeletes = 0;            UpdateRequest req = new UpdateRequest();            req.add(inputDocs);            req.setAction(UpdateRequest.ACTION.OPTIMIZE, false, false);            req.setParams(params);            if (this.auth) {                req.setBasicAuthCredentials(this.username, this.password);            }            for (SolrClient solrClient : solrClients) {                solrClient.request(req);            }        } catch (final SolrServerException e) {            throw makeIOException(e);        }        inputDocs.clear();    }    if (deleteIds.size() > 0) {        try {                        UpdateRequest req = new UpdateRequest();            req.deleteById(deleteIds);            req.setAction(UpdateRequest.ACTION.OPTIMIZE, false, false);            req.setParams(params);            if (this.auth) {                req.setBasicAuthCredentials(this.username, this.password);            }            for (SolrClient solrClient : solrClients) {                solrClient.request(req);            }        } catch (final SolrServerException e) {                        throw makeIOException(e);        }        deleteIds.clear();    }}
private static IOException nutch_f2076_0(SolrServerException e)
{    return new IOException(e);}
public Configuration nutch_f2077_0()
{    return config;}
public void nutch_f2078_0(Configuration conf)
{    config = conf;}
public Map<String, Entry<String, Object>> nutch_f2079_0()
{    Map<String, Entry<String, Object>> properties = new LinkedHashMap<>();    properties.put(SolrConstants.SERVER_TYPE, new AbstractMap.SimpleEntry<>("Specifies the SolrClient implementation to use. This is a string value of one of the following \"cloud\" or \"http\"." + " The values represent CloudSolrServer or HttpSolrServer respectively.", this.type));    properties.put(SolrConstants.SERVER_URLS, new AbstractMap.SimpleEntry<>("Defines the fully qualified URL of Solr into which data should be indexed. Multiple URL can be provided using comma as a delimiter." + " When the value of type property is cloud, the URL should not include any collections or cores; just the root Solr path.", this.urls == null ? "" : String.join(",", urls)));    properties.put(SolrConstants.COLLECTION, new AbstractMap.SimpleEntry<>("The collection used in requests. Only used when the value of type property is cloud.", this.collection));    properties.put(SolrConstants.COMMIT_SIZE, new AbstractMap.SimpleEntry<>("Defines the number of documents to send to Solr in a single update batch. " + "Decrease when handling very large documents to prevent Nutch from running out of memory.\n" + "Note: It does not explicitly trigger a server side commit.", this.batchSize));    properties.put(SolrConstants.WEIGHT_FIELD, new AbstractMap.SimpleEntry<>("Field's name where the weight of the documents will be written. If it is empty no field will be used.", this.weightField));    properties.put(SolrConstants.USE_AUTH, new AbstractMap.SimpleEntry<>("Whether to enable HTTP basic authentication for communicating with Solr. Use the username and password properties to configure your credentials.", this.auth));    properties.put(SolrConstants.USERNAME, new AbstractMap.SimpleEntry<>("The username of Solr server.", this.username));    properties.put(SolrConstants.PASSWORD, new AbstractMap.SimpleEntry<>("The password of Solr server.", this.password));    return properties;}
 static CloudSolrClient nutch_f2080_0(List<String> urls)
{    CloudSolrClient sc = new CloudSolrClient.Builder(urls).withParallelUpdates(true).build();    sc.connect();    return sc;}
 static CloudSolrClient nutch_f2081_0(List<String> urls, String username, String password)
{        CredentialsProvider provider = new BasicCredentialsProvider();    UsernamePasswordCredentials credentials = new UsernamePasswordCredentials(username, password);    provider.setCredentials(AuthScope.ANY, credentials);    HttpClient client = HttpClientBuilder.create().setDefaultCredentialsProvider(provider).build();        CloudSolrClient sc = new CloudSolrClient.Builder(urls).withParallelUpdates(true).withHttpClient(client).build();    sc.connect();    return sc;}
 static SolrClient nutch_f2082_0(String url)
{    return new HttpSolrClient.Builder(url).build();}
 static String nutch_f2083_0(String input)
{    StringBuilder retval = new StringBuilder();    char ch;    for (int i = 0; i < input.length(); i++) {        ch = input.charAt(i);                if (        ch % 0x10000 != 0xffff &&         ch % 0x10000 != 0xfffe &&         (ch <= 0xfdd0 || ch >= 0xfdef) && (ch > 0x1F || ch == 0x9 || ch == 0xa || ch == 0xd)) {            retval.append(ch);        }    }    return retval.toString();}
public ParseResult nutch_f2084_1(Content content, ParseResult parseResult, HTMLMetaTags metaTags, DocumentFragment doc)
{    String lang = null;    Parse parse = parseResult.get(content.getUrl());    if (detect >= 0 && identify < 0) {        lang = detectLanguage(parse, doc);    } else if (detect < 0 && identify >= 0) {        lang = identifyLanguage(parse);    } else if (detect < identify) {        lang = detectLanguage(parse, doc);        if (lang == null) {            lang = identifyLanguage(parse);        }    } else if (identify < detect) {        lang = identifyLanguage(parse);        if (lang == null) {            lang = detectLanguage(parse, doc);        }    } else {                return parseResult;    }    if (lang != null) {        parse.getData().getParseMeta().set(Metadata.LANGUAGE, lang);        return parseResult;    }    return parseResult;}
private String nutch_f2085_0(Parse page, DocumentFragment doc)
{    String lang = getLanguageFromMetadata(page.getData().getParseMeta());    if (lang == null) {        LanguageParser parser = new LanguageParser(doc);        lang = parser.getLanguage();    }    if (lang != null) {        return lang;    }    lang = page.getData().getContentMeta().get(Response.CONTENT_LANGUAGE);    return lang;}
private String nutch_f2086_0(Parse parse)
{    StringBuilder text = new StringBuilder();    if (parse == null)        return null;    String title = parse.getData().getTitle();    if (title != null) {        text.append(title.toString());    }    String content = parse.getText();    if (content != null) {        text.append(" ").append(content.toString());    }        String titleandcontent = text.toString();    if (this.contentMaxlength != -1 && titleandcontent.length() > this.contentMaxlength)        titleandcontent = titleandcontent.substring(0, contentMaxlength);    LanguageIdentifier identifier = new LanguageIdentifier(titleandcontent);    if (onlyCertain) {        if (identifier.isReasonablyCertain())            return identifier.getLanguage();        else            return null;    }    return identifier.getLanguage();}
private static String nutch_f2087_0(Metadata meta)
{    if (meta == null)        return null;        String lang = meta.get("dc.language");    if (lang != null)        return lang;        lang = meta.get("content-language");    if (lang != null)        return lang;        return meta.get("lang");}
 String nutch_f2088_0()
{    return language;}
 void nutch_f2089_0(Node node)
{    NodeWalker walker = new NodeWalker(node);    while (walker.hasNext()) {        Node currentNode = walker.nextNode();        String nodeName = currentNode.getNodeName();        short nodeType = currentNode.getNodeType();        if (nodeType == Node.ELEMENT_NODE) {                        if (htmlAttribute == null) {                htmlAttribute = parseLanguage(((Element) currentNode).getAttribute("lang"));            }                        if ("meta".equalsIgnoreCase(nodeName)) {                NamedNodeMap attrs = currentNode.getAttributes();                                if (dublinCore == null) {                    for (int i = 0; i < attrs.getLength(); i++) {                        Node attrnode = attrs.item(i);                        if ("name".equalsIgnoreCase(attrnode.getNodeName())) {                            if ("dc.language".equalsIgnoreCase(attrnode.getNodeValue())) {                                Node valueattr = attrs.getNamedItem("content");                                if (valueattr != null) {                                    dublinCore = parseLanguage(valueattr.getNodeValue());                                }                            }                        }                    }                }                                if (httpEquiv == null) {                    for (int i = 0; i < attrs.getLength(); i++) {                        Node attrnode = attrs.item(i);                        if ("http-equiv".equalsIgnoreCase(attrnode.getNodeName())) {                            if ("content-language".equals(attrnode.getNodeValue().toLowerCase())) {                                Node valueattr = attrs.getNamedItem("content");                                if (valueattr != null) {                                    httpEquiv = parseLanguage(valueattr.getNodeValue());                                }                            }                        }                    }                }            }        }        if ((dublinCore != null) && (htmlAttribute != null) && (httpEquiv != null)) {            return;        }    }}
 static final String nutch_f2090_0(String lang)
{    if (lang == null) {        return null;    }    String code = null;    String language = null;        String[] langs = lang.split(",| |;|\\.|\\(|\\)|=", -1);    int i = 0;    while ((language == null) && (i < langs.length)) {                code = langs[i].split("-")[0];        code = code.split("_")[0];                language = (String) LANGUAGES_MAP.get(code.toLowerCase());        i++;    }    return language;}
public void nutch_f2091_0(Configuration conf)
{    this.conf = conf;    contentMaxlength = conf.getInt("lang.analyze.max.length", -1);    onlyCertain = conf.getBoolean("lang.identification.only.certain", false);    String[] policy = conf.getStrings("lang.extraction.policy");    for (int i = 0; i < policy.length; i++) {        if (policy[i].equals("detect")) {            detect = i;        } else if (policy[i].equals("identify")) {            identify = i;        }    }}
public Configuration nutch_f2092_0()
{    return this.conf;}
public NutchDocument nutch_f2093_0(NutchDocument doc, Parse parse, Text url, CrawlDatum datum, Inlinks inlinks) throws IndexingException
{        String lang = parse.getData().getParseMeta().get(Metadata.LANGUAGE);        if (lang == null) {        lang = parse.getData().getContentMeta().get(Response.CONTENT_LANGUAGE);    }    if (lang == null || lang.length() == 0) {        lang = "unknown";    }    if (!indexLangs.isEmpty() && !indexLangs.contains(lang)) {        return null;    }    doc.add("lang", lang);    return doc;}
public void nutch_f2094_0(Configuration conf)
{    this.conf = conf;    indexLangs = new HashSet<>(conf.getStringCollection("lang.index.languages"));}
public Configuration nutch_f2095_0()
{    return this.conf;}
public void nutch_f2096_0()
{    try {        ParseUtil parser = new ParseUtil(NutchConfiguration.create());        /* loop through the test documents and validate result */        for (int t = 0; t < docs.length; t++) {            Content content = getContent(docs[t]);            Parse parse = parser.parse(content).get(content.getUrl());            Assert.assertEquals(metalanguages[t], (String) parse.getData().getParseMeta().get(Metadata.LANGUAGE));        }    } catch (Exception e) {        e.printStackTrace(System.out);        Assert.fail(e.toString());    }}
public void nutch_f2097_0()
{    String[][] tests = { { "(SCHEME=ISO.639-1) sv", "sv" }, { "(SCHEME=RFC1766) sv-FI", "sv" }, { "(SCHEME=Z39.53) SWE", "sv" }, { "EN_US, SV, EN, EN_UK", "en" }, { "English Swedish", "en" }, { "English, swedish", "en" }, { "English,Swedish", "en" }, { "Other (Svenska)", "sv" }, { "SE", "se" }, { "SV", "sv" }, { "SV charset=iso-8859-1", "sv" }, { "SV-FI", "sv" }, { "SV; charset=iso-8859-1", "sv" }, { "SVE", "sv" }, { "SW", "sw" }, { "SWE", "sv" }, { "SWEDISH", "sv" }, { "Sv", "sv" }, { "Sve", "sv" }, { "Svenska", "sv" }, { "Swedish", "sv" }, { "Swedish, svenska", "sv" }, { "en, sv", "en" }, { "sv", "sv" }, { "sv, be, dk, de, fr, no, pt, ch, fi, en", "sv" }, { "sv,en", "sv" }, { "sv-FI", "sv" }, { "sv-SE", "sv" }, { "sv-en", "sv" }, { "sv-fi", "sv" }, { "sv-se", "sv" }, { "sv; Content-Language: sv", "sv" }, { "sv_SE", "sv" }, { "sve", "sv" }, { "svenska, swedish, engelska, english", "sv" }, { "sw", "sw" }, { "swe", "sv" }, { "swe.SPR.", "sv" }, { "sweden", "sv" }, { "swedish", "sv" }, { "swedish,", "sv" }, { "text/html; charset=sv-SE", "sv" }, { "text/html; sv", "sv" }, { "torp, stuga, uthyres, bed & breakfast", null } };    for (int i = 0; i < 44; i++) {        Assert.assertEquals(tests[i][1], HTMLLanguageParser.LanguageParser.parseLanguage(tests[i][0]));    }}
private Content nutch_f2098_0(String text)
{    Metadata meta = new Metadata();    meta.add("Content-Type", "text/html");    return new Content(URL, BASE, text.getBytes(), "text/html", meta, NutchConfiguration.create());}
public void nutch_f2099_0()
{    try {        long total = 0;        LanguageIdentifier identifier;        BufferedReader in = new BufferedReader(new InputStreamReader(this.getClass().getResourceAsStream("test-referencial.txt")));        String line = null;        while ((line = in.readLine()) != null) {            String[] tokens = line.split(";");            if (!tokens[0].equals("")) {                StringBuilder content = new StringBuilder();                                BufferedReader testFile = new BufferedReader(new InputStreamReader(this.getClass().getResourceAsStream(tokens[0]), "UTF-8"));                String testLine = null, lang = null;                while ((testLine = testFile.readLine()) != null) {                    content.append(testLine + "\n");                    testLine = testLine.trim();                    if (testLine.length() > 256) {                        identifier = new LanguageIdentifier(testLine);                        lang = identifier.getLanguage();                        Assert.assertEquals(tokens[1], lang);                    }                }                testFile.close();                                long start = System.currentTimeMillis();                System.out.println(content.toString());                identifier = new LanguageIdentifier(content.toString());                lang = identifier.getLanguage();                System.out.println(lang);                total += System.currentTimeMillis() - start;                Assert.assertEquals(tokens[1], lang);            }        }        in.close();        System.out.println("Total Time=" + total);    } catch (Exception e) {        e.printStackTrace();        Assert.fail(e.toString());    }}
protected WebClient nutch_f2100_0(WebClient client)
{    client.getOptions().setJavaScriptEnabled(enableJavascript);    client.getOptions().setCssEnabled(enableCss);    client.getOptions().setRedirectEnabled(enableRedirect);    if (enableJavascript)        client.setJavaScriptTimeout(javascriptTimeout);    client.getOptions().setThrowExceptionOnScriptError(false);    if (enableRedirect)        client.addWebWindowListener(new HtmlUnitWebWindowListener(maxRedirects));    return client;}
public static WebDriver nutch_f2101_1(String url, Configuration conf)
{    long pageLoadTimout = conf.getLong("page.load.delay", 3);    enableJavascript = conf.getBoolean("htmlunit.enable.javascript", true);    enableCss = conf.getBoolean("htmlunit.enable.css", false);    javascriptTimeout = conf.getLong("htmlunit.javascript.timeout", 3500);    int redirects = Integer.parseInt(conf.get("http.redirect.max", "0"));    enableRedirect = redirects <= 0 ? false : true;    maxRedirects = redirects;    WebDriver driver = null;    try {        driver = new HtmlUnitWebDriver();        driver.manage().timeouts().pageLoadTimeout(pageLoadTimout, TimeUnit.SECONDS);        driver.get(url);    } catch (Exception e) {        if (e instanceof TimeoutException) {                        return driver;        }        cleanUpDriver(driver);        throw new RuntimeException(e);    }    return driver;}
public static String nutch_f2102_0(WebDriver driver, Configuration conf)
{    try {        if (conf.getBoolean("take.screenshot", false))            takeScreenshot(driver, conf);        String innerHtml = "";        if (enableJavascript) {            WebElement body = driver.findElement(By.tagName("body"));            innerHtml = (String) ((JavascriptExecutor) driver).executeScript("return arguments[0].innerHTML;", body);        } else            innerHtml = driver.getPageSource().replaceAll("&amp;", "&");        return innerHtml;    } catch (Exception e) {        TemporaryFilesystem.getDefaultTmpFS().deleteTemporaryFiles();        cleanUpDriver(driver);        throw new RuntimeException(e);    }}
public static void nutch_f2103_0(WebDriver driver)
{    if (driver != null) {        try {            driver.close();            driver.quit();            TemporaryFilesystem.getDefaultTmpFS().deleteTemporaryFiles();        } catch (Exception e) {            throw new RuntimeException(e);        }    }}
public static String nutch_f2104_0(String url, Configuration conf)
{    WebDriver driver = getDriverForPage(url, conf);    try {        if (conf.getBoolean("take.screenshot", false))            takeScreenshot(driver, conf);        String innerHtml = "";        if (enableJavascript) {            WebElement body = driver.findElement(By.tagName("body"));            innerHtml = (String) ((JavascriptExecutor) driver).executeScript("return arguments[0].innerHTML;", body);        } else            innerHtml = driver.getPageSource().replaceAll("&amp;", "&");        return innerHtml;    } catch (Exception e) {        TemporaryFilesystem.getDefaultTmpFS().deleteTemporaryFiles();        throw new RuntimeException(e);    } finally {        cleanUpDriver(driver);    }}
private static void nutch_f2105_1(WebDriver driver, Configuration conf)
{    try {        String url = driver.getCurrentUrl();        File srcFile = ((TakesScreenshot) driver).getScreenshotAs(OutputType.FILE);                FileSystem fs = FileSystem.get(conf);        if (conf.get("screenshot.location") != null) {            Path screenshotPath = new Path(conf.get("screenshot.location") + "/" + srcFile.getName());            OutputStream os = null;            if (!fs.exists(screenshotPath)) {                                os = fs.create(screenshotPath);            }            InputStream is = new BufferedInputStream(new FileInputStream(srcFile));            IOUtils.copyBytes(is, os, conf);                    } else {                    }    } catch (Exception e) {        cleanUpDriver(driver);        throw new RuntimeException(e);    }}
public void nutch_f2106_0(WebWindowEvent event)
{}
public void nutch_f2107_0(WebWindowEvent event)
{    redirectCount++;    if (redirectCount > maxRedirects)        throw new RuntimeException("Redirect Count: " + redirectCount + " exceeded the Maximum Redirects allowed: " + maxRedirects);}
public void nutch_f2108_0(WebWindowEvent event)
{}
public void nutch_f2109_1(Configuration conf)
{    this.conf = conf;    this.proxyHost = conf.get("http.proxy.host");    this.proxyPort = conf.getInt("http.proxy.port", 8080);    this.proxyType = Proxy.Type.valueOf(conf.get("http.proxy.type", "HTTP"));    this.proxyException = arrayToMap(conf.getStrings("http.proxy.exception.list"));    this.useProxy = (proxyHost != null && proxyHost.length() > 0);    this.timeout = conf.getInt("http.timeout", 10000);    this.maxContent = conf.getInt("http.content.limit", 1024 * 1024);    this.maxDuration = conf.getInt("http.time.limit", -1);    this.partialAsTruncated = conf.getBoolean("http.partial.truncated", false);    this.userAgent = getAgentString(conf.get("http.agent.name"), conf.get("http.agent.version"), conf.get("http.agent.description"), conf.get("http.agent.url"), conf.get("http.agent.email"));    this.acceptLanguage = conf.get("http.accept.language", acceptLanguage).trim();    this.acceptCharset = conf.get("http.accept.charset", acceptCharset).trim();    this.accept = conf.get("http.accept", accept).trim();    this.mimeTypes = new MimeUtil(conf);        this.useHttp11 = conf.getBoolean("http.useHttp11", true);    this.useHttp2 = conf.getBoolean("http.useHttp2", false);    this.tlsCheckCertificate = conf.getBoolean("http.tls.certificates.check", false);    this.responseTime = conf.getBoolean("http.store.responsetime", true);    this.storeIPAddress = conf.getBoolean("store.ip.address", false);    this.storeHttpRequest = conf.getBoolean("store.http.request", false);    this.storeHttpHeaders = conf.getBoolean("store.http.headers", false);    this.enableIfModifiedsinceHeader = conf.getBoolean("http.enable.if.modified.since.header", true);    this.enableCookieHeader = conf.getBoolean("http.enable.cookie.header", true);    this.robots.setConf(conf);    this.logUtil.setConf(conf);        if (conf.getBoolean("http.agent.rotate", false)) {        String agentsFile = conf.get("http.agent.rotate.file", "agents.txt");        BufferedReader br = null;        try {            Reader reader = conf.getConfResourceAsReader(agentsFile);            br = new BufferedReader(reader);            userAgentNames = new ArrayList<String>();            String word = "";            while ((word = br.readLine()) != null) {                if (!word.trim().isEmpty())                    userAgentNames.add(word.trim());            }            if (userAgentNames.size() == 0) {                                userAgentNames = null;            }        } catch (Exception e) {                        userAgentNames = null;        } finally {            if (br != null) {                try {                    br.close();                } catch (IOException e) {                                }            }        }        if (userAgentNames == null) {                    }    }        if (enableCookieHeader) {        String cookieFile = conf.get("http.agent.host.cookie.file", "cookies.txt");        BufferedReader br = null;        try {            Reader reader = conf.getConfResourceAsReader(cookieFile);            br = new BufferedReader(reader);            hostCookies = new HashMap<String, String>();            String word = "";            while ((word = br.readLine()) != null) {                if (!word.trim().isEmpty()) {                    if (word.indexOf("#") == -1) {                                                String[] parts = word.split("\t");                        if (parts.length == 2) {                            hostCookies.put(parts[0], parts[1]);                        } else {                                                    }                    }                }            }        } catch (Exception e) {                        hostCookies = null;        } finally {            if (br != null) {                try {                    br.close();                } catch (IOException e) {                                }            }        }    }    String[] protocols = conf.getStrings("http.tls.supported.protocols", "TLSv1.2", "TLSv1.1", "TLSv1", "SSLv3");    String[] ciphers = conf.getStrings("http.tls.supported.cipher.suites", "TLS_ECDHE_ECDSA_WITH_AES_256_CBC_SHA384", "TLS_ECDHE_RSA_WITH_AES_256_CBC_SHA384", "TLS_RSA_WITH_AES_256_CBC_SHA256", "TLS_ECDH_ECDSA_WITH_AES_256_CBC_SHA384", "TLS_ECDH_RSA_WITH_AES_256_CBC_SHA384", "TLS_DHE_RSA_WITH_AES_256_CBC_SHA256", "TLS_DHE_DSS_WITH_AES_256_CBC_SHA256", "TLS_ECDHE_ECDSA_WITH_AES_256_CBC_SHA", "TLS_ECDHE_RSA_WITH_AES_256_CBC_SHA", "TLS_RSA_WITH_AES_256_CBC_SHA", "TLS_ECDH_ECDSA_WITH_AES_256_CBC_SHA", "TLS_ECDH_RSA_WITH_AES_256_CBC_SHA", "TLS_DHE_RSA_WITH_AES_256_CBC_SHA", "TLS_DHE_DSS_WITH_AES_256_CBC_SHA", "TLS_ECDHE_ECDSA_WITH_AES_128_CBC_SHA256", "TLS_ECDHE_RSA_WITH_AES_128_CBC_SHA256", "TLS_RSA_WITH_AES_128_CBC_SHA256", "TLS_ECDH_ECDSA_WITH_AES_128_CBC_SHA256", "TLS_ECDH_RSA_WITH_AES_128_CBC_SHA256", "TLS_DHE_RSA_WITH_AES_128_CBC_SHA256", "TLS_DHE_DSS_WITH_AES_128_CBC_SHA256", "TLS_ECDHE_ECDSA_WITH_AES_128_CBC_SHA", "TLS_ECDHE_RSA_WITH_AES_128_CBC_SHA", "TLS_RSA_WITH_AES_128_CBC_SHA", "TLS_ECDH_ECDSA_WITH_AES_128_CBC_SHA", "TLS_ECDH_RSA_WITH_AES_128_CBC_SHA", "TLS_DHE_RSA_WITH_AES_128_CBC_SHA", "TLS_DHE_DSS_WITH_AES_128_CBC_SHA", "TLS_ECDHE_ECDSA_WITH_RC4_128_SHA", "TLS_ECDHE_RSA_WITH_RC4_128_SHA", "SSL_RSA_WITH_RC4_128_SHA", "TLS_ECDH_ECDSA_WITH_RC4_128_SHA", "TLS_ECDH_RSA_WITH_RC4_128_SHA", "TLS_ECDHE_ECDSA_WITH_3DES_EDE_CBC_SHA", "TLS_ECDHE_RSA_WITH_3DES_EDE_CBC_SHA", "SSL_RSA_WITH_3DES_EDE_CBC_SHA", "TLS_ECDH_ECDSA_WITH_3DES_EDE_CBC_SHA", "TLS_ECDH_RSA_WITH_3DES_EDE_CBC_SHA", "SSL_DHE_RSA_WITH_3DES_EDE_CBC_SHA", "SSL_DHE_DSS_WITH_3DES_EDE_CBC_SHA", "SSL_RSA_WITH_RC4_128_MD5", "TLS_EMPTY_RENEGOTIATION_INFO_SCSV", "TLS_RSA_WITH_NULL_SHA256", "TLS_ECDHE_ECDSA_WITH_NULL_SHA", "TLS_ECDHE_RSA_WITH_NULL_SHA", "SSL_RSA_WITH_NULL_SHA", "TLS_ECDH_ECDSA_WITH_NULL_SHA", "TLS_ECDH_RSA_WITH_NULL_SHA", "SSL_RSA_WITH_NULL_MD5", "SSL_RSA_WITH_DES_CBC_SHA", "SSL_DHE_RSA_WITH_DES_CBC_SHA", "SSL_DHE_DSS_WITH_DES_CBC_SHA", "TLS_KRB5_WITH_RC4_128_SHA", "TLS_KRB5_WITH_RC4_128_MD5", "TLS_KRB5_WITH_3DES_EDE_CBC_SHA", "TLS_KRB5_WITH_3DES_EDE_CBC_MD5", "TLS_KRB5_WITH_DES_CBC_SHA", "TLS_KRB5_WITH_DES_CBC_MD5");    tlsPreferredProtocols = new HashSet<String>(Arrays.asList(protocols));    tlsPreferredCipherSuites = new HashSet<String>(Arrays.asList(ciphers));    logConf();}
public Configuration nutch_f2110_0()
{    return this.conf;}
public ProtocolOutput nutch_f2111_1(Text url, CrawlDatum datum)
{    String urlString = url.toString();    try {        URL u = new URL(urlString);        long startTime = System.currentTimeMillis();                Response response = getResponse(u, datum, false);        if (this.responseTime) {            int elapsedTime = (int) (System.currentTimeMillis() - startTime);            datum.getMetaData().put(RESPONSE_TIME, new IntWritable(elapsedTime));        }        int code = response.getCode();        datum.getMetaData().put(Nutch.PROTOCOL_STATUS_CODE_KEY, new Text(Integer.toString(code)));        byte[] content = response.getContent();        Content c = new Content(u.toString(), u.toString(), (content == null ? EMPTY_CONTENT : content), response.getHeader("Content-Type"), response.getHeaders(), mimeTypes);        if (code == 200) {                        return new ProtocolOutput(c);        } else if (code >= 300 && code < 400) {                        String location = response.getHeader("Location");                        if (location == null)                location = response.getHeader("location");            if (location == null)                location = "";            u = new URL(u, location);            int protocolStatusCode;            switch(code) {                case                 300:                    protocolStatusCode = ProtocolStatus.MOVED;                    break;                                case 301:                case                 305:                    protocolStatusCode = ProtocolStatus.MOVED;                    break;                                case 302:                                case 303:                case                 307:                    protocolStatusCode = ProtocolStatus.TEMP_MOVED;                    break;                case                 304:                    protocolStatusCode = ProtocolStatus.NOTMODIFIED;                    break;                default:                    protocolStatusCode = ProtocolStatus.MOVED;            }                        return new ProtocolOutput(c, new ProtocolStatus(protocolStatusCode, u));        } else if (code == 400) {                        if (logger.isTraceEnabled()) {                logger.trace("400 Bad request: " + u);            }            return new ProtocolOutput(c, new ProtocolStatus(ProtocolStatus.GONE, u));        } else if (code == 401) {                        if (logger.isTraceEnabled()) {                logger.trace("401 Authentication Required");            }            return new ProtocolOutput(c, new ProtocolStatus(ProtocolStatus.ACCESS_DENIED, "Authentication required: " + urlString));        } else if (code == 404) {            return new ProtocolOutput(c, new ProtocolStatus(ProtocolStatus.NOTFOUND, u));        } else if (code == 410) {                        return new ProtocolOutput(c, new ProtocolStatus(ProtocolStatus.GONE, "Http: " + code + " url=" + u));        } else {            return new ProtocolOutput(c, new ProtocolStatus(ProtocolStatus.EXCEPTION, "Http code=" + code + ", url=" + u));        }    } catch (Throwable e) {        if (logger.isDebugEnabled() || !logUtil.logShort(e)) {                    } else {                    }        return new ProtocolOutput(null, new ProtocolStatus(e));    }}
public String nutch_f2112_0()
{    return proxyHost;}
public int nutch_f2113_0()
{    return proxyPort;}
public boolean nutch_f2114_0(URL url)
{    return useProxy(url.getHost());}
public boolean nutch_f2115_0(URI uri)
{    return useProxy(uri.getHost());}
public boolean nutch_f2116_0(String host)
{    if (useProxy && proxyException.containsKey(host)) {        return false;    }    return useProxy;}
public int nutch_f2117_0()
{    return timeout;}
public boolean nutch_f2118_0()
{    return enableIfModifiedsinceHeader;}
public boolean nutch_f2119_0()
{    return enableCookieHeader;}
public boolean nutch_f2120_0()
{    return storeIPAddress;}
public boolean nutch_f2121_0()
{    return storeHttpRequest;}
public boolean nutch_f2122_0()
{    return storeHttpHeaders;}
public int nutch_f2123_0()
{    return maxContent;}
public int nutch_f2124_0()
{    return maxDuration;}
public boolean nutch_f2125_0()
{    return partialAsTruncated;}
public String nutch_f2126_0()
{    if (userAgentNames != null) {        return userAgentNames.get(ThreadLocalRandom.current().nextInt(userAgentNames.size()));    }    return userAgent;}
public String nutch_f2127_0(URL url)
{    if (hostCookies != null) {        return hostCookies.get(url.getHost());    }    return null;}
public String nutch_f2128_0()
{    return acceptLanguage;}
public String nutch_f2129_0()
{    return acceptCharset;}
public String nutch_f2130_0()
{    return accept;}
public boolean nutch_f2131_0()
{    return useHttp11;}
public boolean nutch_f2132_0()
{    return tlsCheckCertificate;}
public Set<String> nutch_f2133_0()
{    return tlsPreferredCipherSuites;}
public Set<String> nutch_f2134_0()
{    return tlsPreferredProtocols;}
private static String nutch_f2135_1(String agentName, String agentVersion, String agentDesc, String agentURL, String agentEmail)
{    if ((agentName == null) || (agentName.trim().length() == 0)) {                if (LOG.isErrorEnabled()) {                    }    }    StringBuffer buf = new StringBuffer();    buf.append(agentName);    if (agentVersion != null && !agentVersion.trim().isEmpty()) {        buf.append("/");        buf.append(agentVersion);    }    if (((agentDesc != null) && (agentDesc.length() != 0)) || ((agentEmail != null) && (agentEmail.length() != 0)) || ((agentURL != null) && (agentURL.length() != 0))) {        buf.append(" (");        if ((agentDesc != null) && (agentDesc.length() != 0)) {            buf.append(agentDesc);            if ((agentURL != null) || (agentEmail != null))                buf.append("; ");        }        if ((agentURL != null) && (agentURL.length() != 0)) {            buf.append(agentURL);            if (agentEmail != null)                buf.append("; ");        }        if ((agentEmail != null) && (agentEmail.length() != 0))            buf.append(agentEmail);        buf.append(")");    }    return buf.toString();}
protected void nutch_f2136_1()
{    if (logger.isInfoEnabled()) {                                                                            }}
public byte[] nutch_f2137_0(byte[] compressed, URL url) throws IOException
{    if (LOG.isTraceEnabled()) {        LOG.trace("uncompressing....");    }        if (compressed.length == 0)        return compressed;    byte[] content;    if (getMaxContent() >= 0) {        content = GZIPUtils.unzipBestEffort(compressed, getMaxContent());    } else {        content = GZIPUtils.unzipBestEffort(compressed);    }    if (content == null)        throw new IOException("unzipBestEffort returned null");    if (LOG.isTraceEnabled()) {        LOG.trace("fetched " + compressed.length + " bytes of compressed content (expanded to " + content.length + " bytes) from " + url);    }    return content;}
public byte[] nutch_f2138_0(byte[] compressed, URL url) throws IOException
{        if (compressed.length == 0)        return compressed;    if (LOG.isTraceEnabled()) {        LOG.trace("inflating....");    }    byte[] content;    if (getMaxContent() >= 0) {        content = DeflateUtils.inflateBestEffort(compressed, getMaxContent());    } else {        content = DeflateUtils.inflateBestEffort(compressed);    }    if (content == null)        throw new IOException("inflateBestEffort returned null");    if (LOG.isTraceEnabled()) {        LOG.trace("fetched " + compressed.length + " bytes of compressed content (expanded to " + content.length + " bytes) from " + url);    }    return content;}
protected static void nutch_f2139_0(HttpBase http, String[] args) throws Exception
{    String url = null;    String usage = "Usage: Http [-verbose] [-timeout N] url";    if (args.length == 0) {        System.err.println(usage);        System.exit(-1);    }    for (int i = 0; i < args.length; i++) {                if (args[i].equals("-timeout")) {                        http.timeout = Integer.parseInt(args[++i]) * 1000;        } else if (args[i].equals("-verbose")) {                } else if (i != args.length - 1) {            System.err.println(usage);            System.exit(-1);        } else                        url = args[i];    }    ProtocolOutput out = http.getProtocolOutput(new Text(url), new CrawlDatum());    Content content = out.getContent();    System.out.println("Status: " + out.getStatus());    if (content != null) {        System.out.println("Content Type: " + content.getContentType());        System.out.println("Content Length: " + content.getMetadata().get(Response.CONTENT_LENGTH));        System.out.println("Content:");        String text = new String(content.getContent());        System.out.println(text);    }}
public BaseRobotRules nutch_f2140_0(Text url, CrawlDatum datum, List<Content> robotsTxtContent)
{    return robots.getRobotRulesSet(this, url, robotsTxtContent);}
private HashMap<String, String> nutch_f2141_0(String[] input)
{    if (input == null || input.length == 0) {        return new HashMap<String, String>();    }    HashMap<String, String> hm = new HashMap<>();    for (int i = 0; i < input.length; i++) {        if (!"".equals(input[i].trim())) {            hm.put(input[i], input[i]);        }    }    return hm;}
public void nutch_f2142_0(Configuration conf)
{    super.setConf(conf);    allowForbidden = conf.getBoolean("http.robots.403.allow", true);}
protected static String nutch_f2143_0(URL url)
{        String protocol = url.getProtocol().toLowerCase();            String host = url.getHost().toLowerCase();    int port = url.getPort();    if (port == -1) {        port = url.getDefaultPort();    }    /*     * Robot rules apply only to host, protocol, and port where robots.txt is     * hosted (cf. NUTCH-1752). Consequently     */    String cacheKey = protocol + ":" + host + ":" + port;    return cacheKey;}
public BaseRobotRules nutch_f2144_1(Protocol http, URL url, List<Content> robotsTxtContent)
{    if (LOG.isTraceEnabled() && isWhiteListed(url)) {        LOG.trace("Ignoring robots.txt (host is whitelisted) for URL: {}", url);    }    String cacheKey = getCacheKey(url);    BaseRobotRules robotRules = CACHE.get(cacheKey);    if (robotRules != null) {                return robotRules;    } else if (LOG.isTraceEnabled()) {        LOG.trace("cache miss " + url);    }    boolean cacheRule = true;    URL redir = null;    if (isWhiteListed(url)) {                        robotRules = EMPTY_RULES;                    } else {        try {            URL robotsUrl = new URL(url, "/robots.txt");            Response response = ((HttpBase) http).getResponse(robotsUrl, new CrawlDatum(), true);            if (robotsTxtContent != null) {                addRobotsContent(robotsTxtContent, robotsUrl, response);            }                        if (response.getCode() == 301 || response.getCode() == 302) {                String redirection = response.getHeader("Location");                if (redirection == null) {                                        redirection = response.getHeader("location");                }                if (redirection != null) {                    if (!redirection.startsWith("http")) {                                                redir = new URL(url, redirection);                    } else {                        redir = new URL(redirection);                    }                    response = ((HttpBase) http).getResponse(redir, new CrawlDatum(), true);                    if (robotsTxtContent != null) {                        addRobotsContent(robotsTxtContent, redir, response);                    }                }            }            if (            response.getCode() == 200)                robotRules = parseRules(url.toString(), response.getContent(), response.getHeader("Content-Type"), agentNames);            else if ((response.getCode() == 403) && (!allowForbidden))                                robotRules = FORBID_ALL_RULES;            else if (response.getCode() >= 500) {                                cacheRule = false;                robotRules = EMPTY_RULES;            } else                                robotRules = EMPTY_RULES;        } catch (Throwable t) {            if (LOG.isInfoEnabled()) {                            }                        cacheRule = false;            robotRules = EMPTY_RULES;        }    }    if (cacheRule) {                CACHE.put(cacheKey, robotRules);        if (redir != null && !redir.getHost().equalsIgnoreCase(url.getHost()) && "/robots.txt".equals(redir.getFile())) {                                    CACHE.put(getCacheKey(redir), robotRules);        }    }    return robotRules;}
protected void nutch_f2145_0(List<Content> robotsTxtContent, URL robotsUrl, Response robotsResponse)
{    byte[] robotsBytes = robotsResponse.getContent();    if (robotsBytes == null)        robotsBytes = new byte[0];    Content content = new Content(robotsUrl.toString(), robotsUrl.toString(), robotsBytes, robotsResponse.getHeader("Content-Type"), robotsResponse.getHeaders(), getConf());    robotsTxtContent.add(content);}
public void nutch_f2146_0()
{    rules = parser.parseRules("testRobotsAgent", ROBOTS_STRING.getBytes(), CONTENT_TYPE, SINGLE_AGENT);    for (int counter = 0; counter < TEST_PATHS.length; counter++) {        Assert.assertTrue("testing on agent (" + SINGLE_AGENT + "), and " + "path " + TEST_PATHS[counter] + " got " + rules.isAllowed(TEST_PATHS[counter]), rules.isAllowed(TEST_PATHS[counter]) == RESULTS[counter]);    }    rules = parser.parseRules("testRobotsAgent", ROBOTS_STRING.getBytes(), CONTENT_TYPE, MULTIPLE_AGENTS);    for (int counter = 0; counter < TEST_PATHS.length; counter++) {        Assert.assertTrue("testing on agents (" + MULTIPLE_AGENTS + "), and " + "path " + TEST_PATHS[counter] + " got " + rules.isAllowed(TEST_PATHS[counter]), rules.isAllowed(TEST_PATHS[counter]) == RESULTS[counter]);    }}
public void nutch_f2147_0()
{            rules = parser.parseRules("testCrawlDelay", ROBOTS_STRING.getBytes(), CONTENT_TYPE, SINGLE_AGENT);    Assert.assertTrue("testing crawl delay for agent " + SINGLE_AGENT + " : ", (rules.getCrawlDelay() == 10000));        rules = parser.parseRules("testCrawlDelay", ROBOTS_STRING.getBytes(), CONTENT_TYPE, UNKNOWN_AGENT);    Assert.assertTrue("testing crawl delay for agent " + UNKNOWN_AGENT + " : ", (rules.getCrawlDelay() == Long.MIN_VALUE));}
public void nutch_f2148_0() throws IOException
{    channel = connection.createChannel();}
public void nutch_f2149_0(String exchangeName, String exchangeOptions, String queueName, String queueOptions, String bindingKey, String bindingArguments) throws IOException
{    String exchangeType = exchangeDeclare(exchangeName, exchangeOptions);    queueDeclare(queueName, queueOptions);    switch(exchangeType) {        case "fanout":            channel.queueBind(queueName, exchangeName, "");            break;        case "direct":            channel.queueBind(queueName, exchangeName, getValue(bindingKey, DEFAULT_ROUTING_KEY));            break;        case "headers":            channel.queueBind(queueName, exchangeName, "", RabbitMQOptionParser.parseOptionAndConvertValue(bindingArguments));            break;        case "topic":            channel.queueBind(queueName, exchangeName, getValue(bindingKey, DEFAULT_ROUTING_KEY));            break;        default:            break;    }}
public void nutch_f2150_0(String exchangeName, String routingKey, RabbitMQMessage message) throws IOException
{    channel.basicPublish(getValue(exchangeName, DEFAULT_EXCHANGE_NAME), getValue(routingKey, DEFAULT_ROUTING_KEY), new AMQP.BasicProperties.Builder().contentType(message.getContentType()).headers(message.getHeaders()).build(), message.getBody());}
public void nutch_f2151_0() throws IOException
{    try {        channel.close();        connection.close();    } catch (TimeoutException e) {        throw makeIOException(e);    }}
private String nutch_f2152_0(String name, String options) throws IOException
{    Map<String, String> values = RabbitMQOptionParser.parseOption(options);    String type = values.getOrDefault("type", DEFAULT_EXCHANGE_TYPE);    channel.exchangeDeclare(getValue(name, DEFAULT_EXCHANGE_NAME), type, Boolean.parseBoolean(values.getOrDefault("durable", DEFAULT_EXCHANGE_DURABLE)));    return type;}
private void nutch_f2153_0(String name, String options) throws IOException
{    Map<String, String> values = RabbitMQOptionParser.parseOption(options);    channel.queueDeclare(getValue(name, DEFAULT_QUEUE_NAME), Boolean.parseBoolean(values.getOrDefault("durable", DEFAULT_QUEUE_DURABLE)), Boolean.parseBoolean(values.getOrDefault("exclusive", DEFAULT_QUEUE_EXCLUSIVE)), Boolean.parseBoolean(values.getOrDefault("auto-delete", DEFAULT_QUEUE_AUTO_DELETE)), RabbitMQOptionParser.parseSubOption(values.getOrDefault("arguments", DEFAULT_QUEUE_ARGUMENTS)));}
private static String nutch_f2154_0(String value, String defaultValue)
{    if (value == null || value.trim().isEmpty()) {        return defaultValue;    }    return value;}
private static Integer nutch_f2155_0(Integer value, Integer defaultValue)
{    if (value == null) {        return defaultValue;    }    return value;}
private static IOException nutch_f2156_0(Exception e)
{    return new IOException(e);}
public Map<String, Object> nutch_f2157_0()
{    return headers;}
public void nutch_f2158_0(final Map<String, Object> headers)
{    this.headers = headers;}
public void nutch_f2159_0(final String headers)
{    this.headers = RabbitMQOptionParser.parseOptionAndConvertValue(headers);}
public void nutch_f2160_0(final String key, final Object value)
{    this.headers.put(key, value);}
public byte[] nutch_f2161_0()
{    return body;}
public void nutch_f2162_0(final byte[] body)
{    this.body = body;}
public String nutch_f2163_0()
{    return contentType;}
public void nutch_f2164_0(final String contentType)
{    this.contentType = contentType;}
 static Map<String, String> nutch_f2165_0(final String option)
{    Map<String, String> values = new HashMap<>();    if (option.isEmpty()) {        return values;    }    String[] split = option.split(",");    for (String s : split) {        String[] ss = s.split("=");        values.put(ss[0], ss[1]);    }    return values;}
 static Map<String, Object> nutch_f2166_0(final String option)
{    Map<String, Object> values = new HashMap<>();    if (option.isEmpty()) {        return values;    }    String[] split = option.split(",");    for (String s : split) {        String[] ss = s.split("=");        values.put(ss[0], convert(ss[1]));    }    return values;}
 static Map<String, Object> nutch_f2167_0(final String subOption)
{    Map<String, Object> values = new HashMap<>();    if (subOption.isEmpty()) {        return values;    }    String[] split = subOption.replaceAll("\\{|}", "").split(";");    for (String s : split) {        String[] ss = s.split(":");        values.put(ss[0], convert(ss[1]));    }    return values;}
private static Object nutch_f2168_0(String s)
{    try {        return Integer.parseInt(s);    } catch (Exception ex) {        }    try {        return Float.parseFloat(s);    } catch (Exception ex) {        }    if (s.equalsIgnoreCase("true") || s.equalsIgnoreCase("false")) {        return Boolean.parseBoolean(s);    }    return s;}
protected boolean nutch_f2169_0()
{    return sign;}
protected String nutch_f2170_0()
{    return hostOrDomain;}
protected String nutch_f2171_0()
{    return regex;}
public String nutch_f2172_1(String url)
{    String host = null;    String domain = null;    if (hasHostDomainRules) {        host = URLUtil.getHost(url);        try {            domain = URLUtil.getDomainName(url);        } catch (MalformedURLException e) {                }            }    for (RegexRule rule : rules) {                if (rule.hostOrDomain() != null && !rule.hostOrDomain().equals(host) && !rule.hostOrDomain().equals(domain)) {                        continue;        }                if (rule.match(url)) {            return rule.accept() ? url : null;        }    }    ;    return null;}
public void nutch_f2173_1(Configuration conf)
{    this.conf = conf;    Reader reader = null;    try {        reader = getRulesReader(conf);    } catch (Exception e) {        if (LOG.isErrorEnabled()) {                    }        throw new RuntimeException(e.getMessage(), e);    }    try {        rules = readRules(reader);    } catch (IOException e) {        if (LOG.isErrorEnabled()) {                    }        throw new RuntimeException(e.getMessage(), e);    }}
public Configuration nutch_f2174_0()
{    return this.conf;}
private List<RegexRule> nutch_f2175_0(Reader reader) throws IOException, IllegalArgumentException
{    BufferedReader in = new BufferedReader(reader);    List<RegexRule> rules = new ArrayList<RegexRule>();    String line;    String hostOrDomain = null;    while ((line = in.readLine()) != null) {        if (line.length() == 0) {            continue;        }        char first = line.charAt(0);        boolean sign = false;        switch(first) {            case '+':                sign = true;                break;            case '-':                sign = false;                break;            case ' ':            case '\n':            case             '#':                continue;            case '>':                hostOrDomain = line.substring(1).trim();                hasHostDomainRules = true;                continue;            case '<':                hostOrDomain = null;                continue;            default:                throw new IOException("Invalid first character: " + line);        }        String regex = line.substring(1);        if (LOG.isTraceEnabled()) {            LOG.trace("Adding rule [" + regex + "] for " + hostOrDomain);        }        RegexRule rule = createRule(sign, regex, hostOrDomain);        rules.add(rule);    }    return rules;}
public static void nutch_f2176_0(RegexURLFilterBase filter, String[] args) throws IOException, IllegalArgumentException
{    BufferedReader in = new BufferedReader(new InputStreamReader(System.in));    String line;    while ((line = in.readLine()) != null) {        String out = filter.filter(line);        if (out != null) {            System.out.print("+");            System.out.println(out);        } else {            System.out.print("-");            System.out.println(line);        }    }}
protected void nutch_f2177_0(int loops, String file)
{    try {        bench(loops, new FileReader(SAMPLES + SEPARATOR + file + ".rules"), new FileReader(SAMPLES + SEPARATOR + file + ".urls"));    } catch (Exception e) {        Assert.fail(e.toString());    }}
protected void nutch_f2178_1(int loops, Reader rules, Reader urls)
{    long start = System.currentTimeMillis();    try {        URLFilter filter = getURLFilter(rules);        FilteredURL[] expected = readURLFile(urls);        for (int i = 0; i < loops; i++) {            test(filter, expected);        }    } catch (Exception e) {        Assert.fail(e.toString());    }    }
protected void nutch_f2179_0(int loops, String rulesFile, String urlsFile)
{    try {        bench(loops, new FileReader(SAMPLES + SEPARATOR + rulesFile), new FileReader(SAMPLES + SEPARATOR + urlsFile));    } catch (Exception e) {        Assert.fail(e.toString());    }}
protected void nutch_f2180_0(String rulesFile, String urlsFile)
{    try {        test(new FileReader(SAMPLES + SEPARATOR + rulesFile), new FileReader(SAMPLES + SEPARATOR + urlsFile));    } catch (Exception e) {        Assert.fail(e.toString());    }}
protected void nutch_f2181_0(String file)
{    try {        test(new FileReader(SAMPLES + SEPARATOR + file + ".rules"), new FileReader(SAMPLES + SEPARATOR + file + ".urls"));    } catch (Exception e) {        Assert.fail(e.toString());    }}
protected void nutch_f2182_0(Reader rules, Reader urls)
{    try {        test(getURLFilter(rules), readURLFile(urls));    } catch (Exception e) {        Assert.fail(e.toString());    }}
protected void nutch_f2183_0(URLFilter filter, FilteredURL[] expected)
{    for (int i = 0; i < expected.length; i++) {        String result = filter.filter(expected[i].url);        if (result != null) {            Assert.assertTrue(expected[i].url, expected[i].sign);        } else {            Assert.assertFalse(expected[i].url, expected[i].sign);        }    }}
private static FilteredURL[] nutch_f2184_0(Reader reader) throws IOException
{    BufferedReader in = new BufferedReader(reader);    List<FilteredURL> list = new ArrayList<FilteredURL>();    String line;    while ((line = in.readLine()) != null) {        if (line.length() != 0) {            list.add(new FilteredURL(line));        }    }    return (FilteredURL[]) list.toArray(new FilteredURL[list.size()]);}
public static WebDriver nutch_f2185_1(String url, Configuration conf)
{    WebDriver driver = null;    long pageLoadWait = conf.getLong("page.load.delay", 3);    try {        String driverType = conf.get("selenium.driver", "firefox");        boolean enableHeadlessMode = conf.getBoolean("selenium.enable.headless", false);        switch(driverType) {            case "firefox":                String geckoDriverPath = conf.get("selenium.grid.binary", "/root/geckodriver");                driver = createFirefoxWebDriver(geckoDriverPath, enableHeadlessMode);                break;            case "chrome":                String chromeDriverPath = conf.get("selenium.grid.binary", "/root/chromedriver");                driver = createChromeWebDriver(chromeDriverPath, enableHeadlessMode);                break;                        case "remote":                String seleniumHubHost = conf.get("selenium.hub.host", "localhost");                int seleniumHubPort = Integer.parseInt(conf.get("selenium.hub.port", "4444"));                String seleniumHubPath = conf.get("selenium.hub.path", "/wd/hub");                String seleniumHubProtocol = conf.get("selenium.hub.protocol", "http");                URL seleniumHubUrl = new URL(seleniumHubProtocol, seleniumHubHost, seleniumHubPort, seleniumHubPath);                String seleniumGridDriver = conf.get("selenium.grid.driver", "firefox");                switch(seleniumGridDriver) {                    case "firefox":                        driver = createFirefoxRemoteWebDriver(seleniumHubUrl, enableHeadlessMode);                        break;                    case "chrome":                        driver = createChromeRemoteWebDriver(seleniumHubUrl, enableHeadlessMode);                        break;                    case "random":                        driver = createRandomRemoteWebDriver(seleniumHubUrl, enableHeadlessMode);                        break;                    default:                                                driver = createDefaultRemoteWebDriver(seleniumHubUrl, enableHeadlessMode);                        break;                }                break;            default:                                FirefoxOptions options = new FirefoxOptions();                driver = new FirefoxDriver(options);                break;        }                driver.manage().timeouts().pageLoadTimeout(pageLoadWait, TimeUnit.SECONDS);        driver.get(url);    } catch (Exception e) {        if (e instanceof TimeoutException) {                        return driver;        } else {                    }        cleanUpDriver(driver);        throw new RuntimeException(e);    }    return driver;}
public static WebDriver nutch_f2186_0(String firefoxDriverPath, boolean enableHeadlessMode)
{    System.setProperty("webdriver.gecko.driver", firefoxDriverPath);    FirefoxOptions firefoxOptions = new FirefoxOptions();    if (enableHeadlessMode) {        firefoxOptions.addArguments("--headless");    }    WebDriver driver = new FirefoxDriver(firefoxOptions);    return driver;}
public static WebDriver nutch_f2187_0(String chromeDriverPath, boolean enableHeadlessMode)
{        System.setProperty("webdriver.chrome.driver", chromeDriverPath);    ChromeOptions chromeOptions = new ChromeOptions();    chromeOptions.addArguments("--no-sandbox");    chromeOptions.addArguments("--disable-extensions");        if (enableHeadlessMode) {        chromeOptions.addArguments("--headless");    }    WebDriver driver = new ChromeDriver(chromeOptions);    return driver;}
public static WebDriver nutch_f2188_0(String operaDriverPath, boolean enableHeadlessMode)
{        System.setProperty("webdriver.opera.driver", operaDriverPath);    OperaOptions operaOptions = new OperaOptions();        operaOptions.addArguments("--no-sandbox");    operaOptions.addArguments("--disable-extensions");        if (enableHeadlessMode) {        operaOptions.addArguments("--headless");    }    WebDriver driver = new OperaDriver(operaOptions);    return driver;}
public static RemoteWebDriver nutch_f2189_0(URL seleniumHubUrl, boolean enableHeadlessMode)
{    FirefoxOptions firefoxOptions = new FirefoxOptions();    if (enableHeadlessMode) {        firefoxOptions.setHeadless(true);    }    RemoteWebDriver driver = new RemoteWebDriver(seleniumHubUrl, firefoxOptions);    return driver;}
public static RemoteWebDriver nutch_f2190_0(URL seleniumHubUrl, boolean enableHeadlessMode)
{    ChromeOptions chromeOptions = new ChromeOptions();    if (enableHeadlessMode) {        chromeOptions.setHeadless(true);    }    RemoteWebDriver driver = new RemoteWebDriver(seleniumHubUrl, chromeOptions);    return driver;}
public static RemoteWebDriver nutch_f2191_0(URL seleniumHubUrl, boolean enableHeadlessMode)
{                Random r = new Random();    int min = 0;                                int max = 1;    int num = r.nextInt((max - min) + 1) + min;    if (num == 0) {        return createFirefoxRemoteWebDriver(seleniumHubUrl, enableHeadlessMode);    }    return createChromeRemoteWebDriver(seleniumHubUrl, enableHeadlessMode);}
public static RemoteWebDriver nutch_f2192_0(URL seleniumHubUrl, boolean enableHeadlessMode)
{    return createFirefoxRemoteWebDriver(seleniumHubUrl, enableHeadlessMode);}
public static void nutch_f2193_1(WebDriver driver)
{    if (driver != null) {        try {                        driver.quit();            TemporaryFilesystem.getDefaultTmpFS().deleteTemporaryFiles();        } catch (Exception e) {                            }    }}
public static String nutch_f2194_1(String url, Configuration conf)
{    WebDriver driver = getDriverForPage(url, conf);    try {        if (conf.getBoolean("take.screenshot", false)) {            takeScreenshot(driver, conf);        }        String innerHtml = driver.findElement(By.tagName("body")).getAttribute("innerHTML");        return innerHtml;            } catch (Exception e) {        TemporaryFilesystem.getDefaultTmpFS().deleteTemporaryFiles();                        throw new RuntimeException(e);    } finally {        cleanUpDriver(driver);    }}
public static String nutch_f2195_0(String url)
{    return getHtmlPage(url, null);}
private static void nutch_f2196_1(WebDriver driver, Configuration conf)
{    try {        String url = driver.getCurrentUrl();        File srcFile = ((TakesScreenshot) driver).getScreenshotAs(OutputType.FILE);                FileSystem fs = FileSystem.get(conf);        if (conf.get("screenshot.location") != null) {            Path screenshotPath = new Path(conf.get("screenshot.location") + "/" + srcFile.getName());            OutputStream os = null;            if (!fs.exists(screenshotPath)) {                                os = fs.create(screenshotPath);            }            InputStream is = new BufferedInputStream(new FileInputStream(srcFile));            IOUtils.copyBytes(is, os, conf);                    } else {                    }    } catch (Exception e) {                cleanUpDriver(driver);        throw new RuntimeException(e);    }}
public NutchDocument nutch_f2197_0(NutchDocument doc, Parse parse, Text url, CrawlDatum datum, Inlinks inlinks) throws IndexingException
{        String[] tags = parse.getData().getParseMeta().getValues(RelTagParser.REL_TAG);    if (tags != null) {        for (int i = 0; i < tags.length; i++) {            doc.add("tag", tags[i]);        }    }    return doc;}
public void nutch_f2198_0(Configuration conf)
{    this.conf = conf;}
public Configuration nutch_f2199_0()
{    return this.conf;}
public ParseResult nutch_f2200_0(Content content, ParseResult parseResult, HTMLMetaTags metaTags, DocumentFragment doc)
{        Parse parse = parseResult.get(content.getUrl());        Parser parser = new Parser(doc);    Set<?> tags = parser.getRelTags();    Iterator<?> iter = tags.iterator();    Metadata metadata = parse.getData().getParseMeta();    while (iter.hasNext()) metadata.add(REL_TAG, (String) iter.next());    return parseResult;}
 Set<String> nutch_f2201_0()
{    return tags;}
 void nutch_f2202_1(Node node)
{    if (node.getNodeType() == Node.ELEMENT_NODE) {                if ("a".equalsIgnoreCase(node.getNodeName())) {            NamedNodeMap attrs = node.getAttributes();            Node hrefNode = attrs.getNamedItem("href");                        if (hrefNode != null) {                Node relNode = attrs.getNamedItem("rel");                                if (relNode != null) {                                        if ("tag".equalsIgnoreCase(relNode.getNodeValue())) {                        String tag = parseTag(hrefNode.getNodeValue());                        if (!StringUtil.isEmpty(tag)) {                            if (!tags.contains(tag)) {                                tags.add(tag);                                                            }                        }                    }                }            }        }    }        NodeList children = node.getChildNodes();    for (int i = 0; children != null && i < children.getLength(); i++) parse(children.item(i));}
private static final String nutch_f2203_0(String url)
{    String tag = null;    try {        URL u = new URL(url);        String path = u.getPath();        tag = URLDecoder.decode(path.substring(path.lastIndexOf('/') + 1), "UTF-8");    } catch (Exception e) {                tag = null;    }    return tag;}
public void nutch_f2204_0(Configuration conf)
{    this.conf = conf;}
public Configuration nutch_f2205_0()
{    return this.conf;}
public NutchDocument nutch_f2206_1(NutchDocument doc, Parse parse, Text url, CrawlDatum datum, Inlinks inlinks) throws IndexingException
{    String mimeType;    String contentType;    Writable tcontentType = datum.getMetaData().get(new Text(Response.CONTENT_TYPE));    if (tcontentType != null) {        contentType = tcontentType.toString();    } else {        contentType = parse.getData().getMeta(Response.CONTENT_TYPE);    }    if (contentType == null) {        mimeType = tika.detect(url.toString());    } else {        mimeType = MIME.forName(MimeUtil.cleanMimeType(contentType));    }    contentType = mimeType;    if (LOG.isInfoEnabled()) {            }    if (trie != null) {        if (trie.shortestMatch(contentType) == null) {                        if (acceptMode) {                return doc;            }            return null;        } else {                        if (acceptMode) {                return null;            }        }    }    return doc;}
public void nutch_f2207_1(Configuration conf)
{    this.conf = conf;    MIME = new MimeUtil(conf);        String file = conf.get(MIMEFILTER_REGEX_FILE, "");    if (file != null) {        if (file.isEmpty()) {                    } else {            Reader reader = conf.getConfResourceAsReader(file);            try {                readConfiguration(reader);            } catch (IOException e) {                if (LOG.isErrorEnabled()) {                                    }                throw new RuntimeException(e.getMessage(), e);            }        }    }}
private void nutch_f2208_0(Reader reader) throws IOException
{    BufferedReader in = new BufferedReader(reader);    String line;    List<String> rules = new ArrayList<String>();    while (null != (line = in.readLine())) {        if (line.length() == 0) {            continue;        }        char first = line.charAt(0);        switch(first) {            case ' ':            case '\n':            case             '#':                break;            case '+':                acceptMode = true;                break;            case '-':                acceptMode = false;                break;            default:                rules.add(line);                break;        }    }    trie = new PrefixStringMatcher(rules);}
public Configuration nutch_f2209_0()
{    return this.conf;}
public static void nutch_f2210_1(String[] args) throws IOException, IndexingException
{    Option helpOpt = new Option("h", "help", false, "show this help message");    @SuppressWarnings("static-access")    Option rulesOpt = OptionBuilder.withArgName("file").hasArg().withDescription("Rules file to be used in the tests relative to the conf directory").isRequired().create("rules");    Options options = new Options();    options.addOption(helpOpt).addOption(rulesOpt);    CommandLineParser parser = new GnuParser();    HelpFormatter formatter = new HelpFormatter();    String rulesFile;    try {        CommandLine line = parser.parse(options, args);        if (line.hasOption("help") || !line.hasOption("rules")) {            formatter.printHelp("org.apache.nutch.indexer.filter.MimeTypeIndexingFilter", options, true);            return;        }        rulesFile = line.getOptionValue("rules");    } catch (UnrecognizedOptionException e) {        formatter.printHelp("org.apache.nutch.indexer.filter.MimeTypeIndexingFilter", options, true);        return;    } catch (Exception e) {                e.printStackTrace();        return;    }    MimeTypeIndexingFilter filter = new MimeTypeIndexingFilter();    Configuration conf = NutchConfiguration.create();    conf.set(MimeTypeIndexingFilter.MIMEFILTER_REGEX_FILE, rulesFile);    filter.setConf(conf);    BufferedReader in = new BufferedReader(new InputStreamReader(System.in));    String line;    while ((line = in.readLine()) != null && !line.isEmpty()) {        Metadata metadata = new Metadata();        metadata.set(Response.CONTENT_TYPE, line);        ParseImpl parse = new ParseImpl("text", new ParseData(new ParseStatus(), "title", new Outlink[0], metadata));        NutchDocument doc = filter.filter(new NutchDocument(), parse, new Text("http://www.example.com/"), new CrawlDatum(), new Inlinks());        if (doc != null) {            System.out.print("+ ");            System.out.println(line);        } else {            System.out.print("- ");            System.out.println(line);        }    }}
public void nutch_f2211_0() throws Exception
{    for (int i = 0; i < MIME_TYPES.length; i++) {        Metadata metadata = new Metadata();        metadata.add(Response.CONTENT_TYPE, MIME_TYPES[i]);        ParseImpl parse = new ParseImpl("text", new ParseData(new ParseStatus(), "title", new Outlink[0], metadata));        parses[i] = parse;    }}
public void nutch_f2212_0() throws Exception
{    String file = conf.get(MimeTypeIndexingFilter.MIMEFILTER_REGEX_FILE, "");    Assert.assertEquals(String.format("Property %s must not be present in the the configuration file", MimeTypeIndexingFilter.MIMEFILTER_REGEX_FILE), "", file);    filter.setConf(conf);        for (int i = 0; i < parses.length; i++) {        NutchDocument doc = filter.filter(new NutchDocument(), parses[i], new Text("http://www.example.com/"), new CrawlDatum(), new Inlinks());        Assert.assertNotNull("All documents must be allowed by default", doc);    }}
public void nutch_f2213_0() throws Exception
{    conf.set(MimeTypeIndexingFilter.MIMEFILTER_REGEX_FILE, "allow-images.txt");    filter.setConf(conf);    for (int i = 0; i < parses.length; i++) {        NutchDocument doc = filter.filter(new NutchDocument(), parses[i], new Text("http://www.example.com/"), new CrawlDatum(), new Inlinks());        if (MIME_TYPES[i].contains("image")) {            Assert.assertNotNull("Allow only images", doc);        } else {            Assert.assertNull("Block everything else", doc);        }    }}
public void nutch_f2214_0() throws Exception
{    conf.set(MimeTypeIndexingFilter.MIMEFILTER_REGEX_FILE, "block-html.txt");    filter.setConf(conf);    for (int i = 0; i < parses.length; i++) {        NutchDocument doc = filter.filter(new NutchDocument(), parses[i], new Text("http://www.example.com/"), new CrawlDatum(), new Inlinks());        if (MIME_TYPES[i].contains("html")) {            Assert.assertNull("Block only HTML documents", doc);        } else {            Assert.assertNotNull("Allow everything else", doc);        }    }}
public ParseResult nutch_f2215_0(Content content)
{    String contentType = content.getContentType();    String[] params = (String[]) TYPE_PARAMS_MAP.get(contentType);    if (params == null)        return new ParseStatus(ParseStatus.FAILED, "No external command defined for contentType: " + contentType).getEmptyParseResult(content.getUrl(), getConf());    String command = params[0];    int timeout = Integer.parseInt(params[1]);    String encoding = params[2];    if (LOG.isTraceEnabled()) {        LOG.trace("Use " + command + " with timeout=" + timeout + "secs");    }    String text = null;    String title = null;    try {        byte[] raw = content.getContent();        String contentLength = content.getMetadata().get(Response.CONTENT_LENGTH);        if (contentLength != null && raw.length != Integer.parseInt(contentLength)) {            return new ParseStatus(ParseStatus.FAILED, ParseStatus.FAILED_TRUNCATED, "Content truncated at " + raw.length + " bytes. Parser can't handle incomplete " + contentType + " file.").getEmptyParseResult(content.getUrl(), getConf());        }        ByteArrayOutputStream os = new ByteArrayOutputStream(BUFFER_SIZE);        ByteArrayOutputStream es = new ByteArrayOutputStream(BUFFER_SIZE / 4);        CommandRunner cr = new CommandRunner();        cr.setCommand(command + " " + contentType);        cr.setInputStream(new ByteArrayInputStream(raw));        cr.setStdOutputStream(os);        cr.setStdErrorStream(es);        cr.setTimeout(timeout);        cr.evaluate();        if (cr.getExitValue() != 0)            return new ParseStatus(ParseStatus.FAILED, "External command " + command + " failed with error: " + es.toString()).getEmptyParseResult(content.getUrl(), getConf());        text = os.toString(encoding);    } catch (Exception e) {                return new ParseStatus(e).getEmptyParseResult(content.getUrl(), getConf());    }    if (text == null)        text = "";    if (title == null)        title = "";        Outlink[] outlinks = OutlinkExtractor.getOutlinks(text, getConf());    ParseData parseData = new ParseData(ParseStatus.STATUS_SUCCESS, title, outlinks, content.getMetadata());    return ParseResult.createParseResult(content.getUrl(), new ParseImpl(text, parseData));}
public void nutch_f2216_0(Configuration conf)
{    this.conf = conf;    Extension[] extensions = PluginRepository.get(conf).getExtensionPoint("org.apache.nutch.parse.Parser").getExtensions();    String contentType, command, timeoutString, encoding;    for (int i = 0; i < extensions.length; i++) {        Extension extension = extensions[i];                if (!extension.getDescriptor().getPluginId().equals("parse-ext"))            continue;        contentType = extension.getAttribute("contentType");        if (contentType == null || contentType.equals(""))            continue;        command = extension.getAttribute("command");        if (command == null || command.equals(""))            continue;                encoding = extension.getAttribute("encoding");        if (encoding == null)            encoding = Charset.defaultCharset().name();        timeoutString = extension.getAttribute("timeout");        if (timeoutString == null || timeoutString.equals(""))            timeoutString = "" + TIMEOUT_DEFAULT;        TYPE_PARAMS_MAP.put(contentType, new String[] { command, timeoutString, encoding });    }}
public Configuration nutch_f2217_0()
{    return this.conf;}
protected void nutch_f2218_0() throws ProtocolException, IOException
{            String path = System.getProperty("test.data");    if (path != null) {        File tempDir = new File(path);        if (!tempDir.exists())            tempDir.mkdir();        tempFile = File.createTempFile("nutch.test.plugin.ExtParser.", ".txt", tempDir);    } else {                tempFile = File.createTempFile("nutch.test.plugin.ExtParser.", ".txt");    }    urlString = tempFile.toURI().toURL().toString();    FileOutputStream fos = new FileOutputStream(tempFile);    fos.write(expectedText.getBytes());    fos.close();        Protocol protocol = new ProtocolFactory(NutchConfiguration.create()).getProtocol(urlString);    content = protocol.getProtocolOutput(new Text(urlString), new CrawlDatum()).getContent();    protocol = null;}
protected void nutch_f2219_0()
{        content = null;}
public void nutch_f2220_0() throws ParseException
{    String contentType;        if (!System.getProperty("os.name").equalsIgnoreCase("linux")) {        System.err.println("Current OS is " + System.getProperty("os.name") + ".");        System.err.println("No test is run on OS other than linux.");        return;    }    Configuration conf = NutchConfiguration.create();        for (int i = 0; i < 10; i++) {                contentType = "application/vnd.nutch.example.cat";        content.setContentType(contentType);        parse = new ParseUtil(conf).parseByExtensionId("parse-ext", content).get(content.getUrl());        Assert.assertEquals(expectedText, parse.getText());                contentType = "application/vnd.nutch.example.md5sum";        content.setContentType(contentType);        parse = new ParseUtil(conf).parseByExtensionId("parse-ext", content).get(content.getUrl());        Assert.assertTrue(parse.getText().startsWith(expectedMD5sum));    }}
public Node nutch_f2221_0()
{    return (null != m_docFrag) ? (Node) m_docFrag : (Node) m_doc;}
public Node nutch_f2222_0()
{    return m_currentNode;}
public java.io.Writer nutch_f2223_0()
{    return null;}
protected void nutch_f2224_0(Node newNode) throws org.xml.sax.SAXException
{    Node currentNode = m_currentNode;    if (null != currentNode) {        currentNode.appendChild(newNode);        } else if (null != m_docFrag) {        m_docFrag.appendChild(newNode);    } else {        boolean ok = true;        short type = newNode.getNodeType();        if (type == Node.TEXT_NODE) {            String data = newNode.getNodeValue();            if ((null != data) && (data.trim().length() > 0)) {                throw new org.xml.sax.SAXException("Warning: can't output text before document element!  Ignoring...");            }            ok = false;        } else if (type == Node.ELEMENT_NODE) {            if (m_doc.getDocumentElement() != null) {                throw new org.xml.sax.SAXException("Can't have more than one root on a DOM!");            }        }        if (ok)            m_doc.appendChild(newNode);    }}
public void nutch_f2225_0(Locator locator)
{}
public void nutch_f2226_0() throws org.xml.sax.SAXException
{}
public void nutch_f2227_0() throws org.xml.sax.SAXException
{}
public void nutch_f2228_0(String ns, String localName, String name, Attributes atts) throws org.xml.sax.SAXException
{    Element elem;        if ((null == ns) || (ns.length() == 0))        elem = m_doc.createElementNS(null, name);    else        elem = m_doc.createElementNS(ns, name);    append(elem);    try {        int nAtts = atts.getLength();        if (0 != nAtts) {            for (int i = 0; i < nAtts; i++) {                                if (atts.getType(i).equalsIgnoreCase("ID"))                    setIDAttribute(atts.getValue(i), elem);                String attrNS = atts.getURI(i);                if ("".equals(attrNS))                                        attrNS = null;                                                                String attrQName = atts.getQName(i);                                if (attrQName.startsWith("xmlns:"))                    attrNS = "http://www.w3.org/2000/xmlns/";                                elem.setAttributeNS(attrNS, attrQName, atts.getValue(i));            }        }                m_elemStack.push(elem);        m_currentNode = elem;        } catch (java.lang.Exception de) {                throw new org.xml.sax.SAXException(de);    }}
public void nutch_f2229_0(String ns, String localName, String name) throws org.xml.sax.SAXException
{    m_elemStack.pop();    m_currentNode = m_elemStack.isEmpty() ? null : (Node) m_elemStack.peek();}
public void nutch_f2230_0(String id, Element elem)
{}
public void nutch_f2231_0(char[] ch, int start, int length) throws org.xml.sax.SAXException
{    if (isOutsideDocElem() && XMLCharacterRecognizer.isWhiteSpace(ch, start, length))                return;    if (m_inCData) {        cdata(ch, start, length);        return;    }    String s = new String(ch, start, length);    Node childNode;    childNode = m_currentNode != null ? m_currentNode.getLastChild() : null;    if (childNode != null && childNode.getNodeType() == Node.TEXT_NODE) {        ((Text) childNode).appendData(s);    } else {        Text text = m_doc.createTextNode(s);        append(text);    }}
public void nutch_f2232_0(char[] ch, int start, int length) throws org.xml.sax.SAXException
{    if (isOutsideDocElem() && XMLCharacterRecognizer.isWhiteSpace(ch, start, length))                return;    String s = new String(ch, start, length);    append(m_doc.createProcessingInstruction("xslt-next-is-raw", "formatter-to-dom"));    append(m_doc.createTextNode(s));}
public void nutch_f2233_0(String name) throws org.xml.sax.SAXException
{}
public void nutch_f2234_0(String name) throws org.xml.sax.SAXException
{}
public void nutch_f2235_0(String name) throws org.xml.sax.SAXException
{    append(m_doc.createEntityReference(name));}
public void nutch_f2236_0(char[] ch, int start, int length) throws org.xml.sax.SAXException
{    if (isOutsideDocElem())                return;    String s = new String(ch, start, length);    append(m_doc.createTextNode(s));}
private boolean nutch_f2237_0()
{    return (null == m_docFrag) && m_elemStack.size() == 0 && (null == m_currentNode || m_currentNode.getNodeType() == Node.DOCUMENT_NODE);}
public void nutch_f2238_0(String target, String data) throws org.xml.sax.SAXException
{    append(m_doc.createProcessingInstruction(target, data));}
public void nutch_f2239_0(char[] ch, int start, int length) throws org.xml.sax.SAXException
{        if (ch == null || start < 0 || length >= (ch.length - start) || length < 0)        return;    append(m_doc.createComment(new String(ch, start, length)));}
public void nutch_f2240_0() throws org.xml.sax.SAXException
{    m_inCData = true;    append(m_doc.createCDATASection(""));}
public void nutch_f2241_0() throws org.xml.sax.SAXException
{    m_inCData = false;}
public void nutch_f2242_0(char[] ch, int start, int length) throws org.xml.sax.SAXException
{    if (isOutsideDocElem() && XMLCharacterRecognizer.isWhiteSpace(ch, start, length))                return;    String s = new String(ch, start, length);        Node n = m_currentNode.getLastChild();    if (n instanceof CDATASection)        ((CDATASection) n).appendData(s);    else if (n instanceof Comment)        ((Comment) n).appendData(s);}
public void nutch_f2243_0(String name, String publicId, String systemId) throws org.xml.sax.SAXException
{}
public void nutch_f2244_0() throws org.xml.sax.SAXException
{}
public void nutch_f2245_0(String prefix, String uri) throws org.xml.sax.SAXException
{/*     *      * if((null != m_currentNode) && (m_currentNode.getNodeType() ==     * Node.ELEMENT_NODE)) { String qname; if(((null != prefix) &&     * (prefix.length() == 0)) || (null == prefix)) qname = "xmlns"; else qname     * = "xmlns:"+prefix;     *      * Element elem = (Element)m_currentNode; String val =     * elem.getAttribute(qname);      * { elem.setAttributeNS("http://www.w3.org/XML/1998/namespace", qname,     * uri); } }     */}
public void nutch_f2246_0(String prefix) throws org.xml.sax.SAXException
{}
public void nutch_f2247_0(String name) throws org.xml.sax.SAXException
{}
public String nutch_f2248_0()
{    return "LP[el=" + elName + ",attr=" + attrName + ",len=" + childLen + "]";}
public void nutch_f2249_0(Configuration conf)
{        Collection<String> forceTags = new ArrayList<String>(1);    this.conf = conf;    linkParams.clear();    linkParams.put("a", new LinkParams("a", "href", 1));    linkParams.put("area", new LinkParams("area", "href", 0));    if (conf.getBoolean("parser.html.form.use_action", true)) {        linkParams.put("form", new LinkParams("form", "action", 1));        if (conf.get("parser.html.form.use_action") != null)            forceTags.add("form");    }    linkParams.put("frame", new LinkParams("frame", "src", 0));    linkParams.put("iframe", new LinkParams("iframe", "src", 0));    linkParams.put("script", new LinkParams("script", "src", 0));    linkParams.put("link", new LinkParams("link", "href", 0));    linkParams.put("img", new LinkParams("img", "src", 0));    linkParams.put("source", new LinkParams("source", "src", 0));        String[] ignoreTags = conf.getStrings("parser.html.outlinks.ignore_tags");    for (int i = 0; ignoreTags != null && i < ignoreTags.length; i++) {        if (!forceTags.contains(ignoreTags[i]))            linkParams.remove(ignoreTags[i]);    }        srcTagMetaName = this.conf.get("parser.html.outlinks.htmlnode_metadata_name");    keepNodenames = (srcTagMetaName != null && srcTagMetaName.length() > 0);    blockNodes = new HashSet<>(conf.getTrimmedStringCollection("parser.html.line.separators"));}
public boolean nutch_f2250_0(StringBuffer sb, Node node, boolean abortOnNestedAnchors)
{    if (getTextHelper(sb, node, abortOnNestedAnchors, 0)) {        return true;    }    return false;}
public void nutch_f2251_0(StringBuffer sb, Node node)
{    getText(sb, node, false);}
private boolean nutch_f2252_0(StringBuffer sb, Node node, boolean abortOnNestedAnchors, int anchorDepth)
{    boolean abort = false;    NodeWalker walker = new NodeWalker(node);    while (walker.hasNext()) {        Node currentNode = walker.nextNode();        String nodeName = currentNode.getNodeName();        short nodeType = currentNode.getNodeType();        Node previousSibling = currentNode.getPreviousSibling();        if (previousSibling != null && blockNodes.contains(previousSibling.getNodeName().toLowerCase())) {            appendParagraphSeparator(sb);        } else if (blockNodes.contains(nodeName.toLowerCase())) {            appendParagraphSeparator(sb);        }        if ("script".equalsIgnoreCase(nodeName)) {            walker.skipChildren();        }        if ("style".equalsIgnoreCase(nodeName)) {            walker.skipChildren();        }        if (abortOnNestedAnchors && "a".equalsIgnoreCase(nodeName)) {            anchorDepth++;            if (anchorDepth > 1) {                abort = true;                break;            }        }        if (nodeType == Node.COMMENT_NODE) {            walker.skipChildren();        }        if (nodeType == Node.TEXT_NODE) {                        String text = currentNode.getNodeValue();            text = text.replaceAll("\\s+", " ");            text = text.trim();            if (text.length() > 0) {                appendSpace(sb);                sb.append(text);            } else {                appendParagraphSeparator(sb);            }        }    }    return abort;}
private void nutch_f2253_0(StringBuffer buffer)
{    if (buffer.length() == 0) {        return;    }    char lastChar = buffer.charAt(buffer.length() - 1);    if ('\n' != lastChar) {                while (lastChar == ' ') {            buffer.deleteCharAt(buffer.length() - 1);            lastChar = buffer.charAt(buffer.length() - 1);        }        if ('\n' != lastChar) {            buffer.append('\n');        }    }}
private void nutch_f2254_0(StringBuffer buffer)
{    if (buffer.length() == 0) {        return;    }    char lastChar = buffer.charAt(buffer.length() - 1);    if (' ' != lastChar && '\n' != lastChar) {        buffer.append(' ');    }}
public boolean nutch_f2255_0(StringBuffer sb, Node node)
{    NodeWalker walker = new NodeWalker(node);    while (walker.hasNext()) {        Node currentNode = walker.nextNode();        String nodeName = currentNode.getNodeName();        short nodeType = currentNode.getNodeType();        if ("body".equalsIgnoreCase(nodeName)) {                        return false;        }        if (nodeType == Node.ELEMENT_NODE) {            if ("title".equalsIgnoreCase(nodeName)) {                getText(sb, currentNode);                return true;            }        }    }    return false;}
public String nutch_f2256_0(Node node)
{    NodeWalker walker = new NodeWalker(node);    while (walker.hasNext()) {        Node currentNode = walker.nextNode();        String nodeName = currentNode.getNodeName();        short nodeType = currentNode.getNodeType();                if (nodeType == Node.ELEMENT_NODE) {            if ("body".equalsIgnoreCase(nodeName)) {                                return null;            }            if ("base".equalsIgnoreCase(nodeName)) {                NamedNodeMap attrs = currentNode.getAttributes();                for (int i = 0; i < attrs.getLength(); i++) {                    Node attr = attrs.item(i);                    if ("href".equalsIgnoreCase(attr.getNodeName())) {                        return attr.getNodeValue();                    }                }            }        }    }        return null;}
private boolean nutch_f2257_0(Node node)
{    String val = node.getNodeValue();    for (int i = 0; i < val.length(); i++) {        if (!Character.isWhitespace(val.charAt(i)))            return false;    }    return true;}
private boolean nutch_f2258_0(Node node, NodeList children, int childLen, LinkParams params)
{    if (childLen == 0) {                if (params.childLen == 0)            return false;        else            return true;    } else if ((childLen == 1) && (children.item(0).getNodeType() == Node.ELEMENT_NODE) && (params.elName.equalsIgnoreCase(children.item(0).getNodeName()))) {                return true;    } else if (childLen == 2) {        Node c0 = children.item(0);        Node c1 = children.item(1);        if ((c0.getNodeType() == Node.ELEMENT_NODE) && (params.elName.equalsIgnoreCase(c0.getNodeName())) && (c1.getNodeType() == Node.TEXT_NODE) && hasOnlyWhiteSpace(c1)) {                        return true;        }        if ((c1.getNodeType() == Node.ELEMENT_NODE) && (params.elName.equalsIgnoreCase(c1.getNodeName())) && (c0.getNodeType() == Node.TEXT_NODE) && hasOnlyWhiteSpace(c0)) {                        return true;        }    } else if (childLen == 3) {        Node c0 = children.item(0);        Node c1 = children.item(1);        Node c2 = children.item(2);        if ((c1.getNodeType() == Node.ELEMENT_NODE) && (params.elName.equalsIgnoreCase(c1.getNodeName())) && (c0.getNodeType() == Node.TEXT_NODE) && (c2.getNodeType() == Node.TEXT_NODE) && hasOnlyWhiteSpace(c0) && hasOnlyWhiteSpace(c2)) {                        return true;        }    }    return false;}
public void nutch_f2259_0(URL base, ArrayList<Outlink> outlinks, Node node)
{    NodeWalker walker = new NodeWalker(node);    while (walker.hasNext()) {        Node currentNode = walker.nextNode();        String nodeName = currentNode.getNodeName();        short nodeType = currentNode.getNodeType();        NodeList children = currentNode.getChildNodes();        int childLen = (children != null) ? children.getLength() : 0;        if (nodeType == Node.ELEMENT_NODE) {            nodeName = nodeName.toLowerCase();            LinkParams params = (LinkParams) linkParams.get(nodeName);            if (params != null) {                if (!shouldThrowAwayLink(currentNode, children, childLen, params)) {                    StringBuffer linkText = new StringBuffer();                    getText(linkText, currentNode, true);                    if (linkText.toString().trim().length() == 0) {                                                NodeWalker subWalker = new NodeWalker(currentNode);                        while (subWalker.hasNext()) {                            Node subNode = subWalker.nextNode();                            if (subNode.getNodeType() == Node.ELEMENT_NODE) {                                if (subNode.getNodeName().toLowerCase().equals("img")) {                                    NamedNodeMap subAttrs = subNode.getAttributes();                                    Node alt = subAttrs.getNamedItem("alt");                                    if (alt != null) {                                        String altTxt = alt.getTextContent();                                        if (altTxt != null && altTxt.trim().length() > 0) {                                            if (linkText.length() > 0)                                                linkText.append(' ');                                            linkText.append(altTxt);                                        }                                    }                                } else {                                                                }                            } else if (subNode.getNodeType() == Node.TEXT_NODE) {                                String txt = subNode.getTextContent();                                if (txt != null && txt.length() > 0) {                                    if (linkText.length() > 0)                                        linkText.append(' ');                                    linkText.append(txt);                                }                            }                        }                    }                    NamedNodeMap attrs = currentNode.getAttributes();                    String target = null;                    boolean noFollow = false;                    boolean post = false;                    for (int i = 0; i < attrs.getLength(); i++) {                        Node attr = attrs.item(i);                        String attrName = attr.getNodeName();                        if (params.attrName.equalsIgnoreCase(attrName)) {                            target = attr.getNodeValue();                        } else if ("rel".equalsIgnoreCase(attrName) && "nofollow".equalsIgnoreCase(attr.getNodeValue())) {                            noFollow = true;                        } else if ("method".equalsIgnoreCase(attrName) && "post".equalsIgnoreCase(attr.getNodeValue())) {                            post = true;                        }                    }                    if (target != null && !noFollow && !post)                        try {                            URL url = URLUtil.resolveURL(base, target);                            Outlink outlink = new Outlink(url.toString(), linkText.toString().trim());                            outlinks.add(outlink);                                                        if (keepNodenames) {                                MapWritable metadata = new MapWritable();                                metadata.put(new Text(srcTagMetaName), new Text(nodeName));                                outlink.setMetadata(metadata);                            }                        } catch (MalformedURLException e) {                                                }                }                                if (params.childLen == 0)                    continue;            }        }    }}
public static final void nutch_f2260_0(HTMLMetaTags metaTags, Node node, URL currURL)
{    metaTags.reset();    getMetaTagsHelper(metaTags, node, currURL);}
private static final void nutch_f2261_0(HTMLMetaTags metaTags, Node node, URL currURL)
{    if (node.getNodeType() == Node.ELEMENT_NODE) {        if ("body".equalsIgnoreCase(node.getNodeName())) {                        return;        }        if ("meta".equalsIgnoreCase(node.getNodeName())) {            NamedNodeMap attrs = node.getAttributes();            Node nameNode = null;            Node equivNode = null;            Node contentNode = null;                        for (int i = 0; i < attrs.getLength(); i++) {                Node attr = attrs.item(i);                String attrName = attr.getNodeName().toLowerCase();                if (attrName.equals("name")) {                    nameNode = attr;                } else if (attrName.equals("http-equiv")) {                    equivNode = attr;                } else if (attrName.equals("content")) {                    contentNode = attr;                }            }            if (nameNode != null) {                if (contentNode != null) {                    String name = nameNode.getNodeValue().toLowerCase();                    metaTags.getGeneralTags().add(name, contentNode.getNodeValue());                    if ("robots".equals(name)) {                        String directives = contentNode.getNodeValue().toLowerCase();                        int index = directives.indexOf("none");                        if (index >= 0) {                            metaTags.setNoIndex();                            metaTags.setNoFollow();                        }                        index = directives.indexOf("all");                        if (index >= 0) {                                                }                        index = directives.indexOf("noindex");                        if (index >= 0) {                            metaTags.setNoIndex();                        }                        index = directives.indexOf("nofollow");                        if (index >= 0) {                            metaTags.setNoFollow();                        }                        index = directives.indexOf("noarchive");                        if (index >= 0) {                            metaTags.setNoCache();                        }                    }                                }            }            if (equivNode != null) {                if (contentNode != null) {                    String name = equivNode.getNodeValue().toLowerCase();                    String content = contentNode.getNodeValue();                    metaTags.getHttpEquivTags().setProperty(name, content);                    if ("pragma".equals(name)) {                        content = content.toLowerCase();                        int index = content.indexOf("no-cache");                        if (index >= 0)                            metaTags.setNoCache();                    } else if ("refresh".equals(name)) {                        int idx = content.indexOf(';');                        String time = null;                        if (idx == -1) {                                                        time = content;                        } else                            time = content.substring(0, idx);                        try {                            metaTags.setRefreshTime(Integer.parseInt(time));                                                        metaTags.setRefresh(true);                        } catch (Exception e) {                            ;                        }                        URL refreshUrl = null;                        if (metaTags.getRefresh() && idx != -1) {                                                        idx = content.toLowerCase().indexOf("url=");                            if (idx == -1) {                                                                                                idx = content.indexOf(';') + 1;                            } else                                idx += 4;                            if (idx != -1) {                                String url = content.substring(idx);                                try {                                    refreshUrl = new URL(url);                                } catch (Exception e) {                                                                        try {                                        refreshUrl = new URL(currURL, url);                                    } catch (Exception e1) {                                        refreshUrl = null;                                    }                                }                            }                        }                        if (metaTags.getRefresh()) {                            if (refreshUrl == null) {                                                                                                refreshUrl = currURL;                            }                            metaTags.setRefreshHref(refreshUrl);                        }                    }                }            }        } else if ("base".equalsIgnoreCase(node.getNodeName())) {            NamedNodeMap attrs = node.getAttributes();            Node hrefNode = attrs.getNamedItem("href");            if (hrefNode != null) {                String urlString = hrefNode.getNodeValue();                URL url = null;                try {                    if (currURL == null)                        url = new URL(urlString);                    else                        url = new URL(currURL, urlString);                } catch (Exception e) {                    ;                }                if (url != null)                    metaTags.setBaseHref(url);            }        }    }    NodeList children = node.getChildNodes();    if (children != null) {        int len = children.getLength();        for (int i = 0; i < len; i++) {            getMetaTagsHelper(metaTags, children.item(i), currURL);        }    }}
private static String nutch_f2262_0(byte[] content)
{    int length = content.length < CHUNK_SIZE ? content.length : CHUNK_SIZE;                    String str = new String(content, 0, length, StandardCharsets.US_ASCII);    Matcher metaMatcher = metaPattern.matcher(str);    String encoding = null;    if (metaMatcher.find()) {        Matcher charsetMatcher = charsetPattern.matcher(metaMatcher.group(1));        if (charsetMatcher.find())            encoding = charsetMatcher.group(1);    }    if (encoding == null) {                metaMatcher = charsetPatternHTML5.matcher(str);        if (metaMatcher.find()) {            encoding = metaMatcher.group(1);        }    }    if (encoding == null) {                if (content.length >= 3 && content[0] == (byte) 0xEF && content[1] == (byte) 0xBB && content[2] == (byte) 0xBF) {            encoding = "UTF-8";        } else if (content.length >= 2) {            if (content[0] == (byte) 0xFF && content[1] == (byte) 0xFE) {                encoding = "UTF-16LE";            } else if (content[0] == (byte) 0xFE && content[1] == (byte) 0xFF) {                encoding = "UTF-16BE";            }        }    }    return encoding;}
public ParseResult nutch_f2263_1(Content content)
{    HTMLMetaTags metaTags = new HTMLMetaTags();    URL base;    try {        base = new URL(content.getBaseUrl());    } catch (MalformedURLException e) {        return new ParseStatus(e).getEmptyParseResult(content.getUrl(), getConf());    }    String text = "";    String title = "";    Outlink[] outlinks = new Outlink[0];    Metadata metadata = new Metadata();        DocumentFragment root;    try {        byte[] contentInOctets = content.getContent();        InputSource input = new InputSource(new ByteArrayInputStream(contentInOctets));        EncodingDetector detector = new EncodingDetector(conf);        detector.autoDetectClues(content, true);        detector.addClue(sniffCharacterEncoding(contentInOctets), "sniffed");        String encoding = detector.guessEncoding(content, defaultCharEncoding);        metadata.set(Metadata.ORIGINAL_CHAR_ENCODING, encoding);        metadata.set(Metadata.CHAR_ENCODING_FOR_CONVERSION, encoding);        input.setEncoding(encoding);        if (LOG.isTraceEnabled()) {            LOG.trace("Parsing...");        }        root = parse(input);    } catch (IOException e) {        return new ParseStatus(e).getEmptyParseResult(content.getUrl(), getConf());    } catch (DOMException e) {        return new ParseStatus(e).getEmptyParseResult(content.getUrl(), getConf());    } catch (SAXException e) {        return new ParseStatus(e).getEmptyParseResult(content.getUrl(), getConf());    } catch (Exception e) {                return new ParseStatus(e).getEmptyParseResult(content.getUrl(), getConf());    }        HTMLMetaProcessor.getMetaTags(metaTags, root, base);        metadata.addAll(metaTags.getGeneralTags());    if (LOG.isTraceEnabled()) {        LOG.trace("Meta tags for " + base + ": " + metaTags.toString());    }        if (!metaTags.getNoIndex()) {                StringBuffer sb = new StringBuffer();        if (LOG.isTraceEnabled()) {            LOG.trace("Getting text...");        }                utils.getText(sb, root);        text = sb.toString();        sb.setLength(0);        if (LOG.isTraceEnabled()) {            LOG.trace("Getting title...");        }                utils.getTitle(sb, root);        title = sb.toString().trim();    }    if (!metaTags.getNoFollow()) {                        ArrayList<Outlink> l = new ArrayList<Outlink>();        URL baseTag = base;        String baseTagHref = utils.getBase(root);        if (baseTagHref != null) {            try {                baseTag = new URL(base, baseTagHref);            } catch (MalformedURLException e) {                baseTag = base;            }        }        if (LOG.isTraceEnabled()) {            LOG.trace("Getting links...");        }        utils.getOutlinks(baseTag, l, root);        outlinks = l.toArray(new Outlink[l.size()]);        if (LOG.isTraceEnabled()) {            LOG.trace("found " + outlinks.length + " outlinks in " + content.getUrl());        }    }    ParseStatus status = new ParseStatus(ParseStatus.SUCCESS);    if (metaTags.getRefresh()) {        status.setMinorCode(ParseStatus.SUCCESS_REDIRECT);        status.setArgs(new String[] { metaTags.getRefreshHref().toString(), Integer.toString(metaTags.getRefreshTime()) });    }    ParseData parseData = new ParseData(status, title, outlinks, content.getMetadata(), metadata);    ParseResult parseResult = ParseResult.createParseResult(content.getUrl(), new ParseImpl(text, parseData));        ParseResult filteredParse = this.htmlParseFilters.filter(content, parseResult, metaTags, root);    if (metaTags.getNoCache()) {                for (Map.Entry<org.apache.hadoop.io.Text, Parse> entry : filteredParse) entry.getValue().getData().getParseMeta().set(Nutch.CACHING_FORBIDDEN_KEY, cachingPolicy);    }    return filteredParse;}
private DocumentFragment nutch_f2264_0(InputSource input) throws Exception
{    if ("tagsoup".equalsIgnoreCase(parserImpl))        return parseTagSoup(input);    else        return parseNeko(input);}
private DocumentFragment nutch_f2265_0(InputSource input) throws Exception
{    HTMLDocumentImpl doc = new HTMLDocumentImpl();    DocumentFragment frag = doc.createDocumentFragment();    DOMBuilder builder = new DOMBuilder(doc, frag);    org.ccil.cowan.tagsoup.Parser reader = new org.ccil.cowan.tagsoup.Parser();    reader.setContentHandler(builder);    reader.setFeature(org.ccil.cowan.tagsoup.Parser.ignoreBogonsFeature, true);    reader.setFeature(org.ccil.cowan.tagsoup.Parser.bogonsEmptyFeature, false);    reader.setProperty("http://xml.org/sax/properties/lexical-handler", builder);    reader.parse(input);    return frag;}
private DocumentFragment nutch_f2266_1(InputSource input) throws Exception
{    DOMFragmentParser parser = new DOMFragmentParser();    try {        parser.setFeature("http://cyberneko.org/html/features/scanner/allow-selfclosing-iframe", true);        parser.setFeature("http://cyberneko.org/html/features/augmentations", true);        parser.setProperty("http://cyberneko.org/html/properties/default-encoding", defaultCharEncoding);        parser.setFeature("http://cyberneko.org/html/features/scanner/ignore-specified-charset", true);        parser.setFeature("http://cyberneko.org/html/features/balance-tags/ignore-outside-content", false);        parser.setFeature("http://cyberneko.org/html/features/balance-tags/document-fragment", true);        parser.setFeature("http://cyberneko.org/html/features/report-errors", LOG.isTraceEnabled());    } catch (SAXException e) {    }        HTMLDocumentImpl doc = new HTMLDocumentImpl();    doc.setErrorChecking(false);    DocumentFragment res = doc.createDocumentFragment();    DocumentFragment frag = doc.createDocumentFragment();    parser.parse(input, frag);    res.appendChild(frag);    try {        while (true) {            frag = doc.createDocumentFragment();            parser.parse(input, frag);            if (!frag.hasChildNodes())                break;            if (LOG.isInfoEnabled()) {                            }            res.appendChild(frag);        }    } catch (Exception e) {            }    ;    return res;}
public static void nutch_f2267_0(String[] args) throws Exception
{    String name = args[0];    String url = "file:" + name;    File file = new File(name);    byte[] bytes = new byte[(int) file.length()];    @SuppressWarnings("resource")    DataInputStream in = new DataInputStream(new FileInputStream(file));    in.readFully(bytes);    Configuration conf = NutchConfiguration.create();    HtmlParser parser = new HtmlParser();    parser.setConf(conf);    Parse parse = parser.getParse(new Content(url, url, bytes, "text/html", new Metadata(), conf)).get(url);    System.out.println("data: " + parse.getData());    System.out.println("text: " + parse.getText());}
public void nutch_f2268_0(Configuration conf)
{    this.conf = conf;    this.htmlParseFilters = new HtmlParseFilters(getConf());    this.parserImpl = getConf().get("parser.html.impl", "neko");    this.defaultCharEncoding = getConf().get("parser.character.encoding.default", "windows-1252");    this.utils = new DOMContentUtils(conf);    this.cachingPolicy = getConf().get("parser.caching.forbidden.policy", Nutch.CACHING_FORBIDDEN_CONTENT);}
public Configuration nutch_f2269_0()
{    return this.conf;}
public static boolean nutch_f2270_0(char ch)
{    return (ch == 0x20) || (ch == 0x09) || (ch == 0xD) || (ch == 0xA);}
public static boolean nutch_f2271_0(char[] ch, int start, int length)
{    int end = start + length;    for (int s = start; s < end; s++) {        if (!isWhiteSpace(ch[s]))            return false;    }    return true;}
public static boolean nutch_f2272_0(StringBuffer buf)
{    int n = buf.length();    for (int i = 0; i < n; i++) {        if (!isWhiteSpace(buf.charAt(i)))            return false;    }    return true;}
public static boolean nutch_f2273_0(String s)
{    if (null != s) {        int n = s.length();        for (int i = 0; i < n; i++) {            if (!isWhiteSpace(s.charAt(i)))                return false;        }    }    return true;}
public void nutch_f2274_0()
{    conf = NutchConfiguration.create();    conf.setBoolean("parser.html.form.use_action", true);    utils = new DOMContentUtils(conf);    DOMFragmentParser parser = new DOMFragmentParser();    try {        parser.setFeature("http://cyberneko.org/html/features/scanner/allow-selfclosing-iframe", true);    } catch (SAXException e) {    }    for (int i = 0; i < testPages.length; i++) {        DocumentFragment node = new HTMLDocumentImpl().createDocumentFragment();        try {            parser.parse(new InputSource(new ByteArrayInputStream(testPages[i].getBytes())), node);            testBaseHrefURLs[i] = new URL(testBaseHrefs[i]);        } catch (Exception e) {            Assert.assertTrue("caught exception: " + e, false);        }        testDOMs[i] = node;    }    try {        answerOutlinks = new Outlink[][] { { new Outlink("http://www.nutch.org", "anchor") }, { new Outlink("http://www.nutch.org/", "home"), new Outlink("http://www.nutch.org/docs/bot.html", "bots") }, { new Outlink("http://www.nutch.org/", "separate this"), new Outlink("http://www.nutch.org/docs/ok", "from this") }, { new Outlink("http://www.nutch.org/", "home"), new Outlink("http://www.nutch.org/docs/1", "1"), new Outlink("http://www.nutch.org/docs/2", "2") }, { new Outlink("http://www.nutch.org/frames/top.html", ""), new Outlink("http://www.nutch.org/frames/left.html", ""), new Outlink("http://www.nutch.org/frames/invalid.html", ""), new Outlink("http://www.nutch.org/frames/right.html", "") }, { new Outlink("http://www.nutch.org/maps/logo.gif", ""), new Outlink("http://www.nutch.org/index.html", ""), new Outlink("http://www.nutch.org/maps/#bottom", ""), new Outlink("http://www.nutch.org/bot.html", ""), new Outlink("http://www.nutch.org/docs/index.html", "") }, { new Outlink("http://www.nutch.org/index.html", "whitespace test") }, {}, { new Outlink("http://www.nutch.org/dummy.jsp", "test2") }, {}, { new Outlink("http://www.nutch.org/;x", "anchor1"), new Outlink("http://www.nutch.org/g;x", "anchor2"), new Outlink("http://www.nutch.org/g;x?y#s", "anchor3") }, {         new Outlink("http://www.nutch.org/g", "anchor1"), new Outlink("http://www.nutch.org/g?y#s", "anchor2"), new Outlink("http://www.nutch.org/;something?y=1", "anchor3"), new Outlink("http://www.nutch.org/;something?y=1#s", "anchor4"), new Outlink("http://www.nutch.org/;something?y=1;somethingelse", "anchor5") }, { new Outlink("http://www.nutch.org/g", ""), new Outlink("http://www.nutch.org/g1", ""), new Outlink("http://www.nutch.org/g2", "bla bla"), new Outlink("http://www.nutch.org/test.gif", "bla bla") }, { new Outlink("http://www.nutch.org/movie.mp4", "") } };    } catch (MalformedURLException e) {    }}
private static boolean nutch_f2275_0(String s1, String s2)
{    StringTokenizer st1 = new StringTokenizer(s1);    StringTokenizer st2 = new StringTokenizer(s2);    while (st1.hasMoreTokens()) {        if (!st2.hasMoreTokens())            return false;        if (!st1.nextToken().equals(st2.nextToken()))            return false;    }    if (st2.hasMoreTokens())        return false;    return true;}
public void nutch_f2276_0()
{    if (testDOMs[0] == null)        setup();    for (int i = 0; i < testPages.length; i++) {        StringBuffer sb = new StringBuffer();        utils.getText(sb, testDOMs[i]);        String text = sb.toString();        Assert.assertTrue("expecting text: " + answerText[i] + System.getProperty("line.separator") + System.getProperty("line.separator") + "got text: " + text, equalsIgnoreWhitespace(answerText[i], text));    }}
public void nutch_f2277_0()
{    if (testDOMs[0] == null)        setup();    for (int i = 0; i < testPages.length; i++) {        StringBuffer sb = new StringBuffer();        utils.getTitle(sb, testDOMs[i]);        String text = sb.toString();        Assert.assertTrue("expecting text: " + answerText[i] + System.getProperty("line.separator") + System.getProperty("line.separator") + "got text: " + text, equalsIgnoreWhitespace(answerTitle[i], text));    }}
public void nutch_f2278_0()
{    if (testDOMs[0] == null)        setup();    for (int i = 0; i < testPages.length; i++) {        ArrayList<Outlink> outlinks = new ArrayList<Outlink>();        if (i == SKIP) {            conf.setBoolean("parser.html.form.use_action", false);            utils.setConf(conf);        } else {            conf.setBoolean("parser.html.form.use_action", true);            utils.setConf(conf);        }        utils.getOutlinks(testBaseHrefURLs[i], outlinks, testDOMs[i]);        Outlink[] outlinkArr = new Outlink[outlinks.size()];        outlinkArr = (Outlink[]) outlinks.toArray(outlinkArr);        compareOutlinks(answerOutlinks[i], outlinkArr);    }}
private static final void nutch_f2279_0(StringBuffer sb, Outlink[] o)
{    for (int i = 0; i < o.length; i++) {        sb.append(o[i].toString());        sb.append(System.getProperty("line.separator"));    }}
private static final String nutch_f2280_0(Outlink[] o)
{    StringBuffer sb = new StringBuffer();    appendOutlinks(sb, o);    return sb.toString();}
private static final void nutch_f2281_0(Outlink[] o1, Outlink[] o2)
{    if (o1.length != o2.length) {        Assert.assertTrue("got wrong number of outlinks (expecting " + o1.length + ", got " + o2.length + ")" + System.getProperty("line.separator") + "answer: " + System.getProperty("line.separator") + outlinksString(o1) + System.getProperty("line.separator") + "got: " + System.getProperty("line.separator") + outlinksString(o2) + System.getProperty("line.separator"), false);    }    for (int i = 0; i < o1.length; i++) {        if (!o1[i].equals(o2[i])) {            Assert.assertTrue("got wrong outlinks at position " + i + System.getProperty("line.separator") + "answer: " + System.getProperty("line.separator") + "'" + o1[i].getToUrl() + "', anchor: '" + o1[i].getAnchor() + "'" + System.getProperty("line.separator") + "got: " + System.getProperty("line.separator") + "'" + o2[i].getToUrl() + "', anchor: '" + o2[i].getAnchor() + "'", false);        }    }}
protected Parse nutch_f2282_0(byte[] contentBytes)
{    String dummyUrl = "http://example.com/";    return parser.getParse(new Content(dummyUrl, dummyUrl, contentBytes, "text/html", new Metadata(), conf)).get(dummyUrl);}
public void nutch_f2283_1()
{    for (String[] testPage : encodingTestPages) {        String name = testPage[0];        Charset charset = Charset.forName(testPage[1]);        byte[] contentBytes = testPage[2].getBytes(charset);        Parse parse = parse(contentBytes);        String text = parse.getText();        String title = parse.getData().getTitle();        String keywords = parse.getData().getMeta("keywords");                                        Assert.assertEquals("Title not extracted properly (" + name + ")", encodingTestKeywords, title);        for (String keyword : encodingTestKeywords.split(",\\s*")) {            Assert.assertTrue(keyword + " not found in text (" + name + ")", text.contains(keyword));        }        Assert.assertNotNull("No keywords extracted", keywords);        Assert.assertEquals("Keywords not extracted properly (" + name + ")", encodingTestKeywords, keywords);    }}
public void nutch_f2284_1()
{    byte[] contentBytes = resolveBaseUrlTestContent.getBytes(StandardCharsets.UTF_8);        Parse parse = parse(contentBytes);        Outlink[] outlinks = parse.getData().getOutlinks();    Assert.assertEquals(1, outlinks.length);    Assert.assertEquals("http://www.example.com/index.html", outlinks[0].getToUrl());}
public void nutch_f2285_0()
{    DOMFragmentParser parser = new DOMFragmentParser();    ;    try {        currURLsAndAnswers = new URL[][] { { new URL("http://www.nutch.org"), null }, { new URL("http://www.nutch.org"), null }, { new URL("http://www.nutch.org"), null }, { new URL("http://www.nutch.org"), null }, { new URL("http://www.nutch.org"), null }, { new URL("http://www.nutch.org"), null }, { new URL("http://www.nutch.org"), null }, { new URL("http://www.nutch.org/foo/"), new URL("http://www.nutch.org/") }, { new URL("http://www.nutch.org"), new URL("http://www.nutch.org/base/") } };    } catch (Exception e) {        Assert.assertTrue("couldn't make test URLs!", false);    }    for (int i = 0; i < tests.length; i++) {        byte[] bytes = tests[i].getBytes();        DocumentFragment node = new HTMLDocumentImpl().createDocumentFragment();        try {            parser.parse(new InputSource(new ByteArrayInputStream(bytes)), node);        } catch (Exception e) {            e.printStackTrace();        }        HTMLMetaTags robotsMeta = new HTMLMetaTags();        HTMLMetaProcessor.getMetaTags(robotsMeta, node, currURLsAndAnswers[i][0]);        Assert.assertTrue("got index wrong on test " + i, robotsMeta.getNoIndex() == answers[i][0]);        Assert.assertTrue("got follow wrong on test " + i, robotsMeta.getNoFollow() == answers[i][1]);        Assert.assertTrue("got cache wrong on test " + i, robotsMeta.getNoCache() == answers[i][2]);        Assert.assertTrue("got base href wrong on test " + i + " (got " + robotsMeta.getBaseHref() + ")", ((robotsMeta.getBaseHref() == null) && (currURLsAndAnswers[i][1] == null)) || ((robotsMeta.getBaseHref() != null) && robotsMeta.getBaseHref().equals(currURLsAndAnswers[i][1])));    }}
public ParseResult nutch_f2286_0(Content content, ParseResult parseResult, HTMLMetaTags metaTags, DocumentFragment doc)
{    Parse parse = parseResult.get(content.getUrl());    String url = content.getBaseUrl();    ArrayList<Outlink> outlinks = new ArrayList<Outlink>();    walk(doc, parse, metaTags, url, outlinks);    if (outlinks.size() > 0) {        Outlink[] old = parse.getData().getOutlinks();        String title = parse.getData().getTitle();        List<Outlink> list = Arrays.asList(old);        outlinks.addAll(list);        ParseStatus status = parse.getData().getStatus();        String text = parse.getText();        Outlink[] newlinks = (Outlink[]) outlinks.toArray(new Outlink[outlinks.size()]);        ParseData parseData = new ParseData(status, title, newlinks, parse.getData().getContentMeta(), parse.getData().getParseMeta());                parseResult.put(content.getUrl(), new ParseText(text), parseData);    }    return parseResult;}
private void nutch_f2287_0(Node n, Parse parse, HTMLMetaTags metaTags, String base, List<Outlink> outlinks)
{    if (n instanceof Element) {        String name = n.getNodeName();        if (name.equalsIgnoreCase("script")) {            /*         * String lang = null; Node lNode =         * n.getAttributes().getNamedItem("language"); if (lNode == null) lang =         * "javascript"; else lang = lNode.getNodeValue();         */            StringBuffer script = new StringBuffer();            NodeList nn = n.getChildNodes();            if (nn.getLength() > 0) {                for (int i = 0; i < nn.getLength(); i++) {                    if (i > 0)                        script.append('\n');                    script.append(nn.item(i).getNodeValue());                }                                                                                Outlink[] links = getJSLinks(script.toString(), "", base);                if (links != null && links.length > 0)                    outlinks.addAll(Arrays.asList(links));                                return;            }        } else {                        NamedNodeMap attrs = n.getAttributes();            int len = attrs.getLength();            for (int i = 0; i < len; i++) {                                                                                                Node anode = attrs.item(i);                Outlink[] links = null;                if (anode.getNodeName().startsWith("on")) {                    links = getJSLinks(anode.getNodeValue(), "", base);                } else if (anode.getNodeName().equalsIgnoreCase("href")) {                    String val = anode.getNodeValue();                    if (val != null && val.toLowerCase().indexOf("javascript:") != -1) {                        links = getJSLinks(val, "", base);                    }                }                if (links != null && links.length > 0)                    outlinks.addAll(Arrays.asList(links));            }        }    }    NodeList nl = n.getChildNodes();    for (int i = 0; i < nl.getLength(); i++) {        walk(nl.item(i), parse, metaTags, base, outlinks);    }}
public ParseResult nutch_f2288_0(Content c)
{    String script = new String(c.getContent());    Outlink[] outlinks = getJSLinks(script, "", c.getUrl());    if (outlinks == null)        outlinks = new Outlink[0];        String title;    int idx = script.indexOf('\n');    if (idx != -1) {        if (idx > MAX_TITLE_LEN)            idx = MAX_TITLE_LEN;        title = script.substring(0, idx);    } else {        idx = Math.min(MAX_TITLE_LEN, script.length());        title = script.substring(0, idx);    }    ParseData pd = new ParseData(ParseStatus.STATUS_SUCCESS, title, outlinks, c.getMetadata());    return ParseResult.createParseResult(c.getUrl(), new ParseImpl(script, pd));}
private Outlink[] nutch_f2289_1(String plainText, String anchor, String base)
{    final List<Outlink> outlinks = new ArrayList<Outlink>();    URL baseURL = null;    try {        baseURL = new URL(base);    } catch (Exception e) {        if (LOG.isErrorEnabled()) {                    }    }    try {        Matcher matcher = STRING_PATTERN.matcher(plainText);        String url;        while (matcher.find()) {            url = matcher.group(2);            Matcher matcherUri = URI_PATTERN.matcher(url);            if (!matcherUri.matches()) {                continue;            }            if (url.startsWith("www.")) {                url = "http://" + url;            } else {                                try {                    url = new URL(baseURL, url).toString();                } catch (MalformedURLException ex) {                    if (LOG.isTraceEnabled()) {                        LOG.trace(" - failed URL parse '" + url + "' and baseURL '" + baseURL + "'", ex);                    }                    continue;                }            }            url = url.replaceAll("&amp;", "&");            if (LOG.isTraceEnabled()) {                LOG.trace(" - outlink from JS: '" + url + "'");            }            outlinks.add(new Outlink(url, anchor));        }    } catch (Exception ex) {                if (LOG.isErrorEnabled()) {                    }    }    final Outlink[] retval;        if (outlinks != null && outlinks.size() > 0) {        retval = outlinks.toArray(new Outlink[0]);    } else {        retval = new Outlink[0];    }    return retval;}
public static void nutch_f2290_0(String[] args) throws Exception
{    if (args.length < 2) {        System.err.println(JSParseFilter.class.getName() + " file.js baseURL");        return;    }    InputStream in = new FileInputStream(args[0]);    BufferedReader br = new BufferedReader(new InputStreamReader(in, "UTF-8"));    StringBuffer sb = new StringBuffer();    String line = null;    while ((line = br.readLine()) != null) sb.append(line + "\n");    br.close();    JSParseFilter parseFilter = new JSParseFilter();    parseFilter.setConf(NutchConfiguration.create());    Outlink[] links = parseFilter.getJSLinks(sb.toString(), "", args[1]);    System.out.println("Outlinks extracted: " + links.length);    for (int i = 0; i < links.length; i++) System.out.println(" - " + links[i]);}
public void nutch_f2291_0(Configuration conf)
{    this.conf = conf;}
public Configuration nutch_f2292_0()
{    return this.conf;}
public void nutch_f2293_0()
{    conf = NutchConfiguration.create();    conf.set("file.content.limit", "-1");    conf.set("plugin.includes", "protocol-file|parse-(html|js)");}
public Outlink[] nutch_f2294_1(String sampleFile) throws ProtocolException, ParseException, IOException
{    String urlString;    Parse parse;    urlString = "file:" + sampleDir + fileSeparator + sampleFile;        Protocol protocol = new ProtocolFactory(conf).getProtocol(urlString);    Content content = protocol.getProtocolOutput(new Text(urlString), new CrawlDatum()).getContent();    parse = new ParseUtil(conf).parse(content).get(content.getUrl());        return parse.getData().getOutlinks();}
public void nutch_f2295_0() throws ProtocolException, ParseException, IOException
{    String[] filenames = new File(sampleDir).list();    for (int i = 0; i < filenames.length; i++) {        Outlink[] outlinks = getOutlinks(filenames[i]);        if (filenames[i].endsWith("parse_pure_js_test.js")) {            assertEquals("number of outlinks in .js test file should be X", 2, outlinks.length);            assertEquals("http://search.lucidimagination.com/p:nutch", outlinks[0].getToUrl());            assertEquals("http://search-lucene.com/nutch", outlinks[1].getToUrl());        } else {            assertTrue("number of outlinks in .html file should be at least 2", outlinks.length >= 2);            Set<String> outlinkSet = new TreeSet<>();            for (Outlink o : outlinks) {                outlinkSet.add(o.getToUrl());            }            assertTrue("http://search.lucidimagination.com/p:nutch not in outlinks", outlinkSet.contains("http://search.lucidimagination.com/p:nutch"));            assertTrue("http://search-lucene.com/nutch not in outlinks", outlinkSet.contains("http://search-lucene.com/nutch"));        }    }}
public void nutch_f2296_0(Configuration conf)
{    this.conf = conf;            String[] values = conf.getStrings("metatags.names", "*");    for (String val : values) {        metatagset.add(val.toLowerCase(Locale.ROOT));    }}
public Configuration nutch_f2297_0()
{    return this.conf;}
private void nutch_f2298_1(Metadata metadata, String metatag, String value)
{    String lcMetatag = metatag.toLowerCase(Locale.ROOT);    if (metatagset.contains("*") || metatagset.contains(lcMetatag)) {        if (LOG.isDebugEnabled()) {                    }        metadata.add("metatag." + lcMetatag, value);    }}
private void nutch_f2299_1(Metadata metadata, String metatag, String[] values)
{    String lcMetatag = metatag.toLowerCase(Locale.ROOT);    if (metatagset.contains("*") || metatagset.contains(lcMetatag)) {        for (String value : values) {            if (LOG.isDebugEnabled()) {                            }            metadata.add("metatag." + lcMetatag, value);        }    }}
public ParseResult nutch_f2300_0(Content content, ParseResult parseResult, HTMLMetaTags metaTags, DocumentFragment doc)
{    Parse parse = parseResult.get(content.getUrl());    Metadata metadata = parse.getData().getParseMeta();        for (String mdName : metadata.names()) {        addIndexedMetatags(metadata, mdName, metadata.getValues(mdName));    }    Metadata generalMetaTags = metaTags.getGeneralTags();    for (String tagName : generalMetaTags.names()) {        addIndexedMetatags(metadata, tagName, generalMetaTags.getValues(tagName));    }    Properties httpequiv = metaTags.getHttpEquivTags();    for (Enumeration<?> tagNames = httpequiv.propertyNames(); tagNames.hasMoreElements(); ) {        String name = (String) tagNames.nextElement();        String value = httpequiv.getProperty(name);        addIndexedMetatags(metadata, name, value);    }    return parseResult;}
public Metadata nutch_f2301_0(String fileName, Configuration conf)
{    Metadata metadata = null;    try {        String urlString = "file:" + sampleDir + fileSeparator + fileName;        Protocol protocol = new ProtocolFactory(conf).getProtocol(urlString);        Content content = protocol.getProtocolOutput(new Text(urlString), new CrawlDatum()).getContent();        Parse parse = new ParseUtil(conf).parse(content).get(content.getUrl());        metadata = parse.getData().getParseMeta();    } catch (Exception e) {        e.printStackTrace();        Assert.fail(e.toString());    }    return metadata;}
public void nutch_f2302_0()
{    Configuration conf = NutchConfiguration.create();        Metadata parseMeta = parseMeta(sampleFile, conf);    Assert.assertEquals(description, parseMeta.get("metatag.description"));    Assert.assertEquals(keywords, parseMeta.get("metatag.keywords"));}
public void nutch_f2303_0()
{    Configuration conf = NutchConfiguration.create();    conf.set("metatags.names", "keywords,DC.creator");    conf.set("index.parse.md", "metatag.keywords,metatag.dc.creator");    Metadata parseMeta = parseMeta(sampleFileMultival, conf);    String failMessage = "One value of metatag with multiple values is missing: ";    Set<String> valueSet = new TreeSet<String>();    for (String val : parseMeta.getValues("metatag.dc.creator")) {        valueSet.add(val);    }    String[] expectedValues1 = { "Doug Cutting", "Michael Cafarella" };    for (String val : expectedValues1) {        Assert.assertTrue(failMessage + val, valueSet.contains(val));    }    valueSet.clear();    for (String val : parseMeta.getValues("metatag.keywords")) {        valueSet.add(val);    }    String[] expectedValues2 = { "robot d'indexation", "web crawler", "Webcrawler" };    for (String val : expectedValues2) {        Assert.assertTrue(failMessage + val, valueSet.contains(val));    }}
public void nutch_f2304_0(Configuration conf)
{    this.conf = conf;}
public Configuration nutch_f2305_0()
{    return conf;}
public ParseResult nutch_f2306_1(Content content)
{    String text = null;    Vector<Outlink> outlinks = new Vector<>();    try {        byte[] raw = content.getContent();        String contentLength = content.getMetadata().get(Response.CONTENT_LENGTH);        if (contentLength != null && raw.length != Integer.parseInt(contentLength)) {            return new ParseStatus(ParseStatus.FAILED, ParseStatus.FAILED_TRUNCATED, "Content truncated at " + raw.length + " bytes. Parser can't handle incomplete files.").getEmptyParseResult(content.getUrl(), getConf());        }        ExtractText extractor = new ExtractText();                TagParser parser = new TagParser(extractor);                                SWFReader reader = new SWFReader(parser, new InStream(raw));                reader.readFile();        text = extractor.getText();        String atext = extractor.getActionText();        if (atext != null && atext.length() > 0)            text += "\n--------\n" + atext;                String[] links = extractor.getUrls();        for (int i = 0; i < links.length; i++) {            Outlink out = new Outlink(links[i], "");            outlinks.add(out);        }        Outlink[] olinks = OutlinkExtractor.getOutlinks(text, conf);        if (olinks != null)            for (int i = 0; i < olinks.length; i++) {                outlinks.add(olinks[i]);            }    } catch (Exception e) {                        return new ParseStatus(ParseStatus.FAILED, "Can't be handled as SWF document. " + e).getEmptyParseResult(content.getUrl(), getConf());    }    if (text == null)        text = "";    Outlink[] links = (Outlink[]) outlinks.toArray(new Outlink[outlinks.size()]);    ParseData parseData = new ParseData(ParseStatus.STATUS_SUCCESS, "", links, content.getMetadata());    return ParseResult.createParseResult(content.getUrl(), new ParseImpl(text, parseData));}
public static void nutch_f2307_0(String[] args) throws IOException
{    FileInputStream in = new FileInputStream(args[0]);    byte[] buf = new byte[in.available()];    in.read(buf);    in.close();    SWFParser parser = new SWFParser();    ParseResult parseResult = parser.getParse(new Content("file:" + args[0], "file:" + args[0], buf, "application/x-shockwave-flash", new Metadata(), NutchConfiguration.create()));    Parse p = parseResult.get("file:" + args[0]);    System.out.println("Parse Text:");    System.out.println(p.getText());    System.out.println("Parse Data:");    System.out.println(p.getData());}
public String nutch_f2308_0()
{    StringBuffer res = new StringBuffer();    Iterator<String> it = strings.iterator();    while (it.hasNext()) {        if (res.length() > 0)            res.append(' ');        res.append(it.next());    }    return res.toString();}
public String nutch_f2309_0()
{    StringBuffer res = new StringBuffer();    String[] strings = (String[]) actionStrings.toArray(new String[actionStrings.size()]);    Arrays.sort(strings);    for (int i = 0; i < strings.length; i++) {        if (i > 0)            res.append('\n');        res.append(strings[i]);    }    return res.toString();}
public String[] nutch_f2310_0()
{    String[] res = new String[urls.size()];    int i = 0;    Iterator<String> it = urls.iterator();    while (it.hasNext()) {        res[i] = it.next();        i++;    }    return res;}
public void nutch_f2311_0(int arg0, String arg1, int arg2, int[] arg3, int arg4) throws IOException
{    tagDefineFontInfo(arg0, arg1, arg2, arg3);}
public void nutch_f2312_0(int fontId, String fontName, int flags, int[] codes) throws IOException
{            fontCodes.put(Integer.valueOf(fontId), codes);}
public SWFVectors nutch_f2313_0(int id, int flags, String name, int numGlyphs, int ascent, int descent, int leading, int[] codes, int[] advances, Rect[] bounds, int[] kernCodes1, int[] kernCodes2, int[] kernAdjustments) throws IOException
{    fontCodes.put(Integer.valueOf(id), (codes != null) ? codes : new int[0]);    return null;}
public void nutch_f2314_0(int fieldId, String fieldName, String initialText, Rect boundary, int flags, AlphaColor textColor, int alignment, int fontId, int fontSize, int charLimit, int leftMargin, int rightMargin, int indentation, int lineSpacing) throws IOException
{    if (initialText != null) {        strings.add(initialText);    }}
public SWFText nutch_f2315_0(int id, Rect bounds, Matrix matrix) throws IOException
{    lastBounds = curBounds;    curBounds = bounds;    return new TextDumper();}
public SWFText nutch_f2316_0(int id, Rect bounds, Matrix matrix) throws IOException
{    lastBounds = curBounds;    curBounds = bounds;    return new TextDumper();}
public void nutch_f2317_0(int fontId, int textHeight)
{    this.fontId = fontId;}
public void nutch_f2318_0(int y)
{    if (firstY)        firstY = false;    else                strings.add("\n");}
public void nutch_f2319_0(int[] glyphIndices, int[] glyphAdvances)
{    int[] codes = (int[]) fontCodes.get(fontId);    if (codes == null) {                strings.add("\n**** ?????????????? ****\n");        return;    }        char[] chars = new char[glyphIndices.length];    for (int i = 0; i < chars.length; i++) {        int index = glyphIndices[i];        if (        index >= codes.length) {            chars[i] = (char) index;        } else {            chars[i] = (char) (codes[index]);        }    }    strings.add(new String(chars));}
public void nutch_f2320_0(Color color)
{}
public void nutch_f2321_0(int x)
{}
public void nutch_f2322_0()
{    strings.add("\n");}
public SWFActions nutch_f2323_0() throws IOException
{    return new NutchSWFActions(actionStrings, urls);}
public SWFActions nutch_f2324_0(int arg0) throws IOException
{    return new NutchSWFActions(actionStrings, urls);}
public void nutch_f2325_0(String[] values) throws IOException
{    for (int i = 0; i < values.length; i++) {        if (!strings.contains(values[i]))            strings.add(values[i]);    }    super.lookupTable(values);    dict = values;}
public void nutch_f2326_0() throws IOException
{    stack.pop();    super.defineLocal();}
public void nutch_f2327_0(int vars, int mode)
{}
public void nutch_f2328_0(String url, String target) throws IOException
{    stack.push(url);    stack.push(target);    strings.remove(url);    strings.remove(target);    urls.add(url);    super.getURL(url, target);}
public SWFActionBlock.TryCatchFinally nutch_f2329_0(String var) throws IOException
{    strings.remove(var);    return super._try(var);}
public void nutch_f2330_0(String var) throws IOException
{    strings.remove(var);    super.comment(var);}
public void nutch_f2331_0(String var) throws IOException
{    stack.push(var);    strings.remove(var);    super.gotoFrame(var);}
public void nutch_f2332_0(String var) throws IOException
{    strings.remove(var);    super.ifJump(var);}
public void nutch_f2333_0(String var) throws IOException
{    strings.remove(var);    super.jump(var);}
public void nutch_f2334_0(String var) throws IOException
{    strings.remove(var);    super.jumpLabel(var);}
public void nutch_f2335_0(int var) throws IOException
{    if (dict != null && var >= 0 && var < dict.length) {        stack.push(dict[var]);    }    super.lookup(var);}
public void nutch_f2336_0(String var) throws IOException
{    stack.push(var);    strings.remove(var);    super.push(var);}
public void nutch_f2337_0(String var) throws IOException
{    stack.push(var);    strings.remove(var);    super.setTarget(var);}
public SWFActionBlock nutch_f2338_0(String var, String[] params) throws IOException
{    stack.push(var);    strings.remove(var);    if (params != null) {        for (int i = 0; i < params.length; i++) {            strings.remove(params[i]);        }    }    return this;}
public SWFActionBlock nutch_f2339_0(String var, int arg1, int arg2, String[] params, int[] arg3) throws IOException
{    stack.push(var);    strings.remove(var);    if (params != null) {        for (int i = 0; i < params.length; i++) {            strings.remove(params[i]);        }    }    return this;}
public void nutch_f2340_0(int num, String var) throws IOException
{    stack.push(var);    strings.remove(var);    super.waitForFrame(num, var);}
public void nutch_f2341_0(String var) throws IOException
{    stack.push(var);    strings.remove(var);    super.waitForFrame(var);}
public void nutch_f2342_0() throws IOException
{    while (stack.size() > 0) {        strings.remove(stack.pop());    }}
public SWFActionBlock nutch_f2343_0(int arg0, int arg1) throws IOException
{    return this;}
public SWFActionBlock nutch_f2344_0(int arg0) throws IOException
{    return this;}
public void nutch_f2345_0() throws IOException
{    super.add();}
public void nutch_f2346_0() throws IOException
{    super.asciiToChar();}
public void nutch_f2347_0() throws IOException
{    super.asciiToCharMB();}
public void nutch_f2348_0(int var) throws IOException
{    if (dict != null && var >= 0 && var < dict.length) {        stack.push(dict[var]);    }    super.push(var);}
public void nutch_f2349_0() throws IOException
{    strings.remove(stack.pop());    super.callFunction();}
public void nutch_f2350_0() throws IOException
{    strings.remove(stack.pop());    super.callMethod();}
public void nutch_f2351_0() throws IOException
{        String val = (String) stack.pop();    strings.remove(val);    super.getMember();}
public void nutch_f2352_0() throws IOException
{            stack.pop();    String name = (String) stack.pop();    strings.remove(name);    super.setMember();}
public void nutch_f2353_0() throws IOException
{    super.setProperty();}
public void nutch_f2354_0() throws IOException
{    super.setVariable();}
public void nutch_f2355_0() throws IOException
{    strings.remove(stack.pop());    super.call();}
public void nutch_f2356_0() throws IOException
{    strings.remove(stack.pop());    super.setTarget();}
public void nutch_f2357_0() throws IOException
{    strings.remove(stack.pop());    super.pop();}
public void nutch_f2358_0(boolean arg0) throws IOException
{    stack.push("" + arg0);    super.push(arg0);}
public void nutch_f2359_0(double arg0) throws IOException
{    stack.push("" + arg0);    super.push(arg0);}
public void nutch_f2360_0(float arg0) throws IOException
{    stack.push("" + arg0);    super.push(arg0);}
public void nutch_f2361_0() throws IOException
{    stack.push("");    super.pushNull();}
public void nutch_f2362_0(int arg0) throws IOException
{    stack.push("" + arg0);    super.pushRegister(arg0);}
public void nutch_f2363_0() throws IOException
{    stack.push("???");    super.pushUndefined();}
public void nutch_f2364_0() throws IOException
{    stack.pop();    super.getProperty();}
public void nutch_f2365_0() throws IOException
{    strings.remove(stack.pop());    super.getVariable();}
public void nutch_f2366_0(boolean arg0) throws IOException
{    stack.push("" + arg0);    super.gotoFrame(arg0);}
public void nutch_f2367_0(int arg0) throws IOException
{    stack.push("" + arg0);    super.gotoFrame(arg0);}
public void nutch_f2368_0(String arg0) throws IOException
{    stack.push("" + arg0);    strings.remove(arg0);    super.gotoFrame(arg0);}
public void nutch_f2369_0() throws IOException
{    stack.pop();    super.newObject();}
public SWFActionBlock nutch_f2370_0() throws IOException
{    return this;}
public Object nutch_f2371_0(Object o)
{        if (this.size() > maxSize) {        String val = (String) remove(0);        strings.remove(val);    }    return super.push(o);}
public Object nutch_f2372_0()
{        if (this.size() == 0)        return null;    else        return super.pop();}
public void nutch_f2373_0() throws ProtocolException, ParseException
{    String urlString;    Protocol protocol;    Content content;    Parse parse;    Configuration conf = NutchConfiguration.create();    for (int i = 0; i < sampleFiles.length; i++) {        urlString = "file:" + sampleDir + fileSeparator + sampleFiles[i];        protocol = new ProtocolFactory(conf).getProtocol(urlString);        content = protocol.getProtocolOutput(new Text(urlString), new CrawlDatum()).getContent();        parse = new ParseUtil(conf).parse(content).get(content.getUrl());        String text = parse.getText().replaceAll("[ \t\r\n]+", " ").trim();        Assert.assertTrue(sampleTexts[i].equals(text));    }}
public static synchronized BoilerpipeExtractor nutch_f2374_1(String boilerpipeExtractorName)
{        if (!extractorRepository.containsKey(boilerpipeExtractorName)) {                boilerpipeExtractorName = "de.l3s.boilerpipe.extractors." + boilerpipeExtractorName;                try {            ClassLoader loader = BoilerpipeExtractor.class.getClassLoader();            Class extractorClass = loader.loadClass(boilerpipeExtractorName);                        extractorRepository.put(boilerpipeExtractorName, (BoilerpipeExtractor) extractorClass.getConstructor().newInstance());        } catch (ClassNotFoundException e) {                    } catch (InstantiationException e) {                    } catch (Exception e) {                    }    }    return extractorRepository.get(boilerpipeExtractorName);}
 Node nutch_f2375_0()
{    return (null != m_docFrag) ? (Node) m_docFrag : (Node) m_doc;}
 Node nutch_f2376_0()
{    return m_currentNode;}
 java.io.Writer nutch_f2377_0()
{    return null;}
protected void nutch_f2378_0(Node newNode) throws org.xml.sax.SAXException
{    Node currentNode = m_currentNode;    if (null != currentNode) {        currentNode.appendChild(newNode);        } else if (null != m_docFrag) {        m_docFrag.appendChild(newNode);    } else {        boolean ok = true;        short type = newNode.getNodeType();        if (type == Node.TEXT_NODE) {            String data = newNode.getNodeValue();            if ((null != data) && (data.trim().length() > 0)) {                throw new org.xml.sax.SAXException("Warning: can't output text before document element!  Ignoring...");            }            ok = false;        } else if (type == Node.ELEMENT_NODE) {            if (m_doc.getDocumentElement() != null) {                throw new org.xml.sax.SAXException("Can't have more than one root on a DOM!");            }        }        if (ok)            m_doc.appendChild(newNode);    }}
public void nutch_f2379_0(Locator locator)
{}
public void nutch_f2380_0() throws org.xml.sax.SAXException
{}
public void nutch_f2381_0() throws org.xml.sax.SAXException
{}
public void nutch_f2382_0(String ns, String localName, String name, Attributes atts) throws org.xml.sax.SAXException
{    Element elem;    if (upperCaseElementNames)        name = name.toUpperCase();        if ((null == ns) || (ns.length() == 0) || ns.equals(defaultNamespaceURI))        elem = m_doc.createElementNS(null, name);    else        elem = m_doc.createElementNS(ns, name);    append(elem);    try {        int nAtts = atts.getLength();        if (0 != nAtts) {            for (int i = 0; i < nAtts; i++) {                                if (atts.getType(i).equalsIgnoreCase("ID"))                    setIDAttribute(atts.getValue(i), elem);                String attrNS = atts.getURI(i);                if ("".equals(attrNS))                                        attrNS = null;                                                                String attrQName = atts.getQName(i);                                if (attrQName.startsWith("xmlns:"))                    attrNS = "http://www.w3.org/2000/xmlns/";                                elem.setAttributeNS(attrNS, attrQName, atts.getValue(i));            }        }                m_elemStack.push(elem);        m_currentNode = elem;        } catch (java.lang.Exception de) {                throw new org.xml.sax.SAXException(de);    }}
public void nutch_f2383_0(String ns, String localName, String name) throws org.xml.sax.SAXException
{    if (!m_elemStack.isEmpty()) {        m_elemStack.pop();    }    m_currentNode = m_elemStack.isEmpty() ? null : (Node) m_elemStack.peek();}
public void nutch_f2384_0(String id, Element elem)
{}
public void nutch_f2385_0(char[] ch, int start, int length) throws org.xml.sax.SAXException
{    if (isOutsideDocElem() && XMLCharacterRecognizer.isWhiteSpace(ch, start, length))                return;    if (m_inCData) {        cdata(ch, start, length);        return;    }    String s = new String(ch, start, length);    Node childNode;    childNode = m_currentNode != null ? m_currentNode.getLastChild() : null;    if (childNode != null && childNode.getNodeType() == Node.TEXT_NODE) {        ((Text) childNode).appendData(s);    } else {        Text text = m_doc.createTextNode(s);        append(text);    }}
public void nutch_f2386_0(char[] ch, int start, int length) throws org.xml.sax.SAXException
{    if (isOutsideDocElem() && XMLCharacterRecognizer.isWhiteSpace(ch, start, length))                return;    String s = new String(ch, start, length);    append(m_doc.createProcessingInstruction("xslt-next-is-raw", "formatter-to-dom"));    append(m_doc.createTextNode(s));}
public void nutch_f2387_0(String name) throws org.xml.sax.SAXException
{}
public void nutch_f2388_0(String name) throws org.xml.sax.SAXException
{}
public void nutch_f2389_0(String name) throws org.xml.sax.SAXException
{    append(m_doc.createEntityReference(name));}
public void nutch_f2390_0(char[] ch, int start, int length) throws org.xml.sax.SAXException
{    if (isOutsideDocElem())                return;    String s = new String(ch, start, length);    append(m_doc.createTextNode(s));}
private boolean nutch_f2391_0()
{    return (null == m_docFrag) && m_elemStack.size() == 0 && (null == m_currentNode || m_currentNode.getNodeType() == Node.DOCUMENT_NODE);}
public void nutch_f2392_0(String target, String data) throws org.xml.sax.SAXException
{    append(m_doc.createProcessingInstruction(target, data));}
public void nutch_f2393_0(char[] ch, int start, int length) throws org.xml.sax.SAXException
{        if (ch == null || start < 0 || length >= (ch.length - start) || length < 0)        return;    append(m_doc.createComment(new String(ch, start, length)));}
public void nutch_f2394_0() throws org.xml.sax.SAXException
{    m_inCData = true;    append(m_doc.createCDATASection(""));}
public void nutch_f2395_0() throws org.xml.sax.SAXException
{    m_inCData = false;}
public void nutch_f2396_0(char[] ch, int start, int length) throws org.xml.sax.SAXException
{    if (isOutsideDocElem() && XMLCharacterRecognizer.isWhiteSpace(ch, start, length))                return;    String s = new String(ch, start, length);        Node n = m_currentNode.getLastChild();    if (n instanceof CDATASection)        ((CDATASection) n).appendData(s);    else if (n instanceof Comment)        ((Comment) n).appendData(s);}
public void nutch_f2397_0(String name, String publicId, String systemId) throws org.xml.sax.SAXException
{}
public void nutch_f2398_0() throws org.xml.sax.SAXException
{}
public void nutch_f2399_0(String prefix, String uri) throws org.xml.sax.SAXException
{/*     *      * if((null != m_currentNode) && (m_currentNode.getNodeType() ==     * Node.ELEMENT_NODE)) { String qname; if(((null != prefix) &&     * (prefix.length() == 0)) || (null == prefix)) qname = "xmlns"; else qname     * = "xmlns:"+prefix;     *      * Element elem = (Element)m_currentNode; String val =     * elem.getAttribute(qname);      * { elem.setAttributeNS("http://www.w3.org/XML/1998/namespace", qname,     * uri); } }     */}
public void nutch_f2400_0(String prefix) throws org.xml.sax.SAXException
{}
public void nutch_f2401_0(String name) throws org.xml.sax.SAXException
{}
public boolean nutch_f2402_0()
{    return upperCaseElementNames;}
public void nutch_f2403_0(boolean upperCaseElementNames)
{    this.upperCaseElementNames = upperCaseElementNames;}
public String nutch_f2404_0()
{    return defaultNamespaceURI;}
public void nutch_f2405_0(String defaultNamespaceURI)
{    this.defaultNamespaceURI = defaultNamespaceURI;}
public String nutch_f2406_0()
{    return "LP[el=" + elName + ",attr=" + attrName + ",len=" + childLen + "]";}
public void nutch_f2407_0(Configuration conf)
{        Collection<String> forceTags = new ArrayList<String>(1);    this.conf = conf;    linkParams.clear();    linkParams.put("a", new LinkParams("a", "href", 1));    linkParams.put("area", new LinkParams("area", "href", 0));    if (conf.getBoolean("parser.html.form.use_action", true)) {        linkParams.put("form", new LinkParams("form", "action", 1));        if (conf.get("parser.html.form.use_action") != null)            forceTags.add("form");    }    linkParams.put("frame", new LinkParams("frame", "src", 0));    linkParams.put("iframe", new LinkParams("iframe", "src", 0));    linkParams.put("script", new LinkParams("script", "src", 0));    linkParams.put("link", new LinkParams("link", "href", 0));    linkParams.put("img", new LinkParams("img", "src", 0));    linkParams.put("source", new LinkParams("source", "src", 0));        String[] ignoreTags = conf.getStrings("parser.html.outlinks.ignore_tags");    for (int i = 0; ignoreTags != null && i < ignoreTags.length; i++) {        ignoredTags.add(ignoreTags[i].toLowerCase());        if (!forceTags.contains(ignoreTags[i]))            linkParams.remove(ignoreTags[i]);    }        srcTagMetaName = this.conf.get("parser.html.outlinks.htmlnode_metadata_name");    keepNodenames = (srcTagMetaName != null && srcTagMetaName.length() > 0);    blockNodes = new HashSet<>(conf.getTrimmedStringCollection("parser.html.line.separators"));}
private boolean nutch_f2408_0(StringBuffer sb, Node node, boolean abortOnNestedAnchors)
{    if (getTextHelper(sb, node, abortOnNestedAnchors, 0)) {        return true;    }    return false;}
public void nutch_f2409_0(StringBuffer sb, Node node)
{    getText(sb, node, false);}
private boolean nutch_f2410_0(StringBuffer sb, Node node, boolean abortOnNestedAnchors, int anchorDepth)
{    boolean abort = false;    NodeWalker walker = new NodeWalker(node);    while (walker.hasNext()) {        Node currentNode = walker.nextNode();        String nodeName = currentNode.getNodeName();        short nodeType = currentNode.getNodeType();        Node previousSibling = currentNode.getPreviousSibling();        if (previousSibling != null && blockNodes.contains(previousSibling.getNodeName().toLowerCase())) {            appendParagraphSeparator(sb);        } else if (blockNodes.contains(nodeName.toLowerCase())) {            appendParagraphSeparator(sb);        }        if ("script".equalsIgnoreCase(nodeName)) {            walker.skipChildren();        }        if ("style".equalsIgnoreCase(nodeName)) {            walker.skipChildren();        }        if (abortOnNestedAnchors && "a".equalsIgnoreCase(nodeName)) {            anchorDepth++;            if (anchorDepth > 1) {                abort = true;                break;            }        }        if (nodeType == Node.COMMENT_NODE) {            walker.skipChildren();        }        if (nodeType == Node.TEXT_NODE) {                        String text = currentNode.getNodeValue();            text = text.replaceAll("\\s+", " ");            text = text.trim();            if (text.length() > 0) {                appendSpace(sb);                sb.append(text);            } else {                appendParagraphSeparator(sb);            }        }    }    return abort;}
private void nutch_f2411_0(StringBuffer buffer)
{    if (buffer.length() == 0) {        return;    }    char lastChar = buffer.charAt(buffer.length() - 1);    if ('\n' != lastChar) {                while (lastChar == ' ') {            buffer.deleteCharAt(buffer.length() - 1);            lastChar = buffer.charAt(buffer.length() - 1);        }        if ('\n' != lastChar) {            buffer.append('\n');        }    }}
private void nutch_f2412_0(StringBuffer buffer)
{    if (buffer.length() == 0) {        return;    }    char lastChar = buffer.charAt(buffer.length() - 1);    if (' ' != lastChar && '\n' != lastChar) {        buffer.append(' ');    }}
public boolean nutch_f2413_0(StringBuffer sb, Node node)
{    NodeWalker walker = new NodeWalker(node);    while (walker.hasNext()) {        Node currentNode = walker.nextNode();        String nodeName = currentNode.getNodeName();        short nodeType = currentNode.getNodeType();        if ("body".equalsIgnoreCase(nodeName)) {                        return false;        }        if (nodeType == Node.ELEMENT_NODE) {            if ("title".equalsIgnoreCase(nodeName)) {                getText(sb, currentNode);                return true;            }        }    }    return false;}
public String nutch_f2414_0(Node node)
{    NodeWalker walker = new NodeWalker(node);    while (walker.hasNext()) {        Node currentNode = walker.nextNode();        String nodeName = currentNode.getNodeName();        short nodeType = currentNode.getNodeType();                if (nodeType == Node.ELEMENT_NODE) {            if ("body".equalsIgnoreCase(nodeName)) {                                return null;            }            if ("base".equalsIgnoreCase(nodeName)) {                NamedNodeMap attrs = currentNode.getAttributes();                for (int i = 0; i < attrs.getLength(); i++) {                    Node attr = attrs.item(i);                    if ("href".equalsIgnoreCase(attr.getNodeName())) {                        return attr.getNodeValue();                    }                }            }        }    }        return null;}
private boolean nutch_f2415_0(Node node)
{    String val = node.getNodeValue();    for (int i = 0; i < val.length(); i++) {        if (!Character.isWhitespace(val.charAt(i)))            return false;    }    return true;}
private boolean nutch_f2416_0(Node node, NodeList children, int childLen, LinkParams params)
{    if (childLen == 0) {                if (params.childLen == 0)            return false;        else            return true;    } else if ((childLen == 1) && (children.item(0).getNodeType() == Node.ELEMENT_NODE) && (params.elName.equalsIgnoreCase(children.item(0).getNodeName()))) {                return true;    } else if (childLen == 2) {        Node c0 = children.item(0);        Node c1 = children.item(1);        if ((c0.getNodeType() == Node.ELEMENT_NODE) && (params.elName.equalsIgnoreCase(c0.getNodeName())) && (c1.getNodeType() == Node.TEXT_NODE) && hasOnlyWhiteSpace(c1)) {                        return true;        }        if ((c1.getNodeType() == Node.ELEMENT_NODE) && (params.elName.equalsIgnoreCase(c1.getNodeName())) && (c0.getNodeType() == Node.TEXT_NODE) && hasOnlyWhiteSpace(c0)) {                        return true;        }    } else if (childLen == 3) {        Node c0 = children.item(0);        Node c1 = children.item(1);        Node c2 = children.item(2);        if ((c1.getNodeType() == Node.ELEMENT_NODE) && (params.elName.equalsIgnoreCase(c1.getNodeName())) && (c0.getNodeType() == Node.TEXT_NODE) && (c2.getNodeType() == Node.TEXT_NODE) && hasOnlyWhiteSpace(c0) && hasOnlyWhiteSpace(c2)) {                        return true;        }    }    return false;}
public void nutch_f2417_0(URL base, ArrayList<Outlink> outlinks, Node node)
{    NodeWalker walker = new NodeWalker(node);    while (walker.hasNext()) {        Node currentNode = walker.nextNode();        String nodeName = currentNode.getNodeName();        short nodeType = currentNode.getNodeType();        NodeList children = currentNode.getChildNodes();        int childLen = (children != null) ? children.getLength() : 0;        if (nodeType == Node.ELEMENT_NODE) {            nodeName = nodeName.toLowerCase();            LinkParams params = (LinkParams) linkParams.get(nodeName);            if (params != null) {                if (!shouldThrowAwayLink(currentNode, children, childLen, params)) {                    StringBuffer linkText = new StringBuffer();                    getText(linkText, currentNode, true);                    NamedNodeMap attrs = currentNode.getAttributes();                    String target = null;                    boolean noFollow = false;                    boolean post = false;                    for (int i = 0; i < attrs.getLength(); i++) {                        Node attr = attrs.item(i);                        String attrName = attr.getNodeName();                        if (params.attrName.equalsIgnoreCase(attrName)) {                            target = attr.getNodeValue();                        } else if ("rel".equalsIgnoreCase(attrName) && "nofollow".equalsIgnoreCase(attr.getNodeValue())) {                            noFollow = true;                        } else if ("method".equalsIgnoreCase(attrName) && "post".equalsIgnoreCase(attr.getNodeValue())) {                            post = true;                        }                    }                    if (target != null && !noFollow && !post)                        try {                            URL url = URLUtil.resolveURL(base, target);                            Outlink outlink = new Outlink(url.toString(), linkText.toString().trim());                            outlinks.add(outlink);                                                        if (keepNodenames) {                                MapWritable metadata = new MapWritable();                                metadata.put(new Text(srcTagMetaName), new Text(nodeName));                                outlink.setMetadata(metadata);                            }                        } catch (MalformedURLException e) {                                                }                }                                if (params.childLen == 0)                    continue;            }        }    }}
public void nutch_f2418_0(URL base, ArrayList<Outlink> outlinks, List<Link> tikaExtractedOutlinks)
{    String target = null;    String anchor = null;    boolean noFollow = false;    for (Link link : tikaExtractedOutlinks) {        target = link.getUri();        noFollow = (link.getRel().toLowerCase().equals("nofollow")) ? true : false;        anchor = link.getText();        if (!ignoredTags.contains(link.getType())) {            if (target != null && !noFollow) {                try {                    URL url = URLUtil.resolveURL(base, target);                                        anchor = anchor.replaceAll("\\s+", " ");                    anchor = anchor.trim();                    outlinks.add(new Outlink(url.toString(), anchor));                } catch (MalformedURLException e) {                                }            }        }    }}
public static final void nutch_f2419_0(HTMLMetaTags metaTags, Node node, URL currURL)
{    metaTags.reset();    getMetaTagsHelper(metaTags, node, currURL);}
private static final void nutch_f2420_0(HTMLMetaTags metaTags, Node node, URL currURL)
{    if (node.getNodeType() == Node.ELEMENT_NODE) {        if ("body".equalsIgnoreCase(node.getNodeName())) {                        return;        }        if ("meta".equalsIgnoreCase(node.getNodeName())) {            NamedNodeMap attrs = node.getAttributes();            Node nameNode = null;            Node equivNode = null;            Node contentNode = null;                        for (int i = 0; i < attrs.getLength(); i++) {                Node attr = attrs.item(i);                String attrName = attr.getNodeName().toLowerCase();                if (attrName.equals("name")) {                    nameNode = attr;                } else if (attrName.equals("http-equiv")) {                    equivNode = attr;                } else if (attrName.equals("content")) {                    contentNode = attr;                }            }            if (nameNode != null) {                if (contentNode != null) {                    String name = nameNode.getNodeValue().toLowerCase();                    metaTags.getGeneralTags().add(name, contentNode.getNodeValue());                    if ("robots".equals(name)) {                        String directives = contentNode.getNodeValue().toLowerCase();                        int index = directives.indexOf("none");                        if (index >= 0) {                            metaTags.setNoIndex();                            metaTags.setNoFollow();                        }                        index = directives.indexOf("all");                        if (index >= 0) {                                                }                        index = directives.indexOf("noindex");                        if (index >= 0) {                            metaTags.setNoIndex();                        }                        index = directives.indexOf("nofollow");                        if (index >= 0) {                            metaTags.setNoFollow();                        }                        index = directives.indexOf("noarchive");                        if (index >= 0) {                            metaTags.setNoCache();                        }                    } else                     if (name.equals("pragma")) {                        String content = contentNode.getNodeValue().toLowerCase();                        if (content.contains("no-cache")) {                            metaTags.setNoCache();                        }                    } else if (name.equals("refresh")) {                        String content = contentNode.getNodeValue().toLowerCase();                        setRefresh(metaTags, content, currURL);                    } else if (name.equals("content-location")) {                        String urlString = contentNode.getNodeValue();                        URL url = null;                        try {                            if (currURL == null) {                                url = new URL(urlString);                            } else {                                url = new URL(currURL, urlString);                            }                            metaTags.setBaseHref(url);                        } catch (MalformedURLException e) {                                                }                    }                }            }            if (equivNode != null) {                if (contentNode != null) {                    String name = equivNode.getNodeValue().toLowerCase();                    String content = contentNode.getNodeValue();                    metaTags.getHttpEquivTags().setProperty(name, content);                    if ("pragma".equals(name)) {                        content = content.toLowerCase();                        int index = content.indexOf("no-cache");                        if (index >= 0)                            metaTags.setNoCache();                    } else if ("refresh".equals(name)) {                        setRefresh(metaTags, content, currURL);                    }                }            }        } else if ("base".equalsIgnoreCase(node.getNodeName())) {            NamedNodeMap attrs = node.getAttributes();            Node hrefNode = attrs.getNamedItem("href");            if (hrefNode != null) {                String urlString = hrefNode.getNodeValue();                URL url = null;                try {                    if (currURL == null)                        url = new URL(urlString);                    else                        url = new URL(currURL, urlString);                } catch (Exception e) {                    ;                }                if (url != null)                    metaTags.setBaseHref(url);            }        }    }    NodeList children = node.getChildNodes();    if (children != null) {        int len = children.getLength();        for (int i = 0; i < len; i++) {            getMetaTagsHelper(metaTags, children.item(i), currURL);        }    }}
private static void nutch_f2421_0(HTMLMetaTags metaTags, String content, URL currURL)
{    int idx = content.indexOf(';');    String time = null;    if (idx == -1) {                time = content;    } else        time = content.substring(0, idx);    try {        metaTags.setRefreshTime(Integer.parseInt(time));                metaTags.setRefresh(true);    } catch (Exception e) {        ;    }    URL refreshUrl = null;    if (metaTags.getRefresh() && idx != -1) {                idx = content.toLowerCase().indexOf("url=");        if (idx == -1) {                                    idx = content.indexOf(';') + 1;        } else            idx += 4;        if (idx != -1) {            String url = content.substring(idx);            try {                refreshUrl = new URL(url);            } catch (Exception e) {                                try {                    refreshUrl = new URL(currURL, url);                } catch (Exception e1) {                    refreshUrl = null;                }            }        }    }    if (metaTags.getRefresh()) {        if (refreshUrl == null) {                                    refreshUrl = currURL;        }        metaTags.setRefreshHref(refreshUrl);    }}
public ParseResult nutch_f2422_0(Content content)
{    HTMLDocumentImpl doc = new HTMLDocumentImpl();    doc.setErrorChecking(false);    DocumentFragment root = doc.createDocumentFragment();    return getParse(content, doc, root);}
 ParseResult nutch_f2423_1(Content content, HTMLDocumentImpl doc, DocumentFragment root)
{    String mimeType = content.getContentType();    URL base;    try {        base = new URL(content.getBaseUrl());    } catch (MalformedURLException e) {        return new ParseStatus(e).getEmptyParseResult(content.getUrl(), getConf());    }        CompositeParser compositeParser = (CompositeParser) tikaConfig.getParser();    Parser parser = compositeParser.getParsers().get(MediaType.parse(mimeType));    if (parser == null) {        String message = "Can't retrieve Tika parser for mime-type " + mimeType;                return new ParseStatus(ParseStatus.FAILED, message).getEmptyParseResult(content.getUrl(), getConf());    }        byte[] raw = content.getContent();    Metadata tikamd = new Metadata();    ContentHandler domHandler;        if (useBoilerpipe && boilerpipeMimeTypes.contains(mimeType)) {        BoilerpipeContentHandler bpHandler = new BoilerpipeContentHandler((ContentHandler) new DOMBuilder(doc, root), BoilerpipeExtractorRepository.getExtractor(boilerpipeExtractorName));        bpHandler.setIncludeMarkup(true);        domHandler = (ContentHandler) bpHandler;    } else {        DOMBuilder domBuilder = new DOMBuilder(doc, root);        domBuilder.setUpperCaseElementNames(upperCaseElementNames);        domBuilder.setDefaultNamespaceURI(XHTMLContentHandler.XHTML);        domHandler = (ContentHandler) domBuilder;    }    LinkContentHandler linkContentHandler = new LinkContentHandler();    ParseContext context = new ParseContext();    if (parseEmbedded) {        context.set(Parser.class, new AutoDetectParser(tikaConfig));    }    TeeContentHandler teeContentHandler = new TeeContentHandler(domHandler, linkContentHandler);    if (HTMLMapper != null)        context.set(HtmlMapper.class, HTMLMapper);    tikamd.set(Metadata.CONTENT_TYPE, mimeType);    try {        parser.parse(new ByteArrayInputStream(raw), (ContentHandler) teeContentHandler, tikamd, context);    } catch (Exception e) {                return new ParseStatus(ParseStatus.FAILED, e.getMessage()).getEmptyParseResult(content.getUrl(), getConf());    }    HTMLMetaTags metaTags = new HTMLMetaTags();    String text = "";    String title = "";    Outlink[] outlinks = new Outlink[0];    org.apache.nutch.metadata.Metadata nutchMetadata = new org.apache.nutch.metadata.Metadata();                HTMLMetaProcessor.getMetaTags(metaTags, root, base);    if (LOG.isTraceEnabled()) {        LOG.trace("Meta tags for " + base + ": " + metaTags.toString());    }        if (!metaTags.getNoIndex()) {                StringBuffer sb = new StringBuffer();        if (LOG.isTraceEnabled()) {            LOG.trace("Getting text...");        }                utils.getText(sb, root);        text = sb.toString();        sb.setLength(0);        if (LOG.isTraceEnabled()) {            LOG.trace("Getting title...");        }                utils.getTitle(sb, root);        title = sb.toString().trim();    }    if (!metaTags.getNoFollow()) {                        ArrayList<Outlink> l = new ArrayList<Outlink>();        URL baseTag = base;        String baseTagHref = tikamd.get("Content-Location");        if (baseTagHref != null) {            try {                baseTag = new URL(base, baseTagHref);            } catch (MalformedURLException e) {                LOG.trace("Invalid <base href=\"{}\">", baseTagHref);            }        }        if (LOG.isTraceEnabled()) {            LOG.trace("Getting links (base URL = {}) ...", baseTag);        }                                List<Link> tikaExtractedOutlinks = linkContentHandler.getLinks();        utils.getOutlinks(baseTag, l, tikaExtractedOutlinks);        outlinks = l.toArray(new Outlink[l.size()]);        if (LOG.isTraceEnabled()) {            LOG.trace("found " + outlinks.length + " outlinks in " + content.getUrl());        }    }        String[] TikaMDNames = tikamd.names();    for (String tikaMDName : TikaMDNames) {        if (tikaMDName.equalsIgnoreCase(Metadata.TITLE))            continue;        String[] values = tikamd.getValues(tikaMDName);        for (String v : values) nutchMetadata.add(tikaMDName, v);    }    if (outlinks.length == 0) {        outlinks = OutlinkExtractor.getOutlinks(text, getConf());    }    ParseStatus status = new ParseStatus(ParseStatus.SUCCESS);    if (metaTags.getRefresh()) {        status.setMinorCode(ParseStatus.SUCCESS_REDIRECT);        status.setArgs(new String[] { metaTags.getRefreshHref().toString(), Integer.toString(metaTags.getRefreshTime()) });    }    ParseData parseData = new ParseData(status, title, outlinks, content.getMetadata(), nutchMetadata);    ParseResult parseResult = ParseResult.createParseResult(content.getUrl(), new ParseImpl(text, parseData));        ParseResult filteredParse = this.htmlParseFilters.filter(content, parseResult, metaTags, root);    if (metaTags.getNoCache()) {                for (Map.Entry<org.apache.hadoop.io.Text, Parse> entry : filteredParse) entry.getValue().getData().getParseMeta().set(Nutch.CACHING_FORBIDDEN_KEY, cachingPolicy);    }    return filteredParse;}
public void nutch_f2424_1(Configuration conf)
{    this.conf = conf;    this.tikaConfig = null;                String customConfFile = conf.get("tika.config.file");    if (customConfFile != null) {        try {                        URL customTikaConfig = conf.getResource(customConfFile);            if (customTikaConfig != null) {                tikaConfig = new TikaConfig(customTikaConfig, this.getClass().getClassLoader());            }        } catch (Exception e1) {            String message = "Problem loading custom Tika configuration from " + customConfFile;                    }    }    if (tikaConfig == null) {        try {            tikaConfig = new TikaConfig(this.getClass().getClassLoader());        } catch (Exception e2) {            String message = "Problem loading default Tika configuration";                    }    }        String htmlmapperClassName = conf.get("tika.htmlmapper.classname");    if (StringUtils.isNotBlank(htmlmapperClassName)) {        try {            Class<?> HTMLMapperClass = Class.forName(htmlmapperClassName);            boolean interfaceOK = HtmlMapper.class.isAssignableFrom(HTMLMapperClass);            if (!interfaceOK) {                throw new RuntimeException("Class " + htmlmapperClassName + " does not implement HtmlMapper");            }            HTMLMapper = (HtmlMapper) HTMLMapperClass.getConstructor().newInstance();        } catch (Exception e) {            String message = "Can't generate instance for class " + htmlmapperClassName;                        throw new RuntimeException(message);        }    }    htmlParseFilters = new HtmlParseFilters(conf);    utils = new DOMContentUtils(conf);    cachingPolicy = conf.get("parser.caching.forbidden.policy", Nutch.CACHING_FORBIDDEN_CONTENT);    upperCaseElementNames = conf.getBoolean("tika.uppercase.element.names", true);    useBoilerpipe = conf.get("tika.extractor", "none").equals("boilerpipe");    boilerpipeExtractorName = conf.get("tika.extractor.boilerpipe.algorithm", "ArticleExtractor");    boilerpipeMimeTypes = new HashSet<>(Arrays.asList(conf.getTrimmedStrings("tika.extractor.boilerpipe.mime.types", "text/html", "application/xhtml+xml")));    parseEmbedded = conf.getBoolean("tika.parse.embedded", true);}
public Configuration nutch_f2425_0()
{    return this.conf;}
 static boolean nutch_f2426_0(char ch)
{    return (ch == 0x20) || (ch == 0x09) || (ch == 0xD) || (ch == 0xA);}
 static boolean nutch_f2427_0(char[] ch, int start, int length)
{    int end = start + length;    for (int s = start; s < end; s++) {        if (!isWhiteSpace(ch[s]))            return false;    }    return true;}
 static boolean nutch_f2428_0(StringBuffer buf)
{    int n = buf.length();    for (int i = 0; i < n; i++) {        if (!isWhiteSpace(buf.charAt(i)))            return false;    }    return true;}
 static boolean nutch_f2429_0(String s)
{    if (null != s) {        int n = s.length();        for (int i = 0; i < n; i++) {            if (!isWhiteSpace(s.charAt(i)))                return false;        }    }    return true;}
public void nutch_f2430_0() throws Exception
{    conf = NutchConfiguration.create();    utils = new DOMContentUtils(conf);    conf.set("plugin.includes", "parse-tika");    TikaParser parser = new TikaParser();    parser.setConf(conf);    for (int i = 0; i < testPages.length; i++) {        try {            String url = testBaseHrefs[i];            testBaseHrefURLs[i] = new URL(url);            Content content = new Content(url, url, testPages[i].getBytes(StandardCharsets.UTF_8), "text/html", new Metadata(), conf);            HTMLDocumentImpl doc = new HTMLDocumentImpl();            doc.setErrorChecking(false);            DocumentFragment root = doc.createDocumentFragment();            parser.getParse(content, doc, root);            testDOMs[i] = root;        } catch (Exception e) {            Assert.assertTrue("caught exception: " + e, false);        }    }    answerOutlinks = new Outlink[][] { { new Outlink("http://www.nutch.org", "anchor") }, { new Outlink("http://www.nutch.org/", "home"), new Outlink("http://www.nutch.org/docs/bot.html", "bots") }, { new Outlink("http://www.nutch.org/", "separate this"), new Outlink("http://www.nutch.org/docs/ok", "from this") }, { new Outlink("http://www.nutch.org/", "home"), new Outlink("http://www.nutch.org/docs/1", "1"), new Outlink("http://www.nutch.org/docs/2", "2") }, { new Outlink("http://www.nutch.org/frames/top.html", ""), new Outlink("http://www.nutch.org/frames/left.html", ""), new Outlink("http://www.nutch.org/frames/invalid.html", ""), new Outlink("http://www.nutch.org/frames/right.html", "") }, { new Outlink("http://www.nutch.org/maps/logo.gif", ""), new Outlink("http://www.nutch.org/index.html", ""), new Outlink("http://www.nutch.org/maps/#bottom", ""), new Outlink("http://www.nutch.org/bot.html", ""), new Outlink("http://www.nutch.org/docs/index.html", "") }, { new Outlink("http://www.nutch.org/index.html", "whitespace test") }, {}, {}, {}, { new Outlink("http://www.nutch.org/;x", "anchor1"), new Outlink("http://www.nutch.org/g;x", "anchor2"), new Outlink("http://www.nutch.org/g;x?y#s", "anchor3") }, {     new Outlink("http://www.nutch.org/g", "anchor1"), new Outlink("http://www.nutch.org/g?y#s", "anchor2"), new Outlink("http://www.nutch.org/;something?y=1", "anchor3"), new Outlink("http://www.nutch.org/;something?y=1#s", "anchor4"), new Outlink("http://www.nutch.org/;something?y=1;somethingelse", "anchor5") }, {} };}
private static boolean nutch_f2431_0(String s1, String s2)
{    StringTokenizer st1 = new StringTokenizer(s1);    StringTokenizer st2 = new StringTokenizer(s2);    while (st1.hasMoreTokens()) {        if (!st2.hasMoreTokens())            return false;        if (!st1.nextToken().equals(st2.nextToken()))            return false;    }    if (st2.hasMoreTokens())        return false;    return true;}
public void nutch_f2432_0() throws Exception
{    if (testDOMs[0] == null)        setup();    for (int i = 0; i < testPages.length; i++) {        StringBuffer sb = new StringBuffer();        utils.getText(sb, testDOMs[i]);        String text = sb.toString();        Assert.assertTrue("expecting text: " + answerText[i] + System.getProperty("line.separator") + System.getProperty("line.separator") + "got text: " + text, equalsIgnoreWhitespace(answerText[i], text));    }}
public void nutch_f2433_0() throws Exception
{    if (testDOMs[0] == null)        setup();    for (int i = 0; i < testPages.length; i++) {        StringBuffer sb = new StringBuffer();        utils.getTitle(sb, testDOMs[i]);        String text = sb.toString();        Assert.assertTrue("expecting text: " + answerText[i] + System.getProperty("line.separator") + System.getProperty("line.separator") + "got text: " + text, equalsIgnoreWhitespace(answerTitle[i], text));    }}
public void nutch_f2434_0() throws Exception
{    if (testDOMs[0] == null)        setup();    for (int i = 0; i < testPages.length; i++) {        ArrayList<Outlink> outlinks = new ArrayList<Outlink>();        if (i == SKIP) {            conf.setBoolean("parser.html.form.use_action", false);            utils.setConf(conf);        } else {            conf.setBoolean("parser.html.form.use_action", true);            utils.setConf(conf);        }        utils.getOutlinks(testBaseHrefURLs[i], outlinks, testDOMs[i]);        Outlink[] outlinkArr = new Outlink[outlinks.size()];        outlinkArr = outlinks.toArray(outlinkArr);        compareOutlinks(answerOutlinks[i], outlinkArr);    }}
private static final void nutch_f2435_0(StringBuffer sb, Outlink[] o)
{    for (int i = 0; i < o.length; i++) {        sb.append(o[i].toString());        sb.append(System.getProperty("line.separator"));    }}
private static final String nutch_f2436_0(Outlink[] o)
{    StringBuffer sb = new StringBuffer();    appendOutlinks(sb, o);    return sb.toString();}
private static final void nutch_f2437_0(Outlink[] o1, Outlink[] o2)
{    if (o1.length != o2.length) {        Assert.assertTrue("got wrong number of outlinks (expecting " + o1.length + ", got " + o2.length + ")" + System.getProperty("line.separator") + "answer: " + System.getProperty("line.separator") + outlinksString(o1) + System.getProperty("line.separator") + "got: " + System.getProperty("line.separator") + outlinksString(o2) + System.getProperty("line.separator"), false);    }    for (int i = 0; i < o1.length; i++) {        if (!o1[i].equals(o2[i])) {            Assert.assertTrue("got wrong outlinks at position " + i + System.getProperty("line.separator") + "answer: " + System.getProperty("line.separator") + "'" + o1[i].getToUrl() + "', anchor: '" + o1[i].getAnchor() + "'" + System.getProperty("line.separator") + "got: " + System.getProperty("line.separator") + "'" + o2[i].getToUrl() + "', anchor: '" + o2[i].getAnchor() + "'", false);        }    }}
public void nutch_f2438_0()
{    conf = NutchConfiguration.create();    conf.set("file.content.limit", "-1");    conf.setBoolean("tika.parse.embedded", true);}
public String nutch_f2439_0(String fileName) throws ProtocolException, ParseException
{    String urlString = "file:" + sampleDir + fileSeparator + fileName;    Protocol protocol = new ProtocolFactory(conf).getProtocol(urlString);    Content content = protocol.getProtocolOutput(new Text(urlString), new CrawlDatum()).getContent();    Parse parse = new ParseUtil(conf).parseByExtensionId("parse-tika", content).get(content.getUrl());    return parse.getText();}
public void nutch_f2440_0() throws ProtocolException, ParseException
{    for (int i = 0; i < sampleFiles.length; i++) {        String found = getTextContent(sampleFiles[i]);        Assert.assertTrue("text found : '" + found + "'", found.contains(expectedText));    }}
public void nutch_f2441_0() throws ProtocolException, ParseException
{    String urlString;    Protocol protocol;    Content content;    Parse parse;    Configuration conf = NutchConfiguration.create();    for (int i = 0; i < sampleFiles.length; i++) {        urlString = "file:" + sampleDir + fileSeparator + sampleFiles[i];        protocol = new ProtocolFactory(conf).getProtocol(urlString);        content = protocol.getProtocolOutput(new Text(urlString), new CrawlDatum()).getContent();        parse = new ParseUtil(conf).parseByExtensionId("parse-tika", content).get(content.getUrl());                                                        ParseData theParseData = parse.getData();        Outlink[] theOutlinks = theParseData.getOutlinks();        Assert.assertTrue("There aren't 2 outlinks read!", theOutlinks.length == 2);                boolean hasLink1 = false, hasLink2 = false;        for (int j = 0; j < theOutlinks.length; j++) {            if (theOutlinks[j].getToUrl().equals("http://www-scf.usc.edu/~mattmann/")) {                hasLink1 = true;            }            if (theOutlinks[j].getToUrl().equals("http://www.nutch.org/")) {                hasLink2 = true;            }        }        if (!hasLink1 || !hasLink2) {            Assert.fail("Outlinks read from sample rss file are not correct!");        }    }}
protected Parse nutch_f2442_0(byte[] contentBytes)
{    String dummyUrl = "http://example.com/";    return parser.getParse(new Content(dummyUrl, dummyUrl, contentBytes, "text/html", new Metadata(), conf)).get(dummyUrl);}
public void nutch_f2443_1()
{    for (String[] testPage : encodingTestPages) {        String name = testPage[0];        Charset charset = Charset.forName(testPage[1]);        byte[] contentBytes = testPage[2].getBytes(charset);        Parse parse = parse(contentBytes);        String text = parse.getText();        String title = parse.getData().getTitle();        String keywords = parse.getData().getMeta("keywords");                                        Assert.assertEquals("Title not extracted properly (" + name + ")", encodingTestKeywords, title);        for (String keyword : encodingTestKeywords.split(",\\s*")) {            Assert.assertTrue(keyword + " not found in text (" + name + ")", text.contains(keyword));        }        Assert.assertNotNull("No keywords extracted", keywords);        Assert.assertEquals("Keywords not extracted properly (" + name + ")", encodingTestKeywords, keywords);    }}
public void nutch_f2444_1()
{    byte[] contentBytes = resolveBaseUrlTestContent.getBytes(StandardCharsets.UTF_8);        Parse parse = parse(contentBytes);        Outlink[] outlinks = parse.getData().getOutlinks();    Assert.assertEquals(1, outlinks.length);    Assert.assertEquals("http://www.example.com/index.html", outlinks[0].getToUrl());}
public void nutch_f2445_0() throws ProtocolException, ParseException
{    String urlString;    Protocol protocol;    Content content;    Parse parse;    for (int i = 0; i < sampleFiles.length; i++) {        urlString = "file:" + sampleDir + fileSeparator + sampleFiles[i];        Configuration conf = NutchConfiguration.create();        protocol = new ProtocolFactory(conf).getProtocol(urlString);        content = protocol.getProtocolOutput(new Text(urlString), new CrawlDatum()).getContent();        parse = new ParseUtil(conf).parseByExtensionId("parse-tika", content).get(content.getUrl());        Assert.assertEquals("121", parse.getData().getMeta("width"));        Assert.assertEquals("48", parse.getData().getMeta("height"));    }}
public void nutch_f2446_0()
{    conf = NutchConfiguration.create();    conf.set("file.content.limit", "-1");}
public String nutch_f2447_0(String fileName) throws ProtocolException, ParseException
{    String urlString = "file:" + sampleDir + fileSeparator + fileName;    Protocol protocol = new ProtocolFactory(conf).getProtocol(urlString);    Content content = protocol.getProtocolOutput(new Text(urlString), new CrawlDatum()).getContent();    Parse parse = new ParseUtil(conf).parseByExtensionId("parse-tika", content).get(content.getUrl());    return parse.getText();}
public void nutch_f2448_0() throws ProtocolException, ParseException
{    for (int i = 0; i < sampleFiles.length; i++) {        String found = getTextContent(sampleFiles[i]);        Assert.assertTrue("text found : '" + found + "'", found.startsWith(expectedText));    }}
public void nutch_f2449_0() throws ProtocolException, ParseException
{    String[] filenames = new File(sampleDir).list();    for (int i = 0; i < filenames.length; i++) {        if (filenames[i].endsWith(".doc") == false)            continue;        Assert.assertTrue("can't read content of " + filenames[i], getTextContent(filenames[i]).length() > 0);    }}
public void nutch_f2450_0() throws ProtocolException, ParseException
{    String urlString;    Content content;    Parse parse;    Configuration conf = NutchConfiguration.create();    Protocol protocol;    ProtocolFactory factory = new ProtocolFactory(conf);    System.out.println("Expected : " + expectedText);    for (int i = 0; i < sampleFiles.length; i++) {        urlString = "file:" + sampleDir + fileSeparator + sampleFiles[i];        if (sampleFiles[i].startsWith("ootest") == false)            continue;        protocol = factory.getProtocol(urlString);        content = protocol.getProtocolOutput(new Text(urlString), new CrawlDatum()).getContent();        parse = new ParseUtil(conf).parseByExtensionId("parse-tika", content).get(content.getUrl());        String text = parse.getText().replaceAll("[ \t\r\n]+", " ").trim();                                Assert.assertTrue(text != null && text.length() > 0);        System.out.println("Found " + sampleFiles[i] + ": " + text);    }}
public void nutch_f2451_0() throws ProtocolException, ParseException
{    String urlString;    Protocol protocol;    Content content;    Parse parse;    for (int i = 0; i < sampleFiles.length; i++) {        urlString = "file:" + sampleDir + fileSeparator + sampleFiles[i];        Configuration conf = NutchConfiguration.create();        protocol = new ProtocolFactory(conf).getProtocol(urlString);        content = protocol.getProtocolOutput(new Text(urlString), new CrawlDatum()).getContent();        parse = new ParseUtil(conf).parseByExtensionId("parse-tika", content).get(content.getUrl());        int index = parse.getText().indexOf(expectedText);        Assert.assertTrue(index > 0);    }}
public void nutch_f2452_0()
{    Configuration conf = NutchConfiguration.create();    TikaParser parser = new TikaParser();    parser.setConf(conf);    try {        currURLsAndAnswers = new URL[][] { { new URL("http://www.nutch.org"), null }, { new URL("http://www.nutch.org"), null }, { new URL("http://www.nutch.org"), null }, { new URL("http://www.nutch.org"), null }, { new URL("http://www.nutch.org"), null }, { new URL("http://www.nutch.org"), null }, { new URL("http://www.nutch.org"), null }, { new URL("http://www.nutch.org/foo/"), new URL("http://www.nutch.org/") }, { new URL("http://www.nutch.org"), new URL("http://www.nutch.org/base/") }, { new URL("http://www.nutch.org"), null } };    } catch (Exception e) {        Assert.assertTrue("couldn't make test URLs!", false);    }    for (int i = 0; i < tests.length; i++) {        byte[] bytes = tests[i].getBytes();        HTMLDocumentImpl doc = new HTMLDocumentImpl();        doc.setErrorChecking(false);        DocumentFragment root = doc.createDocumentFragment();        String url = "http://www.nutch.org";        Content content = new Content(url, url, bytes, "text/html", new Metadata(), conf);        Parse parse = null;        try {            parse = parser.getParse(content, doc, root).get(url);        } catch (Exception e) {            e.printStackTrace();        }        HTMLMetaTags robotsMeta = new HTMLMetaTags();        HTMLMetaProcessor.getMetaTags(robotsMeta, root, currURLsAndAnswers[i][0]);        Assert.assertEquals("got noindex wrong on test " + i, answers[i][0], robotsMeta.getNoIndex());        Assert.assertEquals("got nofollow wrong on test " + i, answers[i][1], robotsMeta.getNoFollow());        Assert.assertEquals("got nocache wrong on test " + i, answers[i][2], robotsMeta.getNoCache());        Assert.assertTrue("got base href wrong on test " + i + " (got " + robotsMeta.getBaseHref() + ")", ((robotsMeta.getBaseHref() == null) && (currURLsAndAnswers[i][1] == null)) || ((robotsMeta.getBaseHref() != null) && robotsMeta.getBaseHref().equals(currURLsAndAnswers[i][1])));        if (tests[i].contains("meta-refresh redirect")) {                        URL metaRefreshUrl = robotsMeta.getRefreshHref();            Assert.assertNotNull("failed to get meta-refresh redirect", metaRefreshUrl);            Assert.assertEquals("failed to get meta-refresh redirect", "http://example.com/", metaRefreshUrl.toString());            Assert.assertEquals("failed to add meta-refresh redirect to parse status", "http://example.com/", parse.getData().getStatus().getArgs()[0]);        }    }}
public void nutch_f2453_0() throws ProtocolException, ParseException
{    String urlString;    Protocol protocol;    Content content;    Parse parse;    Configuration conf = NutchConfiguration.create();    urlString = "file:" + sampleDir + fileSeparator + rtfFile;    protocol = new ProtocolFactory(conf).getProtocol(urlString);    content = protocol.getProtocolOutput(new Text(urlString), new CrawlDatum()).getContent();    parse = new ParseUtil(conf).parseByExtensionId("parse-tika", content).get(content.getUrl());    String text = parse.getText();    Assert.assertEquals("The quick brown fox jumps over the lazy dog", text.trim());    String title = parse.getData().getTitle();    Metadata meta = parse.getData().getParseMeta();    Assert.assertEquals("test rft document", title);    Assert.assertEquals("tests", meta.get(DublinCore.SUBJECT));}
public ParseResult nutch_f2454_1(final Content content)
{    String resultText = null;    String resultTitle = null;    Outlink[] outlinks = null;    List<Outlink> outLinksList = new ArrayList<Outlink>();    try {        final String contentLen = content.getMetadata().get(Response.CONTENT_LENGTH);        final int len = Integer.parseInt(contentLen);        if (LOG.isDebugEnabled()) {                    }        final byte[] contentInBytes = content.getContent();        if (contentLen != null && contentInBytes.length != len) {            return new ParseStatus(ParseStatus.FAILED, ParseStatus.FAILED_TRUNCATED, "Content truncated at " + contentInBytes.length + " bytes. Parser can't handle incomplete zip file.").getEmptyParseResult(content.getUrl(), getConf());        }        ZipTextExtractor extractor = new ZipTextExtractor(getConf());                resultText = extractor.extractText(new ByteArrayInputStream(contentInBytes), content.getUrl(), outLinksList);    } catch (Exception e) {        return new ParseStatus(ParseStatus.FAILED, "Can't be handled as Zip document. " + e).getEmptyParseResult(content.getUrl(), getConf());    }    if (resultText == null) {        resultText = "";    }    if (resultTitle == null) {        resultTitle = "";    }    outlinks = (Outlink[]) outLinksList.toArray(new Outlink[0]);    final ParseData parseData = new ParseData(ParseStatus.STATUS_SUCCESS, resultTitle, outlinks, content.getMetadata());    if (LOG.isTraceEnabled()) {        LOG.trace("Zip file parsed sucessfully !!");    }    return ParseResult.createParseResult(content.getUrl(), new ParseImpl(resultText, parseData));}
public void nutch_f2455_0(Configuration conf)
{    this.conf = conf;}
public Configuration nutch_f2456_0()
{    return this.conf;}
public static void nutch_f2457_0(String[] args) throws IOException
{    if (args.length < 1) {        System.out.println("ZipParser <zip_file>");        System.exit(1);    }    File file = new File(args[0]);    String url = "file:" + file.getCanonicalPath();    FileInputStream in = new FileInputStream(file);    byte[] bytes = new byte[in.available()];    in.read(bytes);    in.close();    Configuration conf = NutchConfiguration.create();    ZipParser parser = new ZipParser();    parser.setConf(conf);    Metadata meta = new Metadata();    meta.add(Response.CONTENT_LENGTH, "" + file.length());    ParseResult parseResult = parser.getParse(new Content(url, url, bytes, "application/zip", meta, conf));    Parse p = parseResult.get(url);    System.out.println(parseResult.size());    System.out.println("Parse Text:");    System.out.println(p.getText());    System.out.println("Parse Data:");    System.out.println(p.getData());}
public String nutch_f2458_1(InputStream input, String url, List<Outlink> outLinksList) throws IOException
{    String resultText = "";    ZipInputStream zin = new ZipInputStream(input);    ZipEntry entry;    while ((entry = zin.getNextEntry()) != null) {        if (!entry.isDirectory()) {            int size = (int) entry.getSize();            byte[] b = new byte[size];            for (int x = 0; x < size; x++) {                int err = zin.read();                if (err != -1) {                    b[x] = (byte) err;                }            }            String newurl = url + "/";            String fname = entry.getName();            newurl += fname;            URL aURL = new URL(newurl);            String base = aURL.toString();            int i = fname.lastIndexOf('.');            if (i != -1) {                                Tika tika = new Tika();                String contentType = tika.detect(fname);                try {                    Metadata metadata = new Metadata();                    metadata.set(Response.CONTENT_LENGTH, Long.toString(entry.getSize()));                    metadata.set(Response.CONTENT_TYPE, contentType);                    Content content = new Content(newurl, base, b, contentType, metadata, this.conf);                    Parse parse = new ParseUtil(this.conf).parse(content).get(content.getUrl());                    ParseData theParseData = parse.getData();                    Outlink[] theOutlinks = theParseData.getOutlinks();                    for (int count = 0; count < theOutlinks.length; count++) {                        outLinksList.add(new Outlink(theOutlinks[count].getToUrl(), theOutlinks[count].getAnchor()));                    }                    resultText += entry.getName() + " " + parse.getText() + " ";                } catch (ParseException e) {                    if (LOG.isInfoEnabled()) {                                            }                }            }        }    }    return resultText;}
public void nutch_f2459_0() throws ProtocolException, ParseException
{    String urlString;    Protocol protocol;    Content content;    Parse parse;    Configuration conf = NutchConfiguration.create();    for (int i = 0; i < sampleFiles.length; i++) {        urlString = "file:" + sampleDir + fileSeparator + sampleFiles[i];        protocol = new ProtocolFactory(conf).getProtocol(urlString);        content = protocol.getProtocolOutput(new Text(urlString), new CrawlDatum()).getContent();        parse = new ParseUtil(conf).parseByExtensionId("parse-zip", content).get(content.getUrl());        Assert.assertTrue("Extracted text does not start with <" + expectedText + ">: <" + parse.getText() + ">", parse.getText().startsWith(expectedText));    }}
public static HashMap<String, Integer> nutch_f2460_0(String line)
{    HashMap<String, Integer> dict = new HashMap<String, Integer>();    String[] dictarray = line.split(",");    for (String field : dictarray) {        dict.put(field.split(":")[0], Integer.valueOf(field.split(":")[1]));    }    return dict;}
public static String nutch_f2461_0(String line) throws IOException
{    double prob_ir = 0;    double prob_r = 0;    String result = "1";    String[] linearray = line.replaceAll("[^a-zA-Z ]", "").toLowerCase().split(" ");        if (!ismodel) {        Configuration configuration = new Configuration();        FileSystem fs = FileSystem.get(configuration);        BufferedReader bufferedReader = new BufferedReader(new InputStreamReader(fs.open(new Path("naivebayes-model"))));        uniquewords_size = Integer.valueOf(bufferedReader.readLine());        bufferedReader.readLine();        numof_ir = Integer.valueOf(bufferedReader.readLine());        numwords_ir = Integer.valueOf(bufferedReader.readLine());        wordfreq_ir = unflattenToHashmap(bufferedReader.readLine());        bufferedReader.readLine();        numof_r = Integer.valueOf(bufferedReader.readLine());        numwords_r = Integer.valueOf(bufferedReader.readLine());        wordfreq_r = unflattenToHashmap(bufferedReader.readLine());        ismodel = true;        bufferedReader.close();    }    for (String word : linearray) {        if (wordfreq_ir.containsKey(word))            prob_ir += Math.log(wordfreq_ir.get(word)) + 1 - Math.log(numwords_ir + uniquewords_size);        else            prob_ir += 1 - Math.log(numwords_ir + uniquewords_size);        if (wordfreq_r.containsKey(word))            prob_r += Math.log(wordfreq_r.get(word)) + 1 - Math.log(numwords_r + uniquewords_size);        else            prob_r += 1 - Math.log(numwords_r + uniquewords_size);    }    prob_ir += Math.log(numof_ir) - Math.log(numof_ir + numof_r);    prob_r += Math.log(numof_r) - Math.log(numof_ir + numof_r);    if (prob_ir > prob_r)        result = "0";    else        result = "1";    return result;}
public boolean nutch_f2462_1(String text)
{    try {        return classify(text);    } catch (IOException e) {            }    return false;}
public boolean nutch_f2463_0(String url)
{    return containsWord(url, wordlist);}
public boolean nutch_f2464_0(String text) throws IOException
{        if (Classify.classify(text).equals("1"))        return true;    return false;}
public void nutch_f2465_1() throws Exception
{        if (!FileSystem.get(conf).exists(new Path("naivebayes-model"))) {                Train.start(inputFilePath);    } else {            }}
public boolean nutch_f2466_0(String url, ArrayList<String> wordlist)
{    for (String word : wordlist) {        if (url.contains(word)) {            return true;        }    }    return false;}
public void nutch_f2467_1(Configuration conf)
{    this.conf = conf;    inputFilePath = conf.get(TRAINFILE_MODELFILTER);    dictionaryFile = conf.get(DICTFILE_MODELFILTER);    if (inputFilePath == null || inputFilePath.trim().length() == 0 || dictionaryFile == null || dictionaryFile.trim().length() == 0) {        String message = "ParseFilter: NaiveBayes: trainfile or wordlist not set in the parsefilte.naivebayes.trainfile or parsefilte.naivebayes.wordlist";        if (LOG.isErrorEnabled()) {                    }        throw new IllegalArgumentException(message);    }    try {        if ((FileSystem.get(conf).exists(new Path(inputFilePath))) || (FileSystem.get(conf).exists(new Path(dictionaryFile)))) {            String message = "ParseFilter: NaiveBayes: " + inputFilePath + " or " + dictionaryFile + " not found!";            if (LOG.isErrorEnabled()) {                            }            throw new IllegalArgumentException(message);        }        BufferedReader br = null;        String CurrentLine;        Reader reader = conf.getConfResourceAsReader(dictionaryFile);        br = new BufferedReader(reader);        while ((CurrentLine = br.readLine()) != null) {            wordlist.add(CurrentLine);        }    } catch (IOException e) {            }    try {        train();    } catch (Exception e) {            }}
public Configuration nutch_f2468_0()
{    return this.conf;}
public ParseResult nutch_f2469_1(Content content, ParseResult parseResult, HTMLMetaTags metaTags, DocumentFragment doc)
{    Parse parse = parseResult.get(content.getUrl());    String url = content.getBaseUrl();    ArrayList<Outlink> tempOutlinks = new ArrayList<Outlink>();    String text = parse.getText();    if (!filterParse(text)) {                                                Outlink[] out = null;        for (int i = 0; i < parse.getData().getOutlinks().length; i++) {                        if (filterUrl(parse.getData().getOutlinks()[i].getToUrl())) {                tempOutlinks.add(parse.getData().getOutlinks()[i]);                            } else {                            }        }        out = new Outlink[tempOutlinks.size()];        for (int i = 0; i < tempOutlinks.size(); i++) {            out[i] = tempOutlinks.get(i);        }        parse.getData().setOutlinks(out);    } else {            }    return parseResult;}
public static String nutch_f2470_0(String tomatch, String line)
{    int index = line.indexOf(tomatch);    if (index == -1) {        return line;    } else {        return line.substring(0, index) + line.substring(index + tomatch.length());    }}
public static void nutch_f2471_0(HashMap<String, Integer> dict, String key)
{    if (!key.equals("")) {        if (dict.containsKey(key))            dict.put(key, dict.get(key) + 1);        else            dict.put(key, 1);    }}
public static String nutch_f2472_0(HashMap<String, Integer> dict)
{    String result = "";    for (String key : dict.keySet()) {        result += key + ":" + dict.get(key) + ",";    }        result = result.substring(0, result.length() - 1);    return result;}
public static void nutch_f2473_0(String filepath) throws IOException
{                int numof_ir = 0;    int numof_r = 0;    int numwords_ir = 0;    int numwords_r = 0;    HashSet<String> uniquewords = new HashSet<String>();    HashMap<String, Integer> wordfreq_ir = new HashMap<String, Integer>();    HashMap<String, Integer> wordfreq_r = new HashMap<String, Integer>();    String line = "";    String target = "";    String[] linearray = null;        Configuration configuration = new Configuration();    FileSystem fs = FileSystem.get(configuration);    BufferedReader bufferedReader = new BufferedReader(configuration.getConfResourceAsReader(filepath));    while ((line = bufferedReader.readLine()) != null) {        target = line.split("\t")[0];        line = replacefirstoccuranceof(target + "\t", line);        linearray = line.replaceAll("[^a-zA-Z ]", "").toLowerCase().split(" ");                if (target.equals("0")) {            numof_ir += 1;            numwords_ir += linearray.length;            for (int i = 0; i < linearray.length; i++) {                uniquewords.add(linearray[i]);                updateHashMap(wordfreq_ir, linearray[i]);            }        } else {            numof_r += 1;            numwords_r += linearray.length;            for (int i = 0; i < linearray.length; i++) {                uniquewords.add(linearray[i]);                updateHashMap(wordfreq_r, linearray[i]);            }        }    }        Path path = new Path("naivebayes-model");    Writer writer = new BufferedWriter(new OutputStreamWriter(fs.create(path, true)));    writer.write(String.valueOf(uniquewords.size()) + "\n");    writer.write("0\n");    writer.write(String.valueOf(numof_ir) + "\n");    writer.write(String.valueOf(numwords_ir) + "\n");    writer.write(flattenHashMap(wordfreq_ir) + "\n");    writer.write("1\n");    writer.write(String.valueOf(numof_r) + "\n");    writer.write(String.valueOf(numwords_r) + "\n");    writer.write(flattenHashMap(wordfreq_r) + "\n");    writer.close();    bufferedReader.close();}
public ParseResult nutch_f2474_1(Content content, ParseResult parseResult, HTMLMetaTags metaTags, DocumentFragment doc)
{    Parse parse = parseResult.get(content.getUrl());    String html = new String(content.getContent());    String text = parse.getText();    for (Map.Entry<String, RegexRule> entry : rules.entrySet()) {        String field = entry.getKey();        RegexRule regexRule = entry.getValue();        String source = null;        if (regexRule.source.equalsIgnoreCase("html")) {            source = html;        }        if (regexRule.source.equalsIgnoreCase("text")) {            source = text;        }        if (source == null) {                    }        if (matches(source, regexRule.regex)) {            parse.getData().getParseMeta().set(field, "true");        } else {            parse.getData().getParseMeta().set(field, "false");        }    }    return parseResult;}
public void nutch_f2475_1(Configuration conf)
{    this.conf = conf;        String pluginName = "parsefilter-regex";    Extension[] extensions = PluginRepository.get(conf).getExtensionPoint(HtmlParseFilter.class.getName()).getExtensions();    for (int i = 0; i < extensions.length; i++) {        Extension extension = extensions[i];        if (extension.getDescriptor().getPluginId().equals(pluginName)) {            attributeFile = extension.getAttribute("file");            break;        }    }        if (attributeFile != null && attributeFile.trim().equals("")) {        attributeFile = null;    }    if (attributeFile != null) {        if (LOG.isInfoEnabled()) {                    }    } else {        if (LOG.isWarnEnabled()) {                    }    }        String file = conf.get("parsefilter.regex.file");    String stringRules = conf.get("parsefilter.regex.rules");    if (regexFile != null) {        file = regexFile;    } else if (attributeFile != null) {        file = attributeFile;    }    Reader reader = null;    if (stringRules != null) {                reader = new StringReader(stringRules);    } else {        reader = conf.getConfResourceAsReader(file);    }    try {        if (reader == null) {            reader = new FileReader(file);        }        readConfiguration(reader);    } catch (IOException e) {            }}
public Configuration nutch_f2476_0()
{    return this.conf;}
private boolean nutch_f2477_0(String value, Pattern pattern)
{    if (value != null) {        Matcher matcher = pattern.matcher(value);        return matcher.find();    }    return false;}
private synchronized void nutch_f2478_1(Reader configReader) throws IOException
{    if (rules.size() > 0) {        return;    }    String line;    BufferedReader reader = new BufferedReader(configReader);    while ((line = reader.readLine()) != null) {        if (StringUtils.isNotBlank(line) && !line.startsWith("#")) {            line = line.trim();            String[] parts = line.split("\\s");            if (parts.length == 3) {                String field = parts[0].trim();                String source = parts[1].trim();                String regex = parts[2].trim();                rules.put(field, new RegexRule(source, regex));            } else {                            }        }    }}
public void nutch_f2479_0() throws Exception
{    Configuration conf = NutchConfiguration.create();    String file = SAMPLES + SEPARATOR + "regex-parsefilter.txt";    RegexParseFilter filter = new RegexParseFilter(file);    filter.setConf(conf);    String url = "http://nutch.apache.org/";    String html = "<body><html><h1>nutch</h1><p>this is the extracted text blablabla</p></body></html>";    Content content = new Content(url, url, html.getBytes("UTF-8"), "text/html", new Metadata(), conf);    Parse parse = new ParseImpl("nutch this is the extracted text blablabla", new ParseData());    ParseResult result = ParseResult.createParseResult(url, parse);    result = filter.filter(content, result, null, null);    Metadata meta = parse.getData().getParseMeta();    assertEquals("true", meta.get("first"));    assertEquals("true", meta.get("second"));}
public void nutch_f2480_0() throws Exception
{    Configuration conf = NutchConfiguration.create();    String file = SAMPLES + SEPARATOR + "regex-parsefilter.txt";    RegexParseFilter filter = new RegexParseFilter(file);    filter.setConf(conf);    String url = "http://nutch.apache.org/";    String html = "<body><html><h2>nutch</h2><p>this is the extracted text no bla</p></body></html>";    Content content = new Content(url, url, html.getBytes("UTF-8"), "text/html", new Metadata(), conf);    Parse parse = new ParseImpl("nutch this is the extracted text bla", new ParseData());    ParseResult result = ParseResult.createParseResult(url, parse);    result = filter.filter(content, result, null, null);    Metadata meta = parse.getData().getParseMeta();    assertEquals("false", meta.get("first"));    assertEquals("false", meta.get("second"));}
public void nutch_f2481_0(Configuration conf)
{    this.conf = conf;    this.maxContentLength = conf.getInt("file.content.limit", 1024 * 1024);    this.crawlParents = conf.getBoolean("file.crawl.parent", true);    this.symlinksAsRedirects = conf.getBoolean("file.crawl.redirect_noncanonical", true);}
public Configuration nutch_f2482_0()
{    return this.conf;}
public void nutch_f2483_0(int maxContentLength)
{    this.maxContentLength = maxContentLength;}
public ProtocolOutput nutch_f2484_0(Text url, CrawlDatum datum)
{    String urlString = url.toString();    try {        URL u = new URL(urlString);        int redirects = 0;        while (true) {            FileResponse response;                        response = new FileResponse(u, datum, this, getConf());                        int code = response.getCode();            if (code == 200) {                                return new ProtocolOutput(response.toContent());            } else if (code == 304) {                                return new ProtocolOutput(response.toContent(), ProtocolStatus.STATUS_NOTMODIFIED);            } else if (code == 401) {                                return new ProtocolOutput(response.toContent(), new ProtocolStatus(ProtocolStatus.ACCESS_DENIED));            } else if (code == 404) {                                return new ProtocolOutput(response.toContent(), ProtocolStatus.STATUS_NOTFOUND);            } else if (code >= 300 && code < 400) {                                u = new URL(response.getHeader("Location"));                if (LOG.isTraceEnabled()) {                    LOG.trace("redirect to " + u);                }                if (symlinksAsRedirects) {                    return new ProtocolOutput(response.toContent(), new ProtocolStatus(ProtocolStatus.MOVED, u));                } else if (redirects == MAX_REDIRECTS) {                    LOG.trace("Too many redirects: {}", url);                    return new ProtocolOutput(response.toContent(), new ProtocolStatus(ProtocolStatus.REDIR_EXCEEDED, u));                }                redirects++;            } else {                                throw new FileError(code);            }        }    } catch (Exception e) {        e.printStackTrace();        return new ProtocolOutput(null, new ProtocolStatus(e));    }}
public static void nutch_f2485_0(String[] args) throws Exception
{    int maxContentLength = Integer.MIN_VALUE;    boolean dumpContent = false;    String urlString = null;    String usage = "Usage: File [-maxContentLength L] [-dumpContent] url";    if (args.length == 0) {        System.err.println(usage);        System.exit(-1);    }    for (int i = 0; i < args.length; i++) {        if (args[i].equals("-maxContentLength")) {            maxContentLength = Integer.parseInt(args[++i]);        } else if (args[i].equals("-dumpContent")) {            dumpContent = true;        } else if (i != args.length - 1) {            System.err.println(usage);            System.exit(-1);        } else            urlString = args[i];    }    File file = new File();    file.setConf(NutchConfiguration.create());    if (    maxContentLength != Integer.MIN_VALUE)        file.setMaxContentLength(maxContentLength);            ProtocolOutput output = file.getProtocolOutput(new Text(urlString), new CrawlDatum());    Content content = output.getContent();    System.err.println("URL: " + content.getUrl());    System.err.println("Status: " + output.getStatus());    System.err.println("Content-Type: " + content.getContentType());    System.err.println("Content-Length: " + content.getMetadata().get(Response.CONTENT_LENGTH));    System.err.println("Last-Modified: " + content.getMetadata().get(Response.LAST_MODIFIED));    String redirectLocation = content.getMetadata().get("Location");    if (redirectLocation != null) {        System.err.println("Location: " + redirectLocation);    }    if (dumpContent) {        System.out.print(new String(content.getContent()));    }    file = null;}
public BaseRobotRules nutch_f2486_0(Text url, CrawlDatum datum, List<Content> robotsTxtContent)
{    return RobotRulesParser.EMPTY_RULES;}
public int nutch_f2487_0(int code)
{    return code;}
public int nutch_f2488_0()
{    return code;}
public String nutch_f2489_0(String name)
{    return headers.get(name);}
public byte[] nutch_f2490_0()
{    return content;}
public Content nutch_f2491_0()
{    return new Content(orig, base, (content != null ? content : EMPTY_CONTENT), getHeader(Response.CONTENT_TYPE), headers, this.conf);}
private void nutch_f2493_0(java.io.File f) throws IOException
{    String path = f.toString();    if (this.file.crawlParents)        this.content = list2html(f.listFiles(), path, "/".equals(path) ? false : true);    else        this.content = list2html(f.listFiles(), path, false);        headers.set(Response.CONTENT_LENGTH, Integer.valueOf(this.content.length).toString());    headers.set(Response.CONTENT_TYPE, "text/html");    headers.set(Response.LAST_MODIFIED, HttpDateFormat.toString(f.lastModified()));            this.code = 200;}
private byte[] nutch_f2494_0(java.io.File[] list, String path, boolean includeDotDot)
{    StringBuffer x = new StringBuffer("<html><head>");    x.append("<title>Index of " + path + "</title></head>\n");    x.append("<body><h1>Index of " + path + "</h1><pre>\n");    if (includeDotDot) {        x.append("<a href='../'>../</a>\t-\t-\t-\n");    }        java.io.File f;    for (int i = 0; i < list.length; i++) {        f = list[i];        String name = f.getName();        String time = HttpDateFormat.toString(f.lastModified());        if (f.isDirectory()) {                                                            x.append("<a href='" + name + "/" + "'>" + name + "/</a>\t");            x.append(time + "\t-\n");        } else if (f.isFile()) {            x.append("<a href='" + name + "'>" + name + "</a>\t");            x.append(time + "\t" + f.length() + "\n");        } else {                }    }    x.append("</pre></body></html>\n");    return new String(x).getBytes();}
public void nutch_f2495_0()
{    conf = NutchConfiguration.create();}
public void nutch_f2496_0() throws ProtocolException
{    for (String testTextFile : testTextFiles) {        setContentType(testTextFile);    }}
public void nutch_f2497_0(String testTextFile) throws ProtocolException
{    String urlString = "file:" + sampleDir + fileSeparator + testTextFile;    Assert.assertNotNull(urlString);    Protocol protocol = new ProtocolFactory(conf).getProtocol(urlString);    ProtocolOutput output = protocol.getProtocolOutput(new Text(urlString), datum);    Assert.assertNotNull(output);    Assert.assertEquals("Status code: [" + output.getStatus().getCode() + "], not equal to: [" + ProtocolStatus.SUCCESS + "]: args: [" + output.getStatus().getArgs() + "]", ProtocolStatus.SUCCESS, output.getStatus().getCode());    Assert.assertNotNull(output.getContent());    Assert.assertNotNull(output.getContent().getContentType());    Assert.assertEquals(expectedMimeType, output.getContent().getContentType());    Assert.assertNotNull(output.getContent().getMetadata());    Assert.assertEquals(expectedMimeType, output.getContent().getMetadata().get(Response.CONTENT_TYPE));}
private void nutch_f2498_0()
{    __passiveHost = null;    __passivePort = -1;    __systemName = null;}
private void nutch_f2499_0(String reply) throws MalformedServerReplyException
{    int i, index, lastIndex;    String octet1, octet2;    StringBuffer host;    reply = reply.substring(reply.indexOf('(') + 1, reply.indexOf(')')).trim();    host = new StringBuffer(24);    lastIndex = 0;    index = reply.indexOf(',');    host.append(reply.substring(lastIndex, index));    for (i = 0; i < 3; i++) {        host.append('.');        lastIndex = index + 1;        index = reply.indexOf(',', lastIndex);        host.append(reply.substring(lastIndex, index));    }    lastIndex = index + 1;    index = reply.indexOf(',', lastIndex);    octet1 = reply.substring(lastIndex, index);    octet2 = reply.substring(index + 1);        try {        index = Integer.parseInt(octet1);        lastIndex = Integer.parseInt(octet2);    } catch (NumberFormatException e) {        throw new MalformedServerReplyException("Could not parse passive host information.\nServer Reply: " + reply);    }    index <<= 8;    index |= lastIndex;    __passiveHost = host.toString();    __passivePort = index;}
protected Socket nutch_f2500_0(int command, String arg) throws IOException, FtpExceptionCanNotHaveDataConnection
{    Socket socket;    if (pasv() != FTPReply.ENTERING_PASSIVE_MODE)        throw new FtpExceptionCanNotHaveDataConnection("pasv() failed. " + getReplyString());    try {        __parsePassiveModeReply(getReplyStrings()[0]);    } catch (MalformedServerReplyException e) {        throw new FtpExceptionCanNotHaveDataConnection(e.getMessage());    }                                                                                    socket = _socketFactory_.createSocket(__passiveHost, __passivePort);    if (!FTPReply.isPositivePreliminary(sendCommand(command, arg))) {        socket.close();        return null;    }    if (__remoteVerificationEnabled && !verifyRemote(socket)) {        InetAddress host1, host2;        host1 = socket.getInetAddress();        host2 = getRemoteAddress();        socket.close();                throw new FtpExceptionCanNotHaveDataConnection("Host attempting data connection " + host1.getHostAddress() + " is not same as server " + host2.getHostAddress() + " So we intentionally close it for security precaution.");    }    if (__dataTimeout >= 0)        socket.setSoTimeout(__dataTimeout);    return socket;}
public void nutch_f2501_0(int timeout)
{    __dataTimeout = timeout;}
public void nutch_f2502_0() throws IOException
{    __initDefaults();    super.disconnect();}
public void nutch_f2503_0(boolean enable)
{    __remoteVerificationEnabled = enable;}
public boolean nutch_f2504_0()
{    return __remoteVerificationEnabled;}
public boolean nutch_f2505_0(String username, String password) throws IOException
{    user(username);    if (FTPReply.isPositiveCompletion(getReplyCode()))        return true;        if (!FTPReply.isPositiveIntermediate(getReplyCode()))        return false;    return FTPReply.isPositiveCompletion(pass(password));}
public boolean nutch_f2506_0() throws IOException
{    return FTPReply.isPositiveCompletion(quit());}
public void nutch_f2507_0(String path, List<FTPFile> entries, int limit, FTPFileEntryParser parser) throws IOException, FtpExceptionCanNotHaveDataConnection, FtpExceptionUnknownForcedDataClose, FtpExceptionControlClosedByForcedDataClose
{    Socket socket = __openPassiveDataConnection(FTPCommand.LIST, path);    if (socket == null)        throw new FtpExceptionCanNotHaveDataConnection("LIST " + ((path == null) ? "" : path));    BufferedReader reader = new BufferedReader(new InputStreamReader(socket.getInputStream()));                int count = 0;    String line = parser.readNextEntry(reader);    while (line != null) {        FTPFile ftpFile = parser.parseFTPEntry(line);                if (ftpFile == null) {            line = parser.readNextEntry(reader);            continue;        }        entries.add(ftpFile);        count += line.length();                if (limit >= 0 && count > limit) {                        break;        }        line = parser.readNextEntry(reader);    }                socket.close();    try {        int reply = getReply();        if (!_notBadReply(reply))            throw new FtpExceptionUnknownForcedDataClose(getReplyString());    } catch (FTPConnectionClosedException e) {                throw new FtpExceptionControlClosedByForcedDataClose(e.getMessage());    }}
public void nutch_f2508_0(String path, OutputStream os, int limit) throws IOException, FtpExceptionCanNotHaveDataConnection, FtpExceptionUnknownForcedDataClose, FtpExceptionControlClosedByForcedDataClose
{    Socket socket = __openPassiveDataConnection(FTPCommand.RETR, path);    if (socket == null)        throw new FtpExceptionCanNotHaveDataConnection("RETR " + ((path == null) ? "" : path));    InputStream input = socket.getInputStream();                                int len;    int count = 0;    byte[] buf = new byte[org.apache.commons.net.io.Util.DEFAULT_COPY_BUFFER_SIZE];    while ((len = input.read(buf, 0, buf.length)) != -1) {        count += len;                if (limit >= 0 && count > limit) {            os.write(buf, 0, len - (count - limit));                        break;        }        os.write(buf, 0, len);        os.flush();    }                socket.close();    try {        int reply = getReply();        if (!_notBadReply(reply))            throw new FtpExceptionUnknownForcedDataClose(getReplyString());    } catch (FTPConnectionClosedException e) {                throw new FtpExceptionControlClosedByForcedDataClose(e.getMessage());    }}
private boolean nutch_f2509_0(int reply)
{    if (FTPReply.isPositiveCompletion(reply)) {        } else if (reply == 426) {                        } else if (reply == 450) {                        } else if (reply == 451) {                        } else {                return false;    }    return true;}
public boolean nutch_f2510_0(int fileType) throws IOException
{    if (FTPReply.isPositiveCompletion(type(fileType))) {        /*       * __fileType = fileType; __fileFormat = FTP.NON_PRINT_TEXT_FORMAT;       */        return true;    }    return false;}
public String nutch_f2511_0() throws IOException, FtpExceptionBadSystResponse
{        if (__systemName == null && FTPReply.isPositiveCompletion(syst())) {        __systemName = (getReplyStrings()[0]).substring(4);    } else {        throw new FtpExceptionBadSystResponse("Bad response of SYST: " + getReplyString());    }    return __systemName;}
public boolean nutch_f2512_0() throws IOException
{    return FTPReply.isPositiveCompletion(noop());}
public void nutch_f2513_0(int to)
{    timeout = to;}
public void nutch_f2514_0(int length)
{    maxContentLength = length;}
public void nutch_f2515_0(boolean followTalk)
{    this.followTalk = followTalk;}
public void nutch_f2516_0(boolean keepConnection)
{    this.keepConnection = keepConnection;}
public ProtocolOutput nutch_f2517_1(Text url, CrawlDatum datum)
{    String urlString = url.toString();    try {        URL u = new URL(urlString);        int redirects = 0;        while (true) {            FtpResponse response;                        response = new FtpResponse(u, datum, this, getConf());            int code = response.getCode();            datum.getMetaData().put(Nutch.PROTOCOL_STATUS_CODE_KEY, new Text(Integer.toString(code)));            if (code == 200) {                                return new ProtocolOutput(response.toContent());            } else if (code >= 300 && code < 400) {                                if (redirects == MAX_REDIRECTS)                    throw new FtpException("Too many redirects: " + url);                String loc = response.getHeader("Location");                try {                    u = new URL(u, loc);                } catch (MalformedURLException mue) {                                        return new ProtocolOutput(null, new ProtocolStatus(mue));                }                redirects++;                if (LOG.isTraceEnabled()) {                    LOG.trace("redirect to " + u);                }            } else {                                throw new FtpError(code);            }        }    } catch (Exception e) {                return new ProtocolOutput(null, new ProtocolStatus(e));    }}
protected void nutch_f2518_0()
{    try {        if (this.client != null && this.client.isConnected()) {            this.client.logout();            this.client.disconnect();        }    } catch (IOException e) {        }}
public static void nutch_f2519_0(String[] args) throws Exception
{    int timeout = Integer.MIN_VALUE;    int maxContentLength = Integer.MIN_VALUE;    @SuppressWarnings("unused")    String logLevel = "info";    boolean followTalk = false;    boolean keepConnection = false;    boolean dumpContent = false;    String urlString = null;    String usage = "Usage: Ftp [-logLevel level] [-followTalk] [-keepConnection] [-timeout N] [-maxContentLength L] [-dumpContent] url";    if (args.length == 0) {        System.err.println(usage);        System.exit(-1);    }    for (int i = 0; i < args.length; i++) {        if (args[i].equals("-logLevel")) {            logLevel = args[++i];        } else if (args[i].equals("-followTalk")) {            followTalk = true;        } else if (args[i].equals("-keepConnection")) {            keepConnection = true;        } else if (args[i].equals("-timeout")) {            timeout = Integer.parseInt(args[++i]) * 1000;        } else if (args[i].equals("-maxContentLength")) {            maxContentLength = Integer.parseInt(args[++i]);        } else if (args[i].equals("-dumpContent")) {            dumpContent = true;        } else if (i != args.length - 1) {            System.err.println(usage);            System.exit(-1);        } else {            urlString = args[i];        }    }    Ftp ftp = new Ftp();    ftp.setFollowTalk(followTalk);    ftp.setKeepConnection(keepConnection);    if (    timeout != Integer.MIN_VALUE)        ftp.setTimeout(timeout);    if (    maxContentLength != Integer.MIN_VALUE)        ftp.setMaxContentLength(maxContentLength);            Content content = ftp.getProtocolOutput(new Text(urlString), new CrawlDatum()).getContent();    System.err.println("Content-Type: " + content.getContentType());    System.err.println("Content-Length: " + content.getMetadata().get(Response.CONTENT_LENGTH));    System.err.println("Last-Modified: " + content.getMetadata().get(Response.LAST_MODIFIED));    if (dumpContent) {        System.out.print(new String(content.getContent()));    }    ftp = null;}
public void nutch_f2520_0(Configuration conf)
{    this.conf = conf;    this.maxContentLength = conf.getInt("ftp.content.limit", 1024 * 1024);    this.timeout = conf.getInt("ftp.timeout", 10000);    this.userName = conf.get("ftp.username", "anonymous");    this.passWord = conf.get("ftp.password", "anonymous@example.com");    this.serverTimeout = conf.getInt("ftp.server.timeout", 60 * 1000);    this.keepConnection = conf.getBoolean("ftp.keep.connection", false);    this.followTalk = conf.getBoolean("ftp.follow.talk", false);    this.robots.setConf(conf);}
public Configuration nutch_f2521_0()
{    return this.conf;}
public BaseRobotRules nutch_f2522_0(Text url, CrawlDatum datum, List<Content> robotsTxtContent)
{    return robots.getRobotRulesSet(this, url, robotsTxtContent);}
public int nutch_f2523_0()
{    return BUFFER_SIZE;}
public int nutch_f2524_0(int code)
{    return code;}
public int nutch_f2525_0()
{    return code;}
public String nutch_f2526_0(String name)
{    return headers.get(name);}
public byte[] nutch_f2527_0()
{    return content;}
public Content nutch_f2528_0()
{    return new Content(orig, base, (content != null ? content : EMPTY_CONTENT), getHeader(Response.CONTENT_TYPE), headers, this.conf);}
private byte[] nutch_f2531_0(List<FTPFile> list, String path, boolean includeDotDot)
{            StringBuffer x = new StringBuffer("<html><head>");    x.append("<title>Index of " + path + "</title></head>\n");    x.append("<body><h1>Index of " + path + "</h1><pre>\n");    if (includeDotDot) {        x.append("<a href='../'>../</a>\t-\t-\t-\n");    }    for (int i = 0; i < list.size(); i++) {        FTPFile f = (FTPFile) list.get(i);        String name = f.getName();        String time = HttpDateFormat.toString(f.getTimestamp());        if (f.isDirectory()) {                        if (name.equals(".") || name.equals(".."))                continue;            x.append("<a href='" + name + "/" + "'>" + name + "/</a>\t");            x.append(time + "\t-\n");        } else if (f.isFile()) {            x.append("<a href='" + name + "'>" + name + "</a>\t");            x.append(time + "\t" + f.getSize() + "\n");        } else {                        }    }    x.append("</pre></body></html>\n");    return new String(x).getBytes();}
public BaseRobotRules nutch_f2532_1(Protocol ftp, URL url, List<Content> robotsTxtContent)
{        String protocol = url.getProtocol().toLowerCase();            String host = url.getHost().toLowerCase();    if (LOG.isTraceEnabled() && isWhiteListed(url)) {        LOG.trace("Ignoring robots.txt (host is whitelisted) for URL: {}", url);    }    BaseRobotRules robotRules = CACHE.get(protocol + ":" + host);    if (robotRules != null) {                return robotRules;    } else if (LOG.isTraceEnabled()) {        LOG.trace("cache miss " + url);    }    boolean cacheRule = true;    if (isWhiteListed(url)) {                        robotRules = EMPTY_RULES;                    } else {        try {            Text robotsUrl = new Text(new URL(url, "/robots.txt").toString());            ProtocolOutput output = ((Ftp) ftp).getProtocolOutput(robotsUrl, new CrawlDatum());            ProtocolStatus status = output.getStatus();            if (robotsTxtContent != null) {                robotsTxtContent.add(output.getContent());            }            if (status.getCode() == ProtocolStatus.SUCCESS) {                robotRules = parseRules(url.toString(), output.getContent().getContent(), CONTENT_TYPE, agentNames);            } else {                                robotRules = EMPTY_RULES;            }        } catch (Throwable t) {            if (LOG.isInfoEnabled()) {                            }                        cacheRule = false;            robotRules = EMPTY_RULES;        }    }    if (cacheRule)                CACHE.put(protocol + ":" + host, robotRules);    return robotRules;}
public void nutch_f2536_0(Configuration conf)
{    super.setConf(conf);}
public static void nutch_f2537_0(String[] args) throws Exception
{    Http http = new Http();    http.setConf(NutchConfiguration.create());    main(http, args);}
protected Response nutch_f2538_0(URL url, CrawlDatum datum, boolean redirect) throws ProtocolException, IOException
{    return new HttpResponse(this, url, datum);}
public URL nutch_f2539_0()
{    return url;}
public int nutch_f2540_0()
{    return code;}
public String nutch_f2541_0(String name)
{    return headers.get(name);}
public Metadata nutch_f2542_0()
{    return headers;}
public byte[] nutch_f2543_0()
{    return content;}
private void nutch_f2544_0(URL url) throws IOException
{    String page = HtmlUnitWebDriver.getHtmlPage(url.toString(), conf);    content = page.getBytes("UTF-8");}
private void nutch_f2545_0(InputStream in) throws HttpException, IOException
{        int contentLength = Integer.MAX_VALUE;    String contentLengthString = headers.get(Response.CONTENT_LENGTH);    if (contentLengthString != null) {        contentLengthString = contentLengthString.trim();        try {            if (!contentLengthString.isEmpty())                contentLength = Integer.parseInt(contentLengthString);        } catch (NumberFormatException e) {            throw new HttpException("bad content length: " + contentLengthString);        }    }    if (http.getMaxContent() >= 0 && contentLength > http.getMaxContent())                        contentLength = http.getMaxContent();    ByteArrayOutputStream out = new ByteArrayOutputStream(Http.BUFFER_SIZE);    byte[] bytes = new byte[Http.BUFFER_SIZE];    int length = 0;        if (contentLength == 0) {        content = new byte[0];        return;    }        int i = in.read(bytes);    while (i != -1) {        out.write(bytes, 0, i);        length += i;        if (length >= contentLength) {            break;        }        if ((length + Http.BUFFER_SIZE) > contentLength) {                                    i = in.read(bytes, 0, (contentLength - length));        } else {            i = in.read(bytes);        }    }    content = out.toByteArray();}
private void nutch_f2546_0(PushbackInputStream in, StringBuffer line) throws HttpException, IOException
{    boolean doneChunks = false;    int contentBytesRead = 0;    byte[] bytes = new byte[Http.BUFFER_SIZE];    ByteArrayOutputStream out = new ByteArrayOutputStream(Http.BUFFER_SIZE);    while (!doneChunks) {        if (Http.LOG.isTraceEnabled()) {            Http.LOG.trace("Http: starting chunk");        }        readLine(in, line, false);        String chunkLenStr;                        int pos = line.indexOf(";");        if (pos < 0) {            chunkLenStr = line.toString();        } else {            chunkLenStr = line.substring(0, pos);                        }        chunkLenStr = chunkLenStr.trim();        int chunkLen;        try {            chunkLen = Integer.parseInt(chunkLenStr, 16);        } catch (NumberFormatException e) {            throw new HttpException("bad chunk length: " + line.toString());        }        if (chunkLen == 0) {            doneChunks = true;            break;        }        if (http.getMaxContent() >= 0 && (contentBytesRead + chunkLen) > http.getMaxContent())            chunkLen = http.getMaxContent() - contentBytesRead;                int chunkBytesRead = 0;        while (chunkBytesRead < chunkLen) {            int toRead = (chunkLen - chunkBytesRead) < Http.BUFFER_SIZE ? (chunkLen - chunkBytesRead) : Http.BUFFER_SIZE;            int len = in.read(bytes, 0, toRead);            if (len == -1)                throw new HttpException("chunk eof after " + contentBytesRead + " bytes in successful chunks" + " and " + chunkBytesRead + " in current chunk");                                                            out.write(bytes, 0, len);            chunkBytesRead += len;        }        readLine(in, line, false);    }    if (!doneChunks) {        if (contentBytesRead != http.getMaxContent())            throw new HttpException("chunk eof: !doneChunk && didn't max out");        return;    }    content = out.toByteArray();    parseHeaders(in, line, null);}
private int nutch_f2547_0(PushbackInputStream in, StringBuffer line) throws IOException, HttpException
{    readLine(in, line, false);    int codeStart = line.indexOf(" ");    int codeEnd = line.indexOf(" ", codeStart + 1);        if (codeEnd == -1)        codeEnd = line.length();    int code;    try {        code = Integer.parseInt(line.substring(codeStart + 1, codeEnd));    } catch (NumberFormatException e) {        throw new HttpException("bad status line '" + line + "': " + e.getMessage(), e);    }    return code;}
private void nutch_f2548_0(StringBuffer line) throws IOException, HttpException
{        int colonIndex = line.indexOf(":");    if (colonIndex == -1) {        int i;        for (i = 0; i < line.length(); i++) if (!Character.isWhitespace(line.charAt(i)))            break;        if (i == line.length())            return;        throw new HttpException("No colon in header:" + line);    }    String key = line.substring(0, colonIndex);        int valueStart = colonIndex + 1;    while (valueStart < line.length()) {        int c = line.charAt(valueStart);        if (c != ' ' && c != '\t')            break;        valueStart++;    }    String value = line.substring(valueStart);    headers.set(key, value);}
private static int nutch_f2550_0(PushbackInputStream in, StringBuffer line, boolean allowContinuedLine) throws IOException
{    line.setLength(0);    for (int c = in.read(); c != -1; c = in.read()) {        switch(c) {            case '\r':                if (peek(in) == '\n') {                    in.read();                }            case '\n':                if (line.length() > 0) {                                        if (allowContinuedLine)                        switch(peek(in)) {                            case ' ':                            case                             '\t':                                in.read();                                continue;                        }                }                                return line.length();            default:                line.append((char) c);        }    }    throw new EOFException();}
private static int nutch_f2551_0(PushbackInputStream in) throws IOException
{    int value = in.read();    in.unread(value);    return value;}
public boolean nutch_f2552_0(X509Certificate[] certificates)
{    return true;}
public boolean nutch_f2553_0(X509Certificate[] certificates)
{    return true;}
public X509Certificate[] nutch_f2554_0()
{    return this.standardTrustManager.getAcceptedIssuers();}
public void nutch_f2555_0(X509Certificate[] arg0, String arg1) throws CertificateException
{}
public void nutch_f2556_0(X509Certificate[] arg0, String arg1) throws CertificateException
{}
public void nutch_f2557_0(Configuration conf)
{    super.setConf(conf);}
public static void nutch_f2558_0(String[] args) throws Exception
{    Http http = new Http();    http.setConf(NutchConfiguration.create());    main(http, args);}
protected Response nutch_f2559_0(URL url, CrawlDatum datum, boolean redirect) throws ProtocolException, IOException
{    return new HttpResponse(this, url, datum);}
public URL nutch_f2560_0()
{    return url;}
public int nutch_f2561_0()
{    return code;}
public String nutch_f2562_0(String name)
{    return headers.get(name);}
public Metadata nutch_f2563_0()
{    return headers;}
public byte[] nutch_f2564_0()
{    return content;}
private SSLSocket nutch_f2565_0(Socket socket, String sockHost, int sockPort) throws Exception
{    SSLSocketFactory factory;    if (http.isTlsCheckCertificates()) {        factory = (SSLSocketFactory) SSLSocketFactory.getDefault();    } else {        SSLContext sslContext = SSLContext.getInstance("TLS");        sslContext.init(null, new TrustManager[] { new DummyX509TrustManager(null) }, null);        factory = sslContext.getSocketFactory();    }    SSLSocket sslsocket = (SSLSocket) factory.createSocket(socket, sockHost, sockPort, true);    sslsocket.setUseClientMode(true);        Set<String> protocols = new HashSet<String>(Arrays.asList(sslsocket.getSupportedProtocols()));    Set<String> ciphers = new HashSet<String>(Arrays.asList(sslsocket.getSupportedCipherSuites()));        protocols.retainAll(http.getTlsPreferredProtocols());    ciphers.retainAll(http.getTlsPreferredCipherSuites());    sslsocket.setEnabledProtocols(protocols.toArray(new String[protocols.size()]));    sslsocket.setEnabledCipherSuites(ciphers.toArray(new String[ciphers.size()]));    return sslsocket;}
private void nutch_f2567_0(PushbackInputStream in, StringBuffer line) throws HttpException, IOException
{    boolean doneChunks = false;    int contentBytesRead = 0;    byte[] bytes = new byte[Http.BUFFER_SIZE];    ByteArrayOutputStream out = new ByteArrayOutputStream(Http.BUFFER_SIZE);    while (true) {        if (Http.LOG.isTraceEnabled()) {            Http.LOG.trace("Http: starting chunk");        }        readLine(in, line, false);        String chunkLenStr;                        int pos = line.indexOf(";");        if (pos < 0) {            chunkLenStr = line.toString();        } else {            chunkLenStr = line.substring(0, pos);                        }        chunkLenStr = chunkLenStr.trim();        int chunkLen;        try {            chunkLen = Integer.parseInt(chunkLenStr, 16);        } catch (NumberFormatException e) {            throw new HttpException("bad chunk length: " + line.toString());        }        if (chunkLen == 0) {            doneChunks = true;            break;        }        if (http.getMaxContent() >= 0 && (contentBytesRead + chunkLen) > http.getMaxContent()) {                        chunkLen = http.getMaxContent() - contentBytesRead;        }                int chunkBytesRead = 0;        while (chunkBytesRead < chunkLen) {            int toRead = (chunkLen - chunkBytesRead) < Http.BUFFER_SIZE ? (chunkLen - chunkBytesRead) : Http.BUFFER_SIZE;            int len = in.read(bytes, 0, toRead);            if (len == -1)                throw new HttpException("chunk eof after " + contentBytesRead + " bytes in successful chunks" + " and " + chunkBytesRead + " in current chunk");                                                            out.write(bytes, 0, len);            chunkBytesRead += len;        }        contentBytesRead += chunkBytesRead;        if (http.getMaxContent() >= 0 && contentBytesRead >= http.getMaxContent()) {            Http.LOG.trace("Http: content limit reached");            break;        }        readLine(in, line, false);    }    content = out.toByteArray();    if (!doneChunks) {                if (contentBytesRead != http.getMaxContent())            throw new HttpException("chunk eof: !doneChunk && didn't max out");        return;    }        parseHeaders(in, line, null);}
private int nutch_f2568_0(PushbackInputStream in, StringBuffer line, StringBuffer lineSeparator) throws IOException, HttpException
{    readLine(in, line, false, 2048, lineSeparator);    int codeStart = line.indexOf(" ");    int codeEnd;    int lineLength = line.length();        for (codeEnd = codeStart + 1; codeEnd < lineLength; codeEnd++) {        if (!Character.isDigit(line.charAt(codeEnd)))            break;            }    try {        return Integer.parseInt(line.substring(codeStart + 1, codeEnd));    } catch (NumberFormatException e) {        throw new HttpException("Bad status line, no HTTP response code: " + line, e);    }}
private static int nutch_f2571_0(PushbackInputStream in, StringBuffer line, boolean allowContinuedLine) throws IOException
{    return readLine(in, line, allowContinuedLine, Http.BUFFER_SIZE, null);}
private static int nutch_f2572_0(PushbackInputStream in, StringBuffer line, boolean allowContinuedLine, int maxBytes, StringBuffer lineSeparator) throws IOException
{    line.setLength(0);    int bytesRead = 0;    for (int c = in.read(); c != -1 && bytesRead < maxBytes; c = in.read(), bytesRead++) {        switch(c) {            case '\r':                if (lineSeparator != null) {                    lineSeparator.append((char) c);                }                if (peek(in) == '\n') {                    in.read();                    if (lineSeparator != null) {                        lineSeparator.append((char) c);                    }                }                        case '\n':                if (lineSeparator != null) {                    lineSeparator.append((char) c);                }                if (line.length() > 0) {                                        if (allowContinuedLine)                        switch(peek(in)) {                            case ' ':                            case                             '\t':                                in.read();                                if (lineSeparator != null) {                                    lineSeparator.replace(0, lineSeparator.length(), "");                                }                                continue;                        }                }                                return line.length();            default:                line.append((char) c);        }    }    if (bytesRead >= maxBytes) {        throw new IOException("Line exceeds max. buffer size: " + line.substring(0, Math.min(32, line.length())));    }    return line.length();}
private static int nutch_f2573_0(PushbackInputStream in) throws IOException
{    int value = in.read();    in.unread(value);    return value;}
public void nutch_f2574_0() throws Exception
{    conf = new Configuration();    conf.addResource("nutch-default.xml");    conf.addResource("nutch-site-test.xml");    conf.setBoolean("store.http.headers", true);    http = new Http();    http.setConf(conf);}
public void nutch_f2575_0() throws Exception
{    server.close();}
private void nutch_f2576_1(int port, String response) throws Exception
{    server = new ServerSocket();    server.bind(new InetSocketAddress("127.0.0.1", port));    Pattern requestPattern = Pattern.compile("(?i)^GET\\s+(\\S+)");    while (true) {                Socket socket = server.accept();                try (BufferedReader in = new BufferedReader(new InputStreamReader(socket.getInputStream(), StandardCharsets.UTF_8));            PrintWriter out = new PrintWriter(new OutputStreamWriter(socket.getOutputStream(), StandardCharsets.UTF_8), true)) {            String line;            while ((line = in.readLine()) != null) {                                if (line.trim().isEmpty()) {                    break;                }                Matcher m = requestPattern.matcher(line);                if (m.find()) {                                        if (!m.group(1).startsWith("/")) {                        response = "HTTP/1.1 400 Bad request\r\n\r\n";                    }                }            }                        out.print(response);        } catch (Exception e) {                    }    }}
private void nutch_f2577_1(String response) throws InterruptedException
{    Thread serverThread = new Thread(() -> {        try {            runServer(port, response);        } catch (Exception e) {                    }    });    serverThread.start();    Thread.sleep(50);}
private Response nutch_f2578_1(String page, int expectedCode) throws Exception
{    URL url = new URL("http", "127.0.0.1", port, page);        CrawlDatum crawlDatum = new CrawlDatum();    Response response = http.getResponse(url, crawlDatum, true);    assertEquals("HTTP Status Code for " + url, expectedCode, response.getCode());    return response;}
public void nutch_f2579_0() throws Exception
{    setUp();            launchServer(responseHeader + simpleContent);    fetchPage("/", 200);}
public void nutch_f2580_0() throws Exception
{    setUp();    launchServer(responseHeader + simpleContent);    fetchPage("?171", 200);}
public void nutch_f2581_0() throws Exception
{    setUp();    launchServer(responseHeader + "Content-Length: thousand\r\n" + simpleContent);    fetchPage("/", 200);}
public void nutch_f2582_0() throws Exception
{    setUp();    launchServer("HTTP/1.1 200: OK\r\n" + simpleContent);    fetchPage("/", 200);}
public void nutch_f2583_0() throws Exception
{    setUp();    launchServer(responseHeader + "Client-Transfer-Encoding: chunked\r\n" + simpleContent);    fetchPage("/", 200);}
public void nutch_f2584_0() throws Exception
{    setUp();    launchServer("HTTP/1.1 302 Found\r\nLocation: http://example.com/\r\n" + "Transfer-Encoding: chunked\r\n\r\nNot a valid chunk.");    Response fetched = fetchPage("/", 302);    assertNotNull("No redirect Location.", fetched.getHeader("Location"));    assertEquals("Wrong redirect Location.", "http://example.com/", fetched.getHeader("Location"));}
public void nutch_f2585_0() throws Exception
{    setUp();    String text = "This is a text containing non-ASCII characters: \u00e4\u00f6\u00fc\u00df";    launchServer(text);    Response fetched = fetchPage("/", 200);    assertEquals("Wrong text returned for response with no status line.", text, new String(fetched.getContent(), StandardCharsets.UTF_8));    server.close();    text = "<!DOCTYPE html>\n<html>\n<head>\n" + "<title>Testing no HTTP header </title>\n" + "<meta charset=\"utf-8\">\n" + "</head>\n<body>This is a text containing non-ASCII characters:" + "\u00e4\u00f6\u00fc\u00df</body>\n</html";    launchServer(text);    fetched = fetchPage("/", 200);    assertEquals("Wrong text returned for response with no status line.", text, new String(fetched.getContent(), StandardCharsets.UTF_8));}
public void nutch_f2586_1() throws Exception
{    setUp();    launchServer(responseHeader + "Set-Cookie: UserID=JohnDoe;\r\n  Max-Age=3600;\r\n  Version=1\r\n" + simpleContent);    Response fetched = fetchPage("/", 200);        assertNotNull("Failed to set multi-line \"Set-Cookie\" header.", fetched.getHeader("Set-Cookie"));    assertTrue("Failed to set multi-line \"Set-Cookie\" header.", fetched.getHeader("Set-Cookie").contains("Version=1"));}
public void nutch_f2587_0() throws Exception
{    setUp();    StringBuilder response = new StringBuilder();    response.append(responseHeader);    for (int i = 0; i < 80; i++) {        response.append("X-Custom-Header-");        for (int j = 0; j < 10000; j++) {            response.append('x');        }        response.append(": hello\r\n");    }    response.append("\r\n" + simpleContent);    launchServer(response.toString());        fetchPage("/", 200);}
public void nutch_f2588_0() throws Exception
{    setUp();    StringBuilder response = new StringBuilder();    response.append(responseHeader);    response.append("Content-Type: text/html\r\n");    response.append("Transfer-Encoding: chunked\r\n");        for (int i = 0; i < 80; i++) {        response.append(String.format("\r\n400\r\n%02x\r\n", i));        for (int j = 0; j < 1012; j++) {            response.append('x');        }        response.append(String.format("\r\n%02x\r\n", i));        response.append("\r\n");    }    response.append("\r\n0\r\n\r\n");    launchServer(response.toString());    Response fetched = fetchPage("/", 200);    assertEquals("Chunked content not truncated according to http.content.limit", 65536, fetched.getContent().length);}
public void nutch_f2589_0(boolean redirection) throws Exception
{    conf = new Configuration();    conf.addResource("nutch-default.xml");    conf.addResource("nutch-site-test.xml");    http = new Http();    http.setConf(conf);    server = new Server();    if (redirection) {        root = new Context(server, "/redirection", Context.SESSIONS);        root.setAttribute("newContextURL", "/redirect");    } else {        root = new Context(server, "/", Context.SESSIONS);    }    ServletHolder sh = new ServletHolder(org.apache.jasper.servlet.JspServlet.class);    root.addServlet(sh, "*.jsp");    root.setResourceBase(RES_DIR);}
public void nutch_f2590_0() throws Exception
{    server.stop();}
public void nutch_f2591_0() throws Exception
{    startServer(47504, false);    fetchPage("/basic-http.jsp", 200);    fetchPage("/redirect301.jsp", 301);    fetchPage("/redirect302.jsp", 302);    fetchPage("/nonexists.html", 404);    fetchPage("/brokenpage.jsp", 500);}
public void nutch_f2592_0() throws Exception
{        startServer(47503, true);    fetchPage("/redirection", 302);}
private void nutch_f2593_0(int portno, boolean redirection) throws Exception
{    port = portno;    setUp(redirection);    SelectChannelConnector connector = new SelectChannelConnector();    connector.setHost("127.0.0.1");    connector.setPort(port);    server.addConnector(connector);    server.start();}
private void nutch_f2594_0(String page, int expectedCode) throws Exception
{    URL url = new URL("http", "127.0.0.1", port, page);    CrawlDatum crawlDatum = new CrawlDatum();    Response response = http.getResponse(url, crawlDatum, true);    ProtocolOutput out = http.getProtocolOutput(new Text(url.toString()), crawlDatum);    Content content = out.getContent();    assertEquals("HTTP Status Code for " + url, expectedCode, response.getCode());    if (page.compareTo("/nonexists.html") != 0 && page.compareTo("/brokenpage.jsp") != 0 && page.compareTo("/redirection") != 0) {        assertEquals("ContentType " + url, "text/html", content.getContentType());    }}
private static SSLContext nutch_f2595_1()
{    try {        SSLContext context = SSLContext.getInstance("SSL");        context.init(null, new TrustManager[] { new DummyX509TrustManager(null) }, null);        return context;    } catch (Exception e) {        if (LOG.isErrorEnabled()) {                    }        throw new HttpClientError(e.toString());    }}
private SSLContext nutch_f2596_0()
{    if (this.sslcontext == null) {        this.sslcontext = createEasySSLContext();    }    return this.sslcontext;}
public Socket nutch_f2597_0(String host, int port, InetAddress clientHost, int clientPort) throws IOException, UnknownHostException
{    return getSSLContext().getSocketFactory().createSocket(host, port, clientHost, clientPort);}
public Socket nutch_f2598_0(final String host, final int port, final InetAddress localAddress, final int localPort, final HttpConnectionParams params) throws IOException, UnknownHostException, ConnectTimeoutException
{    if (params == null) {        throw new IllegalArgumentException("Parameters may not be null");    }    int timeout = params.getConnectionTimeout();    if (timeout == 0) {        return createSocket(host, port, localAddress, localPort);    } else {                return ControllerThreadSocketFactory.createSocket(this, host, port, localAddress, localPort, timeout);    }}
public Socket nutch_f2599_0(String host, int port) throws IOException, UnknownHostException
{    return getSSLContext().getSocketFactory().createSocket(host, port);}
public Socket nutch_f2600_0(Socket socket, String host, int port, boolean autoClose) throws IOException, UnknownHostException
{    return getSSLContext().getSocketFactory().createSocket(socket, host, port, autoClose);}
public boolean nutch_f2601_0(Object obj)
{    return ((obj != null) && obj.getClass().equals(DummySSLProtocolSocketFactory.class));}
public int nutch_f2602_0()
{    return DummySSLProtocolSocketFactory.class.hashCode();}
public boolean nutch_f2603_0(X509Certificate[] certificates)
{    return true;}
public boolean nutch_f2604_0(X509Certificate[] certificates)
{    return true;}
public X509Certificate[] nutch_f2605_0()
{    return this.standardTrustManager.getAcceptedIssuers();}
public void nutch_f2606_0(X509Certificate[] arg0, String arg1) throws CertificateException
{}
public void nutch_f2607_0(X509Certificate[] arg0, String arg1) throws CertificateException
{}
 static synchronized HttpClient nutch_f2608_0()
{    return client;}
public void nutch_f2609_1(Configuration conf)
{    super.setConf(conf);    Http.conf = conf;    this.maxThreadsTotal = conf.getInt("fetcher.threads.fetch", 10);    this.proxyUsername = conf.get("http.proxy.username", "");    this.proxyPassword = conf.get("http.proxy.password", "");    this.proxyRealm = conf.get("http.proxy.realm", "");    agentHost = conf.get("http.agent.host", "");    authFile = conf.get("http.auth.file", "");    configureClient();    try {        setCredentials();    } catch (Exception ex) {        if (LOG.isErrorEnabled()) {                                }    }}
public static void nutch_f2610_0(String[] args) throws Exception
{    Http http = new Http();    http.setConf(NutchConfiguration.create());    main(http, args);}
protected Response nutch_f2611_0(URL url, CrawlDatum datum, boolean redirect) throws ProtocolException, IOException
{    resolveCredentials(url);    return new HttpResponse(this, url, datum, redirect);}
private void nutch_f2612_0()
{        ProtocolSocketFactory factory;    if (tlsCheckCertificate) {        factory = new SSLProtocolSocketFactory();    } else {        factory = new DummySSLProtocolSocketFactory();    }    Protocol https = new Protocol("https", factory, 443);    Protocol.registerProtocol("https", https);    HttpConnectionManagerParams params = connectionManager.getParams();    params.setConnectionTimeout(timeout);    params.setSoTimeout(timeout);    params.setSendBufferSize(BUFFER_SIZE);    params.setReceiveBufferSize(BUFFER_SIZE);                    params.setMaxTotalConnections(conf.getInt("mapreduce.tasktracker.map.tasks.maximum", 5) * conf.getInt("fetcher.threads.fetch", maxThreadsTotal));                params.setDefaultMaxConnectionsPerHost(conf.getInt("fetcher.threads.fetch", maxThreadsTotal));                client.getParams().setConnectionManagerTimeout(timeout);    HostConfiguration hostConf = client.getHostConfiguration();    ArrayList<Header> headers = new ArrayList<Header>();        if (!acceptLanguage.isEmpty()) {        headers.add(new Header("Accept-Language", acceptLanguage));    }    if (!acceptCharset.isEmpty()) {        headers.add(new Header("Accept-Charset", acceptCharset));    }    if (!accept.isEmpty()) {        headers.add(new Header("Accept", accept));    }        headers.add(new Header("Accept-Encoding", "x-gzip, gzip, deflate"));    hostConf.getParams().setParameter("http.default-headers", headers);        if (useProxy) {        hostConf.setProxy(proxyHost, proxyPort);        if (proxyUsername.length() > 0) {            AuthScope proxyAuthScope = getAuthScope(this.proxyHost, this.proxyPort, this.proxyRealm);            NTCredentials proxyCredentials = new NTCredentials(this.proxyUsername, this.proxyPassword, Http.agentHost, this.proxyRealm);            client.getState().setProxyCredentials(proxyAuthScope, proxyCredentials);        }    }}
private static HttpFormAuthConfigurer nutch_f2614_1(Element credElement, String authMethod)
{    if ("formAuth".equals(authMethod)) {        HttpFormAuthConfigurer formConfigurer = new HttpFormAuthConfigurer();        String str = credElement.getAttribute("loginUrl");        if (StringUtils.isNotBlank(str)) {            formConfigurer.setLoginUrl(str.trim());        } else {            throw new IllegalArgumentException("Must set loginUrl.");        }        str = credElement.getAttribute("loginFormId");        if (StringUtils.isNotBlank(str)) {            formConfigurer.setLoginFormId(str.trim());        } else {            throw new IllegalArgumentException("Must set loginFormId.");        }        str = credElement.getAttribute("loginRedirect");        if (StringUtils.isNotBlank(str)) {            formConfigurer.setLoginRedirect(Boolean.parseBoolean(str));        }        NodeList nodeList = credElement.getChildNodes();        for (int j = 0; j < nodeList.getLength(); j++) {            Node node = nodeList.item(j);            if (!(node instanceof Element))                continue;            Element element = (Element) node;            if ("loginPostData".equals(element.getTagName())) {                Map<String, String> loginPostData = new HashMap<String, String>();                NodeList childNodes = element.getChildNodes();                for (int k = 0; k < childNodes.getLength(); k++) {                    Node fieldNode = childNodes.item(k);                    if (!(fieldNode instanceof Element))                        continue;                    Element fieldElement = (Element) fieldNode;                    String name = fieldElement.getAttribute("name");                    String value = fieldElement.getAttribute("value");                    loginPostData.put(name, value);                }                formConfigurer.setLoginPostData(loginPostData);            } else if ("additionalPostHeaders".equals(element.getTagName())) {                Map<String, String> additionalPostHeaders = new HashMap<String, String>();                NodeList childNodes = element.getChildNodes();                for (int k = 0; k < childNodes.getLength(); k++) {                    Node fieldNode = childNodes.item(k);                    if (!(fieldNode instanceof Element))                        continue;                    Element fieldElement = (Element) fieldNode;                    String name = fieldElement.getAttribute("name");                    String value = fieldElement.getAttribute("value");                    additionalPostHeaders.put(name, value);                }                formConfigurer.setAdditionalPostHeaders(additionalPostHeaders);            } else if ("removedFormFields".equals(element.getTagName())) {                Set<String> removedFormFields = new HashSet<String>();                NodeList childNodes = element.getChildNodes();                for (int k = 0; k < childNodes.getLength(); k++) {                    Node fieldNode = childNodes.item(k);                    if (!(fieldNode instanceof Element))                        continue;                    Element fieldElement = (Element) fieldNode;                    String name = fieldElement.getAttribute("name");                    removedFormFields.add(name);                }                formConfigurer.setRemovedFormFields(removedFormFields);            } else if ("loginCookie".equals(element.getTagName())) {                                                NodeList childNodes = element.getChildNodes();                for (int k = 0; k < childNodes.getLength(); k++) {                    Node fieldNode = childNodes.item(k);                    if (!(fieldNode instanceof Element))                        continue;                    Element fieldElement = (Element) fieldNode;                    if ("policy".equals(fieldElement.getTagName())) {                        String policy = fieldElement.getTextContent();                        formConfigurer.setCookiePolicy(policy);                                            }                }            }        }        return formConfigurer;    } else {        throw new IllegalArgumentException("Unsupported authMethod: " + authMethod);    }}
private void nutch_f2615_0(URL url)
{    if (formConfigurer != null) {        HttpFormAuthentication formAuther = new HttpFormAuthentication(formConfigurer, client, this);        try {            formAuther.login();        } catch (Exception e) {            throw new RuntimeException(e);        }        return;    }    if (defaultUsername != null && defaultUsername.length() > 0) {        int port = url.getPort();        if (port == -1) {            if ("https".equals(url.getProtocol()))                port = 443;            else                port = 80;        }        AuthScope scope = new AuthScope(url.getHost(), port);        if (client.getState().getCredentials(scope) != null) {            if (LOG.isTraceEnabled())                LOG.trace("Pre-configured credentials with scope - host: " + url.getHost() + "; port: " + port + "; found for url: " + url);                        return;        }        if (LOG.isTraceEnabled())            LOG.trace("Pre-configured credentials with scope -  host: " + url.getHost() + "; port: " + port + "; not found for url: " + url);        AuthScope serverAuthScope = getAuthScope(url.getHost(), port, defaultRealm, defaultScheme);        NTCredentials serverCredentials = new NTCredentials(defaultUsername, defaultPassword, agentHost, defaultRealm);        client.getState().setCredentials(serverAuthScope, serverCredentials);    }}
private static AuthScope nutch_f2616_0(String host, int port, String realm, String scheme)
{    if (host.length() == 0)        host = null;    if (port < 0)        port = -1;    if (realm.length() == 0)        realm = null;    if (scheme.length() == 0)        scheme = null;    return new AuthScope(host, port, realm, scheme);}
private static AuthScope nutch_f2617_0(String host, int port, String realm)
{    return getAuthScope(host, port, realm, "");}
public void nutch_f2618_0(Configuration conf)
{    this.conf = conf;}
public Configuration nutch_f2619_0()
{    return conf;}
public HttpAuthentication nutch_f2620_1(Metadata header)
{    if (header == null)        return null;    try {        Collection<String> challenge = new ArrayList<String>();        challenge.add(header.get(WWW_AUTHENTICATE));        for (String challengeString : challenge) {            if (challengeString.equals("NTLM"))                challengeString = "Basic realm=techweb";            if (LOG.isTraceEnabled())                LOG.trace("Checking challengeString=" + challengeString);            HttpAuthentication auth = HttpBasicAuthentication.getAuthentication(challengeString, conf);            if (auth != null)                return auth;                }    } catch (Exception e) {            }    return null;}
public void nutch_f2621_0(Configuration conf)
{    this.conf = conf;}
public Configuration nutch_f2622_0()
{    return this.conf;}
public List<String> nutch_f2623_0()
{    return credentials;}
public String nutch_f2624_0()
{    return realm;}
public static HttpBasicAuthentication nutch_f2625_0(String challenge, Configuration conf)
{    if (challenge == null)        return null;    Matcher basicMatcher = basic.matcher(challenge);    if (basicMatcher.matches()) {        String realm = basicMatcher.group(1);        Object auth = authMap.get(realm);        if (auth == null) {            HttpBasicAuthentication newAuth = null;            try {                newAuth = new HttpBasicAuthentication(realm, conf);            } catch (HttpAuthenticationException hae) {                if (LOG.isTraceEnabled()) {                    LOG.trace("HttpBasicAuthentication failed for " + challenge);                }            }            authMap.put(realm, newAuth);            return newAuth;        } else {            return (HttpBasicAuthentication) auth;        }    }    return null;}
public static final Pattern nutch_f2626_0()
{    return basic;}
public String nutch_f2627_0()
{    return loginUrl;}
public HttpFormAuthConfigurer nutch_f2628_0(String loginUrl)
{    this.loginUrl = loginUrl;    return this;}
public String nutch_f2629_0()
{    return loginFormId;}
public HttpFormAuthConfigurer nutch_f2630_0(String loginForm)
{    this.loginFormId = loginForm;    return this;}
public Map<String, String> nutch_f2631_0()
{    return loginPostData == null ? new HashMap<String, String>() : loginPostData;}
public HttpFormAuthConfigurer nutch_f2632_0(Map<String, String> loginPostData)
{    this.loginPostData = loginPostData;    return this;}
public Map<String, String> nutch_f2633_0()
{    return additionalPostHeaders == null ? new HashMap<String, String>() : additionalPostHeaders;}
public HttpFormAuthConfigurer nutch_f2634_0(Map<String, String> additionalPostHeaders)
{    this.additionalPostHeaders = additionalPostHeaders;    return this;}
public boolean nutch_f2635_0()
{    return loginRedirect;}
public HttpFormAuthConfigurer nutch_f2636_0(boolean redirect)
{    this.loginRedirect = redirect;    return this;}
public Set<String> nutch_f2637_0()
{    return removedFormFields == null ? new HashSet<String>() : removedFormFields;}
public HttpFormAuthConfigurer nutch_f2638_0(Set<String> removedFormFields)
{    this.removedFormFields = removedFormFields;    return this;}
public void nutch_f2639_0(String policy)
{    this.cookiePolicy = policy;}
public String nutch_f2640_0()
{    return this.cookiePolicy;}
public void nutch_f2641_0() throws Exception
{        CookieHandler.setDefault(new CookieManager());    String pageContent = httpGetPageContent(authConfigurer.getLoginUrl());    List<NameValuePair> params = getLoginFormParams(pageContent);    sendPost(authConfigurer.getLoginUrl(), params);}
private void nutch_f2642_1(String url, List<NameValuePair> params) throws Exception
{    PostMethod post = null;    try {        if (authConfigurer.isLoginRedirect()) {            post = new PostMethod(url) {                @Override                public boolean getFollowRedirects() {                    return true;                }            };        } else {            post = new PostMethod(url);        }                                        setLoginHeader(post);                        this.setCookieParams(authConfigurer, post.getParams());        post.addParameters(params.toArray(new NameValuePair[0]));        int rspCode = client.executeMethod(post);        if (LOG.isDebugEnabled()) {                                                            for (Header header : post.getRequestHeaders()) {                            }        }        String rst = IOUtils.toString(post.getResponseBodyAsStream());            } finally {        if (post != null) {            post.releaseConnection();        }    }}
public boolean nutch_f2643_0()
{    return true;}
private void nutch_f2644_1(HttpFormAuthConfigurer formConfigurer, HttpMethodParams params) throws NoSuchFieldException, SecurityException, IllegalArgumentException, IllegalAccessException
{        if (formConfigurer.getCookiePolicy() != null) {        String policy = formConfigurer.getCookiePolicy();        Object p = FieldUtils.readDeclaredStaticField(CookiePolicy.class, policy);        if (null != p) {                        params.setParameter(HttpMethodParams.COOKIE_POLICY, p);        }    }}
private void nutch_f2645_0(PostMethod post)
{    Map<String, String> headers = new HashMap<String, String>();    headers.putAll(defaultLoginHeaders);        headers.putAll(authConfigurer.getAdditionalPostHeaders());    for (Entry<String, String> entry : headers.entrySet()) {        post.addRequestHeader(entry.getKey(), entry.getValue());    }    post.addRequestHeader("Cookie", getCookies());}
private String nutch_f2646_0(String url) throws IOException
{    GetMethod get = new GetMethod(url);    try {        for (Entry<String, String> entry : authConfigurer.getAdditionalPostHeaders().entrySet()) {            get.addRequestHeader(entry.getKey(), entry.getValue());        }        client.executeMethod(get);        Header cookieHeader = get.getResponseHeader("Set-Cookie");        if (cookieHeader != null) {            setCookies(cookieHeader.getValue());        }        String rst = IOUtils.toString(get.getResponseBodyAsStream());        return rst;    } finally {        get.releaseConnection();    }}
private List<NameValuePair> nutch_f2647_1(String pageContent) throws UnsupportedEncodingException
{    List<NameValuePair> params = new ArrayList<NameValuePair>();    Document doc = Jsoup.parse(pageContent);    Element loginform = doc.getElementById(authConfigurer.getLoginFormId());    if (loginform == null) {                loginform = doc.select("form[name=" + authConfigurer.getLoginFormId() + "]").first();        if (loginform == null) {                        throw new IllegalArgumentException("No form exists: " + authConfigurer.getLoginFormId());        }    }    Elements inputElements = loginform.getElementsByTag("input");        for (Element inputElement : inputElements) {        String key = inputElement.attr("name");        String value = inputElement.attr("value");        if (authConfigurer.getLoginPostData().containsKey(key) || authConfigurer.getRemovedFormFields().contains(key)) {                        continue;        }        params.add(new NameValuePair(key, value));    }        for (Entry<String, String> entry : authConfigurer.getLoginPostData().entrySet()) {        params.add(new NameValuePair(entry.getKey(), entry.getValue()));    }    return params;}
public String nutch_f2648_0()
{    return cookies;}
public void nutch_f2649_0(String cookies)
{    this.cookies = cookies;}
public boolean nutch_f2650_0()
{    return authConfigurer.isLoginRedirect();}
public void nutch_f2651_0(boolean redirect)
{    this.authConfigurer.setLoginRedirect(redirect);}
public URL nutch_f2652_0()
{    return url;}
public int nutch_f2653_0()
{    return code;}
public String nutch_f2654_0(String name)
{    return headers.get(name);}
public Metadata nutch_f2655_0()
{    return headers;}
public byte[] nutch_f2656_0()
{    return content;}
public void nutch_f2657_0() throws Exception
{    ContextHandler context = new ContextHandler();    context.setContextPath("/");    context.setResourceBase(RES_DIR);    ServletHandler sh = new ServletHandler();    sh.addServletWithMapping("org.apache.jasper.servlet.JspServlet", "*.jsp");    context.addHandler(sh);    context.addHandler(new SessionHandler());    server = new Server();    server.addHandler(context);    conf = new Configuration();    conf.addResource("nutch-default.xml");    conf.addResource("nutch-site-test.xml");    http = new Http();    http.setConf(conf);}
public void nutch_f2658_0() throws Exception
{    server.stop();    for (int i = 0; i < 5; i++) {        if (!server.isStopped()) {            Thread.sleep(1000);        }    }}
public void nutch_f2659_0() throws Exception
{    startServer(47500);    fetchPage("/cookies.jsp", 200);    fetchPage("/cookies.jsp?cookie=yes", 200);}
public void nutch_f2660_0() throws Exception
{    startServer(47500);    fetchPage("/noauth.jsp", 200);}
public void nutch_f2661_0() throws Exception
{    startServer(47502);    fetchPage("/basic.jsp", 200);}
public void nutch_f2662_0() throws Exception
{    startServer(47500);    fetchPage("/basic.jsp", 200);    fetchPage("/basic.jsp?case=1", 200);    fetchPage("/basic.jsp?case=2", 200);    server.start();}
public void nutch_f2663_0() throws Exception
{    startServer(47501);    fetchPage("/basic.jsp", 200);    fetchPage("/basic.jsp?case=1", 401);    fetchPage("/basic.jsp?case=2", 401);}
public void nutch_f2664_0() throws Exception
{    startServer(47500);    fetchPage("/digest.jsp", 200);}
public void nutch_f2665_0() throws Exception
{    startServer(47501);    fetchPage("/ntlm.jsp", 200);}
private void nutch_f2666_0(int portno) throws Exception
{    SocketConnector listener = new SocketConnector();    listener.setHost("127.0.0.1");    server.addConnector(listener);    for (int p = portno; p < portno + 10; p++) {        port = portno;        listener.setPort(port);        try {            server.start();            break;        } catch (Exception e) {            if (p == portno + 9) {                throw e;            }        }    }}
private void nutch_f2667_0(String page, int expectedCode) throws Exception
{    URL url = new URL("http", "127.0.0.1", port, page);    Response response = null;    response = http.getResponse(url, new CrawlDatum(), true);    int code = response.getCode();    Assert.assertEquals("HTTP Status Code for " + url, expectedCode, code);}
public String nutch_f2668_1(WebDriver driver)
{        String accumulatedData = "";    try {                JavascriptExecutor jsx = (JavascriptExecutor) driver;        jsx.executeScript("document.body.innerHTML=document.body.innerHTML " + accumulatedData + ";");    } catch (Exception e) {            }    return accumulatedData;}
public boolean nutch_f2669_0(String URL)
{    return true;}
public String nutch_f2670_1(WebDriver driver)
{    String accumulatedData = "";    try {        driver.findElement(By.tagName("body")).getAttribute("innerHTML");        Configuration conf = NutchConfiguration.create();        new WebDriverWait(driver, conf.getLong("libselenium.page.load.delay", 3));        List<WebElement> atags = driver.findElements(By.tagName("a"));        int numberofajaxlinks = atags.size();        for (int i = 0; i < numberofajaxlinks; i++) {            if (atags.get(i).getAttribute("href") != null && atags.get(i).getAttribute("href").equals("javascript:void(null);")) {                atags.get(i).click();                if (i == numberofajaxlinks - 1) {                                        JavascriptExecutor jsx = (JavascriptExecutor) driver;                    jsx.executeScript("document.body.innerHTML=document.body.innerHTML " + accumulatedData + ";");                    continue;                }                accumulatedData += driver.findElement(By.tagName("body")).getAttribute("innerHTML");                                driver.navigate().refresh();                new WebDriverWait(driver, conf.getLong("libselenium.page.load.delay", 3));                atags = driver.findElements(By.tagName("a"));            }        }    } catch (Exception e) {            }    return accumulatedData;}
public boolean nutch_f2671_0(String URL)
{    return true;}
public String nutch_f2672_0(WebDriver driver)
{    return null;}
public boolean nutch_f2673_0(String url)
{    return true;}
public void nutch_f2674_0(Configuration conf)
{    super.setConf(conf);}
public static void nutch_f2675_0(String[] args) throws Exception
{    Http http = new Http();    http.setConf(NutchConfiguration.create());    main(http, args);}
protected Response nutch_f2676_0(URL url, CrawlDatum datum, boolean redirect) throws ProtocolException, IOException
{    return new HttpResponse(this, url, datum);}
public URL nutch_f2677_0()
{    return url;}
public int nutch_f2678_0()
{    return code;}
public String nutch_f2679_0(String name)
{    return headers.get(name);}
public Metadata nutch_f2680_0()
{    return headers;}
public byte[] nutch_f2681_0()
{    return content;}
private void nutch_f2683_0(URL url) throws IOException
{    if (handlers == null)        loadSeleniumHandlers();    String processedPage = "";    for (InteractiveSeleniumHandler handler : this.handlers) {        if (!handler.shouldProcessURL(url.toString())) {            continue;        }        WebDriver driver = HttpWebClient.getDriverForPage(url.toString(), conf);        processedPage += handler.processDriver(driver);        HttpWebClient.cleanUpDriver(driver);    }    content = processedPage.getBytes("UTF-8");}
private int nutch_f2684_0(PushbackInputStream in, StringBuffer line) throws IOException, HttpException
{    readLine(in, line, false);    int codeStart = line.indexOf(" ");    int codeEnd = line.indexOf(" ", codeStart + 1);        if (codeEnd == -1)        codeEnd = line.length();    int code;    try {        code = Integer.parseInt(line.substring(codeStart + 1, codeEnd));    } catch (NumberFormatException e) {        throw new HttpException("bad status line '" + line + "': " + e.getMessage(), e);    }    return code;}
private void nutch_f2685_0(StringBuffer line) throws IOException, HttpException
{        int colonIndex = line.indexOf(":");    if (colonIndex == -1) {        int i;        for (i = 0; i < line.length(); i++) if (!Character.isWhitespace(line.charAt(i)))            break;        if (i == line.length())            return;        throw new HttpException("No colon in header:" + line);    }    String key = line.substring(0, colonIndex);        int valueStart = colonIndex + 1;    while (valueStart < line.length()) {        int c = line.charAt(valueStart);        if (c != ' ' && c != '\t')            break;        valueStart++;    }    String value = line.substring(valueStart);    headers.set(key, value);}
private static int nutch_f2687_0(PushbackInputStream in, StringBuffer line, boolean allowContinuedLine) throws IOException
{    line.setLength(0);    for (int c = in.read(); c != -1; c = in.read()) {        switch(c) {            case '\r':                if (peek(in) == '\n') {                    in.read();                }            case '\n':                if (line.length() > 0) {                                        if (allowContinuedLine)                        switch(peek(in)) {                            case ' ':                            case                             '\t':                                in.read();                                continue;                        }                }                                return line.length();            default:                line.append((char) c);        }    }    throw new EOFException();}
private static int nutch_f2688_0(PushbackInputStream in) throws IOException
{    int value = in.read();    in.unread(value);    return value;}
public void nutch_f2689_0(java.security.cert.X509Certificate[] chain, String authType) throws CertificateException
{}
public void nutch_f2690_0(java.security.cert.X509Certificate[] chain, String authType) throws CertificateException
{}
public java.security.cert.X509Certificate[] nutch_f2691_0()
{    return new java.security.cert.X509Certificate[] {};}
public void nutch_f2692_1(Configuration conf)
{    super.setConf(conf);        List<okhttp3.Protocol> protocols = new ArrayList<>();    if (useHttp2) {        protocols.add(okhttp3.Protocol.HTTP_2);    }    protocols.add(okhttp3.Protocol.HTTP_1_1);    okhttp3.OkHttpClient.Builder builder = new OkHttpClient.Builder().protocols(    protocols).retryOnConnectionFailure(    true).followRedirects(    false).connectTimeout(timeout, TimeUnit.MILLISECONDS).writeTimeout(timeout, TimeUnit.MILLISECONDS).readTimeout(timeout, TimeUnit.MILLISECONDS);    if (!tlsCheckCertificate) {        builder.sslSocketFactory(trustAllSslSocketFactory, (X509TrustManager) trustAllCerts[0]);        builder.hostnameVerifier(new HostnameVerifier() {            @Override            public boolean verify(String hostname, SSLSession session) {                return true;            }        });    }    if (!accept.isEmpty()) {        getCustomRequestHeaders().add(new String[] { "Accept", accept });    }    if (!acceptLanguage.isEmpty()) {        getCustomRequestHeaders().add(new String[] { "Accept-Language", acceptLanguage });    }    if (!acceptCharset.isEmpty()) {        getCustomRequestHeaders().add(new String[] { "Accept-Charset", acceptCharset });    }    if (useProxy) {        Proxy proxy = new Proxy(proxyType, new InetSocketAddress(proxyHost, proxyPort));        String proxyUsername = conf.get("http.proxy.username");        if (proxyUsername == null) {            ProxySelector selector = new ProxySelector() {                @SuppressWarnings("serial")                private final List<Proxy> noProxyList = new ArrayList<Proxy>() {                    {                        add(Proxy.NO_PROXY);                    }                };                @SuppressWarnings("serial")                private final List<Proxy> proxyList = new ArrayList<Proxy>() {                    {                        add(proxy);                    }                };                @Override                public List<Proxy> select(URI uri) {                    if (useProxy(uri)) {                        return proxyList;                    }                    return noProxyList;                }                @Override                public void connectFailed(URI uri, SocketAddress sa, IOException ioe) {                                    }            };            builder.proxySelector(selector);        } else {            /*         * NOTE: the proxy exceptions list does NOT work with proxy         * username/password because an okhttp3 bug         * (https://github.com/square/okhttp/issues/3995) when using the         * ProxySelector class with proxy auth. If a proxy username is present,         * the configured proxy will be used for ALL requests.         */            if (proxyException.size() > 0) {                            }            builder.proxy(proxy);            String proxyPassword = conf.get("http.proxy.password");            Authenticator proxyAuthenticator = new Authenticator() {                @Override                public Request authenticate(okhttp3.Route route, okhttp3.Response response) throws IOException {                    String credential = okhttp3.Credentials.basic(proxyUsername, proxyPassword);                    return response.request().newBuilder().header("Proxy-Authorization", credential).build();                }            };            builder.proxyAuthenticator(proxyAuthenticator);        }    }    if (storeIPAddress || storeHttpHeaders || storeHttpRequest) {        builder.addNetworkInterceptor(new HTTPHeadersInterceptor());    }    client = builder.build();}
public boolean nutch_f2693_0(String hostname, SSLSession session)
{    return true;}
public List<Proxy> nutch_f2694_0(URI uri)
{    if (useProxy(uri)) {        return proxyList;    }    return noProxyList;}
public void nutch_f2695_1(URI uri, SocketAddress sa, IOException ioe)
{    }
public Request nutch_f2696_0(okhttp3.Route route, okhttp3.Response response) throws IOException
{    String credential = okhttp3.Credentials.basic(proxyUsername, proxyPassword);    return response.request().newBuilder().header("Proxy-Authorization", credential).build();}
public okhttp3.Response nutch_f2697_0(Interceptor.Chain chain) throws IOException
{    Connection connection = chain.connection();    String ipAddress = null;    if (storeIPAddress) {        InetAddress address = connection.socket().getInetAddress();        ipAddress = address.getHostAddress();    }    Request request = chain.request();    okhttp3.Response response = chain.proceed(request);    String httpProtocol = response.protocol().toString().toUpperCase(Locale.ROOT);    if (useHttp2 && "H2".equals(httpProtocol)) {                httpProtocol = "HTTP/2";    }    StringBuilder requestverbatim = null;    StringBuilder responseverbatim = null;    if (storeHttpRequest) {        requestverbatim = new StringBuilder();        requestverbatim.append(request.method()).append(' ');        requestverbatim.append(request.url().encodedPath());        String query = request.url().encodedQuery();        if (query != null) {            requestverbatim.append('?').append(query);        }        requestverbatim.append(' ').append(httpProtocol).append("\r\n");        Headers headers = request.headers();        for (int i = 0, size = headers.size(); i < size; i++) {            String key = headers.name(i);            String value = headers.value(i);            requestverbatim.append(key).append(": ").append(value).append("\r\n");        }        requestverbatim.append("\r\n");    }    if (storeHttpHeaders) {        responseverbatim = new StringBuilder();        responseverbatim.append(httpProtocol).append(' ').append(response.code());        if (!response.message().isEmpty()) {            responseverbatim.append(' ').append(response.message());        }        responseverbatim.append("\r\n");        Headers headers = response.headers();        for (int i = 0, size = headers.size(); i < size; i++) {            String key = headers.name(i);            String value = headers.value(i);            responseverbatim.append(key).append(": ").append(value).append("\r\n");        }        responseverbatim.append("\r\n");    }    okhttp3.Response.Builder builder = response.newBuilder();    if (ipAddress != null) {        builder = builder.header(Response.IP_ADDRESS, ipAddress);    }    if (requestverbatim != null) {        byte[] encodedBytesRequest = Base64.getEncoder().encode(requestverbatim.toString().getBytes());        builder = builder.header(Response.REQUEST, new String(encodedBytesRequest));    }    if (responseverbatim != null) {        byte[] encodedBytesResponse = Base64.getEncoder().encode(responseverbatim.toString().getBytes());        builder = builder.header(Response.RESPONSE_HEADERS, new String(encodedBytesResponse));    }        return builder.build();}
protected List<String[]> nutch_f2698_0()
{    return customRequestHeaders;}
protected OkHttpClient nutch_f2699_0()
{    return client;}
protected Response nutch_f2700_0(URL url, CrawlDatum datum, boolean redirect) throws ProtocolException, IOException
{    return new OkHttpResponse(this, url, datum);}
public static void nutch_f2701_0(String[] args) throws Exception
{    OkHttp okhttp = new OkHttp();    okhttp.setConf(NutchConfiguration.create());    main(okhttp, args);}
public void nutch_f2702_0(TruncatedContentReason val)
{    value = val;}
public TruncatedContentReason nutch_f2703_0()
{    return value;}
public boolean nutch_f2704_0()
{    return value != TruncatedContentReason.NOT_TRUNCATED;}
private final byte[] nutch_f2705_1(final ResponseBody responseBody, TruncatedContent truncated, int maxContent, int maxDuration, boolean partialAsTruncated) throws IOException
{    if (responseBody == null) {        return new byte[] {};    }    long endDueFor = -1;    if (maxDuration != -1) {        endDueFor = System.currentTimeMillis() + (maxDuration * 1000);    }    int maxContentBytes = Integer.MAX_VALUE;    if (maxContent >= 0) {        maxContentBytes = Math.min(maxContentBytes, maxContent);    }    BufferedSource source = responseBody.source();    int bytesRequested = 0;    int bufferGrowStepBytes = 8192;    while (source.buffer().size() <= maxContentBytes) {        bytesRequested += Math.min(bufferGrowStepBytes, /*           * request one byte more than required to reliably detect truncated           * content, but beware of integer overflows           */        (maxContentBytes == Integer.MAX_VALUE ? maxContentBytes : (1 + maxContentBytes)) - bytesRequested);        boolean success = false;        try {            success = source.request(bytesRequested);        } catch (IOException e) {            if (partialAsTruncated && source.buffer().size() > 0) {                                truncated.setReason(TruncatedContentReason.DISCONNECT);                            } else {                throw e;            }        }        if (LOG.isDebugEnabled()) {                    }        if (!success) {                        break;        }        if (endDueFor != -1 && endDueFor <= System.currentTimeMillis()) {                        truncated.setReason(TruncatedContentReason.TIME);            break;        }        if (source.buffer().size() >= maxContentBytes) {                    }                bytesRequested = (int) source.buffer().size();    }    int bytesBuffered = (int) source.buffer().size();    int bytesToCopy = bytesBuffered;    if (maxContent >= 0 && bytesToCopy > maxContent) {                truncated.setReason(TruncatedContentReason.LENGTH);        bytesToCopy = maxContentBytes;    }    byte[] arr = new byte[bytesToCopy];    source.buffer().readFully(arr);    if (LOG.isDebugEnabled()) {            }    return arr;}
public URL nutch_f2706_0()
{    return url;}
public int nutch_f2707_0()
{    return code;}
public String nutch_f2708_0(String name)
{    return headers.get(name);}
public Metadata nutch_f2709_0()
{    return headers;}
public byte[] nutch_f2710_0()
{    return content;}
public void nutch_f2711_0() throws Exception
{    conf = NutchConfiguration.create();    conf.addResource("nutch-default.xml");            conf.addResource("nutch-site-test.xml");    conf.setBoolean("store.http.headers", true);    http = new ProtocolFactory(conf).getProtocolById("org.apache.nutch.protocol.okhttp.OkHttp");}
public void nutch_f2712_0() throws Exception
{    server.close();}
public static String nutch_f2713_0(ProtocolOutput response)
{    return response.getContent().getMetadata().get(Response.RESPONSE_HEADERS);}
public static String nutch_f2714_0(ProtocolOutput response, String header)
{    for (String line : getHeaders(response).split("\r\n")) {        String[] parts = line.split(": ", 1);        if (parts[0].equals(header)) {            return parts[1];        }    }    return null;}
private void nutch_f2715_1(int port, byte[] response) throws Exception
{    server = new ServerSocket();    server.bind(new InetSocketAddress("127.0.0.1", port));    Pattern requestPattern = Pattern.compile("(?i)^GET\\s+(\\S+)");    while (true) {                Socket socket = server.accept();                try (BufferedReader in = new BufferedReader(new InputStreamReader(socket.getInputStream(), StandardCharsets.UTF_8))) {            String line;            while ((line = in.readLine()) != null) {                                if (line.trim().isEmpty()) {                    break;                }                Matcher m = requestPattern.matcher(line);                if (m.find()) {                                        if (!m.group(1).startsWith("/")) {                        response = "HTTP/1.1 400 Bad request\r\n\r\n".getBytes(StandardCharsets.UTF_8);                    }                }            }            socket.getOutputStream().write(response);        } catch (Exception e) {                    }    }}
private void nutch_f2716_0(String response) throws InterruptedException
{    launchServer(response.getBytes(StandardCharsets.UTF_8));}
private void nutch_f2717_1(byte[] response) throws InterruptedException
{    Thread serverThread = new Thread(() -> {        try {            runServer(port, response);        } catch (Exception e) {                    }    });    serverThread.start();    Thread.sleep(50);}
private ProtocolOutput nutch_f2718_1(String page, int expectedCode) throws MalformedURLException
{    URL url = new URL("http", "127.0.0.1", port, page);        CrawlDatum crawlDatum = new CrawlDatum();    ProtocolOutput out = http.getProtocolOutput(new Text(url.toString()), crawlDatum);    int httpStatusCode = -1;    if (crawlDatum.getMetaData().containsKey(Nutch.PROTOCOL_STATUS_CODE_KEY)) {        httpStatusCode = Integer.parseInt(crawlDatum.getMetaData().get(Nutch.PROTOCOL_STATUS_CODE_KEY).toString());    }    assertEquals("HTTP Status Code for " + url, expectedCode, httpStatusCode);    return out;}
public void nutch_f2719_0() throws Exception
{    setUp();            launchServer(responseHeader + simpleContent);    fetchPage("/", 200);}
public void nutch_f2720_0() throws Exception
{    setUp();    launchServer(responseHeader + simpleContent);    fetchPage("?171", 200);}
public void nutch_f2721_0() throws Exception
{    setUp();    launchServer(responseHeader + "Content-Length: thousand\r\n" + simpleContent);    fetchPage("/", 200);}
public void nutch_f2722_0() throws Exception
{    setUp();    launchServer("HTTP/1.1 200: OK\r\n" + simpleContent);    fetchPage("/", 200);}
public void nutch_f2723_0() throws Exception
{    setUp();    launchServer(responseHeader + "Client-Transfer-Encoding: chunked\r\n" + simpleContent);    fetchPage("/", 200);}
public void nutch_f2724_0() throws Exception
{    setUp();    launchServer("HTTP/1.1 302 Found\r\nLocation: http://example.com/\r\n" + "Transfer-Encoding: chunked\r\n\r\nNot a valid chunk.");    ProtocolOutput fetched = fetchPage("/", 302);    assertNotNull("No redirect Location.", getHeader(fetched, "Location"));    assertEquals("Wrong redirect Location.", "http://example.com/", getHeader(fetched, "Location"));}
public void nutch_f2725_0() throws Exception
{    setUp();    String text = "This is a text containing non-ASCII characters: \u00e4\u00f6\u00fc\u00df";    launchServer(text);    ProtocolOutput fetched = fetchPage("/", 200);    assertEquals("Wrong text returned for response with no status line.", text, new String(fetched.getContent().getContent(), StandardCharsets.UTF_8));    server.close();    text = "<!DOCTYPE html>\n<html>\n<head>\n" + "<title>Testing no HTTP header </title>\n" + "<meta charset=\"utf-8\">\n" + "</head>\n<body>This is a text containing non-ASCII characters:" + "\u00e4\u00f6\u00fc\u00df</body>\n</html";    launchServer(text);    fetched = fetchPage("/", 200);    assertEquals("Wrong text returned for response with no status line.", text, new String(fetched.getContent().getContent(), StandardCharsets.UTF_8));}
public void nutch_f2726_1() throws Exception
{    setUp();    launchServer(responseHeader + "Set-Cookie: UserID=JohnDoe;\r\n  Max-Age=3600;\r\n  Version=1\r\n" + simpleContent);    ProtocolOutput fetched = fetchPage("/", 200);        assertNotNull("Failed to set multi-line \"Set-Cookie\" header.", getHeader(fetched, "Set-Cookie"));    assertTrue("Failed to set multi-line \"Set-Cookie\" header.", getHeader(fetched, "Set-Cookie").contains("Version=1"));}
public void nutch_f2727_0() throws Exception
{    setUp();    StringBuilder response = new StringBuilder();    response.append(responseHeader);    for (int i = 0; i < 80; i++) {        response.append("X-Custom-Header-");        for (int j = 0; j < 10000; j++) {            response.append('x');        }        response.append(": hello\r\n");    }    response.append("\r\n" + simpleContent);    launchServer(response.toString());        fetchPage("/", -1);}
public void nutch_f2728_0() throws Exception
{    setUp();    StringBuilder response = new StringBuilder();    response.append(responseHeader);    response.append("Content-Type: text/html\r\n");    response.append("Transfer-Encoding: chunked\r\n");        for (int i = 0; i < 80; i++) {        response.append(String.format("\r\n400\r\n%02x\r\n", i));        for (int j = 0; j < 1012; j++) {            response.append('x');        }        response.append(String.format("\r\n%02x\r\n", i));        response.append("\r\n");    }    response.append("\r\n0\r\n\r\n");    launchServer(response.toString());    ProtocolOutput fetched = fetchPage("/", 200);    assertEquals("Chunked content not truncated according to http.content.limit", 65536, fetched.getContent().getContent().length);    assertNotNull("Content truncation not marked", fetched.getContent().getMetadata().get(Response.TRUNCATED_CONTENT));    assertEquals("Content truncation not marked", Response.TruncatedContentReason.LENGTH.toString().toLowerCase(), fetched.getContent().getMetadata().get(Response.TRUNCATED_CONTENT_REASON));}
public void nutch_f2729_0() throws Exception
{    setUp();    int[] kBs = { 63, 64, 65 };    for (int kB : kBs) {        StringBuilder response = new StringBuilder();        response.append(responseHeader);        response.append("Content-Type: text/plain\r\nContent-Length: " + (kB * 1024) + "\r\n\r\n");        for (int i = 0; i < kB; i++) {            for (int j = 0; j < 16; j++) {                                response.append("abcdefghijklmnopqurstuvxyz0123456789-ABCDEFGHIJKLMNOPQURSTUVXYZ\n");            }        }        launchServer(response.toString());        ProtocolOutput fetched = fetchPage("/", 200);        assertEquals("Content not truncated according to http.content.limit", Math.min(kB * 1024, 65536), fetched.getContent().getContent().length);        if (kB * 1024 > 65536) {            assertNotNull("Content truncation not marked", fetched.getContent().getMetadata().get(Response.TRUNCATED_CONTENT));            assertEquals("Content truncation not marked", Response.TruncatedContentReason.LENGTH.toString().toLowerCase(), fetched.getContent().getMetadata().get(Response.TRUNCATED_CONTENT_REASON));        }                server.close();    }}
public void nutch_f2730_0() throws Exception
{    setUp();    int[] kBs = { 63, 64, 65 };    for (int kB : kBs) {        StringBuilder payload = new StringBuilder();        for (int i = 0; i < kB; i++) {            for (int j = 0; j < 16; j++) {                                payload.append("abcdefghijklmnopqurstuvxyz0123456789-ABCDEFGHIJKLMNOPQURSTUVXYZ\n");            }        }        ByteArrayOutputStream bytes = new ByteArrayOutputStream();        GZIPOutputStream gzip = new GZIPOutputStream(bytes);        gzip.write(payload.toString().getBytes(StandardCharsets.UTF_8));        gzip.close();        StringBuilder responseHead = new StringBuilder();        responseHead.append(responseHeader);        responseHead.append("Content-Type: text/plain\r\nContent-Length: " + bytes.size() + "\r\nContent-Encoding: gzip\r\n\r\n");        ByteArrayOutputStream response = new ByteArrayOutputStream();        response.write(responseHead.toString().getBytes(StandardCharsets.UTF_8));        response.write(bytes.toByteArray());        launchServer(response.toByteArray());        ProtocolOutput fetched = fetchPage("/", 200);        assertEquals("Content not truncated according to http.content.limit", Math.min(kB * 1024, 65536), fetched.getContent().getContent().length);        if (kB * 1024 > 65536) {            assertNotNull("Content truncation not marked", fetched.getContent().getMetadata().get(Response.TRUNCATED_CONTENT));            assertEquals("Content truncation not marked", Response.TruncatedContentReason.LENGTH.toString().toLowerCase(), fetched.getContent().getMetadata().get(Response.TRUNCATED_CONTENT_REASON));        }                server.close();    }}
public void nutch_f2731_0() throws Exception
{    setUp();    conf.setBoolean("http.partial.truncated", true);    http.setConf(conf);    String testContent = "This is a text.";    launchServer(responseHeader + "Content-Length: 50000\r\n\r\n" + testContent);    ProtocolOutput fetched = fetchPage("/", 200);    assertEquals("Content not saved as truncated", testContent, new String(fetched.getContent().getContent(), StandardCharsets.UTF_8));    assertNotNull("Content truncation not marked", fetched.getContent().getMetadata().get(Response.TRUNCATED_CONTENT));}
public void nutch_f2732_0() throws Exception
{    setUp();    conf.setInt("http.content.limit", -1);    http.setConf(conf);    StringBuilder response = new StringBuilder();    response.append(responseHeader);            int kB = 128;    response.append("Content-Type: text/plain\r\nContent-Length: " + (kB * 1024) + "\r\n\r\n");    for (int i = 0; i < kB; i++) {        for (int j = 0; j < 16; j++) {                        response.append("abcdefghijklmnopqurstuvxyz0123456789-ABCDEFGHIJKLMNOPQURSTUVXYZ\n");        }    }    launchServer(response.toString());    ProtocolOutput fetched = fetchPage("/", 200);    assertEquals("Content truncated although http.content.limit == -1", (kB * 1024), fetched.getContent().getContent().length);}
public void nutch_f2733_0(boolean redirection) throws Exception
{    conf = NutchConfiguration.create();    conf.addResource("nutch-default.xml");            conf.addResource("nutch-site-test.xml");    http = new ProtocolFactory(conf).getProtocolById("org.apache.nutch.protocol.okhttp.OkHttp");    server = new Server();    if (redirection) {        root = new Context(server, "/redirection", Context.SESSIONS);        root.setAttribute("newContextURL", "/redirect");    } else {        root = new Context(server, "/", Context.SESSIONS);    }    ServletHolder sh = new ServletHolder(org.apache.jasper.servlet.JspServlet.class);    root.addServlet(sh, "*.jsp");    root.setResourceBase(RES_DIR);}
public void nutch_f2734_0() throws Exception
{    server.stop();}
public void nutch_f2735_0() throws Exception
{    startServer(47504, false);    fetchPage("/basic-http.jsp", 200);    fetchPage("/redirect301.jsp", 301);    fetchPage("/redirect302.jsp", 302);    fetchPage("/nonexists.html", 404);    fetchPage("/brokenpage.jsp", 500);}
public void nutch_f2736_0() throws Exception
{        startServer(47503, true);    fetchPage("/redirection", 302);}
private void nutch_f2737_0(int portno, boolean redirection) throws Exception
{    port = portno;    setUp(redirection);    SelectChannelConnector connector = new SelectChannelConnector();    connector.setHost("127.0.0.1");    connector.setPort(port);    server.addConnector(connector);    server.start();}
private void nutch_f2738_0(String page, int expectedCode) throws Exception
{    URL url = new URL("http", "127.0.0.1", port, page);    CrawlDatum crawlDatum = new CrawlDatum();    ProtocolOutput out = http.getProtocolOutput(new Text(url.toString()), crawlDatum);    int httpStatusCode = -1;    if (crawlDatum.getMetaData().containsKey(Nutch.PROTOCOL_STATUS_CODE_KEY)) {        httpStatusCode = Integer.parseInt(crawlDatum.getMetaData().get(Nutch.PROTOCOL_STATUS_CODE_KEY).toString());    }    Content content = out.getContent();    assertEquals("HTTP Status Code for " + url, expectedCode, httpStatusCode);    if (page.compareTo("/nonexists.html") != 0 && page.compareTo("/brokenpage.jsp") != 0 && page.compareTo("/redirection") != 0) {        assertEquals("ContentType " + url, "text/html", content.getContentType());    }}
public void nutch_f2739_0(Configuration conf)
{    super.setConf(conf);}
public static void nutch_f2740_0(String[] args) throws Exception
{    Http http = new Http();    http.setConf(NutchConfiguration.create());    main(http, args);}
protected Response nutch_f2741_0(URL url, CrawlDatum datum, boolean redirect) throws ProtocolException, IOException
{    return new HttpResponse(this, url, datum);}
public URL nutch_f2742_0()
{    return url;}
public int nutch_f2743_0()
{    return code;}
public String nutch_f2744_0(String name)
{    return headers.get(name);}
public Metadata nutch_f2745_0()
{    return headers;}
public byte[] nutch_f2746_0()
{    return content;}
private void nutch_f2747_0(URL url) throws IOException
{    String page = HttpWebClient.getHtmlPage(url.toString(), conf);    content = page.getBytes("UTF-8");}
private int nutch_f2748_0(PushbackInputStream in, StringBuffer line) throws IOException, HttpException
{    readLine(in, line, false);    int codeStart = line.indexOf(" ");    int codeEnd = line.indexOf(" ", codeStart + 1);        if (codeEnd == -1)        codeEnd = line.length();    int code;    try {        code = Integer.parseInt(line.substring(codeStart + 1, codeEnd));    } catch (NumberFormatException e) {        throw new HttpException("bad status line '" + line + "': " + e.getMessage(), e);    }    return code;}
private void nutch_f2749_0(StringBuffer line) throws IOException, HttpException
{        int colonIndex = line.indexOf(":");    if (colonIndex == -1) {        int i;        for (i = 0; i < line.length(); i++) if (!Character.isWhitespace(line.charAt(i)))            break;        if (i == line.length())            return;        throw new HttpException("No colon in header:" + line);    }    String key = line.substring(0, colonIndex);        int valueStart = colonIndex + 1;    while (valueStart < line.length()) {        int c = line.charAt(valueStart);        if (c != ' ' && c != '\t')            break;        valueStart++;    }    String value = line.substring(valueStart);    headers.set(key, value);}
private static int nutch_f2751_0(PushbackInputStream in, StringBuffer line, boolean allowContinuedLine) throws IOException
{    line.setLength(0);    for (int c = in.read(); c != -1; c = in.read()) {        switch(c) {            case '\r':                if (peek(in) == '\n') {                    in.read();                }            case '\n':                if (line.length() > 0) {                                        if (allowContinuedLine)                        switch(peek(in)) {                            case ' ':                            case                             '\t':                                in.read();                                continue;                        }                }                                return line.length();            default:                line.append((char) c);        }    }    throw new EOFException();}
private static int nutch_f2752_0(PushbackInputStream in) throws IOException
{    int value = in.read();    in.unread(value);    return value;}
public boolean nutch_f2753_1(Configuration conf)
{    try {        exchange = conf.get(RabbitMQConstants.EXCHANGE_NAME);        routingKey = conf.get(RabbitMQConstants.ROUTING_KEY);        headersStatic = conf.get(RabbitMQConstants.HEADERS_STATIC, "");        String uri = conf.get(RabbitMQConstants.SERVER_URI);        client = new RabbitMQClient(uri);        client.openChannel();        boolean binding = conf.getBoolean(RabbitMQConstants.BINDING, false);        if (binding) {            String queueName = conf.get(RabbitMQConstants.QUEUE_NAME);            String queueOptions = conf.get(RabbitMQConstants.QUEUE_OPTIONS);            String exchangeOptions = conf.get(RabbitMQConstants.EXCHANGE_OPTIONS);            String bindingArguments = conf.get(RabbitMQConstants.BINDING_ARGUMENTS, "");            client.bind(exchange, exchangeOptions, queueName, queueOptions, routingKey, bindingArguments);        }                return true;    } catch (Exception e) {                return false;    }}
public void nutch_f2754_1(Object event, Configuration conf)
{    try {        RabbitMQMessage message = new RabbitMQMessage();        message.setBody(getJSONString(event).getBytes());        message.setHeaders(headersStatic);        client.publish(exchange, routingKey, message);    } catch (Exception e) {            }}
private String nutch_f2755_1(Object obj)
{    ObjectMapper mapper = new ObjectMapper();    try {        return mapper.writeValueAsString(obj);    } catch (JsonProcessingException e) {            }    return null;}
public void nutch_f2756_0(Configuration arg0)
{}
public Configuration nutch_f2757_0()
{    return null;}
public void nutch_f2758_0(Configuration conf)
{    super.setConf(conf);    if (conf == null)        return;    defaultMaxDepth = conf.getInt("scoring.depth.max", DEFAULT_MAX_DEPTH);    if (defaultMaxDepth <= 0) {        defaultMaxDepth = DEFAULT_MAX_DEPTH;    }}
public CrawlDatum nutch_f2759_1(Text fromUrl, ParseData parseData, Collection<Entry<Text, CrawlDatum>> targets, CrawlDatum adjust, int allCount) throws ScoringFilterException
{    if (targets.isEmpty()) {        return adjust;    }    String depthString = parseData.getMeta(DEPTH_KEY);    if (depthString == null) {                targets.clear();        return adjust;    }    int curDepth = Integer.parseInt(depthString);    int curMaxDepth = defaultMaxDepth;    IntWritable customMaxDepth = null;        String maxDepthString = parseData.getMeta(MAX_DEPTH_KEY);    if (maxDepthString != null) {        curMaxDepth = Integer.parseInt(maxDepthString);        customMaxDepth = new IntWritable(curMaxDepth);    }    if (curDepth >= curMaxDepth) {                        targets.clear();        return adjust;    }    Iterator<Entry<Text, CrawlDatum>> it = targets.iterator();    while (it.hasNext()) {        Entry<Text, CrawlDatum> e = it.next();                e.getValue().getMetaData().put(DEPTH_KEY_W, new IntWritable(curDepth + 1));                if (customMaxDepth != null) {            e.getValue().getMetaData().put(MAX_DEPTH_KEY_W, customMaxDepth);        }    }    return adjust;}
public float nutch_f2760_0(Text url, CrawlDatum datum, float initSort) throws ScoringFilterException
{        int curDepth, curMaxDepth;    IntWritable maxDepth = (IntWritable) datum.getMetaData().get(MAX_DEPTH_KEY_W);    if (maxDepth != null) {        curMaxDepth = maxDepth.get();    } else {        curMaxDepth = defaultMaxDepth;    }    IntWritable depth = (IntWritable) datum.getMetaData().get(DEPTH_KEY_W);    if (depth == null) {                curDepth = curMaxDepth;    } else {        curDepth = depth.get();    }    int mul = curMaxDepth - curDepth;    return initSort * (1 + mul);}
public float nutch_f2761_0(Text url, NutchDocument doc, CrawlDatum dbDatum, CrawlDatum fetchDatum, Parse parse, Inlinks inlinks, float initScore) throws ScoringFilterException
{    return initScore;}
public void nutch_f2762_0(Text url, CrawlDatum datum) throws ScoringFilterException
{        if (datum.getMetaData().get(MAX_DEPTH_KEY_W) == null)        datum.getMetaData().put(MAX_DEPTH_KEY_W, new IntWritable(defaultMaxDepth));        if (datum.getMetaData().get(DEPTH_KEY_W) == null)        datum.getMetaData().put(DEPTH_KEY_W, new IntWritable(1));}
public void nutch_f2763_0(Text url, CrawlDatum datum) throws ScoringFilterException
{        if (datum.getMetaData().get(MAX_DEPTH_KEY_W) != null) {                String depthString = datum.getMetaData().get(MAX_DEPTH_KEY_W).toString();        datum.getMetaData().remove(MAX_DEPTH_KEY_W);        int depth = Integer.parseInt(depthString);        datum.getMetaData().put(MAX_DEPTH_KEY_W, new IntWritable(depth));    } else {                datum.getMetaData().put(MAX_DEPTH_KEY_W, new IntWritable(defaultMaxDepth));    }        datum.getMetaData().put(DEPTH_KEY_W, new IntWritable(1));}
public void nutch_f2764_0(Text url, Content content, Parse parse) throws ScoringFilterException
{    String depth = content.getMetadata().get(DEPTH_KEY);    if (depth != null) {        parse.getData().getParseMeta().set(DEPTH_KEY, depth);    }    String maxdepth = content.getMetadata().get(MAX_DEPTH_KEY);    if (maxdepth != null) {        parse.getData().getParseMeta().set(MAX_DEPTH_KEY, maxdepth);    }}
public void nutch_f2765_0(Text url, CrawlDatum datum, Content content) throws ScoringFilterException
{    IntWritable depth = (IntWritable) datum.getMetaData().get(DEPTH_KEY_W);    if (depth != null) {        content.getMetadata().set(DEPTH_KEY, depth.toString());    }    IntWritable maxdepth = (IntWritable) datum.getMetaData().get(MAX_DEPTH_KEY_W);    if (maxdepth != null) {        content.getMetadata().set(MAX_DEPTH_KEY, maxdepth.toString());    }}
public void nutch_f2766_0(Text url, CrawlDatum old, CrawlDatum datum, List<CrawlDatum> inlinked) throws ScoringFilterException
{        int newDepth = DEFAULT_MAX_DEPTH;    if (old != null) {        IntWritable oldDepth = (IntWritable) old.getMetaData().get(DEPTH_KEY_W);        if (oldDepth != null) {            newDepth = oldDepth.get();        } else {                        initialScore(url, old);        }    }    for (CrawlDatum lnk : inlinked) {        IntWritable depth = (IntWritable) lnk.getMetaData().get(DEPTH_KEY_W);        if (depth != null && depth.get() < newDepth) {            newDepth = depth.get();        }    }    datum.getMetaData().put(DEPTH_KEY_W, new IntWritable(newDepth));}
public Configuration nutch_f2767_0()
{    return conf;}
public void nutch_f2768_0(Configuration conf)
{    this.conf = conf;    normalizedScore = conf.getFloat("link.analyze.normalize.score", 1.00f);}
public CrawlDatum nutch_f2769_0(Text fromUrl, ParseData parseData, Collection<Entry<Text, CrawlDatum>> targets, CrawlDatum adjust, int allCount) throws ScoringFilterException
{    return adjust;}
public float nutch_f2770_0(Text url, CrawlDatum datum, float initSort) throws ScoringFilterException
{    return datum.getScore() * initSort;}
public float nutch_f2771_0(Text url, NutchDocument doc, CrawlDatum dbDatum, CrawlDatum fetchDatum, Parse parse, Inlinks inlinks, float initScore) throws ScoringFilterException
{    if (dbDatum == null) {        return initScore;    }    return (normalizedScore * dbDatum.getScore());}
public void nutch_f2772_0(Text url, CrawlDatum datum) throws ScoringFilterException
{    datum.setScore(initialScore);}
public void nutch_f2773_0(Text url, CrawlDatum datum) throws ScoringFilterException
{}
public void nutch_f2774_0(Text url, Content content, Parse parse) throws ScoringFilterException
{    parse.getData().getContentMeta().set(Nutch.SCORE_KEY, content.getMetadata().get(Nutch.SCORE_KEY));}
public void nutch_f2775_0(Text url, CrawlDatum datum, Content content) throws ScoringFilterException
{    content.getMetadata().set(Nutch.SCORE_KEY, "" + datum.getScore());}
public void nutch_f2776_0(Text url, CrawlDatum old, CrawlDatum datum, List<CrawlDatum> inlinked) throws ScoringFilterException
{}
public Configuration nutch_f2777_0()
{    return conf;}
public void nutch_f2778_0(Configuration conf)
{    this.conf = conf;    scorePower = conf.getFloat("indexer.score.power", 0.5f);    internalScoreFactor = conf.getFloat("db.score.link.internal", 1.0f);    externalScoreFactor = conf.getFloat("db.score.link.external", 1.0f);    countFiltered = conf.getBoolean("db.score.count.filtered", false);}
public void nutch_f2779_0(Text url, CrawlDatum datum) throws ScoringFilterException
{}
public void nutch_f2780_0(Text url, CrawlDatum datum) throws ScoringFilterException
{    datum.setScore(0.0f);}
public float nutch_f2781_0(Text url, CrawlDatum datum, float initSort) throws ScoringFilterException
{    return datum.getScore() * initSort;}
public void nutch_f2782_0(Text url, CrawlDatum old, CrawlDatum datum, List<CrawlDatum> inlinked) throws ScoringFilterException
{    float adjust = 0.0f;    for (int i = 0; i < inlinked.size(); i++) {        CrawlDatum linked = inlinked.get(i);        adjust += linked.getScore();    }    if (old == null)        old = datum;    datum.setScore(old.getScore() + adjust);}
public void nutch_f2783_0(Text url, CrawlDatum datum, Content content)
{    content.getMetadata().set(Nutch.SCORE_KEY, "" + datum.getScore());}
public void nutch_f2784_0(Text url, Content content, Parse parse)
{    parse.getData().getContentMeta().set(Nutch.SCORE_KEY, content.getMetadata().get(Nutch.SCORE_KEY));}
public CrawlDatum nutch_f2785_1(Text fromUrl, ParseData parseData, Collection<Entry<Text, CrawlDatum>> targets, CrawlDatum adjust, int allCount) throws ScoringFilterException
{    float score = scoreInjected;    String scoreString = parseData.getContentMeta().get(Nutch.SCORE_KEY);    if (scoreString != null) {        try {            score = Float.parseFloat(scoreString);        } catch (Exception e) {                    }    }    int validCount = targets.size();    if (countFiltered) {        score /= allCount;    } else {        if (validCount == 0) {                        return adjust;        }        score /= validCount;    }        float internalScore = score * internalScoreFactor;    float externalScore = score * externalScoreFactor;    for (Entry<Text, CrawlDatum> target : targets) {        try {            String toHost = new URL(target.getKey().toString()).getHost();            String fromHost = new URL(fromUrl.toString()).getHost();            if (toHost.equalsIgnoreCase(fromHost)) {                target.getValue().setScore(internalScore);            } else {                target.getValue().setScore(externalScore);            }        } catch (MalformedURLException e) {                        target.getValue().setScore(externalScore);        }    }        return adjust;}
public float nutch_f2786_0(Text url, NutchDocument doc, CrawlDatum dbDatum, CrawlDatum fetchDatum, Parse parse, Inlinks inlinks, float initScore) throws ScoringFilterException
{    if (dbDatum == null) {        return initScore;    }    return (float) Math.pow(dbDatum.getScore(), scorePower) * initScore;}
public void nutch_f2787_1(Configuration conf)
{    markGoneAfter = conf.getInt("scoring.orphan.mark.gone.after", DEFAULT_GONE_TIME);    markOrphanAfter = conf.getInt("scoring.orphan.mark.orphan.after", DEFAULT_ORPHAN_TIME);    if (markGoneAfter > markOrphanAfter) {            }}
public void nutch_f2788_0(Text url, CrawlDatum old, CrawlDatum datum, List<CrawlDatum> inlinks) throws ScoringFilterException
{    int now = (int) (System.currentTimeMillis() / 1000);        if (inlinks.size() > 0) {                datum.getMetaData().put(ORPHAN_KEY_WRITABLE, new IntWritable(now));    } else {        orphanedScore(url, datum);    }}
public void nutch_f2789_0(Text url, CrawlDatum datum)
{        if (datum.getMetaData().containsKey(ORPHAN_KEY_WRITABLE)) {                IntWritable writable = (IntWritable) datum.getMetaData().get(ORPHAN_KEY_WRITABLE);        int lastInlinkTime = writable.get();        int now = (int) (System.currentTimeMillis() / 1000);        int elapsedSinceLastInLinkTime = now - lastInlinkTime;        if (elapsedSinceLastInLinkTime > markOrphanAfter) {                        datum.setStatus(CrawlDatum.STATUS_DB_ORPHAN);        } else if (elapsedSinceLastInLinkTime > markGoneAfter) {                        datum.setStatus(CrawlDatum.STATUS_DB_GONE);        }    }}
public void nutch_f2790_0() throws Exception
{    Configuration conf = NutchConfiguration.create();    conf.setInt("scoring.orphan.mark.gone.after", 5);    conf.setInt("scoring.orphan.mark.orphan.after", 10);    ScoringFilter filter = new OrphanScoringFilter();    filter.setConf(conf);    Text url = new Text("http://nutch.apache.org/");    CrawlDatum datum = new CrawlDatum();    datum.setStatus(CrawlDatum.STATUS_DB_NOTMODIFIED);    List<CrawlDatum> emptyListOfInlinks = new ArrayList<CrawlDatum>();    List<CrawlDatum> populatedListOfInlinks = new ArrayList<CrawlDatum>();    populatedListOfInlinks.add(datum);        filter.updateDbScore(url, null, datum, populatedListOfInlinks);    int firstOrphanTime = getTime(datum);    assertTrue(datum.getMetaData().containsKey(OrphanScoringFilter.ORPHAN_KEY_WRITABLE));        try {        Thread.sleep(1000);    } catch (Exception e) {    }        filter.updateDbScore(url, null, datum, populatedListOfInlinks);    int secondOrphanTime = getTime(datum);    assertTrue(secondOrphanTime > firstOrphanTime);            filter.updateDbScore(url, null, datum, emptyListOfInlinks);    int thirdOrphanTime = getTime(datum);    assertEquals(thirdOrphanTime, secondOrphanTime);    assertEquals("Expected status db_notmodified but got " + CrawlDatum.getStatusName(datum.getStatus()), CrawlDatum.STATUS_DB_NOTMODIFIED, datum.getStatus());        try {        Thread.sleep(1000);    } catch (Exception e) {    }            filter.updateDbScore(url, null, datum, emptyListOfInlinks);    assertEquals("Expected status db_notmodified but got " + CrawlDatum.getStatusName(datum.getStatus()), CrawlDatum.STATUS_DB_NOTMODIFIED, datum.getStatus());        try {        Thread.sleep(5000);    } catch (Exception e) {    }        filter.updateDbScore(url, null, datum, emptyListOfInlinks);    int fourthOrphanTime = getTime(datum);    assertEquals(fourthOrphanTime, thirdOrphanTime);    assertEquals("Expected status db_gone but got " + CrawlDatum.getStatusName(datum.getStatus()), CrawlDatum.STATUS_DB_GONE, datum.getStatus());        try {        Thread.sleep(5000);    } catch (Exception e) {    }        filter.updateDbScore(url, null, datum, emptyListOfInlinks);    assertEquals("Expected status db_orphan but got " + CrawlDatum.getStatusName(datum.getStatus()), CrawlDatum.STATUS_DB_ORPHAN, datum.getStatus());}
protected int nutch_f2791_0(CrawlDatum datum)
{    IntWritable writable = (IntWritable) datum.getMetaData().get(OrphanScoringFilter.ORPHAN_KEY_WRITABLE);    return writable.get();}
public void nutch_f2792_0(Configuration conf)
{    this.conf = conf;}
public float nutch_f2793_1(Text url, Content content, Parse parse)
{    float score = 1;    try {        if (!Model.isModelCreated) {            Model.createModel(conf);        }        String metatags = parse.getData().getParseMeta().get("metatag.keyword");        String metaDescription = parse.getData().getParseMeta().get("metatag.description");        int[] ngramArr = Model.retrieveNgrams(conf);        int mingram = ngramArr[0];        int maxgram = ngramArr[1];        DocVector docVector = Model.createDocVector(parse.getText() + metaDescription + metatags, mingram, maxgram);        if (docVector != null) {            score = Model.computeCosineSimilarity(docVector);                    } else {            throw new Exception("Could not create DocVector from parsed text");        }    } catch (Exception e) {            }    return score;}
public CrawlDatum nutch_f2794_0(Text fromUrl, ParseData parseData, Collection<Entry<Text, CrawlDatum>> targets, CrawlDatum adjust, int allCount)
{    float score = Float.parseFloat(parseData.getContentMeta().get(Nutch.SCORE_KEY));    for (Entry<Text, CrawlDatum> target : targets) {        target.getValue().setScore(score);    }    return adjust;}
public void nutch_f2795_0(HashMap<String, Integer> termFreqVector)
{    this.termFreqVector = termFreqVector;}
public void nutch_f2796_0(int pos, long freq)
{    termVector.put(pos, freq);}
public float nutch_f2797_0(DocVector docVector)
{    float product = 0;    for (Map.Entry<String, Integer> entry : termFreqVector.entrySet()) {        if (docVector.termFreqVector.containsKey(entry.getKey())) {            product += docVector.termFreqVector.get(entry.getKey()) * entry.getValue();        }    }    return product;}
public float nutch_f2798_0()
{    float sum = 0;    for (Map.Entry<String, Integer> entry : termFreqVector.entrySet()) {        sum += entry.getValue() * entry.getValue();    }    return (float) Math.sqrt(sum);}
public static DocVector nutch_f2800_1(String content, int mingram, int maxgram)
{    LuceneTokenizer tokenizer;    if (mingram > 1 && maxgram > 1) {                tokenizer = new LuceneTokenizer(content, TokenizerType.STANDARD, StemFilterType.PORTERSTEM_FILTER, mingram, maxgram);    } else if (mingram > 1) {        maxgram = mingram;                tokenizer = new LuceneTokenizer(content, TokenizerType.STANDARD, StemFilterType.PORTERSTEM_FILTER, mingram, maxgram);    } else if (stopWords != null) {        tokenizer = new LuceneTokenizer(content, TokenizerType.STANDARD, stopWords, true, StemFilterType.PORTERSTEM_FILTER);    } else {        tokenizer = new LuceneTokenizer(content, TokenizerType.STANDARD, true, StemFilterType.PORTERSTEM_FILTER);    }    TokenStream tStream = tokenizer.getTokenStream();    HashMap<String, Integer> termVector = new HashMap<>();    try {        CharTermAttribute charTermAttribute = tStream.addAttribute(CharTermAttribute.class);        tStream.reset();        while (tStream.incrementToken()) {            String term = charTermAttribute.toString();                        if (termVector.containsKey(term)) {                int count = termVector.get(term);                count++;                termVector.put(term, count);            } else {                termVector.put(term, 1);            }        }        DocVector docVector = new DocVector();        docVector.setTermFreqVector(termVector);        return docVector;    } catch (IOException e) {            }    return null;}
public static float nutch_f2801_0(DocVector docVector)
{    float[] scores = new float[docVectors.size()];    int i = 0;    float maxScore = 0;    for (DocVector corpusDoc : docVectors) {        float numerator = docVector.dotProduct(corpusDoc);        float denominator = docVector.getL2Norm() * corpusDoc.getL2Norm();        float currentScore = numerator / denominator;        scores[i++] = currentScore;        maxScore = (currentScore > maxScore) ? currentScore : maxScore;    }        return maxScore;}
public static int[] nutch_f2802_0(Configuration conf)
{    int[] ngramArr = new int[2];        String[] ngramStr = conf.getStrings("scoring.similarity.ngrams", "1,1");        ngramArr[0] = Integer.parseInt(ngramStr[0]);    if (ngramStr.length > 1) {                ngramArr[1] = Integer.parseInt(ngramStr[1]);    } else {                ngramArr[1] = ngramArr[0];    }    return ngramArr;}
public Configuration nutch_f2803_0()
{    return conf;}
public void nutch_f2804_0(Configuration conf)
{    this.conf = conf;    switch(conf.get("scoring.similarity.model", "cosine")) {        case "cosine":            similarityModel = (SimilarityModel) new CosineSimilarity();            break;    }    similarityModel.setConf(conf);}
public void nutch_f2805_0(Text url, Content content, Parse parse) throws ScoringFilterException
{    float score = similarityModel.setURLScoreAfterParsing(url, content, parse);    parse.getData().getContentMeta().set(Nutch.SCORE_KEY, score + "");}
public CrawlDatum nutch_f2806_0(Text fromUrl, ParseData parseData, Collection<Entry<Text, CrawlDatum>> targets, CrawlDatum adjust, int allCount) throws ScoringFilterException
{    similarityModel.distributeScoreToOutlinks(fromUrl, parseData, targets, adjust, allCount);    return adjust;}
protected TokenStreamComponents nutch_f2807_0(String fieldName)
{    Tokenizer source = new ClassicTokenizer();    TokenStream filter = new LowerCaseFilter(source);    if (stopSet != null) {        filter = new StopFilter(filter, stopSet);    }    switch(stemFilterType) {        case PORTERSTEM_FILTER:            filter = new PorterStemFilter(filter);            break;        case ENGLISHMINIMALSTEM_FILTER:            filter = new EnglishMinimalStemFilter(filter);            break;        default:            break;    }    return new TokenStreamComponents(source, filter);}
public TokenStream nutch_f2808_0()
{    return tokenStream;}
private TokenStream nutch_f2809_0(String content)
{    tokenStream = generateTokenStreamFromText(content, tokenizer);    tokenStream = new LowerCaseFilter(tokenStream);    if (stopSet != null) {        tokenStream = applyStopFilter(stopSet);    }    tokenStream = applyStemmer(stemFilterType);    return tokenStream;}
private TokenStream nutch_f2810_0(String content, TokenizerType tokenizerType)
{    Tokenizer tokenizer = null;    switch(tokenizerType) {        case CLASSIC:            tokenizer = new ClassicTokenizer();            break;        case STANDARD:        default:            tokenizer = new StandardTokenizer();    }    tokenizer.setReader(new StringReader(content));    tokenStream = tokenizer;    return tokenStream;}
private TokenStream nutch_f2811_0(String content, int mingram, int maxgram)
{    Tokenizer tokenizer = new StandardTokenizer();    tokenizer.setReader(new StringReader(content));    tokenStream = new LowerCaseFilter(tokenizer);    tokenStream = applyStemmer(stemFilterType);    ShingleFilter shingleFilter = new ShingleFilter(tokenStream, mingram, maxgram);    shingleFilter.setOutputUnigrams(false);    tokenStream = (TokenStream) shingleFilter;    return tokenStream;}
private TokenStream nutch_f2812_0(CharArraySet stopWords)
{    tokenStream = new StopFilter(tokenStream, stopWords);    return tokenStream;}
private TokenStream nutch_f2813_0(StemFilterType stemFilterType)
{    switch(stemFilterType) {        case ENGLISHMINIMALSTEM_FILTER:            tokenStream = new EnglishMinimalStemFilter(tokenStream);            break;        case PORTERSTEM_FILTER:            tokenStream = new PorterStemFilter(tokenStream);            break;        default:            break;    }    return tokenStream;}
protected void nutch_f2814_1()
{    try {        if (LOG.isInfoEnabled()) {                    }                configfile = getConf().getResource(getConf().get("subcollections.config", DEFAULT_FILE_NAME));        InputStream input = getConf().getConfResourceAsInputStream(getConf().get("subcollections.config", DEFAULT_FILE_NAME));        parse(input);    } catch (Exception e) {        if (LOG.isWarnEnabled()) {                    }    }}
protected void nutch_f2815_1(InputStream input)
{    Element collections = DomUtil.getDom(input);    if (collections != null) {        NodeList nodeList = collections.getElementsByTagName(Subcollection.TAG_COLLECTION);        if (LOG.isInfoEnabled()) {                    }        for (int i = 0; i < nodeList.getLength(); i++) {            Element scElem = (Element) nodeList.item(i);            Subcollection subCol = new Subcollection(getConf());            subCol.initialize(scElem);            collectionMap.put(subCol.name, subCol);        }    } else if (LOG.isInfoEnabled()) {            }}
public static CollectionManager nutch_f2816_1(Configuration conf)
{    String key = "collectionmanager";    ObjectCache objectCache = ObjectCache.get(conf);    CollectionManager impl = (CollectionManager) objectCache.getObject(key);    if (impl == null) {        try {            if (LOG.isInfoEnabled()) {                            }            impl = new CollectionManager(conf);            objectCache.setObject(key, impl);        } catch (Exception e) {            throw new RuntimeException("Couldn't create CollectionManager", e);        }    }    return impl;}
public Subcollection nutch_f2817_0(final String id)
{    return (Subcollection) collectionMap.get(id);}
public void nutch_f2818_0(final String id) throws IOException
{    final Subcollection subCol = getSubColection(id);    if (subCol != null) {        collectionMap.remove(id);    }}
public Subcollection nutch_f2819_0(final String id, final String name)
{    Subcollection subCol = null;    if (!collectionMap.containsKey(id)) {        subCol = new Subcollection(id, name, getConf());        collectionMap.put(id, subCol);    }    return subCol;}
public List<Subcollection> nutch_f2820_0(final String url)
{    List<Subcollection> collections = new ArrayList<Subcollection>();    final Iterator iterator = collectionMap.values().iterator();    while (iterator.hasNext()) {        final Subcollection subCol = (Subcollection) iterator.next();        if (subCol.filter(url) != null) {            collections.add(subCol);        }    }    if (LOG.isTraceEnabled()) {        LOG.trace("subcollections:" + Arrays.toString(collections.toArray()));    }    return collections;}
public Collection nutch_f2821_0()
{    return collectionMap.values();}
public void nutch_f2822_0() throws IOException
{    try {        final FileOutputStream fos = new FileOutputStream(new File(configfile.getFile()));        final Document doc = new DocumentImpl();        final Element collections = doc.createElement(Subcollection.TAG_COLLECTIONS);        final Iterator iterator = collectionMap.values().iterator();        while (iterator.hasNext()) {            final Subcollection subCol = (Subcollection) iterator.next();            final Element collection = doc.createElement(Subcollection.TAG_COLLECTION);            collections.appendChild(collection);            final Element name = doc.createElement(Subcollection.TAG_NAME);            name.setNodeValue(subCol.getName());            collection.appendChild(name);            final Element whiteList = doc.createElement(Subcollection.TAG_WHITELIST);            whiteList.setNodeValue(subCol.getWhiteListString());            collection.appendChild(whiteList);            final Element blackList = doc.createElement(Subcollection.TAG_BLACKLIST);            blackList.setNodeValue(subCol.getBlackListString());            collection.appendChild(blackList);        }        DomUtil.saveDom(fos, collections);        fos.flush();        fos.close();    } catch (FileNotFoundException e) {        throw new IOException(e.toString());    }}
public String nutch_f2823_0()
{    return name;}
public String nutch_f2824_0()
{    return key;}
public String nutch_f2825_0()
{    return id;}
public List<String> nutch_f2826_0()
{    return whiteList;}
public String nutch_f2827_0()
{    return wlString;}
public String nutch_f2828_0()
{    return blString;}
public void nutch_f2829_0(ArrayList<String> whiteList)
{    this.whiteList = whiteList;}
public String nutch_f2830_0(String urlString)
{        Iterator<String> i = blackList.iterator();    while (i.hasNext()) {        String row = (String) i.next();        if (urlString.contains(row))            return null;    }        i = whiteList.iterator();    while (i.hasNext()) {        String row = (String) i.next();        if (urlString.contains(row))            return urlString;    }    return null;}
public void nutch_f2831_0(Element collection)
{    this.id = DOMUtil.getChildText(collection.getElementsByTagName(TAG_ID).item(0)).trim();    this.name = DOMUtil.getChildText(collection.getElementsByTagName(TAG_NAME).item(0)).trim();    this.wlString = DOMUtil.getChildText(collection.getElementsByTagName(TAG_WHITELIST).item(0)).trim();    parseList(this.whiteList, wlString);        NodeList nodeList = collection.getElementsByTagName(TAG_BLACKLIST);    if (nodeList.getLength() > 0) {        this.blString = DOMUtil.getChildText(nodeList.item(0)).trim();        parseList(this.blackList, blString);    }        nodeList = collection.getElementsByTagName(TAG_KEY);    if (nodeList.getLength() == 1) {        this.key = DOMUtil.getChildText(nodeList.item(0)).trim();    }}
protected void nutch_f2832_0(List<String> list, String text)
{    list.clear();    StringTokenizer st = new StringTokenizer(text, "\n\r");    while (st.hasMoreElements()) {        String line = (String) st.nextElement();        line = line.trim();        if (caseInsensitive) {            line = line.toLowerCase();        }        list.add(line);    }}
public void nutch_f2833_0(String list)
{    this.blString = list;    parseList(blackList, list);}
public void nutch_f2834_0(String list)
{    this.wlString = list;    parseList(whiteList, list);}
public void nutch_f2835_0(Configuration conf)
{    this.conf = conf;    fieldName = conf.get("subcollection.default.fieldname", "subcollection");    metadataSource = conf.get("subcollection.metadata.source", "subcollection");    caseInsensitive = conf.getBoolean("subcollection.case.insensitive", false);}
public Configuration nutch_f2836_0()
{    return this.conf;}
private void nutch_f2837_0(NutchDocument doc, String url)
{    for (Subcollection coll : CollectionManager.getCollectionManager(getConf()).getSubCollections(url)) {        if (coll.getKey() == null) {            doc.add(fieldName, coll.getName());        } else {            doc.add(coll.getKey(), coll.getName());        }    }}
public NutchDocument nutch_f2838_0(NutchDocument doc, Parse parse, Text url, CrawlDatum datum, Inlinks inlinks) throws IndexingException
{        String subcollection = parse.getData().getMeta(metadataSource);    if (subcollection != null) {        subcollection = subcollection.trim();        if (subcollection.length() > 0) {            doc.add(fieldName, subcollection);            return doc;        }    }    String sUrl = url.toString();    if (caseInsensitive) {        sUrl = sUrl.toLowerCase();    }    addSubCollectionField(doc, sUrl);    return doc;}
public void nutch_f2839_0() throws Exception
{    Subcollection sc = new Subcollection(NutchConfiguration.create());    sc.setWhiteList("www.nutch.org\nwww.apache.org");    sc.setBlackList("jpg\nwww.apache.org/zecret/");        Assert.assertEquals("http://www.apache.org/index.html", sc.filter("http://www.apache.org/index.html"));        Assert.assertEquals(null, sc.filter("http://www.apache.org/zecret/index.html"));    Assert.assertEquals(null, sc.filter("http://www.apache.org/img/image.jpg"));        Assert.assertEquals(null, sc.filter("http://www.google.com/"));}
public void nutch_f2840_0()
{    StringBuffer xml = new StringBuffer();    xml.append("<?xml version=\"1.0\" encoding=\"UTF-8\"?>");    xml.append("<!-- just a comment -->");    xml.append("<subcollections>");    xml.append("<subcollection>");    xml.append("<name>nutch collection</name>");    xml.append("<id>nutch</id>");    xml.append("<whitelist>");    xml.append("http://lucene.apache.org/nutch/\n");    xml.append("http://wiki.apache.org/nutch/\n");    xml.append("</whitelist>");    xml.append("<blacklist>");    xml.append("http://www.xxx.yyy\n");    xml.append("</blacklist>");    xml.append("</subcollection>");    xml.append("</subcollections>");    InputStream is = new ByteArrayInputStream(xml.toString().getBytes());    CollectionManager cm = new CollectionManager();    cm.parse(is);    Collection<?> c = cm.getAll();        Assert.assertEquals(1, c.size());    Subcollection collection = (Subcollection) c.toArray()[0];        Assert.assertEquals("nutch", collection.getId());        Assert.assertEquals("nutch collection", collection.getName());        Assert.assertEquals(2, collection.whiteList.size());    String wlUrl = (String) collection.whiteList.get(0);    Assert.assertEquals("http://lucene.apache.org/nutch/", wlUrl);    wlUrl = (String) collection.whiteList.get(1);    Assert.assertEquals("http://wiki.apache.org/nutch/", wlUrl);        Assert.assertEquals("http://lucene.apache.org/nutch/", collection.filter("http://lucene.apache.org/nutch/"));        Assert.assertEquals(1, collection.blackList.size());    String blUrl = (String) collection.blackList.get(0);    Assert.assertEquals("http://www.xxx.yyy", blUrl);        Assert.assertEquals(null, collection.filter("http://www.google.com/"));}
public NutchDocument nutch_f2841_1(NutchDocument doc, Parse parse, Text urlText, CrawlDatum datum, Inlinks inlinks) throws IndexingException
{    try {        URL url = new URL(urlText.toString());        DomainSuffix d = URLUtil.getDomainSuffix(url);        doc.add("tld", d.getDomain());    } catch (Exception ex) {            }    return doc;}
public void nutch_f2842_0(Configuration conf)
{    this.conf = conf;}
public Configuration nutch_f2843_0()
{    return this.conf;}
public float nutch_f2844_0(Text url, NutchDocument doc, CrawlDatum dbDatum, CrawlDatum fetchDatum, Parse parse, Inlinks inlinks, float initScore) throws ScoringFilterException
{    NutchField tlds = doc.getField("tld");    float boost = 1.0f;    if (tlds != null) {        for (Object tld : tlds.getValues()) {            DomainSuffix entry = tldEntries.get(tld.toString());            if (entry != null)                boost *= entry.getBoost();        }    }    return initScore * boost;}
public CrawlDatum nutch_f2845_0(Text fromUrl, Text toUrl, ParseData parseData, CrawlDatum target, CrawlDatum adjust, int allCount, int validCount) throws ScoringFilterException
{    return adjust;}
public float nutch_f2846_0(Text url, CrawlDatum datum, float initSort) throws ScoringFilterException
{    return initSort;}
public void nutch_f2847_0(Text url, CrawlDatum datum) throws ScoringFilterException
{}
public void nutch_f2848_0(Text url, CrawlDatum datum) throws ScoringFilterException
{}
public void nutch_f2849_0(Text url, Content content, Parse parse) throws ScoringFilterException
{}
public void nutch_f2850_0(Text url, CrawlDatum datum, Content content) throws ScoringFilterException
{}
public void nutch_f2851_0(Text url, CrawlDatum old, CrawlDatum datum, List<CrawlDatum> inlinked) throws ScoringFilterException
{}
public Configuration nutch_f2852_0()
{    return conf;}
public void nutch_f2853_0(Configuration conf)
{    this.conf = conf;}
public CrawlDatum nutch_f2854_0(Text fromUrl, ParseData parseData, Collection<Entry<Text, CrawlDatum>> targets, CrawlDatum adjust, int allCount) throws ScoringFilterException
{    return adjust;}
protected Reader nutch_f2855_0(Configuration conf) throws IOException
{    String stringRules = conf.get(URLFILTER_AUTOMATON_RULES);    if (stringRules != null) {        return new StringReader(stringRules);    }    String fileRules = conf.get(URLFILTER_AUTOMATON_FILE);    return conf.getConfResourceAsReader(fileRules);}
protected RegexRule nutch_f2856_0(boolean sign, String regex)
{    return new Rule(sign, regex);}
protected RegexRule nutch_f2857_0(boolean sign, String regex, String hostOrDomain)
{    return new Rule(sign, regex, hostOrDomain);}
public static void nutch_f2858_0(String[] args) throws IOException
{    main(new AutomatonURLFilter(), args);}
protected boolean nutch_f2859_0(String url)
{    return automaton.run(url);}
protected URLFilter nutch_f2860_0(Reader rules)
{    try {        return new AutomatonURLFilter(rules);    } catch (IOException e) {        Assert.fail(e.toString());        return null;    }}
public void nutch_f2861_0()
{    test("WholeWebCrawling");    test("IntranetCrawling");    bench(50, "Benchmarks");    bench(100, "Benchmarks");    bench(200, "Benchmarks");    bench(400, "Benchmarks");    bench(800, "Benchmarks");}
private void nutch_f2862_0(Reader configReader) throws IOException
{        BufferedReader reader = new BufferedReader(configReader);    String line = null;    while ((line = reader.readLine()) != null) {        if (StringUtils.isNotBlank(line) && !line.startsWith("#")) {                        domainSet.add(StringUtils.lowerCase(line.trim()));        }    }}
public void nutch_f2863_1(Configuration conf)
{    this.conf = conf;        String pluginName = "urlfilter-domain";    Extension[] extensions = PluginRepository.get(conf).getExtensionPoint(URLFilter.class.getName()).getExtensions();    for (int i = 0; i < extensions.length; i++) {        Extension extension = extensions[i];        if (extension.getDescriptor().getPluginId().equals(pluginName)) {            attributeFile = extension.getAttribute("file");            break;        }    }        if (attributeFile != null && attributeFile.trim().equals("")) {        attributeFile = null;    }    if (attributeFile != null) {        if (LOG.isInfoEnabled()) {                    }    } else {        if (LOG.isWarnEnabled()) {                    }    }        String file = conf.get("urlfilter.domain.file");    String stringRules = conf.get("urlfilter.domain.rules");    if (domainFile != null) {        file = domainFile;    } else if (attributeFile != null) {        file = attributeFile;    }    Reader reader = null;    if (stringRules != null) {                reader = new StringReader(stringRules);    } else {        reader = conf.getConfResourceAsReader(file);    }    try {        if (reader == null) {            reader = new FileReader(file);        }        readConfiguration(reader);    } catch (IOException e) {            }}
public Configuration nutch_f2864_0()
{    return this.conf;}
public String nutch_f2865_1(String url)
{        if (domainSet.size() == 0)        return url;    try {                        String domain = URLUtil.getDomainName(url).toLowerCase().trim();        String host = URLUtil.getHost(url);        String suffix = null;        DomainSuffix domainSuffix = URLUtil.getDomainSuffix(url);        if (domainSuffix != null) {            suffix = domainSuffix.getDomain();        }        if (domainSet.contains(suffix) || domainSet.contains(domain) || domainSet.contains(host)) {            return url;        }                return null;    } catch (Exception e) {                        return null;    }}
public void nutch_f2866_0() throws Exception
{    String domainFile = SAMPLES + SEPARATOR + "hosts.txt";    Configuration conf = NutchConfiguration.create();    DomainURLFilter domainFilter = new DomainURLFilter(domainFile);    domainFilter.setConf(conf);    Assert.assertNotNull(domainFilter.filter("http://lucene.apache.org"));    Assert.assertNotNull(domainFilter.filter("http://hadoop.apache.org"));    Assert.assertNotNull(domainFilter.filter("http://www.apache.org"));    Assert.assertNull(domainFilter.filter("http://www.google.com"));    Assert.assertNull(domainFilter.filter("http://mail.yahoo.com"));    Assert.assertNotNull(domainFilter.filter("http://www.foobar.net"));    Assert.assertNotNull(domainFilter.filter("http://www.foobas.net"));    Assert.assertNotNull(domainFilter.filter("http://www.yahoo.com"));    Assert.assertNotNull(domainFilter.filter("http://www.foobar.be"));    Assert.assertNull(domainFilter.filter("http://www.adobe.com"));}
public void nutch_f2867_0() throws Exception
{        String domainFile = SAMPLES + SEPARATOR + "this-file-does-not-exist.txt";    Configuration conf = NutchConfiguration.create();    DomainURLFilter domainFilter = new DomainURLFilter(domainFile);    domainFilter.setConf(conf);    Assert.assertNotNull(domainFilter.filter("http://lucene.apache.org"));    Assert.assertNotNull(domainFilter.filter("http://hadoop.apache.org"));    Assert.assertNotNull(domainFilter.filter("http://www.apache.org"));    Assert.assertNotNull(domainFilter.filter("http://www.google.com"));    Assert.assertNotNull(domainFilter.filter("http://mail.yahoo.com"));    Assert.assertNotNull(domainFilter.filter("http://www.foobar.net"));    Assert.assertNotNull(domainFilter.filter("http://www.foobas.net"));    Assert.assertNotNull(domainFilter.filter("http://www.yahoo.com"));    Assert.assertNotNull(domainFilter.filter("http://www.foobar.be"));    Assert.assertNotNull(domainFilter.filter("http://www.adobe.com"));}
private void nutch_f2868_0(Reader configReader) throws IOException
{        BufferedReader reader = new BufferedReader(configReader);    String line = null;    while ((line = reader.readLine()) != null) {        if (StringUtils.isNotBlank(line) && !line.startsWith("#")) {                        domainSet.add(StringUtils.lowerCase(line.trim()));        }    }}
public void nutch_f2869_1(Configuration conf)
{    this.conf = conf;        String pluginName = "urlfilter-domainblacklist";    Extension[] extensions = PluginRepository.get(conf).getExtensionPoint(URLFilter.class.getName()).getExtensions();    for (int i = 0; i < extensions.length; i++) {        Extension extension = extensions[i];        if (extension.getDescriptor().getPluginId().equals(pluginName)) {            attributeFile = extension.getAttribute("file");            break;        }    }        if (attributeFile != null && attributeFile.trim().equals("")) {        attributeFile = null;    }    if (attributeFile != null) {        if (LOG.isInfoEnabled()) {                    }    } else {        if (LOG.isWarnEnabled()) {                    }    }        String file = conf.get("urlfilter.domainblacklist.file");    String stringRules = conf.get("urlfilter.domainblacklist.rules");    if (domainFile != null) {        file = domainFile;    } else if (attributeFile != null) {        file = attributeFile;    }    Reader reader = null;    if (stringRules != null) {                reader = new StringReader(stringRules);    } else {        reader = conf.getConfResourceAsReader(file);    }    try {        if (reader == null) {            reader = new FileReader(file);        }        readConfiguration(reader);    } catch (IOException e) {            }}
public Configuration nutch_f2870_0()
{    return this.conf;}
public String nutch_f2871_1(String url)
{    try {                        String domain = URLUtil.getDomainName(url).toLowerCase().trim();        String host = URLUtil.getHost(url);        String suffix = null;        DomainSuffix domainSuffix = URLUtil.getDomainSuffix(url);        if (domainSuffix != null) {            suffix = domainSuffix.getDomain();        }        if (domainSet.contains(suffix) || domainSet.contains(domain) || domainSet.contains(host)) {                        return null;        }                return url;    } catch (Exception e) {                        return null;    }}
public void nutch_f2872_0() throws Exception
{    String domainBlacklistFile = SAMPLES + SEPARATOR + "hosts.txt";    Configuration conf = NutchConfiguration.create();    DomainBlacklistURLFilter domainBlacklistFilter = new DomainBlacklistURLFilter(domainBlacklistFile);    domainBlacklistFilter.setConf(conf);    Assert.assertNull(domainBlacklistFilter.filter("http://lucene.apache.org"));    Assert.assertNull(domainBlacklistFilter.filter("http://hadoop.apache.org"));    Assert.assertNull(domainBlacklistFilter.filter("http://www.apache.org"));    Assert.assertNotNull(domainBlacklistFilter.filter("http://www.google.com"));    Assert.assertNotNull(domainBlacklistFilter.filter("http://mail.yahoo.com"));    Assert.assertNull(domainBlacklistFilter.filter("http://www.foobar.net"));    Assert.assertNull(domainBlacklistFilter.filter("http://www.foobas.net"));    Assert.assertNull(domainBlacklistFilter.filter("http://www.yahoo.com"));    Assert.assertNull(domainBlacklistFilter.filter("http://www.foobar.be"));    Assert.assertNotNull(domainBlacklistFilter.filter("http://www.adobe.com"));}
public void nutch_f2873_1(Configuration conf)
{    this.conf = conf;    try {        reloadRules();    } catch (Exception e) {                throw new RuntimeException(e.getMessage(), e);    }}
public Configuration nutch_f2874_0()
{    return this.conf;}
public String nutch_f2875_1(String url)
{    URL u;    try {        u = new URL(url);    } catch (Exception e) {                return null;    }    String hostname = u.getHost();        for (Rule rule : hostRules.get(hostname)) {        if (rule.match(u)) {            return null;        }    }        for (Rule rule : domainRules.get(hostname)) {        if (rule.match(u)) {            return null;        }    }            int start = 0;    int pos;    while ((pos = hostname.indexOf('.', start)) != -1) {        start = pos + 1;        String domain = hostname.substring(start);        for (Rule rule : domainRules.get(domain)) {            if (rule.match(u)) {                return null;            }        }    }        for (Rule rule : domainRules.get(".")) {        if (rule.match(u)) {            return null;        }    }        return url;}
public void nutch_f2876_0() throws IOException
{    String fileRules = conf.get(URLFILTER_FAST_FILE);    try (Reader reader = conf.getConfResourceAsReader(fileRules)) {        reloadRules(reader);    }}
private void nutch_f2877_1(Reader rules) throws IOException
{    domainRules.clear();    hostRules.clear();    BufferedReader reader = new BufferedReader(rules);    String current = null;    boolean host = false;    int lineno = 0;    String line;    try {        while ((line = reader.readLine()) != null) {            lineno++;            line = line.trim();            if (line.indexOf("#") != -1) {                                line = line.substring(0, line.indexOf("#")).trim();            }            if (StringUtils.isBlank(line)) {                continue;            }            if (line.startsWith("Host")) {                host = true;                current = line.split("\\s+")[1];            } else if (line.startsWith("Domain")) {                host = false;                current = line.split("\\s+")[1];            } else {                if (current == null) {                    continue;                }                Rule rule = null;                try {                    if (CATCH_ALL_RULE.matcher(line).matches()) {                        rule = DenyAllRule.getInstance();                    } else if (line.startsWith("DenyPathQuery")) {                        rule = new DenyPathQueryRule(line.split("\\s+")[1]);                    } else if (line.startsWith("DenyPath")) {                        rule = new DenyPathRule(line.split("\\s+")[1]);                    } else {                                                continue;                    }                } catch (Exception e) {                                        continue;                }                if (host) {                    LOG.trace("Adding host rule [{}] [{}]", current, rule);                    hostRules.put(current, rule);                } else {                    LOG.trace("Adding domain rule [{}] [{}]", current, rule);                    domainRules.put(current, rule);                }            }        }    } catch (IOException e) {                throw e;    }}
public boolean nutch_f2878_0(URL url)
{    return pattern.matcher(url.toString()).find();}
public String nutch_f2879_0()
{    return pattern.toString();}
public boolean nutch_f2880_0(URL url)
{    String haystack = url.getPath();    return pattern.matcher(haystack).find();}
public static Rule nutch_f2881_0()
{    return instance;}
public boolean nutch_f2882_0(URL url)
{    return true;}
public boolean nutch_f2883_0(URL url)
{    String haystack = url.getFile();    return pattern.matcher(haystack).find();}
protected URLFilter nutch_f2884_0(Reader rules)
{    try {        return new FastURLFilter(rules);    } catch (IOException e) {        Assert.fail(e.toString());        return null;    }}
public void nutch_f2885_0()
{    test("fast-urlfilter-test.txt", "test.urls");    test("fast-urlfilter-benchmark.txt", "Benchmarks.urls");}
public void nutch_f2886_0()
{    bench(50, "fast-urlfilter-benchmark.txt", "Benchmarks.urls");    bench(100, "fast-urlfilter-benchmark.txt", "Benchmarks.urls");    bench(200, "fast-urlfilter-benchmark.txt", "Benchmarks.urls");    bench(400, "fast-urlfilter-benchmark.txt", "Benchmarks.urls");    bench(800, "fast-urlfilter-benchmark.txt", "Benchmarks.urls");}
public List<Pattern> nutch_f2887_0()
{    return exemptions;}
public boolean nutch_f2888_0(String fromUrl, String toUrl)
{        return this.filter(toUrl) != null;}
protected Reader nutch_f2889_0(Configuration conf) throws IOException
{    String fileRules = conf.get(DB_IGNORE_EXTERNAL_EXEMPTIONS_FILE);    return conf.getConfResourceAsReader(fileRules);}
public static void nutch_f2890_0(String[] args)
{    if (args.length != 1) {        System.out.println("Error: Invalid Args");        System.out.println("Usage: " + ExemptionUrlFilter.class.getName() + " <url>");        return;    }    String url = args[0];    ExemptionUrlFilter instance = new ExemptionUrlFilter();    instance.setConf(NutchConfiguration.create());    System.out.println(instance.filter(null, url));}
public String nutch_f2891_0(String url)
{    if (trie.shortestMatch(url) == null)        return null;    else        return url;}
private TrieStringMatcher nutch_f2892_0(Reader reader) throws IOException
{    BufferedReader in = new BufferedReader(reader);    List<String> urlprefixes = new ArrayList<>();    String line;    while ((line = in.readLine()) != null) {        if (line.length() == 0)            continue;        char first = line.charAt(0);        switch(first) {            case ' ':            case '\n':            case             '#':                continue;            default:                urlprefixes.add(line);        }    }    return new PrefixStringMatcher(urlprefixes);}
public static void nutch_f2893_0(String[] args) throws IOException
{    PrefixURLFilter filter;    if (args.length >= 1)        filter = new PrefixURLFilter(args[0]);    else        filter = new PrefixURLFilter();    BufferedReader in = new BufferedReader(new InputStreamReader(System.in));    String line;    while ((line = in.readLine()) != null) {        String out = filter.filter(line);        if (out != null) {            System.out.println(out);        }    }}
public void nutch_f2894_1(Configuration conf)
{    this.conf = conf;    String pluginName = "urlfilter-prefix";    Extension[] extensions = PluginRepository.get(conf).getExtensionPoint(URLFilter.class.getName()).getExtensions();    for (int i = 0; i < extensions.length; i++) {        Extension extension = extensions[i];        if (extension.getDescriptor().getPluginId().equals(pluginName)) {            attributeFile = extension.getAttribute("file");            break;        }    }    if (attributeFile != null && attributeFile.trim().equals(""))        attributeFile = null;    if (attributeFile != null) {        if (LOG.isInfoEnabled()) {                    }    } else {                    }    String file = conf.get("urlfilter.prefix.file");    String stringRules = conf.get("urlfilter.prefix.rules");        if (attributeFile != null)        file = attributeFile;    Reader reader = null;    if (stringRules != null) {                reader = new StringReader(stringRules);    } else {        reader = conf.getConfResourceAsReader(file);    }    if (reader == null) {        trie = new PrefixStringMatcher(new String[0]);    } else {        try {            trie = readConfiguration(reader);        } catch (IOException e) {            if (LOG.isErrorEnabled()) {                            }                        throw new RuntimeException(e.getMessage(), e);        }    }}
public Configuration nutch_f2895_0()
{    return this.conf;}
public static Test nutch_f2896_0()
{    return new TestSuite(TestPrefixURLFilter.class);}
public static void nutch_f2897_0(String[] args)
{    TestRunner.run(suite());}
public void nutch_f2898_0() throws IOException
{    filter = new PrefixURLFilter(prefixes);}
public void nutch_f2899_0()
{    for (int i = 0; i < urls.length; i++) {        assertTrue(urlsModeAccept[i] == filter.filter(urls[i]));    }}
protected Reader nutch_f2900_0(Configuration conf) throws IOException
{    String stringRules = conf.get(URLFILTER_REGEX_RULES);    if (stringRules != null) {        return new StringReader(stringRules);    }    String fileRules = conf.get(URLFILTER_REGEX_FILE);    return conf.getConfResourceAsReader(fileRules);}
protected RegexRule nutch_f2901_0(boolean sign, String regex)
{    return new Rule(sign, regex);}
protected RegexRule nutch_f2902_0(boolean sign, String regex, String hostOrDomain)
{    return new Rule(sign, regex, hostOrDomain);}
public static void nutch_f2903_0(String[] args) throws IOException
{    RegexURLFilter filter = new RegexURLFilter();    filter.setConf(NutchConfiguration.create());    main(filter, args);}
protected boolean nutch_f2904_0(String url)
{    return pattern.matcher(url).find();}
protected URLFilter nutch_f2905_0(Reader rules)
{    try {        return new RegexURLFilter(rules);    } catch (IOException e) {        Assert.fail(e.toString());        return null;    }}
public void nutch_f2906_0()
{    test("WholeWebCrawling");    test("IntranetCrawling");    bench(50, "Benchmarks");    bench(100, "Benchmarks");    bench(200, "Benchmarks");    bench(400, "Benchmarks");    bench(800, "Benchmarks");}
public void nutch_f2907_0()
{    test("nutch1838");}
public String nutch_f2908_0(String url)
{    if (url == null)        return null;    String _url;    if (ignoreCase)        _url = url.toLowerCase();    else        _url = url;    if (filterFromPath) {        try {            URL pUrl = new URL(_url);            _url = pUrl.getPath();        } catch (MalformedURLException e) {                }    }    String a = suffixes.shortestMatch(_url);    if (a == null) {        if (modeAccept)            return url;        else            return null;    } else {        if (modeAccept)            return null;        else            return url;    }}
public void nutch_f2909_1(Reader reader) throws IOException
{        if (reader == null) {        if (LOG.isWarnEnabled()) {                    }        suffixes = new SuffixStringMatcher(new String[0]);        modeAccept = false;        ignoreCase = false;        return;    }    BufferedReader in = new BufferedReader(reader);    List<String> aSuffixes = new ArrayList<String>();    boolean allow = false;    boolean ignore = false;    String line;    while ((line = in.readLine()) != null) {        line = line.trim();        if (line.length() == 0)            continue;        char first = line.charAt(0);        switch(first) {            case ' ':            case '\n':            case             '#':                break;            case '-':                allow = false;                if (line.contains("P"))                    filterFromPath = true;                if (line.contains("I"))                    ignore = true;                break;            case '+':                allow = true;                if (line.contains("P"))                    filterFromPath = true;                if (line.contains("I"))                    ignore = true;                break;            default:                aSuffixes.add(line);        }    }    if (ignore) {        for (int i = 0; i < aSuffixes.size(); i++) {            aSuffixes.set(i, ((String) aSuffixes.get(i)).toLowerCase());        }    }    suffixes = new SuffixStringMatcher(aSuffixes);    modeAccept = allow;    ignoreCase = ignore;}
public static void nutch_f2910_0(String[] args) throws IOException
{    SuffixURLFilter filter;    if (args.length >= 1)        filter = new SuffixURLFilter(new FileReader(args[0]));    else {        filter = new SuffixURLFilter();        filter.setConf(NutchConfiguration.create());    }    BufferedReader in = new BufferedReader(new InputStreamReader(System.in));    String line;    while ((line = in.readLine()) != null) {        String out = filter.filter(line);        if (out != null) {            System.out.println("ACCEPTED " + out);        } else {            System.out.println("REJECTED " + out);        }    }}
public void nutch_f2911_1(Configuration conf)
{    this.conf = conf;    String pluginName = "urlfilter-suffix";    Extension[] extensions = PluginRepository.get(conf).getExtensionPoint(URLFilter.class.getName()).getExtensions();    for (int i = 0; i < extensions.length; i++) {        Extension extension = extensions[i];        if (extension.getDescriptor().getPluginId().equals(pluginName)) {            attributeFile = extension.getAttribute("file");            break;        }    }    if (attributeFile != null && attributeFile.trim().equals(""))        attributeFile = null;    if (attributeFile != null) {        if (LOG.isInfoEnabled()) {                    }    } else {                    }    String file = conf.get("urlfilter.suffix.file");    String stringRules = conf.get("urlfilter.suffix.rules");        if (attributeFile != null)        file = attributeFile;    Reader reader = null;    if (stringRules != null) {                reader = new StringReader(stringRules);    } else {        reader = conf.getConfResourceAsReader(file);    }    try {        readConfiguration(reader);    } catch (IOException e) {        if (LOG.isErrorEnabled()) {                    }        throw new RuntimeException(e.getMessage(), e);    }}
public Configuration nutch_f2912_0()
{    return this.conf;}
public boolean nutch_f2913_0()
{    return modeAccept;}
public void nutch_f2914_0(boolean modeAccept)
{    this.modeAccept = modeAccept;}
public boolean nutch_f2915_0()
{    return ignoreCase;}
public void nutch_f2916_0(boolean ignoreCase)
{    this.ignoreCase = ignoreCase;}
public void nutch_f2917_0(boolean filterFromPath)
{    this.filterFromPath = filterFromPath;}
public void nutch_f2918_0() throws IOException
{    filter = new SuffixURLFilter(new StringReader(suffixes));}
public void nutch_f2919_0()
{    filter.setIgnoreCase(false);    filter.setModeAccept(true);    for (int i = 0; i < urls.length; i++) {        Assert.assertTrue(urlsModeAccept[i] == filter.filter(urls[i]));    }}
public void nutch_f2920_0()
{    filter.setIgnoreCase(false);    filter.setModeAccept(false);    for (int i = 0; i < urls.length; i++) {        Assert.assertTrue(urlsModeReject[i] == filter.filter(urls[i]));    }}
public void nutch_f2921_0()
{    filter.setIgnoreCase(true);    filter.setModeAccept(true);    for (int i = 0; i < urls.length; i++) {        Assert.assertTrue(urlsModeAcceptIgnoreCase[i] == filter.filter(urls[i]));    }}
public void nutch_f2922_0()
{    filter.setIgnoreCase(true);    filter.setModeAccept(false);    for (int i = 0; i < urls.length; i++) {        Assert.assertTrue(urlsModeRejectIgnoreCase[i] == filter.filter(urls[i]));    }}
public void nutch_f2923_0()
{    filter.setModeAccept(true);    filter.setFilterFromPath(false);    for (int i = 0; i < urls.length; i++) {        Assert.assertTrue(urlsModeAcceptAndNonPathFilter[i] == filter.filter(urls[i]));    }}
public void nutch_f2924_0()
{    filter.setModeAccept(true);    filter.setFilterFromPath(true);    for (int i = 0; i < urls.length; i++) {        Assert.assertTrue(urlsModeAcceptAndPathFilter[i] == filter.filter(urls[i]));    }}
public String nutch_f2925_0(String urlString)
{    return isValid(urlString) ? urlString : null;}
public Configuration nutch_f2926_0()
{    return conf;}
public void nutch_f2927_0(Configuration conf)
{    this.conf = conf;}
private boolean nutch_f2928_0(String value)
{    if (value == null) {        return false;    }    Matcher matchUrlPat = URL_PATTERN.matcher(value);    if (!LEGAL_ASCII_PATTERN.matcher(value).matches()) {        return false;    }        if (!matchUrlPat.matches()) {        return false;    }    if (!isValidScheme(matchUrlPat.group(PARSE_URL_SCHEME))) {        return false;    }    if (!isValidAuthority(matchUrlPat.group(PARSE_URL_AUTHORITY))) {        return false;    }    if (!isValidPath(matchUrlPat.group(PARSE_URL_PATH))) {        return false;    }    if (!isValidQuery(matchUrlPat.group(PARSE_URL_QUERY))) {        return false;    }    return true;}
private boolean nutch_f2929_0(String scheme)
{    if (scheme == null) {        return false;    }    return SCHEME_PATTERN.matcher(scheme).matches();}
private boolean nutch_f2930_0(String authority)
{    if (authority == null) {        return false;    }    Matcher authorityMatcher = AUTHORITY_PATTERN.matcher(authority);    if (!authorityMatcher.matches()) {        return false;    }    boolean ipV4Address = false;    boolean hostname = false;        String hostIP = authorityMatcher.group(PARSE_AUTHORITY_HOST_IP);    Matcher matchIPV4Pat = IP_V4_DOMAIN_PATTERN.matcher(hostIP);    ipV4Address = matchIPV4Pat.matches();    if (ipV4Address) {                for (int i = 1; i <= 4; i++) {            String ipSegment = matchIPV4Pat.group(i);            if (ipSegment == null || ipSegment.length() <= 0) {                return false;            }            try {                if (Integer.parseInt(ipSegment) > 255) {                    return false;                }            } catch (NumberFormatException e) {                return false;            }        }    } else {                hostname = DOMAIN_PATTERN.matcher(hostIP).matches();    }        if (hostname) {                        char[] chars = hostIP.toCharArray();        int size = 1;        for (int i = 0; i < chars.length; i++) {            if (chars[i] == '.') {                size++;            }        }        String[] domainSegment = new String[size];        int segCount = 0;        int segLen = 0;        Matcher atomMatcher = ATOM_PATTERN.matcher(hostIP);        while (atomMatcher.find()) {            domainSegment[segCount] = atomMatcher.group();            segLen = domainSegment[segCount].length() + 1;            hostIP = (segLen >= hostIP.length()) ? "" : hostIP.substring(segLen);            segCount++;        }        String topLevel = domainSegment[segCount - 1];        if (topLevel.length() < 2) {            return false;        }                if (!ALPHA_PATTERN.matcher(topLevel.substring(0, 1)).matches()) {            return false;        }                if (segCount < 2) {            return false;        }    }    if (!hostname && !ipV4Address) {        return false;    }    String port = authorityMatcher.group(PARSE_AUTHORITY_PORT);    if (port != null) {        if (!PORT_PATTERN.matcher(port).matches()) {            return false;        }    }    String extra = authorityMatcher.group(PARSE_AUTHORITY_EXTRA);    return isBlankOrNull(extra);}
private boolean nutch_f2931_0(String value)
{    return ((value == null) || (value.trim().length() == 0));}
private boolean nutch_f2932_0(String path)
{    if (path == null) {        return false;    }    if (!PATH_PATTERN.matcher(path).matches()) {        return false;    }    int slash2Count = countToken("//", path);    int slashCount = countToken("/", path);    int dot2Count = countToken("..", path);    return (dot2Count <= 0) || ((slashCount - slash2Count - 1) > dot2Count);}
private boolean nutch_f2933_0(String query)
{    if (query == null) {        return true;    }    return QUERY_PATTERN.matcher(query).matches();}
private int nutch_f2934_0(String token, String target)
{    int tokenIndex = 0;    int count = 0;    while (tokenIndex != -1) {        tokenIndex = target.indexOf(token, tokenIndex);        if (tokenIndex > -1) {            tokenIndex++;            count++;        }    }    return count;}
public void nutch_f2935_0()
{    UrlValidator url_validator = new UrlValidator();    Assert.assertNotNull(url_validator);    Assert.assertNull("Filtering on a null object should return null", url_validator.filter(null));    Assert.assertNull("Invalid url: example.com/file[/].html", url_validator.filter("example.com/file[/].html"));    Assert.assertNull("Invalid url: http://www.example.com/space here.html", url_validator.filter("http://www.example.com/space here.html"));    Assert.assertNull("Invalid url: /main.html", url_validator.filter("/main.html"));    Assert.assertNull("Invalid url: www.example.com/main.html", url_validator.filter("www.example.com/main.html"));    Assert.assertNull("Invalid url: ftp:www.example.com/main.html", url_validator.filter("ftp:www.example.com/main.html"));    Assert.assertNull("Inalid url: http://999.000.456.32/nutch/trunk/README.txt", url_validator.filter("http://999.000.456.32/nutch/trunk/README.txt"));    Assert.assertNull("Invalid url: http://www.example.com/ma|in\\toc.html", url_validator.filter(" http://www.example.com/ma|in\\toc.html"));    Assert.assertNotNull("Valid url: https://issues.apache.org/jira/NUTCH-1127", url_validator.filter("https://issues.apache.org/jira/NUTCH-1127"));    Assert.assertNotNull("Valid url: http://domain.tld/function.cgi?url=http://fonzi.com/&amp;name=Fonzi&amp;mood=happy&amp;coat=leather", url_validator.filter("http://domain.tld/function.cgi?url=http://fonzi.com/&amp;name=Fonzi&amp;mood=happy&amp;coat=leather"));    Assert.assertNotNull("Valid url: http://validator.w3.org/feed/check.cgi?url=http%3A%2F%2Ffeeds.feedburner.com%2Fperishablepress", url_validator.filter("http://validator.w3.org/feed/check.cgi?url=http%3A%2F%2Ffeeds.feedburner.com%2Fperishablepress"));    Assert.assertNotNull("Valid url: ftp://alfa.bravo.pi/foo/bar/plan.pdf", url_validator.filter("ftp://alfa.bravo.pi/mike/check/plan.pdf"));}
public NutchDocument nutch_f2936_0(NutchDocument doc, Parse parse, Text url, CrawlDatum datum, Inlinks inlinks) throws IndexingException
{    if (conf != null)        this.setConf(conf);    if (urlMetaTags == null || doc == null)        return doc;    for (String metatag : urlMetaTags) {        Text metadata = (Text) datum.getMetaData().get(new Text(metatag));        if (metadata != null)            doc.add(metatag, metadata.toString());    }    return doc;}
public Configuration nutch_f2937_0()
{    return conf;}
public void nutch_f2938_0(Configuration conf)
{    this.conf = conf;    if (conf == null)        return;    urlMetaTags = conf.getStrings(CONF_PROPERTY);}
public CrawlDatum nutch_f2939_0(Text fromUrl, ParseData parseData, Collection<Entry<Text, CrawlDatum>> targets, CrawlDatum adjust, int allCount) throws ScoringFilterException
{    if (urlMetaTags == null || targets == null || parseData == null)        return adjust;    Iterator<Entry<Text, CrawlDatum>> targetIterator = targets.iterator();    while (targetIterator.hasNext()) {        Entry<Text, CrawlDatum> nextTarget = targetIterator.next();        for (String metatag : urlMetaTags) {            String metaFromParse = parseData.getMeta(metatag);            if (metaFromParse == null)                continue;            nextTarget.getValue().getMetaData().put(new Text(metatag), new Text(metaFromParse));        }    }    return adjust;}
public void nutch_f2940_0(Text url, CrawlDatum datum, Content content)
{    if (urlMetaTags == null || content == null || datum == null)        return;    for (String metatag : urlMetaTags) {        Text metaFromDatum = (Text) datum.getMetaData().get(new Text(metatag));        if (metaFromDatum == null)            continue;        content.getMetadata().set(metatag, metaFromDatum.toString());    }}
public void nutch_f2941_0(Text url, Content content, Parse parse)
{    if (urlMetaTags == null || content == null || parse == null)        return;    for (String metatag : urlMetaTags) {        String metaFromContent = content.getMetadata().get(metatag);        if (metaFromContent == null)            continue;        parse.getData().getParseMeta().set(metatag, metaFromContent);    }}
public float nutch_f2942_0(Text url, CrawlDatum datum, float initSort) throws ScoringFilterException
{    return initSort;}
public float nutch_f2943_0(Text url, NutchDocument doc, CrawlDatum dbDatum, CrawlDatum fetchDatum, Parse parse, Inlinks inlinks, float initScore) throws ScoringFilterException
{    return initScore;}
public void nutch_f2944_0(Text url, CrawlDatum datum) throws ScoringFilterException
{    return;}
public void nutch_f2945_0(Text url, CrawlDatum datum) throws ScoringFilterException
{    return;}
public void nutch_f2946_0(Text url, CrawlDatum old, CrawlDatum datum, List<CrawlDatum> inlinked) throws ScoringFilterException
{    return;}
public void nutch_f2947_0(Configuration conf)
{    super.setConf(conf);    if (conf == null)        return;    urlMetaTags = conf.getStrings(CONF_PROPERTY);}
public Configuration nutch_f2948_0()
{    return conf;}
protected String nutch_f2949_0(String urlString) throws MalformedURLException
{    URL u = new URL(urlString);    int pos = urlString.indexOf(AJAX_URL_PART);    StringBuilder sb = new StringBuilder(urlString.substring(0, pos));        String escapedFragment = escape(urlString.substring(pos + AJAX_URL_PART.length()));        if (u.getQuery() == null) {        sb.append("?");    } else {        sb.append("&");    }        sb.append(ESCAPED_URL_PART);    sb.append(escapedFragment);    return sb.toString();}
protected String nutch_f2950_0(String urlString) throws MalformedURLException
{    URL u = new URL(urlString);    StringBuilder sb = new StringBuilder();        sb.append(u.getProtocol());    sb.append("://");    sb.append(u.getHost());    if (u.getPort() != -1) {        sb.append(":");        sb.append(u.getPort());    }    sb.append(u.getPath());        String queryString = u.getQuery();        int ampPos = queryString.indexOf("&");    String keyValuePair = null;        if (ampPos == -1) {        keyValuePair = queryString;        queryString = "";    } else {                keyValuePair = queryString.substring(ampPos + 1);                queryString = queryString.replaceFirst("&" + keyValuePair, "");    }        keyValuePair = keyValuePair.replaceFirst(ESCAPED_URL_PART, "");        String unescapedFragment = unescape(keyValuePair);        if (queryString.length() > 0) {        sb.append("?");        sb.append(queryString);    }        sb.append("#!");    sb.append(unescapedFragment);    return sb.toString();}
protected String nutch_f2951_0(String fragmentPart)
{    try {        fragmentPart = URLDecoder.decode(fragmentPart, "UTF-8");    } catch (Exception e) {        }    return fragmentPart;}
protected String nutch_f2952_0(String fragmentPart)
{    String hex = null;    StringBuilder sb = new StringBuilder(fragmentPart.length());    for (byte b : fragmentPart.getBytes(utf8)) {        if (b < 33) {            sb.append('%');            hex = Integer.toHexString(b & 0xFF).toUpperCase();                        if (hex.length() % 2 != 0) {                sb.append('0');            }            sb.append(hex);        } else if (b == 35) {            sb.append("%23");        } else if (b == 37) {            sb.append("%25");        } else if (b == 38) {            sb.append("%26");        } else if (b == 43) {            sb.append("%2B");        } else {            sb.append((char) b);        }    }    return sb.toString();}
public void nutch_f2953_0(Configuration conf)
{    this.conf = conf;}
public Configuration nutch_f2954_0()
{    return this.conf;}
public void nutch_f2955_0() throws Exception
{        normalizeTest("http://example.org/#!k=v", "http://example.org/?_escaped_fragment_=k=v");        normalizeTest("http://example.org/#!k=v&something=is wrong", "http://example.org/?_escaped_fragment_=k=v%26something=is%20wrong");        normalizeTest("http://example.org/path.html?queryparam=queryvalue#!key1=value1&key2=value2", "http://example.org/path.html?queryparam=queryvalue&_escaped_fragment_=key1=value1%26key2=value2");}
public void nutch_f2956_0() throws Exception
{        normalizeTest("http://example.org/?_escaped_fragment_=key=value", "http://example.org/#!key=value", URLNormalizers.SCOPE_INDEXER);    normalizeTest("http://example.org/?key=value&_escaped_fragment_=key=value", "http://example.org/?key=value#!key=value", URLNormalizers.SCOPE_INDEXER);    normalizeTest("http://example.org/page.html?key=value&_escaped_fragment_=key=value%26something=is%20wrong", "http://example.org/page.html?key=value#!key=value&something=is wrong", URLNormalizers.SCOPE_INDEXER);}
private void nutch_f2957_0(String weird, String normal) throws Exception
{    assertEquals(normal, normalizer.normalize(weird, URLNormalizers.SCOPE_DEFAULT));}
private void nutch_f2958_0(String weird, String normal, String scope) throws Exception
{    assertEquals(normal, normalizer.normalize(weird, scope));}
public static void nutch_f2959_0(String[] args) throws Exception
{    new TestAjaxURLNormalizer("test").testNormalizer();}
private static boolean nutch_f2960_0(int c)
{    return (0x41 <= c && c <= 0x5A) || (0x61 <= c && c <= 0x7A) || (0x30 <= c && c <= 0x39);}
private static boolean nutch_f2961_0(int c)
{    return (0x41 <= c && c <= 0x46) || (0x61 <= c && c <= 0x66) || (0x30 <= c && c <= 0x39);}
public String nutch_f2962_0(String urlString, String scope) throws MalformedURLException
{    if (    "".equals(urlString))        return urlString;        urlString = urlString.trim();    URL url = new URL(urlString);    String protocol = url.getProtocol();    String host = url.getHost();    int port = url.getPort();    String file = url.getFile();    boolean changed = false;    boolean normalizePath = false;    if (    !urlString.startsWith(protocol))        changed = true;    if ("http".equals(protocol) || "https".equals(protocol) || "ftp".equals(protocol)) {        if (host != null && url.getAuthority() != null) {                        String newHost = host.toLowerCase(Locale.ROOT);            if (!host.equals(newHost)) {                host = newHost;                changed = true;            } else if (!url.getAuthority().equals(newHost)) {                                                changed = true;            }        } else {                        changed = true;        }        if (port == url.getDefaultPort()) {                                    port = -1;            changed = true;        }        normalizePath = true;        if (file == null || "".equals(file)) {            file = "/";            changed = true;                        normalizePath = false;        } else if (!file.startsWith("/")) {            file = "/" + file;            changed = true;                        normalizePath = false;        }        if (url.getRef() != null) {                        changed = true;        }    } else if (protocol.equals("file")) {        normalizePath = true;    }        String file2 = unescapePath(file);    file2 = escapePath(file2);    if (!file.equals(file2)) {        changed = true;        file = file2;    }    if (normalizePath) {                if (changed) {            url = new URL(protocol, host, port, file);        }        file2 = getFileWithNormalizedPath(url);        if (!file.equals(file2)) {            changed = true;            file = file2;        }    }    if (changed) {        url = new URL(protocol, host, port, file);        urlString = url.toString();    }    return urlString;}
private String nutch_f2963_0(URL url) throws MalformedURLException
{    String file;    if (hasNormalizablePathPattern.matcher(url.getPath()).find()) {                try {            file = url.toURI().normalize().toURL().getFile();                                    int start = 0;            while (file.startsWith("/..", start) && ((start + 3) == file.length() || file.charAt(3) == '/')) {                start += 3;            }            if (start > 0) {                file = file.substring(start);            }        } catch (URISyntaxException e) {            file = url.getFile();        }    } else {        file = url.getFile();    }        if (file.isEmpty()) {        file = "/";    } else if (!file.startsWith("/")) {        file = "/" + file;    }    return file;}
private String nutch_f2964_0(String path)
{    StringBuilder sb = new StringBuilder();    Matcher matcher = unescapeRulePattern.matcher(path);    int end = -1;    int letter;        while (matcher.find()) {                sb.append(path.substring(end + 1, matcher.start()));                letter = Integer.valueOf(matcher.group().substring(1), 16);        if (letter < 128 && unescapedCharacters[letter]) {                        sb.append(Character.valueOf((char) letter));        } else {                        sb.append(matcher.group().toUpperCase(Locale.ROOT));        }        end = matcher.start() + 2;    }    letter = path.length();        if (end <= letter - 1) {        sb.append(path.substring(end + 1, letter));    }        return sb.toString();}
private String nutch_f2965_0(String path)
{    StringBuilder sb = new StringBuilder(path.length());        byte[] bytes = path.getBytes(utf8);    for (int i = 0; i < bytes.length; i++) {        byte b = bytes[i];                if (b < 0 || escapedCharacters[b]) {                        sb.append('%');                        String hex = Integer.toHexString(b & 0xFF).toUpperCase(Locale.ROOT);                        if (hex.length() % 2 != 0) {                sb.append('0');                sb.append(hex);            } else {                                sb.append(hex);            }        } else if (b == 0x25) {                        if ((i + 2) >= bytes.length) {                                sb.append("%25");            } else {                byte e1 = bytes[i + 1];                byte e2 = bytes[i + 2];                if (isHexCharacter(e1) && isHexCharacter(e2)) {                                        i += 2;                    sb.append((char) b);                    sb.append((char) e1);                    sb.append((char) e2);                } else {                    sb.append("%25");                }            }        } else {                        sb.append((char) b);        }    }    return sb.toString();}
public static void nutch_f2966_0(String[] args) throws IOException
{    BasicURLNormalizer normalizer = new BasicURLNormalizer();    normalizer.setConf(NutchConfiguration.create());    String scope = URLNormalizers.SCOPE_DEFAULT;    if (args.length >= 1) {        scope = args[0];        System.out.println("Scope: " + scope);    }    String line, normUrl;    BufferedReader in = new BufferedReader(new InputStreamReader(System.in, utf8));    while ((line = in.readLine()) != null) {        try {            normUrl = normalizer.normalize(line, scope);            System.out.println(normUrl);        } catch (MalformedURLException e) {            System.out.println("failed: " + line);        }    }    System.exit(0);}
public void nutch_f2967_0() throws Exception
{        normalizeTest("http://foo.com/%66oo.html", "http://foo.com/foo.html");        normalizeTest("http://foo.com/%66oo.htm%6c", "http://foo.com/foo.html");    normalizeTest("http://foo.com/%66oo.ht%6dl", "http://foo.com/foo.html");        normalizeTest("http://foo.com/%66oo.ht%6d%6c", "http://foo.com/foo.html");        normalizeTest("http://foo.com/%66oo.htm%C0", "http://foo.com/foo.htm%C0");        normalizeTest("http://foo.com/%66oo.htm%1A", "http://foo.com/foo.htm%1A");        normalizeTest("http://foo.com/%66oo.htm%c0", "http://foo.com/foo.htm%C0");        normalizeTest("http://foo.com/you%20too.html", "http://foo.com/you%20too.html");        normalizeTest("http://foo.com/you too.html", "http://foo.com/you%20too.html");        normalizeTest("http://foo.com/file.html%23cz", "http://foo.com/file.html%23cz");        normalizeTest("http://foo.com/fast/dir%2fcz", "http://foo.com/fast/dir%2Fcz");        normalizeTest("http://foo.com/\u001a!", "http://foo.com/%1A!");        normalizeTest("http://foo.com/\u0001!", "http://foo.com/%01!");        normalizeTest("http://mydomain.com/en Espa\u00F1ol.aspx", "http://mydomain.com/en%20Espa%C3%B1ol.aspx");}
public void nutch_f2968_0() throws Exception
{        normalizeTest("http://x.com/s?q=a%26b&m=10", "http://x.com/s?q=a%26b&m=10");    normalizeTest("http://x.com/show?http%3A%2F%2Fx.com%2Fb", "http://x.com/show?http%3A%2F%2Fx.com%2Fb");    normalizeTest("http://google.com/search?q=c%2B%2B", "http://google.com/search?q=c%2B%2B");        normalizeTest("http://x.com/s?q=a+b", "http://x.com/s?q=a+b");                normalizeTest("http://b\u00fccher.de/", "http://b\u00fccher.de/");        normalizeTest("http://x.com/./a/../%66.html", "http://x.com/f.html");        normalizeTest("http://x.com/?x[y]=1", "http://x.com/?x%5By%5D=1");        normalizeTest("http://x.com/foo\u0080", "http://x.com/foo%C2%80");    normalizeTest("http://x.com/foo%c2%80", "http://x.com/foo%C2%80");}
public void nutch_f2969_0() throws Exception
{        normalizeTest(" http://foo.com/ ", "http://foo.com/");        normalizeTest("HTTP://foo.com/", "http://foo.com/");        normalizeTest("http://Foo.Com/index.html", "http://foo.com/index.html");    normalizeTest("http://Foo.Com/index.html", "http://foo.com/index.html");        normalizeTest("http://foo.com:80/index.html", "http://foo.com/index.html");    normalizeTest("http://foo.com:81/", "http://foo.com:81/");        normalizeTest("http://example.com:/", "http://example.com/");    normalizeTest("https://example.com:/foobar.html", "https://example.com/foobar.html");        normalizeTest("http://foo.com", "http://foo.com/");        normalizeTest("http://foo.com/foo.html#ref", "http://foo.com/foo.html");                normalizeTest("http://foo.com/..", "http://foo.com/");    normalizeTest("http://foo.com/aa/./foo.html", "http://foo.com/aa/foo.html");    normalizeTest("http://foo.com/aa/../", "http://foo.com/");    normalizeTest("http://foo.com/aa/bb/../", "http://foo.com/aa/");    normalizeTest("http://foo.com/aa/..", "http://foo.com/");    normalizeTest("http://foo.com/aa/bb/cc/../../foo.html", "http://foo.com/aa/foo.html");    normalizeTest("http://foo.com/aa/bb/../cc/dd/../ee/foo.html", "http://foo.com/aa/cc/ee/foo.html");    normalizeTest("http://foo.com/../foo.html", "http://foo.com/foo.html");    normalizeTest("http://foo.com/../../foo.html", "http://foo.com/foo.html");    normalizeTest("http://foo.com/../aa/../foo.html", "http://foo.com/foo.html");    normalizeTest("http://foo.com/aa/../../foo.html", "http://foo.com/foo.html");    normalizeTest("http://foo.com/aa/../bb/../foo.html/../../", "http://foo.com/");    normalizeTest("http://foo.com/../aa/foo.html", "http://foo.com/aa/foo.html");    normalizeTest("http://foo.com/../aa/../foo.html", "http://foo.com/foo.html");    normalizeTest("http://foo.com/a..a/foo.html", "http://foo.com/a..a/foo.html");    normalizeTest("http://foo.com/a..a/../foo.html", "http://foo.com/foo.html");    normalizeTest("http://foo.com/foo.foo/../foo.html", "http://foo.com/foo.html");    normalizeTest("http://foo.com//aa/bb/foo.html", "http://foo.com/aa/bb/foo.html");    normalizeTest("http://foo.com/aa//bb/foo.html", "http://foo.com/aa/bb/foo.html");    normalizeTest("http://foo.com/aa/bb//foo.html", "http://foo.com/aa/bb/foo.html");    normalizeTest("http://foo.com//aa//bb//foo.html", "http://foo.com/aa/bb/foo.html");    normalizeTest("http://foo.com////aa////bb////foo.html", "http://foo.com/aa/bb/foo.html");    normalizeTest("http://foo.com/aa?referer=http://bar.com", "http://foo.com/aa?referer=http://bar.com");        normalizeTest("file:///foo/bar.txt", "file:///foo/bar.txt");    normalizeTest("ftp:/", "ftp:/");    normalizeTest("http:", "http:/");    normalizeTest("http:////", "http:/");    normalizeTest("http:///////", "http:/");        normalizeTest("http://example.com?a=1", "http://example.com/?a=1");        normalizeTest("http://www.example.com/a/c/../b/search?q=foobar|", "http://www.example.com/a/b/search?q=foobar%7C");    normalizeTest("http://www.example.com/a/c/../b/search?q=foobar%", "http://www.example.com/a/b/search?q=foobar%25");    normalizeTest("http://www.example.com/a/c/../b/search?q=foobar\"", "http://www.example.com/a/b/search?q=foobar%22");    normalizeTest("http://www.example.com/a/c/../b/search?q=foobar^", "http://www.example.com/a/b/search?q=foobar%5E");    normalizeTest("http://www.example.com/a/c/../b/search?q=foobar<", "http://www.example.com/a/b/search?q=foobar%3C");    normalizeTest("http://www.example.com/a/c/../b/search?q=foobar>", "http://www.example.com/a/b/search?q=foobar%3E");    normalizeTest("http://www.example.com/a/c/../b/search?q=foobar`", "http://www.example.com/a/b/search?q=foobar%60");        normalizeTest("http://www.example.com/p%zz%77%v", "http://www.example.com/p%25zzw%25v");        normalizeTest("http://www.example.com/search?q=foobar%", "http://www.example.com/search?q=foobar%25");    normalizeTest("http://www.example.com/search?q=foobar%2", "http://www.example.com/search?q=foobar%252");    normalizeTest("http://www.example.com/search?q=foobar%25", "http://www.example.com/search?q=foobar%25");    normalizeTest("http://www.example.com/search?q=foobar%252", "http://www.example.com/search?q=foobar%252");        normalizeTest("file:/var/www/html/foo/../bar/index.html", "file:/var/www/html/bar/index.html");    normalizeTest("file:/var/www/html/////./bar/index.html", "file:/var/www/html/bar/index.html");}
public void nutch_f2970_0() throws Exception
{        normalizeTest("http://foo.com/{{stuff}} ", "http://foo.com/%7B%7Bstuff%7D%7D");}
private void nutch_f2971_0(String weird, String normal) throws Exception
{    Assert.assertEquals("normalizing: " + weird, normal, normalizer.normalize(weird, URLNormalizers.SCOPE_DEFAULT));    try {        (new URL(normal)).toURI();    } catch (MalformedURLException | URISyntaxException e) {        Assert.fail("Output of normalization fails to validate as URL or URI: " + e.getMessage());    }}
public static void nutch_f2972_0(String[] args) throws Exception
{    new TestBasicURLNormalizer().testNormalizer();}
private synchronized void nutch_f2973_0(Reader configReader) throws IOException
{    if (hostsMap.size() > 0) {        return;    }    BufferedReader reader = new BufferedReader(configReader);    String line, host, target;    int delimiterIndex;    while ((line = reader.readLine()) != null) {        if (StringUtils.isNotBlank(line) && !line.startsWith("#")) {            line = line.trim();            delimiterIndex = line.indexOf(" ");            host = line.substring(0, delimiterIndex);            target = line.substring(delimiterIndex + 1);            hostsMap.put(host, target);        }    }}
public Configuration nutch_f2974_0()
{    return conf;}
public void nutch_f2975_1(Configuration conf)
{    this.conf = conf;        String pluginName = "urlnormalizer-host";    Extension[] extensions = PluginRepository.get(conf).getExtensionPoint(URLNormalizer.class.getName()).getExtensions();    for (int i = 0; i < extensions.length; i++) {        Extension extension = extensions[i];        if (extension.getDescriptor().getPluginId().equals(pluginName)) {            attributeFile = extension.getAttribute("file");            break;        }    }        if (attributeFile != null && attributeFile.trim().equals("")) {        attributeFile = null;    }    if (attributeFile != null) {        if (LOG.isInfoEnabled()) {                    }    } else {        if (LOG.isWarnEnabled()) {                    }    }        String file = conf.get("urlnormalizer.hosts.file");    String stringRules = conf.get("urlnormalizer.hosts.rules");    if (hostsFile != null) {        file = hostsFile;    } else if (attributeFile != null) {        file = attributeFile;    }    Reader reader = null;    if (stringRules != null) {                reader = new StringReader(stringRules);    } else {        reader = conf.getConfResourceAsReader(file);    }    try {        if (reader == null) {            reader = new FileReader(file);        }        readConfiguration(reader);    } catch (IOException e) {            }}
public String nutch_f2976_0(String urlString, String scope) throws MalformedURLException
{    String host = new URL(urlString).getHost();        if (hostsMap.containsKey(host)) {        return replaceHost(urlString, host, hostsMap.get(host));    }        String[] hostParts = host.split("\\.");        StringBuilder hostBuffer = new StringBuilder();        String wildCardHost = new String();        hostBuffer.append(hostParts[hostParts.length - 1]);    for (int i = hostParts.length - 2; i > 0; i--) {                hostBuffer.insert(0, hostParts[i] + ".");                wildCardHost = "*." + hostBuffer.toString();                if (hostsMap.containsKey(wildCardHost)) {                        return replaceHost(urlString, host, hostsMap.get(wildCardHost));        }    }    return urlString;}
protected String nutch_f2977_0(String urlString, String host, String target)
{    int hostIndex = urlString.indexOf(host);    StringBuilder buffer = new StringBuilder();    buffer.append(urlString.substring(0, hostIndex));    buffer.append(target);    buffer.append(urlString.substring(hostIndex + host.length()));    return buffer.toString();}
public void nutch_f2978_0() throws Exception
{    Configuration conf = NutchConfiguration.create();    String hostsFile = SAMPLES + SEPARATOR + "hosts.txt";    HostURLNormalizer normalizer = new HostURLNormalizer(hostsFile);    normalizer.setConf(conf);        Assert.assertEquals("http://www.example.org/page.html", normalizer.normalize("http://example.org/page.html", URLNormalizers.SCOPE_DEFAULT));        Assert.assertEquals("http://example.net/path/to/something.html", normalizer.normalize("http://www.example.net/path/to/something.html", URLNormalizers.SCOPE_DEFAULT));        Assert.assertEquals("http://example.com/?does=it&still=work", normalizer.normalize("http://example.com/?does=it&still=work", URLNormalizers.SCOPE_DEFAULT));    Assert.assertEquals("http://example.com/buh", normalizer.normalize("http://http.www.example.com/buh", URLNormalizers.SCOPE_DEFAULT));    Assert.assertEquals("http://example.com/blaat", normalizer.normalize("http://whatever.example.com/blaat", URLNormalizers.SCOPE_DEFAULT));}
public String nutch_f2979_0(String urlString, String scope) throws MalformedURLException
{    return urlString;}
public Configuration nutch_f2980_0()
{    return conf;}
public void nutch_f2981_0(Configuration conf)
{    this.conf = conf;}
public void nutch_f2982_0()
{    Configuration conf = NutchConfiguration.create();    PassURLNormalizer normalizer = new PassURLNormalizer();    normalizer.setConf(conf);    String url = "http://www.example.com/test/..//";    String result = null;    try {        result = normalizer.normalize(url, URLNormalizers.SCOPE_DEFAULT);    } catch (MalformedURLException mue) {        Assert.fail(mue.toString());    }    Assert.assertEquals(url, result);}
private synchronized void nutch_f2983_0(Reader configReader) throws IOException
{    if (protocolsMap.size() > 0) {        return;    }    BufferedReader reader = new BufferedReader(configReader);    String line, host;    String protocol;    int delimiterIndex;    while ((line = reader.readLine()) != null) {        if (StringUtils.isNotBlank(line) && !line.startsWith("#")) {            line = line.trim();            delimiterIndex = line.indexOf(" ");                        if (delimiterIndex == -1) {                delimiterIndex = line.indexOf("\t");            }            host = line.substring(0, delimiterIndex);            protocol = line.substring(delimiterIndex + 1).trim();            protocolsMap.put(host, protocol);        }    }}
public Configuration nutch_f2984_0()
{    return conf;}
public void nutch_f2985_1(Configuration conf)
{    this.conf = conf;        String pluginName = "urlnormalizer-protocol";    Extension[] extensions = PluginRepository.get(conf).getExtensionPoint(URLNormalizer.class.getName()).getExtensions();    for (int i = 0; i < extensions.length; i++) {        Extension extension = extensions[i];        if (extension.getDescriptor().getPluginId().equals(pluginName)) {            attributeFile = extension.getAttribute("file");            break;        }    }        if (attributeFile != null && attributeFile.trim().equals("")) {        attributeFile = null;    }    if (attributeFile != null) {        if (LOG.isInfoEnabled()) {                    }    } else {        if (LOG.isWarnEnabled()) {                    }    }        String file = conf.get("urlnormalizer.protocols.file");    String stringRules = conf.get("urlnormalizer.protocols.rules");    if (protocolsFile != null) {        file = protocolsFile;    } else if (attributeFile != null) {        file = attributeFile;    }    Reader reader = null;    if (stringRules != null) {                reader = new StringReader(stringRules);    } else {        reader = conf.getConfResourceAsReader(file);    }    try {        if (reader == null) {            reader = new FileReader(file);        }        readConfiguration(reader);    } catch (IOException e) {            }}
public String nutch_f2986_0(String url, String scope) throws MalformedURLException
{    return normalize(url, null, scope);}
public String nutch_f2987_0(String url, CrawlDatum crawlDatum, String scope) throws MalformedURLException
{        URL u = new URL(url);        String host = u.getHost();        if (protocolsMap.containsKey(host)) {        String protocol = u.getProtocol();        String requiredProtocol = protocolsMap.get(host);                if (!protocol.equals(requiredProtocol)) {                        StringBuilder buffer = new StringBuilder(requiredProtocol);            buffer.append(PROTOCOL_DELIMITER);            buffer.append(host);            buffer.append(u.getPath());            String queryString = u.getQuery();            if (queryString != null) {                buffer.append(QUESTION_MARK);                buffer.append(queryString);            }            url = buffer.toString();        }    }    return url;}
public void nutch_f2988_0() throws Exception
{    Configuration conf = NutchConfiguration.create();    String protocolsFile = SAMPLES + SEPARATOR + "protocols.txt";    ProtocolURLNormalizer normalizer = new ProtocolURLNormalizer(protocolsFile);    normalizer.setConf(conf);        assertEquals("http://example.org/", normalizer.normalize("https://example.org/", URLNormalizers.SCOPE_DEFAULT));    assertEquals("http://example.net/", normalizer.normalize("https://example.net/", URLNormalizers.SCOPE_DEFAULT));        assertEquals("http://example.org/", normalizer.normalize("https://example.org/", URLNormalizers.SCOPE_DEFAULT));    assertEquals("http://example.net/", normalizer.normalize("https://example.net/", URLNormalizers.SCOPE_DEFAULT));        assertEquals("https://example.io/", normalizer.normalize("https://example.io/", URLNormalizers.SCOPE_DEFAULT));    assertEquals("https://example.nl/", normalizer.normalize("https://example.nl/", URLNormalizers.SCOPE_DEFAULT));        assertEquals("https://example.io/", normalizer.normalize("http://example.io/", URLNormalizers.SCOPE_DEFAULT));    assertEquals("https://example.nl/", normalizer.normalize("http://example.nl/", URLNormalizers.SCOPE_DEFAULT));}
public Configuration nutch_f2989_0()
{    return conf;}
public void nutch_f2990_0(Configuration conf)
{    this.conf = conf;}
public String nutch_f2991_0(String urlString, String scope) throws MalformedURLException
{    URL url = new URL(urlString);    String queryString = url.getQuery();    if (queryString == null) {        return urlString;    }    List<String> queryStringParts = Arrays.asList(queryString.split("&"));    Collections.sort(queryStringParts);    StringBuilder sb = new StringBuilder();    sb.append(url.getProtocol());    sb.append("://");    sb.append(url.getHost());    if (url.getPort() > -1) {        sb.append(":");        sb.append(url.getPort());    }    sb.append(url.getPath());    sb.append("?");    sb.append(StringUtils.join(queryStringParts, "&"));    if (url.getRef() != null) {        sb.append("#");        sb.append(url.getRef());    }    return sb.toString();}
public void nutch_f2992_0() throws Exception
{    Configuration conf = NutchConfiguration.create();    QuerystringURLNormalizer normalizer = new QuerystringURLNormalizer();    normalizer.setConf(conf);    assertEquals("http://example.com/?a=b&c=d", normalizer.normalize("http://example.com/?c=d&a=b", URLNormalizers.SCOPE_DEFAULT));    assertEquals("http://example.com/a/b/c", normalizer.normalize("http://example.com/a/b/c", URLNormalizers.SCOPE_DEFAULT));    assertEquals("http://example.com:1234/a/b/c", normalizer.normalize("http://example.com:1234/a/b/c", URLNormalizers.SCOPE_DEFAULT));    assertEquals("http://example.com:1234/a/b/c#ref", normalizer.normalize("http://example.com:1234/a/b/c#ref", URLNormalizers.SCOPE_DEFAULT));    assertEquals("http://example.com:1234/a/b/c?a=b&c=d#ref", normalizer.normalize("http://example.com:1234/a/b/c?c=d&a=b#ref", URLNormalizers.SCOPE_DEFAULT));    assertEquals("http://example.com/?a=b&a=c&c=d", normalizer.normalize("http://example.com/?c=d&a=b&a=c", URLNormalizers.SCOPE_DEFAULT));}
protected java.util.HashMap<String, java.util.List<Rule>> nutch_f2993_0()
{    return new HashMap<String, List<Rule>>();}
public HashMap<String, List<Rule>> nutch_f2994_0()
{    return scopedRulesThreadLocal.get();}
public void nutch_f2995_1(Configuration conf)
{    super.setConf(conf);    if (conf == null)        return;        String filename = getConf().get("urlnormalizer.regex.file");    String stringRules = getConf().get("urlnormalizer.regex.rules");    Reader reader = null;    if (stringRules != null) {        reader = new StringReader(stringRules);    } else {        reader = getConf().getConfResourceAsReader(filename);    }    List<Rule> rules = null;    if (reader == null) {                rules = EMPTY_RULES;    } else {        try {            rules = readConfiguration(reader);        } catch (Exception e) {                        rules = EMPTY_RULES;        }    }    defaultRules = rules;}
 void nutch_f2996_1(Reader reader, String scope)
{    List<Rule> rules = readConfiguration(reader);    getScopedRules().put(scope, rules);    }
public String nutch_f2997_1(String urlString, String scope)
{    HashMap<String, List<Rule>> scopedRules = getScopedRules();    List<Rule> curRules = scopedRules.get(scope);    if (curRules == null) {                String configFile = getConf().get("urlnormalizer.regex.file." + scope);        if (configFile != null) {                        try {                Reader reader = getConf().getConfResourceAsReader(configFile);                curRules = readConfiguration(reader);                scopedRules.put(scope, curRules);            } catch (Exception e) {                            }        }        if (curRules == EMPTY_RULES || curRules == null) {                        scopedRules.put(scope, EMPTY_RULES);        }    }    if (curRules == EMPTY_RULES || curRules == null) {        curRules = defaultRules;    }    Iterator<Rule> i = curRules.iterator();    while (i.hasNext()) {        Rule r = (Rule) i.next();        Matcher matcher = r.pattern.matcher(urlString);        urlString = matcher.replaceAll(r.substitution);    }    return urlString;}
public String nutch_f2998_0(String urlString, String scope) throws MalformedURLException
{    return regexNormalize(urlString, scope);}
private List<Rule> nutch_f2999_1(String filename)
{    if (LOG.isInfoEnabled()) {            }    try {        FileReader reader = new FileReader(filename);        return readConfiguration(reader);    } catch (Exception e) {                return EMPTY_RULES;    }}
private List<Rule> nutch_f3000_1(Reader reader)
{    List<Rule> rules = new ArrayList<Rule>();    try {                Document doc = DocumentBuilderFactory.newInstance().newDocumentBuilder().parse(new InputSource(reader));        Element root = doc.getDocumentElement();        if ((!"regex-normalize".equals(root.getTagName())) && (LOG.isErrorEnabled())) {                    }        NodeList regexes = root.getChildNodes();        for (int i = 0; i < regexes.getLength(); i++) {            Node regexNode = regexes.item(i);            if (!(regexNode instanceof Element))                continue;            Element regex = (Element) regexNode;            if ((!"regex".equals(regex.getTagName())) && (LOG.isWarnEnabled())) {                            }            NodeList fields = regex.getChildNodes();            String patternValue = null;            String subValue = null;            for (int j = 0; j < fields.getLength(); j++) {                Node fieldNode = fields.item(j);                if (!(fieldNode instanceof Element))                    continue;                Element field = (Element) fieldNode;                if ("pattern".equals(field.getTagName()) && field.hasChildNodes())                    patternValue = ((Text) field.getFirstChild()).getData();                if ("substitution".equals(field.getTagName()) && field.hasChildNodes())                    subValue = ((Text) field.getFirstChild()).getData();                if (!field.hasChildNodes())                    subValue = "";            }            if (patternValue != null && subValue != null) {                Rule rule = new Rule();                try {                    rule.pattern = Pattern.compile(patternValue);                } catch (PatternSyntaxException e) {                    if (LOG.isErrorEnabled()) {                                            }                    continue;                }                rule.substitution = subValue;                rules.add(rule);            }        }    } catch (Exception e) {        if (LOG.isErrorEnabled()) {                    }        return EMPTY_RULES;    }    if (rules.size() == 0)        return EMPTY_RULES;    return rules;}
public static void nutch_f3001_0(String[] args) throws PatternSyntaxException, IOException
{    RegexURLNormalizer normalizer = new RegexURLNormalizer();    normalizer.setConf(NutchConfiguration.create());    HashMap<String, List<Rule>> scopedRules = normalizer.getScopedRules();    Iterator<Rule> i = normalizer.defaultRules.iterator();    System.out.println("* Rules for 'DEFAULT' scope:");    while (i.hasNext()) {        Rule r = i.next();        System.out.print("  " + r.pattern.pattern() + " -> ");        System.out.println(r.substitution);    }        if (args.length > 1) {        normalizer.normalize("http://test.com", args[1]);    }    if (scopedRules.size() > 1) {        Iterator<String> it = scopedRules.keySet().iterator();        while (it.hasNext()) {            String scope = it.next();            if (URLNormalizers.SCOPE_DEFAULT.equals(scope))                continue;            System.out.println("* Rules for '" + scope + "' scope:");            i = ((List<Rule>) scopedRules.get(scope)).iterator();            while (i.hasNext()) {                Rule r = (Rule) i.next();                System.out.print("  " + r.pattern.pattern() + " -> ");                System.out.println(r.substitution);            }        }    }    if (args.length > 0) {        System.out.println("\n---------- Normalizer test -----------");        String scope = URLNormalizers.SCOPE_DEFAULT;        if (args.length > 1)            scope = args[1];        System.out.println("Scope: " + scope);        System.out.println("Input url:  '" + args[0] + "'");        System.out.println("Output url: '" + normalizer.normalize(args[0], scope) + "'");    }    System.exit(0);}
public boolean nutch_f3002_0(File f)
{    if (f.getName().endsWith(".xml") && f.getName().startsWith("regex-normalize-"))        return true;    return false;}
public void nutch_f3003_0() throws Exception
{    normalizeTest((NormalizedURL[]) testData.get(URLNormalizers.SCOPE_DEFAULT), URLNormalizers.SCOPE_DEFAULT);}
public void nutch_f3004_0() throws Exception
{    Iterator<String> it = testData.keySet().iterator();    while (it.hasNext()) {        String scope = it.next();        normalizeTest((NormalizedURL[]) testData.get(scope), scope);    }}
private void nutch_f3005_1(NormalizedURL[] urls, String scope) throws Exception
{    for (int i = 0; i < urls.length; i++) {        String url = urls[i].url;        String normalized = normalizer.normalize(urls[i].url, scope);        String expected = urls[i].expectedURL;                Assert.assertEquals(urls[i].expectedURL, normalized);    }}
private void nutch_f3006_1(int loops, String scope)
{    long start = System.currentTimeMillis();    try {        NormalizedURL[] expected = (NormalizedURL[]) testData.get(scope);        if (expected == null)            return;        for (int i = 0; i < loops; i++) {            normalizeTest(expected, scope);        }    } catch (Exception e) {        Assert.fail(e.toString());    }    }
private NormalizedURL[] nutch_f3007_0(String scope) throws IOException
{    File f = new File(sampleDir, "regex-normalize-" + scope + ".test");    @SuppressWarnings("resource")    BufferedReader in = new BufferedReader(new InputStreamReader(new FileInputStream(f), "UTF-8"));    List<NormalizedURL> list = new ArrayList<NormalizedURL>();    String line;    while ((line = in.readLine()) != null) {        if (line.trim().length() == 0 || line.startsWith("#") || line.startsWith(" "))            continue;        list.add(new NormalizedURL(line));    }    return (NormalizedURL[]) list.toArray(new NormalizedURL[list.size()]);}
public static void nutch_f3008_1(String[] args) throws Exception
{    if (args.length == 0) {        System.err.println("TestRegexURLNormalizer [-bench <iter>] <scope>");        System.exit(-1);    }    boolean bench = false;    int iter = -1;    String scope = null;    for (int i = 0; i < args.length; i++) {        if (args[i].equals("-bench")) {            bench = true;            iter = Integer.parseInt(args[++i]);        } else            scope = args[i];    }    if (scope == null) {        System.err.println("Missing required scope name.");        System.exit(-1);    }    if (bench && iter < 0) {        System.err.println("Invalid number of iterations: " + iter);        System.exit(-1);    }    TestRegexURLNormalizer test = new TestRegexURLNormalizer();    NormalizedURL[] urls = (NormalizedURL[]) test.testData.get(scope);    if (urls == null) {                scope = URLNormalizers.SCOPE_DEFAULT;        urls = (NormalizedURL[]) test.testData.get(scope);    }    if (bench) {        test.bench(iter, scope);    } else {        test.normalizeTest(urls, scope);    }}
private synchronized void nutch_f3009_0(Reader configReader) throws IOException
{    if (slashesMap.size() > 0) {        return;    }    BufferedReader reader = new BufferedReader(configReader);    String line, host;    String rule;    int delimiterIndex;    while ((line = reader.readLine()) != null) {        if (StringUtils.isNotBlank(line) && !line.startsWith("#")) {            line = line.trim();            delimiterIndex = line.indexOf(" ");                        if (delimiterIndex == -1) {                delimiterIndex = line.indexOf("\t");            }            host = line.substring(0, delimiterIndex);            rule = line.substring(delimiterIndex + 1).trim();            if (rule.equals("+")) {                slashesMap.put(host, true);            } else {                slashesMap.put(host, false);            }        }    }}
public Configuration nutch_f3010_0()
{    return conf;}
public void nutch_f3011_1(Configuration conf)
{    this.conf = conf;        String pluginName = "urlnormalizer-slash";    Extension[] extensions = PluginRepository.get(conf).getExtensionPoint(URLNormalizer.class.getName()).getExtensions();    for (int i = 0; i < extensions.length; i++) {        Extension extension = extensions[i];        if (extension.getDescriptor().getPluginId().equals(pluginName)) {            attributeFile = extension.getAttribute("file");            break;        }    }        if (attributeFile != null && attributeFile.trim().equals("")) {        attributeFile = null;    }    if (attributeFile != null) {        if (LOG.isInfoEnabled()) {                    }    } else {        if (LOG.isWarnEnabled()) {                    }    }        String file = conf.get("urlnormalizer.slashes.file");    String stringRules = conf.get("urlnormalizer.slashes.rules");    if (slashesFile != null) {        file = slashesFile;    } else if (attributeFile != null) {        file = attributeFile;    }    Reader reader = null;    if (stringRules != null) {                reader = new StringReader(stringRules);    } else {        reader = conf.getConfResourceAsReader(file);    }    try {        if (reader == null) {            reader = new FileReader(file);        }        readConfiguration(reader);    } catch (IOException e) {            }}
public String nutch_f3012_0(String url, String scope) throws MalformedURLException
{    return normalize(url, null, scope);}
public String nutch_f3013_0(String url, CrawlDatum crawlDatum, String scope) throws MalformedURLException
{        URL u = new URL(url);        String host = u.getHost();        if (slashesMap.containsKey(host)) {                String protocol = u.getProtocol();        String path = u.getPath();                if (path.length() > 1) {            String queryString = u.getQuery();                        boolean rule = slashesMap.get(host);                        int lastIndexOfSlash = path.lastIndexOf(SLASH);            boolean trailingSlash = (lastIndexOfSlash == path.length() - 1);                        if (!trailingSlash && rule) {                                int lastIndexOfDot = path.lastIndexOf(DOT);                if (path.length() < 6 || lastIndexOfDot == -1 || lastIndexOfDot < path.length() - 6) {                    StringBuilder buffer = new StringBuilder(protocol);                    buffer.append(PROTOCOL_DELIMITER);                    buffer.append(host);                    buffer.append(path);                    buffer.append(SLASH);                    if (queryString != null) {                        buffer.append(QUESTION_MARK);                        buffer.append(queryString);                    }                    url = buffer.toString();                }            } else             if (trailingSlash && !rule) {                StringBuilder buffer = new StringBuilder(protocol);                buffer.append(PROTOCOL_DELIMITER);                buffer.append(host);                buffer.append(path.substring(0, lastIndexOfSlash));                if (queryString != null) {                    buffer.append(QUESTION_MARK);                    buffer.append(queryString);                }                url = buffer.toString();            }        }    }    return url;}
public void nutch_f3014_0() throws Exception
{    Configuration conf = NutchConfiguration.create();    String slashesFile = SAMPLES + SEPARATOR + "slashes.txt";    SlashURLNormalizer normalizer = new SlashURLNormalizer(slashesFile);    normalizer.setConf(conf);        assertEquals("http://example.org/", normalizer.normalize("http://example.org/", URLNormalizers.SCOPE_DEFAULT));    assertEquals("http://example.net/", normalizer.normalize("http://example.net/", URLNormalizers.SCOPE_DEFAULT));        assertEquals("http://example.org", normalizer.normalize("http://example.org", URLNormalizers.SCOPE_DEFAULT));    assertEquals("http://example.net", normalizer.normalize("http://example.net", URLNormalizers.SCOPE_DEFAULT));    assertEquals("http://example.org/", normalizer.normalize("http://example.org/", URLNormalizers.SCOPE_DEFAULT));    assertEquals("http://example.net/", normalizer.normalize("http://example.net/", URLNormalizers.SCOPE_DEFAULT));        assertEquals("http://www.example.org/page/", normalizer.normalize("http://www.example.org/page", URLNormalizers.SCOPE_DEFAULT));    assertEquals("http://www.example.net/path/to/something", normalizer.normalize("http://www.example.net/path/to/something/", URLNormalizers.SCOPE_DEFAULT));        assertEquals("http://example.org/buh/", normalizer.normalize("http://example.org/buh/", URLNormalizers.SCOPE_DEFAULT));    assertEquals("http://example.net/blaat", normalizer.normalize("http://example.net/blaat", URLNormalizers.SCOPE_DEFAULT));        assertEquals("http://example.nl/buh/", normalizer.normalize("http://example.nl/buh/", URLNormalizers.SCOPE_DEFAULT));    assertEquals("http://example.de/blaat", normalizer.normalize("http://example.de/blaat", URLNormalizers.SCOPE_DEFAULT));        assertEquals("http://www.example.org/page/?a=b&c=d", normalizer.normalize("http://www.example.org/page?a=b&c=d", URLNormalizers.SCOPE_DEFAULT));    assertEquals("http://www.example.net/path/to/something?a=b&c=d", normalizer.normalize("http://www.example.net/path/to/something/?a=b&c=d", URLNormalizers.SCOPE_DEFAULT));        assertEquals("http://www.example.org/noise.mp3", normalizer.normalize("http://www.example.org/noise.mp3", URLNormalizers.SCOPE_DEFAULT));    assertEquals("http://www.example.org/page.html", normalizer.normalize("http://www.example.org/page.html", URLNormalizers.SCOPE_DEFAULT));    assertEquals("http://www.example.org/page.shtml", normalizer.normalize("http://www.example.org/page.shtml", URLNormalizers.SCOPE_DEFAULT));        assertEquals("http://www.example.org/this.is.not.an_extension/", normalizer.normalize("http://www.example.org/this.is.not.an_extension", URLNormalizers.SCOPE_DEFAULT));}
protected void nutch_f3015_0(int seconds)
{    interval = seconds * 1000L;}
protected void nutch_f3016_0(int seconds)
{    duration = seconds * 1000L;}
protected CrawlDatum nutch_f3017_0(CrawlDatum datum, long currentTime)
{    datum.setStatus(fetchStatus);    datum.setFetchTime(currentTime);    return datum;}
protected byte[] nutch_f3018_0()
{    return signatureImpl.calculate(content, null);}
protected void nutch_f3019_1()
{    byte[] data = Arrays.copyOf(content.getContent(), content.getContent().length + 1);        data[content.getContent().length] = '2';    content.setContent(data);    }
protected List<CrawlDatum> nutch_f3020_0(CrawlDatum fetchDatum)
{    List<CrawlDatum> parseDatums = new ArrayList<CrawlDatum>(0);    if (fetchDatum.getStatus() == CrawlDatum.STATUS_FETCH_SUCCESS) {        CrawlDatum signatureDatum = new CrawlDatum(CrawlDatum.STATUS_SIGNATURE, 0);        signatureDatum.setSignature(getSignature());        parseDatums.add(signatureDatum);    }    return parseDatums;}
protected boolean nutch_f3021_0(CrawlDatum datum)
{    if (datum.getStatus() != expectedDbStatus)        return false;    return true;}
protected boolean nutch_f3022_1(int maxErrors) throws IOException
{    long now = System.currentTimeMillis();    CrawlDbUpdateUtil<CrawlDbReducer> updateDb = new CrawlDbUpdateUtil<CrawlDbReducer>(new CrawlDbReducer(), context);    /* start with a db_unfetched */    CrawlDatum dbDatum = new CrawlDatum();    dbDatum.setStatus(CrawlDatum.STATUS_DB_UNFETCHED);        schedule.initializeSchedule(dummyURL, dbDatum);    dbDatum.setFetchTime(now);        long maxTime = (now + duration);    long nextTime = now;    long lastFetchTime = -1;        boolean ok = true;    CrawlDatum fetchDatum = new CrawlDatum();    /*     * Keep copies because CrawlDbReducer.reduce() and     * FetchSchedule.shouldFetch() may alter the references. Copies are used for     * verbose logging in case of an error.     */    CrawlDatum copyDbDatum = new CrawlDatum();    CrawlDatum copyFetchDatum = new CrawlDatum();    CrawlDatum afterShouldFetch = new CrawlDatum();    int errorCount = 0;    while (nextTime < maxTime) {                fetchDatum.set(dbDatum);        copyDbDatum.set(dbDatum);        if (schedule.shouldFetch(dummyURL, fetchDatum, nextTime)) {                        if (lastFetchTime > -1) {                            }            lastFetchTime = nextTime;            afterShouldFetch.set(fetchDatum);            fetchDatum = fetch(fetchDatum, nextTime);            copyFetchDatum.set(fetchDatum);            List<CrawlDatum> values = new ArrayList<CrawlDatum>();            values.add(dbDatum);            values.add(fetchDatum);            values.addAll(parse(fetchDatum));            List<CrawlDatum> res = updateDb.update(values);            assertNotNull("null returned", res);            assertFalse("no CrawlDatum", 0 == res.size());            assertEquals("more than one CrawlDatum", 1, res.size());            if (!check(res.get(0))) {                                                                                if (++errorCount >= maxErrors) {                    if (maxErrors > 0) {                                            }                    return false;                } else {                                        ok = false;                }            }            /* use the returned CrawlDatum for the next fetch */            dbDatum = res.get(0);        }        nextTime += interval;    }    return ok;}
public static void nutch_f3023_1(Configuration conf, FileSystem fs, Path crawldb, List<URLCrawlDatum> init) throws Exception
{    LOG.trace("* creating crawldb: " + crawldb);    Path dir = new Path(crawldb, CrawlDb.CURRENT_NAME);    Option wKeyOpt = MapFile.Writer.keyClass(Text.class);    org.apache.hadoop.io.SequenceFile.Writer.Option wValueOpt = SequenceFile.Writer.valueClass(CrawlDatum.class);    MapFile.Writer writer = new MapFile.Writer(conf, new Path(dir, "part-r-00000"), wKeyOpt, wValueOpt);    Iterator<URLCrawlDatum> it = init.iterator();    while (it.hasNext()) {        URLCrawlDatum row = it.next();                writer.append(new Text(row.url), row.datum);    }    writer.close();}
public void nutch_f3024_0(Text key, CrawlDatum value) throws IOException, InterruptedException
{    values.add(value);}
public List<CrawlDatum> nutch_f3025_0()
{    return values;}
public CrawlDatum nutch_f3026_0() throws UnsupportedOperationException
{    throw new UnsupportedOperationException("Dummy context");}
public Text nutch_f3027_0() throws UnsupportedOperationException
{    throw new UnsupportedOperationException("Dummy context with no keys");}
public void nutch_f3028_0()
{}
public Counter nutch_f3029_0(Enum<?> arg0)
{    return dummyCounters.getGroup("dummy").getCounterForName("dummy");}
public Counter nutch_f3030_0(String arg0, String arg1)
{    return dummyCounters.getGroup("dummy").getCounterForName("dummy");}
public void nutch_f3031_0(String arg0) throws UnsupportedOperationException
{    throw new UnsupportedOperationException("Dummy context with no status");}
public String nutch_f3032_0() throws UnsupportedOperationException
{    throw new UnsupportedOperationException("Dummy context with no status");}
public float nutch_f3033_0()
{    return 1f;}
public OutputCommitter nutch_f3034_0()
{    throw new UnsupportedOperationException("Dummy context without committer");}
public boolean nutch_f3035_0()
{    return false;}
public boolean nutch_f3036_0()
{    return false;}
public TaskAttemptID nutch_f3037_0() throws UnsupportedOperationException
{    throw new UnsupportedOperationException("Dummy context without TaskAttemptID");}
public Path[] nutch_f3038_0()
{    return null;}
public String[] nutch_f3039_0()
{    return null;}
public URI[] nutch_f3040_0() throws IOException
{    return null;}
public URI[] nutch_f3041_0() throws IOException
{    return null;}
public Class<? extends Reducer<?, ?, ?, ?>> nutch_f3042_0() throws ClassNotFoundException
{    return null;}
public RawComparator<?> nutch_f3043_0()
{    return null;}
public Configuration nutch_f3044_0()
{    return conf;}
public Credentials nutch_f3045_0()
{    return null;}
public Path[] nutch_f3046_0()
{    return null;}
public String[] nutch_f3047_0()
{    return null;}
public RawComparator<?> nutch_f3048_0()
{    return null;}
public Class<? extends InputFormat<?, ?>> nutch_f3049_0() throws ClassNotFoundException
{    return null;}
public String nutch_f3050_0()
{    return null;}
public JobID nutch_f3051_0()
{    return null;}
public String nutch_f3052_0()
{    return null;}
public boolean nutch_f3053_0()
{    return false;}
public Path[] nutch_f3054_0() throws IOException
{    return null;}
public Path[] nutch_f3055_0() throws IOException
{    return null;}
public Class<?> nutch_f3056_0()
{    return null;}
public Class<?> nutch_f3057_0()
{    return null;}
public Class<? extends Mapper<?, ?, ?, ?>> nutch_f3058_0() throws ClassNotFoundException
{    return null;}
public int nutch_f3059_0()
{    return 0;}
public int nutch_f3060_0()
{    return 0;}
public int nutch_f3061_0()
{    return 0;}
public Class<? extends OutputFormat<?, ?>> nutch_f3062_0() throws ClassNotFoundException
{    return null;}
public Class<?> nutch_f3063_0()
{    return null;}
public Class<?> nutch_f3064_0()
{    return null;}
public Class<? extends Partitioner<?, ?>> nutch_f3065_0() throws ClassNotFoundException
{    return null;}
public boolean nutch_f3066_0()
{    return false;}
public String nutch_f3067_0()
{    return null;}
public IntegerRanges nutch_f3068_0(boolean arg0)
{    return null;}
public Class<? extends Reducer<?, ?, ?, ?>> nutch_f3069_0() throws ClassNotFoundException
{    return null;}
public RawComparator<?> nutch_f3070_0()
{    return null;}
public boolean nutch_f3071_0()
{    return false;}
public boolean nutch_f3072_0()
{    return false;}
public String nutch_f3073_0()
{    return null;}
public Path nutch_f3074_0() throws IOException
{    return null;}
public static Reducer<Text, CrawlDatum, Text, CrawlDatum>.Context nutch_f3075_0()
{    DummyContext context = new DummyContext();    Configuration conf = context.getConfiguration();    conf.addResource("nutch-default.xml");    conf.addResource("crawl-tests.xml");    return (Reducer<Text, CrawlDatum, Text, CrawlDatum>.Context) context;}
public static void nutch_f3076_0(FileSystem fs, Path urlPath, List<String> urls) throws IOException
{    generateSeedList(fs, urlPath, urls, new ArrayList<String>());}
public static void nutch_f3077_0(FileSystem fs, Path urlPath, List<String> urls, List<String> metadata) throws IOException
{    FSDataOutputStream out;    Path file = new Path(urlPath, "urls.txt");    fs.mkdirs(urlPath);    out = fs.create(file);    Iterator<String> urls_i = urls.iterator();    Iterator<String> metadata_i = metadata.iterator();    String url;    String md;    while (urls_i.hasNext()) {        url = urls_i.next();        out.writeBytes(url);        if (metadata_i.hasNext()) {            md = metadata_i.next();            out.writeBytes(md);        }        out.writeBytes("\n");    }    out.flush();    out.close();}
public static Server nutch_f3078_0(int port, String staticContent) throws UnknownHostException
{    Server webServer = new org.mortbay.jetty.Server();    SocketConnector listener = new SocketConnector();    listener.setPort(port);    listener.setHost("127.0.0.1");    webServer.addConnector(listener);    ContextHandler staticContext = new ContextHandler();    staticContext.setContextPath("/");    staticContext.setResourceBase(staticContent);    staticContext.addHandler(new ResourceHandler());    webServer.addHandler(staticContext);    return webServer;}
public List<CrawlDatum> nutch_f3079_1(List<CrawlDatum> values)
{    List<CrawlDatum> result = new ArrayList<CrawlDatum>(0);    if (values == null || values.size() == 0) {        return result;    }        Collections.shuffle(values);    reduceDriver = ReduceDriver.newReduceDriver(reducer);    reduceDriver.getConfiguration().addResource(configuration);    reduceDriver.withInput(dummyURL, values);    List<Pair<Text, CrawlDatum>> reduceResult;    try {        reduceResult = reduceDriver.run();        for (Pair<Text, CrawlDatum> p : reduceResult) {            if (p.getFirst().equals(dummyURL)) {                result.add(p.getSecond());            }        }    } catch (IOException e) {                return result;    }    return result;}
public List<CrawlDatum> nutch_f3080_0(CrawlDatum dbDatum, CrawlDatum fetchDatum)
{    List<CrawlDatum> values = new ArrayList<CrawlDatum>();    if (dbDatum != null)        values.add(dbDatum);    if (fetchDatum != null)        values.add(fetchDatum);    return update(values);}
public List<CrawlDatum> nutch_f3081_0(CrawlDatum... values)
{    return update(Arrays.asList(values));}
public void nutch_f3082_0(Text key, CrawlDatum value) throws IOException, InterruptedException
{    values.add(value);}
public List<CrawlDatum> nutch_f3083_0()
{    return values;}
public CrawlDatum nutch_f3084_0() throws UnsupportedOperationException
{    throw new UnsupportedOperationException("Dummy context");}
public Text nutch_f3085_0() throws UnsupportedOperationException
{    throw new UnsupportedOperationException("Dummy context with no keys");}
public void nutch_f3086_0()
{}
public Counter nutch_f3087_0(Enum<?> arg0)
{    return dummyCounters.getGroup("dummy").getCounterForName("dummy");}
public Counter nutch_f3088_0(String arg0, String arg1)
{    return dummyCounters.getGroup("dummy").getCounterForName("dummy");}
public void nutch_f3089_0(String arg0) throws UnsupportedOperationException
{    throw new UnsupportedOperationException("Dummy context with no status");}
public String nutch_f3090_0() throws UnsupportedOperationException
{    throw new UnsupportedOperationException("Dummy context with no status");}
public float nutch_f3091_0()
{    return 1f;}
public OutputCommitter nutch_f3092_0()
{    throw new UnsupportedOperationException("Dummy context without committer");}
public boolean nutch_f3093_0()
{    return false;}
public boolean nutch_f3094_0()
{    return false;}
public TaskAttemptID nutch_f3095_0() throws UnsupportedOperationException
{    throw new UnsupportedOperationException("Dummy context without TaskAttemptID");}
public Path[] nutch_f3096_0()
{    return null;}
public String[] nutch_f3097_0()
{    return null;}
public URI[] nutch_f3098_0() throws IOException
{    return null;}
public URI[] nutch_f3099_0() throws IOException
{    return null;}
public Class<? extends Reducer<?, ?, ?, ?>> nutch_f3100_0() throws ClassNotFoundException
{    return null;}
public RawComparator<?> nutch_f3101_0()
{    return null;}
public Configuration nutch_f3102_0()
{    return null;}
public Credentials nutch_f3103_0()
{    return null;}
public Path[] nutch_f3104_0()
{    return null;}
public String[] nutch_f3105_0()
{    return null;}
public RawComparator<?> nutch_f3106_0()
{    return null;}
public Class<? extends InputFormat<?, ?>> nutch_f3107_0() throws ClassNotFoundException
{    return null;}
public String nutch_f3108_0()
{    return null;}
public JobID nutch_f3109_0()
{    return null;}
public String nutch_f3110_0()
{    return null;}
public boolean nutch_f3111_0()
{    return false;}
public Path[] nutch_f3112_0() throws IOException
{    return null;}
public Path[] nutch_f3113_0() throws IOException
{    return null;}
public Class<?> nutch_f3114_0()
{    return null;}
public Class<?> nutch_f3115_0()
{    return null;}
public Class<? extends Mapper<?, ?, ?, ?>> nutch_f3116_0() throws ClassNotFoundException
{    return null;}
public int nutch_f3117_0()
{    return 0;}
public int nutch_f3118_0()
{    return 0;}
public int nutch_f3119_0()
{    return 0;}
public Class<? extends OutputFormat<?, ?>> nutch_f3120_0() throws ClassNotFoundException
{    return null;}
public Class<?> nutch_f3121_0()
{    return null;}
public Class<?> nutch_f3122_0()
{    return null;}
public Class<? extends Partitioner<?, ?>> nutch_f3123_0() throws ClassNotFoundException
{    return null;}
public boolean nutch_f3124_0()
{    return false;}
public String nutch_f3125_0()
{    return null;}
public IntegerRanges nutch_f3126_0(boolean arg0)
{    return null;}
public Class<? extends Reducer<?, ?, ?, ?>> nutch_f3127_0() throws ClassNotFoundException
{    return null;}
public RawComparator<?> nutch_f3128_0()
{    return null;}
public boolean nutch_f3129_0()
{    return false;}
public boolean nutch_f3130_0()
{    return false;}
public String nutch_f3131_0()
{    return null;}
public Path nutch_f3132_0() throws IOException
{    return null;}
public List<CrawlDatum> nutch_f3133_1(List<CrawlDatum> values)
{    if (values == null || values.size() == 0) {        return new ArrayList<CrawlDatum>(0);    }        Collections.shuffle(values);    DummyContext context = new DummyContext();    try {        Iterable<CrawlDatum> iterable_values = (Iterable) values;        reducer.reduce(dummyURL, iterable_values, (Reducer<Text, CrawlDatum, Text, CrawlDatum>.Context) context);    } catch (IOException e) {            } catch (InterruptedException e) {            }    return context.getValues();}
public List<CrawlDatum> nutch_f3134_0(CrawlDatum dbDatum, CrawlDatum fetchDatum)
{    List<CrawlDatum> values = new ArrayList<CrawlDatum>();    if (dbDatum != null)        values.add(dbDatum);    if (fetchDatum != null)        values.add(fetchDatum);    return update(values);}
public List<CrawlDatum> nutch_f3135_0(CrawlDatum... values)
{    return update(Arrays.asList(values));}
public void nutch_f3136_0() throws Exception
{    super.setUp();    conf = NutchConfiguration.create();    inc_rate = conf.getFloat("db.fetch.schedule.adaptive.inc_rate", 0.2f);    dec_rate = conf.getFloat("db.fetch.schedule.adaptive.dec_rate", 0.2f);    interval = 100;    lastModified = 0;}
public void nutch_f3137_0()
{    FetchSchedule fs = new AdaptiveFetchSchedule();    fs.setConf(conf);    CrawlDatum p = prepareCrawlDatum();    Text url = new Text("http://www.example.com");    changed = FetchSchedule.STATUS_UNKNOWN;    fs.setFetchSchedule(url, p, p.getFetchTime(), p.getModifiedTime(), curTime, lastModified, changed);    validateFetchInterval(changed, p.getFetchInterval());    changed = FetchSchedule.STATUS_MODIFIED;    fs.setFetchSchedule(url, p, p.getFetchTime(), p.getModifiedTime(), curTime, lastModified, changed);    validateFetchInterval(changed, p.getFetchInterval());    p.setFetchInterval(interval);    changed = FetchSchedule.STATUS_NOTMODIFIED;    fs.setFetchSchedule(url, p, p.getFetchTime(), p.getModifiedTime(), curTime, lastModified, changed);    validateFetchInterval(changed, p.getFetchInterval());}
public CrawlDatum nutch_f3138_0()
{    CrawlDatum p = new CrawlDatum();    p.setStatus(CrawlDatum.STATUS_DB_UNFETCHED);    p.setFetchInterval(interval);    p.setScore(1.0f);    p.setFetchTime(0);    return p;}
private void nutch_f3139_0(int changed, int getInterval)
{    if (changed == FetchSchedule.STATUS_UNKNOWN) {        assertEquals(getInterval, interval);    } else if (changed == FetchSchedule.STATUS_MODIFIED) {        calculateInterval = (int) (interval - (interval * dec_rate));        assertEquals(getInterval, calculateInterval);    } else if (changed == FetchSchedule.STATUS_NOTMODIFIED) {        calculateInterval = (int) (interval + (interval * inc_rate));        assertEquals(getInterval, calculateInterval);    }}
public void nutch_f3140_0() throws Exception
{    conf = CrawlDBTestUtil.createContext().getConfiguration();    fs = FileSystem.get(conf);    fs.delete(testdir, true);}
public void nutch_f3141_0()
{    delete(testdir);}
private void nutch_f3142_0(Path p)
{    try {        fs.delete(p, true);    } catch (IOException e) {    }}
public void nutch_f3143_0() throws Exception
{        ArrayList<URLCrawlDatum> list = new ArrayList<URLCrawlDatum>();    list.add(new URLCrawlDatum(new Text("http://www.example.com"), new CrawlDatum(CrawlDatum.STATUS_DB_GONE, 0, 0.0f)));    list.add(new URLCrawlDatum(new Text("http://www.example1.com"), new CrawlDatum(CrawlDatum.STATUS_DB_FETCHED, 0, 0.0f)));    list.add(new URLCrawlDatum(new Text("http://www.example2.com"), new CrawlDatum(CrawlDatum.STATUS_DB_UNFETCHED, 0, 0.0f)));    dbDir = new Path(testdir, "crawldb");    newCrawlDb = new Path(testdir, "newcrawldb");        CrawlDBTestUtil.createCrawlDb(conf, fs, dbDir, list);        conf.setBoolean(CrawlDb.CRAWLDB_PURGE_404, true);    conf.setBoolean(CrawlDbFilter.URL_NORMALIZING, true);    conf.setBoolean(CrawlDbFilter.URL_FILTERING, false);    conf.setInt("urlnormalizer.loop.count", 2);    Job job = NutchJob.getInstance(conf);    job.setJobName("Test CrawlDbFilter");    Path current = new Path(dbDir, "current");    if (FileSystem.get(conf).exists(current)) {        FileInputFormat.addInputPath(job, current);    }    job.setInputFormatClass(SequenceFileInputFormat.class);    job.setMapperClass(CrawlDbFilter.class);    job.setReducerClass(CrawlDbReducer.class);    FileOutputFormat.setOutputPath(job, newCrawlDb);    job.setOutputFormatClass(MapFileOutputFormat.class);    job.setOutputKeyClass(Text.class);    job.setOutputValueClass(CrawlDatum.class);    job.setJarByClass(CrawlDbFilter.class);    job.waitForCompletion(true);    Path fetchlist = new Path(new Path(newCrawlDb, "part-r-00000"), "data");    ArrayList<URLCrawlDatum> l = readContents(fetchlist);        Assert.assertEquals(2, l.size());}
private ArrayList<URLCrawlDatum> nutch_f3144_0(Path fetchlist) throws IOException
{        Option fFile = SequenceFile.Reader.file(fetchlist);    SequenceFile.Reader reader = new SequenceFile.Reader(conf, fFile);    ArrayList<URLCrawlDatum> l = new ArrayList<URLCrawlDatum>();    READ: do {        Text key = new Text();        CrawlDatum value = new CrawlDatum();        if (!reader.next(key, value)) {            break READ;        }        l.add(new URLCrawlDatum(key, value));    } while (true);    reader.close();    return l;}
public void nutch_f3145_0() throws Exception
{    init1.add(url10);    init1.add(url11);    init2.add(url20);    init2.add(url21);    long time = System.currentTimeMillis();    cd1 = new CrawlDatum();    cd1.setFetchInterval(1.0f);    cd1.setFetchTime(time);    cd1.getMetaData().put(new Text("name"), new Text("cd1"));    cd1.getMetaData().put(new Text("cd1"), new Text("cd1"));    cd2 = new CrawlDatum();    cd2.setFetchInterval(1.0f);    cd2.setFetchTime(time + 10000);    cd2.getMetaData().put(new Text("name"), new Text("cd2"));    cd3 = new CrawlDatum();    cd3.setFetchInterval(1.0f);    cd3.setFetchTime(time + 10000);    cd3.getMetaData().putAll(cd1.getMetaData());    cd3.getMetaData().putAll(cd2.getMetaData());    expected.put(url10, cd3);    expected.put(url11, cd1);    expected.put(url21, cd2);    conf = NutchConfiguration.create();    fs = FileSystem.get(conf);    testDir = new Path("test-crawldb-" + new java.util.Random().nextInt());    fs.mkdirs(testDir);}
public void nutch_f3146_0()
{    try {        if (fs.exists(testDir))            fs.delete(testDir, true);    } catch (Exception e) {    }    try {        reader.close();    } catch (Exception e) {    }}
public void nutch_f3147_1() throws Exception
{    Path crawldb1 = new Path(testDir, "crawldb1");    Path crawldb2 = new Path(testDir, "crawldb2");    Path output = new Path(testDir, "output");    createCrawlDb(conf, fs, crawldb1, init1, cd1);    createCrawlDb(conf, fs, crawldb2, init2, cd2);    CrawlDbMerger merger = new CrawlDbMerger(conf);        merger.merge(output, new Path[] { crawldb1, crawldb2 }, false, false);        reader = new CrawlDbReader();    String crawlDb = output.toString();    Iterator<String> it = expected.keySet().iterator();    while (it.hasNext()) {        String url = it.next();                CrawlDatum cd = expected.get(url);        CrawlDatum res = reader.get(crawlDb, url, conf);                System.out.println("url=" + url);        System.out.println(" cd " + cd);        System.out.println(" res " + res);                Assert.assertNotNull(res);        Assert.assertTrue(cd.equals(res));    }    reader.close();    fs.delete(testDir, true);}
private void nutch_f3148_1(Configuration config, FileSystem fs, Path crawldb, TreeSet<String> init, CrawlDatum cd) throws Exception
{        Path dir = new Path(crawldb, CrawlDb.CURRENT_NAME);    Option wKeyOpt = MapFile.Writer.keyClass(Text.class);    org.apache.hadoop.io.SequenceFile.Writer.Option wValueOpt = SequenceFile.Writer.valueClass(CrawlDatum.class);    MapFile.Writer writer = new MapFile.Writer(config, new Path(dir, "part-r-00000"), wKeyOpt, wValueOpt);    Iterator<String> it = init.iterator();    while (it.hasNext()) {        String key = it.next();        writer.append(new Text(key), cd);    }    writer.close();}
public void nutch_f3149_1()
{        Reducer<Text, CrawlDatum, Text, CrawlDatum>.Context context = CrawlDBTestUtil.createContext();    Configuration conf = context.getConfiguration();    CrawlDbUpdateUtil updateDb = null;    try {        updateDb = new CrawlDbUpdateUtil(new CrawlDbReducer(), context);    } catch (IOException e) {        e.printStackTrace();    }    int retryMax = conf.getInt("db.fetch.retry.max", 3);    for (String sched : schedules) {                conf.set("db.fetch.schedule.class", "org.apache.nutch.crawl." + sched);        FetchSchedule schedule = FetchScheduleFactory.getFetchSchedule(conf);        for (int i = 0; i < fetchDbStatusPairs.length; i++) {            byte fromDbStatus = fetchDbStatusPairs[i][1];            for (int j = 0; j < fetchDbStatusPairs.length; j++) {                byte fetchStatus = fetchDbStatusPairs[j][0];                CrawlDatum fromDb = null;                if (fromDbStatus == -1) {                                                } else {                    fromDb = new CrawlDatum();                    fromDb.setStatus(fromDbStatus);                                        schedule.initializeSchedule(CrawlDbUpdateUtil.dummyURL, fromDb);                }                                byte toDbStatus = fetchDbStatusPairs[j][1];                if (fetchStatus == -1) {                    if (fromDbStatus == -1) {                                                toDbStatus = STATUS_DB_UNFETCHED;                    } else {                                                toDbStatus = fromDbStatus;                    }                } else if (fetchStatus == STATUS_FETCH_RETRY) {                                        if (fromDb == null || fromDb.getRetriesSinceFetch() < retryMax) {                        toDbStatus = STATUS_DB_UNFETCHED;                    } else {                        toDbStatus = STATUS_DB_GONE;                    }                }                String fromDbStatusName = (fromDbStatus == -1 ? "<not in CrawlDb>" : getStatusName(fromDbStatus));                String fetchStatusName = (fetchStatus == -1 ? "<only inlinks>" : CrawlDatum.getStatusName(fetchStatus));                                List<CrawlDatum> values = new ArrayList<CrawlDatum>();                for (int l = 0; l <= 2; l++) {                                        CrawlDatum fetch = null;                    if (fetchStatus == -1) {                                                if (l == 0)                            continue;                    } else {                        fetch = new CrawlDatum();                        if (fromDb != null) {                            fetch.set(fromDb);                        } else {                                                        schedule.initializeSchedule(CrawlDbUpdateUtil.dummyURL, fetch);                        }                        fetch.setStatus(fetchStatus);                        fetch.setFetchTime(System.currentTimeMillis());                    }                    if (fromDb != null)                        values.add(fromDb);                    if (fetch != null)                        values.add(fetch);                    for (int n = 0; n < l; n++) {                        values.add(linked);                    }                    List<CrawlDatum> res = updateDb.update(values);                    if (res.size() != 1) {                        fail("CrawlDb update didn't result in one single CrawlDatum per URL");                        continue;                    }                    byte status = res.get(0).getStatus();                    if (status != toDbStatus) {                        fail("CrawlDb update for " + fromDbStatusName + " and " + fetchStatusName + " and " + l + " inlinks results in " + getStatusName(status) + " (expected: " + getStatusName(toDbStatus) + ")");                    }                    values.clear();                }            }        }    }}
public void nutch_f3150_1()
{        Configuration conf = CrawlDBTestUtil.createContext().getConfiguration();    Injector.InjectReducer injector = new Injector.InjectReducer();    CrawlDbUpdateTestDriver<Injector.InjectReducer> injectDriver = new CrawlDbUpdateTestDriver<Injector.InjectReducer>(injector, conf);    ScoringFilters scfilters = new ScoringFilters(conf);    for (String sched : schedules) {                conf.set("db.fetch.schedule.class", "org.apache.nutch.crawl." + sched);        FetchSchedule schedule = FetchScheduleFactory.getFetchSchedule(conf);        List<CrawlDatum> values = new ArrayList<CrawlDatum>();        for (int i = 0; i < fetchDbStatusPairs.length; i++) {            byte fromDbStatus = fetchDbStatusPairs[i][1];            byte toDbStatus = fromDbStatus;            if (fromDbStatus == -1) {                toDbStatus = STATUS_DB_UNFETCHED;            } else {                CrawlDatum fromDb = new CrawlDatum();                fromDb.setStatus(fromDbStatus);                schedule.initializeSchedule(CrawlDbUpdateUtil.dummyURL, fromDb);                values.add(fromDb);            }                        CrawlDatum injected = new CrawlDatum(STATUS_INJECTED, conf.getInt("db.fetch.interval.default", 2592000), 0.1f);            schedule.initializeSchedule(CrawlDbUpdateUtil.dummyURL, injected);            try {                scfilters.injectedScore(CrawlDbUpdateUtil.dummyURL, injected);            } catch (ScoringFilterException e) {                            }            values.add(injected);            List<CrawlDatum> res = injectDriver.update(values);            if (res.size() != 1) {                fail("Inject didn't result in one single CrawlDatum per URL");                continue;            }            byte status = res.get(0).getStatus();            if (status != toDbStatus) {                fail("Inject for " + (fromDbStatus == -1 ? "" : getStatusName(fromDbStatus) + " and ") + getStatusName(STATUS_INJECTED) + " results in " + getStatusName(status) + " (expected: " + getStatusName(toDbStatus) + ")");            }            values.clear();        }    }}
public void nutch_f3151_1()
{        Context context = CrawlDBTestUtil.createContext();    Configuration conf = context.getConfiguration();    ;        for (String sched : schedules) {        String desc = "test notmodified by signature comparison + " + sched;                conf.set("db.fetch.schedule.class", "org.apache.nutch.crawl." + sched);        ContinuousCrawlTestUtil crawlUtil = new CrawlTestFetchNotModified(context);        try {            if (!crawlUtil.run(20)) {                fail("failed: " + desc);            }        } catch (IOException e) {            e.printStackTrace();        }    }        for (String sched : schedules) {        String desc = "test notmodified by HTTP 304 + " + sched;                conf.set("db.fetch.schedule.class", "org.apache.nutch.crawl." + sched);        ContinuousCrawlTestUtil crawlUtil = new CrawlTestFetchNotModifiedHttp304(context);        try {            if (!crawlUtil.run(20)) {                fail("failed: " + desc);            }        } catch (IOException e) {            e.printStackTrace();        }    }}
protected boolean nutch_f3152_1(CrawlDatum result)
{    if (lastFetchTime > 0 && (currFetchTime - lastFetchTime) > maxFetchInterval) {                return false;    }    switch(result.getStatus()) {        case STATUS_DB_NOTMODIFIED:                        if ((previousDbState == STATUS_DB_FETCHED || previousDbState == STATUS_DB_NOTMODIFIED)) {                if (lastSignature != null && result.getSignature() != null && SignatureComparator._compare(lastSignature, result.getSignature()) != 0) {                                        return false;                }                                return checkModifiedTime(result, firstFetchTime);            }                        break;        case STATUS_DB_FETCHED:            if (previousDbState == STATUS_DB_UNFETCHED) {                                return checkModifiedTime(result, firstFetchTime);            } else if (lastSignature != null && result.getSignature() != null && SignatureComparator._compare(lastSignature, result.getSignature()) != 0) {                                                return checkModifiedTime(result, currFetchTime);            } else {                            }            break;        case STATUS_DB_UNFETCHED:            /**             * Status db_unfetched is possible with {@link AdaptiveFetchSchedule}             * because {@link CrawlDbReducer#reduce} calls             * {@link FetchSchedule#forceRefetch} to force a re-fetch if fetch             * interval grows too large.             */            if (schedule.getClass() == AdaptiveFetchSchedule.class) {                                if (result.getSignature() != null) {                                        return false;                }                                firstFetchTime = 0;                return true;            }    }        return false;}
private boolean nutch_f3153_1(CrawlDatum result, long modifiedTime)
{    if (modifiedTime == result.getModifiedTime()) {        return true;    }        return false;}
protected CrawlDatum nutch_f3154_0(CrawlDatum datum, long currentTime)
{    lastFetchTime = currFetchTime;    currFetchTime = currentTime;    if (lastFetchTime > 0)        elapsedDuration += (currFetchTime - lastFetchTime);    previousDbState = datum.getStatus();    lastSignature = datum.getSignature();    datum = super.fetch(datum, currentTime);    if (firstFetchTime == 0) {        firstFetchTime = currFetchTime;    } else if (elapsedDuration < (duration / 2)) {                changeContent();        firstFetchTime = currFetchTime;    }    return datum;}
protected CrawlDatum nutch_f3155_1(CrawlDatum datum, long currentTime)
{    lastFetchTime = currFetchTime;    currFetchTime = currentTime;    previousDbState = datum.getStatus();    lastSignature = datum.getSignature();    int httpCode;    /*       * document is "really" fetched (no HTTP 304) - if last-modified time or       * signature are unset (page has not been fetched before or fetch is       * forced) - for test purposes, we simulate a modified after "one year"       */    if (datum.getModifiedTime() == 0 && datum.getSignature() == null || (currFetchTime - firstFetchTime) > (duration / 2)) {        firstFetchTime = currFetchTime;        httpCode = 200;        datum.setStatus(STATUS_FETCH_SUCCESS);                changeContent();    } else {        httpCode = 304;        datum.setStatus(STATUS_FETCH_NOTMODIFIED);    }        datum.setFetchTime(currentTime);    return datum;}
public void nutch_f3156_1()
{        ContinuousCrawlTestUtil crawlUtil = new ContinuousCrawlTestUtil(STATUS_FETCH_GONE, STATUS_DB_GONE);    try {        if (!crawlUtil.run(20)) {            fail("fetch_gone did not result in a db_gone (NUTCH-1245)");        }    } catch (IOException e) {        e.printStackTrace();    }}
public void nutch_f3157_1()
{        Context context = CrawlDBTestUtil.createContext();    Configuration conf = context.getConfiguration();    int fetchIntervalMax = conf.getInt("db.fetch.interval.max", 0);    conf.setInt("db.fetch.interval.default", 3 + (int) (fetchIntervalMax * 1.5));    ContinuousCrawlTestUtil crawlUtil = new ContinuousCrawlTestUtil(context, STATUS_FETCH_GONE, STATUS_DB_GONE);    try {        if (!crawlUtil.run(0)) {            fail("fetch_gone did not result in a db_gone (NUTCH-1245)");        }    } catch (IOException e) {        e.printStackTrace();    }}
public void nutch_f3158_1()
{        Context context = CrawlDBTestUtil.createContext();    Configuration conf = context.getConfiguration();    for (String sched : schedules) {                conf.set("db.fetch.schedule.class", "org.apache.nutch.crawl." + sched);        ContinuousCrawlTestUtil crawlUtil = new CrawlTestSignatureReset(context);        try {            if (!crawlUtil.run(20)) {                fail("failed: signature not reset");            }        } catch (IOException e) {            e.printStackTrace();        }    }}
protected CrawlDatum nutch_f3159_1(CrawlDatum datum, long currentTime)
{    datum = super.fetch(datum, currentTime);    counter++;        if (counter % 2 == 1) {        fetchState = STATUS_FETCH_SUCCESS;    } else {        fetchState = noContentStates[(counter % 6) / 2][0];    }        datum.setStatus(fetchState);    return datum;}
protected boolean nutch_f3160_1(CrawlDatum result)
{    if (result.getStatus() == STATUS_DB_NOTMODIFIED && !(fetchState == STATUS_FETCH_SUCCESS || fetchState == STATUS_FETCH_NOTMODIFIED)) {                return false;    }    if (result.getSignature() != null && !(result.getStatus() == STATUS_DB_FETCHED || result.getStatus() == STATUS_DB_NOTMODIFIED)) {                    }    return true;}
public void nutch_f3161_0() throws Exception
{    conf = CrawlDBTestUtil.createContext().getConfiguration();    fs = FileSystem.get(conf);    fs.delete(testdir, true);}
public void nutch_f3162_0()
{    delete(testdir);}
private void nutch_f3163_0(Path p)
{    try {        fs.delete(p, true);    } catch (IOException e) {    }}
public void nutch_f3164_0() throws Exception
{    final int NUM_RESULTS = 2;    ArrayList<URLCrawlDatum> list = new ArrayList<URLCrawlDatum>();    for (int i = 0; i <= 100; i++) {        list.add(createURLCrawlDatum("http://aaa/" + pad(i), 1, i));    }    createCrawlDB(list);    Path generatedSegment = generateFetchlist(NUM_RESULTS, conf, false);    Path fetchlist = new Path(new Path(generatedSegment, CrawlDatum.GENERATE_DIR_NAME), "part-r-00000");    ArrayList<URLCrawlDatum> l = readContents(fetchlist);        Collections.sort(l, new ScoreComparator());        Assert.assertEquals(NUM_RESULTS, l.size());        Assert.assertEquals("http://aaa/100", (l.get(0).url.toString()));    Assert.assertEquals("http://aaa/099", (l.get(1).url.toString()));}
private String nutch_f3165_0(int i)
{    String s = Integer.toString(i);    while (s.length() < 3) {        s = "0" + s;    }    return s;}
public int nutch_f3166_0(URLCrawlDatum tuple1, URLCrawlDatum tuple2)
{    if (tuple2.datum.getScore() - tuple1.datum.getScore() < 0) {        return -1;    }    if (tuple2.datum.getScore() - tuple1.datum.getScore() > 0) {        return 1;    }    return 0;}
public void nutch_f3167_0() throws Exception
{    ArrayList<URLCrawlDatum> list = new ArrayList<URLCrawlDatum>();    list.add(createURLCrawlDatum("http://www.example.com/index1.html", 1, 1));    list.add(createURLCrawlDatum("http://www.example.com/index2.html", 1, 1));    list.add(createURLCrawlDatum("http://www.example.com/index3.html", 1, 1));    createCrawlDB(list);    int maxPerHost = 1;    Configuration myConfiguration = new Configuration(conf);    myConfiguration.setInt(Generator.GENERATOR_MAX_COUNT, maxPerHost);    Path generatedSegment = generateFetchlist(Integer.MAX_VALUE, myConfiguration, false);    Path fetchlistPath = new Path(new Path(generatedSegment, CrawlDatum.GENERATE_DIR_NAME), "part-r-00000");    ArrayList<URLCrawlDatum> fetchList = readContents(fetchlistPath);        int expectedFetchListSize = Math.min(maxPerHost, list.size());    Assert.assertEquals("Failed to apply generate.max.count by host", expectedFetchListSize, fetchList.size());    maxPerHost = 2;    myConfiguration = new Configuration(conf);    myConfiguration.setInt(Generator.GENERATOR_MAX_COUNT, maxPerHost);    generatedSegment = generateFetchlist(Integer.MAX_VALUE, myConfiguration, false);    fetchlistPath = new Path(new Path(generatedSegment, CrawlDatum.GENERATE_DIR_NAME), "part-r-00000");    fetchList = readContents(fetchlistPath);        expectedFetchListSize = Math.min(maxPerHost, list.size());    Assert.assertEquals("Failed to apply generate.max.count by host", expectedFetchListSize, fetchList.size());    maxPerHost = 3;    myConfiguration = new Configuration(conf);    myConfiguration.setInt(Generator.GENERATOR_MAX_COUNT, maxPerHost);    generatedSegment = generateFetchlist(Integer.MAX_VALUE, myConfiguration, false);    fetchlistPath = new Path(new Path(generatedSegment, CrawlDatum.GENERATE_DIR_NAME), "part-r-00000");    fetchList = readContents(fetchlistPath);        expectedFetchListSize = Math.min(maxPerHost, list.size());    Assert.assertEquals("Failed to apply generate.max.count by host", expectedFetchListSize, fetchList.size());}
public void nutch_f3168_0() throws Exception
{    ArrayList<URLCrawlDatum> list = new ArrayList<URLCrawlDatum>();    list.add(createURLCrawlDatum("http://a.example.com/index.html", 1, 1));    list.add(createURLCrawlDatum("http://b.example.com/index.html", 1, 1));    list.add(createURLCrawlDatum("http://c.example.com/index.html", 1, 1));    createCrawlDB(list);    int maxPerDomain = 1;    Configuration myConfiguration = new Configuration(conf);    myConfiguration.setInt(Generator.GENERATOR_MAX_COUNT, maxPerDomain);    myConfiguration.set(Generator.GENERATOR_COUNT_MODE, Generator.GENERATOR_COUNT_VALUE_DOMAIN);    Path generatedSegment = generateFetchlist(Integer.MAX_VALUE, myConfiguration, false);    Path fetchlistPath = new Path(new Path(generatedSegment, CrawlDatum.GENERATE_DIR_NAME), "part-r-00000");    ArrayList<URLCrawlDatum> fetchList = readContents(fetchlistPath);        int expectedFetchListSize = Math.min(maxPerDomain, list.size());    Assert.assertEquals("Failed to apply generate.max.count by domain", expectedFetchListSize, fetchList.size());    maxPerDomain = 2;    myConfiguration = new Configuration(myConfiguration);    myConfiguration.setInt(Generator.GENERATOR_MAX_COUNT, maxPerDomain);    generatedSegment = generateFetchlist(Integer.MAX_VALUE, myConfiguration, false);    fetchlistPath = new Path(new Path(generatedSegment, CrawlDatum.GENERATE_DIR_NAME), "part-r-00000");    fetchList = readContents(fetchlistPath);        expectedFetchListSize = Math.min(maxPerDomain, list.size());    Assert.assertEquals("Failed to apply generate.max.count by domain", expectedFetchListSize, fetchList.size());    maxPerDomain = 3;    myConfiguration = new Configuration(myConfiguration);    myConfiguration.setInt(Generator.GENERATOR_MAX_COUNT, maxPerDomain);    generatedSegment = generateFetchlist(Integer.MAX_VALUE, myConfiguration, false);    fetchlistPath = new Path(new Path(generatedSegment, CrawlDatum.GENERATE_DIR_NAME), "part-r-00000");    fetchList = readContents(fetchlistPath);        expectedFetchListSize = Math.min(maxPerDomain, list.size());    Assert.assertEquals("Failed to apply generate.max.count by domain", expectedFetchListSize, fetchList.size());}
public void nutch_f3169_0() throws IOException, Exception
{    ArrayList<URLCrawlDatum> list = new ArrayList<URLCrawlDatum>();    list.add(createURLCrawlDatum("http://www.example.com/index.html", 1, 1));    list.add(createURLCrawlDatum("http://www.example.net/index.html", 1, 1));    list.add(createURLCrawlDatum("http://www.example.org/index.html", 1, 1));    createCrawlDB(list);    Configuration myConfiguration = new Configuration(conf);    myConfiguration.set("urlfilter.suffix.file", "filter-all.txt");    Path generatedSegment = generateFetchlist(Integer.MAX_VALUE, myConfiguration, true);    Assert.assertNull("should be null (0 entries)", generatedSegment);    generatedSegment = generateFetchlist(Integer.MAX_VALUE, myConfiguration, false);    Path fetchlistPath = new Path(new Path(generatedSegment, CrawlDatum.GENERATE_DIR_NAME), "part-r-00000");    ArrayList<URLCrawlDatum> fetchList = readContents(fetchlistPath);        Assert.assertEquals(list.size(), fetchList.size());}
private ArrayList<URLCrawlDatum> nutch_f3170_0(Path fetchlist) throws IOException
{        Option rFile = SequenceFile.Reader.file(fetchlist);    SequenceFile.Reader reader = new SequenceFile.Reader(conf, rFile);    ArrayList<URLCrawlDatum> l = new ArrayList<URLCrawlDatum>();    READ: do {        Text key = new Text();        CrawlDatum value = new CrawlDatum();        if (!reader.next(key, value)) {            break READ;        }        l.add(new URLCrawlDatum(key, value));    } while (true);    reader.close();    return l;}
private Path nutch_f3171_0(int numResults, Configuration config, boolean filter) throws IOException, ClassNotFoundException, InterruptedException
{        Generator g = new Generator(config);    Path[] generatedSegment = g.generate(dbDir, segmentsDir, -1, numResults, Long.MAX_VALUE, filter, false);    if (generatedSegment == null)        return null;    return generatedSegment[0];}
private void nutch_f3172_0(ArrayList<URLCrawlDatum> list) throws IOException, Exception
{    dbDir = new Path(testdir, "crawldb");    segmentsDir = new Path(testdir, "segments");    fs.mkdirs(dbDir);    fs.mkdirs(segmentsDir);        CrawlDBTestUtil.createCrawlDb(conf, fs, dbDir, list);}
private URLCrawlDatum nutch_f3173_0(final String url, final int fetchInterval, final float score)
{    return new CrawlDBTestUtil.URLCrawlDatum(new Text(url), new CrawlDatum(CrawlDatum.STATUS_DB_UNFETCHED, fetchInterval, score));}
public void nutch_f3174_0() throws Exception
{    conf = CrawlDBTestUtil.createContext().getConfiguration();    urlPath = new Path(testdir, "urls");    crawldbPath = new Path(testdir, "crawldb");    fs = FileSystem.get(conf);    if (fs.exists(urlPath))        fs.delete(urlPath, false);    if (fs.exists(crawldbPath))        fs.delete(crawldbPath, true);}
public void nutch_f3175_0() throws IOException
{    fs.delete(testdir, true);}
public void nutch_f3176_0() throws IOException, ClassNotFoundException, InterruptedException
{    ArrayList<String> urls = new ArrayList<String>();            ArrayList<String> metadata = new ArrayList<String>();    for (int i = 0; i < 100; i++) {        urls.add("http://zzz.com/" + i + ".html");        metadata.add("\tnutch.score=2." + i + "\tnutch.fetchInterval=171717\tkey=value");    }    CrawlDBTestUtil.generateSeedList(fs, urlPath, urls, metadata);    Injector injector = new Injector(conf);    injector.inject(crawldbPath, urlPath);        List<String> read = readCrawldb();    Collections.sort(read);    Collections.sort(urls);    Assert.assertEquals(urls.size(), read.size());    Assert.assertTrue(read.containsAll(urls));    Assert.assertTrue(urls.containsAll(read));        ArrayList<String> urls2 = new ArrayList<String>();    for (int i = 0; i < 100; i++) {        urls2.add("http://xxx.com/" + i + ".html");                        urls2.add("http://zzz.com/" + i + ".html");    }    CrawlDBTestUtil.generateSeedList(fs, urlPath, urls2);    injector = new Injector(conf);    conf.setBoolean("db.injector.update", true);    injector.inject(crawldbPath, urlPath);    urls.addAll(urls2);        read = readCrawldb();    Collections.sort(read);    Collections.sort(urls);        Assert.assertEquals(urls.size() - 100, read.size());    Assert.assertTrue(read.containsAll(urls));    Assert.assertTrue(urls.containsAll(read));        Map<String, CrawlDatum> records = readCrawldbRecords();            Text writableKey = new Text("key");    Text writableValue = new Text("value");    for (String url : urls) {        if (url.indexOf("http://zzz") == 0) {                        Assert.assertTrue(records.get(url).getFetchInterval() == 171717);                        Assert.assertTrue(records.get(url).getScore() != 1.0);                        Assert.assertEquals(writableValue, records.get(url).getMetaData().get(writableKey));        }    }}
private List<String> nutch_f3177_0() throws IOException
{    Path dbfile = new Path(crawldbPath, CrawlDb.CURRENT_NAME + "/part-r-00000/data");    System.out.println("reading:" + dbfile);    Option rFile = SequenceFile.Reader.file(dbfile);    @SuppressWarnings("resource")    SequenceFile.Reader reader = new SequenceFile.Reader(conf, rFile);    ArrayList<String> read = new ArrayList<String>();    READ: do {        Text key = new Text();        CrawlDatum value = new CrawlDatum();        if (!reader.next(key, value))            break READ;        read.add(key.toString());    } while (true);    return read;}
private HashMap<String, CrawlDatum> nutch_f3178_0() throws IOException
{    Path dbfile = new Path(crawldbPath, CrawlDb.CURRENT_NAME + "/part-r-00000/data");    System.out.println("reading:" + dbfile);    Option rFile = SequenceFile.Reader.file(dbfile);    @SuppressWarnings("resource")    SequenceFile.Reader reader = new SequenceFile.Reader(conf, rFile);    HashMap<String, CrawlDatum> read = new HashMap<String, CrawlDatum>();    READ: do {        Text key = new Text();        CrawlDatum value = new CrawlDatum();        if (!reader.next(key, value))            break READ;        read.put(key.toString(), value);    } while (true);    return read;}
public void nutch_f3179_0() throws Exception
{    init1.put(url10, urls10);    init1.put(url11, urls11);    init2.put(url20, urls20);    init2.put(url21, urls21);    expected.put(url10, urls10_expected);    expected.put(url11, urls11_expected);    expected.put(url20, urls20_expected);    expected.put(url21, urls21_expected);    conf = NutchConfiguration.create();    fs = FileSystem.get(conf);    testDir = new Path("build/test/test-linkdb-" + new java.util.Random().nextInt());    fs.mkdirs(testDir);}
public void nutch_f3180_0()
{    try {        if (fs.exists(testDir))            fs.delete(testDir, true);    } catch (Exception e) {    }    try {        reader.close();    } catch (Exception e) {    }}
public void nutch_f3181_1() throws Exception
{    Configuration conf = NutchConfiguration.create();    FileSystem fs = FileSystem.get(conf);    fs.mkdirs(testDir);    Path linkdb1 = new Path(testDir, "linkdb1");    Path linkdb2 = new Path(testDir, "linkdb2");    Path output = new Path(testDir, "output");    createLinkDb(conf, fs, linkdb1, init1);    createLinkDb(conf, fs, linkdb2, init2);    LinkDbMerger merger = new LinkDbMerger(conf);        merger.merge(output, new Path[] { linkdb1, linkdb2 }, false, false);        reader = new LinkDbReader(conf, output);    Iterator<String> it = expected.keySet().iterator();    while (it.hasNext()) {        String url = it.next();                String[] vals = expected.get(url);        Inlinks inlinks = reader.getInlinks(new Text(url));                Assert.assertNotNull(inlinks);        ArrayList<String> links = new ArrayList<String>();        Iterator<?> it2 = inlinks.iterator();        while (it2.hasNext()) {            Inlink in = (Inlink) it2.next();            links.add(in.getFromUrl());        }        for (int i = 0; i < vals.length; i++) {                        Assert.assertTrue(links.contains(vals[i]));        }    }    reader.close();    fs.delete(testDir, true);}
private void nutch_f3182_1(Configuration config, FileSystem fs, Path linkdb, TreeMap<String, String[]> init) throws Exception
{        Path dir = new Path(linkdb, LinkDb.CURRENT_NAME);    Option wKeyOpt = MapFile.Writer.keyClass(Text.class);    org.apache.hadoop.io.SequenceFile.Writer.Option wValueOpt = SequenceFile.Writer.valueClass(Inlinks.class);    MapFile.Writer writer = new MapFile.Writer(config, new Path(dir, "part-00000"), wKeyOpt, wValueOpt);    Iterator<String> it = init.keySet().iterator();    while (it.hasNext()) {        String key = it.next();        Inlinks inlinks = new Inlinks();        String[] vals = init.get(key);        for (int i = 0; i < vals.length; i++) {            Inlink in = new Inlink(vals[i], vals[i]);            inlinks.add(in);        }        writer.append(new Text(key), inlinks);    }    writer.close();}
public void nutch_f3183_0()
{    Configuration conf = NutchConfiguration.create();    Signature signature1 = SignatureFactory.getSignature(conf);    Signature signature2 = SignatureFactory.getSignature(conf);    Assert.assertNotNull(signature1);    Assert.assertNotNull(signature2);    Assert.assertEquals(signature1, signature2);}
public void nutch_f3184_0()
{    Configuration conf = NutchConfiguration.create();    Signature textProf = new TextProfileSignature();    textProf.setConf(conf);    String text = "Hello World The Quick Brown Fox Jumped Over the Lazy Fox";    ParseData pd = new ParseData(ParseStatus.STATUS_SUCCESS, "Hello World", new Outlink[0], new Metadata());    byte[] signature1 = textProf.calculate(new Content(), new ParseImpl(text, pd));    Assert.assertNotNull(signature1);    List<String> words = Arrays.asList(text.split("\\s"));    Collections.shuffle(words);    String text2 = String.join(" ", words);    byte[] signature2 = textProf.calculate(new Content(), new ParseImpl(text2, pd));    Assert.assertNotNull(signature2);    Assert.assertEquals(StringUtil.toHexString(signature1), StringUtil.toHexString(signature2));}
public void nutch_f3185_1()
{        ContinuousCrawlTestUtil crawlUtil = new ContinuousCrawlTestFetchRetry();        try {        if (!crawlUtil.run(150)) {            fail("fetch_retry did not result in a db_gone if retry counter > maxRetries (NUTCH-578)");        }    } catch (IOException e) {        e.printStackTrace();    }}
protected CrawlDatum nutch_f3186_0(CrawlDatum datum, long currentTime)
{    datum.setStatus(fetchStatus);    datum.setFetchTime(currentTime);    totalRetries++;    return datum;}
protected boolean nutch_f3187_1(CrawlDatum result)
{    if (result.getRetriesSinceFetch() > retryMax) {            } else if (result.getRetriesSinceFetch() == Byte.MAX_VALUE) {            } else if (result.getRetriesSinceFetch() < 0) {                return false;    }        if (totalRetries < retryMax) {        if (result.getStatus() == STATUS_DB_UNFETCHED) {                        result.getRetriesSinceFetch();            return true;        }    } else {        if (result.getStatus() == STATUS_DB_GONE) {                        return true;        }    }        return false;}
public void nutch_f3188_1()
{        Context context = CrawlDBTestUtil.createContext();    Configuration conf = context.getConfiguration();        conf.setLong("db.fetch.interval.default", 172800);        conf.setLong("db.fetch.schedule.adaptive.min_interval", 86400);        conf.setLong("db.fetch.schedule.adaptive.max_interval", 604800);        conf.setLong("db.fetch.interval.max", 604800);    conf.set("db.fetch.schedule.class", "org.apache.nutch.crawl.AdaptiveFetchSchedule");    ContinuousCrawlTestUtil crawlUtil = new CrawlTestFetchScheduleNotModifiedFetchTime(context);    crawlUtil.setInterval(FetchSchedule.SECONDS_PER_DAY / 3);    try {        if (!crawlUtil.run(100)) {            fail("failed: sync_delta calculation with AdaptiveFetchSchedule");        }    } catch (IOException e) {        e.printStackTrace();    }}
protected CrawlDatum nutch_f3189_0(CrawlDatum datum, long currentTime)
{        fetchTime = currentTime;    return super.fetch(datum, currentTime);}
protected boolean nutch_f3190_1(CrawlDatum result)
{    if (result.getStatus() == STATUS_DB_NOTMODIFIED) {                long secondsUntilNextFetch = (result.getFetchTime() - fetchTime) / 1000L;        if (secondsUntilNextFetch < -1) {                                    return false;        }        if (secondsUntilNextFetch < 60) {                                            }                if (secondsUntilNextFetch + 60 < minInterval || secondsUntilNextFetch - 60 > maxInterval) {                            }    }    return true;}
public void nutch_f3191_0() throws Exception
{    conf = CrawlDBTestUtil.createContext().getConfiguration();    fs = FileSystem.get(conf);    fs.delete(testdir, true);    urlPath = new Path(testdir, "urls");    crawldbPath = new Path(testdir, "crawldb");    segmentsPath = new Path(testdir, "segments");    server = CrawlDBTestUtil.getServer(conf.getInt("content.server.port", 50000), "build/test/data/fetch-test-site");    server.start();}
public void nutch_f3192_0() throws Exception
{    server.stop();    for (int i = 0; i < 5; i++) {        if (!server.isStopped()) {            Thread.sleep(1000);        }    }    fs.delete(testdir, true);}
public void nutch_f3193_0() throws IOException, ClassNotFoundException, InterruptedException
{        ArrayList<String> urls = new ArrayList<String>();    addUrl(urls, "index.html");    addUrl(urls, "pagea.html");    addUrl(urls, "pageb.html");    addUrl(urls, "dup_of_pagea.html");    addUrl(urls, "nested_spider_trap.html");    addUrl(urls, "exception.html");    CrawlDBTestUtil.generateSeedList(fs, urlPath, urls);        Injector injector = new Injector(conf);    injector.inject(crawldbPath, urlPath);        Generator g = new Generator(conf);    Path[] generatedSegment = g.generate(crawldbPath, segmentsPath, 1, Long.MAX_VALUE, Long.MAX_VALUE, false, false);    long time = System.currentTimeMillis();        Fetcher fetcher = new Fetcher(conf);        conf.setBoolean("fetcher.parse", true);    fetcher.fetch(generatedSegment[0], 1);    time = System.currentTimeMillis() - time;        int minimumTime = (int) ((urls.size() + 1) * 1000 * conf.getFloat("fetcher.server.delay", 5));    Assert.assertTrue(time > minimumTime);        Path content = new Path(new Path(generatedSegment[0], Content.DIR_NAME), "part-r-00000/data");    @SuppressWarnings("resource")    SequenceFile.Reader reader = new SequenceFile.Reader(conf, SequenceFile.Reader.file(content));    ArrayList<String> handledurls = new ArrayList<String>();    READ_CONTENT: do {        Text key = new Text();        Content value = new Content();        if (!reader.next(key, value))            break READ_CONTENT;        String contentString = new String(value.getContent());        if (contentString.indexOf("Nutch fetcher test page") != -1) {            handledurls.add(key.toString());        }    } while (true);    reader.close();    Collections.sort(urls);    Collections.sort(handledurls);        Assert.assertEquals(urls.size(), handledurls.size());        Assert.assertTrue(handledurls.containsAll(urls));    Assert.assertTrue(urls.containsAll(handledurls));    handledurls.clear();        Path parseData = new Path(new Path(generatedSegment[0], ParseData.DIR_NAME), "part-r-00000/data");    reader = new SequenceFile.Reader(conf, SequenceFile.Reader.file(parseData));    READ_PARSE_DATA: do {        Text key = new Text();        ParseData value = new ParseData();        if (!reader.next(key, value))            break READ_PARSE_DATA;                                Metadata contentMeta = value.getContentMeta();        if (contentMeta.get(Nutch.SEGMENT_NAME_KEY) != null && contentMeta.get(Nutch.SIGNATURE_KEY) != null) {            handledurls.add(key.toString());        }    } while (true);    Collections.sort(handledurls);    Assert.assertEquals(urls.size(), handledurls.size());    Assert.assertTrue(handledurls.containsAll(urls));    Assert.assertTrue(urls.containsAll(handledurls));}
private void nutch_f3194_0(ArrayList<String> urls, String page)
{    urls.add("http://127.0.0.1:" + server.getConnectors()[0].getPort() + "/" + page);}
public void nutch_f3195_0()
{    boolean failedNoAgentName = false;    conf.set("http.agent.name", "");    try {        conf.setBoolean("fetcher.parse", false);        Fetcher fetcher = new Fetcher(conf);        fetcher.fetch(null, 1);    } catch (IllegalArgumentException iae) {        String message = iae.getMessage();        failedNoAgentName = message.equals("Fetcher: No agents listed in " + "'http.agent.name' property.");    } catch (Exception e) {    }    Assert.assertTrue(failedNoAgentName);}
public void nutch_f3196_1()
{    configuration = NutchConfiguration.create();    configuration.setBoolean(IndexerMapReduce.INDEXER_BINARY_AS_BASE64, true);    Charset[] testCharsets = { StandardCharsets.UTF_8, Charset.forName("iso-8859-1"), Charset.forName("iso-8859-2") };    for (Charset charset : testCharsets) {                String htmlDoc = testHtmlDoc;        if (charset != StandardCharsets.UTF_8) {            htmlDoc = htmlDoc.replaceAll("utf-8", charset.name());            if (charset.name().equalsIgnoreCase("iso-8859-1")) {                                htmlDoc = htmlDoc.replaceAll("\\s*<[^>]+\\slang=\"cs\".+?\\n", "");            } else if (charset.name().equalsIgnoreCase("iso-8859-2")) {                                htmlDoc = htmlDoc.replaceAll("\\s*<[^>]+\\slang=\"fr\".+?\\n", "");            }        }        Content content = new Content(testUrl, testUrl, htmlDoc.getBytes(charset), htmlContentType, htmlMeta, configuration);        NutchDocument doc = runIndexer(crawlDatumDbFetched, crawlDatumFetchSuccess, parseText, parseData, content);        assertNotNull("No NutchDocument indexed", doc);        String binaryContentBase64 = (String) doc.getField("binaryContent").getValues().get(0);                String binaryContent = new String(Base64.decodeBase64(binaryContentBase64), charset);                assertEquals("Binary content (" + charset + ") not correctly saved as base64", htmlDoc, binaryContent);    }}
public NutchDocument nutch_f3197_1(CrawlDatum dbDatum, CrawlDatum fetchDatum, ParseText parseText, ParseData parseData, Content content)
{    List<NutchWritable> values = new ArrayList<NutchWritable>();    values.add(new NutchWritable(dbDatum));    values.add(new NutchWritable(fetchDatum));    values.add(new NutchWritable(parseText));    values.add(new NutchWritable(parseData));    values.add(new NutchWritable(content));    reduceDriver = ReduceDriver.newReduceDriver(reducer);    reduceDriver.getConfiguration().addResource(configuration);    reduceDriver.withInput(testUrlText, values);    List<Pair<Text, NutchIndexAction>> reduceResult;    NutchDocument doc = null;    try {        reduceResult = reduceDriver.run();        for (Pair<Text, NutchIndexAction> p : reduceResult) {            if (p.getSecond().action != NutchIndexAction.DELETE) {                doc = p.getSecond().doc;            }        }    } catch (IOException e) {            }    return doc;}
public void nutch_f3198_0() throws IndexingException
{    Configuration conf = NutchConfiguration.create();    conf.addResource("nutch-default.xml");    conf.addResource("crawl-tests.xml");    String class1 = "NonExistingFilter";    String class2 = "org.apache.nutch.indexer.basic.BasicIndexingFilter";    conf.set(IndexingFilters.INDEXINGFILTER_ORDER, class1 + " " + class2);    IndexingFilters filters = new IndexingFilters(conf);    filters.filter(new NutchDocument(), new ParseImpl("text", new ParseData(new ParseStatus(), "title", new Outlink[0], new Metadata())), new Text("http://www.example.com/"), new CrawlDatum(), new Inlinks());}
public void nutch_f3199_0() throws IndexingException
{    Configuration conf = NutchConfiguration.create();    conf.addResource("nutch-default.xml");    conf.addResource("crawl-tests.xml");    IndexingFilters filters = new IndexingFilters(conf);    NutchDocument doc = filters.filter(null, new ParseImpl("text", new ParseData(new ParseStatus(), "title", new Outlink[0], new Metadata())), new Text("http://www.example.com/"), new CrawlDatum(), new Inlinks());    Assert.assertNull(doc);}
public void nutch_f3200_0() throws IndexingException
{    Configuration conf = NutchConfiguration.create();    conf.addResource("nutch-default.xml");    conf.addResource("crawl-tests.xml");    String class1 = "org.apache.nutch.indexer.basic.BasicIndexingFilter";    conf.set(IndexingFilters.INDEXINGFILTER_ORDER, class1);    IndexingFilters filters1 = new IndexingFilters(conf);    NutchDocument fdoc1 = filters1.filter(new NutchDocument(), new ParseImpl("text", new ParseData(new ParseStatus(), "title", new Outlink[0], new Metadata())), new Text("http://www.example.com/"), new CrawlDatum(), new Inlinks());        String class2 = "org.apache.nutch.indexer.metadata.MetadataIndexer";        Metadata md = new Metadata();    md.add("example", "data");        conf.set("index.content.md", "example");        conf.set(IndexingFilters.INDEXINGFILTER_ORDER, class1 + " " + class2);    IndexingFilters filters2 = new IndexingFilters(conf);    NutchDocument fdoc2 = filters2.filter(new NutchDocument(), new ParseImpl("text", new ParseData(new ParseStatus(), "title", new Outlink[0], md)), new Text("http://www.example.com/"), new CrawlDatum(), new Inlinks());    Assert.assertEquals(fdoc1.getFieldNames().size(), fdoc2.getFieldNames().size());}
public void nutch_f3201_0()
{    Metadata met = new Metadata();    met.add(CONTENTTYPE, null);    met.add(CONTENTTYPE, "text/bogus");    met.add(CONTENTTYPE, "text/bogus2");    met = writeRead(met);    Assert.assertNotNull(met);    Assert.assertEquals(met.size(), 1);    boolean hasBogus = false, hasBogus2 = false;    String[] values = met.getValues(CONTENTTYPE);    Assert.assertNotNull(values);    Assert.assertEquals(values.length, 2);    for (int i = 0; i < values.length; i++) {        if (values[i].equals("text/bogus")) {            hasBogus = true;        }        if (values[i].equals("text/bogus2")) {            hasBogus2 = true;        }    }    Assert.assertTrue(hasBogus && hasBogus2);}
public void nutch_f3202_0()
{    String[] values = null;    Metadata meta = new Metadata();    values = meta.getValues(CONTENTTYPE);    Assert.assertEquals(0, values.length);    meta.add(CONTENTTYPE, "value1");    values = meta.getValues(CONTENTTYPE);    Assert.assertEquals(1, values.length);    Assert.assertEquals("value1", values[0]);    meta.add(CONTENTTYPE, "value2");    values = meta.getValues(CONTENTTYPE);    Assert.assertEquals(2, values.length);    Assert.assertEquals("value1", values[0]);    Assert.assertEquals("value2", values[1]);            meta.add(CONTENTTYPE, "value1");    values = meta.getValues(CONTENTTYPE);    Assert.assertEquals(3, values.length);    Assert.assertEquals("value1", values[0]);    Assert.assertEquals("value2", values[1]);    Assert.assertEquals("value1", values[2]);}
public void nutch_f3203_0()
{    String[] values = null;    Metadata meta = new Metadata();    values = meta.getValues(CONTENTTYPE);    Assert.assertEquals(0, values.length);    meta.set(CONTENTTYPE, "value1");    values = meta.getValues(CONTENTTYPE);    Assert.assertEquals(1, values.length);    Assert.assertEquals("value1", values[0]);    meta.set(CONTENTTYPE, "value2");    values = meta.getValues(CONTENTTYPE);    Assert.assertEquals(1, values.length);    Assert.assertEquals("value2", values[0]);    meta.set(CONTENTTYPE, "new value 1");    meta.add("contenttype", "new value 2");    values = meta.getValues(CONTENTTYPE);    Assert.assertEquals(2, values.length);    Assert.assertEquals("new value 1", values[0]);    Assert.assertEquals("new value 2", values[1]);}
public void nutch_f3204_0()
{    String[] values = null;    Metadata meta = new Metadata();    Properties props = new Properties();    meta.setAll(props);    Assert.assertEquals(0, meta.size());    props.setProperty("name-one", "value1.1");    meta.setAll(props);    Assert.assertEquals(1, meta.size());    values = meta.getValues("name-one");    Assert.assertEquals(1, values.length);    Assert.assertEquals("value1.1", values[0]);    props.setProperty("name-two", "value2.1");    meta.setAll(props);    Assert.assertEquals(2, meta.size());    values = meta.getValues("name-one");    Assert.assertEquals(1, values.length);    Assert.assertEquals("value1.1", values[0]);    values = meta.getValues("name-two");    Assert.assertEquals(1, values.length);    Assert.assertEquals("value2.1", values[0]);}
public void nutch_f3205_0()
{    Metadata meta = new Metadata();    Assert.assertNull(meta.get("a-name"));    meta.add("a-name", "value-1");    Assert.assertEquals("value-1", meta.get("a-name"));    meta.add("a-name", "value-2");    Assert.assertEquals("value-1", meta.get("a-name"));}
public void nutch_f3206_0()
{    Metadata meta = new Metadata();    Assert.assertFalse(meta.isMultiValued("key"));    meta.add("key", "value1");    Assert.assertFalse(meta.isMultiValued("key"));    meta.add("key", "value2");    Assert.assertTrue(meta.isMultiValued("key"));}
public void nutch_f3207_0()
{    String[] names = null;    Metadata meta = new Metadata();    names = meta.names();    Assert.assertEquals(0, names.length);    meta.add("name-one", "value");    names = meta.names();    Assert.assertEquals(1, names.length);    Assert.assertEquals("name-one", names[0]);    meta.add("name-two", "value");    names = meta.names();    Assert.assertEquals(2, names.length);}
public void nutch_f3208_0()
{    Metadata meta = new Metadata();    meta.remove("name-one");    Assert.assertEquals(0, meta.size());    meta.add("name-one", "value-1.1");    meta.add("name-one", "value-1.2");    meta.add("name-two", "value-2.2");    Assert.assertEquals(2, meta.size());    Assert.assertNotNull(meta.get("name-one"));    Assert.assertNotNull(meta.get("name-two"));    meta.remove("name-one");    Assert.assertEquals(1, meta.size());    Assert.assertNull(meta.get("name-one"));    Assert.assertNotNull(meta.get("name-two"));    meta.remove("name-two");    Assert.assertEquals(0, meta.size());    Assert.assertNull(meta.get("name-one"));    Assert.assertNull(meta.get("name-two"));}
public void nutch_f3209_0()
{    Metadata meta1 = new Metadata();    Metadata meta2 = new Metadata();    Assert.assertFalse(meta1.equals(null));    Assert.assertFalse(meta1.equals("String"));    Assert.assertTrue(meta1.equals(meta2));    meta1.add("name-one", "value-1.1");    Assert.assertFalse(meta1.equals(meta2));    meta2.add("name-one", "value-1.1");    Assert.assertTrue(meta1.equals(meta2));    meta1.add("name-one", "value-1.2");    Assert.assertFalse(meta1.equals(meta2));    meta2.add("name-one", "value-1.2");    Assert.assertTrue(meta1.equals(meta2));    meta1.add("name-two", "value-2.1");    Assert.assertFalse(meta1.equals(meta2));    meta2.add("name-two", "value-2.1");    Assert.assertTrue(meta1.equals(meta2));    meta1.add("name-two", "value-2.2");    Assert.assertFalse(meta1.equals(meta2));    meta2.add("name-two", "value-2.x");    Assert.assertFalse(meta1.equals(meta2));}
public void nutch_f3210_0()
{    Metadata result = null;    Metadata meta = new Metadata();    result = writeRead(meta);    Assert.assertEquals(0, result.size());    meta.add("name-one", "value-1.1");    result = writeRead(meta);    Assert.assertEquals(1, result.size());    Assert.assertEquals(1, result.getValues("name-one").length);    Assert.assertEquals("value-1.1", result.get("name-one"));    meta.add("name-two", "value-2.1");    meta.add("name-two", "value-2.2");    result = writeRead(meta);    Assert.assertEquals(2, result.size());    Assert.assertEquals(1, result.getValues("name-one").length);    Assert.assertEquals("value-1.1", result.getValues("name-one")[0]);    Assert.assertEquals(2, result.getValues("name-two").length);    Assert.assertEquals("value-2.1", result.getValues("name-two")[0]);    Assert.assertEquals("value-2.2", result.getValues("name-two")[1]);}
private Metadata nutch_f3211_0(Metadata meta)
{    Metadata readed = new Metadata();    try {        ByteArrayOutputStream out = new ByteArrayOutputStream();        meta.write(new DataOutputStream(out));        readed.readFields(new DataInputStream(new ByteArrayInputStream(out.toByteArray())));    } catch (IOException ioe) {        Assert.fail(ioe.toString());    }    return readed;}
public void nutch_f3212_0()
{    Assert.assertEquals("Content-Type", SpellCheckedMetadata.getNormalizedName("Content-Type"));    Assert.assertEquals("Content-Type", SpellCheckedMetadata.getNormalizedName("ContentType"));    Assert.assertEquals("Content-Type", SpellCheckedMetadata.getNormalizedName("Content-type"));    Assert.assertEquals("Content-Type", SpellCheckedMetadata.getNormalizedName("contenttype"));    Assert.assertEquals("Content-Type", SpellCheckedMetadata.getNormalizedName("contentype"));    Assert.assertEquals("Content-Type", SpellCheckedMetadata.getNormalizedName("contntype"));}
public void nutch_f3213_0()
{    String[] values = null;    SpellCheckedMetadata meta = new SpellCheckedMetadata();    values = meta.getValues("contentype");    Assert.assertEquals(0, values.length);    meta.add("contentype", "value1");    values = meta.getValues("contentype");    Assert.assertEquals(1, values.length);    Assert.assertEquals("value1", values[0]);    meta.add("Content-Type", "value2");    values = meta.getValues("contentype");    Assert.assertEquals(2, values.length);    Assert.assertEquals("value1", values[0]);    Assert.assertEquals("value2", values[1]);            meta.add("ContentType", "value1");    values = meta.getValues("Content-Type");    Assert.assertEquals(3, values.length);    Assert.assertEquals("value1", values[0]);    Assert.assertEquals("value2", values[1]);    Assert.assertEquals("value1", values[2]);}
public void nutch_f3214_0()
{    String[] values = null;    SpellCheckedMetadata meta = new SpellCheckedMetadata();    values = meta.getValues("contentype");    Assert.assertEquals(0, values.length);    meta.set("contentype", "value1");    values = meta.getValues("contentype");    Assert.assertEquals(1, values.length);    Assert.assertEquals("value1", values[0]);    meta.set("Content-Type", "value2");    values = meta.getValues("contentype");    Assert.assertEquals(1, values.length);    Assert.assertEquals("value2", values[0]);    meta.set("contenttype", "new value 1");    meta.add("contenttype", "new value 2");    values = meta.getValues("contentype");    Assert.assertEquals(2, values.length);    Assert.assertEquals("new value 1", values[0]);    Assert.assertEquals("new value 2", values[1]);}
public void nutch_f3215_0()
{    String[] values = null;    SpellCheckedMetadata meta = new SpellCheckedMetadata();    Properties props = new Properties();    meta.setAll(props);    Assert.assertEquals(0, meta.size());    props.setProperty("name-one", "value1.1");    meta.setAll(props);    Assert.assertEquals(1, meta.size());    values = meta.getValues("name-one");    Assert.assertEquals(1, values.length);    Assert.assertEquals("value1.1", values[0]);    props.setProperty("name-two", "value2.1");    meta.setAll(props);    Assert.assertEquals(2, meta.size());    values = meta.getValues("name-one");    Assert.assertEquals(1, values.length);    Assert.assertEquals("value1.1", values[0]);    values = meta.getValues("name-two");    Assert.assertEquals(1, values.length);    Assert.assertEquals("value2.1", values[0]);}
public void nutch_f3216_0()
{    SpellCheckedMetadata meta = new SpellCheckedMetadata();    Assert.assertNull(meta.get("a-name"));    meta.add("a-name", "value-1");    Assert.assertEquals("value-1", meta.get("a-name"));    meta.add("a-name", "value-2");    Assert.assertEquals("value-1", meta.get("a-name"));}
public void nutch_f3217_0()
{    SpellCheckedMetadata meta = new SpellCheckedMetadata();    Assert.assertFalse(meta.isMultiValued("key"));    meta.add("key", "value1");    Assert.assertFalse(meta.isMultiValued("key"));    meta.add("key", "value2");    Assert.assertTrue(meta.isMultiValued("key"));}
public void nutch_f3218_0()
{    String[] names = null;    SpellCheckedMetadata meta = new SpellCheckedMetadata();    names = meta.names();    Assert.assertEquals(0, names.length);    meta.add("name-one", "value");    names = meta.names();    Assert.assertEquals(1, names.length);    Assert.assertEquals("name-one", names[0]);    meta.add("name-two", "value");    names = meta.names();    Assert.assertEquals(2, names.length);}
public void nutch_f3219_0()
{    SpellCheckedMetadata meta = new SpellCheckedMetadata();    meta.remove("name-one");    Assert.assertEquals(0, meta.size());    meta.add("name-one", "value-1.1");    meta.add("name-one", "value-1.2");    meta.add("name-two", "value-2.2");    Assert.assertEquals(2, meta.size());    Assert.assertNotNull(meta.get("name-one"));    Assert.assertNotNull(meta.get("name-two"));    meta.remove("name-one");    Assert.assertEquals(1, meta.size());    Assert.assertNull(meta.get("name-one"));    Assert.assertNotNull(meta.get("name-two"));    meta.remove("name-two");    Assert.assertEquals(0, meta.size());    Assert.assertNull(meta.get("name-one"));    Assert.assertNull(meta.get("name-two"));}
public void nutch_f3220_0()
{    SpellCheckedMetadata meta1 = new SpellCheckedMetadata();    SpellCheckedMetadata meta2 = new SpellCheckedMetadata();    Assert.assertFalse(meta1.equals(null));    Assert.assertFalse(meta1.equals("String"));    Assert.assertTrue(meta1.equals(meta2));    meta1.add("name-one", "value-1.1");    Assert.assertFalse(meta1.equals(meta2));    meta2.add("name-one", "value-1.1");    Assert.assertTrue(meta1.equals(meta2));    meta1.add("name-one", "value-1.2");    Assert.assertFalse(meta1.equals(meta2));    meta2.add("name-one", "value-1.2");    Assert.assertTrue(meta1.equals(meta2));    meta1.add("name-two", "value-2.1");    Assert.assertFalse(meta1.equals(meta2));    meta2.add("name-two", "value-2.1");    Assert.assertTrue(meta1.equals(meta2));    meta1.add("name-two", "value-2.2");    Assert.assertFalse(meta1.equals(meta2));    meta2.add("name-two", "value-2.x");    Assert.assertFalse(meta1.equals(meta2));}
public void nutch_f3221_0()
{    SpellCheckedMetadata result = null;    SpellCheckedMetadata meta = new SpellCheckedMetadata();    result = writeRead(meta);    Assert.assertEquals(0, result.size());    meta.add("name-one", "value-1.1");    result = writeRead(meta);    meta.add("Contenttype", "text/html");    Assert.assertEquals(1, result.size());    Assert.assertEquals(1, result.getValues("name-one").length);    Assert.assertEquals("value-1.1", result.get("name-one"));    meta.add("name-two", "value-2.1");    meta.add("name-two", "value-2.2");    result = writeRead(meta);    Assert.assertEquals(3, result.size());    Assert.assertEquals(1, result.getValues("name-one").length);    Assert.assertEquals("value-1.1", result.getValues("name-one")[0]);    Assert.assertEquals(2, result.getValues("name-two").length);    Assert.assertEquals("value-2.1", result.getValues("name-two")[0]);    Assert.assertEquals("value-2.2", result.getValues("name-two")[1]);    Assert.assertEquals("text/html", result.get(Metadata.CONTENT_TYPE));}
public final void nutch_f3222_0()
{    @SuppressWarnings("unused")    SpellCheckedMetadata result;    long start = System.currentTimeMillis();    for (int i = 0; i < NUM_ITERATIONS; i++) {        SpellCheckedMetadata scmd = constructSpellCheckedMetadata();        result = writeRead(scmd);    }    System.out.println(NUM_ITERATIONS + " spellchecked metadata I/O time:" + (System.currentTimeMillis() - start) + "ms.");}
private SpellCheckedMetadata nutch_f3223_0(SpellCheckedMetadata meta)
{    SpellCheckedMetadata readed = new SpellCheckedMetadata();    try {        ByteArrayOutputStream out = new ByteArrayOutputStream();        meta.write(new DataOutputStream(out));        readed.readFields(new DataInputStream(new ByteArrayInputStream(out.toByteArray())));    } catch (IOException ioe) {        Assert.fail(ioe.toString());    }    return readed;}
public static final SpellCheckedMetadata nutch_f3224_0()
{    SpellCheckedMetadata scmd = new SpellCheckedMetadata();    scmd.add("Content-type", "foo/bar");    scmd.add("Connection", "close");    scmd.add("Last-Modified", "Sat, 09 Dec 2006 15:09:57 GMT");    scmd.add("Server", "Foobar");    scmd.add("Date", "Sat, 09 Dec 2006 18:07:20 GMT");    scmd.add("Accept-Ranges", "bytes");    scmd.add("ETag", "\"1234567-89-01234567\"");    scmd.add("Content-Length", "123");    scmd.add(Nutch.SEGMENT_NAME_KEY, "segmentzzz");    scmd.add(Nutch.SIGNATURE_KEY, "123");    return scmd;}
public void nutch_f3225_0() throws URLFilterException
{    Configuration conf = NutchConfiguration.create();    String class1 = "NonExistingFilter";    String class2 = "org.apache.nutch.urlfilter.prefix.PrefixURLFilter";    conf.set(URLFilters.URLFILTER_ORDER, class1 + " " + class2);    URLFilters normalizers = new URLFilters(conf);    normalizers.filter("http://someurl/");}
public void nutch_f3226_0()
{    Configuration conf = NutchConfiguration.create();    String clazz1 = "org.apache.nutch.net.urlnormalizer.regex.RegexURLNormalizer";    String clazz2 = "org.apache.nutch.net.urlnormalizer.basic.BasicURLNormalizer";    conf.set("urlnormalizer.order", clazz1 + " " + clazz2);    URLNormalizers normalizers = new URLNormalizers(conf, URLNormalizers.SCOPE_DEFAULT);    Assert.assertNotNull(normalizers);    try {        normalizers.normalize("http://www.example.com/", URLNormalizers.SCOPE_DEFAULT);    } catch (MalformedURLException mue) {        Assert.fail(mue.toString());    }        try {        String normalizedSlashes = normalizers.normalize("http://www.example.com//path/to//somewhere.html", URLNormalizers.SCOPE_DEFAULT);        Assert.assertEquals(normalizedSlashes, "http://www.example.com/path/to/somewhere.html");    } catch (MalformedURLException mue) {        Assert.fail(mue.toString());    }        try {        String normalizedHost = normalizers.normalize("http://www.example.org//path/to//somewhere.html", URLNormalizers.SCOPE_DEFAULT);        Assert.assertEquals(normalizedHost, "http://www.example.org/path/to/somewhere.html");    } catch (MalformedURLException mue) {        Assert.fail(mue.toString());    }        int pos1 = -1, pos2 = -1;    URLNormalizer[] impls = normalizers.getURLNormalizers(URLNormalizers.SCOPE_DEFAULT);    for (int i = 0; i < impls.length; i++) {        if (impls[i].getClass().getName().equals(clazz1))            pos1 = i;        if (impls[i].getClass().getName().equals(clazz2))            pos2 = i;    }    if (pos1 != -1 && pos2 != -1) {        Assert.assertTrue("RegexURLNormalizer before BasicURLNormalizer", pos1 < pos2);    }}
public void nutch_f3227_0()
{    Outlink[] outlinks = null;    outlinks = OutlinkExtractor.getOutlinks(null, conf);    Assert.assertNotNull(outlinks);    Assert.assertEquals(0, outlinks.length);    outlinks = OutlinkExtractor.getOutlinks("", conf);    Assert.assertNotNull(outlinks);    Assert.assertEquals(0, outlinks.length);}
public void nutch_f3228_0()
{    Outlink[] outlinks = OutlinkExtractor.getOutlinks("Test with http://www.nutch.org/index.html is it found? " + "What about www.google.com at http://www.google.de " + "A longer URL could be http://www.sybit.com/solutions/portals.html", conf);    Assert.assertTrue("Url not found!", outlinks.length == 3);    Assert.assertEquals("Wrong URL", "http://www.nutch.org/index.html", outlinks[0].getToUrl());    Assert.assertEquals("Wrong URL", "http://www.google.de", outlinks[1].getToUrl());    Assert.assertEquals("Wrong URL", "http://www.sybit.com/solutions/portals.html", outlinks[2].getToUrl());}
public void nutch_f3229_0()
{    Outlink[] outlinks = OutlinkExtractor.getOutlinks("Test with http://www.nutch.org/index.html is it found? " + "What about www.google.com at http://www.google.de " + "A longer URL could be http://www.sybit.com/solutions/portals.html", "http://www.sybit.de", conf);    Assert.assertTrue("Url not found!", outlinks.length == 3);    Assert.assertEquals("Wrong URL", "http://www.nutch.org/index.html", outlinks[0].getToUrl());    Assert.assertEquals("Wrong URL", "http://www.google.de", outlinks[1].getToUrl());    Assert.assertEquals("Wrong URL", "http://www.sybit.com/solutions/portals.html", outlinks[2].getToUrl());}
public void nutch_f3230_0()
{    Outlink[] outlinks = OutlinkExtractor.getOutlinks("Test with ftp://www.nutch.org is it found? " + "What about www.google.com at ftp://www.google.de", conf);    Assert.assertTrue("Url not found!", outlinks.length > 1);    Assert.assertEquals("Wrong URL", "ftp://www.nutch.org", outlinks[0].getToUrl());    Assert.assertEquals("Wrong URL", "ftp://www.google.de", outlinks[1].getToUrl());}
public void nutch_f3231_0() throws Exception
{    Set<Outlink> set = new HashSet<>();    Outlink o = new Outlink("http://www.example.com", "Example");    set.add(o);    set.add(o);    assertEquals("Adding the same Outlink twice", 1, set.size());}
public void nutch_f3232_0() throws Exception
{    Set<Outlink> set = new HashSet<>();    Outlink o = new Outlink("http://www.example.com", "Example");    Outlink o1 = new Outlink("http://www.example.com", "Example");    assertTrue("The two Outlink objects are the same", o.equals(o1));    set.add(o);    set.add(o1);    assertEquals("The set should contain only 1 Outlink", 1, set.size());}
public void nutch_f3233_0() throws Exception
{    String title = "The Foo Page";    Outlink[] outlinks = new Outlink[] { new Outlink("http://foo.com/", "Foo"), new Outlink("http://bar.com/", "Bar") };    Metadata metaData = new Metadata();    metaData.add("Language", "en/us");    metaData.add("Charset", "UTF-8");    ParseData r = new ParseData(ParseStatus.STATUS_SUCCESS, title, outlinks, metaData);    WritableTestUtils.testWritable(r, null);}
public void nutch_f3234_0() throws Exception
{    Outlink[] outlinks = new Outlink[128];    for (int i = 0; i < outlinks.length; i++) {        outlinks[i] = new Outlink("http://outlink.com/" + i, "Outlink" + i);    }    ParseData original = new ParseData(ParseStatus.STATUS_SUCCESS, "Max Outlinks Title", outlinks, new Metadata());    ParseData data = (ParseData) WritableTestUtils.writeRead(original, null);    Assert.assertEquals(outlinks.length, data.getOutlinks().length);}
public void nutch_f3235_0() throws Exception
{    conf = NutchConfiguration.create();    conf.set("plugin.includes", ".*");    conf.set("parse.plugin.file", "org/apache/nutch/parse/parse-plugin-test.xml");    parserFactory = new ParserFactory(conf);}
public void nutch_f3236_0() throws Exception
{    Extension ext = parserFactory.getExtensions("text/html").get(0);    Assert.assertEquals("parse-tika", ext.getDescriptor().getPluginId());    ext = parserFactory.getExtensions("text/html; charset=ISO-8859-1").get(0);    Assert.assertEquals("parse-tika", ext.getDescriptor().getPluginId());    ext = parserFactory.getExtensions("foo/bar").get(0);    Assert.assertEquals("parse-tika", ext.getDescriptor().getPluginId());}
public void nutch_f3237_0() throws Exception
{    Parser[] parsers = parserFactory.getParsers("text/html", "http://foo.com");    Assert.assertNotNull(parsers);    Assert.assertEquals(1, parsers.length);    Assert.assertEquals("org.apache.nutch.parse.tika.TikaParser", parsers[0].getClass().getName());    parsers = parserFactory.getParsers("text/html; charset=ISO-8859-1", "http://foo.com");    Assert.assertNotNull(parsers);    Assert.assertEquals(1, parsers.length);    Assert.assertEquals("org.apache.nutch.parse.tika.TikaParser", parsers[0].getClass().getName());    parsers = parserFactory.getParsers("application/x-javascript", "http://foo.com");    Assert.assertNotNull(parsers);    Assert.assertEquals(1, parsers.length);    Assert.assertEquals("org.apache.nutch.parse.js.JSParseFilter", parsers[0].getClass().getName());    parsers = parserFactory.getParsers("text/plain", "http://foo.com");    Assert.assertNotNull(parsers);    Assert.assertEquals(1, parsers.length);    Assert.assertEquals("org.apache.nutch.parse.tika.TikaParser", parsers[0].getClass().getName());    Parser parser1 = parserFactory.getParsers("text/plain", "http://foo.com")[0];    Parser parser2 = parserFactory.getParsers("*", "http://foo.com")[0];    Assert.assertEquals("Different instances!", parser1.hashCode(), parser2.hashCode());                parsers = parserFactory.getParsers("text/rss", "http://foo.com");    Assert.assertNotNull(parsers);    Assert.assertEquals(1, parsers.length);    Assert.assertEquals("org.apache.nutch.parse.tika.TikaParser", parsers[0].getClass().getName());}
public void nutch_f3238_0() throws Exception
{    String page = "Hello World The Quick Brown Fox Jumped Over the Lazy Fox";    ParseText s = new ParseText(page);    WritableTestUtils.testWritable(s);}
public String nutch_f3239_0(String hello)
{    return hello + " World";}
public void nutch_f3240_0() throws PluginRuntimeException
{    System.err.println("start up Plugin: " + getDescriptor().getPluginId());}
public void nutch_f3241_0() throws PluginRuntimeException
{    System.err.println("shutdown Plugin: " + getDescriptor().getPluginId());}
public void nutch_f3242_0() throws Exception
{    this.conf = NutchConfiguration.create();    conf.set("plugin.includes", ".*");            fPluginCount = 5;    createDummyPlugins(fPluginCount);    this.repository = PluginRepository.get(conf);}
public void nutch_f3243_0() throws Exception
{    for (int i = 0; i < fFolders.size(); i++) {        File folder = fFolders.get(i);        delete(folder);        folder.delete();    }}
public void nutch_f3244_0()
{    String string = getPluginFolder();    File file = new File(string);    if (!file.exists()) {        file.mkdir();    }    Assert.assertTrue(file.exists());}
public void nutch_f3245_0()
{    PluginDescriptor[] descriptors = repository.getPluginDescriptors();    int k = descriptors.length;    Assert.assertTrue(fPluginCount <= k);    for (int i = 0; i < descriptors.length; i++) {        PluginDescriptor descriptor = descriptors[i];        if (!descriptor.getPluginId().startsWith("getPluginFolder()")) {            continue;        }        Assert.assertEquals(1, descriptor.getExportedLibUrls().length);        Assert.assertEquals(1, descriptor.getNotExportedLibUrls().length);    }}
public void nutch_f3246_0() throws IOException
{    Configuration config = NutchConfiguration.create();    PluginRepository repo = PluginRepository.get(config);    Job job = NutchJob.getInstance(config);    config = job.getConfiguration();    PluginRepository repo1 = PluginRepository.get(config);    Assert.assertTrue(repo == repo1);        config = new Configuration();    config.addResource("nutch-default.xml");    config.addResource("nutch-site.xml");    repo = PluginRepository.get(config);    job = NutchJob.getInstance(config);    config = job.getConfiguration();    repo1 = PluginRepository.get(config);    Assert.assertTrue(repo1 != repo);}
public void nutch_f3247_0()
{    String xpId = " sdsdsd";    ExtensionPoint extensionPoint = repository.getExtensionPoint(xpId);    Assert.assertEquals(extensionPoint, null);    Extension[] extension1 = repository.getExtensionPoint(getGetExtensionId()).getExtensions();    Assert.assertEquals(extension1.length, fPluginCount);    for (int i = 0; i < extension1.length; i++) {        Extension extension2 = extension1[i];        String string = extension2.getAttribute(getGetConfigElementName());        Assert.assertEquals(string, getParameterValue());    }}
public void nutch_f3248_0() throws PluginRuntimeException
{    Extension[] extensions = repository.getExtensionPoint(getGetExtensionId()).getExtensions();    Assert.assertEquals(extensions.length, fPluginCount);    for (int i = 0; i < extensions.length; i++) {        Extension extension = extensions[i];        Object object = extension.getExtensionInstance();        if (!(object instanceof HelloWorldExtension))            Assert.fail(" object is not a instance of HelloWorldExtension");        ((ITestExtension) object).testGetExtension("Bla ");        String string = ((ITestExtension) object).testGetExtension("Hello");        Assert.assertEquals("Hello World", string);    }}
public void nutch_f3249_0()
{    PluginDescriptor[] descriptors = repository.getPluginDescriptors();    for (int i = 0; i < descriptors.length; i++) {        PluginDescriptor descriptor = descriptors[i];        Assert.assertNotNull(descriptor.getClassLoader());    }}
public void nutch_f3250_0() throws IOException
{    PluginDescriptor[] descriptors = repository.getPluginDescriptors();    for (int i = 0; i < descriptors.length; i++) {        PluginDescriptor descriptor = descriptors[i];        if (!descriptor.getPluginId().startsWith("getPluginFolder()")) {            continue;        }        String value = descriptor.getResourceString("key", Locale.UK);        Assert.assertEquals("value", value);        value = descriptor.getResourceString("key", Locale.TRADITIONAL_CHINESE);        Assert.assertEquals("value", value);    }}
private String nutch_f3251_0()
{    String[] strings = conf.getStrings("plugin.folders");    if (strings == null || strings.length == 0)        Assert.fail("no plugin directory setuped..");    String name = strings[0];    return new PluginManifestParser(conf, this.repository).getPluginFolder(name).toString();}
private void nutch_f3252_0(int pCount)
{    String string = getPluginFolder();    try {        File folder = new File(string);        folder.mkdir();        for (int i = 0; i < pCount; i++) {            String pluginFolder = string + File.separator + "DummyPlugin" + i;            File file = new File(pluginFolder);            file.mkdir();            fFolders.add(file);            createPluginManifest(i, file.getAbsolutePath());            createResourceFile(file.getAbsolutePath());        }    } catch (IOException e) {        e.printStackTrace();    }}
private void nutch_f3253_0(String pFolderPath) throws FileNotFoundException, IOException
{    Properties properties = new Properties();    properties.setProperty("key", "value");    properties.store(new FileOutputStream(pFolderPath + File.separator + "messages" + ".properties"), "");}
private void nutch_f3254_0(File path) throws IOException
{    File[] files = path.listFiles();    for (int i = 0; i < files.length; ++i) {        if (files[i].isDirectory())            delete(files[i]);        files[i].delete();    }}
private void nutch_f3255_0(int i, String pFolderPath) throws IOException
{    FileWriter out = new FileWriter(pFolderPath + File.separator + "plugin.xml");    String xml = "<?xml version=\"1.0\" encoding=\"UTF-8\"?>" + "<!--this is just a simple plugin for testing issues.-->" + "<plugin id=\"org.apache.nutch.plugin." + i + "\" name=\"" + i + "\" version=\"1.0\" provider-name=\"joa23\" " + "class=\"org.apache.nutch.plugin.SimpleTestPlugin\">" + "<extension-point id=\"aExtensioID\" " + "name=\"simple Parser Extension\" " + "schema=\"schema/testExtensionPoint.exsd\"/>" + "<runtime><library name=\"libs/exported.jar\"><extport/></library>" + "<library name=\"libs/not_exported.jar\"/></runtime>" + "<extension point=\"aExtensioID\">" + "<implementation name=\"simple Parser Extension\" " + "id=\"aExtensionId.\" class=\"org.apache.nutch.plugin.HelloWorldExtension\">" + "<parameter name=\"dummy-name\" value=\"a simple param value\"/>" + "</implementation></extension></plugin>";    out.write(xml);    out.flush();    out.close();}
private String nutch_f3256_0()
{    return "a simple param value";}
private static String nutch_f3257_0()
{    return "aExtensioID";}
private static String nutch_f3258_0()
{    return "dummy-name";}
public static void nutch_f3259_0(String[] args) throws IOException
{    new TestPluginSystem().createPluginManifest(1, "/");}
public void nutch_f3260_0() throws Exception
{    String page = "<HTML><BODY><H1>Hello World</H1><P>The Quick Brown Fox Jumped Over the Lazy Fox.</BODY></HTML>";    String url = "http://www.foo.com/";    SpellCheckedMetadata metaData = new SpellCheckedMetadata();    metaData.add("Host", "www.foo.com");    metaData.add("Content-Type", "text/html");    Content r = new Content(url, url, page.getBytes("UTF8"), "text/html", metaData, conf);    WritableTestUtils.testWritable(r);    Assert.assertEquals("text/html", r.getMetadata().get("Content-Type"));    Assert.assertEquals("text/html", r.getMetadata().get("content-type"));    Assert.assertEquals("text/html", r.getMetadata().get("CONTENTYPE"));}
public void nutch_f3261_0() throws Exception
{    Content c = null;    Metadata p = new Metadata();    c = new Content("http://www.foo.com/", "http://www.foo.com/", "".getBytes("UTF8"), "text/html; charset=UTF-8", p, conf);    Assert.assertEquals("text/html", c.getContentType());    c = new Content("http://www.foo.com/foo.html", "http://www.foo.com/", "".getBytes("UTF8"), "", p, conf);    Assert.assertEquals("text/html", c.getContentType());    c = new Content("http://www.foo.com/foo.html", "http://www.foo.com/", "".getBytes("UTF8"), null, p, conf);    Assert.assertEquals("text/html", c.getContentType());    c = new Content("http://www.foo.com/", "http://www.foo.com/", "<html></html>".getBytes("UTF8"), "", p, conf);    Assert.assertEquals("text/html", c.getContentType());    c = new Content("http://www.foo.com/foo.html", "http://www.foo.com/", "<html></html>".getBytes("UTF8"), "text/plain", p, conf);    Assert.assertEquals("text/html", c.getContentType());    c = new Content("http://www.foo.com/foo.png", "http://www.foo.com/", "<html></html>".getBytes("UTF8"), "text/plain", p, conf);    Assert.assertEquals("text/html", c.getContentType());    c = new Content("http://www.foo.com/", "http://www.foo.com/", "".getBytes("UTF8"), "", p, conf);    Assert.assertEquals(MimeTypes.OCTET_STREAM, c.getContentType());    c = new Content("http://www.foo.com/", "http://www.foo.com/", "".getBytes("UTF8"), null, p, conf);    Assert.assertNotNull(c.getContentType());}
public void nutch_f3262_0() throws Exception
{    conf = NutchConfiguration.create();    conf.set("plugin.includes", ".*");    conf.set("http.agent.name", "test-bot");    factory = new ProtocolFactory(conf);}
public void nutch_f3263_0()
{        try {        factory.getProtocol("xyzxyz://somehost");        Assert.fail("Must throw ProtocolNotFound");    } catch (ProtocolNotFound e) {        } catch (Exception ex) {        Assert.fail("Must not throw any other exception");    }    Protocol httpProtocol = null;        try {        httpProtocol = factory.getProtocol("http://somehost");        Assert.assertNotNull(httpProtocol);    } catch (Exception ex) {        Assert.fail("Must not throw any other exception");    }        try {        Assert.assertTrue(httpProtocol == factory.getProtocol("http://somehost"));    } catch (ProtocolNotFound e) {        Assert.fail("Must not throw any exception");    }}
public void nutch_f3264_0()
{    Assert.assertTrue(factory.contains("http", "http"));    Assert.assertTrue(factory.contains("http", "http,ftp"));    Assert.assertTrue(factory.contains("http", "   http ,   ftp"));    Assert.assertTrue(factory.contains("smb", "ftp,smb,http"));    Assert.assertFalse(factory.contains("smb", "smbb"));}
public void nutch_f3265_0() throws Exception
{    conf = NutchConfiguration.create();    fs = FileSystem.get(conf);    testDir = new Path(conf.get("hadoop.tmp.dir"), "merge-" + System.currentTimeMillis());    seg1 = new Path(testDir, "seg1");    seg2 = new Path(testDir, "seg2");    out = new Path(testDir, "out");        System.err.println("Creating large segment 1...");    DecimalFormat df = new DecimalFormat("0000000");    Text k = new Text();    Path ptPath = new Path(new Path(seg1, ParseText.DIR_NAME), "part-00000");    Option kOpt = MapFile.Writer.keyClass(Text.class);    org.apache.hadoop.io.SequenceFile.Writer.Option vOpt = SequenceFile.Writer.valueClass(ParseText.class);    MapFile.Writer w = new MapFile.Writer(conf, ptPath, kOpt, vOpt);    long curSize = 0;    countSeg1 = 0;    FileStatus fileStatus = fs.getFileStatus(ptPath);    long blkSize = fileStatus.getBlockSize();    while (curSize < blkSize * 2) {        k.set("seg1-" + df.format(countSeg1));        w.append(k, new ParseText("seg1 text " + countSeg1));        countSeg1++;                curSize += 40;    }    w.close();    System.err.println(" - done: " + countSeg1 + " records.");    System.err.println("Creating large segment 2...");    ptPath = new Path(new Path(seg2, ParseText.DIR_NAME), "part-00000");    Option wKeyOpt = MapFile.Writer.keyClass(Text.class);    org.apache.hadoop.io.SequenceFile.Writer.Option wValueOpt = SequenceFile.Writer.valueClass(ParseText.class);    w = new MapFile.Writer(conf, ptPath, wKeyOpt, wValueOpt);    curSize = 0;    countSeg2 = 0;    while (curSize < blkSize * 2) {        k.set("seg2-" + df.format(countSeg2));        w.append(k, new ParseText("seg2 text " + countSeg2));        countSeg2++;                curSize += 40;    }    w.close();    System.err.println(" - done: " + countSeg2 + " records.");}
public void nutch_f3266_0() throws Exception
{    fs.delete(testDir, true);}
public void nutch_f3267_0() throws Exception
{    SegmentMerger merger = new SegmentMerger(conf);    merger.merge(out, new Path[] { seg1, seg2 }, false, false, -1);        FileStatus[] stats = fs.listStatus(out);        Assert.assertEquals(1, stats.length);    Path outSeg = stats[0].getPath();    Text k = new Text();    ParseText v = new ParseText();    MapFile.Reader[] readers = MapFileOutputFormat.getReaders(new Path(outSeg, ParseText.DIR_NAME), conf);    int cnt1 = 0, cnt2 = 0;    for (MapFile.Reader r : readers) {        while (r.next(k, v)) {            String ks = k.toString();            String vs = v.getText();            if (ks.startsWith("seg1-")) {                cnt1++;                Assert.assertTrue(vs.startsWith("seg1 "));            } else if (ks.startsWith("seg2-")) {                cnt2++;                Assert.assertTrue(vs.startsWith("seg2 "));            }        }        r.close();    }    Assert.assertEquals(countSeg1, cnt1);    Assert.assertEquals(countSeg2, cnt2);}
public void nutch_f3268_0() throws Exception
{    conf = NutchConfiguration.create();    fs = FileSystem.get(conf);    rnd = new Random();}
public void nutch_f3269_0() throws Exception
{    Assert.assertEquals(Byte.valueOf(CrawlDatum.STATUS_FETCH_SUCCESS), Byte.valueOf(executeSequence(CrawlDatum.STATUS_FETCH_GONE, CrawlDatum.STATUS_FETCH_SUCCESS, 256, false)));}
public void nutch_f3270_0() throws Exception
{        Path testDir = new Path(conf.get("hadoop.tmp.dir"), "merge-" + System.currentTimeMillis());    Path segment1 = new Path(testDir, "20140110114943");    Path segment2 = new Path(testDir, "20140110114832");    Path segment3 = new Path(testDir, "20140110114558");    Path segment4 = new Path(testDir, "20140110114930");    Path segment5 = new Path(testDir, "20140110114545");    Path segment6 = new Path(testDir, "20140110114507");    Path segment7 = new Path(testDir, "20140110114903");    Path segment8 = new Path(testDir, "20140110114724");    createSegment(segment1, CrawlDatum.STATUS_FETCH_SUCCESS, true);    createSegment(segment2, CrawlDatum.STATUS_FETCH_SUCCESS, true);    createSegment(segment3, CrawlDatum.STATUS_FETCH_SUCCESS, true);    createSegment(segment4, CrawlDatum.STATUS_FETCH_SUCCESS, true);    createSegment(segment5, CrawlDatum.STATUS_FETCH_SUCCESS, true);    createSegment(segment6, CrawlDatum.STATUS_FETCH_SUCCESS, false);    createSegment(segment7, CrawlDatum.STATUS_FETCH_SUCCESS, true);    createSegment(segment8, CrawlDatum.STATUS_FETCH_SUCCESS, true);        Path mergedSegment = merge(testDir, new Path[] { segment1, segment2, segment3, segment4, segment5, segment6, segment7, segment8 });    Byte status = Byte.valueOf(status = checkMergedSegment(testDir, mergedSegment));    Assert.assertEquals(Byte.valueOf(CrawlDatum.STATUS_FETCH_SUCCESS), status);}
public void nutch_f3271_0() throws Exception
{    for (int i = 0; i < rnd.nextInt(16) + 16; i++) {        byte expectedStatus = (byte) (rnd.nextInt(6) + 0x21);        while (expectedStatus == CrawlDatum.STATUS_FETCH_RETRY || expectedStatus == CrawlDatum.STATUS_FETCH_NOTMODIFIED) {                        expectedStatus = (byte) (rnd.nextInt(6) + 0x21);        }        byte randomStatus = (byte) (rnd.nextInt(6) + 0x21);        int rounds = rnd.nextInt(16) + 32;        boolean withRedirects = rnd.nextBoolean();        byte resultStatus = executeSequence(randomStatus, expectedStatus, rounds, withRedirects);        Assert.assertEquals("Expected status = " + CrawlDatum.getStatusName(expectedStatus) + ", but got " + CrawlDatum.getStatusName(resultStatus) + " when merging " + rounds + " segments" + (withRedirects ? " with redirects" : ""), expectedStatus, resultStatus);    }}
public void nutch_f3272_0() throws Exception
{    Assert.assertEquals(Byte.valueOf(CrawlDatum.STATUS_FETCH_SUCCESS), Byte.valueOf(executeSequence(CrawlDatum.STATUS_FETCH_GONE, CrawlDatum.STATUS_FETCH_SUCCESS, 128, true)));}
public void nutch_f3273_0() throws Exception
{        Path testDir = new Path(conf.get("hadoop.tmp.dir"), "merge-" + System.currentTimeMillis());    Path segment1 = new Path(testDir, "00001");    Path segment2 = new Path(testDir, "00002");    Path segment3 = new Path(testDir, "00003");    createSegment(segment1, CrawlDatum.STATUS_FETCH_GONE, false);    createSegment(segment2, CrawlDatum.STATUS_FETCH_GONE, true);    createSegment(segment3, CrawlDatum.STATUS_FETCH_SUCCESS, false);        Path mergedSegment = merge(testDir, new Path[] { segment1, segment2, segment3 });    Byte status = Byte.valueOf(status = checkMergedSegment(testDir, mergedSegment));    Assert.assertEquals(Byte.valueOf(CrawlDatum.STATUS_FETCH_SUCCESS), status);}
public void nutch_f3274_0() throws Exception
{        Path testDir = new Path(conf.get("hadoop.tmp.dir"), "merge-" + System.currentTimeMillis());    Path segment = new Path(testDir, "00001");    createSegment(segment, CrawlDatum.STATUS_FETCH_SUCCESS, true, true);        Path mergedSegment = merge(testDir, new Path[] { segment });    Byte status = Byte.valueOf(status = checkMergedSegment(testDir, mergedSegment));    Assert.assertEquals(Byte.valueOf(CrawlDatum.STATUS_FETCH_SUCCESS), status);}
public void nutch_f3275_0() throws Exception
{        Path testDir = new Path(conf.get("hadoop.tmp.dir"), "merge-" + System.currentTimeMillis());    Path segment1 = new Path(testDir, "00001");    Path segment2 = new Path(testDir, "00002");    createSegment(segment1, CrawlDatum.STATUS_FETCH_SUCCESS, false);    createSegment(segment2, CrawlDatum.STATUS_FETCH_SUCCESS, true);        Path mergedSegment = merge(testDir, new Path[] { segment1, segment2 });    Byte status = Byte.valueOf(status = checkMergedSegment(testDir, mergedSegment));    Assert.assertEquals(Byte.valueOf(CrawlDatum.STATUS_FETCH_SUCCESS), status);}
protected byte nutch_f3276_0(byte firstStatus, byte lastStatus, int rounds, boolean redirect) throws Exception
{        Path testDir = new Path(conf.get("hadoop.tmp.dir"), "merge-" + System.currentTimeMillis());        DecimalFormat df = new DecimalFormat("0000000");        Path[] segmentPaths = new Path[rounds];    for (int i = 0; i < rounds; i++) {        String segmentName = df.format(i);        segmentPaths[i] = new Path(testDir, segmentName);    }        createSegment(segmentPaths[0], firstStatus, false);        for (int i = 1; i < rounds - 1; i++) {                byte status = (byte) (rnd.nextInt(6) + 0x21);                boolean addRedirect = redirect ? rnd.nextBoolean() : false;                        boolean addFetch = addRedirect ? rnd.nextBoolean() : true;        createSegment(segmentPaths[i], status, addFetch, addRedirect);    }            createSegment(segmentPaths[rounds - 1], lastStatus, true, redirect ? rnd.nextBoolean() : false);        Path mergedSegment = merge(testDir, segmentPaths);        return checkMergedSegment(testDir, mergedSegment);}
protected byte nutch_f3277_1(Path testDir, Path mergedSegment) throws Exception
{        MapFile.Reader[] readers = MapFileOutputFormat.getReaders(new Path(mergedSegment, CrawlDatum.FETCH_DIR_NAME), conf);    Text key = new Text();    CrawlDatum value = new CrawlDatum();    byte finalStatus = 0x0;    for (MapFile.Reader reader : readers) {        while (reader.next(key, value)) {                                    if (CrawlDatum.hasFetchStatus(value) && key.toString().equals("http://nutch.apache.org/")) {                finalStatus = value.getStatus();            }        }                reader.close();    }        fs.delete(testDir, true);            return finalStatus;}
protected Path nutch_f3278_0(Path testDir, Path[] segments) throws Exception
{        Path out = new Path(testDir, "out");        SegmentMerger merger = new SegmentMerger(conf);    merger.merge(out, segments, false, false, -1);    FileStatus[] stats = fs.listStatus(out);    Assert.assertEquals(1, stats.length);    return stats[0].getPath();}
protected void nutch_f3279_0(Path segment, byte status, boolean redirect) throws Exception
{    if (redirect) {        createSegment(segment, status, false, true);    } else {        createSegment(segment, status, true, false);    }}
protected void nutch_f3280_1(Path segment, byte status, boolean fetch, boolean redirect) throws Exception
{            String url = "http://nutch.apache.org/";        String redirectUrl = "http://nutch.apache.org/i_redirect_to_the_root/";        CrawlDatum value = new CrawlDatum();        Path crawlFetchPath = new Path(new Path(segment, CrawlDatum.FETCH_DIR_NAME), "part-00000");        Option wKeyOpt = MapFile.Writer.keyClass(Text.class);    org.apache.hadoop.io.SequenceFile.Writer.Option wValueOpt = SequenceFile.Writer.valueClass(CrawlDatum.class);    MapFile.Writer writer = new MapFile.Writer(conf, crawlFetchPath, wKeyOpt, wValueOpt);        if (redirect) {                        value = new CrawlDatum();        value.setStatus(CrawlDatum.STATUS_LINKED);        writer.append(new Text(url), value);    }        if (fetch) {                        value.setStatus(status);                writer.append(new Text(url), value);    }        if (redirect) {                        value.setStatus(CrawlDatum.STATUS_FETCH_REDIR_TEMP);        writer.append(new Text(redirectUrl), value);    }        writer.close();}
public void nutch_f3281_1()
{    boolean isRunning = false;    for (int i = 0; i < port.length; i++) {        try {            startServer(port[i]);            isRunning = true;            break;        } catch (Exception e) {                    }    }    if (!isRunning) {            } else {                WebClient client = WebClient.create(ENDPOINT_ADDRESS + server.getPort());        @SuppressWarnings("unused")        Response response = client.path("admin").get();                response = client.path("stop").get();        }}
private void nutch_f3282_0(int port) throws Exception
{    NutchServer.setPort(port);    NutchServer.startServer();}
public void nutch_f3283_0(String target, HttpServletRequest req, HttpServletResponse res, int dispatch) throws IOException, ServletException
{    Request base_request = (req instanceof Request) ? (Request) req : HttpConnection.getCurrentConnection().getRequest();    res.addHeader("X-TestbedHandlers", this.getClass().getSimpleName());    handle(base_request, res, target, dispatch);}
public void nutch_f3284_0(HttpServletResponse res, String name, String value)
{    name = "X-" + this.getClass().getSimpleName() + "-" + name;    res.addHeader(name, value);}
public void nutch_f3285_0(Request req, HttpServletResponse res, String target, int dispatch) throws IOException, ServletException
{    try {        int del = random ? r.nextInt(delay) : delay;        Thread.sleep(del);        addMyHeader(res, "Delay", String.valueOf(del));    } catch (Exception e) {    }}
public void nutch_f3286_0(Request req, HttpServletResponse res, String target, int dispatch) throws IOException, ServletException
{    HttpURI u = req.getUri();    String uri = u.toString();        addMyHeader(res, "URI", uri);        req.setHandled(true);    res.addHeader("X-Handled-By", getClass().getSimpleName());    if (uri.endsWith("/robots.txt")) {        return;    }    res.setContentType("text/html");    try {        OutputStream os = res.getOutputStream();        byte[] bytes = testA.getBytes("UTF-8");        os.write(bytes);                String p = "<p>URI: " + uri + "</p>\r\n";        os.write(p.getBytes());                String base;        if (u.getPath().length() > 5) {            base = u.getPath().substring(0, u.getPath().length() - 5);        } else {            base = u.getPath();        }        String prefix = u.getScheme() + "://" + u.getHost();        if (u.getPort() != 80 && u.getPort() != -1)            base += ":" + u.getPort();        if (!base.startsWith("/"))            prefix += "/";        prefix = prefix + base;        for (int i = 0; i < 10; i++) {            String link = "<p><a href='" + prefix;            if (!prefix.endsWith("/")) {                link += "/";            }            link += i + ".html'>outlink " + i + "</a></p>\r\n";            os.write(link.getBytes());        }                for (int i = 0; i < 5; i++) {                        int h = r.nextInt(1000000);            String link = "<p><a href='http://www.fake-" + h + ".com/'>fake host " + h + "</a></p>\r\n";            os.write(link.getBytes());        }                String link = "<p><a href='" + u.getScheme() + "://" + u.getHost();        if (u.getPort() != 80 && u.getPort() != -1)            link += ":" + u.getPort();        link += "/'>site " + u.getHost() + "</a></p>\r\n";        os.write(link.getBytes());        os.write(testB.getBytes());        res.flushBuffer();    } catch (IOException ioe) {    }}
public void nutch_f3287_1(Request req, HttpServletResponse res, String target, int dispatch) throws IOException, ServletException
{    }
public void nutch_f3288_0(ServletRequest req, ServletResponse res, FilterChain chain) throws IOException, ServletException
{    ((HttpServletResponse) res).addHeader("X-Handled-By", "AsyncProxyHandler");    ((HttpServletResponse) res).addHeader("X-TestbedHandlers", "AsyncProxyHandler");    try {        chain.doFilter(req, res);    } catch (Throwable e) {        ((HttpServletResponse) res).sendError(HttpServletResponse.SC_BAD_REQUEST, e.toString());    }}
public void nutch_f3289_0(FilterConfig arg0) throws ServletException
{}
public void nutch_f3290_0(Request req, HttpServletResponse res, String target, int dispatch) throws IOException, ServletException
{        req.setHandled(true);    res.addHeader("X-Handled-By", getClass().getSimpleName());    addMyHeader(res, "URI", req.getUri().toString());    res.sendError(HttpServletResponse.SC_NOT_FOUND, "Not found: " + req.getUri().toString());}
public static void nutch_f3291_1(String[] args) throws Exception
{    if (args.length == 0) {        System.err.println("TestbedProxy [-seg <segment_name> | -segdir <segments>] [-port <nnn>] [-forward] [-fake] [-delay nnn] [-debug]");        System.err.println("-seg <segment_name>\tpath to a single segment (can be specified multiple times)");        System.err.println("-segdir <segments>\tpath to a parent directory of multiple segments (as above)");        System.err.println("-port <nnn>\trun the proxy on port <nnn> (special permissions may be needed for ports < 1024)");        System.err.println("-forward\tif specified, requests to all unknown urls will be passed to");        System.err.println("\t\toriginal servers. If false (default) unknown urls generate 404 Not Found.");        System.err.println("-delay\tdelay every response by nnn seconds. If delay is negative use a random value up to nnn");        System.err.println("-fake\tif specified, requests to all unknown urls will succeed with fake content");        System.exit(-1);    }    Configuration conf = NutchConfiguration.create();    int port = conf.getInt("segment.proxy.port", 8181);    boolean forward = false;    boolean fake = false;    boolean delay = false;    boolean debug = false;    int delayVal = 0;    HashSet<Path> segs = new HashSet<Path>();    for (int i = 0; i < args.length; i++) {        if (args[i].equals("-segdir")) {            FileSystem fs = FileSystem.get(conf);            FileStatus[] fstats = fs.listStatus(new Path(args[++i]));            Path[] paths = HadoopFSUtil.getPaths(fstats);            segs.addAll(Arrays.asList(paths));        } else if (args[i].equals("-port")) {            port = Integer.parseInt(args[++i]);        } else if (args[i].equals("-forward")) {            forward = true;        } else if (args[i].equals("-delay")) {            delay = true;            delayVal = Integer.parseInt(args[++i]);        } else if (args[i].equals("-fake")) {            fake = true;        } else if (args[i].equals("-debug")) {            debug = true;        } else if (args[i].equals("-seg")) {            segs.add(new Path(args[++i]));        } else {                        System.exit(-1);        }    }        Server server = new Server();    SocketConnector connector = new SocketConnector();    connector.setPort(port);    connector.setResolveNames(false);    server.addConnector(connector);        HandlerList list = new HandlerList();    server.addHandler(list);    if (debug) {                list.addHandler(new LogDebugHandler());    }    if (delay) {                list.addHandler(new DelayHandler(delayVal));    }                Iterator<Path> it = segs.iterator();    while (it.hasNext()) {        Path p = it.next();        try {            SegmentHandler segment = new SegmentHandler(conf, p);            list.addHandler(segment);                    } catch (Exception e) {                    }    }    if (forward) {                ServletHandler servlets = new ServletHandler();        servlets.addServletWithMapping(AsyncProxyServlet.class, "/*");        servlets.addFilterWithMapping(LogDebugHandler.class, "/*", Handler.ALL);        list.addHandler(servlets);    }    if (fake) {                list.addHandler(new FakeHandler());    }    list.addHandler(new NotFoundHandler());        server.start();    server.join();}
public boolean nutch_f3292_0(Path p)
{    return p.getName().startsWith("part-");}
public CrawlDatum nutch_f3293_0(Text url) throws IOException
{    synchronized (crawlLock) {        if (crawl == null)            crawl = getReaders(CrawlDatum.FETCH_DIR_NAME);    }    return (CrawlDatum) getEntry(crawl, url, new CrawlDatum());}
public Content nutch_f3294_0(Text url) throws IOException
{    synchronized (cLock) {        if (content == null)            content = getReaders(Content.DIR_NAME);    }    return (Content) getEntry(content, url, new Content());}
private MapFile.Reader[] nutch_f3295_0(String subDir) throws IOException
{    Path dir = new Path(segmentDir, subDir);    FileSystem fs = dir.getFileSystem(conf);    Path[] names = FileUtil.stat2Paths(fs.listStatus(dir, SegmentPathFilter.INSTANCE));        Arrays.sort(names);    MapFile.Reader[] parts = new MapFile.Reader[names.length];    for (int i = 0; i < names.length; i++) {        parts[i] = new MapFile.Reader(names[i], conf);    }    return parts;}
private Writable nutch_f3296_0(MapFile.Reader[] readers, Text url, Writable entry) throws IOException
{    return MapFileOutputFormat.getEntry(readers, PARTITIONER, url, entry);}
public void nutch_f3297_0() throws IOException
{    if (content != null) {        closeReaders(content);    }    if (parseText != null) {        closeReaders(parseText);    }    if (parseData != null) {        closeReaders(parseData);    }    if (crawl != null) {        closeReaders(crawl);    }}
private void nutch_f3298_0(MapFile.Reader[] readers) throws IOException
{    for (int i = 0; i < readers.length; i++) {        readers[i].close();    }}
public void nutch_f3299_1(Request req, HttpServletResponse res, String target, int dispatch) throws IOException, ServletException
{    try {        String uri = req.getUri().toString();                addMyHeader(res, "URI", uri);        Text url = new Text(uri.toString());        CrawlDatum cd = seg.getCrawlDatum(url);        if (cd != null) {            addMyHeader(res, "Res", "found");                        ProtocolStatus ps = (ProtocolStatus) cd.getMetaData().get(Nutch.WRITABLE_PROTO_STATUS_KEY);            if (ps != null) {                Integer TrCode = protoCodes.get(ps.getCode());                if (TrCode != null) {                    res.setStatus(TrCode.intValue());                } else {                    res.setStatus(HttpServletResponse.SC_OK);                }                addMyHeader(res, "ProtocolStatus", ps.toString());            } else {                res.setStatus(HttpServletResponse.SC_OK);            }            Content c = seg.getContent(url);            if (c == null) {                                req.setHandled(true);                res.addHeader("X-Handled-By", getClass().getSimpleName());                return;            }            byte[] data = c.getContent();                        Metadata meta = c.getMetadata();            String[] names = meta.names();                        for (int i = 0; i < names.length; i++) {                boolean my = true;                char ch = names[i].charAt(0);                if (Character.isLetter(ch) && Character.isUpperCase(ch)) {                                        my = false;                }                String[] values = meta.getValues(names[i]);                for (int k = 0; k < values.length; k++) {                    if (my) {                        addMyHeader(res, names[i], values[k]);                    } else {                        res.addHeader(names[i], values[k]);                    }                }            }            req.setHandled(true);            res.addHeader("X-Handled-By", getClass().getSimpleName());            res.setContentType(meta.get(Metadata.CONTENT_TYPE));            res.setContentLength(data.length);            OutputStream os = res.getOutputStream();            os.write(data, 0, data.length);            res.flushBuffer();        } else {            addMyHeader(res, "Res", "not found");                    }    } catch (Exception e) {        e.printStackTrace();                addMyHeader(res, "Res", "Exception: " + StringUtils.stringifyException(e));    }}
public void nutch_f3300_0() throws Exception
{    File sampleSegmentDir = new File(System.getProperty("test.build.data", "."), "test-segments");    File tempDir = Files.createTempDirectory("temp").toFile();    String[] crawledFiles = { "c463a4381eb837f9f5d45978cfbde79e_.html", "a974b8d74f7779ab6c6f90b9b279467e_.html", "6bc6497314656a3129732efd708e9f96_.html", "6e88c40abe26cad0a726102997aed048_.html", "5cafdd88f4e9cf3f0cd4c298c6873358_apachecon-europe.html", "932dc10a76e894a2baa8ea4086ad72a8_apachecon-north-america.html", "8540187d75b9cd405b8fa97d665f9f90_.html", "e501bc976c8693b4d28a55b79c390a32_.html", "6add662f9f5758b7d75eec5cfa1f340b_.html", "d4f20df3c37033dc516067ee1f424e4e_.html", "d7b8fa9a02cdc95546030d04be4a98f3_solr.html", "3cbe876e3a8e7a397811de3bb6a945cd_.html", "5b987dde0da79d7f2e3f22b46437f514_bot.html", "3d742820d9a701a1f02e10d5bf5ae633_credits.html", "693673f3c73d04a26276effdea69b7ee_downloads.html", "4f7e3469dafabb4c3b87b00531f81aa4_index.html", "15c5330675be8a69995aab18ff9859e0_javadoc.html", "bc624e1b49e29870ef095819bb0e977a_mailing_lists.html", "a7d66b68754c3665c66e62225255e3fd_version_control.html", "32fb7fe362e1a0d8a1b15addf2a00bdc_1.9-rel", "54ab3db10fe7b26415a04e21045125a8_1zE.html", "1012a41c08092c40340598bd8ee0bfa6_PGa.html", "c830cfc5c28bed10e69d5b83e9c1bcdc_nutch_2.3", "687d915dc264a77f35c61ba841936730_oHY.html", "2bf1afb650010128b4cf4afe677db3c5_1pav9xl.html", "550cab79e14110bbee61c36c61c830b0_1pbE15n.html", "664ff07b46520cc1414494ae49da91f6_.html", "04223714e648a6a43d7c8af8b095f733_.html", "3c8ccb865cd72cca06635d74c7f2f3c4_.html", "90fe47b28716a2230c5122c83f0b8562_Becoming_A_Nutch_Developer.html", "ac0fefe70007d40644e2b8bd5da3c305_FAQ.html", "bc9bc7f11c1262e8924032ab1c7ce112_NutchPropertiesCompleteList.html", "78d04611985e7375b441e478fa36f610_.html", "64adaebadd44e487a8b58894e979dc70_CHANGES.txt", "a48e9c2659b703fdea3ad332877708d8_.html", "159d66d679dd4442d2d8ffe6a83b2912_sponsorship.html", "66f1ce6872c9195c665fc8bdde95f6dc_thanks.html", "ef7ee7e929a048c4a119af78492095b3_.html", "e4251896a982c2b2b68678b5c9c57f4d_.html", "5384764a16fab767ebcbc17d87758a24_.html", "a6ba75a218ef2a09d189cb7dffcecc0f_.html", "f2fa63bd7a3aca63841eed4cd10fb519_SolrCloud.html", "f8de0fbda874e1a140f1b07dcebab374_NUTCH-1047.html", "9c120e94f52d690e9cfd044c34134649_NUTCH-1591.html", "7dd70378379aa452279ce9200d0a5fed_NUTCH-841.html", "ddf78b1fe5c268d59fd62bc745815b92_.html", "401c9f04887dbbf8d29ad52841b8bdb3_ApacheNutch.html", "8f984e2d3c2ba68d1695288f1738deaf_Nutch.html", "c2ef09a95a956207cea073a515172be2_FrontPage.html", "90d9b76e8eabdab1cbcc29bea437c7ae_NutchRESTAPI.html" };    CommonCrawlDataDumper dumper = new CommonCrawlDataDumper(new CommonCrawlConfig());    dumper.dump(tempDir, sampleSegmentDir, null, false, null, false, "", false);    Collection<File> tempFiles = FileUtils.listFiles(tempDir, FileFilterUtils.fileFileFilter(), FileFilterUtils.directoryFileFilter());    for (String expectedFileName : crawledFiles) {        assertTrue("Missed file " + expectedFileName + " in dump", hasFile(expectedFileName, tempFiles));    }}
private boolean nutch_f3301_0(String fileName, Collection<File> files)
{    for (File f : files) {        if (f.getName().equals(fileName)) {            return true;        }    }    return false;}
public void nutch_f3302_0() throws Exception
{    String testUrl = "http://apache.org";    String result = DumpFileUtil.getUrlMD5(testUrl);    assertEquals("991e599262e04ea2ec76b6c5aed499a7", result);}
public void nutch_f3303_0() throws Exception
{    String testUrl = "http://apache.org";    String basePath = "/tmp";    String fullDir = DumpFileUtil.createTwoLevelsDirectory(basePath, DumpFileUtil.getUrlMD5(testUrl));    assertEquals("/tmp/96/ea", fullDir);    String basePath2 = "/this/path/is/not/existed/just/for/testing";    String fullDir2 = DumpFileUtil.createTwoLevelsDirectory(basePath2, DumpFileUtil.getUrlMD5(testUrl));    assertNull(fullDir2);}
public void nutch_f3304_0() throws Exception
{    String testUrl = "http://apache.org";    String baseName = "test";    String extension = "html";    String fullDir = DumpFileUtil.createFileName(DumpFileUtil.getUrlMD5(testUrl), baseName, extension);    assertEquals("991e599262e04ea2ec76b6c5aed499a7_test.html", fullDir);    String tooLongBaseName = "testtesttesttesttesttesttesttesttesttesttesttesttesttesttesttesttesttest";    String fullDir2 = DumpFileUtil.createFileName(DumpFileUtil.getUrlMD5(testUrl), tooLongBaseName, extension);    assertEquals("991e599262e04ea2ec76b6c5aed499a7_testtesttesttesttesttesttesttest.html", fullDir2);    String tooLongExtension = "testtesttesttesttesttesttesttesttesttesttesttesttesttesttesttesttesttest";    String fullDir3 = DumpFileUtil.createFileName(DumpFileUtil.getUrlMD5(testUrl), baseName, tooLongExtension);    assertEquals("991e599262e04ea2ec76b6c5aed499a7_test.testt", fullDir3);}
public void nutch_f3305_0()
{        conf.setInt(EncodingDetector.MIN_CONFIDENCE_KEY, -1);    Metadata metadata = new Metadata();    EncodingDetector detector;    Content content;    String encoding;    content = new Content("http://www.example.com", "http://www.example.com/", contentInOctets, "text/plain", metadata, conf);    detector = new EncodingDetector(conf);    detector.autoDetectClues(content, true);    encoding = detector.guessEncoding(content, "windows-1252");        Assert.assertEquals("windows-1252", encoding.toLowerCase());    metadata.clear();    metadata.set(Response.CONTENT_TYPE, "text/plain; charset=UTF-16");    content = new Content("http://www.example.com", "http://www.example.com/", contentInOctets, "text/plain", metadata, conf);    detector = new EncodingDetector(conf);    detector.autoDetectClues(content, true);    encoding = detector.guessEncoding(content, "windows-1252");    Assert.assertEquals("utf-16", encoding.toLowerCase());    metadata.clear();    content = new Content("http://www.example.com", "http://www.example.com/", contentInOctets, "text/plain", metadata, conf);    detector = new EncodingDetector(conf);    detector.autoDetectClues(content, true);    detector.addClue("windows-1254", "sniffed");    encoding = detector.guessEncoding(content, "windows-1252");    Assert.assertEquals("windows-1254", encoding.toLowerCase());        conf.setInt(EncodingDetector.MIN_CONFIDENCE_KEY, 50);    metadata.clear();    metadata.set(Response.CONTENT_TYPE, "text/plain; charset=UTF-16");    content = new Content("http://www.example.com", "http://www.example.com/", contentInOctets, "text/plain", metadata, conf);    detector = new EncodingDetector(conf);    detector.autoDetectClues(content, true);    detector.addClue("utf-32", "sniffed");    encoding = detector.guessEncoding(content, "windows-1252");    Assert.assertEquals("utf-8", encoding.toLowerCase());}
public void nutch_f3306_0()
{    byte[] testBytes = SHORT_TEST_STRING.getBytes();    testZipUnzip(testBytes);    testBytes = LONGER_TEST_STRING.getBytes();    testZipUnzip(testBytes);    testBytes = WEBPAGE.getBytes();    testZipUnzip(testBytes);}
public void nutch_f3307_0()
{    byte[] testBytes = SHORT_TEST_STRING.getBytes();    testZipUnzipBestEffort(testBytes);    testBytes = LONGER_TEST_STRING.getBytes();    testZipUnzipBestEffort(testBytes);    testBytes = WEBPAGE.getBytes();    testZipUnzipBestEffort(testBytes);}
public void nutch_f3308_0()
{    byte[] testBytes = SHORT_TEST_STRING.getBytes();    testTruncation(testBytes);    testBytes = LONGER_TEST_STRING.getBytes();    testTruncation(testBytes);    testBytes = WEBPAGE.getBytes();    testTruncation(testBytes);}
public void nutch_f3309_0()
{    byte[] testBytes = SHORT_TEST_STRING.getBytes();    testLimit(testBytes);    testBytes = LONGER_TEST_STRING.getBytes();    testLimit(testBytes);    testBytes = WEBPAGE.getBytes();    testLimit(testBytes);}
public void nutch_f3310_0(byte[] origBytes)
{    byte[] compressedBytes = GZIPUtils.zip(origBytes);    Assert.assertTrue("compressed array is not smaller!", compressedBytes.length < origBytes.length);    byte[] uncompressedBytes = null;    try {        uncompressedBytes = GZIPUtils.unzip(compressedBytes);    } catch (IOException e) {        e.printStackTrace();        Assert.assertTrue("caught exception '" + e + "' during unzip()", false);    }    Assert.assertTrue("uncompressedBytes is wrong size", uncompressedBytes.length == origBytes.length);    for (int i = 0; i < origBytes.length; i++) if (origBytes[i] != uncompressedBytes[i])        Assert.assertTrue("uncompressedBytes does not match origBytes", false);}
public void nutch_f3311_0(byte[] origBytes)
{    byte[] compressedBytes = GZIPUtils.zip(origBytes);    Assert.assertTrue("compressed array is not smaller!", compressedBytes.length < origBytes.length);    byte[] uncompressedBytes = GZIPUtils.unzipBestEffort(compressedBytes);    Assert.assertTrue("uncompressedBytes is wrong size", uncompressedBytes.length == origBytes.length);    for (int i = 0; i < origBytes.length; i++) if (origBytes[i] != uncompressedBytes[i])        Assert.assertTrue("uncompressedBytes does not match origBytes", false);}
public void nutch_f3312_0(byte[] origBytes)
{    byte[] compressedBytes = GZIPUtils.zip(origBytes);    System.out.println("original data has len " + origBytes.length);    System.out.println("compressed data has len " + compressedBytes.length);    for (int i = compressedBytes.length; i >= 0; i--) {        byte[] truncCompressed = new byte[i];        for (int j = 0; j < i; j++) truncCompressed[j] = compressedBytes[j];        byte[] trunc = GZIPUtils.unzipBestEffort(truncCompressed);        if (trunc == null) {            System.out.println("truncated to len " + i + ", trunc is null");        } else {            System.out.println("truncated to len " + i + ", trunc.length=  " + trunc.length);            for (int j = 0; j < trunc.length; j++) if (trunc[j] != origBytes[j])                Assert.assertTrue("truncated/uncompressed array differs at pos " + j + " (compressed data had been truncated to len " + i + ")", false);        }    }}
public void nutch_f3313_0(byte[] origBytes)
{    byte[] compressedBytes = GZIPUtils.zip(origBytes);    Assert.assertTrue("compressed array is not smaller!", compressedBytes.length < origBytes.length);    for (int i = 0; i < origBytes.length; i++) {        byte[] uncompressedBytes = GZIPUtils.unzipBestEffort(compressedBytes, i);        Assert.assertTrue("uncompressedBytes is wrong size", uncompressedBytes.length == i);        for (int j = 0; j < i; j++) if (origBytes[j] != uncompressedBytes[j])            Assert.assertTrue("uncompressedBytes does not match origBytes", false);    }}
private String nutch_f3314_0(String url, File file, String contentType, boolean useMagic) throws IOException
{    return getMimeType(url, Files.toByteArray(file), contentType, useMagic);}
private String nutch_f3315_0(String url, byte[] bytes, String contentType, boolean useMagic)
{    Configuration conf = NutchConfiguration.create();    conf.setBoolean("mime.type.magic", useMagic);    MimeUtil mimeUtil = new MimeUtil(conf);    return mimeUtil.autoResolveContentType(contentType, url, bytes);}
public void nutch_f3316_0()
{    for (String[] testPage : textBasedFormats) {        String mimeType = getMimeType(urlPrefix, testPage[3].getBytes(defaultCharset), testPage[2], true);        assertEquals("", testPage[0], mimeType);    }}
public void nutch_f3317_0()
{    for (String[] testPage : textBasedFormats) {        if (testPage.length > 4 && "requires-mime-magic".equals(testPage[4])) {            continue;        }        String mimeType = getMimeType(urlPrefix + testPage[1], testPage[3].getBytes(defaultCharset), testPage[2], false);        assertEquals("", testPage[0], mimeType);    }}
public void nutch_f3318_0()
{    for (String[] testPage : textBasedFormats) {        String mimeType = getMimeType(urlPrefix, testPage[3].getBytes(defaultCharset), "", true);        assertEquals("", testPage[0], mimeType);    }}
public void nutch_f3319_0() throws IOException
{    for (String[] testPage : binaryFiles) {        File dataFile = new File(sampleDir, testPage[1]);        String mimeType = getMimeType(urlPrefix + testPage[1], dataFile, testPage[2], false);        assertEquals("", testPage[0], mimeType);    }}
public void nutch_f3320_0() throws Exception
{    ULCONTENT[0] = "crawl several billion pages per month";    ULCONTENT[1] = "maintain an index of these pages";    ULCONTENT[2] = "search that index up to 1000 times per second";    ULCONTENT[3] = "operate at minimal cost";}
public void nutch_f3321_0()
{    DOMParser parser = new DOMParser();    try {        parser.setFeature("http://xml.org/sax/features/validation", false);        parser.setFeature("http://apache.org/xml/features/nonvalidating/load-external-dtd", false);        parser.parse(new InputSource(new ByteArrayInputStream(WEBPAGE.getBytes())));    } catch (Exception e) {        e.printStackTrace();    }    StringBuffer sb = new StringBuffer();    NodeWalker walker = new NodeWalker(parser.getDocument());    while (walker.hasNext()) {        Node currentNode = walker.nextNode();        short nodeType = currentNode.getNodeType();        if (nodeType == Node.TEXT_NODE) {            String text = currentNode.getNodeValue();            text = text.replaceAll("\\s+", " ");            sb.append(text);        }    }    Assert.assertTrue("UL Content can NOT be found in the node", findSomeUlContent(sb.toString()));    StringBuffer sbSkip = new StringBuffer();    NodeWalker walkerSkip = new NodeWalker(parser.getDocument());    while (walkerSkip.hasNext()) {        Node currentNode = walkerSkip.nextNode();        String nodeName = currentNode.getNodeName();        short nodeType = currentNode.getNodeType();        if ("ul".equalsIgnoreCase(nodeName)) {            walkerSkip.skipChildren();        }        if (nodeType == Node.TEXT_NODE) {            String text = currentNode.getNodeValue();            text = text.replaceAll("\\s+", " ");            sbSkip.append(text);        }    }    Assert.assertFalse("UL Content can be found in the node", findSomeUlContent(sbSkip.toString()));}
public boolean nutch_f3322_0(String str)
{    for (int i = 0; i < ULCONTENT.length; i++) {        if (str.contains(ULCONTENT[i]))            return true;    }    return false;}
private String nutch_f3323_0(int minLen, int maxLen)
{    int len = minLen + (int) (Math.random() * (maxLen - minLen));    char[] chars = new char[len];    for (int pos = 0; pos < len; pos++) {        chars[pos] = alphabet[(int) (Math.random() * alphabet.length)];    }    return new String(chars);}
public void nutch_f3324_0()
{    int numMatches = 0;    int numInputsTested = 0;    for (int round = 0; round < NUM_TEST_ROUNDS; round++) {                int numPrefixes = (int) (Math.random() * MAX_TEST_PREFIXES);        String[] prefixes = new String[numPrefixes];        for (int i = 0; i < numPrefixes; i++) {            prefixes[i] = makeRandString(0, MAX_PREFIX_LEN);        }        PrefixStringMatcher prematcher = new PrefixStringMatcher(prefixes);                for (int i = 0; i < NUM_TEST_INPUTS_PER_ROUND; i++) {            String input = makeRandString(0, MAX_INPUT_LEN);            boolean matches = false;            int longestMatch = -1;            int shortestMatch = -1;            for (int j = 0; j < prefixes.length; j++) {                if ((prefixes[j].length() > 0) && input.startsWith(prefixes[j])) {                    matches = true;                    int matchSize = prefixes[j].length();                    if (matchSize > longestMatch)                        longestMatch = matchSize;                    if ((matchSize < shortestMatch) || (shortestMatch == -1))                        shortestMatch = matchSize;                }            }            if (matches)                numMatches++;            numInputsTested++;            Assert.assertTrue("'" + input + "' should " + (matches ? "" : "not ") + "match!", matches == prematcher.matches(input));            if (matches) {                Assert.assertTrue(shortestMatch == prematcher.shortestMatch(input).length());                Assert.assertTrue(input.substring(0, shortestMatch).equals(prematcher.shortestMatch(input)));                Assert.assertTrue(longestMatch == prematcher.longestMatch(input).length());                Assert.assertTrue(input.substring(0, longestMatch).equals(prematcher.longestMatch(input)));            }        }    }    System.out.println("got " + numMatches + " matches out of " + numInputsTested + " tests");}
public void nutch_f3325_0()
{    String s = "my string";    String ps = StringUtil.rightPad(s, 0);    Assert.assertTrue(s.equals(ps));    ps = StringUtil.rightPad(s, 9);    Assert.assertTrue(s.equals(ps));    ps = StringUtil.rightPad(s, 10);    Assert.assertTrue((s + " ").equals(ps));    ps = StringUtil.rightPad(s, 15);    Assert.assertTrue((s + "      ").equals(ps));}
public void nutch_f3326_0()
{    String s = "my string";    String ps = StringUtil.leftPad(s, 0);    Assert.assertTrue(s.equals(ps));    ps = StringUtil.leftPad(s, 9);    Assert.assertTrue(s.equals(ps));    ps = StringUtil.leftPad(s, 10);    Assert.assertTrue((" " + s).equals(ps));    ps = StringUtil.leftPad(s, 15);    Assert.assertTrue(("      " + s).equals(ps));}
private String nutch_f3327_0(int minLen, int maxLen)
{    int len = minLen + (int) (Math.random() * (maxLen - minLen));    char[] chars = new char[len];    for (int pos = 0; pos < len; pos++) {        chars[pos] = alphabet[(int) (Math.random() * alphabet.length)];    }    return new String(chars);}
public void nutch_f3328_0()
{    int numMatches = 0;    int numInputsTested = 0;    for (int round = 0; round < NUM_TEST_ROUNDS; round++) {                int numSuffixes = (int) (Math.random() * MAX_TEST_SUFFIXES);        String[] suffixes = new String[numSuffixes];        for (int i = 0; i < numSuffixes; i++) {            suffixes[i] = makeRandString(0, MAX_SUFFIX_LEN);        }        SuffixStringMatcher sufmatcher = new SuffixStringMatcher(suffixes);                for (int i = 0; i < NUM_TEST_INPUTS_PER_ROUND; i++) {            String input = makeRandString(0, MAX_INPUT_LEN);            boolean matches = false;            int longestMatch = -1;            int shortestMatch = -1;            for (int j = 0; j < suffixes.length; j++) {                if ((suffixes[j].length() > 0) && input.endsWith(suffixes[j])) {                    matches = true;                    int matchSize = suffixes[j].length();                    if (matchSize > longestMatch)                        longestMatch = matchSize;                    if ((matchSize < shortestMatch) || (shortestMatch == -1))                        shortestMatch = matchSize;                }            }            if (matches)                numMatches++;            numInputsTested++;            Assert.assertTrue("'" + input + "' should " + (matches ? "" : "not ") + "match!", matches == sufmatcher.matches(input));            if (matches) {                Assert.assertTrue(shortestMatch == sufmatcher.shortestMatch(input).length());                Assert.assertTrue(input.substring(input.length() - shortestMatch).equals(sufmatcher.shortestMatch(input)));                Assert.assertTrue(longestMatch == sufmatcher.longestMatch(input).length());                Assert.assertTrue(input.substring(input.length() - longestMatch).equals(sufmatcher.longestMatch(input)));            }        }    }    System.out.println("got " + numMatches + " matches out of " + numInputsTested + " tests");}
public void nutch_f3329_0() throws Exception
{    assertReverse(urlString1, reversedUrlString1);    assertReverse(urlString2, reversedUrlString2);    assertReverse(urlString3, reversedUrlString3);    assertReverse(urlString4, reversedUrlString4);    assertReverse(urlString5, reversedUrlString5);    assertReverse(urlString5, reversedUrlString5);    assertReverse(urlString6, reversedUrlString6);    assertReverse(urlString7, reversedUrlString7);}
public void nutch_f3330_0() throws Exception
{    assertUnreverse(reversedUrlString1, urlString1);    assertUnreverse(reversedUrlString2, urlString2);    assertUnreverse(reversedUrlString3, urlString3);    assertUnreverse(reversedUrlString4, urlString4);    assertUnreverse(reversedUrlString5, urlString5rev);    assertUnreverse(reversedUrlString6, urlString6);    assertUnreverse(reversedUrlString7, urlString7);}
private static void nutch_f3331_0(String url, String expectedReversedUrl) throws Exception
{    String reversed = TableUtil.reverseUrl(url);    assertEquals(expectedReversedUrl, reversed);}
private static void nutch_f3332_0(String reversedUrl, String expectedUrl)
{    String unreversed = TableUtil.unreverseUrl(reversedUrl);    assertEquals(expectedUrl, unreversed);}
public void nutch_f3333_0() throws Exception
{    URL url = null;    url = new URL("http://lucene.apache.org/nutch");    Assert.assertEquals("apache.org", URLUtil.getDomainName(url));    url = new URL("http://en.wikipedia.org/wiki/Java_coffee");    Assert.assertEquals("wikipedia.org", URLUtil.getDomainName(url));    url = new URL("http://140.211.11.130/foundation/contributing.html");    Assert.assertEquals("140.211.11.130", URLUtil.getDomainName(url));    url = new URL("http://www.example.co.uk:8080/index.html");    Assert.assertEquals("example.co.uk", URLUtil.getDomainName(url));    url = new URL("http://com");    Assert.assertEquals("com", URLUtil.getDomainName(url));    url = new URL("http://www.example.co.uk.com");    Assert.assertEquals("uk.com", URLUtil.getDomainName(url));        url = new URL("http://example.com.nn");    Assert.assertEquals("nn", URLUtil.getDomainName(url));    url = new URL("http://");    Assert.assertEquals("", URLUtil.getDomainName(url));    url = new URL("http://www.edu.tr.xyz");    Assert.assertEquals("xyz", URLUtil.getDomainName(url));    url = new URL("http://www.example.c.se");    Assert.assertEquals("example.c.se", URLUtil.getDomainName(url));        url = new URL("http://www.example.plc.co.im");    Assert.assertEquals("example.plc.co.im", URLUtil.getDomainName(url));        url = new URL("http://www.example.2000.hu");    Assert.assertEquals("example.2000.hu", URLUtil.getDomainName(url));        url = new URL("http://www.example..tw");    Assert.assertEquals("example..tw", URLUtil.getDomainName(url));}
public void nutch_f3334_0() throws Exception
{    URL url = null;    url = new URL("http://lucene.apache.org/nutch");    Assert.assertEquals("org", URLUtil.getDomainSuffix(url).getDomain());    url = new URL("http://140.211.11.130/foundation/contributing.html");    Assert.assertNull(URLUtil.getDomainSuffix(url));    url = new URL("http://www.example.co.uk:8080/index.html");    Assert.assertEquals("co.uk", URLUtil.getDomainSuffix(url).getDomain());    url = new URL("http://com");    Assert.assertEquals("com", URLUtil.getDomainSuffix(url).getDomain());    url = new URL("http://www.example.co.uk.com");    Assert.assertEquals("com", URLUtil.getDomainSuffix(url).getDomain());        url = new URL("http://example.com.nn");    Assert.assertNull(URLUtil.getDomainSuffix(url));    url = new URL("http://");    Assert.assertNull(URLUtil.getDomainSuffix(url));    url = new URL("http://www.edu.tr.xyz");    Assert.assertNull(URLUtil.getDomainSuffix(url));    url = new URL("http://subdomain.example.edu.tr");    Assert.assertEquals("edu.tr", URLUtil.getDomainSuffix(url).getDomain());    url = new URL("http://subdomain.example.presse.fr");    Assert.assertEquals("presse.fr", URLUtil.getDomainSuffix(url).getDomain());    url = new URL("http://subdomain.example.presse.tr");    Assert.assertEquals("tr", URLUtil.getDomainSuffix(url).getDomain());        url = new URL("http://www.example.plc.co.im");    Assert.assertEquals("plc.co.im", URLUtil.getDomainSuffix(url).getDomain());        url = new URL("http://www.example.2000.hu");    Assert.assertEquals("2000.hu", URLUtil.getDomainSuffix(url).getDomain());        url = new URL("http://www.example..tw");    Assert.assertEquals(".tw", URLUtil.getDomainSuffix(url).getDomain());}
public void nutch_f3335_0() throws Exception
{    URL url;    String[] segments;    url = new URL("http://subdomain.example.edu.tr");    segments = URLUtil.getHostSegments(url);    Assert.assertEquals("subdomain", segments[0]);    Assert.assertEquals("example", segments[1]);    Assert.assertEquals("edu", segments[2]);    Assert.assertEquals("tr", segments[3]);    url = new URL("http://");    segments = URLUtil.getHostSegments(url);    Assert.assertEquals(1, segments.length);    Assert.assertEquals("", segments[0]);    url = new URL("http://140.211.11.130/foundation/contributing.html");    segments = URLUtil.getHostSegments(url);    Assert.assertEquals(1, segments.length);    Assert.assertEquals("140.211.11.130", segments[0]);        url = new URL("http://www.example..tw");    segments = URLUtil.getHostSegments(url);    Assert.assertEquals("www", segments[0]);    Assert.assertEquals("example", segments[1]);    Assert.assertEquals("", segments[2]);    Assert.assertEquals("tw", segments[3]);}
public void nutch_f3336_0() throws Exception
{    String aDotCom = "http://www.a.com";    String bDotCom = "http://www.b.com";    String aSubDotCom = "http://www.news.a.com";    String aQStr = "http://www.a.com?y=1";    String aPath = "http://www.a.com/xyz/index.html";    String aPath2 = "http://www.a.com/abc/page.html";    String aPath3 = "http://www.news.a.com/abc/page.html";            Assert.assertEquals(bDotCom, URLUtil.chooseRepr(aDotCom, bDotCom, true));    Assert.assertEquals(bDotCom, URLUtil.chooseRepr(aDotCom, bDotCom, false));            Assert.assertEquals(aDotCom, URLUtil.chooseRepr(aDotCom, aQStr, false));    Assert.assertEquals(aDotCom, URLUtil.chooseRepr(aDotCom, aPath, false));            Assert.assertEquals(aDotCom, URLUtil.chooseRepr(aPath, aDotCom, false));            Assert.assertEquals(aPath2, URLUtil.chooseRepr(aPath, aPath2, false));            Assert.assertEquals(aDotCom, URLUtil.chooseRepr(aDotCom, aPath, true));            Assert.assertEquals(aDotCom, URLUtil.chooseRepr(aPath, aDotCom, true));                    Assert.assertEquals(aPath2, URLUtil.chooseRepr(aPath, aPath2, true));    Assert.assertEquals(aPath, URLUtil.chooseRepr(aPath, aPath3, true));            Assert.assertEquals(aDotCom, URLUtil.chooseRepr(aDotCom, aSubDotCom, true));}
public void nutch_f3337_0() throws Exception
{        URL u436 = new URL("http://a/b/c/d;p?q#f");    Assert.assertEquals("http://a/b/c/d;p?q#f", u436.toString());    URL abs = URLUtil.resolveURL(u436, "?y");    Assert.assertEquals("http://a/b/c/d;p?y", abs.toString());        URL u566 = new URL("http://www.fleurie.org/entreprise.asp");    abs = URLUtil.resolveURL(u566, "?id_entrep=111");    Assert.assertEquals("http://www.fleurie.org/entreprise.asp?id_entrep=111", abs.toString());    URL base = new URL(baseString);    Assert.assertEquals("base url parsing", baseString, base.toString());    for (int i = 0; i < targets.length; i++) {        URL u = URLUtil.resolveURL(base, targets[i][0]);        Assert.assertEquals(targets[i][1], targets[i][1], u.toString());    }}
public void nutch_f3338_0() throws Exception
{    Assert.assertEquals("http://www.evir.com", URLUtil.toUNICODE("http://www.xn--evir-zoa.com"));    Assert.assertEquals("http://uni-tbingen.de/", URLUtil.toUNICODE("http://xn--uni-tbingen-xhb.de/"));    Assert.assertEquals("http://www.medizin.uni-tbingen.de:8080/search.php?q=abc#p1", URLUtil.toUNICODE("http://www.medizin.xn--uni-tbingen-xhb.de:8080/search.php?q=abc#p1"));}
public void nutch_f3339_0() throws Exception
{    Assert.assertEquals("http://www.xn--evir-zoa.com", URLUtil.toASCII("http://www.evir.com"));    Assert.assertEquals("http://xn--uni-tbingen-xhb.de/", URLUtil.toASCII("http://uni-tbingen.de/"));    Assert.assertEquals("http://www.medizin.xn--uni-tbingen-xhb.de:8080/search.php?q=abc#p1", URLUtil.toASCII("http://www.medizin.uni-tbingen.de:8080/search.php?q=abc#p1"));}
public void nutch_f3340_0() throws Exception
{        Assert.assertEquals("file:/path/file.html", URLUtil.toASCII("file:/path/file.html"));    Assert.assertEquals("file:/path/file.html", URLUtil.toUNICODE("file:/path/file.html"));}
public static void nutch_f3341_0(Writable before) throws Exception
{    testWritable(before, null);}
public static void nutch_f3342_0(Writable before, Configuration conf) throws Exception
{    Assert.assertEquals(before, writeRead(before, conf));}
public static Writable nutch_f3343_0(Writable before, Configuration conf) throws Exception
{    DataOutputBuffer dob = new DataOutputBuffer();    before.write(dob);    DataInputBuffer dib = new DataInputBuffer();    dib.reset(dob.getData(), dob.getLength());    Writable after = (Writable) before.getClass().getConstructor().newInstance();    if (conf != null) {        ((Configurable) after).setConf(conf);    }    after.readFields(dib);    return after;}
